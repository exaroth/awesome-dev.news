{"id":"6W9","title":"HN","displayTitle":"HN","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":74,"items":[{"title":"Microsoft suspended the email account of an ICC prosecutor at The Hague","url":"https://www.nytimes.com/2025/06/20/technology/us-tech-europe-microsoft-trump-icc.html","date":1750507575,"author":"blinding-streak","guid":164301,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44336915"},{"title":"Scaling our observability platform by embracing wide events and replacing OTel","url":"https://clickhouse.com/blog/scaling-observability-beyond-100pb-wide-events-replacing-otel","date":1750497801,"author":"valyala","guid":164247,"unread":true,"content":"<blockquote><p> Our internal system grew from 19 PiB to 100 PB of uncompressed logs and from ~40 trillion to 500 trillion rows.</p><p> We absorbed a 20× surge in event volume using under 10% of the CPU previously needed.</p><p> The required parsing and marshalling of events in OpenTelemetry proved a bottleneck and didn’t scale - our custom pipeline addressed this.</p><p> ClickHouse-native observability UI for seamless exploration, correlation, and root-cause analysis with Lucene-like syntax.</p></blockquote><p>About a year ago, we<a href=\"https://clickhouse.com/blog/building-a-logging-platform-with-clickhouse-and-saving-millions-over-datadog\"> shared the story of LogHouse </a>- our internal logging platform built to monitor ClickHouse Cloud. At the time, it managed what felt like a massive 19 PiB of data. More than just solving our observability challenges, LogHouse also saved us millions by replacing an increasingly unsustainable Datadog bill. The response to that post was overwhelming. It was clear our experience resonated with others facing similar struggles with traditional observability vendors and underscored just how critical effective data management is at scale.</p><p>A year later, LogHouse has grown beyond anything we anticipated and is now storing over 100 petabytes of uncompressed data across nearly 500 trillion rows. That kind of scale forced a series of architectural changes, new tools, and hard-earned lessons that we felt were worth sharing - not least that OpenTelemetry (OTel) isn’t always the panacea of Observability (though we still love it), and that sometimes custom pipelines are essential.</p><p><strong>In our case, this shift enabled us to handle a 20x increase in event volume using less than 10% of the CPU for our most critical data source - a transformation with massive implications for cost and efficiency.</strong></p><p>Other parts of our stack have also changed, not least due to the <a href=\"https://clickhouse.com/blog/clickhouse-acquires-hyperdx-the-future-of-open-source-observability\">ClickHouse acquisition of HyperDX</a>. Not only did this give us a first-party ClickHouse-native UI, but it also led to the <a href=\"https://clickhouse.com/blog/clickstack-a-high-performance-oss-observability-stack-on-clickhouse\">creation of ClickStack </a>- an opinionated, end-to-end observability stack built around ClickHouse. With HyperDX, we’ve started transitioning away from our Grafana-based custom UI, moving toward a more integrated experience for exploration, correlation, and root cause analysis.</p><p>As more teams adopt ClickHouse for observability and realize just how much they can store and query affordably, we hope these insights prove as useful as our first post. If you’re curious about this journey, when and where OTel is appropriate, and how we scaled a log pipeline to 100PB…read on.</p><div><div><div><p>Interested in seeing how ClickHouse works on your data? Get started with ClickHouse Cloud in minutes and receive $300 in free credits.</p></div><a target=\"_self\" href=\"https://clickhouse.cloud/signUp?loc=blog-global-cta&amp;utm_source=clickhouse&amp;utm_medium=web&amp;utm_campaign=blog\"></a></div></div><p>Over the past year, our approach to observability has undergone a significant transformation. We've continued to leverage OpenTelemetry to gather general-purpose logs, but as our systems have scaled, we began to reach its limits. While OTel remains a valuable part of our toolkit, it couldn't fully deliver the performance and precision we needed for our most demanding workloads. This prompted us to develop purpose-built tools tailored to our critical systems and rethink where generic solutions truly fit. Along the way, we've broadened the range of data we collect and revamped how we present insights to engineers.</p><p>When we last wrote about LogHouse, we were proud to handle 19 PiB of uncompressed data across 37 trillion rows. Today, those numbers feel like a distant memory. LogHouse now stores over 100 petabytes of uncompressed data, representing nearly 500 trillion rows.\nHere's a quick look at the breakdown:</p><table><thead><tr></tr></thead><tbody></tbody></table><p>These numbers also tell a story. In our original post, 100% of our telemetry flowed through OpenTelemetry, with every log line collected via the same general-purpose pipeline. But as the scale and complexity of our data grew, so did the need for specialization.\nWhile our total volume has grown more than 5x, the breakdown reveals a deliberate shift in strategy: today, the vast majority of our data comes from “SysEx”, a new purpose-built exporter we developed to handle high-throughput, high-fidelity system logs from ClickHouse itself. This shift marks a turning point in how we think about observability pipelines - and brings us to our first key topic.</p><p>We hope the following helps comprehend the scale at which LogHouse operates.</p><p>Initially, we used OpenTelemetry (OTel) for all log collection. It was a great starting point and an established industry standard which allowed us to quickly establish a baseline where every pod in our Kubernetes environment shipped logs to ClickHouse. However, as we scaled, we identified two key reasons to build a specialized tool for shipping our core ClickHouse server telemetry.</p><p>First, while OTel capably captured the ClickHouse text log via stdout, this represents only a narrow slice of the telemetry ClickHouse exposes. Any ClickHouse expert knows that the real gold lies in its  - a rich, structured collection of logs, metrics, and operational insights that go far beyond what’s printed to standard output. These tables capture everything from query execution details to disk I/O and background task states, and unlike ephemeral logs, they can be retained indefinitely within a cluster. For both real-time debugging and historical analysis, this data is invaluable. We wanted all of it in LogHouse.</p><p>Second, the inefficiency of the OTel pipeline for this specific task became obvious as we scaled.</p><p>The data journey involved:</p><ol><li>A customer's ClickHouse instance writes logs as JSON to stdout.</li><li>The kubelet persists these logs in </li><li>An OTel collector collects these logs from the disk, parsing and marshalling the JSON into an in-memory representation.</li><li>The collector transforms these into the OTel log format - again an in-memory representation.</li><li>Finally, they are inserted back into another ClickHouse instance (LogHouse) over the native format (requiring another transformation within the ClickHouse Go client).</li></ol><blockquote><p>Note: The architecture described here is simplified. In reality, our OTel pipeline is more involved. Logs were first collected at the edge in JSON, converted into the OTel format, and sent over OTLP to a set of gateway instances. These gateways (also OTel collectors) performed additional processing before finally converting the data into ClickHouse’s native format for ingestion. Each step introduced overhead, latency, and further complexity.</p></blockquote><p>At our scale, this pipeline introduced two critical problems: inefficiency and data loss. First, we were burning substantial compute on repeated data transformations. Native ClickHouse types were being flattened into JSON, mapped into the OTel log format, and then re-ingested - only to be reinterpreted by ClickHouse on the other end. This not only wasted CPU cycles but also degraded the fidelity of the data.\nEven more importantly, we were hitting hard resource limits on the collectors themselves. Deployed as agents on each Kubernetes node, they were subject to strict CPU and memory constraints via standard Kubernetes limits. As traffic spiked, many collectors ran so hot they began dropping log lines outright - unable to keep up with the volume emitted by ClickHouse. We were losing data at the edge before it ever had a chance to reach LogHouse.\nWe found ourselves at a crossroads: either dramatically scale up the resource footprint of our OTel agents (and gateways) or rethink the entire ingestion model. We chose the latter.</p><blockquote><p>Note: To put the cost in perspective - handling 20 million rows per second through the OpenTelemetry pipeline without dropping events would require an estimated 8,000 CPU cores across agents and collectors. That’s an enormous footprint dedicated solely to log collection, making it clear that the general-purpose approach was unsustainable at our scale.</p></blockquote><p>Our solution was to develop the , or . This is a specialized tool designed to transfer data from one ClickHouse instance to another as efficiently as possible. We wanted to go directly from the system tables in a customer's pod to the tables in LogHouse, preserving native ClickHouse types and eliminating all intermediate conversions. This has the fantastic side benefit that any query our engineers use to troubleshoot a live instance can be trivially adapted to query historical data across our entire fleet in LogHouse, as the table schemas are identical, with the addition of some enrichment columns (such as the Pod Name, ClickHouse version, etc).</p><p>Firstly we should emphasize that SysEx performs a <strong>literal byte-for-byte copy</strong> of data from the source to the destination. This preserves full fidelity, eliminates unnecessary CPU overhead, and avoids the pitfalls of repeated marshalling.</p><p>The architecture is simple and powerful. We run a pool of SysEx scrapers connecting to our customer's ClickHouse instances. A hash ring assigns each customer pod to a specific scraper replica to distribute the load. These scrapers then run SELECT queries against the source pod's system tables and stream the data directly into LogHouse, without any deserialization. The scrapers simply coordinate and forward bytes between the source and destination.\nScraping system tables requires careful handling to ensure no data is missed due to buffer flushes. Fortunately, nearly all system table data is inherently time-series in nature. SysEx leverages this by querying within a sliding time window, deliberately trailing real time by a small buffer - typically five minutes. This delay allows for any internal buffers to flush, ensuring that when a scraper queries a node, all relevant rows for that time window are present and complete. This strategy has proven reliable and meets our internal SLAs for timely and complete event delivery to LogHouse.</p><p>SysEx is written in Go, like most of our infrastructure components for ClickHouse Cloud. Naturally, this raises a question for anyone familiar with the Go ClickHouse client: how do we avoid the built-in marshalling and unmarshalling of data when reading from and writing to ClickHouse? By default, the client converts data into Go-native types, which would defeat the purpose of a byte-for-byte copy. To solve this, we <a href=\"https://github.com/ClickHouse/clickhouse-go/pull/1233\">contributed improvements</a> to the Go client that allow us to <strong>bypass internal marshalling entirely</strong>, enabling SysEx to stream data in its native format directly from the source cluster to LogHouse - without decoding, re-encoding, or allocating intermediary data structures.</p><p>This approach is broadly equivalent to a simple bash command:</p><pre><code>curl -s -u  | curl -s -X POST --data-binary @- </code></pre><p>An actual go implementation for the curious can be found <a href=\"https://pastila.nl/?00740eb5/722157a76fc25f54212d7097b805253b#OSfprbH3wM1dGgtyMAnmuw==\">here</a>.</p><p>Most importantly, SysEx doesn’t require the heavy buffering that OTel does, thanks to its pull-based model. Because scrapers query data at a steady, controlled rate, we don’t risk dropping logs when LogHouse is temporarily unavailable or when the source experiences a spike in telemetry. Instead, SysEx naturally handles backfill by scraping historical windows, ensuring reliable delivery without overloading the system or requiring complex retry buffers.</p><p>One of the key challenges with the SysEx approach is that it assumes the source and target schemas match. But in reality, as any ClickHouse user knows, system table schemas change frequently. Engineers continuously add new metrics and columns to support emerging features and accelerate issue diagnosis, which means the schema is a moving target.</p><p>To handle this, we generate schemas dynamically. When SysEx encounters a system table, it inspects and hashes its schema to determine if a matching table already exists in LogHouse. If it does, the data is inserted there. If not, a new schema version is created for this system table e.g. .</p><p>At query time, we use ClickHouse’s <a href=\"https://clickhouse.com/blog/clickhouse-release-25-01#better-merge-tables\">Merge table engine</a> to unify all schema iterations into a single logical view. This allows us to query across multiple versions of a system table seamlessly. The engine automatically resolves schema differences by selecting only the columns that are compatible across tables, or by restricting the query to tables that contain the requested columns. This gives us forward compatibility as schemas evolve, without sacrificing query simplicity or requiring manual schema management.</p><p>As we continued to scale and refine our observability capabilities, one of our primary focuses was capturing in-memory system tables, such as . Unlike the time-series data we’ve been capturing, these tables provide a snapshot of the server’s state at a specific point in time. To handle this, we implemented a periodic snapshot process, capturing these in-memory tables and storing them in LogHouse.</p><p>This approach not only allows us to capture the state of the cluster at any given moment, but also provides time-travel through critical details like table schemas and cluster settings. With this additional data, we are able to enhance our diagnostic capabilities by performing cluster-wide or ClickHouse Cloud-wide analyses. This we can join against service settings or query characteristics like used_functions to pinpoint anomalies, making it easier to identify the root causes of issues as they arise. By correlating queries with particular schemas, we further improved our ability to proactively identify and resolve performance or reliability problems for our customers.</p><p>One of the many powerful capabilities we've unlocked with SysEx is the ability to take the same <a href=\"https://clickhouse.com/blog/common-issues-you-can-solve-using-advanced-monitoring-dashboards\">Advanced Dashboard queries</a>  that customers use to monitor their individual ClickHouse instances and run them across our entire fleet of customer instances simultaneously.</p><p>For release analysis, we can now execute proven diagnostic queries before and after deployments to immediately identify behavioral changes across our entire fleet. This has been rolled into our comprehensive release analysis process. Queries that analyze query performance patterns, resource utilization trends, and error rates complete in real time, allowing us to quickly spot regressions or validate improvements at fleet scale.</p><p>Secondly, our support dashboards can now embed the same deep diagnostic queries that customers rely on, but with enriched context from our centralized telemetry. When investigating customer issues, support engineers can run familiar Advanced Dashboard queries while simultaneously correlating with network logs, Kubernetes events, data and control plane events - all within the same interface.</p><p>The efficiency gains from this SysEx are staggering. Consider these stats from LogHouse:</p><ul><li> Use over 800 CPU cores to ship 2 million logs per second.</li><li><strong>LogHouse Scrapers (SysEx):</strong> Use just 70 CPU cores to ship 37 million logs per second.</li></ul><p>This specialized approach has allowed us to handle a 20x increase in event volume with less than 10 percent of the CPU footprint for our most important data source. Most importantly, it means we no longer drop events at the edge. To achieve this same level of reliability with our previous OTel-based pipeline, <strong>we would have needed over 8,000 CPU cores</strong>. SysEx delivers it with a fraction of the resources, maintaining full fidelity and consistent delivery.</p><p>If you’ve read this far, you might be wondering: when is OpenTelemetry still the right choice, and is it still useful?\nWe firmly believe that it is. While our architecture has evolved to meet challenges at extreme scale, such as parsing and processing over 20 million log lines per second, OpenTelemetry remains a critical part of our stack. It offers a standardized, vendor-neutral format and provides an excellent onboarding experience for new users - and is hence the default choice for ClickStack. Unlike SysEx, which is tightly integrated with ClickHouse internals, OpenTelemetry decouples producers from consumers, which is a major architectural advantage, especially for users who want flexibility across observability platforms.</p><p>It is also well suited for scenarios where SysEx cannot operate. SysEx is pull-based and relies on querying live system tables, which means the service must be healthy and responsive. If a service is crash-looping or down, SysEx is unable to scrape data because the necessary system tables are unavailable. OpenTelemetry, by contrast, operates in a passive fashion. It captures logs emitted to  and , even when the service is in a failed state. This allows us to collect logs during incidents and perform root cause analysis even if the service never became fully healthy.\nFor this reason, we continue to run OpenTelemetry across all ClickHouse services. The key difference is in what we collect. Previously, we ingested everything, including trace-level logs. Now, we collect only info-level and above. This significantly reduces the data volume and allows our OTel collectors and gateways to operate with far fewer resources. The result is a smaller, more focused pipeline that still accounts for the 2 million log lines per second referenced earlier.</p><p>Collecting all this data is just the beginning. Making it usable and accessible is what really matters. In the first iteration of LogHouse, we built a highly customized observability experience on top of Grafana. It served us well, but as our internal data sources grew and diversified, particularly with the introduction of SysEx and wide-column telemetry, it became clear we needed something more deeply integrated with ClickHouse.</p><p>This challenge was not unique to us. Many teams building observability solutions on ClickHouse have encountered the same issue. Getting data into ClickHouse was straightforward, but building a UI that fully unlocked its value required significant engineering effort. For smaller teams or companies without dedicated frontend resources, ClickHouse-powered observability was often out of reach.</p><p>HyperDX changed that. It provided a first-party, ClickHouse-native UI that supports log and trace exploration, correlation, and analysis at scale. Its workflows are designed with ClickHouse in mind, optimizing queries and minimizing latency. When we evaluated HyperDX prior to the acquisition, it was already clear that it addressed many of the pain points we and others had experienced. The ability to query using Lucene syntax dramatically simplifies data exploration and is often sufficient. Importantly, it still allows us to query in SQL - something which we still find essential for more complex event analysis - see “SQL for more complex analysis”.</p><p>A key reason HyperDX was such a compelling fit was the schema-agnostic <a href=\"https://clickhouse.com/blog/clickstack-a-high-performance-oss-observability-stack-on-clickhouse\">approach introduced in v2.0</a>. It doesn't require log tables to conform to a single, rigid structure. This flexibility is critical for a system like LogHouse, which ingests data from numerous sources:</p><ul><li>It seamlessly handles the standardized, yet evolving, data format from our  pipeline.</li><li>More importantly, it works out-of-the-box with the highly specialized, wide-column tables produced by  and our other custom exporters. It does this with no prior knowledge of the SysEx schemas, or complex  specializations. It simply inspects the schema behind-the-scenes and adapts to work with them.</li></ul><p>This means our engineering teams can add new data sources with unique, optimal schemas to LogHouse without ever needing to worry about breaking or reconfiguring the user interface. By combining HyperDX's powerful UI and session replay capabilities with LogHouse's massive data repository, we have created a unified and adaptable observability experience for our engineers.</p><p>It is worth emphasizing that Grafana still has its place in our observability stack. Our internal Grafana-based application has some distinct advantages, particularly in how it handles routing and query scoping. Users are required to specify the namespace (effectively a customer service) they intend to query. Behind the scenes, the application knows exactly where data for each service resides and can route queries directly to the appropriate ClickHouse instance within LogHouse. This minimizes unnecessary query execution across unrelated services and helps keep resource usage efficient.</p><p>This is especially important in our environment, where we operate LogHouse databases across many regions. As our previous blog post described, efficiently querying across these distributed systems is critical for performance and reliability. We’re currently exploring how we might push this routing logic to ClickHouse itself, allowing HyperDX to benefit from the same optimization..so stay tuned.</p><p>In addition to its routing capabilities, Grafana remains the home for many of our long-standing dashboards and alerts, particularly those built on Prometheus metrics. These remain valuable, and migrating them is not currently a priority. For example, kube_state_metrics has almost become a de facto standard for cluster health monitoring. These high-level metrics are well suited for alerting, even if they are not ideal for deep investigation. For now, they continue to serve their purpose effectively.</p><p>For now, the two tools serve complementary purposes and coexist effectively within our observability stack.</p><p><em>Store everything, aggregate nothing</em></p><p>The development of SysEx has brought more than just technical gains. It has driven a cultural shift in how we think about observability. By unlocking access to system tables that were previously unavailable, where only standard output logs had been captured, we have embraced a model centered on wide events and high cardinality data.</p><p>Some refer to this as Observability 2.0. <strong>We simply call it LogHouse combined with ClickStack.</strong></p><p>This approach replaces the traditional three-pillar model with something more powerful: a centralized warehouse that can store high-cardinality telemetry from many sources. Each row contains rich context - query identifiers, pod names, version metadata, network details - without needing to pre-aggregate or discard dimensions to fit within the limits of a metric store.</p><p>As engineers, we have adapted to this new model, leaving behind outdated concerns about cardinality explosions. Instead of summarizing at ingest time, we store everything as is and push aggregation to query time. This approach allows for in-depth inspection and flexible exploration without sacrificing fidelity.</p><p>One pattern we have found particularly impactful is logging wide events that include timeseries attributes in place of traditional metrics. For example, here is a log line from SysEx that tracks data pushed from a source ClickHouse instance to the LogHouse cluster:</p><pre><code></code></pre><p>At this point, you may be asking: <strong>how is this different from a traditional metrics store like Prometheus?</strong></p><p>The key difference is that we store . We do not pre-aggregate fields like ; instead, we capture and retain each value and store it together.</p><p>In contrast, a system like Prometheus typically stores either a gauge per series or, more commonly, pre-aggregates values into histograms to support efficient querying. This design introduces significant limitations. For example, storing time series for all label combinations in Prometheus would lead to a cardinality explosion. In our environment, with tens of thousands of unique pod names, each label combination would require its own timeseries just to preserve query-time flexibility. Pre-aggregating with histograms helps control resource usage but comes at the cost of fidelity. It makes certain questions impossible to answer, such as:</p><blockquote><p>\"Which exact insert is represented by this spike in insertDuration - down to the specific instance, table, and time window?\"</p></blockquote><p>With our approach, we avoid these trade-offs entirely. We log each event as a wide row that captures all relevant dimensions and metrics in full. This shifts aggregation and summarization to query time while preserving the ability to drill down into individual events when necessary.</p><p>This model isn’t entirely new. Systems like Elasticsearch have long encouraged the ingestion of wide events and flexible document structures. The difference is that ClickHouse makes this approach operationally viable at scale. Its columnar design allows us to store high-cardinality, high-volume event data efficiently - without the runaway storage costs or query latency that traditionally limited these kinds of approaches to storing events.</p><p>The power of this approach is in how we can use that single event to draw many different conclusions by visualising its various characteristics, and we can always jump back to the raw logs from any given point on a chart.</p><p>First, we can focus on a particular service and see its inserts line by line in series. This is the raw view upon the data.</p><p>We can visualize the insert lag for all tables for this individual instance trivially…</p><p>We may go a layer up and visualise the insert lag for all servers in a region, which have lag &gt; desired.</p><p>And, because Observability is <em>Just another Data Problem</em>, we get to borrow all of the tooling in the data science space for our observability data, so we can visualise our logs in any tool of our choice for which C<a href=\"https://clickhouse.com/docs/integrations/data-visualization\">lickHouse either integrates directly</a> or <a href=\"https://clickhouse.com/docs/integrations/language-clients\">via a client library</a>. For example, Plotly in a Jupyter notebook;</p><pre><code> plotly.express  px\n pandas  pd\n clickhouse_connect\n\nclient = clickhouse_connect.get_client(\n…\n)\nquery = \n\ndf = client.query_df(query)\n\n\ndf[] = pd.to_datetime(df[], unit=)\ndf[] = pd.to_datetime(df[], unit=)\n\nfig = px.timeline(df, x_start=, x_end=, y=)\n\nfig.update_traces(width=)\nfig.update_layout(bargap=)\n\nfig.show()\n</code></pre><blockquote><p>The plot shows scrape time versus wall time, allowing us to inspect each event for duplication. With Plotly, I could size the width of the rectangles as the exact start/end times. The annotations highlight a window where duplicate scrapes occurred, confirming the presence of overlapping data in that range.</p></blockquote><blockquote><p>This plot illustrates the varying insert duration for some tables collected by the LogHouse Scraper.</p></blockquote><p>While I tend to prefer Plotly, we recognize that others may favor more modern visualization libraries. Thanks to ClickHouse's broad integration support, our SREs can choose the best tools for their workflows. Whether it’s Hex, Bokeh, Evidence, or any other platform that supports SQL-driven analysis, they are free to work with the approach that suits them best.</p><p>Here, we saw five views of the same event - demonstrating the flexibility we have to choose how we render at query time, using different charting tools, always with the ability to drill down into the raw line-by-line events.</p><p>HyperDX offers a robust event search interface utilizing Lucene syntax, ideal for quick lookups and filtering. However, to answer more complex observability questions, a more expressive query language is needed. With ClickHouse as the engine behind LogHouse, we can always drop into full SQL</p><p>SQL allows us to express joins, time-based operations, and transformations that would be difficult or impossible to perform in typical log query tools. One example is identifying pod termination times by correlating Kubernetes event streams. The query below uses ASOF JOIN to align Killing and Created events for the same container, calculating the time between termination and restart:</p><pre><code>\n    KE \n    (\n         loghouse.kube_events\n         (FirstTimestamp )  (FirstTimestamp )  (Reason  [])  (FieldPath )\n    ),\n    CE \n    (\n         loghouse.kube_events\n         (FirstTimestamp )  (FirstTimestamp )  (Reason  [])  (FieldPath )\n    )\n\n    Name,\n    KE.FirstTimestamp  killTime,\n    CE.FirstTimestamp  createTime,\n    createTime  killTime  delta,\n    formatReadableTimeDelta(createTime  killTime)  readableDelta\n KE\nASOF  CE  (CE.Name  KE.Name)  (CE.FirstTimestamp  KE.FirstTimestamp)\n createTime  delta \nLIMIT </code></pre><pre><code>┌─Name─────────────────────────────┬────────────killTime─┬──────────createTime─┬─delta─┬─readableDelta─────────────────────┐\n│ c-emerald-tu-48-server-p0jw87g-0 │ 2025-03-10 19:01:39 │ 2025-03-10 20:15:59 │  4460 │ 1 hour, 14 minutes and 20 seconds │\n│ c-azure-wb-13-server-648r93g-0   │ 2025-03-10 11:30:23 │ 2025-03-10 12:28:50 │  3507 │ 58 minutes and 27 seconds         │\n│ c-azure-wb-13-server-3mjrr1g-0   │ 2025-03-10 11:30:23 │ 2025-03-10 12:28:47 │  3504 │ 58 minutes and 24 seconds         │\n│ c-azure-wb-13-server-v31soea-0   │ 2025-03-10 11:30:23 │ 2025-03-10 12:28:46 │  3503 │ 58 minutes and 23 seconds         │\n└──────────────────────────────────┴─────────────────────┴─────────────────────┴───────┴───────────────────────────────────┘\n\n4 rows in set. Elapsed: 0.099 sec. Processed 17.78 million rows, 581.49 MB (180.05 million rows/s., 5.89 GB/s.)\nPeak memory usage: 272.88 MiB.\n</code></pre><p>Sure, we could write a component to track this as a metric, but the power of ClickHouse is that we don’t need to do so. It’s sufficient to store a warehouse of wide events and derive the metric we need at query time from them. So, when a colleague asks, ‘what’s the p95 replacement time for Pods after termination is requested’, we can just find a relevant set of events instead of responding, 'let me ship a new metric ', and getting back to them with an answer after the next release goes out.</p><p>Sold on the immense value of having deep, structured telemetry in a high-performance analytics engine, we've been busy adding more data sinks to LogHouse, mainly at the request of our engineering and support team, who love using LogHouse and want all critical data to live in the warehouse. This year, we've embraced a cultural shift towards high-cardinality, wide-event-based observability as shown above.</p><p>Some of our new data sources, which adhere to this wide event philosophy, include:</p><ul><li> Our open-source tool for monitoring Kubernetes networking, giving us deep insights into cluster traffic.  uses Linux's conntrack system to capture L3/L4 connection data with byte/packet counts. This provides three key capabilities: forensics (time-series connection records with per-minute bandwidth), attribution (mapping connections to specific workloads and pods), and metering (cost tracking for expensive data transfer like cross-region egress). The system processes millions of connection observations per minute, helping us identify costly cross-regional downloads, track cross-AZ traffic patterns, and correlate network usage with actual costs. You can find the project at<a href=\"https://github.com/ClickHouse/kubenetmon\"> https://github.com/ClickHouse/kubenetmon</a>.</li><li><strong>Kubernetes Event Exporter:</strong> We forked the popular exporter and added a native ClickHouse sink, allowing us to analyze Kubernetes API events at scale. You can find our fork <a href=\"https://github.com/ClickHouse/kubernetes-event-exporter\">here</a>. This is hugely useful for understanding why things changed in K8s over time. We’re not stopping there, however! We’re already working on a plan to ingest not just the events, but the entire k8s object model into LogHouse, with snapshots at every change. This would allow us to model the full state of all clusters at any moment in time over the past six months, and step through all of the changes. Instead of just knowing \"Pod X was terminated at 15,\" we’ll see the full cluster state before and after, understand dependencies, resource constraints, and the cascading effects of changes.</li><li> We collect all operational data from our Control Plane department, who had not yet onboarded into LogHouse.</li><li><strong>Real User Monitoring (RUM):</strong> In a project that is still a work in progress, we collect frontend performance metrics from our users' browsers, which are pushed via a public gateway into our OTel pipeline.</li><li> We ingest HTTP-level traffic data from our Istio service mesh, capturing request/response patterns, latencies, and routing decisions. Combined with ClickHouse's system.query_log and kubenetmon's network flows, this creates a powerful tri-dimensional correlation capability. When network usage spikes occur, our support team can trace the complete story: which specific SQL queries were executing, what HTTP requests triggered them, and the exact packet flow patterns. This cross-layer visibility transforms debugging from guesswork into precise root cause analysis - if we see unusual egress traffic, we can immediately identify whether it's from expensive cross-region queries, backup operations, or unexpected replication, making troubleshooting incredibly efficient for the support team.</li></ul><p>It’s been an incredible year of growth for LogHouse. By moving beyond a one-size-fits-all approach and embracing specialized, highly efficient tooling, we’ve scaled our observability platform to remarkable new heights while significantly enhancing our cost performance. Integrating HyperDX is a key part of that evolution, providing a flexible and powerful user experience on top of our petabyte-scale data warehouse. We're excited to see what the next year brings as we continue to build on this strong foundation.</p><p>While SysEx is designed to be efficient and resource-conscious, customers occasionally notice our scrape queries in their logs and metrics. These queries are <strong>tightly constrained with strict memory limits</strong>, but when they error (as they sometimes do) it can create concern. Although the actual resource impact is minimal, we recognize that even lightweight queries can create noise or confusion in sensitive environments.</p><p>To address this, we’re exploring what we call  - the next evolution of SysEx. The goal is to eliminate all in-cluster query execution by entirely decoupling scraping from the live system. One promising direction involves leveraging , where ClickHouse already writes its service logs. In this model, a pool of SysEx workers would mount these disk-based log tables directly, bypassing the need to query the running ClickHouse instance. This design would deliver all the benefits of our current system - native format, high fidelity, minimal transformation - while removing even the perception of operational impact.</p><p>OpenTelemetry remains a critical component of our platform, particularly for early-stage data capture before service tables are available. This is especially useful during crash loops, where structured logs may be unavailable. However, if our zero-impact scraping approach proves successful, it could reduce our reliance on OTel even further by providing a high-fidelity, low-disruption path for log ingestion throughout the lifecycle of a cluster.</p><p>This effort is still in progress, and we’ll share more once we’ve validated the approach in production.</p><p>The JSON type has been available in ClickHouse for some time and <a href=\"https://clickhouse.com/blog/clickhouse-release-25-03\">recently reached GA in version 25.3</a>. It offers a flexible and efficient way to store semi-structured data, dynamically creating columns with appropriate types as new fields appear. It even supports <a href=\"https://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse\">fields with multiple types</a> and gracefully handles schema explosion.</p><p>Despite these advantages, we’re still evaluating how well JSON fits common observability access patterns at scale. For example, querying a string across an entire JSON blob can effectively involve scanning thousands of columns. There are workarounds - such as also storing a raw string version of the JSON alongside the structured data - but we’re still developing best practices in this area.</p><p>Culturally, we have also come to recognize the practical limits of the Map type, which has served us well. Most of our log and resource attributes are small and stable enough that the Map continues to be the right fit. We have found that single-level JSON logs are often all you need, and for exceptions, tools like HyperDX automatically translate map access into <a href=\"https://clickhouse.com/docs/sql-reference/functions/json-functions#jsonextract\">JSONExtract</a> functions. While we plan to adopt JSON more broadly, this is still a work in progress. Expect us to share more in a future update.</p><p>Over the past year, LogHouse has evolved from an ambitious logging system into a foundational observability platform powering everything from performance analysis to real-time debugging across ClickHouse Cloud. What began as a cost-saving measure has become a catalyst for both cultural and technical transformation, shifting us toward high-fidelity, wide-event telemetry at massive scale. By combining specialized tools like SysEx with general-purpose frameworks like OpenTelemetry, and layering on flexible interfaces like HyperDX, we have built a system that not only keeps up with our growth but also unlocks entirely new workflows. The journey is far from over, but the lessons from scaling to 100PB and 500 trillion rows continue to shape how we think about observability as a core data problem we are solving at warehouse scale.</p>","contentLength":36469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44336015"},{"title":"Show HN: We moved from AWS to Hetzner, saved 90%, kept ISO 27001 with Ansible","url":"https://medium.com/@accounts_73078/goodbye-aws-how-we-kept-iso-27001-slashed-costs-by-90-914ccb4b89fc","date":1750496549,"author":"sksjvsla","guid":164194,"unread":true,"content":"<div><h2>The European CTO’s Dilemma: Keeping Compliance outside AWS</h2></div><p>Earlier this year, I faced a dilemma many tech leaders know well. Our entire infrastructure was built on AWS. We loved their powerful, ISO 27001-certified services. Yet, two critical issues kept me up at night:</p><ol><li><strong>The Compliance Black Hole:</strong> It was clear that American cloud providers couldn’t fully shield us from US government jurisdiction. Under the CLOUD Act and FISA, our European customer data was potentially exposed, regardless of the server’s physical location. This undermined our GDPR promises.</li><li><strong>The $2,000/Month Question:</strong> While not a fortune for every company, our $24,000 annual bill felt disproportionate to our actual needs. I asked myself: how often does a well-maintained Linux server actually crash? Isn’t RDS just a managed Postgres instance with scripts I could write myself? That $2,000 a month could buy a phenomenal amount of resilient, dedicated hardware in Europe.</li></ol><p>This wasn’t just about cost or compliance; it was a strategic risk. Was tying our company’s future to a single US-based provider a responsible choice?</p><p>We are a Danish workforce management company doing employee scheduling. Beyond our ISO 27001 certificate, we have a few legal requirements on our operation as well as we perform overtime compensation salary adjustments and are source of truth for time-and-attendance data. Maintaining the tech side of this, is just like maintaining a bank software: Things must be accounted for, always add up and never be lost.</p><p>Born and raised in AWS, many aspects of our legal requirement was architected as AWS native workflows and migrating that to independent alternatives always had to go along with legal requirements.</p><p>Let’s be honest: leaving AWS feels like walking away from a fortress of convenience. You lose the “magic” of deeply integrated services like Lambda, one-click RDS deployments, and the rich ecosystem of built-in compliance tooling that makes ISO 27001 audits smoother.</p><p>Giving this up is the primary source of fear and inaction for most teams. It means trading the comfort of managed services for a higher degree of control and responsibility.</p><p>By migrating to European providers like Hetzner and OVHcloud, the gains weren’t just theoretical. They were immediate and strategic.</p><ul><li> Hosting on European-owned infrastructure gave us undeniable proof of data residency — a game-changer for GDPR audits and ISO 27001 recertification. We could tell our customers exactly where their data was, with no ambiguity.</li><li> Our cloud costs dropped by . This wasn’t a typo. By replacing expensive managed services with our own automated, self-hosted solutions, our budget became predictable and transparent.</li><li> The biggest surprise was how losing AWS’s pre-built tools forced us to get better. We built a powerful infrastructure-as-code setup using Ansible that gave us even tighter security controls and auditability than before.</li></ul><h2>The Blueprint: Key Lessons for Your Own Migration</h2><p>This migration taught us invaluable lessons that can serve as a blueprint for others. Here’s the core of our strategy:</p><ol><li><strong>Ansible as Your Compliance Engine:</strong> Forget simple compliance checks. With properly structured Ansible playbooks, you can tie every line of your server configuration directly to a specific ISO 27001 Annex A control. Your infrastructure code becomes a self-documenting audit trail.</li><li><strong>Monitoring That Rivals AWS:</strong> You don’t need CloudWatch to have enterprise-grade monitoring. A combination of <strong>Prometheus, Grafana, and Loki</strong> allowed us to replicate — and in some ways exceed — the visibility we had on AWS, ensuring faster incident response.</li><li><strong>Security-by-Design Becomes Reality:</strong> When there isn’t a pre-made security solution to click on, you build it into the foundation. This “security-by-design” approach, automated with Ansible, makes your ISMS (Information Security Management System) incredibly robust and easy for developers to follow.</li></ol><p>This wasn’t just a technical project; it was a business transformation.</p><ul><li><strong>We minimized our compliance risk</strong> regarding US surveillance laws.</li><li><strong>We used our European hosting as a sales tool,</strong> strengthening brand trust.</li><li><strong>We returned 90% of our cloud spend to the business</strong></li></ul><p>If this story resonates with you, you’re likely asking: “Could we actually do this? What would it cost? What are the hidden risks?”</p><p>Our journey created a repeatable playbook for migrating from AWS to a sovereign, cost-effective European cloud while maintaining ISO 27001 certification. I offer  for CTOs and founders facing this exact challenge.</p><p>In a one-hour session, we can map out:</p><ul><li>A high-level cost analysis of your current AWS setup vs. a European alternative.</li><li>The key compliance and ISO 27001 risks in your specific situation.</li><li>A realistic timeline and the first 3 steps of a potential migration plan.</li></ul><h2><strong>Interested in exploring this for your company?</strong></h2><p><a href=\"https://www.linkedin.com/in/jknobel/\" rel=\"noopener ugc nofollow\" target=\"_blank\"><strong>Connect with me on LinkedIn</strong></a> and mention this article, or for a faster response, <a href=\"https://calendly.com/datapult/30min\" rel=\"noopener ugc nofollow\" target=\"_blank\"><strong>book a preliminary chat directly on my Calendly</strong></a>.</p>","contentLength":4975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44335920"},{"title":"Cosmoe: BeOS Class Library on Top of Wayland","url":"https://cosmoe.org/index.html","date":1750496401,"author":"Bogdanp","guid":164246,"unread":true,"content":"<p>Cosmoe needs 2 main improvements to be a reliable, full-featured UI library:While I've made incredible strides with getting the BeOS class libraries to talk to Wayland, much work still remains to weed out crashes and incorrect behavior.  Wayland is powerful, but not friendly.Cosmoe implements about 95% of the BeOS API currently.  Notable feature that are not yet implemented include \"offscreen\" BBitmaps for accelerated drawing, and BFilePanel which implements Open and Save dialog boxes.  Some file-related classes like BVolume are only partially implemented.  Also, for security reasons, Wayland forbids certain Window-related actions such as window positioning and centering, so that functionality can never exist.<p>For more details on this items and other in-progress aspects of Cosmoe, please see the </p><a href=\"https://gitlab.com/haydentech/cosmoe-wayland/-/blob/master/TODO?ref_type=heads\">TODO file in the Cosmoe repo</a>.\n\t\t\t\t</p>","contentLength":840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44335907"},{"title":"'Gwada negative': French scientists find new blood type in woman","url":"https://www.lemonde.fr/en/science/article/2025/06/21/gwada-negative-french-scientists-find-new-blood-type-in-woman_6742577_10.html","date":1750491522,"author":"spidersouris","guid":164300,"unread":true,"content":"<p>A French woman from the Caribbean island of Guadeloupe has been identified as the only known carrier of a new blood type, dubbed \"Gwada negative,\" France's blood supply agency has announced. \"The EFS has just discovered the 48 blood group system in the world!\" the agency said in a statement on the social network LinkedIn. \"This discovery was officially recognised in early June in Milan by the International Society of Blood Transfusion (ISBT).\"</p><p>The announcement was made 15 years after researchers received a blood sample from a patient who was undergoing routine tests ahead of surgery, the French Blood Establishment (EFS) said on Friday. The scientific association had until now recognized 47 blood group systems. The discovery was first reported by radio <a href=\"https://www.radiofrance.fr/franceinter/podcasts/l-info-de-france-inter/l-info-de-france-inter-1294478\" target=\"_blank\" rel=\"noopener\" title=\"Nouvelle fenêtre\">France Inter</a>.</p><p>Thierry Peyrard, a medical biologist at the EFS involved in the discovery, told AFP that a \"very unusual\" antibody was first found in the patient in 2011. However, resources at the time did not allow for further research, he added. Scientists were finally able to unravel the mystery in 2019 thanks to \"high-throughput DNA sequencing\", which highlighted a genetic mutation, Peyrard said.</p><p>The patient, who was 54 at the time and lived in Paris, was undergoing routine tests before surgery when the unknown antibody was detected, Peyrard said. This woman \"is undoubtedly the only known case in the world,\" said the expert. \"She is the only person in the world who is compatible with herself,\" he said. Peyrard said the woman inherited the blood type from her father and mother, who each had the mutated gene.</p><p>The name \"Gwada negative\", which refers to the patient's origins and \"sounds good in all languages,\" has been popular with the experts, said Peyrard. He and colleagues are now hoping to find other people with the same blood group. \"Discovering new blood groups means offering patients with rare blood types a better level of care,\" the EFS said.</p><p>The ABO blood group system was first discovered in the early 1900s. Thanks to DNA sequencing the discovery of new blood groups has accelerated in recent years.</p>","contentLength":2083,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44335517"},{"title":"Delta Chat is a decentralized and secure messenger app","url":"https://delta.chat/en/","date":1750487340,"author":"Bluestein","guid":164245,"unread":true,"content":"<p>💬 Reliable instant messaging with multi-profile and multi-device support</p><p>Available on mobile and desktop.</p>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44335065"},{"title":"Sega mistakenly reveals sales numbers of popular games","url":"https://www.gematsu.com/2025/06/sega-mistakenly-reveals-sales-numbers-for-like-a-dragon-infinite-wealth-persona-3-reload-shin-megami-tensei-v-and-more","date":1750486981,"author":"kelt","guid":164111,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44335038"},{"title":"Signal – An Ethical Replacement for WhatsApp","url":"https://greenstarsproject.org/2025/06/15/signal-an-ethical-replacement-for-whatsapp/","date":1750483309,"author":"miles","guid":164086,"unread":true,"content":"<p>You may not need any incentive to do this, but I’m going to argue that moving from WhatsApp to Signal is an important action, right now. There are other alternatives to WhatsApp, of course, but several have received criticism over various issues so I’m going to focus on Signal here. Please comment below if you’d like to propose ethical alternatives to WhatsApp. &nbsp;</p><p>In March I argued that we should <a href=\"https://greenstarsproject.org/2025/03/30/bluesky-an-ethical-replacement-for-x-twitter/\"><strong>choose Bluesky over X/Twitter</strong></a>, based on the obvious reason of boycotting companies connected to Musk. Bluesky is structured as a benefit corporation (like Patagonia) and based on an open network, two features that create strong incentives for the company to not upset its users.</p><blockquote><p>This creates an alignment between Bluesky and its users – Bluesky is disincentivized to make a bad user experience. This also acts as a kind of poison pill against a billionaire takeover, or at least a deterrent against this.</p></blockquote><p>Moving from X/Twitter to Bluesky does require some sacrifice for people who worked hard to build up a following on the former. But we’ve reached a point where support of Musk is unconscionable, whatever the sacrifice involved. The move from WhatsApp to Signal, however, is quite straightforward and completely painless. But in this case, some may be wondering, <em>why should I bother moving at all?</em></p><h2>Ethical issues with WhatsApp</h2><p>My biggest issues with using WhatsApp are related to its parent corporation, Meta (formerly Facebook) and its founder/CEO Mark Zuckerberg. But first, I want to mention a few issues specific to WhatsApp.</p><p>In early 2021, WhatsApp updated its terms of service, requiring users to opt into <a href=\"https://www.androidpolice.com/2021/01/12/whatsapps-new-terms-of-service-are-a-facebook-or-die-ultimatum/\">sharing their data</a> with Facebook – including network details and location (even if you haven’t turned on location sharing). This 180° reversal on an <a href=\"https://www.propublica.org/article/how-facebook-undermines-privacy-protections-for-its-2-billion-whatsapp-users\">earlier promise</a> made by Zuckerberg that “WhatsApp is going to operate completely autonomously” was one of several events over the last 5 years that accelerated an exodus from WhatsApp to Signal. The large amount of metadata that WhatsApp collects is also shared with law enforcement agencies.</p><blockquote><p>WhatsApp shares metadata, unencrypted records that can reveal a lot about a user’s activity, with law enforcement agencies such as the Department of Justice. Some rivals, such as Signal, intentionally gather much less metadata to avoid incursions on its users’ privacy, and thus share far less with law enforcement. – <a href=\"https://www.propublica.org/article/how-facebook-undermines-privacy-protections-for-its-2-billion-whatsapp-users\">ProPublica</a></p></blockquote><p>I have to admit that I don’t give a huge amount of time fretting over these things during normal times. But we no longer live in normal times – people are now guilty by suspicion (or by association, based on their WhatsApp contacts) and convicted without due process.</p><p>Over the years, Zuckerberg and Meta have done more flip flopping than a White Lotus guest (walking around in flip-flops, you see). Facebook has since been <a href=\"https://ec.europa.eu/commission/presscorner/detail/en/IP_17_1369\">fined</a> €110 million by EU antitrust regulators and <a href=\"https://www.ftc.gov/news-events/press-releases/2019/07/ftc-imposes-5-billion-penalty-sweeping-new-privacy-restrictions\">$5 billion</a> by the US Federal Trade Commission for deceiving regulators and users. Following this greater level of scrutiny, Zuckerberg has shown himself to be a person of little scruples, capitulating to whoever will do him favors.</p><p>Meta – a company so bad that it had to change its name to try to clean its reputation. The <a href=\"https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal\">Facebook–Cambridge Analytica scandal</a> involved the collection of user data without consent to create <a href=\"https://en.wikipedia.org/wiki/Psychographics\">psychographic profiles</a> that were used in Donald Trump’s 2016 election campaign and suppressing Black voters in Trinidad and Tobago. WhatsApp, in particular, played a role in the Brazil’s 2018 election.</p><blockquote><p>The vast majority of false information shared on WhatsApp in Brazil during the presidential election favoured the far-right winner, Jair Bolsonaro. The analysis sheds light on the spread of misinformation on the Facebook-owned app, with fears it could be poisoning political debate in one of the largest democracies in the world. – <a href=\"https://www.theguardian.com/world/2019/oct/30/whatsapp-fake-news-brazil-election-favoured-jair-bolsonaro-analysis-suggests\">The Guardian</a>.</p></blockquote><blockquote><p>Companies supporting Jair Bolsonaro are buying a service called “mass blasts,” using the candidate’s list of WhatsApp users or buying lists from agencies specializing in digital strategy. – <a href=\"https://www1.folha.uol.com.br/internacional/en/brazil/2018/10/businessmen-fund-whatsapp-campaign-against-pt.shtml\">Folha de S.Paulo</a>.</p></blockquote><p>In 2018, Zuckerberg testified before US Congress, admitting: “It was my mistake, and I’m sorry.”</p><p>Aww! You’re totally forgiven, Zuck! You’ve learned your lesson, right?</p><p>But wait! Last year, as you probably know, Zuckerberg did the cowardly act of endorsing Trump by implication. I happened to catch this interview with the Zuck live on Bloomberg and was totally gagged (Hey Gen Z readers!). Take a look and marvel at Zuck sticking to his guns and maintaining his neutrality:</p><p>Post-election, Zuck flew down to Florida for dinner with Trump (I’m imagining scenes from the hunting episode of ) and then got rid of DEI and moderation and all that weak stuff. It didn’t take long for Zuck to switch from humility (<em>It was my mistake, and I’m sorry</em>) to going on the Joe Rogan show to say this:</p><blockquote><p>“Masculine energy is good, and obviously, society has plenty of that, but I think corporate culture was really trying to get away from it,” Zuckerberg continued. “I think having a culture that celebrates the aggression a bit more has its own merits that are really positive.” – <a href=\"https://mashable.com/article/mark-zuckerberg-joe-rogan-masculine-feminine-energy\">Mashable</a>.&nbsp;</p></blockquote><p>Signal provides a very similar user experience to WhatsApp, so switching over is pretty seamless. This is not too surprising, considering that the current CEO of Signal LLC., Brian Action, was one of the cofounders of WhatsApp. WhatsApp was acquired by Facebook in 2014, and Acton <a href=\"https://www.reuters.com/business/whatsapp-co-founder-acton-named-signals-interim-ceo-2022-01-10/\">left the company</a> three years later due to differences around the use of customer data and targeted advertising.</p><p>A year later, Acton contributed $50 million to launch a non-profit, the Signal Foundation and a subsidiary, Signal Messenger, LLC.</p><blockquote><p>On 21 February 2018, Moxie Marlinspike and WhatsApp co-founder Brian Acton announced the formation of the Signal Technology Foundation, a 501(c)(3) nonprofit organization whose mission is “to support, accelerate, and broaden Signal’s mission of making private communication accessible and ubiquitous”. – <a href=\"https://en.wikipedia.org/wiki/Signal_(software)#History\">Wikipedia</a></p></blockquote><p>When the Electronic Frontier Foundation <a href=\"https://www.eff.org/pages/secure-messaging-scorecard\">ranked messaging apps</a> for privacy and transparency, Signal was one of the few that received a perfect score. The nonprofit also has an <a href=\"https://ssd.eff.org/module/how-to-use-signal\">updated guide</a> to using Signal, navigating its settings, etc.</p><p>The protection of data and personal privacy is important. The nonprofit Signal Foundation is led by <a href=\"https://en.wikipedia.org/wiki/Meredith_Whittaker\">Meredith Whittaker</a>, a former director of the AI Now Institute at NYU, which examined the social impacts of AI and concentration of power in tech. She is a strong proponent of protecting privacy as a human right and an opponent of <a href=\"https://en.wikipedia.org/wiki/Surveillance_capitalism\">surveillance capitalism</a>. Signal’s predecessor, Open Whisper Systems <a href=\"https://cyberscoop.com/tor-signal-funding-donald-trump-steve-bannon-encryption/\">received funding from</a> journalism nonprofit Freedom of the Press Foundation.</p><p>Signal has been recommended to Democratic Party staffers and officially <a href=\"https://en.wikipedia.org/wiki/Signal_(software)#History\">approved</a> by the US Senate in 2017. Just not for texting top secret plans, obviously!</p><h3>Action plan for migrating from WhatsApp</h3><p>It’s easy to see how we all get drawn into using specific apps like WhatsApp – the user base reaches a critical size and it becomes the app to find everyone on. At the same time, the company becomes more lucrative (and in many cases is taken over by a bigger fish) and then ethical issues creep in. We learn to suppress our concerns because the app is just too convenient and, to quote The Cranberries, <em>Everybody Else Is Doing It, So Why Can’t We? </em>(R.I.P., Dolores).</p><p>The exit plan from WhatsApp is quite simple. Start by installing Signal and setting it up – it takes only a couple of minutes. Then, resume any WhatsApp conversations on Signal if that person is already a Signal user. If they are not, then switch to regular text messaging and gently suggest to that person to switch over to Signal. [Shout out to CeCe for reminding me to install Signal!] Group chats are a good way to get people to switch over as nobody will want to be the person who can’t be bothered making the switch.</p><p>The interference with elections is not OK.</p><p>I’ll leave the last word to future president, AOC:</p><blockquote><p>Meta as in ‘we are a cancer to democracy metastasizing into a global surveillance and propaganda machine for boosting authoritarian regimes and destroying civil society… for profit!'” – <a href=\"https://www.cnet.com/tech/services-and-software/facebook-to-meta-a-new-name-but-the-same-old-problems/\">Alexandria Ocasio-Cortez</a></p></blockquote>","contentLength":8228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44334743"},{"title":"Samsung embeds IronSource spyware app on phones across WANA","url":"https://smex.org/open-letter-to-samsung-end-forced-israeli-app-installations-in-the-wana-region/","date":1750475202,"author":"the-anarchist","guid":164026,"unread":true,"content":"<p>In recent months, we have received numerous reports from users across West Asia and North Africa (WANA) expressing alarm over a little-known but deeply intrusive bloatware application—AppCloud—pre-installed on Samsung’s A and M series smartphones. Without users’ knowledge or consent, this bloatware collects sensitive personal data, cannot be removed without compromising device security, and offers no clear information about its privacy practices.</p><p>AppCloud, developed by the controversial Israeli-founded company ironSource (now owned by the American company <a href=\"https://investors.unity.com/news/news-details/2022/Unity-Announces-Merger-Agreement-with-ironSource/default.aspx\">Unity</a>), is embedded into devices sold in countries where such affiliations carry legal implications. Despite the serious privacy and security risks, Samsung has offered no transparency on how AppCloud functions, what data it collects, or why users cannot opt out.</p><p>This open letter, addressed to Samsung, calls for immediate transparency, accountability, and dialogue. Users deserve to know what is installed on their devices and how their data is being used, especially amid Israel’s espionage campaigns in the region.&nbsp;</p><p>We are writing to urgently request that Samsung be transparent regarding the pre-installation of AppCloud on its A and M series smartphones, particularly in West Asia and North Africa (WANA). We ask that Samsung provide information about AppCloud’s privacy practices, opt-out and removal options, and that Samsung reconsider future pre-installations in light of privacy rights. We also request a meeting with Samsung teams to discuss these concerns further.&nbsp;</p><p>According to our <a href=\"https://smex.org/invasive-israeli-software-is-harvesting-data-from-samsung-users-in-wana/\">analysis</a>, this intrusive software is , deeply integrated into the devices’ operating system, making it nearly impossible for regular users to uninstall it without root access, which voids warranties and poses security risks. Even disabling the bloatware is not effective as it can reappear after system updates.&nbsp;</p><p>The privacy policy is there is no accessible and transparent privacy policy for this bloatware and users are in the dark about what data is collected and how it is used. There is also no straightforward opt-out mechanism. The bloatware collects sensitive user data, including biometric information, IP addresses, device fingerprints.&nbsp;</p><p>The installation of AppCloud is done from the user, which violates GDPR provisions in the EU and relevant data protection laws in the WANA region states.&nbsp;</p><p>AppCloud is developed by ironSource, <strong>an Israel-founded company (now acquired by American company Unity),</strong> raising additional legal and ethical concerns in countries where Israeli companies are barred from operating, such as <a href=\"https://www.lexismiddleeast.com/law/Lebanon/Law_1_1955\">Lebanon</a>. ironSource is notorious for its questionable practices regarding user consent and data privacy.&nbsp;</p><p>Samsung’s terms of service mention third party applications but do not specifically address AppCloud or ironSource, despite the significant data access and control granted to this bloatware app.&nbsp;</p><p>The forced installation of AppCloud <strong>undermines the privacy and security rights of users in the MENA region and beyond.</strong> The lack of transparency and control over personal data is particularly alarming given<a href=\"https://telecomlead.com/smart-phone/smartphone-strategies-in-middle-east-for-market-share-119061\"> Samsung’s <strong>significant market share in the region</strong></a>.</p><p>In light of these concerns, we respectfully request that Samsung:</p><ul><li>Disclose the full privacy policy and data handling practices of AppCloud, making this information easily accessible to all users.</li><li>Offer a straightforward and effective method for users to opt out of AppCloud and remove it from their devices without compromising device functionality or warranty.</li><li>Provide a clear explanation for the decision to pre-install AppCloud on all A and M series devices in the WANA region.</li><li>Reconsider the continued pre-installation of AppCloud on future devices, in line with the right to privacy as established by Article 12 of the Universal Declaration of Human Rights.</li><li>We also request a <strong>meeting with the relevant Samsung teams</strong> to discuss these issues in detail and to better understand the company’s approach to user privacy and data protection in the WANA region.</li></ul><p>We look forward to your prompt response and to working together to ensure the privacy and security of all Samsung users.</p>","contentLength":4132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44334167"},{"title":"Tiny Undervalued Hardware Companions (2024)","url":"https://vermaden.wordpress.com/2024/03/21/tiny-undervalued-hardware-companions/","date":1750472387,"author":"zdw","guid":164193,"unread":true,"content":"<p>After playing/working with computers for more then 25 years I started to appreciate small but handy valuable stuff – like adapters or handlers or … yeah – all kind of stuff. With many of them I did not even knew they existed until I find out about them – mostly accidentally or after long searching for some problem solution. Today I will share them with You – so maybe they will end up handy also for You.</p><p>… and while they make my life easier – they are mostly very cheap too.</p><p>The  is below.</p><ul><li><strong>RJ45 Angle Cable Adapters</strong></li><li><strong>SATA to USB-C or USB-A Adapters</strong></li><li><strong>Angle USB-C and USB-A Adapters</strong></li><li><strong>Tiny USB WiFi or Bluetooth Dongle</strong></li><li><strong>USB-C &lt;=&gt; Micro USB Adapter</strong></li><li><strong>USB-C &lt;=&gt; Laptops/Routers/5.5mmx2.5mm Adapters</strong></li><li><strong>Creative BT-W2 USB-A Bluetooth Adapter</strong></li><li><strong>External Microphone for SONY Headphones</strong></li><li><strong>Dual USB-C and USB-A Pendrive (SanDisk)</strong></li><li><strong>Quad USB-C / USB-A / Lightning / Micro USB Adapter with MicroSD Card Slot</strong></li><li><strong>C13/C14 Power Adapters with Additional C1/C2 or C5/C6 Sockets</strong></li><li><strong>HDMI 3in1 Switch with Remote Control</strong></li></ul><p>The whole article can ‘feel’ like a sponsored entry for the <a href=\"https://aliexpress.com\">https://aliexpress.com</a> portal – but it is not – its just the most cheap place I was able to find these gems. Feel free to share even cheaper one if You have one.</p><p>I mostly use laptops to do various tasks and cables sticking out on the sides perpendicularly does not help. Not many laptops today have the RJ45 LAN socket – but if they do – they are mostly on the side of the laptop.</p><p>Thanks to such angle RJ45 adapters it is no longer a problem.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> page – with the  keywords in their search.</p><h2>RJ45 Angle Cable Adapters</h2><p>The mentioned earlier  are quite bulky – but as an alternative its possible to get a short 40cm cable with smaller plug.</p><p>Not sure if its noticeable on the picture below – but I also cut the top ‘cover’ with knife of the plug – so its easier to detach.</p><p>There are of course all four angles to choose from.</p><p>One may also use the end of that 40cm cable-adapter as a ‘stopper’ to not fall inside the desk hole as shown on the image below.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the  keywords in their search.</p><p>Often I found myself in a situation that the currently available LAN cable was too short to reach and it needed a lot of work to plot another – longer one.</p><p>With these simple ‘join’ adapters it is no longer a problem. You would not use them in a serious Data Center with 10+ GE speeds – but for home 1.0-2.5 GE speeds its more then enough.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the  keywords in their search.</p><h2>SATA to USB-C or USB-A Adapters</h2><p>Multiple times I needed to clone some old disk to new SSD – just to make an old system faster.</p><p>I usually boot from some USB drive with FreeBSD and while new SSD is attached with these adapters – I then execute  command to clone the old HDD disk to new SSD drive … and then just swap them out.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the  keywords in their search.</p><h2>Angle USB-C and USB-A Adapters</h2><p>As we already talked about RJ45 angle adapters … there are also USB-C and USB-A angle adapters.</p><p>The do the same good job with cables to not stick out on a side of a laptop.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the  keywords in their search.</p><p>In the progressing and always changing world yesterday the USB-A was king and tomorrow the USB-C will be.</p><p>There are multiple cases in which you will need these – from simple USB headphones to USB pendrives and other stuff.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the  keywords in their search.</p><h2>Tiny USB WiFi or Bluetooth Dongle</h2><p>Multiple times I have found myself in a situation where it was very convenient to just add some WiFi or Bluetooth chip over USB port and do the job instead of trying to achieve the same without such chips.</p><p>While I usually omit Bluetooth I can not say the same about WiFi … and as FreeBSD lacks a little in that department – using a very tiny chip such as <a href=\"https://vermaden.wordpress.com/2020/10/30/realtek-usb-wifi-review/\">Realtek RTL8188CUS</a> often does the job done.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the  or  keywords in their search.</p><h2>USB-C &lt;=&gt; Micro USB Adapter</h2><p>In the past – in the USB Micro times – I remember using an adapter to be able to charge – then new and uncommon – USB-C devices.</p><p>Fast forward several years and now the situation is the other way around (as expected). The USB-C is the standard and USB Micro devices are less and less common … but there are still here. To not have to keep separate dedicated USB Micro cables I use a small USB-C to USB Micro adapters.</p><p>Such adapter takes USB-C as input power and is able to charge USB Micro devices.</p><p>You can find them for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the  keywords in their search.</p><h2>USB-C &lt;=&gt; Laptops/Routers/5.5mmx2.5mm Adapters</h2><p>When it comes to delivering power to my (and not only) laptops – the new standard seems to be the USB-C connector with ‘requirement’ of 45W or more (it depends).</p><p>Not that long ago I discovered that even laptops as old as 13 years –  – can be powered the same – but with simple and very cheap adapter cables – such as these below. From the left there is  typical router socket – then more modern  (and many more) – then oldschool models from 2011 year –  such as  models.</p><p>All they need is a USB-C power input.</p><p>You need to only meet two requirements – the USB charger that will make enough power for example 20V at 3.25A for 65W that would power  or 20V at 6.75A for 135W that would power . While the official power supply for ThinkPad W520 is 170W – its perfectly fine to use the 135W power adapter from  to power  laptop.</p><p>This makes organizing cables (and chargers) a lot easier – for example – I would not be able to fit 3 ‘dedicated’ ThinkPad chargers in that white cable organizer behind laptops – but I will fir there two powerful 65W and 85W USB-C chargers perfectly fine.</p><p>You can find these power adapters for about  – for example – on <a href=\"https://aliexpress.com\">https://aliexpress.com</a> – with the <b>USB-C ADAPTER LAPTOP ROUTER</b> keywords in their search.</p><h2>Creative BT-W2 USB-A Bluetooth Adapter</h2><p>When I have to cope with Bluetooth technology – its ‘tolerable’ on Android devices such as phones/tablets and mostly nowhere else. After bad audio (just not working) Bluetooth possibilities on FreeBSD I decided to try the hardware solution instead. The audio related Bluetooth on FreeBSD have failed me too many times – to the point called  – that also means I do not want to waste any more time trying to figure the way using FreeBSD Bluetooth stack devices anymore – at least for audio related devices.</p><p>Not so long ago I got the  headphones. I am/was a big fan of the  cable headphones (Jack or Mini Jack based). They have so much BASS and ‘power’ that I could not ask for more … and their cost is very low – like  or less. The only ‘downside’ of the  headphones is that they are audio only – they do not have any microphone at all – they are dedicated for music only – and that is OK – they do GREAT in that role.</p><p>I have tried some Bluetooth based headphones in the past – and they were SHIT to say the least. Not enough ‘power’ – not enough BASS etc. After reading multiple reviews I decided to give  headphones a chance … and I was not disappointed. Its the first time after  cable headphones that ANY Bluetooth based headphone delivered. I was (and I still am) really satisfied with them.</p><p>This is where the USB powered  comes handy. Its also relatively cheap as the cost of used unit is less then  – at least that is the price I payed for mine in Poland. The  allows to connect Bluetooth audio devices everywhere – even on OpenBSD – on the system that cut off Bluetooth stack entirely – and it works well on FreeBSD too. The ‘downside’ of the  headphones is that they do have microphone – but only in Bluetooth node – they have Mini Jack connector – but for audio only …</p><p>This is also only downside of the  solution – it transmits only audio – but w/o microphone. Its more then OK for listening music – but if You have to do live conferencing/meetings on FreeBSD as I do – its a dead end.</p><p>I have tried to find a solution to this problem – to the point that I wanted to abandon  headphones entirely and find some Mini Jack (or Jack) based BASS oriented headphones that will also have a working microphone.</p><p>On my journey I have found a solution that I did not expected at all – and that was the solution that solved all my problems – and allowed me to enjoy the  headphones – but more about that in the next ‘subsection’.</p><h2>External Microphone for SONY Headphones</h2><p>You already know the downsides of the  headphones that were giving me headaches. Now its time to address them.</p><p>After many hours of searching the Internet I have found a very ‘usable’ Mini Jack cable. A cable that came with microphone and a one that perfectly integrated with  headphones … and FreeBSD as well.</p><p>Its available to buy for  on  (and possible other locations) and its called . Thanks to the knowledge that  headphones have Mini Jack port with microphone part – the  cable even comes with volume controls and even come with physical kill switch for microphone.</p><p>After You attach this  to the  headphones it looks (and works) like a natural solution.</p><p>The only ‘downside’ is generally the downside of the  headphones – that You CAN NOT disable their silencing while you speak – so using them in ‘passive’ mode with  is preferred to meet all needs.</p><p>After reading comments to this article I learned that this ‘silencing’ is called  and it can be disabled in the SONY Android app or by holding  on the  until the headphones say <em>“Speak to Chat disabled.”</em> Thank You for that.</p><p>I got used to the fact that I just put my headphones on the desk … but I wanted something more useful – after some searching it was obvious to me that I needed just some headphones handle that I could attach somewhere.</p><p>After another several hours of browsing I have found a ‘part’ that would fit perfectly – a  part from <a href=\"https://aliexpress.com\">https://aliexpress.com</a> that I could find with the  keywords in their search.</p><p>Here is how it works on my desk.</p><p>… and its 360 degrees adjustable as well.</p><h2>Dual USB-C and USB-A Pendrive (SanDisk)</h2><p>With all my ‘bad’ experiences with PTP connections for Android based devices (and other places) I really liked the .</p><p>Its really handy for many transfers … and its more fast then slow as well.</p><p>When You need to connect several USB-A devices the USB ports count often come short fast – this is where this tiny USB-A hub comes handy.</p><p>With its dirt cheap  price (at <a href=\"https://aliexpress.com\">https://aliexpress.com</a> with  keywords) its a ‘steal’ … and it is a 3 port hub – there is another USB-A port at the end of it – the one that is not visible.</p><h2>Quad USB-C / USB-A / Lightning / Micro USB Adapter with MicroSD Card Slot</h2><p>… as we are talking various USB-A or USB-C solutions I could not mention this quad port adapter with MicroSD card slot.</p><p>I do not even remember how many times I have used it to copy/backup contents of my phone(s) and/or tablet(s).</p><p>Nowadays I believe I use the <b>Dual USB-C / USB-A Pendrive</b> more … but not always.</p><p>For  on its not a bad solution to have.</p><p>Batteries … I mean SD card – not included 🙂</p><p>I have often found that the angle with which the power cord sticks out of a PC is definitely not ideal – this is where angle power adapters come handy.</p><p>Here is how it looks (being used) on my PC.</p><h2>C13/C14 Power Adapters with Additional C1/C2 or C5/C6 Sockets</h2><p>After You have spent some time to lay down the C13/C14 power cables just to power your PC its really annoying to do the same for another set of C1/C2 or C5/C6 cables/sockets … but not anymore.</p><p>Now with single cable adapter You are able to power more then one computer – depending on the needs with additional connectors.</p><h2>HDMI 3in1 Switch with Remote Control</h2><p>I happen to have a 2010 FullHD 50 Inch TV that has ONLY ONE port of HDMI kind … and it was pretty annoying to say the least … up to the time I added a HDMI switch/hub to it.</p><p>The HDMI switch along with its remote below.</p><p>For the record – I have used the <b>UGreen 3in1 HDMI Switch with 4K @ 30Hz Capability and Remote</b> and I was able to get one for .</p><p>To not have a mess in the cables its useful to have them organized in some way.</p><p>I use multiple solutions for that.</p><p>Lets start with simple organizers.</p><p>… and a larger/taller one for more capacity/possibilities.</p><p>I also use some IKEA containers …</p><p>… and smaller boxes in which I keep the tiny things.</p><p>I do not even remember after what product these boxes are … and that does not even matter I think.</p><p>While there are many software settings or solutions to prevent screen from locking up – there is one bulletproof solution what just always works – a hardware USB mouse jigger.</p><p>I use a very simple one with 3 modes – but its more then enough for me needs.</p><p>Last but not least – the car FM transmitter.</p><p>My daily ‘real’ driver (I mean on the real road outside) is the  car. I really love it for the simplicity and calm that it provides during the ride – but on the audio side it only has an old FM/AM radio and a CD slot … and not MP3 support in that one.</p><p>This is where the FM transmitter such as mine  comes really handy.</p><p>It supports two modes. One is being a Bluetooth slave of your phone – it just plays on the car speakers anything you are currently playing on your phone – it also has microphone builtin – so You can also use it as a ‘loud’ phone talking device.</p><p>I use it in a more simple mode – I just attach a tiny  pendrive to it – and play a random song of it.</p><p>Besides these features it also has additional USB-A port available to attach a cable to it and charge some device.</p><p>I was able to get one a new one for about .</p><p>The mentioned devices above are probably not the only ones that make my life easier – but definitely the most crucial ones.</p><p>Feel free to share your ‘helper’ hardware in the comments.</p>","contentLength":14151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44333960"},{"title":"Learn you Galois fields for great good (2023)","url":"https://xorvoid.com/galois_fields_for_great_good_00.html","date":1750465285,"author":"signa11","guid":164192,"unread":true,"content":"<p>This is the introduction to a series on Abstract Algebra. In particular, our focus will be on Galois Fields (also known as Finite Fields) and their applications in Computer Science. This is a project I've been excited about for many years now, but have been too busy to dedicate the adequate effort to meet my perfectionism standards (yay perfectionism!).</p><p>Many moons back I was self-learning Galois Fields for some erasure coding theory applications. I was quite disappointed with the lack of accessible resources for computer scientists.\nMany resources assumed either:</p><ol><li>Its beyond your skill level so let's oversimplify (<code>\"it's hard, don't worry about it\"</code>), or</li><li>You had prior Pure Math studies in Abstract Algebra (<code>\"it's easy, just use jargon jargon jargon\"</code>)</li></ol><p>Unfortunately, Abstract Algebra is not standard subject matter in most computer science curriculums. Often computer science mathematics start and end\nwith Discrete Math. If you're lucky, maybe you've also been exposed to Linear Algebra.</p><p>So, ultimately, I ended up self-learning Abstract Algebra from a pure math textbook. But for the great majority of computer scientists, there has to be a better way.\nThis series intends to fill this gap. This is the gentle step-by-step approach with applications implemented with actual code. It's the intro I wanted when I was starting out.</p><p>Abstract algebra is a beautiful subject. It's the idea that the numbers you're familiar with don't matter. The numbers are just\narbitrary labels. What matters is the relationships they have with other numbers when you add or multiply them. If the numbers\ndon't matter, then we can swap those labels for different labels and all the normal math rules will still work.</p><p>For example, we could create an algebra that allows us to add or multiply colors:</p><p>And this is what makes the subject abstract and confusing. How can you just say that numbers don't matter? It doesn't make sense.</p><p>And even so, why would we want to study this? Why would a computer scientist care?</p><p>Well, we use computer algorithms to manipulate data. We encode/decode it, we encrypt/decrypt it, we detect corruption, etc.\nWouldn't it be great if we could use normal math to do those things? Wouldn't it be great if would could add or multiply\nan 8-bit byte by an 8-bit byte and get another 8-bit byte? And if we could do that, could we also do Linear Algebra over Data? Yes, yes, and more yes.\nThis is why studying Abstract Algebra is worthwhile.</p><p>(Hint: Neither 263 nor 9282 are answers, they are not 8-bit numbers)</p><p>You can also make quirky blog posts that make your friends think you've gone crazy, like <a href=\"https://xorvoid.com/milk_and_cookies.html\">milk and cookies</a> 🥛 🍪 😊</p><p>The applications and algorithms are staggering. You interact with implementations of abstract algebra everyday: CRC,\nAES Encryption, Elliptic-Curve Cryptography, Reed-Solomon, Advanced Erasure Codes, Data Hashing/Fingerprinting, Zero-Knowledge Proofs, etc.</p><p>Having a solid-background in Galois Fields and Abstract Algebra is a prerequisite for understanding these applications.</p><h3>Approach: Step-by-step, Active Learning, and Literate Programming</h3><p>In this series, we will start from the very basics of theory and build up step-by-step to interesting applications such as Reed-Solomon,\nAES, etc. As such, the material will be  cumulative. Many exercises will be included to aid understanding. Each section will build gradually,\nbut will assume mastery of the previous section. Active learning is . We will expect that\nreaders are putting in adequate effort to grok the abstract concepts.</p><p>We won't assume too much mathematical background beyond high-school level algebra. However, in some applications (for example: Reed-Solomon),\nfamiliarity with Linear Algebra will be required. We won't explain Linear Algebra since great resources already\nexist <a href=\"https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D\">here</a>. You are encouraged to supplement as needed.</p><p>We will be including code in a Literate Programming style where appropriate.  For example, to aid understanding, we will build some interactive\ncommand-line tools that allow you to play around with various theoretical concepts in practice. All code will be the\n<a href=\"https://www.rust-lang.org/\">Rust Programming Language</a>, but advanced features will be intentionally avoided so that the code will be readable\nby most experienced computer programmers.</p><p>The main goal of this series is understandability and education. As such, the implementations will not be optimal. We will forgo nearly all\noptimizations you'd see in a production quality implementation: lookup-tables, vectorized instructions, clever representations, computer\narchitecture optimizations, etc. It's possible that later posts in the series will discuss optimizations, but this is not a primary goal. At the end,\nwe hope these implementations will serve as good reference implementations. This can have it's own merit since highly optimized algorithms are\noften difficult to read and understand.</p><p>For active learning, I strongly encourage you to do your own implementations and to play around with the command-line tools while reading. If you'd\nlike to open-source your implementations, I'm more than happy to link them here:</p><ul><li>Original: xorvoid (Rust): <a href=\"https://github.com/xorvoid/learn_you_galois_fields_for_great_good\">here</a></li></ul><h3>Planning is Essential, Plans are Worthless</h3><p>Here's the rough plan. We will see how it actually goes:</p><p>Other possible advanced subjects:</p><ul><li><b>Extended Euclidean Algorithm</b></li><li><b>Bit-matrix Representations</b></li><li><b>Fast Multiplication with FFTs</b></li><li><b>Vectorization Implementation Techniques</b></li></ul><p>The first few sections are theory. There's not much coding in these sections, but they are very important for success later in the series.\nDon't skip them.</p>","contentLength":5501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44333388"},{"title":"Plastic bag bans and fees reduce harmful bag litter on shorelines","url":"https://www.science.org/doi/10.1126/science.adp9274","date":1750463210,"author":"miles","guid":164244,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44333187"},{"title":"AbsenceBench: Language models can't tell what's missing","url":"https://arxiv.org/abs/2506.11440","date":1750458412,"author":"JnBrymn","guid":163936,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44332699"},{"title":"AMD's Freshly-Baked MI350: An Interview with the Chief Architect","url":"https://chipsandcheese.com/p/amds-freshly-baked-mi350-an-interview","date":1750454446,"author":"pella","guid":164085,"unread":true,"content":"<p>Transcript below has been edited for conciseness and readability.</p><p>And so we did that and delivered the fastest supercomputer in the world along with Lawrence Livermore, with El Capitan. But so as a consideration there, we wanted to have more compute units for XCD so that we could get 224 total within MI300A. On 350, where it's designed specifically as an accelerator only, a discrete accelerator, we had more flexibility there. And so we decided that having a power of two number of active compute units per die - so 36 physical, like you said, but we enable 32. Four of them, one per shader engine, are used for harvesting and we yield those out in order to give us good high-volume manufacturing through TSMC-N3, which is a leading edge technology. So we have some of the spare ones that allow us to end up with 32 actually enabled.</p><p>And that's a nice power of two, and it's easy to tile tensors if you have a power of two. So most of the tensors that you're working with, or many of them, would be matrices that are based on a power of two. And so it allows you to tile them into the number of compute units easily, and reduces the total tail effect that you may have. Because if you have a non-power of two number of compute units, then some amount of the tensor may not map directly nicely, and so you may have some amount of work that you have to do at the end on just a subset of the compute unit. So we find that there's some optimization there by having a power of two.</p><p>Alan: Yeah, so what we did, as you mentioned, so in MI350, the I/O dies, there's only two of them. And then each of them host four of the accelerator chiplets versus in MI300, we had four of the I/O dies, with each of them hosting two of the accelerator chiplets. So that's what you're talking about.</p><p>So what we did was, we wanted to increase the bandwidth from global, from HBM, which, MI300 was designed for HBM3 and MI350 was specially designed for HBM3E. So we wanted to go from 5.2 or 5.6 gigabit per second up to a full 8 gigabit per second. But we also wanted to do that at the lowest possible power, because delivering the bytes from HBM into the compute cores at the lowest energy per bit gives us more power at a fixed GPU power level, gives us more power into the compute at that same time. So on bandwidth-bound kernels that have a compute element, by reducing the amount of power that we spend in data transport, we can put more power into the compute and deliver a higher performance for those kernels.</p><p>So what we did by combining those two chips together into one was we were able to widen up the buses within those chips; so we deliver more bytes per clock, and therefore we can run them at a lower frequency and also a lower voltage, which gives us the V-squared scaling of voltage for the amount of power that it takes to deliver those bits. So that's why we did that.</p><p>So when we do our total power and thermal architecture of these chips, we consider from the motherboard all the way up to the daughterboards, which are the UBB (Universal Baseboard), the OAM (OCP Accelerator Module) modules in this case, and then up through the stack of CoWoS (Chip on Wafer on Substrate), the I/O dies, which are in this intermediate layer, and then the compute that's above those. So we look at the total thermal density of that whole stack, and the amount of thermal transport or thermal resistance that we have within that stack, and the thermal interface materials that we need in order to build on top of that for heat removal, right?</p><p>And so we offer two different classes of thermal solutions for MI350 series. One of them air-cooled, like you mentioned. The other one is a direct-attach liquid cool. So the cold plate would then, in the liquid cool plate, liquid-cooled case would directly attach to the thermal interface material on top of the chips. So we do thermal modeling of that entire stack, and work directly with all of our technology partners to make sure that the power densities that we build into the chips can be handled by that entire thermal stack up.</p><p>If you would like to support the channel, hit like, hit subscribe. And if you like interviews like this, tell us in the comments below. Also, there will be a transcript on the Chips and Cheese website. If you want to directly monetarily support Chips and Cheese, there's Patreon, as well as Stripe through Substack, and PayPal. So, thank you so much for that interview, Alan.</p>","contentLength":4428,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44332221"},{"title":"Wiki Radio: The thrilling sound of random Wikipedia","url":"https://www.monkeon.co.uk/wikiradio/","date":1750454115,"author":"if-curious","guid":164025,"unread":true,"content":"<div>\n    The thrilling sound of random Wikimedia\n  </div><div>\n    Inspired by <a href=\"https://wikitok.vercel.app/\" target=\"_blank\">WikiTok</a>, I thought I'd make something to discover\n    sounds uploaded to Wikimedia. From political speeches and bird noises to genuine bangers,\n    it's mostly wholesome, though I cant guarantee it won't play you something horrible once in a while.<p>\n    If you want shorter sounds, try it in Revolution 9 Mode.\n\n      </p></div>","contentLength":381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44332170"},{"title":"BYD begins testing solid-state EV batteries in the Seal","url":"https://electrek.co/2025/06/20/byd-tests-solid-state-batteries-seal-ev-with-1000-miles-range/","date":1750452142,"author":"toomuchtodo","guid":164024,"unread":true,"content":"<p><a href=\"https://electrek.co/guides/byd/\">BYD</a> has now begun testing solid-state EV batteries in its Tesla Model 3-rivalling Seal. Initial tests suggest that the total driving range could reach nearly 1,200 miles (1,875 km).</p><h2>BYD begins testing solid-state EV batteries in the Seal</h2><p>It has been over a decade since BYD first began researching and developing the promising new EV battery technology.</p><p>Last year, the company reached a milestone by testing its first solid-state battery cells with capacities of 20 Ah and 60 Ah. We knew BYD was planning to launch its first vehicles powered by the new batteries in 2027 after Sun Huajun, the CTO of BYD’s battery business, <a href=\"https://electrek.co/2025/02/17/byd-confirms-evs-all-solid-state-batteries-2027/\">confirmed the timeline</a> earlier this year.</p><p>At the 2025 China All-Solid-State Battery Innovation and Development Summit, Sun stated that BYD has officially installed solid-state batteries in its popular Seal EV and is now testing them on roads.</p><p>Once testing is finalized, which is expected to occur in 2027, BYD plans to begin installing solid-state batteries in its production vehicles.</p><p>Between 2027 and 2029, production will be limited during the first two years. However, in 2030, BYD plans to begin mass production. BYD has previously said that by the end of the decade, it expects “liquid and solid to be the same price.” In other words, solid-state batteries will be about the same cost as current liquid lithium-ion batteries.</p><p>The Seal, BYD’s Tesla Model 3-rivalling electric sedan, is expected to be the first EV available with solid-state batteries, starting in 2027. Other models will begin to hit the market in 2028 and the following years.</p><p>BYD’s solid-state batteries have an energy density of 400 Wh/kg, or nearly twice that of current lithium-ion batteries.</p><p>According to <a href=\"https://post.smzdm.com/p/a0zp33or/\">local reports</a>, BYD’s solid-state EV batteries set a record by gaining 1,500 km (932 miles) range in just 12 minutes of charging. </p><p>The test charged the battery to just 80%, meaning total EV range could reach upwards of 1,875 km (1,165 miles). Keep in mind, that is CLTC range. On the EPA scale, it would be closer to 1,300 km (808 miles), which is still way more than enough.</p><p>BYD’s Seal currently starts at just 175,800 yuan in China, or about $25,000. When it initially hits the market in 2027 with solid-state batteries, the Seal will likely be priced higher.</p><p>BYD is already dominating the global EV market. It just surpassed Tesla in Europe and the UK in monthly registrations for the first time, and this could be just the start.</p><p>With several new batteries and plenty of other EV technologies, including ultra-fast chargers, smart driving features, and advanced new platforms, BYD is laying the groundwork for more growth over the next few years.</p><p>Not only that, BYD is already known for its low-cost cars like the Seagull (Dolphin Surf in Europe), priced under $10,000 in China. The new tech is expected to unlock longer driving range, faster charging, and lower costs.</p><p>BYD will compete with CATL, Mercedes-Benz, Volkswagen, Stellantis, Nissan, and several others that are also aiming to launch their first EVs with solid-state batteries around 2027 or 2028. Nissan’s director of product planning in Europe, Christop Ambland, confirmed the company’s timeline this week with&nbsp;, saying, “We will be ready for SSB (solid-state batteries) in 2028.”</p><div><p><em>FTC: We use income earning auto affiliate links.</em><a href=\"https://electrek.co/about/#affiliate\">More.</a></p></div>","contentLength":3314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44331898"},{"title":"Show HN: Inspect and extract files from MSI installers directly in your browser","url":"https://pymsi.readthedocs.io/en/latest/msi_viewer.html","date":1750449841,"author":"rmast","guid":163937,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44331518"},{"title":"Harper – an open-source alternative to Grammarly","url":"https://writewithharper.com/","date":1750449105,"author":"ReadCarlBarks","guid":164023,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44331362"},{"title":"The JAWS shark is public domain","url":"https://ironicsans.ghost.io/how-the-jaws-shark-became-public-domain/","date":1750447730,"author":"MBCook","guid":163891,"unread":true,"content":"<p>As we’re all celebrating the <a href=\"https://www.jaws50th.com/?ref=ironicsans.ghost.io\" rel=\"noreferrer\">50th anniversary</a> of the movie , here’s something I bet you didn’t know: Due to a fluke of publishing and copyright law, the  shark is public domain.</p><p>It’s not the  of the shark that’s public domain – or someone would surely be making a low-budget horror prequel about how he became the Amity Island Killer. But I’m talking about the famous shark painting from the movie poster:</p><p>Yep. That painting, the same one that appeared on the cover of the paperback edition of the novel, is public domain.</p><p>This is kind of a wild story.</p><p>When the book first came out, it didn’t have this cover art. An old <a href=\"https://timesmachine.nytimes.com/timesmachine/1974/04/21/119458470.html?pageNumber=266&amp;ref=ironicsans.ghost.io\" rel=\"noreferrer\">article</a> about the book’s origin explains that the author, Peter Benchley, actually had his own idea for the cover. He thought it should show “a peaceful unsuspecting town through the bleached jaws of a shark.” He pitched his idea to Doubleday, who was publishing the hardcover version of the book.</p><p>So Doubleday senior editor Tom Congdon worked with art director Alex Gotfryd and had an artist produce this mock-up:</p><p>Congdon didn’t like it. He said, “the shark’s bones look too liplike and pendulous.” A preview of the cover was shown at a book sales manager’s conference, and there was “considerable resistance” from the salesmen who said it resembled a vagina with teeth.</p><p>The book was supposed to come out in January, 1974, but publication was delayed until February to rework the cover.</p><p>Congdon asked, “Can we have just a fish on the cover?”</p><p>But Gotfryd said no. “The cover’s not big enough. It will look like a sardine.”</p><p>So they settled on no image at all. The first printed cover was just black:</p><p>Bantam had purchased the paperback rights to the book, but when Bantam president Oscar Dystal saw the empty cover, he didn’t like it. He said, “Without an image, no one would know what  was. It could have been a book about dentistry.” It needed a shark.</p><p>So Gotfryd contacted an artist named Paul Bacon who made a rough sketch of the shark’s head, and at Gotfryd’s suggestion added a swimmer for scale. The next day, Bacon came back with the finished artwork that became the new hardcover dust jacket:</p><p>Congdon later said, “We realized that the new version looked like a penis with teeth. But was that bad?”</p><p>Has any other design project ever gone from “we can’t use that because it looks like a vagina with teeth” to “that looks like a penis with teeth but let’s go with it?”</p><p>A year later, when Bantam was preparing to publish the paperback edition, they hired artist Roger Kastel to make an updated version of the cover. He went to the Museum of Natural History to study sharks, and he had a model pose across a couple of stools for reference of what someone looks like swimming. He used those elements in creating the now-famous illustration:</p><p>There are slightly different accounts of how Universal acquired permission to use the illustration on the movie poster. Kastel’s <a href=\"http://www.rogerkastel.com/biography.htm?ref=ironicsans.ghost.io\" rel=\"noreferrer\">official website</a> says that “Universal Studios, so impressed by the work, purchased the right to use this image as the poster for the movie.”</p><p>But a 2012 <a href=\"https://www.empireonline.com/movies/features/jaws-unsung-heroes/?ref=ironicsans.ghost.io\" rel=\"noreferrer\">article</a> in  says that Universal actually got the rights from Bantam for free:</p><blockquote>Impressed by the cover, Universal purloined Kastel’s work for the movie poster — Bantam books chief Oscar Dystel gave it to the filmmakers for free, losing out on millions of dollars — and it quickly became iconic.</blockquote><p>Whether Universal paid for the rights or not, Kastel became bitter about all the places it was showing up.</p><p>In 2015, he <a href=\"https://nypost.com/2015/07/26/how-the-famous-poster-from-jaws-was-created-and-lost/?ref=ironicsans.ghost.io\" rel=\"noreferrer\">told</a> the New York Post, “What really bothered me was that they used the image for merchandising. You see that poster on everything.” And in 2020, Kastel <a href=\"https://michaelpcoleman.wordpress.com/2020/07/18/jaws-iconic-artwork-and-the-artist-behind-it/?ref=ironicsans.ghost.io\" rel=\"noreferrer\">told</a> writer Michael P. Coleman, “We were floored by how they merchandised that image, from t-shirts to cartoons.”</p><p>It’s unclear when exactly Kastel realized that there was something fishy about the painting’s copyright situation. I can only speculate that at some point he wondered if he was entitled to some of the money from all the licensing, and discovered that the copyright to the image had never been properly established.</p><p>See, when he made the painting in 1975, copyright was still ruled by a 1909 law that said you had to include a copyright notice upon publication of a work, and that notice had to include your name. When the book was published, it carried no such notice for the artwork. It only had a copyright notice for the text. That meant that the painting became public domain as soon as it was published.</p><p>In a bit of timing bad luck, a new copyright law enacted just a year after the book came out eliminated the notice requirement.</p><p>So in early 2013, almost 40 years after  was published, Kastel filed a copyright application for the  illustration. But the copyright office denied it.</p><p>He filed an appeal. They denied that, too.</p><p>In 2014, he submitted a final appeal to the Copyright Office Review Board, and the <a href=\"https://www.copyright.gov/rulings-filings/review-board/docs/2014-appeal-JawsIllustration2014.pdf?ref=ironicsans.ghost.io\" rel=\"noreferrer\">decision</a> from the board is <a href=\"https://en.wikisource.org/wiki/Jaws_Illustration_%28USCO_Review_Board%2C_2014%29?ref=ironicsans.ghost.io\" rel=\"noreferrer\">available</a> to read online. The main points are:</p><ul><li>The image was published without a proper copyright notice, so it became public domain under the law at the time.</li><li>The fact that the book said “Copyright © 1974 Peter Benchley” isn’t good enough to let the public know that the cover is copyrighted because that’s not the name of the cover art’s copyright owner.</li><li>This situation is not like magazines and anthologies, which are collective works with multiple contributors that can be covered by a single copyright notice. One illustration on the cover of a novel doesn’t make it a collective work.</li><li>If Benchley had licensed the artwork from Kastel or had some other legal relationship, that might make a difference. But he didn’t, so the works are unrelated and require separate notices.</li></ul><p>So the review board unambiguously rejected the claim:</p><blockquote>The Board has concluded that the copyright notice in the Book, which includes only Peter Benchley’s name, does not meet the statutory notice requirement under the 1909 Act and, as such, the Work was forfeited to the public domain upon its publication.</blockquote><p>So whether Universal paid anything for permission to use the artwork becomes a moot point because it turns out that they didn’t need permission. And anyone could have sold merchandise with the image without paying any licensing fees to Universal, Bantam, or Kastel.</p><p>Kastel <a href=\"https://www.nytimes.com/2023/11/17/arts/roger-karl-kastel-dead-jaws-poster.html?ref=ironicsans.ghost.io\" rel=\"noreferrer\">died</a> in 2023 and, while he didn’t see any  royalties from merchandise that featured his artwork, he did get more work as a result of it, including illustrating the inspired poster for .</p><p>One thing that Kastel definitely did own is the original artwork used for the  book cover and poster. Unfortunately, the painting went missing in 1976 and nobody knows where it is.</p><p>One story goes that the painting went on a national tour to promote the book, making stops to appear in various book store windows, and then disappeared somewhere in Hollywood. Or it may have been last seen hanging in an exhibit at the Society of Illustrators in New York.</p><p>Kastel’s son Matthew thinks it was last seen at the New York Historical Society. At least that’s what he said in an article a few years back for <a href=\"https://www.dailyartmagazine.com/jaws-missing-painting/?ref=ironicsans.ghost.io\" rel=\"noreferrer\">Daily Art Magazine</a> but I suspect he was actually thinking of the Society of Illustrators. Either way, he wrote:</p><blockquote>But the question remains. What happened to my father’s work?<p>Where is one to start on a mystery over 45 years old with no clues? Outside of putting my father’s&nbsp;</p>&nbsp;illustration on the back of milk cartons under the caption&nbsp;, I have no clue.<p>Was the painting simply lost, misplaced, or thrown away like an old movie prop by Universal out of lack of care or ignorance? Or was it stolen somewhere in Universal’s care by an admirer and/or enterprising thief?</p><p>My recourse is limited. By writing this article, I am taking a longshot approach that someone out there reading this may know about its whereabouts or fate and step forward. </p></blockquote><p>If you happen to know where the original  painting is, <a href=\"http://thirdstrikeproductions.com/contact.htm?ref=ironicsans.ghost.io\" rel=\"noreferrer\">here</a> is where you can reach out to Matthew Kastel.</p><p>So that’s the story of how a pop culture icon became freely available for anyone to use. It’s good that creative works eventually become public domain, but I also believe an artist should be able to enjoy the fruits of his work in his lifetime if he wants to. So while I’m usually in favor of the commons, I feel like this time... it’s personal.</p><p>What a story. I was originally inspired by <a href=\"https://www.reddit.com/r/publicdomain/comments/1l3ntf7/stupid_fact_the_book_jaws_is_copyrighted_but_the/?ref=ironicsans.ghost.io\" rel=\"noreferrer\">a reddit post</a> where someone noticed that Wikimedia lists the original  dust jacket as public domain (the “penis with teeth” illustration). I thought that was interesting and dug deeper. As I went down that rabbit hole I learned about the situation with Kastel’s better-known paperback illustration.</p><p>If you liked this article, you can show your support by becoming a <a href=\"https://ironicsans.ghost.io/#/portal/signup\" rel=\"noreferrer\">paid subscriber</a> of this newsletter, a <a href=\"https://ironicsans.ghost.io/#/portal/signup\" rel=\"noreferrer\">free subscriber</a> if you aren’t already, or by giving a <a href=\"https://buymeacoffee.com/ironicsans?ref=ironicsans.ghost.io\" rel=\"noreferrer\">one-time tip</a>, or just leaving a comment or reply to tell me you liked it.</p><p>And here’s a personal fun fact for people who got this far: My wedding was at the Society of Illustrators. We got married surrounded by incredible paintings by artists like J. C. Leyendecker and Norman Rockwell. As far as I recall, the  poster art was not among them.</p><p>Thanks as always for reading. See you next time!</p><p>P.S. Have you played <a href=\"https://gisnep.com/?ref=ironicsans.ghost.io\" rel=\"noreferrer\">Gisnep</a> lately?</p>","contentLength":9227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44331125"},{"title":"EU Eyes Ditching Microsoft Azure for France's OVHcloud","url":"https://www.euractiv.com/section/tech/news/scoop-commission-eyes-ditching-microsoft-azure-for-frances-ovhcloud-over-digital-sovereignty-fears/","date":1750447213,"author":"doener","guid":163839,"unread":true,"content":"<p><strong>The European Commission is in advanced business negotiations with OVHcloud, the France-based major European cloud service provider, to transition its cloud services away from Microsoft, according to three senior sources with internal knowledge of the matter who spoke to Euractiv on condition of anonymity.</strong></p><p>\nThe infrastructure shift is being driven by a push for European digital sovereignty in the cloud market, following <a href=\"https://www.euractiv.com/section/tech/news/microsofts-email-shutdown-of-icc-prosecutor-fuels-eu-fears-of-us-tech-blackmail/\" target=\"_blank\" rel=\"noopener\">concerns raised</a> by a US executive order that led to the shutdown of Microsoft services for an employee of a European-based institution.\n</p><p>\nThe goal of the move would be to ensure that European institutions have greater control over their digital infrastructure and data – an idea that has been championed by the <a href=\"https://www.euractiv.com/section/tech/news/european-industry-big-win-germany-france-both-support-sovereign-eu-based-tech-infrastructure/\" target=\"_blank\" rel=\"noopener\">EuroStack initiative</a>. It is also a blow for US tech behemoth Microsoft, which has been striving to reassure its European customers in the past weeks.\n</p><p>\nOnce the European Commission \"gets its house into order,\" it is expected to set a precedent for national public administrations to direct public procurement funds towards homegrown cloud providers, one source said. The Commission sees itself as a trend setter, they added, aligning with its broader strategy to enhance the EU's digital autonomy and reduce reliance on non-European tech giants.\n</p><p>\nWe understand the Commission has been in discussions with OVHcloud for several weeks. However, an unknown number of other European cloud providers, including Germany's IONOS, France's Scaleway and Italy's Aruba, are also being considered as potential alternatives.\n</p><p>\nA unique aspect of this situation is that, for the first time, the two key digital departments of the Commission – DG CNECT, which drafts and enforces digital policies, and DG DIGIT, the IT department – are under the oversight of a single Commissioner (Henna Virkkunen) with a tech sovereignty portfolio.\n</p><p>\nThis consolidation has made it easier to harmonise the political and technical priorities of the European executive, our sources told us.\n</p><p>\n\"Discussions are indeed underway, both with the Commission and with other public and private institutions and organisations that are evaluating projects to migrate to a sovereign cloud,\" an OVHcloud spokesperson told Euractiv when we sought comment.\n</p><p>\nThe Commission is \"constantly scanning the market\" and already \"has a contract with OVHcloud\" a Commission spokesperson said in response to Euractiv's request for comment. It did not confirm whether the Commission will actually switch away from Microsoft Azure.\n</p><p>\nIn January, Euractiv <a href=\"https://www.euractiv.com/section/tech/news/internal-documents-reveal-commission-fears-over-microsoft-dependency/\" target=\"_blank\" rel=\"noopener\">revealed</a>&nbsp;that the Commission was concerned about its reliance on Microsoft, quoting internal documents.\n</p><p>\nAdditionally, the EU institutions watchdog, the European Data Protection Supervisor <a href=\"https://www.euractiv.com/section/tech/news/eu-commission-breached-data-protection-rules-using-microsoft-365-eu-watchdog-found/\" target=\"_blank\" rel=\"noopener\">found</a> last year that the EU's executive is in breach of data protection rules that apply to EU institutions over its use of Microsoft Azure cloud for some of its data.\n</p>","contentLength":2888,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44331045"},{"title":"Malicious AI swarms can threaten democracy","url":"https://osf.io/preprints/osf/qm9yk_v2","date":1750445477,"author":"anigbrowl","guid":163809,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44330755"},{"title":"YouTube's new anti-adblock measures","url":"https://iter.ca/post/yt-adblock/","date":1750438895,"author":"smitop","guid":163935,"unread":true,"content":"<p>Over the past few months, YouTube has been trying another round of anti-adblock measures. Currently the anti-adblock stuff is being A/B tested, and one of my accounts is in the experimental group. I wrote a filter that partially avoids one of the anti-adblock measures, fake buffering, on uBlock Origin (and Brave browser, since it uses the same filter rules). (It’s already in the default filter lists, you don’t need to manually add the filter.)</p><p>One thing that people have ran into is “fake buffering”, where videos will take a while to load due to a lot of buffering, but only at the very start of the video (there’s no mid-video fake buffering). As I’ll explain, the fake buffering is 80% of the length of the ads you would’ve seen, so even with fake buffering you’re still saving time using an adblocker.</p><p>InnerTube is YouTube’s first party internal API that the web client and mobile apps use to interact with videos and get details about them. There are some legacy API endpoints that don’t go through InnerTube, but none of those are relevant today. InnerTube endpoints look like <code>https://www.youtube.com/youtubei/</code>. One of those endpoints is , which the web client calls when you click on a video to get data about the URL and GVS stream URLs.</p><p>GVS (Google Video Services) is a service that serves video streams for YouTube, Google Drive, and Google Photos. To stream a video from GVS, you need to get a GVS  URL from InnerTube (or the Drive/Photos internal API). GVS URLs are signed and have an expiry time (usually 6 hours), so you can’t construct them on your own, you need to get one from InnerTube. One weird thing about GVS is that it isn’t only hosted from Google’s data center: ISPs can put <a href=\"https://support.google.com/interconnect/answer/9058809\">Google Global Cache</a> servers in their infrastructure so that they can serve YouTube videos without needing to send traffic outside the ISP’s network (only public/unlisted YouTube videos are served by GGC; private YT videos and Drive/Photos videos are always served from Google data centers). GVS URLs look like <code>https://rr1---sn-gvbxgn-tt1e6.googlevideo.com/videoplayback?expire=1750321185&amp;...</code> (but with a lot more query parameters).</p><p>Originally the web client streamed video by just using some query parameters to the GVS URL to specify what range of video it wanted, and GVS responded with the video contents for that range. But for complicated reasons, YouTube decided to improve on this with SABR (Server ABR (Adaptive Bit Rate)), which is YouTube’s proprietary binary protocol for streaming video data, which is better at avoiding buffering than today’s open formats (e.g. MPEG-DASH). One thing that SABR supports is the server sending a backoff to the client, instructing the client to wait some amount of time before trying again instead of sending video/audio data.</p><h2>The source of fake buffering</h2><p>What’s happening is that InnerTube is providing GVS streams that will give a backoff of 80% of the ad duration for ads for the first  request (for the content video, not the ads), so for example if the ad is 15 seconds you’ll get 12 seconds of backoff when blocking ads. If you have an unskippable 6 second ad AND a 15 second unskippable ad together the backoff will be 16.8 seconds. To be clear this isn’t server-side ad insertion; the ad and content streams are still separate (YouTube  doing a server-side ad insertion experiment, but that’s separate from fake buffering). The “Experiencing interruptions” dialogs are likely triggered by long backoffs from GVS.</p><p>This backoff  happen if you’re in the A/B test, regardless of if YouTube thinks you’re using an adblocker. You just don’t notice it if you’re not blocking ads, because the web client starts loading the content video while the ad plays, so the only difference for non-ad-blocking users is that the content video doesn’t start buffering until the ad is 80% over.</p><p>I’ve seen claims online that YouTube is “DAMAGING Computers By SPIKING CPU Usage If You Use Ad Block”. This is completely false; YouTube doesn’t use CPU usage waiting for the backoff to expire (and even if they used a spinloop to implement the wait, maxing a single core for &lt;30 seconds won’t damage any CPU).</p><p>How can you avoid getting backoffed until the unskippable ad is over? Don’t get served an ad in the first place. If you set the <code>playbackContext.contentPlaybackContext.isInlinePlaybackNoAd</code> property in player requests to true, InnerTube won’t serve you any ads and thus won’t include any backoff in the GVS streams.</p><p>We can write a filter rule that makes it so whenever the web client stringifies JSON bound for a server request, we add <code>\"isInlinePlaybackNoAd\":true</code> to the stringified JSON.</p><div><pre tabindex=\"0\"><code data-lang=\"adb\"></code></pre></div><p>How did I know to set that property? It’s referenced in the frontend JavaScript, so I could have spent a bunch of time reading all that. But there’s an easier way - while the web client interacts with InnerTube using JSON, that JSON API is actually generated from a Protocol Buffers definition, and there’s a way you can extract most of the underlying protobuf definition. I used <a href=\"https://github.com/ddd/req2proto\">req2proto</a>, which is a tool to extract protobuf definitions from Google’s internal APIs to get the full definitions used in the  call, and used that to find the  property.</p><p>This method only works for warm navigation, where you’ve already loaded the YouTube single page app and are clicking around within it. When you navigate directly to a watch page, the YouTube backend embeds a player response directly into the page as . Since the player request is made on the backend, we can’t set  on it.  One way to fix this for cold loads is to just remove that initial data to force YouTube to make a player request we can control (but see below before using these):</p><div><pre tabindex=\"0\"><code data-lang=\"adb\"></code></pre></div><p>This approach has some problems though, so you might not want to use it:</p><ul><li>It completely breaks livestreams, and probably some other things I haven’t tested</li><li>It causes the video player to briefly flash</li><li>It slows the page loading time</li></ul><h2>Bypassing the locker script</h2><p>So that filter kinda worked, but sometimes uBlock Origin wasn’t hooking . I investigated further and it turns out YouTube is running an A/B test where sometimes they add this to the frontend HTML as the very first thing in the  tag:</p><div><pre tabindex=\"0\"><code data-lang=\"html\"></code></pre></div><p>This locks a few global objects by using  to set them as non-writable, which prevents later code from overwriting them with a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy\">Proxy</a> that alters their behaviour. So uBlock Origin can only proxy JSON.stringify if it can run before this locker script does. On Firefox this is easily resolvable - you can use a <a href=\"https://github.com/gorhill/ublock/wiki/static-filter-syntax#html-filters\">HTML filter</a> to filter out the script tag from the source HTML before the page even starts being parsed. But that relies on extension APIs that Chromium doesn’t support.</p><p>The “fix” for the locker script so far is to hook  instead of .  is another function that handles the request body before it gets fetched. It would be nice if there was a way to actually defuse the locker script, instead of working around it. The version of the filter that hooks  instead is more complicated because uBO’s scriptlets don’t let you replace text on a key of an object, so the filter in uBO’s filter list injects this JS:</p><div><pre tabindex=\"0\"><code data-lang=\"js\"></code></pre></div><p>Thanks to the <a href=\"https://github.com/uBlockOrigin/uAssets/\">uAssets</a> maintainers for helping with that.</p><p>If you have any questions for me you can DM me on Discord as .</p>","contentLength":7263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44329712"},{"title":"Tuxracer.js play Tux Racer in the browser","url":"https://github.com/ebbejan/tux-racer-js","date":1750438452,"author":"retro_guy","guid":163934,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44329640"},{"title":"Alpha Centauri","url":"https://www.filfre.net/2025/06/alpha-centauri/","date":1750438214,"author":"doppp","guid":164022,"unread":true,"content":"<p>In the spring of 1996, Brian Reynolds and Jeff Briggs took a long, hard look around them and decided that they’d rather be somewhere else.</p><p>At that time, the two men were working for MicroProse Software, for whom they had just completed&nbsp;<a href=\"https://www.filfre.net/2023/01/sequels-in-strategy-gaming-part-1-civilization-ii\"></a>, with Reynolds in the role of primary designer and programmer and Briggs in that of co-designer, producer, and soundtrack composer. They had brought the project in for well under $1 million, all that their bosses were willing to shell out for what they considered to be a game with only limited commercial potential. And yet the early sales were very strong indeed, proof that the pent-up demand for a modestly modernized successor to <a href=\"https://www.filfre.net/2018/03/the-game-of-everything-part-1-making-civilization\">Sid Meier’s masterstroke</a> that Reynolds and Briggs had identified had been very, very real. Which is not to say that they were being given much credit for having proved their managers wrong.</p><p>MicroProse’s executives were really Spectrum Holobyte’s executives, ever since the latter company had acquired the former in December of 1993, in a deal lubricated by oodles of heedless venture capital and unsustainable levels of debt. Everything about the transaction seemed off-kilter; while MicroProse had <a href=\"https://www.filfre.net/tag/microprose/?order=asc\">a long and rich history</a> and product portfolio, Spectrum Holobyte was known for <a href=\"https://www.mobygames.com/group/230/falcon-series/sort:date/\">the  series</a> of ultra-realistic combat flight simulators, for the first version of&nbsp;<a href=\"https://www.filfre.net/2017/06/a-tale-of-the-mirror-world-part-3-a-game-of-falling-shapes\"></a> to run on Western personal computers, and for not a whole lot else. Seeing the writing on the wall, “Wild Bill” Stealey, the partner in crime <a href=\"https://www.filfre.net/2015/03/microproses-simulation-industrial-complex-or-the-ballad-of-sid-and-wild-bill\">with whom Sid Meier had founded MicroProse</a> back in 1982, walked out the door soon after the shark swallowed the whale. The conjoined company went on to lose a staggering $57.8 million in two years, despite such well-received, well-remembered, and reasonably if not extraordinarily popular games as <a href=\"https://www.filfre.net/2020/09/x-com\"></a>, <a href=\"https://www.filfre.net/2020/10/transport-tycoon\"></a>, and&nbsp;<a href=\"https://www.filfre.net/2020/12/ethics-in-strategy-gaming-part-2-colonization\"></a>. By the spring of 1996, the two-headed beast, which was still publishing games under both the Spectrum Holobyte and MicroProse banners, was teetering on the brink of insolvency, with, in the words of its CEO Stephen M. Race, a “negative tangible net worth.” It would require a last-minute injection of foreign investment capital that June to save it from being de-listed from the NASDAQ stock exchange.</p><p>The unexpectedly strong sales of  — the game would eventually sell 3 million copies, enough to make it MicroProse’s best seller ever by a factor of three — were a rare smudge of black in this sea of red ink. Yet Reynolds and Briggs had no confidence in their managers’ ability to build on their success. They thought it was high time to get off the sinking ship, time to get away from a company that was no longer much fun to work at. They wanted to start their own little studio, to make the games they wanted to make their way.</p><p>But that, of course, was easier said than done. They had a proven track record inside the industry, but neither Brian Reynolds nor Jeff Briggs was a household name, even among hardcore gamers. Most of the latter still believed that  was the work of Sid Meier — an easy mistake to make, given how prominently Meier’s name was emblazoned on the box. Reynolds and Briggs needed investors, plus a publisher who would be willing to take a chance on them. Thankfully, the solution to their dilemma was quite literally staring them in the face every time they looked at that  box: they asked Sid Meier to abandon ship with them. After agonizing for a while about the prospect of leaving the company he had co-founded in the formative days of the American games industry, Meier agreed, largely for the same reason that Reynolds and Briggs had made their proposal to him in the first place: it just wasn’t any fun to be here anymore.</p><p>So, a delicate process of disentanglement began. Keenly aware of the legal peril in which their plans placed them, the three partners did everything in their power to make their departure as amicable and non-dramatic as possible. For instance, they staggered their resignations so as not to present an overly united front: Briggs left in May of 1996, Reynolds in June, and Meier in July. Even after officially resigning, Meier agreed to continue at MicroProse for some months more as a part-time consultant, long enough to see through <a href=\"https://www.filfre.net/2023/09/magic-and-loss-part-2-magic-on-the-screen\">his computerized version</a> of the ultra-popular <a href=\"https://www.filfre.net/2023/09/magic-and-loss-part-1-magic-in-the-cards\"></a> collectible-card game. He didn’t even complain when, in an ironic reversal of the usual practice of putting Sid Meier’s name on things that he didn’t actually design, his old bosses made it clear that they intended to scrub him from the credits of this game, which he had spent the better part of two years of his life working on. In return for all of this and for a firm promise to stay in his own lane once he was gone, he was allowed to take with him all of the code he had written during the past decade and a half at MicroProse. “They didn’t want to be making detailed strategy titles any more than we wanted to be making  flight simulators,” writes Meier in his memoir. On the face of it, this was a strange attitude for his former employer to have, given that  was selling so much better than any of its other games. But Brian Reynolds, Jeff Briggs, and Sid Meier were certainly not inclined to look the gift horse in the mouth.</p><p>They decided to call their new company Firaxis Games, a name that had its origin in a piece of music that Briggs had been tinkering with, which he had dubbed “Fiery Axis.” Jason Coleman, a MicroProse programmer who had coded on , quit his job there as well and joined them. Sid Meier’s current girlfriend and future second wife Susan Brookins became their office manager.</p><p>The first office she was given to manage was a cramped space at the back of Absolute Quality, a game-testing service located in Hunt Valley, Maryland, just a stone’s throw away from MicroProse’s offices. Their landlords/flatmates were, if nothing else, a daily reminder of the need to test, test, test when making games. Brian Reynolds (who writes of himself here in the third person):</p><blockquote><p>CEO Jeff Briggs worked the phones to rustle up some funding and did all the hard work of actually putting a new company together. Sid Meier and Brian Reynolds worked to scrape together some playable prototype code, and Jason Coleman wrote the first lines of JACKAL, the engine which these days pretty much holds everything together. Office-manager Susan Brookins found us some office furniture and bought crates of Coke, Sprite, and Dr. Pepper to stash in a mini-fridge Brian had saved from his college days. We remembered that at some indeterminate point in the past we were considered world-class game designers, but our day-to-day lives weren’t providing us with a lot of positive reinforcement on that point. So, for the first nine months of our existence as a company, we clunked over railroad tracks in the morning, played Spy Hunter in the upstairs kitchen, and declared “work at home” days when Absolute Quality had competitors in the office.</p></blockquote><p>Once the necessary financing was secured, the little gang of five moved into a proper office of their own and hired more of their former colleagues, many of whom had been laid off in a round of brutal cost-cutting that had taken place at MicroProse the same summer as the departure of the core trio. These folks bootstrapped Firaxis’s programming and art departments. Thanks to the cachet of the Sid Meier name/brand, the studio was already being seen as a potential force to be reckoned with. Publishers flew out to them instead of the other way around to pitch their services. In the end, Firaxis elected to sign on with Electronic Arts, the biggest publisher of them all.</p><p>The three founding fathers had come into the venture with a tacit understanding about the division of labor. Brian Reynolds would helm a sprawlingly ambitious but fundamentally iterative 4X strategy game, a “spiritual successor” to  and . This was the project that had gotten Electronic Arts’s juices flowing; its box would, it went without saying, feature Sid Meier’s name prominently, no matter how much or how little Meier ultimately had to do with it. Meanwhile Meier himself would have free rein to pursue the quirkier, more esoteric ideas that he had been indulging in ever since finishing . And Briggs would be the utility player, making sure the business side ran smoothly, writing the music, and pitching in wherever help was needed on either partner’s project.</p><p>Sid Meier has a well-earned reputation for working rapidly and efficiently. It’s therefore no surprise that he was the first Firaxis designer to finish a game, and by a wide margin at that. Called simply  — or rather&nbsp;<a href=\"https://www.mobygames.com/game/6/sid-meiers-gettysburg/\"></a> — it was based upon the battle that took place in that Pennsylvania city during the American Civil War. More expansively, it was an attempt to make a wargame that would be appealing to grognards but accessible enough to attract newcomers, by virtue of being real-time rather than turn-based, of being audiovisually attractive, and of offering a whole raft of difficulty levels and tutorials to ease the player into the experience. Upon its release in October of 1997,  magazine called it “a landmark, a real-time-strategy game whose unique treatment of its subject matter points to a [new] direction for the whole genre.” For my own part, being neither a dedicated grognard nor someone who shares the fascination of so many Americans for the Civil War, I will defer to the contemporary journal of record. I’m sure that  does what it does very well, as almost all Sid Meier games do. On the broader question of whether it brought new faces into the grognard fold, the verdict is more mixed. Meier writes today that “it was a success,” but it was definitely not a hit on the scale of <a href=\"https://www.filfre.net/2020/11/ethics-in-strategy-gaming-part-1-panzer-general\">SSI’s </a>, the last wargame to break out of its ghetto in a big way.</p><p>To the hungry eyes of Electronic Arts,  was just the appetizer anyway. The main dish would be .</p><p>The idea for&nbsp; had been batted around intermittently as a possible “sequel to&nbsp;” ever since Sid Meier had made one of the two possible victory conditions of that game the dispatching of a spaceship to that distant star, an achievement what was taken as a proof that the nation so doing had reached the absolute pinnacle of terrestrial achievement. In the wake of the original s release and success, Meier had gone so far as to prototype some approaches to what happens after humanity becomes a star-faring species, only to abandon them for other things. Now, though, the old idea was newly appealing to the principals at Firaxis, for commercial as much as creative reasons. They had left the rights to the  franchise behind them at MicroProse, meaning that a Firaxis&nbsp; was, at least for the time being, not in the cards. But if they made a game called&nbsp; that used many of the same rules, systems, and gameplay philosophies, and that sported the name of Sid Meier on the box… well, people would get the message pretty clearly, wouldn’t they? This would be a sequel to  in all but its lack of a Roman numeral.</p><p>When he actually started to try to make it happen, however, Brian Reynolds learned pretty quickly why Sid Meier had abandoned the idea. What seemed like a no-brainer in the abstract proved beset with complications when you really engaged. The central drama of  was the competition and conflict  civilizations — which is also, not coincidentally, the central drama of human history itself. But where would the drama come from for a  group of enlightened emissaries from an earthly Utopia settling an alien planet? Whom would they compete against? Just exploring and settling and building weren’t enough, Reynolds thought. There needed to be a source of tension. There needed to be an Other.</p><p>So, Brian Reynolds started to read — not history this time, as he had when working on , but science fiction. The eventual manual for  would list seven authors that Reynolds found particularly inspiring, but it seems safe to say that his lodestar was Frank Herbert, the first writer on the list. This meant not only the inevitable <a href=\"https://www.filfre.net/2018/11/controlling-the-spice-part-1-dune-on-page-and-screen\"></a>, but also — and perhaps even more importantly — a more obscure Herbert novel called&nbsp; that wasn’t even still in print at the time. One of its author’s more polarizing creations,&nbsp; is an elliptical, intensely philosophical and even spiritual novel about the attempt of a group of humans to colonize a planet that begins to manifest a form of sentience of its own, and proves more than capable of expressing its displeasure at their presence on its surface. This same conceit would become the central plot hook of .</p><p>Yes, I just used the word “plot.” And make no mistake about its significance. Of the threads that have remained unbroken throughout Sid Meier’s long career in game design, one of the most prominent is this mild-mannered man’s deep-seated antipathy toward any sort of set-piece, pre-scripted storytelling in games. Such a thing is, he has always said, a betrayal of computer games’ defining attribute as a form of media, their interactivity. For it prevents the player from playing her way, having her own fun, writing her own personal story using the sandbox the designer has provided. Firaxis had never been intended as exclusively “Sid Meier’s company,” but it had been envisioned as a studio that would create, broadly speaking, his type of games. For Reynolds to suggest injecting strong narrative elements into the studio’s very first 4X title was akin to Deng Xiaoping <a href=\"https://analog-antiquarian.net/2022/12/30/chapter-25-the-passing-of-the-old-guard/\">suggesting to his politburo</a> that what post-Cultural Revolution China could really use was a shot of capitalism.</p><p>And yet Meier and the others around Reynolds let him get away with it, just as those around Deng did. They did so because he had proven himself with  and&nbsp;, because they trusted him, and because  was at the end of the day his project. They hadn’t gone to the trouble of founding Firaxis in order to second-guess one another.</p><p>Thus Reynolds found himself writing far more snippets of static text for his strategy game than he had ever expected to. He crafted a series of textual “interludes” — they’re described by that word in the game — in which the planet’s slowly dawning consciousness and its rising anger at the primates swarming over its once-pristine surface are depicted in ways that mere mechanics could not entirely capture. They appear when the player reaches certain milestones, being yet one more attempt in the annals of gaming history to negotiate the tricky terrain that lies between emergent and fixed narrative.</p><div><a href=\"https://www.filfre.net/2025/06/alpha-centauri/alpha1/\" rel=\"attachment wp-att-6377\"><img decoding=\"async\" aria-describedby=\"caption-attachment-6377\" src=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha1-300x225.jpg\" alt=\"\" width=\"600\" height=\"450\" srcset=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha1-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2025/06/alpha1-768x576.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/06/alpha1.jpg 800w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a><p>An early interlude, delivering some of the first hints that the planet on which you’ve landed may be more than it seems.</p></div><div align=\"left\"><p><em>Walking alone through the corridors of Morgan Industries, you skim the security reports on recent attacks by the horrific native “mind worms.” Giant swarms, or “boils,” of these mottled 10cm nightmares have wriggled out of the fungal beds of late, and now threaten to overwhelm base perimeters in several sectors. Victims are paralyzed with psi-induced terror, and then experience an unimaginably excruciating death as the worms burrow into the brain to implant their ravenous larvae.</em></p><p><em>Only the most disciplined security squads can overcome their fear long enough to trigger the flame guns which can keep the worms at bay. Clearly you will have to tend carefully to the morale of the troops.</em></p><p><em>Furthermore, since terror and surprise increase human casualties dramatically in these encounters, it will be important to strike first when mind-worm boils are detected. You consider ordering some Former detachments to construct sensors near vulnerable bases to aid in such detection efforts.</em></p></div><p> became a darker game as it became more story-oriented, separating itself in the process from the sanguine tale of limitless human progress that is . Reynolds subverted&nbsp;s original backstory about the perfect society that had finally advanced so far as to colonize the stars. In his new version, progress on Earth has not proved all it was cracked up to be. In fact, the planet his interstellar colonists left behind them was on its last legs, wracked by wars and environmental devastation. It’s strongly implied if not directly stated that earthly humanity is in all likelihood extinct by the time the colonists wake up from cryogenic sleep and look down upon the virgin new world that the game calls simply “Planet.”</p><p>Although the plot was destined to culminate in a reckoning with the consciousness of Planet itself, Brian Reynolds sensed that the game needed other, more grounded and immediate forms of conflict to give it urgency right from the beginning. He created these with another piece of backstory, one as contrived as could possibly be, but not ineffective in its context for all that. As told at length in <a href=\"https://ia904600.us.archive.org/20/items/alpha_centauri_books_download/Sid%20Meier%27s%20Alpha%20Centauri_%20Short%20Stories%20-%20Michael%20Ely.pdf\">a novella</a> that Firaxis began publishing in installments on the game’s website more than six months before its release, mishaps and malevolence aboard the colony ship, which bore the sadly ironic name of , led the colonists to split into seven feuding factions, each of whom inflexibly adhere to their own ideology about the best way to organize human society. The factions each made their way down to the surface of Planet separately, to become s equivalent of&nbsp;s nations. The player chooses one of them to guide.</p><p>So, in addition to the unusually strong plot, we have a heaping dose of political philosophy added to the mix;  is an unapologetically heady game. Brian Reynolds had attended graduate school as a philosophy major in a previous life, and he drew from that background liberally. The factions’ viewpoints are fleshed out largely through a series of epigrams that appear as you research new technologies, that are attributed to whichever of the seven leaders would likely approve most of that development, with an occasional quote from Aristotle or Nietzsche dropped in for good measure.</p><div align=\"left\"><p><em>Fossil fuels in the last century reached their extreme prices because of their inherent utility: they pack a great deal of potential energy into an extremely efficient package. If we can but sidestep the 100 million year production process, we can corner this market once again.</em></p><div align=\"right\"><p><em>— CEO Nwabudike Morgan,\nStrategy Session</em></p></div></div><ul><li>Gaia’s Stepdaughters, staunch environmentalists who believe that humanity must learn to live in harmony with nature to avoid repeating the mistakes that led to the ruination of Earth.</li><li>Morgan Industries, hardcore capitalists whose only complaint about Ayn Rand is that she didn’t go far enough.</li><li>The University of Planet, STEM specialists who are convinced that scientific and technological progress alone would correct all that ails society if people would just let it run unfettered and go where it takes them.</li><li>The Lord’s Believers, a fundamentalist sect who are convinced that God will deliver humanity to paradise if we all just pray really hard and abide by a set of stringent, arbitrary dictates.</li><li>The Spartan Federation, who train their children from birth to be hardened, self-sacrificing warriors like <a href=\"https://analog-antiquarian.net/2020/03/27/chapter-12-the-reforms-of-lycurgus/\">the Spartans of old</a>.</li><li>The Peacekeepers, the closest thing to pragmatists in this rogue’s gallery of ideologues; they value human rights, democracy, dialog, and consensus-building, and can sometimes seem just as wishy-washy and ineffectual in the face of militant extremism as the earthly United Nations that spawned them.</li></ul><p>Unlike the nations that appear in  and&nbsp;, each of the factions in&nbsp; has a very significant set of systemic advantages and disadvantages that to a large extent force even a human player to guide them in a certain direction. For example, the Human Hive is excellent at building heavy infrastructure and pumping out babies, but poor at research, and can never become a democracy; the University of Planet is crazily great at research, but its populace has little patience for extended wars and is vulnerable to espionage. Trying to play a faction against type is, if not completely impossible for the advanced player, not an exercise for the faint of heart.</p><p>There is a lot of food for thought in the backstory of a ruined Earth and the foreground story of an angry Planet, as there is in the factions themselves and their ideologies, and trust me when I say that plenty of people have eaten their fill. Even today, more than a quarter-century after s release, YouTube is full of introspective think-pieces purporting to tell us What It All Means.</p><p>Indeed, if anything, the game’s themes and atmosphere resonate more strongly today than they did when it first came out in February of 1999, at which time the American economy was booming, our world was as peaceful and open as it has ever been, and the fantasy that liberal democracy had won the day and we had reached the end of history could be easily maintained by the optimistic and the complacent. Alas, today  feels far more believable than and its  about the inevitability of <a href=\"https://www.filfre.net/2018/03/the-game-of-everything-part-3-civilization-and-the-narrative-of-progress\">perpetual progress</a>. These days,&nbsp;s depiction of bickering, bitterly entrenched factions warring over the very nature of truth, progressing not at all spiritually or morally even as their technology runs wild in a hundred different perilous directions, strikes many as the more accurate picture of the nature of our species. People play  to engage with modern life; they play&nbsp; to escape from it.</p><div><a href=\"https://www.filfre.net/2025/06/alpha-centauri/alpha9/\" rel=\"attachment wp-att-6384\"><img decoding=\"async\" aria-describedby=\"caption-attachment-6384\" src=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha9-300x225.jpg\" alt=\"\" width=\"600\" height=\"450\" srcset=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha9-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2025/06/alpha9-768x576.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/06/alpha9.jpg 800w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a><p>The original  was ahead of the curve on <a href=\"https://www.filfre.net/2018/05/the-game-of-everything-part-10-civilization-and-the-limits-of-progress\">global warming</a>, prompting accusations of “political correctness” from some gamers. Paying heed to the environment is even more important in , since failing to do so can only aggravate Planet’s innate hostility. The “Eco-Damage” statistic is key.</p></div><p>That said, we must also acknowledge that  is disarmingly good at mirroring the beliefs of its players back at them. Many people like to read a strong environmentalist message in the game, and it’s not hard to see why. Your struggles with the hostile Planet, which is doing everything it can to protect itself against the alien parasites on its surface, is an extreme interpretation of the Gaia hypothesis about Earth, even as s &nbsp;“transcendence” victory — the equivalent of&nbsp;s tech victory that got us here in the first place — sees humanity overcoming its estrangement from its surroundings to literally become one with Planet.</p><p>For what it’s worth, though, in his “Designer’s Notes” at the back of the  manual, the one message that Brian Reynolds explicitly states that he wishes for the game to convey is a very different one: that we ought to be getting on with the space race. “Are we content to stew in our collective juices, to turn inward as our planet runs inexorably out of resources?” he asks. “The stars are waiting for us. We have only to decide that it’s worth the effort to go there.” Personally, although I have nothing against space exploration in the abstract, I must say that I find the idea of space colonization as the solution to the problem of a beleaguered Planet Earth shallow if not actively dangerous. Even in the best-case scenario, many, many generations will pass before a significant number of humans will be able to call another celestial object their permanent home. In the meantime, there is in fact nothing “inexorable” about polluting our own planet and bleeding it dry; we have the means to stop doing so. To steal a phrase from Reynolds, we have only to decide that it’s worth the effort.</p><p>But enough with the ideology and the politics, you might be saying — how does  play as a game? Interestingly, Brian Reynolds himself is somewhat ambivalent on this subject. He recalls that he set aside a week just to play  after he pronounced that game done, so thrilled was he at the way it had come out. Yet he says that he could barely stand to look at  after it was finished. He was very proud of the world-building, the atmosphere, the fiction. But he didn’t feel like he had quite gotten the gameplay mechanics sorted so that they fully supported the fiction. And I can kind of see what he means.</p><p>To state the obvious: the gameplay of  is deeply indebted to&nbsp;. Like, really, really indebted. So indebted that, when you first start to play it, you might be tempted to see it as little more than a cosmetic reskin. The cities of  are now “bases”; the “goody-hut” villages are now supply pods dropped by the&nbsp; in its last hours of life; barbarian tribes are native “mind worms”; settler engineers are terraformers; money is “energy credits”; Wonders of the World are Secret Projects; etc., etc. It is true that, as you continue to play, some aspects will begin to separate themselves from their inspiration. For example, and perhaps most notably, the mind worms prove to be more than just the early-game annoyance that s barbarians are; instead they steadily grow in power and quantity as Planet is angered more and more by your presence. Still, the apple never does roll all that far from the tree.</p><div><a href=\"https://www.filfre.net/2025/06/alpha-centauri/alpha3/\" rel=\"attachment wp-att-6379\"><img decoding=\"async\" aria-describedby=\"caption-attachment-6379\" src=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha3-300x225.jpg\" alt=\"\" width=\"600\" height=\"450\" srcset=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha3-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2025/06/alpha3-768x576.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/06/alpha3.jpg 800w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a><p>Very early in a game of , when only a tiny part of the map has been revealed. Of all the contrivances in the fiction, this idea that you could have looked down on Planet from outer space and still have no clue about the geography of the place might be the most absurd.</p></div><p>Where&nbsp; does innovate in terms of its mechanics, its innovations are iterative rather than transformative. An isometric view replaces s top-down view; this allows you to see the topography of the landscape, which is now important, as the solar farms you use to generate energy function best at higher elevations. The most welcome improvement might be the implementation of territorial borders for each faction, drawn automatically around each cluster of bases. To penetrate the borders of another faction with your own units is considered a hostile act. This eliminates the weirdness that dogged the first two iterations of , which essentially saw your empire as a linked network of city-states rather than a contiguous territorial holding. No longer do the computer players walk in and plop down a city… err, base right in the middle of five of your own; no longer do the infantry units of your alleged allies decide to entrench themselves on the choicest tile of your best base. Unsurprisingly given the increased verisimilitude they yielded, national borders would show up in every iteration of the main series after .</p><p>Other additions are of more dubious value. Brian Reynolds names as one of his biggest regrets his dogged determination to let you design your own units out of the raw materials — chassis, propulsion systems, weapons, armor, and so on — provided by your current state of progression up the tech tree, in the same way that galaxy-spanning 4X games like <a href=\"https://www.filfre.net/2020/10/master-of-magic\"></a> allowed. It proved a time-consuming nightmare to implement in this uni-planetary context. And, as Reynolds admits, it’s doubtful how much it really adds to the game. All that time and effort could likely have been better spent elsewhere.</p><p>When I look at it in a more holistic sense, it strikes me that&nbsp; got itself caught up in what had perchance become a self-defeating cycle for grand-strategy games by the end of the 1990s. Earlier games had had their scope and complexity strictly limited by the restrictions of the relatively primitive hardware on which they ran. Far from being a problem, these limits often served to keep the game manageable for the player. One thinks of 1990’s <a href=\"https://www.filfre.net/2017/03/railroad-tycoon\"></a>, another Sid Meier classic, which only had memory enough for 35 trains and 35 stations; as a result, the growth of your railroad empire was stopped just before it started to become too unwieldy to micro-manage. Even the original&nbsp; was arguably more a beneficiary than a victim of similar constraints. By the time Brian Reynolds made , however, strategy games could become a whole lot bigger and more complex, even as less progress had been made on finding ways to hide some of their complexity from the player who didn’t want to see it and to give her ways of automating the more routine tasks of empire management. Grand-strategy games became ever huger, more intricate machines, whose every valve and dial still had to be manipulated by hand. Some players love this sort of thing, and more power to them. But for a lot of them — a group that includes me — it becomes much, much too much.</p><p>To its credit,&nbsp; is aware of this problem, and does what it can to address it. If you start a new game at one of the two lowest of the six difficulty levels, it assumes you are probably new to the game as a whole, and takes you through a little tutorial when you access each screen for the first time. More thoroughgoingly, it gives you a suite of automation tools that at least nod in the direction of letting you set the high-level direction for your faction while your underlings sweat the details. You can decide whether each of your cities… err, bases should focus on “exploring,” “building,” “discovering,” or “conquering” and leave the rest to its “governor”; you can tell your terraforming units to just, well, terraform in whatever way they think best; you can even tell a unit just to go out and “explore” the blank spaces on your map.</p><div><a href=\"https://www.filfre.net/2025/06/alpha-centauri/alpha7/\" rel=\"attachment wp-att-6382\"><img decoding=\"async\" aria-describedby=\"caption-attachment-6382\" src=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha7-300x225.jpg\" alt=\"\" width=\"600\" height=\"450\" srcset=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha7-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2025/06/alpha7-768x576.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/06/alpha7.jpg 800w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a><p>Is the cure worse than the disease?</p></div><p>Sadly, though, these tools are more limited than they might first appear. The tutorials do a decent job of telling you what the different stuff on each screen is and does, but do almost nothing to explain the concepts that underlie them; that is to say, they tell you how to twiddle a variety of knobs, but don’t tell you  you might want to twiddle them. Meanwhile the automation functions are undermined by being abjectly stupid more often than not. Your governor will happily continue researching string theory while his rioting citizens are burning the place down around his ears. You can try to fine-tune his instructions, but there comes a point when you realize that it’s easier just to do everything yourself. The same applies to most of the automated unit functions. The supreme booby prize has to go to the aforementioned “explore” function. As far as I can determine, it just causes your unit to move in a random direction every turn, which tends to result in it chasing its tail like a dog that sat down in peanut butter rather than charging boldly into the unknown.</p><p>This, then, is the contradiction at the heart of&nbsp;, which is the same one that bothers me in . A game that purports to be about Big Ideas demands that you spend most of your time engaged in the most fiddly sort of busywork. I hasten to state once again that this is not automatically a bad thing; again, some people enjoy that sort of micro-management very much. For my own part, I can get into it a bit at the outset, but once I have a dozen bases all demanding constant attention and 50 or 60 units pursuing their various objectives all over the map, I start to lose heart. For me, this problem is the bane of the 4X genre. I’m not enough of an expert on the field to know whether anyone has really come close to solving it; I look forward to finding out as we continue our journey through gaming history. As of this writing, though, my 4X gold standards remain  and&nbsp;, because their core systems are simple enough that the late game never becomes completely overwhelming.</p><p>Speaking of&nbsp;: alongside the questionable idea of custom-built units,  also lifts from that game the indubitably welcome one of a “diplomatic victory,” which eliminates the late-game tedium of having to hunt down every single enemy base and unit for a conquest victory that you know is going to be yours. If you can persuade or intimidate enough of the other factions to vote for you in the “Planetary Council” — or if you can amass such a large population of your own that you can swamp the vote — you can make an inevitability a reality by means of an election. Likewise, you can also win an “economic” victory by becoming crazy rich. These are smart additions that work as advertised. They may only nibble at the edges of the central problem I mentioned above, but, hey, credit where it’s due.</p><p>Aesthetically,&nbsp; is a marked improvement over&nbsp;, which, trapped in the Windows 3.1 visual paradigm as it was, could feel a bit like “playing” a really advanced Excel spreadsheet. But  also exhibits a cold — not to say sterile — personality, with none of the goofy humor that has always been one of s most underrated qualities, serving to nip any pretentiousness in the bud by reminding us that the designers too know how silly a game that can pit Abraham Lincoln against Mahatma Gandhi in a nuclear-armed standoff ultimately is. There’s nothing like that understanding on display in  — much less the campy troupe of live-action community-theater advisors who showed up to chew the scenery in . The look and feel of  is more <a href=\"https://www.filfre.net/2016/11/the-prophet-of-cyberspace\">William Gibson</a> than Mel Brooks.</p><p>While the aesthetics of  represent a departure from what came before, we’re back to the same old same old when it comes to the actual interface, just with more  packed into the menus and sub-menus. I’m sure that Brian Reynolds did what he could, but it will nevertheless come off as a convoluted mess to the uninitiated modern player. It’s heavily dependent on modes, a big no-no in GUI design since the days when <a href=\"https://www.filfre.net/2014/02/macintosh\">the Apple Macintosh was a brand new product</a>. If you’re anything like me, you’ll accidentally move a unit about ten times in any given evening of play because you thought you were in “view” mode when you were actually in “move” mode. And no, there is no undo function, a feature for which I’d happily trade the ability to design my own units.</p><div><a href=\"https://www.filfre.net/2025/06/alpha-centauri/alpha4/\" rel=\"attachment wp-att-6380\"><img decoding=\"async\" aria-describedby=\"caption-attachment-6380\" src=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha4-300x225.jpg\" alt=\"\" width=\"600\" height=\"450\" srcset=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha4-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2025/06/alpha4-768x576.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/06/alpha4.jpg 800w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a><p>The exit dialog is one of the few exceptions to  as a humor-free zone. “Please don’t go,” says a passable imitation of HAL from . “The drones need you.” Note that this is a game in which you click “OK” to cancel. Somewhere out there a human-factors interface consultant is shuddering in horror.</p></div><p>As so often happens in reviews like these, I find now that I’ve highlighted the negative here more than I really intended to.  is by no means a bad game; on the contrary, for some players it is a genuinely great one. It is, however, a sharply bifurcated game, whose fiction and gameplay are rather at odds with one another. The former is thoughtful and bold, even disturbing in a way that  never dared to be. The latter is pretty much what you would expect from a game that was promoted as “ in space,” and, indeed, that was crafted by the same man who gave us . A quick survey of YouTube reveals the two halves of the whole all too plainly. Alongside those earnest think-pieces about What It All Means, there are plenty of videos that offer tips on the minutiae of its systems and show off the host’s skill at beating it at superhuman difficulty levels, untroubled by any of its deeper themes or messages.</p><p>As you’ve probably gathered from the tone of this article,  leaves me with mixed feelings. I’m already getting annoyed by the micro-management by the time I get into the mid-game, even as I miss a certain magic sauce that is part and parcel of . There’s something almost mythical or allegorical about going from inventing the wheel to sending a colony ship on its way out to the stars. Going from Biogentics to the “Threshold of Transcendence” in  is less relatable. And while the story and the additional philosophical textures that  brings to the table are thought-provoking, they can only be fully appreciated once. After that, you’re mostly just clicking past the interludes and epigrams to get on to building the next thing you need for your extraterrestrial empire.</p><p>In fact, it seems to me that  at the gameplay level favors the competitive player more than <a href=\"https://www.filfre.net/2011/07/the-rise-of-experiential-games\">the experiential one</a>; being firmly in the experiential camp myself, this may explain why it doesn’t completely agree with me. It’s a more fiercely zero-sum affair than . Those players most interested in the development side of things can’t ensure a long period of peaceful growth by choosing to play against only one or two rivals. All seven factions are always in this game, and they seem to me far more prone to conflict than those of , what with the collection of mutually antithetical ideologies that are such inseparable parts of their identities. Suffice to say that the other faction leaders are exactly the self-righteous jerks that rigid ideological extremists tend to be in real life. This does not lend itself to peace and harmony on Planet even before the mind worms start to rise up en masse. Even when playing as the Peacekeepers, I found myself spending a lot more time fighting wars in  than I ever did in&nbsp;, where I was generally able to set up a peaceful, trustworthy democracy, forge strong diplomatic and trading links with my neighbors, and ride my strong economy and happy and prosperous citizenry to the stars. Playing , by contrast, is more like being one of seven piranhas in a fishbowl than a valued member of a community of nations. If you can find one reliable ally, you’re doing pretty darn well on the diplomatic front. Intervals of peace tend to be the disruption in the status quo of war rather than the other way around.</p><div><a href=\"https://www.filfre.net/2025/06/alpha-centauri/alpha5/\" rel=\"attachment wp-att-6381\"><img decoding=\"async\" aria-describedby=\"caption-attachment-6381\" src=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha5-300x225.jpg\" alt=\"\" width=\"600\" height=\"450\" srcset=\"https://www.filfre.net/wp-content/uploads/2025/06/alpha5-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2025/06/alpha5-768x576.jpg 768w, https://www.filfre.net/wp-content/uploads/2025/06/alpha5.jpg 800w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a><p>The other factions spend an inordinate amount of time trying to extort money out of you.</p></div><p>There was always an understanding at Firaxis that, for all that&nbsp; was the best card they had to play at that point in time from a commercial standpoint, its sales probably weren’t destined to rival those of . For the  franchise has always attracted a fair number of people from outside the core gaming demographics, even if it is doubtful how many of them really buckle down to play it.</p><p>Nonetheless,  did about as well as one could possibly expect after its release in February of 1999. (Electronic Arts would surely have preferred to have the game a few months earlier, to hit the Christmas buying season, but one of the reasons Firaxis had been founded had been to avoid such compromises.) Sales of up to 1 million units have been claimed for it by some of the principals involved. Even if that figure is a little inflated, as I suspect it may be, the game likely sold well into the high hundreds of thousands.</p><p>By 1999, an expansion pack for a successful game like&nbsp; was almost obligatory. And indeed, it’s hard to get around the feeling that&nbsp;<em>Alpha Centauri: Alien Crossfire</em>, which shipped in October of that year, was created more out of obligation than passion. Neither the navel-gazers nor the zero-summers among the original game’s fan base seem all that hugely fond of it. Patched together by a committee of no fewer than eight designers, with the name of Brian Reynolds the very last one listed, it adds no fewer than seven new factions, which only serve to muddy the narrative and gameplay waters without adding much of positive interest to the equation; the two alien factions that appear out of nowhere seem particularly out of place. If you ask me,  is best played in its original form — certainly when you first start out with it, and possibly forever.</p><p>Be that as it may, the end of the second millennium saw Firaxis now firmly established as a studio and a brand, both of which would prove very enduring. The company remains with us to this day, still one of the leading lights in the field of 4X strategy, the custodian of the beloved …</p><p>Yes,&nbsp;. For their next big trick, Firaxis was about to get the chance to make a game under the name that they thought they’d left behind forever when they said farewell to MicroProse.</p><p><strong>Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.</strong></p><p> The book&nbsp;<em>Sid Meier’s Memoir!: A Life in Computer Games</em> by Sid Meier with Jennifer Lee Noonan.&nbsp; of August 1996, January 1998, September 1998, April 1999, and January 2000;  of July 1997;  241. Also the&nbsp; manual, one of the last examples of such a luxuriously rambling 250-page tome that the games industry would produce.</p>","contentLength":39748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44329600"},{"title":"Show HN: Nxtscape – an open-source agentic browser","url":"https://github.com/nxtscape/nxtscape","date":1750437355,"author":"felarof","guid":163752,"unread":true,"content":"<p>Hi HN - we're Nithin and Nikhil, twin brothers and founders of nxtscape.ai (YC S24). We're building Nxtscape (\"next-scape\") - an open-source, agentic browser for the AI era.</p><p>-- Why bother building a new browser?\nFor the first time since Netscape was released in 1994, it feels like we can reimagine browsers from scratch for the age of AI agents. The web browser of tomorrow might not look like what we have today.</p><p>We saw how tools like Cursor gave developers a 10x productivity boost, yet the browser—where everyone else spends their entire workday—hasn't fundamentally changed.</p><p>And honestly, we feel like we're constantly fighting the browser we use every day. It's not one big thing, but a series of small, constant frustrations. I'll have 70+ tabs open from three different projects and completely lose my train of thought. And simple stuff like reordering tide pods from amazon or filling out forms shouldn't need our full attention anymore. AI can handle all of this, and that's exactly what we're building.</p><p>-- What makes us different\nWe know others are exploring this space (Perplexity, Dia), but we want to build something open-source and community-driven. We're not a search or ads company, so we can focus on being privacy-first – Ollama integration, BYOK (Bring Your Own Keys), ad-blocker.</p><p>Btw we love what Brave started and stood for, but they've now spread themselves too thin across crypto, search, etc. We are laser-focused on one thing: making browsers work for YOU with AI. And unlike Arc (which we loved too but got abandoned), we're 100% open source. Fork us if you don't like our direction.</p><p>-- Our journey hacking a new browser\nTo build this, we had to fork Chromium. Honestly, it feels like the only viable path today—we've seen others like Brave (started with electron) and Microsoft Edge learn this the hard way.</p><p>We also started with why not just build an extension. But realized we needed more control. Similar to the reason why Cursor forked VSCode. For example, Chrome has this thing called the Accessibility Tree - basically a cleaner, semantic version of the DOM that screen readers use. Perfect for AI agents to understand pages, but you can't use it through extension APIs.</p><p>That said, working with the 15M-line C++ chromium codebase has been an adventure. We've both worked on infra at Google and Meta, but Chromium is a different beast. Tools like Cursor's indexing completely break at this scale, so we've had to get really good with grep and vim. And the build times are brutal—even with our maxed-out M4 Max MacBook, a full build takes about 3 hours.</p><p>Full disclosure: we are still very early, but we have a working prototype on GitHub. It includes an early version of a \"local Manus\" style agent that can automate simple web tasks, plus an AI sidebar for questions, and other productivity features (grouping tabs, saving/resuming sessions, etc.).</p><p>Looking forward to any and all comments!</p>","contentLength":2921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44329457"},{"title":"How to Design Programs 2nd Ed (2024)","url":"https://htdp.org/","date":1750434719,"author":"AbuAssar","guid":163890,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44328904"},{"title":"Visualizing environmental costs of war in Hayao Miyazaki's Nausicaä","url":"https://jgeekstudies.org/2025/06/20/wilted-lands-and-wounded-worlds-visualizing-environmental-costs-of-war-in-hayao-miyazakis-nausicaa-of-the-valley-of-the-wind/","date":1750433009,"author":"zdw","guid":163722,"unread":true,"content":"<p><em>Upland High School, Upland, CA, USA.</em></p><p><em>Email: audrey.a.aguirre (at) gmail (dot) com</em></p><p>Past studies on <em>Nausicaä of the Valley of the Wind</em> (風の谷のナウシカ; Topcraft, 1984) have primarily focused on its ecological themes and anti-war messages through analysis of the narrative as a whole or Nausicaä’s character. These studies address the ethical and environmental consequences of war shown through the dystopian nature of the film’s setting and its religious symbolism. However, I have seen almost no research on how visual storytelling contributes to these messages.</p><p>This paper addresses how the visual representation of the environmental consequences of war in <em>Nausicaä of the Valley of the Wind</em> can impact our views of those issues in our world. The paper will show that the visuals in the film are not simply aesthetic decisions, but a crucial narrative device to convey the effects of war on both people and nature.</p><p>Therefore, this paper explores how Miyazaki uses elements of mise-en-scène such as color, lighting, body language and other visual storytelling elements to communicate and add emphasis to the anti-war messaging of the film; especially those that display both the ecological and human consequences of war. I argue that the film’s use of visuals not only supports the anti-war themes of the film as a whole but also adds a stronger emotional and moral weight to the story by reflecting real-life war technologies in its visuals. This allowed audiences to reflect on real-life issues regarding the environmental and human consequences of warfare. In this way, the film created a bridge between fantasy and reality, urging its viewers to strive for a more peaceful and environmentally conscious world.</p><p>Animated films, like many other art forms, can convey so much emotion and be filled to the brim with meaningful messages and ideas. Especially with animation, the director is able to display a myriad of stunning visuals that can be impossible to recreate in live action film. Throughout history we as humans have used storytelling and art to reflect on and understand the world and the current issues we face. With the state of our world being as it is, with conflict at every corner, with global warming and other environmental issues being more pressing matters than ever before, it is clear that the environmental messages in  are of the utmost importance to our society today. This is why it is important to ask, how can the depiction of the environmental impact of war in this film help us change our view of and approach to these issues in real life?</p><p>Hayao Miyazaki’s 1984 film <em>Nausicaä of the Valley of the Wind </em>follows the pacifistic and kindhearted princess of the Valley of the Wind as she navigates the apocalyptic landscape of the film, searching for a way to undo the damage caused by the wars of the past and prevent further damage in the present. Miyazaki’s filmography as a whole contains many important messages regarding the way in which humans interact with and affect the natural world. His films, including , focus on the relationships between humans and animals, exploring the imbalance between the human and natural worlds. With  being the only film directed by Miyazaki that depicts a more modern style of warfare, this film could help to open people’s eyes to the damage war puts not just on the humans involved but on the environment which we all share.</p><p><em>Nausicaä of the Valley of the Wind</em> was originally a manga of the same name that Hayao Miyazaki wrote for Animage, with it being released from February 1982 to March 1994. It saw great success amongst Japanese readers through its run. This prompted the manga’s adaptation into an animated film which was directed by Miyazaki and released to Japanese audiences in 1984. Due to the extreme inconsistencies between the original film and Showmen Inc.’s English dub of –renamed – Miyazaki considered never releasing his films to foreign audiences again. This dub had changed the names of most characters, cut nearly 30 minutes off of the film and completely altered its message. Later, a deal was made between Studio Ghibli and Walt Disney Studios Home Entertainment, and Miayzaki allowed them to dub his films under the condition that they make no cuts and keep the original meaning intact (Cinematheque, 2016). Because of this, Disney’s English dub will be the main focus of this paper (Walt Disney Studios Home Entertainment, 1985).</p><p>For the purpose of this article, when referring to the natural world it would more specifically be described as the surrounding elements that are essential for the well-being of both human and non-human life which includes green spaces, wildlife habitats, biodiversity and clean air, water and soil. Additionally, there are several terms which one who has not watched the film would not be able to understand, and these terms are as follows.</p><p>The ohmu (Fig. 1) are enormous, powerful and intelligent pill bug-like animals which are feared by the people in the film due to the fact that they could be considered the kings of the toxic jungle.</p><p>The Toxic Jungle is a vast forest where the air, water, soil and plants are poisonous to all but the giant arthropods which inhabit it. The toxic jungle only became so poisonous after the Giant Warriors were used in the last days of the Ceramic War. The Giant Warriors (Fig. 2) are giant, biomechanical lifeforms which are treated, and act, more as weapons rather than as independent beings.</p><p>The Ceramic War is an apocalyptic war that occurred 1000 years before the events of the film in what was called the Ceramic Period, which destroyed civilization, caused an ecocide and created the vast Toxic Jungle during the Seven Days of Fire. The Seven Days of Fire was a seven-day period at the end of the Ceramic War in which the Giant Warriors were deployed.</p><p>When analyzing Miyazaki’s films, including <em>Nausicaä of the Valley of the Wind</em>, often the messages regarding warfare’s effects on the natural world focus more on the direct impacts rather than the indirect ones. Additionally, many researchers look at the film from a spiritual lens with DeWeese-Boyd (2009) believing Nausicaä serves as a Christ-like figure, while both Morgan (2015) and Nunes (2021) believe that Nausicaä serves as an example of how to restore balance between humans and nature. Despite that, these authors see her role in restoring balance differently. Nunes believes that Nausicaä gave herself to nature, sacrificing her free will in order to heal the earth without personal bias. On the other hand, Morgan believes that she serves as an example of how the fragmentation between the mind, body, spirit and nature can be restored through respect and care for the natural world.</p><p>In addition to this, there are some researchers who believe that  can be used as a teaching tool, with Kleese (2024) believing that, since the film is not directly associated with any singular real-life issue but with many different events and issues, it can be used in classrooms to help children understand the importance of nature and finding a democratic solution to both environmental and political issues in a more digestible way.</p><p>It is no wonder the previous research on Nausicaä has such a spiritual focus, as Miyazaki’s filmography as a whole often focuses on the relationship and imbalance between humans and the natural world in a very spiritual manner. However,  not only explores the direct impacts of war and other human activities, but also their indirect ramifications. Thus, this paper will hopefully fill a gap in the existing literature by analyzing the film’s messages regarding warfare’s indirect effects on the environment and how this exercise can give us insight to our real-world problems.</p><h2>The importance of the environment</h2><p>Climate change and other environmental crises such as soil and water contamination are all issues which have been recognized as important by the public eye. From everyday people to those in important positions, most can agree that the health of the natural world is important to the health and survival of life on earth. As UN Secretary-General António Guterres said at the biodiversity COP in Montreal 2022, “Without nature, we have nothing” (Abbasi et al., 2023). Access to clean water is undeniably fundamental to life on Earth, be it human or not. Even with this being common knowledge, pollution has damaged water quality to a point where it is causing a rise in waterborne diseases and damaging the health of both freshwater and saltwater ecosystems (Abbasi et al., 2023). Additionally, the rising temperatures, extreme weather events, air pollution and the heightened spread of infectious diseases are just a few examples of major health issues exacerbated by climate change (Abbasi et al., 2023). As the human population grows, we see that demands on Earth’s ecosystems are becoming more unsustainable; with the way that we currently treat our environment, long-term human and non-human security are clearly at stake (dos Santos, 2024).</p><p>The ever-worsening health of our natural world impacts us in many ways. Land degradation and other environmental issues which may cause an area to be less habitable can lead to the disruption of social and economic systems. Shortages of land, shelter, food and water exacerbate poverty and the poor living conditions in many areas of the world (Anonymous, 2004), this in turn leads to mass migration and conflict over usable land and resources necessary for life (Abbasi et al., 2023). While the degradation of land can in itself cause wars to erupt between peoples disputing over the usable land, war can and has caused additional land degradation. In Afghanistan, for example, forests have been leveled and its land and farmlands polluted from the years of use of fuel, chemicals and mines during wartime (Bonds, 2015).</p><p>Certain practices of war have more devastating impacts on the longevity of our natural world. One poignant example of how destructive war can be to our planet can be seen in the burning of Kuwaiti oil wells that took place during the The Gulf War as a part of their scorched earth tactics. This resulted both in the soil becoming contaminated with excessive amounts of hydrocarbons and heavy metals and in the release of massive amounts of particulate matter and other pollutants into the atmosphere (Aldawsari, 2024).</p><p>Everything from the way a shot is framed to the smallest detail captured within the shot can have an effect on the viewers of a film. Displaying intense emotional imagery has been proven to have significant psychological effects on viewers. For instance, the Kuleshov effect is a famous example of how film influences viewers’ emotional perception; it has shown that point-of-view editing practices influence viewers’ emotional interpretation of neutral facial expressions in a face-scene-face sequence (Cao et al., 2024). With this in mind it is evident that the things that films show their viewers can and do impact how they view the world.</p><p>At the same, time films utilize the elements of mise-en-scène to convey messages to their audience. Mise-en-scène is a French term meaning what is put into a scene or frame and consists of all the visual information in front of the camera (Caprio, 2021). Understanding these elements may help understand what the director of a film wants to convey with any given scene and understand how the visuals impact the audience.</p><p>All forms of filmography are capable of affecting their audiences with the previously mentioned methods. However, animation is often able to employ visuals and different types of shots which can often be extremely difficult or expensive to replicate in live action film. Because of this and because animated films in the West are often avoidant of more serious topics, seeing as they are viewed as being only for small children, that Japanese animation, or anime, can be an immensely powerful and impactful type of filmmaking. Several studies have shown that anime can influence its audience and evoke positive changes in them (e.g., Yusof et al., 2024). With the unique perspective that anime provides, which is more provocative, tragic and contains far more complicated storylines than the ones seen in American popular cinema, anime has proven itself to be a tool for understanding the complex human–environment relationship and environmental problems (Mumcu &amp; Yılmaz, 2018).</p><p>Within the anime community Hayao Miyazaki has carved out an image for himself as a masterful director who is skilled in creating both enchanting fantasies and incredibly thought-provoking films (Mumcu &amp; Yılmaz, 2018). He is quite well renowned for his beautiful landscapes, heavy ecological themes and overall beautiful and touching storytelling. Several of his films such as , <em>Nausicaä of the Valley of the Wind</em> and even  feature themes of ecological imbalance as the main plot points of the films. Even those which do not have these themes quite as ingrained in the plot still have some commentary on the matter or feature more lighthearted takes on nature’s relationship with humanity. This can be seen in the stories of  and . Additionally, many of his films also approach themes of war and conflict, such as , <em>Nausicaä of the Valley of the Wind</em>, , ,  and . This leaves only six of his fifteen films, all of which have seen great success, with no themes of interest to this paper. For this reason, it is evident that Miyazaki is a perfect choice for the subject matter of this paper.</p><p>With only six of the fifteen films Miyazaki has directed or written having no themes of interest to this paper, one may wonder what it is that makes  more suitable than any of his other films. Well, though many of his films touch on environmental issues with Miyazaki even going as far as saying “I’ve come to a point where I just can’t make a movie without addressing the problem of humanity as part of an ecosystem” in an interview with Asia Pulse, May 16, 1997, not many of his films include war in the main storyline. Only five do, as mentioned above. There are only two films, and , that cover both environmental issues and war. The major factor which puts  over  is the setting; is set in Japan’s late Muromachi Period, which was characterized by rapid industrialization and frequent conflicts. On the other hand, is set in the post-apocalyptic future which bears much resemblance to our world, with some current technologies such as guns, grenades and tanks being included, as well as fictitious technologies which closely mirror real-life technologies (e.g., how the Giant Warriors function similarly to nuclear bombs).</p><p>As previously stated, the Kuleshov effect demonstrates that intense visuals can alter one’s interpretation of the world and this in itself proves that analysis via the elements of mise-en-scène is a viable method for breaking down  with the purpose of determining how the visual depiction of the environmental impact of war can affect our view of and approach to these issues in real life. However, this isn’t the only reason mise-en-scène analysis was used within this study, since it has been used for teaching aspiring filmmakers how films communicate messages with visuals; as my question focuses on visual depictions in film, mise-en-scène analysis was perfectly suited for my project. The elements of mise-en-scène are as follows: settings and props, costume, hair and makeup, facial expressions and body language, lighting and color and lastly positioning. Each of the 38 scenes of the film were analyzed to see how these elements are utilized and what effect it can have on the audience.</p><p>Throughout the film the elements of mise-en-scène can be seen in use in many ways. Despite the subtle differences in the amount each element is used in the different parts of the story, the overall message urges viewers to rethink their stance on warfare and its technologies, not just for the impact it directly has on humans, but also for the sake of the natural world.</p><p>One example of how body language, facial expressions and color are used to push the film’s message regarding the use of war technologies can be perfectly seen in the opening credits, which appear in scene two directly after the narrator introduces the world reading aloud the words that can be seen in figure one that state “One thousand years have passed since the collapse of industrialized civilization. The Toxic Jungle now spreads, threatening the survival of the last of the human race.” Right after this is read as the credits roll a tapestry is panned over, shown in Figure 3.</p><p>The tapestry shows how the creation of the Giant Warriors led to the creation of the Toxic Jungle and the fall of humanity. Within this tapestry, while in the process of building the warriors they appear confident in their body language and facial expressions, they are clearly calm and all is well. However, the colors of the warrior are bright and clash with that of the people which are more muted browns rather than the bright blues reds and yellows of the warrior. This itself already sets up for the destructive and overpowering nature of the Giant Warriors before they have even been finished and this is only confirmed when they are soon after in the tapestry shown wreaking havoc on the very same people who created them. In contrast to their poised expression and way of holding themselves previously in the tapestry, here they appear to be in great distress and panic. While this on its own is a striking visual representation of how dangerous technologies can be, even to their own creators, these visuals are given new meaning when we see the Giant Warriors in actual use towards the end of the film.</p><p>The audience was shown the similarities between the Giant Warriors and nuclear bombs in several instances towards the end of the film; in particular, there is one which really drives home the parallels between them (Figs. 4 and 5). In this scene, the Giant Warrior is being used by the main antagonist, its form being completely fictional and bearing no resemblance to real-life technologies, with the warrior shooting a beam of light from its mouth. The explosion that this caused very closely resembles the mushroom shape of nuclear bombs. Other aspects of this scene that help to draw similarities between this fictional tool of destruction and the very real nuclear weapons we have, such as the fact that before it dies the warrior only sets off two explosions. This could be a reflection of the fact that these technologies have only seen practical use twice, once in Hiroshima and once in Nagasaki. While these parallels alone drive a case for the film’s anti-war messaging, other elements regarding the Giant Warriors in other scenes help to push this narrative as well.</p><p>Every scene which has the Giant Warrior in it, from the beginning to the end of the film, is decisively negative. Around the middle of the film there is a scene where the warrior is in a sort of incubation. The film explains that the warrior needs time to develop before it is able to be used as a war machine and walk on its own. The warrior is depicted within this scene with its colors being both muddy and bloody, with the lighting highlighting not only its strange shape but also how slimy it appears. This all helps to show how grotesque it is, even when it hasn’t started to be destructive. With the clear parallels it has to nuclear weapons, it becomes obvious from nearly every mention and appearance of the Giant Warrior that if it is so grotesque and dangerous, then nuclear weapons must be just as horrifying. This specific scene very well pushes the idea that not only is the use of nuclear weapons immoral but so too is the development of them.</p><p>Aside from nuclear weapons, there are other very real weapons displayed in the film that we continue to use to this day. Such weapons include machine guns, shotguns, hand grenades, flash grenades and even larger things like gunships or tanks. While nearly all of these are represented in a very realistic way, the gunships are undoubtedly designed with significant creative liberty as the bodies of these aircrafts do not resemble any real aircraft. However, this does not mean these do not give a good representation of the flaws of such technologies. For examples, see Figures 6 and 7.</p><p>All of the previously mentioned technologies of war are portrayed within the film in a distinctly negative way. However, this does not mean that the film means to say there is absolutely no acceptable use of such things; there are also times when these are shown to be neutral or even positive. In truth there is a strong possibility that this film means to say that these technologies themselves are not evil, but the way that we as humans interact with them can make them that way. Nausicaä herself uses her gun towards the beginning of the film to remove part of an ohmu shell and she uses flash grenades to stun an ohmu, saving Lord Yupa. In the former instance, the use of the gun is shown in a completely neutral context, with it not even being used on any living thing and with the entire scene remaining quite peaceful; even the choices of lighting and colors being brighter than other parts of the Toxic Jungle adding to the serenity of the scene. This scene shows that such things can be used without causing any harm whatsoever. In the other instance the grenades are clearly used to stun the ohmu in order to protect and reduce harm for both parties.</p><p>In the instances in which these technologies are used in a harmful way, the negativity of it is conveyed both in the expression of the characters witnessing it or by the colors shown in the scene. With the scene of the Giant Warrior being used towards the end of the film, the characters on both sides are clearly in shock and awe of just how destructive this technology can be. Additionally, the use of color in that scene helps in pushing just how the use of these technologies causes far more harm than good. The warrior itself is melting into this dark bloody red sludge; this nasty red is contrasted with its sharp green eyes that appear completely devoid of any sort of soul. Its visage is utterly grotesque and it remains that way till it falls apart.</p><p>Other technologies that are more often used in warfare today are also critiqued. Towards the end of the film conflicts arise in the Valley with the people finally fighting back against the Tolmakian forces that have been occupying their land. Prior to this, the valley is shown to be a very peaceful and beautiful place with plenty of lush greenery and farmlands; the downfall of this serene environment begins when spores are found in the forest surrounding the valley. See Figures 8 and 9 for the before and after, respectively.</p><p>Initially, the people mean only to use their tools to burn the spores, using fire in moderation in order to solve their problem. However, things quickly get out of hand and they realize that they have no choice but to burn the entire forest down if they don’t want the Toxic Jungle to spread into the valley. This is the first time in the film that the valley is shown with colors like black, brown and others that are associated with decay, being more prevalent than greens and other more natural or lively colors. Throughout the entire film the use of fire is heavily frowned upon with Ohbaba warning the Tolmekians of why they should not even attempt to burn down the toxic jungle and, towards the end of the film, several characters going on about why they prefer the ways of the water and the wind over the way of fire since “Too much fire gives birth to nothing. Fire can reduce a forest to ashes in a day, while it takes the water and the wind 100 years to grow one”. With this film Miyazaki is urging us to stop relying on fire to solve our problems, both in a literal and figurative sense. This is shown not just through the speech of the characters but also in the way that the scenery changes and the way the characters react to the use of excess fire.</p><h2>Human &amp; environmental impact</h2><p>In the film the human and environmental impact of human activities such as war are explored in many scenes. One example of this can be seen in one scene towards the end of the film where Nausicaä and another character, Asbel, fly into the city of Pejite together on Nausicaä’s glider (Figs. 10, 11). It had previously been occupied by the Tolmekian forces but as they fly over it, the city is desolate and run down. The film reveals that the people of Pejite baited some of the ohmu into the city and let them wreak havoc in order to drive out the Tolmekians occupying the city. The film tries to show the audience how immoral this decision was not just from the sorrowful and ashamed expressions on the faces of the Pejite refugees and Asbel, the prince of Pejite. It also shows the impact these actions have had on the fauna, as the scenery is full of deceased animals of the Toxic Jungle. Additionally, while it is apparent that the city was once livable, now the characters need to use their masks to even be able to breathe within its premises. This shows that the damage done here was severe as some buildings were broken down, lives were lost and the land has been made uninhabitable for the foreseeable future.</p><p>The Pejite use the same strategy again later in the film. In this second instance, they lure the ohmu into the Valley of the Wind in order to keep the Giant Warrior out of the hands of the Tolmekians. There were several scenes which depict this event since it is the major conflict of the film; however, since the previous scene examined focused more on the environmental impact of this act, here the focus is on the human impact. In the scene Mito, who serves sort of as the assistant to Princess Nausicaä, returns to the Valley of the Wind and informs both the people of the valley and the occupying forces of the Tolmekians of the ohmu horde heading towards them. The audience is shown in this scene how much panic and distress the news brings both to the innocent citizens of the occupied territory and to the occupying forces, who were the only real target of this attack. The expressive use of facial expression and body language in this scene helps drive home just how impactful this is, even to the people who were never meant to be a target of this attack. Another scene that comes soon after reinforces this idea, when the insects begin their attack on the valley. This scene is utter chaos, the people attempt to seek shelter and the Tolmekian soldiers who were previously standing without shelter, emboldened by their control of the Giant Warrior, now scramble seeking shelter and clearly fearing for their lives. The expressiveness of this scene comes not only from the facial expressions and body language but also from the shaky and strange positioning of the camera which gives the scene a more panicked feel. In addition, the colors were muddled and the lighting was quite dim, with sudden flashes of bright blaring light that fed into the chaotic nature of the scene. This all works to show just how impactful efforts of war can be to humans, their environment and the other creatures who share it.</p><p>Certain aspects of the method I chose when designing this project limited the results of my research. For instance, because I focused only on the visuals, my data does not take into account the role that the script, voice acting (especially the original) or soundtrack played in delivering certain messages to the audience. Additionally, while one can analyze, interpret, and hypothesize the main message(s) in the film and in each scene, it is impossible to extrapolate for all audiences. During the research process for this study, many additional questions have come up, including the following. Firstly, what effects do the audio elements of  have on the audience? Secondly, can  even be compared to Western religions given the fact that it was made in Japan? Lastly, how is the film interpreted by younger viewers versus older viewers?</p><p>Through its stunning visual story-telling, <em>Nausicaä of the Valley of the Wind</em> invites its audience to reconsider the toll war takes not just on people, but also the environment. With color, body language, facial expressions, and the overall environment of scenes, Miyazaki warns against the dangers of harmful technology and the moral implications of its use. From the violent and gruesome images of the Giant Warrior and destroyed landscape as a result of the use of war technology, to the depleted and unsafe landscapes and fearful gazes of terrified civilians robbed of their homes and any sense of normalcy, the film seeks to depict the enormous destructiveness that results from war. Additionally, Miyazaki’s illustrations are much more than fantasy, nodding toward the real world where parallel destruction occurs via nuclear weapons, environmental crises and modern warfare. The overarching message of the film’s visuals serves not only to paint a narrative, but to urge us to meet conflict with empathy, to protect our natural environment, and understand that it takes more courage and wisdom to find peace than to wage war. Although this data is subjective, seeing as it was based on my interpretation of the film’s visuals, it still suggests that visual storytelling has the potential to convey the ecologically damaging effects of war. Additionally, this discussion makes a contribution to the academic conversations on film, war and environmental sustainability all at the same time. Offering a visual, scene-based approach to analyzing storytelling that depicts war and environmental crisis in anime. Despite all this, further research is needed on large audience reception of these visual messages, cultural responses to these messages, or the influence auditory elements of film can have in delivering these messages.</p><p><strong>Abbasi, K.; Ali, P.; Barbour, V.; et al.</strong> (2023) Time to treat the climate &amp; nature crisis as one indivisible global health emergency. The Indian Journal of Medical Research 158(4): 330–333.</p><p> (2024) War practices and experiences: analyzing their effects on the environment in the Gulf Cooperation Council (GCC) region. The American Journal of Management and Economics Innovations 6(8): 64–88.</p><p> (2004) War’s environmental impact. Alternatives Journal 30(4): 26.</p><p> (2015) Legitimating the environmental injustices of war: toxic exposures and media silence in Iraq and Afghanistan. Environmental Politics 25(3): 395–413.</p><p><strong>Cao, Z.; Jin, S.; Yang, C.; et al.</strong> (2024) Reexamining the Kuleshov Effect: behavioral and neural evidence from authentic film experiments. PLOS ONE 19(8): e0308295.</p><p> (2009) Shojo savior: Princess Nausicaä, ecological pacifism, and the green gospel. Journal of Religion and Popular Culture 21(2): 1–16.</p><p> (2021) Climate change, air pollution, and human health in the Kruger to Canyons Biosphere Region, South Africa, and Amazonas, Brazil: a narrative review. Atmosphere 15(5): 562.</p><p> (2024) Democracy and kinship in Nausicaä of the Valley of the Wind. Climate Literacy in Education 2(1): 67–73.</p><p> (2019) The Art of Nausicaä of the Valley of the Wind. VIZ Media LLC, San Francisco.</p><p> (2015) Creatures in crisis: apocalyptic environmental visions in Miyazaki’s Nausicaä of the Valley of the Wind and Princess Mononoke. Resilience: A Journal of the Environmental Humanities 2(3): 172–183.</p><p> (2018) Anime landscapes as a tool for analyzing the human–environment relationship: Hayao Miyazaki films. Arts 7(2): 16.</p><p> (2021) The toxic heroine in Nausicaä of the Valley of the Wind. In: Ferstl, P. (Ed.) Dialogues between Media, Vol. 5. De Gruyter, Berlin. Pp. 83–94.</p><p><strong>Walt Disney Studios Home Entertainment [Translator].</strong> (1985) Nausicaä of the Valley of the Wind. Directed by Hayao Miyazaki. Studio Ghibli. English dub.</p><p><strong>Yusof, N.A.; Hussin, S.A.; Hashim, M.A.; Amin, A.</strong> (2024) Exploring the impact of anime on Muslim teenagers’ moral behaviour. International Journal of Academic Research in Business and Social Sciences 14(7): 861–873.</p><p>ChatGPT (GPT-4, OpenAI) was used to improve the writing style of this article. The author reviewed, edited, and revised the ChatGPT-generated texts to her own liking and takes ultimate responsibility for the content of this publication. Special thanks to my AP Research teacher and classmates for their feedback and support during the development of this paper.</p><p> is a student of Upland High School. She has been fascinated with Studio Ghibli’s Films since the first time she saw one and couldn’t get enough of the studio’s beautiful animation and wonderful storytelling. For the longest time her two favorite films had been and . She can’t be sure what the future has in store for her but she hopes she can see many more meaningful films such as the ones she’ve loved from Studio Ghibli.</p>","contentLength":32889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44328598"},{"title":"Phoenix.new – Remote AI Runtime for Phoenix","url":"https://fly.io/blog/phoenix-new-the-remote-ai-runtime/","date":1750431424,"author":"wut42","guid":163634,"unread":true,"content":"<div><p>I’m Chris McCord, the creator of Elixir’s Phoenix framework. For the past several months, I’ve been working on a skunkworks project at Fly.io, and it’s time to show it off.</p></div><p>I wanted LLM agents to work just as well with Elixir as they do with Python and JavaScript. Last December, in order to figure out what that was going to take, I started a little weekend project to find out how difficult it would be to build a coding agent in Elixir.</p><p>A few weeks later, I had it spitting out working Phoenix applications and driving a full in-browser IDE. I knew this wasn’t going to stay a weekend project.</p><p>If you follow me on Twitter, you’ve probably seen me teasing this work as it picked up steam. We’re at a point where we’re pretty serious about this thing, and so it’s time to make a formal introduction.</p><p>World, meet <a href=\"https://phoenix.new\" title=\"\">Phoenix.new</a>, a batteries-included fully-online coding agent tailored to Elixir and Phoenix. I think it’s going to be the fastest way to build collaborative, real-time applications.</p><h2><a href=\"https://fly.io/blog/phoenix-new-the-remote-ai-runtime/#whats-interesting-about-phoenix-new\" aria-label=\"Anchor\"></a></h2><p>First, even though it runs entirely in your browser, Phoenix.new gives both you and your agent a root shell, in an ephemeral virtual machine (a <a href=\"https://fly.io/docs/machines/overview/\" title=\"\">Fly Machine</a>) that gives our agent loop free rein to install things and run programs  — without any risk of messing up your local machine. You don’t think about any of this; you just open up the VSCode interface, push the shell button, and there you are, on the isolated machine you share with the Phoenix.new agent.</p><p>Second, it’s an agent system I built specifically for Phoenix. Phoenix is about real-time collaborative applications, and Phoenix.new knows what that means. To that end, Phoenix.new includes, in both its UI and its agent tools, a full browser. The Phoenix.new agent uses that browser “headlessly” to check its own front-end changes and interact with the app. Because it’s a full browser, instead of trying to iterate on screenshots, the agent sees real page content and JavaScript state – with or without a human present.</p><p>Agents build software the way you did when you first got started, the way you still do today when you prototype things. They don’t carefully design Docker container layers and they don’t really do release cycles. An agent wants to pop a shell and get its fingernails dirty.</p><p>A fully isolated virtual machine means Phoenix.new’s fingernails can get  If it wants to add a package to , it can do that and then run  or  and check the output. Sure. Every agent can do that. But if it wants to add an APT package to the base operating system, it can do that too, and make sure it worked. It owns the whole environment.</p><p>This offloads a huge amount of tedious, repetitive work.</p><p>At his <a href=\"https://youtu.be/LCEmiRjPEtQ?si=sR_bdu6-AqPXSNmY&amp;t=1902\" title=\"\">AI Startup School talk last week</a>, Andrej Karpathy related his experience of building a restaurant menu visualizer, which takes camera pictures of text menus and transforms all the menu items into pictures. The code, which he vibe-coded with an LLM agent, was the easy part; he had it working in an afternoon. But getting the app online took him a whole week.</p><p>With Phoenix.new, I’m taking dead aim at this problem. The apps we produce live in the cloud from the minute they launch. They have private, shareable URLs (we detect anything the agent generates with a bound port and give it a preview URL underneath , with integrated port-forwarding), they integrate with Github, and they inherit all the infrastructure guardrails of Fly.io: hardware virtualization, WireGuard, and isolated networks.</p><div><p>Github’s  CLI is installed by default. So the agent knows how to clone any repo, or browse issues, and you can even authorize it for internal repositories to get it working with your team’s existing projects and dependencies.</p></div><p>Full control of the environment also closes the loop between the agent and deployment. When Phoenix.new boots an app, it watches the logs, and tests the application. When an action triggers an error, Phoenix.new notices and gets to work.</p><h2><a href=\"https://fly.io/blog/phoenix-new-the-remote-ai-runtime/#watch-it-build-in-real-time\" aria-label=\"Anchor\"></a></h2><p><a href=\"https://phoenix.new\" title=\"\">Phoenix.new</a> can interact with web applications the way users do: with a real browser.</p><p>The Phoenix.new environment includes a headless Chrome browser that our agent knows how to drive. Prompt it to add a front-end feature to your application, and it won’t just sketch the code out and make sure it compiles and lints. It’ll pull the app up itself and poke at the UI, simultaneously looking at the page content, JavaScript state, and server-side logs.</p><p>Phoenix is all about <a href=\"https://fly.io/blog/how-we-got-to-liveview/\" title=\"\">“live” real-time</a> interactivity, and gives us seamless live reload. The user interface for Phoenix.new itself includes a live preview of the app being worked on, so you can kick back and watch it build front-end features incrementally. Any other  tabs you have open also update as it goes. It’s wild.</p><p>Phoenix.new can already build real, full-stack applications with WebSockets, Phoenix’s Presence features, and real databases. I’m seeing it succeed at business and collaborative applications right now.</p><p>But there’s no fixed bound on the tasks you can reasonably ask it to accomplish. If you can do it with a shell and a browser, I want Phoenix.new to do it too. And it can do these tasks with or without you present.</p><p>For example: set a  and tell the agent about it. The agent knows enough to go explore it with , and it’ll propose apps based on the schemas it finds. It can model Ecto schemas off the database. And if MySQL is your thing, the agent will just  a MySQL client and go to town.</p><p>Frontier model LLMs have vast world knowledge. They generalize extremely well. At ElixirConfEU, I did a <a href=\"https://www.youtube.com/watch?v=ojL_VHc4gLk&amp;t=3923s\" title=\"\">demo vibe-coding Tetris</a> on stage. Phoenix.new nailed it, first try, first prompt. It’s not like there’s gobs of Phoenix LiveView Tetris examples floating around the Internet! But lots of people have published Tetris code, and lots of people have written LiveView stuff, and 2025 LLMs can connect those dots.</p><p>At this point you might be wondering – can I just ask it to build a Rails app? Or an Expo React Native app? Or Svelte? Or Go?</p><p>Our system prompt is tuned for Phoenix today, but all languages you care about are already installed. We’re still figuring out where to take this, but adding new languages and frameworks definitely ranks highly in my plans.</p><p>Agents can do real work, today, with or without a human present. Buckle up: the future of development, at least in the common case, probably looks less like cracking open a shell and finding a file to edit, and more like popping into a CI environment with agents working away around the clock.</p><p>Local development isn’t going away. But there’s going to be a shift in where the majority of our iterations take place. I’m already using Phoenix.new to triage  Github issues and pick problems to solve. I close my laptop, grab a cup of coffee, and wait for a PR to arrive — Phoenix.new knows how PRs work, too. We’re already here, and this space is just getting started.</p><p>This isn’t where I thought I’d end up when I started poking around. The Phoenix and LiveView journey was much the same. Something special was there and the projects took on a life of their own. I’m excited to share this work now, and see where it might take us. I can’t wait to see what folks build.</p>","contentLength":7163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44328326"},{"title":"Congestion pricing in Manhattan is a predictable success","url":"https://www.economist.com/united-states/2025/06/19/congestion-pricing-in-manhattan-is-a-predictable-success","date":1750429609,"author":"edward","guid":163721,"unread":true,"content":"<p data-component=\"paragraph\">, a speech therapist in New York City, was dreading the introduction of congestion pricing. To see her patients in Queens and Manhattan she sometimes drives across the East River a couple of times a day. The idea of paying a $9 toll each day infuriated her. Yet since the policy was actually implemented, she has changed her mind. A journey which used to take an hour or more can now be as quick as 15 minutes. “Well, this is very nice,” she admits thinking. Ms Ryan is not alone. Polls show more New Yorkers now support the toll than oppose it. A few months ago, it saw staunch opposition.</p>","contentLength":594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44328032"},{"title":"Meta announces Oakley smart glasses","url":"https://www.theverge.com/news/690133/meta-oakley-hstn-ai-glasses-price-date","date":1750425422,"author":"jmsflknr","guid":163720,"unread":true,"content":"<div><p>Meta is announcing its next pair of smart glasses with Oakley. The limited-edition Oakley Meta HSTN (pronounced “how-stuhn”) model costs $499 and is <a href=\"https://www.meta.com/ai-glasses/oakley-meta-hstn/\">available for preorder</a> starting July 11th. Other Oakley models with Meta’s tech will be available starting at $399 later this summer. </p></div><div><p>Like the <a href=\"https://www.theverge.com/23922425/ray-ban-meta-smart-glasses-review\">existing Meta Ray-Ban glasses</a>, the Oakley model features a front-facing camera, along with open-ear speakers and microphones that are built into the frame. After they are paired with a phone, the glasses can be used to listen to music or podcasts, conduct phone calls, or chat with Meta AI. By utilizing the onboard camera and microphones, Meta AI can also answer <a href=\"https://www.theverge.com/news/667613/ray-ban-meta-smart-glasses-ai-detailed-responses-call-a-volunteer\">questions about what someone is seeing</a> and even <a href=\"https://www.theverge.com/2025/1/24/24351013/ray-ban-meta-smart-glasses-translation-wearables\">translate languages</a>. </p></div><div><p>Given the Oakley design, Meta is positioning these new glasses as being geared towards athletes. They have an IPX4 water resistance rating and offer double the battery life of the Meta Ray-Bans, providing 8 hours of use, along with a charging case that can power them for up to 48 hours. The built-in camera now shoots in 3K video, up from 1080p for the Meta Ray-Bans. </p></div><div><p>The new lineup comes in five Oakley frame and lens combos, all of which are compatible with prescriptions for an extra cost. The frame colors are warm grey, black, brown smoke, and clear, with several lens options available, including transitions. The limited-edition $499 model, available for order starting July 11th, features gold accents and gold Oakley <a href=\"https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.oakley.com%2Fen-us%2Flp%2Fprizm\" rel=\"sponsored\">PRIZM</a> lenses. The glasses will be on sale in the US, Canada, the UK, Ireland, France, Italy, Spain, Austria, Belgium, Australia, Germany, Sweden, Norway, Finland, and Denmark.</p></div><div><p>Meta recently signed a <a href=\"https://www.theverge.com/24253481/meta-ceo-mark-zuckerberg-ar-glasses-orion-ray-bans-ai-decoder-interview\">multi-year deal with EssilorLuxottica</a>, the parent company behind Ray-Ban, Oakley, and other eyewear brands. The Meta Ray-Bans have sold over two million pairs to date, and EssilorLuxottica <a href=\"https://www.theverge.com/news/613292/meta-ray-ban-2-million-10-million-capacity-subscription-essilor-luxottica-earnings\">recently disclosed</a> that it plans to sell 10 million smart glasses with Meta annually by 2026. “This is our first step into the performance category,” Alex Himel, Meta’s head of wearables, tells me. “There’s more to come.”</p></div>","contentLength":2078,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44327468"},{"title":"Klong: A Simple Array Language","url":"https://t3x.org/klong/","date":1750423449,"author":"tosh","guid":163751,"unread":true,"content":"<img src=\"https://t3x.org/klong/klong.png\" alt=\"Klong Logo\"><p>Klong is an array language, like K, but without the <a href=\"https://t3x.org/klong/ambiguity.html\">ambiguity</a>. If\nyou know K or APL, you may be disappointed by Klong. If you don't\nknow any array languages, it might explode your brain. Use at your\nown risk!\n</p><p>A Klong program is a set of functions that use various pre-defined\noperators to manipulate lists (vectors) and (multi-dimensional)\narrays. Here is a program that checks whether a number  is prime\n(for ):\n</p><p>Note that Klong is a mathematical notation rather than a programming\nlanguage. If you try to use it like your favorite functional/procedural/OO\nprogramming language, you will only get frustrated. Here's an\n<a href=\"https://t3x.org/klong/prime.html\">explanation of the above program</a>.\n</p><p>The Reference Manual (<a href=\"https://t3x.org/klong/klong-ref.txt.html\">klong-ref.txt</a>) provides a complete and detailed\nsemi-formal description of the Klong language. It is probably the\nbest starting point for exploring Klong.\n</p><p>The Quick Reference (<a href=\"https://t3x.org/klong/klong-qref.txt.html\">klong-qref.txt</a>) summarizes the syntax and\nsemantics of the language. It will probably only make sense if you\nalready know K or APL.\n</p><p>Then there is a Really Short Introduction to Klong (<a href=\"https://t3x.org/klong/klong-intro.txt.html\">klong-intro.txt</a>),\nwhich you might want to read if you have never used an array language\nbefore.\n</p><p>Finally, if you already know K, here is a (probably incomplete)\nsummary of differences between Klong and K: <a href=\"https://t3x.org/klong/klong-vs-k.txt.html\">klong-vs-k.txt</a>.\n</p><h2>Compiling and Installing Klong</h2><p>Klong is written in pure ANSI C (C99), so it should compile on any\nsystem providing a C compiler. Just run make and make test. It also\ncompiles natively on Plan 9!\n</p><p>To install Klong, just copy the  binary to  or some\nsimilar place and point the  environment variable to the\n directory of the Klong source tree.\n</p><p>Files ending in  are Klong programs, you can load them with\n or  (with or without the suffix).\n</p><p>In case my Klong interpreter is not fast enough for you, Brian\nGurraci has created a vectorized version of Klong named KlongPy.\nYou can find it on GitHub: <a href=\"https://github.com/briangu/klongpy\">https://github.com/briangu/klongpy</a></p>","contentLength":1879,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44327173"},{"title":"Show HN: SnapQL – Desktop app to query Postgres with AI","url":"https://github.com/NickTikhonov/snap-ql","date":1750417698,"author":"nicktikhonov","guid":163725,"unread":true,"content":"<p>SnapQL is an open-source desktop app (built with Electron) that lets you query your Postgres database using natural language. It’s schema-aware, so you don’t need to copy-paste your schema or write complex SQL by hand.</p><p>Everything runs locally — your OpenAI API key, your data, and your queries — so it's secure and private. Just connect your DB, describe what you want, and SnapQL writes and runs the SQL for you.</p>","contentLength":420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44326620"},{"title":"Oklo, the Earth's Two-billion-year-old only Known Natural Nuclear Reactor (2018)","url":"https://www.iaea.org/newscenter/news/meet-oklo-the-earths-two-billion-year-old-only-known-natural-nuclear-reactor","date":1750413170,"author":"keepamovin","guid":163633,"unread":true,"content":"<p>Physicist Francis Perrin sat at a nuclearfuel-processing plant down in the south of France, thinking to himself: “This cannot be possible.” It was 1972. On the one hand, there was a dark piece of radioactive natural uranium ore, extracted from a mine in Africa. On the other, accepted scientific data about the constant ratio of radioactive uranium in ore.</p><p>Examination of this high-grade ore from a mine in Gabon was found to contain a lower proportion of uranium-235 (U-235) — the fissile sort. Only a tiny bit less, but enough to make the researchers sit back and scratch their heads.</p><p>The physicists’ first, logical response to such an unusual ratio of U-235 was that this was not natural uranium. All natural uranium today contains 0.720% of U-235. If you were to extract it from the Earth’s crust, or from rocks from the moon or in meteorites, that’s what you would find. But that bit of rock from Oklo contained only 0.717%.</p><p>What did this mean? At first, all the physicists could think of was that the uranium ore had gone through artificial fission, i.e.&nbsp;that some of the U-235 isotopes had been forced to split in a nuclear chain reaction. This could explain why the ratio was lower than normal.</p><p>But after complementary analyses, Perrin and his peers confirmed that the uranium ore was completely natural. Even more bedazzling, they discovered a footprint of fission products in the ore. The conclusion: the uranium ore was natural and had gone through fission. There was only one possible explanation — the rock was evidence of natural fission that occurred over two billion years ago.</p><p>“After more studies, including on-site examinations, they discovered that the uranium ore had gone through fission on its own,” said Ludovic Ferrière, curator of the rock collection at Vienna’s Natural History Museum, where a part of the curious rock will be presented to the public in 2019. “There was no other explanation.”</p><p>For such a phenomenon to have happened naturally, these uranium deposits in western Equatorial Africa must have had to contain a critical mass of U-235 to start the reaction. Back in those days, they did.</p><p>&nbsp;A second contributing factor was that, for a nuclear chain reaction to happen and be maintained, there needed to be a moderator. In this case: water. Without water to slow the neutrons down, controlled fission would not have been possible. The atoms would simply not have split.</p><p>“Like in a man-made light-water nuclear reactor, the fission reactions, without anything to slow down the neutrons, to moderate them, simply stop,” said Peter Woods, team leader in charge of uranium production at the IAEA. “The water acted in Oklo as a moderator, absorbing the neutrons, controlling the chain reaction.”</p><p>The specific geological context in what today is Gabon also helped. The chemical concentrations of total uranium (including U-235) were high enough, and the individual deposits thick and large enough. And, lastly, Oklo managed to survive the passing of time. Experts suspect there may have been other such natural reactors in the world, but these must have been destroyed by geological processes, eroded away or subducted — &nbsp;or simply not yet found.&nbsp;</p><p>“That’s what makes it so fascinating: that &nbsp;the circumstances of time, geology, water came together for this to happen at all,” Woods said. “And that it was preserved until today. The detective story has been successfully solved.”</p><p><strong>A rock sample in the IAEA’s &nbsp;home city</strong></p>","contentLength":3488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44326145"},{"title":"I will do anything to end homelessness except build more homes (2018)","url":"https://www.mcsweeneys.net/articles/i-will-do-anything-to-end-homelessness-except-build-more-homes","date":1750406854,"author":"2color","guid":163509,"unread":true,"content":"<p>Homelessness in America has reached crisis levels, and I am determined to do everything in my power to fix the problem as long as it doesn’t involve changing zoning laws or my ability to drive alone to work or, well, changing anything, really. I’m more than happy to give a hungry man a sandwich once a year and then brag to my friends about it as long as he doesn’t sit down anywhere in my line of sight to eat it. Same goes for hungry women because I’m also a feminist.</p><p>This is so important because everyone should have a bed to sleep in at night, and also, nothing destroys property values faster than a desperate person on a sidewalk asking for change. I’m not saying I don’t care about human suffering; I just care much, much more about my immediate self-interest because I’m the kind of person who contributes to society by starting companies that leverage technology to build smart tea kettles that brew themselves while you sleep at night. I’m a fucking innovator.</p><p>I’m innovating for win-win-whatever solutions where I win, my community wins, and we do whatever to get rid of homelessness. Fixing the problem means lots of things: letters to the editor of my local newspaper, bombastic statements to the press that will make the fruit of my loins cringe for generations, and especially writing vaguely discriminatory, definitely ugly posts on social media about the crisis as it unfolds in my community. Also, I call the police a lot.</p><p>Ending homelessness doesn’t mean building more homes because this town is full of homes already, especially mine, which is a single-family mini-mansion on an acre lot that I inherited from my parents and/or managed to purchase with the kind of job and bank terms and economic equality that don’t exist anymore for anyone and only ever really existed for well-educated white Americans. Either that or it’s a magnificent luxury condo with expansive views that I don’t want marred by more luxury condos or—god forbid—affordable housing.</p><p>Every room in my Instagram-worthy abode is either filled with clutter or rented out nightly to hipsters from another gentrified, monotone city also suffering from a homelessness crisis—this is a national epidemic, after all. I’m a good person, a generous person, and what made me the person I am is having to work hard for everything my parents gave me, and everything I will, in turn, give to my children.</p><p>Listen, I know that the unholy concentration of wealth in America is a big, big problem, but so is having to constantly say no to people asking for change as I whizz into Whole Foods in my Tesla or Prius (depending on how my startup investments pan out). What’s the point of having all this money if I have to feel bad about it? Also, has anyone actually verified that the homeless people claiming to be veterans aren’t just pulling some elaborate fraud? I’ve never actually met a veteran and I forget for, like, decades at a time that the military even exists because the bubble of privilege where I reside is literally impregnable, but I’m suspicious nonetheless.</p><p>I know we need more housing, but I was here first, and I’m not giving up even one blade of grass on my water-guzzling, pesticide-leaching lawn or a single burner on my twelve-burner Viking range that I never actually use to house another human soul. Tough luck, homeless people. You and your allies can call me names, but I won’t hear you over the lushness of my climate-inappropriate rose bushes and the stucco walls I’m paying some desperate immigrant under the table to build for me on the cheap before I low-key call  and have them deported.</p><p>Look, if you give people homes, the next thing you know, they’re going to start to get their lives together and then get jobs and start organizing. Then they’ll expand Medicare to everyone and build a fucking light rail line instead of a goddamn border wall, and no one will drive anymore, and cars will die out, and the air will get clean, and can you imagine the problems we’ll have then?</p><p>No. Stop it with the new housing; I’d rather have a homeless crisis.</p>","contentLength":4103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44325617"},{"title":"Learn Makefiles","url":"https://makefiletutorial.com/","date":1750406755,"author":"dsego","guid":163446,"unread":true,"content":"<p><b>I built this guide because I could never quite wrap my head around Makefiles.</b> They seemed awash with hidden rules and esoteric symbols, and asking simple questions didn’t yield simple answers. To solve this, I sat down for several weekends and read everything I could about Makefiles. I've condensed the most critical knowledge into this guide. Each topic has a brief description and a self contained example that you can run yourself.</p><p>If you mostly understand Make, consider checking out the <a href=\"https://makefiletutorial.com/#makefile-cookbook\">Makefile Cookbook</a>, which has a template for medium sized projects with ample comments about what each part of the Makefile is doing.</p><p>Good luck, and I hope you are able to slay the confusing world of Makefiles!</p><p>Makefiles are used to help decide which parts of a large program need to be recompiled. In the vast majority of cases, C or C++ files are compiled. Other languages typically have their own tools that serve a similar purpose as Make. Make can also be used beyond compilation too, when you need a series of instructions to run depending on what files have changed. This tutorial will focus on the C/C++ compilation use case.</p><p>Here's an example dependency graph that you might build with Make. If any file's dependencies changes, then the file will get recompiled:</p><h2>What alternatives are there to Make?</h2><p>Interpreted languages like Python, Ruby, and raw Javascript don't require an analogue to Makefiles. The goal of Makefiles is to compile whatever files need to be compiled, based on what files have changed. But when files in interpreted languages change, nothing needs to get recompiled. When the program runs, the most recent version of the file is used.</p><h2>The versions and types of Make</h2><p>There are a variety of implementations of Make, but most of this guide will work on whatever version you're using. However, it's specifically written for GNU Make, which is the standard implementation on Linux and MacOS. All the examples work for Make versions 3 and 4, which are nearly equivalent other than some esoteric differences.</p><p>To run these examples, you'll need a terminal and \"make\" installed. For each example, put the contents in a file called , and in that directory run the command . Let's start with the simplest of Makefiles:</p><blockquote><p>Note: Makefiles  be indented using TABs and not spaces or  will fail.</p></blockquote><p>Here is the output of running the above example:</p><pre><code>\necho \"Hello, World\"\nHello, World</code></pre><p>That's it! If you're a bit confused, here's a video that goes through these steps, along with describing the basic structure of Makefiles.</p><p>A Makefile consists of a set of . A rule generally looks like this:</p><pre><code>\n\tcommand\n\tcommand\n\tcommand</code></pre><ul><li>The  are file names, separated by spaces. Typically, there is only one per rule.</li><li>The  are a series of steps typically used to make the target(s). These <em>need to start with a tab character</em>, not spaces.</li><li>The  are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called </li></ul><p>Let's start with a hello world example:</p><pre><code>\n\techo \n\techo </code></pre><p>There's already a lot to take in here. Let's break it down:</p><ul><li>We have one  called </li><li>This target has two </li><li>This target has no </li></ul><p>We'll then run . As long as the  file does not exist, the commands will run. If  does exist, no commands will run.</p><p>It's important to realize that I'm talking about  as both a  and a . That's because the two are directly tied together. Typically, when a target is run (aka when the commands of a target are run), the commands will create a file with the same name as the target. In this case, the  does not create the .</p><p>Let's create a more typical Makefile - one that compiles a single C file. But before we do, make a file called  that has the following contents:</p><p>Then create the Makefile (called , as always):</p><p>This time, try simply running . Since there's no target supplied as an argument to the  command, the first target is run. In this case, there's only one target (). The first time you run this,  will be created. The second time, you'll see <code>make: 'blah' is up to date</code>. That's because the  file already exists. But there's a problem: if we modify  and then run , nothing gets recompiled.</p><p>We solve this by adding a prerequisite:</p><pre><code>\n\tcc blah.c -o blah</code></pre><p>When we run  again, the following set of steps happens:</p><ul><li>The first target is selected, because the first target is the default target</li><li>This has a prerequisite of </li><li>Make decides if it should run the  target. It will only run if  doesn't exist, or  is </li></ul><p>This last step is critical, and is the . What it's attempting to do is decide if the prerequisites of  have changed since  was last compiled. That is, if  is modified, running  should recompile the file. And conversely, if  has not changed, then it should not be recompiled.</p><p>To make this happen, it uses the filesystem timestamps as a proxy to determine if something has changed. This is a reasonable heuristic, because file timestamps typically will only change if the files are\nmodified. But it's important to realize that this isn't always the case. You could, for example, modify a file, and then change the modified timestamp of that file to something old. If you did, Make would incorrectly guess that the file hadn't changed and thus could be ignored.</p><p>Whew, what a mouthful. <strong>Make sure that you understand this. It's the crux of Makefiles, and might take you a few minutes to properly understand</strong>. Play around with the above examples or watch the video above if things are still confusing.</p><p>The following Makefile ultimately runs all three targets. When you run  in the terminal, it will build a program called  in a series of steps:</p><ul><li>Make selects the target , because the first target is the default target</li><li> requires , so make searches for the  target</li><li> requires , so make searches for the  target</li><li> has no dependencies, so the  command is run</li><li>The  command is then run, because all of the  dependencies are finished</li><li>The top  command is run, because all the  dependencies are finished</li><li>That's it:  is a compiled c program</li></ul><pre><code>\n\tcc blah.o -o blah \n\tcc -c blah.c -o blah.o \n\techo  &gt; blah.c </code></pre><p>If you delete , all three targets will be rerun. If you edit it (and thus change the timestamp to newer than ), the first two targets will run. If you run  (and thus change the timestamp to newer than ), then only the first target will run. If you change nothing, none of the targets will run. Try it out!</p><p>This next example doesn't do anything new, but is nontheless a good additional example. It will always run both targets, because  depends on , which is never created.</p><pre><code>\n\techo \n\ttouch some_file\n\n\n\techo </code></pre><p> is often used as a target that removes the output of other targets, but it is not a special word in Make. You can run  and  on this to create and delete .</p><p>Note that  is doing two new things here:</p><ul><li>It's a target that is not first (the default), and not a prerequisite. That means it'll never run unless you explicitly call </li><li>It's not intended to be a filename. If you happen to have a file named , this target won't run, which is not what we want. See  later in this tutorial on how to fix this</li></ul><pre><code>\n\ttouch some_file\n\n\n\trm -f some_file</code></pre><p>Variables can only be strings. You'll typically want to use , but  also works. See <a href=\"https://makefiletutorial.com/#variables-pt-2\">Variables Pt 2</a>.</p><p>Here's an example of using variables:</p><pre><code>files := file1 file2\n\n\techo \n\ttouch some_file\n\n\n\ttouch file1\n\n\ttouch file2\n\n\n\trm -f file1 file2 some_file</code></pre><p>Single or double quotes have no meaning to Make. They are simply characters that are assigned to the variable. Quotes  useful to shell/bash, though, and you need them in commands like . In this example, the two commands behave the same:</p><pre><code>a := one two\nb := 'one two' \n\tprintf '$a'\n\tprintf $b</code></pre><p>Reference variables using either  or </p><pre><code>x := dude\n\n\n\techo \n\techo ${x}\n\n\t\n\techo $x </code></pre><p>Making multiple targets and you want all of them to run? Make an  target.\nSince this is the first rule listed, it will run by default if  is called without specifying a target.</p><pre><code>\n\ttouch one\n\n\ttouch two\n\n\ttouch three\n\n\n\trm -f one two three\n</code></pre><p>When there are multiple targets for a rule, the commands will be run for each target.  is an <a href=\"https://makefiletutorial.com/#automatic-variables\">automatic variable</a> that contains the target name.</p><pre><code>\n\nf1.o f2.o:\n\techo </code></pre><p>Both  and  are called wildcards in Make, but they mean entirely different things.  searches your filesystem for matching filenames. I suggest that you always wrap it in the  function, because otherwise you may fall into a common pitfall described below.</p><pre><code>\n\tls -la  </code></pre><p> may be used in the target, prerequisites, or in the  function.</p><p>Danger:  may not be directly used in a variable definitions</p><p>Danger: When  matches no files, it is left as it is (unless run in the  function)</p><pre><code>thing_wrong := *.o \nthing_right := </code></pre><p> is really useful, but is somewhat confusing because of the variety of situations it can be used in.</p><ul><li>When used in \"matching\" mode, it matches one or more characters in a string. This match is called the stem.</li><li>When used in \"replacing\" mode, it takes the stem that was matched and replaces that in a string.</li><li> is most often used in rule definitions and in some specific functions.</li></ul><p>See these sections on examples of it being used:</p><pre><code>\n\techo \n\techo \n\techo \n\techo \n\n\ttouch hey\n\n\n\ttouch one\n\n\n\ttouch two\n\n\n\trm -f hey one two\n</code></pre><p>Make loves c compilation. And every time it expresses its love, things get confusing. Perhaps the most confusing part of Make is the magic/automatic rules that are made. Make calls these \"implicit\" rules. I don't personally agree with this design decision, and I don't recommend using them, but they're often used and are thus useful to know. Here's a list of implicit rules:</p><ul><li>Compiling a C program:  is made automatically from  with a command of the form <code>$(CC) -c $(CPPFLAGS) $(CFLAGS) $^ -o $@</code></li><li>Compiling a C++ program:  is made automatically from  or  with a command of the form <code>$(CXX) -c $(CPPFLAGS) $(CXXFLAGS) $^ -o $@</code></li><li>Linking a single object file:  is made automatically from  by running the command <code>$(CC) $(LDFLAGS) $^ $(LOADLIBES) $(LDLIBS) -o $@</code></li></ul><p>The important variables used by implicit rules are:</p><ul><li>: Program for compiling C programs; default </li><li>: Program for compiling C++ programs; default </li><li>: Extra flags to give to the C compiler</li><li>: Extra flags to give to the C++ compiler</li><li>: Extra flags to give to the C preprocessor</li><li>: Extra flags to give to compilers when they are supposed to invoke the linker</li></ul><p>Let's see how we can now build a C program without ever explicitly telling Make how to do the compilation:</p><pre><code>CC = gcc \nCFLAGS = -g \n\techo  &gt; blah.c\n\n\n\trm -f blah*</code></pre><p>Static pattern rules are another way to write less in a Makefile. Here's their syntax:</p><pre><code>\n   commands</code></pre><p>The essence is that the given  is matched by the  (via a  wildcard). Whatever was matched is called the . The stem is then substituted into the , to generate the target's prereqs.</p><p>A typical use case is to compile  files into  files. Here's the :</p><pre><code>objects = foo.o bar.o all.o\n -o all\n\n -c foo.c -o foo.o\n\n -c bar.c -o bar.o\n\n -c all.c -o all.o\n\n\n\techo  &gt; all.c\n\n\n\ttouch \n\trm -f *.c *.o all</code></pre><p>Here's the more , using a static pattern rule:</p><pre><code>objects = foo.o bar.o all.o\n -o all\n\n: %.o: %.c\n\t -c  -o \n\techo  &gt; all.c\n\n\n\ttouch \n\trm -f *.c *.o all</code></pre><h2>Static Pattern Rules and Filter</h2><p>While I introduce the <a href=\"https://makefiletutorial.com/#the-filter-function\">filter function</a> later on, it's common to use in static pattern rules, so I'll mention that here. The  function can be used in Static pattern rules to match the correct files. In this example, I made up the  and  extensions.</p><pre><code>obj_files = foo.result bar.o lose.o\nsrc_files = foo.raw bar.c lose.c\n\n: %.o: %.c\n\techo : %.result: %.raw\n\techo  \n\n%.c %.raw:\n\ttouch \n\trm -f </code></pre><p>Pattern rules are often used but quite confusing. You can look at them as two ways:</p><ul><li>A way to define your own implicit rules</li><li>A simpler form of static pattern rules</li></ul><p>Let's start with an example first:</p><pre><code>\n%.o : %.c\n\t\t -c  -o </code></pre><p>Pattern rules contain a '%' in the target. This '%' matches any nonempty string, and the other characters match themselves. ‘%’ in a prerequisite of a pattern rule stands for the same stem that was matched by the ‘%’ in the target.</p><p>Double-Colon Rules are rarely used, but allow multiple rules to be defined for the same target. If these were single colons, a warning would be printed and only the second set of commands would run.</p><pre><code>\n\techo \n\techo </code></pre><p>Add an  before a command to stop it from being printedYou can also run make with  to add an  before each line  </p><pre><code>\n\t@echo \n\techo </code></pre><p>Each command is run in a new shell (or at least the effect is as such)</p><pre><code>\n\tcd ..\n\t\n\techo `pwd`\n\n\t\n\tcd ..;echo `pwd`\n\n\t\n\tcd ..; \\\n\techo `pwd`\n</code></pre><p>The default shell is . You can change this by changing the variable SHELL:</p><pre><code>SHELL=/bin/bash\n\n\n\techo </code></pre><p>If you want a string to have a dollar sign, you can use . This is how to use a shell variable in  or .</p><p>Note the differences between Makefile variables and Shell variables in this next example.</p><pre><code>make_var = I am a make variable\n\n\tsh_var='I am a shell variable'; echo $$sh_var\n\n\t\n\techo </code></pre><h2>Error handling with , , and </h2><p>Add  when running make to continue running even in the face of errors. Helpful if you want to see all the errors of Make at once.Add a  before a command to suppress the errorAdd  to make to have this happen for every command.</p><h2>Interrupting or killing make</h2><p>Note only: If you  make, it will delete the newer targets it just made.</p><p>To recursively call a makefile, use the special  instead of  because it will pass the make flags for you and won't itself be affected by them.</p><pre><code>new_contents = \n\tmkdir -p subdir\n\tprintf  | sed -e 's/^ //' &gt; subdir/makefile\n\tcd subdir &amp;&amp; \n\trm -rf subdir\n</code></pre><h2>Export, environments, and recursive make</h2><p>When Make starts, it automatically creates Make variables out of all the environment variables that are set when it's executed.</p><pre><code>\n\techo $$shell_env_var\n\n\t\n\techo </code></pre><p>The  directive takes a variable and sets it the environment for all shell commands in all the recipes:</p><pre><code>shell_env_var=Shell env var, created inside of Make\n shell_env_var\n\n\techo \n\techo $$shell_env_var</code></pre><p>As such, when you run the  command inside of make, you can use the  directive to make it accessible to sub-make commands. In this example,  is exported such that the makefile in subdir can use it.</p><pre><code>new_contents = \n\tmkdir -p subdir\n\tprintf  | sed -e 's/^ //' &gt; subdir/makefile\n\t@echo \n\t@cd subdir &amp;&amp; cat makefile\n\t@echo \n\tcd subdir &amp;&amp; \ncooly =  cooly\n\n\trm -rf subdir</code></pre><p>You need to export variables to have them run in the shell as well.  </p><pre><code>one=this will only work locally\n two=we can run subcommands with this\n\n\n\t@echo \n\t@echo $$one\n\t@echo \n\t@echo $$two</code></pre><p> exports all variables for you.</p><pre><code>\nnew_contents = \n\ncooly = \n\tmkdir -p subdir\n\tprintf  | sed -e 's/^ //' &gt; subdir/makefile\n\t@echo \n\t@cd subdir &amp;&amp; cat makefile\n\t@echo \n\tcd subdir &amp;&amp; \n\trm -rf subdir</code></pre><p>There's a nice <a href=\"http://www.gnu.org/software/make/manual/make.html#Options-Summary\">list of options</a> that can be run from make. Check out , , . </p><p>You can have multiple targets to make, i.e.  runs the  goal, then , and then .</p><p>There are two flavors of variables:  </p><ul><li>recursive (use ) - only looks for the variables when the command is , not when it's .  </li><li>simply expanded (use ) - like normal imperative programming -- only those defined so far get expanded</li></ul><pre><code>\none = one ${later_variable}\n\ntwo := two ${later_variable}\n\nlater_variable = later\n\n\n\techo \n\techo </code></pre><p>Simply expanded (using ) allows you to append to a variable. Recursive definitions will give an infinite loop error.  </p><pre><code>one = hello\n\none := ${one} there\n\n\n\techo </code></pre><p> only sets variables if they have not yet been set</p><pre><code>one = hello\none ?= will not be set\ntwo ?= will be set\n\n\n\techo \n\techo </code></pre><p>Spaces at the end of a line are not stripped, but those at the start are. To make a variable with a single space, use </p><pre><code>with_spaces = hello   \nafter = there\n\nnullstring =\nspace = \n\techo \n\techo startend</code></pre><p>An undefined variable is actually an empty string!</p><pre><code>foo := start\nfoo += more\n\n\n\techo </code></pre><p>You can override variables that come from the command line by using .\nHere we ran make with </p><pre><code> option_one = did_override\n\noption_two = not_override\n\n\techo \n\techo </code></pre><p>The <a href=\"https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html\">define directive</a> is not a function, though it may look that way. I've seen it used so infrequently that I won't go into details, but it's mainly used for defining <a href=\"https://www.gnu.org/software/make/manual/html_node/Canned-Recipes.html#Canned-Recipes\">canned recipes</a> and also pairs well with the <a href=\"https://www.gnu.org/software/make/manual/html_node/Eval-Function.html#Eval-Function\">eval function</a>.</p><p>/ simply creates a variable that is set to a list of commands. Note here that it's a bit different than having a semi-colon between commands, because each is run in a separate shell, as expected.</p><pre><code>one =  blah=; echo $$blah\n\n two\n blah=\necho $$blah\n\n\t@echo \n\t@\n\t@echo \n\t@</code></pre><h2>Target-specific variables</h2><p>Variables can be set for specific targets</p><pre><code>\n\techo one is defined: \n\techo one is nothing: </code></pre><h2>Pattern-specific variables</h2><p>You can set variables for specific target </p><pre><code>\n\techo one is defined: \n\techo one is nothing: </code></pre><pre><code>foo = ok\n\n (, ok)\n\techo \n\techo </code></pre><h2>Check if a variable is empty</h2><pre><code>nullstring =\nfoo =  (,)\n\techo  (,)\n\techo </code></pre><h2>Check if a variable is defined</h2><p>ifdef does not expand variable references; it just sees if something is defined at all</p><pre><code>bar =\nfoo =  foo\n\techo  bar\n\techo </code></pre><p>This example shows you how to test make flags with  and . Run this example with  to see it print out the echo statement.</p><pre><code> (,)\n\techo </code></pre><p> are mainly just for text processing. Call functions with  or . Make has a decent amount of <a href=\"https://www.gnu.org/software/make/manual/html_node/Functions.html\">builtin functions</a>.</p><pre><code>bar := ${subst not,, }\n\n\t@echo </code></pre><p>If you want to replace spaces or commas, use variables</p><pre><code>comma := ,\nempty:=\nspace := \nfoo := a b c\nbar := \n\t@echo </code></pre><p>Do NOT include spaces in the arguments after the first. That will be seen as part of the string.</p><pre><code>comma := ,\nempty:=\nspace := \nfoo := a b c\nbar := \n\t@echo </code></pre><p><code>$(patsubst pattern,replacement,text)</code> does the following:</p><p>\"Finds whitespace-separated words in text that match pattern and replaces them with replacement. Here pattern may contain a ‘%’ which acts as a wildcard, matching any number of any characters within a word. If replacement also contains a ‘%’, the ‘%’ is replaced by the text that matched the ‘%’ in pattern. Only the first ‘%’ in the pattern and replacement is treated this way; any subsequent ‘%’ is unchanged.\" (<a href=\"https://www.gnu.org/software/make/manual/html_node/Text-Functions.html#Text-Functions\">GNU docs</a>)</p><p>The substitution reference <code>$(text:pattern=replacement)</code> is a shorthand for this.</p><p>There's another shorthand that replaces only suffixes: <code>$(text:suffix=replacement)</code>. No  wildcard is used here.</p><p>Note: don't add extra spaces for this shorthand. It will be seen as a search or replacement term.</p><pre><code>foo := a.o b.o l.a c.o\none := \ntwo := $(foo:%.o=%.c)\n\nthree := $(foo:.o=.c)\n\n\n\techo \n\techo \n\techo </code></pre><p>The foreach function looks like this: . It converts one list of words (separated by spaces) to another.  is set to each word in list, and  is expanded for each word.This appends an exclamation after each word:</p><pre><code>foo := who are you\n\nbar := \n\t@echo </code></pre><p> checks if the first argument is nonempty. If so, runs the second argument, otherwise runs the third.</p><pre><code>foo := \nempty :=\nbar := \n\t@echo \n\t@echo </code></pre><p>Make supports creating basic functions. You \"define\" the function just by creating a variable, but use the parameters , , etc. You then call the function with the special <a href=\"https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function\"></a> builtin function. The syntax is <code>$(call variable,param,param)</code>.  is the variable, while , , etc. are the params.</p><pre><code>sweet_new_fn = Variable Name: $(0) First: $(1) Second: $(2) Empty Variable: $(3)\n\n\n\t@echo </code></pre><p>shell - This calls the shell, but it replaces newlines with spaces!</p><p>The  function is used to select certain elements from a list that match a specific pattern. For example, this will select all elements in  that end with .</p><pre><code>obj_files = foo.result bar.o lose.o\nfiltered_files = \n\t@echo </code></pre><p>Filter can also be used in more complex ways:</p><ol><li><p><strong>Filtering multiple patterns</strong>: You can filter multiple patterns at once. For example, <code>$(filter %.c %.h, $(files))</code> will select all  and  files from the files list.</p></li><li><p>: If you want to select all elements that do not match a pattern, you can use . For example, <code>$(filter-out %.h, $(files))</code> will select all files that are not  files.</p></li><li><p>: You can nest filter functions to apply multiple filters. For example, <code>$(filter %.o, $(filter-out test%, $(objects)))</code> will select all object files that end with  but don't start with .</p></li></ol><p>The include directive tells make to read one or more other makefiles. It's a line in the makefile that looks like this:</p><p>This is particularly useful when you use compiler flags like  that create Makefiles based on the source. For example, if some c files includes a header, that header will be added to a Makefile that's written by gcc. I talk about this more in the <a href=\"https://makefiletutorial.com/#makefile-cookbook\">Makefile Cookbook</a></p><p>Use vpath to specify where some set of prerequisites exist. The format is <code>vpath &lt;pattern&gt; &lt;directories, space/colon separated&gt;</code> can have a , which matches any zero or more characters.\nYou can also do this globallyish with the variable VPATH</p><pre><code> %.h ../headers ../other-directory\n\n\n\ttouch some_binary\n\n\n\tmkdir ../headers\n\n\n\ttouch ../headers/blah.h\n\n\n\trm -rf ../headers\n\trm -f some_binary\n</code></pre><p>The backslash (\"\\\") character gives us the ability to use multiple lines when the commands are too long</p><pre><code>\n\techo This line is too long, so \\\n\t\tit is broken up into multiple lines</code></pre><p>Adding  to a target will prevent Make from confusing the phony target with a file name. In this example, if the file  is created, make clean will still be run. Technically, I should have used it in every example with  or , but I wanted to keep the examples clean. Additionally, \"phony\" targets typically have names that are rarely file names, and in practice many people skip this.</p><pre><code>\n\ttouch some_file\n\ttouch clean\n\n\n\trm -f some_file\n\trm -f clean</code></pre><p>The make tool will stop running a rule (and will propogate back to prerequisites) if a command returns a nonzero exit status. will delete the target of a rule if the rule fails in this manner. This will happen for all targets, not just the one it is before like PHONY. It's a good idea to always use this, even though make does not for historical reasons.  </p><pre><code>\n\ttouch one\n\tfalse\n\n\n\ttouch two\n\tfalse</code></pre><p>Let's go through a really juicy Make example that works well for medium sized projects.</p><p>The neat thing about this makefile is it automatically determines dependencies for you. All you have to do is put your C/C++ files in the  folder.</p><pre><code>\nTARGET_EXEC := final_program\n\nBUILD_DIR := ./build\nSRC_DIRS := ./src\n\n\nSRCS := \nOBJS := $(SRCS:%=/%.o)\n\n\nDEPS := $(OBJS:.o=.d)\n\n\nINC_DIRS := \nINC_FLAGS := \nCPPFLAGS :=  -MMD -MP\n\n/:  -o /%.c.o: %.c\n\tmkdir -p  -c  -o /%.cpp.o: %.cpp\n\tmkdir -p  -c  -o \n\trm -r </code></pre>","contentLength":21811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44325611"},{"title":"Break Up Big Tech: Civil Society Declaration – People vs. Big Tech","url":"https://peoplevsbig.tech/break-up-big-tech-civil-society-declaration/","date":1750406542,"author":"janandonly","guid":163401,"unread":true,"content":"<p>We, people and civil society organisations from Europe and around the world, call on the European Commission to act now to break up the powerful Big Tech monopolies that have a stranglehold over our digital world. Big Tech isn’t just dominating markets – it’s dominating European democracy.</p><p>Europe needs a thriving and diverse digital economy that serves the needs of European citizens, not billionaire tech CEOs. President von der Leyen has affirmed that in the EU, “<a href=\"https://www.zeit.de/politik/2025-04/ursula-von-der-leyen-eu-usa-donald-trump-english/komplettansicht\"><u>we don’t have bros or oligarchs making the rules</u></a>.” The Commission must now stand up to the tech oligarchy by strongly enforcing the EU’s digital rules and competition law.</p><p>Right now, the Commission has a once-in-a-generation opportunity to dismantle Google’s advertising monopoly, which is destroying the news media, ripping off consumers, and was ruled illegal in a landmark US judgement.</p><h2>Big Tech’s monopoly power threatens democracy</h2><p>We cannot address Big Tech’s harms without first confronting its power. A handful of tech giants have concentrated control of our core digital infrastructure – including search engines, social media, app stores, and cloud. The companies’ unchecked power over their digital empires enables them to abuse people’s rights, exploit businesses, and crush competitors.</p><p>Spanish Prime Minister Pedro Sanchez has warned that tech billionaires want “<a href=\"https://www.politico.eu/article/spain-pedro-sanchez-big-tech-billionaires-democracy-social-media/\"></a>”. When a small number of billionaires and tech giants control the internet, they wield their power – and their vast profits – to influence political discourse and interfere with democratic laws. This year, tech CEOs and the Trump administration have <a href=\"https://www.techpolicy.press/tracking-recent-statements-on-the-enforcement-of-eu-tech-laws/#US-Leaders\"></a> to try to thwart the EU’s landmark digital laws that hold Big Tech to account.</p><h2>Break up Big Tech monopolies</h2><p>Teresa Ribera, the EU’s competition chief, <a href=\"https://www.bloomberg.com/news/articles/2024-12-05/google-split-still-on-the-table-eu-s-new-antitrust-chief-says\"></a> that break-ups can prevent Big Tech from grabbing too much market power. These corporations treat billion-euro fines as the cost of doing business, while behavioural remedies are ineffective and often flouted by the companies. Forcing these giants to sell off parts of their businesses will curb conflicts of interest, level the digital playing field, and make the companies easier to hold accountable for their growing societal harms.</p><p>Breaking up tech monopolies is a step towards a freer, fairer internet. Europe can and must resist threats from Big Tech and the Trump administration, and stand firm in upholding EU law against Big Tech. Break up Google. Break up Big Tech.</p><ol><li>Foundation The London Story</li><li>Xnet, Institute for Democratic Digitalisation – Spain</li><li>Corporate Europe Observatory (CEO)</li><li>Enforce (Irish Council for Civil Liberties)</li><li>Save Social - Networks for democracy</li><li>Canadian Anti-Monopoly Project (CAMP)</li><li>Hope and Courage Collective, Ireland</li><li>German NGO Forum on Environment &amp; Development</li><li>Another Europe Is Possible</li><li>Epicenter.works - for digital rights</li><li>European Federation of Journalists (EFJ)</li><li>Deutscher Journalisten-Verband (DJV, German Journalists Association)</li><li>Global Project Against Hate and Extremism</li></ol>","contentLength":2977,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44325596"},{"title":"Hurl: Run and test HTTP requests with plain text","url":"https://github.com/Orange-OpenSource/hurl","date":1750391703,"author":"flykespice","guid":163332,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44324592"},{"title":"Show HN: Tool to Automatically Create Organized Commits for PRs","url":"https://github.com/edverma/git-smart-squash","date":1750389779,"author":"edverma2","guid":163510,"unread":true,"content":"<p>I've found it helps PR reviewers when they can look through a set of commits with clear messages and logically organized changes. Typically reviewers prefer a larger quantity of smaller changes versus a smaller quantity of larger changes. Sometimes it gets really messy to break up a change into sufficiently small PRs, so thoughtful commits are a great way of further subdividing changes in PRs. It can be pretty time consuming to do this though, so this tool automates the process with the help of AI.</p><p>The tool sends the diff of your git branch against a base branch to an LLM provider. The LLM provider responds with a set of suggested commits with sensible commit messages, change groupings, and descriptions. When you explicitly accept the proposed changes, the tool re-writes the commit history on your branch to match the LLM's suggestion. Then you can force push your branch to your remote to make it match.</p><p>The default AI provider is your locally running Ollama server. Cloud providers can be explicitly configured via CLI argument or in a config file, but keeping local models as the default helps to protect against unintentional data sharing. The tool always creates a backup branch in case you need to easily revert in case of changing your mind or an error in commit re-writing. Note that re-writing commit history to a remote branch requires a force push, which is something your team/org will need to be ok with. As long as you are working on a feature branch this is usually fine, but it's always worth checking if you are not sure.</p>","contentLength":1547,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44324457"},{"title":"Show HN: Ts-SSH – SSH over Tailscale without running the daemon","url":"https://github.com/derekg/ts-ssh","date":1750388585,"author":"i8code","guid":163724,"unread":true,"content":"<p>ts-ssh solves a specific problem: accessing machines on your Tailnet from\n  environments where you can't install the full Tailscale daemon (like CI/CD runners or\n   restricted systems).</p><pre><code>  It uses Tailscale's tsnet library to establish userspace connectivity, then provides\n  a standard SSH experience. Works with existing workflows since it supports normal SSH\n   features like ProxyCommand, key auth, and terminal handling.\n\n  Some features that proved useful:\n  • Parallel command execution across multiple hosts\n  • Built-in tmux session management for multi-host work\n  • SCP-style file transfers\n  • Works on Linux/macOS/Windows (AMD64 and ARM64)\n\n  The codebase is interesting from a development perspective - it was written almost\n  entirely using AI tools (mainly Claude Code, with some OpenAI and Jules). Not as an\n  experiment, but because it actually worked well for this kind of systems programming.\n   Happy to discuss the workflow if anyone's curious about that aspect.\n\n  Source and binaries are on GitHub. Would appreciate feedback from anyone dealing with\n   similar connectivity challenges.</code></pre>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44324378"},{"title":"Asterinas: A new Linux-compatible kernel project","url":"https://lwn.net/SubscriberLink/1022920/ad60263cd13c8a13/","date":1750384576,"author":"howtofly","guid":163508,"unread":true,"content":"<blockquote><table><tbody><tr><td><p>\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider <a href=\"https://lwn.net/subscribe/\">subscribing to LWN</a>.  Thank you\nfor visiting LWN.net!\n</p></td></tr></tbody></table></blockquote><div><p>This article was contributed by Ronja Koistinen</p></div><a href=\"https://asterinas.github.io/\">Asterinas</a> is a new\nLinux-ABI-compatible kernel project written in Rust, based on what the\nauthors call a \"framekernel architecture\".  The project overlaps somewhat\nwith the goals of the <a href=\"https://rust-for-linux.com/\">Rust for Linux\nproject</a>, but approaches the problem space from a different direction by\ntrying to get the best from both monolithic and microkernel designs.\n\n<p>\nTraditionally, monolithic kernels lump everything into one kernel-mode\naddress space, whereas microkernels only implement a minimal <a href=\"https://en.wikipedia.org/wiki/Trusted_computing_base\">trusted\ncomputing base (TCB)</a> in kernel space and rely on user-mode services for\nmuch of the operating system's functionality.  This separation implies the\nuse of interprocess communication (IPC) between the microkernel and those\nservices. This IPC often has a performance impact, which is a big part of\nwhy microkernels have remained relatively unpopular.\n\n</p><p>\nThe core of Asterinas's \"framekernel\" design is the encapsulation of all\ncode that needs Rust's  features inside a library, enabling\nthe rest of the kernel (the services) to be developed using safe\nabstractions.  Those services remain within the kernel's address space, but\nonly have access to the resources that the core library gives to them.\nThis design is meant to improve the safety of the system while retaining\nthe simple and performant shared-memory architecture of monolithic\nkernels. The <a href=\"https://asterinas.github.io/book/\">Asterinas book</a>\non the project's website provides a nice <a href=\"https://asterinas.github.io/book/kernel/the-framekernel-architecture.html\">\narchitectural mission statement and overview</a>.\n\n\n</p><p>\nThe aptness of the \"framekernel\" nomenclature can perhaps be debated.  The\nframe part refers to the development framework wrapping the unsafe\nparts behind a memory-safe API.  The concept of the TCB is, of\ncourse, not exclusive to microkernel architectures but, because there are\nstrong incentives to strictly scrutinize and, in some contexts, even <a href=\"https://en.wikipedia.org/wiki/Formal_verification\">formally\nverify</a> the TCB of a system, keeping the TCB as small as possible is a\ncentral aspect of microkernel designs.\n\n\n</p><p>\nAn update on the project is available on the Asterinas blog in the\nJune&nbsp;4 post titled \"<a href=\"https://asterinas.github.io/2025/06/04/kernel-memory-safety-mission-accomplished.html\">Kernel\nMemory Safety: Mission Accomplished</a>\".  The post explains the team's\nmotivations and the need for the industry to address memory-safety\nproblems; it provides some illustrations that explain how the framekernel\nis different from monolithic kernels and microkernels. It also takes a\nmoment to emphasize that the benefits of Rust don't stop with memory\nsafety; there are improvements to <a href=\"https://jacko.io/safety_and_soundness.html\">soundness</a> as well.\nPerhaps most importantly, the post highlights the upcoming Asterinas\npresentation at the <a href=\"https://www.usenix.org/conference/atc25/technical-sessions\">2025\nUSENIX Annual Technical Conference</a>.\n</p><p>\nIn their paper, the authors compare Asterinas to some prior Rust-based\noperating-system work, exploring the benefits of the language's\nmemory-safety features and explain how Asterinas differs from that previous\nwork.  Specifically, the paper contrasts Asterinas with <a href=\"https://www.usenix.org/conference/osdi20/presentation/narayanan-vikram\">\nRedLeaf</a>, an operating system written in Rust and presented at the 14th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI 20)\nin 2020.  Asterinas uses hardware isolation to permit running user-space\nprograms written in any programming language, aims to be general-purpose,\nand provides a Linux-compatible ABI, while RedLeaf is a microkernel that is\ndesigned  to use the hardware's isolation features, and the\nproject focuses on different things.\n</p><p>\nAnother project of interest is <a href=\"https://tockos.org/\">Tock</a>, an\nembedded system that targets SoCs with limited hardware protection\nfunctionality. Like Asterinas, Tock also divides the kernel into a\ntrusted core allowed to use  and untrusted \"capsules\" that\nare not.  As mentioned, Asterinas does rely on hardware protection and\nisn't intended for strictly embedded use, which differentiates it from\nTock.\n\n\n</p><p>\nIt bears mentioning that the Rust for Linux project, which is introducing\nRust code into the upstream Linux kernel, has similar goals as\nAsterinas. It also aims to encapsulate kernel interfaces with safe\nabstractions in such a way that drivers can be written in Rust without any\nneed for .\n\n\n</p><h4>Work toward formal verification</h4><p>\nOne goal of shrinking the TCB of an operating system is to make it feasible\nto have it formally verified.  In February 2025, the Asterinas blog\nfeatured <a href=\"https://asterinas.github.io/2025/02/13/towards-practical-formal-verification-for-a-general-purpose-os-in-rust.html\">a\npost detailing plans to do just that</a>.  The best known formally verified\nkernel is <a href=\"https://sel4.systems/About/\">seL4</a>, an L4-family\nmicrokernel.\n\n</p><p>\nAsterinas aims to use the framekernel approach to achieve a system that has\na small, formally verified TCB akin to a lean microkernel, but also a\nsimple shared-memory architecture with Linux ABI compatibility, all at the\nsame time.  This is a radical departure from any previously formally\nverified kernel; the blog post describes those kernels as deliberately\nsmall and limited compared to \"<q>full-fledged, UNIX-style OSes</q>\".\n\n\n</p><p>\nThe Asterinas project is collaborating with a security-auditing company\ncalled <a href=\"https://www.certik.com/\">CertiK</a> to use <a href=\"https://github.com/verus-lang/verus\">Verus</a> to formally verify the\nkernel.  There is an extensive <a href=\"https://github.com/asterinas/slides/blob/f62c764ea9c4831a747dbe8fa415b56e48493482/slides/2025-01-28%20Asterinas%20Security%20Assessment%20by%20CertiK.pdf\">\nreport</a> available from CertiK on how Asterinas was audited and the\nissues that were found.\n\n\n</p><p>\nThe Asterinas kernel is only one result of the project. The other two are\n<a href=\"https://crates.io/crates/ostd\">OSTD</a>, described as \"<q>a Rust\nOS framework that facilitates the development of and innovation in OS\nkernels written in Rust</q>\", and <a href=\"https://asterinas.github.io/book/osdk/guide/index.html\">OSDK</a>, a\nCargo addon to assist with the development, building, and testing of\nkernels based on OSTD.\n\n\n</p><p>\nThere are four stated goals for OSTD as a separate crate. One is to lower\nthe entry bar for operating-system innovation and to lay the groundwork for\nnewcomers to operating-system development. The second is to enhance memory\nsafety for operating systems written in Rust; other projects can benefit\nfrom its encapsulation and abstraction of low-level operations. The third is\nto promote code reuse across Rust-based operating-system projects. The\nfourth is to boost productivity by enabling testing of new code in user\nmode, allowing developers to iterate without having to reboot.\n\n\n</p><p>\nIt is worth emphasizing that the kernels that can be written with OSTD do\nnot have to be Linux-compatible or, in any way, Unix-like. The APIs\nprovided are more generic than that; they are memory-safe abstractions for\nfunctionality like x86 hardware management, booting, virtual memory, SMP,\ntasks, users, and timers.  Like most Rust crates, OSTD is <a href=\"https://docs.rs/ostd/0.14.1/ostd/index.html\">documented on\ndocs.rs</a>.\n\n\n</p><p>\nAsterinas reports Intel, among others, as a sponsor of the project.\nIntel's interest is likely related to its <a href=\"https://www.intel.com/content/www/us/en/developer/tools/trust-domain-extensions/overview.html\">Trust\nDomain Extensions (TDX)</a> feature, which provides hardware modes and\nfeatures to facilitate isolation of virtual machines, and memory\nencryption.  The Asterinas book has a brief <a href=\"https://asterinas.github.io/book/osdk/guide/intel-tdx.html\">section\non TDX</a>, and the OSDK supports it.\n\n\n</p><p>\nThe OSTD, or at least the parts that Asterinas ends up using, seems to\nessentially be the restricted TCB that allows . For an\nillustrative example, we could take a look at the  kernel\ncomponent's <a href=\"https://github.com/asterinas/asterinas/blob/ecb33ca98d2b2ac680daf1d2a48e4d011db2fbcf/kernel/comps/network/src/buffer.rs\">source\ncode</a> and see that the buffer code uses DMA, locking, allocation, and\nvirtual-memory code from the OSTD through memory-safe APIs.\n\n\n</p><p>\nAsterinas was first released under the Mozilla Public License in early\n2024; it has undergone rapid development over the past year.  GitHub <a href=\"https://github.com/asterinas/asterinas/graphs/contributors\">lists 45\nindividual committers</a>, but the majority of the commits are from a\nhandful of PhD students from the Southern University of Science and\nTechnology, Peking University, and Fudan University, as well as a Chinese\ncompany called <a href=\"https://www.antgroup.com/en\">Ant Group</a>, which\nis a sponsor of Asterinas.\n\n</p><p>\nAt the time of writing, Asterinas supports two architectures, x86 and RISC-V.\nIn the January blog post linked above, it was reported that Asterinas\nsupported 180 Linux system calls, but the number has since grown to <a href=\"https://github.com/asterinas/asterinas/blob/1fe0fef41003c824b780b7b228f7b01a46497be0/kernel/src/syscall/arch/x86.rs\">206\non x86</a>.  As of version 6.7, Linux has 368 system calls in total, so there is\nsome way to go yet.\n\n\n</p><p>\nOverall, Asterinas is in early development. There have been no releases,\nrelease announcements, changelogs, or much of anything other than Git tags\nand a short installation guide in the documentation.  The <a href=\"https://crates.io/crates/ostd/reverse_dependencies\">Dependents\ntab</a> of the OSTD crate on crates.io shows that no unrelated, published\ncrate yet uses OSTD.\n\n\n</p><p>\nIt does not seem like Asterinas is able to run any applications yet.  <a href=\"https://github.com/asterinas/asterinas/issues/1868\">Issue #1868</a>\nin Asterinas's repository outlines preliminary plans toward a first\ndistribution.  The initial focus on a custom initramfs and some rudimentary\nuser-space applications, followed by being able to <a href=\"https://github.com/asterinas/asterinas/issues/1851\">run\nDocker</a>. There are initial plans to bootstrap a distribution based on\nNix. Notably (but unsurprisingly), this issue mentions that Asterinas\ndoesn't support loading Linux kernel modules, nor does it ever\nplan to.\n\n\n</p><p>\nThe <a href=\"https://asterinas.github.io/book/kernel/roadmap.html\">Roadmap</a>\nsection of the Asterinas book says that the near-term goals are to expand\nthe support for CPU architectures and hardware, as well as to focus on\nreal-world usability in the cloud by providing a host OS for virtual\nmachines.  Apparently, the support for Linux virtio devices is already\nthere, so a major hurdle has already been cleared.  In particular, the\nChinese cloud market, in the form of Aliyun (also known as Alibaba Cloud)\n<a href=\"https://github.com/asterinas/asterinas/issues/1501\">is a\nfocus</a>.  The primary plans involve creating a container host OS with a\ntight, formally verified TCB and support for some trusted-computing\nfeatures in Intel hardware, for the Chinese cloud service.\n\n\n</p><p>\nWhile both Rust for Linux and Asterinas have similar goals (providing a\nsafer kernel by relying on Rust's memory safety), their scopes and\napproaches are different.  Rust for Linux focuses on safe abstractions\nstrictly for new device drivers to be written in safe Rust, but this leaves\nthe rest of the kernel untouched.\nAsterinas, on the other hand, aims to build a whole new kernel from the ground\nup, restricting the -permitting core to the absolute minimum,\nwhich can then be formally verified.  Asterinas also focuses on\ncontainers and cloud computing, at least for now, while Rust for Linux looks to\nbenefit the whole of the Linux ecosystem.\n\n\n</p><p>\nDespite the stated cloud focus, there is more going on, for example building\nsupport for <a href=\"https://github.com/asterinas/asterinas/issues/2008\">X11</a>\nand <a href=\"https://github.com/asterinas/asterinas/issues/2112\">Xfce</a>.\nAlso, the OSTD could, of course, prove interesting for OS development\nenthusiasts irrespective of the Asterinas project, but so far it remains unknown\nand untested by a wider audience.\n\n</p><p>\nAsterinas is certainly a refreshingly innovative take on principles for\noperating-system development, leaning on the safety and soundness\nfoundations provided by the Rust language and compiler. So far it is at an\nearly exploratory stage driven by enthusiastic Chinese researchers and\ndoesn't see any serious practical use, but it is worth keeping an eye\non. It will be interesting to see the reception it will get from the\nRust for Linux team and the Linux community at large.</p>","contentLength":10809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44324084"},{"title":"FedFlix — Public Domain Stock Footage Library","url":"https://public.resource.org/ntis.gov/index.html","date":1750381663,"author":"bookofjoe","guid":163400,"unread":true,"content":"<td>\n\t\t\t\t\t\t\tPublic.Resource.Org, Inc. \n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t1005 Gravenstein Hwy North \n\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\tSebastopol CA 95472 \n\t\t\t\t\t\t</td><td>\n\t\t\t\t\t\t\tAssociate Director \n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\tProduct and Program Management \n\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\tNTIS \n\t\t\t\t\t\t</td>","contentLength":222,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44323914"},{"title":"Open source can't coordinate?","url":"https://matklad.github.io/2025/05/20/open-source-cant-coordinate.html","date":1750381572,"author":"LorenDB","guid":163318,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44323904"},{"title":"Infinite Mac OS X","url":"https://blog.persistent.info/2025/03/infinite-mac-os-x.html","date":1750378617,"author":"kristianp","guid":163288,"unread":true,"content":"<p> Infinite Mac can now <a href=\"https://infinitemac.org/?filter=macosx\">run early Mac OS X</a>, with <a href=\"https://infinitemac.org/2001/Mac%20OS%20X%2010.1\">10.1</a> and <a href=\"https://infinitemac.org/2003/Mac%20OS%20X%2010.3\">10.3</a> being the best supported versions. It’s not particularly snappy, but as someone who lived through that period, I can tell you that it wasn’t much better on real hardware. Infinite HD has also been rebuilt to have some notable indie software from that era.</p><p>I’ve been <a href=\"https://github.com/search?q=repo%3Amihaip%2Finfinite-mac+219&amp;type=commits&amp;s=committer-date&amp;o=desc\">tracking</a> DingusPPC progress since my initial <a href=\"https://blog.persistent.info/2023/12/dingusppc.html\">port</a> and making the occasional contribution <a href=\"https://github.com/dingusdev/dingusppc/commits?author=mihaip\">myself</a>, with the hope of using it to run Mac OS X in Infinite Mac. While it has continued to improve, I reached a plateau last summer; my attempts would result in either kernel panics or graphical corruption. I tried to reduce the problem a bit via a <a href=\"https://github.com/dingusdev/dingusppc/pull/120\">deterministic execution mode</a>, but it wasn’t really clear where to go next. I decided to take a break from this emulator and explore alternate paths of getting Mac OS X to run.</p><p><a href=\"https://github.com/sebastianbiallas/pearpc\">PearPC</a> was the obvious choice – it was created with the express purpose of emulating Mac OS X on x86 Windows and Linux machines in the early 2000s. By <a href=\"https://www.osnews.com/story/7085/pearpc-01-is-it-a-miracle/\">all accounts</a>, it did this successfully for a few years, until interest waned after the Intel switch (sadly one of the <a href=\"https://pearpc.sourceforge.net/index.html#:~:text=Yesterday%20evening%20at%2023%3A00%20one%20of%20my%20best%20friends%2C%20one%20of%20my%20best%20critics%2C%20one%20of%20the%20most%20valuable%20programmers%20of%20PearPC%2C%20the%20one%20who%20could%20help%20me%20in%20all%20situations%2C%20the%20one%20with%20whom%20I%20had%20the%20best%20time%20of%20my%20life%2C%20died%20after%20getting%20hit%20by%20a%20train.\">authors passed away</a> around then). I had earlier <a href=\"https://blog.persistent.info/2023/12/dingusppc.html#:~:text=I%20also%20briefly%20considered%20PearPC%20(which%20is%20much%20more%20focused%20on%20early%20PowerMacs%20than%20QEMU)%2C%20but%20it%E2%80%99s%20also%20in%20a%20state%20of%20abandonment%20(development%20mostly%20stopped%20in%202005%2C%20with%20a%20brief%20resurrection%20in%202015).\">dismissed it</a> as a “dead” codebase, but I decided that the satisfaction of getting something working compensated for dealing with legacy C++ (complete with its own <a href=\"https://github.com/sebastianbiallas/pearpc/blob/master/src/tools/str.h\">string class</a>, <a href=\"https://github.com/sebastianbiallas/pearpc/blob/master/src/tools/snprintf.cc\">sprintf implementation</a>, and <a href=\"https://github.com/sebastianbiallas/pearpc/blob/master/src/system/gif.cc\">GIF decoder</a>). An encouraging discovery was that <a href=\"https://github.com/kanjitalk755\">kanjitalk755</a> (the de-facto Basilisk II and SheepShaver maintainer) had somewhat recently set up <a href=\"https://github.com/sebastianbiallas/pearpc/compare/master...kanjitalk755:pearpc:macos_sdl2\">an experimental branch</a> of PearPC that built and ran on modern macOS. I was able to replicate their work without too much trouble, and with that existence proof I started on my sixth port of an emulator to WebAssembly/Emscripten and the Infinite Mac runtime.</p><p>In some ways PearPC not being actively developed made things easier –&nbsp;I didn’t have to worry about merging in changes from upstream, or agonize over how to structure my modifications to make them easier to contribute back. It was also helpful that PearPC was already a multi-platform codebase and thus had the right layers of abstraction to make adding another target pretty easy. As a bonus, it didn’t make pervasive use of threads or other harder-to-port concepts. Over the course of a few days, I was able to <a href=\"https://github.com/mihaip/pearpc/commit/a63c1145964843c09e4d21fc55c182ccb53e82ce\">get it to build</a>, <a href=\"https://github.com/mihaip/pearpc/commit/b13f6813321af579895122a71aa09ba2cb793717\">output video</a>, <a href=\"https://github.com/mihaip/pearpc/commit/dde5322f13a2ca64727cd94a98795f5c1f20f6b0\">load disk images</a>, and <a href=\"https://github.com/mihaip/pearpc/commit/526ba08845122e991268c5d995167efbd083ac8e\">get mouse and keyboard input</a> hooked up. It was pretty satisfying to have Mac OS X 10.2 running in a browser more reliably than it previously had.</p><p>Performance is still not as good as DingusPPC’s – the biggest bottleneck is the lack of any kind of caching in the MMU, so all loads and stores are expensive since they involve complex address computations. DingusPPC has a much more mature <a href=\"https://github.com/dingusdev/dingusppc/blob/master/zdocs/developers/cpu/powerpc/mmuemu.md\">tiered cache</a> that appears to be quite effective. More generally, while PearPC may be more stable than DingusPPC at running 10.2-10.4, it’s a much less principled codebase (I came across <a href=\"https://github.com/sebastianbiallas/pearpc/commit/152604c0af3f0613bae37cf4287f188a42be0c06\">many mystery commits</a>) and it “cheats” in many ways (it has a <a href=\"https://github.com/sebastianbiallas/pearpc/blob/7eb63de8a92b86e1b7438019ddcf935005112501/src/io/prom/promboot.cc#L977\">custom</a> firmware and <a href=\"https://github.com/sebastianbiallas/pearpc/blob/7eb63de8a92b86e1b7438019ddcf935005112501/ppccfg.example#L75-L78\">video driver</a>, and only the <a href=\"https://github.com/sebastianbiallas/pearpc/blob/7eb63de8a92b86e1b7438019ddcf935005112501/src/cpu/cpu_generic/ppc_alu.cc#L116-L117\">subset</a> of PowerPC instructions that are needed for Mac OS X are implemented). I’m still holding out hope for DingusPPC to be the fast, stable, and correct choice for the long term.</p><p>I implemented the “unified decoding table” approach in PearPC’s interpreter one opcode family at a time. When I got to the floating point operations, I assumed it was going to be another mechanical change. I was instead surprised to see that behavior regressed – I got some rendering glitches in the Dock, and the Finder windows would not open at all. After some debugging, I noticed that the <a href=\"https://github.com/mihaip/pearpc/blob/3e979205950b9d540577204d8e86a494f53157a2/src/cpu/cpu_generic/ppc_dec.cc#L106-L172\">dispatching for opcode groups 59 and 63</a> didn’t just do a basic lookup on the relevant instruction bits. It first checked the  bit of the <a href=\"https://en.wikipedia.org/wiki/Machine_state_register\">Machine State Register (MSR)</a>, and if it was not set it would throw a “floating point unavailable” exception.</p><p>I initially thought this was the emulator being pedantic – all PowerPC chips used in Macs had an FPU, so this should never happen. However, setting a breakpoint showed that the exception was being hit pretty frequently during Mac OS X startup. The <a href=\"https://github.com/apple-oss-distributions/xnu/tree/xnu-124.7\">xnu kernel sources</a> of that time period are available, and though I’m not familiar with the details, there are places where the FP bit <a href=\"https://github.com/apple-oss-distributions/xnu/blob/xnu-124.7/osfmk/ppc/cswtch.s#L201\">is</a><a href=\"https://github.com/apple-oss-distributions/xnu/blob/xnu-124.7/osfmk/ppc/cswtch.s#L1012\">cleared</a> and a <a href=\"https://github.com/apple-oss-distributions/xnu/blob/xnu-124.7/osfmk/ppc/cswtch.s#L656-L707\">handler for the resulting exception</a> is registered. I assume this is an optimization to avoid having to save/restore FPU registers during context switches (if they’re not being used). The upshot was that once I implemented the equivalent  check in my optimized dispatch code, the rendering problems went away.</p><p>This reminded me of the rendering glitches that I had encountered when trying to run Mac OS X under DingusPPC. Even when booting from the 10.2 install CD (which does not kernel panic) I would end up with missing text and other issues:</p><p>Checking the DingusPPC sources showed that it never checked the  bit, and always allowed floating point instructions to go through. I did a quick hack to check it and raise an exception if needed, and the glitches went away!</p><p>The <a href=\"https://github.com/dingusdev/dingusppc/pull/135\">proper implementation</a> was a bit more complicated, and I ended up <a href=\"https://github.com/dingusdev/dingusppc/pull/136\">revising it a bit</a> to avoid a performance hit (and another contributor did <a href=\"https://github.com/dingusdev/dingusppc/commit/82a48899f0c28a9418a07b56b5d39f7e161b1549\">another pass</a>). But at the end of it all, DingusPPC became a lot more stable, which was a nice side effect. Better yet, it can run 10.1 reliably, which PearPC cannot. I ended up using a combination of both emulators to run a broader subset of early Mac OS X (unfortunately 10.0 is still unstable, and the Public Beta kernel panics immediately, but I’m holding out hope for the future).</p><p>Part of the appeal of Infinite Mac is that the emulated machines also have an “Infinite HD” mounted with a lot of era-appropriate software to try. With Mac OS X running, it was time to build an alternate version that went beyond the 80s and 90s classic Mac apps I had collected. I had my favorites, but I also <a href=\"https://hachyderm.io/@mihaip/113977444999284253\">put out a call for suggestions</a> and got plenty of ideas.</p><p>For actually building the disk image, I extended the <a href=\"https://blog.persistent.info/2022/03/blog-post.html#:~:text=Building%20Disk%20Images%2C%20or%20Docker%201995%2Dstyle\">automated approach</a> that I first launched the site with. Disk images were even more popular in the early days of Mac OS X than they are today, so I <a href=\"https://github.com/mihaip/infinite-mac/commit/a7f69373a7b2eb1d85ea33e975180b4a2ca02a44\">added a way</a> to import .dmgs as additional folders in the generated image. However, I quickly discovered that despite having the same extension, there are <a href=\"https://github.com/libyal/libmodi/blob/main/documentation/Mac%20OS%20disk%20image%20types.asciidoc#1-overview\">many variants</a>, and the  that ships with modern macOS cannot always mount images generated more than 20 years ago. In the end I ended up with a <a href=\"https://github.com/mihaip/infinite-mac/commit/119000a268bb3f21180e06ea899331b2307b695e\">Rube Goldberg approach</a> that first extracts the raw partition via <a href=\"https://github.com/Lekensteyn/dmg2img\">dmg2img</a> and then recreates a “modern” disk image that can be mounted and copied from.</p><p>As for getting the actual software, the usual sites like <a href=\"https://macintoshgarden.org/\">Macintosh Garden</a> do have some from that era, but it’s not a priority for them. Early to mid 2000s Mac OS X software appears to be a bit of a blind spot –&nbsp;it’s too new to be truly “retro”, but too old to still be available from the original vendor (<a href=\"https://rogueamoeba.com/legacy/\">though</a><a href=\"https://files.omnigroup.com/software/MacOSX/\">there</a><a href=\"https://download-cdn.panic.com/\">are</a><a href=\"https://c-command.com/dropdmg/support#older-versions\">exceptions</a>). I ended up using the <a href=\"https://web.archive.org/\">Wayback Machine</a> a lot. As a bonus, I also installed the companion “Developer” CDs for each Mac OS X version, so tools like Project Builder and Interface Builder are also accessible.</p><p>The only limitation that I ran into is that my disk build process is centered around HFS, but HFS+ was the default of that time period, and it introduced more advanced capabilities like longer file names containing arbitrary Unicode characters. Files from disk images that rely HFS+ features do not translate losslessly, but luckily this was not an issue for most software. To actually mount multiple drives (up to 3, between the boot disk, Infinite HD, and <a href=\"https://blog.persistent.info/2023/09/infinite-mac-improved-persistence.html\">Saved HD</a>), I <a href=\"https://github.com/mihaip/dingusppc/commit/51019e4fa0109fa8268cd547b92dee0e7065ac5c\">ended</a><a href=\"https://github.com/mihaip/pearpc/commit/93f224a8a9676479ff79aad9bf7632d400a5c693\">up</a><a href=\"https://github.com/mihaip/dingusppc/commit/b220d7b9ed39953780d2a42f9f83caaf8a6cf06e\">borrowing</a> a clever solution from a <a href=\"https://github.com/joevt/dingusppc/\">DingusPPC fork</a>: a multi-partition disk image is created on the fly from an arbitrary number of partition images that are specified at startup.</p><p>To make the addition of Mac OS X to Infinite Mac complete, I also wanted to have an Aqua mode for the site’s controls, joining the <a href=\"https://blog.persistent.info/search/label/Infinite%20Mac#:~:text=To%20reduce%20the%20cognitive%20dissonance%20(and%20to%20have%20a%20bit%20of%20fun)%2C%20I%20made%20the%20UI%20resemble%20the%20look%2Dand%2Dfeel%20of%20the%20OS%20that%20is%20being%20booted\">classic, Platinum</a>, and <a href=\"https://blog.persistent.info/search/label/Infinite%20Mac#:~:text=With%20the%20initial%20emulator%20being%20brought%20up%2C%20there%20were%20some%20more%20fun%20tasks%2C%20like%20adding%20a%20NeXT%2Dstyle%20monitor%20frame%20and%20a%20NeXT%20appearance%20to%20the%20Infinite%20Mac%20controls%20(working%20on%20them%20is%20giving%20me%20Kaleidoscope%20scheme%20flashbacks).\">NeXT</a> appearances. That prompted the question: <a href=\"https://512pixels.net/projects/aqua-screenshot-library/\">which Aqua</a>?</p><p>Though the more subdued versions from 10.3 and 10.4 are my favorites, I decided to go with the 10.0/10.1 one since it has the biggest nostalgia factor. I wanted to use the exact same image assets as the OS, and since they make heavy use of semi-transparency, regular screenshots were not going to be good enough. I used <a href=\"https://github.com/fuzziqersoftware/resource_dasm\">resource_dasm</a> and <a href=\"https://kwasi-ich.de/software/aqua/\">pxm2tga</a> to extract the original assets from <a href=\"http://www.atpm.com/11.06/customizing.shtml\">Extras.rsrc</a> and create <a href=\"https://github.com/mihaip/infinite-mac/commit/cbc6a3ea8ac0523fbea27dbd57770837c60465db\">my own version of Aqua</a>:</p><p>If the recent rumors of a <a href=\"https://www.bloomberg.com/news/articles/2025-03-10/apple-readies-dramatic-design-overhauls-for-ios-19-ipados-19-and-macos-16\">big UI revamp</a> do come true, it’ll be nice to have this reference point of its ancestor.</p><p>The ability to mount multiple images means that you can also have a Mac OS 9 partition and start the Classic compatibility environment (this only works under 10.1 – PearPC never supported Classic). You can thus emulate classic Mac apps inside an emulated Mac OS X inside a WebAssembly virtual machine:</p><p>There was a recent storm in a teacup about <a href=\"https://mjtsai.com/blog/2025/01/30/repeating-calculator-operations/\">a Calculator behavior change</a>. Using these Mac OS X images, it’s possible to verify that versions through 10.3 didn’t have the “repeatedly press equals” behavior, but 10.4 did.</p><p>Though I’ve moved away from custom domain names, I thought <a href=\"https://macosx.app\">macosx.app</a> would make a nice <a href=\"https://system6.app\">addition</a><a href=\"https://system7.app\">to</a><a href=\"https://macos8.app\">my</a><a href=\"https://macos9.app\">collection</a>. Unfortunately it’s taken, though in a rather weird way. I even contacted the YouTuber whose video it redirects to, and he said he was not the one that registered it. It expires in a couple of months, so maybe I’ll be able to grab it.</p><blockquote>“When Alexander saw the breadth of his domain, he wept for there were no more worlds to conquer.”\n— <a href=\"https://www.theparisreview.org/blog/2020/03/19/and-alexander-wept/\">Some Frenchman</a></blockquote><p>Mac OS X support catches Infinite Mac up to the modern day, unless I happen to get access to some <a href=\"https://web.archive.org/web/20171006210639/https://twitter.com/mcclure111/status/916405883202129921\">time travel mechanics</a>. There are of course two more CPU transitions to go through and numerous small changes, but Tiger is fundamentally recognizable to any current-day macOS user.</p><p>Except that in the retrocomputing world, it’s always possible to go deeper or more obscure. <a href=\"https://en.wikipedia.org/wiki/A/UX\">A/UX</a> is not something that I’m very familiar with, but it was a contemporary of classic Mac OS and would be interesting to compare to NeXTStep. <a href=\"https://github.com/pruten/shoebill\">Shoebill</a> runs it, and the codebase looks approachable enough to port. Then there’s <a href=\"https://github.com/mihaip/infinite-mac/issues/167\">Lisa</a>, the <a href=\"https://en.wikipedia.org/wiki/Apple_Pippin\">Pippin</a> (DingusPPC has some <a href=\"https://github.com/dingusdev/dingusppc/blob/master/machines/machinepippin.cpp\">nascent support</a>), and further afield the Newton (via <a href=\"https://github.com/pguyot/Einstein\">Einstein</a>?). We’ll see what moves me next.</p><p>When I first began exploring ways of running Mac OS X, I mentioned that <a href=\"https://blog.persistent.info/2023/12/dingusppc.html#:~:text=The%20obvious%20choice%20was%20QEMU%20%E2%80%93%20it%20has%20very%20broad%20guest%20OS%20support%20and%20is%20very%20actively%20developed.%20However%2C%20it%E2%80%99s%20also%20a%20beast%20of%20a%20project%20to%20build%20and%20navigate%20around%3B%20it%20didn%E2%80%99t%20seem%20like%20it%20would%20be%20something%20I%20would%20able%20to%20make%20much%20progress%20on%20while%20working%20on%20it%20for%20a%20few%20hours%20a%20week.\">QEMU seemed too daunting to port</a> to WebAssembly given my limited time. Furthermore, the performance of the <a href=\"https://github.com/atrosinenko/qemujs\">qemu.js</a> experiment from a few years ago made it seem like even if it did run, it would be much too slow to be usable. However, I recently became aware of <a href=\"https://github.com/ktock/qemu-wasm\">qemu-wasm</a> via <a href=\"https://fosdem.org/2025/schedule/event/fosdem-2025-6290-running-qemu-inside-browser/\">this FOSDEM presentation</a>. The performance of its Linux guest <a href=\"https://ktock.github.io/qemu-wasm-demo/\">demos</a> is encouraging: I ran an impromptu bennmark of computing an MD5 checksum of 100 MB of data and it completed it in 8 seconds (vs. 13 for DingusPPC and 18 for PearPC). There’s still a big gap between that and a graphical guest like Mac OS X, but it’s nice to have this existence proof.</p>","contentLength":10989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44323719"},{"title":"Giant, all-seeing telescope is set to revolutionize astronomy","url":"https://www.science.org/content/article/giant-all-seeing-telescope-set-revolutionize-astronomy","date":1750375059,"author":"gammarator","guid":163399,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44323389"},{"title":"Show HN: I wrote a new BitTorrent tracker in Elixir","url":"https://github.com/Dahrkael/ExTracker","date":1750373389,"author":"dahrkael","guid":163217,"unread":true,"content":"<p>I'm currently in a journey to learn and improve my Elixir and Go skills (my daily job uses C++) and looking through my backlog for projects to take on I decided Elixir is the perfect language to write a highly-parallel BitTorrent tracker.\nSo I have spent my free time these last 3 months writing one! Now I think it has enough features to present it to the world (and a docker image to give it a quick try).</p><p>I know some people see trackers as relics of the past now that DHT and PEX are common but I think they still serve a purpose in today's Internet (purely talking about public trackers). That said there is not a lot going on in terms of new developments since everyone just throws opentracker in a vps a calls it a day (honorable exceptions: aquatic and torrust).</p><p>I plan to continue development for the foreseeable future and add some (optional) esoteric features along the way so if anyone currently operates a tracker please give a try and enjoy the lack of crashes.</p><p>note: only swarm_printout.ex has been vibe coded, the rest has all been written by hand.</p>","contentLength":1060,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44323253"},{"title":"Literate programming tool for any language","url":"https://github.com/zyedidia/Literate","date":1750371525,"author":"LorenDB","guid":163317,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44323045"},{"title":"Estrogen: A Trip Report","url":"https://smoothbrains.net/posts/2025-06-15-estrogen.html","date":1750364115,"author":"sebg","guid":163141,"unread":true,"content":"<p>I have <a href=\"https://en.wikipedia.org/wiki/Gender_dysphoria\">gender dysphoria</a>. I find labels overly reifying; I feel reluctant to call myself , per se: when prompted to state my gender identity or preferred pronouns, I fold my hands into the <a href=\"https://www.yogapedia.com/definition/6871/dhyana-mudra\"></a> and state that I . Mostly people seem to vibe it, but sometimes it feels a little like weasel words. Other times, when I’m in a sillier mood, I’ll tell people I’m  – if only because it sounds like something I’d put in my station wagon. Of course, my faithful Subaru Outback was made before 2008, which means it wants the <a href=\"https://www.amazon.com/Genuine-Subaru-SOA868V9210-Coolant-Gallon/dp/B007L72U1C/\">green, long-life genderfluid</a>…</p><p>I experience an ongoing <a href=\"https://slatestarcodex.com/2013/02/18/typical-mind-and-gender-identity/\">brain-body map</a><a href=\"https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/\">prediction error</a> – my brain seems to expect a differently shaped body to the one I wound up with. I have been acutely aware of this since before I hit puberty. Out of shame and embarassment, I suppressed this, but I also made a promise to myself that if I hadn’t come out by the time I turned thirty then I was allowed to get as weird as I needed to.</p><p>During the COVID-19 pandemic I went through a phase of using self-administered ketamine therapy to refactor a long list of maladaptive behavioural patterns, and eventually this particular issue became impossible to ignore. I had avoided reifying it for long enough, and this wasn’t working for me – I had to try something different. One evening in July 2021, I sat down with a close friend. <em>I am going to put a large amount of ketamine up my nose</em>, I said. <em>Your job is to start asking me questions about my sexuality.</em></p><p>Not long after, I had jumped through the relevant bureaucratic hoops,  and subsequently found myself cycling home from the pharmacy with a paper bag filled with repurposed menopause medication – a starter pack of  estradiol patches, to be applied twice a week.</p><p>While the  effects of estrogen are <a href=\"https://genderdysphoria.fyi/en/second-puberty-fem\">well-documented</a>, back when I came out I had difficulty finding detailed phenomenological reports of the  effects of estrogen. I did wind up reading a large number of anecdotal reports on <a href=\"https://www.reddit.com/r/asktransgender\">Reddit</a>, and found that in aggregate, people tend to report <a href=\"https://www.reddit.com/r/asktransgender/comments/rfpokm/mental_effects_of_estrogen_hrt/\">positive subjective effects</a>. One could propose a number of non-exclusive hypotheses as to why – I’ll attempt to review these later in this post.</p><p>Did it make sense for me to try this? It was time to find out for myself. I unboxed the patches and placed one on my stomach.</p><p>Estrogen receptors are located throughout the body. Of these, there are two main types – <a href=\"https://en.wikipedia.org/wiki/ERα\">ERα</a> and <a href=\"https://en.wikipedia.org/wiki/Estrogen_receptor_beta\">ERβ</a>. These have similar <a href=\"https://en.wikipedia.org/wiki/Ligand_(biochemistry)#binding_affinity\">binding affinities</a> for estradiol, but are expressed in different proportions in different bodily tissues, and can have different effects on gene regulation.</p><blockquote><p>Estradiol is a steroid hormone that influences the serotonergic, dopaminergic, and glutamatergic systems. Estradiol exerts its effects through classical mechanisms by binding to nuclear estrogen receptors α, and β, or through nonclassical mechanisms through binding to membrane bound estrogen receptors α, β, and <a href=\"https://en.wikipedia.org/wiki/GPER\">GPER</a>.</p></blockquote><p>The effects are so wide-ranging that any review I can write will no doubt oversimplify things. That said, I’d like to highlight two findings relevant to neurotransmitter levels:</p><ul><li><a href=\"https://en.wikipedia.org/wiki/Dopamine\"></a> synthesis is upregulated by ERα and downregulated by ERβ via <a href=\"https://en.wikipedia.org/wiki/Tyrosine_hydroxylase\">tyrosine hydroxylase</a> transcription. Potentially, they work in tandem to maintain homeostatic levels, but ERα has greater influence at higher estradiol levels.</li></ul><p>These neurotransmitters are, of course, <a href=\"https://x.com/AskYatharth/status/1615157727625637888\">stereotypically</a> associated with  and .</p><blockquote><p>Evidence from neuroimaging findings to link estrogen and the\nserotonergic system in humans are still relatively sparse. Animal\ndata support ovariectomy to decrease <a href=\"https://en.wikipedia.org/wiki/5-HT1_receptor\">5-HT1</a> binding, <a href=\"https://en.wikipedia.org/wiki/5-HT2A_receptor\">5-HT2A</a> binding and expression, and <a href=\"https://en.wikipedia.org/wiki/Serotonin_transporter\">5-HT transporter</a> binding sites and expression. These findings have\nbeen shown to be reversible with estrogen replacement therapy.</p></blockquote><p>This process involves several steps: First, researchers remove the ovaries from female rats, and then divide them into two groups – one receiving estrogen treatment and the other serving as a control. Next, finely sliced brain samples are taken from both groups and exposed to a radioactive ligand, which binds to the receptor of interest. Finally, these radioactive samples are used to <a href=\"https://en.wikipedia.org/wiki/Autoradiograph\">create images on radiosensitive film</a>, which is then developed and analysed.</p><p>This is not the kind of procedure we generally perform on humans. Additionally, these preclinical rat model studies concern ovarectomized female rats and are intended to inform treatment programmes for postmenopausal human women – so their relevance to humans starting from an androgenic baseline is possibly somewhat limited. Still, there’s a <a href=\"https://academic.oup.com/endo/article-abstract/131/2/662/2496261\">couple</a><a href=\"https://www.sciencedirect.com/science/article/abs/pii/096007609500075B\">of</a> these studies I’d like to highlight, which found estrogen caused:</p><p>That said, there  exist a <a href=\"https://pubmed.ncbi.nlm.nih.gov/12900319/\">couple</a><a href=\"https://www.fertstert.org/article/S0015-0282(03)00973-7/fulltext\">of</a> studies assessing the influence of estrogen on 5-HT2A receptor binding in humans, using a radioactive ligand and <a href=\"https://en.wikipedia.org/wiki/Positron_emission_tomography\">positron emission tomography</a>. In both studies, five postmenopausal women were assessed both before and after hormone replacement therapy, and both found estrogen increased 5-HT2A receptor binding in <a href=\"https://en.wikipedia.org/wiki/Prefrontal_cortex\">prefrontal regions</a>. The resolution is pretty low, but see for yourself:</p><p>Alright, so what are these  and  receptors responsible for? These are glutamate and serotonin receptors, respectively – and these are also the specific receptors that are <a href=\"https://en.wikipedia.org/wiki/Receptor_antagonist\"></a> by <a href=\"https://psychonautwiki.org/wiki/Ketamine\">ketamine</a> and <a href=\"https://en.wikipedia.org/wiki/Agonist\"></a> by most <a href=\"https://psychonautwiki.org/wiki/Serotonergic_psychedelic\">serotonergic psychedelics</a>. If recreational drugs targeting these receptors can engender euphoric subjective effects – what might estrogen be capable of?</p><p>The subjective perceptual and psychological effects of estrogen are wide-ranging and subtle. I’ll start by discussing the more mundane sensory changes I experienced before moving on to those which might be more nebulous or ineffable.</p><p>At the time of writing, I’ve been on and off estrogen for a period of nearly three years. My initial dosage was one of the <a href=\"https://en.wikipedia.org/wiki/Estradiol\">estradiol</a> patches, but I doubled this after a short while. I have also tried <a href=\"https://en.wikipedia.org/wiki/Estradiol_valerate\">estradiol valerate</a> pills, twice or three times daily – though this turned out to be too low, and I wound up switching back to patches. I have found using patches to result in the most striking and noticeable subjective effects. I have not yet tried injected estrogen, though I anticipate doing so before long.</p><p>Additionally, at one point I tried taking a <a href=\"https://en.wikipedia.org/wiki/Progesterone\">progesterone</a> suppository. This made me feel quite stupid the following day, so I did not try this again.</p><p>I’ve long been in the bad habit of rolling out of bed and grabbing a Monster Zero straight from the fridge first thing in the morning, and I tend to follow this up with Diet Coke throughout the day. This means that I’m fairly attuned to the taste of artificial sweeteners, so naturally the change in taste perception was the first thing I noticed – within a day or two of first putting the patches on, I found that  things tasted ; and  things tasted both  and more  – and the cinnamon taste in my standard reference Diet Coke really .</p><p>This was rather exciting; I was not expecting to find that the <a href=\"https://en.wikipedia.org/wiki/Taste#Basic_tastes\">primary tastes</a> were not in fact primal, but in fact could shift around inside a lower-dimensional <a href=\"https://en.wikipedia.org/wiki/Latent_space\">latent space</a>. This got me theorising – as I wrote <a href=\"https://smoothbrains.net/posts/2024-06-18-an-informal-review-of-anthropic-qualia-states.html#the-somatic-field\">elsewhere</a>:</p><blockquote><p>Perhaps taste could be built out of something like <a href=\"https://en.wikipedia.org/wiki/Dyad_(music)\"></a> vibrations, tuned by evolution towards <a href=\"https://en.wikipedia.org/wiki/Consonance_and_dissonance\">consonance or dissonance</a> in order to generate an attractive or aversive response in the organism?</p></blockquote><p>It took me a little while before I noticed any change to my sense of smell, but this was more a factor of encountering the relevant stimulus. It was boys. Boys smelt different.</p><p>Much earlier in life, I’d had to convince myself I was gay by using the fact that boys smelt . This was very much no longer the case, and I began to notice wide variation in the way boys smelt, which sometimes was really quite unpleasant – , even.</p><p>I have somatic sensory issues. Skin sensations have always been overwhelming – my mother will confirm that I would scream if she attempted to dress me in wool, and in adulthood I avoid buying clothing with sleeves. By default, my skin feels like a bag of white noise – and when things get bad it can feel like my whole body is covered with randomised pinprick sensations, like minuscule <a href=\"https://smoothbrains.net/posts/2024-03-01-5-meo-dmt.html\">topological defects</a> in the <a href=\"https://smoothbrains.net/posts/2024-06-18-an-informal-review-of-anthropic-qualia-states.html#the-somatic-field\">somatic field</a>.</p><p>This has interfered with my ability to experience intimacy; simply lying in bed with somebody could be a stressful time for me. Estrogen ramps all of this way down in intensity – it’s a tremendous relief.</p><p>Perhaps my cleaning habits can provide an objective measure. Because things like sweat on my skin or leftover food in my mouth constitute intolerable sensory distractions, I’d tend to shower up to four times a day and brush my teeth about as often – since starting estrogen, I’m much less neurotic about both of these things.</p><p>A less turbulent nervous system also seems to be less disruptive for sleep. Beforehand, I took it for granted that I would often wake up throughout the night in a state of discomfort, whereas while I am on estrogen I reliably wake up in the morning feeling well-rested.</p><p>This one’s quite subtle – it was the kind of thing that was more noticeable when I experimented with deliberately spiking my hormones. I’ll do my best to explain. It’s as if I took the entire volumetric representation of the space around me and increased the degree to which every point within that could influence the location of every other point, recursively. This allows everything to elastically settle into a more harmonious equilibrium. This effect is basically identical to what a small dose of psychedelics can do, specifically a tryptamine like <a href=\"https://psychonautwiki.org/wiki/Psilocybin_mushrooms\">psilocybin</a> or <a href=\"https://psychonautwiki.org/wiki/DMT\">DMT</a>.</p><p>It’s hard to say what the utility of this might be. The balance between entropy and harmony is an important one – too much entropy and it’s hard to tell signal from noise, and too much harmony and you might miss important details. I did feel that with a more parsimonious model of the space around me, I got better at driving – though my friends would say I got more  at driving. Competence might be orthogonal to confidence, but I maintain that parallel parking is much easier now.</p><p>I ride my bicycle every morning – this is my primary meditative practice. I am also surrounded by steep hills, so I noticed within a couple of days that I could not activate my quads and hamstrings as hard as I was used to. This happened much faster than could possibly be accounted for by muscular atrophy, so I surmised that this must be a neuromuscular phenomenon. Later on I switched back to my own hormones for a short period, and once again the change was quite rapid. There was nothing quite like the rush of  uphill once again.</p><p>Being less strong honestly sucked pretty bad, and this required some psychological adjustment.  The flipside of this was that I found estrogen to be a  muscle relaxant, and ultimately this made the effect a net positive.</p><p>Around the time I transitioned was also the period when I was exploring some quite extreme ketamine-assisted <a href=\"https://www.fasciaresearch.com/literature/sensory-innervation/InnervationExcerpt.pdf\">myofascial release</a> techniques in order to shake off a lifetime’s worth of accumulated tension from things like bad ergonomics and social anxiety. I’d say estrogen has been partially instrumental in getting me from a place where I’m constantly attacking myself with a foam roller and massage gun just to feel comfortable in my own body – to one where massage is more of a light maintenance task, like a bird preening its feathers.</p><p>I have spent a big chunk of my life navigating chronic emotional disaffection – high school sucked, and later I had an acute week-long dissociative episode when I was twenty-one which I’m not sure I ever quite came back from. Suffice it to say I lived an emotionally stagnant existence for most of my twenties – so when the hormones opened things up, I got quite attached to my new feelings.</p><p>Funny things were  – I recall a moment about a week after I started the hormone treatment, when I laughed at something I saw on YouTube – the surge of joy was like an electric cauteriser through my breastbone. Music <a href=\"https://www.youtube.com/watch?v=XGWbIw8oK9I\"></a> now. I can lean in to the sense of affection I feel towards my friends. I cry more frequently; but this is clearly critical for releasing tension that would otherwise remain below the surface.</p><p>There’s another side to all this. I have had to navigate a number of situations where I now found myself unable to dissociate from some issue in my life that had been bothering me – I  to do something. Often this felt destructive; in retrospect there’s things I could have handled with far more grace and care, but instead I chose to drive a bulldozer through them.</p><p>For better or worse, this is what the hormones can do. It’s a bit of an epistemic nightmare – do I take action to deal with the thing that’s bugging me, or would it be better to skip my hormones for a day or two and see if I consider things differently? I can only recommend entering into this groundless game of instrumental hormone manipulation if one is comfortable taking responsibility for epistemic frame-shifting.</p><p>I’d engaged with a number of deliberate psychological interventions in the lead-up to coming out, with the general aim of managing my social self-awareness. I knew I needed the confidence. If I was to socially transition, I’d need to not get too overwhelmed or hung up on what other people thought of me.</p><blockquote><p>Here’s how I usually explain it to people: You have , which corresponds to everything currently in your sensorium. Then you have , which is a subset of that – like the beam of a spotlight – and most importantly, you have  over it, you can choose where to point it and how wide or narrow you would like it to be.</p></blockquote><p>Sometimes we might feel that our attention is involuntarily yanked around by invisible aversive forces that are seemingly beyond our control – for instance, I might find it challenging to make eye contact with people at a party, and spend the whole time with my attention collapsed and pointed at the floor.</p><p>I think what Alexander Technique does is <a href=\"https://x.com/m_ashcroft/status/1807008725548363845\">teach mindfulness of this class of phenomena and how to  attention out of them</a>. Prior to transition, I deliberately experimented with this form of attentional modulation – primarily in the kind of social setting that I would normally find overwhelming, but also just while riding my bicycle outside. I think this kind of practice has a lot of potential for helping undo the archetypal trauma-induced behavioural patterns displayed by socially anxious autistic people. Personally I found it to be remarkably effective, and after some months of this I felt that my anxiety disorder was mostly in remission.</p><p>So it was a humungous letdown when I found that all of this got  on estrogen. I cringed my way through social events, and returned to staring at the floor – but I didn’t let this stop me. I thought of it like Goku training in the <a href=\"https://dragonball.fandom.com/wiki/Gravity_Machine\">one hundred times Earth gravity chamber</a>. I just learned it all again from scratch.</p><p>If someone feels that they’d rather have a feminine body, estrogen is going to satisfy this desire – . This is obvious. I’m more interested in finer-grained,  rather than  sensory phenomena. What are the other reasons that estrogen might ? I’d like to propose a number of theories – I’ll try to order them from least speculative to most speculative.</p><blockquote><p>There was no point where I didn’t feel somehow removed from the world around me – this disconcerting sensation was present from my earliest memories. As a child I just didn’t really see the point of practically anything I was doing, or that anyone else was doing; it held no real emotional resonance or meaning for me. Whatever interests I chose to pursue felt more like an obligatory way of filling time, not something that had any value or importance in its own right.</p><p>I always felt the lack of spontaneity characteristic of depersonalization disorder, and whenever I chose to say anything, it felt rehearsed and acted out as if I had to engage my every word and action manually. Most of the time I would choose to say nothing at all. My feelings seemed to be kept at a distance, happening as something separate from an interior “me” who didn’t truly experience these emotions and seemingly couldn’t be touched by them. I was painfully conscious of all of these things.</p></blockquote><p>She continues with a visual description which I found particularly fascinating:</p><blockquote><p>In sufferers of depersonalization, symptoms can become more prominent in the form of sudden attacks – and it gets worse the more you keep thinking about it. Later that night, I step outside to get some air, and the thought enters my mind that the trees, cars, and houses on our street could just be particularly elaborate Lego pieces. The clouds in the night sky could easily pass for a simple rendering in Blender. Isn’t at least half of what we see practically a hallucination that’s filled in by our brain without us even noticing? If all these things were just renderings, it seems like it would be easy to take advantage of that.</p><p>I can almost envision everything on our street coming apart piece by piece like an exploded technical diagram. The asphalt, the curb, the patches of grass, all of them could just lift into the air and drift apart, nothing but thin surfaces, almost like abstractions or mere representations. If I were to take a shovel and start digging a hole in the road, it would just be an indentation in that surface, pushing it to extend a bit in one direction or another – but underneath it, nothing. The houses along the street are just outgrowths of the surface, a sort of puckering in it, like a ball on a rubber sheet to demonstrate how gravity is the curvature of spacetime.</p></blockquote><p>When I read this, I could not help but think two things:</p><ul><li>Wow, <em>this sounds just like my life</em>.</li><li>Wow, .</li></ul><p>I related very strongly to both her description of feeling distanced from life – as well as her description of the <a href=\"https://smoothbrains.net/posts/2024-06-18-an-informal-review-of-anthropic-qualia-states.html#the-visual-field\">visual field</a> being – as she has written <a href=\"https://www.reddit.com/r/asktransgender/comments/7t1z1w/comment/dt9e6xx/\">elsewhere</a> – <em>nothing but some strange infinitesimally thin surface stretched over infinite hollowness</em>. Both of these effects increase when I experiment with ketamine, and I also notice that both of these effects reverse when I use estrogen. In particular, the way in which estrogen alters <a href=\"https://smoothbrains.net/posts/2025-06-15-estrogen.html#attentional-modulation\">attentional modulation</a> also seems responsible for an increase in <a href=\"https://slehar.wordpress.com/2014/09/12/amodal-perception/\">amodal perception</a>, which in turn makes the visual field feel less  – though I don’t necessarily regard this as  or . It just is.</p><p>I’d also previously read <a href=\"https://x.com/slatestarcodex\">Scott Alexander</a>’s blog post, <a href=\"https://slatestarcodex.com/2017/06/28/why-are-transgender-people-immune-to-optical-illusions\">Why Are Transgender People Immune To Optical Illusions</a>, in which he speculates that if ketamine is an  which causes depersonalisation – and if estrogen <em>upregulates NDMA receptor expression</em> – then it’s possible that changes to the NMDA receptor network could be what’s responsible for the relevant changes in phenomenology.</p><p>Right now I don’t have much to add beyond: <em>wow, I think this checks out</em>. The question remains – does estrogen correct some kind of underlying <em>NMDA receptor expression deficit</em>, which ultimately leads to the psychological problems correlated with gender dysphoria – and, how does this relate to gender dysphoria itself?</p><p>Much of my personal research simply consists of reading a large number of Reddit comments. As such, I sorely wish for there to exist an equivalent paper to the DMT phenomenology one, but which scrapes <a href=\"http://reddit.com/r/asktransgender/\">transgender</a><a href=\"https://www.reddit.com/r/AskMtFHRT/\">support</a><a href=\"https://www.reddit.com/r/TransDIY/\">subreddits</a> for subjective reports instead. However, until such time as one exists, the reader may just have to take my word for it when I claim a particular effect of estrogen is “commonly reported”.</p><blockquote><p>Yes! I was at the art museum yesterday and I became utterly infatuated with a shade of blue I’ve never seen before. Sat and stared at it for like 15 minutes.</p><p>Then again, I may just be happier now.</p></blockquote><p>Referring back to the DMT phenomenology paper,  or  colours were reported in 25.2% of experiences. Personally, I didn’t experience any shift in colour perception, but I did find the other <a href=\"https://smoothbrains.net/posts/2025-06-15-estrogen.html#visual-perception\">visual perception</a> changes I experienced to be distinctively  in nature – as I mentioned earlier, particularly reminiscent of a tryptamine like <a href=\"https://psychonautwiki.org/wiki/Psilocybin_mushrooms\">psilocybin</a> or <a href=\"https://psychonautwiki.org/wiki/DMT\">DMT</a>. This is especially noticeable when I deliberately spike my levels with an extra patch, and on some days I suspect I even notice a slight amount of increased <a href=\"https://x.com/MatthijsCox/status/1611363519093694465\">symmetrical texture repetition</a>.</p><p>Estrogen is known to <em>upregulate 5-HT2A receptor expression</em>, which is of course the same serotonin receptor which is agonised by most <a href=\"https://en.wikipedia.org/wiki/Psychedelic_drug\">serotonergic psychedelics</a>. It seems quite reasonable to me to assume that this is what’s responsible for the various reported sensory enhancements in addition to the <a href=\"https://smoothbrains.net/posts/2025-06-15-estrogen.html#emotional-modulation\">changes in mood</a>.</p><p>I now have an additional question. In addition to correcting some kind of <em>NMDA receptor expression deficit</em> inherent to the gender dysphoric neurotype, does estrogen also correct a <em>5-HT2A receptor expression deficit</em> – or does tripping on estrogen ?</p><p>As <a href=\"https://smoothbrains.net/posts/2025-06-15-estrogen.html#motor-output\">mentioned above</a>, I found estrogen to be an incredibly powerful muscle relaxant. Using the <a href=\"https://slatestarcodex.com/2016/09/12/its-bayes-all-the-way-up/\">Bayesian brain</a> model to understand this, it seems as if my nervous system holds <a href=\"https://www.lesswrong.com/w/priors\"></a> for how tense every muscle in my body should be in response to a given situation – and these priors are  under the influence of estrogen.</p><p>I have to credit estrogen with helping fix a number of long-standing neck and upper back problems which I’ve been dealing with for most of my life. It’s sufficiently powerful that I am skeptical that I would have been able to fix these issues while I was on testosterone. Notably, these issues don’t return when I stop taking estrogen.</p><p>The effects feel more foundational than this, however; estrogen feels like it reshapes my body map itself, <a href=\"https://smoothbrains.net/posts/2024-05-29-what-is-a-bodymind-knot.html\">smoothing out knots</a> – like an elastic membrane being tightened, or a soap bubble reaching equilibrium. I’ve seen it “commonly reported” that estrogen makes people feel , and I suspect that this is what people might tend to mean by that.</p><p>Could this be related to the serotonergic activity? Might the estrogen be unwinding a lifetime of accumulated neuromuscular trauma through a form of low-dose psychedelic therapy? I suspect this effect is also responsible for my <a href=\"https://smoothbrains.net/posts/2025-06-15-estrogen.html#emotional-modulation\">changes in mood</a> – do emotions resonate more freely through a more parsimonious bodymind?</p><blockquote><p>Every circuit has its own <a href=\"https://en.wikipedia.org/wiki/Hausdorff_dimension\">natural density/dimensionality</a> it’s designed for, and my intuition is that organs closer to the brain are designed to have higher dimensionality. In some sense this makes them more capable of general processing, but also more prone to the particular deficits expressed in autism, with the brain as the apex of this hierarchy.</p><p>Over time, civilization has thrown humanity increasingly high-dimensional challenges, leading to evolution progressively ‘dialing the dimensionality knob up’ on our nervous systems. Perhaps we can view dysfunctional autists as those who overshot the human nervous system’s current ‘Goldilocks zone’ for dimensionality and have nervous systems dominated by static/turbulence as a result. There may be different ‘flavors’ of autism, depending on which brain regions and tissues have elevated dimensionality.</p></blockquote><p>When I read this some years ago, I had something of an <em>I’m in this picture and I don’t like it</em> moment. I don’t know that his theory is necessarily true, but I certainly felt that my own sensorium was <em>dominated by static/turbulence</em>.</p><p>As <a href=\"https://smoothbrains.net/posts/2025-06-15-estrogen.html#somatic-perception\">mentioned above</a>, I found that estrogen toned down my ongoing somatic sensitivities to more manageable levels – and there’s a handful of trans women I’ve spoken to who agreed with me that it turned the static down.</p><blockquote><p>My guess is something like joint issues → poor proprioception → all sensory experience is noisy and confusing → the brain, which is embodied and spends most of its time trying to process sensory experience, learns a different reasoning style → different reasoning style is less context-dependent (producing symptoms of autism) → different reasoning style when trying to interpret bodily correlates of gender (eg sex hormones) → transgender.</p></blockquote><p>Personally, I don’t have any joint issues, and I think that his theory of dyphoria could be simpler than this. Perhaps autistic sensory sensitivities mean that the brain is constanly dealing with having to reject overly noisy sensory input, leading to a stressed out, overly tense, disembodied nervous system – and this is what ultimately manifests as dysphoria? However, this would only explain , and not .</p><blockquote><p>It has previously been argued that autism-spectrum conditions can be understood as resulting from a predictive-processing mechanism in which an inflexibly high weight is given to sensory-prediction errors that results in overfitting their predictive models to the world. Deficits in executive functioning, theory of mind, and central coherence are all argued to flow naturally from this core underlying mechanism.</p><p>The diametric model of autism and psychosis suggests a simple extension of this hypothesis. If people on the autism spectrum give an inflexibly high weight to sensory input, could it be that people with a predisposition to psychosis (i.e., people high in ) give an inflexibly  weight to sensory input?</p></blockquote><p>Andersen carefully describes the terms  and  as he uses them in the paper, emphasizing that these categories should be viewed as flexible and not defined by dysfunction:</p><blockquote><p>In this article I refer to this axis as the <em>autism-schizotypy continuum</em>. For convenience, I refer to people on either end of this continuum as being an “autistic type” or a “schizotype”, although it should be understood that there are no clear-cut “types” and that these differences are continuous rather than categorical.</p><p>According to these models, everyone falls somewhere on the autism–schizotypy continuum, and neither autistic-like traits nor positive schizotypy represent dysfunction. Instead, each side of the continuum is accompanied by its own set of cognitive-perceptual strengths and weaknesses. People high in autistic-like traits are detail-oriented, have a focused attentional style that allows them to ignore distractors, have some advantages in sensory-discrimination abilities, and have highly developed systemizing skills, allowing them to learn and use complicated rules-based systems.</p><p>People high in positive schizotypy tend to be imaginative and creative and have a more diffuse attentional style (compared with the average person) that allows them to switch their attention more easily. There is also some evidence that people high in positive schizotypy tend to direct their attention toward highly abstract, “big-picture” concerns rather than focusing on details.</p></blockquote><p>Andersen proposes that in the case of schizotypy, lower sensitivity to prediction errors permits sensory input to flow further up the predictive processing hierarchy, which is what results in the observed behavioural traits:</p><blockquote><p>In autism, inflexibly high precision weighting of sensory input means that prediction matching tends to take place at relatively low levels of the processing hierarchy. Inflexibly low precision weighting of sensory input with positive schizotypy would have the opposite effect. Because the schizotype is, on average, handling fewer sensory-prediction errors than the autistic type (because they pay attention only to the large errors and ignore the smaller ones), prediction errors will tend to propagate farther up the processing hierarchy, affecting values, goals, and beliefs at higher levels of abstraction.</p></blockquote><p>At this stage, I had to ask myself if the hormone I’d been taking which seemed to reduce my symptoms of autism was doing so by reducing an inherent oversensitivity to prediction errors? If this was the case, might it also be pushing me further towards the other end of the <em>autism-schizotypy continuum</em>? What might that look like? The paper has this to say about schizotypal patterns of belief:</p><blockquote><p>Although the autistic type may rely more on culturally inherited high-level belief systems, the schizotype’s proclivity for tinkering with high-level priors may lead to the construction of relatively idiosyncratic high-level belief systems. In our own culture, this could manifest as having odd or (seemingly) unlikely beliefs about high-level causes. This may include beliefs in the paranormal, idiosyncratic religious beliefs (e.g., being “spiritual but not religious”), or believing conspiracy theories, all of which are associated with positive schizotypy.</p></blockquote><p>I’ll outline some of the psychological changes I’ve noticed in myself since starting estrogen. The term “schizo” is used very informally in today’s internet vernacular, making it difficult to discuss these concepts in a sensible manner – but if the reader is comfortable playing armchair psychologist, perhaps they can judge for themselves whether the following makes me more “schizo”:</p><ul><li>Increased  of other people’s internal states, resulting in a mixture of higher empathy and higher social anxiety. I’m somewhat more neurotic about potential threats.</li><li>Decreased  and , for instance with tedious matters like finances.</li></ul><p>Armchair diagnoses aside, I do wish to assert that these psychological changes are quite similar to the kind of psychological changes I tend to experience while on a mild dose of psychedelics. So far as the pharmacology goes, there is an argument to be made that psychedelics induce a temporary state of psychosis via 5-HT2A agonism. From <a href=\"https://journals.sagepub.com/doi/full/10.1177/0269881120959637\">Pivotal mental states</a> (Brouwer and Carhart-Harris, 2021):</p><blockquote><p>The psychotomimetic (psychosis-mimicking) effects of classic 5-HT2A receptor agonist psychedelics have been well documented. Importantly, psychedelics are felt to be useful models of  psychotic states that may be more likely to display psychedelic-like phenomena, such as changes in perception, cognition and ego functioning. Conversely, established psychotic disorders such as schizophrenia are more likely to feature characteristics of  cognition such as fixed delusions. Selective 5-HT2A receptor antagonism attenuates the main characteristic subjective effects of LSD, psilocybin and ayahuasca and the intensity of psychedelic states is reliably predicted by 5-HT2A receptor occupancy.</p></blockquote><p>It’s important to note that the authors are specifically discussing  rather than , and I couldn’t find any evidence that  involves 5-HT2A receptor signalling. That said, given the two are related, and given that estrogen <em>upregulates 5-HT2A receptor expression</em>, could estrogen be responsible for increased positive schizotypy via a similar mechanism to psychedelics?</p><p>I’d like to review what I’ve claimed so far:</p><p>First of all, I should note that I don’t expect these claims about estrogen phenomenology to generalise from trans women to cis women, and I’d also be cautious about generalizing neuroendocrinological findings from postmenopausal women to people starting from an androgenic baseline.  All this aside, I think this should be mostly sufficient to explain why estrogen might make somebody , especially if they are predisposed to depersonalisation, disembodiment, or autistic sensory sensitivities. However, I don’t think we’re that much closer to understanding whether hormone replacement therapy is actually correcting some kind of .</p><blockquote><p>The simplest explanation which fits the data (including nonbrain intersex conditions) is that sexual differentiation is a fragile rube goldberg machine, prone to random breakage. I speculate that humans have intersex brains so often because of evolution pulling out all stops for large brains and breaking things as a side effect.</p></blockquote><p>While I don’t think people should have to convince the medical system of the validity of their internal experience in order to justify a hormone prescription – other people , and I don’t think this is likely to change anytime soon. So I think this research is  – not least because this is an issue that directly affects an unusually productive and talented segment of society, many of whom I consider my friends.</p><p>Here’s how I’d like to thread the needle. Gender dysphoria occupies an unusual epistemic status within a society not known for <a href=\"https://cosmicindigestion.substack.com/p/around-here-we-take-our-phenomenology\">taking phenomenology seriously</a>, because – at least in liberal spaces – people’s self reports are generally never questioned.</p><p>I’m not complaining – I don’t think this is a bad thing, even though I can be picky with my metaepistemics sometimes. What I would like to see is further development into phenomenological models of gender dysphoria. <a href=\"https://genderdysphoria.fyi/\">Existing models</a> are already quite comprehensive, covering phenomena from high-level <a href=\"https://genderdysphoria.fyi/en/social-dysphoria\"></a> to low-level <a href=\"https://genderdysphoria.fyi/en/physical-dysphoria\"></a> – but I think they could utilise  detail, as it may provide essential clues to what’s going on.</p><p>Could it be the case that gender dysphoria is a morphic resonance phenomenon – and estrogen helps access the cosmic feminine unconscious by loading a different configuration file from the akashic records?  After all, if estrogen does make me more schizotypal… ?</p>","contentLength":32644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44322153"},{"title":"Compiling LLMs into a MegaKernel: A path to low-latency inference","url":"https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17","date":1750360854,"author":"matt_d","guid":163216,"unread":true,"content":"<p>One of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single also known as a . In this design, the system launches just  GPU kernel to execute the entire model — from layer-by-layer computation to inter-GPU communication — without interruption. This approach offers several key performance advantages:</p><ol><li><strong>Eliminates kernel launch overhead</strong>, even in multi-GPU settings,by avoiding repeated kernel invocations;</li><li><strong>Enables software pipelining</strong>, allowing the kernel to begin loading data for the next layer while computing the current one;</li><li><strong>Overlaps computation and communication</strong>, as a megakernel can simultaneously execute compute operations and inter-GPU communication to hide latency.</li></ol><p>Despite these advantages, compiling an LLM into a megakernel is highly challenging. Existing high-level ML frameworks — such as <a href=\"https://pytorch.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\">PyTorch</a>, <a href=\"https://github.com/triton-lang/triton\" rel=\"noopener ugc nofollow\" target=\"_blank\">Triton</a>, and <a href=\"https://tvm.apache.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\">TVM</a> — do not natively support end-to-end megakernel generation. Additionally, modern LLM systems are built from a diverse collection of specialized kernel libraries: <a href=\"https://github.com/NVIDIA/nccl\" rel=\"noopener ugc nofollow\" target=\"_blank\">NCCL</a> or <a href=\"https://developer.nvidia.com/nvshmem\" rel=\"noopener ugc nofollow\" target=\"_blank\">NVSHMEM</a> for communication, <a href=\"https://github.com/flashinfer-ai/flashinfer\" rel=\"noopener ugc nofollow\" target=\"_blank\">FlashInfer</a> or <a href=\"https://github.com/Dao-AILab/flash-attention\" rel=\"noopener ugc nofollow\" target=\"_blank\">FlashAttention</a> for efficient attention, and CUDA or <a href=\"https://github.com/triton-lang/triton\" rel=\"noopener ugc nofollow\" target=\"_blank\">Triton</a> for custom computation. This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel.</p><p><strong>Can we automate this process through compilation?</strong> Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed <a href=\"https://github.com/mirage-project/mirage\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>— a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers.</p><p>A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers.</p><p>Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single NVIDIA A100 40GB GPU, MPK reduces per-token decoding latency from  — as achieved by optimized systems like vLLM and SGLang — to , approaching the theoretical lower bound of  (based on loading 16 GB of weights with 1.6 TB/s memory bandwidth).</p><p>Beyond single-GPU optimization, MPK fuses computation and inter-GPU communication into a single megakernel. This design enables MPK to maximally overlap computation and communication. As a result, the performance improvements of MPK over current systems <strong>increase with the number of GPUs</strong>, making it particularly effective for multi-GPU deployments.</p><p>The rest of this blog dives deeper into how MPK works:</p><ul><li> introduces the , which transforms an LLM’s computation graph into an optimized task graph;</li><li> covers the , which executes this task graph within a megakernel to achieve high throughput and low latency.</li></ul><p>The computation performed by a large language model (LLM) is typically represented as a , where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel. However, this <strong>kernel-per-operator execution model</strong> often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units.</p><p>Consider a typical example: an allreduce operation following a matrix multiplication. In existing kernel-per-operator systems, the allreduce kernel must wait until the entire matmul kernel completes. In reality, though, each chunk of data for the allreduce only depends on a portion of the matmul output. This mismatch between logical and actual data dependencies limits the potential for overlapping computation and communication.</p><p>To address this issue, MPK introduces a compiler that automatically transforms the LLM’s computation graph into a fine-grained . This task graph explicitly captures dependencies at the sub-kernel level, enabling more aggressive pipelining across layers.</p><ul><li>Each  (shown as a rectangle in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM).</li><li>Each  (shown as a circle) represents a synchronization point between tasks.</li><li>Each task has an outgoing edge to a , which is activated once all associated tasks complete.</li><li>Each tasks also has an incoming edge from a , indicating the task can start execution as soon as the event is activated.</li></ul><p>Task graphs allow MPK to uncover pipelining opportunities that would be missed in computation graphs. For example, MPK can construct an optimized task graph where each allreduce task depends only on the corresponding matmul task that produces its input — enabling partial execution and overlap.</p><p>In addition to generating an optimized task graph, MPK also automatically generates <strong>high-performance CUDA implementations</strong> for each task using the <a href=\"https://github.com/mirage-project/mirage\" rel=\"noopener ugc nofollow\" target=\"_blank\">Mirage kernel superoptimizer</a>. This ensures that each task runs efficiently on a GPU SM. (For more about the kernel superoptimizer, see <a rel=\"noopener\" href=\"https://zhihaojia.medium.com/generating-fast-gpu-kernels-without-programming-in-cuda-triton-3fdd4900d9bc\" data-discover=\"true\">this post</a>.)</p><p>MPK includes an on-GPU runtime system that <strong>executes the task graph entirely within a single GPU megakernel</strong>, allowing for fine-grained control over task execution and scheduling without any kernel launches during inference.</p><p>To achieve this, MPK statically partitions all streaming multiprocessors (SMs) on a GPU into two roles:  and . The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead.</p><p>Each  operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop:</p><ol><li>Fetch the next task from its queue.</li><li>Execute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers).</li><li>Notify the triggering event upon task completion.</li></ol><p>This design ensures that workers remain fully utilized while enabling task execution to proceed asynchronously across layers and operations.</p><p>Scheduling decisions are handled by MPK’s , each of which runs on a . Because each SM can accommodate multiple warps, up to four schedulers can run concurrently per SM. Each scheduler maintains a queue of activated events. It continuously:</p><ol><li>Dequeues activated events whose dependencies are satisfied (i.e., all prerequisite tasks have completed).</li><li>Launches the set of tasks that depend on the activated event.</li></ol><p>This decentralized scheduling mechanism minimizes coordination overhead while enabling scalable execution across SMs.</p><p>Figure 3 illustrates MPK’s execution timeline. Each rectangle represents a task running on a worker; each circle represents an event. As a task completes, it increments the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event.</p><p>This design allows for <strong>fine-grained software pipelining</strong> and <strong>overlap between computation and communication</strong>. For example:</p><ul><li>Matmul tasks can execute in parallel with attention tasks from different layers.</li><li>Allreduce communication can begin as soon as partial matmul results are available.</li></ul><p>Because all scheduling and task transitions occur within a single kernel context, <strong>the overhead between tasks is extremely low</strong> — typically just  — enabling efficient execution of multi-layer, multi-GPU LLM workloads.</p><p>Our vision for MPK is to make <strong>megakernel compilation both easy to use and highly performant</strong>. Currently you can compile an LLM into a megakernel with just a few dozen lines of Python code — mainly to specify the megakernel’s inputs and outputs. We’re excited about this direction, and there’s still much more to explore. Some of the key areas we’re actively working on include:</p><ul><li><strong>Support for modern GPU architectures. </strong>One of our next milestones is extending MPK to support next-generation architectures such as . A major challenge lies in integrating warp specialization — a key optimization for newer GPUs — with MPK’s megakernel execution model.</li><li><strong>Handling workload dynamism. </strong>MPK currently builds a static task graph, which limits its ability to handle dynamic workloads such as  models. We’re developing new compilation strategies that allow MPK to support dynamic control flow and conditional execution inside megakernels.</li><li><strong>Advanced scheduling and task assignment:</strong> MPK unlocks a new level of  at the task level. While our current implementation uses simple round-robin scheduling to distribute tasks across SMs, we see exciting opportunities in <strong>advanced scheduling policies</strong> — such as priority-aware or throughput-optimized strategies — for use cases like latency-SLO-driven serving or hybrid batching.</li></ul><p>We believe MPK represents a foundational shift in how LLM inference workloads are compiled and executed on GPUs, and we’re eager to collaborate with the community to push this vision forward.</p><p>To learn more about MPK and explore our code and documentation, please visit our project website: <a href=\"https://github.com/mirage-project/mirage\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/mirage-project/mirage</a>.</p><p>We welcome feedback, contributions, and collaborations from the community!</p>","contentLength":9283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44321672"},{"title":"Juneteenth in Photos","url":"https://texashighways.com/travel-news/the-history-of-juneteenth-in-photos/","date":1750354919,"author":"ohjeez","guid":163091,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44320851"},{"title":"In praise of “normal” engineers","url":"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/","date":1750354601,"author":"zdw","guid":163053,"unread":true,"content":"<p><em>This article was originally <a href=\"https://refactoring.fm/p/in-praise-of-normal-engineers\">commissioned by Luca Rossi</a> (paywalled) for refactoring.fm, on February 11th, 2025. Luca edited a version of it that emphasized the importance of building “10x engineering teams” . It was later picked up by IEEE Spectrum (!!!), who scrapped most of the teams content and published a <a href=\"https://spectrum.ieee.org/10x-engineer\">different, shorter piece</a> on March 13th.</em></p><p><em>This is my personal edit. It is not exactly identical to either of the versions that have been publicly released to date. It contains a lot of the source material for the talk I gave last week at #LDX3 in London, “<a href=\"https://speakerdeck.com/charity/in-praise-of-normal-engineers-ldx3\">In Praise of ‘Normal’ Engineers</a>” (slides), and a couple weeks ago at CraftConf.&nbsp;</em></p><h2>In Praise of “Normal” Engineers</h2><p>Most of us have encountered a few engineers who seem practically magician-like, a class apart from the rest of us in their ability to reason about complex mental models, leap to non-obvious yet elegant solutions, or emit waves of high quality code at unreal velocity.<img data-recalc-dims=\"1\" decoding=\"async\" data-attachment-id=\"10015\" data-permalink=\"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/normal-praise-black-squish/\" data-orig-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?fit=1024%2C1024&amp;ssl=1\" data-orig-size=\"1024,1024\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"In Praise of “Normal” Engineers\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?fit=300%2C300&amp;ssl=1\" data-large-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?fit=660%2C660&amp;ssl=1\" src=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?resize=174%2C174&amp;ssl=1\" alt=\"In Praise of &quot;Normal&quot; Engineers\" width=\"174\" height=\"174\" srcset=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-praise-black-squish.png?w=1024&amp;ssl=1 1024w\" sizes=\"(max-width: 174px) 100vw, 174px\"></p><p>I have run into any number of these incredible beings over the course of my career. I think this is what explains the curious durability of the “10x engineer” meme. It may be based on flimsy, shoddy research, and the claims people have made to defend it have often been&nbsp;risible (e.g. “10x engineers have dark backgrounds, are rarely seen doing UI work, are poor mentors and interviewers”), or blatantly double down on stereotypes (“we look for young dudes in hoodies that remind us of Mark Zuckerberg”). But damn if it doesn’t resonate with experience. It just feels true.</p><p>The problem is not the idea that there are engineers who are 10x as productive as other engineers. I don’t have a problem with this statement; in fact, that much seems self-evidently true. The problems I do have are twofold.</p><h2>Measuring productivity is fraught and imperfect</h2><p>First: how are you measuring productivity? I have a problem with the implication that there is One True Metric of productivity that you can standardize and sort people by. Consider, for a moment, the sheer combinatorial magnitude of skills and experiences at play:</p><p>Also: people and their skills and abilities are not static. At one point, I was a pretty good DBRE (I even co-wrote the book on it). Maybe I was even a 10x DB engineer then, but certainly not now. I haven’t debugged a query plan in years.</p><p>“10x engineer” makes it sound like 10x productivity is an immutable characteristic of a person. But someone who is a 10x engineer in a particular skill set is still going to have infinitely more areas where they are normal or average (or less). I know a lot of world class engineers, but I’ve never met anyone who is 10x better than everyone else across the board, in every situation.</p><h2>Engineers don’t own software, teams own software</h2><p>Second, and even more importantly: So what? It doesn’t matter. Individual engineers don’t own software, teams own software. <strong>The smallest unit of software ownership and delivery is the engineering team</strong>. It doesn’t matter how fast an individual engineer can write software, what matters is how fast the team can collectively write, test, review, ship, maintain, refactor, extend, architect, and revise the software that they own.</p><p>Everyone uses the same software delivery pipeline. If it takes the slowest engineer at your company five hours to ship a single line of code, it’s going to take the fastest engineer at your company five hours to ship a single line of code. The time spent writing code is typically dwarfed by the time spent on every other part of the software development lifecycle.</p><p>If you have services or software components that are owned by a single engineer, that person is a single point of failure.<img data-recalc-dims=\"1\" fetchpriority=\"high\" decoding=\"async\" data-attachment-id=\"10013\" data-permalink=\"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/normal-spof/\" data-orig-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?fit=1024%2C1024&amp;ssl=1\" data-orig-size=\"1024,1024\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"normal-spof\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?fit=300%2C300&amp;ssl=1\" data-large-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?fit=660%2C660&amp;ssl=1\" src=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?resize=226%2C226&amp;ssl=1\" alt=\"\" width=\"226\" height=\"226\" srcset=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-spof.png?w=1024&amp;ssl=1 1024w\" sizes=\"(max-width: 226px) 100vw, 226px\"></p><p>I’m not saying this should never happen. It’s quite normal at startups to have individuals owning software, because the biggest existential risk that you face is not moving fast enough, not finding product market fit, and going out of business. But as you start to grow up as a company, as users start to demand more from you, and you start planning for the survival of the company to extend years into the future…ownership needs to get handed over to a team. Individual engineers get sick, go on vacation, and leave the company, and the business has got to be resilient to that.</p><p>If teams own software, then the key job of any engineering leader is to craft high-performing engineering teams. If you must 10x something, 10x this. <strong>Build 10x engineering teams.</strong></p><h2>The best engineering orgs are the ones where normal engineers can do great work</h2><p>When people talk about world-class engineering orgs, they often have in mind teams that are top-heavy with staff and principal engineers, or recruiting heavily from the ranks of ex-FAANG employees or top universities.</p><p>But I would argue that a truly great engineering org is one where you don’t HAVE to be one of the “best” or most pedigreed engineers in the world to get shit done and have a lot of impact on the business.</p><p>I think it’s actually the other way around. A truly great engineering organization is one where perfectly normal, workaday software engineers, with decent software engineering skills and an ordinary amount of expertise, can consistently move fast, ship code, respond to users, understand the systems they’ve built, and move the business forward a little bit more, day by day, week by week.</p><p>Any asshole can build an org where the most experienced, brilliant engineers in the world can build product and make progress. That is not hard. And putting all the spotlight on individual ability has a way of letting your leaders off the hook for doing their jobs. It is a HUGE competitive advantage if you can build sociotechnical systems where less experienced engineers can convert their effort and energy into product and business momentum.</p><p>A truly great engineering org also happens to be one that mints world-class software engineers. But we’re getting ahead of ourselves, here.</p><h2>Let’s talk about “normal” for a moment</h2><p>A lot of technical people got really attached to our identities as smart kids. The software industry tends to reflect and reinforce this preoccupation at every turn, from Netflix’s “we look for the top 10% of global talent” to Amazon’s talk about “bar-raising” or Coinbase’s recent claim to “hire the top .1%”. (Seriously, guys? Ok, well, Honeycomb is going to hire only the top !)</p><p>In this essay, I would like to challenge us to set that baggage to the side and think about ourselves as .</p><p>It can be humbling to think of ourselves as normal people, but most of us are in fact pretty normal people (albeit with many years of highly specialized practice and experience), and<img data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" data-attachment-id=\"10011\" data-permalink=\"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/normal-made-not-born/\" data-orig-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?fit=1024%2C1024&amp;ssl=1\" data-orig-size=\"1024,1024\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"normal-made-not-born\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?fit=300%2C300&amp;ssl=1\" data-large-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?fit=660%2C660&amp;ssl=1\" src=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?resize=264%2C264&amp;ssl=1\" alt=\"\" width=\"264\" height=\"264\" srcset=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-made-not-born.png?w=1024&amp;ssl=1 1024w\" sizes=\"auto, (max-width: 264px) 100vw, 264px\"> there is . Even those of us who are certified geniuses on certain criteria are likely quite normal in other ways — kinesthetic, emotional, spatial, musical, linguistic, etc.</p><p>Software engineering both selects for and develops certain types of intelligence, particularly around abstract reasoning, but  is born a great software engineer. <strong>Great engineers are made, not born</strong>. I just don’t think there’s a lot more we can get out of thinking of ourselves as a special class of people, compared to the value we can derive from thinking of ourselves collectively as relatively normal people who have practiced a fairly niche craft for a very long time.</p><h2>Build sociotechnical systems with “normal people” in mind</h2><p>When it comes to hiring talent and building teams, yes, absolutely, we should focus on identifying the ways people are exceptional and talented and strong. But when it comes to building sociotechnical systems for software delivery, we should focus on all the ways people are .</p><p>Normal people have cognitive biases — confirmation bias, recency bias, hindsight bias. We work hard, we care, and we do our best; but we also forget things, get impatient, and zone out. Our eyes are inexorably drawn to the color red (unless we are colorblind). We develop habits and ways of doing things, and resist changing them. When we see the same text block repeatedly, we stop reading it.</p><p>We are embodied beings who can get overwhelmed and fatigued. If an alert wakes us up at 3 am, we are much more likely to make mistakes while responding to that alert than if we tried to do the same thing at 3pm. Our emotional state can affect the quality of our work. Our relationships impact our ability to get shit done.</p><p>When your systems are designed to be used by normal engineers, all that excess brilliance they have can get poured into the product itself, instead of wasting it on navigating the system itself.</p><h2>How do you turn normal engineers into 10x engineering teams?</h2><p>None of this should be terribly surprising; it’s all well known wisdom. In order to build the kind of sociotechnical systems for software delivery that enable normal engineers to move fast, learn continuously, and deliver great results as a team, you should:</p><h4>Shrink the interval between when you write the code and when the code goes live.</h4><p>Make it as short as possible; the shorter the better. I’ve written and given talks about this many, many times. The shorter the interval, the lower the cognitive carrying costs. The faster you can iterate, the better. The more of your brain can go into the product instead of the process of building it.</p><p>One of the most powerful things you can do is have a short, fast enough deploy cycle that you can ship one commit per deploy. I’ve referred to this as the “software engineering death spiral” … when the deploy cycle takes so long that you end up batching together a bunch of engineers’ diffs in every build. The slower it gets, the more you batch up, and the harder it becomes to figure out what happened or roll back. The longer it takes, the more people you need, the higher the coordination costs, and the more slowly everyone moves.</p><p>Deploy time is the feedback loop at the heart of the development process. It is almost impossible to overstate the centrality of keeping this short and tight.</p><h4>Make it easy and fast to roll back or recover from mistakes.</h4><p>Developers should be able to deploy their own code, figure out if it’s working as intended or not, and if not, roll forward or back swiftly and easily. No muss, no fuss, no thinking involved.</p><h4>Make it easy to do the right thing and hard to do the wrong thing. <img data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" data-attachment-id=\"10018\" data-permalink=\"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/normal-sparkles/\" data-orig-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?fit=1024%2C1024&amp;ssl=1\" data-orig-size=\"1024,1024\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"normal-sparkles\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?fit=300%2C300&amp;ssl=1\" data-large-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?fit=660%2C660&amp;ssl=1\" src=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?resize=137%2C137&amp;ssl=1\" alt=\"\" width=\"137\" height=\"137\" srcset=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-sparkles.png?w=1024&amp;ssl=1 1024w\" sizes=\"auto, (max-width: 137px) 100vw, 137px\"></h4><p>Wrap designers and design thinking into all the touch points your engineers have with production systems. Use your platform engineering team to think about how to empower people to swiftly make changes and self-serve, but also remember that a lot of times people will be engaging with production late at night or when they’re very stressed, tired, and&nbsp;possibly freaking out. Build guard rails. The fastest way to ship a single line of code should also be the easiest way to ship a single line of code.</p><h4>Invest in instrumentation and observability.</h4><p>You’ll never know — not really — what the code you wrote does just by reading it. The only way to be sure is by instrumenting your code and watching real users run it in production. Good, friendly sociotechnical systems invest  in tools for sense-making.</p><p>Being able to visualize your work is what makes engineering abstractions accessible to actual engineers. You shouldn’t have to be a world-class engineer just to debug your own damn code.</p><h4>Devote engineering cycles to internal tooling and enablement.</h4><p>If fast, safe deploys, with guard rails, instrumentation, and highly parallelized test suites are “everybody’s job”, they will end up nobody’s job. Engineering productivity isn’t something you can outsource. Managing the interfaces between your software vendors and your own teams is both a science and an art. Making it look easy and intuitive is really hard. It needs an owner.</p><h4>Build an inclusive culture.</h4><p>Growth is the norm, growth is the baseline. People do their best work when they feel a sense of belonging. An inclusive culture is one where everyone feels safe to ask questions, explore, and make mistakes; where everyone is held to the same high standard, and given the support and encouragement they need to achieve their goals.</p><h4>Diverse teams are resilient teams.<img data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" data-attachment-id=\"10017\" data-permalink=\"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/normal-transp-rainbow/\" data-orig-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?fit=1024%2C1024&amp;ssl=1\" data-orig-size=\"1024,1024\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"normal-transp-rainbow\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?fit=300%2C300&amp;ssl=1\" data-large-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?fit=660%2C660&amp;ssl=1\" src=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?resize=196%2C196&amp;ssl=1\" alt=\"\" width=\"196\" height=\"196\" srcset=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-transp-rainbow.png?w=1024&amp;ssl=1 1024w\" sizes=\"auto, (max-width: 196px) 100vw, 196px\"></h4><p>Yeah, a team of super-senior engineers who all share a similar background can move incredibly fast, but a monoculture is fragile. Someone gets sick, someone gets pregnant, you start to grow and you need to integrate people from other backgrounds and the whole team can get derailed — fast.</p><p>When your teams are used to operating with a mix of genders, racial backgrounds, identities, age ranges, family statuses, geographical locations, skill sets, etc — when this is just table stakes, standard operating procedure — you’re better equipped to roll with it when life happens.</p><h4>Assemble engineering teams from a range of levels.</h4><p>The best engineering teams aren’t top-heavy with staff engineers and principal engineers. The best engineering teams are ones where nobody is running on autopilot, banging out a login page for the 300th time; everyone is working on something that challenges them and pushes their boundaries. Everyone is learning, everyone is teaching, everyone is pushing their own boundaries and growing. All the time.</p><p>By the way — all of that work you put into making your systems resilient, well-designed, and humane is the same work you would need to do to help onboard new engineers, develop junior talent, or let engineers move between teams.</p><p>It gets used and reused. Over and over and over again.</p><h2>The only meaningful measure of productivity is impact to the business</h2><p>The only thing that actually matters when it comes to engineering productivity is whether or not you are moving the business materially forward.</p><p>Which means…we can’t do this in a vacuum. The most important question is whether or not we are working on the right thing, which is a problem engineering can’t answer without help from product, design, and the rest of the business.</p><p>Software engineering isn’t about writing lots of lines of code, it’s about solving business problems using technology.</p><p>Senior and intermediate engineers are actually the workhorses of the industry. They move the business forward, step by step, day by day. They get to put their heads down and crank instead of constantly looking around the org and solving coordination problems. If you have to be a staff+ engineer to move the product forward, something is seriously wrong.</p><h2>Great engineering orgs mint world-class engineers</h2><p>A great engineering org is one where you don’t HAVE to be one of the best engineers in the world to have a lot of impact. But — rather ironically — great engineering orgs mint world class engineers like nobody’s business.</p><p>The best engineering orgs are not the ones with the smartest, most experienced people in the world, they’re the ones where normal software engineers can consistently make progress, deliver value to users, and move the business forward, day after day.<img data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" data-attachment-id=\"10019\" data-permalink=\"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/normal-system-does/\" data-orig-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?fit=1024%2C1024&amp;ssl=1\" data-orig-size=\"1024,1024\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"normal-system-does\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?fit=300%2C300&amp;ssl=1\" data-large-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?fit=660%2C660&amp;ssl=1\" src=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?resize=253%2C253&amp;ssl=1\" alt=\"\" width=\"253\" height=\"253\" srcset=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-system-does.png?w=1024&amp;ssl=1 1024w\" sizes=\"auto, (max-width: 253px) 100vw, 253px\"></p><p>Places where engineers can get shit done and have a lot of impact are a magnet for top performers. Nothing makes engineers happier than building things, solving problems, making progress.</p><p>If you’re lucky enough to have world-class engineers in your org, good for you! Your role as a leader is to leverage their brilliance for the good of your customers and your other engineers, without coming to depend on their brilliance. After all, these people don’t belong to you. They may walk out the door at any moment, and that has to be okay.</p><p>These people can be phenomenal assets, assuming they can be team players and keep their egos in check. Which is probably why so many tech companies seem to obsess over identifying and hiring them, especially in Silicon Valley.</p><p>But companies categorically overindex on finding these people after they’ve already been minted, which ends up reinforcing and replicating all the prejudices and inequities of the world at large. Talent may be evenly distributed across populations, but opportunity is not.</p><h2>Don’t hire the “best” people. Hire the right people.</h2><p>We (by which I mean the entire human race) place too much emphasis on individual agency and characteristics, and not enough on the systems that shape us and inform our behaviors.</p><p>I feel like a whole slew of issues (candidates self-selecting out of the interview process, diversity of applicants, etc) would be improved simply by shifting the focus on engineering hiring and interviewing away from this inordinate emphasis on hiring the BEST PEOPLE and realigning around the more reasonable and accurate RIGHT PEOPLE. <img data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" data-attachment-id=\"10023\" data-permalink=\"https://charity.wtf/2025/06/19/in-praise-of-normal-engineers/normal-hire/\" data-orig-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?fit=1024%2C1024&amp;ssl=1\" data-orig-size=\"1024,1024\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"normal-hire\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?fit=300%2C300&amp;ssl=1\" data-large-file=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?fit=660%2C660&amp;ssl=1\" src=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?resize=182%2C182&amp;ssl=1\" alt=\"\" width=\"182\" height=\"182\" srcset=\"https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/charity.wtf/wp-content/uploads/2025/06/normal-hire.png?w=1024&amp;ssl=1 1024w\" sizes=\"auto, (max-width: 182px) 100vw, 182px\"></p><p>It’s a competitive advantage to build an environment where people can be hired for their unique strengths, not their lack of weaknesses; where the emphasis is on composing teams rather than hiring the BEST people; where inclusivity is a given both for ethical reasons and&nbsp;because it raises the bar for performance for everyone. Inclusive culture is what actual meritocracy depends on.</p><p>This is the kind of place that engineering talent (and good humans) are drawn to like a moth to a flame. . It feels  to move the business forward. It feels  to sharpen your skills and improve your craft. It’s the kind of place that people go when they want to become world class engineers. And it’s the kind of place where world class engineers want to stick around, to train up the next generation.</p>","contentLength":17307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44320806"},{"title":"Show HN: EnrichMCP – A Python ORM for Agents","url":"https://github.com/featureform/enrichmcp","date":1750354341,"author":"bloppe","guid":163142,"unread":true,"content":"<p>I've been working with the Featureform team on their new open-source project, [EnrichMCP][1], a Python ORM framework that helps AI agents understand and interact with your data in a structured, semantic way.</p><p>EnrichMCP is built on top of [MCP][2] and acts like an ORM, but for agents instead of humans. You define your data model using SQLAlchemy, APIs, or custom logic, and EnrichMCP turns it into a type-safe, introspectable interface that agents can discover, traverse, and invoke.</p><p>It auto-generates tools from your models, validates all I/O with Pydantic, handles relationships, and supports schema discovery. Agents can go from user → orders → product naturally, just like a developer navigating an ORM.</p><p>We use this internally to let agents query production systems, call APIs, apply business logic, and even integrate ML models. It works out of the box with SQLAlchemy and is easy to extend to any data source.</p>","contentLength":916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44320772"},{"title":"Homegrown Closures for Uxn","url":"https://krzysckh.org/b/Homegrown-closures-for-uxn.html","date":1750354175,"author":"todsacerdoti","guid":163287,"unread":true,"content":"<p>For a week or so now, I've been writing <a href=\"https://github.com/krzysckh/nienor\">niënor</a>, a <em>\"lispy\nenvironment for uxn\"</em>. I did not want it to become a full-blown lisp\nbut just another way of writing <a href=\"https://wiki.xxiivv.com/site/uxntal.html\">uxntal</a>. Uxntal is a\nbit too dense for my liking, and I prefer lisp/scheme s-expression\nsyntax. Niënor is just a compiler and a macroexpander that takes in\nscheme-like code and spits out <a href=\"https://wiki.xxiivv.com/site/uxn.html\">uxn</a> roms.</p><p>This article describes my homegrown method of creating lexically\nscoped closures in this environment.</p><p>When ignoring lexical scope lambdas are really simple to implement if\nwe can already compile named functions.</p><p>Here is some simplified code with commentary from the compiler that\ndoes exactly that (I've skipped some boring parts). We simply give the\nanonymous function a name, skip the compilation for now (add to\nepilogue), and  the name (that will get resolved later\nby the compiler) in its place.</p><div><pre><code></code></pre></div><p>Or, lambdas with extra steps.</p><p>Closures are anonymous functions that capture variables from the\nenvironment. Consider this classic example:</p><div><pre><code></code></pre></div><p>Here, the lambda captures the variable , to later add\nit to . If we would try to simply name the lambda and\ncompile it somewhere else, we wouldn't be able to resolve the symbol\n, as it wouldn't be visible in global scope.</p><p>We could simply  closures. Niënor was meant\nto be quite low-level and closures are, well, quite high-level-ish in my\nopinion. That is a valid method, but not really a . I\nkeep closures close to my heart, so I really wanted to have them in my\nlispy uxn.</p><p>I decided to create objects that tied the required environment to\nfunctions. Well, kind of... Because there are no types on runtime, I\ncan't really detect if I'm trying to call a closure, a function, or\nsomething else.</p><p>This, for example, is perfectly valid way to restart a program:</p><div><pre><code></code></pre></div><p>This means that we  to generate executable code at\nruntime to later jump to - yikes!</p><p>I thought of copying the entire function to somewhere else and\nreplacing the unbound variables with environment-specified values during\nruntime, but that wouldn't work as absolute jumps would always go back\nto the  function, not our copied version. We could\n(probably) find all internal jumps and calculate new positions at\nruntime, but that's expensive (slow) and hard.</p><p>The solution i came up with (while showering) is the following:</p><ol type=\"1\"><li>At compile time, when we know what values from the environment the\nfunction will need, add them as parameters</li></ol><div><pre><code></code></pre></div><p>This makes it so all the variables will always be bound, and we can\ngenerate code for this like for a normal lambda. Of course, we can't\njust return this to the user, because they will expect a single-argument\nfunction with environment attached.</p><ol start=\"2\" type=\"1\"><li>Generate wrapper at runtime</li></ol><p>At runtime, we generate a  - a \"function\" we'll return\nto the user, that adds the environment to the normal function call.</p><p>For example, if we called the upper-mentioned \nwith 32 at runtime, this would happen:</p><div><pre><code></code></pre></div><p>Then if  was to be called with , the\nprogram counter would firstly land on the freshly generated closure\ncode,  would be pushed and \nwould be called with  and  as\n and .</p><p>This is a small example of a gui program that draws hearts in random\nspots.</p><div><pre><code></code></pre></div><p> returns a closure, we'll take a closer look\nat what the generated code for it looks like. This is a decompilation\nspat out by  with some extra commentary on\ntop.</p><div><pre><code></code></pre></div><p>So, the code that gets generated at runtime in this case would\nbe:</p><div><pre><code></code></pre></div><p>This is the entirety of the  that we'll return to the\nuser.</p><p>I've implemented  &amp;  to\nmanually manage the memory left in the RAM after the ROM. They are also\nused to allocate closures - this means that when the user is done with a\nclosure, they can simply  it to return unused memory to\nthe pool.</p><div><pre><code></code></pre></div><p>Because memory used by f1  got freed, f3 got allocated\nin the same spot .</p><p>This has not been battle-tested yet and is (of course) purely\nexperimental. Thank you for reading. If I've sparked your interest, you\ncan download niënor and follow its development <a href=\"https://github.com/krzysckh/nienor\">here</a>.</p>","contentLength":3874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44320747"},{"title":"How OpenElections uses LLMs","url":"https://thescoop.org/archives/2025/06/09/how-openelections-uses-llms/index.html","date":1750349506,"author":"m-hodges","guid":163215,"unread":true,"content":"<p>In the 12-plus years that we’ve been turning official precinct election results into data at <a href=\"https://github.com/openelections\">OpenElections</a>, the single biggest problem has been converting pictures of results into CSV files. Many of the precinct results files we get are image PDFs, and for those there are essentially two options: data entry or Optical Character Recognition. The former has some advantages, but not many. While most people are not great at manual repetitive tasks, you can improve with lots of practice, to the point where the results are very accurate. In the past we did pay for data entry services, and while we developed working relationships with two individuals in particular, the results almost always contained some mistakes and the cost could run into the hundreds of dollars pretty quickly. For a volunteer project, it just didn’t make sense.</p><p>We also used commercial OCR software, most often Able2Extract, which did pretty well, but had a harder time with PDFs that had markings or were otherwise difficult to parse. Thankfully, most election results PDFs are in one of a small handful of formats, which makes things a bit less complicated, but commercial OCR has too many restrictions.</p><p>For parsing image PDFs into CSV files, Google’s Gemini is my model of choice, for two main reasons. First, the results are usually very, very accurate (with a few caveats I’ll detail below), and second, Gemini’s large context window means it’s possible to work with PDF files that can be multiple MBs in size. Here are some examples using image PDFs from Texas counties of how OpenElections uses Gemini for its work.</p><section><p>The Limestone County file containing its 2024 general election results isn’t too bad for an image PDF:</p><div><div><div><figure></figure></div></div></div><p>It has clear black text on a white background without markings. But two big issues make it hard for most OCR software to deal with: the two-column layout, with results from races on the left and the right; and those annoying dots between the end of candidate values and the vote totals. It’s like a delimited layout within a fixed-width layout. If you use OCR software, generally you have to draw the boxes around areas of PDFs like this in order to make the extraction results usable. <a href=\"https://github.com/openelections/openelections-sources-tx/blob/master/2024/general/2024%20Limestone%20County%2C%20TX%20precinct-level%20results.pdf\">This PDF</a> isn’t too large at 42 pages, but that’s still a fair bit of manual labor to get the results, and even then there would be some cleanup required.</p><p>This is where good LLMs should be able to make a difference, because what you want is high-quality OCR results  the ability to provide some domain or business logic to the process without having to do it all yourself. You can see from <a href=\"https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221vZq4hi_eCqR58TkuzqPugDcOc2kE1tms%22%5D,%22action%22:%22open%22,%22userId%22:%22112158284796315028405%22,%22resourceKeys%22:%7B%7D%7D&amp;usp=sharing\">this Google Gemini session</a> that I didn’t have to provide much in the way of instructions after giving an example of the CSV output and some basic office standardization, just “The results are split into two columns on each page; parse the left column first and then the right column.”</p><p>How did Gemini do? Pretty well, almost perfectly. The numbers are accurate, according to some spot checks of candidate totals from <a href=\"https://results.texas-election.com/county\">the Texas Secretary of State website</a>. It did make some formatting mistakes; removing a blank column in some of the Registered Voters and Ballots Cast rows, for example. But that’s a quick fix, and the <a href=\"https://github.com/openelections/openelections-data-tx/blob/master/2024/counties/20241105__tx__general__limestone__precinct.csv\">finished result</a> is exactly what we need. It’s easy to be impressed, but it’s also just 42 pages and had a simple format.</p></section><section><p>The PDF with results from Live Oak County comes in a common format that features a green background. But Live Oak’s image PDF is a black and white scan with different variations of shading, plus we don’t want the four columns containing percentages. For commercial OCR software, this would be a real problem thanks to the layout alone. Indeed, for electronic PDFs that are produced using the same software, we’ve got <a href=\"https://github.com/openelections/openelections-data-tx/blob/master/python-parsers/greenbox.py\">a Python script that converts the PDF to text and parses it into a CSV file</a>. But this one is different:</p><div><div><div><figure></figure></div></div></div><p>The <a href=\"https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221gLcHgzgEkJsJYe8q1FeBjR9vBy55C8gb%22%5D,%22action%22:%22open%22,%22userId%22:%22112158284796315028405%22,%22resourceKeys%22:%7B%7D%7D&amp;usp=sharing\">prompt to convert this 90-page image PDF</a> is like the first one: an example tailored to the first set of results and the unusual placement of the registered voters and ballots cast figures. Gemini repeated the earlier mistake of removing a blank column from the Registered Voters and Ballots Cast rows, but otherwise was spot on in its accuracy. Here’s the <a href=\"https://github.com/openelections/openelections-data-tx/blob/master/2024/counties/20241105__tx__general__live_oak__precinct.csv\">fixed CSV result</a>.</p></section><section><p>One of the areas where LLMs, even Gemini, can struggle with is sustained processes. Converting a few or a few dozen pages is usually pretty simple work for high-performing models, but what about hundreds of pages? <a href=\"https://github.com/openelections/openelections-sources-tx/blob/master/2024/general/2024%20Cameron%20County%2C%20TX%20precinct-level%20results.pdf\">Cameron County’s PDF</a>, all 11.7 MB of it, offers a good challenge, and not just owing to its size:</p><div><div><div><figure></figure></div></div></div><p>Notice how the “Precinct 16” is slightly obscured by an actual punch-hole in this document, and the same is true at the bottom of the image with “Overvotes” and “Undervotes”. Both of those issues could trip up commercial OCR engines. Providing an example of the output, as in the Limestone example, should help fill those literal holes, along with further instructions to ignore the  column entirely. The first attempt at parsing the 653-page PDF eventually “worked” in that it produced a CSV file. But I had to urge Gemini to “continue” multiple times, and it appeared to need more attention starting about halfway through. Most important, the vote figures in the CSV file were close, but not always correct. Back to the drawing board.</p><p>The <a href=\"https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221-Wiq7tYnEC12-TOckMUja1EyUp_lJ_bQ%22%5D,%22action%22:%22open%22,%22userId%22:%22112158284796315028405%22,%22resourceKeys%22:%7B%7D%7D&amp;usp=sharing\">process that generated an accurate CSV file</a> involved splitting the single PDF into multiple parts of about 100 pages each and feeding them one at a time to Gemini. That did mean copying and pasting the output, and one drawback of providing a lot of information in one session was that some of the offices didn’t get quoted properly in the CSV file (to be fair, this probably wouldn’t matter if I were using Gemini’s structured output feature). That meant a little bit of clean-up work, but again, the <a href=\"https://github.com/openelections/openelections-data-tx/blob/master/2024/counties/20241105__tx__general__cameron__precinct.csv\">end result</a> is an accurate precinct results file in about an hour. From a 653-page image PDF, with no data entry.</p><p>Could other models do similar work? Probably so, especially for smaller PDFs. But there are couple of other things that make Gemini the first choice for this: its <a href=\"https://aistudio.google.com\">AI Studio</a> UI allows me to turn the temperature down to 0 (less creativity) and, for models where the “thinking mode” is optional, the ability to disable it if the task at hand is pretty straight-forward. In the six weeks since we started working on Texas precinct results, we’ve been able to convert them for more than half of the state’s 254 counties, including many image PDFs like the ones on display here. That pace simply wouldn’t be possible with data entry or traditional OCR software.</p><p>Speed isn’t the most important factor here, though: accuracy is, and using LLMs still means a system of checks to ensure that the results are what the originals say they are. One step in that is taken care of by a suite of tests that run every time a new or changed CSV gets pushed to one of our data repositories. Those tests look for some formatting issues, duplicate records and basic math inconsistencies. A second step - for now manual - is verifying that multiple totals derived from the precinct CSV match the numbers in an official cumulative report <a href=\"https://www.co.live-oak.tx.us/upload/page/1218/2024/Entered%20By%20Bec/official%20cumulative%20results.pdf\">like this one from Live Oak County</a>. A better version of that could also involve using LLMs to produce both cumulative and precinct-level data, but that would raise the possibility that a model makes similar mistakes in different documents. If you have ideas, head over to <a href=\"https://github.com/openelections\">our GitHub organization</a> and get involved.</p></section>","contentLength":7474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44320001"},{"title":"Curved-Crease Sculpture","url":"https://erikdemaine.org/curved/","date":1750342395,"author":"wonger_","guid":163027,"unread":true,"content":"<div><p>Simons Center for Geometry and Physics Art Gallery, 2012\n</p></div><div><p>Smithsonian American Art Museum permanent collection\n     Renwick Gallery exhibit in 2012\n</p></div>","contentLength":147,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44318874"},{"title":"Show HN: A DOS-like hobby OS written in Rust and x86 assembly","url":"https://github.com/krustowski/rou2exOS","date":1750340337,"author":"krustowski","guid":163028,"unread":true,"content":"<p>To try it out, simply build the project yourself from source, or use attached bootable ISO image of the system (in Releases on Github) and run it in QEMU.</p>","contentLength":154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44318588"},{"title":"End of 10: Upgrade your old Windows 10 computer to Linux","url":"https://endof10.org/","date":1750338872,"author":"doener","guid":162978,"unread":true,"content":"<div><div><p>Support for Windows 10 ends on October 14, 2025.</p><p>Microsoft wants you to buy a new computer.</p><p>But what if you could make your current one fast and secure again?</p></div></div><p>If you bought your computer after 2010, there's most likely no\nreason to throw it out. By just installing an up-to-date Linux\noperating system you can keep using it for years to come.</p><p>Installing an operating system may sound difficult, but you don't have to\ndo it alone. With any luck, there are people in your area\nready to help!</p><ol><li><strong>No New Hardware, No Licensing Costs</strong>A new laptop costs a lot of money, but several Linux operating systems\nare available for free. Software updates are also free, forever. You can\nof course show your support with donations!</li><li>Windows comes with lots of ads and spyware. This slows down your computer,\nlets companies spy on you, and increases your energy bills.</li><li>Production of a computer accounts for 75+% of carbon emissions over its lifecycle.\nKeeping a functioning device longer is a hugely effective way to reduce emissions.\nWith a Linux operating system you can use your device longer.</li><li><strong>Community &amp; Professional Support</strong>There are local repair cafes and independent, professional services and\ncomputer shops available for providing you help. You can find support in online\nforums, too.</li><li>Linux grants you the four freedoms of software. You are free to use, study, share, and\nimprove the program, for as long as you wish. You are in control of your device.</li></ol><p>These organizations have joined us in support of the campaign.</p><p>Then find your closest repair cafe or independent computer shop\nand enjoy your brand-new, old computer!</p>","contentLength":1596,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44318420"},{"title":"What would a Kubernetes 2.0 look like","url":"https://matduggan.com/what-would-a-kubernetes-2-0-look-like/","date":1750334454,"author":"Bogdanp","guid":163090,"unread":true,"content":"<p>Around 2012-2013 I started to hear a  in the sysadmin community about a technology called \"Borg\". It was (apparently) some sort of Linux container system inside of Google that ran all of their stuff. The terminology was a bit baffling, with something called a \"Borglet\" inside of clusters with \"cells\" but the basics started to leak. There was a concept of \"services\" and a concept of \"jobs\", where applications could use services to respond to user requests and then jobs to complete batch jobs that ran for much longer periods of time. </p><p>Then on June 7th, 2014, we got our first commit of Kubernetes. The Greek word for 'helmsman' that absolutely no one could pronounce correctly for the first three years. (Is it koo-ber-NET-ees? koo-ber-NEET-ees? Just give up and call it k8s like the rest of us.) </p><p>Microsoft, RedHat, IBM, Docker join the Kubernetes community pretty quickly after this, which raised Kubernetes from an interesting Google thing to \"maybe this is a real product?\" On July 21st 2015 we got the v1.0 release as well as the creation of the CNCF. </p><p>In the ten years since that initial commit, Kubernetes has become a large part of my professional life. I use it at home, at work, on side projects—anywhere it makes sense. It's a tool with a steep learning curve, but it's also a massive force multiplier. We no longer \"manage infrastructure\" at the server level; everything is declarative, scalable, recoverable and (if you’re lucky) self-healing.</p><p>But the journey hasn't been without problems. Some common trends have emerged, where mistakes or misconfiguration arise from where Kubernetes isn't opinionated enough. Even ten years on, we're still seeing a lot of churn inside of ecosystem and people stepping on well-documented landmines. So, knowing what we know now, what could we do differently to make this great tool even more applicable to more people and problems? </p><p>Let's start with the positive stuff. Why are we still talking about this platform now? </p><p>Containers as a tool for software development make perfect sense. Ditch the confusion of individual laptop configuration and have one standard, disposable concept that works across the entire stack. While tools like Docker Compose allowed for some deployments of containers, they were clunky and still required you as the admin to manage a lot of the steps. I set up a Compose stack with a deployment script that would remove the instance from the load balancer, pull the new containers, make sure they started and then re-added it to the LB, as did lots of folks. </p><p>K8s allowed for this concept to scale out, meaning it was possible to take a container from your laptop and deploy an identical container across thousands of servers. This flexibility allowed organizations to revisit their entire design strategy, dropping monoliths and adopting more flexible (and often more complicated) micro-service designs. </p><p>If you think of the history of Operations as a sort of \"naming timeline from pets to cattle\", we started with what I affectionately call the \"Simpsons\" era. Servers were bare metal boxes set up by teams, they often had one-off names that became slang inside of teams and everything was a snowflake. The longer a server ran, the more cruft it picked up until it became a scary operation to even reboot them, much less attempt to rebuild them. I call it the \"Simpsons\" era because among the jobs I was working at the time, naming them after Simpsons characters was surprisingly common. Nothing fixed itself, everything was a manual operation. </p><p>Then we transition into the \"01 Era\". Tools like Puppet and Ansible have become common place, servers are more disposable and you start to see things like bastion hosts and other access control systems become the norm. Servers aren't all facing the internet, they're behind a load balancer and we've dropped the cute names for stuff like \"app01\" or \"vpn02\". Organizations designed it so they could lose some of their servers some of the time. However failures still weren't self-healing, someone still had to SSH in to see what broke, write up a fix in the tooling and then deploy it across the entire fleet. OS upgrades were still complicated affairs. </p><p>We're now in the \"UUID Era\". Servers exist to run containers, they are entirely disposable concepts. Nobody cares about how long a particular version of the OS is supported for, you just bake a new AMI and replace the entire machine. K8s wasn't the only technology enabling this, but it was the one that accelerated it. Now the idea of a bastion server with SSH keys that I go to the underlying server to fix problems is seen as more of a \"break-glass\" solution. Almost all solutions are \"destroy that Node, let k8s reorganize things as needed, make a new Node\". </p><p>A lot of the Linux skills that were critical to my career are largely nice to have now, not need to have. You can be happy or sad about that, I certainly switch between the two emotions on a regular basis, but it's just the truth. </p><p>The k8s jobs system isn't perfect, but it's so much better than the \"snowflake cron01 box\" that was an extremely common sight at jobs for years. Running on a cron schedule or running from a message queue, it was now possible to reliably put jobs into a queue, have them get run, have them restart if they didn't work and then move on with your life. </p><p>Not only does this free up humans from a time-consuming and boring task, but it's also simply a more efficient use of resources. You are still spinning up a pod for every item in the queue, but your teams have a lot of flexibility inside of the \"pod\" concept for what they need to run and how they want to run it. This has really been a quality of life improvement for a lot of people, myself included, who just need to be able to easily background tasks and not think about them again. </p><p><strong>Service Discoverability and Load Balancing</strong></p><p>Hard-coded IP addresses that lived inside of applications as the template for where requests should be routed has been a curse following me around for years. If you were lucky, these dependencies weren't based on IP address but were actually DNS entries and you could change the thing behind the DNS entry without coordinating a deployment of a million applications. </p><p>K8s allowed for simple DNS names to call other services. It removed an entire category of errors and hassle and simplified the entire thing down. With the Service API you had a stable, long lived IP and hostname that you could just point things towards and not think about any of the underlying concepts. You even have concepts like ExternalName that allow you to treat external services like they're in the cluster. </p><h2>What would I put in a Kubernetes 2.0?</h2><p>YAML was appealing because it wasn't JSON or XML, which is like saying your new car is great because it's neither a horse nor a unicycle. It demos nicer for k8s, looks nicer sitting in a repo and has the  of being a simple file format. In reality. YAML is just too much for what we're trying to do with k8s and it's not a safe enough format. Indentation is error-prone, the files don't scale great (you really don't want a super long YAML file), debugging can be annoying. YAML has  subtle behaviors outlined in its spec.</p><p>I still remember not believing what I was seeing the first time I saw the Norway Problem. For those lucky enough to not deal with it, the Norway Problem in YAML is when 'NO' gets interpreted as false. Imagine explaining to your Norwegian colleagues that their entire country evaluates to false in your configuration files. Add in accidental numbers from lack of quotes, the list goes on and on. There are much better posts on why YAML is crazy than I'm capable of writing: <a href=\"https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell\">https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell</a></p><p>HCL is already the format for Terraform, so at least we'd only have to hate one configuration language instead of two. It's strongly typed with explicit types. There's already good validation mechanisms. It is specifically designed to do the job that we are asking YAML to do and it's not much harder to read. It has built-in functions people are already using that would allow us to remove some of the third-party tooling from the YAML workflow. </p><p>I would wager 30% of Kubernetes clusters today are  being managed with HCL via Terraform. We don't need the Terraform part to get a lot of the benefits of a superior configuration language. </p><p>The only downsides are that HCL is slightly more verbose than YAML, and its Mozilla Public License 2.0 (MPL-2.0) would require careful legal review for integration into an Apache 2.0 project like Kubernetes. However, for the quality-of-life improvements it offers, these are hurdles worth clearing.</p><p>Let's take a simple YAML file. </p><pre><code># YAML doesn't enforce types\nreplicas: \"3\"  # String instead of integer\nresources:\n  limits:\n    memory: 512  # Missing unit suffix\n  requests:\n    cpu: 0.5m    # Typo in CPU unit (should be 500m)</code></pre><p>Even in the most basic example, there are footguns everywhere. HCL and the type system would catch all of these problems. </p><pre><code>replicas = 3  # Explicitly an integer\n\nresources {\n  limits {\n    memory = \"512Mi\"  # String for memory values\n  }\n  requests {\n    cpu = 0.5  # Number for CPU values\n  }\n}</code></pre><p>Take a YAML file like this that you probably have 6000 in your k8s repo. Now look at HCL without needing external tooling. </p><pre><code># Need external tools or templating for dynamic values\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  # Can't easily generate or transform values\n  DATABASE_URL: \"postgres://user:password@db:5432/mydb\"\n  API_KEY: \"static-key-value\"\n  TIMESTAMP: \"2023-06-18T00:00:00Z\"  # Hard-coded timestamp</code></pre><pre><code>resource \"kubernetes_config_map\" \"app_config\" {\n  metadata {\n    name = \"app-config\"\n  }\n  \n  data = {\n    DATABASE_URL = \"postgres://${var.db_user}:${var.db_password}@${var.db_host}:${var.db_port}/${var.db_name}\"\n    API_KEY      = var.api_key != \"\" ? var.api_key : random_string.api_key.result\n    TIMESTAMP    = timestamp()\n  }\n}\n\nresource \"random_string\" \"api_key\" {\n  length  = 32\n  special = false\n}</code></pre><p>Here's all the pros you get with this move. </p><ol><li>: Preventing type-related errors before deployment</li><li>: Reducing duplication and improving maintainability</li><li><strong>Functions and Expressions</strong>: Enabling dynamic configuration generation</li><li>: Supporting environment-specific configurations</li><li>: Simplifying repetitive configurations</li><li>: Improving documentation and readability</li><li>: Making errors easier to identify and fix</li><li>: Enabling reuse of configuration components</li><li>: Preventing invalid configurations</li><li>: Supporting complex data manipulations</li></ol><p>I know, I'm the 10,000 person to write this. Etcd has done a fine job, but it's a little crazy that it is the only tool for the job. For smaller clusters or smaller hardware configuration, it's a large use of resources in a cluster type where you will never hit the node count where it pays off. It's also a strange relationship between k8s and etcd now, where k8s is basically the only etcd customer left. </p><p>What I'm suggesting is taking the work of <a href=\"https://github.com/k3s-io/kine\" rel=\"noreferrer\">kine</a> and making it official. It makes sense for the long-term health of the project to have the ability to plug in more backends, adding this abstraction means it (should) be easier to swap in new/different backends in the future and it also allows for more specific tuning depending on the hardware I'm putting out there. </p><p>What I suspect this would end up looking like is much like this: <a href=\"https://github.com/canonical/k8s-dqlite\">https://github.com/canonical/k8s-dqlite</a>. Distributed SQlite in-memory with Raft consensus and almost zero upgrade work required that would allow cluster operators to have more flexibility with the persistence layer of their k8s installations. If you have a conventional server setup in a datacenter and etcd resource usage is not a problem, great! But this allows for lower-end k8s to be a nicer experience and (hopefully) reduces dependence on the etcd project. </p><h3>Beyond Helm: A Native Package Manager</h3><p>Helm is a perfect example of a temporary hack that has grown to be a permanent dependency. I'm grateful to the maintainers of Helm for all of their hard work, growing what was originally a hackathon project into the de-facto way to install software into k8s clusters. It has done as good a job as something could in fulfilling that role without having a deeper integration into k8s. </p><p>All that said, Helm is a nightmare to use. The Go templates are tricky to debug, often containing complex logic that results in really confusing error scenarios. The error messages you get from those scenarios are often gibberish. Helm isn't a very good package system because it fails at some of the basic tasks you need a package system to do, which are transitive dependencies and resolving conflicts between dependencies. </p><p>Tell me what this conditional logic is trying to do:</p><pre><code># A real-world example of complex conditional logic in Helm\n{{- if or (and .Values.rbac.create .Values.serviceAccount.create) (and .Values.rbac.create (not .Values.serviceAccount.create) .Values.serviceAccount.name) }}\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: {{ template \"myapp.fullname\" . }}\n  labels:\n    {{- include \"myapp.labels\" . | nindent 4 }}\n{{- end }}</code></pre><p>Or if I provide multiple values files to my chart, which one wins:</p><pre><code>helm install myapp ./mychart -f values-dev.yaml -f values-override.yaml --set service.type=NodePort</code></pre><p>Ok, what if I want to manage my application and all the application dependencies with a Helm chart. This makes sense, I have an application that itself has dependencies on other stuff so I want to put them all together. So I define my sub-charts or umbrella charts inside of my Chart.yaml. </p><pre><code>dependencies:\n- name: nginx\n  version: \"1.2.3\"\n  repository: \"&lt;https://example.com/charts&gt;\"\n- name: memcached\n  version: \"1.2.3\"\n  repository: \"&lt;https://another.example.com/charts&gt;\"\n</code></pre><p>But assuming I have multiple applications, it's entirely possible that I have 2 services both with a dependency on nginx or whatever like this:</p><p>Helm doesn't handle this situation gracefully because template names are global with their templates loaded alphabetically. Basically you need to:</p><ul><li>Don't declare a dependency on the same chart more than once (hard to do for a lot of microservices)</li><li>If you do have the same chart declared multiple times, has to use the exact same version</li></ul><p>The list of issues goes on and on. </p><ul><li>Cross-Namespace installation stinks</li><li>Chart verification process is a pain and nobody uses it</li></ul><p>Let's just go to the front page of artifacthub:</p><p>I'll grab elasticsearch cause that seems important. </p><p>Seems  for the Official Elastic helm chart. Certainly  will be right, it's an absolute critical dependency for the entire industry. </p><p>Nope. Also how is the maintainer of the chart \"Kubernetes\" and it's  not marked as a . Like Christ how much more verified does it get.</p><ul><li>No metadata in chart searching. You can only search by name and description, not by features, capabilities, or other metadata.</li></ul><ul><li>Helm doesn't strictly enforce semantic versioning</li></ul><pre><code># Chart.yaml with non-semantic version\napiVersion: v2\nname: myapp\nversion: \"v1.2-alpha\" </code></pre><ul><li>If you uninstall and reinstall a chart with CRDs, it might delete resources created by those CRDs. This one has screwed me  and is crazy unsafe. </li></ul><p>I could keep writing for another 5000 words and still wouldn't have outlined all the problems. There isn't a way to make Helm good enough for the task of \"package manager for all the critical infrastructure on the planet\". </p><h4>What would a k8s package system look like?</h4><p>Let's call our hypothetical package system KubePkg, because if there's one thing the Kubernetes ecosystem needs, it's another abbreviated name with a 'K' in it. We would try to copy as much of the existing work inside the Linux ecosystem while taking advantage of the CRD power of k8s. My idea looks something like this:</p><p>The packages are bundles like a Linux package:</p><p>There's a definition file that accounts for as many of the real scenarios that you actually encounter when installing a thing. </p><pre><code>apiVersion: kubepkg.io/v1\nkind: Package\nmetadata:\n  name: postgresql\n  version: 14.5.2\nspec:\n  maintainer:\n    name: \"PostgreSQL Team\"\n    email: \"<a href=\"https://matduggan.com/cdn-cgi/l/email-protection\" data-cfemail=\"09646860677d6860676c7b7a4979667a7d6e7b6c7a7865276c71686479656c276a6664\">[email&nbsp;protected]</a>\"\n  description: \"PostgreSQL database server\"\n  website: \"https://postgresql.org\"\n  license: \"PostgreSQL\"\n  \n  # Dependencies with semantic versioning\n  dependencies:\n    - name: storage-provisioner\n      versionConstraint: \"&gt;=1.0.0\"\n    - name: metrics-collector\n      versionConstraint: \"^2.0.0\"\n      optional: true\n  \n  # Security context and requirements\n  security:\n    requiredCapabilities: [\"CHOWN\", \"SETGID\", \"SETUID\"]\n    securityContextConstraints:\n      runAsUser: 999\n      fsGroup: 999\n    networkPolicies:\n      - ports:\n        - port: 5432\n          protocol: TCP\n    \n  # Resources to be created (embedded or referenced)\n  resources:\n    - apiVersion: v1\n      kind: Service\n      metadata:\n        name: postgresql\n      spec:\n        ports:\n        - port: 5432\n    - apiVersion: apps/v1\n      kind: StatefulSet\n      metadata:\n        name: postgresql\n      spec:\n        # StatefulSet definition\n  \n  # Configuration schema using JSON Schema\n  configurationSchema:\n    type: object\n    properties:\n      replicas:\n        type: integer\n        minimum: 1\n        default: 1\n      persistence:\n        type: object\n        properties:\n          size:\n            type: string\n            pattern: \"^[0-9]+[GMK]i$\"\n            default: \"10Gi\"\n  \n  # Lifecycle hooks with proper sequencing\n  hooks:\n    preInstall:\n      - name: database-prerequisites\n        job:\n          spec:\n            template:\n              spec:\n                containers:\n                - name: init\n                  image: postgres:14.5\n    postInstall:\n      - name: database-init\n        job:\n          spec:\n            # Job definition\n    preUpgrade:\n      - name: backup\n        job:\n          spec:\n            # Backup job definition\n    postUpgrade:\n      - name: verify\n        job:\n          spec:\n            # Verification job definition\n    preRemove:\n      - name: final-backup\n        job:\n          spec:\n            # Final backup job definition\n  \n  # State management for stateful applications\n  stateManagement:\n    backupStrategy:\n      type: \"snapshot\"  # or \"dump\"\n      schedule: \"0 2 * * *\"  # Daily at 2 AM\n      retention:\n        count: 7\n    recoveryStrategy:\n      type: \"pointInTime\"\n      verificationJob:\n        spec:\n          # Job to verify recovery success\n    dataLocations:\n      - path: \"/var/lib/postgresql/data\"\n        volumeMount: \"data\"\n    upgradeStrategies:\n      - fromVersion: \"*\"\n        toVersion: \"*\"\n        strategy: \"backup-restore\"\n      - fromVersion: \"14.*.*\"\n        toVersion: \"14.*.*\"\n        strategy: \"in-place\"</code></pre><p>There's a real signing process that would be required and allow you more control over the process. </p><pre><code>apiVersion: kubepkg.io/v1\nkind: Repository\nmetadata:\n  name: official-repo\nspec:\n  url: \"https://repo.kubepkg.io/official\"\n  type: \"OCI\"  # or \"HTTP\"\n  \n  # Verification settings\n  verification:\n    publicKeys:\n      - name: \"KubePkg Official\"\n        keyData: |\n          -----BEGIN PUBLIC KEY-----\n          MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvF4+...\n          -----END PUBLIC KEY-----\n    trustPolicy:\n      type: \"AllowList\"  # or \"KeyRing\"\n      allowedSigners:\n        - \"KubePkg Official\"\n        - \"Trusted Partner\"\n    verificationLevel: \"Strict\"  # or \"Warn\", \"None\"</code></pre><p>Like how great would it be to have something where I could automatically update packages without needing to do anything on my side. </p><pre><code>apiVersion: kubepkg.io/v1\nkind: Installation\nmetadata:\n  name: postgresql-main\n  namespace: database\nspec:\n  packageRef:\n    name: postgresql\n    version: \"14.5.2\"\n  \n  # Configuration values (validated against schema)\n  configuration:\n    replicas: 3\n    persistence:\n      size: \"100Gi\"\n    resources:\n      limits:\n        memory: \"4Gi\"\n        cpu: \"2\"\n  \n  # Update policy\n  updatePolicy:\n    automatic: false\n    allowedVersions: \"14.x.x\"\n    schedule: \"0 2 * * 0\"  # Weekly on Sunday at 2am\n    approvalRequired: true\n  \n  # State management reference\n  stateRef:\n    name: postgresql-main-state\n    \n  # Service account to use\n  serviceAccountName: postgresql-installer</code></pre><p>What k8s needs is a system that meets the following requirements:</p><ol><li>: Everything is a Kubernetes resource with proper status and events</li><li><strong>First-Class State Management</strong>: Built-in support for stateful applications</li><li>: Robust signing, verification, and security scanning</li><li><strong>Declarative Configuration</strong>: No templates, just structured configuration with schemas</li><li>: Comprehensive lifecycle hooks and upgrade strategies</li><li>: Linux-like dependency management with semantic versioning</li><li>: Complete history of changes with who, what, and when, not what Helm currently provides. </li><li>: Support for organizational policies and compliance. </li><li><strong>Simplified User Experience</strong>: Familiar Linux-like package management commands. It seems wild that we're trying to go a different direction from the package systems that have worked for decades. </li></ol><p>Try to imagine, across the entire globe, how much time and energy has been invested in trying to solve any one of the following three problems. </p><ol><li>I need this pod in this cluster to talk to that pod in that cluster. </li><li>There is a problem happening somewhere in the NAT traversal process and I need to solve it</li><li>I have run out of IP addresses with my cluster because I didn't account for how many you use. Remember: A company starting with a /20 subnet (4,096 addresses), deploys 40 nodes with 30 pods each, and suddenly realizes they're approaching their IP limit. Not that many nodes!</li></ol><p>I am not suggesting the entire internet switches over to IPv6 and right now k8s happily supports IPv6-only if you want and a dualstack approach. But I'm saying now is the time to flip the default and just go IPv6. You eliminate a huge collection of problems all at once. </p><ul><li>Flatter, less complicated network topology inside of the cluster. </li><li>The distinction between multiple clusters becomes a thing organizations can choose to ignore if they want if they want to get public IPs.</li><li>Easier to understand exactly the flow of traffic inside of your stack. </li></ul><p>It has nothing to do with driving IPv6 adoption across the entire globe and just an acknowledgement that we no longer live in a world where you have to accept the weird limitations of IPv4 in a universe where you may need 10,000 IPs suddenly with very little warning. </p><p>The benefits for organizations with public IPv6 addresses is pretty obvious, but there's enough value there for cloud providers and users that even the corporate overlords might get behind it. AWS never needs to try and scrounge up more private IPv4 space inside of a VPC. That's gotta be worth something. </p><p>The common rebuttal to these ideas is, \"Kubernetes is an open platform, so the community can build these solutions.\" While true, this argument misses a crucial point: <strong>defaults are the most powerful force in technology.</strong> The \"happy path\" defined by the core project dictates how 90% of users will interact with it. If the system defaults to expecting signed packages and provides a robust, native way to manage them, that is what the ecosystem will adopt.</p><p>This is an ambitious list, I know. But if we're going to dream, let's dream big. After all, we're the industry that thought naming a technology 'Kubernetes' would catch on, and somehow it did!</p><p>We see this all the time in other areas like mobile developer and web development, where platforms assess their situation and make  jumps forward. Not all of these are necessarily projects that the maintainers or companies  take on but I think they're all ideas that  should at least revisit and think \"is it worth doing now that we're this nontrivial percentage of all datacenter operations on the planet\"? </p>","contentLength":23667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44317825"},{"title":"Guess I'm a rationalist now","url":"https://scottaaronson.blog/?p=8908","date":1750328526,"author":"nsoonhui","guid":162943,"unread":true,"content":"<p>A week ago I attended <a href=\"https://less.online/\">LessOnline</a>, a rationalist blogging conference featuring many people I’ve known for years—Scott Alexander, Eliezer Yudkowsky, Zvi Mowshowitz, Sarah Constantin, Carl Feynman—as well as people I’ve known only online and was delighted to meet in person, like Joe Carlsmith and Jacob Falkovich and Daniel Reeves.  The conference was at <a href=\"https://www.lighthaven.space/\">Lighthaven</a>, a bewildering maze of passageways, meeting-rooms, sleeping quarters, gardens, and vines off Telegraph Avenue in Berkeley, which has recently emerged as the nerd Shangri-La, or Galt’s Gulch, or Shire, or whatever.  I did two events at this year’s LessOnline: a conversation with Nate Soares about the <a href=\"https://www.lesswrong.com/w/orthogonality-thesis\">Orthogonality Thesis</a>, and an ask-me-anything session about quantum computing and theoretical computer science (no new ground there for regular consumers of my content).</p><p>What I’ll remember most from LessOnline is not the sessions, mine or others’, but the unending conversation among hundreds of people all over the grounds, which took place in parallel with the sessions and before and after them, from morning till night (and through the night, apparently, though I’ve gotten too old for that).  It felt like a single conversational archipelago, the largest in which I’ve ever taken part, and the conference’s real point.  (Attendees were exhorted, in the opening session, to skip as many sessions as possible in favor of intense small-group conversations—not only because it was better but also because the session rooms were too small.)</p><p>Within the conversational blob, just making my way from one building to another could take hours.  My mean free path was approximately five feet, before someone would notice my nametag and stop me with a question.  Here was my favorite opener:</p><p>“You’re Scott Aaronson?!  The quantum physicist who’s always getting into arguments on the Internet, and who’s essentially always right, but who sustains an unreasonable amount of psychic damage in the process?”</p><p>“Yes,” I replied, not bothering to correct the “physicist” part.</p><p>One night, I walked up to Scott Alexander, who sitting on the ground, with his large bald head and a blanket he was using as a robe, resembled a monk.  “Are you enjoying yourself?” he asked.</p><p>I replied, “you know, after all these years of being coy about it, I think I’m finally ready to become a Rationalist.  Is there, like, an initiation ritual or something?”</p><p>Scott said, “Oh, you were already initiated a decade ago; you just didn’t realize it at the time.”  Then he corrected himself: “two decades ago.”</p><p>The first thing I did, after coming out as a Rationalist, was to get into a heated argument with Other Scott A., Joe  Carlsmith, and other fellow-Rationalists about the ideas I set out twelve years ago in my <a href=\"https://www.scottaaronson.com/papers/giqtm3.pdf\">Ghost in the Quantum Turing Machine</a> essay.  Briefly, my argument was that the irreversibility and ephemerality of biological life, which contrasts with the copyability, rewindability, etc. of programs running on digital computers, and which can ultimately be traced back to microscopic details of the universe’s initial state, subject to the No-Cloning Theorem of quantum mechanics, which then get chaotically amplified during brain activity … might be a clue to a deeper layer of the world, one that we understand about as well as the ancient Greeks understood Newtonian physics, but which is the layer where mysteries like free will and consciousness will ultimately need to be addressed.</p><p>I got into this argument partly because it came up, but partly also because this seemed like the biggest conflict between my beliefs and the consensus of my fellow Rationalists.  Maybe part of me wanted to demonstrate that my intellectual independence remained intact—sort of like a newspaper that gets bought out by a tycoon, and then immediately runs an investigation into the tycoon’s corruption, as well as his diaper fetish, just to prove it can.</p><p>The funny thing, though, is that all my beliefs are the same as they were before.  I’m still a computer scientist, an academic, a straight-ticket Democratic voter, a liberal Zionist, a Jew, etc. (all identities, incidentally, well-enough represented at LessOnline that I don’t even think I was the unique attendee in the intersection of them all).</p><p>Given how much I resonate with what the Rationalists are trying to do, why did it take me so long to identify as one?</p><p>Firstly, while 15 years ago I shared the Rationalists’ interests, sensibility, and outlook, and their stances on most issues, I also found them bizarrely, inexplicably obsessed with the question of whether AI would soon become superhumanly powerful and change the basic conditions of life on earth, and with how to make the AI transition go well.  Why , as opposed to all the other sci-fi scenarios one could worry about, not to mention all the nearer-term risks to humanity?</p><p>Suffice it to say that empirical developments have since caused me to withdraw my objection.  Sometimes weird people are weird merely because they see the future sooner than others.  Indeed, it seems to me that the biggest thing the Rationalists got wrong about AI was to  how soon the revolution would happen, and to overestimate how many new ideas would be needed for it (mostly, as we now know, it just took lots more compute and training data).  Now that I, too, spend some of my time working on AI alignment, I was able to use LessOnline in part for research meetings with colleagues.</p><p>A second reason I didn’t identify with the Rationalists was cultural: they were, and are, centrally a bunch of twentysomethings who “work” at an ever-changing list of Berkeley- and San-Francisco-based “orgs” of their own invention, and who live in group houses where they explore their exotic sexualities, gender identities, and fetishes, sometimes with the aid of psychedelics.  I, by contrast, am a straight, monogamous, middle-aged tenured professor, married to another such professor and raising two kids who go to normal schools.  Hanging out with the Rationalists always makes me feel older and younger at the same time.</p><p>So what changed?  For one thing, with the march of time, a significant fraction of Rationalists now have marriages, children, or both—indeed, a highlight of LessOnline was the many adorable toddlers running around the Lighthaven campus.  Rationalists are successfully reproducing!  Some because of explicit pronatalist ideology, or because they were persuaded by Bryan Caplan’s arguments in .  But others simply because of the same impulses that led their ancestors to do the same for eons.  And perhaps because, like the Mormons or Amish or Orthodox Jews, but unlike typical secular urbanites, the Rationalists in something.  For all their fears around AI, they don’t  doomy, but buzz with ideas about how to build a better world for the next generation.</p><p>At a LessOnline parenting session, hosted by Julia Wise, I was surrounded by parents who worry about the same things I do: how do we raise our kids to be independent and agentic yet socialized and reasonably well-behaved, technologically savvy yet not droolingly addicted to iPad games?  What schooling options will let them accelerate in math, save them from the crushing monotony that we experienced?  How much of our own lives should we sacrifice on the altar of our kids’ “enrichment,” versus trusting Judith Rich Harris that such efforts quickly hit a point of diminishing returns?</p><p>A third reason I didn’t identify with the Rationalists was, frankly, that they gave off some (not all) of the vibes of a cult, with Eliezer as guru.  Eliezer writes in parables and koans.  He teaches that the fate of life on earth hangs in the balance, that the select few who understand the stakes have the terrible burden of steering the future.  Taking what Rationalists call the “outside view,” <em>how good is the track record for this sort of thing?</em></p><p>OK, but what did I actually see at Lighthaven?  I saw something that seemed to resemble a cult only insofar as the Beatniks, the Bloomsbury Group, the early Royal Society, or any other community that believed in something did.  When Eliezer himself—the bearded, cap-wearing Moses who led the nerds from bondage to their Promised Land in Berkeley—showed up, he was argued with like anyone else.  Eliezer has in any case largely passed his staff to a new generation: Nate Soares and Zvi Mowshowitz have found new and, in various ways, better ways of talking about AI risk; Scott Alexander has for the last decade written the blog that’s the community’s intellectual center; figures from Kelsey Piper to Jacob Falkovich to Aella have taken Rationalism in new directions, from mainstream political engagement to the … err … <a href=\"https://aella.substack.com/p/my-birthday-gangbang\">statistical analysis of orgies</a>.</p><p>I’ll say this, though, on the naysayers’ side: it’s  hard to make dancing to AI-generated pop songs about Bayes’ theorem and Tarski’s definition of truth not feel cringe, as I can now attest from experience.</p><p>The cult thing brings me to the deepest reason I hesitated for so long to identify as a Rationalist: namely, I was scared that if I did, people whose approval I craved (including my academic colleagues, but also just randos on the Internet) would sneer at me.  For years, I searched of some way of explaining this community’s appeal so reasonable that it would silence the sneers.</p><p>It took years of psychological struggle, and (frankly) solidifying my own place in the world, to follow the true path, which of course is not to give a shit what some haters think of my life choices.  Consider: five years ago, it felt obvious to me that the entire Rationalist community might be about to implode, under existential threat from Cade Metz’s  article, as well as RationalWiki and SneerClub and all the others laughing at the Rationalists and accusing them of every evil.  Yet last week at LessOnline, I saw a community that’s never been thriving more, with a beautiful real-world campus, excellent writers on every topic who felt like this was the place to be, and even a crop of kids.  How many of the sneerers are living such fulfilled lives?  To judge from their own angry, depressed self-disclosures, probably not many.</p><p>But are the sneerers right that, even if the Rationalists are enjoying their own lives, they’re making other people’s lives miserable?  Are they closet far-right monarchists, like Curtis Yarvin?  I liked how  put it in its recent, long and (to my mind) <a href=\"https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile\">devastating profile of Yarvin</a>:</p><blockquote><p>The most generous engagement with Yarvin’s ideas has come from bloggers associated with the rationalist movement, which prides itself on weighing evidence for even seemingly far-fetched claims. Their formidable patience, however, has also worn thin. “He never addressed me as an equal, only as a brainwashed person,” Scott Aaronson, an eminent computer scientist, said of their conversations. “He seemed to think that if he just gave me one more reading assignment about happy slaves singing or one more monologue about F.D.R., I’d finally see the light.”</p></blockquote><p>The closest to right-wing politics that I witnessed at LessOnline was a session, with <a href=\"https://www.vox.com/authors/kelsey-piper\">Kelsey Piper</a> and current and former congressional staffers, about the prospects for moderate Democrats to articulate a moderate, pro-abundance agenda that would resonate with the public and finally defeat MAGA.</p><p>But surely the Rationalists are incels, bitter that they can’t get laid?  Again, the closest I saw was a session where Jacob Falkovich helped a standing-room-only crowd of mostly male nerds confront their fears around dating and understand women better, with Rationalist women eagerly volunteering to answer questions about their perspective.  Gross, right?  (Also, for those already in relationships, Eliezer’s primary consort and former couples therapist <a href=\"https://www.grettaduleba.com/\">Gretta Duleba</a> did a session on relationship conflict.)</p><p>So, yes, when it comes to the Rationalists, I’m going to believe my own lying eyes over the charges of the sneerers.  The sneerers can even say about me, in their favorite formulation, that I’ve “gone mask off,” confirmed the horrible things they’ve always suspected.  Yes, the mask is off—and beneath the mask is the same person I always was, who has an inordinate fondness for the <a href=\"https://en.wikipedia.org/wiki/Busy_beaver\">Busy Beaver function</a> and the complexity class <a href=\"https://arxiv.org/abs/1004.0377\">BQP/qpoly</a>, and who uses too many filler words and moves his hands too much, and who strongly supports the Enlightenment, and who once feared that his best shot at happiness in life would be to earn women’s pity rather than their contempt.  Incorrectly, as I’m glad to report.  From my nebbishy nadir to the present, a central thing that’s changed is that, from my family to my academic colleagues to the Rationalist community to my blog readers, I finally found some people who want what I have to sell.</p><p>My replies to comments on this post might be light, as I’ll be accompanying my daughter on a school trip to the Galapagos Islands!</p><p>A few weeks ago, I was “ambushed” into leading a session on philosophy and theoretical computer science at UT Austin.  (I.e., asked to show up for the session, but thought I’d just be a participant rather than the main event.)  The session was then <a href=\"https://www.youtube.com/watch?v=OST1DjD08Hg\">recorded and placed on YouTube</a>—and surprisingly, given the circumstances, some people seemed to like it!</p><p>Friend-of-the-blog <a href=\"https://www.alonrosen.net/\">Alon Rosen</a> has asked me to announce a <a href=\"https://cs.unibocconi.eu/call-nominations-trevisan-prize-2025\">call for nominations</a> for a new theoretical computer science prize, in memory of my former professor (and fellow TCS blogger) <a href=\"https://scottaaronson.blog/?p=8057\">Luca Trevisan</a>, who was lost to the world too soon.</p><p>And one more: Mahdi Cheraghchi has asked me to announce the STOC’2025 online poster session, registration deadline June 12; <a href=\"https://acm-stoc.org/stoc2025/call-for-posters.html\">see here for more</a>.  Incidentally, I’ll be at STOC in Prague to give a plenary on quantum algorithms; I look forward to meeting any readers who are there!</p>","contentLength":13849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44317180"},{"title":"Show HN: Claude Code Usage Monitor – real-time tracker to dodge usage cut-offs","url":"https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor","date":1750326403,"author":"Maciej-roboblog","guid":162857,"unread":true,"content":"<p>I kept slamming into Claude Code limits mid-session and couldn’t find a quick way to see how close I was getting, so I hacked together a tiny local tracker.</p><p>Streams your prompt + completion usage in real time</p><p>Predicts whether you’ll hit the cap before the session ends</p><p>Runs 100 % locally (no auth, no server)</p><p>Presets for Pro, Max × 5, Max × 20 — tweak a JSON if your plan’s different</p><p>It’s already spared me a few “why did my run just stop?” moments, but it’s still rough around the edges. Feedback, bug reports, and PRs welcome!</p>","contentLength":541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44317012"},{"title":"Base44 sells to Wix for $80M cash","url":"https://techcrunch.com/2025/06/18/6-month-old-solo-owned-vibe-coder-base44-sells-to-wix-for-80m-cash/","date":1750325474,"author":"myth_drannon","guid":162886,"unread":true,"content":"<p>There’s a lot of talk in the startup world about how AI makes individuals so productive that it could give rise to a generation of <a href=\"https://www.amazon.com/Single-Handed-Unicorn-Billion-Dollar-Solopreneurs-Business/dp/B0DWSRS62N\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">“solo unicorns”</a> — one-person companies worth over $1 billion.</p><p>While an actual solo unicorn remains a mythical creature, Israeli developer Maor Shlomo provided compelling evidence Wednesday that the concept might not be impossible.&nbsp;</p><p>Shlomo sold his 6-month-old, bootstrapped vibe-coding startup Base44 to Wix for $80 million, Wix <a href=\"https://www.globenewswire.com/news-release/2025/06/18/3101508/0/en/Wix-Further-Expands-into-Vibe-Coding-with-Acquisition-of-Base44-a-Hyper-Growth-Startup-that-Simplifies-Web-and-App-Creation-with-AI.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">announced</a> Wednesday. And the deal was cash, Wix confirmed to TechCrunch.&nbsp;</p><p>Admittedly, this wasn’t a billion dollars or close to it. And Shlomo wasn’t truly solo — he had eight employees, Wix confirmed. They will collectively receive $25 million of the $80 million as a “retention” bonus. Wix declined to give details on that part of the deal, like how long they have to stay in their jobs to get full payouts.</p><p>In its six months as a stand-alone company, Base44 reportedly grew to 250,000 users, hitting 10,000 users within its first three weeks. According to <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7336025796509077504/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Shlomo’s posts</a> on X and LinkedIn, the company was profitable, generating $189,000 in profit in May even after covering high LLM token costs, which he also documented publicly.</p><p>Base44 spread mostly through word of mouth as Shlomo, a 31-year-old programmer, shared his building journey on LinkedIn and Twitter. The project began as a side venture, he told <a href=\"https://www.calcalistech.com/ctechnews/article/s1iflnlelx\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Israeli tech news site CTech</a>.&nbsp;&nbsp;</p><p>“Base44 is a moonshot experiment — helping everyone, technical or not, build software without coding at all,” he <a rel=\"nofollow\" href=\"https://www.linkedin.com/posts/maor-shlomo-1088b4144_excited-to-share-a-project-ive-been-working-activity-7297302969652277252-vcj8?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAEjzLUBg333G2D9H7FJaye0FUkOP4V2yos\">explained on LinkedIn</a> when he launched it to the public.</p><p>It’s one of the newer crop of vibe-coding products designed for non-programmers. Users enter text prompts, and the platform builds complete applications, with database, storage, authentication, analytics, and integration. It also supports email, texting, and maps, with a roadmap for more enterprise-grade security support.</p><p>Base44 isn’t unique in this area. Other vibe coders like <a href=\"https://techcrunch.com/2025/04/22/adaptive-computer-wants-to-reinvent-the-pc-with-vibe-coding-for-non-programmers/\">Adaptive Computer</a> handle similar infrastructure work. But Base44’s fast rise was astounding all the same.</p><p>Shlomo was already known in the Israeli startup community through his previous startup, the Insight Partners-backed data analytics company <a href=\"https://techcrunch.com/2021/05/18/explorium-scores-75m-series-c-just-10-months-after-b-round/\">Explorium</a>. His brother is also a co-founder of an <a href=\"https://techcrunch.com/2025/01/27/hackers-are-targeting-machine-identities-token-security-just-raised-20m-to-stop-them/\">AI security startup, Token Security,</a> which just raised $20 million led by Notable Capital (formerly GGV Capital) and a bunch of Israeli tech angels.</p><p>He quickly gained partnership agreements  for Base44 with big Israeli tech companies like eToro and Similarweb.</p><p>After posting about his decision to use Anthropic’s Claude LLM through AWS instead of models by OpenAI — mostly for cost-per-performance reasons — Amazon invited Base44 to demo at a Tel Aviv AWS event last month, which <a href=\"https://www.linkedin.com/posts/maor-shlomo-1088b4144_it-was-an-honor-to-present-base44-on-the-activity-7333519004138942464-EVoB/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAEjzLUBg333G2D9H7FJaye0FUkOP4V2yos\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Shlomo documented.</a></p><p>“Crazy f***ing journey so far,” Shlomo <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7341088575049891840/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">posted on LinkedIn</a> when announcing the news of the acquisition. Despite the growth and the profits — or really because of it — he sold his still-bootstrapped company because “the scale and volume we need is not something we can organically grow into&nbsp;… If we were able to get so far organically, bootstrapped, I’m excited to see our new pace now that we have all the resources in place,” he wrote.</p><p>For its part, Wix picked up a proven, fast-growing, local vibe-coding platform for a relative song because of its youth. <a href=\"https://techcrunch.com/2025/04/22/why-openai-wanted-to-buy-cursor-but-opted-for-the-fast-growing-windsurf/\">OpenAI paid $3 billion for Windsurf,</a> which was founded in 2021.&nbsp;</p><p>Wix, of course, offers no-code website building that look professionally designed. Adding a profitable LLM vibe-coding product to its offerings is a logical move.</p><p>Shlomo could not be immediately reached for additional comment.</p>","contentLength":3643,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44316920"},{"title":"From LLM to AI Agent: What's the Real Journey Behind AI System Development?","url":"https://www.codelink.io/blog/post/ai-system-development-llm-rag-ai-workflow-agent","date":1750325368,"author":"codelink","guid":162942,"unread":true,"content":"<p><strong node=\"[object Object]\">AI agents are a hot topic, but not every AI system needs to be one.</strong></p><p>While agents promise autonomy and decision-making power, simpler &amp; more cost-saving solutions better serve many real-world use cases. The key lies in choosing the right architecture for the problem at hand.</p><p>In this post, we'll explore recent developments in Large Language Models (LLMs) and discuss key concepts of AI systems.</p><p>We've worked with LLMs across projects of varying complexity, from zero-shot prompting to chain-of-thought reasoning, from RAG-based architectures to sophisticated workflows and autonomous agents.</p><p>This is an emerging field with evolving terminology. The boundaries between different concepts are still being defined, and classifications remain fluid. As the field progresses, new frameworks and practices emerge to build more reliable AI systems.</p><p>To demonstrate these different systems, we'll walk through a familiar use case – a resume-screening application – to reveal the unexpected leaps in capability (and complexity) at each level.</p><p>A pure LLM is essentially a lossy compression of the internet, a snapshot of knowledge from its training data. It excels at tasks involving this stored knowledge: summarizing novels, writing essays about global warming, explaining special relativity to a 5-year-old, or composing haikus.</p><p>However, without additional capabilities, an LLM cannot provide real-time information like the current temperature in NYC. This distinguishes pure LLMs from chat applications like ChatGPT, which enhance their core LLM with real-time search and additional tools.</p><p>That said, not all enhancements require external context. There are several prompting techniques, including in-context learning and few-shot learning that help LLMs tackle specific problems without the need of context retrieval.</p><ul><li>To check if a resume is a good fit for a job description, an LLM with one-shot prompting and in-context learning can be utilized to classify it as Passed or Failed.</li></ul><h2>RAG (Retrieval Augmented Generation)</h2><p>Retrieval methods enhance LLMs by providing relevant context, making them more current, precise, and practical. You can grant LLMs access to internal data for processing and manipulation. This context allows the LLM to extract information, create summaries, and generate responses. RAG can also incorporate real-time information through the latest data retrieval.</p><ul><li>The resume screening application can be improved by retrieving internal company data, such as engineering playbooks, policies, and past resumes, to enrich the context and make better classification decisions.</li><li>Retrieval typically employs tools like vectorization, vector databases, and semantic search.</li></ul><p>LLMs can automate business processes by following well-defined paths. They're most effective for consistent, well-structured tasks.</p><p>Tool use enables workflow automation. By connecting to APIs, whether for calculators, calendars, email services, or search engines, LLMs can leverage reliable external utilities instead of relying on their internal, non-deterministic capabilities.</p><ul><li>An AI workflow can connect to the hiring portal to fetch resumes and job descriptions → Evaluate qualifications based on experience, education, and skills → Send appropriate email responses (rejection or interview invitation).</li><li>For this resume scanning workflow, the LLM requires access to the database, email API, and calendar API. It follows predefined steps to automate the process programmatically.</li></ul><p>AI Agents are systems that reason and make decisions independently. They break down tasks into steps, use external tools as needed, evaluate results, and determine the following actions: whether to store results, request human input, or proceed to the next step.</p><p>This represents another layer of abstraction above tool use &amp; AI workflow, automating both planning and decision-making.</p><p>While AI workflows require explicit user triggers (like button clicks) and follow programmatically defined paths, AI Agents can initiate workflows independently and determine their sequence and combination dynamically.</p><ul><li>An AI Agent can manage the entire recruitment process, including parsing CVs, coordinating availability via chat or email, scheduling interviews, and handling schedule changes.</li><li>This comprehensive task requires the LLM to access databases, email and calendar APIs, plus chat and notification systems.</li></ul><h3>1. Not every system requires an AI agent</h3><p>Start with simple, composable patterns and add complexity as needed. For some systems, retrieval alone suffices. In our resume screening example, a straightforward workflow works well when the criteria and actions are clear. Consider an Agent approach only when greater autonomy is needed to reduce human intervention.</p><h3>2. Focus on reliability over capability</h3><p>The non-deterministic nature of LLMs makes building dependable systems challenging. While creating proofs of concept is quick, scaling to production often reveals complications. Begin with a sandbox environment, implement consistent testing methods, and establish guardrails for reliability.</p>","contentLength":5038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44316909"},{"title":"SpaceX Starship 36 Anomaly","url":"https://twitter.com/NASASpaceflight/status/1935548909805601020","date":1750308572,"author":"Ankaios","guid":162608,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44315529"},{"title":"Elliptic Curves as Art","url":"https://elliptic-curves.art/","date":1750305747,"author":"nill0","guid":162885,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44315321"},{"title":"Mathematicians hunting prime numbers discover infinite new pattern","url":"https://www.scientificamerican.com/article/mathematicians-hunting-prime-numbers-discover-infinite-new-pattern-for/","date":1750303709,"author":"georgecmu","guid":164243,"unread":true,"content":"<p data-block=\"sciam/paragraph\">For centuries, prime numbers have captured the imaginations of mathematicians, who continue to search for new patterns that help identify them and the way they’re distributed among other numbers. Primes are whole numbers that are greater than 1 and are divisible by only 1 and themselves. The three smallest <a href=\"https://www.scientificamerican.com/article/simple-formula-makes-prime-numbers-easy-but-a-million-dollar-mystery-remains/\">prime numbers</a> are 2, 3 and 5. It's easy to find out if small numbers are prime—one simply needs to check what numbers can factor them. When mathematicians consider large numbers, however, the task of discerning <a href=\"https://www.scientificamerican.com/article/these-prime-numbers-are-so-memorable-that-people-hunt-for-them/\">which ones are prime</a> quickly mushrooms in difficulty. Although it might be practical to check if, say, the numbers 10 or 1,000 have more than two factors, that strategy is unfavorable or even untenable for checking if gigantic numbers are prime or composite. For instance, the <a href=\"https://www.mersenne.org/primes/?press=M136279841\">largest known prime number</a>, which is 2 − 1, is 41,024,320 digits long. At first, that number may seem mind-bogglingly large. Given that there are infinitely many positive integers of all different sizes, however, this number is minuscule compared with even larger primes.</p><p data-block=\"sciam/paragraph\">Furthermore, mathematicians want to do more than just tediously attempt to <a href=\"https://www.scientificamerican.com/article/new-number-systems-seek-their-lost-primes/\">factor numbers one by one</a> to determine if any given integer is prime. “We’re interested in the prime numbers because there are infinitely many of them, but it’s very difficult to identify any patterns in them,” says Ken Ono, a mathematician at the University of Virginia. Still, one main goal is to determine how prime numbers are distributed within larger sets of numbers.</p><p data-block=\"sciam/paragraph\">Recently, Ono and two of his colleagues—William Craig, a mathematician at the U.S. Naval Academy, and Jan-Willem van Ittersum, a mathematician at the University of Cologne in Germany—identified a whole new approach for finding prime numbers. “We have described infinitely many new kinds of criteria for exactly determining the set of prime numbers, all of which are very different from ‘If you can’t factor it, it must be prime,’” Ono says. He and his colleagues’ paper, <a href=\"https://www.pnas.org/doi/10.1073/pnas.2409417121\">published in the <i>Proceedings of the National Academy of Sciences USA</i></a> was runner-up for a physical science prize that recognizes scientific excellence and originality. In some sense, the finding offers an infinite number of new definitions for what it means for numbers to be prime, Ono notes.</p><h2>On supporting science journalism</h2><p>If you're enjoying this article, consider supporting our award-winning journalism by<a href=\"https://www.scientificamerican.com/getsciam/\">subscribing</a>. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.</p><p data-block=\"sciam/paragraph\">At the heart of the team’s strategy is a notion called integer partitions. “The theory of partitions is very old,” Ono says. It dates back to the 18th-century Swiss mathematician Leonhard Euler, and it has continued to be expanded and refined by mathematicians over time. “Partitions, at first glance, seem to be the stuff of child’s play,” Ono says. “How many ways can you add up numbers to get other numbers?” For instance, the number 5 has seven partitions: 4 + 1, 3 + 2, 3 + 1 + 1, 2 + 2 + 1, 2 + 1 + 1 + 1 and 1 + 1 + 1 + 1 + 1.</p><p data-block=\"sciam/paragraph\">Yet the concept turns out to be powerful as a hidden key that unlocks new ways of detecting primes. “It is remarkable that such a classical combinatorial object—the partition function—can be used to detect primes in this novel way,” says Kathrin Bringmann, a mathematician at the University of Cologne. (Bringmann has worked with Ono and Craig before, and she’s currently van Ittersum’s postdoctoral adviser, but she wasn’t involved with this research.) Ono notes that the idea for this approach originated in a question posed by one of his former students, Robert Schneider, who’s now a mathematician at Michigan Technological University.</p><p data-block=\"sciam/paragraph\">Ono, Craig and van Ittersum proved that prime numbers are the solutions of an infinite number of a particular type of polynomial equation in partition functions. Named <a href=\"https://www.scientificamerican.com/article/gnarly-centuries-old-mathematical-quandaries-get-new-solutions/\">Diophantine equations</a> after third-century mathematician Diophantus of Alexandria (and studied long before him), these expressions can have integer solutions or rational ones (meaning they can be written as a fraction). In other words, the finding shows that “integer partitions detect the primes in infinitely many natural ways,” the researchers wrote in their paper.</p><p data-block=\"sciam/paragraph\">George Andrews, a mathematician at Pennsylvania State University, who edited the  paper but wasn’t involved with the research, describes the finding as “something that's brand new” and “not something that was anticipated,” making it difficult to predict “where it will lead.”</p><p data-block=\"sciam/paragraph\">The discovery goes beyond probing the distribution of prime numbers. “We’re actually nailing all the prime numbers on the nose,” Ono says. In this method, you can plug an integer that is 2 or larger into particular equations, and if they are true, then the integer is prime. One such equation is (3− 13 + 18− 8)() + (12 − 120 + 212)() − 960() = 0, where (), () and () are well-studied partition functions. “More generally,” for a particular type of partition function, “we prove that there are infinitely many such prime detecting equations with constant coefficients,” the researchers wrote in their paper. Put more simply, “it’s almost like our work gives you infinitely many new definitions for prime,” Ono says. “That’s kind of mind-blowing.”</p><p data-block=\"sciam/paragraph\">The team’s findings could lead to many new discoveries, Bringmann notes. “Beyond its intrinsic mathematical interest, this work may inspire further investigations into the surprising algebraic or analytic properties hidden in combinatorial functions,” she says. In combinatorics—the mathematics of counting—combinatorial functions are used to describe the number of ways that items in sets can be chosen or arranged. “More broadly, it shows the richness of connections in mathematics,” she adds. “These kinds of results often stimulate fresh thinking across subfields.”</p><p data-block=\"sciam/paragraph\">Bringmann suggests some potential ways that mathematicians could build on the research. For instance, they could explore what other types of mathematical structures could be found using partition functions or look for ways that the main result could be expanded to study different types of numbers. “Are there generalizations of the main result to other sequences, such as composite numbers or values of arithmetic functions?” she asks.</p><p data-block=\"sciam/paragraph\">“Ken Ono is, in my opinion, one of the most exciting mathematicians around today,” Andrews says. \"This isn’t the first time that he has seen into a classic problem and brought really new things to light.”</p><p data-block=\"sciam/paragraph\">There remains a glut of <a href=\"https://www.scientificamerican.com/article/the-riemann-hypothesis-the-biggest-problem-in-mathematics-is-a-step-closer/\">open questions about prime numbers</a>, many of which are long-standing. Two examples are the <a href=\"https://www.scientificamerican.com/article/prime-number-puzzle-has-stumped-mathematicians-for-more-than-a-century/\">twin prime conjecture</a> and <a href=\"https://www.scientificamerican.com/article/goldbachs-prime-numbers/\">Goldbach’s conjecture</a>. The twin prime conjecture states that there are infinitely many twin primes—prime numbers that are separated by a value of two. The numbers 5 and 7 are twin primes, as are 11 and 13. Goldbach’s conjecture states that “every even number bigger than 2 is a sum of two primes in at least one way,” Ono says. But no one has proven this conjecture to be true.</p><p data-block=\"sciam/paragraph\">“Problems like that have befuddled mathematicians and number theorists for generations, almost throughout the entire history of number theory,” Ono says. Although his team’s recent finding doesn’t solve those problems, he says, it’s a profound example of how mathematicians are pushing boundaries to better understand the mysterious nature of prime numbers.</p>","contentLength":7525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44315188"},{"title":"The Zed Debugger Is Here","url":"https://zed.dev/blog/debugger","date":1750300935,"author":"SupremumLimit","guid":162573,"unread":true,"content":"<p>Over 2,000 developers asked, and we delivered.</p><p>Debugging in Zed is now a reality—and it's a big leap toward Zed 1.0.</p><p>We set out to build a debugger with three primary focuses:</p><ul><li>Fast: Spend less time context switching and more time debugging</li><li>Familiar: In line with Zed's design language and supports everything expected from a typical debugger flow</li><li>Configurable: You're able to customize the UI, keybindings, debug configurations and more</li></ul><p>Out of the box, Zed supports debugging popular languages including Rust, C/C++, JavaScript, Go, and Python.\nWith our extension system, Zed can support any debug adapter that implements the <a href=\"https://microsoft.github.io/debug-adapter-protocol/\">Debug Adapter Protocol (DAP)</a>.</p><p>To simplify the setup process, we've introduced locators, a system that translates build configurations into debug configurations. Meaning that you can write a build task once in  and reference it from  — or, even better, rely on Zed's automatic configuration.</p><p>Zed automatically runs locators on built-in or language server-generated runnables, so in many cases you won't even need to write a debug configuration to get up and running.</p><p>We currently support locators for Cargo, Python, JavaScript, and Go, with more coming in the future.\nFor more information on configuring a debug session, <a href=\"https://zed.dev/docs/debugger\">see our documentation</a>.</p><p>Once in a debug session, Zed makes it easy to inspect your program's state, such as threads, variables, breakpoints, the call stack, and more.</p><div><figure><figcaption>Setting some breakpoints and running the test in a debug session.</figcaption></figure></div><p>The debugger panel is fully customizable too, just drag and rearrange tabs in whatever order you want; you can even move the debug panel around so it fits your workflow.</p><p>Zed also supports keyboard-driven debugging for users that prefer to keep their hands on the keyboard.\nYou can step through code, toggle breakpoints, and navigate a debug session without ever touching the mouse.</p><div><figure><figcaption>Navigating through the Debugger surfaces using only the keyboard.</figcaption></figure></div><p>Special thanks to <a href=\"https://github.com/RemcoSmitsDev\">Remco Smits</a> for driving a lot of the heavy lifting on this project—your contributions have been critical to getting us here.</p><p>Zed's debugger supports debugging a variety of languages through the Debug Adapter Protocol.\nBut simply implementing the protocol wasn't enough—we needed an architecture that could scale to collaborative debugging, support extensions, and efficiently cache and manage responses from debug adapters.</p><p>To achieve this, we built a two-layer architecture: a data layer that communicates directly with the debug adapters, and a UI layer that fetches data from the data layer to render the interface.</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rust\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rust\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>This separation means the UI layer only requests what it needs, allowing the data layer to lazily fetch information and avoid unnecessary requests.\nIt also makes the data layer solely responsible for maintaining session state, caching responses, and invalidating stale data.\nThis architecture will make implementing collaborative debugging significantly easier, since the same UI code can be reused across multiplayer sessions—and we only send essential data across the wire, preserving bandwidth.</p><p>Supporting every debug adapter out of the box wasn't feasible—there are over <a href=\"https://microsoft.github.io/debug-adapter-protocol/implementors/adapters/\">70 DAP implementations</a>, each with its own quirks.\nTo solve this, we <a href=\"https://zed.dev/docs/extensions/debugger-extensions\">extended</a> Zed's extension API to support debugger integration.</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rust\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>Adding DAP support via an extension involves defining a custom schema that integrates with our JSON server, implementing logic for downloading and launching the adapter, processing debug configuration to add sane default values, and integrating with locators for automatic configuration.\nThis design follows our approach to LSP extensions, giving extension authors full control to bring their own debug adapters to Zed with minimal friction.</p><p>We also wanted inline variable values to work out of the box.\nSurprisingly, the <a href=\"https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_inlineValue\">inline values request</a> is a part of the <a href=\"https://microsoft.github.io/language-server-protocol/\">Language Server Protocol (LSP)</a> instead of the DAP.\nUsing the inline values approach would limit Zed to only showing inline values for DAPs which integrate with LSPs, which isn't many.\nA naive workaround might be to use regular expressions to match variable names between the source code and debugger values, but that quickly breaks down when dealing with scopes, and comments.\nInstead, we turned to <a href=\"https://tree-sitter.github.io/tree-sitter/\">Tree-sitter</a>. After all Zed is built by the creators of Tree-sitter!</p><p>Through Tree-sitter queries, we can accurately identify variables within the current execution scope, and easily support any language through  files without relying on an LSP server to be tightly integrated with a debug adapter.\nAt launch, inline values are supported for Python, Rust, and Go.\nMore languages will be supported in the coming weeks.</p><p>When we set out to build the debugger, we wanted to make it seamless to use, out of the way, and in line with Zed's high standard of quality.\nNow that we've built a strong foundation that is compatible with any debug adapter, we're ready to explore and implement advanced features such as:</p><ul><li>New views: While we support all the fundamental views, we're planning on adding more advanced views such as a watch list, memory view, disassembly view, and a stack trace view</li><li>Automatic configuration: We're going to add support for more languages and build systems</li></ul>","contentLength":5185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44314977"},{"title":"TI to invest $60B to manufacture foundational semiconductors in the U.S.","url":"https://www.ti.com/about-ti/newsroom/news-releases/2025/texas-instruments-plans-to-invest-more-than--60-billion-to-manufacture-billions-of-foundational-semiconductors-in-the-us.html","date":1750297847,"author":"TMWNN","guid":162549,"unread":true,"content":"<p><b>Unleashing what’s next in American innovation</b></p><p>Today, TI is the largest foundational semiconductor manufacturer in the U.S., producing analog and embedded processing chips that are critical for smartphones, vehicles, data centers, satellites and nearly every other electronic device. In order to meet the steadily growing demand for these essential chips, TI is building on its legacy of technology leadership and expanding its U.S. manufacturing presence to help its customers pioneer the next wave of technological breakthroughs.</p><p><b>Igniting intelligence with Apple</b></p><p>“Texas Instruments' American-made chips help bring Apple products to life, and together, we’ll continue to create opportunity, drive innovation, and invest in the future of advanced manufacturing across the U.S.,” said Apple’s CEO Tim Cook.</p><p><b>Fueling the future with Ford</b></p><p>Ford and TI are working to strengthen American manufacturing, combining Ford’s automotive expertise with TI’s semiconductor technology to help drive innovation and secure a robust, domestic supply chain for the future of mobility. “At Ford, 80% of the vehicles we sell in the U.S. are assembled in the U.S., and we are proud to stand with technology leaders like TI that continue to invest in manufacturing in the U.S.,” said Jim Farley, President and CEO of Ford Motor Company.</p><p><b>Connecting patient care with Medtronic</b></p><p>Medtronic and TI are partnering to improve lives when it matters most. “At Medtronic, our life-saving medical technologies rely on semiconductors to deliver precision, performance, and innovation at scale,” said Geoff Martha, Medtronic chairman and CEO. “Texas Instruments has been a vital partner – especially during the global chip shortages – helping us maintain supply continuity and accelerate the development of breakthrough therapies. We’re proud to leverage TI’s U.S.-manufactured semiconductors as we work to transform healthcare and improve outcomes for patients around the world.”</p><p>NVIDIA is partnering with TI to unleash the next generation of artificial intelligence architectures. “NVIDIA and TI share the goal to revitalize U.S. manufacturing by building more of the infrastructure for AI factories here in the U.S.,” said Jensen Huang, founder and CEO of NVIDIA. “We look forward to continuing our collaboration with TI by developing products for advanced AI infrastructure.”</p><p><b>Securing high-speed satellite internet with SpaceX</b></p><p>SpaceX is increasingly leveraging TI’s high-speed process technology to connect its Starlink satellite internet service with TI’s latest 300mm SiGe technology manufactured in Sherman, Texas. “Our fundamental mission is to revolutionize global connectivity and eliminate the digital divide. Core to this mission is constantly pushing the boundaries of what is possible,” said Gwynne Shotwell, president and COO of SpaceX. “SpaceX is manufacturing tens of thousands of Starlink kits a day – all right here in the U.S. – and we are making huge investments in PCB manufacturing and silicon packaging to expand even further. TI’s U.S.-made semiconductors are crucial for securing a U.S. supply chain for our products, and their advanced silicon manufacturing capabilities provide the performance and reliability needed to help us meet the growing demand for high-speed internet all around the world.”</p>","contentLength":3339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44314759"},{"title":"Andrej Karpathy: Software in the era of AI [video]","url":"https://www.youtube.com/watch?v=LCEmiRjPEtQ","date":1750293201,"author":"sandslash","guid":161758,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44314423"},{"title":"MCP Specification – version 2025-06-18 changes","url":"https://modelcontextprotocol.io/specification/2025-06-18/changelog","date":1750291187,"author":"owebmaster","guid":162548,"unread":true,"content":"<p>This document lists changes made to the Model Context Protocol (MCP) specification since\nthe previous revision, <a href=\"https://modelcontextprotocol.io/specification/2025-03-26\">2025-03-26</a>.</p><ol><li>Add  field to , providing for completion requests to include\npreviously-resolved variables (PR <a href=\"https://github.com/modelcontextprotocol/modelcontextprotocol/pull/598\" target=\"_blank\" rel=\"noreferrer\">#598</a>).</li><li>Add  field for human-friendly display names, so that  can be used as a programmatic\nidentifier (PR <a href=\"https://github.com/modelcontextprotocol/modelcontextprotocol/pull/663\" target=\"_blank\" rel=\"noreferrer\">#663</a>)</li></ol><p>For a complete list of all changes that have been made since the last protocol revision,\n<a href=\"https://github.com/modelcontextprotocol/specification/compare/2025-03-26...2025-06-18\" target=\"_blank\" rel=\"noreferrer\">see GitHub</a>.</p>","contentLength":428,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44314289"},{"title":"Show HN: Unregistry – “docker push” directly to servers without a registry","url":"https://github.com/psviderski/unregistry","date":1750288630,"author":"psviderski","guid":161669,"unread":true,"content":"<p>I got tired of the push-to-registry/pull-from-registry dance every time I needed to deploy a Docker image.</p><p>In certain cases, using a full-fledged external (or even local) registry is annoying overhead. And if you think about it, there's already a form of registry present on any of your Docker-enabled hosts — the Docker's own image storage.</p><p>So I built Unregistry [1] that exposes Docker's (containerd) image storage through a standard registry API. It adds a `docker pussh` command that pushes images directly to remote Docker daemons over SSH. It transfers only the missing layers, making it fast and efficient.</p><pre><code>  docker pussh myapp:latest user@server\n</code></pre>\nUnder the hood, it starts a temporary unregistry container on the remote host, pushes to it through an SSH tunnel, and cleans up when done.<p>I've built it as a byproduct while working on Uncloud [2], a tool for deploying containers across a network of Docker hosts, and figured it'd be useful as a standalone project.</p><p>Would love to hear your thoughts and use cases!</p>","contentLength":1016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44314085"},{"title":"New US visa rules will force foreign students to unlock social media profiles","url":"https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening","date":1750288263,"author":"sva_","guid":161757,"unread":true,"content":"<p>Foreign students will be required to unlock their social media profiles to allow US diplomats to review their online activity before receiving educational and exchange visas, the state department has announced. Those who fail to do so will be suspected of hiding that activity from US officials.</p><p>The new guidance, unveiled by the state department on Wednesday, directs US diplomats to conduct an online presence review to look for “any indications of hostility toward the citizens, culture, government, institutions, or founding principles of the United States”.</p><p>A cable separately obtained by Politico also instructs diplomats to flag any “advocacy for, aid or support for foreign terrorists and other threats to US national security” and “support for unlawful antisemitic harassment or violence”.</p><p>The screening for “antisemitic” activity matches similar guidance given at US Citizenship and Immigration Services under the Department of Homeland Security and has been criticised as an effort to crack down on opposition to the conduct of Israel’s war in Gaza.</p><p>The new state department checks are directed at students and other applicants for visas in the F, M and J categories, which refer to academic and vocational education, as well as cultural exchanges.</p><p>“It is an expectation from American citizens that their government will make every effort to make our country safer, and that is exactly what the <a href=\"https://www.theguardian.com/us-news/trump-administration\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Trump administration</a> is doing every single day,” said a senior state department official, adding that Marco Rubio was “helping to make America and its universities safer while bringing the state Department into the 21st century”.</p><p>The Trump administration paused the issuance of new education visas late last month as it mulled new social media vetting strategies. The US had also targeted Chinese students for special scrutiny amid a tense negotiation over tariffs and the supply of rare-earth metals and minerals to the United States.</p><p>The state department directive allowed diplomatic posts to resume the scheduling of interviews for educational and exchange visas, but added that consular officers would conduct a “comprehensive and thorough vetting” of all applicants applying for F, M and J visas.</p><p>“To facilitate this vetting, all applicants for F, M and J non-immigrant visas will be asked to adjust the privacy settings on all their social media profiles to ‘public’”, the official said. “The enhanced social media vetting will ensure we are properly screening every single person attempting to visit our country.”</p>","contentLength":2560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44314054"},{"title":"Fang, the CLI Starter Kit","url":"https://github.com/charmbracelet/fang","date":1750286432,"author":"bewuethr","guid":161756,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44313901"},{"title":"The Missing 11th of the Month (2015)","url":"https://drhagen.com/blog/the-missing-11th-of-the-month/","date":1750283146,"author":"xk3","guid":162547,"unread":true,"content":"<p>On November 28th, 2012, Randall Munroe published <a href=\"https://xkcd.com/1140/\">an xkcd&nbsp;comic</a>&nbsp;that was&nbsp;a calendar in which&nbsp;the size of each date was&nbsp;proportional to&nbsp;how often each date is referenced by its ordinal name&nbsp;(e.g. \"October 14th\") in the <a href=\"https://books.google.com/ngrams\">Google Ngrams database</a> since 2000. Most of the large&nbsp;days are pretty much what you would expect:&nbsp;<a href=\"https://en.wikipedia.org/wiki/Independence_Day_(United_States)\">July 4th</a>, <a href=\"https://en.wikipedia.org/wiki/Christmas\">December 25th</a>, the 1st of every month, the last day of most months, and of course a <a href=\"https://en.wikipedia.org/wiki/September_11_attacks\">September 11th</a> that shoves&nbsp;its neighbors&nbsp;into the margins. There are not many days that seem to be smaller than the typical size. <a href=\"https://en.wikipedia.org/wiki/Leap_year\">February 29th</a> is a tiny speck, for instance. But if&nbsp;you stare at the comic&nbsp;long enough, you may get the impression that the 11th of most&nbsp;months is unusually small. The title&nbsp;text of the comic concurs, reading \"In months other than September, the 11th is mentioned substantially less often than any other date. It's been that way since long before 9/11 and I have no idea why.\" After digging into the raw data, I believe I have figured out why.</p><p>First I confirmed&nbsp;that the  is actually interesting. There are 31 days and one of them  to be smallest. Maybe the  isn't an outlier; it's just on the smaller end and our eyes&nbsp;are picking up on a pattern that doesn't exist. To confirm this is real,&nbsp;I compared actual numbers, not text size. The Ngrams database returns the total number times a phrase is mentioned in a given year normalized by the total number of books published that year. The database only goes up to the year 2008, so it is presumably unchanged from when Randall queried it in 2012.</p><p>I&nbsp;retrieved the count for each day for the year (,  etc.) and took the median over the months for each day (median of , , etc.) for&nbsp;each year. This summarizes&nbsp;how often the  and the other 30 days of the month appear in a given year. Using the median prevents outlier days like  from dragging up the average for its&nbsp;corresponding ordinal&nbsp;(the ). Only if a ordinal&nbsp;is unusual for at least 6&nbsp;of the 12 months will its median appear unusual.</p><p>I took the&nbsp;median for each ordinal over the years 2000-2008. The graph below is a histogram of the 31 medians. The  of the month stands out far above them all&nbsp;and the  just barely distinguishes itself from the remainder. Being the first day and the middle day of the month, these two make sense.&nbsp;However, the  stands out as the lowest by a significant&nbsp;margin (p-value &lt; 0.05), with no immediate explanation.</p><p>This deficit&nbsp;has been around for a long time. Below is all the ordinals for every year in the data set, 1800-2008. The data is smoothed over eleven&nbsp;years to flatten out the noise. Even at the beginning, the  is significantly lower than the main group. This mild deficit continues for a few decades and then something weird happens in 1860s; the&nbsp;11th suddenly diverges from its place just below the pack. The gap between the  and the ordinary ordinals expands rapidly until the  is about half of what one would expect it to be throughout the first half of the twentieth century. The gap shrinks in the second half of the twentieth century, but still&nbsp;persists at a smaller level&nbsp;until&nbsp;the end.</p><p>Astute graph readers will notice that something else weird is going on. There are four other lines that are much lower than they should be. From highest to lowest, they are the , the , the , and the .&nbsp;They were&nbsp;even lower than the  from 1800 until&nbsp;the 1890s. However, starting around 1900, their gaps started shrinking even as the  diverged until the gap disappeared completely in the 1930s. There is an interesting story there, but because their effect doesn't persist to the present, I'll continue to focus on the  and leave the others&nbsp;for a <a href=\"https://drhagen.com/blog/the-missing-23rd-of-the-month/\">future post</a>.</p><p>When I began this study, I was hoping to find a hidden taboo of holding events on the 11th or typographical&nbsp;bias&nbsp;against the&nbsp;shorthand ordinal. Alas, the reason is far is far more mundane: a numeral  looks a lot like a capital  or a lowercase  or a lowercase  in most of the fonts used for printing books. An  also looks like an , apparently. Google's&nbsp;algorithms made mistakes when reading the  from a page, interpreting&nbsp;the ordinal&nbsp;as some other word.</p><p>We can find some of these mistakes by directly searching for nonsense phrases like  or  or . There are nine possible combinations&nbsp;of , , and  that a  could be mistaken for. &nbsp;Five of them can actually be found in the database for at least one month: , , , , and . Also found was , , and , in which only one letter was misread. I collectively refer to these errors as&nbsp;.&nbsp;<a href=\"https://books.google.com/\">Google books</a>&nbsp;queries a newer database than the one on which Ngrams was built, but bona fide examples of the misreads can still be found. <a href=\"https://books.google.com/books?id=OJo3AAAAMAAJ&amp;dq=%22January%20IIth%22&amp;pg=RA3-PA34#v=onepage&amp;q=%22January%20IIth%22&amp;f=false\">Here is something</a> that Google books thinks says : <img alt=\"January IIth\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_IIth_light.png#only-light\"><img alt=\"January IIth\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_IIth_dark.png#only-dark\">. <a href=\"https://books.google.com/books?id=EcJQAQAAIAAJ&amp;dq=%22February%20llth%22&amp;pg=RA1-PA79#v=onepage&amp;q=%22February%20llth%22&amp;f=false\">And here is one</a> for : <img alt=\"February llth\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/February_llth_light.png#only-light\"><img alt=\"February llth\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/February_llth_dark.png#only-dark\">. <a href=\"https://books.google.com/books?id=zYHk_df06QsC&amp;dq=%22March%20lith%22&amp;pg=PA402#v=onepage&amp;q=%22March%20lith%22&amp;f=false\">And finally one</a>&nbsp;for : <img alt=\"March lith\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/March_lith_light.png#only-light\"><img alt=\"March lith\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/March_lith_dark.png#only-dark\">. There are hordes of these in the database. You can find other ordinals that were misread as well, but the  with its slender and ambiguous s was misread far more often than the others.</p><p>I added back in every instance of , , etc. to  and did the same to the other months. The graph below shows that the  gets a big&nbsp;boost by adding back&nbsp;the&nbsp;nonsense phrases. Before the 1860s,&nbsp;the difference between the  and the main group&nbsp;is erased. After the 1860s, about a quarter to a third of the difference is erased.</p><p>So where did&nbsp;the rest of the missing  go? Well, starting in the 1860s, the Google algorithm starts to make a rather peculiar error—it misreads  as . <a href=\"https://books.google.com/books?id=r7QaAAAAYAAJ&amp;dq=%22january%20nth%22&amp;pg=RA1-PA82#v=onepage&amp;q=%22january%20nth%22&amp;f=false\">Here is one&nbsp;example from a page full</a> of s: <img alt=\"January nth\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_nth_light.png#only-light\"><img alt=\"January nth\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_nth_dark.png#only-dark\">. In some years, the number of incorrect reads actually exceeds the number of correct reads. I added  to  and did the same for all the months. The graph below shows both the&nbsp;&nbsp;and its sum with the .&nbsp;There was&nbsp;little impact&nbsp;before the 1860s, but then this error alone accounts for nearly all of the missing .</p><p>When the  misreads and&nbsp; misreads are both added back into&nbsp;the , the gap disappears across the entire timeline and the &nbsp;looks like an ordinary day of the year.&nbsp;This suggests that the misreading of the  as , , , etc. is sufficient to explain the unusually low incidence of the  as a day of the month.</p><p>While it makes sense that the  was&nbsp;misread more than others, why is the misread rate not uniform? What happened in the 1860s that caused the dramatic rise in the error rate? I suspect that it has something to do with&nbsp;a special device invented in the 1860s_—_the typewriter. <a href=\"https://web.archive.org/web/20190706134959/https://chronicle.com/blogs/linguafranca/2012/03/14/old-style-versus-lining-figures/\">The earliest typewriters did not have a separate key for the numeral .</a>&nbsp;Typists were expected to use the lowercase  to represent a . When the algorithm read , it was far more&nbsp;correct than we have been&nbsp;giving it credit. There are not that many documents in Google books that are typewritten, but&nbsp;this popular new contraption had a powerful effect on the evolution of fonts. The  and  were identical on the increasingly familiar typewriters, and&nbsp;the fonts even of printed materials began to reflect this expectation. Compare the s and s in this font <a href=\"https://books.google.com/books?id=i-lOAAAAYAAJ&amp;dq=%22january%2011th%22%201850&amp;pg=PA115#v=onepage&amp;q&amp;f=false\">from&nbsp;1850</a>: <img alt=\"council_meeting\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/council_meeting_light.png#only-light\"><img alt=\"council_meeting\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/council_meeting_dark.png#only-dark\">.&nbsp;There is a clear difference between an  with no serifs on the top and the  with a pronounced serif. Compare that to a font <a href=\"https://books.google.com/books?id=STtaT_fheaMC&amp;lpg=PA334&amp;dq=%22january%2011th%22%201920&amp;pg=PA334#v=onepage&amp;q&amp;f=false\">from 1920</a>: <img alt=\"hotel_on_sunday\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/hotel_on_sunday_light.png#only-light\"><img alt=\"hotel_on_sunday\" src=\"https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/hotel_on_sunday_dark.png#only-dark\">.&nbsp;The characters are identical except for the kerning. Even to this day, most fonts represent both the  and the  as tall characters with two serifs on the bottom and one left-facing serif at the top. The only difference is that the serif on the  is slightly more angled than on the . (In this post, I used a special monospace font to make it easier to tell the difference.)&nbsp;The print quality of more recent books (post 1970s) has reduced the rate of&nbsp;failure, but it still has not gone away entirely, so that the remaining failures were noticeable in the xkcd comic.</p><p>The largest open question is why  was chosen so often. It seems like such a strange error to make. The word  is a legal word&nbsp;in mathematical and scientific publications, so that should help its chances of getting picked. In most fonts the top of the  is really thin, and is likely invisible in many texts on which they trained the algorithm. But there is a big different in height between  and , especially in the typewriter era, which is where the errors occur. And the phrase  is nonsense so that should have hurt its chances of being selected. Is it possible there&nbsp;was an error in one of the modern training texts that had an  labeled as , thereby confusing the algorithm? The only way to know for sure would be to crack open the source code of Google's text-reading algorithm. This is left as an exercise for the reader.</p><h6>The <a href=\"https://github.com/drhagen/xkcd11th\">code used for the analysis in this post</a> is available on&nbsp;Github.</h6>","contentLength":8488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44313550"},{"title":"Bento: A Steam Deck in a Keyboard","url":"https://github.com/lunchbox-computer/bento","date":1750281686,"author":"MichaelThatsIt","guid":162546,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44313379"},{"title":"Websites are tracking you via browser fingerprinting","url":"https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html","date":1750280106,"author":"gnabgib","guid":161668,"unread":true,"content":"<p>Clearing your cookies is not enough to protect your privacy online.&nbsp;</p><p><a href=\"https://dl.acm.org/doi/10.1145/3696410.3714548\" rel=\"noopener\" target=\"_blank\">New research</a> led by Texas A&amp;M University found that websites are covertly using browser fingerprinting — a method to uniquely identify a web browser — to track people across browser sessions and sites.</p><p>“Fingerprinting has always been a concern in the privacy community, but until now, we had no hard proof that it was actually being used to track users,” said Dr. Nitesh Saxena, cybersecurity researcher, professor of computer science and engineering and associate director of the Global Cyber Research Institute&nbsp;at Texas A&amp;M. “Our work helps close that gap.”</p><p>When you visit a website, your browser shares a surprising amount of information, like your screen resolution, time zone, device model and more. When combined, these details create a “fingerprint” that’s often unique to your browser. Unlike cookies — which users can delete or block — fingerprinting is much harder to detect or prevent. Most users have no idea it’s happening, and even privacy-focused browsers struggle to fully block it.</p><p>“Think of it as a digital signature you didn’t know you were leaving behind,” explained co-author Zengrui Liu, a former doctoral student in Saxena’s lab. “You may look anonymous, but your device or browser gives you away.”</p><p>This research marks a turning point in how computer scientists understand the real-world use of browser fingerprinting by connecting it with the use of ads.</p><p>“While prior works have studied browser fingerprinting and its usage on different&nbsp;websites, ours is the first to correlate browser fingerprints and ad behaviors, essentially establishing the relationship between web tracking and fingerprinting,” said co-author Dr. Yinzhi Cao, associate professor of computer science and technical director of the Information Security Institute at Johns Hopkins University.</p>","contentLength":1891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44313206"}],"tags":["hn"]}