{"id":"6W9","title":"HN","displayTitle":"HN","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":33,"items":[{"title":"My Life in Weeks","url":"https://weeks.ginatrapani.org/","date":1739648069,"author":"bookofjoe","guid":215,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061498"},{"title":"New SF public health chief was part of McKinsey opioid-marketing operation","url":"https://sfstandard.com/2025/02/14/san-francisco-department-public-health-daniel-tsai-opioids-mckinsey/","date":1739647936,"author":"iancmceachern","guid":214,"unread":true,"content":"<div><p>Dr. David Juurlink, an expert on tramadol, called the drug a “minor player in the opioid crisis, but a player nevertheless.”</p></div><div><p>He added, “To the extent that McKinsey helped advertise it as a notionally safer opioid, I think they did a disservice in doing so. The main reason I say tramadol is a minor player is because it wasn’t prescribed like candy, like OxyContin was.”</p></div><div><p>McKinsey’s method of targeting high-volume prescribers was part of its playbook to juice opioid sales, despite mounting evidence that the drugs could be highly addictive. In other emails uncovered in the McKinsey documents, employees wrote excitedly about finding doctors who were willing to write opioid prescriptions. <a href=\"https://www.industrydocuments.ucsf.edu/opioids/docs/#id=hlmn0255\">In one instance</a> in 2015, a McKinsey partner wrote, “The challenge which we need to start working on is to identify the sweet spot of docs so we can do targeting. … Fun be[g]ins on Monday!”</p></div><div><p>San Francisco’s fentanyl crisis is part of a broader trend of opioid overdoses <a href=\"https://www.cdc.gov/overdose-prevention/about/understanding-the-opioid-overdose-epidemic.html\">that traces back to the 1990s,</a> when prescription opioids became popular among doctors for chronic pain management. Companies such as Purdue Pharma, which manufactured OxyContin, brought in consulting firms like McKinsey to help with sales strategies. After a Department of Justice probe and settlement, McKinsey <a href=\"https://www.justice.gov/usao-ma/media/1380236/dl\">acknowledged that it knew</a> the dangers of OxyContin but continued working with Purdue Pharma — even after several of the drugmaker’s executives pled guilty in 2007 to misrepresenting addiction risks.&nbsp;</p></div><div><p>McKinsey, along with a slew of drug companies and pharmacies, agreed to pay billions in settlement funds over their roles in fueling opioid addiction. California received <a href=\"https://oag.ca.gov/news/press-releases/attorney-general-becerra-announces-573-million-nationwide-settlement-mckinsey\">roughly $60 million</a> from the 2021 McKinsey settlement. San Francisco, <a href=\"https://www.sfcityattorney.org/2023/05/17/san-francisco-city-attorney-announces-230-million-settlement-with-walgreens-after-victory-in-opioid-litigation/\">under the 2023 settlement</a> of an opioid-related lawsuit, was expected to receive about $230 million from Walgreens.</p></div><div><p>In both instances, the funds were slated to be used for opioid recovery efforts.</p></div><div><p>In 2019, McKinsey said it would no longer work on opioid-related businesses.&nbsp;Last year, McKinsey formally apologized for its Purdue Pharma work, <a href=\"https://www.mckinsey.com/about-us/opioidfacts\">saying it was “deeply sorry”</a> for its role in selling OxyContin. “This terrible public health crisis and our past work for opioid manufacturers will always be a source of profound regret for our firm,” the company said in a statement.</p></div><div><p>At the San Francisco Department of Public Health, Tsai replaced Dr. Grant Colfax, who took the reins in 2019 and led the city through the pandemic <a href=\"https://sfstandard.com/2025/01/16/grant-colfax-resign-san-francisco-public-health/\">before stepping down in January</a>. The role paid $546,133 in 2024, one of the highest city salaries.&nbsp;&nbsp;</p></div><div><p>On Monday, the San Francisco Health Commission unanimously nominated Tsai as director of Public Heath. Dr. Laurie Green, president of the commission, said the governing body conducted a “multi-hour” interview.</p></div>","contentLength":2776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061482"},{"title":"Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5","url":"https://github.com/b4rtaz/distributed-llama/discussions/162","date":1739635889,"author":"b4rtazz","guid":213,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43059579"},{"title":"Carbon capture more costly than switching to renewables, researchers find","url":"https://techxplore.com/news/2025-02-carbon-capture-renewables.html","date":1739631990,"author":"Brajeshwar","guid":212,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058997"},{"title":"Dust from car brakes more harmful than exhaust, study finds","url":"https://e360.yale.edu/digest/brake-pads-lung-damage-study","date":1739631975,"author":"Brajeshwar","guid":211,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058993"},{"title":"Diablo hackers uncovered a speedrun scandal","url":"https://arstechnica.com/gaming/2025/02/the-diablo-hackers-that-debunked-a-record-speedrun/","date":1739628000,"author":"pitwin","guid":210,"unread":true,"content":"<p>But simply splitting a run into segments doesn't explain away all of the problems the TAS team found. Getting Naj's Puzzler on dungeon level 9, for instance, still requires outside modification of a save file, which is specifically prohibited by <a href=\"https://kb.speeddemosarchive.com/Rules\">longstanding Speed Demos Archive rules</a> that \"manually editing/adding/removing game files is generally not allowed.\" Groobo's apparent splicing of multiple game versions and differently seeded save files also seems to go against SDA rules, which say that \"there obviously needs to be continuity between segments in terms of inventory, experience points or whatever is applicable for the individual game.\"</p><p>After being presented with the TAS team's evidence, SDA <a href=\"https://speeddemosarchive.com/\">wrote</a> that \"it has been determined that Groobo's run very likely does not stem from only legitimate techniques, and as such, has itself been banished barring new developments.\" But Groobo's record is <a href=\"https://www.guinnessworldrecords.com/world-records/110580-fastest-completion-of-an-rpg-videogame\">still listed as the \"Fastest completion of an RPG videogame\"</a> by Guinness World Records, which has not offered a substantive response to the team's findings (Guinness has not responded to a request for comment from Ars Technica).</p><figure><div><div>\n      A recent  speedrun on a confirmed legitimate dungeon seed.\n\n          </div></div></figure><p>This might seem like a pretty petty issue to spend weeks of time and attention debunking. But at a recent presentation attended by Ars, Cecil said he was motivated to pursue it because \"it did harm. Groobo's alleged cheating in 2009 completely stopped interest in speedrunning this category [of ]. No one tried, no one could.\"</p><p>Because of Groobo's previously unknown modifications to make an impossible-to-beat run, \"this big running community just stopped trying to run this game in that category,\" Cecil said. \"For more than a decade, this had a chilling impact on that community.\" With Groobo's run out of the way, though, new runners are <a href=\"https://www.youtube.com/watch?v=bXG1vW6VEKA\">setting new records on confirmed legitimate RNG seeds</a>, and <a href=\"https://www.youtube.com/watch?v=F9mn5CpQCFw\">with the aid of TAS tools</a>.</p><p>In the end, Cecil said he hopes the evidence regarding Groobo's run will make people look more carefully at other record submissions. \"Groobo had created a number of well-respected ... speedruns,\" he said. \"[People thought] there wasn't any good reason to doubt him. In other words, there was bias in familiarity. This was a familiar character. Why would they cheat?\"</p>","contentLength":2302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058522"},{"title":"Show HN: I Built a Reddit-style Bluesky client – still rough, but open to ideas","url":"https://threadsky.app/","date":1739625557,"author":"lakshikag","guid":247,"unread":true,"content":"<p>A place for solo or small-team developers to discuss game development. Showcase progress, share tips, ask for help, etc.</p>","contentLength":120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058285"},{"title":"Show HN: Kreuzberg – Modern async Python library for document text extraction","url":"https://github.com/Goldziher/kreuzberg","date":1739614043,"author":"nhirschfeld","guid":209,"unread":true,"content":"<p>I'm excited to showcase Kreuzberg!</p><p>Kreuzberg is a modern Python library built from the ground up with async/await, type hints, and optimized I/O handling.</p><p>It provides a unified interface for extracting text from documents (PDFs, images, office files) without external API dependencies.</p><p>Key technical features:\n- Built with modern Python best practices (async/await, type hints, functional-first)\n- Optimized async I/O with anyio for multi-loop compatibility\n- Smart worker process pool for CPU-bound tasks (OCR, doc conversion)\n- Efficient batch processing with concurrent extractions\n- Clean error handling with context-rich exceptions</p><p>I built this after struggling with existing solutions that were either synchronous-only, required complex deployments, or had poor async support. The goal was to create something that works well in modern async Python applications, can be easily dockerized or used in serverless contexts, and relies only on permissive OSS.</p><p>Key advantages over alternatives:\n- True async support with optimized I/O\n- Minimal dependencies (much smaller than alternatives)\n- Perfect for serverless and async web apps\n- Local processing without API calls\n- Built for modern Python codebases with rigorous typing and testing</p><p>The library is MIT licensed and open to contributions.</p>","contentLength":1289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43057375"},{"title":"Jane Street's Figgie card game","url":"https://www.figgie.com/","date":1739613560,"author":"eamag","guid":208,"unread":true,"content":"<p><a href=\"https://www.janestreet.com/\">Jane Street</a>'s fast-paced\n              Figgie game simulates exciting elements of markets and trading. At\n              Jane Street, Figgie is a game we teach and also one we really\n              enjoy playing.\n            </p><p><a href=\"https://www.figgie.com/faqs.html\">Read our FAQs</a>\n              for more. If you have a question that isn’t answered there, we’d\n              like to hear\n              <a href=\"mailto:figgie@janestreet.com\">what’s missing</a> and what\n              would be helpful to know, and we’ll do our best to update FAQs\n              along the way.\n            </p>","contentLength":507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43057344"},{"title":"The 20 year old PSP can now connect to WPA2 WiFi Networks","url":"https://wololo.net/2025/02/14/the-20-year-old-psp-can-now-connect-to-wpa2-wifi-networks/","date":1739590316,"author":"zdw","guid":207,"unread":true,"content":"<div><img data-recalc-dims=\"1\" fetchpriority=\"high\" decoding=\"async\" aria-describedby=\"caption-attachment-49719\" src=\"https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/psp_wpa2_support.jpg?resize=848%2C477&amp;ssl=1\" alt=\"\" width=\"848\" height=\"477\" srcset=\"https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/psp_wpa2_support.jpg?w=848&amp;ssl=1 848w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/psp_wpa2_support.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/psp_wpa2_support.jpg?resize=768%2C432&amp;ssl=1 768w\" sizes=\"(max-width: 848px) 100vw, 848px\"><p>Screenshot source: Zekiu_ on youtube</p></div><p><a href=\"https://wololo.net/tag/acid_snake\">Acid_Snake</a> and the <a href=\"https://wololo.net/tag/ark-4/\">ARK</a> Development team have released a significant update to the ARK custom Firmware for the Sony PSP. Custom Firmware now allows the Playstation Portable to connect to WPA2 encrypted Wifi networks. This is thanks to the recently released  plugin, created by developer  and published on the PSP Homebrew discord.</p><h2>Playstation Portable gets WPA2 Wifi access</h2><p>The PSP has been out of official support from Sony for years, but lots of enthusiasts keep maintaining this great handheld through homebrew and custom Firmware updates. As technology evolves around us, older devices such as the PlayStation Portable can lose some of their features.</p><p>For example, as WPA2 has become the defacto encryption standard for home wifi networks (WPA3’s adoption rate remains low), older devices such as the PSP, that do not support these new* encryption standards become technically unable to access the internet.</p><p>Wifi access was a very strong feature of the PSP when it was released, and, although it’s probably less important nowadays, losing that feature because newer networks aren’t compatible is a bummer.</p><p>WPA2 support has been a request by many enthusiasts for years on PSP discussion channels, and it seems that the wpa2psp plugin by developer Moment now brings this to life. According to Acid_Snake, the developer was kind enough to provide the source code of the plugin, which allowed the ARK team to embed it into the ARK Custom Firmware for PSP.</p><div><img data-recalc-dims=\"1\" decoding=\"async\" aria-describedby=\"caption-attachment-49720\" src=\"https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/moment_wpa2psp_release.png?resize=1024%2C321&amp;ssl=1\" alt=\"\" width=\"1024\" height=\"321\" srcset=\"https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/moment_wpa2psp_release.png?resize=1024%2C321&amp;ssl=1 1024w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/moment_wpa2psp_release.png?resize=300%2C94&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/moment_wpa2psp_release.png?resize=768%2C241&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2025/02/moment_wpa2psp_release.png?w=1124&amp;ssl=1 1124w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"></div><p><a href=\"https://www.reddit.com/r/psphacks/comments/1iimnft/wpa2_now_works_on_psp_thanks_to_a_new_plugin/\">This reddit thread</a> by Nebula_NL covers a lot of details on how to install and use the plugin. But the bottom line is: install the latest release of the ARK CFW on your PSP, and take it from there. (Note that you can also manually install the plugin if you’re using another CFW than ARK)</p><p>This is of course the first iteration of this plugin, and it comes with limitations, specifically:</p><ul><li>2.4 GHz Only\n<ul><li>WPA2 support works with 2.4 GHz WiFi.</li><li>If your router uses a single SSID for both 2.4 GHz and 5 GHz, you may need to separate them and connect your PSP to the 2.4 GHz network.</li></ul></li><li>WPA2 AES Only\n<ul><li>Requires WPA2 with AES (AES-CCMP) encryption.</li><li>TKIP is not supported and will not work.</li></ul></li><li>WEP/WPA Compatibility\n<ul><li>While WPA2 is active, WEP and WPA encryption will not work.</li><li>To use WEP or WPA again, disable WPA2, and they will function normally.</li></ul></li><li>WPA2/WPA3 Mixed Mode\n<ul><li>If your router is set to WPA2/WPA3 mixed mode, your PSP may struggle to obtain an IP address.</li><li>Try manually setting the IP address instead of using DHCP in [AUTO] mode.</li></ul></li></ul><h2>Download and install ARK-4 + enable WPA2 Support for the PSP</h2><p><em>* WPA2 was certified in 2004… It’s “new” from the PSP’s perspective which launched the same year and didn’t “need” to support it at the time. WPA3 launched in 2018 but its adoption is taking time</em></p>","contentLength":2772,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43055671"},{"title":"Did Semgrep Just Get a Lot More Interesting?","url":"https://fly.io/blog/semgrep-but-for-real-now/","date":1739580031,"author":"ghuntley","guid":206,"unread":true,"content":"<p><a href=\"https://ghuntley.com/stdlib/\" title=\"\">This bit by Geoffrey Huntley</a> is super interesting to me and, despite calling out that LLM-driven development agents like Cursor have something like a 40% success rate at actually building anything that passes acceptance criteria, makes me think that more of the future of our field belongs to people who figure out how to use this weird bags of model weights than any of us are comfortable with. </p><p>I’ve been dinking around with Cursor for a week now (if you haven’t, I think it’s something close to malpractice not to at least take it — or something like it — for a spin) and am just now from this post learning that Cursor has this <a href=\"https://docs.cursor.com/context/rules-for-ai\" title=\"\">rules feature</a>. </p><p>The important thing for me is not how Cursor rules work, but rather how Huntley uses them. He turns them back on themselves, writing rules to tell Cursor how to organize the rules, and then teach Cursor how to write (under human supervision) its own rules.</p><p>Cursor kept trying to get Huntley to use Bazel as a build system. So he had cursor write a rule for itself: “no bazel”. And there was no more Bazel. If I’d known I could do this, I probably wouldn’t have bounced from the Elixir project I had Cursor doing, where trying to get it to write simple unit tests got it all tangled up trying to make <a href=\"https://hexdocs.pm/mox/Mox.html\" title=\"\">Mox</a> work. </p><p>But I’m burying the lead. </p><p>Security people have been for several years now somewhat in love with a tool called <a href=\"https://github.com/semgrep/semgrep\" title=\"\">Semgrep</a>. Semgrep is a semantics-aware code search tool; using symbolic variable placeholders and otherwise ordinary code, you can write rules to match pretty much arbitary expressions and control flow. </p><p>If you’re an appsec person, where you obviously go with this is: you build a library of Semgrep searches for well-known vulnerability patterns (or, if you’re like us at Fly.io, you work out how to get Semgrep to catch the Rust concurrency footgun of RWLocks inside if-lets).</p><p>The reality for most teams though is “ain’t nobody got time for that”. </p><p>But I just checked and, unsurprisingly, 4o <a href=\"https://chatgpt.com/share/67aa94a7-ea3c-8012-845c-6c9491b33fe4\" title=\"\">seems to do reasonably well</a> at generating Semgrep rules? Like: I have no idea if this rule is actually any good. But it looks like a Semgrep rule?</p><p>What interests me is this: it seems obvious that we’re going to do more and more “closed-loop” LLM agent code generation stuff. By “closed loop”, I mean that the thingy that generates code is going to get to run the code and watch what happens when it’s interacted with. You’re just a small bit of glue code and a lot of system prompting away from building something like that right now: <a href=\"https://x.com/chris_mccord/status/1882839014845374683\" title=\"\">Chris McCord is building</a> a thingy that generates whole Elixir/Phoenix apps and runs them as Fly Machines. When you deploy these kinds of things, the LLM gets to see the errors when the code is run, and it can just go fix them. It also gets to see errors and exceptions in the logs when you hit a page on the app, and it can just go fix them.</p><p>With a bit more system prompting, you can get an LLM to try to generalize out from exceptions it fixes and generate unit test coverage for them. </p><p>With a little bit more system prompting, you can probably get an LLM to (1) generate a Semgrep rule for the generalized bug it caught, (2) test the Semgrep rule with a positive/negative control, (3) save the rule, (4) test the whole codebase with Semgrep for that rule, and (5) fix anything it finds that way. </p><p>That is a lot more interesting to me than tediously (and probably badly) trying to predict everything that will go wrong in my codebase a priori and Semgrepping for them. Which is to say: Semgrep — which I have always liked — is maybe a lot more interesting now? And tools like it?</p>","contentLength":3614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43054673"},{"title":"Show HN: VimLM – A Local, Offline Coding Assistant for Vim","url":"https://github.com/JosefAlbers/VimLM","date":1739576081,"author":"JosefAlbers","guid":246,"unread":true,"content":"<p>VimLM is a local, offline coding assistant for Vim. It’s like Copilot but runs entirely on your machine—no APIs, no tracking, no cloud.</p><p>- Deep Context: Understands your codebase (current file, selections, references).  \n- Conversational: Iterate with follow-ups like \"Add error handling\".  \n- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  \n- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.</p><p>Perfect for privacy-conscious devs or air-gapped environments.</p><p>Try it:  \n```\npip install vimlm\nvimlm\n```</p>","contentLength":558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43054244"},{"title":"A decade later, a decade lost (2024)","url":"https://meyerweb.com/eric/thoughts/2024/06/07/a-decade-later-a-decade-lost/","date":1739574636,"author":"ZeWaka","guid":205,"unread":true,"content":"<p>I woke up this morning about an hour ahead of my alarm, the sky already light, birds calling.&nbsp; After a few minutes, a brief patter of rain swept across the roof and moved on.</p><p>I just lay there, not really thinking.&nbsp; Feeling.&nbsp; Remembering.</p><p>Almost sixteen years to the minute before I awoke, my second daughter was born.&nbsp; Almost ten years to the same minute before, she’d turned six years old, already semi-unconscious, and died not quite twelve hours later.</p><p>So she won’t be taking her first solo car drive today.&nbsp; She won’t be celebrating with dinner at her favorite restaurant in the whole world.&nbsp; She won’t kiss her niece good night or affectionately rag on her siblings.</p><p>Or maybe she wouldn’t have done any of those things anyway, after a decade of growth and changes and paths taken.&nbsp; What would she really be like, at sixteen?</p><p>We will never know.&nbsp; We can’t even guess.&nbsp; All of that, everything she might have been, is lost.</p><p>This afternoon, we’ll visit Rebecca’s grave, and then go to hear her name read in remembrance at one of her very happiest places, <a href=\"https://en.wikipedia.org/wiki/Anshe_Chesed_Fairmount_Temple\">Anshe Chesed Fairmount Temple</a>, for the last time.&nbsp; At the end of the month, the temple will close as part of a merger.&nbsp; Another loss.</p><p>A decade ago, I said that I felt the weight of all the years she would never have, and that they might crush me.&nbsp; Over time, I have come to realize all the things she never saw or did adds to that weight.&nbsp; Even though it seems like it should be the same weight.&nbsp; Somehow, it isn’t.</p><p>I was talking about all of this with a therapist a few days ago, about the time and the losses and their accumulated weight.&nbsp; I said, “I don’t know how to be okay when I failed my child in the most fundamental way possible.”</p><p>“You didn’t fail her,” they said gently.</p><p>“I know that,” I replied. “But I don’t feel it.”</p><p>A decade, it turns out, does not change that.&nbsp; I’m not sure now that any stretch of time ever could.</p>","contentLength":1935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43054069"},{"title":"We were wrong about GPUs","url":"https://fly.io/blog/wrong-about-gpu/","date":1739572591,"author":"mxstbr","guid":204,"unread":true,"content":"<div><p>We’re building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren’t going anywhere, but: GPUs aren’t going anywhere.</p></div><p>A Fly Machine is a <a href=\"https://fly.io/blog/docker-without-docker/\">Docker/OCI container</a> running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It’s a Fly Machine that can do fast CUDA.</p><p>Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn’t fit the moment. It’s a bet that doesn’t feel like it’s paying off.</p><p><strong>If you’re using Fly GPU Machines, don’t freak out; we’re not getting rid of them.</strong> But if you’re waiting for us to do something bigger with them, a v2 of the product, you’ll probably be waiting awhile.</p><p>GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines <a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor\">Intel’s Cloud Hypervisor</a>, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.</p><p>GPUs <a href=\"https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html\">terrified our security team</a>. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers</p><div><p>(not even bidirectional: in common configurations, GPUs talk to each other)</p></div><p>with arbitrary, end-user controlled computation, all operating outside our normal security boundary.</p><p>We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren’t mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there’s a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.</p><p>We funded two very large security assessments, from <a href=\"https://www.atredis.com/\">Atredis</a> and <a href=\"https://tetrelsec.com/\">Tetrel</a>, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.</p><p>Security wasn’t directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.</p><p>We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we’d have been on Nvidia’s driver happy-path.</p><p>Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.</p><p>Instead, we burned months trying (and ultimately failing) to get Nvidia’s host drivers working to map <a href=\"https://www.nvidia.com/en-us/data-center/virtual-solutions/\">virtualized GPUs</a> into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.</p><p>I’m not sure any of this really mattered in the end. There’s a segment of the market we weren’t ever really able to explore because Nvidia’s driver support kept us from thin-slicing GPUs. We’d have been able to put together a really cheap offering for developers if we hadn’t run up against that, and developers love “cheap”, but I can’t prove that those customers are real.</p><p>On the other hand, we’re committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer’s OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our  orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!</p><p>And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.</p><p>The biggest problem: developers don’t want GPUs. They don’t even want AI/ML models. They want LLMs.  may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But  don’t care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can’t just give them a GPU.</p><p>For those developers, who probably make up most of the market, it doesn’t seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of “tokens per second” aren’t counting milliseconds.</p><div><p>(you should all feel sympathy for us)</p></div><p>This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they’ll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn’t seem to matter yet, so the market doesn’t care.</p><p>Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.</p><p>People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.</p><div><p>Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.</p></div><p>We think there’s probably a market for users doing lightweight ML work getting tiny GPUs. <a href=\"https://www.nvidia.com/en-us/technologies/multi-instance-gpu/\">This is what Nvidia MIG does</a>, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it’s not baked; we can’t use it. And I’m not sure how many of those customers there are, or whether we’d get the density of customers per server that we need.</p><p><a href=\"https://fly.io/blog/cutting-prices-for-l40s-gpus-in-half\">That leaves the L40S customers</a>. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they’re the one part we have in our inventory people seem to get a lot of use out of. We’re happy with them. But they’re just another kind of compute that some apps need; they’re not a driver of our core business. They’re not the GPU bet paying off.</p><p>Really, all of this is just a long way of saying that for most software developers, “AI-enabling” their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.</p><p>A very useful way to look at a startup is that it’s a race to learn stuff. So, what’s our report card?</p><p>First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of  models, the world <a href=\"https://github.com/elixir-nx/bumblebee\" title=\"\">Elixir Bumblebee</a> looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.</p><p>But <a href=\"https://www.cursor.com/\" title=\"\">Cursor happened</a>, and, as they say, how are you going to keep ‘em down on the farm once they’ve seen Karl Hungus? It seems much clearer where things are heading.</p><p>GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.</p><p>Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn’t a winning strategy. I’d rather we’d flopped the nut straight, but I think going in on this hand was the right call.</p><p>A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our <a href=\"https://fly.io/blog/the-exit-interview-jp/\" title=\"\">costs here aren’t recoverable</a>. But the hardware parts that aren’t generating revenue will ultimately get liquidated; like with <a href=\"https://fly.io/blog/32-bit-real-estate/\" title=\"\">our portfolio of IPv4 addresses</a>, I’m even more comfortable making bets backed by tradable assets with durable value.</p><p>In the end, I don’t think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I’m very happy about is that we didn’t compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we’re scaling back our GPU ambitions without having sacrificed <a href=\"https://fly.io/blog/sandboxing-and-workload-isolation/\" title=\"\">any of our isolation story</a>, and, ironically, GPUs  are making that story a lot more important. The same thing goes for our Fly Machine developer experience.</p><p>We started this company building a Javascript runtime for edge computing. We learned that our customers didn’t want a new Javascript runtime; they just wanted their native code to work. <a href=\"https://news.ycombinator.com/item?id=22616857\" title=\"\">We shipped containers</a>, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That’s usually how we figure out the right answers:  by being wrong about a lot of stuff.</p>","contentLength":9514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43053844"},{"title":"The hardest working font in Manhattan","url":"https://aresluna.org/the-hardest-working-font-in-manhattan/","date":1739569549,"author":"robinhouston","guid":203,"unread":true,"content":"<p>\n\t\tIn 2007, on my first trip to New York City, I grabbed a brand-new DSLR camera and photographed all the fonts I was supposed to love. I admired American Typewriter in all of the I &lt;3 NYC logos, watched Akzidenz Grotesk and Helvetica fighting over the subway signs, and even caught an occasional appearance of the flawlessly-named Gotham, still a year before it skyrocketed in popularity via Barack Obama’s first campaign. \n\t</p><p>\n\t\tBut there was one font I didn’t even notice, even though it was everywhere around me.\n\t</p><p>\t\t\n\t\tLast year in New York, I walked over 100 miles and took thousands of photos of one and one font only.\n\t</p><p>\t\t\n\t\tThe font’s name is Gorton.\n\t</p><p>\t\t\n\t\tIt’s hard to believe today that there was a time before I knew of Gorton and all its quirks and mysteries. The first time I realized the font even existed was some time in 2017, when I was researching for <a target=\"_blank\" href=\"https://shifthappens.site\">my book about the history of typing</a>. \n\t</p><p>\t\t\n\t\tMany keyboards, especially older ones, sported a particular distinctive font on their keycaps. It was unusually square in proportions, and a weird mélange of “mechanical” and “childish.”\n\t</p><p>\t\t \n\t\tThe more I looked at it, the more I realized how bizarre and amateurish it was. The G always felt like it was about to roll away on its side. There was a goofy wavy hook sticking out of Q. P and R were often too wide. &amp; and @ symbols would be laughed away in a type crit, and the endings of C felt like grabbing something next to it – a beginning of a ligature that never came.\n\t</p><p>\t\t\n\t\tThe strangeness extended to the digits. There was a top-flatted 3 resembling a Cyrillic letter, 7 sloping down in a unique way, a very geometric 4, an unusual – perhaps even naïve – symmetry between 6 and 9, and a conflation of O with 0 that would be a fireable offense elsewhere.\n\t</p><p>\t\t\t\n\t\tLooking at just a few keyboards, it was also obvious that it wasn’t just one rigid font. There were always variations, sometimes even on one keyboard. 0 came square, dotted, or slashed. The usually very narrow letter I sometimes sported serifs. The R and the 6 moved their middles higher or lower. There also seemed to be a narrower version of the font, deployed when a keycap needed a word and not just a letter. (Lowercase letters existed too, but not very often.) \n\t</p><p>\t\t\n\t\tMy first thought was: What a mess. Is this how “<a target=\"_blank\" href=\"https://fonts.ilovetypography.com/category/grotesque\">grotesque</a>” fonts got their name?\n\t</p><p>\t\t\n\t\tThen, the second thought: I kind of like it.\n\t</p><figcaption>The most distinctive letterforms of Gorton</figcaption><p>\t\t\n\t\tBut what font was it? What The Font website posited TT Rounds, Identifont suggested it could be Divulge, my early guess was DIN Rounded or something related to road signage. Whatever it was, a flat R clearly separated it from Helvetica, and the shapes were not as round as even the un-rounded Gotham’s.\n\t</p><p>\t\t\n\t\tA few places for keyboard nerds referred to the font as “Gorton,” but that phrase yielded zero results anywhere I typically looked for fonts I could download and install.\n\t</p><p>\t\t\t\t\n\t\tI originally thought this had to do with how keys were made. Only in newer keyboards are the letters printed on top of the keys, or charred from their surface by a laser. In older ones – those from the early 1960s laboratory computers, or the 1980s microcomputers –&nbsp;the way every key was constructed was by first molding the letter from plastic of one color, and then grabbing a different plastic and molding the key around the letter. A Gorton letter was as physical as the key itself. It made the keys virtually indestructible – the legend could not wear off any more than its key – and I imagined required some specialized keyboard-making machinery that came with the “keyboard font” already there.\n\t</p><figcaption>\n\t\tAn example of a “double-shot” key from above and from below\n\t</figcaption><p>\t\t\t\n\t\tBut then, I started seeing Gorton in other places.\n\t</p><p>\t\t\t\t\t\n\t\tHours of looking at close-ups of keys made me sensitive to the peculiar shapes of some of its letters. No other font had a Q, a 9, or a C that looked like this.\n\t</p><p>\t\t\t\t\t\n\t\tOne day, I saw what felt like Gorton on a ferry traversing the waters Bay Area. A few weeks later, I spotted it on a sign in a national park. Then on an intercom. On a street lighting access cover. In an elevator. At my dentist’s office. In an alley. \n\t</p><p>\t\t\t\t\t\t\n\t\tThese had one thing in common. All of the letters were carved into the respective base material – metal, plastic, wood. The removed shapes were often filled in with a different color, but sometimes left alone.\n\t</p><p>\t\t\t\t\t\n\t\tAt one point someone explained to me Gorton must have been a routing font, meant to be carved out by a milling machine rather than painted on top or impressed with an inked press.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSome searches quickly led me to George Gorton Machine Co., a Wisconsin-based company which produced various engraving machines. The original model 1 led to model 1A and then 3U and then, half a decade later, P1-2. They were all pantograph engravers: They allowed you to install one or more letter templates and then trace their shape by hand. A matching rotating cutter would mimic your movements, and the specially configured arms would enlarge or reduce the output to the size you wanted.\n\t\t</p><p>\t\t\t\t\t\t\t\n\t\tThis immediately explained both the metaphorical and literal rough edges of Gorton.\n\t</p><p>\t\t\t\t\t\n\t\tA lot of typography has roots in calligraphy – someone holding a brush in their hand and making natural but delicate movements that result in nuanced curves filled with thoughtful interchanges between thin and thick. Most of the fonts you ever saw follow those rules; even the most “mechanical” fonts have surprising humanistic touches if you inspect them close enough.\n\t</p><p>\t\t\t\n\t\tBut not Gorton. Every stroke of Gorton is exactly the same thickness (typographers would call such fonts “monoline”). Every one of its endings is exactly the same rounded point. The italic is merely an oblique, slanted without any extra consideration, and while the condensed version has some changes compared to the regular width, those changes feel almost perfunctory.\n\t</p><p>\t\n\t\tMonoline fonts are not respected highly, because every type designer will tell you: This is not how you design a font. \n\t</p><p>\t\t\n\t\tIt seemed at this point that perhaps P1-2 and its predecessors were a somewhat popular machining product during the 20th century’s middle decades. But casual research through materials preserved by some of George Gorton Machine Company’s fans – including <a target=\"_blank\" href=\"http://gorton-machine.org/\">the grandson of the founder</a> – revealed something even more interesting. Gorton the font was a lot older than I expected. \n\t</p><p>\t\t\n\t\tI found a 1935 catalog showing the very same font. Then one from 1925. And then, there was one all the way from 1902, showing the shapes I was starting to be mildly obsessed with.\n\t</p><p>\t\t\t\n\t\tTo put it in perspective: the font I first assumed was a peer to 1950s Helvetica was already of retirement age the day Helvetica was born. Gorton was older than Gill Sans, Futura, or Johnston’s London Underground font. It was contemporaneous to what today we recognize as the first modern sans serif font, Akzidenz-Grotesk, released but three years before the end of the century.\n\t</p><p>\n\t\tImagine how stripped down and exotic Gorton must have felt right next to George Gorton Machine’s then-current logo!\n\t</p><p>\t\t\t\t\t\n\t\tI started researching Gorton more. Unfortunately, as I already suspected, no one ever wrote “I used Gorton to typeset this,” because Gorton was a tenuous name at best. It was the first font, and perhaps originally the  font that came with the engraver, so it suffered a nameless fate, familiar later to many bespoke bitmap fonts adorning the screens of early computers.\n\t</p><p>\t\t\t\t\t\n\t\tThe difference from these fonts, however, was that Gorton was meant to travel. And so, since searching for it by name was impossible, for months and years I just kept looking around for the now-familiar shapes.\n\t</p><p>\n\t\tGorton wasn’t just on computer keyboards, intercom placards, and sidewalk messages visited by many shoes. Gorton was there on typewriter keyboards, too. And on office signs and airline name tags. On boats, desk placards, rulers, and various home devices from fridges to tape dispensers.\n\t</p><p>\t\t\t\t\t\n\t\tIt was also asked to help in situations other fonts rarely did. I spotted Gorton on overengineered buttons that were put to heavy industrial and military use. I saw it carved into surfaces of traffic control devices, elevators and escalators, locomotives and subway trains, submarines and jet fighters. Gorton made its way to peace- and wartime nuclear facilities, it was there on the elevator at the Kennedy Space Center with labels marked EARTH and SPACE… and it went  and then the Moon, as key legends on Apollo’s onboard computer.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut why? Why would anyone choose this kind of an ugly font where so many nicer fonts have already been around for ages?\n\t</p><p>\t\t\t\t\t\n\t\tSome of it might be the power of the default. Popular engraving equipment comes with a built-in font that’s so standard it reuses the router’s name? Of course you will see it, the same way you saw a lot of Arial in the 1990s, or Calibri today.\n\t</p><p>\n\t\tGorton was also convenient. If your previous engraving work required you do to the routing equivalent of handwriting or lettering – every letter done by hand – then a modern font you could simply  and one designed with “a minimum of sharp corners for rapid tracing with a smooth stroke,” must have felt like a breath of fresh air.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut why engraving to begin with? Because the affordable and casual printing options we enjoy today – the office laser printers and home inkjets, the FedEx Kinko’s, the various cheap labelers – weren’t there. Even things that today feel obsolete, like dot matrix printers, Letraset, and popular letter stencils, were yet to be invented. Often, your only realistic option was the complicated and time-consuming lettering by hand.\n\t</p><p>\t\t\t\n\t\tOn top of that, Gorton’s longevity must have felt attractive. Ink smudges. Paint fades away. Paper can catch fire (quickly) or germs (slowly). Carve something into plastic, on the other hand, and it can survive decades. Substitute plastic for metal, and you just turned decades into centuries. The text is not added atop heavy-duty material. The text  the material.\n\t</p><figcaption>Various items from the 20th century typeset in Gorton</figcaption><p>\t\t\t\t\t\n\t\tI felt good about all my findings: What a strange story of a strange routing font! \n\t</p><p>\n\t\tBut it turns out I was just getting started. Because soon, I noticed Gorton as ink on paper, and as paint on metal.\n\t</p><p>We’re used to the flexibility of fonts today. Fonts as bits inside a computer can become a website, paint on paper, CNC routing, a wall projection, and many other things. But those freedoms weren’t as easy back when fonts were made out of metal. Life’s not as much fun outside of the glamor of a TTF file, and a routing font couldn’t immediately become a regular font – so seeing Gorton being additive and not subtractive was an unexpected discovery.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tIt turns out that there developed a small cottage industry of things that extended Gorton past its engraving origins.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tA company called Keuffel &amp; Esser Co. grabbed Gorton’s machines, and used them to create lettering sets called Leroy. This was Gorton abstracted away – still a pantograph, but cheap, small, completely manual, and a vastly simplified one: no possibility to make things bigger and smaller, and no carving –&nbsp;instead, you’d mount a special pen and draw letters by tracing them.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tAnother company, Wood-Regan Instrument Co., made a similar set called (semi-eponymously) Wrico. But then, they simplified the process even more. Instead of a pantograph, they offered for sale a set of simple lettering guides used to guide your pen directly on paper.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSome of the traditional draftspeople pooh-poohed these inventions – one handbook wrote “[Those are] of value chiefly to those who are not skilled in lettering. A professional show-card writer could work better and faster without it. A Leroy or Wrico lettering set permits work that is neat, rapid, and nearly foolproof, if not inspired.”\n\t</p><p>\t\t\t\n\t\tBut the products ended up being popular and influential. Their output appeared in many technical documents, but spread even a bit further than that. Eventually, there were stencils made by Unitech, Lutz, Tacro, Teledyne Post, Tamaya, Tech Graphic, Ridgway’s, Faber Castell, Zephyr, Charvoz, Rotring, Pickett, and probably many more.\n\t</p><p>\t\t\t\n\t\tThen, both EC Comics and All-Star Comics <a target=\"_blank\" href=\"https://kleinletters.com/Blog/wizards-of-leroy-and-wrico-lettering/\">used Leroy in the 1940s and 1950s</a>, most notably in the first comic book that introduced Wonder Woman. This was Gorton spreading further than just technical documents, and inspiring more people.\n\t</p><p>\t\t\t\n\t\tElsewhere silkscreening – a pretty cool technique of applying paint on surfaces through a wet mesh of fabric – took Gorton and Leroy in a different direction, by allowing paint on metal.\n\t</p><p>\t\t\t\n\t\tThere was more. The popular plastic letters attached to felt boards, popularized by restaurants decades ago, and more recently revisited by Instagram mom influencers, also clearly derive from Gorton and Leroy.\n\t</p><p>\n\t\tI also counted at least three different systems of “Gorton movable type” – some where you could assemble physical letters, and some where you could impress them into soft materials using steel types – and I imagine there were probably more.\n\t</p><p>\t\t\t\n\t\tLetraset, a cheap technique of applying a font by rubbing a letter from a provided sheet onto paper, popular throughout the 1960s, introduced first- or second-hand Leroy too – and so did a few competitors.\n\t</p><p>\t\t\t\t\t\n\t\tIn the regulatory space, the U.S. military canonized Gorton in 1968 as a standard called MIL-SPEC-33558 for aircraft and other equipment dials, cancelled it in 1998… then brought it back again in 2007. NATO and STANAG followed. ANSI, American standardization body, made a more rounded Leroy an official font for technical lettering via <a target=\"_blank\" href=\"https://archive.org/details/ansi-y14.2m-1971-line-conventions-and-lettering\">ANSI Y14.2M</a>, and so did institutions like the US National Park Service.\n\t</p><p>\t\t\t\t\n\t\tGorton went on and on and on. The early <a target=\"_blank\" href=\"https://hackaday.com/2021/03/30/hershey-fonts-not-chocolate-the-origin-of-vector-lettering/\">Hershey vector fonts</a>, developed on very early computers and still popular in CAD applications today, were also derived from Gorton/Leroy shapes, simplified so that the already-simple curves weren’t even necessary –&nbsp;any letter could now be drawn by a series of straight lines.\n\t</p><p>\t\t\t\t\t\n\t\tAnd even in the first universe Gorton inhabited things weren’t standing still. \n\t</p><p>\n\t\tAs the engraving industry learned what’s popular and what is not, the offerings started getting more and more sophisticated. A promotional booklet called “The Whereabouts of 230 Engraving Machines” listed Gorton customers ranging from biscuit makers to fire engine constructors. <a target=\"_blank\" href=\"http://gorton-machine.org/forms/form_933/index.html\">Other</a><a target=\"_blank\" href=\"http://gorton-machine.org/forms/form_1370a/index.html\">catalogs</a><a target=\"_blank\" href=\"http://gorton-machine.org/forms/form_2070/index.html\">proudly listed</a> applications like book covers, billiard balls, organ keys, and toothbrushes, as well as “tools making more tools” – using Gorton engravers to create legends for other machines.\n\t</p><p>\t\t\n\t\tAfter you bought your pantograph engraver, you could buy attachments for sometimes surprising use cases:\n\t</p><p>\n\t\tThe original machine-shop pantographs were supplanted by smaller portable units (called Pantoetchers) on one side, and by increasingly complex  devices on the other. First generation of those were still huge room-size endeavors with Nixie tubes and complex interfaces labeled… in Gorton itself. \n\t</p><p>\n\t\tBut the technology matured quickly and soon more and more early manual “tracer-guided” pantographs that forced the operator to put letters side by side and then trace them by hand, were superseded by <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Numerical_control\">computerized ones</a>, with both the composition and the routing completely automated. They came from George Gorton Machine Co., and from competitors like New Hermes or H.P. Preis.\n\t</p><p>\t\t\t\t\n\t\tYou no longer had to buy the chromium-plated brass alphabets weighing up to 13 pounds, choosing the right size from 3/8´´ to 3´´ ahead of time (pantographs allowed for reductions and enlargements, but only gave you a few steps within a specific range.) \n\t</p><p>\n\t\tNow, fonts came as digits or formulas built into computer memory, or – for a moment in time –&nbsp;as separate cartridges you’d insert in eager slots. (And yes, before you ask:&nbsp;there were <a target=\"_blank\" href=\"https://archive.org/details/gorton-master-copy-type-for-all-pantograph-machines/page/n5/mode/2up\">other routing monoline fonts</a>, too. But I really don’t care about any of them.)\n\t</p><p>\t\t\t\t\t\n\t\tIt was the same story as in word processing right next door, where old-fashioned Gutenberg-era typesetting was being replaced by increasingly smaller and cheaper computers equipped with first-laughable-then-capable software.\n\t</p><p>\t\t\t\t\t\n\t\tAnd automation came for the Leroy branch of the tree as well. A few companies grabbed Leroy lettering templates and abstracted them away once more. They created curious small typewriter/plotter hybrids where typing letters on a keyboard would make the pen draw them on paper for you. (I own one of them, a Max Cadliner. It might be one of the strangest typewriters I’ve seen – a weird combination of a machine pretending to be another machine pretending to be a human hand.)\n\t</p><p>\t\t\t\t\n\t\tIf this was a Gorton typewriter, there were also Gorton , even more sophisticated 1980s machines whose text could be programmed in advance rather than typed one line at a time, and mixed with graphics.\n\t</p><p>\t\t\t\n\t\tI don’t think the – by now 80 years and counting – fractal explosion of Gorton made its original creators rich.\n\t</p><p>\t\t\t\n\t\tCopy protection in the world of typography is complicated. The font’s name can be trademarked and other companies legally prevented from using it, and you can’t just grab matrices or font files and copy them without appropriate licenses. But take any text output using a font and then redraw it – and you are within your right to do so, and even to sell the final result. At least in America, or in some other countries until somewhat recently, the shapes of the letters themselves are not legally protected.\n\t</p><p>\t\t\t\n\t\tThis is why Keuffel &amp; Esser, Wood-Regan Instrument, and Letraset could potentially grab Gorton and claim it their own, as long as they didn’t name it Gorton. \n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut of course, Gorton was barely named “Gorton” to begin with. In the early days of George Gorton pantographs, as the default pantograph font, it came without a name. (The font sets for purchase were called “standard copies.”) Then, as other fonts were added, it was retroactively named Gorton Normal – the name of the company and the most generic word possible.\n\t</p><p>\t\t\t\t\t\n\t\tLeroy lettering sets started with one font, so similarly to Gorton the font started to be known as “Leroy,” then “Series C,” then “Gothic.” New Hermes called it simply “Block,” Letraset went with “Engineering Standard,” and Rotring – another producer of little computerized plotters&nbsp;– with “Universal.” I’ve also seen “A style,” “Plain Gothic,” and, mysteriously, “Standpoint.” \n\t</p><p>\t\t\t\t\t\t\t\n\t\tI don’t think this was meant to be disrespectful. “Standard,“ “Universal,” “A style” might not have had the connotations of “generic” we associate with them today, but rather meaning “the only one you need,” “approved of by millions,” or “the ultimate.”\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut there  one name that felt somewhat inconsiderate. It appeared in one product in the 1980s, a few decades after the birth of another font whose name became recognizable and distinguished. In that product, Gorton was referred to as “Linetica.”\n\t</p><figcaption>A few rare examples of Gorton Extended in use</figcaption><p>\t\t\t\t\t\t\n\t\tEach of these reappearances made small changes to the shapes of some letters. Leroy’s ampersand was a departure from Gorton’s. Others softened the middle of the digit 3, and Wrico got rid of its distinctive shape altogether. Sometimes the tail of the Q got straightened, the other times K cleaned up. Punctuation –&nbsp;commas, quotes, question marks –&nbsp;was almost always redone. But even without hunting down the proof confirming the purchase of a Gorton’s pantograph or a Leroy template set as a starting point, the lineage of its lines was obvious. (The remixes riffed off of Gorton Condensed or the normal, squareish edition… and at times both. The extended version – not that popular to begin with – was often skipped.)\n\t</p><figcaption>Classic Gorton vs. Gorton Modified</figcaption><p>\t\t\t\t\t\t\n\t\tThe only “official” update to Gorton I know of, and one actually graced with a name, was Gorton Modified. It was made some time in the 1970s by one of the main keyboard keycap manufacturers, Comptec (later Signature Plastics). It was almost a fusion of Gorton and Futura, with more rounded letterforms. Gone was the quirkiness of 3, 7, Q, C, and the strange, tired ampersand. This is the version people might recognize from some of the 1980s computers, or mechanical keyboards today. \n\t</p><p>\n\t\tIt is also that last Gorton that mattered.\n\t</p><figcaption>A collection of movies and TV shows featuring Gorton</figcaption><p>\t\t\t\t\t\t\t\n\t\tMy every walk in Chicago or San Francisco was counting down “time to Gorton” – sometimes mere minutes before I saw a placard or an intercom with the familiar font.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tThis might be embarrassing to admit, but I have never been so happy seeing a font in the wild, particularly as there was almost always some new surprise – a numero, a line going through the Z, a new use, or a new imperfection. And, for a font that didn’t exist, I saw it surprisingly often.\n\t</p><p>\t\t\t\t\t\t\t\t\t\n\t\tI even spotted Gorton a few times in Spain, or the U.K., and didn’t make too much of it, not thinking about the likelihood of machines from George Gorton’s company in a small town of Racine, Wisconsin making it all the way to different continents. In hindsight I should have.\n\t</p><figcaption>Gorton on old British cars, with a particularly delightful Rolls Royce logo made by a simple duplication of the classic Gorton letter R</figcaption><p>\t\t\t\t\t\t\n\t\tIt was only on a trip to Australia where something started connecting. Here, once more, I saw Gorton on the streets, put to work in all sorts of unglamorous situations:\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSome letterforms in the above photos felt slightly odd, and so did Gorton on the heavy machinery in an abandoned shipyard on an island near Sydney:\n\t</p><p>\t\t\t\t\t\t\t\n\t\tAnd a visit to a naval museum cemented it all:\n\t</p><p>\n\t\tIt was Gorton, although with some consistent quirks: 2, 5, 6, and 9 were shorter, the centers of M and W didn’t stretch all the way across, and the distinctive shape of S was slightly different here.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tFortunately, this time around, a type designer familiar with my now-public obsession with Gorton clued me in. Gorton didn’t actually originate from Racine, Wisconsin in the late 19th century. It started a bit earlier, and quite a bit further away, at a photographic lens maker in the U.K. called Taylor, Taylor &amp; Hobson. \n\t</p><p>\t\t\t\t\t\t\t\n\t\tIn 1894, TT&amp;H needed some way to put markings on their lenses. This being late 19th century, their options were limited to manual engraving, which must have felt tricky given the small font sizes necessary. So the company did what makers sometimes do – instead of searching for a solution that might not have even existed, they made new types of machines to carve out letters, and then designed a font to be used with them.\n\t</p><p>\n\t\tI don’t know how this first proto-Gorton was designed – unfortunately, Taylor, Taylor &amp; Hobson’s history seems sparse and despite personal travels to U.K. archives, I haven’t found anything interesting – but I know simple technical writing standards existed already, and likely influenced the appearance of the newfangled routing font.\n\t</p><figcaption>From a 1895 “Free-hand lettering” book by Frank T. Daniels</figcaption><p>\t\t\t\t\t\n\t\tThis was perhaps the first modern pantograph engraver, and perhaps even the arrival of a concept of an engraving font – the first time technical writing was able to be replicated consistently via the aid of the machine.\n\t</p><p>\t\t\t\n\t\tNo wonder that other companies came knocking. Only a few years later, still deep within the 19th century, Taylor, Taylor &amp; Hobson <a target=\"_blank\" href=\"http://gorton-machine.org/archives/TTH_license_1898/index.html\">licensed their stuff to a fledgling American company</a> named after its founder. Gorton Model 1 was the first U.S. version of the engraver, and the TT&amp;H font must have been slightly adjusted on arrival. \n\t</p><figcaption>A Taylor-Hobson pantograph in use in 1942</figcaption><p>\t\t\t\n\t\tThis adds to the accomplishments of Gorton – the font was actually  than even Akzidenz-Grotesk, and has been used on World War II equipment and later on on British rifles and motorcycles (and 3,775 finger posts in <a target=\"_blank\" href=\"https://www.yorkshiredales.org.uk/behind-the-signs-the-man-and-the-machine/\">one of the UK’s national parks</a>), but it complicates the story of the name even more. Turns out, the font without a name has even less of a name than I suspected.\n\t</p><p>\t\t\t\n\t\tIf the Taylor, Taylor &amp; Hobson (or, Taylor-Hobson, as their engravers were known) “branch” of Gorton were more used, should it usurp the at least somewhat popular Gorton name? Or should it just because it was first and the letterform changes were small? Does it matter? Where does one font end and another begin? (Unsurprisingly, TT&amp;H didn’t properly name the font either, eventually calling it “A style” for regular and “C style” for condensed variants. Google searches for “taylor hobson font” are a lot more sparse than those for Gorton.)  \n\t</p><div><div><div>GortonGorton Condensed</div></div></div><figcaption>The Gorton quasisuperfamily</figcaption><p>\n\t\tIn the end, I’m sticking with Gorton for the whole branch since that feels the most well-known name, but I feel ill-equipped to make that call for everyone. You might choose to call it Gorton, Leroy, TT&amp;H, Taylor-Hobson, or one of the many other names. (Just, ideally, not Linetica.)\n\t</p><figcaption>A comparison of all major editions of Gorton</figcaption><p>\t\t\t\t\t\n\t\tAnd so, throughout the 20th century, Gorton has lived two parallel lives –&nbsp;one originating in the U.K. and later expanding to its colonies and the rest of Europe, and another one in America. \n\t</p><p>\t\t\t\t\t\t\t\n\t\tI am still tracing various appearences of Gorton and perhaps you, dear reader, will help me with that. (<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Frequency_illusion\">Chances are</a>, you will see Gorton later today!) I’m curious about whether Gorton made it to Eastern Europe, Africa, or Asia. I’m interested in seeing if it appeared in Germany where the objectively better-designed DIN fonts became much more popular in Gorton’s niche.\n\t</p><p>\n\t\tThe history of this strange font spans over a century and I’ve seen it in so many countries by now, used in so many situations. But it’s impossible for me to say Gorton is the most hard-working font in the world.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tTo this title, there are many contenders. Garamond has a head start of 300+ years and has been released in more versions than letters in any alphabet. Helvetica is so famous and used so much that even its ugly copy, Arial, became a household name. Whatever font MS Office or a popular operating system appoint to be “the default” – from Times New Roman through Calibri to Roboto – immediately enjoys the world premiere that any Hollywood movie would be envious of. There is even a 5×7 pixel font originally started by Hitachi that you can see everywhere on cheap electronic displays in cash registers and intercoms.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut there is one place in the world where Gorton pulls triple duty, and I feel confident in saying at least this: Gorton is the hardest working font in Manhattan.\n\t</p><p>\t\t\t\t\t\t\n\t\tIn 2007, on my first trip to New York City, I grabbed my brand-new DSLR camera and photographed all the fonts I was supposed to love: American Typewriter, Helvetica, Gotham. But, in hindsight, I missed the most obvious one.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tGorton is everywhere in Manhattan. It’s there in the elevators, in the subway, on ambulances, in various plaques outside and inside buildings. And god knows it’s there on so, so many intercoms.\n\t</p><p>\t\t\t\t\t\n\t\tI wouldn’t be surprised if there weren’t a single block without any Gorton in a whole of Manhattan.\n\t</p><figcaption>A complete inventory of Gorton outside, near my hotel, between 5th and 7th avenues and 25th and 35th streets. I didn’t have access to the interiors of most buildings.</figcaption><p>\t\n\t\tThe omnipresence of Gorton makes it easy to collect all the type crimes layered on top of the font’s already dubious typographical origins. Walking through Manhattan, you can spot the abominable lowercase that should better be forgotten:\n\t</p><p>\t\t\t\t\t\t\t\n\t\tYou can see all sorts of kerning mistakes:\n\t</p><p>\t\t\t\t\t\t\t\t\t\n\t\tYou will notice the many, many routing imperfections – an unfinished stroke, a shaky hand, or services of a pantograph that never felt the loving touch of regular maintenance:\n\t</p><p>\n\t\tThere are all the strange decisions to haphazardly mix various styles of Gorton, or even to mix Gorton with other fonts:\n\t</p><p>\t\t\t\t\t\t\t\t\t\t\t\n\t\tYou can even spot reappearing strange characters like a weirdly deep 3, or a flattened 4:\n\t</p><p>\n    I wish I understood how they came to be, but I have a hunch. The nature of pantographic reproduction is that Gorton carved into metal is not that far away from the original Gorton font template you started with! So in addition to the George Gorton and Taylor Hobson originals, and the other named and above-the-table copies, they might have been bigger or smaller Gorton . I have one myself, carved into acrylic, of unknown provenance and even more nameless than I thought possible for an already name-free font.\n  </p><p>\n\t\tBut New York Gorton holds pleasant surprises, too. Despite the simplicity of Gorton itself, the combinations of font sizes, cutter sizes, materials, reproductions, and applications can still yield some striking effects:\n\t\n\t</p><figcaption>\n\t\t\tAll my Gorton walks in Manhattan in 2024\n\t\t</figcaption>\n\n\t\tThis was what made me walk 100 miles. Over and over again, Gorton found ways to make itself interesting. Without hyperbole, I consider the above photos simply beautiful.\n\t<p>\n\t\tIn a city that never sleeps, Gorton wasn’t allowed to sleep, either. Even in the richest and most glamorous neighborhoods of Manhattan, the font would be there, doing the devil’s work without complaining. Gorton made Gotham feel bougie; American Typewriter touristy.\n\t</p><p>\n\t\tAnd once in a while, I’d find Gorton that would wink at me with a story –&nbsp;followed by that aching in the heart as I realized I’d never know what the story was.\n\t</p><p>\t\t\t\n\t\tYou’re not supposed to fall in love with an ugly font. No one collects specimens of Arial. No one gets into eBay fights for artifacts set in Papyrus. No one walks a hundred miles in a hot New York summer, sweating beyond imagination, getting shouted at by security guys, to capture photos of Comic Sans.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSo why do I love Gorton so much? \n\t</p><p>\t\t\t\t\t\t\t\n\t\tThe Occam’s Razor seems sharp on this one. Perhaps I like it because I’m a boy and Gorton is often attached to heavy machinery. \n\t</p><p>\t\t\t\t\n\t\tBut there must be more to it. Perhaps it’s all about the strange contrasts Gorton represents. The font is so ubiquitous, but also profoundly unrecognizable, sporting no designer and no name. Gorton is a decidedly toy-like, amateurish font deployed to for some of the most challenging type jobs: nuclear reactors, power plants, spacecraft. More than most other fonts, Gorton feels it’s been made by machines for machines –&nbsp;but in its use, it’s also the font that allows you to see so many human mistakes and imperfections.\n\t</p><p>\t\t\t\t\n\t\tGorton also feels mistake-friendly. The strange limitations of Gorton mean that some of the transgressions of other fonts don’t apply here. The monoline nature of the font means that messing with the size of Gorton is okay: Shrinking the font for small caps or superscript, for example, gives you still-valid letterforms, almost by accident. \n\t</p><p>\n\t\tStretching or slanting Gorton is not as much a typographical crime as it would be with other fonts because you don’t stretch the tip of the router itself.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tThere are genuinely moments where I felt Gorton gave people freedoms to maul it decades before variable fonts allowed us similar flexibility.\n\t\tAnd on top of that, the simplicity of the letterforms themselves feels compatible with the typical naïveté of Gorton’s typesetting. \n\t</p><figcaption>Various accessories and attachments allowing you to shift Gorton around in a way other fonts would not allow</figcaption><p>\n    Sure, there are really bad renditions that are inexcusable. \n\n\t\tBut most of the time, the imperfections and bad decisions are what makes Gorton come alive. They don’t feel like a profound misunderstandings of typography, typesetting, or Gorton itself. They don’t feel like abuses or aberrations. No, they feel exactly how Gorton was supposed to be used – haphazardly, without much care, to solve a problem and walk away. (Later routing fonts copied Helvetica, but seeing Helvetica in this context with all the same mistakes grates so much more.)\n\t</p><p>\t\t\t\n\t\tThe transgressions are not really transgressions. They all feel honest. The font and its siblings just show up to work without pretense, without ego, without even sporting a nametag. Gorton isn’t meant to be admired, celebrated, treasured. It’s meant to do some hard work and never take credit for it. Gorton feels like it was always a font, and never a typeface. (Depending on how rigid you are with your definitions, some versions of Gorton – especially those without instructions on how letters are positioned against each other – might not even <a target=\"_blank\" href=\"https://mastodon.design/@fhardwig@mastodon.social/113515144112560218\">classify as a font</a>!)\n\t</p><p>\t\t\t\t\t\n\t\tAnd I think I love Gorton because over the years I grew a little tired of the ultra flat displays rendering miniature pixels with immaculate precision. \t\n\t\tWith Gorton, carving into metal or plastic means good-looking fixes are impossible:\n\t</p><p>\n\t\tAnd unsurprising given its roots, Gorton has dimensionality that most fonts cannot ever enjoy: A routing tip picked in the 1980s and a sun coming in from just the right angle forty years later can create a moment that thousands of letterpress cards can only dream of.\n\t</p><p>\n\t\tPerhaps above everything else, Gorton is all about . \n  </p><p>\n    Every kind of engraving has it, of course. But these are not precise submillimeter letters at the bottom of your MacBook Pro or Apple Watch. This is the utilitarian, often harried, sometimes downright  Gorton, carved into steel of a  \n\t\tmid-century intercom and filled in with <a target=\"_blank\" href=\"https://youtu.be/llzdLgMurvw?si=8S7px9gg8iH4iav2&amp;t=101\">special paste or wax</a>, or put on an office placard made out of a special two-layer material made especially so engraving it reveals the second color underneath, without the need for infill. \n\t</p><p>\t\t\t\n\t\t(This is also true when it comes to the original reason I learned of Gorton. Letters on keycaps show the same artifacts – you just have to look very, very closely.)\n\t</p><p>\n\t\tThat’s the last, and perhaps the best thing to fall in love with. \n\t</p><p>\n\t\tYou won’t be able to fully appreciate it here, of course, but maybe this will give an approximation of how beautiful Gorton’s non-beauty can be:\n\t</p><p>\t\t\t\t\t\n\t\tThis has been a strange thing to write. Gorton has been around for over 135 years and used in so many countries for so many reasons, and yet I found no single article about it. \n\t</p><p>\t\t\t\t\t\n\t\tI feel the burden of being an amateur historian, wanting to know and share so much more, but only being able to provide little. I don’t know the full extent of Gorton’s use. I don’t know who designed it. My chronology is rickety and pieced together from a few breadcrumbs. I dream of seeing the original drawings or drafts once laid on the tables of Taylor, Taylor &amp; Hobson offices, or some notes, or some correspondence. I fear they might no longer exist.\n\t</p><p>\t\t\t\t\t\n\t\tAlso, if part of the allure of Gorton is shying away from the limelight and not being admired, am I doing it a disservice by writing about it?\n\t</p><p>\t\t\t\t\t\n\t\tBut mostly, I can’t shake the feeling that we all missed a window. That this essay can’t be just a celebration, but also needs to be the beginnings of a eulogy.\n\t</p><p>\t\t\t\t\t\n\t\tWalking around New York, you get a sense that even Gorton carved into metal can disappear. Some of the signs are rusted or destroyed beyond repair. Others get replaced by more modern, charmless equivalents.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tGorton itself is obsolete. All <a target=\"_blank\" href=\"https://spkeyboards.com/\">the keyboards that use Gorton Modified</a> you can still buy new today are tipping a hat to nostalgia. The omnipresence of Gorton in New York City is already time shifted from its decades of glory, a simple confirmation of what Robert Moses knew so well: that once built, cities don’t change all that much. But few of the new placards use Gorton, and none of the new intercoms do. \n\t</p><p>\n\t\tTaylor, Taylor &amp; Hobson went through multiple splits and mergers and survives as a subsidiary of Ametek, chiefly working on measuring devices. George Gorton Machine Co. from Racine has been bought by Kearny &amp; Trecker, which became Cross &amp; Trecker, was acquired by Giddings &amp; Lewis, and then acquired  by ThyssenKrupp, but not before the Gorton branch was spun off as Lars, and in a sequence of events now resembling a telenovella, eventually bought by Famco in 1987. I do not believe any corporate grandchildren of TT&amp;H and George Gorton’s company are today selling Gorton in any capacity.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tIt will take decades, perhaps even centuries, but one day the last of this font will be gone. The modern recreations (<a target=\"_blank\" href=\"https://aresluna.org/the-hardest-working-font-in-manhattan/the-hardest-working-font-in-manhattan/recreations\">I eventually found quite a few</a>) won’t help. They are perhaps all missing a point, anyway.\n\t</p><p>\n\t\tBut there’s a somewhat silver lining. Yes, when Gorton is carved into fresh metal, there might be nothing more pretty than seeing its depths glistening in the sun.\n\t</p><p>\n\t\tBut fresh, shining metal is at this point rare. Fortunately, the Gorton I love most is the weathered Gorton.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tManhattan’s tired Gorton is the best variant of Gorton: infill cracked by hot summers followed by frigid winters, the surface scratched by keys or worn out by many finger presses, the routing snafus meeting decades of wear and tear. Gorton’s no stranger to water, snow, rust, or dirt.\n\t</p><p>\t\t\n\t\tThis is, perhaps, how you become gortonpilled. You learn to recognize the 7 with a crooked hook, the Q with a swung dash, the strange top-heavy 3, the simple R. You start noticing the endings of each character being consistently circular, rather than occasionally flat. A routing mistake, suspicious kerning, or the absence of lowercase are not a wrongdoing – they’re a .\n\t</p><p>\t\t\t\t\t\t\t\n\t\tYou find yourself enchanted with how this simple font went so very far. And then you touch the letters, just to be sure. If you can  them, chances are this is Gorton.\t\t\n\t</p>","contentLength":38054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43053419"},{"title":"Show HN: Transform your codebase into a single Markdown doc for feeding into AI","url":"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html","date":1739539403,"author":"tesserato","guid":245,"unread":true,"content":"<p>CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.\nThe output for the current repository can be found <a href=\"https://github.com/tesserato/CodeWeaver/blob/main/codebase.md\">here</a>.</p><ul><li><strong>Comprehensive Codebase Documentation:</strong> Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format.</li><li> Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.</li><li>  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types).</li><li> Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.</li><li><strong>Simple Command-Line Interface:</strong>  Offers an intuitive command-line interface with straightforward options for customization.</li></ul><p>If you have Go installed, run <code>go install github.com/tesserato/CodeWeaver@latest</code>to install the latest version of CodeWeaver or <code>go install github.com/tesserato/CodeWeaver@vX.Y.Z</code> to install a specific version.</p><p>Alternatively, download the appropriate pre built executable from the <a href=\"https://github.com/tesserato/CodeWeaver/releases\">releases page</a>.</p><p>If necessary, make the  executable by using the  command:</p><table><thead><tr></tr></thead><tbody><tr><td>The root directory to scan and document.</td></tr><tr><td>The name of the output Markdown file.</td></tr><tr><td><code>-ignore \"&lt;regex patterns&gt;\"</code></td><td>Comma-separated list of regular expression patterns for paths to exclude.</td></tr><tr><td><code>-included-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were included in the documentation.</td></tr><tr><td><code>-excluded-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were excluded from the documentation.</td></tr><tr><td>Display this help message and exit.</td></tr></tbody></table><h2><strong>Generate documentation for the current directory:</strong></h2><p>This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).</p><h2><strong>Specify a different input directory and output file:</strong></h2><pre><code>./codeweaver -dir=my_project -output=project_docs.md\n</code></pre><p>This command will process the  directory and save the documentation to .</p><h2><strong>Ignore specific file types and directories:</strong></h2><pre><code>./codeweaver -ignore=\"\\.log,temp,build\" -output=detailed_docs.md\n</code></pre><p>This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.</p><h2><strong>Save lists of included and excluded paths:</strong></h2><pre><code>./codeweaver -ignore=\"node_modules\" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md\n</code></pre><p>This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .</p><p>Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.</p><p>CodeWeaver is released under the <a href=\"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/LICENSE\">MIT License</a>. See the  file for complete license details.</p>","contentLength":3311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43048027"},{"title":"Show HN: A New Way to Learn Languages","url":"https://www.langturbo.com/","date":1739534938,"author":"sebnun","guid":244,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43047554"},{"title":"Show HN: SQL Noir – Learn SQL by solving crimes","url":"https://www.sqlnoir.com/","date":1739483356,"author":"chrisBHappy","guid":243,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43041827"},{"title":"Show HN: I made my own OS from scratch because I was bored","url":"https://jotalea.com.ar/misc/jotaleaos/","date":1739393712,"author":"Jotalea","guid":242,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43029686"},{"title":"Show HN: yknotify – Notify when YubiKey needs touch on macOS","url":"https://github.com/noperator/yknotify","date":1739391899,"author":"noperator","guid":241,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43029385"},{"title":"Show HN: A no-build fullstack SSR TypeScript web framework","url":"https://jsr.io/@fullsoak/fullsoak","date":1739390092,"author":"thesephi","guid":240,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43029089"},{"title":"Show HN: Game Bub – open-source FPGA retro emulation handheld","url":"https://eli.lipsitz.net/posts/introducing-gamebub/","date":1739380285,"author":"elipsitz","guid":239,"unread":true,"content":"<p>I’m excited to announce the project I’ve been working on for the last year and a half: , an open-source FPGA based retro emulation handheld, with support for Game Boy, Game Boy Color, and Game Boy Advance games.</p><p>Game Bub can play physical cartridges, as well as emulated cartridges using ROM files loaded from a microSD card. Game Bub also supports the <a href=\"https://en.wikipedia.org/wiki/Game_Link_Cable\">Game Link Cable</a> in both GB and GBA modes for multiplayer games. I designed the hardware with a number of bonus features, like video out (HDMI) via a custom dock, a rumble motor, real-time clock (for certain games). Additionally, the hardware is designed with extensibility in mind, allowing future software improvements to expand its capabilities.</p><p>Game Bub has a custom-designed 6 layer PCB featuring a Xilinx XC7A100T FPGA with integrated memory,  display, speakers, rechargable battery, GB/GBA cartridge slot, all packaged up in a custom 3D-printed enclosure.</p><p>Check out the instructions, code, and design files <a href=\"https://github.com/elipsitz/gamebub\">on GitHub</a>. Note that building a Game Bub unit is fairly complex. If you might be interested in buying a complete Game Bub kit, please <a href=\"https://forms.gle/m1FFUqpCde7x5u5AA\">fill out this form</a> to help me gauge interest.</p><p>I had a lot of fun implementing a Game Boy at the hardware level, and I started thinking about how far I could take the project. I was using a Pynq-Z2 development board, which was definitely the right way to get started, but it came with a lot of limitations.</p><p>I had to use an external monitor for audio/video, and an external gamepad for input, but a real Game Boy, of course, is a portable handheld. I also wanted to add Game Boy Advance support, but the memory architecture of the Pynq-Z2 had access latency that <a href=\"https://eli.lipsitz.net/posts/fpga-gameboy-emulator/#fixing-an-audio-bug-on-the-game-boy-color\">was just barely acceptable for the Game Boy</a>, and would have been completely unacceptable for the Game Boy Advance. I also wanted to make something less “hacky”: a real device that I could play and give to people, not just a bare PCB.</p><p>Furthermore, while there are open-source FPGA retrogaming projects (e.g. <a href=\"https://en.wikipedia.org/wiki/MiSTer\">MiSTer</a>), there doesn’t appear to be anything open-source that supports physical Game Boy and Game Boy Advance cartridges, let alone an open-source handheld device.</p><p>Thus, I somewhat naively set out to design what would become by far my most complex electrical engineering and hardware design project to date.</p><p>I set out some goals for the project:</p><ul><li>Build a standalone, rechargable battery-powered FPGA handheld</li><li>Minimize cost and complexity by using off-the-shelf components wherever possible</li><li>Capable of playing Game Boy, Game Boy Color, and Game Boy Advance games</li><li>Capable of using physical cartridges, or emulating cartridges (reading ROM files off of a microSD card)</li><li>Easy to use: graphical menu and in-game overlay</li><li>Integrated display and speakers, with headphone support</li><li>Integrated peripherals (rumble, real-time clock, accelerometer) for emulated cartridges</li><li>HDMI video output support for playing on a big screen</li><li>Decent looking design with good ergonomics</li><li>Expansion opportunities in the future: support for more systems, Wi-Fi, etc.</li></ul><p>And finally, since I was building this project for fun and learning, I wanted to be able to fully understand every single component of the system. I wanted to use my own emulator cores (e.g. not just port them from <a href=\"https://mister-devel.github.io/MkDocs_MiSTer/\">MiSTer</a>), do my own board design, and write my own drivers to interface with peripherals.</p><h3>A brief rant about FPGA retrogaming<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#a-brief-rant-about-fpga-retrogaming\">#</a></h3><p>There’s a lot of misleading marketing and hype out there around FPGA retrogaming. Some claim that FPGA retrogaming devices are not emulators (because they supposedly “act like [the system] at the gate level”), that they achieve “perfect accuracy”, or that they’re superior to software emulators.</p><p>In my opinion, this is blatantly wrong and actively harmful. FPGA retrogaming devices are emulators: they pretend to be something they’re not. And they’re only as accurate as they’re programmed to be, since they’re recreations. An FPGA can make certain aspects of accuracy easier to achieve, but it doesn’t guarantee it.</p><p>Software emulators can be extremely accurate. Furthermore, perfect accuracy (if it’s even possible) is by no means a requirement to play an entire system’s library of games. Some people claim that FPGA emulators are the only way to “preserve” a system, but I’d argue that software emulators are a significantly more accessible (no special hardware needed!) way to further this goal.</p><p>I believe that FPGA emulators have only one real advantage over software emulators: they can more easily interface with original hardware, such as physical cartridges or other consoles via link cables.</p><p>I did this project not because I think that FPGA emulators are inherently better than software emulators, but because I think they’re interesting and fun to build.</p><p>I began work on the project by doing some initial research and sketching out a high level design.</p><p>My previous FPGA emulator project used a Xilinx Zynq chip, which integrates FPGA fabric (“PL”) with a dual-core ARM processor running Linux (“PS”). I implemented the entire emulator on the FPGA, and used the Linux system to configure the FPGA, render the UI, and load ROM files from the filesystem.</p><p>I decided to keep this same division of responsibilities: using the FPGA to do the core emulation, with a separate processor to do support tasks. However, to make the overall design easier to reason about, I decided to to use an FPGA-only chip (without any hard processor cores), and an external microcontroller (MCU) to do the tasks that the ARM cores did before.</p><p>The FPGA would consume input, directly interface to the game cartridges (through level shifters to support both the 3.3 volt GBA and 5 volt Game Boy), and output audio and video to the speakers and display. The MCU would handle the UI, read ROM files from the microSD card, initialize peripherals (display, DAC, IMU), handle power sequencing, and load the FPGA configuration.</p><p>I wanted to have Wi-Fi and Bluetooth support: Wi-Fi for software updates, and the possibility of emulating the <a href=\"https://en.wikipedia.org/wiki/Game_Boy_Advance_Wireless_Adapter\">Game Boy Advance Wireless Adapter</a>, and Bluetooth to support wireless game controllers (when connected to an external display). To reduce complexity (and avoid the need for careful RF design), I looked only for complete Wi-Fi/Bluetooth modules with integrated antennas.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-block-diagram.svg\" alt=\"An early block diagram I sketched out\"><figcaption><p>An early block diagram I sketched out</p></figcaption></figure><p>I also drew out rough sketches of what the final device might look like: placement of buttons, screen, speakers, ports, cartridge slot, and battery. I settled on a vertical Game Boy Color-esque design (as opposed to a horizontal Game Boy Advance-style design), because I felt that this would maximize the space in the back of the device for full-size Game Boy Color cartridges and a battery.</p><p>After sketching out the goals and high level design, I started component selection: picking out each non-trivial component of the system, evaluating features and requirements (e.g. how they communicate, power consumption and voltages needed).</p><p>Since I intended to have this manufactured and assembled at JLCPCB, I strongly preferred parts that were available in their part library. One technique I even used for narrowing down part choices was finding the relevant category in their part search, and sorting by their stock count.</p><p>I initially planned to use an <a href=\"https://en.wikipedia.org/wiki/RP2040\">RP2040</a> microcontroller, with a separate ESP32-WROOM module to support Wi-Fi and Bluetooth.</p><p>The ESP32 supports both Bluetooth Classic and LE, which is essential for supporting a wide range of controllers, and the RP2040 has USB host support, to support wired controllers.</p><p>During the schematic design process, I ended up simplifying the RP2040 + ESP32 combination to just a single ESP32-S3 module for a few reasons:</p><ul><li>I started running out of GPIOs on the RP2040, and I was dedicating 4 of them (2 for UART, 1 for reset, 1 for booting in firmware download mode) to communication with the ESP32. Plus, the ESP32-S3 has more GPIOs overall.</li><li>I wanted to write the MCU firmware in Rust, and the ESP32-S3 had support for the Rust standard library (via ESP-IDF and <a href=\"https://github.com/esp-rs/esp-idf-hal\">esp-idf-hal</a>). This seemed like it would be easier to get the software up and running.</li><li>Fewer components means easier routing and assembly</li><li>The ESP32-S3 has an SDIO module (for interfacing with the microSD card), and FAT filesystem support (via ESP-IDF). It would be possible to do this with the RP2040 PIO, but having a proper peripheral and driver for this makes it a lot easier.</li><li>The ESP32-S3 is more powerful than the RP2040, and would probably be able to render a smoother UI.</li></ul><p>However, the ESP32-S3 has one main disadvantage compared to the original ESP32: it doesn’t have Bluetooth Classic support, only LE. This would greatly limit the range of supported wireless controllers, but I believed the compromise was worth it. I also decided to scrap USB host support, because supporting USB-C dual role (switchable device or host) would have added a lot of additional complexity.</p><p>If the RP2350 microcontroller (the successor to the RP2040) had been available when I started this project, I may very well have chosen it, since it has even more power, PIO blocks, memory, and GPIO pins. I might have paired it with an RM2 radio module for Wi-Fi and Bluetooth.</p><p>I wanted a display that would support integer scaling for the Game Boy Advance, which has a 240x160 pixel screen. I was also looking for a screen roughly on the order of 3.0-3.5 inches wide (diagonal), to be comfortable to hold in the hand.</p><p>I found the ER-TFT035IPS-6 LCD module from <a href=\"https://www.buydisplay.com/\">EastRising</a>, with a 3.5 inch display, and a 320x480 pixel resolution. This allows for a 2x integer scale for the Game Boy Advance (and a 2x scale plus centering for the 160x144 Game Boy display). This checked off almost all of the boxes: integer scaling, a good size, available at a reasonable price, pretty good documentation (for the ILI9488 LCD controller).</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/er-tft035ips-6.jpg\" alt=\"ER-TFT035IPS-6 LCD module\" width=\"1000\" height=\"550\"><figcaption><p>ER-TFT035IPS-6 LCD module</p></figcaption></figure><p>The main issue, which actually ended up being fairly annoying, is that it’s a 320x480 display, not 480x320. Meaning, it’s oriented in portrait mode, not landscape. I rotated the device 90 degrees to fit in a landscape orientation, but this created two issues:</p><ul><li>In landscape orientation, the bottom of the display (containing the LCD driver chip and the flex cable) faces to the left or the right, which means that larger bazels are required on the left and right of the display to center the “active area” of the LCD within the handheld.</li><li>In landscape orientation, the display refreshes from left to right, not top to bottom.</li></ul><p>The problem with refreshing from left to right is that the Game Boy and Game Boy Advance (and almost every other system) refresh from top to bottom. This means that the display can’t be refreshed perfectly in sync with the game (zero buffering), and single buffering leads to unsightly diagonal tearing. Instead, I had to use triple buffering, where the game is writing to one framebuffer, the LCD driver is reading from another buffer, and there’s one spare swap buffer. This increases the amount of memory used – and because it needed to be accessed by both the game and LCD driver simultaneously (dual port), it needed to be stored in internal block RAM in the FPGA, a scarce resource.</p><p>So, even though the Game Boy emulator uses &lt;10% of the total logic resources of the FPGA, and the Game Boy Advance uses around 30%, I had to use a large (more expensive, and power hungry) FPGA so that I had enough block RAM.</p><p>I also stuck a standard size HDMI port into the design, connected directly to the FPGA. HDMI has a few additional, non-video signals that need level shifting from 5V to 3.3V (I opted for discrete transistors), and it requires the source (me!) to supply a small amount of power.</p><p>I had never previously designed anything that used a lithium ion battery, so I had a fair amount of learning to do. <a href=\"https://learn.adafruit.com/li-ion-and-lipoly-batteries/overview\">Adafruit</a> was a helpful resource. I needed a way to charge the battery from USB power, and a way to measure how charged it is.</p><p>Lithium ion batteries can be dangerous if misused. Safely charging a battery is non-trivial, and requires a feedback loop and adjustable voltage sources. A dedicated IC seemed like the best way to do this. A lot of hobbyists use the ultra-cheap TP4056 1A battery charger, but I’d read about a lot of issues it has around safely charging the battery while using it. I decided instead to opt for the <a href=\"https://www.ti.com/lit/ds/symlink/bq24073.pdf\">TI BQ2407x</a> series of battery charger ICs. They seem to be widely used in commercial products, came with a comprehensive datasheet, and had a few critical features: programmable input and charge current limits, safety timers, and “power path management” for safely charging the battery while the device is on.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/lipo-discharge-curve.png\" alt=\"Typical discharge curve for a 3.7V lipo battery (source: Adafruit)\" width=\"579\" height=\"402\"><figcaption><p>Typical discharge curve for a 3.7V lipo battery (source: <a href=\"https://learn.adafruit.com/assets/979\">Adafruit</a>)</p></figcaption></figure><p>There are a few ways to measure the charge level of the battery, which generally relies on the fact that a lithium ion battery’s voltage depends on its charge level. A fully charged battery is about 4.2 volts, a battery with between 80% and 20% charge is about 3.7 volts, and below that a drained battery falls off pretty quickly to under 3.0 volts. If all you want is a coarse estimate of the battery level, you can use an ADC to read the voltage and estimate whether the battery is fully charged or nearly discharged. However, since the voltage curve is nearly flat between 20% and 80% charge (and is also dependent on the load), this can’t give the fine-grained battery percentage that we’re used to on phones and laptops. Instead, I opted for a discrete fuel gauge IC, the <a href=\"https://www.analog.com/en/products/max17048.html\">MAX17048</a>. It’s simple to integrate and inexpensive.</p><p>I decided to use a push button for the main power switch, because I needed to be able to do a graceful shutdown, where the microcontroller could save state (e.g. the current save file for an emulated cartridge) before it actually powered off.</p><p>I briefly considered using an ultra-low power, always on microcontroller to act as a custom <a href=\"https://en.wikipedia.org/wiki/Power_management_integrated_circuit\">PMIC</a> to provide power switch functionality (and perhaps avoid the need for a separate real-time clock IC, and even a battery gauge). While this would have been flexible and really cool, I figured it wasn’t worth the additional complexity.</p><p>The main system power ranges from about 3.4 V when the battery is discharged, to 4.2 V when the battery is fully charged, up to 5.0 V when the device is plugged in with USB.</p><p>The ESP32-S3 module required 3.3 V, and most of the other ICs in the system did too. The main exception is the FPGA, which requires a 1.0 V core power rail, a 1.8 V “auxiliary” power rail, and a 3.3 V power rail for I/O. Moreover, according to the <a href=\"https://docs.amd.com/v/u/en-US/ds181_Artix_7_Data_Sheet\">Xilinx Artix-7 datasheet (DS181)</a>, these power rails need to be powered on in a particular sequence: for my use, this means 1.0 V, then 1.8 V, then 3.3 V. Additionally, I needed a 5.0 V supply to interface with Game Boy / Game Boy Color cartridges.</p><p>There are multi-rail power regulators available, and a lot of FPGA development boards use them. However, they all seemed to be expensive and difficult to purchase in low quantities. Instead, I opted for separate power regulators for each rail. I used <a href=\"https://en.wikipedia.org/wiki/Buck_converter\">buck converters</a> instead of <a href=\"https://en.wikipedia.org/wiki/Linear_regulator\">linear regulators</a> to maximize power efficiency.</p><p>I used the <a href=\"https://www.ti.com/product/TLV62585\">TLV62585</a> converter for the 3.3 V, 1.8 V, and 1.0 V rails. This is a simple, performant buck converter with a “power good” output, which is useful for power sequencing: you can connect the  output of one regulator to the  pin of the next regulator, to power on the rails in the desired order.</p><p>For the 5.0 V rail, I used the <a href=\"https://www.ti.com/product/TPS61022\">TPS61022</a> boost converter. This converter is way overkill for the 5.0 V rail (which might use 75mA ), but it was readily available, and conveniently compatible with the same 1µH inductor as the buck converters.</p><p>According to the FPGA datasheet, the XC7A100T consumes more than 100mW of static power. That is, it consumes that as long as it’s connected to power, even if it’s doing absolutely nothing. I figured I might want to support a low power sleep mode, so I decided to split the FPGA into a separate power domain with an explicit power enable signal from the MCU. I also used an <a href=\"https://www.diodes.com/datasheet/download/AP2191.pdf\">AP2191W</a> load switch for the FPGA’s 3.3 V rail to be able to keep the 1.0 V → 1.8 V → 3.3 V sequencing.</p><p>I wanted the device to have both speakers and a 3.5mm headphone jack. Ultimately, the FPGA generates an <a href=\"https://en.wikipedia.org/wiki/I%C2%B2S\">I2S</a> digital audio signal, and I needed a <a href=\"https://en.wikipedia.org/wiki/Digital-to-analog_converter\">DAC</a> to convert it to an analog audio signal, and then an amplifier to drive the speakers (or headphones). I wanted digital volume control (to support volume buttons, rather than a volume knob or slider), and I needed some way to switch the audio output between speakers and the headphones, depending on whether or not headphones are plugged in. With no real audio experience, this seemed like a daunting task.</p><p>While searching for multiple separate components, I stumbled upon the <a href=\"https://www.ti.com/product/TLV320DAC3101\">TLV320DAC3101</a>. It combines a stereo DAC with a speaker amplifier and a headphone driver. Additionally, it supports digital volume control, and headphone detection. I think this chip is a good example of how thoughtful component selection can simplify the overall design. Looking through the datasheet, it required a 1.8 V core voltage (unlike essentially every other component other than the FPGA) and a fair amount of configuration registers to set over I2C, but it had all of the features I needed.</p><p>I was originally planning to have just a single (mono) speaker, but I figured if I had a stereo DAC, I might as well put two in there. I chose the <a href=\"https://www.sameskydevices.com/product/audio/speakers/miniature-(10-mm~40-mm)/ces-20134-088pmb\">CES-20134-088PMB</a>, an enclosed microspeaker with a JST-SH connector. Having an enclosed speaker simplified audio design, because as it turns out, you can’t just stick a speaker to a board and expect it to sound okay (Same Sky, the manufacturer of that speaker, <a href=\"https://www.sameskydevices.com/blog/how-to-design-a-micro-speaker-enclosure\">has a blog post explaining some of the nuances</a>).</p><p>I prefer the feeling of clicky, tactile buttons (such as those found in the GBA SP, Nintendo DS (original), Nintendo 3DS, Switch) compared to “mushy” membrane buttons (such as those found in the Game Boy Color, original GBA, and Nintendo DS Lite). I learned that the tactile switches used in the GBA SP are a <a href=\"https://tech.alpsalpine.com/e/products/detail/SKRRAAE010/\">widely available off-the-shelf part from Alps Alpine</a>. I used similar, but smaller buttons for the Start/Select/Home buttons, and a right-angle button from the same manufacturer for side volume and power buttons.</p><p>Although I only had plans to support Game Boy and Game Boy Advance (requiring a D-pad, A and B buttons, L and R shoulder buttons, and Start/Select), I opted to add two more “X” and “Y” face buttons to leave the possibility open of supporting more systems in the future.</p><p>The L and R buttons posed an additional challenge – I found numerous right-angle tactile buttons (to be soldered onto the back, facing towards the top). However, none of them seemed to have the actuator (the part of the button you make contact with) far enough away from the PCB to be easily pressed. At first, I thought about making a separate shoulder button board to move them at the correct distance, but then I started looking at what existing devices do for inspiration. The Game Boy Advance SP actually uses a more complex mechanism for the shoulder buttons: rather than a simple actuator like the face buttons, there’s a hinge with a <a href=\"https://en.wikipedia.org/wiki/Torsion_spring\">torsion spring</a> that hits the actuator at an angle. This is actually part of what makes the shoulder buttons pleasant to press: you don’t need to hit them from exactly the right direction, because they pivot. I ended up just going with a standard right-angle tactile button, opting to solve the problem with the mechanism in the enclosure.</p><figure><figcaption><p>GBA SP shoulder button mechanism</p></figcaption></figure><p>One of my main goals was to allow ROM files to be loaded from a microSD card, rather than only being able to be played from a physical cartridge. To do this, I’d need dedicated RAM for the FPGA to hold the game. Game Boy Advance games, typically, are a maximum of 32 MB. They don’t make SRAMs that large (and if they did, they’d be very expensive). Instead, I needed to use <a href=\"https://en.wikipedia.org/wiki/Dynamic_random-access_memory\">DRAM</a>.</p><p>Asynchronous SRAM is very simple: supply a read address to the address pins, and some amount of nanoseconds later, the data you’re reading appears on the data pins. DRAM is more complex: the simplest kind is “single data rate synchronous DRAM” (SDR SDRAM, or just <a href=\"https://en.wikipedia.org/wiki/Synchronous_dynamic_random-access_memory\">SDRAM</a>, distinguishing it from the significantly more complex DDR SDRAM). However, even SDRAM is non-trivial to use. DRAM is organized into banks, rows, and columns, and accessing DRAM requires sending commands to “activate” (open) a row before reading out “columns”, and then “precharging” (closing) a row. Handling all of this requires a DRAM controller (see this <a href=\"https://www.fpga4fun.com/SDRAM2.html\">simple description of the state machine</a> required). This isn’t terribly complex, but I was signing myself up for more work.</p><p>Alternatively, I could have chosen a PSRAM chip (essentially DRAM with an integrated controller to make it have a more SRAM-like interface). However, I couldn’t find a PSRAM part that I was happy with (cost, availability, interface), and so I ended up going with the inexpensive W9825G6KH 32MB 16-bit SDRAM.</p><p>I also decided to stick a 512 KiB SRAM chip in the design in case I ended up needing some more simple memory later, like for emulating the SRAM used for Game Boy cartridge save files. Despite being 1/64 the capacity, this chip was about 3x the cost of the SDRAM. This ended up being a wise decision, since a lot of my internal FPGA block ram was eaten up by the triple buffer for the display (see above).</p><h3>Cartridge and Link Ports<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#cartridge-and-link-ports\">#</a></h3><p>The cartridge slot and link ports are no-name parts from AliExpress, easily available for cheap. These seem to mostly be GBA SP compatible, and are often used as repair parts.</p><p>The Game Boy Advance can play both Game Boy [Color] and Game Boy Advance games. These run at different voltages and use different protocols, so the device needed some way of determining which type of cartridge is inserted.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gb-vs-gba-cartridge-bottom.jpg\" alt=\"GBA cartridge (top) vs GB cartridge (bottom)\" width=\"1440\" height=\"519\"><figcaption><p>GBA cartridge (top) vs GB cartridge (bottom)</p></figcaption></figure><p>The cartridges are physically different at the bottom: GBA cartridges (the top cartridge in the image) have a notch on either side. The GBA has a  that senses the absence of a notch on an inserted cartridge and switches the device into Game Boy Color mode.</p><p>I measured the size and position of this notch, and searched Digi-Key and Mouser for switches that met these constraints. In the end, I was only able to find a single switch that would work.</p><h3>Miscellaneous peripherals<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#miscellaneous-peripherals\">#</a></h3><p>I used the surprisingly cheap <a href=\"https://www.st.com/en/mems-and-sensors/lsm6ds3tr-c.html\">LSM6DS3TR-C</a> IMU from ST. This tiny IMU has a 3-axis accelerometer and gyroscope, more than sufficient for emulating the few GB/GBA cartridges that have motion controls.</p><p>For keeping track of time even when the device was off, I used the <a href=\"https://www.nxp.com/part/PCF8563T\">PCF8563T</a> real-time clock chip. I chose this because it was 1) I2C (no additional pins required), 2) cheap, and 3) readily available from JLCPCB. Interestingly, all of the real-time clock chips I found count in seconds/minutes/hours/days/months/years. This makes sense for a really simple device with minimal computational power. However, it’s annoying for my purposes, since all I really want is a timestamp I can pass to some other datetime library, and converting between the calendar time and a unix timestamp is non-trivial due to how the chips incompletely handle leap years.</p><p>I picked up a few cheap coin vibration motors to use for vibration support (for the rare cartridge that had a built-in vibration motor).</p><p>I also used a <a href=\"https://www.ti.com/product/TCA9535\">TCA9535</a> I2C I/O expander to connect the face buttons to the MCU. I ran out of pins, and while I  have used the FPGA as a sort of I/O expander, I figured I’d make it simpler for myself (and allow the buttons to be used even if the FPGA was powered off) by letting the MCU read them itself.</p><p>For this project, as with my previous ones, I used <a href=\"https://www.kicad.org/\">KiCad</a> to create my schematic and do PCB layout. I really can’t recommend KiCad enough: it’s a great program, intuitive to use, and it’s free and open source.</p><p>This was a very ambitious project for my level of electrical engineering experience, and creating the schematic took a couple of weeks. I spent a lot of time designing the circuit for each component, because I was afraid I’d do something wrong and end up with a stack of useless boards without the skills needed to debug them. A lot of the component selection actually happened in parallel with schematic design, as I found new requirements or problems and had to change components.</p><p>I gained a lot of experience reading component datasheets. It’s a really valuable skill, both for component selection and for creating designs that use the components. Nearly every datasheet has a “typical application” section, where the manufacturer shows how the component would fit into a circuit. At minimum, this has power supply information (e.g. these voltages to these pins with these decoupling capacitors). For more complex components like the DAC, it also has information about power sequencing, different ways the device could be connected to the rest of the system, a register list, that sort of thing. Some components also included PCB layout recommendations. This information was all really helpful, and gave me a good deal of confidence that my board would work as long as I read through the datasheet and followed the manufacturer’s recommendations.</p><p>Then I got to the FPGA. Nearly every component has a single datasheet. Some of them have an additional application note or two. Particularly complex chips (like the ESP32-S3 microcontroller) have a separate datasheet, reference manual, and hardware design guide. The Xilinx Series 7 FPGAs have . Overviews, packaging and pinout, configuration guides, BGA design rules, power specifications, clocking resources, I/O specifications, PCB layout guides, design checklists… even a 4MB Excel spreadsheet for estimating power consumption! And believe me, Xilinx didn’t just write documentation for fun: there’s so much documentation because the chip  this much documentation.</p><p>Designing with the FPGA was overwhelming, and  beyond my experience level. At several points I genuinely considered dropping the project altogether. Fortunately, I persevered, and gradually internalized a lot of the information. I also read through the schematics of any open-source Artix-7 development board I could get my hands on. Seeing what other people were doing gave me more confidence that I was doing the right thing.</p><p>Eventually, after I placed all of the components, connected them, ensured all of the nets were labeled, and ran KiCad’s electrical rules checker (ERC) to find obvious mistakes, I moved on to layout.</p><p>I did PCB layout at the same time as some of the initial enclosure CAD. The mechanics of how everything fit together influenced the placement of the display connector, cartridge slot, buttons, speakers, and connectors. After I came up with a plausible enclosure design, I placed some of the first key components onto the PCB and locked them into place while I did the rest of the routing.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-shell-design.png\" alt=\"Rough enclosure design to help with board layout\" width=\"2000\" height=\"1298\"><figcaption><p>Rough enclosure design to help with board layout</p></figcaption></figure><p>I first focused on components that would be hardest to route. Primarily, the FPGA: the package I was using (CSG324) is a <a href=\"https://en.wikipedia.org/wiki/Ball_grid_array\">BGA</a>, 18x18 with 0.8mm pitch between pins. “Fanning out” all of the I/O signals requires careful routing, and at 0.8mm pitch, it’s difficult to do this routing with cheap PCB manufacturing techniques. I ended up being able to do this routing with a 6-layer PCB (three signal, two ground, one power), with 0.1mm track width and spacing, and 0.4/0.25mm vias. Fortunately, this is all within the realm of JLCPCB’s capabilities.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/bga-fanout.png\" alt=\"BGA fanout with thin traces and small vias\" width=\"1442\" height=\"1102\"><figcaption><p>BGA fanout with thin traces and small vias</p></figcaption></figure><p>As I routed signals out from the FPGA to other parts, I assigned those signals to the FPGA pins. Similarly, with the MCU, I assigned signals to pins in a way that made routing easier. Certain signals had restrictions (e.g. on the FPGA, the main 50 MHz clock signal can only go into certain pins, or the configuration bitstream can only go to certain pins, or certain pins are differential pairs for HDMI output), but overall, I had a lot of flexibility with pin assignment.</p><p>KiCad has a feature where it automatically backs up your project as you work on it. I changed the settings to save every 5 minutes and not delete old backups, which allowed me to generate this timelapse of my layout process:</p><figure><figcaption><p>Revision 1 board layout timelapse</p></figcaption></figure><p>Once I finished placing and routing all of the components, I ran the design rules checker (DRC) and fixed issues. I hesitated for a while before sending the PCB for manufacturing. I re-read the schematics, reviewed the layout, and eventually felt confident enough that I was done. I submitted the order to JLCPCB, and after a few questions by their engineers about component placement, they started manufacturing it.</p><h2>Board testing and bring-up<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#board-testing-and-bring-up\">#</a></h2><p>After two weeks or so, I received the assembled boards in the mail:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/rev1-initial-boards.jpg\" alt=\"An assembled board and an unassembled board\" width=\"1440\" height=\"1026\"><figcaption><p>An assembled board and an unassembled board</p></figcaption></figure><p>First, I probed the power rail test points with a multimeter to check for shorts. Then, I plugged the boards in for the first time, and pressed the power button. To my delight, the green LED turned on, indicating that the power button circuit, power path, and 3.3V regulator worked. The microcontroller USB enumerated, and I could see that it logged some errors (since I hadn’t flashed anything to it yet).</p><p>I intended to write the MCU firmware in Rust, but I did initial board testing and bring-up with <a href=\"https://micropython.org/\">MicroPython</a>. This would let me interactively type in Python and write basic scripts to communicate with the peripherals on the board and make sure I had connected everything correctly. I didn’t have to worry about writing efficient or well-organized code, and could just focus on functionality.</p><p>I flashed the MicroPython firmware image, and wrote a couple lines of Python to blink the LED. I powered on the FPGA power domain, and checked that the , , and  rails had the correct voltage.</p><p>Next, I wrote a simple bitstream for the FPGA that read the state of the buttons and produced a pattern on the shared signals between the FPGA and the MCU. I wrote simple Python code to configure the FPGA, loaded up the bitstream, and polled the signals from the FPGA. Pressing buttons changed the state, and confirmed that the FPGA was properly powered, and configurable from the MCU.</p><p>After I confirmed the FPGA worked, I started writing a simple display driver to initialize the LCD and push some pixels from the MCU over SPI. The initialization sequence uses a number of LCD-specific parameters (voltages, gamma correction, etc.), that I learned from the LCD manufacturer’s example code.</p><figure><figcaption><p>(Slowly) pushing pixels to the LCD</p></figcaption></figure><p>The LCD module’s controller, an ILI9488, has a few quirks: despite claiming that it supports 16-bit colors over SPI, it actually only supports 18-bit colors. This unfortunately meant that the MCU’s LCD driver would be more inefficient than I expected, since it has to expand 16-bit colors to 18-bit before sending them over the bus. This didn’t end up being a huge issue, however, because the FPGA is the one driving the display most of the time.</p><p>Another quirk (hardware bug?) is that the ILI9488 doesn’t stop driving its SPI output line, even when its chip-select signal is inactive. This means that the chip will interfere with any other communication on the bus… including the FPGA, which sits on the same bus. I never actually needed to read any data back from the LCD (and even if I did, it supports <a href=\"https://en.wikipedia.org/wiki/Serial_Peripheral_Interface#Three-wire\">three-wire SPI</a>), so I just cut the trace between the LCD’s SDO line and the SPI bus.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/lcd-debugging.jpg\" alt=\"Debugging the LCD test code\" width=\"1440\" height=\"954\"><figcaption><p>Debugging the LCD test code</p></figcaption></figure><h3>Trouble with power domains<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#trouble-with-power-domains\">#</a></h3><p>I started trying to communicate with the I2C peripherals (I/O expander, RTC, etc.), and found that nothing was responding. A bit of probing with a logic analyzer revealed that the SCL/SDA lines were being held low, and that powering on the FPGA power domain let the lines be pulled high and communication to happen.</p><p>I deduced that this was due to the DAC, which had its IOVDD powered by , which likely caused its protection diodes to pull the IO lines (SCL and SDA) low:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dac-iovdd-issue.png\" alt=\"The problematic portion of the schematic\" width=\"1862\" height=\"656\"><figcaption><p>The problematic portion of the schematic</p></figcaption></figure><p>I tested out this theory by cutting the PCB traces connecting the DAC’s IOVDD and  with a knife. After this, I2C worked even with the FPGA power disabled. Then, I tested a possible fix by adding a wire to power the DAC’s IOVDD from the  rail. I confirmed that I could still talk to the other I2C devices, and once enabling FPGA power, that I could talk to the DAC too.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dac-iovdd-rework.jpg\" alt=\"DAC IOVDD rework\" width=\"1440\" height=\"1126\"></figure><p>While bringing up the LCD, I saw that the FPGA was also pulling down the shared SPI bus lines while it was unpowered. Not enough to prevent communication with the LCD, but it still wasn’t great. Between this and the DAC issue, I learned an important EE lesson: be careful when connecting components in different power domains together. A tristate buffer, such as the <a href=\"https://www.ti.com/product/SN74LVC1G125\">74LVC1G125</a>, could have helped here to isolate the buses.</p><p>Once I2C was working, I wrote some basic driver code for the fuel gauge, real-time clock, IMU, and I/O expander, just to check that they all worked correctly. I also checked that the MCU could read from and write to the attached microSD card.</p><h4>Audio and video output from the FPGA<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#audio-and-video-output-from-the-fpga\">#</a></h4><p>Next, I updated my testing FPGA bitstream to output a test pattern over the <a href=\"https://hackaday.com/2024/01/25/displays-we-love-hacking-parallel-rgb/\">LCD parallel interface (“DPI”)</a>, and a test tone to the DAC over the I2S interface. Then, I began poking on the MCU side to configure the LCD controller and DAC appropriately.</p><p>With some amount of trial and error, I convinced the LCD to accept input from the FPGA. Most of the trial and error revolved around the rotation of the LCD module. Soon after, I configured the DAC properly, and it played the test tone from the FPGA over the speakers and the headphones.</p><figure><figcaption><p>WIP video output from the FPGA</p></figcaption></figure><p>At this point, much of the board was working, so I soldered on the rest of the components (cartridge slot, cartridge switch, link port, shoulder buttons).</p><p>With the cartridge slot in place, I had everything I needed to port over the Game Boy emulator from my last project. I did a quick-and-dirty port of the emulator, with some hacking around to connect the core to the audio, video, and the physical cartridge. I was able to play the first Game Boy game on the device far sooner than I was expecting:</p><figure><figcaption><p>Pokemon Silver running from cartridge</p></figcaption></figure><p>I spent the next month or so implementing things on the FPGA. I started on the SPI receiver implementation, so that the MCU and FPGA could communicate.</p><p>It was relatively straightforward to write <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/main/scala/platform/handheld/SpiReceiver.scala\">the initial version</a>, which 4x oversampled the SPI signals from the main system clock. For the Game Boy, that was ~8 MHz, for a maximum SPI speed of 2 MHz. The MicroPython ESP32-S3 SPI implementation supported only single SPI, so that allowed for a maximum transfer speed of 256 KB/s. This was sufficient to do most of my initial testing, but I later <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/main/scala/platform/handheld/SpiReceiverFifo.scala\">wrote an improved SPI receiver</a> to run with an internal 200 MHz clock (from a PLL that turned on and off with the chip-select signal to save power), communicating with the rest of the system via a pair of FIFOs. This added a lot of complexity and edge cases, but it greatly improved performance, allowing the bus to run at 40 MHz.</p><p>I wrote the SPI interface to the FPGA with memory-like semantics: each SPI transfer starts with a command byte, encoding whether it’s a read or write transfer, the size of each word in the transfer (8, 16, or 32 bits), and whether the “target address” should autoincrement as the transfer progresses. Then, a 32-bit address, followed by reading or writing the data. Each thing that the MCU might want to access (control registers, blocks of memory) are mapped into the 32-bit address space.</p><p>As with my previous FPGA project, I wrote almost all of the FPGA code in <a href=\"https://www.chisel-lang.org/\">Chisel</a>, a Scala-based HDL. The remaining bits were the top-level Verilog. Chisel made it really simple to parametrize, compose, and test the various modules that I wrote.</p><p>Once I had the SPI receiver working, I wrote controllers for the on-board SRAM and SDRAM. The SRAM was relatively simple (although I still got it slightly wrong at first). The SDRAM was a bit tricky, and even as I write this I’m not quite satisfied with its performance, and intend to rewrite it in the future.</p><p>I exposed the SRAM and SDRAM interfaces to the MCU via SPI, which allowed me to read and write to these pieces of memory from the MCU. I used this a lot for testing: writing patterns into memory and reading them back to ensure that read and write both worked.</p><p>Side note: SDRAM has to be continuously refreshed, otherwise the stored data decays over time. It depends on the chip, but typically each row has to be read and written back (or auto-refreshed, which does the same thing) at least once every 64 milliseconds to avoid losing state. What I found interesting, however, is that the data can actually persist for quite a bit longer. I discovered that when I was reconfiguring the FPGA between tests, most of the test data that I had previously written would still stick around even without being refreshed. In the first few seconds some bits would start flipping, and over the course of a few minutes, most of what was written was completely unintelligible.</p><p>With the SDRAM controller and SPI receiver written, I was then able to implement the “emulated cartridge” part of the Game Boy emulator, where the MCU reads a ROM file off of the microSD card and sends it to the FPGA to be stored in SDRAM. Then, the FPGA “emulates” a cartridge (rather than interfacing with a real physical cartridge). After a few stupid mistakes, I was able to run test ROMs and homebrew. As an added bonus, since I was using my own SDRAM controller directly, I didn’t have any of <a href=\"https://eli.lipsitz.net/posts/fpga-gameboy-emulator/#fixing-an-audio-bug-on-the-game-boy-color\">the performance issues I’d faced before</a> when accessing the ROM stored in memory.</p><h2>Writing the microcontroller firmware in Rust<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#writing-the-microcontroller-firmware-in-rust\">#</a></h2><p>By this point I had tested, in some form or another, all of the different components of the system. I’m really surprised that everything worked in my first board revision – even the rework I did early on wasn’t actually required for functionality.</p><p>I decided now was a good time to start building an interactive GUI. Up until this point, I had just been running commands in the MicroPython REPL. However, I didn’t want to build a whole UI in Python just to throw it away later, so I also started working on the “production” Rust firmware.</p><p>In the last few years, a lot of progress has been made towards making Rust on the ESP32 chips work well, even on the chips that use the Xtensa ISA. I followed the <a href=\"https://docs.esp-rs.org/book/\">Rust on ESP Book</a> and quickly had an environment set up. I opted for the <a href=\"https://docs.esp-rs.org/book/overview/using-the-standard-library.html\">“Rust with the Standard Library”</a> approach, so that I could benefit from <a href=\"https://idf.espressif.com/\">ESP-IDF</a>, especially the built-in support for USB and SD cards with the FAT filesystem.</p><p>I started porting over the drivers I had written in Python. I found embedded Rust to be a bit verbose in some cases, but overall pleasant to use and worth the (little) trouble.</p><p>I starting writing my own minimal GUI framework for basic menus. I poked around with the <a href=\"https://docs.rs/embedded-graphics/latest/embedded_graphics/\"></a> library, but soon found that the typical patterns I was expecting to use weren’t a great fit for Rust. I also started planning out different screens and realized that I probably actually wanted to use a more comprehensive UI framework.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-gui-main-menu.png\" alt=\"Early main menu screen\" width=\"480\" height=\"320\"></figure><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-gui-rom-select.png\" alt=\"Early rom select screen\" width=\"480\" height=\"320\"></figure><p>Ultimately, I settled on <a href=\"https://github.com/slint-ui/slint\">Slint</a>, a Rust-native declarative GUI framework with excellent support for embedded devices. Slint has a custom DSL to describe the UI and composable components. After a bit of practice I found myself to be really productive with it. I enjoyed using Slint, and I’d use it again in the future. The authors are responsive on GitHub, and the project has steadily improved over the year or so that I’ve been using it.</p><p>There were a few rough edges for my use case, however:</p><ul><li>The built-in GUI elements and examples were all heavily oriented around mouse or touchscreen navigation. Game Bub only has buttons for navigation, however, so I had to make my own widgets (buttons, lists) that worked with key navigation. This involved a few hacks, because Slint’s focus handling was a little bit simplistic.</li><li>The built-in GUI styles looked (in my opinion) bad on a low DPI screen. Text was excessively anti-aliased and hard to read at small sizes. This was also fixed by building my own widgets.</li><li>Slint doesn’t have a great story around supporting different “screens” – I had to build some of my own infrastructure to be able to support navigation between the main menu, games, rom select, settings, etc.</li></ul><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-main-menu.png\" alt=\"Main menu\" width=\"480\" height=\"320\"></figure><p>The GUI is rendered on the MCU, and then the rendered framebuffer is sent over to the FPGA. Slint supports partial rendering, where only the parts of the screen that have changed are updated, which improved performance. The FPGA maintains a copy of the framebuffer and ultimately is responsible for driving the display. This has a few advantages over driving the display directly from the MCU:</p><ul><li>Sending a framebuffer at 40 MHz QSPI to the FPGA is 16x faster than sending it to the LCD controller at 10 MHz (the fastest speed supported by the ILI9488)</li><li>The UI is rendered at 240x160 to improve performance and maintain the GBA aesthetic, but the LCD controller doesn’t have a scaler, so the MCU would have to send 4x the pixels. The FPGA can easily scale the UI framebuffer itself.</li><li>The FPGA can composite the emulator output with a semi-transparent “overlay” to support an in-game menu, volume / brightness bars, battery notifications, etc.</li><li>An external display (e.g. monitor or TV) can be driven by the FPGA via HDMI</li></ul><p>I spent some time making a variety of firmware improvements, mostly polish and quality-of-life. I added a settings screen to set the date and time, whether to use Game Boy (DMG) or Game Boy Color (CGB) mode when playing Game Boy games, etc.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-settings-early.png\" alt=\"Settings screen\" width=\"480\" height=\"320\"></figure><p>Then I improved the ROM select file browser, and added a battery level indicator.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-load-rom.png\" alt=\"Rom select screen\" width=\"480\" height=\"320\"></figure><p>I also got sick of having to take the microSD card out of the device and connect it to my computer through a series of adapters (microSD to SD to USB-A to USB-C), so I implemented a basic utility to expose the microSD card as a USB Mass Storage Device, using <a href=\"https://github.com/hathach/tinyusb\">TinyUSB</a> and the ESP32-S3’s USB-OTG capabilities.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-usb-storage.png\" alt=\"USB Mass Storage screen\" width=\"480\" height=\"320\"></figure><p>It was a little bit more difficult than I expected, because USB Mass Storage requires the device to provide raw block access. This means that the filesystem has to be unmounted by the device, otherwise the device and host could conflict and corrupt the filesystem. The ESP32-S3 also only supports USB Full Speed, for a practical maximum transfer speed of ~600KB/sec. It’s really useful for transferring save files or updating the FPGA bitstreams, but less useful for transferring a large number of ROM files.</p><p>Later, I implemented <a href=\"https://gbdev.io/pandocs/MBC7.html\">MBC7</a> support in the Game Boy emulator for Kirby Tilt ’n Tumble, using the on-board accelerometer.</p><p>After I implemented a decent amount of software functionality, I decided to finish the enclosure design. The bare board just wasn’t cutting it anymore, and the taped LCD module/loose speakers/rubber-banded battery contraption was fragile.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gamebub-without-shell.jpg\" alt=\"Game Bub looking rough without an enclosure\" width=\"1440\" height=\"1724\"><figcaption><p>Game Bub looking rough without an enclosure</p></figcaption></figure><p>I came into this project without any CAD or 3D printing experience. I looked at a few different CAD software packages, and I ultimately settled on <a href=\"https://www.freecad.org/\">FreeCAD</a>, primarily because it was free and open source. I learned how to use the software with some video tutorials. FreeCAD, unfortunately, was a little bit rough around the edges and I ended up running into some annoying issues. Nevertheless, I powered through and finished the design.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/enclosure-rev1-cad.png\" alt=\"FreeCAD view of the enclosure and some buttons\" width=\"2000\" height=\"1524\"><figcaption><p>FreeCAD view of the enclosure and some buttons</p></figcaption></figure><p>I found parametric modeling, where the geometry of the model is defined by constraints and dimensions, to be intuitive. However overall, I found 3D CAD to be very time consuming. I think a large part of this is my inexperience, but thinking in three dimensions is a lot more difficult than, say, a 2D PCB layout. Creating a full assembly was even more difficult: I had to visualize how the front and rear pieces would fit together, where the screws would go, and how the buttons, screen, speaker, cartridge slot, battery, and ports would all fit in. This project definitely pushed the boundaries of my (previously non-existent) product design skills.</p><p>After finishing the design, I printed out the technical drawing at a 1:1 scale and physically placed the board and other components down as a final check. Then, I sent it to JLCPCB for manufacturing. I opted for <a href=\"https://en.wikipedia.org/wiki/Stereolithography\">SLA resin printing</a>, for high precision and a smooth finish.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/shell-rev1-technical-drawing.png\" alt=\"Enclosure technical drawing\" width=\"1440\" height=\"971\"><figcaption><p>Enclosure technical drawing</p></figcaption></figure><p>After a couple weeks, I got the finished enclosure and custom buttons back.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/enclosure-rev1-outside.jpg\" alt=\"Front and rear half, outside\" width=\"1440\" height=\"1009\"><figcaption><p>Front and rear half, outside</p></figcaption></figure><p>I put the buttons, speakers, and screen into the enclosure, screwed on the PCB, and put the whole thing together.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/assembling-front.jpg\" alt=\"Assembling the front side\" width=\"1440\" height=\"1544\"><figcaption><p>Assembling the front side</p></figcaption></figure><figure><figcaption><p>Game Bub, fully assembled and functional</p></figcaption></figure><p>I wasn’t sure how dimensionally accurate the 3D printing would be, so I added a lot of extra clearance around the buttons and ports. As it turned out, the printing was very precise, so the buttons rattled around a little in the oversized button holes.</p><p>It’s a little bit chunky (smaller than an original Game Boy, though!) and the ergonomics aren’t ideal, but I was really happy to finally have an enclosure. It actually started (sort of) looking like a real product, and I wasn’t constantly worried about breaking it anymore.</p><h2>Game Boy Advance support<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#game-boy-advance-support\">#</a></h2><p>I won’t go into all of the details of how I wrote the emulator here (this article is already long enough!). If you’re interested, my <a href=\"https://eli.lipsitz.net/posts/fpga-gameboy-emulator/\">previous article about my Game Boy FPGA emulator</a> goes into detail about the general process of writing an emulator, and for a high-level introduction to the Game Boy Advance (from a technical perspective), I recommend <a href=\"https://www.copetti.org/writings/consoles/game-boy-advance/\">Rodrigo Copetti’s article</a>. In general, I tried to implement the emulator the way it might actually have been implemented in the original hardware: each cycle of the FPGA corresponds to one actual hardware cycle (no cheating!).</p><p>As with the Game Boy, I did nearly all of my development with a simulator backed by <a href=\"https://www.veripool.org/verilator/\">Verilator</a> and <a href=\"https://www.libsdl.org\">SDL</a>. By the end of the development process, the simulator was running at about 8% of the real-time speed (on an M3 MacBook Air with excellent single-core performance), which was a bit painful.</p><p>The Game Boy Advance CPU, the <a href=\"https://en.wikipedia.org/wiki/ARM7#ARM7TDMI\">ARM7TDMI</a>, is significantly more complicated than the Game Boy’s SM83 (a <a href=\"https://en.wikipedia.org/wiki/Zilog_Z80\">Z80</a> / <a href=\"https://en.wikipedia.org/wiki/Intel_8080\">8080</a>-ish hybrid). However, in some ways, it was easier to understand and implement: the ARM7TDMI is much closer to a simple modern processor architecture, and it’s extensively documented by ARM. For example, the <a href=\"https://developer.arm.com/documentation/ddi0210/c/?lang=en\">ARM7TDMI Technical Reference Manual</a> has block diagrams and detailed cycle-by-cycle instruction timing descriptions.</p><p>I had a lot of fun <a href=\"https://github.com/elipsitz/gamebub/tree/handheld/fpga/src/main/scala/gba/cpu\">implementing the CPU</a>. The architecture has a three-stage pipeline (fetch, decode, execute) – a division that feels natural when you implement it in hardware. The ARM7TDMI has two instruction sets: the standard 32-bit ARM instruction set, and the compressed 16-bit THUMB instruction set. I implemented the CPU the way it works in hardware, where the only difference between ARM and THUMB is the decode stage.</p><p>As I was implementing the CPU, I wrote <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/test/scala/gba/cpu/ARM7TDMISpec.scala\">test cases</a> for each instruction. Each test checks the functionality of the instruction: processor state, register values after, as well as the cycle-by-cycle behavior and interaction with the memory bus. This was helpful for catching regressions as I implemented more and more control logic. It was also really satisfying to be able to implement individual instructions, then write the tests, and check that everything worked.</p><p>Chisel made it easy to write out the <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/main/scala/gba/cpu/Control.scala\">CPU control logic</a>. The CPU control logic is a state machine that generates microarchitectural control signals (e.g. bus A should hold the value from the first read register, bus B should hold an immediate value, the memory unit should start fetching the computed address, etc.). Chisel allowed me to collect common functionality into functions (e.g.  to set up the signals to dispatch the next decoded instruction, or  to signal that the pipeline should be flushed and a new instruction should be fetched from the current program counter).</p><p>I found it helpful to draw out timing diagrams with <a href=\"https://wavedrom.com\">WaveDrom</a> when working through instructions, especially to deal with the pipelined memory bus.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/timing-arm7tdmi-branch.svg\" alt=\"My timing diagram of the ARM7TDMI branch instructions\"><figcaption><p>My timing diagram of the ARM7TDMI branch instructions</p></figcaption></figure><p>By mid-May (about a month later), I finished the CPU implementation (with occasional bug fixes after) and moved onto the rest of the system.</p><h3>PPU, MMIO, and everything else<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#ppu-mmio-and-everything-else\">#</a></h3><p>Over the next month and a half, I implemented the majority of the rest of the Game Boy Advance. The CPU interacts with the rest of the system via <a href=\"http://problemkaputt.de/gbatek-gba-i-o-map.htm\">memory-mapped IO (MMIO)</a> registers. Unlike the Game Boy CPU, which can only access memory a single byte at a time, the ARM7TDMI can make 8-bit, 16-bit, and 32-bit accesses. This complicates MMIO, and the different hardware registers and memory regions in the GBA respond to different access widths in different ways.</p><p>I started with the Picture Processing Unit (PPU), which produces the video output. The author of <a href=\"https://github.com/nba-emu/NanoBoyAdvance\">NanoBoyAdvance</a>, fleroviux, had helpfully documented the <a href=\"https://nba-emu.github.io/hw-docs/ppu/ppu.html\">PPU VRAM access patterns</a>, which gave a lot of insight into how the PPU might work internally. <a href=\"https://www.coranac.com/tonc/text/toc.htm\">Tonc</a> was also immensely helpful for implementing the PPU and testing individual pieces of functionality.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/tonc-test.png\" alt=\"(Sort of) running a Tonc PPU demo\" width=\"963\" height=\"699\"><figcaption><p>(Sort of) running a Tonc PPU demo</p></figcaption></figure><p>The PPU took a few weeks, and then I moved onto DMA, followed by hardware timers, and audio. Of course, as I’d try new tests, demos, and games, I’d uncover bugs and fix them.</p><figure><figcaption><p>Kirby  in Dream Land</p></figcaption></figure><p>Game Boy and Game Boy Advance cartridges use the same 32-pin connector. However, they work very differently. The Game Boy cartridge bus is asynchronous: the game outputs the 16-bit address (64 KiB address space) on one set of pins and lowers the  pin. Some time later, the 8-bit read data from the ROM stabilizes on a separate set of pins.</p><p>For the GBA, Nintendo extended the bus data width to 16-bit and the address space to 25-bit (32 MiB). However, they kept roughly the same set of pins, accomplishing this by multiplexing the 24 data/address pins: the console outputs the address (in increments of the data word size of 16-bits, for a 24-bit physical address), then lowers the  signal to “latch” the address in the cartridge. Then, each time the console pulses the  pin, the cartridge increments its latched address and outputs the next data over the same pins. This allows for a continuous read of sequential data without having to send a new address for each access. The GBA also allows games to <a href=\"http://problemkaputt.de/gbatek-gba-system-control.htm\">configure cartridge access timings</a> to support different ROM chips.</p><p>I had to do a lot of my own research here. Software emulators don’t need to care about the precise timing of the cartridge bus, so there wasn’t much documentation. To figure out the exact cycle-accurate timing, I used a <a href=\"https://www.saleae.com/\">Saleae</a> logic analyzer and connected it to the cartridge bus. I wrote a test program for the GBA to do different types of accesses (reads, writes, sequential, non-sequential, DMA) with different timing configurations.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/cartridge-bus-analyzer.jpg\" alt=\"Cartridge bus analysis setup\" width=\"1440\" height=\"1080\"><figcaption><p>Cartridge bus analysis setup</p></figcaption></figure><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/trace-gba-cartridge.png\" alt=\"Portion of a trace\" width=\"2582\" height=\"734\"></figure><p>After coming up with numerous scenarios (especially around the interaction between DMA and the CPU, and starting and stopping burst accesses), I came up with a consistent model for how cartridge accesses worked. I created some timing diagrams to help:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/cartridge-timing-diagram.svg\" alt=\"Timing diagram of a non-sequential access followed by a sequential access\"><figcaption><p>Timing diagram of a non-sequential access followed by a sequential access</p></figcaption></figure><p>Finally, I started implementing the cartridge controller state machine based on my observations, paired with an emulated cartridge implementation. With the emulated cartridge, I was able to properly run real games in the simulator.</p><p>I quickly implemented physical cartridge support, to be able to finally run it on the actual FPGA. I connected the signals, built a new bitstream, and… it didn’t work at all. The Game Boy Advance boot screen ran, but it didn’t get any further than that. I implemented the emulated cartridge on the FPGA (reading ROM files from the SD card), and it worked! Which was great, but physical cartridges still didn’t.</p><p>I used the logic analyzer to observe how my emulator was interacting with the cartridge compared to how an actual GBA, and found numerous issues.</p><p>One of the first things I noticed was short <a href=\"https://en.wikipedia.org/wiki/Glitch#Electronics_glitch\">glitches</a> on the  line. I knew these had to be glitches (rather than incorrect logic), because they were 8 nanoseconds long, much shorter than the ~59.6ns clock period. Since the cartridge latches the address on a falling edge of , glitches cause it to latch an address when it shouldn’t, screwing up reads.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/cartridge-bus-glitches.png\" alt=\"Glitches on the cartridge bus\" width=\"2177\" height=\"600\"><figcaption><p>Glitches on the cartridge bus</p></figcaption></figure><p>Here, I learned an important lesson in digital design: output signals should come directly from flip-flops, with no logic in between.</p><p>After each flip-flop outputs a new value (on the rising edge of the clock), the signals propagate through the chip. As they propagate, taking different paths of different lengths throughout the chip, the output from each lookup table (LUT) is unstable. These values only stabilize near the end of the clock cycle (assuming the design met timing closure), and then each flip-flop stores the stable value at the next rising edge. If you output a signal from logic, this instability is visible from outside of the chip, manifesting as glitches in the output signal. If you instead output the signal from a flip-flop, it’ll change only on each clock edge, remaining stable in the middle.</p><p>And of course, I had written the cartridge controller without thinking about this, and  of the output signals were generated from logic. I rewrote the controller to output everything from flip-flops, which had a series of cascading changes since all of the signals now had to be computed one clock cycle earlier than I expected.</p><p>There were other issues too – part of the problem was that my emulated cartridge model was too permissive, and didn’t catch some fairly obvious incorrect behavior. After a few days of intensive debugging with the logic analyzer, I got to the point where I could play games from physical cartridges.</p><figure><figcaption><p>Metroid: Zero Mission running from the cartridge</p></figcaption></figure><h4>Cartridge prefetch buffer<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#cartridge-prefetch-buffer\">#</a></h4><p>The ARM7TDMI has a single shared instruction and data memory bus. As a result, a long series of sequential memory accesses is rare. Even a linear piece of code without branches that includes “load” or “store” instructions would produce a series of non-sequential memory accesses, as the CPU fetches an instruction from one location, loads a register from a different location, and then goes back to fetching the next instruction.</p><p>This poses a real performance issue on the GBA, because every non-sequential access from the cartridge incurs a multi-cycle penalty. Nintendo attempted to mitigate this somewhat with the “prefetch buffer” (<a href=\"https://mgba.io/2015/06/27/cycle-counting-prefetch/#game-pak-prefetch\">read this post by endrift, the author of mGBA, for more details</a>) which attempts to keep a cartridge read burst active between CPU accesses. Without emulating the prefetch buffer, some games lag (I noticed this the most in Mario Kart Super Circuit, and some rooms of Metroid: Zero Mission).</p><p>The prefetch buffer, while simple in theory, is not well documented and has a lot of corner cases and weird interactions. Emulator developers often start by taking a shortcut: making all cartridge accesses take a single cycle when the prefetch buffer is enabled. This wouldn’t work for me, since I actually had to interface with the physical cartridge.</p><p>So, I set out to do some more research to figure out exactly how the prefetch buffer worked. After making some educated guesses and tests, I came up with a reasonable model of how it might work.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/prefetch-notes.jpg\" alt=\"Notes about the prefetch state machine\" width=\"1440\" height=\"1744\"><figcaption><p>Notes about the prefetch state machine</p></figcaption></figure><p>Actually implementing it took a lot of work, and I kept stumbling upon more and more corner cases. Eventually I got to the point where all games appeared to run at full speed, and most importantly, didn’t randomly crash. My implementation isn’t perfect: there are still a few <a href=\"https://github.com/mgba-emu/suite\">mGBA test suite</a> timing tests I don’t pass, but it’s certainly sufficient to play games.</p><ul><li>: standard duplex SPI, used for communicating with accessories</li><li>: custom multi-drop UART-like protocol, used to link up to four GBAs together for multiplayer games</li><li>: the Nintendo N64 and GameCube controller protocol, used to connect to a GameCube</li><li>: duplex <a href=\"https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter\">UART</a> with flow control, </li><li>: controlling the four pins individually as <a href=\"https://en.wikipedia.org/wiki/General-purpose_input/output\">GPIO</a>, </li></ul><p>The timing of these isn’t well documented, so I did my own research.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gba-link-multiplayer-transfer.png\" alt=\"A multiplayer mode transfer with no attached consoles\" width=\"2012\" height=\"558\"><figcaption><p>A  mode transfer with no attached consoles</p></figcaption></figure><p>I did a lot of testing with examples from the <a href=\"https://github.com/afska/gba-link-connection\">gba-link-connection</a> library, intended for homebrew GBA games, but helpful for testing the different transfer modes in a controlled environment.</p><figure><figcaption><p>Multiplayer Mario Kart with Game Bub and a GBA</p></figcaption></figure><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/animal-crossing-gamecube-link.jpg\" alt=\"Game Bub linked to a GameCube playing Animal Crossing\" width=\"1440\" height=\"1801\"><figcaption><p>Game Bub linked to a GameCube playing Animal Crossing</p></figcaption></figure><p>During the emulator development, I had used various test ROMs (mentioned before) to test basic functionality in isolation. As my emulator became mature enough to run commercial games, however, I started to shift some of my focus to accuracy-focused test ROMs.</p><p>These test ROMs (such as the <a href=\"https://github.com/mgba-emu/suite\">mGBA test suite</a>) generally test really specific hardware quirks and timing. For example, they might test what happens when you run an instruction that ARM calls “unpredictable”, or the exact number of cycles it takes to service an interrupt in specific scenarios, or <a href=\"https://bmchtech.github.io/post/multiply/\">the value of the “carry” flag after performing a multiplication</a>. These are the kinds of things that don’t actually matter for playing games, but present a fun challenge and a way to “score” your emulator against others. This also highlights the collaborative nature of the emulation development community: people sharing their research and helping each other out.</p><p>I won’t talk about all of the tests here (for my emulator’s test results, <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/docs/accuracy.md\">see this page</a>). But I do want to mention the <a href=\"https://tcrf.net/AGS_Aging_Cartridge\"></a>. This is an official test cartridge from Nintendo, likely used as part of a factory test or RMA procedure. Apparently, Nintendo has  used it to test their emulators (e.g. their GBA emulator on the Nintendo Switch). This test has generally been considered to be difficult to pass (it tests some specific hardware quirks), but it’s easier now that the tests have been <a href=\"https://github.com/DenSinH/AGSTests/\">thoroughly reverse engineered and documented</a>. Still, passing it is a nice milestone:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/agb-aging-cartridge.png\" alt=\"Passing the AGB Aging Cartridge\" width=\"964\" height=\"700\"><figcaption><p>Passing the AGB Aging Cartridge</p></figcaption></figure><h2>Second hardware revision<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#second-hardware-revision\">#</a></h2><p>Towards the end of 2024, approximately one year after I originally designed Game Bub, I decided to make a second hardware revision. Over the past year, I had been keeping track of all of the things I would want to change in a future revision. Since the first version of Game Bub miraculously worked without any major issues, this list was primarily minor issues and ergonomics changes.</p><p>I fixed the <a href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#trouble-with-power-domains\">minor I2C power issues</a>, removed the <a href=\"https://en.wikipedia.org/wiki/Reference_designator\">reference designators</a> from the PCB silkscreen (they looked messy with the dense board, and I didn’t use them for anything anyway), and changed around some test points. I improved the rumble circuit to be more responsive, and switched to a PCB-mounted vibration motor.</p><p>The first version of Game Bub was fairly thick, measuring 12.9mm at the top and 21.9mm on the bottom. The thickness of the rear enclosure was dictated by the thickness of Game Boy cartridges, but I made several changes to the front. I moved the  (8.5mm!) link port to the back, and removed the HDMI port (more on that later). I changed the headphone jack (5.0mm tall – no wonder they started getting removed from phones) to a mid-mount one that sunk into the PCB and reduced the overall height.</p><p>I also switched from an  module (3.1mm depth) to an  (2.4mm depth). I should have done this from the beginning, I just didn’t even know the ESP32-S3-MINI existed. This had the side effect of giving me 3 more GPIOs, which allowed me to put the FPGA and LCD on separate SPI busses, avoiding the minor issue of an unpowered FPGA interfering with LCD communication, and allowed for faster boot because the LCD could be configured at the same time as the FPGA.</p><p>I switched the speakers, from the fully-enclosed <a href=\"https://www.sameskydevices.com/product/audio/speakers/miniature-(10-mm~40-mm)/ces-20134-088pmb\">CES-20134-088PMB</a> to the <a href=\"https://www.sameskydevices.com/product/audio/speakers/miniature-(10-mm~40-mm)/cms-160903-18s-x8\">CMS-160903-18S-X8</a>. I made this change primarily for ease of assembly. The first speaker had a wire connector that plugged into the board, and I found it difficult to connect during assembly without having the wire interfere with buttons. The new speaker is smaller and has a spring contact connector, so it just presses against the PCB as the device is assembled. This required some speaker enclosure design – an unenclosed speaker in free air sounds quiet and tinny.</p><p>I reworked the layout of the face buttons and D-pad to match the spacing of the Nintendo DSi. This allowed me to use the silicone membranes from the DSi for an improved button feel and reduced rattling. I was also hoping to use the plastic buttons from the DSi (which were higher quality compared to my 3D printed buttons), but even with the new thinner design, the buttons weren’t quite tall enough to be easily pressed.</p><p>I created another timelapse of my modifications to produce the second version of the PCB:</p><figure><figcaption><p>Revision 2 board layout timelapse</p></figcaption></figure><p>For the second revision of the enclosure, I switched to <a href=\"https://en.wikipedia.org/wiki/Fusion_360\">Fusion 360</a> for the CAD work. While I would have preferred to keep using FreeCAD, I found that it was making it harder for me to be productive. Fusion 360 has a free version for hobbyists (with some limitations that have gradually increased over time), and overall I’ve found it very pleasant to use.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/enclosure-rev2-cad.png\" alt=\"Fusion 360 view of the second enclosure, fully assembled\" width=\"1774\" height=\"1472\"><figcaption><p>Fusion 360 view of the second enclosure, fully assembled</p></figcaption></figure><p>Unlike with the first revision, I waited until I had a final design for both the enclosure and the PCB before getting anything manufactured. This let me go back and forth, making small modifications to each of them as needed.</p><p>I wanted to make the end result look more polished and professional, so I contracted a factory to produce custom LCD cover glass, made out of 0.7mm thick tempered glass with a black silkscreen. It was relatively expensive for a low quantity order, but I’m really happy with how it turned out.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/lcd-cover-glass.jpg\" alt=\"Custom LCD cover glass with adhesive backing\" width=\"1440\" height=\"1369\"><figcaption><p>Custom LCD cover glass with adhesive backing</p></figcaption></figure><h3>Manufacturing and assembly<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#manufacturing-and-assembly\">#</a></h3><p>I got the PCBs manufactured and assembled, this time with black solder mask to look .</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/pcb-rev-2.jpg\" alt=\"Assembled PCB, revision 2\" width=\"1440\" height=\"1073\"><figcaption><p>Assembled PCB, revision 2</p></figcaption></figure><p>I had two enclosures made. The first was black PA-12 Nylon, printed with <a href=\"https://en.wikipedia.org/wiki/Multi-jet_fusion\">MJF</a>. Nylon is strong and durable, and the MJF 3D printing technology produces a slightly grainy surface that’s really pleasant to hold in your hand.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/nylon-closeup.jpg\" alt=\"Closeup of the nylon grainy texture\" width=\"1440\" height=\"894\"><figcaption><p>Closeup of the nylon grainy texture</p></figcaption></figure><p>The second one was made of transparent resin (SLA, like before). This lets me show off the PCB that I worked so hard on, and evokes the transparent electronics trend from the 90s.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gamebub-transparent.jpg\" alt=\"Transparent Game Bub\" width=\"1440\" height=\"1920\"></figure><p>Assembly was a lot easier this time around: the silicone membranes held the face buttons in place, the speakers had a spring contact instead of wires, and the shoulder button assembly was better. In the first revision, I had excessively large tolerances because I wasn’t sure how precise the 3D printing would be. In the second version, I was able to shrink these.</p><p>The final product looked and felt a lot better, too. The edges were more rounded, and the device was thinner and easier to hold. The buttons felt  better to press and didn’t rattle around, and the cover glass over the LCD added polish.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gamebubs-side-by-side.jpg\" alt=\"First revision (left), second revision (center and right)\" width=\"1440\" height=\"742\"><figcaption><p>First revision (left), second revision (center and right)</p></figcaption></figure><p>I previously mentioned that I removed the full-size HDMI port from the first revision. I had first planned to change it to a mini-HDMI or micro-HDMI port to reduce the size, but I was worried about durability.</p><p>What I  wanted to do was output video through the USB-C port, avoiding the need for any HDMI port at all. Unfortunately, I had already concluded earlier that I wouldn’t be able to output <a href=\"https://en.wikipedia.org/wiki/DisplayPort\">DisplayPort</a> video signals from the FPGA, which meant that I couldn’t use the standard USB-C DisplayPort alternate mode.</p><p>However, an idea struck me towards the end of 2024: I didn’t actually  to use the DisplayPort alt-mode. The USB-C connector, in addition to the USB 2.0 D+/D- pins, has four differential pairs (for USB superspeed). Conveniently, HDMI  uses four differential pairs. The USB specification allows for vendor-specific alt-modes, so I could just implement my own, outputting the HDMI signal directly from the FPGA over the additional pins. Then I could build a custom dock that takes those pins and connects them to the data lines of an HDMI port.</p><p>According to the USB specification, alternate modes must be negotiated by both sides first, using the USB-C Power Delivery (USB-PD) protocol, to prevent them from interfering with devices that aren’t expecting them. I don’t actually have a USB-PD controller in Game Bub (too much added complexity), so I took a shortcut: have a microcontroller in the dock communicate with the Game Bub over regular USB and perform a handshake before enabling HDMI output from the FPGA. Once Game Bub detects that it’s been disconnected from the dock, it can just switch back to using the internal display.</p><p>I realized that the dock also presents another opportunity for controller support. I originally wanted to build wireless controller support into the handheld, but the ESP32-S3 only supports Bluetooth Low Energy, and the majority of controllers use Bluetooth Classic. Fortunately, the <a href=\"https://en.wikipedia.org/wiki/Raspberry_Pi#Raspberry_Pi_Pico\">Raspberry Pi Pico W</a> (with an RP2040 MCU) supports both types of Bluetooth, so I just decided to use that as the microcontroller on the dock. Game controllers connect to the dock over Bluetooth, and the Pico sends the controller inputs to the device. I wired up the  and  USB-C pins as a direct connection between the FPGA and the dock for low latency input.</p><p>The RP2040 acts as the USB host, and Game Bub only needs to be a device. I also added a USB hub chip and some additional USB ports on the back of the dock to allow for wired controller support too. Just like with wireless controllers, the dock handles the direct controller communication, and just passes inputs back to the main Game Bub unit.</p><p>Since the dock is so simple (comparatively), it only took about a day to design and lay out.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dock-pcb.jpg\" alt=\"Assembled dock PCB\" width=\"1440\" height=\"1016\"></figure><p>I had also hoped to use the dock to solve another problem around HDMI output: HDMI sinks (monitors, TVs) pull the HDMI data lines up to 3.3 volts, and can actually backfeed power to the HDMI source. For Game Bub, this meant that a powered-off unit would turn itself on when connected over HDMI. I used a HDMI buffer chip in the dock to try to alleviate this problem, but the chip I used wasn’t actually properly suited to this use-case and interfered with video output, so I had to carefully rework the board to bypass the chip. I’ll have to fix it in a later revision.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dock-rework.jpg\" alt=\"Bypassing the HDMI buffer chip\" width=\"1440\" height=\"1359\"><figcaption><p>Bypassing the HDMI buffer chip</p></figcaption></figure><p>After the rework, HDMI output worked! The rest of the features are still a work in progress.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/bare-gamebub-in-dock.jpg\" alt=\"Game Bub PCB on the dock, connected to an external monitor\" width=\"1440\" height=\"2116\"><figcaption><p>Game Bub PCB on the dock, connected to an external monitor</p></figcaption></figure><p>Congratulations on reading this far! This writeup ended up being incredibly long, even with a lot of details left out.</p><p>I’m proud of what I accomplished over the last year and a half: I met all of my goals to produce a polished handheld FPGA retrogaming device. I pushed my electrical engineering and product design skills to the limit, and learned a lot in the process. Professional product and hardware designers deserve  respect.</p><p>I deliberately designed this project with lots of possible extension opportunities to keep me occupied for a long time. I worked hard to get to the point where I’m comfortable sharing Game Bub with the world, but I still have a long list of TODOs for the future.</p><p>In the near term, I’m going to work on finishing the dock, implementing wireless controller support (and maybe wired). I plan to use the <a href=\"https://github.com/ricardoquesada/bluepad32\">Bluepad32</a> library to do so.</p><p>I also want to improve the accuracy of my Game Boy Advance emulator: my goal here is to someday pass the entire <a href=\"https://github.com/mgba-emu/suite\">mGBA test suite</a>. I hope that I can contribute back to the wonderful  community with my emulator, and I plan to write-up some of my research around the GBA cartridge interface and link port.</p><p>I have a long list of mostly minor changes to make to the MCU firmware: improving UI render performance, bits of polish like low battery notifications, eliminating display glitching when reloading the FPGA, and that sort of thing. I also plan to add more utilities, like a cartridge dumper and save backup/restore feature.</p><p>Some day, I want to emulate the <a href=\"https://en.wikipedia.org/wiki/Game_Boy_Advance_Wireless_Adapter\">Game Boy Advance Wireless Adapter</a> over Wi-Fi, e.g. with <a href=\"https://docs.espressif.com/projects/esp-idf/en/stable/esp32s3/api-reference/network/esp_now.html\">ESP-NOW</a>. This won’t be compatible with the original wireless adapter, unfortunately, since that uses raw 2.4 GHz modulation rather than Wi-Fi.</p><p>I designed Game Bub with extremely low production volumes in mind, using off-the-shelf commodity parts to keep the overall cost down. However, there are a few things I would have liked to be able to do, but are only possible with much higher volumes:</p><ul><li>A better LCD module (likely custom): native landscape mode to avoid the need for triple-buffering. Ideally a 720x480 resolution display, to allow for 3x GBA scaling and filter effects.</li><li>High-quality injection molded case and buttons: 3D printing is great for low volume production, but an injection molded case would be great. It would be more precise (allowing for tighter tolerances), stronger, and allow for significantly more color options.</li><li>Custom battery pack: or at least customizing the length of the connector wire. The current solution is hacky and doesn’t make the best use of internal space, due to limited off-the-shelf battery options.</li><li>Smaller BGA parts for SRAM and SDRAM to free up board space (and move internal signals to 1.8 volts): this is actually something that would be possible in smaller volumes too, if I were willing to send parts from Mouser or DigiKey to JLCPCB for assembly.</li></ul>","contentLength":69379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43027335"},{"title":"Show HN: Letting LLMs Run a Debugger","url":"https://github.com/mohsen1/llm-debugger-vscode-extension","date":1739354054,"author":"mohsen1","guid":202,"unread":true,"content":"<p>I just built an experimental VSCode extension called LLM Debugger. It’s a proof-of-concept that lets a large language model take charge of debugging. Instead of only looking at the static code, the LLM also gets to see the live runtime state—actual variable values, function calls, branch decisions, and more. The idea is to give it enough context to help diagnose issues faster and even generate synthetic data from running programs.</p><p>* Active Debugging: It integrates with Node.js debug sessions to gather runtime info (like variable states and stack traces).</p><p>* Automated Breakpoints: It automatically sets and manages breakpoints based on both code analysis and LLM suggestions.</p><p>* LLM Guidance: With live debugging context, the LLM can suggest actions like stepping through code or adjusting breakpoints in real time.</p><p>I built this out of curiosity to see if combining static code with runtime data could help LLMs solve bugs more effectively. It’s rough around the edges and definitely not production-ready</p><p>I’m not planning on maintaining it further. But I thought it was a fun experiment and wanted to share it with you all.</p><p>Check out the attached video demo to see it in action. Would love to hear your thoughts and any feedback you might have!</p>","contentLength":1250,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43023698"},{"title":"TL;DW: Too Long; Didn't Watch Distill YouTube Videos to the Relevant Information","url":"https://tldw.tube/","date":1739326517,"author":"pkaeding","guid":201,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43021044"},{"title":"Show HN: Mapping the Unix Magic Poster – An Interactive Annotation Project","url":"https://drio.github.io/unixmagic/","date":1739312557,"author":"drio","guid":238,"unread":true,"content":"<p>I built this as a static site that lets us annotate the Unix Magic poster by placing markers on references and adding descriptions to explain them. I've added a few so far, but there's much more to document.</p><p>What I love about this approach is that contributions happen not just on the site itself but also through PRs, where we can discuss and refine the details of each reference. Feel free to send a PR!</p><p>Would love feedback, suggestions, and PRs from the community!</p>","contentLength":465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43019136"},{"title":"Show HN: I made a tiny book using a pen-plotter and AI","url":"https://muffinman.io/blog/the-tiny-book-of-great-joys/","date":1739209951,"author":"stankot","guid":237,"unread":true,"content":"<p>If you are interested in how I over-engineered the process of making a tiny book for my wife, using AI, a pen plotter, a 3D printer, and a lot of time, you are in the right place. The book is titled  , and here is how it turned out:</p><p>My wife is delighted with it, so it was worth all the effort.</p><p>This post will take you through the process. It will be a long one, but please stick around - I promise there will be a lot of pretty pictures.</p><p>Here is the outline of the post:</p><p>I had this idea for a while after seeing something similar somewhere on the internet.. Since then, I always wanted to make one for my wife - a physically small book with a bunch of small drawings of our memories together, inside jokes, and little things she likes.</p><p>I wanted the illustrations to be hand-drawn, and I had a plan to ask my friend to do them. But I knew he would refuse any kind of payment, so I felt bad adding more work to his plate. So I shelved the idea, but every now and then, it would pop up in my head.</p><p>Fast forward a few years - we got a kid, and our routine completely changed. We are enjoying it a lot, but it can be very exhausting, and every day seems identical to the last. That's why I decided I needed to do something for her to break the routine. The book idea seemed perfect - personal and handcrafted - so I gave it a try.</p><p>To be able to do everything myself, I went to create digital drawings and then draw them on paper using my trusty pen plotter.</p><p>With the idea in place, I moved on to creating the drawings - which turned out to be a challenge of its own.</p><p>For pen plotting, one needs vector files, so I started drawing in Figma. Unfortunately, I quickly realized that my drawing skills would not get me the result I had envisioned. Determined to do it this time, I decided to try using AI to generate images.</p><p>I got myself a Midjourney subscription and started playing with it. It took a lot of failed attempts to figure out how to get drawings that were simple and had a strong hand-drawn feel to them. Even then, I ended up editing every one of them, but more on that later.</p><p>One of the first images I was satisfied with (it didn't end up in the book, though):</p><p>It took a lot of time, but it was fun. Failed attempts were often quirky and funny, and I was learning how to use the tool. And it made me feel like a secret agent, doing it next to my wife, who had no idea what I was up to.</p><p>I may be wrong, but I think Midjourney wasn't built for the kind of illustrations I had in mind. I was after simple, hand-drawn illustrations that felt personal. Luckily, I found a style reference () that worked well for my case. I used it to generate almost all of the drawings that ended up in the book. For those who haven't used Midjourney - you can use images as style references to influence the style of images you want to generate.</p><p>Most of my images were generated using that  code and a  between 150 and 400 (it can go from 0 to 1000).</p><p>As for the prompts, these are the key terms I combined with the description and the style reference:</p><ul><li>isolated on white background</li></ul><p>It took me a lot of tries - between 10 and 30 attempts for each image you see in the book.</p><p>Once I solved the image generation part, I had to figure out how to turn them into vector files for plotting. The first thing I tried was something similar to halftone. As you can see below, in this process, the images completely lost the hand-drawn feel.</p><p>Then I remembered <a href=\"https://www.instagram.com/p/CNJ_ZBOHZKj/\">this plot</a> of Marble Machine X I did a while ago, for which I used AutoTrace to convert the original image to a vector file. The great thing about AutoTrace is that it supports \"centerline tracing\". And this time, I learned that Inkscape has a great AutoTrace plugin, which made it even easier to convert.</p><h3>What makes centerline tracing different</h3><p>Most of the tools that convert raster to vector images do it by outlining shapes. This is not suitable for plotting, as each line in the original image becomes a sausage-like shape. Centerline tracing, on the other hand, tries to draw a single line following the middle path through shapes. Don't worry if it sounds confusing; the example below should make things clearer.</p><p>Here is the image of Link from  generated by Midjourney:</p><p>After applying a common vectorization technique, we get this. As you can see, each line in the original drawing is now outlined, creating this messy-looking image.</p><p>But if we use centerline tracing, it suddenly looks a lot more like a drawing. It is not perfect, but don't worry - we are going to clean it up in the next step.</p><p>In the points where lines touch or cross, AutoTrace is not sure which line to follow and creates these funky-looking joints. Here is an exaggerated example to show you what I'm talking about. Input is the raster image at the top and the vectorized result is at the bottom:</p><p>But I found out that if I roughly separate these lines, I get a much better result.</p><p>Let's now apply this technique to the image of Link we've seen above. After separating lines (and some cleaning up) this is the image I ended up with. It is rough, but it is only used as an input for the tracing process, so it doesn't really matter. This was manual and somewhat tedious process, but I enjoyed it overall. It was a sort of meditation for me.</p><p>And finally, when we trace this image, we get a really nice and clean vector file perfect for plotting. <img alt=\"Very clean vectorized image of Link\" src=\"https://muffinman.io/img/tiny-book/link-05-after.png\"></p><p>Here is another example. We start with the image I generated using Midjourney:</p><p>After editing, removing details and separating lines, we get this one:</p><p>And the traced vector result:</p><p>You'll notice that in both examples I did  . I did that for pretty much all of the images, to fix things I wasn't able to polish using prompts. I also removed a lot of details to make sure images are crisp and readable at the small size.</p><p>All of this took a lot of experimentation, but it gave me a pretty solid workflow which I used to generate all of the images. The complete flow looks like this:</p><ul><li>Generate images using Midjourney.</li><li>Upscale them two times, because upscaled images were easier to edit and tracing was more precise.</li><li>Clean up, redraw and separate lines by hand using Gimp.</li><li>Use Inkscape plugin to run AutoTrace centerline tracing.</li></ul><p>It took me a while to generate all the images, and the fact that I was trying to keep it a secret from my wife didn't help. I think I did it over the span of two weeks, mostly in the evening after she would go to bed.</p><h3> never stood a chance!</h3><p>Before we continue I just want to show you two funky images of Link that really made me laugh:</p><p>With the drawings ready, I turned to the next crucial part - the text. I first wanted to write everything by hand, photograph it and then vectorize it in the same way I did with the images. But it was a hassle - I had to do a lot of editing for text to look as my handwriting.</p><p>Evil Mad Scientist, the maker of my pen plotter, has a fantastic tool called <a href=\"https://wiki.evilmadscientist.com/Hershey_Text\">Hershey Text</a>. It contains a bunch of single-line fonts ideal for plotting. I chose the EMS Elfin font as it looked playful and hand-drawn. I used it to write all of the text in the book and I think it turned out great.</p><p>The tricky part with bookbinding is that pages are not printed in order, but in a way that when you fold the sheets in half, you get the right order. I used Figma to design the layout, with a great care to make sure pages are in order after double-sided plotting.</p><p>Here is the layout laid out on A4 sized paper. Sorry for blurring the text, but a lot of it is very personal and I want to keep it for our eyes only.</p><p>Plotting is the part that went the smoothest, but not without hiccups. I usually use Pigma Micron blackliner markers. They use archival quality ink and they are literally indestructible. But this time, even the thinnest one I had was too thick for the book this small.</p><p>Here you can see the first  using markers of 0.2mm and 0.1mm thickness respectively. Lines got a bit smudged and looked much thicker than I expected. This was also the moment I realized I need to remove  from the images to make them readable at this size.</p><p>I needed to find a thinner pen.</p><h3>Technical pen to the rescue</h3><p>Blackliner markers were made as a more practical replacement for technical pens. But from what I've read, an old-school technical pen was the only thing capable of achieving super-fine lines I wanted. I went online and ordered Rotring Isograph 0.2mm. As soon as it arrived I sneaked out to my study and did another test plot using it. Oh boy, was I happy when I saw the result:</p><p>Lines were thin and crisp and at this point I was convinced the project will be a success!</p><p>All of the first plots were done on 120gsm printer paper. It is somewhat thick paper and drawings looked fantastic. Unfortunately, when I bound the pages together, the drawings and letters would get transferred on the opposite pages. I could probably get away with it, considering the whole hand made feel of the book. But I wanted it to be perfect.</p><p>A friend advised me to leave ink to dry for a few hours. I left each side to dry for 24 hours, but it smudged again. Next time I tried putting the plot (before cutting the pages) between two sheets of papers and pressing it with heavy books. I did that for more than 24 hours, but still after cutting and bounding the pages, they got smudged again. At this point I was becoming somewhat desperate. As the last resort I ordered different, 100gsm paper and to my relief it worked! Crisis averted!</p><p>In the final version you can still see tiny traces on a few pages, but these are barely visible and don't really bother me.</p><p>After plotting and cutting I was left with a stack of somewhat delicate pages. Now, it was finally time to turn them into a book.</p><p>As you can imagine, I had zero bookbinding experience. There are a lot of resources online, but two of them were crucial for my project as they were on how to bind tiny books:</p><p>After reading and watching these and a few generic articles on bookbinding, I gathered enough info to try doing it myself. I thought I was super clever because I 3D printed sides and spine of the book. I designed sewing holes in the spine so I can connect the pages directly to it without using glue. It was a decent idea, but it left a gap between two  . Still, I went with it for the first try.</p><p>I laid everything down on the canvas that the book would be wrapped in and started assembling it. But I made a crucial mistake - I used super glue. It dries quickly, it is stiff, and doesn't glue 3D printed plastic well and it dissolved the paper I used. Long story short, I made a mess. But I didn't stress too much, I just proclaimed that version is a prototype and used it as a learning experience.</p><p>I ordered proper bookbinding glue (PVA). While I was waiting for it, I focused on properly sewing the pages together.</p><p>The first time I sewed the pages together, I poked the holes by hand and they were somewhat uneven. Again, it was nothing major, but I didn't like it. So I designed and 3D printed a simple tool to help me drill the holes evenly.</p><p>The tool has two parts, and the pages fit snugly between them. Both top and bottom parts have holes, so I was able to put the needle through and poke perfectly even holes in the pages. I'm very proud of this silly contraption.</p><p>Here you can see all of the eight sheets with sewing holes.</p><p>Fun fact, I designed all 3D parts using JavaScript and <a href=\"https://replicad.xyz/\">Replicad</a> library. Here is <a href=\"https://studio.replicad.xyz/workbench?from-url=https://muffinman.io/img/tiny-book/model.js\">a link</a> if you want to play with the model in your browser.</p><p>But I ditched the 3D printed spine and used the technique called , which works great when you have only two signatures. It made signatures way more tight than when I connected them separately to the 3D printed spine.</p><p>When the glue arrived, I plotted everything again and took it from the top. I swapped 3D printed sides for cardboard. Using proper glue was a game changer. I had enough time to apply it before it hardened, and when it dried it stayed flexible. And when it got onto my fingers, it was easy to remove. Everything was much cleaner, and I finally managed to put it all together.</p><p>Unfortunately, I was rushing to finish the book, so I didn't take any photos of the process. But here are a few I do have:</p><p>If you are an experienced bookbinder and reading this, I'm sorry for the bookbinding crimes I probably committed. I promise I won't use super glue again.</p><p>It looked great! It was not perfect (more on that below), but I was super happy with how it turned out. It had a distinct handcrafted feel to it, the images turned out fantastic, and I think I really managed to bring out a personal touch with it.</p><p>On the day I finished the book and gave it to my wife, we were both exhausted (our kid was teething, and we had a very rough night), so I thought she would appreciate a little pick-me-up.</p><p>When I gave it to her, the first thing she asked was, \"Will I cry?\". She was brave, but it definitely got her all mushy and made her day. After reading, she carefully put it on the shelf, out of the reach of the little one.</p><p>Then I asked her if she ever suspected I was preparing a surprise for her, and she said that she had no idea. But she also said that she thought it was weird that I would often plot something and not brag about it to her afterwards. It was true, I love showing her my work, but luckily she didn't give it too much thought, and I was able to finish my secret project.</p><h3>One thing I would like to fix</h3><p>Like I mentioned, the book isn't perfect. The sides are a bit too large, so the pages seem too deep inside when the book is closed. For the same reason, the end pages turned out to be a bit short, which gives it a weird, uneven look. It is purely aesthetic, but I think it is the only thing keeping it from being perfect.</p><p>Lesson learned if I ever end up doing something similar.</p><p>It took way longer than it should have—it took me a month and a half to finish it. It took so long because I did it in secrecy, which meant working late in the evenings when my wife and kid were asleep. A bunch of little failures... ehm, I mean  also prolonged the project. And finally, I had to order multiple things, so I was blocked a few times while I was waiting for four different deliveries.</p><p>But the final assembly took me around two and a half hours from start to finish - plotting, cutting, sewing, and bookbinding. Mostly because I had already practiced all of them and defined the exact process.</p><p>It was so much fun. I love projects that span across multiple disciplines. This one touched AI, drawing, plotting, modeling, 3D printing, sewing, and bookbinding. I encountered a lot of little hiccups, but I also learned about all of them. Some of the errors I made could have been avoided if I had been more patient. But I hope you'll cut me some slack - I was super excited and eager to see how it would turn out, and I had limited time windows when I could do it in secrecy. Still, I need to take it as a lesson - being patient will help me save time when doing projects like this one.</p><p>The highlight for me was that I could do it without an illustrator. Love it or hate it, AI ended up being a fantastic tool that filled the gap in my skill set, which was crucial for making the book.</p><p>I hope you enjoyed this write-up as much as I enjoyed making the book and writing the post. And I do hope I inspired you to try making something of your own. If I did, please reach out on GitHub, I would love to see it.</p>","contentLength":15245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43003052"},{"title":"Show HN: Global 3D topography explorer","url":"https://topography.jessekv.com/","date":1739202885,"author":"jessekv","guid":236,"unread":true,"content":"<div>\n                Click catchements or regions on the map to render them in 3D.\n                <a href=\"https://jessekv.com/post/watersheds/\" target=\"_blank\">Learn more</a></div>","contentLength":105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43001688"},{"title":"Show HN: A unique generated maze to share with your valentine","url":"https://love.berk.es/","date":1739198155,"author":"berkes","guid":235,"unread":true,"content":"<dd><p> is an art project. It is so called <a href=\"https://en.wikipedia.org/wiki/Generative_art\">generative art</a>: I write the software, and the software creates art autonomously. \n            A nice introduction into generative art is <a href=\"https://www.youtube.com/watch?v=4Se0_w0ISYk\">this talk by Tim Holman</a>.\n          </p></dd><dd><p>\n          In a gallery in London, I came across gorgeaus screenprints by <a href=\"https://rbyrneart.com/\">Ricky Byrne</a>. I loved their use of color and hand-produced feel. Attention for color, layout, tension.\n          So I started experimenting with maze generation algorithms in Rust, with Nannou.<p>\n          In the process, I decided to make it a web app, for valentine, so everyone can create their own maze. And ported the Rust code to TypeScript in a tiny web app. </p><a href=\"https://www.youtube.com/watch?v=HyK_Q5rrcr4\">The Coding Train has a great tutorial set on maze generation</a>. I used the same common recursive backtracking algorithm, because the aesthetics are what I was looking for. </p></dd><dd><p>\n          The names you provide are used to generate <a href=\"https://en.wikipedia.org/wiki/Random_seed \">unique randomnes</a>. \n          This is used in a <a href=\"https://en.wikipedia.org/wiki/Maze_generation_algorithm\">maze generation algorithm</a> to create a maze. \n          I deliberately chose to animate the maze generation, to show the process. It is a slow process, but I think it is interesting to watch it carve out your maze. </p></dd><dd><p>\n          Saving, copying and sharing the maze is disabled until the maze is generated. Once the maze is generated, you can save, copy and share the maze. \n          Sharing, copying won't work on all browsers and is affected by some browser plugins or settings. The best result is on Chrome on Android. \n          </p></dd><dd><p>, the only parameter you can provide is the names. The maze is generated based on these names, and the algorithm is fixed. This is by design. Only the two names determine the artwork. \n            However, you can change the source code, see below.\n          </p></dd><dd><p>\n            The code is available on <a href=\"https://github.com/berkes/art/tree/main/lost-in-love\">GitHub</a>. Feel free to fork, change and improve it, or just have fun with it. </p></dd><dd><ul><li><a href=\"https://nannou.cc/\">Nannou</a> for the original maze generation</li><li><a href=\"https://vitejs.dev/\">Vite</a> for the build and web stuff</li></ul></dd>","contentLength":1890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000764"},{"title":"Show HN: HTML visualization of a PDF file's internal structure","url":"https://github.com/desgeeko/pdfsyntax/blob/main/docs/browse.md","date":1739195573,"author":"desgeeko","guid":234,"unread":true,"content":"<p>Hi,\nI've just finished a rebuild of this function and added a lot of new features: info, page index, minimap, inverted index,... \nI think it may be useful for inspection, debugging or just as a learning resource showcasing the PDF file format.\nThis is a pet project and I would be happy to receive some feedback!\nRegards</p>","contentLength":320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000303"},{"title":"Show HN: Infinite horizontal arrays of text editors","url":"https://zeminary.com/arrays/app.html","date":1739150705,"author":"tsydenzhap","guid":233,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42995991"},{"title":"Show HN: Searchable library of free audiobooks","url":"https://booksearch.party/","date":1739137953,"author":"libridev","guid":232,"unread":true,"content":"<p>\n                If you're seeking free audiobooks and an efficient book finder, booksearch.party is an excellent resource. This platform aggregates a vast collection of audiobooks for free from various sources, including LibriVox, Project Gutenberg, and Lit2Go, into a user-friendly, searchable database.</p><p>LibriVox offers a wide range of public domain audiobooks, read by volunteers worldwide. Their catalog includes thousands of titles across various genres, all available for free.</p><p>Project Gutenberg provides over 60,000 free eBooks, many of which have been converted into audiobooks. This extensive collection includes classic literature and historical texts, all accessible without cost.</p><p>Lit2Go is a free online collection of public domain fiction, poetry, and nonfiction audiobooks. It offers a user-friendly interface where users can browse texts and listen to individual chapters.</p><p>By compiling these resources, booksearch.party serves as a comprehensive book finder, allowing users to easily search and access a wide array of free audiobooks. Whether you're interested in classic literature, historical documents, or educational materials, this platform simplifies the process of discovering and enjoying audiobooks at no cost.</p>","contentLength":1230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42994440"},{"title":"Show HN: My first side project, streamlined book clubs on Slack","url":"https://booktalk.club/","date":1739107690,"author":"Papamanolis","guid":231,"unread":true,"content":"<p>Get personalized suggestions and find the</p><p>perfect reads for your team</p>","contentLength":68,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42990540"},{"title":"Show HN: Daily-notes.nvim – fuzzy time journal and planning plugin","url":"https://github.com/fdavies93/daily-notes.nvim","date":1739083330,"author":"fdavies93","guid":230,"unread":true,"content":"<p>I wrote an nvim plugin that does fuzzy time parsing on plain english dates to help you create + organise periodic notes. I use it daily at work and home. Hope it's helpful to others. :)</p><p>note: not using NLP, LLMs or 'true' fuzzy parsing as per academic literature; just normal recursive descent parsing</p>","contentLength":300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42988946"}],"tags":["hn"]}