{"id":"6W9","title":"HN","displayTitle":"HN","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":72,"items":[{"title":"Huawei releases an open weight model trained on Huawei Ascend GPUs","url":"https://arxiv.org/abs/2505.21411","date":1751441801,"author":"buyucu","guid":180369,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44441089"},{"title":"Hilbert's sixth problem: derivation of fluid equations via Boltzmann's theory","url":"https://arxiv.org/abs/2503.01800","date":1751416308,"author":"nsoonhui","guid":180368,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44439242"},{"title":"Australians to face age checks from search engines","url":"https://ia.acs.org.au/article/2025/australians-to-face-age-checks-from-search-engines.html","date":1751414366,"author":"stubish","guid":180367,"unread":true,"content":"<p>Australians using search engines while logged in to accounts from the likes of Google and Microsoft will have their age checked by the end of 2025, under a new online safety code co-developed by technology companies and registered by the eSafety Commissioner.</p><p>Search engines operating in Australia will need to implement age assurance technologies for logged-in users in \"no later than six months”, under <a href=\"https://www.esafety.gov.au/sites/default/files/2025-06/Schedule-3-Internet-Search-Engine-Services-Online-Safety-Code-%28Class-1C-and-Class-2-Material%29.pdf\">new rules</a> published on Monday.</p><p>While only logged-in users will be required to have their age checked, many Australians typically surf the web while logged into accounts from Google, which <a href=\"https://ia.acs.org.au/article/2024/google-claims-it-doesn-t-dominate-search-in-australia.html\">dominates Australia’s search market</a> and also runs Gmail and YouTube; and Microsoft, which runs the Bing search engine and email platform Outlook.</p><p>If a search engine’s age assurance systems believe a signed-in user is “likely to be an Australian child” under the age of 18, they will need to set safety tools such as “safe search” functions at their highest setting by default to filter out pornography and high impact violence, including in advertising.</p><p>Currently, Australians must be at least 13 years of age to manage their own Google or Microsoft account.</p><p>Age assurance methods can include age verification systems, which use government documents or ID; age estimation systems, which typically use biometrics; and age inference systems, which use data about online activity or accounts to infer age.</p><p>Search engines will not be required to implement age assurance measures for users who are not logged in to their services, according to the new rules.</p><p>“Internet search engine services are designed for general public use, with or without an account,” the code states.  </p><p>However, users who are not logged in should also expect “default blurring of images of online pornography and high-impact violence material detected in search results”.</p><p>Other compliance measures in the code which search providers must abide by include improving search and age assurance technologies over time, preventing autocomplete predictions “that are sexually explicit or violent”, and responding to searches about eating disorders or self-harm with crisis prevention information.</p><p>Google and Microsoft were contacted for comment.</p><p>Earlier this year Google said it would begin <a href=\"https://ia.acs.org.au/article/2025/google-will-use-ai-to-estimate-a-user-s-age.html\">using artificial intelligence to estimate users' ages</a>, beginning with tests in the United States, while Microsoft previously stated it had explored age assurance methods while considering potential impacts for user safety and privacy.</p><h4><b>Changes ‘designed to protect’ Australian kids</b></h4><p>The new rules for search engine operators were “designed to protect\" Australian children, according to the code.</p><p>Drafting of the code was co-led by Digital Industry Group Inc. (DIGI), which was contacted for comment as it counts Google, Microsoft, and Yahoo among its members.</p><p>eSafety Commissioner Julie Inman Grant said she had registered three new codes submitted by the online industry, which covered harmful content on search engines, enterprise hosting services, and internet carriage services such as telecommunication firms.</p><p>The codes had been in the works since July 2024 and failure to comply with them could result in civil penalties of up to $49.5 million per breach, her office said.</p><p>The Commissioner said she had sought extra safety commitments from the industry on six outstanding codes, which covered the likes of app stores, device manufacturers, social media, and messaging services.</p><p>“It's critical to ensure the layered safety approach which also places responsibility and accountability at critical chokepoints in the tech stack including the app stores and at the device level, the physical gateways to the internet where kids sign-up and first declare their ages,” Inman Grant said.</p><h4><b>Push to protect children who use AI chatbots</b></h4><p>Members of the technology industry had also been asked to use the remaining six codes to strengthen their protections against generative AI chatbots engaging in harmful behaviours with children, Inman Grant said.</p><p>“We are already receiving anecdotal reports from school nurses, that kids as young as 10 are spending up to five hours a day with AI chatbots, at times engaging in sexualised conversations and being directed by the chatbots to engage in harmful sexual acts or behaviours,” she said.</p><p>Inman Grant said she would consider the changes proposed by the industry and would aim to make her final determination on the six outstanding codes by the end of July.</p><p>\"If I am not satisfied these industry codes meet appropriate community safeguards, I will move to developing mandatory standards,” she said.</p>","contentLength":4599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44439058"},{"title":"Using Sun Ray thin clients in 2025","url":"https://catstret.ch/202506/sun-ray-shenanigans/","date":1751412635,"author":"todsacerdoti","guid":179510,"unread":true,"content":"<p>i’ve used thin clients at home for quite a while - both for their  use (remotely accessing a desktop of another system); and in the sense of “modern thin clients are x86 boxes that are wildly overpowered for what they run, so they make good mini servers.”</p><p>recently, i saw a bulk lot of Sun Ray thin clients pop up on Trade Me (NZ’s eBay-like auction site) - and with very little idea of how many clients were actually included in this lot, i jumped on it. after a 9 hour round-trip drive (on some of the worst roads i’ve seen!), i returned home with the back of my car completely packed with Sun Rays. time for some interesting shenanigans!</p><p>when picking all of these up from the seller, i had guesstimated there was maybe 30 clients in total. turns out i was off by quite a bit.</p><p>i ended up bringing home:</p><ul><li>3x <a href=\"https://dogemicrosystems.ca/pub/Sun/System_Handbook/Sun_syshbk_V3.4/Systems/SunRay270/SunRay270.html\">Sun Ray 270</a> - 17” (1280x1024) LCD screens with integrated Sun Ray clients</li><li>4x <a href=\"https://web.archive.org/web/20170218173523/https://incarta.com.au/uvo\">Incarta Uvo</a> - 24” 1080p LCD screens with integrated clients\n    <ul><li>i can’t find any info about these other than the linked page on the Wayback Machine - if you know more about these, please send me an email!</li></ul></li><li>about 40 smart cards, for authentication/hotdesking</li><li>a small pile of Sun Type 7 USB keyboards, and some Sun-branded optical mice</li></ul><p>so that’s  clients all up!</p><p>a few days prior to picking all this up, i rented a storage unit in a local facility, and put some garage shelving units in there - and boy howdy i’m glad i did!</p><h2>setting up the Sun Ray Server Software</h2><p>looking at the Oracle (eugh.) documentation for the Sun Ray Server Software, it appeared there were two options: run it on ancient Linux, or run it on ancient Solaris. Oracle dropped support for the Sun Rays in 2014, as part of extinguishing everything Sun Microsystems stood for after the 2010 acquisition. i didn’t  want to have a RHEL 6 box kicking around, nor did i want to deal with trying to make Solaris 10 work in a VM on my home Proxmox cluster, so i did some digging.</p><p>enter  - well, in my case, OpenIndiana. illumos is, essentially, a fork of the pre-Oracle-acquisition OpenSolaris codebase. OpenIndiana is one of many illumos  (in a very similar sense to Linux distributions), and OpenIndiana is more suited for desktop use than most other illumos distributions. the OpenIndiana documentation has <a href=\"https://docs.openindiana.org/handbook/sunray/\">a section on setting up the Sun Ray Server Software on OpenIndiana</a>, but even with that in hand there was a lot of pieces to figure out on my own!</p><p>this is mostly a copy of the docs from the OpenIndiana handbook, with some adjustments to fix things i ran into. i did this on top of a text-only install - <code>OpenIndiana Hipster 2025.04 Text Install DVD (64-bit x86)</code> was the install media i used (from <a href=\"https://www.openindiana.org/downloads/\">https://www.openindiana.org/downloads/</a>).</p><p>to get the desktop environment installed:</p><div><div><pre><code># pkg install mate_install\n</code></pre></div></div><p>unlocking the dependencies for SRSS:</p><div><div><pre><code># pkg change-facet facet.version-lock.gnome/gnome-session=false\n# pkg change-facet facet.version-lock.gnome/gnome-settings-daemon=false\n# pkg change-facet facet.version-lock.system/display-manager/gdm=false\n# pkg change-facet facet.version-lock.library/gnome/libgnomekbd=false\n# pkg change-facet facet.version-lock.gnome/window-manager/metacity=false\n# pkg change-facet facet.version-lock.library/desktop/gnome-desktop=false\n# pkg change-facet facet.version-lock.cde/cde-runtime=false\n# pkg change-facet facet.version-lock.library/motif=false\n# pkg change-facet facet.version-lock.library/tooltalk=false\n# pkg change-facet facet.version-lock.compatibility/packages/SUNWxwplt=false\n</code></pre></div></div><p>setting up the package source, and installing the SRSS dependencies:</p><div><div><pre><code># pkg set-publisher --search-before=openindiana.org -g http://pkg.toc.de/sunray sunray\n# pkg set-publisher --non-sticky openindiana.org\n# pkg install sunray-essential\n</code></pre></div></div><p>after unpacking the Sun Ray Server Software installers (both the Solaris and Linux versions) into , i ran the  script from the OI Handbook, then tried to install SRSS, which bombed out spectacularly with package manager rejections of the <code>This version is excluded by installed incorporation consolidation/userland/userland-incorporation@...</code> sort. so here’s the correct (read: “worked for me!”) steps:</p><div><div><pre><code># /root/update_dhcp_dependency /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/\n# pkg set-publisher -g /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/ sunray\n# pkg uninstall entire userland-incorporation\n# pkg install SUNWut-srss SUNWut-srwc SUNWuti\n</code></pre></div></div><p>to make SRSS happy with isc-dhcp:</p><div><div><pre><code># rpm2cpio /root/srs_5.4.0.0-Linux.i386/Components/10-SRSS/Content/Sun_Ray_Core_Services_4.5/Linux/Packages/SUNWuto-4.5-44.i386.rpm | bsdtar -C /root -xf - ./opt/SUNWut/lib/dhcp/\n# sed 's#$UTDHCPDIR | sort#$UTDHCPDIR | gsort#g' -i.bak /root/opt/SUNWut/lib/dhcp/isc/dhcp_config_linux \n# cp -R /root/opt/SUNWut/lib/dhcp/isc /opt/SUNWut/lib/dhcp/\n# cp /opt/SUNWut/lib/dhcp/isc/dhcp_config_linux /opt/SUNWut/lib/dhcp/isc/dhcp_config_solaris\n# ln -s /opt/SUNWut/lib/dhcp/isc /etc/opt/SUNWut/dhcp\n</code></pre></div></div><p>then apply the needed patch to :</p><p>now, get the ancient JRE in place:</p><div><div><pre><code># cd /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Java_Runtime_Environment/Solaris\n# ./jre-6u41-solaris-i586.sh\n# mv ./jre1.6.0_41 /opt/\n# ln -s /opt/jre1.6.0_41 /etc/opt/SUNWut/jre\n</code></pre></div></div><p>and, since i wanted the web administration tools to work too:</p><div><div><pre><code># bsdtar -C /opt -xf /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Apache_Tomcat/apache-tomcat-5.5.36.tar.gz\n# ln -s /opt/apache-tomcat /opt/apache-tomcat-5.5.36\n</code></pre></div></div><p>i then configured the Sun Ray server:</p><div><div><pre><code># /opt/SUNWut/sbin/utconfig\n# /opt/SUNWut/sbin/utpolicy -a -z both -g -M\n# /opt/SUNWut/sbin/utadm -L on\n# /opt/SUNWut/sbin/utstart -c\n</code></pre></div></div><h3>getting the Sun Ray firmware in place</h3><p>since i was using version 5.4.x of the Sun Ray Server Software, the client firmware wasn’t part of the install - from version 5.3 onwards, you had to have an Oracle support contract to get firmware updates. sigh.</p><p>thankfully, getting a 5.2.x release (with the firmware included!) wasn’t hard. i grabbed a 5.2.x release for Linux, found the RPM with the firmware in it (), and extracted that with .</p><p>the Solaris version of SRSS wants to find the firmware in a different place than the Linux version it seems - the Linux versions put it in , but on Solaris/OpenIndiana, it needs to be in <code>/opt/SUNWutdfw/lib/firmware</code>. easy enough.</p><p>once in place, this was all it took to set up the TFTP server, and make SRSS populate the right places with the firmware:</p><div><div><pre><code># mkdir /tftpboot\n# cd /tftpboot\n# ln -f -s . tftpboot\n# /opt/SUNWut/sbin/utfwadm -AaV -G force\n</code></pre></div></div><p>i wanted to use some of the integrated-into-screens Sun Rays to replace some of the Raspberry Pis (and old iMacs) around the house showing Home Assistant dashboards. i also wanted to set up the Sun Ray server so that when i inserted a particular smart card into a client, it would bring up an RDP session to my existing “desktop” (a Fedora VM running Xrdp).</p><p>these both turned out to be… interesting to get working.</p><p>the Sun Ray Server Software has a built-in method for connecting to Microsoft RDP servers - the Sun Ray Windows Connector, also known as .\nas you might have guessed, it’s broken as fuck on OpenIndiana, even putting aside the fact that the newest RDP server it knows how to handle would be in the Windows Server 2003 era.</p><p>so, let’s hack something together with XFreeRDP!</p><p>i wanted to be able to specify what RDP server each token would connect to. this was a fairly common use case back in the day, and some people wrote helpers to allow things like that - one of which being <a href=\"https://web.archive.org/web/20131212042126/https://blogs.oracle.com/danielc/entry/meta_kiosk_how_to_run\">Daniel Cifuentes’ meta-kiosk</a>, which i borrowed some ideas from.</p><p>after much trial and error, i got something working!</p><div><figure><pre><code data-lang=\"shell\">/freerdp\n</code></pre></figure></div><div><figure><pre><code data-lang=\"shell\">\n\nopenbox  &amp;\n/opt/SUNWut/bin/utscreenresize  all  &amp;\n\n/opt/SUNWut/sbin/utuser  |  | zenity 1\nxterm  xfreerdp /cert:tofu /f  /dynamic-resolution /gfx +gfx-thin-client /smartcard /bpp:24 </code></pre></figure></div><p>after throwing those in place, install the dependencies and configure the session:</p><div><div><pre><code># pkg install openbox freerdp\n# printf \"KIOSK_SESSION=freerdp\\n\" | /opt/SUNWut/sbin/utkiosk -i FreeRDP\n</code></pre></div></div><p>then it’s just a matter of adding the needed data to each token, and assigning the tokens to the FreeRDP session:</p><div><div><pre><code># /opt/SUNWut/sbin/utkioskoverride -s kiosk -r OpenPlatform.47905167523905788499 -c FreeRDP\n</code></pre></div></div><p>upon inserting that token into a client…</p><p>with much the same setup as the RDP sessions, it’s pretty easy to start a kiosk-mode Firefox, pulling the URL to open from the token data:</p><div><figure><pre><code data-lang=\"shell\">/kiosk-browser\n</code></pre></figure></div><div><figure><pre><code data-lang=\"shell\">\n\nopenbox  &amp;\n/opt/SUNWut/bin/utscreenresize  all  &amp;\n\n\nxset s off\nxset s noblank\nxset /opt/SUNWut/sbin/utuser  |  | zenity 1\nfirefox </code></pre></figure></div><p>a problem, though. Firefox would show its first-run “Welcome to Firefox” popup… every time. Sun Ray kiosk sessions run as a random user named   (where  is a number), and after the kiosk session ends the home directory of the kiosk user gets fully deleted, so the user can be recycled for other sessions. given i wanted to use this with some always-on Sun Rays, with no input devices attached…</p><p>thankfully, Firefox policies allow turning that off! throwing this hunk of JSON into <code>/etc/firefox/policies/policies.json</code> fixed that:</p><div><figure><pre><code data-lang=\"json\"></code></pre></figure></div><p>and with that, i could create a token for an individual client (the tokens for this are , where the MAC is all lower-case), set that token’s “Other Info” field to the URL to show, and assign the kiosk session to that pseudo-token the same way as with smart card tokens.</p><p>this was a lot of fun to get working. i need to take a break from reading the Sun Ray Administration Guide though, so here’s my thinking for a potential part 2:</p><ul><li>i want to see how well the multi-head stuff works in SRSS - which joins multiple physical clients together into one desktop session, using the peripherals connected to the “primary” client. unfortunately the Xinerama support is weird (Xinerama and xrandr are mutually exclusive…), but if i can make it play ball it could be a neat thing to use.</li><li>i want to try and find a newer firmware package too, but that might be a little bit of a lost cause, given i refuse to give Oracle a bunch of money.</li><li>maybe i’ll set up another OpenIndiana VM and configure the HA failover in SRSS?</li></ul><p>for now, though… that’s all.</p>","contentLength":10036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44438900"},{"title":"Building a Personal AI Factory","url":"https://www.john-rush.com/posts/ai-20250701.html","date":1751404469,"author":"derek","guid":179509,"unread":true,"content":"<p>I keep several claude code windows open, each on its own git-worktree. o3 and sonnet 4 create plans, sonnet 3.7 or sonnet 4 execute the plan, and o3 checks the results against the original ask. Any issues found are fed back into the plan template and the code is regenerated. The factory improves itself.</p><p>Read on to see what might be useful for you.</p><h2>Guiding Principle – Fix Inputs, Not Outputs</h2><p>When something goes wrong, I don’t hand-patch the generated code. I don’t argue with claude. Instead, I adjust the plan, the prompts, or the agent mix so the next run is correct by construction.</p><p>If you know <a href=\"https://factorio.com/\">Factorio</a> you know it’s all about building a factory that can produce itself. If not, picture a top-down sandbox where conveyor belts and machines endlessly craft parts because the factory must grow. Do the same thing with AI agents: build a factory of agents that can produce code, verify it, and improve themselves over time.</p><h2>Basic day to day workflow - building the factory</h2><p>My main interface is <a href=\"https://claude.ai/code\">claude code</a>. It’s my computer now. I also have a local mcp which runs <a href=\"https://github.com/block/goose\">Goose</a> and o3. Goose only because I’ve already got it setup to use the models hosted in our Azure OpenAI subscription. Looking to improve this at some point, but it works for now.</p><p>I’ll give a high level task to claude code, which calls over to o3 to generate a plan. o3 is a good planner and can ask a bunch of good questions to clarify the job to be done. I then have it write out a  file with both my original ask and an implementation plan.</p><p>First, sonnet 4 reads the plan, verifies it, and turns it into a task list. Next claude code execute the plan, either with sonnet 3.7 or sonnet 4 depending on the complexity of the task. Because most of my day-to-day is in clojure I tend to use sonnet 4 to get the parens right.\nOne important instruction is to have claude write commits as it goes for each task step. This way either claude or I can revert to a previous state if something goes wrong.</p><h3>Step 3: Verification → Feedback into Inputs</h3><p>Once the code is generated, I have sonnet 4 verify the code against the original plan. Then I have o3 verify the code against the original plan and original ask. o3 is uncompromising. Claude wants to please, so will keep unnecessary backwards compatibility code in place. o3 will call that out and ask for it to be removed. Claude also tends to add “lint ignore flags” to the code which o3 will also call out. Having both models verify the code catches issues and saves me back and forth with claude.</p><p>Any issue sonnet 4 or o3 finds gets baked back into the plan template, not fixed inline.</p><p>Git worktrees let me open concurrent claude code instances and build multiple features at once. I still merge manually, but I’m no longer babysitting a single agent.</p><ul><li>Outputs are disposable; plans and prompts compound.</li><li>Debugging at the source scales across every future task.</li><li>It transforms agents from code printers into self-improving colleagues.</li></ul><p>Example: an agent once wrote code that would load an entire CSV into memory. I made it switch to streaming and had the agent write instructions to the plan to always use streaming for CSVs. Now, my plan checker flags any code that doesn’t use streaming for CSVs, and I don’t have to remember this in every PR review. The factory improves itself.</p><p>I’ve started to encode more complex workflows, where I have specific agents (behind mcps) for building specific tasks.</p><p>One MCP will sweep all the clojure code generated and then apply our local style rules. These rules are part of the instructions for the original plan and agent but often the generated code will have style issues. Especially once claude gets in the lint/test/debug cycle. This focused agent means we have tighter behavior and can apply our style rules consistently.</p><p>I’ve started doing this for internal libraries as well. It’s good at looking at generated code and replacing things like retries and  with our retry library.</p><p>I’m also building out a collection of these small agents. Each one can take a small specific task, and by composing them together I can build more complex workflows. For example, I can take an api doc, and a set of internally defined business cases and have a composition of agents build integrations, tests, and documentation for the api. This is a powerful way to build out features and integrations without having to do all the work by hand.</p><p>You don’t get there in one big step. Here’s the secret sauce: </p><p>It’s essentially free to fire off a dozen attempts at a task - so I do. All agents run in parallel. When one fails, stalls, or lacks context, I feed that lesson into the next iteration. I resist the urge to fix outputs, instead I fix the inputs.</p><p>That loop is the factory: the code itself is disposable; the instructions and agents are the real asset.</p><p>I’m working on a few things to improve the factory:</p><ul><li>Better overall coordination of the agents. I tend to kick things off manually, but I want to have a more automated way to manage the workflow and dependencies between agents.</li><li>Aligning our business docs with the agents. Changing the information we capture to be at a higher level of abstraction so that the agents can use it more effectively. This means moving away from low level implementation details and focusing on use cases.</li><li>More complex workflows. I’ve been able to build some pretty complex workflows with the current setup, but I want to push it further. This means more agents, more coordination, and more complex interactions between them.</li><li>Maximize token usage across providers. I’m pretty limited by bedrock’s token limits especially for sonnet 4. Going to need to be able to switch between the claude max plan and bedrock w/out interruption.</li></ul><p>That’s where my factory sits today: good enough to ship code while I refill my coffee, not yet good enough to bump me off the payroll. Constraints will shift, but the core principle remains: .</p>","contentLength":5911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44438065"},{"title":"Effectiveness of trees in reducing temperature, outdoor heat exposure in Vegas","url":"https://iopscience.iop.org/article/10.1088/2752-5295/ade17d","date":1751403545,"author":"PaulHoule","guid":179508,"unread":true,"content":"<h2>We apologize for the inconvenience...</h2><p>To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. </p><p>If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.</p>","contentLength":269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437948"},{"title":"The Roman Roads Research Association","url":"https://www.romanroads.org/","date":1751401972,"author":"bjourne","guid":180366,"unread":true,"content":"<p><a href=\"https://www.eventbrite.co.uk/e/road-construction-and-transport-infrastructure-in-the-roman-empire-registration-823562416357\" target=\"_blank\">Booking</a></p><p><a href=\"https://www.eventbrite.co.uk/e/788098332327?aff=oddtdtcreator\" target=\"_blank\">Booking</a></p><p><a href=\"https://youtu.be/DLc8lQvVcvM\" target=\"_blank\">https://youtu.be/DLc8lQvVcvM</a></p><p><a href=\"http://www.romanroads.org/gazetteer/cheshire/cheshire.html\" target=\"_blank\">Roman Roads in Cheshire</a></p>","contentLength":65,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437758"},{"title":"Fakespot shuts down today after 9 years of detecting fake product reviews","url":"https://blog.truestar.pro/fakespot-shuts-down/","date":1751401595,"author":"doppio19","guid":179507,"unread":true,"content":"<p>Today marks the end of an era. After nearly a decade of helping millions of shoppers navigate the murky waters of online reviews, <a href=\"https://blog.mozilla.org/en/mozilla/building-whats-next/?ref=blog.truestar.pro\">Fakespot has officially closed its doors</a>. If you tried to check a product listing this morning and found Fakespot not working, you're not alone. The service has permanently shut down.</p><p> Fakespot, the popular fake review detection tool acquired by Mozilla in 2023, shut down today, July 1, 2025. Founded by Saoud Khalifah in 2016, it helped millions identify unreliable Amazon reviews with 90% accuracy before Mozilla discontinued it due to sustainability challenges.</p><p>Back in 2016, Saoud Khalifah bought a product on Amazon, trusting the glowing reviews, only to discover he'd been duped by fake feedback. Instead of just leaving his own angry review, Khalifah took a more proactive approach: he built Fakespot.</p><p>What started as one person's frustration with deceptive sellers became a tool that analyzed millions of reviews across Amazon and other major retailers like eBay and Walmart. The premise was simple but powerful: use AI to spot patterns that human shoppers might miss, like suspiciously similar language or reviewer profiles that didn't quite add up.</p><h2>The magnitude of the deception</h2><p>Fakespot's technology revealed some eye-opening statistics. About 43% of the best-selling Amazon products had reviews that were unreliable or fabricated, according to a <a href=\"https://getcircuit.com/route-planner/blog/amazon-fake-review-analysis?ref=blog.truestar.pro\">study by app company Circuit</a>. The problem was even worse in certain categories. Clothing and jewelry led the pack with a staggering 88% of reviews deemed unreliable.</p><p>These numbers painted a sobering picture of the online shopping landscape. Most of us rely on product reviews as a major factor when deciding what to buy, but nearly half of the feedback you read might not be genuine.</p><p>Three years later, <a href=\"https://blog.mozilla.org/en/mozilla/fakespot-joins-mozilla-firefox-shopping-announcement/?ref=blog.truestar.pro\">Mozilla acquired Fakespot</a>, bringing the startup's 13-person team into the Firefox family. Mozilla integrated Fakespot's technology directly into Firefox as the \"Mozilla Review Checker\" feature, making it easier than ever for users to verify product reviews without installing separate extensions.</p><p>For many users, this felt like a perfect match. Mozilla's reputation for privacy and transparency aligned beautifully with Fakespot's mission to bring honesty to online shopping.</p><p>But as Mozilla announced in May, not all acquisitions fit into a sustainable long-term model. The company made the difficult decision to discontinue both Pocket and Fakespot as part of a strategic refocus on Firefox's core features and AI-powered innovations.</p><p>The reasons were practical, if devastating for users. A flood of reviews lamenting the closure have appeared on Fakespot's extension page on the Chrome Web Store:</p><p>Fakespot's mission resonated strongly with consumers, but <a href=\"https://www.distractify.com/p/why-is-fakespot-shutting-down?ref=blog.truestar.pro\">Mozilla couldn't find a sustainable model</a> to keep it running. Resources that once supported the service would now flow toward Firefox features like vertical tabs, smart search, and additional AI-powered features.</p><p>As we say goodbye to Fakespot, it's worth reflecting on what it accomplished. For nine years, it served as a defender against fraud in an increasingly deceptive marketplace. It gave shoppers a fighting chance against promotional reviewers and bot farms that undermine trust in online shopping.</p><p>For those of us who came to rely on Fakespot's review analysis before making purchases, its absence leaves us less confident in our buying decisions. The need for trustworthy review analysis hasn't gone away. If anything, it's more critical than ever.</p><p>I know I'm not alone in feeling this gap, which is why I've begun building a tool that aims to be the spiritual successor to Fakespot. <a href=\"https://truestar.pro/?ref=blog.truestar.pro\">TrueStar</a> will use modern AI, streamlined analysis techniques, and sustainable economics to keep costs manageable while maintaining the accuracy shoppers need.</p><div data-layout=\"minimal\"><div><div><a href=\"https://truestar.pro/?ref=blog.truestar.pro\">\n                            Get notified\n                        </a></div></div></div><h2>Quick answers about Fakespot's closure</h2><p><strong>When did Fakespot shut down?</strong>Fakespot officially closed on July 1, 2025, with the Mozilla Review Checker feature in Firefox having ended on June 10, 2025.</p><p><strong>Why did Fakespot shut down?</strong>Mozilla couldn't find a sustainable business model for Fakespot despite its popularity, choosing to redirect resources to core Firefox features and AI-powered browser tools.</p><p><strong>What happened to Fakespot?</strong>Mozilla acquired Fakespot in 2023 but announced in May 2025 that both Fakespot and Pocket would be discontinued as part of a strategic refocus on Firefox development.</p><p><strong>What are the best Fakespot alternatives?</strong>While several options exist including ReviewMeta, The Review Index, and emerging tools like <a href=\"https://truestar.pro/?ref=blog.truestar.pro\">TrueStar</a>, the market is still developing sustainable solutions that balance accuracy with affordability.</p><p>As Fakespot's servers go dark, let's raise a glass to the tool that made online shopping so much more trustworthy for nearly a decade. Thanks to Saoud Khalifah and his team for showing us what's possible when technology serves truth over profit.</p><p>Rest in peace, Fakespot. You fought the good fight. 🥂</p><p><em>If you found this article helpful, consider sharing it with others who might be wondering why their favorite review checker stopped working today. Let's keep the conversation about online authenticity going.</em></p>","contentLength":5181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437712"},{"title":"Figma files for proposed IPO","url":"https://www.figma.com/blog/s1-public/","date":1751398754,"author":"kualto","guid":179307,"unread":true,"content":"<p>Figma, Inc. (“Figma”) today announced that it has filed a registration statement on Form S-1 with the U.S. Securities and Exchange Commission (“SEC”) relating to a proposed initial public offering of its Class A common stock. Figma has applied to list its Class A common stock on the New York Stock Exchange under the symbol “.”</p><p>The number of shares to be offered and the price range for the proposed offering have not yet been determined. The offering is subject to market conditions, and there can be no assurance as to whether or when the offering may be completed, or as to the actual size or terms of the offering.</p><p><em>Morgan Stanley, Goldman Sachs &amp; Co. LLC, Allen &amp; Company LLC, and J.P. Morgan will act as joint lead book-running managers for the proposed offering. BofA Securities, Wells Fargo Securities, and RBC Capital Markets will act as book-running managers for the proposed offering. William Blair and Wolfe | Nomura Alliance will act as co-managers for the proposed offering.</em></p><p><em>The proposed offering will be made available only by means of a prospectus. Copies of the preliminary prospectus, when available, may be obtained from Morgan Stanley &amp; Co. LLC, Attention: Prospectus Department, 180 Varick Street, 2nd Floor, New York, New York 10014, or by email at prospectus@morganstanley.com; Goldman Sachs &amp; Co. LLC, Attention: Prospectus Department, 200 West Street, New York, New York 10282, by telephone at (866) 471-2526, or by email at prospectus-ny@ny.email.gs.com; Allen &amp; Company LLC, Attention: Prospectus Department, 711 Fifth Avenue, New York, New York 10022, by telephone at (212) 339-2220, or by email at allenprospectus@allenco.com; or J.P. Morgan Securities LLC, c/o Broadridge Financial Solutions, 1155 Long Island Avenue, Edgewood, New York 11717 or by email at prospectus-eq_fi@jpmchase.com and postsalemanualrequests@broadridge.com.</em></p><p><em>A registration statement on Form S-1 relating to these securities has been filed with the SEC but has not yet become effective. These securities may not be sold, nor may offers to buy be accepted, prior to the time the registration statement becomes effective. This press release shall not constitute an offer to sell or the solicitation of an offer to buy these securities, nor shall there be any sale of these securities in any state or jurisdiction in which such offer, solicitation, or sale would be unlawful prior to registration or qualification under the securities laws of any such state or jurisdiction.</em></p><p>Figma is where teams come together to turn ideas into the world’s best digital products and experiences. Founded in 2012, Figma has evolved from a design tool to a connected, AI-powered platform that helps teams go from idea to shipped product. Whether you’re ideating, designing, building, or shipping, Figma makes the entire design and product development process more collaborative, efficient, and fun––while keeping everyone on the same page.</p>","contentLength":2935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437316"},{"title":"Sam Altman Slams Meta's AI Talent Poaching: 'Missionaries Will Beat Mercenaries'","url":"https://www.wired.com/story/sam-altman-meta-ai-talent-poaching-spree-leaked-messages/","date":1751393318,"author":"spenvo","guid":180365,"unread":true,"content":"<p> Altman is hitting back at Meta CEO Mark Zuckerberg’s recent AI talent-poaching spree. In a full-throated response sent to OpenAI researchers Monday evening and obtained by WIRED, Altman made his pitch for why staying at OpenAI is the only answer for those looking to build artificial general intelligence, hinting that the company is evaluating compensation for the entire research organization.</p><p>He also dismissed Meta’s recruiting efforts, saying what the company is doing could lead to deep cultural problems down the road.</p><p>“We have gone from some nerds in the corner to the most interesting people in the tech industry (at least),” he wrote on Slack. “AI Twitter is toxic; Meta is acting in a way that feels somewhat distasteful; I assume things will get even crazier in the future. After I got fired and came back I said that was not the craziest thing that would happen in OpenAl history; certainly neither is this.”</p><p>The news comes on the heels of a major announcement from Zuckerberg. On Monday, the Meta CEO <a href=\"https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/\">sent a memo</a> to staff introducing the company’s new superintelligence team, which will be helmed by Alexandr Wang, formerly of Scale AI, and Nat Friedman, who previously led GitHub. The list of new hires also included a number of <a href=\"https://www.wired.com/story/four-openai-researchers-leave-meta/\">people from OpenAI</a>, including Shengjia Zhao, Shuchao Bi, Jiahui Yu, and Hongyu Ren. OpenAI’s chief research officer, Mark Chen, <a href=\"https://www.wired.com/story/openai-meta-leadership-talent-rivalry/\">told staff</a> that it felt like “someone has broken into our home and stolen something.”</p><p>Altman struck a different tone about the departures in his note on Monday.</p><p>“Meta has gotten a few great people for sure, but on the whole, it is hard to overstate how much they didn't get their top people and had to go quite far down their list; they have been trying to recruit people for a super long time, and I've lost track of how many people from here they've tried to get to be their Chief Scientist,” he wrote. “I am proud of how mission-oriented our industry is as a whole; of course there will always be some mercenaries.”</p><p>He added that “Missionaries will beat mercenaries” and noted that OpenAI is assessing compensation for the entire research organization. “I believe there is much, much more upside to OpenAl stock than Meta stock,” he wrote. “But I think it's important that huge upside comes after huge success; what Meta is doing will, in my opinion, lead to very deep cultural problems. We will have more to share about this soon but it's very important to me we do it fairly and not just for people who Meta happened to target.”</p><p>Altman then made his pitch for people to remain at OpenAI. “I have never been more confident in our research roadmap,” he wrote. “We are making an unprecedented bet on compute, but I love that we are doing it and I'm confident we will make good use of it. Most importantly of all, I think we have the most special team and culture in the world. We have work to do to improve our culture for sure; we have been through insane hypergrowth. But we have the core right in a way that I don't think anyone else quite does, and I'm confident we can fix the problems.”</p><p>“And maybe more importantly than that, we actually care about building AGI in a good way,” he added. “Other companies care more about this as an instrumental goal to some other mission. But this is our top thing, and always will be. Long after Meta has moved on to their next flavor of the week, or defending their social moat, we will be here, day after day, year after year, figuring out how to do what we do better than anyone else. A lot of other efforts will rise and fall too.”</p><p>A number of high-ranking employees who’ve worked at Meta followed up in Slack with their own stories about why OpenAI’s culture is superior. “[T]hey constantly rotate their top focus,” wrote one. Another said: “Yes we’re quirky and weird, but that’s what makes this place a magical cradle of innovation,” wrote one. “OpenAI is weird in the most magical way. We contain multitudes.”</p>","contentLength":4000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44436579"},{"title":"Code-GUI bidirectional editing via LSP","url":"https://jamesbvaughan.com/bidirectional-editing/","date":1751388199,"author":"jamesbvaughan","guid":179506,"unread":true,"content":"<p>I built a small proof-of-concept for a system that enables real-time\nbidirectional editing between any modern code editor and a GUI, enabled by an\nLSP server.</p><p>I like working on small projects at home that benefit from CAD. I’m also a\nprogrammer with a personal development environment that I’ve spent years making\nas cozy as possible. Naturally I’ve been interested in finding code-based CAD\nsystem to use for my projects that allows me to use that cozy development\nenvironment.</p><blockquote>For example: One idea I’m exploring is “bidirectional editing”, so geometry can\nbe manipulated using either:<ul><li>a purpose-built graphical UI, or</li><li>the textual codeCAD language</li></ul><p>If you graphically drag a point around, the coordinates in the source code\nshould automatically update.\nIf you edit the source code, the graphical UI should automatically update.</p><p>A simple way to test this idea is to throw a  in the UI that\ndisplays the corresponding source code.\nBut to me, that feels terrible because I never want to be coding in some janky,\nin-browser  — I want to be working with source code in Emacs, with\nall of my familiar key bindings, color schemes, autocomplete, and decades of\ncozy practice.</p><p><strong>That’s the core appeal of a textual programming language.</strong></p><p>But doing this properly is an absolute boatload of work:</p><ul><li>How does the system rewrite source code? Is it mediated by files on disk with\nreload on save? How do the editor and UI stay in sync and avoid clobbering\neach other’s unsaved changes? <strong>Maybe we need an LSP server?</strong></li><li>The language interpreter needs to preserve comments and flow them through,\neven when the UI makes edits to the code.</li><li>What about whitespace / pretty-printing?</li></ul><p>How much of this needs to be built to evaluate whether bidirectional editing\n“fits nicely in the hand”?</p></blockquote><blockquote><p>Maybe we need an LSP server?</p></blockquote><p>I’ve been a happy user of LSP servers since they became commonplace in Neovim\nsetups, but I have almost no experience with language server internals.\nI had certainly never considered that they could facilitate bidirectional\nediting with a GUI.</p><p>That line from Kevin’s post was a proper nerd-snipe because a few hours later I\nhad built this proof-of-concept:</p><p>What you’re seeing here is a text editor next to a GUI, and data live-updating\nboth ways between them, made possible by a small server that uses LSP to\ncommunicate with the text editor and WebSockets to communicate with a web app.</p><p>I’ve shared more technical details and the code for this demo <a href=\"https://github.com/jamesbvaughan/bidirectional-number-editor\">here on\nGitHub</a>.</p><p>Bidirectional editing isn’t new.\nWhat’s new, as far as I’m aware, is real-time bidirectional editing <em>that works\nwith your favorite text editor.</em></p><p>I’ve tried out a handful of code-based CAD systems, but so far I haven’t found\nany that achieve more than two out of these three features:</p><ul><li>Real-time-ish updates in the GUI from changes made in the code</li><li>Real-time-ish updates in the code from changes made in the GUI</li><li>Works well with my preferred code editor</li></ul><p><a href=\"https://www.autodesk.com/products/fusion-360/overview#top\">Fusion 360</a> has\ndecent bidirectional editing for parameters, but it’s not fully code-based and\nit certainly doesn’t let me use my own editor.</p><p><a href=\"https://openscad.org/\">OpenSCAD</a> doesn’t require the use of its own text\neditor, and it’s possible to trigger reloads in the GUI via file watching\nwhen you save source files in external editors, but it only goes one way.</p><p><a href=\"https://zoo.dev/design-studio\">Zoo</a> has some bidirectional editing, but only\nwith its built-in editor.</p><p><a href=\"https://www.arcol.io/\">Arcol</a>, the tool that I help build at my day job, is\ninnovating in CAD interface design in some exciting ways, but we’re building for\narchitects, not programmers.</p><p>This is just a toy demo, but it’s enough to excite me about the possibility of a\nsystem that achieves  of those points!</p><p>I don’t plan to develop this demo further, at least not anytime soon, but I hope\nit inspires people to find more creative uses (abuses?) of LSP servers.</p><p>One of the best code-CAD environments I’ve worked in is OpenSCAD + Neovim with\nthe <a href=\"https://github.com/Leathong/openscad-LSP\">OpenSCAD LSP server</a>, only using\nthe OpenSCAD GUI for the viewer, not the built-in text editor.\nOpenSCAD is fundamentally not built for GUI editing, but since it’s open source\nand has a nice language server already, it could be a good place to develop a\nmore interesting demo of this concept.</p><p>Like Kevin’s post said, doing this properly will be a boatload of work.\nHandling conflict resolution, incremental edits, and the more complex general\nLSP server internals are all serious tasks, let alone creating a whole new\nlanguage for CAD.</p><p>I’m looking forward to seeing what Kevin comes up with for codeCAD!</p>","contentLength":4463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435716"},{"title":"Show HN: Core – open source memory graph for LLMs – shareable, user owned","url":"https://github.com/RedPlanetHQ/core","date":1751387064,"author":"Manik_agg","guid":179512,"unread":true,"content":"<p>I keep running in the same problem of each AI app “remembers” me in its own silo. ChatGPT knows my project details, Cursor forgets them, Claude starts from zero… so I end up re-explaining myself dozens of times a day across these apps.</p><p>1. Not portable – context is vendor-locked; nothing travels across tools.</p><p>2. Not relational – most memory systems store only the latest fact (“sticky notes”) with no history or provenance.</p><p>3. Not yours – your AI memory is sensitive first-party data, yet you have no control over where it lives or how it’s queried.</p><p>- CORE (Context Oriented Relational Engine): An open source, shareable knowledge graph (your memory vault) that lets any LLM (ChatGPT, Cursor, Claude, SOL, etc.) share and query the same persistent context.</p><p>- Temporal + relational: Every fact gets a full version history (who, when, why), and nothing is wiped out when you change it—just timestamped and retired.</p><p>- Local-first or hosted: Run it offline in Docker, or use our hosted instance. You choose which memories sync and which stay private.</p>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435500"},{"title":"The Fed says this is a cube of $1M. They're off by half a million","url":"https://calvin.sh/blog/fed-lie/","date":1751386975,"author":"c249709","guid":179072,"unread":true,"content":"<p>At the Federal Reserve Bank of Chicago’s Money Museum, there’s a big transparent cube on display. It’s filled with tightly packed stacks of  bills, claiming to contain .</p><p>The plaque proudly declares:</p><blockquote><p>Have you ever wondered what one million dollars looks like?\nYou don’t have to wonder anymore because you can see it right in front of you!</p></blockquote><p>But I don’t trust signs. I trust counting.</p><p>I first tried counting the stacks right there in the room. The cube was tall, so I had to step back to see the whole thing, squinting at the stacks, trying to follow each row. I lost track almost immediately.</p><p>Also, people were starting to look at me funny. Apparently, staring intensely at a pile of cash while muttering numbers isn’t normal museum behavior.</p><p>Then, I tried with a photo. I zoomed all the way in on my phone, dragging my finger across the screen, mentally tallying as I went.</p><p>Still couldn’t keep count.</p><p>All I wanted was a way to click on things in a photo and have the number go up.</p><p>You’d think this would already exist, a browser based tool for counting things.</p><p>Turns out it… doesn’t. At least, not as a web app I can find on Google.</p><p>There are some clunky old Windows programs, niche scientific tools, and image analysis software that assumes you’re trying to count cells under a microscope, not people, penguins, or stacks of $1 bills in a Federal Reserve cube.</p><p>It’s stupidly simple: upload an image, click to drop a dot, and it tells you how many you’ve placed. That’s it. But somehow, nothing like it existed.</p><p>I originally made it to investigate this very cube, but I figured other people might need to count stuff in pictures.</p><p>Count your enemies. Count your blessings. Count your stacks of cash.</p><p>Because when someone tells you it’s a million dollars, you might want to double check.</p><a href=\"https://calvin.sh/blog/fed-lie/cube-labeled.png\" data-v-3b6c5e00=\"\"></a><p>Assuming each bundle contains  bills*, that’s</p><p>So yeah. They’re off by .</p><p>That’s  in extra cash.</p><blockquote><p>“Hey so… we’re $550,400 over budget on the million-dollar cube project.”</p></blockquote><p>If you knock  from each dimension (basically pealing away the outermost layer of money bundles), the math actually gets kinda close</p><p>but since dollar bills are much wider than they’re tall, it wouldn’t look like a cube anymore.</p><p>Maybe the Fed is playing the long game.</p><p>At the Fed’s  inflation target, this cube will be worth  million in today’s dollars in:</p><p>Can’t wait to come back in 2047 and say: “Nice. Nailed it.”</p><p>Sure, it does technically contain .</p><p>And also  of bonus money.</p><p>Which is kind of like ordering a burger and getting three.</p><p>I mean, sure, free stuff. But it’s not what you asked for.</p><p>You can only see the outer stacks. For all we know, the middle is just air and crumpled-up old newspaper.</p><p>A money shell. A decorative cube. A fiscal illusion. The world’s most expensive piñata (but don’t hit it, security is watching).</p><p>And get this: just the outermost layer is already worth:</p><p>You’d only need a 3-layer-thick shell to blow past a million:</p><h2>How  you make a million dollar cube?</h2><p>Turns out U.S. dollars are extremely non-cube-friendly. Each bill is  wide by  tall, a nice and even aspect ratio of:</p><p>Each 100-bill bundle is  inches thick.</p><ul><li> stacks</li></ul><p>Which gives you a lovely almost-cube:</p><ul><li> wide</li><li> deep</li><li> tall</li></ul><p>Not perfect. Not terrible. At least it’s honest, unlike that other cube.</p><p>Maybe it’s  million.</p><p>Maybe it’s an empty box with a money shell.</p><p>Most likely it’s  million.</p><p>All I know is I built a tool, did the math, and triple-checked the stacks.</p><p>The sign says you don’t have to wonder.\nBut I did anyway.</p><p>And now… you don’t have to either.</p>","contentLength":3525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435484"},{"title":"HN Slop: AI startup ideas generated from Hacker News","url":"https://www.josh.ing/hn-slop","date":1751383905,"author":"coloneltcb","guid":180364,"unread":true,"content":"<div>© 2025 Just Joshing, LLC. All rights reserved.</div>","contentLength":47,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434938"},{"title":"Ask HN: Who is hiring? (July 2025)","url":"https://news.ycombinator.com/item?id=44434576","date":1751382096,"author":"whoishiring","guid":179306,"unread":true,"content":"Please state the location and include REMOTE for remote work, REMOTE (US)\nor similar if the country is restricted, and ONSITE when remote work is  an option.<p>Please only post if you personally are part of the hiring company—no\nrecruiting firms or job boards. One post per company. If it isn't a household name,\nexplain what your company does.</p><p>Please only post if you are actively filling a position and are committed\nto responding to applicants.</p><p>Commenters: please don't reply to job posts to complain about\nsomething. It's off topic here.</p><p>Readers: please only email if you are personally interested in the job.</p><p>Don't miss these other fine threads:</p>","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434576"},{"title":"Ask HN: Who wants to be hired? (July 2025)","url":"https://news.ycombinator.com/item?id=44434574","date":1751382096,"author":"whoishiring","guid":180363,"unread":true,"content":"Share your information if you are looking for work. Please use this format:<pre><code>  Location:\n  Remote:\n  Willing to relocate:\n  Technologies:\n  Résumé/CV:\n  Email:\n</code></pre>\nPlease only post if you are personally looking for work. Agencies, recruiters, job boards,\nand so on, are off topic here.<p>Readers: please only email these addresses to discuss work opportunities.</p>","contentLength":355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434574"},{"title":"Feasibility study of a mission to Sedna - Nuclear propulsion and solar sailing","url":"https://arxiv.org/abs/2506.17732","date":1751378891,"author":"speckx","guid":179071,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434062"},{"title":"Grammarly acquires Superhuman","url":"https://www.reuters.com/business/grammarly-acquires-email-startup-superhuman-ai-platform-push-2025-07-01/","date":1751378430,"author":"thm","guid":179011,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433994"},{"title":"Converting a large mathematical software package written in C++ to C++20 modules","url":"https://arxiv.org/abs/2506.21654","date":1751377616,"author":"vblanco","guid":179305,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433899"},{"title":"Show HN: I built the tool I wished existed for moving Stripe between countries","url":"https://www.stripemove.com/","date":1751374370,"author":"felphos","guid":179224,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433429"},{"title":"Show HN: Spegel, a Terminal Browser That Uses LLMs to Rewrite Webpages","url":"https://simedw.com/2025/06/23/introducing-spegel/","date":1751374182,"author":"simedw","guid":178963,"unread":true,"content":"<p>TL;DR Spegel is a proof-of-concept terminal web browser that feeds HTML through an LLM and renders the result as markdown directly in your terminal.</p><p>Two weekends ago, after my family had gone to sleep, I found myself unsupervised with a laptop and an itch to build something interesting. A couple of hours later, I had a minimal web browser running in my terminal (no JavaScript, GET requests only) that transformed web content based on my custom prompts.</p><p>Then, a few days later, Google released Gemini 2.5 Pro Lite, significantly faster inference speed, suddenly my little weekend hack became a tad more practical.</p><p>Adapting content to suit individual needs isn’t a new idea, think about translating books or summarising lengthy articles. However, this used to be slow and expensive. LLMs have changed this dramatically, making these transformations quick and easy.</p><p>Spegel (\"mirror\" in Swedish) lets you explore web content through personalized views using your own prompts. A single page can have multiple views, maybe one simplifying everything down to ELI5 or another highlighting key actions. It's entirely up to you and your prompting skills. </p><p>Sometimes you don't want to read through someone's life story just to get to a recipe.\n<img alt=\"Recipe Example\" src=\"https://simedw.com/2025/06/23/introducing-spegel/images/recipe_example.png\"><small>A previous version of this screenshot showed an incorrect recipe on the right. That was due to a bug where large websites got truncated. Thanks to everyone who pointed it out!</small></p><div><pre><code></code></pre></div><p>The pipeline is straightforward.</p><p>Spegel fetches HTML content, processes it through an LLM using prompts stored in a config file (~/.spegel.toml), and outputs markdown rendered via Textual. Prompts and views can be adjusted live during a browsing session.</p><p>This was my first experience using Textual for a TUI, and it's been delightful, possibly too delightful, as I found myself adding a few unnecessary interface elements just because it was easy.</p><p>One gotcha was ensuring only completed lines (ending in newline characters) were streamed; otherwise, the markdown renderer would parse incomplete markdown and fail to recover formatting</p><div><pre><code></code></pre></div><p>There are a lot of great terminal browsers out there, Lynx and Links2 are close to my heart. There are also modern attempts like Browsh that can even render graphs using half-block Unicode characters (▄█). </p><p>Spegel isn’t meant to replace these, it’s more of an exploration or proof-of-concept. It currently doesn't support POST requests (though I have some ideas on handling  elements by creating on-the-fly UIs).</p><p>But most modern websites aren’t designed with terminal browsing in mind. They rely on CSS and JS, making them cumbersome in small terminal windows, full of clutter and noise. Spegel tries to clear away distractions, providing content tailored more closely to your needs.</p><p>Spegel is still in the early stages, so expect some rough edges, but it’s usable and kind of fun to play with.</p><p>Then just run it with a URL:</p><div><pre><code>spegelsimedw.com</code></pre></div><p>Don't forget to configure your own , (<a href=\"https://github.com/simedw/spegel/blob/main/example_config.toml\">example</a>)</p><p>Want to check out the source or contribute? It’s all on GitHub:</p>","contentLength":2997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433409"},{"title":"Show HN: Jobs by Referral: Find jobs in your LinkedIn network","url":"https://jobsbyreferral.com/","date":1751374026,"author":"nicksergeant","guid":179223,"unread":true,"content":"<div><p data-slot=\"text\">JobsByReferral analyzes your professional network to find job openings at companies where you have connections. Get referred by people you already know and dramatically increase your chances of landing interviews.</p></div>","contentLength":213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433386"},{"title":"Scientists identify culprit behind biggest-ever U.S. honey bee die-off","url":"https://www.science.org/content/article/scientists-identify-culprit-behind-biggest-ever-u-s-honeybee-die","date":1751366123,"author":"pseudolus","guid":179010,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432467"},{"title":"Cloudflare to introduce pay-per-crawl for AI bots","url":"https://blog.cloudflare.com/introducing-pay-per-crawl/","date":1751365227,"author":"scotchmi_st","guid":178870,"unread":true,"content":"<div><h2>A changing landscape of consumption&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#a-changing-landscape-of-consumption\" aria-hidden=\"true\"></a></div><p>Many publishers, content creators and website owners currently feel like they have a binary choice — either leave the front door wide open for AI to consume everything they create, or create their own walled garden. But what if there was another way?</p><p>At Cloudflare, we started from a simple principle: we wanted content creators to have control over who accesses their work. If a creator wants to block all AI crawlers from their content, they should be able to do so. If a creator wants to allow some or all AI crawlers full access to their content for free, they should be able to do that, too. Creators should be in the driver’s seat.</p><p>After hundreds of conversations with news organizations, publishers, and large-scale social media platforms, we heard a consistent desire for a third path: They’d like to allow AI crawlers to access their content, but they’d like to get compensated. Currently, that requires knowing the right individual and striking a one-off deal, which is an insurmountable challenge if you don’t have scale and leverage.&nbsp;</p><div><h2>What if I could charge a crawler?&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#what-if-i-could-charge-a-crawler\" aria-hidden=\"true\"></a></div><p>We believe your choice need not be binary — there should be a third, more nuanced option: <b>You can charge for access.</b> Instead of a blanket block or uncompensated open access, we want to empower content owners to monetize their content at Internet scale.</p><div><h2>Introducing pay per crawl</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#introducing-pay-per-crawl\" aria-hidden=\"true\"></a></div><p><a href=\"https://www.cloudflare.com/paypercrawl-signup/\">Pay per crawl</a>, in private beta, is our first experiment in this area.&nbsp;</p><p>Pay per crawl integrates with existing web infrastructure, leveraging HTTP status codes and established authentication mechanisms to create a framework for paid content access.&nbsp;</p><p>Each time an AI crawler requests content, they either present payment intent via request headers for successful access (<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/200\"></a>), or receive a  response with pricing. Cloudflare acts as the Merchant of Record for pay per crawl and also provides the underlying technical infrastructure.</p><div><h3>Publisher controls and pricing</h3><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#publisher-controls-and-pricing\" aria-hidden=\"true\"></a></div><p>Pay per crawl grants domain owners full control over their monetization strategy. They can define a flat, per-request price across their entire site. Publishers will then have three distinct options for a crawler:</p><ul><li><p> Grant the crawler free access to content.</p></li><li><p> Require payment at the configured, domain-wide price.</p></li><li><p> Deny access entirely, with no option to pay.</p></li></ul><p>An important mechanism here is that even if a crawler doesn’t have a billing relationship with Cloudflare, and thus couldn’t be charged for access, a publisher can still choose to ‘charge’ them. This is the functional equivalent of a network level block (an HTTP  response where no content is returned) — but with the added benefit of telling the crawler there could be a relationship in the future.&nbsp;</p><p>While publishers currently can define a flat price across their entire site, they retain the flexibility to bypass charges for specific crawlers as needed. This is particularly helpful if you want to allow a certain crawler through for free, or if you want to negotiate and execute a content partnership outside the pay per crawl feature.&nbsp;</p><p>To ensure integration with each publisher’s existing security posture, Cloudflare enforces Allow or Charge decisions via a rules engine that operates only after existing WAF policies and bot management or bot blocking features have been applied.</p><div><h3>Payment headers and access</h3><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#payment-headers-and-access\" aria-hidden=\"true\"></a></div><p>As we were building the system, we knew we had to solve an incredibly important technical challenge: ensuring we could charge a specific crawler, but prevent anyone from spoofing that crawler. Thankfully, there’s a way to do this using <a href=\"https://developers.cloudflare.com/bots/concepts/bot/verified-bots/web-bot-auth/\"></a> proposals.</p><ul><li><p>Generating an Ed25519 key pair, and making the <a href=\"https://datatracker.ietf.org/doc/html/rfc7517\"></a>-formatted public key available in a hosted directory</p></li><li><p>Registering with Cloudflare to provide the URL of your key directory and user agent information.</p></li></ul><p>Once registration is accepted, crawler requests should always include , , and  headers to identify your crawler and discover paid resources.</p><pre><code>GET /example.html\nSignature-Agent: \"https://signature-agent.example.com\"\nSignature-Input: sig2=(\"@authority\" \"signature-agent\")\n ;created=1735689600\n ;keyid=\"poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U\"\n ;alg=\"ed25519\"\n ;expires=1735693200\n;nonce=\"e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg==\"\n ;tag=\"web-bot-auth\"\nSignature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:</code></pre><p>Once a crawler is set up, determination of whether content requires payment can happen via two flows:</p><h4>Reactive (discovery-first)</h4><p>Should a crawler request a paid URL, Cloudflare returns an <code>HTTP 402 Payment Required</code> response, accompanied by a  header. This signals that payment is required for the requested resource.</p><pre><code>HTTP 402 Payment Required\ncrawler-price: USD XX.XX</code></pre><p>&nbsp;The crawler can then decide to retry the request, this time including a  header to indicate agreement to pay the configured price.</p><pre><code>GET /example.html\ncrawler-exact-price: USD XX.XX </code></pre><p>Alternatively, a crawler can preemptively include a  header in its initial request.</p><pre><code>GET /example.html\ncrawler-max-price: USD XX.XX</code></pre><p>If the price configured for a resource is equal to or below this specified limit, the request proceeds, and the content is served with a successful  response, confirming the charge:</p><pre><code>HTTP 200 OK\ncrawler-charged: USD XX.XX \nserver: cloudflare</code></pre><p>If the amount in a  request is greater than the content owner’s configured price, only the configured price is charged. However, if the resource’s configured price exceeds the maximum price offered by the crawler, an  response is returned, indicating the specified cost. &nbsp;Only a single price declaration header,  or , may be used per request.</p><p>The  or  headers explicitly declare the crawler's willingness to pay. If all checks pass, the content is served, and the crawl event is logged. If any aspect of the request is invalid, the edge returns an <code>HTTP 402 Payment Required</code> response.</p><p>Crawler operators and content owners must configure pay per crawl payment details in their Cloudflare account. Billing events are recorded each time a crawler makes an authenticated request with payment intent and receives an HTTP 200-level response with a  header. Cloudflare then aggregates all the events, charges the crawler, and distributes the earnings to the publisher.</p><div><h2>Content for crawlers today, agents tomorrow&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#content-for-crawlers-today-agents-tomorrow\" aria-hidden=\"true\"></a></div><p>At its core, pay per crawl begins a technical shift in how content is controlled online. By providing creators with a robust, programmatic mechanism for valuing and controlling their digital assets, we empower them to continue creating the rich, diverse content that makes the Internet invaluable.&nbsp;</p><p>We expect pay per crawl to evolve significantly. It’s very early: we believe many different types of interactions and marketplaces can and should develop simultaneously. We are excited to support these various efforts and open standards.</p><p>For example, a publisher or new organization might want to charge different rates for different paths or content types. How do you introduce dynamic pricing based not only upon demand, but also how many users your AI application has? How do you introduce granular licenses at internet scale, whether for training, inference, search, or something entirely new?</p><p>The true potential of pay per crawl may emerge in an agentic world. What if an agentic paywall could operate entirely programmatically? Imagine asking your favorite deep research program to help you synthesize the latest cancer research or a legal brief, or just help you find the best restaurant in Soho — and then giving that agent a budget to spend to acquire the best and most relevant content. By anchoring our first solution on , we enable a future where intelligent agents can programmatically negotiate access to digital resources.&nbsp;</p><p>Pay per crawl is currently in private beta. We’d love to hear from you if you’re either a crawler interested in paying to access content or a content creator interested in charging for access. You can reach out to us at <a href=\"https://www.cloudflare.com/paypercrawl-signup/\"><u>http://www.cloudflare.com/paypercrawl-signup/</u></a> or contact your Account Executive if you’re an existing Enterprise customer.</p>","contentLength":8099,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432385"},{"title":"Show HN: ToplingDB - A Persistent Key-Value Store for External Storage","url":"https://github.com/topling/toplingdb","date":1751364479,"author":"rockeetterark","guid":179511,"unread":true,"content":"<p>As the creator of TerarkDB (acquired by ByteDance in 2019), I have developed ToplingDB in recent years.</p><p>ToplingDB is forked from RocksDB,   where   we have replaced almost all components with more efficient alternatives(db_bench shows ToplingDB is about ~8x faster than RocksDB):</p><p>* MemTable: SkipList is replaced by CSPP(Crash Safe Parallel Patricia trie), which is 8x faster.</p><p>* SST: BlockBasedTable is replaced by ToplingZipTable, implemented by searchable compression algo, it is very small and fast, typically less than 1μs per lookup:</p><pre><code>  * Keys/Indexes are compressed   using NestLoudsTrie(a multi-layer nesting LOUDS succinct trie).\n\n  * Values in a SST are compressed   together with better zip ratio than zstd, and can unzip by a single value at 1GB/sec.\n\n  * BlockCache is no longer needed, double caching(BlockCache &amp; PageCache) is avoided\n</code></pre>\nOther hotspots are also improved:<p>* Flush MemTable to L0 is omited, greatly reducing write amp and is very friendly for large(GB) MemTable</p><pre><code>  * MemTable   serves as the index of Key to \"value position in WAL log\"\n\n  * Since WAL file content almost always in page cache, thus value content can be efficiently accessed by mmap\n\n  * When Flush happens, MemTable is dumpped as an SST and WAL is treated as a blob file\n\n    * CSPP MemTable use integer index instead of physical pointers, thus in-memory format is exactly same with in-file format\n</code></pre>\n* Prefix cache for searching candidate SSTs and prefix cache for scanning by iterators<pre><code>  * Caching fixed len key prefix into an array, binary search it as an uint array\n</code></pre>\n* Distributed compaction(superior replacement to rocksdb remote compaction)<pre><code>  * Gracefully support MergeOperator, CompactionFilter, PropertiesCollector...\n\n  * Out of the box, development efforts are significantly reduced\n\n  * Very easy to share compaction service on spot instances for many DB nodes\n</code></pre>\nUseful Bonus Feature:<p>* Config by json/yaml: can config almost all features</p><p>* Optional embeded WebView: show db structures in web browser, refreshing pages like animation</p><p>* Online update db configs by http</p><p>MySQL integration, ToplingDB has integrated into MySQL by MyTopling, which is forked from MyRocks with great improvements, like improvements of ToplingDB on RocksDB:</p><p>* WBWI(WriteBatchWithIndex): like MemTable, SkipList is replace with CSPP, 20x faster(speedup is more than MemTable).</p><p>* LockManager &amp; LockTracker: 10x faster</p><p>* Encoding &amp; Decoding: 5x faster</p><p>MyRocks has many disadvantages compared to InnoDB, while MyTopling outperforms InnoDB at almost all aspect - excluding feature differences.</p><p>We have create ~100 PRs for RocksDB, in which ~40 were accepted. Our PRs are mostly \"small\" changes, since big changes are not likely accepted.</p><p>ToplingDB has been deployed in numerous production environments.</p>","contentLength":2756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432322"},{"title":"OpenFLOW – Quickly make beautiful infrastructure diagrams local to your machine","url":"https://github.com/stan-smith/OpenFLOW","date":1751351397,"author":"x0z","guid":178962,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44431178"},{"title":"Rust CLI with Clap","url":"https://tucson-josh.com/posts/rust-clap-cli/","date":1751333107,"author":"rajman187","guid":178724,"unread":true,"content":"<p>Types are important. In fact, I'd guess that the expressive type system in rust\nis the single biggest reason why so many developers love the language. Types\nallow us to have a contract between parts of the system about our data and how\nto interact with it. All programming languages have the concept of types, but\nthese exist along several dimensions. Strongly typed vs weakly typed as well as\nstatic vs dynamic typing. Rust stakes out its place as a statically,\nstrongly typed language.</p><p>Many languages that are go-to solutions for creating custom command line tools\nfall in the opposite quadrant with weak, dynamic typing. Whether looking at\ncurrently popular tooling like python and node.js or more traditional solutions\nlike awk and perl, they tend to favor a loose approach to types. Perhaps this\nis the result of an iterative approach to designing CLI tools that might favor\nflexibility. Or it could just be that those languages are already popular,\nleading to an abundance of such programs. Regardless of the reasons, I feel that\nthere is tremendous value for both the developer and user which can arise from\ninteracting with the command line via the sort of strict contract that rust's\ntype system enables.</p><div><p>I assume that if you're already a rust developer, or at least rust-curious, then\nI don't need to convince you of the general value of strong, static typing.\nRather, this is a call to use this same approach for interacting with a command\nline user as you would when developing a library or service API.</p></div><p>At the very lowest level rust exposes command line arguments through the\n function that returns an  struct, an  for the\n arguments passed to start the program. This is illustrated in the Rust\nBook's section on\n<a href=\"https://doc.rust-lang.org/book/ch12-01-accepting-command-line-arguments.html\">accepting command line arguments</a>:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The naive approach seen above obviously lacks robustness as it relies entirely\non argument positioning and also makes a number of other assumptions about the\nresults. Perhaps for very simple tools this solution can work but as the number\nand types of arguments increases, it seems unlikely that a developer would want\nto try and rely on just argument position for the interface to their program.</p><p>A more flexible approach would be to examine all of the arguments passed in and\nparse these for patterns that would allow customary  and  style\noptions. Doing this by hand for every CLI tool would be error-prone and tedious,\nbut fortunately some awesome folks have already done that for you with the\nexcellent <a href=\"https://github.com/clap-rs/clap\">clap</a> crate.</p><h3>The Sound of One Hand Clapping</h3><p>The Command Line Argument Parser for Rust, or clap, is one of the most\nwidely-used crates in the rust ecosystem. GitHub shows that there are over 445k\nrepos which depend on clap at the time of writing. Adding clap to your project\nwill allow you to avoid writing your own parsing logic to interact with the\ncommand line:</p><p>Out of the box clap offers a builder pattern approach that can be used to\nget arguments from the command line without the hassle of parsing an \nof  values:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>Your users can now invoke the above  program from the command line\nand pass in the main argument and optionally enable your x long mode:</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>Clap offers a lot more than just parsing arguments, though. It can also reject\noptions and arguments that are not specified by the programmer and it provides\nbuilt-in help:</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>Okay, so I think we can all agree that clap has some nice features and is far\nmore robust than trying to roll your own command line argument parser, but this\npost started off talking about rust's type system and how that can be used as an\ninterface with the command line user. And that is where clap's  feature\ncomes in.</p><h3>Defining Your CLI Interactions with </h3><p>Clap offers a much more ergonomic way to specify your program's arguments than\nthe builder method shown above, but first you need to include the \nfeature in your dependencies:</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>You can now define rust types in your source which will be translated into an\ninterface contract for your program when called from the command line:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The above program behaves identically to the builder version from the previous\nsection, with a  help option and all the other features that clap offers.\nThe key difference is that we are now using the type system to define the\ninterface rather than imperative calls to a builder. Note that the doc\ncomments for the  struct are used to build the  help subcommand for\nthe resulting application.</p><p>Clap isn't limited to simple structs for the definition of the interface either.\nAs shown above,  works just as you would expect. To build up more\ncomplex command line interactions you can use enums to define subcommand syntax\nwith configuration options for each different subcommand via associated values\n(think  or  subcommands). This offers an elegant solution for managing\nthe complexity that your tool might need to expose to the user.</p><p>There are tons of other great features in clap that can be found in the\n<a href=\"https://docs.rs/clap/latest/clap/index.html\">docs</a>, but rather than get into\nthe specifics of this crate, I want to discuss how type-driven design\ncan elevate command line interfaces to be on equal footing with published\nlibraries and service APIs.</p><p>What can be gained from specifying your software's command line interactions via\nthe rust type system?</p><h3>Advantage 1: Code Maintainability and Readability</h3><p>Perhaps the most obvious benefit of using explicit rust types to define your\ncommand line interface is that it provides a clear, concise definition of what\ninput the program accepts. If you peel away the clap macro calls which annotate\nthe type, it looks just like any other data structure that you would expect to\npass between portions of the program. Because clap builds help from the doc\ncomments, the developer documentation for the type also transcends the command\nline boundary to help users understand how to properly use your software. There\nare no<a href=\"https://tucson-josh.com/posts/rust-clap-cli/#good-for-the-environment-too\">**</a> hidden inputs that will affect your\nprogram. This helps new developers on a project to understand a codebase and\nalso assists maintainers down the road when they need to add new features, as\nthere is a single entry point from which they can start designing their changes.</p><p>Alternative approaches such as using the builder pattern or a custom parsing of\n don't offer this same clarity. At best, these solutions would\nbe contained in one or more functions that abstract away the interface logic. At\nworst these could be scattered across the codebase as each portion of the\nprogram tries to interact directly with the arguments passed in.</p><p>As software grows in complexity the case grows stronger for type-driven CLI\nspecification. Imagine that we are creating a tool which will interact with a\nkey-value store and allow the user to add, remove and list the entries of the\nstore, all of which also require an access token to validate the user. We could\nuse the following to model the interface:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The  type that we've outlined above allows us to clearly express that a\ntoken is always required for all actions, but the  argument is only needed,\nand indeed only allowed, when the user is either adding or removing entries. The\ntype that we have created is concise and removes the complexity one would have\nto deal with if command line arguments were being handled imperatively.</p><h3>Advantage 2: Reduced Test Surface Area and Mock Support</h3><p>Using a crate like clap can eliminate huge swathes of imperative code that would\notherwise be necessary to parse, validate and consume arguments from the command\nline. Every line of code that you don't write saves time on tests that don't\nneed to be created as well. Moving your interaction with the command line from\nimperative functions to a declarative description of possible states moves the\ntesting burden upstream to the maintainers of the clap crate, which is widely\nused and well supported.</p><p>Type-driven command line interaction does more for us than just reducing the\nsurface area, though. It also provides a foundation for more complete unit tests\nby providing the simplest possible mock for an actual command line interaction.\nImagine that our key-value client above delegates each of the top-level actions\n(add, remove, list) to one function each, where more complex operations are\norchestrated. Something like the following:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>Some obvious tests of the above method might involve asserting that \nwould return an  wrapping a <code>KVStoreError::InvalidRequest</code> if we call the\nfunction with , for instance. We could also verify that the key\nreturned by the server matches the key we requested to add:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The above test is simplistic, but it is representative of the way data must be\nstructured from an actual user because of the strict typing. This approach gives\nus a high fidelity mock of a command line interaction.</p><h3>Advantage 3: Semantic Versioning: Not Just for Libraries</h3><p>Semantic versioning, or SemVer, is a widely-used framework for determining how\nsoftware creators should version their releases so that downstream users of that\ncode can confidently know what versions are safe to upgrade to from other\nversions. It leads to the familiar three-part version number consisting of\n where each component conveys different levels of change and\npotential upgrade risk. The rust core team follows SemVer for rust releases and\neven have an extensive\n<a href=\"https://doc.rust-lang.org/cargo/reference/semver.html\">section</a> about the topic\nin the Cargo Book.</p><p>Library maintainers generally follow SemVer so that other developers who depend\non their crate can understand when it is safe to upgrade without needing to\ndelve into the release notes of every single release. Authors of binary tools,\nhowever, have been less likely to strictly follow SemVer, as illustrated by the\n<a href=\"https://blog.rust-lang.org/2025/03/04/Rustup-1.28.1/\">rustup 1.28.0</a> adventure,\nwherein a minor release ended up breaking CI for many rust projects.</p><p>Perhaps the reason why authors of binary CLI tools are less likely to follow\nSemVer is because they have an image in their head of the user being a person\nwho can adapt to changes between versions. The reality, however, is that any\nsufficiently useful CLI tool will eventually be integrated into an automated\ntoolchain that expects input and output to be consistent across versions. Good\nCLI tools end up operating very similarly to a library. Unlike libraries,\nthough, upgrading a binary version doesn't get a chance to throw compiler\nerrors. Worse yet, CLI tools are often integrated in parts of the stack where\nobservability is poor and errors are only discovered when catastrophic failure\nhas already occurred.</p><p>So, how can a strictly-typed approach to command line arguments help us to\nbetter follow SemVer with CLI applications? The answer to this is through\ntooling that already exists,\n<a href=\"https://github.com/obi1kenobi/cargo-semver-checks\">cargo-semver-checks</a>. This\ncargo tool examines your source code and compares it against a prior release\nin order to determine if your changes constitute major, minor or merely\npatch level changes. Importantly, though, you should begin to think of your\ncommand line program more like a library in order to help cargo-semver-checks\nto analyze the importance of changes. Your CLI argument types should be made\n even if this level isn't required for your program to run properly.\nThey are, after all, truly the most public part of the software. A similar\napproach is also reasonable with the types that might represent your program's\noutput, whether they are used to write back to the shell, to files or some other\nform of output. Once you've done this, start versioning your binaries\naccordingly. If cargo-semver-checks warns you that a change is major and you\nonly thought that it was a patch, that's a big warning. Did you really intend to\nmake a major, breaking change? If you did, then don't hesitate to change the\nmajor version number.</p><p>Merely knowing about a tool like cargo-semver-checks and having it installed\nis nice, but we all know that things like this are best when they become an\nautomated part of our workflow. It's easy to add a GitHub Action to run a SemVer\ncheck automatically:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Now, even if you forget to run your SemVer check manually, you probably won't\npush out a binary release that breaks some dependency in a completely hidden\nway.</p><h3>Good for the Environment Too</h3><p>There is a loose end that may have been nagging at some readers going over the\nprevious sections: <em>What about environment variables?</em> After all, many command\nline programs can also look at the shell's environment variables as a source of\ninput. We see this particularly around secrets or omnipresent settings.\nFortunately clap has us covered here too with the crate feature  that lets\nyou specify an environment variable which will be queried when a given argument\nwas not specified as part of the command invocation.</p><p>Let's use this to flesh out the code from our key-value store client example in\nthe <a href=\"https://tucson-josh.com/posts/rust-clap-cli/#advantage-1-code-maintainability-and-readability\">maintainability</a> section\nabove. In that example, it would make a lot of sense to make  an argument\nwhich can be stored in an environment variable as well as be overridden from the\ncommand line.</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>All that was required (aside from adding the  feature to our dependencies)\nwas to add  on line 7. The user can now either pass in the\ntoken via  or by setting the environment variable . The\ngenerated help will automatically pick this up and educate the user about this\noption (line 13 below):</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>We are now able to have a fully type-driven specification of our command line\ninterface that seamlessly incorporates both the arguments passed in as well as\nenvironment variables from the shell. What's not to love?</p><p>\n    If you want to discuss  this post\n     or any other, please feel free to drop me a message on\n    <a href=\"https://www.instagram.com/tucson.josh/\">Instagram</a>\n    or over at\n    <a href=\"https://bsky.app/profile/tucson-josh.com\">Bluesky</a>.\n</p>","contentLength":13399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429695"},{"title":"The Email Startup Graveyard: Why 80%+ of Email Companies Fail","url":"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail","date":1751330464,"author":"skeptrune","guid":178666,"unread":true,"content":"<img loading=\"lazy\" src=\"https://forwardemail.net/img/articles/email-startup-graveyard-fa0072188b.webp\" alt=\"\"><p>While many email startups have invested millions in solving perceived problems, we at <a href=\"https://forwardemail.net\">Forward Email</a> have focused on building reliable email infrastructure from scratch since 2017. This analysis explores the patterns behind email startup outcomes and the fundamental challenges of email infrastructure.</p><div><p>: Most email startups don't build actual email infrastructure from scratch. Many build on top of existing solutions like Amazon SES or open-source systems like Postfix. The core protocols work well - the challenge is in the implementation.</p></div><div><p>: For comprehensive details on our approach, architecture, and security implementation, see our <a href=\"https://forwardemail.net/technical-whitepaper.pdf\">Forward Email Technical Whitepaper</a> and <a href=\"https://forwardemail.net/en/about\">About page</a> which documents our complete development timeline since 2017.</p></div><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-email-startup-failure-matrix\" role=\"button\" aria-label=\"The Email Startup Failure Matrix\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-email-startup-failure-matrix\" data-target=\"#collapse-the-email-startup-failure-matrix\"></a>The Email Startup Failure Matrix</h2><p>Here's every major email startup failure we could find, organized by accelerator, funding, and outcome:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-infrastructure-reality-check\" role=\"button\" aria-label=\"The Infrastructure Reality Check\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-infrastructure-reality-check\" data-target=\"#collapse-the-infrastructure-reality-check\"></a>The Infrastructure Reality Check</h2><div><p>: Every single \"email startup\" is just building UI on top of existing infrastructure. They're not building actual email servers - they're building apps that connect to real email infrastructure.</p></div><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-email-startups-actually-build\" role=\"button\" aria-label=\"Go to top\"></a>What \"Email Startups\" Actually Build</h3><div><p><strong>Key Pattern for Email Success</strong>: The companies that actually succeed in email don't try to reinvent the wheel. Instead, they build <strong>infrastructure and tools that enhance</strong> existing email workflows. <a href=\"https://sendgrid.com/\" target=\"_blank\" rel=\"noopener noreferrer\">SendGrid</a>, <a href=\"https://www.mailgun.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Mailgun</a>, and <a href=\"https://postmarkapp.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Postmark</a> became billion-dollar companies by providing reliable SMTP APIs and delivery services - they work  email protocols, not against them. This is the same approach we take at Forward Email.</p></div><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-most-email-startups-fail\" role=\"button\" aria-label=\"Why Most Email Startups Fail\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-why-most-email-startups-fail\" data-target=\"#collapse-why-most-email-startups-fail\"></a>Why Most Email Startups Fail</h2><div><p>: Email  startups typically fail because they try to replace working protocols, while email  companies can succeed by enhancing existing workflows. The key is understanding what users actually need versus what entrepreneurs think they need.</p></div><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#1-email-protocols-work-implementation-often-doesnt\" role=\"button\" aria-label=\"Go to top\"></a>1. Email Protocols Work, Implementation Often Doesn't</h3><p>The core email protocols are solid, but implementation quality varies widely:</p><p>: Better implementation of existing protocols, not protocol replacement.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#2-network-effects-are-unbreakable\" role=\"button\" aria-label=\"Go to top\"></a>2. Network Effects Are Unbreakable</h3><p>Email's network effect is absolute:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#3-they-often-target-the-wrong-problems\" role=\"button\" aria-label=\"Go to top\"></a>3. They Often Target the Wrong Problems</h3><p>Many email startups focus on perceived issues rather than real pain points:</p><p><strong>Real problems worth solving</strong>: Infrastructure reliability, deliverability, spam filtering, and developer tools.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#4-technical-debt-is-massive\" role=\"button\" aria-label=\"Go to top\"></a>4. Technical Debt Is Massive</h3><p>Building real email infrastructure requires:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#5-the-infrastructure-already-exists\" role=\"button\" aria-label=\"Go to top\"></a>5. The Infrastructure Already Exists</h3><p>Why reinvent when you can use:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#case-studies-when-email-startups-fail\" role=\"button\" aria-label=\"Case Studies: When Email Startups Fail\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-case-studies-when-email-startups-fail\" data-target=\"#collapse-case-studies-when-email-startups-fail\"></a>Case Studies: When Email Startups Fail</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#case-study-the-skiff-disaster\" role=\"button\" aria-label=\"Go to top\"></a>Case Study: The Skiff Disaster</h3><p>Skiff perfectly exemplifies everything wrong with email startups.</p><ul><li>: \"Privacy-first email and productivity platform\"</li><li>: Better email through privacy and encryption</li></ul><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#y-combinator-the-email-app-factory\" role=\"button\" aria-label=\"Go to top\"></a>Y Combinator: The Email App Factory</h4><p><a href=\"https://www.ycombinator.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Y Combinator</a> has funded dozens of email startups. Here's the pattern:</p><p>: Mixed results with some notable exits. Several companies achieved successful acquisitions (reMail to Google, Rapportive to LinkedIn), while others pivoted away from email or were acqui-hired for talent.</p><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#techstars-the-email-graveyard\" role=\"button\" aria-label=\"Go to top\"></a>Techstars: The Email Graveyard</h4><p>: Vague value propositions, no real technical innovation, quick failures.</p><div><p>: VCs love email startups because they sound simple but are actually impossible. The fundamental assumptions that attract investment are exactly what guarantee failure.</p></div><p>VCs love email startups because they sound simple but are actually impossible:</p><p>: None of these assumptions hold true for email.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-technical-reality-modern-email-stacks\" role=\"button\" aria-label=\"The Technical Reality: Modern Email Stacks\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-technical-reality-modern-email-stacks\" data-target=\"#collapse-the-technical-reality-modern-email-stacks\"></a>The Technical Reality: Modern Email Stacks</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-actually-powers-email-startups\" role=\"button\" aria-label=\"Go to top\"></a>What Actually Powers \"Email Startups\"</h3><p>Let's look at what these companies actually run:</p><p>: Most email apps are Electron-based web apps that consume massive amounts of RAM:</p><div><p><strong>Electron Performance Crisis</strong>: Modern email clients built with Electron and React Native suffer from severe memory bloat and performance issues. These cross-platform frameworks, while convenient for developers, create resource-heavy applications that consume hundreds of megabytes to gigabytes of RAM for basic email functionality.</p></div><p>: Constant syncing and inefficient code:</p><ul><li>Background processes that never sleep</li><li>Unnecessary API calls every few seconds</li><li>Poor connection management</li><li>No third-party dependencies except those absolutely required for core functionality</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-acquisition-patterns-success-vs-shutdown\" role=\"button\" aria-label=\"The Acquisition Patterns: Success vs. Shutdown\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-acquisition-patterns-success-vs-shutdown\" data-target=\"#collapse-the-acquisition-patterns-success-vs-shutdown\"></a>The Acquisition Patterns: Success vs. Shutdown</h2><p><strong>Client App Pattern (Usually Fails)</strong>:</p><p><strong>Infrastructure Pattern (Often Succeeds)</strong>:</p><p>:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#industry-evolution-and-consolidation\" role=\"button\" aria-label=\"Industry Evolution and Consolidation\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-industry-evolution-and-consolidation\" data-target=\"#collapse-industry-evolution-and-consolidation\"></a>Industry Evolution and Consolidation</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#natural-industry-progression\" role=\"button\" aria-label=\"Go to top\"></a>Natural Industry Progression</h3><p>The email industry has naturally evolved toward consolidation, with larger companies acquiring smaller ones to integrate features or eliminate competition. This isn't necessarily negative - it's how most mature industries develop.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#post-acquisition-transitions\" role=\"button\" aria-label=\"Go to top\"></a>Post-Acquisition Transitions</h3><p>When email companies are acquired, users often face:</p><ul><li>: Moving to new platforms</li><li>: Loss of specialized functionality</li><li>: Different subscription models</li><li>: Temporary service disruptions</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#user-considerations-during-transitions\" role=\"button\" aria-label=\"Go to top\"></a>User Considerations During Transitions</h3><p>During industry consolidation, users benefit from:</p><ul><li>: Multiple providers offer similar services</li><li><strong>Understanding migration paths</strong>: Most services provide export tools</li><li><strong>Considering long-term stability</strong>: Established providers often offer more continuity</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-hacker-news-reality-check\" role=\"button\" aria-label=\"The Hacker News Reality Check\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-hacker-news-reality-check\" data-target=\"#collapse-the-hacker-news-reality-check\"></a>The Hacker News Reality Check</h2><p>Every email startup gets the same comments on <a href=\"https://news.ycombinator.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Hacker News</a>:</p><p>. These comments appear on every email startup launch because the fundamental problems are always the same.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-modern-ai-email-grift\" role=\"button\" aria-label=\"The Modern AI Email Grift\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-modern-ai-email-grift\" data-target=\"#collapse-the-modern-ai-email-grift\"></a>The Modern AI Email Grift</h2><p>2024 brought a new wave of \"AI-powered email\" startups, with the first major successful exit already happening:</p><p>Adding \"AI\" doesn't solve the fundamental challenges:</p><p>: AI features require significant infrastructure investment while addressing relatively minor pain points.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-actually-works-the-real-email-success-stories\" role=\"button\" aria-label=\"What Actually Works: The Real Email Success Stories\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-what-actually-works-the-real-email-success-stories\" data-target=\"#collapse-what-actually-works-the-real-email-success-stories\"></a>What Actually Works: The Real Email Success Stories</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#infrastructure-companies-the-winners\" role=\"button\" aria-label=\"Go to top\"></a>Infrastructure Companies (The Winners)</h3><p>: They build infrastructure, not apps.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-providers-the-survivors\" role=\"button\" aria-label=\"Go to top\"></a>Email Providers (The Survivors)</h3><div><p><strong>The JMAP Investment Question</strong>: While Fastmail invests resources in <a href=\"https://jmap.io/\" target=\"_blank\" rel=\"noopener noreferrer\">JMAP</a>, a protocol that's <a href=\"https://github.com/zone-eu/wildduck/issues/2#issuecomment-1765190790\" target=\"_blank\" rel=\"noopener noreferrer\">10+ years old with limited adoption</a>, they simultaneously <a href=\"https://www.fastmail.com/blog/why-we-dont-offer-pgp/\" target=\"_blank\" rel=\"noopener noreferrer\">refuse to implement PGP encryption</a> that many users request. This represents a strategic choice to prioritize protocol innovation over user-requested features. Whether JMAP will gain broader adoption remains to be seen, but the current email client ecosystem continues to rely primarily on IMAP/SMTP.</p></div><div><p>: Forward Email powers <a href=\"https://forwardemail.net/en/blog/docs/alumni-email-forwarding-university-case-study\">alumni email solutions for top universities</a>, including the University of Cambridge with 30,000 alumni addresses, delivering $87,000 in annual cost savings compared to traditional solutions.</p></div><p>: They enhance email, don't replace it.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-exception-xobnis-success-story\" role=\"button\" aria-label=\"Go to top\"></a>The Exception: Xobni's Success Story</h3><p><a href=\"https://en.wikipedia.org/wiki/Xobni\" target=\"_blank\" rel=\"noopener noreferrer\">Xobni</a> stands out as one of the few email-related startups that actually succeeded by taking the right approach.</p><ul><li>: Built on top of Outlook instead of replacing it</li><li>: Contact management and email search</li><li>: Worked with existing workflows</li><li>: Targeted business users with real pain points</li></ul><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-xobni-succeeded-where-others-failed\" role=\"button\" aria-label=\"Go to top\"></a>Why Xobni Succeeded Where Others Failed</h4><ol><li><strong>Built on proven infrastructure</strong>: Used Outlook's existing email handling</li><li>: Contact management was genuinely broken</li><li>: Businesses pay for productivity tools</li><li>: Enhanced rather than replaced existing workflows</li></ol><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-founders-continued-success\" role=\"button\" aria-label=\"Go to top\"></a>The Founders' Continued Success</h4><ul><li>: Became an active <a href=\"https://mercury.com/investor-database/matt-brezina\" target=\"_blank\" rel=\"noopener noreferrer\">angel investor</a> with investments in Dropbox, Mailbox, and others</li><li>: Continued building successful companies in the productivity space</li><li>: Demonstrated that email success comes from enhancement, not replacement</li></ul><p>Companies succeed in email when they:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#has-anyone-successfully-reinvented-email\" role=\"button\" aria-label=\"Has Anyone Successfully Reinvented Email?\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-has-anyone-successfully-reinvented-email\" data-target=\"#collapse-has-anyone-successfully-reinvented-email\"></a>Has Anyone Successfully Reinvented Email?</h2><p>This is a crucial question that gets to the heart of email innovation. The short answer is: <strong>no one has successfully replaced email, but some have successfully enhanced it</strong>.</p><p>Looking at email innovations over the past 20 years:</p><p>: All successful innovations  existing email protocols rather than replacing them.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#new-tools-complement-email-but-dont-replace-it\" role=\"button\" aria-label=\"Go to top\"></a>New Tools Complement Email (But Don't Replace It)</h3><ul><li>: Great for team chat, but still sends email notifications</li><li>: Excellent for communities, but uses email for account management</li><li>: Perfect for messaging, but businesses still use email</li><li>: Essential for video calls, but meeting invites come via email</li></ul><div><p>: HEY's founder <a href=\"https://dhh.dk/\" target=\"_blank\" rel=\"noopener noreferrer\">DHH</a> actually uses our service at Forward Email for his personal domain  and has for several years, demonstrating that even email innovators rely on proven infrastructure.</p></div><p><a href=\"https://hey.com/\" target=\"_blank\" rel=\"noopener noreferrer\">HEY</a> by <a href=\"https://basecamp.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Basecamp</a> represents the most serious recent attempt to \"reinvent\" email:</p><ul><li>: Completely new email paradigm with screening, bundling, and workflows</li><li>: Mixed - some love it, most stick with existing email</li><li>: It's still email (SMTP/IMAP) with a different interface</li></ul><p>The most successful email innovations have been:</p><ol><li>: Faster servers, better spam filtering, improved deliverability</li><li>: APIs for sending email, webhooks for tracking</li><li>: CRM integration, marketing automation, transactional email</li></ol><p><strong>None of these replaced email - they made it better.</strong></p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#building-modern-infrastructure-for-existing-email-protocols-our-approach\" role=\"button\" aria-label=\"Building Modern Infrastructure for Existing Email Protocols: Our Approach\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-building-modern-infrastructure-for-existing-email-protocols-our-approach\" data-target=\"#collapse-building-modern-infrastructure-for-existing-email-protocols-our-approach\"></a>Building Modern Infrastructure for Existing Email Protocols: Our Approach</h2><p>Before diving into the failures, it's important to understand what actually works in email. The challenge isn't that email is broken - it's that most companies try to \"fix\" something that already works perfectly.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-email-innovation-spectrum\" role=\"button\" aria-label=\"Go to top\"></a>The Email Innovation Spectrum</h3><p>Email innovation falls into three categories:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-we-focus-on-infrastructure\" role=\"button\" aria-label=\"Go to top\"></a>Why We Focus on Infrastructure</h3><p>We chose to build modern email infrastructure because:</p><ul><li><strong>The problem is implementation</strong>: Most email services use outdated software stacks</li><li>: Not new features that break existing workflows</li><li>: Better APIs and management interfaces</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-actually-works-in-email\" role=\"button\" aria-label=\"Go to top\"></a>What Actually Works in Email</h3><p>The successful pattern is simple: <strong>enhance existing email workflows instead of replacing them</strong>. This means:</p><ul><li>Building faster, more reliable SMTP servers</li><li>Creating better spam filtering without breaking legitimate email</li><li>Providing developer-friendly APIs for existing protocols</li><li>Improving deliverability through proper infrastructure</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#our-approach-why-were-different\" role=\"button\" aria-label=\"Our Approach: Why We're Different\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-our-approach-why-were-different\" data-target=\"#collapse-our-approach-why-were-different\"></a>Our Approach: Why We're Different</h2><ul><li><strong>Build actual infrastructure</strong>: Custom SMTP/IMAP servers from scratch</li><li><strong>Enhance existing workflows</strong>: Work with all email clients</li><li>: APIs and tools that actually work</li></ul><ul><li>Build \"revolutionary\" email clients</li><li>Try to replace existing email protocols</li><li>Add unnecessary AI features</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#how-we-build-email-infrastructure-that-actually-works\" role=\"button\" aria-label=\"How We Build Email Infrastructure That Actually Works\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-how-we-build-email-infrastructure-that-actually-works\" data-target=\"#collapse-how-we-build-email-infrastructure-that-actually-works\"></a>How We Build Email Infrastructure That Actually Works</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#our-anti-startup-approach\" role=\"button\" aria-label=\"Go to top\"></a>Our Anti-Startup Approach</h3><p>While other companies burn millions trying to reinvent email, we focus on building reliable infrastructure:</p><ul><li>: We've been building email infrastructure for 7+ years</li><li>: We're building for the long term</li><li><strong>No \"revolutionary\" claims</strong>: We just make email work better</li></ul><div><p><strong>Government-Grade Compliance</strong>: Forward Email is <a href=\"https://forwardemail.net/en/blog/docs/federal-government-email-service-section-889-compliant\">Section 889 compliant</a> and serves organizations like the US Naval Academy, demonstrating our commitment to meeting stringent federal security requirements.</p></div><div><p><strong>OpenPGP and OpenWKD Implementation</strong>: Unlike Fastmail, which <a href=\"https://www.fastmail.com/blog/why-we-dont-offer-pgp/\" target=\"_blank\" rel=\"noopener noreferrer\">refuses to implement PGP</a> citing complexity concerns, Forward Email provides full OpenPGP support with OpenWKD (Web Key Directory) compliance, giving users the encryption they actually want without forcing them to use experimental protocols like JMAP.</p></div><p><strong>Technical Stack Comparison</strong>:</p><ul><li>= <a href=\"https://blog.apnic.net/2024/10/04/smtp-downgrade-attacks-and-mta-sts/#:~:text=Logs%20indicate%20that%20Proton%20Mail%20uses%C2%A0postfix%2Dmta%2Dsts%2Dresolver%2C%20hinting%20that%20they%20run%20a%20Postfix%20stack\" target=\"_blank\" rel=\"noopener noreferrer\">APNIC blog post</a> confirms Proton uses postfix-mta-sts-resolver, indicating they run a Postfix stack</li></ul><ul><li>: JavaScript across the entire stack vs. 1980s C code</li><li>: Single language eliminates integration complexity</li><li>: Built for modern web development from the ground up</li><li>: Any web developer can understand and contribute</li><li>: Clean, modern codebase without decades of patches</li></ul><div><p>: Our <a href=\"https://forwardemail.net/en/privacy\">privacy policy</a> ensures we don't store forwarded emails to disk storage or databases, don't store metadata about emails, and don't store logs or IP addresses - operating in-memory only for email forwarding services.</p></div><p>: For comprehensive details on our approach, architecture, and security implementation, see our <a href=\"https://forwardemail.net/technical-whitepaper.pdf\">technical whitepaper</a> and extensive technical documentation.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-service-provider-comparison-growth-through-proven-protocols\" role=\"button\" aria-label=\"Go to top\"></a>Email Service Provider Comparison: Growth Through Proven Protocols</h3><div><p>: While other providers chase experimental protocols, Forward Email focuses on what users actually want - reliable IMAP, POP3, SMTP, CalDAV, and CardDAV that works across all devices. Our growth demonstrates the value of this approach.</p></div><table><tbody><tr></tr><tr></tr><tr><td><code>in1-smtp.messagingengine.com</code></td></tr><tr></tr><tr></tr><tr></tr></tbody></table><ul><li> shows strong growth (+21.1%) with over 500K domains using our MX records</li><li><strong>Proven infrastructure wins</strong>: Services with reliable IMAP/SMTP show consistent domain adoption</li><li>: Fastmail's JMAP investment shows slower growth (+14%) compared to providers focusing on standard protocols</li><li>: The defunct startup lost 55.2% of domains, demonstrating the failure of \"revolutionary\" email approaches</li><li>: Domain count growth reflects real user adoption, not marketing metrics</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-we-succeed-where-others-fail\" role=\"button\" aria-label=\"Go to top\"></a>Why We Succeed Where Others Fail</h3><ol><li><strong>We build infrastructure, not apps</strong>: Focus on servers and protocols</li><li><strong>We enhance, don't replace</strong>: Work with existing email clients</li><li>: No VC pressure to \"grow fast and break things\"</li><li>: 7+ years of deep technical experience</li><li>: APIs and tools that actually solve problems</li></ol><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#security-challenges-in-email-infrastructure\" role=\"button\" aria-label=\"Security Challenges in Email Infrastructure\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-security-challenges-in-email-infrastructure\" data-target=\"#collapse-security-challenges-in-email-infrastructure\"></a>Security Challenges in Email Infrastructure</h2><p>Email security is a complex challenge that affects all providers in the industry. Rather than highlighting individual incidents, it's more valuable to understand the common security considerations that all email infrastructure providers must address.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#common-security-considerations\" role=\"button\" aria-label=\"Go to top\"></a>Common Security Considerations</h3><p>All email providers face similar security challenges:</p><ul><li>: Securing user data and communications</li><li>: Managing authentication and authorization</li><li>: Protecting servers and databases</li><li>: Meeting various regulatory requirements like <a href=\"https://gdpr.eu/\" target=\"_blank\" rel=\"noopener noreferrer\">GDPR</a> and <a href=\"https://oag.ca.gov/privacy/ccpa\" target=\"_blank\" rel=\"noopener noreferrer\">CCPA</a></li></ul><div><p>: Our <a href=\"https://forwardemail.net/en/security\">security practices</a> include ChaCha20-Poly1305 encryption for mailboxes, full disk encryption with LUKS v2, and comprehensive protection with encryption-at-rest, encryption-in-memory, and encryption-in-transit.</p></div><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-value-of-transparency\" role=\"button\" aria-label=\"Go to top\"></a>The Value of Transparency</h3><p>When security incidents occur, the most valuable response is transparency and quick action. Companies that:</p><ul><li><strong>Disclose incidents promptly</strong>: Help users make informed decisions</li><li><strong>Provide detailed timelines</strong>: Show they understand the scope of issues</li><li>: Demonstrate technical competence</li><li>: Contribute to industry-wide security improvements</li></ul><p>These responses benefit the entire email ecosystem by promoting best practices and encouraging other providers to maintain high security standards.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#ongoing-security-challenges\" role=\"button\" aria-label=\"Go to top\"></a>Ongoing Security Challenges</h3><p>The email industry continues to evolve its security practices:</p><p>These challenges require ongoing investment and expertise from all providers in the space.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#conclusion-focus-on-infrastructure-not-apps\" role=\"button\" aria-label=\"Conclusion: Focus on Infrastructure, Not Apps\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-conclusion-focus-on-infrastructure-not-apps\" data-target=\"#collapse-conclusion-focus-on-infrastructure-not-apps\"></a>Conclusion: Focus on Infrastructure, Not Apps</h2><p>After analyzing hundreds of email startups:</p><ul><li>: Most email startups fail completely (this figure is likely WAY higher than 80%; we're being nice)</li><li>: Being acquired usually means death for email clients</li><li><strong>Infrastructure can succeed</strong>: Companies building SMTP/API services often thrive</li><li><strong>VC funding creates pressure</strong>: Venture capital creates unrealistic growth expectations</li><li><strong>Technical debt accumulates</strong>: Building email infrastructure is harder than it looks</li></ul><p>Email has been \"dying\" for 20+ years according to startups:</p><ul><li>: \"Social networks will replace email\"</li><li>: \"Mobile messaging will kill email\"</li><li>: \"<a href=\"https://slack.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Slack</a> will replace email\"</li><li>: \"AI will revolutionize email\"</li><li>: \"Remote work needs new communication tools\"</li><li>: \"AI will finally fix email\"</li></ul><p>. It's still growing. It's still essential.</p><p>The lesson isn't that email can't be improved. It's about choosing the right approach:</p><ol><li>: Reliability and performance beat flashy features</li><li><strong>Enhancement beats replacement</strong>: Work with email, don't fight it</li><li><strong>Sustainability beats growth</strong>: Profitable businesses outlast VC-funded ones</li><li>: Tools and APIs create more value than end-user apps</li></ol><p>: Better implementation of proven protocols, not protocol replacement.</p><div><p><strong>Comprehensive Email Service Analysis</strong>: For an in-depth comparison of 79 email services in 2025, including detailed reviews, screenshots, and technical analysis, see our comprehensive guide: <a href=\"https://forwardemail.net/en/blog/best-email-service\">79 Best Email Services</a>. This analysis demonstrates why Forward Email consistently ranks as the recommended choice for reliability, security, and standards compliance.</p></div><p>If you're thinking about building an email startup, consider building email infrastructure instead. The world needs better email servers, not more email apps.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-extended-email-graveyard-more-failures-and-shutdowns\" role=\"button\" aria-label=\"The Extended Email Graveyard: More Failures and Shutdowns\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-extended-email-graveyard-more-failures-and-shutdowns\" data-target=\"#collapse-the-extended-email-graveyard-more-failures-and-shutdowns\"></a>The Extended Email Graveyard: More Failures and Shutdowns</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#googles-email-experiments-gone-wrong\" role=\"button\" aria-label=\"Go to top\"></a>Google's Email Experiments Gone Wrong</h3><p>Google, despite owning <a href=\"https://gmail.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gmail</a>, has killed multiple email projects:</p><ul><li> (2009-2012): \"Email killer\" that nobody understood</li><li> (2010-2011): Social email integration disaster</li><li> email features (2011-2019): Social network email integration</li></ul><p>: Even Google can't successfully reinvent email.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-serial-failure-newton-mails-three-deaths\" role=\"button\" aria-label=\"Go to top\"></a>The Serial Failure: Newton Mail's Three Deaths</h3><ol><li> (2013-2016): Email client acquired by Newton</li><li> (2016-2018): Rebranded, subscription model failed</li></ol><p>: Email clients can't sustain subscription models.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-apps-that-never-launched\" role=\"button\" aria-label=\"Go to top\"></a>The Apps That Never Launched</h3><p>Many email startups died before launching:</p><ul><li> (2014): Calendar-email integration, shut down pre-launch</li><li> (2011): Email management tool, acquired before release</li><li> (2013): Email client, development stopped</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-acquisition-to-shutdown-pattern\" role=\"button\" aria-label=\"Go to top\"></a>The Acquisition-to-Shutdown Pattern</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-infrastructure-consolidation\" role=\"button\" aria-label=\"Go to top\"></a>Email Infrastructure Consolidation</h3><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-open-source-email-graveyard-when-free-isnt-sustainable\" role=\"button\" aria-label=\"The Open-Source Email Graveyard: When &quot;Free&quot; Isn't Sustainable\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-open-source-email-graveyard-when-free-isnt-sustainable\" data-target=\"#collapse-the-open-source-email-graveyard-when-free-isnt-sustainable\"></a>The Open-Source Email Graveyard: When \"Free\" Isn't Sustainable</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#nylas-mail--mailspring-the-fork-that-couldnt\" role=\"button\" aria-label=\"Go to top\"></a>Nylas Mail → Mailspring: The Fork That Couldn't</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#eudora-the-18-year-death-march\" role=\"button\" aria-label=\"Go to top\"></a>Eudora: The 18-Year Death March</h3><ul><li>: Dominant email client for Mac/Windows</li><li>: Open-sourced as \"Eudora OSE\"</li><li>: Even successful email clients eventually die</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#fairemail-killed-by-google-play-politics\" role=\"button\" aria-label=\"Go to top\"></a>FairEmail: Killed by Google Play Politics</h3><p>Open-source email projects fail because:</p><ul><li>: Email protocols are complex to implement correctly</li><li>: Constant security updates required</li><li>: Must work with all email providers</li><li>: Volunteer developers burnout</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-ai-email-startup-surge-history-repeating-with-intelligence\" role=\"button\" aria-label=\"The AI Email Startup Surge: History Repeating with &quot;Intelligence&quot;\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-ai-email-startup-surge-history-repeating-with-intelligence\" data-target=\"#collapse-the-ai-email-startup-surge-history-repeating-with-intelligence\"></a>The AI Email Startup Surge: History Repeating with \"Intelligence\"</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-current-ai-email-gold-rush\" role=\"button\" aria-label=\"Go to top\"></a>The Current AI Email Gold Rush</h3><p>2024's AI email startups:</p><p>VCs are throwing money at \"AI + Email\":</p><ul><li>: \"Revolutionary email experience\"</li><li>: Building on top of existing infrastructure</li><li>: Most will fail within 3 years</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-theyll-all-fail-again\" role=\"button\" aria-label=\"Go to top\"></a>Why They'll All Fail (Again)</h3><ol><li><strong>AI doesn't solve email's non-problems</strong>: Email works fine</li><li>: AI requires reading all your emails</li><li>: AI processing is expensive, email is commodity</li><li>: Can't break Gmail/Outlook dominance</li></ol><ul><li>: Most remaining AI email startups will pivot or shut down</li><li>: Survivors will be acquired, with mixed outcomes</li><li>: \"Blockchain email\" or the next trend will emerge</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-consolidation-catastrophe-when-survivors-become-disasters\" role=\"button\" aria-label=\"The Consolidation Catastrophe: When &quot;Survivors&quot; Become Disasters\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-consolidation-catastrophe-when-survivors-become-disasters\" data-target=\"#collapse-the-consolidation-catastrophe-when-survivors-become-disasters\"></a>The Consolidation Catastrophe: When \"Survivors\" Become Disasters</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-great-email-service-consolidation\" role=\"button\" aria-label=\"Go to top\"></a>The Great Email Service Consolidation</h3><p>The email industry has consolidated dramatically:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#outlook-the-survivor-that-cant-stop-breaking\" role=\"button\" aria-label=\"Go to top\"></a>Outlook: The \"Survivor\" That Can't Stop Breaking</h3><p><strong>Our Real-World Experience</strong>: We regularly help customers whose Outlook setups break our perfectly compliant IMAP implementation.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-postmark-infrastructure-problem\" role=\"button\" aria-label=\"Go to top\"></a>The Postmark Infrastructure Problem</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#recent-email-client-casualties-2024-2025\" role=\"button\" aria-label=\"Go to top\"></a>Recent Email Client Casualties (2024-2025)</h3><p>: Users increasingly report poor experience with the email client.</p><p>: Windows users face licensing issues and subscription confusion.</p><p>: The Mac/iOS email client, based on the failed Sparrow codebase, continues to receive <a href=\"https://airmailapp.com/\" target=\"_blank\" rel=\"noopener noreferrer\">poor reviews</a> for reliability issues.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-extension-and-service-acquisitions\" role=\"button\" aria-label=\"Go to top\"></a>Email Extension and Service Acquisitions</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-survivors-email-companies-that-actually-work\" role=\"button\" aria-label=\"Go to top\"></a>The Survivors: Email Companies That Actually Work</h3><p>Not all email companies fail. Here are the ones that actually work:</p><p>: Bootstrap success story generating <a href=\"https://www.indiehackers.com/product/gmass\" target=\"_blank\" rel=\"noopener noreferrer\">$140K/month</a> as a Gmail extension for email marketing.</p><p>: These companies succeed because they <strong>enhance existing email workflows</strong> rather than trying to replace email entirely. They build tools that work  email infrastructure, not against it.</p>","contentLength":18695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429457"},{"title":"Claude Code now supports hooks","url":"https://docs.anthropic.com/en/docs/claude-code/hooks","date":1751328075,"author":"ramoz","guid":177020,"unread":true,"content":"<p>Claude Code hooks are user-defined shell commands that execute at various points\nin Claude Code’s lifecycle. Hooks provide deterministic control over Claude\nCode’s behavior, ensuring certain actions always happen rather than relying on\nthe LLM to choose to run them.</p><p>Example use cases include:</p><ul><li>: Customize how you get notified when Claude Code is awaiting\nyour input or permission to run something.</li><li>: Run  on .ts files,  on .go files,\netc. after every file edit.</li><li>: Track and count all executed commands for compliance or\ndebugging.</li><li>: Provide automated feedback when Claude Code produces code that\ndoes not follow your codebase conventions.</li><li>: Block modifications to production files or sensitive\ndirectories.</li></ul><p>By encoding these rules as hooks rather than prompting instructions, you turn\nsuggestions into app-level code that executes every time it is expected to run.</p><p>In this quickstart, you’ll add a hook that logs the shell commands that Claude\nCode runs.</p><p>Quickstart Prerequisite: Install  for JSON processing in the command line.</p><h3></h3><p> hooks run before tool calls and can block them while providing\nClaude feedback on what to do differently.</p><p>Select  to run your hook only on Bash tool calls.</p><p>Select  and enter this command:</p><h3></h3><p>For storage location, select  since you’re logging to your home\ndirectory. This hook will then apply to all projects, not just your current\nproject.</p><p>Then press Esc until you return to the REPL. Your hook is now registered!</p><h3></h3><p>Run  again or check  to see your configuration:</p><ul><li> - User settings</li><li> - Project settings</li><li><code>.claude/settings.local.json</code> - Local project settings (not committed)</li><li>Enterprise managed policy settings</li></ul><p>Hooks are organized by matchers, where each matcher can have multiple hooks:</p><ul><li>: Pattern to match tool names (only applicable for  and\n)<ul><li>Simple strings match exactly:  matches only the Write tool</li><li>Supports regex:  or </li><li>If omitted or empty string, hooks run for all matching events</li></ul></li><li>: Array of commands to execute when the pattern matches<ul><li>: Currently only  is supported</li><li>: The bash command to execute</li></ul></li></ul><p>Runs after Claude creates tool parameters and before processing the tool call.</p><ul><li> - File pattern matching</li><li>,  - File editing</li><li>,  - Web operations</li></ul><p>Runs immediately after a tool completes successfully.</p><p>Recognizes the same matcher values as PreToolUse.</p><p>Runs when Claude Code sends notifications.</p><p>Runs when Claude Code has finished responding.</p><p>Hooks receive JSON data via stdin containing session information and\nevent-specific data:</p><p>The exact schema for  depends on the tool.</p><p>The exact schema for  and  depends on the tool.</p><p> is true when Claude Code is already continuing as a result of\na stop hook. Check this value or process the transcript to prevent Claude Code\nfrom running indefinitely.</p><p>There are two ways for hooks to return output back to Claude Code. The output\ncommunicates whether to block and any feedback that should be shown to Claude\nand the user.</p><p>Hooks communicate status through exit codes, stdout, and stderr:</p><ul><li>: Success.  is shown to the user in transcript mode\n(CTRL-R).</li><li>: Blocking error.  is fed back to Claude to process\nautomatically. See per-hook-event behavior below.</li><li>: Non-blocking error.  is shown to the user and\nexecution continues.</li></ul><table><tbody><tr><td>Blocks the tool call, shows error to Claude</td></tr><tr><td>Shows error to Claude (tool already ran)</td></tr><tr><td>N/A, shows stderr to user only</td></tr><tr><td>Blocks stoppage, shows error to Claude</td></tr></tbody></table><p>Hooks can return structured JSON in  for more sophisticated control:</p><p>All hook types can include these optional fields:</p><p>If  is false, Claude stops processing after the hooks run.</p><ul><li>For , this is different from , which only\nblocks a specific tool call and provides automatic feedback to Claude.</li><li>For , this is different from , which\nprovides automated feedback to Claude.</li><li>For , this takes precedence over any  output.</li><li>In all cases,  takes precedence over any\n output.</li></ul><p> accompanies  with a reason shown to the user, not shown\nto Claude.</p><p> hooks can control whether a tool call proceeds.</p><ul><li>“approve” bypasses the permission system.  is shown to the user but\nnot to Claude.</li><li>“block” prevents the tool call from executing.  is shown to Claude.</li><li> leads to the existing permission flow.  is ignored.</li></ul><h4></h4><p> hooks can control whether a tool call proceeds.</p><ul><li>“block” automatically prompts Claude with .</li><li> does nothing.  is ignored.</li></ul><p> hooks can control whether Claude must continue.</p><ul><li>“block” prevents Claude from stopping. You must populate  for Claude\nto know how to proceed.</li><li> allows Claude to stop.  is ignored.</li></ul><p> hooks can control tool execution:</p><p>Claude Code hooks work seamlessly with\n<a href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\">Model Context Protocol (MCP) tools</a>. When MCP servers\nprovide tools, they appear with a special naming pattern that you can match in\nyour hooks.</p><p>MCP tools follow the pattern , for example:</p><ul><li><code>mcp__memory__create_entities</code> - Memory server’s create entities tool</li><li><code>mcp__filesystem__read_file</code> - Filesystem server’s read file tool</li><li><code>mcp__github__search_repositories</code> - GitHub server’s search tool</li></ul><p>You can target specific MCP tools or entire MCP servers:</p><p>Automatically format code after file modifications:</p><p>Customize the notification that is sent when Claude Code requests permission or\nwhen the prompt input has become idle.</p><p>: Claude Code hooks execute arbitrary shell commands on\nyour system automatically. By using hooks, you acknowledge that:</p><ul><li>You are solely responsible for the commands you configure</li><li>Hooks can modify, delete, or access any files your user account can access</li><li>Malicious or poorly written hooks can cause data loss or system damage</li><li>Anthropic provides no warranty and assumes no liability for any damages\nresulting from hook usage</li><li>You should thoroughly test hooks in a safe environment before production use</li></ul><p>Always review and understand any hook commands before adding them to your\nconfiguration.</p><p>Here are some key practices for writing more secure hooks:</p><ol><li><strong>Validate and sanitize inputs</strong> - Never trust input data blindly</li><li><strong>Always quote shell variables</strong> - Use  not </li><li> - Check for  in file paths</li><li> - Specify full paths for scripts</li><li> - Avoid , , keys, etc.</li></ol><p>Direct edits to hooks in settings files don’t take effect immediately. Claude\nCode:</p><ol><li>Captures a snapshot of hooks at startup</li><li>Uses this snapshot throughout the session</li><li>Warns if hooks are modified externally</li><li>Requires review in  menu for changes to apply</li></ol><p>This prevents malicious hook modifications from affecting your current session.</p><ul><li>: 60-second execution limit</li><li>: All matching hooks run in parallel</li><li>: Runs in current directory with Claude Code’s environment</li><li>:<ul><li>PreToolUse/PostToolUse/Stop: Progress shown in transcript (Ctrl-R)</li><li>Notification: Logged to debug only ()</li></ul></li></ul><ol><li>Check if  menu displays your configuration</li><li>Review stdout and stderr format expectations</li><li>Ensure proper quote escaping</li></ol><p>Progress messages appear in transcript mode (Ctrl-R) showing:</p><ul></ul>","contentLength":6611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429225"},{"title":"Melbourne man discovers extensive model train network underneath house","url":"https://www.sbs.com.au/news/article/i-was-shocked-melbourne-mans-unbelievable-find-after-buying-house/m4sksfer8","date":1751327622,"author":"cfcfcf","guid":178665,"unread":true,"content":"<div><ul><li>After finalising the purchase of a home in Melbourne's northern suburbs, a Melbourne man found something unexpected.</li><li>There had been no mention of the expansive model train network beneath the home's floors.</li><li>Coincidentally, new owner Daniel Xu is a keen train enthusiast and engineer.</li></ul></div><div><a target=\"_self\" data-clickevent=\"{&quot;clickURL&quot;:&quot;/news/article/how-australias-biggest-cities-rank-for-public-transport-access/iy0wrwm4k&quot;,&quot;elementText&quot;:&quot;rail lines&quot;,&quot;clickType&quot;:&quot;clickSource&quot;}\" data-testid=\"internal-link\" href=\"https://www.sbs.com.au/news/article/how-australias-biggest-cities-rank-for-public-transport-access/iy0wrwm4k\"></a></div>","contentLength":281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429182"},{"title":"Show HN: A continuation of IRS Direct File that can be self-hosted","url":"https://github.com/openfiletax/openfile","date":1751321339,"author":"elijahwright_","guid":178791,"unread":true,"content":"<p>the IRS recently open sourced most of Direct File, a tax tool it has been working on for a few years now. unfortunately, due to recent events, the IRS isn't working on it anymore. I decided to pick up where they left off and I'm trying to get it ready for next tax season</p>","contentLength":271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44428438"},{"title":"The new skill in AI is not prompting, it's context engineering","url":"https://www.philschmid.de/context-engineering","date":1751316835,"author":"robotswantdata","guid":176933,"unread":true,"content":"<p>Context Engineering is new term gaining traction in the AI world. The conversation is shifting from \"prompt engineering\" to a broader, more powerful concept: . <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://x.com/tobi/status/1935533422589399127\">Tobi Lutke</a> describes it as \"the art of providing all the context for the task to be plausibly solvable by the LLM.” and he is right.</p><p>With the rise of Agents it becomes more important what information we load into the “limited working memory”. We are seeing that the main thing that determines whether an Agents succeeds or fails is the quality of the context you give it. Most agent failures are not model failures anyemore, they are context failures.</p><p>To understand context engineering, we must first expand our definition of \"context.\" It isn't just the single prompt you send to an LLM. Think of it as everything the model sees before it generates a response.</p><ul><li><strong>Instructions / System Prompt:</strong> An initial set of instructions that define the behavior of the model during a conversation, can/should include examples, rules ….</li><li> Immediate task or question from the user.</li><li><strong>State / History (short-term Memory):</strong> The current conversation, including user and model responses that have led to this moment.</li><li> Persistent knowledge base, gathered across many prior conversations, containing learned user preferences, summaries of past projects, or facts it has been told to remember for future use.</li><li><strong>Retrieved Information (RAG):</strong> External, up-to-date knowledge, relevant information from documents, databases, or APIs to answer specific questions.</li><li> Definitions of all the functions or built-in tools it can call (e.g., check_inventory, send_email).</li><li> Definitions on the format of the model's response, e.g. a JSON object.</li></ul><h2><a href=\"https://www.philschmid.de/context-engineering#why-it-matters-from-cheap-demo-to-magical-product\" aria-hidden=\"true\" tabindex=\"-1\"></a>Why It Matters: From Cheap Demo to Magical Product</h2><p>The secret to building truly effective AI agents has less to do with the complexity of the code you write, and everything to do with the quality of the context you provide.</p><p>Building Agents is less about the code you write or framework you use. The difference between a cheap demo and a “magical” agent is about the quality of the context you provide. Imagine an AI assistant is asked to schedule a meeting based on a simple email:</p><blockquote><p>Hey, just checking if you’re around for a quick sync tomorrow.</p></blockquote><p> has poor context. It sees only the user's request and nothing else. Its code might be perfectly functional—it calls an LLM and gets a response—but the output is unhelpful and robotic:</p><blockquote><p>Thank you for your message. Tomorrow works for me. May I ask what time you had in mind?</p></blockquote><p> is powered by rich context. The code's primary job isn't to figure out  to respond, but to  the LLM needs to full fill its goal. Before calling the LLM, you would extend the context to include</p><ul><li>Your calendar information (which shows you're fully booked).</li><li>Your past emails with this person (to determine the appropriate informal tone).</li><li>Your contact list (to identify them as a key partner).</li><li>Tools for send_invite or send_email.</li></ul><p>Then you can generate a response.</p><blockquote><p>Hey Jim! Tomorrow’s packed on my end, back-to-back all day. Thursday AM free if that works for you? Sent an invite, lmk if it works.</p></blockquote><p>The magic isn't in a smarter model or a more clever algorithm. It’s in about providing the right context for the right task. This is why context engineering will matter. Agent failures aren't only model failures; they are context failures.</p><h2><a href=\"https://www.philschmid.de/context-engineering#from-prompt-to-context-engineering\" aria-hidden=\"true\" tabindex=\"-1\"></a>From Prompt to Context Engineering</h2><p>What is context engineering? While \"prompt engineering\" focuses on crafting the perfect set of instructions in a single text string, context engineering is a far broader. Let's put it simply:</p><blockquote><p>Context Engineering is the discipline of designing and building dynamic systems that provides the right information and tools, in the right format, at the right time, to give a LLM everything it needs to accomplish a task.</p></blockquote><ul><li> Context isn't just a static prompt template. It’s the output of a  that runs  the main LLM call.</li><li> Created on the fly, tailored to the immediate task. For one request this could be the calendar data for another the emails or a web search.</li><li><strong>About the right information, tools at the right time:</strong> The core job is to ensure the model isn’t missing crucial details (\"Garbage In, Garbage Out\"). This means providing both knowledge (information) and capabilities (tools) only when required and helpful.</li><li><strong>where the format matters:</strong> How you present information matters. A concise summary is better than a raw data dump. A clear tool schema is better than a vague instruction.</li></ul><p>Building powerful and reliable AI Agents is becoming less about finding a magic prompt or model updates. It is about the engineering of context and providing the right information and tools, in the right format, at the right time. It’s a cross-functional challenge that involves understanding your business use case, defining your  outputs, and structuring all the necessary information so that an LLM can “accomplish the task.\"</p><p>This overview was created with the help of deep and manual research, drawing inspiration and information from several excellent resources, including:</p>","contentLength":5004,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44427757"},{"title":"The original LZEXE (A.K.A. Kosinski) compressor source code has been released","url":"https://clownacy.wordpress.com/2025/05/24/the-original-lzexe-a-k-a-kosinski-compressor-source-code-has-been-released/","date":1751311192,"author":"elvis70","guid":178664,"unread":true,"content":"<p><a href=\"https://forums.sonicretro.org/index.php?posts/1078212/\">Last year</a>, I discovered that the Kosinski compression format is actually LZEXE, which was used for compressing DOS executables back in the 90s and the late 80s. Its developer catalogues three versions on <a href=\"https://bellard.org/lzexe/\" target=\"_blank\" rel=\"noreferrer noopener\">his website</a>: v0.90, v0.91, and v0.91e. While only binaries of v0.91 and v0.91e can be found on the website, v0.90 can be found mirrored on various <a href=\"http://justsolve.archiveteam.org/wiki/LZEXE\" target=\"_blank\" rel=\"noreferrer noopener\">other websites</a>.<p>I got in touch with LZEXE’s developer, Fabrice Bellard, and he was able to release </p><a href=\"https://bellard.org/lzexe/\" target=\"_blank\" rel=\"noreferrer noopener\">LZEXE’s source code</a>, untouched since 1990! It is released under the terms of the MIT licence, allowing it to be freely used in other projects. To maximise performance, the compression logic was written in x86 assembly, while its frontend was written in Pascal. This particular source code appears to be for v0.91.<a href=\"https://forums.sonicretro.org/index.php?threads/accurate-kosinski-compressor.40558/\">my own Kosinski compressor</a> which produced identical data to what could be found in the Mega Drive Sonic games. At the time, I noticed that it did not accurately reproduce the Mega CD BIOS’s compressed Sub-CPU payload data. The inaccuracies were so extensive that it appeared that the BIOS’s data was compressed with a different tool to the Sonic games. Notably, the compressor which was used for the Sonic games suffered from a number of bugs and shortcomings, causing the compressed data to less efficient than it should have been. The Mega CD BIOS developers may have used a different version of the compressor, which lacked these bugs, or which had additional bugs.<p>With this in mind, the source code which has been released may not be for the exact compressor which was used by the Sonic games, though it could be modified to function identically to it. Since the compression logic was written in assembly, it should be simple enough to disassemble the compressor executables and compare them to the source code. Devon did the heavy-lifting of extracting and unpacking the core logic, which </p><a href=\"https://forums.sonicretro.org/index.php?posts/1079853/\">can be found here</a>.<p>With that, we now have the source code of two of the four ‘KENS’ format compressors – Kosinski </p><a href=\"https://forums.sonicretro.org/index.php?threads/i-found-the-saxman-compressor-source-code.39261/\">and Saxman</a>! Unfortunately, I do not have much hope of ever finding the original compressors for, let alone the source code of, the remaining two formats – <a href=\"https://segaretro.org/Enigma_compression\">Enigma</a> and <a href=\"https://segaretro.org/Nemesis_compression\">Nemesis</a> – due to them evidently being custom formats which were designed specifically for the Mega Drive, likely meaning that the compressors and their source code never left the hands of Sega (Enigma encodes plane map data, operating on 16-bit words and specifically acknowledging the separation of bits of the tile’s index from its X/Y flip, palette line, and priority; meanwhile Nemesis encodes tiles, operating on nibbles and bunching data into groups of 32 bytes (8 x 8 4-bit nibbles).</p>","contentLength":2645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426864"},{"title":"End of an Era","url":"https://www.erasmatazz.com/personal/self/end-of-an-era.html","date":1751311031,"author":"marcusestes","guid":177019,"unread":true,"content":"<p>I recall saying to one of my colleagues at Atari way back in 1982 that I wanted to make a game that would be genuine art. A year later I built a game that was my first experiment in that direction: Gossip. It was a ridiculously simple game in which a player attempted to win favor in a group by calling people and telling them how much you liked or disliked some third party. The underlying concept was that “people like people who like people they like.” For some reason, many players had problems absorbing this simple concept.&nbsp;</p><p>I later incorporated many of the ideas in Gossip into Excalibur, my first Arthurian game. But Atari collapsed soon after that, and I had to make a living as a freelance game designer. That kept me busy for the rest of the 1980s, but after I published my last game, Balance of the Planet, in 1990, I resolved to come back to the computer-games-as-art goal. I wanted to make another Arthurian game with much greater emphasis on the interpersonal interactions. I knew that my ambitions would take several years to realize, but I felt that the future lay in this direction, so I went ahead. I started building the game, and it went well. I began approaching publishers, and struck out everywhere. They greatly respected my reputation as one of the top game designers in the world, but they had decided that the only games that would sell had to be variations on Doom or Myst.&nbsp;</p><p>Then the Markle Foundation came to the rescue with an offer that was to bail me out in the short run and send me down the wrong path in the long run. They offered me $350,000 to build a tool for other people to create interactive storyworlds. The idea appealed to me, so I accepted the offer and went to work. Now, most of the money went to various contractors, even though I designed and wrote all the code. The result was the Erasmatron. I don’t have a screenshot of the Erasmatron user interface. My primary goal was to build software that would permit a storyteller to implement many of the dramatic processes that take place in a story. For example, there was a component that permitted one character to spy on two other characters conversing without being seen. There was an extensive system for managing how information traveled through a group of people and how secrets were kept or broken.</p><p>I won’t dwell on Erasmatron, as it evolved into Storytron, but I will note that Erasmatron had no takers. Other than Laura Mixon, nobody ever built anything with Erasmatron.</p><p>This was the culmination of my effort to build a software development environment for interactive storytelling. It is best understood as Erasmatron with a much superior user interface and considerably better support features.</p><p>The central component of Storytron was Deikto, a technology for creating a toy language specific to an interactive storyworld. “Toy language” is my term for a tiny language containing only the words necessary to permit interaction between characters in the storyworld. Toy languages do not need anywhere near as large a vocabulary as a full language. A storyworld for children will have no verbs for sex, higher education, jobs, finance, marriage, alcohol, and many other things. Similarly, a storyworld about corporate politics can dispense with scientific verbs, most economic and financial verbs, verbs for family interactions, and not much about food. A good storyworld designer can, in my estimate, build an adequate toy language for most storyworlds with only a few hundred verbs.&nbsp;</p><p>The verbs are the core of the system; the player can build sentences out of the verbs and all the other words in the system. There are words for actors, props, and stages.</p><p>This description is growing too large. Storytron technology had many other wonderful features that I won’t describe here. The important point I want to make is that nobody was interested in Storytron. I spread the word about it as well as I could, but I’m no salesman. I spent about ten years on Storytron and a great deal of my money hiring contractors to do some of the work that I couldn’t do. And it was all for nothing. Storytron was just too complicated for the audience. I don’t think that was because it was intrinsically too complicated for anybody to understand. My impression is that there just weren’t any people willing to make the big commitment required to learn how to use Storytron. It was easier to learn than professional programming systems like Eclipse or the Microsoft suite of software development applications. But it demanded more of its users than they were willing to invest.</p><p>I made one last effort to make Storytron work using Siboot, a concept that I had developed in 1987. I poured my energy into Siboot, and a number of good people helped me, but after I had expended several years on it, I realized that it was crap. The story felt too mechanical. I realized that it needed a boost in the form of the encounter technology that I had developed for the 1987 version, but at that point I was so discouraged that I just couldn’t go on. I gave up on Siboot.</p><p>I gave up on Storytron around 2018. It was painful to accept that all the energy, all the creativity, all the sweat I had committed to the project was for naught, but I had no choice. I rested for some months, then in 2020, for my 70th birthday, I realized that I was growing old and would not be able to handle a tough technical challenge for much longer. I therefore decided that the time had come for me to make one last effort, and that effort had to an Arthurian game. I re-read the many Arthurian books I had accumulated during previous efforts, girded my loins, and set to work. I made many changes along the way; the final version of Le Morte d’Arthur was quite unlike the original. But it worked. I knew that, after all these years, I had finally achieved my goal of making genuine interactive art. I was proud, tired, and gratified. Not many people played the storyworld, but I didn’t care. That was the world’s failure, not mine.&nbsp;</p><p>I continued to fiddle around with interactive storytelling, discussing issues with a small group of people devoted to the problem. I even made a few attempts to make the technology I used for Le Morte d’Arthur available to others, but, once again, nobody was interested.</p><p>Late in 2024 I happened upon a mention of Narrascope, an annual conference for interactive fiction held once every June. I knew of the conference from previous references, and it occurred to me that I had one last shot at making interactive storytelling technology available to the world. The attendees of Narrascope were not the techie types I had previously dealt with. These were mostly storytellers, weaker on the technology but stronger on the storytelling side. I decided to make my technology available to them, but to do so I would have to strip away all the technical complexity. I set to work building a web page that could edit storyworlds, using HTML, CSS, Javascript, and JSON. My programming powers were fading fast. Time and time again I would send my friend Dave Walker an email declaring that Javascript (or something else) was utterly broken, incapable of executing the simplest program without errors. Dave would ask to see the source code and I would present it to him with detailed notes proving that my code was perfect and Javascript was broken. He’d call me, we’d discuss it, and eventually he’d say something like, “Where did you terminate the loop beginning at line 563?” There would be a long silence, followed by the tiniest  from me. I’d thank him for his help and hang up. A week later, I’d be fuming again about another fundamental flaw in Javascript.&nbsp;</p><p>Narrascope had accepted my lecture proposal, as well as my request to deliver a workshop on my technology. I spent dozens of hours working on the lecture; my lectures have always been top-notch and I wasn’t about to scrimp on this one. I made scores of nifty-keen images to illustrate my points. When will people learn that text doesn’t belong on a slide???</p><p>Meanwhile, I struggled with the program. I didn’t quite get it finished, but it was workable and users could readily see that it was close to completion.&nbsp;</p><p>On the big day I arrived at the airport at 5:00 AM to catch the early flight. We sat on the tarmac for an hour because of a mechanical problem, at which point I realized that I could not possibly make a crucial connection. I had to abort the trip to Narrascope and deliver the lectures via video, which turned out to be disastrous.&nbsp;</p><p>This was my last-gasp effort to stimulate progress in interactive storytelling. \"Once more, into the breach!” I had told myself. Now, more than a week after I delivered my spiel, not one person has answered my call for emails expressing some interest in my technology. Once again, my efforts were in vain.</p><p>And so it is time for me to admit that, after all those decades of work, I have failed, with the single exception of Le Morte d’Arthur. When I designed for myself, I succeeded. When I designed for others, I failed. It’s time to throw in the towel and leave interactive storytelling to others. I don’t think that the world is ready. I feel like Charles Babbage, who invented the programmable computer in 1850. It used gears, levers, and cams and was brilliant. But the world had no need for programmable computers in 1850, so he never got the funding to build his invention. I’m nowhere near as smart as Charles Babbage, but my life dimly echoed his.&nbsp;</p><p>I realized that my opus magnus, Le Morte d’Arthur, is a metaphorical autobiography of sorts. At the least, it expresses my experience working on interactive storytelling. Here is Merlin’s final conversation with Arthur:</p><p>The time has come to close this chapter of my life. Perhaps I shall write a book summarizing my findings. Perhaps I shall not.</p>","contentLength":9850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426845"},{"title":"Xfinity using WiFi signals in your house to detect motion","url":"https://www.xfinity.com/support/articles/wifi-motion","date":1751310230,"author":"bearsyankees","guid":176932,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426726"},{"title":"The hidden JTAG in a Qualcomm/Snapdragon device’s USB port","url":"https://www.linaro.org/blog/hidden-jtag-qualcomm-snapdragon-usb/","date":1751308467,"author":"denysvitali","guid":177018,"unread":true,"content":"<p>EUD stands for Embedded USB Debug: essentially, this is a debug interface built right into almost every Qualcomm SoC since ~2018. Internally it hooks deep into the SoC, providing debug facilities for not just the CPUs but also the myriad of Hexagon co-processor/DSPs; many of the exciting details can be found in <a href=\"https://patents.google.com/patent/US20160124822A1/en\">this patent from way back in 2014</a>.</p><p>In practise, for a non-production device (like a dev board, though some production devices seem to work too), EUD can be enabled by writing a few registers and then starting up the USB phy (though the details vary by generation). Instead of whatever typical gadget you might expect, what appears on your PC is a 7-port USB hub, with 1 port populated by the “EUD control interface”.</p><p>With the right USB commands, a second device will appear, this one exposes an SWD interface! Yes! SWD right over the USB cable, no external tools, no soldering, and no expensive debuggers. Closed case debug that (almost) puts Google’s Suzy-Q to shame!</p><p>For those unfamiliar: JTAG and SWD are both mechanisms for debugging the CPU cores inside a device, just like you can use GDB to debug programs on your computer (or your IDEs integrated debugger). They let you set breakpoints, halt execution, inspect the registers, single step instructions and all sorts of other useful things.</p><p>For quite a while there has been a tantalising <a href=\"https://git.codelinaro.org/clo/la/openocd-org/openocd/-/commits/qcom_changes/?ref_type=heads\">fork of openOCD published by Qualcomm on CodeLinaro</a>, promising EUD integration. However, it relied on an at-the-time proprietary EUD library, which was only available to Qualcomm employees and their OEM partners.</p><p>The device-side part of this (enabling the EUD interface so it shows up on your PC) has been somewhat supported in upstream Linux for a while. Back in August last year there was an attempt to extend this support for some newer platforms which have additional requirements. This <a href=\"https://lore.kernel.org/all/622c0fd6-e4e2-6597-d0a2-ff449d7d2f59@quicinc.com/\">sparked some discussion</a> over the kernel policy: is it acceptable to have drivers in Linux that are only usable by some internal software, gatekept for Qualcomm and their paying partners? The answer appeared to be no, and this seemed to be enough to push Qualcomm in the right direction as after 8 months of silence, here we are!</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>Let’s be fair, it almost definitely builds just fine on Ubuntu 20.10 with Qualcomm’s GCC 8.x toolchain. But that’s not what most people are using, we have to fix this!</p><p>It turns out to be not too bad, just some minor stuff. Somehow they have  and  enabled though, and there is no way we’re gonna get that all passing just yet.</p><p>With everything building, the necessary fixes (and a shiny new ) have been submitted to Qualcomm’s repo <a href=\"https://github.com/quic/eud/pull/2\">here</a>.</p><p>Now we have EUD building, we can try it with OpenOCD. It looks like they based their changes on the latest OpenOCD release 0.12.0, very nice. But wait, this release came out in 2023, and OpenOCD is still in active development… So there’s 2 years worth of changes, and</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>Almost 11k commits! It would really be nice to get this upstream eventually, so maybe let’s just rebase it real quick, we need to point it at the cleaned up EUD fork anyways.</p><p>Among Qualcomm’s changes to support EUD, there are also patches adding Hexagon debugging support (and seemingly some improvements for LLDB as well). These got lost along the way but are almost certainly worth looking into at some point.</p><p>So here we are, a fun day of fixing up and rebasing some codebases, and a very tasty reward!</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>You can find the rebased OpenOCD patches <a href=\"https://github.com/linux-msm/openocd\">over on the linux-msm GitHub</a> along with some quickstart instructions in the README. So far this has been tested on the Snapdragon 845, it should work similarly for the 855 and 865 where we can get away with just poking the enable register and then using Linux or U-Boot to start a USB gadget. Newer SoCs however will probably require additional changes <a href=\"https://lore.kernel.org/all/20240730222439.3469-1-quic_eserrao@quicinc.com/\">like these for SM8450</a>. Let’s hope these old patch series get refreshed now that the tooling side of the story is in better shape!</p><p>Torvalds himself famously doesn’t support the use of debuggers with the kernel (though that certainly hasn’t stopped the wonderful work on kgdb), he <a href=\"https://lkml.org/lkml/2000/9/6/65\">wrote</a> (all the way back in 2000):</p><blockquote><p><em>I don’t like debuggers. Never have, probably never will. I do not condone single-stepping through code to find the bug.</em></p></blockquote><p>So of course, how practically useful JTAG support is really depends on your workflow. In the Qualcomm Landing Team at Linaro, debuggers have never been a staple of our work for all the typical reasons you’d expect (cost and complexity being the main ones), however with more focus being spent on non-kernel things like U-Boot and the secure world this dynamic is shifting.</p><p>U-Boot is an obvious example for us, since it doesn’t currently provide stack traces when it crashes, diagnosis can sometimes be an arduous process which is made infinitely simpler with a .</p><p>We are particularly interested in the possibilities that EUD opens up for debugging a vertically integrated BSP, especially when TF-A, OP-TEE and U-Boot are in the mix via the <a href=\"https://gitlab.com/Linaro/trustedsubstrate/meta-ts\">Trusted Substrate layer for OpenEmbedded</a>. If this is something you’d like to explore with us then don’t hesitate to <a href=\"https://www.linaro.org/contact\">get in touch</a>.</p><p>In addition to the SWD peripheral, there is also a COM (UART) peripheral, and a trace peripheral. These haven’t yet been explored (and aren’t integrated into OpenOCD) but they should allow for a bidirectional serial port and MMIO tracing respectively. These do open up some more interesting use cases around Closed Cased Debugging in production - this appears to have been intentional on Qualcomm’s behalf with EUD being disabled as part of the production signing process, but with the ability to be re-enabled with a (cryptographically validated) “debug policy”.</p><p>Some different SoCs use different addresses for the debug base and CTI base registers, as well as the additional changes required to enable EUD. If you’re able to make this work on your board/SoC, please do <a href=\"https://github.com/linux-msm/openocd/issues/new\">open an issue on the linux-msm fork</a> and let us know what worked for you.</p><p>Additionally, there is a strange quirk where the sticky reset bit of the PRSR register is always set, perhaps relating to SMP. For now the sticky reset behaviour of OpenOCD is <a href=\"https://github.com/linux-msm/openocd/commit/8d154f2270358e3aa35f338c8b6211a5638f22a6\">stubbed out</a> but it would be good to figure out what’s going on.</p><p>SMP support in general is also currently lacking. The config file has <a href=\"https://github.com/linux-msm/openocd/commit/d1a4f90cb70009191da177d80f98332240c68d32\">been updated</a> (using rcar as a reference) to define multiple CPU cores, but this doesn’t seem to behave correctly in Linux. For now it’s recommended to boot with  if you want to actually debug your kernel.</p><p>Whether or not EUD is available on your device seems to depend on a variety of options: there are fuses to configure what debug functionality is allowed, as well as support for an OEM signed “debug policy” which can override this behaviour. On at least one production device (the OnePlus 6) EUD appears to be disabled via fuse, and yet it just works anyway. This device also has “crashdump mode” enabled which is not typical, this suggests that maybe OnePlus shipped the device with a loose debug policy, perhaps by mistake.</p><p>Lastly, while it is of course extremely useful to have proper JTAG for debugging the kernel (especially when it’s so effortless!). The obvious question is: can this be used to gain control of higher execution levels? And unfortunately the answer appears to be no. If you do manage to halt execution in EL2, all registers will read as 0, and not much seems to be possible, at least on a production device. If your board behaves differently do let us know!</p><p>EUD gives us a huge new surface to explore, and offers the potential to greatly improve the experience of low level debugging on Qualcomm boards. We are extremely excited that this is now published and freely available to use, and we very much hope it will become a seamless experience as the tooling and drivers are better integrated.</p><p>It is awesome to see Qualcomm’s commitment to improving the developer experience and making their platforms more open is continuing to be demonstrated in their actions, EUD has the potential to save huge amounts of money on expensive debugging equipment, drastically reduce set-up times and make remote debugging easier too (no doubt it will eventually be integrated into our existing <a href=\"https://github.com/linux-msm/cdba\">remote debugging</a> tooling). Quite simply this raises the foundations for anyone working on Qualcomm platforms, and we can’t wait to see what’s next.</p>","contentLength":8338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426428"},{"title":"Datadog's $65M/year customer mystery solved","url":"https://blog.pragmaticengineer.com/datadog-65m-year-customer-mystery/","date":1751308269,"author":"thunderbong","guid":176931,"unread":true,"content":"<p><em>The internet has been speculating the past few days on which crypto company spent $65M on Datadog in 2022. I confirmed it was Coinbase, and here are the details of what happened. Originally published on 11 May 2023.</em></p><p><a href=\"https://twitter.com/gergelyorosz?ref=blog.pragmaticengineer.com\"></a><em> with a bonus, free issue of the Pragmatic Engineer Newsletter. We cover one out of six topics in today’s subscriber-only <a href=\"https://newsletter.pragmaticengineer.com/p/the-scoop-47?ref=blog.pragmaticengineer.com\">The Scoop issue</a>. To get full newsletters twice a week, </em><a href=\"https://newsletter.pragmaticengineer.com/about?ref=blog.pragmaticengineer.com\"></a></p><p>Datadog is a leading observability tooling provider which went public in 2019, with a current market cap of $28B. The company made $1.67B revenue in 2022, circa $140M per month. On an earnings call a week ago, on 4 May, the CFO mentioned a “large upfront bill that did not recur,” saying:</p><blockquote>“Billings were $511 million, up 15% year-over-year. We had a large upfront bill for a client in Q1 2022 that did not recur at the same level or timing in Q1 2023. Pro forma for this client, billings growth was in the low 30s percent year-over-year.”</blockquote><p>If you’re like me, you’d probably skim over this detail, as it’s 15% here, 30% there. However, analysts attend these calls whose bread and butter is crunching the numbers and figuring out what a company might be trying to hide. A JP Morgan stock analyst did just this, quickly crunching numbers and asking the question:</p><blockquote>“David, looking at the math on this large upfront bill that did not recur, it seems to be about $65 million, if I'm running that correctly. Can you possibly shed a little more light?“</blockquote><p>Datadog’s CFO, David Obstler gave more details:</p><blockquote>“That was a crypto company which continues to be a customer of ours. But that was an early optimizer. We had always talked about some of the industries that were most affected and optimized.“</blockquote><p>So, who is this mysterious crypto company? Investor Turner Novak <a href=\"https://twitter.com/TurnerNovak/status/1654577231937544192?s=20&amp;ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">speculated</a> that it’s Coinbase:</p><p>He added he doesn’t know for certain that it is Coinbase, as other crypto companies have also raised silly amounts of money in the past several years.</p><p>So, did Coinbase spend $65M on Datadog in 2022? Online there’s no shortage of theories, or people pretending to be Coinbase employees, such as this anonymous commenter on Hacker News, <a href=\"https://news.ycombinator.com/item?id=35866061&amp;ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">claiming</a> that the $65M was for a 3-year upfront payment (which information I could not verify). I wanted to find the truth, so I tracked down software engineers at the company. And I got my answer:</p><p><strong>Yes. Coinbase spent $65M with Datadog in 2021, and this was their due bill for that year. </strong>I can confirm this, having talked with both current and former software engineers at Coinbase who shared details of what happened.</p><p>Here’s how Datadog’s CEO explained, on the earnings call on what happened:</p><blockquote>“This is one of those situations where this customer was in an industry that got pretty much decimated over the past year. And their own business was cut in three or four, in terms of the revenue. And when that's the case, we really work with customers to restructure their contracts with us. We want to be part of the solution for them, not part of the problem (...) We restructure their contract, so we keep them as a happy customer for many more years and do a deal that works for everyone with their business profile.”</blockquote><p>And here’s what actually happened, as I understand from talking with engineers at Coinbase.</p><p>Coinbase had an incredible 2021 and did not have to care about costs. The company went public in June that year, and was valued at an eye-popping $86B. In comparison, nearly two years later the company is valued around $14B, a 75% decline.</p><p>During the boom, trading volumes were surging, beating record after record, and Coinbase could barely keep up. Here’s how Coinbase CEO Brian Amstrong summarized it:</p><blockquote>“So, obviously 2021 was just an incredible year for Coinbase, the kind of thing that you see very rarely in your lifetime, in a business career (...) We hit an all time high in our monthly transacting users of 11.4 million, which is 4x year-over-year, 400% pretty incredible.”</blockquote><p>Following the IPO in summer 2021, nobody at the company cared about infra costs; the only focus was growth. The company racked up huge bills for the likes of AWS, Snowflake, and also Datadog. And so, the $65M bill was for Datadog, for 2021. Coinbase settled the bill in Q1 2022.</p><p><strong>In early 2022 Coinbase suddenly needed to cut back infra spending. </strong>The crypto industry hit a sudden chill, affecting Coinbase’s business. As revenue dried up, the company turned its attention to reducing its overly high costs.</p><p>For observability, Coinbase spun up a dedicated team with the goal of moving off of Datadog, and onto a Grafana/Prometheus/Clickhouse stack. A quick summary of these technologies:</p><p><a href=\"https://prometheus.io/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\"></a><strong>: a time series database. </strong>A very popular open-source solution for systems and services monitoring. Prometheus collects metrics from configured targets (services) at given intervals. It evaluates rules and can trigger alerts. It’s mostly written in Go, with some Java, Python and Ruby parts. Prometheus stores time series in-memory and on storage (HDD or SSD), using an efficient and custom format, and supports <a href=\"https://en.wikipedia.org/wiki/Shard_(database_architecture)?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">sharding</a> and <a href=\"https://en.wikipedia.org/wiki/Federated_database_system?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">federation</a>.</p><p>Prometheus <a href=\"https://www.cncf.io/projects/prometheus/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">is part</a> of the Cloud Native Foundation, membership of which indicates that it’s safe to build on top of Prometheus, as it’s actively maintained and will continue to be.</p><p>Prometheus can be self-hosted, but several cloud providers also offer managed Prometheus services: both Google Cloud and AWS have this service in production, while Azure has it in preview.</p><p><a href=\"https://grafana.com/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\"></a><strong>: the frontend for visualizing metrics. </strong>Grafana is a popular source analytics and monitoring visualization solution. If you need to display or dive into metrics or alerts, it’s the go-to tool, and widely used across tech companies. When I was at Uber, Grafana powered many of our graphs. Here’s an example of Grafana dashboards you can <a href=\"https://play.grafana.org/d/000000012/grafana-play-home?orgId=1&amp;ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">try out</a>:</p><p><a href=\"https://clickhouse.com/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\"></a>: log management. A fast and open-source column-oriented database management system, which is a popular choice for log management. Clickhouse is written predominantly in C++, and is widely used across the industry. For example, Cloudflare uses Clickhouse to store all its DNS and HTTP logs – which is more than 10M rows per second! – and Uber uses Clickhouse as its central logging platform.</p><p>Coinbase spun up its in-house approach without the main goal of saving costs, but to have full control and ownership of observability. Observability and reliability is a major differentiator for Coinbase, as it gives a competitive advantage over rivals.</p><p>However, with the crypto market cooling, costs became a major focus, and it was clear the in-house Grafana/Prometheus solution was much cheaper. The Coinbase team had been double-writing the new stack for months, confirming everything worked well, and ironing out any issues.</p><p>So Coinbase was ready to pull the plug on Datadog, but Datadog saved its customer relationship at the last minute by making Coinbase a very appealing deal it could not refuse. In future, the bill for Datadog would be nowhere near the $65M of 2021. As Brian Amstrong said of the crypto market during 2021, a $65M bill is the kind of thing you see very rarely in a business career.</p><p>I asked an engineer at Coinbase who used the in-house stack and Datadog how they felt about the decision to stay on Datadog. They said it was ultimately the right decision, considering the reasonable costs, and the superior Datadog development experience.</p><p>Coinbase could  have engineered a similar experience in-house. However, to provide a similarly seamless developer experience, would have likely taken tens of engineering years.</p><p><strong>“Expensive” in observability tooling is relative. </strong>Let’s assume that today Coinbase “only” spends, say, $10M per year on Datadog. Is this too much? Looking at the headline number, it’s tempting to think so.</p><p>However, let’s look a level deeper. A platform like Datadog helps prevent outages, detects them instantly, and mitigates them faster. In 2022, Coinbase had 17 outages, totalling about 12 hours of downtime. The company’s daily average revenue is around $9M/day, based on their 2022 earnings.</p><p>Assume that Datadog cuts the number of outages by half, by preventing them with early monitoring. That would mean that without Datadog, we’d look at 24 hours’ worth of downtime, not 12. Let’s also assume that using Datadog results in mitigating outages 50% faster than without - thanks to being able to connect health metrics with logs, debug faster, pinpoint the root cause and mitigate faster. In that case, without Datadog, we could be looking at 36 hours worth of total downtime, versus the 12 hours with Datadog. To put it in numbers: the company would make around $9M in revenue it would otherwise lose, Now that $10M/year fee practically pays for itself!</p><p><strong>What can we learn from Coinbase’s cost reduction exercise? </strong>Vendors are tight-lipped about their customers reducing spend, and it is a lucky coincidence that Datadog gave enough hints to find out who their big “early optimizer” customer was, and find out more details. But is the story of Coinbase a one-off?</p><p>I’m not sure that it is. Three months ago, I covered the trend that <a href=\"https://newsletter.pragmaticengineer.com/p/vendor-spend-cuts?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">Tech companies are aggressively cutting back on vendor spend</a> - and two months later, The Wall Street Journal also <a href=\"https://www.wsj.com/articles/corporate-technology-under-new-scrutiny-amid-recession-fears-a69c6583?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">reported</a> on the same topic. Coinbase, to me, seems to have been early to the cost optimizing trend. However, look closely at the <a href=\"https://newsletter.pragmaticengineer.com/p/vendor-spend-cuts?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">responses</a> I gathered, and “AWS” and “Datadog” are the two most mentioned vendors as targets for cost savings. This is simply because infra and observability costs tend to be the highest and AWS is the leader for cloud infra, and Datadog the leader for observability.</p><p>Datadog CEO Olivier Pomel confirmed that this type of optimization is happening across all of their customers, saying:</p><blockquote>“When we look at our data, when we look at what we hear from the hyperscalers also, we also listen carefully to their commentary on what they foresee in the near future, we don't see anything that gives us confidence that we can call an end to optimization in the next quarter or the quarter after that. So as far as our guidance goes and our plan for the year, we assume that this is going to continue at a similar level for the rest of the year.”</blockquote><p>I have since confirmed several large companies with thousands of engineers building their own Grafana/Prometheus stack, planning to migrate off of their current observability vendor and operate the observability stack themselves. But why is this?</p><p><strong>Above $2-5M/year annual spend is where bringing a vendor in-house tends to come up. </strong>And this is because it is around this number where the cost of hiring a whole team to do what a vendor is doing can  make sense.</p><p>As a rule of thumb, you can get infra costs much lower than what vendors charge. This is because both the vendor, and you are probably using the same Cloud infrastructure provider, which is usually AWS, GCP or Azure. However, you would need to hire and staff a dedicated engineering team to build and run that infra.</p><p>So, from a cost perspective, this is the math problem you need to solve. At what point does is this equation become true:</p><p>$infra_cost + $platform_team_cost &lt; $current_vendor_costs</p><p>In this question, $platform_team_costs will be above $1M, and sometimes above $2M. This is because you need to have a team of 4-5 engineers, plus a manager, and their average total compensation will be somewhere between $150-400K/year, depending on your cost basis.</p><p>So when you have a bill that is above $2-3M/year, it can start to look tempting to build, rather than buy. The economics of this decision start to get down to how high of a margin is the vendor charging on top of raw infra? The curious question with Coinbase is: did they consider building, when talking about such a huge projected cost that could justify having a team?</p><p>In the case of Coinbase, building in-house following a $65M bill was a clear no-brainer. They could hire a team of 10 senior and staff-level engineers in the Bay Area, and still have this team cost less than $5M/year. And they then only need to budget for the infra costs, which they can presumably bring down to low double digits per year.</p><p>Coinbase planned to move off Datadog, but ended up staying. However, it is not the only larger tech company thinking about bringing observability in house. I have another exclusive which even Datadog might not be aware of, yet. This report is about Shopify and its plan to move off Datadog. But could recent layoffs change things? I cover details on this topic in <a href=\"https://newsletter.pragmaticengineer.com/p/the-scoop-47?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">the full The Scoop</a>.</p><p><em>This was one out of the five topics covered in this week’s The Scoop. A lot of what I share in The Scoop is exclusive to this publication, meaning it’s not been covered in any other media outlet before and you’re the first to read about it.</em></p><p><em>The full The Scoop edition additionally covers:</em></p><ol><li><strong>Will Shopify migrate onto an in-house observability tool? </strong>Shopify decided to build its own observability platform and migrate off Datadog. This plan looked certain until Shopify cut the very engineering teams that built its new platform. What happens next?.</li><li><strong>Microsoft cuts its compensation targets</strong>. Almost exactly a year ago, Microsoft employees received a welcome surprise: they could expect higher-than-usual compensation increases. Yesterday, another unexpected email came, but its contents were the opposite of last year’s. I talked with managers and engineers at the tech giant for their reaction to disappointing compensation news. .</li><li><strong>Shopify letting go most staff in Germany. </strong>As part of cutting 20% of staff, most people in Germany were made redundant. These layoffs happened a week before a Works Council election in Germany. Is this unlucky timing, or is there more behind the move?.</li><li><strong>Senior compensation trending down in Ukraine. </strong>Ukraine is one of the few countries for which we have access to nationwide data, through job site Djinni. Data for the first part of this year are in, and they point to something not seen recently: senior engineers are making less. Is this a local trend, or could we see it happening in other countries?.</li><li><strong>A follow-up to this week’s public tech company compensation article</strong>. Why was Netflix lower down the list than many software engineers expected? Plus, new details about Roblox and why Jack Dorsey’s total compensation is $2.75. </li></ol>","contentLength":14225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426399"},{"title":"Ask HN: What's the 2025 stack for a self-hosted photo library with local AI?","url":"https://news.ycombinator.com/item?id=44426233","date":1751307050,"author":"jamesxv7","guid":176905,"unread":true,"content":"First of all, this is purely a personal learning project for me, aiming to combine three of my passions: photography, software engineering, and my family memories. I have a large collection of family photos and want to build an interactive experience to explore them, ala Google or Apple Photo features.<p>My goal is to create a system with smart search capabilities, and one of the most important requirements is that it must run entirely on my local hardware. Privacy is key, but the main driver is the challenge and joy of building it myself (an obviously learn).</p><p>The key features I'm aiming for are:</p><p>Automatic identification and tagging of family members (local face recognition).</p><p>Generation of descriptive captions for each photo.</p><p>Natural language search (e.g., \"Show me photos of us at the beach in Luquillo from last summer\").</p><p>I've already prompted AI tools for a high-level project plan, and they provided a solid blueprint (eg, Ollama with LLaVA, a vector DB like ChromaDB, you know it). Now, I'm highly interested in the real-world human experience. I'm looking for advice, learning stories, and the little details that only come from building something similar.</p><p>What tools, models, and best practices would you recommend for a project like this in 2025? Specifically, I'm curious about combining structured metadata (EXIF), face recognition data, and semantic vector search into a single, cohesive application.</p><p>Any and all advice would be deeply appreciated. Thanks!</p>","contentLength":1467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426233"},{"title":"Sony DTC-700 audio DAT player/recorder","url":"https://kevinboone.me/dtc-700.html","date":1751306606,"author":"naves","guid":178663,"unread":true,"content":"<p>Don’t let anyone tell you otherwise: DAT players were fantastic. They\noffered all the advantages of an audio cassette, but with the sound\nquality of a CD. The compact audio cassette was a marvellous invention,\nin its own way; but this technology struggled to provide audio fidelity\nthat would satisfy discerning listeners. Its frequency response was\nlimited, and the unavoidable background hiss was very obvious in quiet\nenvironments. Still, in the 1970s audio cassettes were  way\nmost people listened to music, and I still have a stack of them.</p><p>One thing that made cassettes so popular was that you could record on\nthem. Setting aside the legal issues, you could record from FM radio, or\nfrom vinyl records, or even from microphones. It was easy to make ‘mix\ntapes’ of you favourite tracks, and share them with friends. Cassettes\nwere everywhere – from portable players like the Walkman, to serious\nhardware in hi-fi racks; they were even in cars.\nThere were shops that sold nothing but cassettes, and they sold by the\nmillion.</p><p>Serious hi-fi enthusiasts, however, listened to vinyl records or FM\nradio. There  good-quality cassette decks, but the\n`audiophile’ crowd embraced them with reluctance, if at all. Still, even\nthe most ardent hi-fi junkie couldn’t deny the usefulness of cassettes.\nWhat we needed was something that could record high-quality sources,\nwith no loss of fidelity.</p><p>That’s where DAT, ‘digital audio tape’ comes in. DAT offered digital\nrecording, in a range of qualities, the highest of which exceeded that\nof CD. If you wanted to record from a CD, you could just connect the CD\ntransport’s digital output to the DAT’s digital input, and away you go.\nWell, maybe – more this subject later. Of course, most DAT units could\nrecord from analog sources like radio as well.</p><p>DAT entered the market at about the same time as CD, but was much\nless successful. For all its notional advantages, DAT never really\ncaught on in the domestic market, although it was somewhat more popular\nin professional applications. A companion data storage technology, DDS,\nused the same hardware, and was somewhat more successful although,\nagain, in professional rather than domestic applications.\nSony pulled out of the market in 2005, although I think it was clear\nlong before then that the format was moribund.</p><p>The DTC-700, introduced in 1990, was Sony’s ‘budget’ hi-fi DAT\nplayer/recorder. The more expensive DTC-55ES and DTC-60ES models had\nfancy (and probably snake oil) features like a copper chassis. Yes,\ncopper is a better electrical conductor than steel, but a great chunk of\nsteel like the DTC-700 chassis is a pretty good conductor already. I’ve\nnot been able to find how much a new DTC-700 cost but, even as the\nintroductory model in the range, I imagine it was well into\nsell-a-kidney territory. In 1995, even a five-year-old, second-hand unit\nwas eye-wateringly expensive. These days, you can pick up a refurbished\nunit for about three hundred quid. It’s well worth the money – if you\ncan find tapes. There are lots more digital DDS tapes in circulation\nthat audio tapes; these are not guaranteed to be compatible with audio\nplayers, but early DDS tapes often are.</p><p>The DTC-700 had a flight-deck of controls, because it offered a stack\nof functionality. It had two different digital inputs and an analog\ninput; there was a headphone amplifier with its own volume control; you\ncould skip to specific tracks by their number, or to a particular time;\nand, of course, you could insert the meta-data that made this possible\nwhen you recorded. And, like all serious hi-fi equipment, it had a\nvaccuum-flourescent display, available in different colours. For that\nreal 70s look, you could buy it with mock-walnut case sides.</p><p>Compared to cassettes, DAT recordings sounded fantastic. It wasn’t\nnecessary for the rest of your equipment – amplifier, speakers,\nheadphones – to be of top quality to realize this: the difference\nbetween DAT and cassette was just that striking. In principle, DAT\noffered better-than-CD quality, with its 48kHz sampling rate. In fact,\nDAT set the standard here: 48kHz remains a common sampling rate to this\nday. Folklore has it that Sony was encouraged to adopt 48kHz to make it\nharder to record commercial CDs, which used (and still use) 44.1kHz.\nBack in the 90s, technology hardly existed to resample these different\nformats on-the-fly; eventually, Sony and others started selling DAT\nunits that supported 44.1kHz directly. This wasn’t an entirely welcome\nmove, as I’ll explain later.</p><p>High cost was one of the reasons – perhaps the main reason – why DAT\ndidn’t catch on in the domestic market; but it certainly wasn’t the only\none. Another problem was the lack of original material: recording\nstudios didn’t seem to want to release commercial recordings on DAT.\nTheir reluctance isn’t hard to understand: DAT tapes could be copied an\nunlimited number of times, with no loss of quality. In the the late 80s\nit wasn’t easy to copy a CD onto DAT, because of the different sampling\nrates. But there would have been no such limitation with a DAT-to-DAT\ncopy.</p><p>Representatives of the recording industry were so worried about\nillegal copying that, in the USA and elsewhere, they bullied legislators\ninto placing legal restrictions on the capabilities and sale of DAT\nrecorders. The USA also introducted taxation on the sales of DAT\ndevices, which was supposed to offset the loss in tax revenue that\nillegal copying would create. This made expensive DAT players even more\nexpensive. Sony tamed the objections of the recording industry, to some\nextent, by the simple expedient of buying CBS Records, one of the main\nobjectors. Nevertheless, the DTC-700 still suffers from the anti-copying\nparanoia of the 80s; it will record a CD, but it will write meta-data\nonto the recording to indicate that it’s a copy. The DTC-700, and other\nDAT units of the same vintage, won’t record from another DAT unit, if\nthe meta-data indicates that the source is a copy. There are ways around\nthis limitation, but they’re fiddly.</p><p>Whether illegal copying was a genuine risk or not, there never really\nwas a large selection of original music on DAT. As I recall, there\nwasn’t even a “killer album” for DAT, like Dire Straits’  – an album so popular that people bought CD players just to\nhear it at its best.</p><p>DAT units also tended to have problems with reliability;\nunderstanding why requires a basic understanding of how DAT technology\nworks.</p><p>From a technological perspective, DAT was implemented in an\ninteresting way. “Interesting” in this context means, of course, “weird\nand unreliable”. The DAT tape itself is only 4mm wide – the same as an\naudio cassette. To get sufficient data bandwidth, the tape couldn’t be\nscanned lengthwise, as all previous tape formats were. At the speeds\nthat would have been required, the tape length would have been\nunmanageable. Instead, DAT works in a similar way to a VHS video\nrecorder: the magnetic head is on a rotating drum, aligned at an angle\nto the direction of tape movement. This arrangement allows the whole\nwidth of the tape to be used, not just a couple of narrow strips in the\nmiddle.</p><p>Naturally, the scanning mechanism required close-tolerance alignment\nto operate reliably. Even when adjusted perfectly, the high rate of\nrotation led to mechanical stresses. This was true of VHS as well, but\nVHS players rapidly became throw-away items – eventually nobody really\ncared if they only lasted a year or two. But if you’d just paid the\nprice of a new car for a DAT player, you’d expect a better service life.\nAnd Sony didn’t help itself: the DTC-700 contained a huge number of\nlow-cost, plastic parts in critical locations. A plastic cog might cost\nonly pennies to replace, but stripping the machine down to get to it\ncost a lot more.</p><p>In the end, though, I don’t think it was the price, or the lack of\ncommercial releases, or the questionnable reliability, or the legal\ncomplications that killed off DAT – although all these factors played a\npart. Rather, I think it was just that old bugbear of the consumer\nelectronics industry: market saturation.</p><p>By about 1992, everybody who was ever likely to want a home DAT\nplayer already had one. The format couldn’t readily be improved, because\nit already offered audio fidelity beyond the limits of human hearing. So\nthere wasn’t a “DAT Mark 2” that manufacturers could have sold to eager\ncustomers. If DAT players could have been made more cheaply, this might\nhave expanded the customer base a little. But I doubt that DAT units\ncould ever have become as cheap as cassette players, and certainly not\nas portable, because the electromechanical design was so complex and\nfussy.</p><p>It’s not as if any alternative technology has really presented\nitself. These days, it’s trivially easy to record from digital or analog\nsources, onto hard disk or solid-state storage. Any desktop computer\nwith a soundcard can do this. A number of manufacturers, including Sony,\ndid release self-contained hard-disk audio recorders, but they seem to\nhave enjoyed even less success than DAT. And these days, of course,\nthere’s even less need for such a device than there was in the 90s. If I\nwant to listen to a radio broadcast more than once, I can probably just\nget it from the broadcaster’s website. Some modern radio tuners even\nhave built-in digital recording capabilities. No: if there were any\ndemand for a modern alternative to the DAT recorder, somebody would be\nselling one.</p><p>Many of the audio technologies from my youth have undergone a\nrevivial recently: vinyl records are the obvious example, but even\ncassettes are starting to sell again. Are we likely to see renewed\ninterest in DAT? On the whole, I think probably not. Plenty of people\nlook back with fondness on vinyl and cassette, even on CD; I don’t think\nDAT gives anybody a warm glow.</p>","contentLength":9915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426171"},{"title":"Proton joins suit against Apple for practices that harm developers and consumers","url":"https://proton.me/blog/apple-lawsuit","date":1751306318,"author":"moose44","guid":178662,"unread":true,"content":"<p>Earlier today, Proton filed court papers in the US District Court for the Northern District of California to join an existing class-action lawsuit against Apple. Proton is a plaintiff in the case, but we are representing and suing on behalf of a class of similarly situated developers. Challenging one of the most powerful corporations in the history of capitalism is not a decision we make lightly, but Proton has long championed online freedom, privacy, and security, and we believe this action is necessary to ensure the internet of the future lives up to its potential.</p><h2>Why are we doing this now?</h2><p>We believe that Apple’s conduct, as detailed in the complaint we filed, constitutes further violations of US antitrust law. Without this case, Apple could get away with behavior in the US that is already outlawed in the European Union. If this were to happen, American consumers, and developers focused on the American market, would have to pay higher prices for fewer choices, and be left at a disadvantage.</p><p>There is also urgency to act now because of a <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.courtlistener.com/docket/70356851/korean-publishers-association-v-apple-inc/\">parallel class-action suit</a> by app developers against Apple on May 23, and any settlement there could be binding on all other developers. By joining that lawsuit, we can ensure that this suit will not only be about monetary damages to compensate app developers for the harm caused by Apple’s conduct, but also changes to App Store policies that will improve the state of the internet. We are seeking to permanently end anti-competitive behavior on the App Store, and we are joining this lawsuit to ensure that any future settlement enforces real changes to Apple’s practices and policies to benefit all consumers, developers, and competition, and not just cosmetic changes.</p><p>While the suit does seek monetary damages on behalf of all developers who have been harmed in order to deter future anti-competitive behavior and provide compensation to class members harmed by Apple’s anti-competitive conduct, Proton will donate any money we receive from the lawsuit to organizations fighting for democracy and human rights so that some portion of Apple’s profits made from countries with authoritarian regimes are redirected to freedom. These donations will be coordinated through the nonprofit Proton Foundation, which oversees Proton and ensures that our work always prioritizes the public good over financial gain.</p><p>Apple’s monopoly control of software distribution on iOS devices presents a myriad of problems for consumers, businesses, and society as a whole. Anti-monopoly laws exist because the power gifted by monopoly status inevitably leads to abuse. In the case of oligarchic tech giants, these abuses have wide implications for society, and it’s vital to the future of the internet that they be addressed now.</p><h3>The App Store policies hurt privacy</h3><p>Apple’s App Store policies disproportionately favor the surveillance capitalism business model employed by companies like Meta and Google and therefore entrench an online business model that routinely violates consumers’ personal privacy. All developers are required to pay Apple an annual fee of $99 to be in the App Store, but Apple also takes a 30% cut from payments made through iOS apps, which are forced to use Apple’s payment system.</p><p>Companies that monetize user data in exchange for “free” services that abuse your privacy aren’t affected by this, as they don’t process payments through the App Store. However, privacy-first companies that monetize through subscriptions are disproportionately hit by this fee, putting a major barrier toward the adoption of privacy-first business models. Naturally, these are also the very companies Apple is directly competing with through its disingenuous privacy marketing campaigns. This is a significant driver behind the internet’s descent into widespread surveillance capitalism.</p><h3>Apple’s policies undermine freedom and democracy</h3><p>Apple’s complete control of the App Store has given it a dangerous level of control over app distribution, giving it the power to decide which apps can and cannot be distributed in different markets. Apple argues this control is necessary for security reasons. But the reality is that this has made Apple the single point of failure for free speech and a tool of dictatorships. There have been numerous incidents where Apple has removed or censored apps at the behest of authoritarian governments, in order to continue profiting from those markets.</p><p>For example, the advocacy group <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://en.greatfire.org/\">GreatFire.org</a> publishes important information about the state of censorship in the App Store through its <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://applecensorship.com/news/new-report-unveils-app-censorship-in-chinas-apple-app-store-amid-recent-developments\">AppleCensorship program</a>, which highlights some striking statistics. Sixty-six of the 100 most popular apps worldwide are unavailable to iOS users in China. Additionally, all 240 VPN apps that the group tested were also unavailable to Chinese users. Overall, 27% of apps are missing from the Chinese App Store, more than double the global average of 13%. Many of those missing apps are news apps (including the likes of The New York Times, BBC News, and Reuters) or social networking or messaging apps, strongly implying that this is a matter of censorship, not security. Apple has also been caught removing apps to help suppress protests, such as the <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.theguardian.com/world/2019/oct/10/hong-kong-protests-apple-pulls-tracking-app-after-china-criticism\">2019 case of HKmap.Live</a>, which was removed at the height of the pro-democracy protests in Hong Kong.</p><p><a href=\"https://proton.me/blog/apple-app-store-antitrust\">Proton itself has also been victim of Apple’s censorship</a>. In 2020, Apple threatened to take Proton VPN out of the App Store unless we removed language from our App Store description that said the app could be used to “unblock censored websites.” We don’t question Apple’s right to act on behalf of authoritarians for the sake of profit, but Apple’s monopoly over iOS app distribution means it can enforce this perverse policy on all app developers, forcing them to also be complicit. We believe it is critical for the future of the internet to end the monopoly on app distribution, so that developers and companies who are prepared to fight for democracy can do so.</p><h3>App Store policies lead to a worse user experience</h3><p>Apple’s approach to subscriptions management is designed to ensure it maintains complete control over the relationship between users and developers. To guarantee it gets its 30% cut of subscription revenue, it has imposed ironclad rules that dictate what developers can and cannot say to their users, which has a detrimental impact on the user experience. One basic example of this is that developers cannot tell users that other pricing options or discounts may be available if users upgrade via a website instead of inside the app. Not supporting Apple’s payment system is also considered a violation, which can lead to threats to remove your app, as <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.theverge.com/2020/10/8/21506995/apple-forced-in-app-purchase-protonmail-ceo-wordpress-iap\">happened to Proton</a>.</p><p>But this controlling behavior goes even further. Developers are prohibited from linking to their websites at all. Proton cannot even link to FAQ or customer support pages from its apps, as Apple believes it’s possible that users will then navigate from the support page to a pricing page and upgrade their accounts without paying Apple its fee. This has a direct, negative impact on customer experience.</p><p>It’s also impossible for users to manage their subscriptions from multiple devices, as this would necessitate stepping outside Apple’s walled garden and weakening its control over the user. For example, users who upgraded their accounts on the web and then wish to upgrade or downgrade their subscription are not allowed to do so from their iOS devices. It is similarly impossible for users who purchased a subscription on iOS to change the subscription on the web. In a world where most users are accessing their apps and services over multiple devices, this is an unacceptably poor customer experience.</p><p>Apple, however, goes even further in a bid to maintain its monopoly and trap users within the Apple ecosystem. Apple intentionally cripples third-party apps that compete with Apple services by making functionality that is available to Apple apps unavailable to other apps. For instance, there is no way to set Proton Calendar as the default calendar app on iOS. Furthermore, in a bid to prevent data portability, competing cloud storage services like Proton Drive are unable to seamlessly do background processing, while no such restrictions are known to exist for iCloud.</p><p>These examples of coercive behavior illustrate time and time again that Apple is willing to inflict a worse experience and higher prices on consumers out of corporate greed, and it leverages its monopoly control over the App Store to do so.</p><h3>App Store tariffs cause price inflation</h3><p>Apple’s 30% fees act as an artificial and arbitrary tax on internet commerce, which, much like a tariff, serves to raise prices, as part or all of this fee is inevitably passed on to the customer. Apple claims this fee is necessary to pay for the maintenance of the App Store, but evidence presented in the  case indicated that Apple makes a 78% profit on App Store fees, raising the question of whether these fees are really necessary or a clear example of the company profiting from its illegal monopoly.</p><p>The only reason Apple can get away with this behavior is because there’s no competition in iOS app distribution or iOS in-app payments. If you want to provide an app or service to iOS users, you have to go through Apple’s systems, and you have to use Apple’s system for collecting payments. Breaking this monopoly and ending this punitive tax on the internet would allow companies like Proton to collect payments via less expensive methods, enabling the option to pass these savings on to you, and ultimately reducing the prices you pay.</p><p>The remedies we are seeking would address many of the social ills mentioned above, ensuring that the internet of the future can continue to protect privacy and democracy. Mobile apps are now the dominant platform of the internet and the way the bulk of the world interacts with one another and with the web. Even if app stores started out as niche markets, today they are a critical component of the internet and fundamental to democracy. It is more essential than ever that we fight to create mobile ecosystems that are truly free, competitive, and not beholden to whichever dictator corporate leaders are currently bowing down to.</p><p>This is also why we enter this fight not just representing ourselves, but as a class representative, to ensure that the outcome of this litigation will benefit all app developers and users of apps in this market. We expect this to be a difficult fight that could take many years, but our mission to build an internet that serves the interest of all of society affords us no other choice. By bringing this case, we hope to set an important precedent that free people, not monopolies, will dictate the future of the internet.</p><p><em>Proton is being represented by Quinn Emanuel Urquhart &amp; Sullivan LLP and Cohen Milstein Sellers &amp; Toll PLLC. The full complaint in the case of Proton v. Apple can be found <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://res.cloudinary.com/dbulfrlrz/images/v1751299117/wp-pme/proton-v--apple-class-action-complaint/proton-v--apple-class-action-complaint.pdf?_i=AA\">here</a>.</em></p>","contentLength":10993,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426128"},{"title":"I write type-safe generic data structures in C","url":"https://danielchasehooper.com/posts/typechecked-generic-c-data-structures/","date":1751302520,"author":"todsacerdoti","guid":176848,"unread":true,"content":"<p>I write type safe generic data structures in C using a technique that I haven’t seen elsewhere. It uses unions to associate type information with a generic data structure, but we’ll get to that. My approach works for any type of data structure: maps, arrays, binary trees… but for this article I illustrate the ideas by implementing a basic linked list. Since many people aren’t aware you can do C generics , I figured I’d start simple and build up to this:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>I hesitate to even mention this, because I do not like it, but its worth comparing to the technique at the end of this article. It works like this: you write your data structure in a header, using macros for your types, and then  the header multiple times; once for each type the data structure will be used with.</p><p>While it  generic and type safe, it has downsides:</p><ul><li>makes it hard to find where types and functions are defined (because they’re constructed by macros)</li><li>code completion may not handle them well</li><li>bloats your binary size and build times with copies of the same functions</li><li>requires using type-prefixed functions: <code>Foo_list_prepend() and int_list_prepend()</code> vs just </li></ul><p>Another way to make a data structure generic is to use . It’s not type safe but we’ll get to that.</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Note:  is used for familiarity, but I highly recommend Arenas instead. You can <a href=\"https://www.youtube.com/watch?v=TZ5a3gCCZYo\" target=\"_blank\" rel=\"noopener\">watch</a> or <a href=\"https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator\" target=\"_blank\" rel=\"noopener\">read</a> about them.</p><p>Having  and its  as separate allocations isn’t ideal from a memory and performance perspective. It requires 2 allocations per node when one would do, the  pointer uses memory unnecessarily, and you will likely get two cache misses per node when traversing the list: once getting the next node, and once getting its data. We can fix these issues with…</p><h3>Generics level 2: Inline storage</h3><p>Instead of storing a pointer to the node’s data, we can use a <a href=\"https://en.wikipedia.org/wiki/Flexible_array_member\" target=\"_blank\" rel=\"noopener\">Flexible Array Member</a> to store the data inside the node. To do so, we make a single allocation large enough for both the node and the type it stores:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Now  and the actual contents of  are beside each other in memory, solving the issues of the  approach. Unfortunately we now have to pass the size, but we’ll fix that in the next section</p><p>If you wanted to avoid the , and initialize the node’s memory directly, you could do so with a  function:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><h3>Generics level 3: Type Checking</h3><p>The part you’ve all been waiting for: how to get the compiler to error when we try to add the wrong type to a list. The way I found to do this is to use a union with a  member that has a parameterized type:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>How does that help us? Well, we can use the ternary operator to enforce that the  parameter is the same type as the list’s :</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>The macro also handles passing the item size for us! This is the error Clang produces when adding the wrong type to the list:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Macros get a bad rep, but I think this is fairly understandable. Some things to note:  is never used at runtime, it exists just for type information at compile time. Using a union makes  not consume any memory.</p><p>If you’re writing a generic function that needs to return a pointer to contained data, you can use  to cast the return type from  to the data structure’s  type.  is supported in all three big C compilers (clang, gcc,  msvc since version 19.39).</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>If for some reason you don’t like using the ternary operator to ensure two types are the same, a previous version of this article used a different technique:</p><p>One annoying thing about C compilers released prior to late 2025 is that they do not consider these two variables to have the same type:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Even though the variables have identical type definitions, the compiler still errors because they are . A  avoids the issue:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>You can use this for any type of data structure, even ones with multiple associated types, like a hash map:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>For more detail, like how the  macro is implemented, see the code <a href=\"https://gist.github.com/danielchasehooper/a646a109b62441ca1b4d75d94436b5cf\" target=\"_blank\" rel=\"noopener\">here</a></p><p>Thanks to <a href=\"https://forkingpaths.dev/index.html\" target=\"_blank\" rel=\"noopener\">Martin Fouilleul</a> for the encouragement to finish this post, which I’ve been sitting on for months, and the feedback on early drafts.</p>","contentLength":3921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44425461"},{"title":"A CarFax for Used PCs; Hewlett Packard wants to give old laptops new life","url":"https://spectrum.ieee.org/carmax-used-pcs","date":1751301523,"author":"rubenbe","guid":178869,"unread":true,"content":"<p><a href=\"https://ewastemonitor.info/the-global-e-waste-monitor-2024/\" target=\"_blank\"> E-waste Monitor</a><a href=\"https://spectrum.ieee.org/the-cybersecurity-of-e-waste\" target=\"_self\">e-waste</a></p><p>Many enterprises follow a standard three-year replacement cycle, assuming older computers are inefficient. However, many of these devices are still functional and could perform well with minor upgrades or maintenance. The issue is, no one knows what the weak points are for a particular machine, or what the needed maintenance is, and the <a href=\"https://spectrum.ieee.org/tag/diagnostics\">diagnostics</a> would be too costly and time-consuming. It’s easier to just buy brand new <a data-linked-post=\"2670648506\" href=\"https://spectrum.ieee.org/risc-v-laptops\" target=\"_blank\">laptops</a>.</p><p>When buying a used car, dealerships and individual buyers can access each car’s particular <a href=\"https://www.carfax.com/\" target=\"_blank\">CarFax</a> report, detailing the vehicle’s usage and maintenance history. Armed with this information, dealerships can perform the necessary fixes or upgrades before reselling the car. And individuals can decide whether to trust that vehicle’s performance. We at <a href=\"https://www.hp.com/us-en/home.html\" target=\"_blank\">HP</a> realized that, to prevent unnecessary e-waste, we need to collect and make available usage and maintenance data for each laptop, like a CarFax for used PCs.</p><p>There is a particular challenge to collecting usage data for a PC, however. We need to make sure to protect the user’s privacy and security. So, we set out to design a data-collection protocol for PCs that manages to remain secure.</p><h2>The firmware-level data collector</h2><p> Luckily, the sensors that can collect the necessary data are already installed in each PC. There are thermal sensors that monitor CPU temperature, power-consumption monitors that track <a href=\"https://spectrum.ieee.org/tag/energy-efficiency\">energy efficiency</a>, storage health indicators that assess <a href=\"https://spectrum.ieee.org/tag/solid-state-drive\">solid state drive</a> (<a href=\"https://spectrum.ieee.org/tag/ssd\">SSD</a>) wear levels, performance counters that measure system utilization, fan-rotation-speed sensors that detect cooling efficiency, and more. The key is to collect and store all that data in a secure yet useful way.</p><p> We decided that the best way to do this is to integrate the life-cycle records into the <a href=\"https://spectrum.ieee.org/tag/firmware\">firmware</a> layer. By embedding <a href=\"https://spectrum.ieee.org/tag/telemetry\">telemetry</a> capabilities directly within the firmware, we ensure that device health and usage data is captured the moment it is collected. This data is stored securely on <a href=\"https://spectrum.ieee.org/tag/hp\">HP</a> SSD drives, leveraging hardware-based security measures to protect against unauthorized access or manipulation. </p><p>The secure telemetry protocol we’ve developed at HP works as follows. We gather the critical hardware and sensor data and store it in a designated area of the SSD. This area is write-locked, meaning only authorized firmware components can write to it, preventing accidental modification or tampering. That authorized firmware component we use is the Endpoint Security Controller, a dedicated piece of hardware embedded in business-class HP PCs. It plays a critical role in strengthening platform-level security and works independently from the main CPU to provide foundational protection.</p><p>The endpoint security controller establishes a secure session by retaining the secret key within the controller itself. This mechanism enables read data protection on the SSD—where telemetry and sensitive data are stored—by preventing unauthorized access, even if the operating system is reinstalled or the system environment is otherwise altered.</p><p>Then, the collected data is recorded in a time-stamped file, stored within a dedicated telemetry log on the SSD. Storing these records on the SSD has the benefit of ensuring the data is persistent even if the operating system is reinstalled or some other drastic change in software environment occurs.</p><p>The telemetry log employs a cyclic buffer design, automatically overwriting older entries when the log reaches full capacity. Then, the telemetry log can be accessed by authorized applications at the operating system level.</p><p>The telemetry log serves as the foundation for a comprehensive device history report. Much like a CarFax report for used cars, this report, which we call PCFax, will provide both current users and potential buyers with crucial information.</p><p>The PCFax report aggregates data from multiple sources beyond just the on-device telemetry logs. It combines the secure firmware-level usage data with information from HP’s factory and supply-chain records, digital-services platforms, customer-support service records, diagnostic logs, and more. Additionally, the system can integrate data from external sources including partner sales and service records, refurbishment partner <a href=\"https://spectrum.ieee.org/tag/databases\">databases</a>, third-party component manufacturers like <a href=\"https://spectrum.ieee.org/tag/intel\">Intel</a>, and other original equipment manufacturers. This multisource approach creates a complete picture of the device’s entire life cycle, from manufacturing through all subsequent ownership and service events.</p><p>For IT teams within organizations, we hope the PCFax will bring simplicity and give opportunities for optimization. Having access to fine-grained usage and health information for each device in their fleet can help IT managers decide which devices are sent to which users, as well as when maintenance is scheduled. This data can also help device managers decide which specific devices to replace rather than issuing new computers automatically, enhancing <a href=\"https://spectrum.ieee.org/tag/sustainability\">sustainability</a>. And this can help with security: With real-time monitoring and firmware-level protection, IT teams can mitigate risks and respond swiftly to emerging threats. All of this can facilitate more efficient use of PC resources, cutting down on unnecessary waste.</p><p>We also hope that, much as the CarFax gives people confidence in buying used cars, the PCFax can encourage resale of used PCs. For enterprises and consumers purchasing second-life PCs, it provides detailed visibility into the complete service and support history of each system, including any repairs, upgrades, or performance issues encountered during its initial deployment. By making this comprehensive device history readily available, PCFax enables more PCs to find productive second lives rather than being prematurely discarded, directly addressing the e-waste challenge while providing economic benefits to both sellers and buyers in the secondary PC market.</p><p>While HP’s solutions represent a significant step forward, challenges remain. Standardizing telemetry frameworks across diverse ecosystems is critical for broader adoption. Additionally, educating organizations about the benefits of life-cycle records will be essential to driving uptake. </p><p>We are also working on integrating AI into our dashboards. We hope to use <a href=\"https://spectrum.ieee.org/tag/ai-models\">AI models</a> to analyze historical telemetry data and predict failures before they happen, such as detecting increasing SSD write cycles to forecast impending failure and alert IT teams for proactive replacement, or predicting battery degradation and automatically generating a service ticket to ensure a replacement battery is ready before failure, minimizing downtime.</p><p>We plan to start rolling out these features at the beginning of 2026.</p>","contentLength":6710,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44425253"},{"title":"Donkey Kong Country 2 and Open Bus","url":"https://jsgroth.dev/blog/posts/dkc2-open-bus/","date":1751295672,"author":"colejohnson66","guid":176784,"unread":true,"content":"<p>Donkey Kong Country 2 has a pretty well-known bug in the old SNES emulator ZSNES where some stages have spinning barrels that don’t work properly. One of the earliest pictured here, in the first stage of Krem Quay (third world):</p><p>After you jump into the barrel, you’re supposed to be able to completely control its rotation by pressing left and right on the d-pad, with the barrel only rotating while you’re holding left or right. In ZSNES, this is horribly bugged. Tapping left or right makes the barrel spin forever in that direction, until you press the opposite direction…which simply makes it spin forever in the opposite direction.</p><p>This is more than just annoying - it makes these stages significantly more difficult than the developers intended, since later on the spinning barrels show up over spikes and other hazards:</p><p>This used to be somewhat documented in threads on the ZSNES forums, but those unfortunately seem to have gone offline since last I looked at them, and I can’t find the relevant threads indexed in the Wayback Machine.</p><p>This bug is caused by ZSNES not emulating open bus behavior. I believe this was originally discovered by Anomie roughly two decades ago, who subsequently fixed the same bug in Snes9x. This original fix hardcoded the specific addresses to return the values that the game depends on rather than properly emulating open bus, but it fixed DKC2 and probably didn’t break anything else. The bug was never fixed in ZSNES, which is now a long abandoned project (last release in 2007).</p><p>Purely out of curiosity, I wanted to dig into this a little more to figure out what exactly in the game code causes these barrels to spin forever in an emulator that doesn’t emulate open bus behavior.</p><p>On older platforms like the SNES, reading from an invalid memory address  does not crash the program. There are cases where accessing specific invalid addresses can cause the hardware to lock up, but I don’t believe this can happen on SNES.</p><p>Instead, reading from an invalid address usually triggers <a href=\"https://snes.nesdev.org/wiki/Open_bus\" target=\"_blank\" rel=\"noopener\">open bus behavior</a>, where the CPU re-reads the last value that was put on the data bus. SNES specifically has several different internal buses that can retain different open bus values, but this doesn’t affect DKC2.</p><p>The main SNES CPU is a <a href=\"https://en.wikipedia.org/wiki/WDC_65C816\" target=\"_blank\" rel=\"noopener\">65C816</a> (aka 65816). There’s some other hardware around it as part of the <a href=\"https://en.wikipedia.org/wiki/Ricoh_5A22\" target=\"_blank\" rel=\"noopener\">Ricoh 5A22</a> S-CPU package, such as a multiplication/division unit and a DMA unit, but the core CPU is a 65816.</p><p>65816 is a 16-bit extension of the <a href=\"https://en.wikipedia.org/wiki/MOS_Technology_6502\" target=\"_blank\" rel=\"noopener\">6502</a>, a very popular 8-bit CPU used in many systems including the NES (with slight modifications). The 65816 is mostly backwards compatible with 6502 software, which was not important for the SNES (which has no NES backwards compatibility) but was very important for the <a href=\"https://en.wikipedia.org/wiki/Apple_IIGS\" target=\"_blank\" rel=\"noopener\">Apple IIGS</a> that this CPU was originally designed for.</p><p>I personally think the 65816 ISA is pretty awkward. 8-bit vs. 16-bit operation is based on new processor status flags M (accumulator / memory size) and X (index register size) rather than being encoded into opcode bits, so software needs to frequently execute the new instructions SEP (set processor flags) and REP (reset processor flags) to manually adjust register and memory access sizes as needed. This also makes 65816 disassembly extraordinarily painful without tracing execution in an emulator, since some instructions vary in length depending on the current processor flags - e.g. an immediate operand can be either 1 byte (8-bit) or 2 bytes (16-bit).</p><p>Beyond that (and slightly more relevant to this post), addressing more than 64 KB of memory requires dealing with memory banking which is not fun. The 65816 has a 24-bit address bus, but most addresses are created by combining an 8-bit bank with a 16-bit offset. This is sort of similar to how the earliest x86 CPUs segment memory into 64 KB segments, except 65816 has no address overlap between different 64 KB memory banks.</p><p>Many instructions still operate using 16-bit addresses internally, like on 6502, plus the program counter is still 16-bit. There’s a new 8-bit program bank register (PBR / K) used for instruction fetches, and a new 8-bit data bank register (DBR / B) used for instructions and addressing modes that produce a 16-bit memory address rather than 24-bit. Software needs to manually track and update these bank registers as needed. There are long jump instructions that simultaneously update PBR and PC, but regular jump instructions and conditional branch instructions cannot jump between different program banks.</p><p>The hardware stack and the direct page (65816’s replacement for the zero page) are not banked - they are always located within memory bank $00.</p><p>The SNES memory map is  designed around the 65816’s memory banking. It’s much more useful to think of SNES memory addresses as a separate 8-bit bank and 16-bit offset rather than a single 24-bit address.</p><p>When you’re inside one of these spinning barrels, Donkey Kong Country 2 reads from addresses $2000 and $2001 in bank $B3. In some other banks these addresses would map to either the cartridge or RAM, but in bank $B3 they are not mapped to anything, so reading from them is open bus behavior. Why does the game do this?</p><p>Here’s a disassembly of the part of the game code that performs the open bus read, generated from an execution trace and then edited a bit for clarity (e.g. replacing the relative branch offset with a label). This is part of a routine that’s executed once per frame, beginning when you release left/right on the gamepad while you’re in a spinning barrel:</p><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code></code></pre></td><td><pre tabindex=\"0\"><code data-lang=\"asm\"></code></pre></td></tr></tbody></table></div></div></div><p>This routine accesses a few memory addresses in the $0000-$2001 range. Some of them are through absolute addressing modes that use the current data bank of $B3, while others use direct page addressing modes that always access bank $00. The direct page itself is located at $0000 here, same as the 6502 zero page.</p><p>Banks $B3 and $00 happen to have the same memory map for this $0000-$2001 address range. In banks $00-$3F and $80-$BF, $0000-$1FFF always maps to the first 8 KB of the console’s 128 KB of working RAM (WRAM). $2000-$20FF is entirely unmapped, so the  instruction is an open bus read.</p><p>It’s using a few WRAM addresses here that seem to contain the following, based on what values the game writes to them and what it uses them for:</p><ul><li>$0EE6 ($48 + X=$0E9E): The current barrel orientation</li><li>$0E0A ($0028 + Y=$0DE2): Per-frame rotation amount, as a change to barrel orientation</li><li>$0032: Seems to be used as just a temporary variable</li></ul><p>I imagine the exact orientation/rotation locations in WRAM are different for different barrels.</p><p>The barrel orientation appears to be on a scale where 0x0000 is pointing straight down, 0x4000 is pointing straight left, etc.</p><p>The rotation amount determines the barrel rotation speed. For the barrel I looked at, the game sets the rotation amount to 0x0300 when rotating clockwise and 0xFD00 (-0x0300) when rotating counterclockwise. This makes a full 360 degree rotation take just over 85 frames, a little less than 1.5 seconds at 60 frames per second.</p><p>Okay, starting to step through this routine:</p><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code data-lang=\"asm\"></code></pre></td></tr></tbody></table></div></div></div><p>This part is straightforward: It loads the current orientation, adds the rotation amount, and stores the result in a temporary variable. It executes CLC (clear carry flag) before ADC (add with carry) because the 65816, like the 6502, does not have an add without carry instruction.</p><p>Next is the interesting part:</p><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code data-lang=\"asm\"></code></pre></td></tr></tbody></table></div></div></div><p>It XORs the updated orientation with the previous orientation, bitwise ANDs the result with an open bus read from $2000, and then branches based on whether the bitwise AND produced zero. The spinning forever bug triggers when the branch is  taken because the bitwise AND result is  zero.</p><p>On actual hardware, the 16-bit open bus read from $2000 always returns 0x2020. This is because the last byte read from the bus is always the high byte of the $2000 absolute address encoded in the instruction bytes, little-endian:</p><p>Since the 65816 only has an 8-bit data bus, it implements 16-bit reads by performing two consecutive 8-bit reads, which in this case will both return 0x20. Hence the 16-bit value 0x2020.</p><p>So, in practice, that part of the routine behaves equivalently to this:</p><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code data-lang=\"asm\"></code></pre></td></tr></tbody></table></div></div></div><p>Moving on, when the AND result is zero and the  branch is taken, it does this:</p><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code data-lang=\"asm\"></code></pre></td></tr></tbody></table></div></div></div><p>It loads the pre-XOR rotated orientation from the temporary variable, writes it to the permanent orientation location in WRAM, then returns. The rotation will continue next frame.</p><p>When the AND is non-zero and the branch is not taken, it does this:</p><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code data-lang=\"asm\"></code></pre></td></tr></tbody></table></div></div></div><p>First, it zeroes out the rotation amount. 65816 has a dedicated instruction STZ (store zero) for zeroing out a memory location, but STZ doesn’t support any Y-indexed addressing modes like what the game uses here (absolute indexed Y).</p><p>Next, it loads the pre-XOR rotated orientation, adds 0x1000, and masks out all but the highest 3 bits. This is a crude but fast way of approximately rounding to the nearest multiple of 0x2000.</p><p>Finally, it writes the rounded orientation to the permanent location in WRAM and then returns.</p><p>All together, in a higher level language, this routine is doing this:</p><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code data-lang=\"python\"></code></pre></td></tr></tbody></table></div></div></div><p>With the open bus read returning 0x2020, the XOR-then-AND result will be non-zero when adding the rotation amount to orientation changes either bit 5 (0x0020) or bit 13 (0x2000). Given a rotation amount of 0x0300 or 0xFD00, bit 5 is always 0, so only bit 13 can ever change.</p><p>For convenience, here’s the orientation values diagram again:\n<img src=\"https://jsgroth.dev/blog/images/dkc2/orientation.png\" alt=\"Barrel Orientation\"></p><p>Looking at this, an orientation change of 0x2000 corresponds to a single-step change in cardinal or ordinal direction. This means that bit 13 will change when the barrel either reaches or passes over one of these 8 directions. Whether it changes upon reaching or upon passing over depends on the rotation direction, but it’s not really significant from a player perspective since it’s only a 1-frame difference and only in specific cases.</p><p>Rounding to the nearest multiple of 0x2000 ensures that the stopped barrel points exactly in a cardinal or ordinal direction, since it may have passed over the direction on the final rotation frame.</p><p>So, if you replace the open bus read with a constant 0x2000, I think this logic makes sense! When you release the d-pad, the barrel continues to rotate in the same direction until it reaches the next cardinal or ordinal direction, and then the rotation stops with the barrel pointing exactly in that direction.</p><p>At this point I am pretty sure the open bus read was simply caused by a typo.</p><p>I think that  instruction (absolute addressing) was supposed to be  (immediate addressing).  just happens to work because the 16-bit open bus read from $2000 returns a value that is functionally equivalent to 0x2000 in this logic as long as the per-frame rotation amount always has its lowest 6 bits set to 0.</p><p>The incorrect opcode is executed at bank $B3 offset $EDAC, which maps to $33EDAC in the game’s 4 MB of ROM. Changing this byte from 0x2D (AND with absolute addressing) to 0x29 (AND with immediate addressing) makes the spinning barrels work correctly even if open bus reads always return 0. The exact location in ROM probably varies between different revisions of the game; I only looked at one revision.</p><p>This was purely an academic exercise since the game works perfectly fine in just about every SNES emulator other than the long-obsolete ZSNES, but my curiosity is satisfied.</p>","contentLength":11310,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44424194"},{"title":"There are no new ideas in AI, only new datasets","url":"https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only","date":1751294626,"author":"bilsbie","guid":176847,"unread":true,"content":"<p><a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/\" rel=\"\">“Moore’s Law for AI”</a></p><p>Although I don’t really agree with this specific framing for a number of reasons, I can’t deny the trend of progress. Every year, our AIs get a little bit smarter, a little bit faster, and a little bit cheaper, with no end in sight.</p><p>Most people think that this continuous improvement comes from a steady supply of ideas from the research community across academia – mostly MIT, Stanford, CMU – and industry – mostly Meta, Google, and a handful of Chinese labs, with lots of research done at other places that we’ll never get to learn about.</p><p>And we certainly have made a lot of progress due to research, especially on the systems side of things. This is how we’ve made models cheaper in particular. Let me cherry-pick a few notable examples from the last couple years:</p><p><a href=\"https://arxiv.org/abs/2205.14135)\" rel=\"\">FlashAttention</a></p><p><a href=\"https://arxiv.org/abs/2211.17192\" rel=\"\">speculative decoding</a><a href=\"https://arxiv.org/pdf/2302.01318\" rel=\"\">DeepMind</a></p><p><a href=\"https://kellerjordan.github.io/posts/muon/\" rel=\"\">Muon</a></p><p><a href=\"https://arxiv.org/abs/2501.12948\" rel=\"\">DeepSeek-R1</a></p><p><a href=\"https://arxiv.org/\" rel=\"\">ArXiv</a></p><p><a href=\"https://www.notion.so/There-Are-No-New-Ideas-in-AI-Only-New-Data-1cf5109a45d880e6b0d5d6e3a4ba2fdc?pvs=21\" rel=\"\">AlexNet model</a></p><p>4. Reasoning: in 2024 OpenAI released O1, which led to DeepSeek R1</p><p>If you squint just a little, these four things (DNNs → Transformer LMs → RLHF → Reasoning) summarize everything that’s happened in AI. We had DNNs (mostly image recognition systems), then we had text classifiers, then we had chatbots, now we have reasoning models (whatever those are).</p><p>Say we want to make a fifth such breakthrough; it could help to study the four cases we have here. What new research ideas led to these groundbreaking events?</p><p><strong>all the underlying mechanisms of these breakthroughs existed in the 1990s,</strong></p><p>Supervised learning via cross-entropy, the main way we pre-train language models, emerged from Claude Shannon’s work in the 1940s.</p><p><a href=\"https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf\" rel=\"\">introduction of policy-gradient methods in 1992</a></p><p><strong>enabled us to learn from a new data source:</strong></p><p><a href=\"http://(https://www.image-net.org/\" rel=\"\">ImageNet</a></p><p>3. RLHF allowed us to learn from human labels indicating what “good text” is (mostly a vibes thing)</p><p><a href=\"http://incompleteideas.net/IncIdeas/KeytoAI.html\" rel=\"\">“verifiers”</a></p><p>Remind yourself that each of these milestones marks the first time the respective data source (ImageNet, The Web, Humans, Verifiers) was used at scale. Each milestone was followed by a frenzy of activity: researchers compete to (a) siphon up the remaining useful data from any and all available sources and (b) make better use of the data we have through new tricks to make our systems more efficient and less data-hungry. (I expect we’ll see this trend in reasoning models throughout 2025 and 2026 as researchers compete to find, categorize, and verify everything that might be verified.)</p><p>There’s something to be said for the fact that our actual technical innovations may not make a huge difference in these cases. Examine the counterfactual. If we hadn’t invented AlexNet, maybe another architecture would have come along that could handle ImageNet. If we never discovered Transformers, perhaps we would’ve settled with LSTMs or SSMs or found something else entirely to learn from the mass of useful training data we have available on the Web.</p><p>This jibes with the theory some people have that nothing matters but data. Some researchers have observed that for all the training techniques, modeling tricks, and hyperparameter tweaks we make, the thing that makes the biggest difference by-and-large is changing the data.</p><p><a href=\"https://arxiv.org/abs/2212.10544\" rel=\"\">developing a new BERT-like model using an architecture other than transformers</a></p><p><em>*there is an upper bound to what we might learn from a given dataset*</em></p><p><a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\" rel=\"\">The Bitter Lesson</a></p><p>The obvious takeaway is that our next paradigm shift isn’t going to come from an improvement to RL or a fancy new type of neural net. It’s going to come when we unlock a source of data that we haven’t accessed before, or haven’t properly harnessed yet.</p><p><a href=\"https://www.dexerto.com/entertainment/how-many-videos-are-there-on-youtube-2197264/\" rel=\"\">a random site on the Web</a></p><p>It’s safe to say that as soon as our models get efficient enough, or our computers grow beefy enough, Google is going to start training models on YouTube. They own the thing, after all; it would be silly not to use the data to their advantage.</p><p>It’s hard to say whether YouTube or robots or something else will be the Next Big Thing for AI. We seem pretty deeply entrenched in the camp of language models right now, but we also seem to be running out of language data pretty quickly. But if we want to make progress in AI, maybe we should stop looking for new ideas, and start looking for new data.</p>","contentLength":4194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44423983"},{"title":"Show HN: TokenDagger – A tokenizer faster than OpenAI's Tiktoken","url":"https://github.com/M4THYOU/TokenDagger","date":1751286838,"author":"matthewolfe","guid":176699,"unread":true,"content":"<p>TokenDagger is a drop-in replacement for OpenAI’s Tiktoken (the tokenizer behind Llama 3, Mistral, GPT-3.*, etc.). It’s written in C++ 17 with thin Python bindings, keeps the exact same BPE vocab/special-token rules, and focuses on raw speed.</p><p>I’m teaching myself LLM internals by re-implementing the stack from first principles. Profiling TikToken’s Python/Rust implementation showed a lot of time was spent doing regex matching. Most of my perf gains come from a) using a faster jit-compiled regex engine; and b) simplifying the algorithm to forego regex matching special tokens at all.</p><p>Benchmarking code is included. Notable results show:\n- 4x faster code sample tokenization on a single thread.\n- 2-3x higher throughput when tested on a 1GB natural language text file.</p>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44422480"},{"title":"Reverse Engineering Vercel's BotID","url":"https://www.nullpt.rs/reversing-botid","date":1751285985,"author":"hazebooth","guid":176930,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44422356"},{"title":"Show HN: New Ensō – first public beta","url":"https://untested.sonnet.io/notes/new-enso-first-public-beta/","date":1751281375,"author":"rpastuszak","guid":176698,"unread":true,"content":"<p>The new version of <a href=\"https://enso.sonnet.io\" target=\"_blank\" rel=\"noopener noreferrer\">Ensō</a> (codename: Occult Vampire Keanu) is available for public testing!</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/IYuLv6hkrd-2817.png\" alt=\"\" width=\"2817\" height=\"1470\">\nThis is a temporary icon I used for testing. I am considering creating a simplified version of it. PS. here's the <a href=\"https://www.potato.horse/p/MVrXRp0XziSuakcrMGzO8\" target=\"_blank\" rel=\"noopener noreferrer\">original image</a> (on <a href=\"https://potato.horse\" target=\"_blank\" rel=\"noopener noreferrer\">potato.horse</a>, of course)</p><p>Following <a href=\"https://untested.sonnet.io/notes/miss-make-it-stupid-simple/\">MISS</a>, my focus is on removing distractions over adding new features. This can be surprisingly challenging (e.g. how do I tell users about feature X or Y without breaking their flow?) but also gives me time to focus on polishing the app.</p><p>(we will discuss these in more detail in future posts)</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-short-version-as-explained-by-hermes-trismegistus\" aria-hidden=\"true\" tabindex=\"-1\"></a>Short version (as explained by Hermes Trismegistus)</h3><ul><li> Simplified, more accessible UI</li></ul><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-an-even-more-simple-streamlined-ui-following-the-miss-philosophy\" aria-hidden=\"true\" tabindex=\"-1\"></a>An even more simple, streamlined UI, following the <a href=\"https://untested.sonnet.io/notes/miss-make-it-stupid-simple/\">MISS</a> philosophy.</h3><p>Most of the UI has been moved to the application menu bar for easier discoverability and shortcut access. So far no one has missed the old inline UI, but you can read more about it towards the end of this note.</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-5-accessibility-friendly-themes-to-choose-from\" aria-hidden=\"true\" tabindex=\"-1\"></a>5½ Accessibility-friendly themes to choose from</h3><p>We have  5½ predefined themes focussed on accessibility and specific use patterns based on feedback I've collected over the years.</p><ul><li>writing during the day in regular light conditions</li><li>writing in low light for devices with OLED screens\n</li><li>writing in extremely low light conditions, with reduced light exposure  (See <a href=\"https://untested.sonnet.io/notes/midnight/\">Midnight</a>, <a href=\"https://untested.sonnet.io/notes/obsidian-for-vampires/\">Obsidian for Vampires</a>)\n<ul><li>designed for OLED screens</li><li>the main use case here is writing at night, to put myself to sleep.</li></ul></li></ul><p>5½ and not 6 because one theme still needs some work. Is there a specific use case or theme you'd like to see in Ensō? Let me know!</p><p>This is one of the few truly new features in Ensō. Coffeeshop mode allows you to stop worrying that someone standing behind you might see what you're typing. The text itself is concealed but you still know what you're writing.  Use  to toggle on and off at any time.</p><p>I've been using it for a couple of months and found it super helpful, especially for journaling in public places, but not only (read more here: <a href=\"https://untested.sonnet.io/notes/sketch-enso-coffeeshop-mode/\">Sketch - Ensō Coffeeshop Mode</a>).</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-a-few-smaller-accessibility-improvements\" aria-hidden=\"true\" tabindex=\"-1\"></a>A few smaller accessibility improvements</h3><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/rwyV2cqnxt-974.png\" alt=\"\" width=\"974\" height=\"348\">\nNote: if you remove the  menu and call it , MacOs won't add its AI crap to your settings.</p><ul><li> toggle autocorrect, autocapitalise, spelling</li><li> control text size (previously not possible in the native version)</li></ul><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-a-new-polished-text-rendering-engine\" aria-hidden=\"true\" tabindex=\"-1\"></a>A new, polished text rendering engine</h3><p>The new text rendering engine allows for better control over typography settings, supports alternative display modes like Coffeeshop, and uses a custom caret.</p><p>I don't know how to describe it objectively (and I obviously lack the distance to) but writing in the new UI feels different, more fluid. The text is easy to read, but also somewhat softer (though not blurry).</p><p>Less is more, so why do I care about it?  less is more. I want Ensō to feel familiar and high-quality, like a good Moleskine notebook. I want people to feel comfortable paying $10 for a typing app without text selection. I want them to enjoy it as much as I do. Fewer features allow me to focus more on what  there.</p><p>Ensō will be published via the AppStore by default. We will keep the old version on Gumroad, but there's no reason to maintain it, since the new version is better in every possible way and functionally the same by default.</p><p>The reasons I decided to <a href=\"https://untested.sonnet.io/notes/skip-the-appstore-and-use-gumroad/\">skip the AppStore and use Gumroad</a>, plus what I learned from that are beyond the scope of this note (you can click the link to request that particular write-up).</p><ul><li>several users complained that Gumroad payment looked, for the lack of a better word, shady, especially at the step with a PayPal payment screen. The ones who messaged me still bought the app, but I imagine there were many who turned back.</li><li>AppStore with all its flaws makes delivering apps... slow and annoying, but also relatively easy without much code.</li><li>I can add OTA updates and re-publish Ensō via Gumroad later, which makes sense as an iterative improvement.</li></ul><p><strong>The Gumroad version of Ensō will stay as a backup, but will not be maintained.</strong></p><p>I've been using Ensō daily for 6 years. I've also received a ton of high-quality feedback, not via analytics but from users who were kind enough to reach out to me. I like to think that I have a fairly good idea of how and why people use Ensō.</p><p>The previous version of Ensō would pass an anonymous impression event on load. Now, by design, no network traffic is made at all. Here's our new Privacy Page.</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/2seRd5O6WS-1588.png\" alt=\"\" width=\"1588\" height=\"842\">\nCurrent version of our Privacy page (<a href=\"https://enso.sonnet.io/app-privacy\" target=\"_blank\" rel=\"noopener noreferrer\">source</a>)</p><p>It will come, but the new version is already so much better than the previous, that I feel like waiting for more features would be a wasted opportunity.</p><p>I'm working on a UX that balances discoverability with staying focussed. Each option, each new choice is a chance for you to get distracted, so the key is to do this thoughtfully and with respect towards my users' time.</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-rtl-or-non-ltr-language-support\" aria-hidden=\"true\" tabindex=\"-1\"></a>RTL (or non-LTR) language support</h3><p><strong>This one will be included in the next test build.</strong> Many Ensō users speak languages written in non-Latin alphabets (to my knowledge, mainly Persian, Arabic and Hebrew).</p><p>It makes me both grateful and somewhat sad that one (non-techie) user went as far as even sharing a code sample with me when asking for fixing the issue. Adding rudimentary RTL support can be as simple as a one-line change in your code. Even if it's not perfect - it's still a huge improvement that your non-Latin script users will notice, believe me.</p><p>The previous version of Ensō displayed the UI in the same space as the text. That's not the case any more.</p><p>I'm still considering adding a hamburger menu in the main app canvas, however only two (less frequent) users of Ensō have brought it up so far.</p><ol><li>ease of use, reducing distractions\n</li></ol><p>There's tension between 1. and 2. as every new feature implies more choices on the user's part; every new choice is an opportunity for distraction. This might seem pedantic, but small, seemingly insignificant changes do add up.</p><p>Removing things is harder than adding them (see 3.). Perhaps that's why commits with negative LoC count feel so good.</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/POXoaXUuMD-1668.png\" alt=\"\" width=\"1668\" height=\"1559\">\nWhere to go from here?</p><ol><li>Collect the test feedback and respond to it</li><li>Prepare basic marketing materials\n<ul><li>I might put an ad on social media, trying to get people off it (<a href=\"https://sonnet.io/posts/sit/\" target=\"_blank\" rel=\"noopener noreferrer\">Sit.</a>) but what I call marketing is mostly talking about Ensō and related subjects here, plus engaging with communities I already know, such as forums</li></ul></li></ol><ul><li> Windows and Linux support — I'll revisit it in the next few months. I'm moving towards supporting myself from my own projects and I need to be selective how I use my time. If you're interested in testing a Windows or Linux build, <a href=\"mailto:hello@sonnet.io\">let me know</a>.</li><li> Quick Save - hitting  would automatically save a snapshot of your notes to a predefined directory with a time-stamped file name, e.g. </li><li> Toybox - an optional menu feature with experimental tools released episodically, such as:<ul><li> visual experiments (e.g. different typography styles or letters and words turning into vines that grow as you type)</li></ul></li></ul><p>If Toybox becomes a reality, it'll be buried in the menus to avoid distractions and will act mainly as my platform for experimentation and play with users. If there's a chance it might introduce more distractions - it'll become a separate app. (<a href=\"https://untested.sonnet.io/notes/kind-software/\">Kind software</a>)</p><p>Every day in small chunks and some days in longer stretches.</p><p>I'm approaching this just like <a href=\"https://untested.sonnet.io/notes/exhibition-in-porto-janusz-enters-the-fashion-industry-draft-1/\">My Recent Art Exhibition</a> - working on different things simultaneously, focussing on their interplay rather than looking at each feature in isolation.</p><p>While I believe you should <a href=\"https://untested.sonnet.io/notes/share-your-unfinished-scrappy-work/\">Share your unfinished, scrappy work</a>, I know Ensō well enough that I can allow myself more flexibility. This style of work gives me a lot of joy and the end results have so far been better than expected.</p><p><strong>The new Ensō is not the type of project I can share in small unfinished bits, feature by feature.</strong> I will repeat this ad nauseam: I want to give you something that will get out of your way but also feel beautiful, polished, yours.</p><p>This is akin to good typography or UX - when it's there, you don't notice it, but at a subconscious level, you feel more comfortable with the tool and want to spend more time using it. That has been my experience so far.</p><p><strong>Tauri is much more mature than when I released the first macOS version of Ensō.</strong> I spent weeks getting the previous version to build properly on Mac with notarisation, provisioning profiles and undocumented AppStore Connect APIs. Now, most of the things just work (sometimes with a bit of scripting, which is where Claude Code turned out to be indispensable).</p><p>I'm not an \"IndieHacker\", I'm not in a rush, I'm a wannabe-carpenter (<a href=\"https://untested.sonnet.io/notes/projects-and-apps-i-built-for-my-own-well-being/\">Brief History of Galician Carpentry</a>) and Ensō happens to be made of stuff that can be worked in a carpentry-like manner. The small feature set means I can afford to take time to work on this with enough care, which I hope shows in the final product.</p><p><strong>Building a theme switcher can be a weirdly complex problem</strong> (if you complicate it well enough). The difficult part was letting users set themes for dark/light/sync with OS mode, with previews, making it obvious when changes are saved, all in a single piece of UI, with max 2-3 clicks.</p><p>Most of my attempts at this resulted in something that looks more than the Dwarf Fortress GUI than a simple theme picker. I understand now why almost no one is doing this and why the few who do split the UI in several steps.</p><p><strong>I'm still happy with using a browser as the text rendering engine.</strong> Especially with Safari, the amount of control over typography is just excellent (e.g.  ).</p><p>I wish there was an easy way of getting the native accent colour from the OS, but that's not possible at the moment.  can be customised, but not read.</p><p><strong>I'm not planning to remove the free web version of Ensō.</strong> I want to get paid for my work, but people reach out to me and buy it with virtually no marketing. I'm hopeful, even optimistic that the trust I've earned so far, as well as the quality of the final product, will be enough for it to grow slowly but steadily.</p><p>That's all for today. Thanks for reading!</p>","contentLength":9803,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44421776"},{"title":"The provenance memory model for C","url":"https://gustedt.wordpress.com/2025/06/30/the-provenance-memory-model-for-c/","date":1751275514,"author":"HexDecOctBin","guid":176651,"unread":true,"content":"<p>[The wordpress markdown inclusion does a very bad job, it seems, there have been some idiotic formatting errors. I hope that these are fixed, now.]</p><div><p>A years-long effort led by <a href=\"https://www.cl.cam.ac.uk/~km569/\">Kayvan\nMemarian</a> and <a href=\"https://www.cl.cam.ac.uk/~pes20/\">Peter\nSewell</a> from Cambridge University,\nUK, <a href=\"https://www.tugraz.at/institute/ibi/institute/team/\">Martin\nUecker</a> from Graz\nUniversity of Technology, Austria, and\n<a href=\"https://icps.icube.unistra.fr/index.php?title=Jens_Gustedt\">myself</a>\n(from ICube/Inria, France) has guided the C community to accept a\ncommon vision of so-called , defining how to trace\nthe origins of pointer values through program execution.  Our\n<a href=\"https://www.open-std.org/JTC1/SC22/WG14/www/docs/n3057.pdf\">provenance-aware memory object model for\nC</a>\nprovides a precise mathematical specification, in place of the\nambiguity of these aspects of the current <a href=\"https://www.iso.org/standard/82075.html\">C\nstandard</a>. It has also\nstimulated and informed discussion of provenance in the broader C,\nC++, Rust, and compiler communities.</p><p>This work has finally resulted in the publication of an international\nstandard, Technical Specification <a href=\"https://webstore.iec.ch/en/publication/107524\">ISO/IEC TS\n6010</a> (edited by <a href=\"https://www.linkedin.com/in/henrykleynhans/\">Henry\nKleynhans</a>, Bloomberg,\nUK).  With the goal of making modern information systems safer and\nmore secure, this official technical specification provides direction\nto all stakeholders in the industry such that they can converge their\nplatforms and tools.</p></div><div><h2>Pointer aliasing and program optimization</h2></div><div><p>We say that two pointer values  and  during the execution of a\nprogram  if they point to the same object in\nmemory. To see that the question if two pointers alias\nhas an influence on the optimization of code, let’s consider the\nfollowing simple example of an iterative function.</p></div><div><p>It implements an approximation algorithm for the reciprocal  of the value  which doesn’t use division.</p></div><div><pre title=\"\">// Reciprocal approximation\n//\n// This interface is horrific, don't use it.\n// This just serves as an artificial example\n// for this article.\n\nconstexpr double ε  = 0x1P-24;\nconstexpr double Π⁻ = 1.0 - ε;\nconstexpr double Π⁺ = 1.0 + ε;\n\nvoid recip(double* aₚ, double* řₚ) {\n    for (;;) {\n        register double Π = (*aₚ)*(*řₚ);\n        if ((Π⁻ &lt; Π) &amp;&amp; (Π &lt; Π⁺)) {\n            break;\n        } else {\n            (*řₚ) *= (2.0 - Π);\n        }\n    }\n}\n\n</pre></div><div><p>The function receives a pointer to a second value  with a\nrough approximation for . It then iteratively approaches  within\na chosen precision : the current values  and  are multiplied\ninto a value  and if that value is sufficiently close to  the\niteration stops. If it is not,  is corrected and the loop\ncontinues.</p></div><div><p>What is interesting for our context of aliasing is that this function\nhas two pointer arguments that both point to a value of the same type\n. One of these pointer targets  is loaded from memory,\nmodified and stored at each iteration. In total, the non-optimized\nfunction as specified above in each iteration has</p><ul><li>3 load and 1 store operations,</li></ul></div><div><p>So loads and stores from memory make up 4 of about 10 operations in\ntotal.</p><p>But wait, can’t this be done better? Yes, obviously, a much better\nversion of this could look as follows.</p></div><div><pre title=\"\">void recip⁺(double* aₚ, double* řₚ) {\n    register double a = *aₚ;\n    register double ř = *řₚ;\n    for (;;) {\n        register double Π = a*ř;\n        if ((Π⁻ &lt; Π) &amp;&amp; (Π &lt; Π⁺)) {\n            break;\n        } else {\n            ř *= (2.0 - Π);\n        }\n    }\n    *řₚ = ř;\n}\n</pre></div><div><p>That is, we load  and  once, at the beginning, and then only\nupdate  when we have finished our computation. The hope here is\nthat these new local variables use “hardware registers” that can be\nused directly by the processor, without going through loads and stores\nfrom and to the program memory.</p><p>So roughly the optimized function saves us 40% of the operations.\nThis means that the optimized function is in general much faster and\nachieves its goal by wasting less energy.</p><p>Unfortunately no C compiler can do this optimization automatically:</p></div><div><blockquote><p><em>The functions  and  and not equivalent.</em></p></blockquote></div><div><p>We didn’t see this, yet, because we failed to discuss an important\nfeature of the original program; the pointers  and  may either\npoint</p><ul></ul><p>Indeed, our optimized function  only covers the first of these\npossibilities and not the second: the second case provides a\ncompletely different algorithm for which I wouldn’t know any use or\nproperties. But since the compiler can’t know which of these cases we\nhave in mind (maybe both?) it cannot do an optimization that excludes\nthe second.</p><p>In general, loads and stores from memory are expensive operations, so\na lot of effort in modern compiler frameworks goes into so-called\nalias analysis, that is in an detailed analysis of which pointers\nmay alias each other (or not). Such alias analysis then can be used to\nmechanically prove whether a specific optimization is feasible or not.</p></div><div><blockquote><p><em>Good alias analysis of pointer targets is crucial for\noptimization in modern compilers.</em></p></blockquote></div><div><p>In the case of  we see that our specification is not precise\nenough to know whether or not the second case (where the pointers\nalias), can be excluded. We also see that misguided assumptions about\npointer aliasing can result in implementing a completely different\nalgorithm. Or, in other words, they may result in a severe bug.</p></div><div><blockquote><p><em>Mislead aliasing assumptions result in bugs.</em></p></blockquote></div><div><p>In one of our\n<a href=\"https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2363.pdf\">papers</a>,\nwe provide a long list of examples where pointer aliasing can lead to\ndifferent interpretations by users and compilers, see also <a href=\"https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2311.pdf\">this paper\nhere</a>.</p><p>Note also that a previous attempt by the C committee to introduce\nstricter aliasing rules had turned into years of flame wars between\nalienated parts of the user community, compiler builders and committee\nmembers.  A communication disaster that was very harmful for the\nindustry in general and to the trust of the community in the C\ncommittee in particular.</p></div><div><p>Modern compilers have several mechanisms that help to deduce more\ninformation about the intended use cases for pointers and from there\nto provide correct optimizations. These mechanisms are</p><ul><li>Type-based alias analysis.</li><li>Flow-based alias analysis.</li></ul><p>For type-based alias analysis, C takes a relatively simple approach,\nat least on the surface level.</p></div><div><blockquote><p><em>Two pointers that point to objects of different non-character type\nare supposed not to alias.</em></p></blockquote></div><div><p>So, for our function  above, it would have been sufficient to\nhave arguments pointing to differently typed objects. Then, an\noptimization similar to  would have been valid.</p><p>… notice the “supposed” in the phrase above. In C, it is entirely\nthe programmers responsibility to ensure that two differently typed\nobjects are in fact different. If you are casting spells to convert\npointers from one target type to another, you have to prove for\nyourself that you may pass such pointers as arguments to functions\nwithout causing problems, there. You are completely on your own.</p><p>Flow-based analysis in C is supported by several tools</p><ul><li>The  storage class.</li><li>The  pointer qualifier.</li><li>The  qualifier on the target type.</li></ul></div><div><p>The first ensures that the address of a so-declared object is never\ntaken. And in C, when there are no pointers, there is no\naliasing. Although this is easy enough to use and\nprovides feedback if one tries to obtain the address of such an\nobject, it seems that this is much less used than it should.</p></div><div><p>The second is much more subtle. If a pointer parameter of a function\nis declared with , every user of the function has to ensure\nthat the pointer value is the unique view on the object from within\nfor each call to the function. The definition in the standard of that\nfeature is obscure and, again, it has the same drawback as type-based\nalias analysis in C: you are completely on your own.</p><p>The third is a feature that imposes that each access to a \nqualified object reloads the object from memory, regardless what the\ncompiler might know about its previous value. Using this feature\nexcessively is like stepping on the break: basically all optimization\nopportunities concerning that object are lost.</p><p>As extensions, some compilers also have command line flags that help\nthe user to steer through. For example gcc has the options\n and  to switch aliasing\nanalysis as defined by the C standard on or off.</p></div><div><h3>And then, there is provenance …</h3></div><div><p>In the current C standard provenance is only a hidden concept, the\nword “provenance” appears in it exactly 0 times. Nevertheless, it\nmarks an important assumption for existing C, namely that somehow\ncompilers are allowed to assume that two pointers don’t alias when\nthey are known to come from different “sources” in the program.</p></div><div><p>For programmers, the current state is a disaster. In many cases there\nis no way to know which assumptions a compiler makes:</p><ul><li>There is no way (but  and types, see above) to claim that\ntwo pointers never alias.</li><li>It is difficult (but for ) to specify that they might\nalias such as to force optimization to be more restricted,\neither. This even in situations where, for example, the base types\nare in fact different, but where the user knows that the underlying\nobject might still be the same.</li></ul></div><div><p>The concept of provenance created many difficulties because it was left undefined exactly what it meant, so each compiler implementer had their own ideas how to assert that two pointers do (or don’t) alias, and how to use that in optimization.</p></div><div><p>When digging deeper we observed several problem spots, and it took us\nin fact a long time to understand the different assumptions.</p></div><div><p>The first question that comes up is to figure out at which granularity\nwe can do or want to do aliasing analysis. At the level of bytes,\nwords, basic type objects, structure objects, memory allocations, or,\nthe whole address space?</p><p>The main problem here is that in C we can move from one pointer\naddress to the next via arithmetic. The easiest examples are\narrays. If we have access to an element in the middle of an array, we\ncan move back and forth by simply adding or subtracting an integer\nvalue to the corresponding pointer value. How would we (the\nprogrammers) or the compiler know if two pointers point into the some\narray but at different elements?  What happens if we start to step\nbackwards from such pointers?</p><p>This picture becomes even more confusing, if we allow pointer\narithmetic on the byte level. In C, the  macro allows us to\naccess arbitrary members of a structure by using such pointer\narithmetic.</p></div><div><h4>Pointer equality and object life time</h4></div><div><p>When you are programming with pointers in C, you are hopefully aware\nof the “dangling pointer” problem. This refers to the fact that for\nexample a pointer to a local variable becomes invalid once you leave\nthe scope (e.g. function) where it is declared.</p><p>For aliasing analysis, this becomes even more complicated: not only\ndoes a pointer become invalid, the address of the dead object may be\nreused in a different context. So two pointers may even be equal, one\npointing to a dead object, the other to a new one.</p></div><div><p>There is another situation where two pointer values are the same, but\nwhere they talk about two different objects. This occurs when two\narrays,  and , say, happen to live in adjacent memory\nlocations. If  has  elements, the pointer value  (the\nend of ) might be the same as  (the start of ).  So then\nwe have two pointer values that are the same but that have quite a\ndifferent meaning for the programmer.</p></div><div><h4>Information flow for addresses</h4></div><div><p>Another concept influences aliasing analysis a lot, namely the\npossibility that a pointer value escapes from a limited scope of the\nprogram (e.g the address of a local variable) and becomes known in\nother parts of the program.</p><p>For example a variable that is declared  in a block is usually\nonly visible there. In general the compiler masters very well which\npointers may alias with a pointer value that corresponds to the\naddress of that  variable. If the address of that variable\n“escapes” from the local scope, for example by passing it as an\nargument to a function, the address could come back into the same\nscope via unknown paths that are difficult to control.</p></div><div><p>There are surprisingly many ways that pointer values as a whole or\nparts of them may escape from one context and reappear in\nanother. Among them are</p><ul><li>manipulations of the byte-representations of pointers for example\n<ul></ul></li><li>manipulation of integers that are the result of pointer-to-integer\nconversions.</li></ul></div><div><p>Perhaps surprisingly the latter is an important point that had to be\nhandled carefully in our proposal. The reason is that pointer\nbit-manipulations are used in contexts where the available memory\nrelatively constrained, such as systems programming for\nexample. Bit-manipulation tricks are then used to save on the size of\ndata structures. This has not only the advantage that storage space\nfor the data can be reduced, but also that a reduced size also\nimproves the performance for data that are used intensively due to\nimproved use of processor caches.</p><p>A famous example is the XOR trick for doubly linked lists, where a\ndata structure stores the XOR of the bit patterns of two pointer\nvalues. So for that particular use we have an integer (here of type\n) that contains information that comes from two different\npointers, that then is used to sometimes reconstruct one or the other\nof these pointer values.</p></div><div><pre title=\"\">typedef struct elem elem;\nstruct elem {\n    uintptr_t both;\n    double data;\n};\n\nvoid elem_store(elem* e, elem* prev, elem* next, double val) {\n    (*e) = (elem){\n        both = (uintptr_t)prev ^ (uintptr_t)next,\n        data = val,\n    };\n}\n</pre></div><div><p>Such a data structure can then be used for elements of a list that can\nbe traversed consistently forward and backward, but the memory\nfootprint is only that of a simply-linked list:</p></div><div><pre title=\"\">elem* elem_next(elem const* e, elem* prev) {\n    return (elem*)((uintptr_t)prev ^ e-&gt;both);\n}\n\nelem* elem_prev(elem const* e, elem* next) {\n    return (elem*)((uintptr_t)next ^ e-&gt;both);\n}\n</pre></div><div><p>As you can see, besides their names these two functions are completely\nidentical, and I can’t imagine any compiler being able to track the\norigin of a pointer that one of these two functions returns.</p></div><div><h3>Tracking provenance through integers?</h3></div><div><p>The fact that in C pointer values are closely related to integers\ncreates a lot of confusion. In the case of aliasing analysis, the lack\nof separation between those terms has it that we have to integrate a\nmodel of information flow of pointer values (or just some bits or\nbytes of them) through integers and back to pointer values.</p><p>When Peter and Kayvan started their investigations, they had to\nconsider different possibilities, one of them being to track\ninformation about pointers through such a chain of conversions. It\nturned out, that such a model would be possible (they provided a sound\nspecification for it) but that it would come at an important cost. For\ncode as for the XOR trick used above a pointer value (the result of a\ncall to  for example) would have two origins ( and\n in a call to ) and not only one. Since such\ndifferent origins could then accumulate if we do more operations,\nbasically an integer and a pointer derived from it, could have an\narbitrary number of origins.</p></div><div><p>Such a model with multiple origins of pointers seemed complicated and\nimpractical. Complicated for users, because they would have to be\naware that information about pointers could be used in surprising ways\nby a sophisticated compiler. Impractical for compilers because keeping\ntrack of all possible in-flow of information would result in a\ncombinatorial explosion of the state of the abstract machine.</p><p>Presented with such a complication, one possibility would have been to\njust “forbid” using pointers in that way. We could have stated\nsomething along the lines of “<em>if a pointer has several origins, the\nbehavior is undefined</em>” and thus leaving everything (the “undefined”\npart) to compiler implementations. But because these situation appear\nin real life code, this would have left these important parts\nnon-portable between different compilers and architectures.</p><p>So the overall conclusion was not to ban such usage of pointers\nthrough integers, but to formalize it and label such pointers as\n, that is that no user has to fear that compilers will\npresent them with optimization that uses knowledge of such information\nflow, and no compiler has the pressure to optimize such code, either.</p></div><div><p>The provenance model that we came up with, and which is at the base of\nTS&nbsp;6010, tries to take all of these aspects into account with the\ngoal to provide something that at the same time can easily be referred\nto by users and compiler implementers. It provides some compromise\nbetween the expectations of the two communities in the sense that it\ndoes not leave all the liberty they might dream of to compiler\nproviders for optimization, and it still has some sort of complexity\nand difficulty for users.</p><p>In the following we present the most important parts of that model.</p></div><div><p>As said above, we have to agree upon the granularity of memory\naccesses for which aliasing of pointers will be considered. When we\ncombed through the existing standard (C11 an C17 at the time) we\nquickly noticed that there was not even agreement within that\nstandard. When it talks about what is found at the other end of a\npointer, it talks about “object”, “space”, “memory” or\n“storage” and even some combinations of these.</p></div><div><p>It seemed important to us to emphasize that pointers and addresses are\nalready an abstraction that does not necessarily denote a physical\ndevice: most modern platforms nowadays form so-called virtual address\nspaces. Such “virtual addresses” then are in general transformed by\nlow-level tools to “physical addresses” that represent concrete memory\nhardware. To make that distinction clearer we decided to use the term\n“storage” in most places where one of the terms noted above appear.</p><p>Another important observation to have is that we even have to talk\nabout things that do not have an address. For example if we declare a\nvariable width the  storage class, we cannot receive a\npointer to that object and the whole point is that it is not necessary\nfor the compiler to realize this variable in the main memory.</p><p>Then, the aspect of temporality also comes into play. A chunk of\nmemory that is obtained for example by a call to  can be\nreturned to the system by calling  and then might again be\nserved by another call to . It is important that the two\nentities to which the pointer refers are seen as completely different\nand that the fact that they reside in the same memory location is a\nsimple coincidence.</p></div><div><p>For the granularity, we decided to go on the level of maximal region\nin which “legally” a C program could operate. Since inside any\nallocation or declared object all bytes are reachable by\ncharacter-pointer arithmetic, we decided to take this as the level of\ngranularity. Therefore</p></div><div><blockquote><p>A  is the maximal region of storage that is\nprovided by</p><ul><li>an allocation ( and similar),</li><li>an object with temporary lifetime.</li></ul></blockquote></div><div><p>Note here that the second point talks about the definition of a\nvariable, not its declaration. For local variables these two coincide,\nbut for file scope variables (outside any function) there can be\ndeclarations (with ) that are not definitions. The definition\nof a variable is always unique and specifies  it is located,\nnamely in our terminology here, where the storage instance that\nrepresents it comes to be.</p></div><div><blockquote><p>If a storage instance is addressable, the conversion of a pointer to\nits start to the type  points into an array, called\nits , where each element is one byte of the storage\ninstance.</p></blockquote></div><div><p>By that definition, conversions of pointers to character types and to\n are defined and it is uniquely prescribed how arithmetic on a\nbyte level works.</p></div><div><blockquote><p>A storage instance has a lifetime that expands</p><ul><li>from the allocation (typically ) to the deallocation\n(typically )</li><li>from the definition of the variable to the point where the block of the\ndefinition left (for a VLA)</li><li>from the point of entering the block of the definition until it is\nleft (for other variables with automatic storage duration)</li><li>from the point of entering the innermost block that contains the\nexpression until that scope is left (for compound literals with\nautomatic storage duration)</li><li>from the start of the program execution until its end (for \nobjects)</li><li>from the start of the thread execution until its end (for\n objects)</li><li>during the evaluation of the full expression that contains it (for\nobjects of temporary lifetime).</li></ul></blockquote></div><div><p>So in addition to the “where” above, this definition describes \nthe storage instance that represents an object comes to be and ceases\nto exist.</p><p>If you are not familiar with all the concepts in that item list, just\nignore these. The importance here really is to make it clear for the\nfeatures that you use in your program and know about, that in general\ntheir lifetime is limited and that any such allocation or definition\ngives rise to one single storage instance per context in which the\nconstruct is met.</p><p>For example in the following code we see three storage instances in\naction</p></div><div><pre title=\"\">{\n    size_t n  = 32;\n    double (*A)[n][n] = malloc(sizeof(*A));\n    ...\n    free(A);\n    A = nullptr;\n    ...\n}\n</pre></div><div><ul><li>The one for , a variable of integer type, and for which the\nlifetime starts when we enter the block at the  and that ends\nwhen we leave it at .</li><li>The storage instance that is allocated by the call to  and\nthat is deallocated by the call to .</li></ul></div><div><p>But then there is also a storage instance for the pointer  itself\nthat, similar to , lives during the execution of the block. In\nparticular, after we freed the contents of  we may still access it\nto set it to a null pointer value.</p><p>For a case where the notion of storage instance is perhaps a bit less\nintuitive we note that calls to  are a bit peculiar with\nrespect to that definition. In a call</p></div><div><pre title=\"\">void* p = realloc(q, 77);\n</pre></div><div><p>we first have the storage instance to which  points. Then, if the\ncall is successful, that old storage instance is released and a\npointer to a new storage instance is stored in . Even if these two\npointers are identical (possibly the storage instances start a the\nsame address) they are nevertheless considered as two different\nentities.</p><p>With the term storage instance immediately comes the notion of\nprovenance.</p></div><div><blockquote><p>The  of a valid pointer value is the storage instance\ninto which (or one beyond which) the pointer value points.</p></blockquote></div><div><p>With the exception of one particular border case (<a href=\"https://gustedt.wordpress.com/2025/06/30/the-provenance-memory-model-for-c/#ambiguities\" title=\"Ambiguities\">see\nbelow</a>) the provenance of a valid pointer\nvalue is unique.</p></div><div><p>To be useful in an aliasing model, the concept of an address space is\nnot provided with enough precision in the current C standard. We need\nto talk consistently about addresses, how pointers convert, compare or\nrelate.</p><p>The model we came up with, has the following properties:</p></div><div><ul><li>To each object pointer value corresponds an abstract address that is\na positive integer value.</li><li>Bytes within an addressable storage instance have abstract addresses\nthat increase from start to end.</li><li>If the distinct storage instances  and  are alive at a common\npoint in time, the abstract addresses of all bytes of  are either\nall smaller or all greater than the abstract addresses of all bytes\nof .</li><li>Two pointer values are equal if they correspond to the same abstract\naddress.</li><li>One pointer value is smaller than another pointer value, if both\npoint into the same storage instance and if the address of the first\nis smaller than the one of the second.</li><li>If the platform is able to represent all addresses in some integer\ntype, the type  is provided and a conversion from a\npointer to that type provides the abstract address.</li><li>Conversion from pointers to any integer type are consistent with\nthat mapping to abstract addresses.</li></ul></div><div><p>This model falls short from defining a “flat” address space:</p></div><div><ul><li>Arithmetic on pointers and arithmetic on abstract addresses need not\nto be consistent.</li><li>Even within the same storage instance, the increase from one byte to\nnext may not be one, and may not even be uniform.</li><li>The type  may not exist.</li><li>Conversion to integer types that are too narrow has undefined\nbehavior.</li></ul></div><div><p>The reasons for only having such a lax definition are simple, for each\nof the weird properties in the list there are examples that make it\nnecessary.  In particular, there are platforms with segmented memories\nthat have “bumps” in the address space, and platforms that pack\nadditional bits into pointer values that are not related to the\ncorresponding abstract address.</p></div><div><h3>Exposure and synthesis of pointer values</h3></div><div><p>Another observation is crucial for our model: most aliasing analysis\nisn’t perfect. That is, compilers as well as programmers have limits\nof which tracks of the pointer information they can follow. For\nexample the XOR trick shows that a pointer value can have several\norigins. In all we have to foresee a mechanism that describes the\nboundaries of the assumption that a compiler may make on one hand, and\nthe guarantees that a programmer has to give on the other.</p><p>The mechanism with which we came up has two sides</p></div><div><ul><li>A pointer value is  if information about its abstract\naddress or its in-memory representation leaks to the outside or to\ndistant parts of the program.</li><li>A pointer value is  if it is assembled from outside\ninformation, from byte information or from integer values.</li></ul></div><div><p>The goal is to describe that mechanism in a way such that (in\nprinciple) some auxiliary information could be added to each pointer\nvalue that would either allow</p></div><div><ul><li>compilers and users to establish aliasing properties of a set of\npointer values, or</li><li>easily come to a consensus for situations for which such analysis is\nabandoned an may not be assumed.</li></ul></div><div><p>Let’s now have a look at a possible normative text as it should be\nintegrated into the C standard at some point</p></div><div><blockquote><p>A storage instance becomes  when a pointer value  of effective\ntype  with this provenance is used in the following contexts:</p><ul><li>Any byte of the object representation of  is used in an\nexpression.</li><li>The byte array pointed-to by the first argument of a call to the\n library function intersects with an object representation of\n.</li><li> is converted to an integer.</li><li> is used as an argument to a  conversion specifier of the\n family of library functions.</li></ul></blockquote></div><div><p>The idea of the first bullet point is that if we read bytes of the\nobject representation of a pointer, cascaded  control flow\ncould be used to reconstruct pointers and thus jeopardize any aliasing\nanalysis.  But what we also didn’t wanted, is that “normal” operations\nthat programmers do on pointer representations as a whole would have\nsimilar effect as access.</p><p>Therefore notice that  or similar functions do not appear in\nthe list above; as long as we use it to copy pointer representations\nas a whole, provenance can simply be transferred.  For example, using\n on structure objects that have pointer members is fine: such\nan operation copies the whole pointer without exposing individual\nbytes. So we simply assume that the provenance information is\ntransferred at the same time to the target structure.</p><p>We also don’t want that small changes in the way that we look at a\npointer representation has an influence on aliasing\nanalysis. Therefore we add the following paragraph</p></div><div><blockquote><p>Nevertheless, if the object representation of  is read through an\nlvalue of a pointer type  that has the same representation and\nalignment requirements as , that lvalue has the same provenance\nas  and the provenance does not thereby become exposed.</p></blockquote></div><div><p>Here the term “same representation and alignment” covers for example\nthe possibility to look at</p><ul><li>a pointer that has different qualifiers than the original,</li><li>where one type would be a signed type and the other unsigned, or</li><li>one would be a structure and the other would be another structure\nthat sits at the beginning of the first.</li></ul></div><div><p>Also exposure is meant to be a one-way street, once exposed we will\nnever know where the information about the pointer could creep into\nour program execution.</p></div><div><blockquote><p>Exposure of a storage instance is irreversible and constitutes a\nside effect in the abstract state machine.</p></blockquote></div><div><p>The inverse operation that uses other information to produce a pointer\nlooks as follows:</p></div><div><blockquote><p>A pointer value  is synthesized if it is constructed by one of the\nfollowing:</p><ul><li>Any byte of the object representation of  is changed\n<ul><li>by an explicit byte operation,</li><li>by type punning with a non-pointer object or with a pointer object\nthat only partially overlaps,</li><li>or by a call to  or similar function that does not write\nthe entire pointer representation or where the source object does\nnot have an effective pointer type.</li></ul></li><li>The object representation of  intersects with a byte array\npointed-to by the first argument of a call to the  library\nfunction.</li><li> is converted from an integer value.</li><li> is used as an argument to a  conversion specifier of the\n family of library functions.</li></ul></blockquote></div><div><p>Additionally, we always require that the storage instance that is\nsynthesized has been exposed before.</p></div><div><p>With all of that the situation about <a href=\"https://gustedt.wordpress.com/2025/06/30/the-provenance-memory-model-for-c/#pointer-equality-and-object-life-time\">adjacent storage\ninstances</a> still remains. That\nis, a situation where two arrays  and  are adjacent in\nmemory. Let’s suppose the two arrays have two elements, each, and that\nthe base type has four bytes:</p></div><div><table><tbody><tr></tr></tbody></table></div><div><p>When we are taking addresses the following are valid expressions:\n which points to the element just after the array  and\n which points to the beginning of . So if  and  are\nadjacent objects, the pointer value of the first expression is exactly\nthe same as for the second. So both represent different semantics but\nwith an abstract address that is the same, the byte address of byte\n.</p></div><div><p>So far, so good. Using these pointers with clearly indicated semantics\ndoesn’t pose a problem for aliasing analysis. In particular in our\nmodel a pointer value as given above always has a unique provenance. A\nproblem arises only if</p></div><div><ul><li>The storage instance of  and  have both been exposed, by a\nconversion to , say.</li><li>A pointer  is synthesized that corresponds to the byte address of\n by converting some  value back.</li></ul></div><div><p>Then we are in the dilemma that  could have both provenances, that\nof  or that of .</p><p>Already from the length of the text that is needed to describe the\nsituation you might guess that this is a very rare situation, the\neasiest remedy against its difficulties is just not to have it in the\nfirst place. But in the case that it arises we have to foresee a\nmechanism that is consistent with the model. Since there is generally\nno additional information available that could guide the compiler to\nsee which semantics the programmer meant, the semantics are deduced\nfrom the usage of the pointer value:</p></div><div><blockquote><p>A synthesized pointer value  with two possible provenances  or\n is assumed to have the one provenance among the two that is\nconsistent with its use in expressions.</p></blockquote></div><div><p>That is for example, if we use  in</p></div><div><ul><li>,  or  it is assumed that the provenance is the one\nof </li><li> or  it is assumed that the provenance is the one of\n.</li></ul></div><div><p>A use as in  that has the same value  gives no indication of a\ndirection in which we want to step through the array. So it does not\nfix a choice of one provenance or the other. But  which\nresolves to  has the provenance of  and similarly\n has the one of .</p><p>So if and when you have to use that marginal constructs, make sure\nthat you give clear indications to the compiler into which of the two\nstorage instance you want to walk.</p></div><div><p>If you were brave enough to follow this article up to here, you\nprobably deserve some reassuring words such that you are not left\nalone with a complicated web of choices and interrelationships between\nparts of your code that are impossible to master. In the contrary, our\nmodel provides a very simple method to guarantee sound aliasing\nanalysis by compilers of your every day code:</p></div><div><blockquote><p><em>Do not expose pointer values</em> …</p></blockquote></div><div><p>… if you can avoid it. And in most cases you can. In particular avoid</p></div><div><blockquote><ul><li>taking the address of variables (maybe use  to be sure),</li><li>casts of pointer values to and from integers,</li><li>accessing individual bytes of pointer targets (AKA coversion to\ncharacter pointers),</li><li>conversions of pointer values to any other unrelated pointer type,</li><li>accessing individual bytes of byte-representations of pointer values,</li><li>printing pointer values with  or by using ,</li><li>using the end address of an array to walk backwards into the array,</li></ul></blockquote></div><div><p>Obviously, these features are important for C and contribute for a lot of\nits power. But you should only use them when (and where) you master\nthem in all of their consequences. For example, if in the context of\nsystem’s programming you need the XOR trick for your doubly-linked\nlist, that is fine as long as you are aware, that this might cost you\nsome other optimization opportunities. Or if you are debugging your\ncode, printing pointer values with  can be crucial, but you should\nmake sure that you disable all traces of such printing when you\ncompile for production.</p><p>Generally, using more modern features of C will help your compiler to\nprovide you with more efficient and safer executables. For example, in\naddition to the above</p></div><div><ul><li>not using pointer parameters on functions when you are only\ninterested in the value (our  function is a bad example),</li><li>using  qualification wherever you may,</li><li>not using integer zero as a null pointer,</li><li>using signed and unsigned integer types consistently,</li></ul></div><div><p>all contribute to help your C compiler to do at what it’s best:\nnagging you about things that you might have overlooked.</p></div>","contentLength":32733,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44421185"},{"title":"Want to meet people, try charging them for it?","url":"https://notes.eatonphil.com/2025-06-28-want-to-meet-people-charge-them.html","date":1751263990,"author":"ArneVogel","guid":176471,"unread":true,"content":"<p>I have been blogging consistently since 2017. And one of my goals in\nspeaking publicly was always to connect with like-minded people. I\nalways left my email and hoped people would get in touch. Even while\nmy blog and twitter became popular, passing 1M views and 20k\nfollowers, I basically never had people get in touch to chat or meet up.</p><p>So it felt kind of ridiculous when last November I <a href=\"https://eatonphil.com/chat.html\">started charging\npeople $100 to chat</a>. I mean, who am\nI? But people started showing up fairly immediately. Now granted the\nmoney did not go to me. It went to an education non-profit and I\nmerely received the receipt.</p><p>And at this point I've met a number of interesting people, from VCs to\nbusiness professors to undergraduate students to founders and everyone\nin between. People wanting to talk about trends in databases, about\nhow to succeed as a programmer, about marketing for developers, and so\non. Women and men thoughout North America, Europe, Africa, New\nZealand, India, Nepal, and so on. And I've raised nearly $6000 for\neducational non-profits.</p><p>How is it that you go from giving away your time for free and getting\nno hits to charging and almost immediately getting results? For one,\nevery person responded very positively to it being a fundraiser. It\nalso helps me be entirely shameless about sharing on social media\nevery single time someone donates; because it's such a positive thing.</p><p>But also I think that in \"charging\" for my time it helps people feel\nmore comfortable about actually taking my time, especially when we\nhave never met. It gives you a reasonable excuse to take time from\nan internet rando.</p><p>On the other hand, a lot of people come for advice and I think giving\nadvice is pretty dangerous, especially since my background is not\nsuper conventional. I try to always frame things as just sharing my\nopinion and my perspective and that they should talk with many others\nand not take my suggestions without consideration.</p><p>And there's also the problem that by charging everyone for my time\nnow, I'm no longer available to people who could maybe use it the\nmost. I do mention on my page that I will still take calls from people\nwho don't donate, as my schedule allows. But to be honest I feel less\nincentivized to spend time when people do not donate. So I guess this\nis an issue with the program.</p><p>But I mitigated even this slightly, and significantly jump-started the\nprogram, during <a href=\"https://eatonphil.com/30.html\">my 30th birthday</a> when\nI took calls with any person who donated at least $30.</p><p>Anyway, I picked this path because I have wanted to get involved with\nhelping students figure out their lives and careers. But without a\ndegree I am literally unqualified for many volunteering programs. And\nI always found the time commitments for non-profits painful.</p><p>So until starting this I figured it wouldn't be until I retire that I\nfind some way to make a difference. But ultimately I kept meeting\npeople who were starting their own non-profits now or donated\nsignificantly to help students. Peer pressure. I wanted to do my part\nnow. And 30 minutes of my time in return for a donation receipt has\nbeen an easy trade.</p><p>While only raising a humble $6,000 to date, the <a href=\"https://eatonphil.com/chat.html\">Chat for\nEducation</a> program has been more\nsuccessful than I imagined. I've met many amazing people through\nit. And it's something that should be easy to keep up indefinitely.</p><p>I hope to meet you through it too!</p>","contentLength":3340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44419986"},{"title":"LetsEncrypt – Expiration Notification Service Has Ended","url":"https://letsencrypt.org/2025/06/26/expiration-notification-service-has-ended/","date":1751258979,"author":"zdw","guid":176470,"unread":true,"content":"<p>Since its inception, Let’s Encrypt has been sending expiration notification emails to subscribers that have provided an email address to us via the ACME API. This service ended on June 4, 2025. The decision to end the service is the result of the following factors:</p><ol><li>Over the past 10 years more and more of our subscribers have been able to put reliable automation into place for certificate renewal.</li><li>Providing expiration notification emails means that we have to retain millions of email addresses connected to issuance records. As an organization that values privacy, removing this requirement is important to us.</li><li>Providing expiration notifications costs Let’s Encrypt tens of thousands of dollars per year, money that we believe can be better spent on other aspects of our infrastructure.</li><li>Providing expiration notifications adds complexity to our infrastructure, which takes time and attention to manage and increases the likelihood of mistakes being made. Over the long term, particularly as we add support for new service components, we need to manage overall complexity by phasing out system components that can no longer be justified.</li></ol><p>For those who would like to continue receiving expiration notifications, we recommend using a third party service such as <a href=\"https://redsift.com/pulse-platform/certificates-lite\">Red Sift Certificates Lite</a> (formerly Hardenize). Red Sift’s monitoring service providing expiration emails is free of charge for up to 250 certificates. More monitoring options can be found <a href=\"https://letsencrypt.org/docs/monitoring-options/\">here</a>.</p><p>We have deleted the email addresses provided to Let’s Encrypt via the ACME API that were stored in our CA database in association with issuance data. This doesn’t affect addresses signed up to mailing lists and other systems. They are managed in a separate ISRG system unassociated with issuance data.</p><p>Going forward, if an email address is provided to Let’s Encrypt via the ACME API, Let’s Encrypt will not store the address but will instead forward it to the general ISRG mailing list system unassociated with any account data. If the email address has not been seen before, that system may send an onboarding email with information about how to subscribe to various sources of updates.</p><p>If you’d like to stay informed about technical updates and other news about Let’s Encrypt and our parent nonprofit, <a href=\"https://abetterinternet.org\">ISRG</a>, based on the preferences you choose, you can sign up for our email lists below:</p>","contentLength":2358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44419496"},{"title":"Bought myself an Ampere Altra system","url":"https://marcin.juszkiewicz.com.pl/2025/06/27/bought-myself-an-ampere-altra-system/","date":1751258329,"author":"pabs3","guid":176355,"unread":true,"content":"<p>In the hunt for a development machine, I got to the next phase. I did some\nshopping, and there it is: my own Ampere Altra-based&nbsp;system.</p><p>As you may have read in <a href=\"https://marcin.juszkiewicz.com.pl/2025/06/20/the-hunt-for-a-development-machine/\">my previous post</a>,\nI used several AArch64 systems for local development. And the latest one, an\nApple MacBook Pro, is nice and fast but has some limits — does not support 64k\npage size. Which I need for my&nbsp;work.</p><p>So I have decided to buy myself an Ampere Altra system. As cheap as&nbsp;possible.</p><p>The only part I needed to buy brand new was a motherboard. And the only\n“affordable” one was <a href=\"https://www.asrockrack.com/general/productdetail.pl.asp?Model=ALTRAD8UD-1L2T\">AsrockRack -</a>,\nwhich was a product for data centres (so I was&nbsp;told).</p><p>Next, a used processor. At first, the idea was to buy a Q64-22 (64 cores,\n2.2 GHz clock), but when the seller on eBay was not responding, one of my friends\ndecided to upgrade his Altra systems and offered me a Q80-30 (80 cores, 3.0 GHz&nbsp;clock).</p><p>The  requires cooling. There are not many options for the  4926 socket. I\nfound an Arctic Freezer 4U-M in one of the online stores here in Poland and\nbought the only one they&nbsp;had.</p><p>Working fine (after re-seating three&nbsp;sticks):</p><pre><code>DRAM populated DIMMs:\n  SK0 MC0 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n  SK0 MC1 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n  SK0 MC2 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n  SK0 MC3 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n  SK0 MC4 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n  SK0 MC5 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n  SK0 MC6 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n  SK0 MC7 S0: RDIMM[ad:80] 16GB 3200 ECC 2R x8 RCD[32:86] HMA82GR7CJR8N-XN\n</code></pre><p>The rest of the parts are ordinary ones available in any random&nbsp;store.</p><p>The case was a challenge. The - is a “deep MicroATX” case which\nmeans I needed a case that can take an  motherboard (they are a bit deeper\nthan ).</p><p>I looked through pictures in online stores and selected about 10 candidates. Then\nI started watching reviews and crossed several of them out. Usually because the\nholes for handling cables were too close to the edge of a&nbsp;board.</p><p>Finally, bought an Endorfy 700 Air case. It came with five 120mm fans (3 at the\nfront, one at the rear, and one on the top). There was a lot of space behind\nthe motherboard’s plate for cabling, and a fan splitter so I could connect all\nfive case fans as one to the&nbsp;motherboard.</p><p>And the  compartment has an extra hole for PCIe power&nbsp;cables!</p><p>The - motherboard is expected to be powered by a 12V  only.\nThere is no connection port for the  24-pin plug. Instead, there is an\nadapter that takes only power-on and power-good signals from it and connects to\nthe small 4-pin port on the&nbsp;motherboard.</p><p>There are three  connectors available. I used two of them. The  power supply I bought comes with two such cables and has an option for\nconnecting the third&nbsp;one.</p><p>What else is needed? Some  for storage (Lexar ) and a random\ngraphics card (Radeon Pro , remembering the old&nbsp;times).</p><p>And I was ready to build the&nbsp;system.</p><p>The motherboard feels small once the  cooler is mounted. And a low-profile\ngraphics card is almost&nbsp;invisible:</p><p>The back side of the case shows where I hid the  24-pin adapter and most of\nthe&nbsp;cables.</p><p>Of course, if I went cheap, then how cheap did it end up&nbsp;being?</p><table><tbody></tbody></table><p>In total: 7 732  (around 1 800 ). About 500  more than I anticipated at&nbsp;first.</p><p>The  upgrade to 80 cores was extra 100 . Memory came from another seller,\nas the first one ended their sale before I was ready to buy, resulting in an\nextra 40  to the&nbsp;cost.</p><p>At first, I wanted to have  per core, but with the  upgrade, I would have\nneeded to go to 256 , and that would have been another 250-300 &nbsp;extra.</p><p>The case had a 30% discount due to a promotion, and the  came with 7%&nbsp;cashback.</p><p>My current plans for this system&nbsp;are:</p><ul><li>create some  instances:<ul></ul></li><li>put a Radeon  inside and check whether it would&nbsp;work</li><li>try to use it as a&nbsp;desktop</li><li>do some crazy&nbsp;experiments</li></ul><p>All Linux systems will have both 4k and 64k page size&nbsp;kernels.</p>","contentLength":4035,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44419446"},{"title":"Gridfinity: The modular, open-source grid storage system","url":"https://gridfinity.xyz/","date":1751254641,"author":"nateb2022","guid":176469,"unread":true,"content":"<h3>Get started with online generators!</h3><h3>The best things in life are integer multiples of 42x42x7mm.</h3><p>Gridfinity could be your workshop's ultimate modular storage system to keep you productive, organized, and safe. It is free, open source, and almost 100% 3D printable.</p><section><p>Now Gridfinity is in the hands of a thriving community that continually adapts it to their needs. We invite you to join this community by using and adapting the system!</p></section><div><b>This site is a work in progress!</b> Come join us on <a href=\"https://discord.com/invite/voidstarlab\">#gridfinity</a> in Zack's discord to help!\n</div>","contentLength":517,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44419091"},{"title":"Use keyword-only arguments in Python dataclasses","url":"https://chipx86.blog/2025/06/29/tip-use-keyword-only-arguments-in-python-dataclasses/","date":1751244336,"author":"Bogdanp","guid":176650,"unread":true,"content":"<p>Python <a href=\"https://docs.python.org/3/library/dataclasses.html\">dataclasses</a> are a really nice feature for constructing classes that primarily hold or work with data. They can be a good alternative to using dictionaries, since they allow you to add methods, dynamic properties, and subclasses. They can also be a good alternative to building your own class by hand, since they don’t need a custom  that reassigns attributes and provide methods like  out of the box.</p><p>One small tip to keeping dataclasses maintainable is to always construct them with , like so:</p><div><pre title=\"\">from dataclasses import dataclass\n\n\n@dataclass(kw_only=True)\nclass MyDataClass:\n    x: int\n    y: str\n    z: bool = True\n</pre></div><p>This will construct an  that looks like this:</p><div><pre title=\"\">class MyDataClass:\n    def __init__(\n        self,\n        *,\n        x: int,\n        y: str,\n        z: bool = True,\n    ) -&gt; None:\n        self.x = x\n        self.y = y\n        self.z = z\n</pre></div><div><pre title=\"\">class MyDataClass:\n    def __init__(\n        self,\n        x: int,\n        y: str,\n        z: bool = True,\n    ) -&gt; None:\n        self.x = x\n        self.y = y\n        self.z = z\n</pre></div><p>That  in the argument list means everything that follows must be passed as a keyword argument, instead of a positional argument.</p><p>There are two reasons you probably want to do this:</p><ol><li>It allows you to reorder the fields on the dataclass without breaking callers. Positional arguments means a caller can use <code>MyDataClass(1, 'foo', False)</code>, and if you remove/reorder any of these arguments, you’ll break those callers unexpectedly. By forcing callers to use <code>MyDataClass(x=1, y='foo', z=False)</code>, you remove this risk.</li><li>It allows subclasses to add required fields. Normally, any field with a default value (like  above) will force any fields following it to also have a default. And that includes  fields defined by subclasses. Using  gives subclasses the flexibility to decide for themselves which fields must be provided by the caller and which have a default.</li></ol><p>These reasons are more important for library authors than anything. We spend a lot of time trying to ensure backwards-compatibility and forwards-extensibility in <a href=\"https://www.reviewboard.org\">Review Board</a>, so this is an important topic for us. And if you’re developing something reusable with dataclasses, it might be for you, too.</p><p> One important point I left out is Python compatibility. This flag was introduced in Python 3.10, so if you’re supporting older versions, you won’t be able to use this just yet. If you want to optimistically enable this just on 3.10+, one approach would be:</p><div><pre title=\"\">import sys\nfrom dataclasses import dataclass\n\n\nif sys.version_info[:2] &gt;= (3, 10):\n    dataclass_kwargs = {\n        'kw_only': True,\n    }\nelse:\n    dataclass_kwargs = {}\n\n...\n\n@dataclass(**dataclass_kwargs)\nclass MyDataClass:\n    ...\n...\n</pre></div><p>But this won’t solve the subclassing issue, so you’d still need to ensure any subclasses use default arguments if you want to support versions prior to 3.10.</p>","contentLength":2851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44418036"},{"title":"Nearly 20% of cancer drugs defective in four African nations","url":"https://www.dw.com/en/nearly-20-of-cancer-drugs-defective-in-4-african-nations/a-73062221","date":1751239389,"author":"woldemariam","guid":175535,"unread":true,"content":"<p>An alarming number of people across Africa may be taking cancer drugs that don't contain the vital ingredients needed to contain or reduce their disease.</p><p>It's a concerning finding with roots in a complex problem: how to regulate a range of therapeutics across the continent.</p><p>A US and pan-African research group published the findings this week in . The researchers had collected dosage information, sometimes covertly, from a dozen hospitals and 25 pharmacies across Ethiopia, Kenya, Malawi and Cameroon.</p><p>They tested nearly 200 unique products across several brands. Around 17% — roughly one in six — were found to have incorrect active ingredient levels, including products used in major hospitals.</p><p>Patients who receive&nbsp;insufficient dosages of these ingredients could see their tumors&nbsp;keep growing, and possibly even spread.</p><p>Similar numbers of substandard antibiotics, antimalarial and tuberculosis drugs have been <a href=\"https://www.dw.com/en/fighting-the-spread-of-fake-drugs-in-africa/a-42100713\">reported in the past</a>, but this is the first time that such a study has found high levels of falsified or defective anticancer drugs in circulation.</p><p>\"I was not surprised by these results,\" said Lutz Heide, a pharmacist at the University of Tübingen in Germany who has previously worked for the Somali Health Ministry and has spent the past decade researching substandard and falsified medicines.</p><p>Heide was not part of the investigative group, but said the report shed light on a problem not previously measured.</p><p>\"I was delighted that, finally, someone published such a systemic report,\" he said. \"That is a first, really significant systematic study of this area.\"</p><h2>Causes need addressing, but it's not straightforward</h2><p>\"There are many possible causes for bad-quality products,\"&nbsp;Marya Lieberman of the University of Notre Dame in the US,&nbsp;the investigation's senior researcher,&nbsp;told DW.</p><p>Those causes can include faults in the manufacturing process or product decay due to poor storage conditions. But some drugs are also counterfeit, and that increases the risk of discrepancies between what's on the product label and the actual medicine within.</p><div><h2 aria-label=\"Embedded video — Fake drug pandemic in Africa\">Fake drug pandemic in Africa</h2></div><p>Spotting substandard and falsified products can be difficult. Usually, a medical professional or patient is only able to perform a visual inspection — literally checking a label for discrepancies or pills and syringes for color differences — to spot falsified products.</p><p>But that's not a reliable method. In the study, barely a quarter of the substandard products were identified through visual inspection. Laboratory testing identified the rest.</p><p>Fixing the problem, Lieberman said, will require improving regulation and providing screening technologies and training where they're needed.</p><p>\"If you can't test it, you can't regulate it,\" she said.&nbsp;\"The cancer medications are difficult to handle and analyze because they're very toxic, and so many labs don't want to do that. And that's a core problem for the sub-Saharan countries where we worked. Even though several of those countries have quite good labs, they don't have the facilities that are needed for safe handling of the chemo drugs established.\"</p><h2>Not only cancer treatments are affected</h2><p>Nearly a decade ago, the World Health Organization found around one&nbsp;in 10 medicines used in low and middle-income countries were substandard or falsified. Independent research conducted since has backed those figures up, sometimes finding rates that are potentially twice as high.</p><p>\"This could lead to treatment failure, adverse reactions, disease progression,\" health economist Sachiko Ozawa told DW. Ozawa contributed to the investigation on anticancer drugs and has separately researched other cases of defective medicines. &nbsp;&nbsp;</p><p>\"For the community, there's also economic losses in terms of wasted resources,” she said. “So countries may be spending a lot of money on medications that are not going to be effective.\"</p><p>While high-income countries can monitor supply chains and have stringent regulatory systems in place to identify and withdraw suspect products, the infrastructure to do that is far from common in other regions.</p><p>In those places, poor access to affordable medication often drives patients to less-regulated marketplaces. Inadequate governance and regulation, as well as a scarcity of surveillance and diagnostic equipment to test pharmaceuticals, are all contributing to the problem in Africa.</p><p>\"In high-income countries, I think there's a much more secure supply chain where you know the manufacturers are vetted, it has to go through very stringent regulatory processes to get approval...it gets tested more frequently,\" said Ozawa.</p><p>The <a href=\"https://www.dw.com/en/world-health-organization-who/t-38396203\">WHO</a> told DW that following the report's findings, it was working with the four affected countries to address the problem.</p><p>\"We are concerned with the findings the article has highlighted. WHO is in contact with national authorities of four impacted countries and obtaining relevant data,\" it said in a statement. \"We expect to assess full information to evaluate the situation, which often takes time and capacity. But we're committed to address these issues working with the relevant countries and partners.\"</p><p>The WHO also reiterated its ongoing call for countries to improve their regulatory frameworks to \"prevent incidents of substandard and falsified medicines, including in settings of cancer programs.\"</p><div><h2 aria-label=\"Embedded video — South African men don heels to spotlight male breast cancer\">South African men don heels to spotlight male breast cancer</h2></div><h2>Prevention, detection and response</h2><p>In 2017, the WHO's review of substandard and falsified medicines offered three solutions based around prevention, detection and response.</p><p>Stopping the manufacture and sale of those medicines is the primary preventative measure, but where defective products make it to market, surveillance and response programs can prevent poor quality medicines from reaching patients.</p><p>But regulatory reform sought by experts and authorities takes time. More immediate solutions are being developed in the form of better screening technologies.</p><p>Lieberman is working on a \"paper lab\" — a type of test that can be used by trained professionals to chemically test the quality of a product before it's administered to a patient. Other laboratory technologies are also under development.</p><p>One comforting point is that while a significant proportion of the medication circulating in medical facilities in the four African countries was defective, the majority of the products tested met required standards.</p><p>\"[With] two-thirds of the suppliers, all the products [were] good quality, so there are good quality suppliers,\" said Heide. \"But a few of them really have a suspiciously high number of failing samples.\"</p><p><em>Edited by: Derrick Williams</em></p>","contentLength":6592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44417549"},{"title":"Finding a former Australian prime minister’s passport number on Instagram (2020)","url":"https://mango.pdf.zone/finding-former-australian-prime-minister-tony-abbotts-passport-number-on-instagram/","date":1751235752,"author":"guiambros","guid":175517,"unread":true,"content":"<p>So you know when you’re flopping about at home, minding your own business, drinking from your water bottle in a way that does not possess  intent to subvert the Commonwealth of Australia?</p><p>It’s a feeling I know all too well, and in which I was vigorously partaking when I got this message in “the group chat”.</p><p>The man in question is <a href=\"https://en.wikipedia.org/wiki/Tony_Abbott\">Tony Abbott</a>, one of Australia’s  former Prime Ministers.</p><p>For security reasons, we try to change our Prime Minister every six months, and to never use the same Prime Minister on multiple websites.</p><p>This particular former PM had just posted a picture of his boarding pass on Instagram (Instagram, in case you don’t know it, is an app you can open up on your phone any time to look at ads).</p><p>My friend (who we will refer to by their group chat name, 𝖍𝖔𝖌𝖌𝖊 𝖒𝖔𝖆𝖉𝖊) is asking whether I can “hack this man” not because I am the kind of person who regularly commits 𝒄𝒚𝒃𝒆𝒓 𝒕𝒓𝒆𝒂𝒔𝒐𝒏 on a whim, but because we’d recently been talking about boarding passes.</p><p>I’d said that people post pictures of their boarding passes all the time, not knowing that it can sometimes be used to get their passport number and stuff. They just post it being like “omg going on holidayyyy 😍😍😍”, unaware that they’re posting cringe.</p><p>Meanwhile, some hacker is rubbing their hands together, being all “yumyum identity fraud 👀” in their dark web Discord, because this happens a .</p><p>So there I was, making intense and meaningful eye contact with this chat bubble, asking me if I could “hack this man”.</p><p>Of course, my friend wasn’t  asking me to hack the former Prime Minister.</p><p>I mean… what are you gonna do,  click it? Are you gonna let a  that’s like 50% advertising tracking ID tell you what to do? Wouldn’t you be ?</p><p>The former Prime Minister had just posted his boarding pass. Was that ? Was someone in danger? I didn’t know.</p><p>What I did know was: the  I could do for my country would be to have a casual browse 👀</p><h2>Investigating the boarding pass photo</h2><p>So I had a bit of a casual browse, and got the picture of the boarding pass, and then…. I didn’t know what was supposed to happen after that.</p><p>Well, I’d heard that it’s bad to post your boarding pass online, because if you do, a bored 17 year-old Russian boy called “Katie-senpai” might somehow use it to commit identity fraud. But I don’t know anyone like that, so I just clumsily googled some stuff.</p><h4>Googling how 2 hakc boarding pass</h4><p>Eventually I found <a href=\"https://null-byte.wonderhowto.com/how-to/hackers-use-hidden-data-airline-boarding-passes-hack-flights-0180728/\">a blog post</a> explaining that yes, pictures of boarding passes can indeed be used for Crimes. The part you wanna be looking at for all your criming needs is the barcode, because it’s got the “Booking Reference” (e.g. ) in it.</p><p>Why do you want the booking reference? It’s one of the two things you need to log in to the airline website to manage your flight.</p><p>The second one is your… last name. I was really hoping the second one would be like a password or something. But, no, it’s the booking reference the airline emails you and prints on your boarding pass. And it also lets you log in to the airline website?</p><p>That sounds suspiciously like a password to me, but like I’m still fine to pretend it’s not if you are.</p><p>I’ve been practicing every morning at sunrise, but still can’t scan barcodes with my eyes. I had to settle for a barcode scanner app on my phone, but when I tried to scan the picture in the Instagram post, it didn’t work :((</p><h3>Step 2: Scan the barcode, but more</h3><p>Well, maybe it wasn’t scanning because the picture was too blurry.</p><p>I spent around 15 minutes in an “enhance, ENHANCE” montage, fiddling around with the image, increasing the contrast, and so on. Despite the montage taking up way too much of the 22 minute episode, I couldn’t even get the barcode to scan.</p><h3>Step 2: Notice that the Booking Reference is printed right there on the paper</h3><p>After staring at this image for 15 minutes, I noticed the Booking Reference is just… printed on the baggage receipt.</p><p>But it did not prepare me for this.</p><h3>Step 3: Visit the airline’s website</h3><p>After recovering from  emotional rollercoaster, I went to <a href=\"https://qantas.com.au\">qantas.com.au</a>, and clicked “Manage Booking”. In case you don’t know it because you live in a country with fast internet, Qantas is the main airline here in Australia.</p><p>(I also very conveniently started recording my screen, which is gonna pay off  in just a moment.)</p><h3>Step 4: Type in the Booking Reference</h3><p>Well, the login form was just… , and it was asking for a Booking Reference and a last name. I had just flawlessly read the Booking Reference from the boarding pass picture, and, well… I knew the last name.</p><p>I did hesitate for a split-second, but… no, I had to know.</p><h3>Can I get a YIKES in the chat</h3><p>Leave a comment if you really felt that.</p><p>I guess I was now logged the heck in as Tony Abbott? And for all I know, everyone else who saw his Instagram post was right there with me. It’s kinda wholesome, to imagine us all there together. But also probably suboptimal in a governmental sense.</p><h5>Was there anything secret in here?</h5><p>I then just incredibly browsed the page, browsed it so hard.</p><p>I saw Tony Abbott’s name, flight times, and Frequent Flyer number, but not really anything  secret-looking. Not gonna be committing any cyber treason with a Frequent Flyer number. The flight was in the past, so I couldn’t change anything, either.</p><p>The page said the flight had been booked by a travel agent, so I guessed some information would be missing because of that.</p><p>I clicked around and scrolled a considerable length, but still didn’t find any government secrets.</p><p>Some people might give up here. But I, the Icarus of computers, was simply too dumb to know when to stop.</p><h3>We’re not done just because a  says we’re done</h3><p>I wanted to see if there were juicy things hidden  the page. To do it, I had to use the  hacker tool I know.</p><p>Listen. This is the only part of the story that might be confused for highly elite computer skill. It’s not, though. Maybe later someone will show you this same thing to try and flex, acting like only  know how to do it. You will not go gently into that good night. You will refuse to acknowledge their flex, killing them instantly.</p><h5>How does “Inspect Element” work?</h5><p>“Inspect Element”, as it’s called, is a feature of Google Chrome that lets you see the computer’s internal representation (HTML) of the page you’re looking at. Kinda like opening up a clock and looking at the cool cog party inside.</p><p>Everything you see when you use “Inspect Element” was already downloaded to your computer, you just hadn’t asked Chrome to show it to you yet. Just like how the cogs were already in the watch, you just hadn’t opened it up to look.</p><p>But let us dispense with frivolous cog talk. Cheap tricks such as “Inspect Element” are used by programmers to try and understand how the website works. This is ultimately futile: Nobody can understand how websites work. Unfortunately, it kinda  like hacking the first time you see it.</p><p>If you’d like to know more about it, I’ve prepared a short video.</p><h3>Browsing the “Manage Booking” page’s HTML</h3><p>I scrolled around the page’s HTML, not really knowing what it meant, furiously trying to find anything that looked out of place or secret.</p><p>I eventually realised that manually reading HTML with my eyes was not an efficient way of defending my country, and Ctrl + F’d the HTML for “passport”.</p><p>At this point I was fairly sure I was looking at the  secret government-issued ID of the <em>28th Prime Minister of the Commonwealth of Australia, servant to her Majesty Queen Elizabeth II</em> and I was  worried that I was somehow doing something wrong, but like, not enough to stop.</p><h3>….anything  in this page?</h3><p>Well damn, if Tony Abbott’s passport number is in this treasure trove of computer spaghetti, maybe there’s wayyyyy more. Perhaps this HTML contains the lost launch codes to the Sydney Opera House, or Harold Holt.</p><p>Maybe there’s a phone number?</p><p>Searching for  and  didn’t get anywhere, so I searched for , the first 3 digits of an Australian phone number, using my colossal and highly celestial galaxy brain.</p><p>A weird pile of what I could only describe as extremely uppercase letters came up. It looked like this:</p><pre tabindex=\"0\"><code>RQST QF HK1 HNDSYD/03EN|FQTV QF HK1|CTCM QF HK1 614[phone number]|CKIN QF HN1 DO NOT SEAT ROW [row number] PLS SEAT LAST ROW OF [row letter] WINDOW\n</code></pre><p>So, there’s a lot going on here. There is indeed a phone number in here. But what the heck is all this  stuff?</p><p>I realised this was like… Qantas staff talking to eachother  Tony Abbott, but not  him?</p><p>In what is surely the subtweeting of the century, it has a section saying <code>HITOMI CALLED RQSTING FASTTRACK FOR MR. ABBOTT</code>. Hitomi must be requesting a “fasttrack” (I thought that was only a thing in movies???) from another Qantas employee.</p><h5>This is messed up for many reasons</h5><p>What is even going on here? Why do Qantas flight staff talk to eachother via this passenger information field? Why do they send these messages, and your passport number  you when you log in to their website? I’ll never know because I suddenly got distracted with</p><p>I realised the allcaps muesli I saw must be some airline code for something. Furious and intense googling led me to several ancient forbidden PDFs that explained some of the codes.</p><p>Apparently, they’re called “SSR codes” (Special Service Request). There are codes for things like “Vegetarian lacto-ovo meal” (), “Vegetarian oriental meal” (), and even “Vegetarian vegan meal” (). Because I was curious about these codes, here’s some for you to be curious about too (tag urself, I’m ):</p><pre tabindex=\"0\"><code>RFTV    Reason for Travel\nUMNR    Unaccompanied minor\nPDCO    Carbon Offset (chargeable)\nWEAP    Weapon\nDEPA    Deportee—accompanied by an escort\nESAN    Passenger with Emotional Support Animal in Cabin\n</code></pre><p>The phone number I found looked like this: <code>CTCM QF HK1 [phone number]</code>. Googling “SSR CTCM” led me to the <a href=\"https://guides.developer.iata.org/docs/ctcm\">developer guide</a> for some kind of airline association, which I assume I am basically a member of now.</p><h4>Is the phone number actually his?</h4><p>I thought maybe the phone number belonged to the travel agency, but I <a href=\"https://portal.iata.org/faq/articles/en_US/FAQ/Is-it-mandatory-for-travel-agents-to-provide-the-passenger-s-mobile-phone-and-email-address-1448977338174\">checked</a> and it has to be the passenger’s real phone number. That would be, if my calculations are correct,,,, *steeples fingers* Tony Abbott’s phone number.</p><p>I’d now found Tony Abbott’s:</p><ul><li>Weird Qantas staff comments.</li></ul><p>My friend who messaged me had .</p><p>Tony Abbott’s passport is probably a <a href=\"https://en.wikipedia.org/wiki/Australian_passport#Diplomatic_and_Official_Passport\">Diplomatic passport</a>, which is used to “represent the Australian Government overseas in an official capacity”.</p><p>By this point I’d had enough defending my country, and had recently noticed some new thoughts in my brain, which were:</p><ul><li><em>i gotta get someone, somehow, to reset tony abbott’s passport number</em></li><li><em>can you even reset passport numbers</em></li><li><em>is it possible that i’ve done a crime</em></li></ul><blockquote><p>In this act, I, your well-meaning but ultimately incompetent protagonist, attempt to do the following things:</p></blockquote><ul><li>⬜ figure out whether i have done a crime</li><li>⬜ notify someone (tony abbott?) that this happened</li><li>⬜ get permission to publish this here blog post</li><li>⬜ tell qantas about the security issue so they can fix it</li></ul><blockquote><p>Spoilers: This takes almost six months.</p></blockquote><h4>Let’s skip the boring bits</h4><p>I contacted a  of people about this. If my calculations are correct, I called at least 30 phone numbers, to say nothing of The Emails. If you laid all the people I contacted end to end along the equator, they would die, and you would be arrested. Eventually I started keeping track of who I talked to in a note I now refer to as “the hashtag struggle”.</p><p>I’m gonna skip a considerable volume of tedious and ultimately unsatisfying telephony, because it’s been a long day of scrolling already, and you need to save your strength.</p><p>Alright strap yourself in and enjoy as I am drop-kicked through the goal posts of life.</p><h2>Part 1: is it possible that i’ve done a crime</h2><p>I didn’t  anything I did sounded like a crime, but I knew that sometimes when the other person is rich or famous, things can suddenly  crimes. Like, was there going to be some Monarch Law or something? Was Queen Elizabeth II gonna be mad about this?</p><p>My usual defence against being arrested for hacking is making sure the person being hacked is okay with it. You heard me, it’s the power of ✨consent✨. But this time I could uh only get it in retrospect, which is a bit yikes.</p><p>So I was wondering like… was logging in with someone else’s booking reference a crime? Was  someone else’s passport number a crime? What if they were, say, the former Prime Minister? Would I get in trouble for publishing a blog post about it? I mean you’re reading the blog post right now so obviousl</p><p>Update: I have been arrested.</p><h3>Just straight up Reading The Law</h3><p>It turned out I could just google these things, and before I knew it I was reading “the legislation”. It’s the rules of the law, just written down.</p><p>Look, reading pages of HTML? No worries. Especially if it’s to defend my country. But whoever wrote the legislation was just making up words.</p><p>Eventually, I was able to divine the following wisdoms from the Times New Roman tea leaves:</p><ul><li>Defamation is where you get in trouble for publishing something that makes someone look bad.<ul><li>But, it’s fine for me to blog about it, since it’s not defamation if you can prove it’s </li></ul></li><li>Having Tony Abbott’s passport number isn’t a crime<ul><li>But using it to commit identity fraud would be</li></ul></li><li>There are laws about what it’s okay to do on a computer<ul><li>The things it’s okay to do are: If u EVER even LOOK at a computer the wrong way, the FBI will instantly slam dunk you in a legal fashion dependent on the legislation in your area</li></ul></li></ul><p>I am possibly the furthest thing you can be from a lawyer. So, I’m sure I don’t need to tell you not to take this as legal advice. But, if you  the kind of person who takes legal advice from mango blog posts, who am I to stand in your way? Not a lawyer, that’s who. Don’t do it.</p><p>You know what, maybe I needed help. From an adult. Someone whose 3-year old kid has been buying iPad apps for months because their parents can’t figure out how to turn it off.</p><p><em>“Yeah, maybe I should get some of that free government legal advice”</em>, I thought to myself, legally. That seemed like a pretty common thing, so I thought it should be easy to do. I took a big sip of water and googled “free legal advice”.</p><h3>trying to ask a lawyer if i gone and done a crime</h3><p>Before I went and told everyone about my HTML frolicking, I spent a week calling legal aid numbers, lawyers, and otherwise trying to figure out if I’d done a crime.</p><p>During this time, I didn’t tell  what I’d done. I asked if any laws would be broken if “someone” had “logged into a website with someone’s publicly-posted password and found the personal information of a former politician”. Do you see how that’s not even a lie? I’m starting to see how lawyers do it.</p><p>First I call the state government’s Legal Aid number.\nThey tell me they don’t  here, and I should call another Legal Aid place named something slightly different.</p><p>The second place tells me they don’t  either, and I should call the First Place and “hopefully you get someone more senior”.</p><p>I call the First Place again, and they say “oh you’ve been given the run around!”. You see where this is going.</p><p>Let’s skip a lot of phone calls. Take my hand as I whisk you towards the slightly-more-recent past. Based on advice I got from two independent lawyers that was definitely not legal advice: I haven’t done a crime.</p><p>Helllllll yeah. But I mean it’s a little late because I forgot to mention that by this point I had already emailed explicit details of my activities to the Australian Government.</p><ul><li>☑️ figure out whether i have done a crime</li><li>⬜ notify someone (tony abbott?) that this happened</li><li>⬜ get permission to publish this here blog post</li><li>⬜ tell qantas about the security issue so they can fix it</li></ul><h2>Part 2: trying to report the problem to someone, anyone, please</h2><p>I had Tony Abbott’s passport number, phone number, and weird Qantas messages about him. I was the only one who  I had these.</p><p>Anyone who saw that Instagram post could also have them. I felt like I had to like,  someone about this. Someone with like, responsibilities. Someone with an email signature.</p><h3>wait but do u see the irony in this, u have his phone number right there so u could just-</h3><p>Yes I see it thank u for pointing this out, wise, astute, and ultimately self-imposed heading. I  I could just call the number any time and hear a “G’day” I’d  be able to forget. I knew I had a rare opportunity to call someone and have them ask “how did you get this number!?”.</p><p>But you can’t just  that.</p><p>You can’t just call someone’s phone number that you got by rummaging around in the HTML ball pit. Tony Abbott didn’t  me to have his phone number, because he didn’t give it to me. Maybe if it was urgent, or I had no other option, sure. But I was pretty sure I should do this the Nice way, and show that I come in peace.</p><p>I wanted to show that I come in peace because there’s also this pretty yikes thing that happens where you email someone being all like “henlo ur website let me log in with username  and password , maybe u wanna change that??? could just be me but let me kno what u think xoxo alex” and then they reply being like “oh so you’re a HACKER and a CRIMINAL and you’ve HACKED ME AND MY FAMILY TOO and this is a RANSOM and ur from the DARK WEB i know what that is i’ve seen several episodes of mr robot WELL watch out kiddO bc me and my lawyers are bulk-installing tens of thousands of copies of <a href=\"https://www.mcafee.com/en-us/antivirus/gaming.html\">McAfee® Gamer Security</a> as we speak, so i’d like 2 see u try”</p><p>I googled “tony abbott contact”, but there’s only his <a href=\"https://tonyabbott.com.au\">official website</a>. There’s no phone number on it, only a “contact me” form.</p><p>Yeah right, have you  the incredible volume of #content people want to say at politicians? No way anyone’s reading that form.</p><p>I later decided to try anyway, using the same Inspect Element ritual from earlier. Looking at the network requests the page makes, I divined that the “Contact me” form just straight up does not work. When you click “submit”, you get an error, and nothing gets sent.</p><p>Well rip I guess. I eventually realised the people to talk to were probably the government.</p><p>In the beginning, humans developed the concept of language by banging rocks together and saying “oof, oog, and so on”. Then something went horribly wrong, and now people unironically begin every sentence with “in regards to”. Our story begins here.</p><p>The government has like fifty thousand million different departments, and they all know which acronyms to call each other, but you don’t. If you EVER call it DMP&amp;C instead of DPM&amp;C you are gonna be express email forwarded into a nightmare realm the likes of which cannot be expressed in  number of spreadsheet cells, in spite of all the good people they’ve lost trying.</p><p>I didn’t even know where to begin with this. Desperately, I called Tony Abbott’s former political party, who were all like</p><p>Skip skip skip a few more calls like this.</p><h3>Maybe I knew someone who knew someone</h3><p> right, the true government channels were the friends we made along the way.</p><p>I asked hacker friends who seemed like they might know government security people. “Where do I report a security issue with like…. a person, not a website?”</p><p>They told me to call… 1300 CYBER1?</p><p>I don’t really have a good explanation for this so I’m just gonna post the screenshots.</p><p>You  I smashed that call button on . Did they just make it  then realise you need one more digit for a phone number? Incredible.</p><h3>Calling </h3><blockquote><p>“Yes yes hello, ring ring, is this 1300 cyber one”? They  to say yes if you ask that. They’re legally obligated.</p></blockquote><p>The person who picked up gave me an email address for <a href=\"https://www.asd.gov.au/\">ASD</a> (the Australian flavour of America’s NSA), and told me to email them the details.</p><h3>Emailing the government my crimes</h3><p>Feeling like the digital equivalent of three kids in a trenchcoat, I broke out my best Government Email dialect and emailed ASD, asking for them to call me if they were the right place to tell about this.</p><p>Fooled by my flawless disguise, they replied  (in a relative sense) asking for more details.</p><p>I  could provide them with more information, so I did, because I love to cooperate with the Australian government.</p><p>I also asked whether they could give me permission to publish this blog post, and they were all like “Seen 2:35pm”. Eventually, after another big day of getting left on read by the government, they replied, being all like “thanks kiddO, we’re doing like, an  and stuff, so we’ll take it from here”.</p><p>Overall, ASD were really nice to me about it and happy that I’d helped. They encouraged me to report this kind of thing to them if it happened again, but I’m not really in the business of uhhhhhhhh whatever the heck this is.</p><p>By the way, at this point in the story (chronologically) I had  idea if what I was emailing the government was actually the confession to a crime, since I hadn’t talked to a lawyer yet. This is widely regarded as a bad move. I do not recommend anyone else use “but I’m being so helpful and earnest!!!” as a legal defence. But also I’m not a lawyer, so idk, maybe it works?</p><h3>Wholesomely emailing the government</h3><p>At one point in what was surely an unforgettable email chain, the person I was emailing added a P.S. containing…. the answer to the puzzle hidden on this website. The one you’re reading this blog on right now. Hello.\nI guess they must have found this website (hi asd) by stalking the email address I was sending from. This is unprecedented and everything, but:</p><ol><li>The puzzle says to <a href=\"https://twitter.com/mangopdf\">tweet</a> the answer at me, not email me</li><li>The prize for doing the puzzle is me tweeting this gif of a shakas to you</li></ol><p>So I guess I emailed the shakas gif to the government??? Yeah, I guess I did.</p><p>I asked them if they could give me permission to write this blog post, or who to ask, and they were like “uhhhhhhhhhhh” and gave me two government media email addresses to try. Listen I don’t wanna be an “ummm they didn’t reply to my emAiLs” kinda person buT they simply left me no choice.</p><p>Still, defending the Commonwealth was in ASD’s hands now, and that’s a win for me at this point.</p><ul><li>☑️ figure out whether i have done a crime</li><li>☑️ notify someone (The Government) that this happened</li><li>⬜ get permission to publish this here blog post</li><li>⬜ tell qantas about the security issue so they can fix it</li></ul><h2>Part 3: Telling Qantas the bad news</h2><p>Hey remember like fifteen minutes ago when this post was about webpages?</p><p>I’m guessing Qantas didn’t  to send the customer their passport number, phone number, and staff comments about them, so I wanted to let them know their website was doing that. Maybe the website was well meaning, but ultimately caused more harm than good, like how that time the bike path railings on the Golden Gate Bridge accidentally <a href=\"https://www.theguardian.com/us-news/2020/jun/06/golden-gate-bridge-san-francisco-sings\">turned it into the world’s largest harmonica</a>.</p><p>But why does the website even send you all that stuff in the first place? I don’t know, but to speculate wildly: Maybe the website just sends you  the data it knows about you, and then only shows you your name, flight times, etc, while leaving the passport number etc. still in the page.</p><p>If that were true, then Qantas would want to unblend the digital smoothie they’ve sent you, if you will. They’d want to change it so that they only send you your name and flight times and stuff (which are a key ingredient of the smoothie to be sure), not the whole identity fraud smoothie.</p><p>I wanted to tell them the smoothie thing, but how do I contact them?</p><p>The first place to check is usually , maybe that’ll w-</p><p>Okay fine maybe I should just email  surely that’s it? I could only find a phone number to report security problems to, and I wasn’t sure if it was like…. airport security?</p><p>So I just… called the number and was like “heyyyy uhhhh I’d like to report a cyber security issue?”, and the person was like “yyyyya just email <a href=\"https://mango.pdf.zone/cdn-cgi/l/email-protection#6516000610170c111c2514040b1104164b060a084b0410\"></a>” and i was like “ok sorrY”.</p><h3>Time to email Qantas I guess</h3><p>I emailed Qantas, being like “beep boop here is how the computer problem works”.</p><p>(Have you been wondering about the little dots in this post? Click this one for the rest of the email .)</p><p>A few days later, I got this reply.</p><h3>And then I never heard from this person again</h3><p>Airlines were going through kinda a  at the time, so I guess that’s what happened?</p><p>After filling up my “get left on read” combo meter, I desperately resorted to calling Qantas’ secret media hotline number.</p><p>They said the issue was being fixed by <a href=\"https://en.wikipedia.org/wiki/Amadeus_CRS\">Amadeus</a>, the company who makes their booking software, rather than with Qantas itself. I’m not sure if that means other Amadeus customers were also affected, or if it was just the way Qantas was using their software, or what.</p><p>It’s common to give companies 90 days to fix the bug, before you publicly disclose it. It’s a tradeoff between giving them enough time to fix it, and people being hacked because of the bug as long as it’s out there.</p><p>But, well, this was kinda a special case. Qantas was going through some #struggles, so it was taking longer. Lots of their staff were stood down, and the world was just generally more cooked. At the same time, hardly anybody was flying at the time, due to see above re: #struggles. So, I gave Qantas as much time as they needed.</p><p>The world is a completely different place, and Qantas replies to me, saying they fixed the bug.\nIt  take five months, which is why it took so long for you and I to be having this weird textual interaction right now.</p><p>I don’t have a valid Booking Reference, so I can’t actually check what’s changed. I asked a friend to check (with an expired Booking Reference), and they said they didn’t see a mention of “documentNumber” anymore, which sounds like the passport number is no longer there. But That’s Not Science, so I don’t know for sure.</p><p>I originally found the bug in March, which was about 60 years ago. BUT we got there baybee, Qantas emailed me saying the bug had been fixed on August 21. They later told me they actually fixed the bug in July, but the person I was talking to didn’t know about it until August.</p><p>Qantas also said this when I asked them to review this post:</p><blockquote><p>Thanks again for letting us have the opportunity to review and again for refraining from posting until the fix was in place for vulnerability.</p></blockquote><blockquote><p>Our standard advice to customers is not to post pictures of the boarding pass, or to at least obscure the key personal information if they do, because of the detail it contains.</p></blockquote><blockquote><p>We appreciate you bringing it to our attention in such a responsible way, so we could fix the issue, which we did a few months ago now.</p></blockquote><p>I also asked Qantas what they did to fix the bug, and they said:</p><blockquote><p>Unfortunately we’re not able to provide the details of fix as it is part of the protection of personal information.</p></blockquote><ul><li>☑️ figure out whether i have done a crime</li><li>☑️ notify someone (The Government) that this happened</li><li>⬜ get permission to publish this here blog post</li><li>☑️ tell qantas about the security issue so they can fix it</li></ul><h2>Part 4: Finding Tony Abbott</h2><p>Like 2003’s , this section was an emotional rollercoaster.</p><p>The government was presumably helping Tony Abbott reset his passport number, and making sure his current one wasn’t being used for any of that yucky identity fraud.</p><p>But, much like Shannon Noll’s 2004 <a href=\"https://www.youtube.com/watch?v=uCda5P_f4S8\"></a>, what  me? I really wanted to write a blog post about it, you know? So I could warn people about the non-obvious risk of sharing their boarding passes, and also make dumb and inaccessible references to the early 2000s.</p><p>The government people I talked to couldn’t give me permission to write this post, so rather than willingly wandering deeper into the procedurally generated labyrinth of government department email addresses (it’s dark in there), I tried to find Tony Abbott or his staff directly.</p><h3>Calling everybody in Australia one by one</h3><p>I called Tony Abbott’s former political party again, and asked them how to contact him, or his office, or  I’m really having a moment rn. They said they weren’t associated with him anymore, and suggested I call , like I was the Queen or something.</p><p>In case you don’t know it, Parliament House is sorta like the White House, I think? The Prime Minister lives there and has a nice little garden out the back with a macadamia tree that never runs out, and everyone works in different colourful sections like “Making it so Everyone Gets a Fair Shake of the Sauce Bottle R&amp;D” and “Mateship” and they all wear matching uniforms with lil kangaroo and emu hats, and they all do a little dance every hour on the hour to celebrate another accident-free day in the Prime Minister’s chocolate factory.</p><h3>calling parliament house i guess</h3><p>Not really sure what to expect, I called up and was all like “yeah bloody g’day, day for it ay, hot enough for ya?”. Once the formalities were out of the way, I skipped my usual explanation of why I was calling and just asked point-blank if they had Tony Abbott’s contact details.</p><p>The person on the phone was casually like “Oh, no, but I can put you through to the , who can give you the contact details of former members”. I was like “…..okay?????”. Was I supposed to know who that was? Isn’t a Serjeant like an army thing?</p><p>But no, the Serjeant-at-arms was just a nice lady who told me “he’s in a temporary office right now, and so doesn’t have a phone number. I can give you an email address or a P.O. box?”. I was like “ok th-thank you your majesty”.</p><p>It felt a bit weird just…. emailing the former PM being like “boy do i have bad news for ”, but I figured he probably wouldn’t read it anyway. If it was  easy to get this email address, everyone had it, and so nobody was likely to be reading the inbox.</p><p>Spoilers: It didn’t work.</p><h3>Finding Tony Abbott’s staff</h3><p>I roll out of bed and stare bleary-eyed into the morning sun, my ultimate nemesis, as Day 40 of not having found Tony Abbott’s staff begins.</p><p>Retinas burning, in a moment of determination/desperation/hubris, I went and asked even  people that  know how to contact Tony Abbott’s staff.</p><p>I asked a journalist friend, who had the kind of ruthlessly efficient ideas that come from, like, being a professional journalist. They suggested I find Tony Abbott’s former staff from when he was PM, and contact their offices and see if they have his contact details.</p><p>It was a strange sounding plan to me, which I thought meant it would  work.</p><p>Apparently Prime Ministers themselves have “ministers” (not prime), and those are their staff. That’s who I was looking for.</p><p>Okay but, the problem was that most of these people are retired now, and the glory days of 2013 are over. Each time I hover over one of their names, I see “so-and-so is a former politician and….” and discard their Wikipedia page like a <a href=\"https://cdn0.woolworths.media/content/wowproductimages/large/248197.jpg\">LeSnak</a> wrapper into the wind.</p><p>Eventually though, I saw  minister.</p><p>That’s the  Prime Minister of Australia (at the time of writing, that is, for all I know we’re three Prime-Ministers deep into 2020 by the time you read this), you know he’s  gonna be easier to find.</p><h3>Let’s call the Prime Minister’s office I guess?</h3><p>Easy google of the number, absolutely no emotional journey resulting in my growth as a person this time.</p><p>When I call, I hear what sounds like two women laughing in the background? One of them answers the phone, slightly out of breath, and says “Hello, Prime Minister’s office?”. I’m like “….hello? Am I interrupting something???”.</p><p>I clumsily explain that I know this is Scott Morrison’s office, but I actually was wondering if they had Tony Abbott’s contact details, because it’s for “a time-sensitive media enquiry”, and I j-\nShe interrupts to explain “so Tony Abbott isn’t Prime Minister anymore, this is Scott Morrison’s office” and I’m like “yA I  please I am desperate for these contact details”.</p><p>She says “We wouldn’t have that information but I’ll just check for you” and then pauses for like, a long time? Like 15 seconds? I can only wonder what was happening on the other end. Then she says “Oh actually I can give you Tony Abbott’s personal assistant’s number? Is that good?”.</p><p>Ummmm YES thanks that’s what I’ve been looking for this whole time? Anyway brb i gotta go be uh a journalist or something.</p><h3>Calling Tony Abbott’s personal assistant’s personal assistant</h3><p>I fumble with my phone, furiously trying to dial the number.</p><p>I ask if I’m speaking to Tony Abbott’s personal assistant. The person on the other end says no, but he  one of Tony Abbott’s staff. It has been a long several months of calling people. The cold ice is starting to thaw. One day, with enough therapy, I may be able to gather the emotional resources necessary to call another government phone number.</p><p>I explain the security issue I want to report, and midway through he interrupts with “sorry….  are you and what’s the organisation you’re calling from?” and I’m like “uhhhh I mean my name is Alex and uhh I’m not calling from any organisation I’m just like a person?? I just found this thing and…”.</p><p>The person is mercifully forgiving, and says that he’ll have to call me back. I stress once again that I’m calling to help them, happy to wait to publish until they feel comfortable, and definitely do not warrant the bulk-installation of antivirus products.</p><h3>Calling Tony Abbott’s personal assistant</h3><p>An hour later, I get a call from a number I don’t recognise.</p><p>He explains that the guy I talked to earlier was  assistant, and he’s Tony Abbott’s PA. Folks, we made it. It’s as easy as that.</p><p>He says he knows what I’m talking about. He’s got . He’s already in the process of getting Tony Abbott a new passport number. This is the stuff. It’s all coming together.</p><p>I ask if I can publish a blog post about it, and we agree I’ll send a draft for him to review.</p><h3>“These things do interest him - he’s quite keen to talk to you”</h3><p>I was like exCUSE me? Tony Abbott, Leader of the <a href=\"https://en.wikipedia.org/wiki/Abbott_Ministry\">69th Ministry of Australia</a>, wants to call me on the ? I suppose I owe this service to my country?</p><p>This story was already completely cooked so sure, whatever. I’d already declared emotional bankruptcy, so nothing was coming as a surprise at this point.</p><p>I asked what he wanted to talk about. “Just to pick your brain on these things”. We scheduled a call for 3:30 on Monday.</p><h3>And then Tony Abbott just… calls me on the phone?</h3><p>Mostly, he wanted to check whether his understanding of how I’d found his passport number was correct (it was). He also wanted to ask me how to learn about “the IT”.</p><p>He asked some intelligent questions, like “how much information is in a boarding pass, and what do people like me need to know to be safe?”, and “why can you get a passport number from a boarding pass, but not from a bus ticket?”.</p><p>The answer is that boarding passes have your password printed on them, and bus tickets don’t. You can use that password to log in to a  (widely regarded as a bad move), and at that point all bets are off, websites can just do whatever they want.</p><p>He was vulnerable, too, about how computers are harder for him to understand.</p><blockquote><p>“It’s a funny old world, today I tried to log in to a [Microsoft] Teams meeting (Teams is one of those apps), and the fire brigade uses a Teams meeting. Anyway I got fairly bamboozled, and I can now log in to a Teams meeting in a way I couldn’t before.</p></blockquote><blockquote><p>It’s, I suppose, a terrible confession of how people my age feel about this stuff.”</p></blockquote><p>Then the Earth stopped spinning on its axis.</p><p>For an instant, time stood still.</p><blockquote><p>“You could drop me in the bush and I’d feel perfectly confident navigating my way out, looking at the sun and direction of rivers and figuring out where to go, but this! Hah!”</p></blockquote><p>This was possibly the most pure and powerful Australian energy a human can possess, and explains how we elected our strongest as our leader. The raw energy did in fact travel through the phone speaker and directly into my brain, killing me instantly.</p><p>When I’d collected myself from various corners of the room, he asked if there was a book about the basics of IT, since he wanted to learn about it. That was kinda humanising, since it made me realise that even famous people are just people too.</p><p>Anyway I hadn’t heard of a book that was any good, so I told a story about my mum instead.</p><h5>A story about my mum instead</h5><p>I said there probably was a book out there about “the basics of IT”, but it wouldn’t help much. I didn’t learn from a book. 13 year old TikTok influencers don’t learn from a book. They just .</p><p>My mum always said when I was growing up that:</p><ol><li>There were “too many buttons”</li><li>She was afraid to press the buttons, because she didn’t know what they did</li></ol><p>I can understand that, since grown ups don’t have the sheer dumb hubris of a child, and that’s what makes them afraid of the buttons.</p><p>Like, when a toddler uses a spoon for the first time, they don’t know what a spoon is, where they are, or who the current Prime Minister is. But they see the spoon, and they see the cereal, and their dumb baby brain is just like “yeA” and they have a red hot go. And like, they get it wrong the first few times, but it doesn’t matter, because they don’t know to be afraid of getting it wrong. So eventually, they get it right.</p><p>Okay so I didn’t tell the spoon thing to Tony Abbott, but I did tell him what I always told my mum, which was: “Mum you just gotta press  the buttons, to find out what they do”.</p><p>He was like “Oh, you just learn by trial and error”. Exactly! Now that I think about it, it’s a bit scary. We are dumb babies learning to use a spoon for the first time, except if you do it wrong some clown writes a blog post about you. Anyway good luck out there to all you big babies.</p><h5>Asking to publish this blog post</h5><p>When I asked Tony Abbott for permission to publish the post you are reading right now while neglecting your responsibilities, he said “well look Alex, I don’t have a problem with it, you’ve alerted me to something I probably should have known about, so if you wanna do that, go for it”.</p><p>At the end of the call, he said “If there’s ever anything you think I need to know, give us a shout”.</p><p>Look you gotta hand it to him. That’s exactly the right way to respond when someone tells you about a security problem. Back at the beginning, I was kinda worried that he might misunderstand, and think I was trying to hack him or something, and that I’d be instantly slam dunked into jail. But nope, he was fine with it. And now you, a sweet and honourable blog post browser, get to learn the dangers of posting your boarding pass by the realest of real-world examples.</p><p>During the call, I was completely in shock from the lost in the bush thing killing me instantly, and so on. But afterwards, when I looked at the quotes, I realised he just wanted to understand what had happened to him, and more about how technology works. That’s the same kind of curiosity  had, that started this whole surrealist three-act drama. That… wasn’t really what I was expecting from Tony Abbott, but it’s what I found.</p><p>The point of this story isn’t to say “wow Tony Abbott got hacked, what a dummy”. The point is that if someone famous can unknowingly post their boarding pass, anyone can.</p><p>Anyway that’s why I vote right wing now baybeeeee.</p><ul><li>☑️ figure out whether i have done a crime</li><li>☑️ notify someone (The Government) that this happened</li><li>☑️ get permission to publish this here blog post</li><li>☑️ tell qantas about the security issue so they can fix it</li></ul><h2>Wait no what the heck did I just read</h2><p>Your boarding pass for a flight can sometimes be used to get your passport number. Don’t post your boarding pass or baggage receipt online, keep it as secret as your passport.</p><p>The Booking Reference on the boarding pass can be used to log in to the airline’s “Manage Booking” page, which sometimes contains the passport number, depending on the airline. I saw that Tony Abbott had posted a photo of his boarding pass on Instagram, and used it to get his passport details, phone number, and internal messages between Qantas flight staff about his flight booking.</p><p>One day, my friend who was also in “the group chat” said “I was thinking…. why didn’t  hack Tony Abbott? And I realised I guess it’s because you have more hubris”.</p><p>I was deeply complimented by this, but that’s not the point. The point is that you, too, can have hubris.</p><p>You know how they say to commit a crime (which once again I insist did not happen in my case) you need means, motive, and opportunity? Means is the ability to use <code>right click &gt; Inspect Element</code>, motive is hubris, and opportunity is the dumb luck of having my friend message me the Instagram post.</p><p>I know, I’ve been saying “hubris” a lot. I mean “the willingness to risk breaking the rules”. Now hold up, don’t go outside and do crimes (unless it’s really funny). I’m not talking about breaking the , I’m talking about rules we just follow without realising, like social rules and conventions.</p><p>Here’s a simple example. You’re at a sufficiently fancy restaurant, like I dunno, with white tablecloths or something? The waiter asks if you’d like “still or sparkling water?”</p><p>If you say “still”, it costs Eleven Dollars. If you say “sparkling”, it costs Eleven Dollars and tastes all gross and fizzy. But if you say “tap water, please”, you just get tap water, what you wanted in the first place?</p><p>When I first saw someone do this I was like “you can  that? I just thought you had to pay Eleven Dollars extra at fancy restaurants!”.</p><p>It’s not written down anywhere that you can ask for tap water. But when I found out you  do that, and like, nothing bad happens, I could suddenly do it too. Miss me with that Eleven Dollars fizzy water.</p><p>Basically, until you’ve broken the rules, the idea that the rules can be broken might just not occur to you. That’s how it felt for me, at least.</p><p>In conclusion, to be a hacker u ask for tap water.</p><h3>Why is it bad for someone else to have your passport number?</h3><p>Hey crime gang, welcome back to Identity Fraud tips and tricks with Alex.</p><p>A passport is government-issued ID. It’s how you prove you’re you. The fact that you have your passport and I don’t is how you prevent  from convincing the government that I’m you and doing crimes in your name.</p><p>Just having the information on the passport is not quite as powerful as a photo of the full physical passport, with your photo and everything.</p><p>With your passport number, someone could:</p><ul><li>Book an international flight as you.</li><li>Apply for anything that requires proof of identity documentation with the government, e.g. Working with children check</li><li>Activate a SIM card (and so get an internet connection that’s traceable to you, not them, hiding them from the government)</li><li>Create a fake physical passport from a template, with the correct passport number (which they then use to cross a border, open a bank account, or anything)</li><li>who knows what else, not me, bc i have never done a crime</li></ul><h3>Am I a big bozo, a big honking goose, if I post my boarding pass on Instagram?</h3><p>Nah, it’s an easy mistake to make. How are you supposed to know not to? It’s not obvious that your boarding pass is secret, like a password. I think it’s on the airline to inform you on the risks you’re taking when you use their stuff.</p><p> now that you’ve read this blog post, I regret to inform you that you  in fact be an entire sack of geese if you go and post your boarding pass now.</p><h3>When did all of this happen?</h3><ul><li>March 22 - <a href=\"https://instagram.com/hontonyabbott\">@hontonyabbott</a> posts a picture of a boarding pass and baggage receipt. I log in to the website and get the passport number, phone number, and internal Qantas comments.</li><li>March 24 - I contact the Australian Signals Directorate (ASD) and let them know what happened.</li><li>March 27 - ASD tells me their investigation is complete, I send them a shakas gif, and they thank me for being a good citizen.</li><li>March 29 - I learn from lawyers that I have not done a crime 💯</li><li>March 30 - I contact Qantas and tell them about the vulnerability.</li><li>May 1 - Tony Abbott calls me, we chat about being dropped in the middle of the bush.</li><li>July 17 - <em>Paper Mario: The Origami King</em> is released for Nintendo Switch.</li><li>August 21 - Qantas emails me saying the security problem has been fixed.</li><li>September 13 - Various friends finish reviewing this post &lt;3</li><li>September 15 - Tony Abbott and Qantas review this post.</li><li>Today - You read this post instead of letting it read you, nice job you.</li></ul><p>Let me answer that question,,, with a question.</p><p>Maybe try drinking some water you big goose. Honk honk, I’m so dehydrated lol. That’s you.</p><p><em>I wrote this because I can’t go back to the Catholic church ever since they excommunicated me in 1633 for insisting the Earth revolves around the sun.</em></p>","contentLength":44791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44417091"},{"title":"Cell Towers Can Double as Cheap Radar Systems for Ports and Harbors (2014)","url":"https://spectrum.ieee.org/cell-tower-signals-can-improve-port-security","date":1751233692,"author":"transpute","guid":176354,"unread":true,"content":"<p>How do you see ships without a pricey radar system? The question has troubled seaports around the world as they work to improve security. Without radar installations, it can be hard for port employees to detect small ships like those employed by pirates or by the&nbsp;terrorists who attacked the  in 2000. A team of researchers in Germany can now offer security teams a new option, though: putting existing cellular towers to work as quick and dirty radar systems.</p><p>Developed at the <a href=\"https://spectrum.ieee.org/tag/fraunhofer\">Fraunhofer</a> Institute for Communications, Information Processing and Ergonomics, the new security system employs a technology known&nbsp;as Passive Coherent Location (PCL), which harnesses the radio signals sent out by cell towers to pinpoint the location of ships entering a harbor. (PCL) works in much the same way as radar—sending signals that bounce off of objects and reading the signals that return to determine the objects’ locations.</p><p>Radar uses strong, directed waves that make it easy to find objects. In contrast,&nbsp;PCL uses the much weaker&nbsp;signals that are being&nbsp;bounced off of objects&nbsp;by cell towers. These bounced waves help a PCL system build a dynamic map of a port and traffic moving through it by looking at where cell signals come into contact with objects in the water.&nbsp;</p><p>While this technique takes advantage of waves that are already being produced by cell towers and doesn’t require the installation of a new radar system, it also means&nbsp;the signals are more difficult to accurately interpret. To make PCL useful, the Fraunhofer team had to write new <a href=\"https://spectrum.ieee.org/tag/algorithms\">algorithms</a> that distinguish the echoes created by objects from the mélange of signal noise.</p><p>In other words, filtering out the strong signals emanating directly from cell towers lets the PCL system concentrate on finding the weaker signals that represent boats in the water. Improvements to the sensitivity of the new system have even allowed it to track craft as they move across the water. In tests of the PCL system, researchers were able to identify small speedboats from as far away as 4 kilometers. And all the equipment for operating a PCL system can fit in a trailer, making it feasible to install in remote locations and on a budget.</p>","contentLength":2192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44416761"},{"title":"China Dominates 44% of Visible Fishing Activity Worldwide","url":"https://oceana.org/press-releases/china-dominates-44-of-visible-fishing-activity-worldwide/","date":1751233430,"author":"scubakid","guid":175478,"unread":true,"content":"<p>Today, on the International Day for the Fight Against Illegal, Unreported, and Unregulated Fishing,Oceana released an&nbsp;<a href=\"https://usa.oceana.org/wp-content/uploads/sites/4/2025/06/Oceana_ChinaFishing_FactSheet_2025-Update-Final_Compressed.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">analysis</a>&nbsp;of China’s global fishing* activity worldwide between 2022 and 2024. The analysis shows China’s global fishing footprint, in which 57,000 of their industrial fishing vessels dominated 44% of the world’s visible fishing activity during this period. &nbsp;</p><p>“To protect our oceans and fisheries, we must know who is fishing and where,”&nbsp;<strong>said Dr. Max Valentine, illegal fishing and transparency campaign director and senior scientist at Oceana.&nbsp;</strong>“It is critical that we have eyes on the seas, paying close attention to the world’s largest fishing fleets, especially from China, which&nbsp;<a href=\"https://www.fisheries.noaa.gov/s3/2023-08/2023RTC-ImprovingIFManagement.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">have been linked</a>&nbsp;to illegal, unreported, and unregulated fishing and human rights abuses at sea. The sheer scale of China’s distant-water fleet has a profound impact on marine ecosystems worldwide. Transparency at sea is essential, not just to track distant-water fleets, but to hold bad actors accountable, protect vulnerable communities, and safeguard the sustainability of our ocean for future generations.”&nbsp;</p><p>Some key takeaways from Oceana’s analysis of China’s apparent fishing activity over a three-year period, from Jan. 1, 2022, to Dec. 31, 2024:&nbsp;</p><ul><li>57,000 fishing vessels, primarily trawlers, flagged to China appeared to fish for more than 110 million hours,&nbsp;</li><li>China’s fishing vessels appeared to conduct 44% of the global fishing activity during this period,&nbsp;</li><li>Chinese vessels accounted for 30% of all fishing activity on the high seas, appearing to fish for more than 8.3 million hours,&nbsp;&nbsp;</li><li>China’s fishing vessels were most active in South Korea (11.8 million hours), Taiwan (4.4 million hours), Japan (1.5 million hours), Kiribati (almost 425,000 hours), and Papua New Guinea (over 415,000 hours),&nbsp;&nbsp;&nbsp;</li><li>China appeared to fish in more than 90 countries’ waters for more than 22 million hours.&nbsp;</li></ul><p>Increased transparency in global fisheries is critical. Oceana calls on governments to require vessel monitoring for both their fishing fleets and vessels they authorize to fish in their waters.&nbsp;&nbsp;&nbsp;</p><p>The analysis used data from Global Fishing Watch** (GFW) — an independent nonprofit founded by Oceana in partnership with Google and SkyTruth. Notably, the analysis reflects only a partial view of China’s fishing activities during this time, as it includes only those vessels flagged to China  transmitting automatic identification system (AIS) data, making them “visible” to public tracking systems.&nbsp;&nbsp;</p><p>Illegal, unreported, and unregulated (IUU) fishing is a low-risk, high-reward activity, especially on the high seas where a fragmented legal framework and lack of effective enforcement allow it to thrive. IUU fishing can include fishing without authorization, ignoring catch limits, operating in closed or protected areas, targeting protected wildlife, and fishing with prohibited gear. These illicit activities can destroy important ocean habitat, severely deplete fish populations, and threaten global food security. These actions not only contribute to overfishing, but also give illegal fishers an unfair advantage over those who play by the rules.  &nbsp;&nbsp;</p><p>Oceana released&nbsp;<a href=\"https://usa.oceana.org/wp-content/uploads/sites/4/2024/07/Oceana-Multiple-Campaign-Topline-07.03.2024.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">the results of a nationwide poll</a>&nbsp;in 2024, which found that American voters overwhelmingly support transparency and traceability in the seafood supply chain. Included among the key findings, 90% of voters agree that imported seafood should be held to the same standards as U.S. caught seafood. Additionally, 91% of voters agree that seafood caught using human trafficking and slave labor should NOT be bought or sold in the U.S. Eighty-five percent of voters agree that all seafood should be traceable from the fishing boat to the dinner plate, and 88% say consumers should be reassured that the seafood they purchase was legally caught. Oceana’s poll, conducted by the nonpartisan polling company Ipsos using the probability-based KnowledgePanel®, surveyed 1,053 registered U.S. voters from June 28 to 30, 2024.     &nbsp;&nbsp;</p><p><strong>Read more about Oceana’s campaign </strong><a href=\"https://usa.oceana.org/our-campaigns/illegal-fishing-and-transparency/\" target=\"_blank\" rel=\"noreferrer noopener\">here</a>.&nbsp;</p><p>*Any and all references to “fishing” should be understood in the context of Global Fishing Watch’s (GFW) fishing detection algorithm, which is a best effort to determine “apparent fishing effort” based on vessel speed and direction data from the automatic identification system (AIS) collected via satellites and terrestrial receivers. As AIS data varies in completeness, accuracy, and quality, and the fishing detection algorithm is a statistical estimate of apparent fishing activity, it is possible that some fishing effort is not identified and, conversely, that some fishing effort identified is not fishing. For these reasons, GFW qualifies all designations of vessel fishing effort, including synonyms of the term “fishing effort,” such as “fishing” or “fishing activity,” as “apparent” rather than certain. Any/all GFW information about “apparent fishing effort” should be considered an estimate and must be relied upon solely at your own risk. GFW is taking steps to make sure fishing effort designations are as accurate as possible.&nbsp;All references to EEZ boundaries and sovereignty are based solely off the Marine Regions “World EEZ v12” definitions.</p><p>**Global Fishing Watch, a provider of open data for use in this analysis, is an international nonprofit organization dedicated to advancing ocean governance through increased transparency of human activity at sea. The views and opinions expressed in this press release and fact sheet are those of the authors, which are not connected with or sponsored, endorsed or granted official status by Global Fishing Watch. By creating and publicly sharing map visualizations, data and analysis tools, Global Fishing Watch aims to enable scientific research and transform the way our ocean is managed. Global Fishing Watch’s public data was used in the production of this fact sheet.&nbsp;</p>","contentLength":6015,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44416710"},{"title":"Anticheat Update Tracking","url":"https://not-matthias.github.io/posts/anticheat-update-tracking/","date":1751231217,"author":"not-matthias","guid":176649,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44416421"},{"title":"Error handling in Rust","url":"https://felix-knorr.net/posts/2025-06-29-rust-error-handling.html","date":1751228905,"author":"emschwartz","guid":175445,"unread":true,"content":"<p>The current standard for error handling, when writing a crate, is to define\none error enum per module, or one for the whole crate\nthat covers all error cases that the module or crate\ncan possibly produce, and each public function that returns a  will use\nsaid error enum.</p><p>This means, that a function will return an error enum, containing error variants that\nthe function cannot even produce. If you match on this error enum, you will\nhave to manually distinguish which of those variants are not applicable in\nyour current scope, based on the documentation of the function (and who reads that anyway? /s).</p><h2>The problem with the status quo</h2><p>What makes Rust so great, is the ability to express requirements via the type\nsystem in a way that makes it very hard for you to violate them, and yet, we\ncollectively decided to create these huge error-enums. I completely understand\nwhere this is coming from. Defining an extra error enum for every function\nand all the conversions between them is extremely tedious. And so everyone and\ntheir mother is building big error types. Well, not Everyone. A small handful of\nindomitable nerds still holds out against the standard.</p><p>An error is a singular bit of information, might be completely independent\nof other errors a function can return, and should probably be represented\nby a struct rather than an enum variant. A function returns one of\na set of those if it goes wrong, but it doesn't define the errors\nthemselves. The first Rust crate I saw that followed this philosophy, was\n<a href=\"https://docs.rs/terrors/latest/terrors/\">terrors</a> (Go ahead, check it out).\nI still think it's beautiful. It's also a little inconvenient.\nYou have to write  a lot and some functions\nhave a lot of possible error points, some of which being\nthe contents of other function's error sets. And yet, you have to spell\nthem out all over again. Still, I really like this crate ... from a distance.</p><p>Speaking of error sets, there is a\n<a href=\"https://docs.rs/error_set/latest/error_set/\">crate</a> with this name, that I\nprefer to use nowadays. Instead of doing Olympia level type acrobatics (like\nterrors) it uses macros. It allows you to define error enums for different\nfunctions in a very concise way and automatically generates the trait\nimplementations for conversions between those. Want a taste?</p><div><pre><code></code></pre></div><p>It allows us to create error sets from variants and from unions with other error sets.\nThe  operator will work if the error set you use it on is a sub-set of the function's\nerror set, and it will find out whether that's the case, even if you don't use the\nunion operator, i.e. this works:</p><div><pre><code></code></pre></div><p>This is still a bit too verbose for my tastes if you use many actual struct errors,\ne.g. because you want some fields on them to carry additional information, or because\nyou want to annotate them with error messages. However, I need them seldomly enough,\nso that I'll happily pay the extra keystrokes to define a wrapper enum for them\n(like the  enum in the first example) for now.</p><p>There are more libraries out there that explore this paradigm in different ways,\ne.g. <a href=\"https://crates.io/crates/smarterr\">SmartErr</a>. And I once saw a crate\nthat offered an attribute macro that you could slap on a function, and then\nit would parse the functions body and generate an error enum and insert it into\nthe functions return type,\nbased on the errors that occured in the function's body. Sadly I didn't find\nit again despite searching for it for an hour. If anyone has a link,\nplease tell me.</p>","contentLength":3339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44416157"},{"title":"Ask HN: What Are You Working On? (June 2025)","url":"https://news.ycombinator.com/item?id=44416093","date":1751228488,"author":"david927","guid":175477,"unread":true,"content":"<div>What are you working on?  Any new ideas which you're thinking about?</div>","contentLength":68,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44416093"},{"title":"YouTube No Translation","url":"https://addons.mozilla.org/en-GB/firefox/addon/youtube-no-translation/","date":1751227854,"author":"doener","guid":178723,"unread":true,"content":"This is an open source Add-on preventing automatic translation on YouTube. It ensures that video titles and description remain in their original language and that the audio track defaults to the original version, providing an authentic viewing experience. ✨ : : Keep titles in their original language : Always use the original audio track (also works on shorts) : Prevent description translations : Choose your preferred subtitle language. If unavailable, subtitles are automatically disabled (auto-generated ones are always ignored)<p>If this Add-On has been useful to you, you can support its development on KO-FI: </p><a rel=\"nofollow\" href=\"https://prod.outgoing.prod.webservices.mozgcp.net/v1/63738e28454fa4a734182242eb21153429226a68670cd80ddd954c4cb982d12b/https%3A//ko-fi.com/yougo\">https://ko-fi.com/yougo</a> 🙏<p>Want to report an issue or ask for a feature : </p><a rel=\"nofollow\" href=\"https://prod.outgoing.prod.webservices.mozgcp.net/v1/d4df9d04b1be2c94e5ef9161e7f09abbe2bdbf98b26e4e01b11d98fde62d4f61/https%3A//github.com/YouG-o/YouTube_No_Translation/issues\">https://github.com/YouG-o/YouTube_No_Translation/issues</a>","contentLength":746,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44416009"},{"title":"Many ransomware strains will abort if they detect a Russian keyboard installed (2021)","url":"https://krebsonsecurity.com/2021/05/try-this-one-weird-trick-russian-hackers-hate/","date":1751221795,"author":"air7","guid":175400,"unread":true,"content":"<p>In a  discussion last week on ransomware attacks, KrebsOnSecurity <a href=\"https://twitter.com/briankrebs/status/1392163072970829830\" target=\"_blank\" rel=\"noopener\">noted</a> that virtually all ransomware strains have a built-in failsafe designed to cover the backsides of the malware purveyors: They simply will not install on a  computer that already has one of many types of virtual keyboards installed — such as Russian or Ukrainian. So many readers had questions in response to the tweet that I thought it was worth a blog post exploring this one weird cyber defense trick.</p><div><img aria-describedby=\"caption-attachment-55609\" decoding=\"async\" src=\"https://krebsonsecurity.com/wp-content/uploads/2021/05/cis.png\" alt=\"\" width=\"764\" height=\"593\"><p>The Commonwealth of Independent States (CIS) more or less matches the exclusion list on an awful lot of malware coming out of Eastern Europe.</p></div><p>The Twitter thread came up in a discussion on <a href=\"https://krebsonsecurity.com/2021/05/a-closer-look-at-the-darkside-ransomware-gang/\" target=\"_blank\" rel=\"noopener\">the ransomware attack against Colonial Pipeline</a>, which earlier this month shut down 5,500 miles of fuel pipe for nearly a week, causing fuel station supply shortages throughout the country and driving up prices. The <a href=\"https://www.fbi.gov/news/pressrel/press-releases/fbi-statement-on-network-disruption-at-colonial-pipeline\" target=\"_blank\" rel=\"noopener\">FBI said</a> the attack was the work of , a new-ish ransomware-as-a-service offering that says it targets only large corporations.</p><p>DarkSide and other Russian-language affiliate moneymaking programs have long barred their criminal associates from installing malicious software on computers in a host of Eastern European countries, including Ukraine and Russia. This prohibition dates back to the earliest days of organized cybercrime, and it is intended to minimize scrutiny and interference from local authorities.</p><p>In Russia, for example, authorities there generally will not initiate a cybercrime investigation against one of their own unless a company or individual within the country’s borders files an official complaint as a victim. Ensuring that no affiliates can produce victims in their own countries is the easiest way for these criminals to stay off the radar of domestic law enforcement agencies.</p><p>Possibly feeling the heat from being <a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2021/05/12/fact-sheet-president-signs-executive-order-charting-new-course-to-improve-the-nations-cybersecurity-and-protect-federal-government-networks/\" target=\"_blank\" rel=\"noopener\">referenced</a> in <a href=\"https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/\" target=\"_blank\" rel=\"noopener\">President Biden’s Executive Order on cybersecurity</a> this past week, the DarkSide group sought to distance itself from their attack against Colonial Pipeline. In a message posted to its victim shaming blog, DarkSide tried to say it was “apolitical” and that it didn’t wish to participate in geopolitics.</p><p>“Our goal is to make money, and not creating problems for society,” the DarkSide criminals wrote last week. “From today we introduce moderation and check each company that our partners want to encrypt to avoid social consequences in the future.”</p><p>But here’s the thing: <em>Digital extortion gangs like DarkSide take great care to make their entire platforms geopolitical, because their malware is engineered to work only in certain parts of the world.</em></p><p>DarkSide, like a great many other malware strains, has a hard-coded do-not-install list of countries which are the principal members of the Commonwealth of Independent States (CIS) — former Soviet satellites that mostly have favorable relations with the Kremlin. The full exclusion list in DarkSide (published by ) is below:</p><div><img aria-describedby=\"caption-attachment-55610\" decoding=\"async\" loading=\"lazy\" src=\"https://krebsonsecurity.com/wp-content/uploads/2021/05/excludelang.png\" alt=\"\" width=\"946\" height=\"494\" srcset=\"https://krebsonsecurity.com/wp-content/uploads/2021/05/excludelang.png 946w, https://krebsonsecurity.com/wp-content/uploads/2021/05/excludelang-768x401.png 768w, https://krebsonsecurity.com/wp-content/uploads/2021/05/excludelang-782x408.png 782w, https://krebsonsecurity.com/wp-content/uploads/2021/05/excludelang-267x140.png 267w\" sizes=\"(max-width: 946px) 100vw, 946px\"></div><p>Simply put, countless malware strains will check for the presence of one of these languages on the system, and if they’re detected the malware will exit and fail to install.</p><p>Will installing one of these languages keep your Windows computer safe from all malware? Absolutely not. There is plenty of malware that doesn’t care where in the world you are. And there is no substitute for adopting a defense-in-depth posture, and avoiding risky behaviors online.</p><p>But is there really a downside to taking this simple, free, prophylactic approach? None that I can see, other than perhaps a sinking feeling of capitulation. The worst that could happen is that you accidentally toggle the language settings and all your menu options are in Russian.</p><p>If this happens (and the first time it does the experience may be a bit jarring) hit the Windows key and the space bar at the same time; if you have more than one language installed you will see the ability to quickly toggle from one to the other. The little box that pops up when one hits that keyboard combo looks like this:</p><p>Cybercriminals are notoriously responsive to defenses which cut into their profitability, so why wouldn’t the bad guys just change things up and start ignoring the language check? Well, they certainly can and maybe even will do that (<a href=\"https://www.fireeye.com/blog/threat-research/2021/05/shining-a-light-on-darkside-ransomware-operations.html\" target=\"_blank\" rel=\"noopener\">a recent version of DarkSide analyzed by Mandiant</a> did  perform the system language check).</p><p>But doing so increases the risk to their personal safety and fortunes by some non-trivial amount, said , chief research officer at New York City-based cyber investigations firm <a href=\"https://www.unit221b.com\" target=\"_blank\" rel=\"noopener\">Unit221B</a>.</p><p>Nixon said because of Russia’s unique legal culture, criminal hackers in that country employ these checks to ensure they are only attacking victims outside of the country.</p><p>“This is for their legal protection,” Nixon said. “Installing a Cyrillic keyboard, or changing a specific registry entry to say ‘RU’, and so forth, might be enough to convince malware that you are Russian and off limits. This can technically be used as a ‘vaccine’ against Russian malware.”</p><p>Nixon said if enough people do this in large numbers, it may in the short term protect some people, but more importantly in the long term it forces Russian hackers to make a choice: Risk losing legal protections, or risk losing income.</p><p>“Essentially, Russian hackers will end up facing the same difficulty that defenders in the West must face — the fact that it is very difficult to tell the difference between a domestic machine and a foreign machine masquerading as a domestic one,” she said.</p><p>KrebsOnSecurity asked Nixon’s colleague at Unit221B — founder  — what he thought about the efficacy of another anti-malware approach suggested by Twitter followers who chimed in on last week’s discussion: Adding entries to the Windows registry that specify the system is running as a virtual machine (VM). In a bid to stymie analysis by antivirus and security firms, some malware authors have traditionally configured their malware to quit installing if it detects it is running in a virtual environment.</p><p>But James said this prohibition is no longer quite so common, particularly since so many organizations have transitioned to virtual environments for everyday use.</p><p>“Being a virtual machine doesn’t stop malware like it used to,” James said. “In fact, a lot of the ransomware we’re seeing now is running on VMs.”</p><p>But James says he loves the idea of everyone adding a language from the CIS country list so much he’s produced his own <a href=\"https://github.com/Unit221B/Russian\" target=\"_blank\" rel=\"noopener\">clickable two-line Windows batch script</a> that adds a Russian language reference in the specific Windows registry keys that are checked by malware. The script effectively allows one’s Windows PC to look like it has a Russian keyboard installed without actually downloading the added script libraries from Microsoft.</p><p>To install a different keyboard language on a Windows 10 computer the old fashioned way, hit the Windows key and X at the same time, then select Settings, and then select “Time and Language.” Select Language, and then scroll down and you should see an option to install another character set. Pick one, and the language should be installed the next time you reboot. Again, if for some reason you need to toggle between languages, Windows+Spacebar is your friend.</p>","contentLength":7213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44415233"},{"title":"Tools I love: mise(-en-place)","url":"https://blog.vbang.dk/2025/06/29/tools-i-love-mise/","date":1751219781,"author":"micvbang","guid":175387,"unread":true,"content":"<p>Once in a while you get introduced to a tool that instantly changes the way you work. For me, <a href=\"https://github.com/jdx/mise\">mise</a> is one of those tools.</p><p>mise is the logical conclusion to a lot of the meta-tooling that exists around language-specific version and package managers like <a href=\"https://asdf-vm.com/\">asdf</a>, <a href=\"https://github.com/nvm-sh/nvm\">nvm</a>, <a href=\"https://docs.astral.sh/uv/\">uv</a>, <a href=\"https://github.com/pyenv/pyenv\">pyenv</a> etc. It makes it exceptionally easy to install, use, and manage software. It also allows you to manage <a href=\"https://mise.jdx.dev/environments/\">environment variables</a> and <a href=\"https://mise.jdx.dev/tasks/\">declare tasks</a> (run commands).</p><p>The first step in getting an intuitive understanding of what mise can help you with is to use it to install a tool. Pick your favorite and try it out; it supports <a href=\"https://mise.jdx.dev/registry.html\"></a>!</p><div><div><pre><code>jj\ncommand_not_found_handler:5: not found: jj\n\nmise use jj\nmise ~/projects/examples_mise/mise.toml tools: jj@0.30.0\n\njj version\njj 0.30.0\n\n ..\n\njj version\ncommand_not_found_handler:5: not found: jj\n\neaxmples_mise\n\njj version\njj 0.30.0\n</code></pre></div></div><p>As the above shows, with mise we’re just one command away from installing and trying out a new tool, e.g. .</p><p>In the above we that mise printed <code>mise ~/projects/examples_mise/mise.toml tools: jj@0.30.0</code>. This tells us that mise has created (or updated) the mise configuration . \nWe also see that if we cd out of , the  command is no longer available. If we cd back into , it becomes available again; unless you explicitly install tools globally, mise will only make the tools available which are mentioned in a  file on the path from your current directory to the root of your file system. That of course means that we could potentially meet multiple  files when going back up to the root of the file system. Mise handles this by concatting the configurations and overwriting conflicting configurations, letting the file furthest down the tree win.</p><p>This is a clever design as it allows us to configure different versions of the same tool to be available in different directories. Let’s have a look at what the  file looks like:</p><p>If we want a specific version of  to be installed in a specific directory, we just update the toml file to say e.g. .</p><p>Let’s see what it looks like to use mise to manage Python versions for two projects with different requirements:</p><div><div><pre><code>tree\n\n├── project_new\n│\t└── mise.toml\n└── project_old\n    └── mise.toml\n\nproject_new/mise.toml\ntools]\npython project_old/mise.toml\ntools]\npython project_new\npython \nPython 3.11.13\n\n ../project_old\npython \nPython 3.8.20\n</code></pre></div></div><p>When we cd into one of the directories listed above, mise automatically makes the version of the tool configured in  available to us. If it isn’t already installed, mise will install it for us. The implication of this is that you can commit a  to your repository, and anyone that has mise installed will automatically get and use the expected dev tools when they enter the project directory. And when it’s time to upgrade a dev tool, you can just update the version number in  and everyone will start using the new version!</p><p>The fact that mise makes tools available to you according to the  file in your current working directory has further implications: it’s not just developer machines that can benefit from using mise; CI/CD pipelines can benefit greatly as well! When you use mise in your pipelines, you avoid the problem of having out of sync versions between developer and build machines. You get to have a single place where you can configure the version of your dev tools everywhere!</p><p>As I mentioned in the beginning, besides managing dev tools, mise also allows you to <a href=\"https://mise.jdx.dev/tasks/toml-tasks.html\">declare and run so-called tasks</a>. Think of a task as an advanced invocation of a bash script. Even if we use tasks as just plain bash scripts (they can do a lot more), it can be a major advantage to declare common operations such as building, testing, linting etc. as mise tasks, since all developers get access to them and will run their commands in exactly the same way every time. If you’re diligent in your naming, you can even make the experience of building or testing across projects identical.</p><p>The following are examples of some very simple Python-related tasks declared in :</p><div><div><pre><code></code></pre></div></div><p>Adding this to  will make the commands  and  available. Again, if you check this in to your repo, the commands will be available to all developers and pipelines. And reusing these names in your rust project means that you can use the same commands to tell cargo to install your crates or run your tests.</p><p>Once you’ve declared your tasks you should of course also use them in your CI/CD pipeline. Doing this makes you less dependent on the particular yaml syntax and arbitrary requirements of your provider, and makes it easier to move to another one if you need to. It also ensures that there’s a standard way to build and test your code, helping to further reduce the amount of “it works on my machine”.</p><p>There’s a lot of depth to what you can use mise to help you automate. It’s a lovely tool and I hope I’ve spiked your interest enough to give it a try!</p><p>Although this is a very obvious problem, I want to make it explicit: a major concern of all software dependency management is control of your supply chain; how easy is it for somebody to insert malicious code into a binary you will run hugely impacts the integrity of your systems and data. Depending on your industry, it might not be feasible to use mise as it’s pretty opaque where your dependencies will be downloaded from.</p>","contentLength":5289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44414987"},{"title":"Loss of key US satellite data could send hurricane forecasting back 'decades'","url":"https://www.theguardian.com/us-news/2025/jun/28/noaa-cuts-hurricane-forecasting-climate","date":1751218748,"author":"trauco","guid":175357,"unread":true,"content":"<p>A critical US atmospheric data collection program will be halted by Monday, giving weather forecasters just days to prepare, according to a <a href=\"https://www.ospo.noaa.gov/data/messages/2025/06/MSG_20250625_1735.html\" data-link-name=\"in body link\">public notice</a> sent this week. Scientists that the Guardian spoke with say the change could set hurricane forecasting back “decades”, just as this year’s season ramps up.</p><p>In a National Oceanic and Atmospheric Administration (Noaa) message sent on Wednesday to its scientists, the agency said that “due to recent service changes” the Defense Meteorological Satellite Program (DMSP) will “discontinue ingest, processing and distribution of all DMSP data no later than June 30, 2025”.</p><p>Due to their unique characteristics and ability to map the entire world twice a day with <a href=\"https://bsky.app/profile/pppapin.bsky.social/post/3lcbcygsfmz2y\" data-link-name=\"in body link\">extremely high resolution</a>, the three DMSP satellites are a primary source of information for scientists to monitor Arctic sea ice and hurricane development. The DMSP partners with Noaa to make weather data collected from the satellites publicly available.</p><p>The reasons for the changes, and which agency was driving them, were not immediately clear. Noaa said they would not affect the quality of forecasting.</p><p>However, the Guardian spoke with several scientists inside and outside of the US government whose work depends on the DMSP, and all said there are no other US programs that can form an adequate replacement for its data.</p><p>“We’re a bit blind now,” said Allison Wing, a hurricane researcher at Florida State University. Wing said the DMSP satellites are the only ones that let scientists see inside the clouds of developing hurricanes, giving them a critical edge in forecasting that now may be jeopardized.</p><p>“Before these types of satellites were present, there would often be situations where you’d wake up in the morning and have a big surprise about what the hurricane looked like,” said Wing. “Given increases in hurricane intensity and increasing prevalence towards rapid intensification in recent years, it’s not a good time to have less information.”</p><p>The satellites also formed a unique source of data for tracking changes to the Arctic and Antarctic, and had been tracking changes to polar sea ice continuously <a href=\"https://www.frontiersin.org/journals/remote-sensing/articles/10.3389/frsen.2022.1021781/full\" data-link-name=\"in body link\">for more than 40 years</a>.</p><p>“These are some of the regions that are changing the fastest around the planet,” said Carlos Moffat, an oceanographer at the University of Delaware who had been working on a research project in Antarctica that depended on DMSP data. “This new announcement about the sea ice data really amounts to blinding ourselves and preventing us from observing these critical systems.”</p><p>Researchers say the satellites themselves are operating normally and do not appear to have suffered any errors that would physically prevent the data from continuing to be collected and distributed, so the abrupt data halt might have been an intentional decision.</p><p>“It’s pretty shocking,” Moffat said. “It’s hard to imagine what would be the logic of removing access now and in such a sudden manner that it’s just impossible to plan for. I certainly don’t know of any other previous cases where we’re taking away data that is being collected, and we’re just removing it from public access.”</p><p>The loss of DMSP comes as Noaa’s weather and climate monitoring services have become critically understaffed this year as <a href=\"https://www.theguardian.com/us-news/donaldtrump\" data-link-name=\"in body link\">Donald Trump</a>’s so-called “department of government efficiency” (Doge) initiative has <a href=\"https://www.theguardian.com/us-news/2025/may/26/trump-doge-la-wildfire-recovery\" data-link-name=\"in body link\">instilled</a><a href=\"https://www.theguardian.com/technology/2025/mar/25/doge-musk-spacex-starlink-contracts\" data-link-name=\"in body link\">draconian</a><a href=\"https://www.theguardian.com/us-news/2025/apr/01/trump-cuts-noaa-spam-emails\" data-link-name=\"in body link\">cuts</a> to federal environmental programs.</p><p>A current Noaa scientist who wishes to remain anonymous for fear of retaliation said that the action to halt the DMSP, when taken in context with other recent moves by the <a href=\"https://www.theguardian.com/us-news/trump-administration\" data-link-name=\"in body link\">Trump administration</a>, amounted to “a systematic destruction of science”.</p><p>The researcher also confirmed that federal hurricane forecasters were left unprepared for the sudden change with only a few days of notice.</p><p>“It’s an instant loss of roughly half of our capabilities,” said the scientist. “You can’t expect us to make accurate forecasts and warnings when you take the useful tools away. It frankly is an embarrassment for the government to pursue a course with less data and just pretend everything will be OK.”</p><p>“This is a huge hit to our forecasting capabilities this season and beyond, especially our ability to predict rapid intensification or estimate the strength of storms in the absence of hurricane hunters,” said Michael Lowry, a meteorologist who has worked at Noaa’s National Hurricane Center and with the Federal Emergency Management Agency. “The permanent discontinuation of data from these satellites is senseless, reckless and puts at risk the lives of tens of millions of Americans living in hurricane alley.”</p><p>The DMSP dates back to 1963, when the Department of Defense determined a need for high-resolution cloud forecasts to help them plan spy missions. The program, which had been the longest-running weather satellite initiative in the federal government, has since evolved into a critical source of information not just on the inner workings of hurricanes, but also on polar sea ice, wildfires, solar flares and the aurora.</p><p>In recent years, the DMSP <a href=\"https://spacenews.com/a-race-against-time-to-replace-aging-military-weather-satellites/\" data-link-name=\"in body link\">had struggled to maintain</a> consistent funding and priority within the Department of Defense as it transitioned away from its cold war mission. The only other nation with similar satellite capability is Japan, and <a href=\"https://nsidc.org/data/user-resources/data-announcements/user-notice-degraded-ssmis-data-delivery-affecting-data-products\" data-link-name=\"in body link\">messages posted earlier in June</a> indicate that scientists had already been considering a switch to <a href=\"https://www.earthdata.nasa.gov/centers/amsr-sips\" data-link-name=\"in body link\">the Japanese data</a> in case of a DMSP outage – though that transition will take time.</p><p>Neither Noaa nor the Department of Defense specified exactly which service changes may have prompted such a critical program to be so abruptly halted.</p><p>In a statement to the Guardian, Noaa’s communications director, Kim Doster, said: “The DMSP is a single dataset in a robust suite of hurricane forecasting and modeling tools in the National Weather Service portfolio. This routine process of data rotation and replacement would go unnoticed in past administrations, but the media is insistent on criticizing the great work that Noaa and its dedicated scientists perform every day.</p><p>“Noaa’s data sources are fully capable of providing a complete suite of cutting-edge data and models that ensure the gold-standard weather forecasting the American people deserve.”</p><p>One Noaa source the Guardian spoke to said the loss of DMSP’s high-resolution data could not be replaced by any other existing Noaa tool.</p><p>A statement from an official at US space force, which is part of the Department of Defense, said: “The National Oceanic and Atmospheric Administration (Noaa) operates the Defense Meteorological Satellite Program (DMSP) for the DoD on behalf of the US Space Force, who has satellite control authority.”</p><p>The official went on to say that Noaa receives the data from the US navy’s Fleet Numerical Meteorology and Oceanography Center (FNMOC) and added: “While the Space Force does provide DMSP data and processing software to DoD users, to include the US Navy, questions about the reasons for FNMOC’s changes to DMSP data processing should be directed to the Navy.</p><p>“Even as FNMOC is making a change on their end, the posture on sharing DMSP data has not changed. Noaa has been making this DMSP data publicly available, and many non-DoD entities use this data that is originally processed by FNMOC.</p><p>“DMSP satellites and instruments are still functional. The data provided to FNMOC is just one way the DoD uses DMSP data. DoD users (including the Navy) will continue to receive and operationally use DMSP data sent to weather satellite direct readout terminals across the DoD.”</p><p>The Guardian is approaching the US navy for comment.</p>","contentLength":7648,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44414853"},{"title":"Personal care products disrupt the human oxidation field","url":"https://www.science.org/doi/10.1126/sciadv.ads7908","date":1751217616,"author":"XzetaU8","guid":175340,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44414719"},{"title":"We accidentally solved robotics by watching 1M hours of YouTube","url":"https://ksagar.bearblog.dev/vjepa/","date":1751213333,"author":"alexcos","guid":175516,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44414171"},{"title":"The $25k car is going extinct?","url":"https://media.hubspot.com/why-the-25000-car-is-going-extinct","date":1751212796,"author":"pseudolus","guid":175515,"unread":true,"content":"<p>Seabaugh says dealers have long preferred to stock pricier vehicles and vehicles with higher trims. During the supply shortages of Covid, however, they took it to another level, as both dealers and manufacturers wanted to focus on vehicles with the highest margins as they tried to overcome lower inventories.</p><p>But, Drury notes, the proliferation of expensive models and higher trims can’t all be pinned on dealers and automakers. He says  have become accustomed to features like complex infotainment systems, cameras, and heated seats and want to trade up for increasingly luxurious settings when they buy new cars.</p><p>In 2021, Ford couldn’t keep up with the demand for fancy versions of the Explorer SUV, <a href=\"https://www.motortrend.com/news/2021-ford-explorer-timberline-first-look-review?_hsenc=p2ANqtz-8aqSJFlFGgZwL5EFP10-i0zaEhm5E3TICWStvy2w7H9eLuMKED-3PuZJZ2N7nJ0q9ucmYC\" data-hs-link-id=\"0\" data-hs-link-id-v2=\"OVZuwOi3\" target=\"_blank\">releasing</a> new higher-trim versions in the middle of the year. For 2025, Honda <a href=\"https://www.caranddriver.com/news/a61646217/2025-honda-odyssey-updated-price/?_hsenc=p2ANqtz-8aqSJFlFGgZwL5EFP10-i0zaEhm5E3TICWStvy2w7H9eLuMKED-3PuZJZ2N7nJ0q9ucmYC\" data-hs-link-id=\"0\" data-hs-link-id-v2=\"0v4AgrF1\" target=\"_blank\">eliminated</a> its lowest trim of the Odyssey minivan.</p><p>“When people keep buying, that's the chicken and the egg, right?” says Drury. “Do I blame the automaker? No. Do I blame the dealer? I can't blame them either. Do I blame the customer? No, because they want these things.”</p>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44414115"},{"title":"Event – Fast, In-Process Event Dispatcher","url":"https://github.com/kelindar/event","date":1751210359,"author":"kelindar","guid":175386,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44413809"},{"title":"The Medley Interlisp Project: Reviving a Historical Software System [pdf]","url":"https://interlisp.org/documentation/young-ccece2025.pdf","date":1751208310,"author":"pamoroso","guid":175444,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44413574"},{"title":"I made my VM think it has a CPU fan","url":"https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html","date":1751205318,"author":"todsacerdoti","guid":175224,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44413185"},{"title":"Bloom Filters by Example","url":"https://llimllib.github.io/bloomfilter-tutorial/","date":1751198169,"author":"ibobev","guid":175339,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44412370"},{"title":"Show HN: Octelium – FOSS Alternative to Teleport, Cloudflare, Tailscale, Ngrok","url":"https://github.com/octelium/octelium","date":1751196257,"author":"geoctl","guid":175223,"unread":true,"content":"<p>I have been working on Octelium for quite a few years now but it was open sourced only by late May 2025. Octelium, as described more in detail in the repo's README, is simply an open source, self-hosted, unified platform for zero trust resource access that is primarily meant to be a modern alternative to corporate VPNs and remote access tools. It can operate as a remote access/corporate VPN (i.e. alternative to Twingate, Tailscale, OpenVPN Access Server, etc...), a ZTNA/BeyondCorp platform (i.e. alterntive to Cloudflare Access, Teleport, Google BeyondCorp, etc...), and it can also operate as an API/AI gateway, an infrastructure for MCP and A2A architectures and meshes, an ngrok alternative, a homelab infrastructure or even as a more advanced Kubernetes ingress. It's basically designed to operate like a unified Kubernetes-like scalable architecture for zero trust secure/remote access that's suitable for different human-to-workload and workload-to-workload environments. You can read more in detail the full set of main features and links about how it works in the repo's README or directly in the docs <a href=\"https://octelium.com/docs\" rel=\"nofollow\">https://octelium.com/docs</a></p>","contentLength":1140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44412207"}],"tags":["hn"]}