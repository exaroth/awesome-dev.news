{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":50,"items":[{"title":"Platform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platfor... Puja Abbassi","url":"https://www.youtube.com/watch?v=Xr0Eb-ybvck","date":1751401964,"author":"CNCF [Cloud Native Computing Foundation]","guid":179271,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPlatform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platforms - Puja Abbassi, Giant Swarm\n\nEveryone is talking about platform engineering. You see smooth demos of golden paths and self-service platforms. However, there’s a significant area of challenges that is less talked about and thus often neglected when designing developer platforms.\n\nIn this talk, we’ll explore the often-overlooked day 2 challenges that platform teams face. We’ll dissect the area of day 2 into the many sub-areas and challenges they pose. Drawing on real-world experiences, including notable migrations that many in this community have faced, we'll shed light on the pain behind developer platforms and discuss solutions to these issues. Among others, we’ll delve into practical strategies for managing versioning and rollouts, and highlight the significant hurdles encountered, such as dependencies on end user teams or GitOps.\n\nJoin us for insights, strategies, and stories from the trenches that will help you navigate the complexities of service iteration in developer platforms.</article>","contentLength":1476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Xr0Eb-ybvck?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=hERRANApN5c","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179127,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb</article>","contentLength":476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/hERRANApN5c?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Con... Gordon Radlein","url":"https://www.youtube.com/watch?v=dlDTX-aDNzg","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179128,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Contribution - Gordon Radlein, Datadog\n\nOpenTelemetry has accelerated the commoditization of instrumentation. Telemetry generation is becoming a solved problem, an implementation detail. But this has created a new challenge: a wealth of standardized signals with no standard meaning. Different systems instrumented with different semantics generating telemetry in their own unique language. And while signal correlation connects specific workloads, it fails when we need to understand our systems at a macro scale by joining disparate datasets.\nThat is, until we all agreed to speak the same language.\n\nJust as English as a lingua franca fueled progress across the internet, OpenTelemetry Semantic Conventions are providing a shared language for our systems. In this talk we’ll discuss why semantic interoperability is the real connective tissue, how it’s fueling deeper insights into our production environments, and the key role it plays in enabling the AI systems that are rapidly ushering in the next revolution of our industry.</article>","contentLength":1500,"flags":null,"enclosureUrl":"https://www.youtube.com/v/dlDTX-aDNzg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Welcome + Opening Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=_rqgWHaEvgc","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179129,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nWelcome + Opening Remarks - Austin Parker, Honeycomb</article>","contentLength":435,"flags":null,"enclosureUrl":"https://www.youtube.com/v/_rqgWHaEvgc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere","url":"https://www.youtube.com/watch?v=Z4umnlRdLtA","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179130,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere\n\nLogs can get very expensive and often how useful all those logs are is unknown, some are but many are not. It is very difficult to know which logs are useful and how exactly they are used. With Chronosphere's Control plane for logs users can now get a comprehensive analysis of value and usage patterns, along with sophisticated recommendations and control actions that allow some or most of the value derived from those logs to be preserved. In order to achieve our goals we have enhanced Fluent Bit to be more flexible in which logs are actioned upon and will share useful future additions to it.</article>","contentLength":1072,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Z4umnlRdLtA?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson","url":"https://www.youtube.com/watch?v=J_hHiwa_3QU","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179131,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson, Chronosphere\n\nHybrid cloud isn’t a stepping stone—it’s a destination. With 39% of CNCF survey respondents already operating in hybrid environments, this model is here to stay. But as teams pursue cloud-native architectures, many skip a critical step: developing a clear cloud strategy and an observability approach to match.\nThe result is predictable— widening visibility gaps, redundant tooling and data, and spiraling costs as teams try to stitch together disconnected, vendor-specific systems never meant to work in concert. Hybrid environments expose these issues quickly, especially when workloads span multiple platforms without a unified way to observe and understand them.\nModernization efforts demand open observability from the start—not as an add-on. Technologies like OpenTelemetry, Fluent Bit, and Prometheus act as connective tissue across clouds, clusters, and on-prem infrastructure, enabling standardization where it’s needed most.\nThis talk outlines how to center open observability in your modernization journey: where to standardize architectural layers, how to maintain a more open approach, and why these decisions have long-term payoff. \nHybrid complexity is inevitable. Leading with open observability is how you stay in control—now and in the future.</article>","contentLength":1761,"flags":null,"enclosureUrl":"https://www.youtube.com/v/J_hHiwa_3QU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data I... Dotan Horovits","url":"https://www.youtube.com/watch?v=C5Y3qnEJSY8","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179132,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data Insights - Dotan Horovits, AWS OpenSearch</article>","contentLength":505,"flags":null,"enclosureUrl":"https://www.youtube.com/v/C5Y3qnEJSY8?version=3","enclosureMime":"","commentsUrl":null},{"title":"Building Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Per... Denton Krietz","url":"https://www.youtube.com/watch?v=zgnY8szpKUw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179118,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nBuilding Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Persistent Queue - Denton Krietz, Bindplane\n\nThe OpenTelemetry Collector’s persistent queue provides a robust mechanism for handling data bursts, destination outages, and processing delays, ensuring no telemetry data is lost—but from experience, it’s consistently one of the collector's least understood features.\n\nIn this talk, we’ll explore the inner workings of the OTel Collector’s persistent queue, including how it buffers data, ensures durability, and enables replay after failures. Attendees will learn how to configure persistent queues for their unique workloads, optimize their telemetry pipeline performance, and troubleshoot common pitfalls.\n\nWhether you’re a site reliability engineer, developer, or observability enthusiast, this talk will equip you with the knowledge to deeply understand persistent queues to optimize your telemetry pipeline in production.</article>","contentLength":1348,"flags":null,"enclosureUrl":"https://www.youtube.com/v/zgnY8szpKUw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Introducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner & Ray Jenkins, Streamfold","url":"https://www.youtube.com/watch?v=xeQnP8Ct7qY","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179119,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nIntroducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner &amp; Ray Jenkins, Streamfold\n\nIn this talk, we'll introduce Rotel—an open-source OpenTelemetry collector built in Rust. Rotel is lightweight and resource-efficient, integrating seamlessly into your development workflow. Its compact design lets you package it with your Python or NodeJS projects, so telemetry collection runs alongside your code without needing additional sidecars.\n\nWe'll explore how rethinking telemetry collection at the edge can empower developers right from the early stages of development, paving the way for broader OpenTelemetry adoption. You’ll learn how Rust’s low-overhead FFI enables native extensions for telemetry filtering, transformation, and enrichment using Python and Typescript.\n\nBy leveraging Rust’s performance strengths, Rotel avoids the overhead of garbage collection, resulting in lower memory usage and reduced latency. Its quick cold start times make it a natural fit for modern cloud-native, serverless, and edge computing environments. Join us to discover how moving telemetry collection closer to the source can help you analyze high-volume, high-fidelity signals more effectively.</article>","contentLength":1585,"flags":null,"enclosureUrl":"https://www.youtube.com/v/xeQnP8Ct7qY?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTele... Diana Todea","url":"https://www.youtube.com/watch?v=wWON2NT41lE","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179120,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTelemetry - Diana Todea, Aircall\n\nBecoming a contributor to an open-source project is a transformative step in any developer's career. This session explores the journey from first-time contributor to active developer, covering best practices for navigating project communities, understanding codebases, and making meaningful contributions. Learn strategies for selecting the right project, mastering collaboration tools, and embracing the culture of open-source development. The audience will be inspired about my one year journey with the open source project OpenTelemetry and how I have built a proof of concept for it and achieved developer status for this project. By the end of this talk, the public will gain insights into the tools to become a better developer and how to build more engagement with the community.</article>","contentLength":1284,"flags":null,"enclosureUrl":"https://www.youtube.com/v/wWON2NT41lE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Telemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchma... Henrik Rexed","url":"https://www.youtube.com/watch?v=tZho5W9L_Z8","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179121,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nTelemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchmark Analysis - Henrik Rexed, Dynatrace\n\nIn a push to standardize observability practices, the cloud-native community has embraced OpenTelemetry, offering a unified framework for metrics, logs, and traces. Prior to this, log processing relied on agents like fluent, evolving into fluentbit. With fluentbit's recent expansion to support additional signals and the OpenTelemetry Collector's emergence, a pertinent question arises: Which is the superior choice for performance?\n\nThis session delves into:\n- Unveiling the distinctions between Fluent Bit and the OpenTelemetry Collector.\n- Sharing the findings derived from a series of benchmark tests.\n- Providing valuable insights to empower the community in selecting the most fitting agent for their cloud-native environments.</article>","contentLength":1240,"flags":null,"enclosureUrl":"https://www.youtube.com/v/tZho5W9L_Z8?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Spec-tacular Game Show - Liudmila Molkova, Ted Young, Tyler Helmuth, Jamie Danielson, Alex Boten","url":"https://www.youtube.com/watch?v=ipFVu0dl5Bw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179122,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPanel: The Spec-tacular Game Show - Liudmila Molkova, Microsoft; Ted Young, Grafana Labs; Tyler Helmuth, Jamie Danielson &amp; Alex Boten, Honeycomb\n\nFrom OTLP to OTTL, engineers are excited about a lot of things. But there is one thing that excites them above all else and that is correcting people. Welcome to “The Spec-tacular Game Show”.\n\nIn this fun game show our panelists will be given incorrect statements about the OpenTelemetry Specification or Semantic Convention. The panelists will buzz in, identify what’s wrong, and state the correction. If none of the panelists know the answer the audience will get a chance to answer to steal the point. The panelist (or audience) with the most points wins!\n\nAfter each question we’ll spend a time explaining why the Spec and Semconv is the way it is and highlight how it produces the production-quality telemetry you know and love. Join us for a fun, relaxing, (snarky) panel about everyone’s favorite part of Otel!</article>","contentLength":1356,"flags":null,"enclosureUrl":"https://www.youtube.com/v/ipFVu0dl5Bw?version=3","enclosureMime":"","commentsUrl":null},{"title":"How To Think About Instrumentation Overhead - Jason Plumb, Splunk","url":"https://www.youtube.com/watch?v=fvmzAX_ZyvM","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179123,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nHow To Think About Instrumentation Overhead - Jason Plumb, Splunk\n\nNovice observability practitioners are often overly obsessed with performance. They might approach instrumentation with skepticism and have concerns about latency degradation or resource consumption. This talk is a primer on the topic of instrumentation overhead, and it will teach you how to think about overhead in an observability context. We will cover the causes of overhead and why overhead is so hard to measure and even harder to predict reliably. Lastly, we will present some practical techniques for understanding overhead in your environment and some strategies for coping with it.</article>","contentLength":1042,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fvmzAX_ZyvM?version=3","enclosureMime":"","commentsUrl":null},{"title":"No Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft","url":"https://www.youtube.com/watch?v=fU6jsw0yaVU","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179124,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNo Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft\n\nThe best telemetry starts at the source—inside the client libraries.\nBut in most cases, that means taking a dependency on the OpenTelemetry API from your library. And while it’s stable, minimal, reliable, and safely no-op unless configured—transitive dependencies are still the bane of any library developer’s existence, and most of us try to avoid them.\n\nTo work around this, people reach for abstractions, plugins, bridges, or even OTel forks that break context propagation. The result? A poor user experience. Users must find the right plugin, install it, wire it up—and still hit the diamond dependency problem, now it just affects a subset of users.\n\nBut what if you could take a truly optional dependency? If OpenTelemetry is on the classpath, instrumentation kicks in. If it’s not, no harm done.\nHow hard is that to pull off? How reliable? How performant?\n\nLet’s explore that—through the lens of the next generation of Azure SDKs for Java. Spoiler: it’s easy and fast, and as a side-bonus, we can fall back to logs-based tracing if OTel is not found.</article>","contentLength":1544,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fU6jsw0yaVU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Closing Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=eDbQfZ9eoNI","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179125,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nClosing Remarks - Austin Parker, Honeycomb</article>","contentLength":425,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eDbQfZ9eoNI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace","url":"https://www.youtube.com/watch?v=di5nhYvUh6w","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179126,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace\n\nThe OTel Java API, SDK, and ecosystem are perfectly adequate for Android developer to get OTel instrumentation into their apps. But for a host of reasons, the match is not perfect, especially for developers who only write in Kotlin, which is the recommended development language for Android by Google, not the least of which is the emergence of Kotlin Multiple Platform (KMP) as a means to share code between Android, iOS, and many other platforms.\n\nThis session will outline the reasons why we at Embrace is trying to kick-start the development of a pure Kotlin ecosystem for OTel, starting with an API and SDK implementation, and how we are doing it in a way where mobile developers can get value incrementally without having to wait until every aspect is fully built out.\n\nWe want OTel to feel natural and idiomatic for Android developers, and this is the first step towards that end.</article>","contentLength":1361,"flags":null,"enclosureUrl":"https://www.youtube.com/v/di5nhYvUh6w?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Docker MCP Catalog: the Secure Way to Discover and Run MCP Servers","url":"https://www.docker.com/blog/docker-mcp-catalog-secure-way-to-discover-and-run-mcp-servers/","date":1751375060,"author":"Nuno Coracao","guid":178895,"unread":true,"content":"<p>The Model Context Protocol (MCP) ecosystem is exploding. In just weeks, our Docker MCP Catalog has surpassed , validating that developers are hungry for a <a href=\"https://www.docker.com/products/mcp-catalog-and-toolkit/\">secure way to run MCP servers</a>. Today, we’re excited to share major updates to the Docker MCP Catalog, including enhanced discovery features and our new open submission process. With hundreds of developers already requesting to publish their MCP servers through Docker, we’re accelerating our mission to make <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">containerized MCP servers</a> the standard for secure AI tool distribution.</p><p>The rapid adoption of MCP servers also highlights a critical problem — the current practice of running them via npx or uvx commands exposes systems to unverified code with full host access, not to mention dependency management friction. In this post, we’ll explain why Docker is investing in the MCP ecosystem, showcase the new catalog capabilities, and share how you can contribute to building a more secure foundation for AI applications.</p><p><strong>Figure 1: The new Docker MCP Catalog, built for easier discovery.</strong></p><h2>Why Docker is building the MCP Catalog</h2><h3>The security issues in MCP distribution</h3><p>Every time a developer runs npx -y @untrusted/mcp-server or uvx some-mcp-tool, they’re making a dangerous trade-off: convenience over security. These commands execute arbitrary code directly on the host system with full access to:</p><ul><li>Environment variables and secrets</li></ul><p>Some MCP clients limit environment variable access, but even that is not a universal practice. This isn’t sustainable. As MCP moves from experimentation to production, we need a fundamentally different approach.</p><p>Docker has spent over a decade solving exactly these problems for cloud-native applications. We’ve built the infrastructure, tools, and trust that developers rely on to run billions of containers in production. Now, we’re applying these same principles to the MCP ecosystem.</p><p>When you run an MCP server from our Catalog, you get:</p><ul><li> verifying the image hasn’t been tampered with</li><li><strong>Software Bill of Materials (SBOMs)</strong> documenting every component</li><li> from your host system</li><li> to only what the server actually needs</li></ul><p>This isn’t about making life harder for developers—it’s about making security the path of least resistance.</p><h2>Introducing the enhanced MCP Catalog</h2><p>We’ve reimagined the MCP Catalog to make it more accessible and easier to navigate. You can still access the MCP Catalog from Docker Hub and the MCP Toolkit in Docker Desktop just like before, or <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">go straight to the MCP catalog</a>. We’ve gone beyond generic container image listings by building features that help you quickly find the right MCP servers for your AI applications.&nbsp;&nbsp;</p><p>: MCP servers are organized by what they actually do:</p><ul><li>Data Integration (databases, APIs, file systems)</li><li>Development Tools (IDEs, code analysis, testing)</li><li>Communication (email, Slack, messaging platforms)</li><li>Productivity (task management, calendars, note-taking)</li><li>Analytics (data processing, visualization, reporting)</li></ul><p>: Find servers by capability, tools, GitHub tags, and categories — not just by name.</p><p>: Every catalog entry clearly shows whether it’s Docker-built (with transparent build signing and verification) or community-built (containerized and maintained by the publisher).</p><p><strong>Figure 2: Discover MCP servers by use cases.</strong></p><h3>How we classify MCP Servers: Built by Docker vs. community-built</h3><p>: When you see “Built by Docker,” you’re getting our complete security treatment. We control the entire build pipeline, providing cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning.</p><p>: These servers are packaged as Docker images by their developers. While we don’t control their build process, they still benefit from container isolation, which is a massive security improvement over direct execution.</p><p><strong>Tiers serve important roles</strong>: Docker-built servers demonstrate the gold standard for security, while community-built servers ensure we can scale rapidly to meet developer demand. Developers can change their mind after submitting a community-built server and opt to resubmit it as a Docker-built server.</p><p><strong>Figure 3: An example of Built by Docker MCP Server.</strong></p><h2>Open for MCP server submission: Join the secure MCP movement</h2><p>Starting today, we’re opening our submission process to the community. Whether you’re an individual developer or an enterprise team, you can feature your MCP servers on the Docker MCP Catalog. By publishing through our catalog, you’re not just distributing your MCP server — you’re helping establish a new security standard for the entire ecosystem while getting your MCP tools available to millions of developers already using Docker via Docker Hub and Docker Desktop. Your containerized server becomes part of the solution, demonstrating that production-ready AI tools don’t require compromising on security.&nbsp;</p><h3>How to submit your MCP server</h3><ol><li> – Package your MCP server as a Docker image</li><li> – Opt for Docker-built (we handle the build) or community-built (you build and maintain it)</li></ol><p>We’re committed to a fast, transparent review process. Quality MCP servers that follow our security guidelines will be published quickly, helping you reach Docker’s 20+ million developer community.</p><p>ClickHouse is one of the first companies to take advantage of Docker’s MCP Catalog, and they opted for the Docker-built tier to ensure maximum security. Here’s why they chose to partner with Docker:</p><p><a href=\"https://clickhouse.com/\" rel=\"nofollow noopener\" target=\"_blank\"></a><em>, we deliver the fastest analytics database – open-source, and designed for real-time data processing and analytics at scale. As agentic AI becomes more embedded in modern applications, developers are using the ClickHouse MCP server to support intelligent, data-driven workflows that demand low latency, high concurrency, and cost efficiency.</em><em>To make it easier for developers to deploy these workloads, we’re featuring </em><a href=\"https://hub.docker.com/mcp/server/clickhouse/overview\" rel=\"nofollow noopener\" target=\"_blank\"></a><em> on Docker’s MCP Catalog, which provides </em><strong><em>a powerful way to reach 20M+ developers</em></strong><em> and makes it easier for Docker users to discover and use our solution. </em><strong><em>We opted for “Built by Docker” with the highest security standard</em></strong><em>, including cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning. Together with Docker, developers can run ClickHouse MCP Server with confidence, knowing it’s secured, verified, and ready for their agentic applications.” – </em>Tanya Bragin, VP of Product and Marketing Clickhouse</p><p>We’re preparing for the future of cloud-native AI applications. Remote MCP servers will enable:</p><ul><li>Managed MCP services that scale automatically</li><li>Shared capabilities across teams without distributing code</li><li>Stricter security boundaries for sensitive operations</li></ul><h3>Integration with the official MCP registry</h3><p>We’re actively collaborating with the MCP community on the upcoming official registry. Our vision is complementary:</p><ul><li>The official registry provides centralized discovery – the “yellow pages” of available MCP servers</li><li>Docker provides the secure runtime and distribution for those listings</li><li>Together, we create a complete ecosystem where discovery and security work hand-in-hand</li></ul><p>The explosive growth of our MCP Catalog, 1 million pulls and hundreds of publisher requests, tells us developers are ready for change. They want the power of MCP, but they need it delivered securely.</p><p>By establishing containers as the standard for MCP server distribution, we’re not trying to own the ecosystem — we’re trying to secure it. Every MCP server that moves from npx execution to containerized deployment is a win for the entire community.</p><ul><li><strong>Explore the enhanced MCP Catalog</strong>: <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">Visit the MCP Catalog</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to discover MCP servers that solve your specific needs securely.</li><li><strong>Use and test hundreds of MCP Servers</strong>: <a href=\"https://www.docker.com/products/docker-desktop/\">Download Docker Desktop</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to download and use any MCP server in our catalog with your favorite clients: Gordon, Claude, Cursor, VSCode, etc</li><li>: Star our repository and watch for updates on the MCP Gateway release and remote server capabilities.</li></ul><p>Together, we’re building more than a catalog — we’re establishing the secure foundation that the MCP ecosystem needs to grow from experimental tool to production-ready platform. Because when it comes to AI applications, security isn’t optional. It’s fundamental.</p>","contentLength":8137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build the highest resilience apps with multi-Region strong consistency in Amazon DynamoDB global tables","url":"https://aws.amazon.com/blogs/aws/build-the-highest-resilience-apps-with-multi-region-strong-consistency-in-amazon-dynamodb-global-tables/","date":1751315448,"author":"Donnie Prakoso","guid":176903,"unread":true,"content":"<p>While tens of thousands of customers are successfully using <a href=\"https://aws.amazon.com/dynamodb/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon DynamoDB</a><a href=\"https://aws.amazon.com/dynamodb/global-tables/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">global tables</a> with eventual consistency, we’re seeing emerging needs for even stronger resilience. Many organizations find that the DynamoDB multi-Availability Zone architecture and eventually consistent global tables meet their requirements, but critical applications like payment processing systems and financial services demand more.</p><p>For these applications, customers require a zero Recovery Point Objective (RPO) during rare Region-wide events, meaning you can direct your app to read the latest data from any Region. Your multi-Region applications always need to access the same data regardless of location.</p><p>Starting today, you can use a new Amazon DynamoDB global tables capability that provides multi-Region strong consistency (MRSC), enabling zero RPO. This capability, first announced as a <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-dynamodb-global-tables-previews-multi-region-strong-consistency/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">preview at AWS re:Invent 2024</a>, simplifies building highly resilient global applications.</p><p>Here’s how you can enable MRSC starting with an existing empty DynamoDB table:</p><p>With MRSC, DynamoDB now offers you the highest level of resilience for your applications. If application processing is interrupted in a Region, you can redirect traffic to another Region containing a MRSC replica and know that you’ll be processing the latest data.</p><p>Let me walk you through how you can use this new capability.</p><p>To start using MRSC, I need to create a global table from an existing DynamoDB table that doesn’t contain any data. I navigate to my existing table, select the  tab and select .</p><p>The MRSC availability architecture requires three AWS Regions. I can configure MRSC with either three full replicas or with two replicas and a witness. A witness contains only replicated change data to provide the required availability without maintaining a full copy of my table data.</p><p>The following screenshot shows how I configure MRSC with two replicas and a witness. To configure MRSC with three full replicas instead, I can clear <strong>Configure Region 2 as Witness</strong>.</p><p>If I need to update my existing table programmatically, I can use the <a href=\"https://aws.amazon.com/developer/learning/q-developer-cli/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon Q CLI</a> to generate the required command with the following prompt:</p><p><code>&gt; Hey Q! Update my existing DynamoDB table called \"demo-mrsc\" in us-east-1 with multi-Region strong consistency across us-east-2 with witness in us-west-2 Regions</code></p><p>Shortly after, Q CLI will respond back with following command:</p><pre><code>&gt; To update your DynamoDB table with multi-Region strong consistency, you'll need to use the update-table command with the appropriate parameters. Here's how you would do it:\n\naws dynamodb update-table \\ \n    --table-name demo-mrsc \\ \n    --replica-updates '[{\"Create\": {\"RegionName\": \"us-east-2\"}}]' \\ \n    --global-table-witness-updates '[{\"Create\": {\"RegionName\": \"us-west-2\"}}]' \\ \n    --multi-region-consistency STRONG \\ \n    --region us-east-1\n</code></pre><p>After it’s finished processing, I can check the status of my MRSC global table. I can see I have a  configured for my DynamoDB global table. A witness reduces costs while still providing the resilience benefits of multi-Region strong consistency.</p><p>Then, in my application, I can use  to read data with strong consistency. Here’s a Python example:</p><pre><code>import boto3\n\n# Configure the DynamoDB client for your region\ndynamodb = boto3.resource('dynamodb', region_name='us-east-2')\ntable = dynamodb.Table('demo-mrsc')\n\npk_id = \"demo#test123\"\n\n# Read with strong consistency across regions\nresponse = table.get_item(\n    Key={\n        'PK': pk_id\n    },\n    ConsistentRead=True\n)\n\nprint(response)\n</code></pre><p>For operations that require the strongest resilience, I can use . For less critical operations where eventual consistency is acceptable, I can omit this parameter to improve performance and reduce costs.</p><p>Here are a couple of things to note:</p><ul><li> – The Amazon DynamoDB multi-Region strong consistency capability is available in following AWS Regions: US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Osaka, Seoul, Tokyo), and Europe (Frankfurt, Ireland, London, Paris)</li></ul><p>Learn more about how you can achieve the highest level of application resilience, enable your applications to be always available and always read the latest data regardless of the Region by visiting <a href=\"https://aws.amazon.com/dynamodb/global-tables/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon DynamoDB global tables</a>.</p>","contentLength":4218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Survey: Pace of Increased Adoption of GitOps Varies Widely","url":"https://devops.com/survey-pace-of-increased-adoption-of-gitops-varies-widely/?utm_source=rss&utm_medium=rss&utm_campaign=survey-pace-of-increased-adoption-of-gitops-varies-widely","date":1751313103,"author":"Mike Vizard","guid":176880,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kusari Adds AI Security Tool to Inspect Code as Pull Requests Are Made","url":"https://devops.com/kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made/?utm_source=rss&utm_medium=rss&utm_campaign=kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made","date":1751310779,"author":"Mike Vizard","guid":176879,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond a RHEL Clone: How Rocky Linux Is Evolving Into Something More","url":"https://devops.com/beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more/?utm_source=rss&utm_medium=rss&utm_campaign=beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more","date":1751308370,"author":"Nathan Blackham","guid":176852,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Amazon EC2 C8gn instances powered by AWS Graviton4 offering up to 600Gbps network bandwidth","url":"https://aws.amazon.com/blogs/aws/new-amazon-ec2-c8gn-instances-powered-by-aws-graviton4-offering-up-to-600gbps-network-bandwidth/","date":1751306492,"author":"Channy Yun (윤석찬)","guid":176811,"unread":true,"content":"<p>You can use C8gn instances to run the most demanding network intensive workloads, such as security and network virtual appliances (virtual ﬁrewalls, routers, load balancers, proxy servers, DDoS appliances), data analytics, and tightly-coupled cluster computing jobs.</p><p><strong><u>EC2 C8gn instances specifications</u></strong> C8gn instances provide up to 192 vCPUs and 384 GiB memory, and offer up to 30 percent higher compute performance compared Graviton3-based <a href=\"https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7gn-instances-graviton3e-processors-and-up-to-200-gbps-network-bandwidth/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">EC2 C7gn instances</a>.</p><p>Here are the specs for C8gn instances:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>If you’re using C7gn instances now, you will have straightforward experience migrating network intensive workloads to C8gn instances because the new instances offer similar vCPU and memory ratios. To learn more, check out the collection of <a href=\"https://aws.amazon.com/ec2/graviton/resources/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Graviton resources</a> to help you start migrating your applications to Graviton instance types.</p>","contentLength":831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Weekly Roundup: Project Rainier, Amazon CloudWatch investigations, AWS MCP servers, and more (June 30, 2025)","url":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-project-rainier-amazon-cloudwatch-investigations-aws-mcp-servers-and-more-june-30-2025/","date":1751301557,"author":"Channy Yun (윤석찬)","guid":176779,"unread":true,"content":"<p>Every time I visit Seattle, the first thing that greets me at the airport is <a href=\"https://en.wikipedia.org/wiki/Mount_Rainier\">Mount Rainier</a>. Did you know that the most innovative project at <a href=\"https://aws.amazon.com/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Web Services (AWS)</a> is named after this mountain?</p><p>Project Rainier is a new project to create what is expected to be the world’s most powerful computer for training AI models across multiple data centers in the United Stages. Anthropic will develop the advanced versions of its <a href=\"https://aws.amazon.com/bedrock/anthropic/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Claude models</a> with five times more computing power than its current largest training cluster.</p><p>The key technology powering Project Rainier is <a href=\"https://aws.amazon.com/ai/machine-learning/trainium/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS custom-designed Trainium2 chips</a>, which are specialized for the immense data processing required to train complex AI models. Thousands of these Trainium2 chips will be connected in a new type of <a href=\"https://aws.amazon.com/ec2/ultraservers/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon EC2 UltraServer</a> and <a href=\"https://aws.amazon.com/ec2/ultraclusters/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">EC2 UltraCluster</a> architecture that allows ultra-fast communication and data sharing across the massive system.</p><p>Learn about the <a href=\"https://www.aboutamazon.com/news/aws/aws-project-rainier-ai-trainium-chips-compute-cluster\">AWS vertical integration of Project Rainer</a>, where it designs every component of the technology stack from chips to software, allows it to optimize the entire system for maximum efficiency and reliability.</p><p> Here are some launches that got my attention:</p><ul><li><a href=\"https://aws.amazon.com/blogs/aws/amazon-fsx-for-openzfs-now-supports-amazon-s3-access-without-any-data-movement/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon S3 access for Amazon FSx for OpenZFS</a> – You can access and analyze your FSx for OpenZFS file data through Amazon S3 Access Points, enabling seamless integration with AWS AI/ML, and analytics services without moving your data out of the file system. You can treat your FSx for OpenZFS data as if it were stored in S3, making it accessible through the S3 API for various applications including Amazon Bedrock, Amazon SageMaker, AWS Glue, and other S3 based cloud-native applications.</li><li><a href=\"https://aws.amazon.com/blogs/aws/new-improve-apache-iceberg-query-performance-in-amazon-s3-with-sort-and-z-order-compaction/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon S3 with sort and z-order compaction for Apache Iceberg tables</a> – You can optimize query performance and reduce costs with new sort and z-order compaction. With S3 Tables, sort compaction automatically organizes data files based on defined column orders, while z-order compaction can be enabled through the maintenance API for efficient multicolumn queries.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/ga-accelerate-troubleshooting-amazon-cloudwatch-investigations/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon CloudWatch investigations</a> – You can accelerate your operational troubleshooting in AWS environments using the Amazon CloudWatch AI-powered investigation feature, which helps identify anomalies, surface related signals, and suggest remediation steps. This capability can be initiated through CloudWatch data widgets, multiple AWS consoles, CloudWatch alarm actions, or Amazon Q chat and enables team collaboration and integration with Slack and Microsoft Teams.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-bedrock-guardrails-tiers-content-filters-denied-topics/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Bedrock Guardrails Standard tier</a> – You can enhance your AI content safety measures using the new Standard tier. It offers improved content filtering and topic denial capabilities across up to 60 languages, better detection of variations including typos, and stronger protection against prompt attacks. This feature lets you configure safeguards to block harmful content, prevent model hallucinations, redact personally identifiable information (PII), and verify factual claims through automated reasoning checks.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-route-53-resolver-endpoints-dns-delegation-private-hosted-zones/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Route 53 Resolver endpoints for private hosted zone</a> – You can simplify DNS management across AWS and on-premises infrastructure using the new Route 53 DNS delegation feature for private hosted zone subdomains, which works with both inbound and outbound Resolver endpoints. You can delegate subdomain authority between your on-premises infrastructure and Route 53 Resolver cloud service using name server records, eliminating the need for complex conditional forwarding rules.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-q-developer-java-upgrade-transformation-cli-generally-available/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Q Developer CLI for Java transformation</a> – You can automate and scale Java application upgrades using the new Amazon Q Developer Java transformation command line interface (CLI). This feature perform upgrades from Java versions 8, 11, 17, or 21 to versions 17 or 21 directly from the command line. This tool offers selective transformation options so you can choose specific steps from transformation plans and customize library upgrades.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/managed-integrations-aws-iot-device-management/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">New AWS IoT Device Management managed integrations</a> – You can simplify Internet of Things (IoT) device management across multiple manufacturers and protocols using the new managed integrations feature, which provides a unified interface for controlling devices whether they connect directly, through hubs or third-party clouds. The feature includes pre-built cloud-to-cloud (C2C) connectors, device data model templates, and SDKs that support ZigBee, Z-Wave, and Wi-Fi protocols, while you can still create custom connectors and data models.</li></ul><p> Check your calendars and sign up for these upcoming AWS events:</p><ul><li><a href=\"https://reinvent.awsevents.com/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS re:Invent</a> – Register now to get a head start on choosing your best learning path, booking travel and accommodations, and bringing your team to learn, connect, and have fun. If you’re an early-career professional, you can apply to the <a href=\"https://reinvent.awsevents.com/all-builders-welcome/\">All Builders Welcome Grant program</a>, which is designed to remove financial barriers and create diverse pathways into cloud technology.</li><li><a href=\"https://aws.amazon.com/events/builders-online-series/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS Builders Online Series</a> – If you’re based in one of the Asia Pacific time zones, join and learn fundamental AWS concepts, architectural best practices, and hands-on demonstrations to help you build, migrate, and deploy your workloads on AWS.</li></ul><p>That’s all for this week. Check back next Monday for another Weekly Roundup!</p>","contentLength":5246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast Code, Real Risks: Guardrails for AI-Generated Software","url":"https://devops.com/fast-code-real-risks-guardrails-for-ai-generated-software/?utm_source=rss&utm_medium=rss&utm_campaign=fast-code-real-risks-guardrails-for-ai-generated-software","date":1751298670,"author":"Mike Vizard","guid":176743,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Next Version of Grok Includes Advanced Coding Assistance: Reports","url":"https://devops.com/next-version-of-grok-includes-advanced-coding-assistance-reports/?utm_source=rss&utm_medium=rss&utm_campaign=next-version-of-grok-includes-advanced-coding-assistance-reports","date":1751298224,"author":"Jon Swartz","guid":176742,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tool Calling with Local LLMs: A Practical Evaluation","url":"https://www.docker.com/blog/local-llm-tool-calling-a-practical-evaluation/","date":1751291322,"author":"Ignasi Lopez Luna","guid":176646,"unread":true,"content":"<h2>Which local model should I use for tool calling?</h2><p>When building GenAI and agentic applications, one of the most pressing and persistent questions is: <em>“Which local model should I use for tool calling?”</em>&nbsp; We kept hearing again and again, from colleagues within Docker and the developer community, ever since we started working on <a href=\"https://www.docker.com/blog/introducing-docker-model-runner/\"></a>, a local inference engine that helps developers run and experiment with local models.&nbsp;</p><p>It’s a deceptively simple question with a surprisingly nuanced answer. Even when we tried to answer it for a very specific case: <em>“What if I just expose 5 simple tools to the model?”</em>We realized we had no definite answer forthat. <a href=\"https://www.docker.com/blog/run-llms-locally/\">Local LLM models</a> offer control, cost-efficiency, and privacy, but when it comes to structured tool use, deciding when and how to act, they can behave very differently. We decided to dig deep and test this properly. We started with manual experimentation, then built a framework to scale our testing. This blog documents that journey and shares which models ranked highest on our tool-calling leaderboard.</p><h2>The first attempt: Manual testing</h2><p>Our first instinct was to build something quickly and try it out manually.</p><p>So we created<a href=\"https://github.com/ilopezluna/chat2cart\" rel=\"nofollow noopener\" target=\"_blank\"></a>, an AI-powered shopping assistant that lets users interact via chat to build, modify, and check out a shopping cart. Through a natural conversation, users can discover products, add or remove items, and complete or cancel their purchase, all from the chat interface.</p><p>To support testing across different LLMs, we added a model selector that makes it easy to switch between local models (via Docker Model Runner or Ollama) and hosted models using the OpenAI API.</p><p>OpenAI’s GPT-4 or GPT-3.5 worked as expected, and the experience was fairly smooth.&nbsp;</p><ul><li>Called tools when they were needed</li><li>Avoided unnecessary tool usage</li><li>Handled tool responses naturally</li></ul><p>But the local models? That’s where the challenges started to surface.</p><h2>What went wrong with local models</h2><p>We started experimenting with some of the local models listed on the<a href=\"https://huggingface.co/spaces/gorilla-llm/berkeley-function-calling-leaderboard\" rel=\"nofollow noopener\" target=\"_blank\"> Berkeley Function-Calling Leaderboard</a>. Our goal was to find smaller models, ideally with fewer than 10 billion parameters, so we tested xLAM-2-8b-fc-r and watt-tool-8B. We quickly ran into several recurring issues:</p><ul><li>: Tools were being called even for greeting messages like “Hi there!”</li><li>: The model would search when it should have added, or tried to remove when the cart was empty</li><li>: Parameters like product_name or quantity were missing or malformed</li><li>: The model often failed to respond to tool output, leading to awkward or incomplete conversations</li></ul><p>At this point, it was clear that manual testing wouldn’t scale. Different models failed in different ways, some struggled with invocation logic, while others mishandled tool arguments or responses.&nbsp; Testing was not only slow, but also unreliable. Because these models are non-deterministic, we had to run each scenario multiple times just to get a reliable read on behavior.</p><p>We needed a testing setup that was repeatable, measurable, and fast.</p><h2>Our second attempt: A scalable testing tool</h2><p>Our goal wasn’t academic rigor.It was: <em>“Give us good-enough answers in 2–3 days, not weeks.”</em></p><p>In a couple of days, we created<a href=\"https://github.com/docker/model-test\" rel=\"nofollow noopener\" target=\"_blank\"></a>, This is a flexible project with the following capabilities</p><ul><li>Define real-world  with multiple valid tool call sequences</li><li>Run them against many models (local &amp; hosted)</li><li>Track , , and </li><li>Log  for analysis (or eventual fine-tuning)</li></ul><p>The core idea behind model-test is simple: simulate realistic tool-using conversations, give the model room to reason and act, and check whether its behavior makes sense.</p><ul><li>A  (e.g. “Add iPhone to cart”)</li><li>The  (optional)</li><li>One or more , because there’s often more than one right answer</li></ul><div><pre>{\n&nbsp;&nbsp;\"prompt\": \"Add iPhone to cart\",\n&nbsp;&nbsp;\"expected_tools_variants\": [\n&nbsp;&nbsp;&nbsp;&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\": \"direct_add\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"tools\": [{ \"name\": \"add_to_cart\", \"arguments\": { \"product_name\": \"iPhone\" } }]\n&nbsp;&nbsp;&nbsp;&nbsp;},\n&nbsp;&nbsp;&nbsp;&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\": \"search_then_add\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"tools\": [\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{ \"name\": \"search_products\", \"arguments\": { \"query\": \"iPhone\" } },\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{ \"name\": \"add_to_cart\", \"arguments\": { \"product_name\": \"iPhone 15\" } }\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]\n&nbsp;&nbsp;&nbsp;&nbsp;}\n&nbsp;&nbsp;]\n}\n</pre></div><p>In this case, we consider both  and <strong>“search first, then add the result”</strong> as acceptable. Even though “iPhone” isn’t a real product name, we’re fine with it. We weren’t aiming for overly strict precision, just realistic behavior.</p><p>Each test case belongs to a test suite. We provide two built-in suites. However, you can run an entire suite, individual test cases, or a selection of multiple test cases. Additionally, you can create your own custom suites to group tests as needed.&nbsp;</p><ul><li>: Greetings, single-step actions</li><li>: Multi-step reasoning and tool chaining</li></ul><p>To make tests feel closer to how real agents behave, we simulate an agent loop up to .</p><p>User: </p><ol><li>Model: <em>“Let me search for iPhone 5…”</em><ol><li>Tool: </li></ol></li><li>Model: <em>“Adding product X to cart…”</em></li><li>Model:  → Great, test passed!</li></ol><p>But if the model still wants to keep going after round 5?</p><p>That’s it, my friend,&nbsp; . Time’s up.</p><p>We deliberately avoided designing tests that require perfect predictions.</p><ul><li>We didn’t demand that the model always know the exact product name.</li><li>What mattered was: <strong>did the tool sequence make sense</strong> for the intent?</li></ul><p>This helped us focus on the kind of reasoning and behavior we actually want in agents, not just perfect token matches.</p><p>Our test outputs distilled down to a final F1 score, encapsulating three core dimensions:</p><div><table><tbody><tr><td><p>Did the model realize a tool was needed?</p></td></tr><tr><td><p>Did it choose the right tool(s) and use them correctly?</p></td></tr><tr><td><p>Whether the tool call arguments were correct?</p></td></tr></tbody></table></div><p>The F1 score is the harmonic mean of two things: precision (how often the model made valid tool calls) and recall (how often it made the tool calls it was supposed to).</p><p>We also tracked latency, the average runtime in seconds, but that wasn’t part of the F1 calculation; it simply helped us evaluate speed and user experience.</p><h2>21 models and 3,570 tests later: Which models nailed tool calling?</h2><p>We tested 21 models across  using 210 batch runs.</p><h3>Overall Rankings (by Tool Selection F1):</h3><div><table><tbody><tr></tr><tr><td><p>claude-3-5-sonnet-20241022</p></td></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>Among all models, OpenAI’s GPT-4 came out on top with a tool selection F1 score of 0.974, completing responses in just under 5 seconds on average. While hosted and not the focus of our local model exploration, it served as a reliable benchmark and provided some ground truths.</p><p>On the local side, Qwen 3 (14B) delivered outstanding results, nearly matching GPT-4 with a 0.971 F1 score, though with significantly higher latency (~142 seconds per interaction).</p><p>If you’re looking for something faster, Qwen 3 (8B) also achieved an F1 score of 0.933, while cutting latency nearly in half (~84 seconds), making it a compelling balance between speed and tool-use accuracy.</p><p>Hosted models like Claude 3 Haiku also performed very well, hitting 0.933 F1 with exceptional speed (3.56 seconds average latency), further illustrating the high bar set by cloud-based offerings.</p><p>Not all models handled tool calling well. The quantized Watt 8B model struggled with parameter accuracy and ended up with a tool selection F1 score of just 0.484. Similarly, the LLaMA-based XLam 8B variant often missed the correct tool path altogether, finishing with an F1 score of 0.570. These models may be suitable for other tasks, but for our structured tool use test, they underdeliver.</p><p>We also experimented with both  and  variants for some models, and in all cases observed <strong>no significant difference</strong> in tool-calling behavior or performance. This suggests that quantization is beneficial for reducing resource usage without negatively impacting accuracy or reasoning quality, at least for the models and scenarios we tested.</p><p>If your goal is maximum tool-calling accuracy, then Qwen 3 (14B) or Qwen 3 (8B) are your best bets, both local, both precise, with the 8B variant being notably faster.</p><p>For a good trade-off between speed and performance, Qwen 2.5 stood out as a solid option. It’s fast enough to support real-time experiences, while still maintaining decent tool selection accuracy.</p><p>If you need something more lightweight, especially for resource-constrained environments, the <a href=\"https://groq.com/introducing-llama-3-groq-tool-use-models/\" rel=\"nofollow noopener\" target=\"_blank\">LLaMA 3 Groq 7B</a> variant offers modest performance at a much lower compute footprint.</p><h2>What we learned and why this matters</h2><p>Our testing confirmed that the Qwen family of models leads the pack among open-source options for tool calling. But as always, there’s a trade-off; you’ll need to balance between accuracy and latency when designing your application</p><ul><li>: Even the 8B version of Qwen3 outperformed any other local model</li><li>: Higher-accuracy models take longer, often significantly.</li></ul><p>Tool calling is core to almost every real-world GenAI application. Whether you’re building agents or creating agentic workflows, your LLM must know when to act and how. Thanks to this simple framework, “We don’t know which model to pick” became “We’ve narrowed it down to three great options, each with clear pros and cons.”</p>","contentLength":8955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Still Running Vulnerable Log4j Instances?","url":"https://devops.com/still-running-vulnerable-log4j-instances/?utm_source=rss&utm_medium=rss&utm_campaign=still-running-vulnerable-log4j-instances","date":1751281674,"author":"Ofer Regev","guid":176565,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serverless CI/CD: Redefining Continuous Delivery in the Modern DevOps Era","url":"https://devops.com/serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era/?utm_source=rss&utm_medium=rss&utm_campaign=serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era","date":1751280883,"author":"Harikrishna Kundariya","guid":176527,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How DevOps Services Improve Software Delivery and Quality","url":"https://devops.com/how-devops-services-improve-software-delivery-and-quality/?utm_source=rss&utm_medium=rss&utm_campaign=how-devops-services-improve-software-delivery-and-quality","date":1751277346,"author":"Vinay Pasilkar","guid":176468,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revolutionizing CI/CD: A Framework for Integrating Generative AI Across the Software Delivery Lifecycle","url":"https://devops.com/revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle/?utm_source=rss&utm_medium=rss&utm_campaign=revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle","date":1751275960,"author":"Anirban Biswas","guid":176467,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Software Migrations Fail: It’s Not the Code","url":"https://devops.com/why-software-migrations-fail-its-not-the-code/?utm_source=rss&utm_medium=rss&utm_campaign=why-software-migrations-fail-its-not-the-code","date":1751273613,"author":"Nishil Macwan","guid":176426,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harnessing AI and Automation for the Future of Innovation in DevOps","url":"https://devops.com/harnessing-ai-and-automation-for-the-future-of-innovation-in-devops/?utm_source=rss&utm_medium=rss&utm_campaign=harnessing-ai-and-automation-for-the-future-of-innovation-in-devops","date":1751268502,"author":"Tony Barbagallo","guid":176391,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Self-Driving Help Desk: Agentic AI’s Role in the Next DevOps Era","url":"https://devops.com/the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era/?utm_source=rss&utm_medium=rss&utm_campaign=the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era","date":1751267594,"author":"Venkat Thiruvengadam","guid":176390,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Agile Methodologies and DevOps Practices Shape Custom Software Product Development","url":"https://devops.com/how-agile-methodologies-and-devops-practices-shape-custom-software-product-development/?utm_source=rss&utm_medium=rss&utm_campaign=how-agile-methodologies-and-devops-practices-shape-custom-software-product-development","date":1751265805,"author":"Roman Davydov","guid":176357,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google Adds Gemini CLI to AI Coding Portfolio","url":"https://devops.com/gemini-cli-the-open-source-ai-agent-thats-revolutionizing-terminal-workflows/?utm_source=rss&utm_medium=rss&utm_campaign=gemini-cli-the-open-source-ai-agent-thats-revolutionizing-terminal-workflows","date":1751264732,"author":"Tom Smith","guid":176356,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-145/?utm_source=rss&utm_medium=rss&utm_campaign=five-great-devops-job-opportunities-145","date":1751263232,"author":"Mike Vizard","guid":176319,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Level Up your bash skill : Mastering Sed command","url":"https://blog.devops.dev/level-up-your-bash-skill-mastering-sed-command-3b6f28ea0e60?source=rss----33f8b2d9a328---4","date":1751133811,"author":"bektiaw","guid":174650,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Install Prometheus & Grafana in AKS Using Helm","url":"https://blog.devops.dev/how-to-install-prometheus-grafana-in-aks-using-helm-d9c46c9d5170?source=rss----33f8b2d9a328---4","date":1751133668,"author":"Nirmal Chandrasiri","guid":174619,"unread":true,"content":"<p>In this article, I’ll walk you through how to install  and  on <strong>Azure Kubernetes Service (AKS)</strong> using  — in a beginner-friendly and straightforward way. This is perfect and easy way to setup prometheus in AKS or EKS using Helm… So let’s dive in&nbsp;:)</p><p>Before anything, make sure you have an  up and&nbsp;running.</p><p>In my case, I created my AKS cluster using . You can find my Terraform code in this repo: Of course you can change the values as you&nbsp;like.</p><p>❗Please export your Azure subscription ID&nbsp;first:</p><p><strong>For Bash (Linux/macOS/WSL):</strong></p><p>export TF_VAR_subscription_id=&lt;your_subscription_id&gt;</p><p><strong>For PowerShell (Windows):</strong></p><p>$env:TF_VAR_subscription_id=&lt;your_subscription_id&gt;</p><p>In order to setup AKS cluster using Terraform use the below commands:</p><p>terraform plan -var-file terraform.tfvars</p><p>terraform apply -var-file terraform.tfvars</p><p>After you’ve created the AKS cluster, connect to it&nbsp;using:</p><pre>az aks get-credentials --resource-group central-monitoring-dev-rg --name central-monitoring-dev-aks</pre><p>Replace central-monitoring-dev-rg and central-monitoring-dev-aks with your own  and (Optional).</p><h3>Add the Prometheus Helm&nbsp;Chart</h3><p>We’ll use the official <strong>Prometheus Community Kubernetes Helm Chart</strong>. Run the command below to add&nbsp;it:</p><pre>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</pre><p>Then update your Helm&nbsp;repos:</p><h3>Install Prometheus and&nbsp;Grafana</h3><p>Now let’s install everything:</p><pre>helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace</pre><p>This will install Prometheus, Grafana, and related components into the prometheus-stack namespace.</p><h3>🌐 Expose Grafana via LoadBalancer</h3><p>By default, Grafana is exposed as a , So let’s change it to a  type so Azure will create an external IP for&nbsp;us:</p><ul><li>First, check the services:</li></ul><pre>kubectl get svc -n prometheus-stack</pre><ul><li>Then edit the Grafana&nbsp;service:</li></ul><pre>kubectl edit svc prometheus-stack-grafana -n prometheus-stack</pre><ul><li>In the editor that opens,&nbsp;change:</li></ul><p>Once the LoadBalancer is ready, grab the  from:</p><pre>kubectl get svc prometheus-stack-grafana -n prometheus-stack</pre><p>Then open it in your browser. copy the external ip address into a new tab and login page will appear to&nbsp;you.</p><ul></ul><blockquote>how do I know the password?<em> You can check the default values using&nbsp;this:</em></blockquote><pre>helm show values prometheus-community/kube-prometheus-stack &gt; prometheus-default-values.yaml</pre><p>Look for adminPassword under Grafana settings — it’s usually set to prom-operator.</p><p>Now you can explore dashboards, set up alerts, and start <strong>monitoring your AKS cluster like a&nbsp;pro</strong>.</p><p>If you navigate to your AKS portal and check the  section, you’ll see the external IP created for Grafana. Just click and log&nbsp;in.</p><p>That’s all! Hope this helps you get started with  on . If you found this helpful, feel free to reach drop a comment if you have any&nbsp;issues.</p><p>See you in the next one!&nbsp;✌️</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d9c46c9d5170\" width=\"1\" height=\"1\" alt=\"\">","contentLength":2802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mastering Kubernetes Operators: Automating Beyond YAML","url":"https://blog.devops.dev/mastering-kubernetes-operators-automating-beyond-yaml-09a47b5ae4b5?source=rss----33f8b2d9a328---4","date":1751133575,"author":"Siddharth Singh","guid":174618,"unread":true,"content":"<div><p>In modern Kubernetes-based environments, we often find ourselves writing tons of YAML files — Deployments, Services, ConfigMaps, and more…</p></div>","contentLength":146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Set Up Prometheus, InfluxDB, and Grafana for Full-Stack Monitoring with Docker Compose","url":"https://blog.devops.dev/monitor-docker-containers-system-metrics-using-grafana-prometheus-7cbc015e2044?source=rss----33f8b2d9a328---4","date":1751133568,"author":"David Lam","guid":174617,"unread":true,"content":"<div><p>Monitoring your infrastructure is critical to keeping systems healthy, optimizing performance, and planning for growth. In this guide…</p></div>","contentLength":136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ansible for DevOps: The Complete Guide to Automation That Actually Works","url":"https://blog.devops.dev/ansible-for-devops-the-complete-guide-to-automation-that-actually-works-ae0c70d6fb56?source=rss----33f8b2d9a328---4","date":1751133563,"author":"Dhruv Patel","guid":174616,"unread":true,"content":"<div><p>Tired of repetitive deployment tasks eating up your sprint time? Here’s how Ansible transforms DevOps workflows with real examples you can…</p></div>","contentLength":143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden “Verbosity Tax” in AI: Why Per-Token Pricing Isn’t What It Seems","url":"https://blog.devops.dev/the-hidden-verbosity-tax-in-ai-why-per-token-pricing-isnt-what-it-seems-a3d446499102?source=rss----33f8b2d9a328---4","date":1751133557,"author":"Simardeep Singh","guid":174615,"unread":true,"content":"<p>As a senior DevOps engineer who’s spent years optimizing cloud costs and capacity planning, I thought I understood pricing models. Then I came across post that made me question everything I knew about AI service economics.</p><p>The post was from Kevin Jiang (for link see references), a developer building at otherhalf.ai, who shared a frustrating discovery: despite switching to Google’s Gemini 2.5 Pro for its lower per-token costs, his team’s AI bills had mysteriously spiked. The culprit? Outputs that were becoming dramatically longer for identical prompts.</p><p>This revelation triggered my DevOps instincts. In infrastructure management, unexpected cost increases usually signal either a misconfiguration or a fundamental shift in service behavior. Kevin’s observation suggested the latter, and I decided to investigate.</p><p>Kevin’s team noticed that Gemini 2.5 Pro, despite advertising lower per-token rates than competitors like Claude Sonnet, was generating responses that were consistently 2x longer than necessary:</p><ul><li>Five lines of comments before writing a single line of&nbsp;code</li><li>50% more whitespace and formatting padding</li><li>Unsolicited debug logs and error&nbsp;handling</li><li>Verbose explanations for simple&nbsp;requests</li></ul><p>From a cost perspective, this creates a hidden multiplier effect:</p><pre>Advertised: Gemini ($1.25/1M tokens) vs Claude ($3.00/1M tokens) = 58% savingsReality: Gemini (2x verbosity) = $2.50 effective rate = 17% more expensive</pre><p>My first step was checking if this was an isolated incident or part of a broader pattern. Recent academic research suggests the&nbsp;latter.</p><p>The paper’s key insight resonates with any infrastructure engineer who’s dealt with cloud pricing optimization:</p><blockquote><em>“Unlike conventional billing, where the quantity and quality of services are verifiable, today’s LLM platforms operate under structural opacity: users are charged based on reported token and API usage, but have no means to confirm that these metrics reflect real or necessary work.”</em></blockquote><h3>The Smoking Gun in Vendor Documentation</h3><p>Digging into Google’s documentation revealed the most compelling evidence. Their <a href=\"https://developers.googleblog.com/en/updated-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/\">September 2024 release notes</a> explicitly mention tuning models for different verbosity levels:</p><blockquote><em>“For use cases like summarization, question answering, and extraction, the default output length of the updated models is ~5–20% shorter than previous models. For chat-based products where users might prefer longer responses by default, you can read our prompting strategies guide to learn more about how to make the models more verbose and conversational.”</em></blockquote><p>Even more telling, Google’s latest <a href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/\">Gemini 2.5 updates</a> tout efficiency improvements, claiming the new Flash model uses “20–30% fewer tokens in our evaluations.” This explicitly acknowledges that previous versions were generating unnecessary tokens.</p><h3>The DevOps Perspective: Infrastructure Implications</h3><p>From an infrastructure management standpoint, this phenomenon breaks fundamental assumptions about cost predictability that we rely on for capacity planning.</p><h3>Traditional vs. LLM Pricing&nbsp;Models</h3><p><strong>Traditional cloud&nbsp;pricing:</strong></p><ul><li>Pay $X for Y resources (compute hours, storage GB, network bandwidth)</li><li>Resource consumption is directly measurable and predictable</li><li>Cost scales linearly with actual&nbsp;usage</li></ul><ul><li>Pay $X per token, but the provider controls token generation patterns</li><li>Token consumption can vary dramatically for identical inputs</li><li>Algorithmic changes on the provider side directly impact your&nbsp;costs</li></ul><p>This introduces a new category of cost drift that’s invisible until you analyze usage patterns over time. Unlike compute or storage costs that scale predictably with load, LLM costs can inflate through unilateral provider&nbsp;changes.</p><p>Traditional infrastructure monitoring focuses on resource utilization metrics: CPU, memory, network, and storage. LLM cost optimization requires tracking entirely different metrics:</p><ul><li><strong>Tokens per request trends</strong> over&nbsp;time</li><li><strong>Output length distribution</strong> for similar prompt&nbsp;types</li><li><strong>Provider efficiency ratios</strong> across different models</li><li> rather than cost per&nbsp;token</li></ul><p>This is analogous to the shift from monitoring raw server metrics to application performance metrics, but with an added layer of complexity: the “application” behavior is controlled by an external provider.</p><h3>Broader Patterns in Cloud Service Evolution</h3><p>This verbosity inflation mirrors other evolutionary patterns I’ve observed in cloud services&nbsp;pricing:</p><h3>The Burstable Instance Precedent</h3><p>Remember when AWS introduced “burstable” EC2 instances (t2, t3, t4g families)? They appeared cheaper than standard instances but could spike costs when applications exceeded baseline CPU credits. The advertised low hourly rate became meaningless without understanding usage patterns.</p><h3>The API Call Proliferation</h3><p>Storage providers like AWS S3 initially charged primarily for storage and bandwidth. Over time, they added charges for API calls, lifecycle transitions, and various request types. What started as simple per-GB pricing became a complex matrix of&nbsp;fees.</p><p>Cloud providers have progressively increased data egress charges while keeping ingress free. This creates vendor lock-in effects where the true cost of a service isn’t apparent until you try to&nbsp;leave.</p><p>LLM verbosity inflation follows the same playbook: advertise low headline rates, then optimize for revenue through usage pattern manipulation that’s difficult for customers to predict or&nbsp;control.</p><h3>Practical Implications for Infrastructure Teams</h3><p>For teams evaluating LLM integration or optimizing existing AI costs, this research suggests several critical considerations:</p><h3>1. Redefine Cost Comparison Metrics</h3><p>Per-token price comparisons are becoming as meaningless as comparing cloud instances purely on CPU count without considering memory, storage, or network performance.</p><p>Instead, benchmark on :</p><pre>Total monthly cost ÷ Successful task completions = True cost per outcome</pre><h3>2. Implement Comprehensive Monitoring</h3><p>Traditional cost monitoring isn’t sufficient. Infrastructure teams need to&nbsp;track:</p><ul><li>Average tokens per request by prompt&nbsp;type</li><li>Output length trends over&nbsp;time</li><li>Provider efficiency ratios</li><li>Token utilization patterns</li></ul><h3>3. Build Provider Diversification Strategy</h3><p>Just as we avoid single-region deployments for availability, avoiding single-provider dependencies becomes crucial for cost predictability. Maintaining the ability to switch providers limits exposure to arbitrary pricing&nbsp;changes.</p><h3>4. Prompt Engineering as Cost Engineering</h3><p>In traditional infrastructure, we optimize resource allocation for cost efficiency. With LLMs, prompt engineering becomes a cost optimization discipline:</p><ul><li>Explicit brevity instructions</li><li>Output format specifications</li></ul><h3>The Research That Validates the&nbsp;Concern</h3><p>The academic community is taking this seriously. Beyond the Max Planck study, <a href=\"https://epoch.ai/data-insights/llm-inference-price-trends\">Epoch AI’s analysis</a> of LLM pricing trends shows that while per-token costs are dropping rapidly, the variation in actual usage costs is increasing due to differences in model efficiency and output patterns.</p><p>Another study, <a href=\"https://arxiv.org/html/2505.18471v1\">“Invisible Tokens, Visible Bills”</a>, proposes auditing frameworks to detect quantity inflation, suggesting this is becoming a recognized industry problem requiring systematic solutions.</p><h3>What This Means Moving&nbsp;Forward</h3><p>The AI market is maturing from a technology race to a business model optimization phase. Providers are discovering that algorithmic control over token generation provides more pricing flexibility than simple rate adjustments.</p><p>For infrastructure teams, this&nbsp;means:</p><p><strong>Cost predictability requires deeper analysis.</strong> Traditional cost modeling based on advertised rates isn’t sufficient. Teams need to test real workloads and monitor actual consumption patterns.</p><p><strong>Vendor evaluation must include efficiency testing.</strong> Beyond accuracy benchmarks, teams need to measure tokens-per-useful-output across providers with representative workloads.</p><p><strong>Monitoring and alerting systems need updates.</strong> Cost anomaly detection should include token efficiency trends, not just absolute spending&nbsp;changes.</p><p><strong>Contract negotiations should address efficiency.</strong> SLAs might need to include token efficiency guarantees or caps on output length for specified prompt&nbsp;types.</p><h3>The Broader Industry&nbsp;Question</h3><p>This investigation raises a fundamental question about AI service pricing: Should providers have unlimited control over consumption patterns when users are charged per unit of consumption?</p><p>Traditional utility models have regulations around metering accuracy and consumption reporting. As AI services become infrastructure-critical, similar transparency requirements might be necessary.</p><h3>Conclusion: The New&nbsp;Normal</h3><p>My observation about Gemini’s verbosity inflation isn’t just an isolated pricing quirk — it’s an early indicator of how AI service economics will evolve. As these services become more central to application infrastructure, understanding and monitoring true efficiency becomes as critical as traditional performance metrics.</p><p>For DevOps engineers, this means adding a new dimension to cost optimization playbooks. The era of simple per-unit pricing in AI is ending, replaced by a more complex landscape where provider algorithms directly impact your&nbsp;costs.</p><p>The key is building monitoring and evaluation frameworks that look beyond advertised rates to measure actual value delivery. Because in the emerging AI infrastructure landscape, the cheapest per-token rate rarely translates to the lowest monthly&nbsp;bill.</p><p><em>This investigation was inspired by and builds upon Kevin Jiang’s original observation. The research methodology and infrastructure perspective represent my own analysis as a senior DevOps engineer.</em></p><h3>Kevin Jiang’s Original&nbsp;Post</h3><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3d446499102\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stuck in Kubernetes Hell? Fix These 8 Common Scenarios!","url":"https://blog.devops.dev/stuck-in-kubernetes-hell-fix-these-8-common-scenarios-0a42376b0ac6?source=rss----33f8b2d9a328---4","date":1751133550,"author":"Devops Diaries","guid":174614,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Handling PostgreSQL Connection Pooling","url":"https://blog.devops.dev/handling-postgresql-connection-pooling-dee3849d0299?source=rss----33f8b2d9a328---4","date":1751133543,"author":"M Mahdi Ramadhan, M. Si","guid":174613,"unread":true,"content":"<blockquote>PostgreSQL: connection limit exceeded.</blockquote><p>PostgreSQL’s powerful features and reliability make it a preferred choice for modern applications. However, in serverless and microservice environments, managing PostgreSQL connections and resources is critical. This article explains:</p><ul><li>What PostgreSQL connection pooling&nbsp;is</li><li>How to configure it (with code examples)</li><li>How to calculate resource (RAM &amp; CPU)&nbsp;needs</li><li>Best practices and alternatives</li></ul><h3>🔍 What is PostgreSQL Connection Pooling?</h3><p>PostgreSQL uses a  model:</p><pre>max_connections = 100  # default</pre><p>In high-concurrency environments (e.g., AWS Lambda), hitting this limit leads&nbsp;to:</p><pre>FATAL: sorry, too many clients already</pre><h3>🚧 Problem Example: Without&nbsp;Pooling</h3><pre>import psycopg2<p>def handler(event, context):</p>    conn = psycopg2.connect(        user=\"user\",<p>        password=\"pass\", // dont use password like this, just sample</p>        host=\"mydb.host\",<p>        port=5432 // sometime for security purposes, dont use default port</p>    )    cur.execute(\"SELECT * FROM users LIMIT 1;\")    conn.close()</pre><blockquote><em>💥 High concurrent Lambda invocations will crash PostgreSQL.</em></blockquote><h3>✅ Solution: PgBouncer Connection Pooling</h3><p>PgBouncer acts as a middleware that reuses backend connections.</p><h3>PgBouncer pgbouncer.ini Configuration</h3><pre>[databases]mydb = host=127.0.0.1 port=5432 dbname=mydblisten_port = 6432<p>pool_mode = transaction  # ensures each client gets a backend connection only during a transaction;</p># other modes include 'session' (holds backend connection for entire client session) and <p># 'statement' (allocates backend per individual SQL statement, least supported but highly efficient)</p>default_pool_size = 20  # number of server-side connections kept in the pool per database<p>max_client_conn = 2000  # max number of client connections PgBouncer will accept</p>reserve_pool_size = 5  # number of additional connections to handle sudden spikes</pre><p>For high availability and better isolation, based on my experienced, you have two main architectural choices:</p><ol><li><strong>Per-microservice PgBouncer</strong>: Each microservice instance deploys its own PgBouncer, which connects to your PostgreSQL cluster (primary and replicas). This offers better fault isolation and connection control per&nbsp;service.</li><li>: Multiple clients or microservices share a centralized PgBouncer instance, which then manages pooled connections to the PostgreSQL primary and replicas. This approach reduces infrastructure overhead but can become a single point of&nbsp;failure.</li></ol><p>In both setups, ensure your PgBouncer is configured with failover awareness or integrated with a load balancer that understands your PostgreSQL master/replica roles to maintain high availability.</p><h3>Application Code (Updated)</h3><p>The application now connects to PgBouncer instead of directly to PostgreSQL. This allows the connection to be managed through a pool, improving scalability and efficiency, especially under high concurrency scenarios.</p><pre>conn = psycopg2.connect(    dbname=\"mydb\",    password=\"pass\",    port=6432</pre><h3>🚡 Understanding Pool Size vs. max_connections</h3><ul><li>max_connections = maximum backend processes PostgreSQL accepts.</li><li>pool_size = active backend connections PgBouncer keeps.</li></ul><pre>max_connections ≥ Σ (PgBouncer_instances × pool_size) + admin buffer</pre><p>based on my experienced, always prepare admin buffer connection for your analytics and monitoring purposes such as health&nbsp;check.</p><ul><li>3 services using PgBouncer</li></ul><pre>max_connections = (3 × 20) + 10 = 70</pre><p>You might wonder: why use PgBouncer instead of a message queue like RabbitMQ?</p><p>The key difference lies in <strong>what problem each one&nbsp;solves</strong>:</p><ul><li> solves the problem of <strong>too many database connections</strong> by reusing a smaller pool of persistent connections to PostgreSQL. It’s ideal when your application (especially serverless or microservices) has many short-lived or bursty&nbsp;queries.</li><li><strong>Message queues (e.g., RabbitMQ, Kafka)</strong> decouple the processing layer from the database entirely. They’re better suited for  like background processing, batch jobs, or high-throughput pipelines.</li></ul><ul><li>Use  when you’re doing direct, real-time database reads/writes and need to scale connections efficiently.</li><li>Use a  when you want to absorb spikes, delay execution, or build resilient processing pipelines that aren’t tightly coupled to the database.</li></ul><p>Based on my experienced, I used both: PgBouncer for interactive queries and RabbitMQ for background jobs.</p><h3>📊 Calculating RAM and CPU Requirements</h3><pre>Total_RAM ≈ shared_buffers + (max_connections × work_mem) + overhead</pre><h3>Example for 200 connections:</h3><pre>work_mem = 4MBshared_buffers = 2GB<p>work_mem_total = 200 × 4MB = 800MB</p>Total_RAM = 2GB + 800MB + ~600MB overhead ≈ 3.4GB</pre><p>✅ Recommended: 4GB RAM&nbsp;minimum</p><h3>CPU Formula (Little’s Law-based):</h3><pre>Required vCPU = max_connections / concurrency_factor</pre><p>Workload Factor OLTP 10 Mixed 6 OLAP&nbsp;4</p><pre>max_connections = 200 (OLTP) ⇒ vCPU ≈ 20</pre><ol><li><strong>Handling ID Locking via PgBouncer</strong>: Since PgBouncer in transaction mode does not support prepared statements or session-level advisory locks, avoid using SELECT MAX(id) + 1 (<strong>this causes race conditions</strong>). Instead:</li></ol><ul><li>Or use SELECT nextval('your_seq') to fetch the next&nbsp;ID</li></ul><pre>INSERT INTO orders (id, customer_id) VALUES (nextval('orders_id_seq'), 123);</pre><p><strong>2. Set Timeout in PostgreSQL for Long-Running Queries</strong>: Use the statement_timeout parameter to prevent long query execution.</p><pre>SET statement_timeout = '5s';  -- timeout after 5 seconds</pre><pre>statement_timeout = 5000  # in milliseconds</pre><p><strong>3. Handling Query Timeouts and Root Cause Analysis</strong>: If queries frequently timeout:</p><ul><li>Check indexes using EXPLAIN&nbsp;ANALYZE</li><li>Monitor for locking/blocking:</li></ul><pre>SELECT * FROM pg_stat_activity WHERE wait_event IS NOT NULL;</pre><p>D. Long-running transactions</p><p>These tools help optimize queries and configurations for pooling environments.</p><p>PostgreSQL connection management is a critical skill in building scalable and resilient cloud-native applications. Based on my experience, many issues in production environments stem from misconfigured connection pools, either from overcommitting memory or underestimating traffic bursts. By correctly calculating pool_size, tuning max_connections, and understanding the implications of RAM and CPU allocations, you can avoid common pitfalls such as backend process exhaustion and connection timeouts.</p><p>As one system architect once said: <em>“Connection pooling is not an optimization — it’s survival.”</em> Especially in environments like serverless and microservices, where concurrency patterns are unpredictable, intelligent pooling and resource planning isn’t optional — it’s foundational.</p><blockquote>Let PostgreSQL scale  use the right tools, the right math, and the right architecture for the right&nbsp;job.</blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dee3849d0299\" width=\"1\" height=\"1\" alt=\"\">","contentLength":6506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Testing AWS EventBridge for Cross-Region Event Replication","url":"https://blog.devops.dev/testing-aws-eventbridge-for-cross-region-event-replication-36f5c71b89cf?source=rss----33f8b2d9a328---4","date":1751133538,"author":"Carlos Biagolini-Jr.","guid":174612,"unread":true,"content":"<p>One of the great advantages of building on AWS is the ability to design systems that operate across geographically distant regions. Whether for disaster recovery, latency optimization, or compliance, multi-region architectures are a powerful feature of the AWS ecosystem. In this proof-of-concept (PoC), we explore how AWS EventBridge — a native event bus service — can be used to deliver event data from one region to another, effectively enabling decoupled, cross-region communication between serverless components.</p><p>The following diagram illustrates the high-level architecture we will implement:</p><ol><li><strong>Client Request Initiation:</strong>External users initiate the flow by sending an HTTP POST request to an Amazon API Gateway endpoint exposed in the N. Virginia region (us-east-1).</li><li><strong>Source Lambda Invocation and Event Creation:</strong>API Gateway triggers the source-lambda function. This function generates a unique identifier (UUID), captures the current UTC timestamp, and merges this metadata with the user's original message payload. The enriched event is then published to a custom EventBridge event bus named poc-eventbridge-cross-region-source-bus within the same region (us-east-1). The Lambda returns the generated ID immediately to the caller to confirm successful receipt.</li><li><strong>Source EventBridge Routing Rule:</strong>A rule configured on the poc-eventbridge-cross-region-source-bus evaluates incoming events. When an event matches the defined pattern (e.g., source: poc.eventbridge.crossregion, detail-type: message-event), the rule routes the event to a target EventBridge bus located in the destination region (us-west-2). This cross-region routing is enabled via the PutEvents API and the ARN of the destination event&nbsp;bus.</li><li><strong>Destination EventBridge Bus and Rule:</strong>In the Oregon region (us-west-2), the custom event bus poc-eventbridge-cross-region-destination-bus receives the forwarded event. A rule on this bus matches the same pattern and is configured to trigger the destination-lambda function whenever such an event is received.</li><li><strong>Destination Lambda Execution:</strong>The destination-lambda function appends a second UTC timestamp to indicate the time of receipt. It then stores the full payload—including the original metadata and both timestamps—into a DynamoDB table named destination_table.</li><li><strong>End-to-End Flow Complete:</strong>At this point, the event has successfully traveled from an external HTTP request in us-east-1, through EventBridge, across regions, and been persisted in a DynamoDB table in us-west-2, completing the cross-region asynchronous processing workflow.</li></ol><p>To follow along, you will need access to an AWS account with sufficient permissions to create and manage the following services:</p><ul></ul><p>This tutorial assumes a basic familiarity with the AWS&nbsp;Console.</p><h3>🔵 Part 1. Create resources in destination region</h3><p>At this first part of the tutorial, we will create resources for the POC in the destination region (Oregon — us-west-2)</p><h4>Step 1: Create a DynamoDB Table in the Destination Region</h4><p>Next, we need a DynamoDB table in the destination region to store the processed messages.</p><p>1.1. In the us-west-2 region, open the <a href=\"https://console.aws.amazon.com/dynamodb/home\">DynamoDB Console</a>.1.2. Click .1.3. Provide the following settings:</p><ul><li>: destination_table</li><li>: id (type:&nbsp;String)</li></ul><p>1.4. Leave all other settings at their default values to enable on-demand (serverless) capacity.1.5. Click .</p><p>This table will receive and store the full payload, including both the source and destination timestamps.</p><h4>Step 2: Create Source and Destination Lambda Functions</h4><p>To start, we need to set up two AWS Lambda functions in different regions: one for sending events (source) and one for receiving and processing them (destination).</p><p>2.1. Navigate to the <a href=\"https://console.aws.amazon.com/lambda/home\">AWS Lambda Console</a>.2.2. Click .2.3. Choose  and configure the function:</p><h4>Step 3: Create the EventBridge Event Bus in the Destination Region</h4><p>In this step, you will configure a custom EventBridge event bus in the destination region (us-west-2). This bus will serve as the receiving endpoint for events routed from the source region, enabling cross-region event processing.</p><p>3.1. Open the <a href=\"https://console.aws.amazon.com/events/home\">Amazon EventBridge Console</a> in the us-west-2 region.3.2. In the left navigation go to , click at .</p><p>3.3. Enter the following name: poc-eventbridge-cross-region-destination-bus</p><p>3.4. Define , as the following policy:</p><pre>{  \"Version\": \"2012-10-17\",    {<p>      \"Sid\": \"AllowAccountToPutEvents\",</p>      \"Effect\": \"Allow\",        \"AWS\": \"arn:aws:iam::123456789012:root\"      \"Action\": \"events:PutEvents\",<p>      \"Resource\": \"arn:aws:events:us-west-2:123456789012:event-bus/poc-eventbridge-cross-region-destination-bus\"</p>    }}</pre><blockquote><em>123456789012 with your actual AWS account ID and adjust the bus name if you are using a different naming convention.</em></blockquote><p>3.6. After pasting the policy, click  to finish creating the Event&nbsp;Bus.</p><p>Your destination Event Bus is now ready to receive events from the source region via cross-region routing.</p><h4>Step 4: Create an EventBridge Rule to Trigger the Destination Lambda</h4><p>In this step, we will configure an EventBridge rule that listens to custom events published on the source event bus and invokes the destination Lambda function directly when those events match a defined&nbsp;pattern.</p><p>4.1. In the  created in the previous step (poc-eventbridge-cross-region-destination-bus), go to the  tab and click .</p><p>4.2. Enter the following details:</p><ul><li>: poc-eventbridge-cross-region-destination-rule</li><li>: Confirm that poc-eventbridge-cross-region-destination-bus is&nbsp;selected</li><li>: Rule with an event&nbsp;pattern</li></ul><p>4.3. Under , configure the following:</p><ul><li>: Select . This is required for custom events sent using the PutEvents API.</li><li>: Choose <strong>Custom pattern (JSON&nbsp;editor)</strong>.</li></ul><pre>{  \"source\": [\"poc.eventbridge.crossregion\"],<p>  \"detail-type\": [\"message-event\"]</p>}</pre><p>This pattern matches only the custom events emitted by the source Lambda function.</p><p>4.4. Under , configure the following:</p><ul><li>: Select </li><li>: Select </li><li>: Choose </li><li>: Select the destination-lambda function deployed in the destination region (us-west-2)</li></ul><p>4.5. Under , allow EventBridge to create a new execution role with permissions to invoke the&nbsp;Lambda.</p><p>4.6. Leave other settings as default and click  to complete the&nbsp;setup.</p><p>This rule is now ready to listen for events on the source bus and invoke the destination Lambda in another region when the event pattern&nbsp;matches.</p><h3>🔴 Part 2. Create Resources in Source&nbsp;Region</h3><p>At this Second part of the tutorial, we will create resources for the POC in the source region (N. Virginia — us-east-1)</p><h4>Step 5: Create the EventBridge Event Bus in the Source&nbsp;Region</h4><p>In this step, you will create a custom EventBridge Event Bus in the . This bus will explicitly receive events from the source Lambda function.</p><p>5.1. Open the <a href=\"https://console.aws.amazon.com/events/home\">Amazon EventBridge Console</a> in the us-east-1 region.5.2. In the left navigation pane, click .5.3. Choose .5.4. Enter the following name: poc-eventbridge-cross-region-source-bus<p>5.5. Leave the resource-based policy section empty — it is not needed since the Lambda is publishing within the same account and region.</p>5.6. Click  to finish creating the source Event&nbsp;Bus.</p><p>This Event Bus will now be used by the source Lambda to explicitly publish&nbsp;events.</p><h4>Step 6: Create an EventBridge Rule to Route from Source to Destination Bus</h4><p>Now that the source Event Bus is ready, we will create a rule in us-east-1 to match custom events and forward them to the destination Event Bus in us-west-2.</p><p>6.1. In the  created in the previous step (poc-eventbridge-cross-region-source-bus), go to the  tab and click .6.2. Enter the following configuration:</p><ul><li>: poc-eventbridge-cross-region-source-rule</li><li>: poc-eventbridge-cross-region-source-bus</li><li>: Rule with an event&nbsp;pattern</li></ul><p>6.3. Under :</p><ul><li>: Select&nbsp;</li><li>: Custom pattern (JSON&nbsp;editor)</li></ul><pre>{  \"source\": [\"poc.eventbridge.crossregion\"],<p>  \"detail-type\": [\"message-event\"]</p>}</pre><p>6.4. Under , choose <strong>Event bus in another account and&nbsp;region</strong>.</p><ul><li>: arn:aws:events:us-west-2:123456789012:event-bus/poc-eventbridge-cross-region-destination-bus</li><li>Replace 123456789012 with your actual AWS account&nbsp;ID.</li></ul><p>6.5. Leave all other settings as default and click .</p><p>This rule now listens to events on the custom source Event Bus and forwards them across regions to the destination Event&nbsp;Bus.</p><h4>Step 7: Create Source Lambda&nbsp;Function</h4><p>7.1. Navigate to the <a href=\"https://console.aws.amazon.com/lambda/home\">AWS Lambda Console</a>.7.2. Click .7.3. Choose  and configure the function:</p><ul><li>: source-lambda</li><li>: Python 3.x (or your preferred runtime)</li></ul><h4>Step 8: Create an API Gateway to Trigger the Source&nbsp;Lambda</h4><p>To expose the source Lambda via HTTP, we will use Amazon API&nbsp;Gateway.</p><p>8.1. In the us-east-1 region, navigate to the <a href=\"https://console.aws.amazon.com/apigateway\">API Gateway Console</a>.8.2. Choose  and click .8.3. Under , click at .8.4. Select POST as method type, choose  and select the source-lambda function.8.5. Use  button to deploy your API, define a desired stage.8.6. Under Stage tab, find the , you will need this url to test our implementation latter.</p><h4>Step 9: Test the End-to-End Flow</h4><p>Once all components are deployed:</p><p>9.1. Send a POST request to the API Gateway endpoint created in Step&nbsp;2.</p><pre>{  \"message\": \"Hello from the East!\"</pre><p>9.2. Check the DynamoDB table poc-eventbridge-cross-region-destination_table in us-west-2 to verify the message was&nbsp;stored.</p><p>This proof-of-concept validated Amazon EventBridge as a fast and reliable option for cross-region message replication. In our test, a message successfully traversed from us-east-1 to us-west-2 in approximately 634 milliseconds, highlighting the platform's low-latency performance.</p><p>While we implemented a basic event pattern, EventBridge supports more advanced rules that enable rich, event-driven architectures. This includes content-based filtering and complex routing logic, allowing for greater flexibility within each event&nbsp;bus.</p><p>Overall, this setup proves EventBridge’s capability to support scalable, asynchronous workflows across AWS regions using minimal, serverless infrastructure.</p><p>If you found this guide helpful, stay connected for more insights on <strong>AI, cloud security, and AWS automation</strong>:</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=36f5c71b89cf\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cilium Egress Gateway API: Implementing as an in-built Security through Kernel","url":"https://blog.devops.dev/cilium-egress-gateway-api-implementing-as-an-in-built-security-through-kernel-46d2670c2a7e?source=rss----33f8b2d9a328---4","date":1751133531,"author":"Aryan Parashar","guid":174611,"unread":true,"content":"<p>Kubernetes changed networking over pods and namespaces which was just limited to routing and security between the application which was controlled by pod networks using Network Policies.</p><p>Cilium implements egress control at Layer 3/4, directly in the Linux kernel using&nbsp;eBPF</p><p>With Many solutions implemented, external connectivity tries to communicate with workloads living outside the Kubernetes Cluster which gets subjected to connectivity constraints and security enforcements.</p><p>Traditional Firewalling usually relies on static IP Addresses, although in case of Kubernetes and Cloud workload we might also need to manage connectivity &amp; Traffic management with Dynamic IPs, which can be managed by Cilium Egress Gateway API and combination of some other Spacemaker solutions or Cloud-Native Solution Projects.</p><p>These traditional firewalling methods can make it really difficult to integrate a Kubernetes Cluster which has a varying and at the same time a dynamic number of nodes into such a&nbsp;network.</p><h3>Cilium Egress Gateway&nbsp;API</h3><p>Cilium Egress Gateway API solve these all problems by specifying which nodes should be used by any particular pod in order to reach outside&nbsp;world.</p><p>Traffic from these pods will be Source NATed to IP address of the node and will reach the external firewall with a predictable IP, enabling firewall to enforce right policy on a specific&nbsp;pod.</p><p>In contrast to other Egress or Gateway API solutions, a Cilium Egress Gateway&nbsp;API:</p><ul><li>Requires Cilium kube-proxy replacement</li><li>Enables connectivity between Kubernetes Cluster and traditional Firewall.</li></ul><h3>Benefits of using Cilium Egress Gateway&nbsp;API</h3><p>Cilium implements egress control at Layer 3/4, directly in the Linux kernel using eBPF, offering:</p><ol><li>Policy-based egress routing: You can define egress policies that route traffic based on pod labels, destination CIDRs, and even IP&nbsp;pools.</li><li>IP Pooling: Assigns dynamic or static public egress IPs per node or pod&nbsp;group.</li><li>Node-aware policies: You can ensure traffic is routed through a specific node with a public&nbsp;IP.</li><li>Failover support: If a node becomes unavailable, Cilium can reroute via another gateway dynamically.</li><li>No sidecar, no proxy overhead: Much lower latency and CPU&nbsp;usage.</li><li>Integration with Gateway API (in progress/alpha): Brings declarative Kubernetes-native way of managing egress at CNI&nbsp;level.</li></ol><h3>Implementing Cilium Egress Gateway&nbsp;API</h3><p>Here’s an example which, we can also find from the official example given by the Cilium and Isovalent.</p><p>Lets say we had 4 nodes as kind-worker, kind-worker2, kind-worker3, kind-worker4 with the 3rd and 4th node been planned to be deployed as Egress GatewayAPI. Now to handle the Egress Gateway functioning, we will carry out the following practices:</p><ul><li>Now as discussed for choosing which node to get deployed/bound with a pod for handling traffic, we’ll taint them from mistakenly scheduling any general-purpose pods</li></ul><pre>kubectl taint node kind-worker3 egress-gw:NoSchedule   kubectl taint node kind-worker4 egress-gw:NoSchedule</pre><ul><li>Label them for better monitoring and observability.</li></ul><pre>kubectl label nodes kind-worker3 egress-gw=true   kubectl label nodes kind-worker4 egress-gw=true</pre><ul><li>All the kind nodes are attached to Docker network called kind (Kubernetes in Docker) which use 172.18.0.0/16 IPV4&nbsp;CIDR.</li></ul><pre>docker network inspect -f '{{range.IPAM.Config}}{{.Subnet}}, {{end}}' kind</pre><p>CIDR is Classless Inter Domain Routing Method for allocating IP address on the internet, designed to improve efficiency and manage growth of internet’s routing&nbsp;table.</p><ul><li>Add a new Dummy interface eth0 to both the kind-workers 3 &amp; 4 with new address 172.18.0.42/16 and then add it to kind-worker 3 and 4 who’s IPs will be used by&nbsp;Cilium.</li></ul><pre>docker exec kind-worker3 ip link add net0 type dummy   docker exec kind-worker3 ip a add 172.18.0.42/16 dev net0   docker exec kind-worker3 ip link set net0 updocker exec kind-worker4 ip link add net0 type dummy   docker exec kind-worker4 ip a add 172.18.0.43/16 dev net0   docker exec kind-worker4 ip link set net0 up</pre><ul><li>Add Cilium and Egress Gateway API, disable Layer 7 proxy as its incomplete with Egress Gateway and attach two network interfaces to Egress Nodes called eth0 and&nbsp;net0.</li></ul><pre>cilium install \\     --version 1.17.1 \\     --set kubeProxyReplacement=true \\     --set egressGateway.enabled=true \\     --set bpf.masquerade=true \\     --set l7Proxy=false \\     --set devices=\"{eth+,net+}\"</pre><ul><li>Verify also that Cilium was started with the Egress Gateway&nbsp;feature:</li></ul><pre>cilium config view | grep egress-gateway</pre><h3>Setting up Egress Server and Egress&nbsp;Policies</h3><p>Needs to be attached to kind network, and we will pass the allowed source IP addresses as environment variables:</p><pre>docker run -d \\  --name remote-outpost \\  -e ALLOWED_IP=172.18.0.42,172.18.0.43 \\<p>   quay.io/isovalent-dev/egressgw-whatismyip:latest</p></pre><p>Retrieve the container’s IP in a variable:</p><pre>OUTPOST=$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' remote-outpost)echo $OUTPOS</pre><ul><li>Now lets deploy two more pods in our external outpost and see if they responds according to the labels we attached to&nbsp;them:</li></ul><pre>kubectl run tiefighter \\     --labels \"org=empire,class=tiefighter\" \\     --image docker.io/tgraf/netperf   kubectl run xwing \\     --labels \"org=alliance,class=xwing\" \\     --image docker.io/tgraf/netperf</pre><ul><li>Now we will add a Gateway Policies egress-gw-policy.yaml to route traffic to the external workload running in the&nbsp;network.</li><li>Resource to specify Cilium which Egress needs to be used for the traffci we’ll use: CiliumEgressGatewayPolicy</li><li>This will allow traffic to flow through one of the two Egress Nodes i.e. kind-worker3 or kind-worker4&nbsp;.</li></ul><ul><li>Here’s the egress-gw-policy.yaml file&nbsp;defined:</li></ul><pre>apiVersion: cilium.io/v2kind: CiliumEgressGatewayPolicy  name: outpost  destinationCIDRs:  selectors:      matchLabels:  egressGateway:      matchLabels:    interface: net0</pre><p>Then deploy the definition file it to the workload:</p><pre>kubectl apply -f egress-gw-policy.yaml</pre><p>If we try to deploy another pod with a new IP address with the org=alliance, then it will be accepted as it accepts the definition file&nbsp;policies</p><pre>kubectl run ywing \\  --labels \"org=alliance,class=ywing\" \\<p>  --image docker.io/tgraf/netperf</p></pre><h3>Accessing the external outpost server from the pods&nbsp;deployed</h3><pre>kubectl exec -ti xwing -- \\  curl --max-time 2 http://172.18.0.7:8000<p>kubectl exec -ti tiefighter -- \\</p>  curl --max-time 2 http://172.18.0.7:8000<p>kubectl exec -ti ywing -- \\</p>  curl --max-time 2 http://172.18.0.7:8000</pre><ol><li>Here since the traffic exits through one of the first two IP addresses been listed thatswhy, this command will give a success result for one amongst the first 2 of the IP&nbsp;address.</li><li>It will also give a success message with the 3rd IP address because it has org=alliance which matches with the policies been listed in the definition file for the Egress GatewayAPI.</li></ol><p>Access the outpost from the X-Wing a few times in a&nbsp;loop:</p><pre>for i in $(seq 1 10); do  kubectl exec -ti xwing -- \\<p>    curl --max-time 2 http://172.18.0.7:8000</p>done</pre><p>Note that traffic always exits the cluster from the same IP address, which means it always uses the same exit&nbsp;node.</p><p>In Cilium OSS, Egress Gateway Policies are used to select a node for a traffic, and that node will always be used for that given&nbsp;traffic.</p><p>For Managing the IP through which our traffic can exit, we will need to use High Availability type while defining our Cilium Egress Gateway&nbsp;API.</p><p>Soon, I will be uploading another Blog on High Availability in Cilium Egress Gateway API and will also integrate other CloudNative Solutions like kube-vip, etc. Till then stay&nbsp;excited:</p><p>If you like my Article then please react to it and connect with me on Linkedin &amp; Twitter if you are also a tech enthusiast. I would love to collaborate with people and share the experience of tech😄😄.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=46d2670c2a7e\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Complete CI/CD Pipeline Guide: Jenkins, Docker & GitHub Automation (2025)","url":"https://blog.devops.dev/the-complete-ci-cd-pipeline-guide-jenkins-docker-github-automation-2025-89479420e7fa?source=rss----33f8b2d9a328---4","date":1751133525,"author":"Ashish Singh","guid":174610,"unread":true,"content":"<div><p>An automated deployment system using Jenkins, Docker, Docker Compose, Git, and GitHub creates a powerful and flexible CI/CD pipeline. This…</p></div>","contentLength":141,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DevSecOps : Pipeline in Jenkins with Email Notification","url":"https://blog.devops.dev/devsecops-pipeline-in-jenkins-with-email-notification-0c7acfd0ce8f?source=rss----33f8b2d9a328---4","date":1751128514,"author":"Saisamarth Udikeri","guid":174571,"unread":true,"content":"<h3>DevSecOps&nbsp;: Pipeline in Jenkins with Email Notification</h3><p>In this article we’ll walk through a complete CI/CD pipeline that integrates security scanning at every stage. We’ll use Jenkins for orchestration, OWASP Dependency-Check and Trivy for vulnerability detection, SonarQube for code quality and security rules, Docker for packaging, and ArgoCD for continuous deployment to Kubernetes.</p><p>DevSecOps is the practice of integrating security “left” into every phase of the software delivery lifecycle — rather than bolting it on at the end. By embedding automated security checks into your CI/CD pipeline, you&nbsp;can:</p><ul><li><strong>Catch vulnerabilities early</strong> (in code and dependencies)</li><li> before merge or&nbsp;deploy</li><li><strong>Automate remediation feedback</strong> to developers</li><li> and compliance overhead</li></ul><h4>2. Overview of the&nbsp;Pipeline</h4><p>Below is a generic Jenkinsfile that implements:</p><ol><li><strong>Update Kubernetes manifests</strong></li></ol><pre>pipeline {  agent any    nodejs 'NodeJS'    SONAR_PROJECT_KEY   = 'MyAppProject'<p>    SONAR_SCANNER_HOME  = tool 'SonarScanner'</p>    SONAR_TOKEN         = credentials('sonar-token')<p>    OWASP_TOOL_HOME     = tool 'OWASP-CLI'</p>    DOCKER_REPO         = 'myorg/myapp'    stage('Checkout') {        git branch: 'main',<p>            url:    'https://github.com/myorg/myapp.git'</p>      }<p>    stage('Install &amp; Build') {</p>      steps {        sh 'npm run build'    }<p>    stage('OWASP Dependency-Check') {</p>      steps {<p>        catchError(buildResult: 'SUCCESS', stageResult: 'SUCCESS') {</p>          sh \"\"\"<p>            ${OWASP_TOOL_HOME}/bin/dependency-check.sh \\</p>              --project \"${SONAR_PROJECT_KEY}\" \\              --format XML \\              --out reports/dependency-check          dependencyCheckPublisher(<p>            pattern: 'reports/dependency-check/dependency-check-report.xml',</p>            stopBuild: false        }    }<p>    stage('SonarQube Analysis') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        withSonarQubeEnv('SonarQube') {            ${SONAR_SCANNER_HOME}/bin/sonar-scanner \\<p>              -Dsonar.projectKey=${SONAR_PROJECT_KEY} \\</p>              -Dsonar.sources=src \\<p>              -Dsonar.host.url=${env.SONAR_HOST_URL} \\</p>              -Dsonar.login=${SONAR_TOKEN}        }      post {          waitForQualityGate abortPipeline: true      }<p>    stage('Build Docker Image') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        script {<p>          def tag = \"${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}\"</p>          docker.build(tag)      }<p>    stage('Trivy Container Scan') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        script {<p>          def tag = \"${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}\"</p>          sh \"\"\"              --severity HIGH,CRITICAL \\              --no-progress \\<p>              ${tag} &gt; reports/trivy-report.txt</p>          \"\"\"      }        always {<p>          archiveArtifacts artifacts: 'reports/trivy-report.txt', fingerprint: true</p>        }    }<p>    stage('Push to Registry') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        script {<p>          docker.withRegistry('', 'docker-creds') {</p>            docker.image(\"${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}\").push()        }    }<p>    stage('Update K8s Manifests') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        withCredentials([usernamePassword(credentialsId: 'git-creds', usernameVariable: 'GIT_USER', passwordVariable: 'GIT_TOKEN')]) {            checkout([              branches: [[name: '*/main']],                url: 'https://github.com/myorg/k8s-manifests.git',<p>                credentialsId: 'git-creds'</p>              ]]            sh \"\"\"<p>              sed -i 's|image:.*|image: ${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}|' deployment.yaml</p>              git config user.email \"ci@myorg.com\"<p>              git config user.name  \"CI Bot\"</p>              git add deployment.yaml<p>              git commit -m \"Update image to 1.0.${env.BUILD_NUMBER}\"</p>              git push origin main          }      }      steps {<p>        sh \"docker rmi ${DOCKER_REPO}:1.0.${env.BUILD_NUMBER} || true\"</p>      }  }    success {        attachLog: true,<p>        attachmentsPattern: 'dependency-check-report/*.html, dependency-check-report/*.xml, trivy-scan-report.txt',</p>        from:    name@gmail.com',<p>        to:      name@gmail.com',</p>        subject: \"✅ Build #${env.BUILD_NUMBER} of ${env.JOB_NAME} Succeeded\",        body: \"\"\"            &lt;body&gt;<p>              &lt;h2 style=\"color: green;\"&gt;Build Succeeded!&lt;/h2&gt;</p>              &lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; ${env.JOB_NAME}&lt;/p&gt;<p>              &lt;p&gt;&lt;strong&gt;Build Number:&lt;/strong&gt; ${env.BUILD_NUMBER}&lt;/p&gt;</p>              &lt;p&gt;&lt;strong&gt;Build URL:&lt;/strong&gt; &lt;a href=\"${env.BUILD_URL}\"&gt;${env.BUILD_URL}&lt;/a&gt;&lt;/p&gt;<p>              &lt;p&gt;&lt;strong&gt;Preview Site:&lt;/strong&gt; &lt;a href=\"site\"&gt;site/&lt;/a&gt;&lt;/p&gt;</p>            &lt;/body&gt;        \"\"\"    }      emailext(        attachmentsPattern: 'dependency-check-report/*.html, dependency-check-report/*.xml, trivy-scan-report.txt',<p>        from:    name@gmail.com',</p>        to:      name@gmail.com',<p>        subject: \"❌ Build #${env.BUILD_NUMBER} of ${env.JOB_NAME} Failed\",</p>        mimeType: 'text/html',          &lt;html&gt;              &lt;h2 style=\"color: red;\"&gt;Build Failed!&lt;/h2&gt;<p>              &lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; ${env.JOB_NAME}&lt;/p&gt;</p>              &lt;p&gt;&lt;strong&gt;Build Number:&lt;/strong&gt; ${env.BUILD_NUMBER}&lt;/p&gt;<p>              &lt;p&gt;&lt;strong&gt;Build URL:&lt;/strong&gt; &lt;a href=\"${env.BUILD_URL}\"&gt;${env.BUILD_URL}&lt;/a&gt;&lt;/p&gt;</p>              &lt;p&gt;Please review the console output and attached reports for details.&lt;/p&gt;          &lt;/html&gt;      )  }</pre><p>Use Docker Compose to stand up Jenkins with Docker‐in‐Docker support:</p><pre>version: '3.8'services:    image: jenkins/jenkins:lts    ports:      - \"50000:50000\"      - DOCKER_GID=${DOCKER_GID}      - /var/run/docker.sock:/var/run/docker.sock<p>      - /usr/bin/docker:/usr/bin/docker</p></pre><p> on host and export DOCKER_GID.</p><p> with docker-compose up&nbsp;-d.</p><ul></ul><ul><li> (e.g. “NodeJS&nbsp;14.x”)</li></ul><ul><li>Git (username/password or&nbsp;PAT)</li></ul><p>Deploy a SonarQube + PostgreSQL stack via Docker&nbsp;Compose:</p><pre>version: '3'services:    image: sonarqube:lts-community      SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar<p>      SONAR_JDBC_USERNAME: sonar</p>      SONAR_JDBC_PASSWORD: sonar      - \"9001:9000\"      - ./conf:/opt/sonarqube/conf<p>      - ./data:/opt/sonarqube/data</p>      - ./extensions:/opt/sonarqube/extensions<p>      - ./logs:/opt/sonarqube/logs</p>      - ./temp:/opt/sonarqube/temp    image: postgres:13      POSTGRES_USER: sonar      POSTGRES_DB: sonar      - ./postgresql_data:/var/lib/postgresql/data</pre><ul><li> and chown 1000:1000.</li><li> with docker-compose up&nbsp;-d.</li><li> at http://&lt;host&gt;:9001 (admin/admin).</li><li> and ; add to Jenkins as&nbsp;secret.</li></ul><h4>5. OWASP Dependency-Check in&nbsp;Jenkins</h4><ol><li> the OWASP Dependency-Check plugin.</li><li> the CLI tool and add it in <strong>Manage Jenkins → Global Tool Configuration</strong>.</li><li> the tool by name (OWASP-CLI) in your Jenkinsfile.</li><li> the XML report with dependencyCheckPublisher.</li></ol><h4>6. Trivy Container Scanning</h4><p><a href=\"https://github.com/aquasecurity/trivy\">Trivy</a> is a simple vulnerability scanner for containers:</p><pre>trivy image --severity HIGH,CRITICAL --exit-code 1 myorg/myapp:latest</pre><ul><li> Trivy on your Jenkins agent (apt, brew or binary download).</li><li> the plain-text report for&nbsp;audit.</li></ul><h4>7. Continuous Deployment with&nbsp;ArgoCD</h4><p>ArgoCD lets you declaratively push Kubernetes manifests:</p><pre>kubectl create namespace argocdkubectl apply -n argocd \\<p>  -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</p>kubectl patch svc argocd-server \\  -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'</pre><ul><li> for sync and rollbacks.</li><li> (argocd login &lt;server&gt;) and  your Git&nbsp;repo.</li><li> an App that points at myorg/k8s-manifests.</li></ul><p>By combining Jenkins, SonarQube, OWASP Dependency-Check, Trivy and ArgoCD, you get a fully automated, policy-enforced DevSecOps pipeline. Shift security left, automate compliance, and deliver faster with confidence. Feel free to fork the sample repos and adapt the placeholders (MyAppProject, myorg/myapp, etc.) to kick off your own secure delivery process&nbsp;today!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0c7acfd0ce8f\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7936,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop Using Loops in JavaScript: 6 Smarter, Cleaner Alternatives You Should Know","url":"https://blog.devops.dev/stop-using-loops-in-javascript-6-smarter-cleaner-alternatives-you-should-know-1b3984e24694?source=rss----33f8b2d9a328---4","date":1751128509,"author":"David Lam","guid":174570,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Practical Guide to Common Podman Commands — Part 2","url":"https://blog.devops.dev/a-practical-guide-to-common-podman-commands-part-2-8a3e6633308a?source=rss----33f8b2d9a328---4","date":1751128505,"author":"Vijayasekhar Deepak","guid":174569,"unread":true,"content":"<div><p>In this blog, I will share the list of mostly used podman commands for your development. This blog is part of the Podman series on how to…</p></div>","contentLength":140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}