{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":42,"items":[{"title":"ChatLoopBackOff: Episode 68 (KANISTER)","url":"https://www.youtube.com/watch?v=-dy_J3VmmOg","date":1755903066,"author":"CNCF [Cloud Native Computing Foundation]","guid":237127,"unread":true,"content":"<article>Join us LIVE as CNCF Ambassador Carlos Santana dives into Kanister, the open source framework for application-level data management on Kubernetes.\n\nCarlos will be exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that highlight how Kanister helps extend Kubernetes for backup, recovery, and data operations.\n\nIf you‚Äôre curious about how cloud native projects approach complex data management challenges, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.\n\nBring your questions, share your experiences, and learn in real time as we explore Kanister together!</article>","contentLength":731,"flags":null,"enclosureUrl":"https://www.youtube.com/v/-dy_J3VmmOg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Blind & Visually Impaired Initiative (BVI) Meeting - 2025-08-19","url":"https://www.youtube.com/watch?v=bJej44Ug8tU","date":1755893803,"author":"CNCF [Cloud Native Computing Foundation]","guid":237082,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":319,"flags":null,"enclosureUrl":"https://www.youtube.com/v/bJej44Ug8tU?version=3","enclosureMime":"","commentsUrl":null},{"title":"SRE.ai Looks to Unify DevOps Workflows Across Multiple SaaS Applications","url":"https://devops.com/sre-ai-looks-to-unify-devops-workflows-across-multiple-saas-applications/?utm_source=rss&utm_medium=rss&utm_campaign=sre-ai-looks-to-unify-devops-workflows-across-multiple-saas-applications","date":1755892226,"author":"Mike Vizard","guid":237069,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digest #177: AWS in 2025, HashiCorp Vault Zero-Day Flaws, Why No SRE, Docker-Compose Tricks","url":"https://www.devopsbulletin.com/p/digest-177-aws-in-2025-hashicorp","date":1755882215,"author":"Mohamed Labouardy","guid":237011,"unread":true,"content":"<p>Welcome to this week‚Äôs edition of the DevOps Bulletin!</p><p>AWS has changed more than you think: EC2 roles and EBS volumes can now be updated live, S3 is consistent and encrypted by default, and tools like VPC Lattice are simplifying networking. Netflix revealed how they still run mostly on Java with Spring Boot and GraphQL, while researchers uncovered nine zero-day flaws in HashiCorp Vault. And if you‚Äôve been debating an SRE team, there‚Äôs a fresh take on why you might not need one.</p><p>On the tutorial front: learn how to set up a safe malware-analysis lab on AWS, try Docker Compose tricks to speed up your workflow, or build a tiny Python agent in ~70 lines. You‚Äôll also find guides on branching strategies, a serverless chat room with AWS, OpenTelemetry configuration gotchas, deploying Tetris on ECS, plus practical explainers on Bash, DBMS, Kitty terminal tweaks, and Copilot instructions.</p><p>Our open-source spotlight features Rendergit to flatten repos into HTML, Zizmor for scanning GitHub Actions, and Data Formulator for AI-powered charting. tfclean tidies Terraform configs, Runecs manages ECS tasks, and ChartDB turns schemas into shareable diagrams.</p><p>All this and more in this week‚Äôs DevOps Bulletin, don‚Äôt miss out!</p><h4>üìò New Book: </h4><blockquote><p>The material comes straight from years of building a FinOps platform for Fortune-500 engineering teams, thousands of AWS, Azure, and GCP accounts, petabytes of data, and enough untagged resources to make a CFO cry.</p><p>Along the way, I kept a lab notebook of what actually worked and, more importantly, what didn‚Äôt. That notebook turned into this book.</p><p>üìö Grab  with 50% off (early access): <a href=\"https://www.manning.com/books/practical-finops\">here</a></p></blockquote><ul><li><p> any git repo into a single static HTML page for humans or LLMs.</p></li><li><p> is a static analysis tool for GitHub Actions. It can find many common security issues in typical GitHub Actions CI/CD setups.</p></li><li><p> can transform data and create rich visualizations iteratively with AI.</p></li></ul><ul><li><p>&nbsp;is a tool for cleaning up Terraform configuration files by automatically removing applied, moved, imported, and removed blocks.</p></li><li><p> allows you to run tasks and manage your services on AWS ECS.</p></li><li><p>&nbsp;is a database diagramming tool that enables you to visualize and design your database with a single query.</p></li></ul><ul><li><p> protects your codebase by controlling who can change what. Set authorization levels, lock down files, and enforce your rules.</p></li></ul><div><p> If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":2462,"flags":null,"enclosureUrl":"https://substackcdn.com/image/youtube/w_728,c_limit/DU7_MQmRDUs","enclosureMime":"","commentsUrl":null},{"title":"Set up a Playwright Browser Server in AWS EC2","url":"https://blog.devops.dev/set-up-a-playwright-browser-server-in-aws-ec2-7f5ccb9819f3?source=rss----33f8b2d9a328---4","date":1755876918,"author":"th@n@n","guid":237010,"unread":true,"content":"<p>Transforming development and test workflows can save time. In this guide, we‚Äôll explore setting up a Playwright browser server on an AWS EC2 instance. Imagine developers and QA teams pushing many commits. Each event triggers a QA test build. Inefficiency lurks here‚Ää‚Äî‚Ääevery build downloads images and installs browsers. Exhausting, isn‚Äôt&nbsp;it?</p><p>But consider this: a centralised Playwright browser server. It smoothens the process, boosting efficiency. Now, envision your SRE team. They want to monitor network calls, keep an eye on application performance, and collect diverse metrics. A singular point for all this data is&nbsp;crucial.</p><p>Finally, let‚Äôs break it down. We compare Playwright‚Äôs launch and launch server&nbsp;modes.</p><p>Alright..!!!, Let‚Äôs see the implementation, there are some prerequisites</p><ol><li>Create an instance in&nbsp;EC2</li><li>Pull the playwright image and execute the container with browser&nbsp;server</li></ol><p>Create a simple instance as per your wish, don‚Äôt forget to download the pem files for ssh connection, used a free tier&nbsp;option.</p><p>We need to add the port 3000 inbound rules, I explain the reason later in the blog. Apart from I enable port 22 to my IP for ssh connection in order to install any softwares.</p><p>Connect to your instance, SSH the key. Here‚Äôs the&nbsp;command</p><pre>ssh -i \"&lt;file_name&gt;.pem\" ec2-user@&lt;ip address or machine name&gt;</pre><ol><li>Update the linux system </li><li>Install docker <strong>sudo yum install -y&nbsp;docker</strong></li><li>Let‚Äôs start the docker service <strong>sudo service docker&nbsp;start</strong></li><li>Check the status of docker <strong>sudo service docker&nbsp;status</strong></li><li>Since docker is installed as root user, we need to add ec2-user to the docker group <strong>sudo usermod -a -G docker&nbsp;ec2-user</strong></li><li>Now run the below&nbsp;command</li></ol><pre>docker run -d \\  --name pw-server \\  -p 3000:3000 \\<p>  mcr.microsoft.com/playwright:v1.54.0-jammy \\</p>  /bin/sh -c \"cd /home/pwuser &amp;&amp; npx -y playwright@1.54.0 run-server --port 3000\"</pre><p>Now check the status of the image and container</p><p>Let‚Äôs check the whether this browser server is available publicly, run the below&nbsp;command</p><pre>[ec2-user@thanan ~]$ curl -I http://&lt;DNS address&gt;:3000/HTTP/1.1 200 OK<p>Date: Sat, 16 Aug 2025 11:32:29 GMT</p>Connection: keep-alive</pre><p><em>Note: Now this is available via </em></p><p>Time to execute the test script, go to your playwright framework, since we run the browser server in docker, make sure the script are run in headless mode. I am refer this <a href=\"https://github.com/thananauto/playwright-tips-tricks\">repo</a> for code execution. Update the below configuration globally in playwright.config.ts</p><p>If you want to execute the scripts in head mode, we need `XServer` running in server, due to security reason, org won‚Äôt easily install this&nbsp;package</p><p>Time to test! Run this&nbsp;command:</p><pre>PW_TEST_CONNECT_WS_ENDPOINT=http://&lt;DNS Host address&gt;:3000/ npx playwright test --project=UI --grep @ui-one</pre><p>Eagerly, await results. They pop as the execution concludes, success in each&nbsp;run.</p><p>From configuring the instance and opening the necessary ports to installing Docker and deploying the Playwright image, While there are still opportunities for improvement, such as handling IP address changes with each instance restart and the need to redeploy the Playwright container upon reboot, this blog serves as a foundational guide.</p><p>If you like this content, üëèüëèüëè&nbsp;here</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7f5ccb9819f3\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Deep Dive into OTA Update Support with KubeEdge for Edge Kubernetes","url":"https://blog.devops.dev/a-deep-dive-into-ota-update-support-with-kubeedge-for-edge-kubernetes-690da554b727?source=rss----33f8b2d9a328---4","date":1755876892,"author":"M Mahdi Ramadhan, M. Si","guid":237009,"unread":true,"content":"<p>I have always been deeply inspired by the orchestration capabilities of Kubernetes. Over the years, many of the digital platforms I have designed and built were intentionally directed toward Kubernetes as the backbone‚Ää‚Äî‚Ääthough always with careful consideration of context, to avoid making it overkill where simpler solutions would&nbsp;suffice.</p><p>That said, one area where Kubernetes reveals its true potential is in . The proliferation of IoT devices and the exponential growth of real-time data processing demands have reshaped the computing paradigm. While centralized cloud data centers remain critical, the need for ultra-low latency, bandwidth efficiency, and local autonomy has pushed workloads closer to where data is generated: the&nbsp;edge.</p><p>This shift introduces unique challenges‚Ää‚Äî‚Ääparticularly when managing and updating thousands of heterogeneous, geographically distributed devices. Standard Kubernetes, while powerful, is inherently cloud-centric and often too heavy for resource-constrained edge environments.</p><p>This is where , an open-source CNCF project, comes in. It extends native Kubernetes capabilities to the edge, enabling orchestrated container workloads, data synchronization, and device lifecycle management. One of its most transformative features is its support for <strong>Over-the-Air (OTA) updates</strong>‚Ää‚Äî‚Ääa critical mechanism to ensure that containerized applications, edge runtimes, and even firmware on physical devices can be securely updated without manual intervention.</p><p>In this article, I want to share a deep, experience-based exploration of how KubeEdge enables OTA updates in practice. I will also include a , a , and a  highlighting pitfalls I have encountered and the mitigation strategies that work in real-world deployments.</p><h3>From Cloud-Centric to Edge-Native Orchestration</h3><p>Traditional cloud computing assumes stable, high-bandwidth connections. But in edge scenarios such as autonomous driving, industrial IoT, or smart agriculture, connectivity may be intermittent, while latency requirements are unforgiving.</p><p>Standard Kubernetes is not edge-ready by default. It assumes persistent cloud connectivity, strong compute nodes, and central control. In contrast:</p><ul><li>Edge devices often run on ARM processors with limited resources.</li><li>Connectivity can drop unexpectedly for hours or&nbsp;days.</li><li>Devices may require direct hardware interaction (sensors, actuators, PLCs).</li></ul><p>KubeEdge was designed to address these gaps by splitting Kubernetes into two logical&nbsp;planes:</p><ul><li>CloudCore ‚Äì runs in the cloud or a central data center; manages policies, device models, OTA jobs, and workload orchestration.</li><li>EdgeCore ‚Äì runs on the edge device; autonomously executes workloads, manages hardware states, and persists operations even if the cloud connection is&nbsp;lost.</li></ul><h3>OTA: The Linchpin of Edge Device Lifecycle Management</h3><p>In real-world edge deployments, the cost of physically updating devices is prohibitive. Imagine dispatching technicians to update 10,000 industrial sensors across factories or retail kiosks across 500 stores. Without OTA, such operations are not feasible.</p><p>KubeEdge addresses this with a multi-layered OTA mechanism:</p><h4>1. Containerized Application OTA</h4><ul><li>Applications deployed as Kubernetes workloads.</li><li>Updated through declarative manifests (Deployment, DaemonSet).</li><li>Rolling updates orchestrated at the edge, resilient to connectivity drops.</li></ul><ul><li>Managed via Device CRDs (Custom Resource Definitions).</li><li>Desired firmware version defined in the Device&nbsp;spec.</li><li>EdgeCore (or a custom mapper) pulls binaries, validates checksums/signatures, flashes the firmware, and updates the status back to CloudCore.</li></ul><h4>3.\tNode OTA (EdgeCore / runtime&nbsp;upgrade)</h4><ul><li>Using NodeUpgradeJob (introduced in&nbsp;v1.19+).</li><li>Securely upgrades KubeEdge edge components, ensuring backward compatibility and checksum validation before activation.</li></ul><p>Together, these cover the full lifecycle of an edge system: from the application layer, down to the firmware and edge runtime&nbsp;itself.</p><h3>Case Study: Smart Retail Kiosk&nbsp;Fleet</h3><p>A retail enterprise deployed 5,000 smart kiosks nationwide. Each kiosk included:</p><ul><li>Containerized applications for payment, ads display, and inventory sync.</li><li>Firmware-controlled peripherals (barcode scanner, receipt printer).</li><li>An edge runtime running KubeEdge EdgeCore.</li></ul><ul><li>Connectivity: Some kiosks connected over 4G with frequent dropouts.</li><li>Heterogeneity: Devices had different firmware baselines depending on vendor&nbsp;batch.</li><li>Security: Regulatory requirement that every update must be signed and auditable.</li></ul><ul><li>Defined a DeviceModel capturing properties: firmware version, display resolution, payment module&nbsp;version.</li><li>Managed application updates by publishing new container images, updating the Deployment spec ‚Äì CloudCore handled synchronization, and EdgeCore performed rolling updates&nbsp;locally.</li><li>For firmware updates, an OTA Job resource was created with a signed binary hosted on an HTTPS OTA server. EdgeCore validated often overlooked, but based on my experience many errors occur during data transmission), updated firmware via serial flashing, and patched the Device CRD status with the new&nbsp;version.</li></ul><p>For EdgeCore runtime upgrades, NodeUpgradeJobs were rolled out in controlled batches (5% of nodes at a&nbsp;time).</p><ul><li>Updates reduced from 3 ‚Äì 4 weeks manual rollout to 2 hours automated deployment.</li><li>98% first-pass update success rate; failures automatically retried once connectivity was restored.</li><li>OTA audit logs integrated with enterprise compliance systems, ensuring traceability.</li></ul><h3>Tutorial: Building a KubeEdge OTA&nbsp;Setup</h3><ul><li>A Kubernetes cluster (v1.25+).</li><li>keadm (KubeEdge deployment tool).</li><li>One VM/cloud server (cloud side), one edge node (VM or Raspberry Pi)</li></ul><p>Step 1: Install CloudCore</p><pre>sudo keadm init  ‚Äì kubeedge-version v1.20.0keadm gettoken</pre><pre>sudo keadm join  ‚Äì cloudcore-ipport &lt;CLOUD_IP&gt;:10000 \\  --token &lt;TOKEN&gt; \\<p>  --kubeedge-version v1.20.0</p></pre><p>Step 3: Define a DeviceModel</p><pre>apiVersion: devices.kubeedge.io/v1beta1kind: DeviceModel  name: firmware-updater-model  properties:      type: string</pre><p>Step 4: Define a Device&nbsp;Instance</p><pre>apiVersion: devices.kubeedge.io/v1beta1kind: Device  name: edge-device-01  nodeName: edge-node-1    name: firmware-updater-model    - name: firmware_version      reported: \"1.0.0\"</pre><p>Step 5: Trigger an OTA&nbsp;Update</p><pre>properties:  - name: firmware_version</pre><pre>kubectl patch device edge-device-01 \\  --type=merge \\<p>  -p '{\"status\":{\"twins\":{\"firmware_version\":{\"reported\":{\"value\":\"2.0.0\"}}}}}'</p></pre><h3>Risk Registry: Challenges and Preparedness in KubeEdge OTA Deployments</h3><p>While KubeEdge provides a robust OTA mechanism, production deployments must account for inherent risks. Below is a risk registry distilled from real-world projects:</p><ul><li>OTA is both a business enabler and an attack vector ‚Äì security and rollback strategies must be non-negotiable.</li><li>Connectivity resilience is paramount; production rollouts must batch, monitor, and retry intelligently.</li><li>Observability and compliance logging are not optional for industries subject to regulation.</li></ul><p>KubeEdge elevates Kubernetes from being a container orchestrator to becoming a full lifecycle management platform for edge computing. Its OTA support ensures that applications, firmware, and runtime components can evolve securely and reliably ‚Äì at&nbsp;scale.</p><p>From smart retail kiosks to industrial automation, OTA in KubeEdge transforms edge infrastructure from brittle, manually maintained systems into self-updating, cloud-native edge&nbsp;fleets.</p><p>As organizations accelerate toward Industry 4.0, 5G, and intelligent IoT, embracing KubeEdge‚Äôs OTA capabilities is not just an optimization ‚Äì it is a necessity for resilience, security, and scalability at the&nbsp;edge.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=690da554b727\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Sales Data Predict Stock Prices? A Data Science Experiment with Python","url":"https://blog.devops.dev/can-sales-data-predict-stock-prices-a-data-science-experiment-with-python-2b0d54d711f0?source=rss----33f8b2d9a328---4","date":1755876888,"author":"Vaishnavi Ganeshkar","guid":237008,"unread":true,"content":"<p>What if a company‚Äôs <strong>sales performance could predict its stock price</strong>? We hear investors talking about revenue, growth, and quarterly reports all the time‚Ää‚Äî‚Ääbut can we actually <strong>see the connection in&nbsp;data</strong>?</p><p>In this blog, we‚Äôll run a fun data experiment: analyzing  and  side by side using Python. By the end, you‚Äôll know whether <strong>better sales = higher stock prices</strong>, and what this means for data analysts, businesses, and investors.</p><ul><li>: Downloaded with the yfinance library (Yahoo Finance&nbsp;API).</li></ul><p>This gives us a way to compare <strong>company‚Äôs sales performance (business side)</strong> with <strong>stock market valuation (investor side)</strong>.</p><h3>‚öôÔ∏è Step 1: Setup Environment</h3><pre>!pip install pandas matplotlib seaborn yfinance</pre><pre>import pandas as pdimport matplotlib.pyplot as pltimport yfinance as yf</pre><p>üìÇ Step 2: Load Sales&nbsp;Data</p><pre># Load sales datasetsales = pd.read_csv(\"sales_data_sample.csv\", encoding=\"latin1\")sales['ORDERDATE'] = pd.to_datetime(sales['ORDERDATE'])</pre><p>We‚Äôll use  and  for monthly aggregation.</p><h3>üìà Step 3: Monthly Sales&nbsp;Trend</h3><pre>monthly_sales = sales.groupby(pd.Grouper(key=\"ORDERDATE\", freq=\"M\"))['SALES'].sum()<p>plt.figure(figsize=(10,5))</p>monthly_sales.plot()<p>plt.title(\"Monthly Sales Trend\")</p>plt.xlabel(\"Month\")<p>plt.ylabel(\"Total Sales\")</p>plt.show()</pre><p>This gives us a clear view of how sales are moving over&nbsp;time.</p><h3>üíπ Step 4: Fetch Stock Market&nbsp;Data</h3><p>Let‚Äôs pick  for our analysis.</p><pre>start_date = sales['ORDERDATE'].min().strftime(\"%Y-%m-%d\")end_date = sales['ORDERDATE'].max().strftime(\"%Y-%m-%d\")<p>stock = yf.download(\"AAPL\", start=start_date, end=end_date)</p>stock['Close'].plot(figsize=(10,5), title=\"Apple Stock Price\")</pre><p><strong>üîÑ Step 5: Merge Sales &amp; Stock&nbsp;Data</strong></p><pre># Monthly Salesmonthly_sales = sales.groupby(pd.Grouper(key=\"ORDERDATE\", freq=\"M\"))['SALES'].sum()stock_monthly = stock['Close'].resample('M').mean()df = pd.concat([monthly_sales, stock_monthly], axis=1)<p>df.columns = [\"Sales\", \"StockPrice\"]</p><p>print(df.dropna().head())   # Check after removing NaN</p></pre><p>Now we have both <strong>Sales and Stock Price in one DataFrame</strong>.</p><p><strong>üìä Step 6: Sales vs Stock Price Scatterplot</strong></p><pre>sns.scatterplot(x=\"Sales\", y=\"StockPrice\", data=df.dropna())plt.title(\"Sales vs Stock Price\")</pre><p>üìå : The scatterplot shows a . Higher sales are often paired with higher stock prices. But it‚Äôs not perfectly linear‚Ää‚Äî‚Ääthere are plenty of scattered points.</p><p><strong>üìà Step 7: Add Regression Trendline</strong></p><pre>sns.regplot(x=\"Sales\", y=\"StockPrice\", data=df, scatter_kws={\"s\":50}, line_kws={\"color\":\"red\"})plt.title(\"Sales vs Stock Price with Trendline\")</pre><ul><li>The  shows a positive correlation ‚Üí as sales increase, stock prices tend to&nbsp;rise.</li><li>The  is the confidence interval ‚Üí wide bands mean stock price is influenced by other factors beyond sales (investor sentiment, economic conditions, etc.).</li><li>Some  show months where strong sales did not immediately push stock prices‚Ää‚Äî‚Ääreminding us that the market is not just logical, it‚Äôs psychological too.</li></ul><h3>üìå Step 8: Correlation Check</h3><p>‚úÖ <strong>Insight from Correlation Output</strong></p><ul><li>The correlation coefficient between  and  is&nbsp;.</li><li>This means there is a <strong>weak positive relationship</strong> between the two variables. In simple&nbsp;terms:</li><li>As , <strong>Stock Prices also tend to increase</strong>, but not strongly.</li><li>Other external factors (market sentiment, competition, economy, company news) are likely influencing stock price much more than just&nbsp;sales.</li><li>For investors: relying only on sales to predict stock price is risky‚Ää‚Äî‚Ääit‚Äôs one piece of the puzzle, not the whole&nbsp;picture.</li><li>For businesses: increasing sales  proportional stock price growth; branding, profitability, and investor confidence also&nbsp;matter.</li></ul><blockquote>‚ÄúFrom the correlation matrix, we see that Sales and Stock Price have a . This suggests a <strong>weak positive relationship</strong>‚Ää‚Äî‚Äämeaning higher sales may push stock prices up slightly, but the effect is not very strong. Clearly, stock prices depend on multiple factors beyond just sales numbers, such as market conditions, company reputation, and investor sentiment.‚Äù</blockquote><h3>üöÄ Why This Matters for&nbsp;You</h3><ul><li>If you‚Äôre a  ‚Üí this project strengthens your portfolio (business + finance&nbsp;domain).</li><li>If you‚Äôre an  ‚Üí remember that sales performance is one of many factors that move stock&nbsp;prices.</li><li>If you‚Äôre just learning  ‚Üí this project teaches you how to merge and analyze cross-domain datasets.</li></ul><p>This experiment shows that while <strong>sales data does impact stock performance</strong>, it‚Äôs not the . The stock market is a mix of fundamentals, sentiment, and sometimes pure unpredictability.</p><p>üëâ Want to go deeper? Extend this project&nbsp;into:</p><ul><li><strong>Forecasting sales impact on stock (ARIMA, LSTM&nbsp;models)</strong></li><li><strong>Comparing multiple companies</strong></li><li><strong>Building dashboards (Power BI, Tableau)</strong> for interactive visualization</li></ul><p>üìå The next time someone says ‚Äúgood sales mean good stock returns,‚Äù you‚Äôll know the data-backed truth: </p><p>‚úÖ That‚Äôs it! You now have a complete  with <strong>dataset, code, plots, insights, and storytelling</strong>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2b0d54d711f0\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4801,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Blue-Green Deployments: A Practical Path to Zero-Downtime Releases","url":"https://blog.devops.dev/blue-green-deployments-a-practical-path-to-zero-downtime-releases-54c83fe0265c?source=rss----33f8b2d9a328---4","date":1755876884,"author":"Madhura Jayashanka","guid":237007,"unread":true,"content":"<p>Downtime is costly. According to <a href=\"https://itic-corp.com/itic-2024-hourly-cost-of-downtime-report/\">ITIC‚Äôs 2024 report</a>, 90% of companies say one hour of downtime costs over , and 41% say it can reach  (<a href=\"https://www.calyptix.com/wp-content/uploads/Hourly-Cost-of-Downtime-ITIC.pdf\">PDF&nbsp;source</a>).</p><p><a href=\"https://dora.dev/research/2024/dora-report/\">DORA‚Äôs research</a> also shows that high-performing teams deliver quickly  stay stable. Earlier benchmarks from <a href=\"https://dora.dev/research/2019/dora-report/2019-dora-accelerate-state-of-devops-report.pdf\">2019</a> and <a href=\"https://dora.dev/research/2021/dora-report/2021-dora-accelerate-state-of-devops-report.pdf\">2021</a> defined elite teams&nbsp;as:</p><ol><li>Change failure rate between&nbsp;</li><li>Time to restore service </li></ol><p>Blue-green deployments are one way to hit these&nbsp;goals.</p><h3>What is Blue-Green Deployment?</h3><p>It means running <strong>two production-ready environments</strong>:</p><ul><li>‚Ää‚Äî‚Ääthe version currently serving&nbsp;users</li><li>‚Ää‚Äî‚Ääthe new version waiting to go&nbsp;live</li></ul><p>The process looks like&nbsp;this:</p><ol><li>Deploy your update to&nbsp;</li><li>Run tests and health&nbsp;checks</li><li>Switch traffic from  to&nbsp;</li><li>If something goes wrong, switch back to  instantly</li></ol><ul><li>Zero downtime during&nbsp;cutover</li><li>Instant rollback if problems&nbsp;appear</li><li>Realistic testing under production conditions</li></ul><ul><li>Temporary extra infrastructure capacity</li><li>Need to keep configuration and secrets in&nbsp;sync</li></ul><ul><li>Uptime is part of your contract or&nbsp;SLA</li><li>Releases are risky (payments, authentication, APIs)</li><li>You want a quick, binary safety&nbsp;switch</li></ul><ul><li>‚Ää‚Äî‚Äägood for gradual rollout with percentage-based traffic</li><li>‚Ää‚Äî‚Äägood if you can accept slower&nbsp;rollback</li></ul><p>There are three common ways to flip traffic between Blue and&nbsp;Green:</p><p><strong>1. Kubernetes Service&nbsp;selector</strong></p><ul><li>Switch label selector from color=blue to color=green</li></ul><p><strong>2. AWS ECS with ALB/CodeDeploy</strong></p><ul><li>Two target groups (Blue and Green) behind an&nbsp;ALB</li><li>CodeDeploy shifts traffic and can auto-rollback</li></ul><ul><li>Define Blue and Green upstreams</li><li>A map variable chooses which backend is&nbsp;active</li><li>nginx -s reload applies the&nbsp;change</li></ul><h3>Example: Kubernetes Blue-Green</h3><pre>apiVersion: v1kind: Service  name: demo-svc  selector:    color: blue   # flip to green to switch traffic    - port: 80</pre><pre>apiVersion: apps/v1kind: Deployment  name: demo-blue  replicas: 3    matchLabels: { app: demo, color: blue }    metadata:<p>      labels: { app: demo, color: blue }</p>    spec:        - name: app<p>          image: ghcr.io/yourorg/demo:1.0.0</p>          ports: [{ containerPort: 8080 }]            httpGet: { path: /healthz, port: 8080 }</pre><p> is identical but with color: green and a different image&nbsp;tag.</p><pre>kubectl patch svc demo-svc -p '{\"spec\":{\"selector\":{\"app\":\"demo\",\"color\":\"green\"}}}'</pre><h3>Example: NGINX Blue-Green</h3><pre>upstream blue_backend  { server 10.0.0.11:8080; }upstream green_backend { server 10.0.0.12:8080; }</pre><pre>map $http_host $active_color { default blue; }  # set to green to flip</pre><p>Change $active_color to green, then reload&nbsp;NGINX.</p><p>Databases need a special strategy. The best approach is the  pattern (<a href=\"https://martinfowler.com/bliki/ParallelChange.html\">source</a>):</p><ul><li>Expand schema safely (add new columns or&nbsp;tables)</li><li>Deploy new code (works with both old and new&nbsp;schema)</li></ul><h3>Health Checks Before Switching</h3><ul><li>Application readiness probe must pass consistently</li><li>Error rates and latency should match the&nbsp;baseline</li><li>Watch CPU, memory, and a key business KPI during the&nbsp;switch</li><li>Keep Blue running in the background for 30‚Äì120 minutes in case rollback is&nbsp;needed</li></ul><ul><li>‚Ää‚Äî‚Ääpatch Service selector back to&nbsp;Blue</li><li>‚Ää‚Äî‚Äächange $active_color back to Blue and&nbsp;reload</li><li>‚Ää‚Äî‚Äätrigger rollback in CodeDeploy (<a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html\">docs</a>)</li></ul><p><strong>Is blue-green the same as canary?</strong> No. Blue-green switches all traffic at once. Canary shifts traffic gradually.</p><p><strong>Does blue-green guarantee zero downtime?</strong> It aims to, as long as health probes and load balancer configs are set correctly.</p><p><strong>What is ‚Äúgood‚Äù stability?</strong> According to <a href=\"https://dora.dev/research/2024/dora-report/\">DORA</a>, good teams keep change failure low and can restore service within about an&nbsp;hour.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=54c83fe0265c\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Truth About Cold Starts in Google Cloud Run & Functions","url":"https://blog.devops.dev/the-truth-about-cold-starts-in-google-cloud-run-functions-efb1c5bccfda?source=rss----33f8b2d9a328---4","date":1755876877,"author":"Engineer","guid":237006,"unread":true,"content":"<div><p>If you‚Äôve been using Google Cloud Run or Cloud Functions, chances are you‚Äôve noticed some requests feel like they‚Äôre waking up from a nap‚Ä¶</p></div>","contentLength":146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cloudflare WAF Best Practices: Features, Challenges, and Alternatives","url":"https://blog.devops.dev/cloudflare-waf-best-practices-features-challenges-and-alternatives-46d7da078f6e?source=rss----33f8b2d9a328---4","date":1755876862,"author":"Maverick Steel","guid":237005,"unread":true,"content":"<p>Web Application Firewalls (WAFs) play a critical role in securing modern web applications from a wide range of threats, such as SQL injection, cross-site scripting (XSS), and other OWASP Top 10 vulnerabilities.</p><p>Among various WAF solutions available today, Cloudflare WAF is one of the most widely adopted due to its robust features, ease of use, and global performance benefits.</p><p>This article explores the best practices for using Cloudflare WAF, highlights some challenges, and introduces an alternative WAF solution‚Ää‚Äî‚ÄäSafeline.</p><h3>Features of Cloudflare WAF</h3><p>Cloudflare WAF includes a broad set of pre-configured security rules that cover OWASP Top 10 vulnerabilities, zero-day exploits, and protocol anomalies. These rules are frequently updated to respond to emerging&nbsp;threats.</p><p>Cloudflare WAF can automatically learn from traffic patterns to reduce false positives, minimizing disruptions to legitimate users while enhancing security.</p><p>Being part of Cloudflare‚Äôs global Content Delivery Network (CDN) gives the WAF low latency and high availability, ensuring security without sacrificing performance.</p><p>Users can create custom firewall rules tailored to their application‚Äôs specific needs using Cloudflare‚Äôs intuitive dashboard or&nbsp;API.</p><p>Cloudflare WAF integrates with bot management features to distinguish between good and bad bots, reducing malicious traffic and improving analytics.</p><h3>Best Practices for Using Cloudflare WAF</h3><ul><li><strong>Regularly Update Rule Sets:</strong> Keep Cloudflare‚Äôs managed rules enabled and updated to protect against newly discovered vulnerabilities.</li><li><strong>Enable Logging and Monitor Alerts:</strong> Continuously monitor WAF logs and alerts to quickly respond to potential threats or false positives.</li><li><strong>Use Custom Rules Judiciously:</strong> While custom rules offer flexibility, improper configuration can cause unintended blocks or gaps; test thoroughly.</li><li><strong>Integrate with Security Ecosystem:</strong> Combine Cloudflare WAF with other security tools like SIEMs for comprehensive threat detection and response.</li><li> Start with learning mode to understand traffic patterns and adjust policies for smoother deployment.</li></ul><h3>Challenges with Cloudflare WAF</h3><ul><li> Despite managed rules and learning modes, some false positives still occur, potentially impacting user experience.</li><li> Advanced features and higher rule set limits are available only on paid plans, which might be expensive for small businesses.</li><li><strong>Limited Deep Customization:</strong> Although flexible, Cloudflare‚Äôs WAF may not meet very niche or complex application requirements compared to specialized WAFs.</li><li><strong>Dependence on Cloudflare Network:</strong> Using Cloudflare WAF means routing all traffic through Cloudflare‚Äôs network, which can be a concern for some organizations regarding data sovereignty or vendor&nbsp;lock-in.</li></ul><h3>Alternatives: SafeLine&nbsp;WAF</h3><p>SafeLine(<a href=\"https://ly.safepoint.cloud/k9fyEuu\">https://ly.safepoint.cloud/k9fyEuu</a>) is an emerging WAF solution that offers a compelling alternative to Cloudflare WAF, especially for enterprises seeking deeper customization and more direct control over deployment environments.</p><ul><li><strong>Highly Customizable Rules Engine:</strong> SafeLine provides a powerful rules engine that allows fine-grained rule creation, enabling tailored protection for unique application requirements.</li><li><strong>On-Premises and Cloud Deployment:</strong> Unlike Cloudflare, Safeline supports hybrid deployments‚Ää‚Äî‚Ääon-premises, cloud, or edge‚Ää‚Äî‚Ääto meet diverse organizational policies and compliance needs.</li><li><strong>Threat Intelligence Integration:</strong> SafeLine integrates with a threat intelligence feed, enabling proactive detection of zero-day vulnerabilities and sophisticated attacks.</li><li><strong>Performance Optimization:</strong> Designed with minimal latency in mind, SafeLine optimizes traffic without relying on a global CDN&nbsp;network.</li><li> SafeLine offers advanced bot detection capabilities that can differentiate human users from automated attacks beyond signature-based methods.</li></ul><h3>When to Consider&nbsp;SafeLine</h3><p>If your organization requires on-premises control, extensive rule customization, or has concerns related to data residency and vendor dependency, SafeLine presents a flexible and powerful alternative to Cloudflare WAF.</p><p>Cloudflare WAF is a robust and widely-used solution that offers strong security features combined with the performance benefits of a global CDN. Following best practices such as updating rules and monitoring logs helps maximize its effectiveness. However, challenges like false positives and vendor dependency warrant consideration of alternatives. Safeline stands out as a strong option for businesses needing customizable, flexible deployment options alongside comprehensive web application protection.</p><p>Choosing the right WAF depends on your specific requirements regarding performance, deployment, customization, and cost. Both Cloudflare WAF and Safeline provide valuable tools to safeguard your web applications in today‚Äôs evolving threat landscape.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=46d7da078f6e\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Red Flags in K8s Configuration That Kills Your Apps","url":"https://blog.devops.dev/red-flags-in-k8s-configuration-that-kills-your-apps-cb738d435989?source=rss----33f8b2d9a328---4","date":1755876815,"author":"Devops Diaries","guid":237004,"unread":true,"content":"<div><p>Deploying apps on Kubernetes feels smooth‚Ää‚Äî‚Ääuntil something breaks in production. More often than not, the culprit isn‚Äôt Kubernetes itself‚Ä¶</p></div>","contentLength":149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI as a Co-pilot for Developers: Boosting Productivity Without Losing Control","url":"https://blog.devops.dev/ai-as-a-co-pilot-for-developers-boosting-productivity-without-losing-control-d5abb9e73419?source=rss----33f8b2d9a328---4","date":1755876812,"author":"Vitor Hansen","guid":237003,"unread":true,"content":"<p>Artificial Intelligence (AI) is no longer just a buzzword in the tech industry, it‚Äôs becoming an essential companion for developers. From generating code snippets to reviewing pull requests and even optimizing CI/CD pipelines, AI is changing the way software is&nbsp;built.</p><p>In this article, we‚Äôll explore what it means to have an ‚ÄúAI co-pilot‚Äù in your development workflow, compare the most popular tools (ChatGPT, Claude, Gemini, and Codeium), and walk through a practical example of using AI to validate code in a CI/CD pipeline.</p><p>An AI co-pilot is a system that assists developers in writing, reviewing, optimizing, and understanding code‚Ää‚Äî‚Ääwithout replacing the human in charge. Think of it as a <strong>pair programming partner that never gets tired</strong>, always has instant suggestions, and can adapt to different coding&nbsp;styles.</p><p>AI co-pilots come in two main&nbsp;forms:</p><ul><li><strong>Context-aware code assistants</strong>‚Ää‚Äî‚ÄäIntegrated into IDEs (VS Code, JetBrains, etc.) to provide inline suggestions and documentation.</li><li><strong>Conversational AI assistants</strong>‚Ää‚Äî‚ÄäChat interfaces where you can paste code, ask for explanations, or request specific implementations.</li></ul><p>The best co-pilots blend both approaches, letting you <strong>move between coding, reviewing, and designing</strong> without breaking&nbsp;flow.</p><h3>2. Core Use Cases for Developers</h3><p>AI co-pilots can be applied across the entire development lifecycle. Here are some of the most common scenarios:</p><ul><li>Creating boilerplate code&nbsp;quickly.</li><li>Suggesting implementation details based on comments.</li><li>Generating functions in multiple languages.</li></ul><ul><li>Highlighting potential bugs and security&nbsp;risks.</li><li>Suggesting better algorithms or data structures.</li><li>Pointing out unused imports or redundant code.</li></ul><ul><li>Explaining cryptic error messages.</li><li>Suggesting possible fixes for runtime&nbsp;issues.</li><li>Reproducing bugs based on&nbsp;logs.</li></ul><ul><li>Improving performance by refactoring loops, queries, or memory&nbsp;usage.</li><li>Suggesting more efficient libraries or&nbsp;APIs.</li><li>Reducing complexity for maintainability.</li></ul><ul><li>Generating docstrings and README&nbsp;files.</li><li>Creating developer onboarding guides from existing&nbsp;code.</li><li>Explaining APIs for both internal and public&nbsp;use.</li></ul><h3>3. Hands-On Example: AI in a CI/CD&nbsp;Pipeline</h3><p>Let‚Äôs see how an AI co-pilot can help in a <strong>Continuous Integration / Continuous Deployment</strong> setup.</p><p> You want every pull request in your repository to be automatically reviewed&nbsp;for:</p><ul></ul><p>Instead of manually going through every file, you integrate an AI service in your CI pipeline.</p><p><strong>Example with GitHub Actions + OpenAI&nbsp;API</strong>:</p><pre>name: AI Code Review  pull_request:  ai-review:    steps:<p>      - name: Checkout repository</p>        uses: actions/checkout@v3<p>      - name: Install dependencies</p>        run: npm install<p>      - name: Run AI Code Review</p>        env:<p>          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}</p>        run: |<p>          node scripts/ai-review.js</p></pre><pre>import OpenAI from \"openai\";import fs from \"fs\";<p>const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });</p><p>const files = [\"src/index.js\", \"src/utils/helpers.js\"];</p>  for (const file of files) {<p>    const code = fs.readFileSync(file, \"utf8\");</p>    const prompt = `<p>      Review the following JavaScript code for:</p>      1. Potential security issues<p>      2. Code smells or anti-patterns</p>      3. Missing test coverage<p>      Provide feedback in a concise bullet-point list.</p>      ${code}    const response = await openai.chat.completions.create({      messages: [{ role: \"user\", content: prompt }],    console.log(`Review for ${file}:`);<p>    console.log(response.choices[0].message.content);</p>  }</pre><p>This simple setup can automatically leave AI-generated feedback as part of the PR process, helping developers catch issues&nbsp;earlier.</p><h3>5. Challenges and Limitations</h3><p>AI co-pilots are powerful, but they‚Äôre not magic. Key challenges include:</p><ul><li>‚Ää‚Äî‚ÄäCode sent to third-party APIs might expose sensitive info.</li><li>‚Ää‚Äî‚ÄäAI suggestions can be wrong or insecure.</li><li>‚Ää‚Äî‚ÄäLarge codebases may exceed prompt size&nbsp;limits.</li><li>‚Ää‚Äî‚ÄäHeavy usage can increase expenses.</li><li>‚Ää‚Äî‚ÄäRelying too much on AI may reduce skill retention.</li></ul><h3>6. Best Practices for Using AI in Development</h3><ol><li>‚Ää‚Äî‚ÄäNever merge AI-generated code without&nbsp;review.</li><li><strong>Keep Sensitive Data Local</strong>‚Ää‚Äî‚ÄäUse self-hosted models for confidential projects.</li><li>‚Ää‚Äî‚ÄäThe better your instructions, the better the&nbsp;output.</li><li>‚Ää‚Äî‚ÄäLet AI suggest, but tests&nbsp;confirm.</li><li>‚Ää‚Äî‚ÄäUse PR labels to flag AI-assisted commits.</li></ol><h3>7. The Future of AI Co-pilots</h3><p>We‚Äôre moving towards a reality where AI will handle more repetitive coding tasks, freeing developers to focus on problem-solving and architecture. The winning strategy isn‚Äôt to fight AI, it‚Äôs to .</p><p>The best developers of tomorrow will be those who can <strong>combine human creativity with AI efficiency</strong>.</p><p> AI isn‚Äôt here to replace developers, it‚Äôs here to make us . By understanding its strengths, limits, and best practices, you can turn AI into a genuine co-pilot that helps you ship faster, with higher quality, and less&nbsp;stress.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d5abb9e73419\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Your Application Resiliently with AWS: From Zero to Millions of Requests","url":"https://blog.devops.dev/scaling-your-application-resiliently-with-aws-from-zero-to-millions-of-requests-611648b92eb5?source=rss----33f8b2d9a328---4","date":1755876805,"author":"Vitor Hansen","guid":237002,"unread":true,"content":"<h3>A strategic and technical guide for architects and engineers aiming to build modern, resilient systems designed for&nbsp;growth.</h3><p>Scalability is a critical challenge every successful software system faces. Too often, teams only address scalability issues after significant growth, leading to downtime, poor user experiences, and costly refactoring. I‚Äôve witnessed first-hand systems crashing under load due to inadequate architecture planning. This post emphasizes the importance of building scalability and resilience from the ground&nbsp;up.</p><h3>2. When (and Why) to&nbsp;Scale?</h3><p>First, let‚Äôs clarify the distinction:  is about how fast your system handles individual requests, while  is about how well your system handles increasing volume.</p><p>You know it‚Äôs time to scale&nbsp;when:</p><ul><li>Response times are increasing</li><li>Servers are consistently maxed&nbsp;out</li><li>Monitoring tools like CloudWatch, X-Ray, or Sentry show consistent bottlenecks</li></ul><p>Scaling is not just a technical necessity, it‚Äôs a business enabler. Building for scale means your system can grow with your users, rather than crumble under their&nbsp;weight.</p><h3>3. Modern Scalable Architecture on&nbsp;AWS</h3><p>Today‚Äôs cloud architectures leverage key AWS components to build scalable and resilient systems:</p><ul><li> Manages, secures, and routes API requests.</li><li> Serverless compute, automatically scaling and handling workloads.</li><li> Managed container orchestration that removes infrastructure management overhead.</li><li> Highly available relational databases.</li><li> NoSQL database offering high performance and scalability.</li><li> Queue management ensuring reliable asynchronous processing.</li><li> Event-driven communication between services.</li><li> Improves system performance by caching frequently accessed&nbsp;data.</li></ul><p>This setup ensures both horizontal scalability and fault tolerance across all&nbsp;layers.</p><p>Consider a common scenario: an application receiving public API requests and processing financial transactions:</p><ul><li> Receives requests, applies rate limiting, and manages authentication.</li><li> Processes incoming requests, scales automatically based on&nbsp;demand.</li><li> Handles longer-running processes and workflows.</li><li> Stores transactional and rapidly accessed&nbsp;data.</li><li> Stores structured data requiring strong consistency.</li><li> Manages communication and workflows asynchronously.</li></ul><p>This architecture scales seamlessly, maintaining consistent performance and resilience even under heavy&nbsp;load.</p><p>Avoid these mistakes when designing scalable AWS&nbsp;systems:</p><ul><li><strong>Ignoring cold starts in Lambda:</strong> Minimize with smaller functions and Provisioned Concurrency.</li><li><strong>Poor scaling configuration:</strong> Test and validate your auto-scaling policies regularly.</li><li> Use DynamoDB or caching where eventual consistency is acceptable.</li><li> Avoid direct service-to-service calls without message buses like SQS/EventBridge.</li></ul><h3>6. Golden Tips for Resilience</h3><p>Enhance your system‚Äôs resilience with these best practices:</p><ul><li>Implement retries and Dead Letter Queues (DLQ) with Amazon&nbsp;SQS.</li><li>Apply the Circuit Breaker pattern with API Gateway and Lambda to prevent cascading failures.</li><li>Use Canary Deployments for safe, gradual rollouts of Lambda functions and ECS services.</li></ul><p>Scalability isn‚Äôt just about adding infrastructure; it‚Äôs primarily about thoughtful architecture design. Building scalable systems from the start ensures your business can adapt quickly to growth without sacrificing reliability or performance.</p><p> AWS, Architecture, Scalability, Serverless, Fargate, Lambda, Best Practices, Cloud</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=611648b92eb5\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Set up GitHub Actions for a Node.js project","url":"https://blog.devops.dev/how-to-setup-github-actions-for-node-js-project-1edd6ce1dbe1?source=rss----33f8b2d9a328---4","date":1755876771,"author":"Mohammad Faisal Khatri","guid":237001,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tackling the DevSecOps Gap in Software Understanding","url":"https://devops.com/tackling-the-devsecops-gap-in-software-understanding/?utm_source=rss&utm_medium=rss&utm_campaign=tackling-the-devsecops-gap-in-software-understanding","date":1755830722,"author":"Alan Shimel","guid":236020,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff: Episode 67 (Kserve)","url":"https://www.youtube.com/watch?v=BjXZxUR8NMo","date":1755808425,"author":"CNCF [Cloud Native Computing Foundation]","guid":235914,"unread":true,"content":"<article>Join us LIVE as CNCF Ambassador Shivay Lamaba dives into KServe, the open source project designed for scalable and reliable ML models serving on Kubernetes.\n\nShivay will be  exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that make KServe a key piece in the cloud native AI/ML ecosystem.\nIf you‚Äôre curious about production-grade model inference, want to see how cloud native communities approach machine learning workloads, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.\n\nBring your questions, share your experiences, and learn in real time as we explore KServe together!</article>","contentLength":751,"flags":null,"enclosureUrl":"https://www.youtube.com/v/BjXZxUR8NMo?version=3","enclosureMime":"","commentsUrl":null},{"title":"White Paper: The Future of DevSecOps in a Fully Autonomous CI/CD Pipeline","url":"https://devops.com/white-paper-the-future-of-devsecops-in-a-fully-autonomous-ci-cd-pipeline/?utm_source=rss&utm_medium=rss&utm_campaign=white-paper-the-future-of-devsecops-in-a-fully-autonomous-ci-cd-pipeline","date":1755796325,"author":"Ravi Shanker Sharma","guid":235800,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MCP Emerges as a Catalyst for Modern DevOps Processes","url":"https://devops.com/mcp-emerges-as-a-catalyst-for-modern-devops-processes/?utm_source=rss&utm_medium=rss&utm_campaign=mcp-emerges-as-a-catalyst-for-modern-devops-processes","date":1755795181,"author":"Mike Vizard","guid":235799,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How AI-Created Code Will Strain DevOps Workflows","url":"https://devops.com/how-ai-created-code-will-strain-devops-workflows/?utm_source=rss&utm_medium=rss&utm_campaign=how-ai-created-code-will-strain-devops-workflows","date":1755793804,"author":"Mike Vizard","guid":235798,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prototyping an AI Tutor with Docker Model Runner","url":"https://www.docker.com/blog/how-to-build-an-ai-tutor-with-model-runner/","date":1755788400,"author":"Sarah Sanders","guid":235826,"unread":true,"content":"<p>Every developer remembers their first docker run hello-world. The mix of excitement and wonder as that simple command pulls an image, creates a container, and displays a friendly message. But what if AI could make that experience even better?</p><p>As a technical writer on Docker‚Äôs Docs team, I spend my days thinking about developer experience. Recently, I‚Äôve been exploring how AI can enhance the way developers learn new tools. Instead of juggling documentation tabs and ChatGPT windows, what if we could embed AI assistance directly into the learning flow? This led me to build <strong>an interactive AI tutor powered by </strong><a href=\"https://www.docker.com/products/model-runner/\"></a> as a proof of concept.</p><h2>The Case for Embedded AI Tutors</h2><p>The landscape of developer education is shifting. While documentation remains essential, we are seeing more developers coding alongside AI assistants. But context-switching between your terminal, documentation, and an external AI chat breaks concentration and flow. An embedded AI tutor changes this dynamic completely.</p><p>Imagine learning Docker with an AI assistant that:</p><ul><li>Lives alongside your development environment</li><li>Maintains context about what you‚Äôre trying to achieve</li><li>Responds quickly without network latency</li><li>Keeps your code and questions completely private</li></ul><p>This isn‚Äôt about replacing documentation. It‚Äôs about offering developers a choice in how they learn. Some prefer reading guides, others learn by doing, and increasingly, many want conversational guidance through complex tasks.</p><p>To build the AI tutor, I kept the architecture rather simple:</p><ul><li>The is a React app with a chat interface. Nothing fancy, just a message history, input field, and loading states.</li><li>The is an /api/chat endpoint that forwards requests to the local LLM through OpenAI-compatible APIs.</li><li>The powering it all is where Docker Model Runner comes in. Docker Model Runner <a href=\"https://www.docker.com/blog/run-llms-locally/\">runs models locally on your machine</a>, exposing models through OpenAI endpoints. I decided to use Docker Model Runner because it promised local development and fast iteration.</li><li>The  was designed with running docker run hello-world in mind:</li></ul><div><pre>You are a Docker tutor with ONE SPECIFIC JOB: helping users run their first \"hello-world\" container.\n\nYOUR ONLY TASK: Guide users through these exact steps:\n1. Check if Docker is installed: docker --version\n2. Run their first container: docker run hello-world\n3. Celebrate their success\n\nSTRICT BOUNDARIES:\n- If a user says they already know Docker: Respond with an iteration of \"I'm specifically designed to help beginners run their first container. For advanced help, please review Docker documentation at docs.docker.com or use Ask Gordon.\"\n- If a user asks about Dockerfiles, docker-compose, or ANY other topic: Respond with \"I only help with running your first hello-world container. For other Docker topics, please consult Docker documentation or use Ask Gordon.\"\n- If a user says they've already run hello-world: Respond with \"Great! You've completed what I'm designed to help with. For next steps, check out Docker's official tutorials at docs.docker.com.\"\n\nALLOWED RESPONSES:\n- Helping install Docker Desktop (provide official download link)\n- Troubleshooting \"docker --version\" command\n- Troubleshooting \"docker run hello-world\" command\n- Explaining what the hello-world output means\n- Celebrating their success\n\nCONVERSATION RULES:\n- Use short, simple messages (max 2-3 sentences)\n- One question at a time\n- Stay friendly but firm about your boundaries\n- If users persist with off-topic questions, politely repeat your purpose\n\nEXAMPLE BOUNDARY ENFORCEMENT:\nUser: \"Help me debug my Dockerfile\"\nYou: \"I'm specifically designed to help beginners run their first hello-world container. For Dockerfile help, please check Docker's documentation or Ask Gordon.\"\n\nStart by asking: \"Hi! I'm your Docker tutor. Is this your first time using Docker?\"\n\n</pre></div><h2>Setting Up Docker Model Runner</h2><p>Getting started with Docker Model Runner proved straightforward. With just a toggle in Docker Desktop‚Äôs settings and TCP support enabled, my local React app connected seamlessly. The setup delivered on Docker Model Runner‚Äôs promise of simplicity.</p><p>During initial testing, the model performed well. I could interact with it through the OpenAI-compatible endpoint, and my React frontend connected without requiring modifications or fine-tuning. I had my prototype up and running in no time.</p><p>To properly evaluate the AI tutor, I approached it from two paths. First, I followed the ‚Äúhappy path‚Äù by interacting as a novice developer might. When I mentioned it was my ‚Äúfirst time‚Äù using Docker, the tutor responded appropriately to my prompts. It walked me through checking if Docker was installed using my terminal before running my container.&nbsp;</p><p>Next, I ventured down the ‚Äúunhappy path‚Äù to test the tutor‚Äôs boundaries. Acting as an experienced developer, I attempted to push beyond basic container operations. The AI tutor maintained its focus and stayed within its designated scope.</p><p>This strict adherence to guidelines wasn‚Äôt about following best practices, but rather about meeting my specific use case. I needed to prototype an AI tutor with clear guardrails that served a single, well-defined purpose. This approach worked for my prototype, but future iterations may expand to cover multiple topics or complement <a href=\"https://docs.docker.com/guides/\" rel=\"nofollow noopener\" target=\"_blank\">specific Docker use-case guides</a>.</p><h2>Reflections on Docker Model Runner</h2><p>Docker Model Runner delivered on its core promise: making AI models accessible through familiar Docker workflows. The vision of models as first-class citizens in the Docker ecosystem proved valuable for rapid local prototyping. The <a href=\"https://docs.docker.com/desktop/release-notes/\" rel=\"nofollow noopener\" target=\"_blank\">recent Docker Desktop releases</a> have brought continuous improvements to Docker Model Runner, including better management commands and expanded API support.</p><p>What worked really well for me:</p><ul><li>Native integration with Docker Desktop, a tool I use all day, every day</li><li>OpenAI-compatible APIs that require no frontend modifications</li><li>GPU acceleration support for faster local inference</li></ul><p>More than anything, simplicity is its standout feature. Within minutes, I had a local LLM running and responding to my React app‚Äôs API calls. The speed from idea to working prototype is exactly what developers need when experimenting with AI tools.</p><p>This prototype proved that embedded AI tutors aren‚Äôt just an idea, they‚Äôre a practical learning tool. Docker Model Runner provided the foundation I needed to test whether contextual AI assistance could enhance developer learning.</p><p>For anyone curious about Docker Model Runner:</p><ul><li>The tool is mature enough for meaningful experiments, and the setup overhead is minimal.</li><li>A basic React frontend and straightforward system prompt were sufficient to validate the concept.</li><li>Running models locally eliminates latency concerns and keeps developer data private.</li></ul><p>Docker Model Runner represents an important step toward making AI models as easy to use as containers. While my journey had some bumps, the destination was worth it: an AI tutor that helps developers learn.</p><p>As I continue to explore the intersection of documentation, developer experience, and AI, Docker Model Runner will remain in my toolkit. The ability to spin up a local model as easily as running a container opens up possibilities for intelligent, responsive developer tools. The future of developer experience might just be a docker model run away.</p><p>The Docker team wants to hear about your experience with Docker Model Runner. Share what‚Äôs working, what isn‚Äôt, and what features you‚Äôd like to see. Your input directly shapes the future of Docker‚Äôs AI products and features. <a href=\"https://docker.qualtrics.com/jfe/form/SV_6AaVyp25bygjirY?DD_Version=4.42.1&amp;Username=sarahsanders454&amp;Mode=local&amp;Tab=Container%2BOverview&amp;OS=Mac\" rel=\"nofollow noopener\" target=\"_blank\">Share feedback with Docker</a>.</p>","contentLength":7530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enterprise AI Development Gets a Major Upgrade: Claude Code Now Bundled with Team and Enterprise Plans","url":"https://devops.com/enterprise-ai-development-gets-a-major-upgrade-claude-code-now-bundled-with-team-and-enterprise-plans/?utm_source=rss&utm_medium=rss&utm_campaign=enterprise-ai-development-gets-a-major-upgrade-claude-code-now-bundled-with-team-and-enterprise-plans","date":1755788099,"author":"Tom Smith","guid":235730,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HoundDog.ai Code Scanner Shifts Data Privacy Responsibility Left","url":"https://devops.com/hounddog-ai-code-scanner-shifts-data-privacy-responsibility-left/?utm_source=rss&utm_medium=rss&utm_campaign=hounddog-ai-code-scanner-shifts-data-privacy-responsibility-left","date":1755784840,"author":"Mike Vizard","guid":235695,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Futurum Signal is Live: Cutting Through the DevOps Noise","url":"https://devops.com/futurum-signal-is-live-cutting-through-the-devops-noise/?utm_source=rss&utm_medium=rss&utm_campaign=futurum-signal-is-live-cutting-through-the-devops-noise","date":1755709536,"author":"Alan Shimel","guid":234661,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-D, with Clayton Coleman and Rob Shaw","url":"http://sites.libsyn.com/419861/llm-d-with-clayton-coleman-and-rob-shaw","date":1755696240,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":234551,"unread":true,"content":"<p dir=\"ltr\">Guests are Clayton Coleman and Rob Shaw. Clayton is a Core contributor to Kubernetes, the containerized cluster manager, and founding architect for OpenShift, the open source platform as a service. Clayton helped launch the shift to cloud native applications and the platforms that enable them. At Google my mission is to make Kubernetes and GKE the best place to run workloads, especially accelerated AI/ML workloads, and especially especially very large model inference at scale with the inference gateway and llm-d. Rob Shaw is an Engineering Director at Redhat and is a contributor to the vLLM project.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week  Links from the interview ","contentLength":715,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD258.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"The Supply Chain Paradox: When ‚ÄúHardened‚Äù Images Become a Vendor Lock-in Trap","url":"https://www.docker.com/blog/hardened-container-images-security-vendor-lock-in/","date":1755695409,"author":"Michael Donovan","guid":234513,"unread":true,"content":"<p>The market for pre-hardened container images is experiencing explosive growth as security-conscious organizations pursue the ultimate efficiency: instant security with minimal operational overhead. The value proposition is undeniably compelling‚Äîhardened images with minimal dependencies promise security ‚Äúout of the box,‚Äù enabling teams to focus on building and shipping applications rather than constantly revisiting low-level configuration management.</p><p>For good reason, enterprises are adopting these pre-configured images to reduce attack surface area and simplify security operations. In theory, hardened images deliver reduced setup time, standardized security baselines, and streamlined compliance validation with significantly less manual intervention.</p><p>Yet beneath this attractive surface lies a fundamental contradiction. While hardened images can genuinely reduce certain categories of supply chain risk and strengthen security posture, they simultaneously create a more subtle form of vendor dependency than traditional licensing models. Organizations are unknowingly building critical operational dependencies on a single vendor‚Äôs design philosophy, build processes, institutional knowledge, responsiveness, and long-term market viability.</p><p>The paradox is striking: in the pursuit of supply chain independence, many organizations are inadvertently creating more concentrated dependencies and potentially weakening their security through stealth vendor lock-in that becomes apparent only when it‚Äôs costly to reverse.</p><h2><strong>The Mechanics of Modern Vendor Lock-in</strong></h2><h3><strong>Unfamiliar Base Systems Create Switching Friction</strong></h3><p>The first layer of lock-in emerges from architectural choices that seem benign during initial evaluation but become problematic at scale. Some hardened image vendors deviate from mainstream distributions, opting to bake their own Linux variants rather&nbsp; than offering widely-adopted options like Debian, Alpine, or Ubuntu. This deviation creates immediate friction for platform engineering teams who must develop vendor-specific expertise to effectively manage these systems. Even if the differences are small, this raises the spectre of edge-cases ‚Äì the bane of platform teams. Add enough edge cases and teams will start to fear adoption.</p><p>While vendors try to standardize their approach to hardening, in reality, it remains a bespoke process. This can create differences from image to image across different open source versions, up and down the stack ‚Äì even from the same vendor. In larger organizations, platform teams may need to offer hardened images from multiple vendors. This creates further compounding complexity. In the end, teams find themselves managing a heterogeneous environment that requires specialized knowledge across multiple proprietary approaches. This increases toil, adds risk, increases documentation requirements and raises the cost of staff turnover.</p><p><strong>Compatibility Barriers and Customization Constraints</strong></p><p>More problematic is how hardened images often break compatibility with standard tooling and monitoring systems that organizations have already invested in and optimized. Open source compatibility gaps emerge when hardened images introduce modifications that prevent seamless integration with established DevOps workflows, forcing organizations to either accept reduced functionality or invest in vendor-specific alternatives.</p><p>Security measures, while well-intentioned, can become so restrictive they prevent necessary business customizations. Configuration lockdown reaches levels where platform teams cannot implement organization-specific requirements without vendor consultation or approval, transforming what should be internal operational decisions into external dependencies.</p><p>Perhaps most disruptive is how hardened images force changes to established CI/CD pipelines and operational practices. Teams discover that their existing automation, deployment scripts, and monitoring configurations require substantial modification to accommodate the vendor‚Äôs approach to security hardening.</p><p>The vendor lock-in trap becomes most apparent when organizations attempt to change direction. While vendors excel at streamlining initial adoption‚Äîproviding migration tools, professional services, and comprehensive onboarding support‚Äîthey systematically downplay the complexity of eventual exit scenarios.</p><p>Organizations accumulate sunk costs through investments in training and vendor-specific tooling that create psychological and financial barriers to switching providers. More critically, expertise about these systems becomes concentrated within vendor organizations rather than distributed among internal teams. Platform engineers find themselves dependent on vendor documentation, support channels, and institutional knowledge to troubleshoot issues and implement changes.</p><h2><strong>The Open Source Transparency Problem</strong></h2><p>The hardened image industry leverages the credibility of open source. But it can also undermine the spirit of open source transparency by creating almost a kind of fork but without the benefits of community.. While vendors may provide source code access, this availability doesn‚Äôt guarantee system understanding or maintainability. The knowledge required to comprehend complex hardening processes often remains concentrated within small vendor teams, making independent verification and modification practically impossible.</p><p>Heavily modified images become difficult for internal teams to audit and troubleshoot. Platform engineers encounter systems that appear familiar on the surface but behave differently under stress or during incident response, creating operational blind spots that can compromise security during critical moments.</p><h3><strong>Trust and Verification Gaps</strong></h3><p>Transparency is only half the equation. Security doesn‚Äôt end at a vendor‚Äôs brand name or marketing claims. Hardened images are part of your production supply chain and should be scrutinized like any other critical dependency. Questions platform teams should ask include:</p><ul><li>How are vulnerabilities identified and disclosed? Is there a public, time-bound process, and is it tied to upstream commits and advisories rather than just public CVEs?</li><li>Could the hardening process itself introduce risks through untested modifications?</li><li>Have security claims been independently validated through audits, reproducible builds, or public attestations?</li><li>Does your SBOM meta-data accurately reflect the full context of your hardened image?&nbsp;</li></ul><p>Transparency plus verification and full disclosure builds durable trust. Without both, hardened images can be difficult to audit, slow to patch, and nearly impossible to replace. Not providing easy-to-understand and easy-to-consume verification artefacts and answers functions as a form of lock-in forcing the customer to trust but not allowing them to verify.</p><h2><strong>Building Independence: A Strategic Framework</strong></h2><p>For platform teams that want to benefit from the security gains of hardened images and reap ease of use while avoiding lock-in, taking a structured approach to hardened vendor decision making is critical.</p><h3><strong>Distribution Compatibility as Foundation</strong></h3><p>Platform engineering leaders must establish mainstream distribution adherence as a non-negotiable requirement. Hardened images should be built from widely-adopted distributions like Debian, Ubuntu, Alpine, or RHEL rather than vendor-specific variants that introduce unnecessary complexity and switching costs.</p><p>Equally important is preserving compatibility with standard package managers and maintaining adherence to the Filesystem Hierarchy Standard (FHS) to preserve tool compatibility and operational familiarity across teams. Key requirements include:</p><ul><li><strong>Package manager preservation</strong>: Compatibility with standard tools (apt, yum, apk) for independent software installation and updates&nbsp;</li><li><strong>File system layout standards</strong>: Adherence to FHS for seamless integration with existing tooling</li><li><strong>Library and dependency compatibility</strong>: No proprietary dependencies that create additional vendor lock-in</li></ul><h3><strong>Enabling Rapid Customization Without Security Compromise</strong></h3><p>Security enhancements should be architected as modular, configurable layers rather than baked-in modifications that resist change. This approach allows organizations to customize security posture while maintaining the underlying benefits of hardened configurations.</p><p>Built-in capability to modify security settings through standard configuration management tools preserves existing operational workflows and prevents the need for vendor-specific automation approaches. Critical capabilities include:</p><ul><li>: Security enhancements as removable, configurable components</li><li><strong>Configuration override mechanisms</strong>: Integration with standard tools (Ansible, Chef, Puppet)</li><li><strong>Whitelist-based customization</strong>: Approved modifications without vendor consultation</li><li>: Continuous verification that customizations don‚Äôt compromise security baselines</li></ul><h3><strong>Community Integration and Upstream Collaboration</strong></h3><p>Organizations should demand that hardened image vendors contribute security improvements back to original distribution maintainers. This requirement ensures that security enhancements benefit the broader community and aren‚Äôt held hostage by vendor business models.</p><p>Evaluating vendor participation in upstream security discussions, patch contributions, and vulnerability disclosure processes provides insight into their long-term commitment to community-driven security rather than proprietary advantage. Essential evaluation criteria include:</p><ul><li><strong>Upstream contribution requirements</strong>: Active contribution of security improvements to distribution maintainers</li><li><strong>True community engagement</strong>: Participation in security discussions and vulnerability disclosure processes</li><li>: Contractual requirements for backward and forward compatibility with official distributions</li></ul><h3><strong>Intelligent Migration Tooling and Transparency</strong></h3><p>AI-powered Dockerfile conversion capabilities should provide automated translation between vendor hardened images and standard distributions, handling complex multi-stage builds and dependency mappings without requiring manual intervention.</p><p>Migration tooling must accommodate practical deployment patterns including multi-service containers and legacy application constraints rather than forcing organizations to adopt idealized single-service architectures. Essential tooling requirements include:</p><ul><li><strong>Automated conversion capabilities</strong>: AI-powered translation between hardened images and standard distributions</li><li><strong>Transparent migration documentation</strong>: Open source tools that generate equivalent configurations for different providers</li><li>: Tools that work equally well for migrating to and away from hardened images</li><li><strong>Real-world architecture support</strong>: Accommodation of practical deployment patterns rather than forcing idealized architectures</li></ul><h3><strong>Practical Implementation Framework</strong></h3><p>Standardized compatibility testing protocols should verify that hardened images integrate seamlessly with existing toolchains, monitoring systems, and operational procedures before deployment at scale. Self-service customization interfaces for common modifications eliminate dependency on vendor support for routine operational tasks.</p><p>Advanced image merging capabilities allow organizations to combine hardened base images with custom application layers while maintaining security baselines, providing flexibility without compromising protection. Implementation requirements include:</p><ul><li><strong>Compatibility testing protocols</strong>: Standardized verification of integration with existing toolchains and monitoring systems</li><li><strong>Self-service customization:</strong>: User-friendly tools for common modifications (CA certificates, custom files, configuration overlays)</li><li><strong>Image merging capabilities</strong>: Advanced tooling for combining hardened bases with custom application layers</li><li>: Service level agreements for maintaining compatibility and providing migration support</li></ul><h2><strong>Conclusion: Security Without Surrendering Control</strong></h2><p>The real question platform teams must ask is this. Does my hardened image vendor strengthen or weaken my own control of my supply chain? The risks of lock-in aren‚Äôt theoretical. All of the factors described above can turn security into an unwanted operational constraint. Platform teams can demand hardened images and hardening process built for independence from the start‚Äî rooted in mainstream distributions, transparent in their build processes, modular in their security layers, supported by strong community involvement, and butressed by tooling that makes migration a choice, not a crisis.</p><p>When security leaders adopt hardened images that preserve compatibility, encourage upstream collaboration, and fit seamlessly into existing workflows, they protect more than just their containers. They protect their ability to adapt and they minimize lock-in while actually improving their security posture. The most secure organizations will be the ones that can harden without handcuffing themselves.</p>","contentLength":12809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Debugging in Production: Leveraging Logs, Metrics and Traces","url":"https://devops.com/debugging-in-production-leveraging-logs-metrics-and-traces/?utm_source=rss&utm_medium=rss&utm_campaign=debugging-in-production-leveraging-logs-metrics-and-traces","date":1755669617,"author":"Neel Shah","guid":234373,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing the end-of-support for the AWS SDK for .NET v3","url":"https://aws.amazon.com/blogs/devops/announcing-the-end-of-support-for-the-aws-sdk-for-net-v3/","date":1755640912,"author":"Ed McLaughlin","guid":233490,"unread":true,"content":"<p>We are announcing the end-of-support for the <a href=\"https://aws.amazon.com/sdk-for-net/\">AWS SDK for .NET</a> v3.x starting on March 1, 2026, in accordance with the <a href=\"https://docs.aws.amazon.com/sdkref/latest/guide/maint-policy.html\">SDK and Tools maintenance policy</a>.&nbsp; On April 28, 2025, the next major version of the AWS SDK for .NET, version 4.x, became generally available (<a href=\"https://aws.amazon.com/blogs/developer/general-availability-of-aws-sdk-for-net-v4-0/\">blog post</a>). Version 4.x of the SDK includes bug fixes, performance enhancements, and modernization for .NET. We strongly encourage you to <a href=\"https://docs.aws.amazon.com/sdk-for-net/v4/developer-guide/net-dg-v4.html\">upgrade</a> to take advantage of these enhancements.</p><p>Existing applications that use the AWS SDK for .NET v3.x will continue to function as intended unless there is a fundamental change to how an AWS service works. This is uncommon, and we will announce it broadly if it happens. Beginning March 1, 2026, the AWS SDK for .NET v3.x will only receive critical bug fixes and security updates, and we will not update it to support new AWS services, new service features, or changes to existing services.&nbsp;After June 1, 2026, when the AWS SDK for .NET v3.x reaches end-of-support, it will no longer receive updates or releases.</p><h2>End of Support Timeline for Version 3</h2><p>The timeline for end-of-support is as follows, as defined by the SDK major version lifecycle:</p><table border=\"1\"><tbody><tr></tr><tr><td width=\"292\">During this phase, the SDK is fully supported. AWS will provide regular SDK releases that include support for new services, API updates for existing services, as well as bug and security fixes.</td></tr><tr><td width=\"292\">During the maintenance mode, AWS limits SDK releases to address critical bug fixes and security issues only. An SDK will not receive API updates for new or existing services, or be updated to support new regions</td></tr><tr><td width=\"292\">When an SDK reaches end-of support, it will no longer receive updates or releases. Previously published releases will continue to be available via public package managers and the code will remain on GitHub. The GitHub repository may be archived.</td></tr></tbody></table><h2>References for AWS SDK for .NET Version 4</h2><p>Use the following references to learn more about the AWS SDK for .NET v4.x along with migration support.</p><p>We recommend you upgrade to the latest major version of the AWS SDK for .NET v4.x by using the&nbsp;<a href=\"https://docs.aws.amazon.com/sdk-for-net/v4/developer-guide/net-dg-v4.html\">migration guide</a>.&nbsp;This major version includes, but is not limited to, performance enhancements, bug fixes, modern .NET libraries and frameworks, and the latest AWS service updates. To learn more, please refer to the <a href=\"https://aws.amazon.com/blogs/developer/general-availability-of-aws-sdk-for-net-v4-0/\">AWS SDK for .NET GA blog post</a>.</p><p>If you need assistance or have feedback, reach out to your usual AWS support contacts. You can also open a <a href=\"https://github.com/aws/aws-sdk-net/discussions\">discussion</a> or <a href=\"https://github.com/aws/aws-sdk-net/issues\">issue</a> on GitHub. Thank you for using the AWS SDK for .NET.</p>","contentLength":2460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing the end-of-support for AWS Tools for PowerShell v4","url":"https://aws.amazon.com/blogs/devops/announcing-the-end-of-support-for-aws-tools-for-powershell-v4/","date":1755640909,"author":"Ed McLaughlin","guid":233489,"unread":true,"content":"<p>We are announcing the end-of-support for the <a href=\"https://aws.amazon.com/powershell/\">AWS Tools for PowerShell v4.x</a> starting on March 1, 2026, in accordance with the <a href=\"https://docs.aws.amazon.com/sdkref/latest/guide/maint-policy.html\">SDK and Tools maintenance policy</a>. On June 23, 2025, the next major version of the AWS Tools for PowerShell, version 5.x, became generally available (<a href=\"https://aws.amazon.com/blogs/developer/aws-tools-for-powershell-v5-now-generally-available/\">blog post</a>). Version 5.x of the AWS Tools for PowerShell includes bug fixes, new features, performance enhancements, and leverages the latest major version of the <a href=\"https://aws.amazon.com/sdk-for-net/\">AWS SDK for .NET v4.x</a>. We strongly encourage you to <a href=\"https://docs.aws.amazon.com/powershell/v5/userguide/migrating-v5.html\">upgrade</a> to the latest version of AWS Tools for PowerShell v5.x to take advantage of these enhancements.</p><p>Existing applications that use the AWS Tools for PowerShell v4.x will continue to function as intended unless there is a fundamental change to how an AWS service works. This is uncommon, and we will announce it broadly if it happens. Beginning March 1, 2026, the AWS Tools for PowerShell v4.x will only receive critical bug fixes and security updates, and we will not update it to support new AWS services, new service features, or changes to existing services. After June 1, 2026, when the AWS Tools for PowerShell v4.x reaches end-of-support, it will no longer receive updates or releases.</p><p>The timeline for end-of-support is as follows, as defined by the SDK major version lifecycle:</p><table border=\"1\"><tbody><tr></tr><tr><td>During this phase, the SDK is fully supported. AWS will provide regular SDK releases that include support for new services, API updates for existing services, as well as bug and security fixes.</td></tr><tr><td>During the maintenance mode, AWS limits SDK releases to address critical bug fixes and security issues only. An SDK will not receive API updates for new or existing services, or be updated to support new regions</td></tr><tr><td>When an SDK reaches end-of support, it will no longer receive updates or releases. Previously published releases will continue to be available via public package managers and the code will remain on GitHub. The GitHub repository may be archived.</td></tr></tbody></table><p>Use the following references to learn more about the AWS Tools for PowerShell v5.x along with migration support.</p><p>We recommend you upgrade to the latest major version of the AWS Tools for PowerShell v5.x by using the <a href=\"https://docs.aws.amazon.com/powershell/v5/userguide/migrating-v5.html\">migration guide</a>. This major version includes, but is not limited to, performance enhancements, bug fixes, modern .NET libraries and frameworks, and the latest AWS service updates. To learn more, please refer to the <a href=\"https://aws.amazon.com/blogs/developer/aws-tools-for-powershell-v5-now-generally-available/\">AWS Tools for PowerShell v5.x GA blog post</a>.</p><p>If you need assistance or have feedback, reach out to your usual AWS support contacts. You can also open a <a href=\"https://github.com/aws/aws-tools-for-powershell/discussions\">discussion</a> or <a href=\"https://github.com/aws/aws-tools-for-powershell/issues\">issue</a> on GitHub. Thank you for using the AWS Tools for PowerShell.</p>","contentLength":2584,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best performance and fastest memory with the new Amazon EC2 R8i and R8i-flex instances","url":"https://aws.amazon.com/blogs/aws/best-performance-and-fastest-memory-with-the-new-amazon-ec2-r8i-and-r8i-flex-instances/","date":1755630984,"author":"Veliswa Boya","guid":233475,"unread":true,"content":"<p>Today, we‚Äôre announcing general availability of the new eighth generation, memory optimized <a href=\"https://aws.amazon.com/pm/ec2/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">Amazon Elastic Compute Cloud (Amazon EC2)</a> R8i and R8i-flex instances powered by custom Intel Xeon 6 processors, available only on AWS. They deliver the highest performance and fastest memory bandwidth among comparable Intel processors in the cloud. These instances deliver up to 15 percent better price performance, 20 percent higher performance, and 2.5 times more memory throughput compared to previous generation instances.</p><p>With these improvements, R8i and R8i-flex instances are ideal for a variety of memory intensive workloads such as SQL and NoSQL databases, distributed web scale in-memory caches (Memcached and Redis), in-memory databases such as SAP HANA, and real-time big data analytics (Apache Hadoop and Apache Spark clusters). For a majority of the workloads that don‚Äôt fully utilize the compute resources, the R8i-flex instances are a great first choice to achieve an additional 5 percent better price performance and 5 percent lower prices.</p><p> In terms of performance, R8i and R8i-flex instances offer 20 percent better performance than R7i instances, with even higher gains for specific workloads. These instances are up to 30 percent faster for PostgreSQL databases, up to 60 percent faster for NGINX web applications, and up to 40 percent faster for AI deep learning recommendation models compared to previous generation R7i instances, with sustained all-core turbo frequency now reaching 3.9 GHz (compared to 3.2 GHz in the previous generation). They also feature a 4.6x larger L3 cache and significantly better memory throughput, offering 2.5 times higher memory bandwidth than the seventh generation. With this higher performance across all the vectors, you can run a greater number of workloads while keeping costs down.</p><p>R8i instances now scale up to 96xlarge with up to 384 vCPUs and 3TB memory (versus 48xlarge sizes in the seventh generation), helping you to scale up database applications. R8i instances are SAP certified to deliver 142,100 aSAPS, which is highest among all comparable machines in on premises and cloud environments, delivering exceptional performance for your mission-critical SAP workloads. R8i-flex instances offer the most common sizes, from large to 16xlarge, and are a great first choice for applications that don‚Äôt fully utilize all compute resources. Both R8i and R8i-flex instances use the latest sixth generation <a href=\"https://aws.amazon.com/ec2/nitro/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">AWS Nitro Cards</a>, delivering up to two times more network and <a href=\"https://aws.amazon.com/ebs/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">Amazon Elastic Block Storage (Amazon EBS)</a> bandwidth compared to the previous generation, which greatly improves network throughput for workloads handling small packets, such as web, application, and gaming servers.</p><p>R8i and R8i-flex instances also support bandwidth configuration with 25 percent allocation adjustments between network and Amazon EBS bandwidth, enabling better database performance, query processing, and logging speeds. Additional enhancements include FP16 datatype support for Intel AMX to support workloads such as deep learning training and inference and other artificial intelligence and machine learning (AI/ML) applications.</p><p>The specs for the R8i instances are as follows.</p><table cellpadding=\"8\"><tbody><tr></tr></tbody><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>The specs for the R8i-flex instances are as follows.</p><table cellpadding=\"8\"><tbody><tr></tr></tbody><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>As stated earlier, R8i-flex instances are more affordable versions of the R8i instances, offering up to 5 percent better price performance at 5 percent lower prices. They‚Äôre designed for workloads that benefit from the latest generation performance but don‚Äôt fully use all compute resources. These instances can reach up to the full CPU performance 95 percent of the time and work well for in-memory databases, distributed web scale cache stores, mid-size in-memory analytics, real-time big data analytics, and other enterprise applications. R8i instances are recommended for more demanding workloads that need sustained high CPU, network, or EBS performance such as analytics, databases, enterprise applications, and web scale in-memory caches.</p><p> R8i and R8i-flex instances are available today in the US East (N. Virginia), US East (Ohio), US West (Oregon), and Europe (Spain) <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\">AWS Regions</a>. As usual with Amazon EC2, you pay only for what you use. For more information, refer to <a href=\"https://aws.amazon.com/ec2/pricing/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">Amazon EC2 Pricing</a>. Check out the full collection of <a href=\"https://aws.amazon.com/ec2/instance-types/\">memory optimized instances</a> to help you start migrating your applications.</p>","contentLength":4373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tuning Linux Swap for Kubernetes: A Deep Dive","url":"https://kubernetes.io/blog/2025/08/19/tuning-linux-swap-for-kubernetes-a-deep-dive/","date":1755628200,"author":"","guid":233464,"unread":true,"content":"<p>The Kubernetes <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/swap-memory-management/\">NodeSwap feature</a>, likely to graduate to  in the upcoming Kubernetes v1.34 release,\nallows swap usage:\na significant shift from the conventional practice of disabling swap for performance predictability.\nThis article focuses exclusively on tuning swap on Linux nodes, where this feature is available. By allowing Linux nodes to use secondary storage for additional virtual memory when physical RAM is exhausted, node swap support aims to improve resource utilization and reduce out-of-memory (OOM) kills.</p><p>However, enabling swap is not a \"turn-key\" solution. The performance and stability of your nodes under memory pressure are critically dependent on a set of Linux kernel parameters. Misconfiguration can lead to performance degradation and interfere with Kubelet's eviction logic.</p><p>In this blogpost, I'll dive into critical Linux kernel parameters that govern swap behavior. I will explore how these parameters influence Kubernetes workload performance, swap utilization, and crucial eviction mechanisms.\nI will present various test results showcasing the impact of different configurations, and share my findings on achieving optimal settings for stable and high-performing Kubernetes clusters.</p><h2>Introduction to Linux swap</h2><p>At a high level, the Linux kernel manages memory through pages, typically 4KiB in size. When physical memory becomes constrained, the kernel's page replacement algorithm decides which pages to move to swap space. While the exact logic is a sophisticated optimization, this decision-making process is influenced by certain key factors:</p><ol><li>Page access patterns (how recently pages are accessed)</li><li>Page dirtyness (whether pages have been modified)</li><li>Memory pressure (how urgently the system needs free memory)</li></ol><h3>Anonymous vs File-backed memory</h3><p>It is important to understand that not all memory pages are the same. The kernel distinguishes between anonymous and file-backed memory.</p><p>: This is memory that is not backed by a specific file on the disk, such as a program's heap and stack. From the application's perspective this is private memory, and when the kernel needs to reclaim these pages, it must write them to a dedicated swap device.</p><p>: This memory is backed by a file on a filesystem. This includes a program's executable code, shared libraries, and filesystem caches. When the kernel needs to reclaim these pages, it can simply discard them if they have not been modified (\"clean\"). If a page has been modified (\"dirty\"), the kernel must first write the changes back to the file before it can be discarded.</p><p>While a system without swap can still reclaim clean file-backed pages memory under pressure by dropping them, it has no way to offload anonymous memory. Enabling swap provides this capability, allowing the kernel to move less-frequently accessed memory pages to disk to conserve memory to avoid system OOM kills.</p><h3>Key kernel parameters for swap tuning</h3><p>To effectively tune swap behavior, Linux provides several kernel parameters that can be managed via .</p><ul><li>: This is the most well-known parameter. It is a value from 0 to 200 (100 in older kernels) that controls the kernel's preference for swapping anonymous memory pages versus reclaiming file-backed memory pages (page cache).\n<ul><li>: The kernel will be aggressive in swapping out less-used anonymous memory to make room for file-cache.</li><li>: The kernel will strongly prefer dropping file cache pages over swapping anonymous memory.</li></ul></li><li>: This parameter tells the kernel to keep a minimum amount of memory free as a buffer. When the amount of free memory drops below the this safety buffer, the kernel starts more aggressively reclaiming pages (swapping, and eventually handling OOM kills).\n<ul><li> It acts as a safety lever to ensure the kernel has enough memory for critical allocation requests that cannot be deferred.</li><li>: Setting a higher  effectively raises the floor for for free memory, causing the kernel to initiate swap earlier under memory pressure.</li></ul></li><li><code>vm.watermark_scale_factor</code>: This setting controls the gap between different watermarks: ,  and , which are calculated based on .\n<ul><li>:\n<ul><li>: When free memory is below this mark, the  kernel process wakes up to reclaim pages in the background. This is when a swapping cycle begins.</li><li>: When free memory hits this minimum level, then aggressive page reclamation will block process allocation. Failing to reclaim pages will cause OOM kills.</li><li>: Memory reclamation stops once the free memory reaches this level.</li></ul></li><li>: A higher  careates a larger buffer between the  and  watermarks. This gives  more time to reclaim memory gradually before the system hits a critical state.</li></ul></li></ul><p>In a typical server workload, you might have a long-running process with some memory that becomes 'cold'. A higher  value can free up RAM by swapping out the cold memory, for other active processes that can benefit from keeping their file-cache.</p><p>Tuning the  and  parameters to move the swapping window early will give more room for  to offload memory to disk and prevent OOM kills during sudden memory spikes.</p><p>To understand the real-impact of these parameters, I designed a series of stress tests.</p><ul><li>: GKE on Google Cloud</li><li>: 1.33.2</li><li>:  (8GiB RAM, 50GB swap on a  disk, without encryption), Ubuntu 22.04</li><li>: A custom Go application designed to allocate memory at a configurable rate, generate file-cache pressure, and simulate different memory access patterns (random vs sequential).</li><li>: A sidecar container capturing system metrics every second.</li><li>: Critical system components (kubelet, container runtime, sshd) were prevented from swapping by setting  in their respective cgroups.</li></ul><p>I ran a stress-test pod on nodes with different swappiness settings (0, 60, and 90) and varied the  and  parameters to observe the outcomes under heavy memory allocation and I/O pressure.</p><h4>Visualizing swap in action</h4><p>The graph below, from a 100MBps stress test, shows swap in action. As free memory (in the \"Memory Usage\" plot) decreases, swap usage () and swap-out activity () increase. Critically, as the system relies more on swap, the I/O activity and corresponding wait time ( in the \"CPU Usage\" plot) also rises, indicating CPU stress.</p><p>My initial tests with default kernel parameters (, , <code>watermark_scale_factor=10</code>) quickly led to OOM kills and even unexpected node restarts under high memory pressure. With selecting appropriate kernel parameters a good balance in node stability and performance can be achieved.</p><p>The swappiness parameter directly influences the kernel's choice between reclaiming anonymous memory (swapping) and dropping page cache. To observe this, I ran a test where one pod generated and held file-cache pressure, followed by a second pod allocating anonymous memory at 100MB/s, to observe the kernel preference on reclaim:</p><p>My findings reveal a clear trade-off:</p><ul><li>: The kernel proactively swapped out the inactive anonymous memory to keep the file cache. This resulted in high and sustained swap usage and significant I/O activity (\"Blocks Out\"), which in turn caused spikes in I/O wait on the CPU.</li><li>: The kernel favored dropping file-cache pages delaying swap consumption. However, it's critical to understand that this <strong>does not disable swapping</strong>. When memory pressure was high, the kernel still swapped anonymous memory to disk.</li></ul><p>The choice is workload-dependent. For workloads sensitive to I/O latency, a lower swappiness is preferable. For workloads that rely on a large and frequently accessed file cache, a higher swappiness may be beneficial, provided the underlying disk is fast enough to handle the load.</p><h4>Tuning watermarks to prevent eviction and OOM kills</h4><p>The most critical challenge I encountered was the interaction between rapid memory allocation and Kubelet's eviction mechanism. When my test pod, which was deliberately configured to overcommit memory, allocated it at a high rate (e.g., 300-500 MBps), the system quickly ran out of free memory.</p><p>With default watermarks, the buffer for reclamation was too small. Before  could free up enough memory by swapping, the node would hit a critical state, leading to two potential outcomes:</p><ol><li> If kubelet's eviction manager detected  was below its threshold, it would evict the pod.</li><li> In some high-rate scenarios, the OOM Killer would activate before eviction could complete, sometimes killing higher priority pods that were not the source of the pressure.</li></ol><p>To mitigate this I tuned the watermarks:</p><ol><li>Increased  to 512MiB: This forces the kernel to start reclaiming memory much earlier, providing a larger safety buffer.</li><li>Increased  to 2000: This widened the gap between the  and  watermarks (from ‚âà337MB to ‚âà591MB in my test node's ), effectively increasing the swapping window.</li></ol><p>This combination gave  a larger operational zone and more time to swap pages to disk during memory spikes, successfully preventing both premature evictions and OOM kills in my test runs.</p><p>Table compares watermark levels from  (Non-NUMA node):</p><table><thead><tr><th> and <code>watermark_scale_factor=10</code></th><th><code>min_free_kbytes=524288KiB</code> and <code>watermark_scale_factor=2000</code></th></tr></thead><tbody><tr><td>Node 0, zone Normal  &nbsp; pages free 583273  &nbsp; min 10504  &nbsp; high 15756  &nbsp; present 1310720 </td><td>Node 0, zone Normal  &nbsp; pages free 470539  &nbsp; low 337017  &nbsp; spanned 1310720 &nbsp; managed 1274542</td></tr></tbody></table><p>The graph below reveals that the kernel buffer size and scaling factor play a crucial role in determining how the system responds to memory load. With the right combination of these parameters, the system can effectively use swap space to avoid eviction and maintain stability.</p><p>Enabling swap in Kubernetes is a powerful tool, but it comes with risks that must be managed through careful tuning.</p><ul><li><p><strong>Risk of performance degradation</strong> Swapping is orders of magnitude slower than accessing RAM. If an application's active working set is swapped out, its performance will suffer dramatically due to high I/O wait times (thrashing). Swap could preferably be provisioned with a SSD backed storage to improve performance.</p></li><li><p><strong>Risk of masking memory leaks</strong> Swap can hide memory leaks in applications, which might otherwise lead to a quick OOM kill. With swap, a leaky application might slowly degrade node performance over time, making the root cause harder to diagnose.</p></li><li><p><strong>Risk of disabling evictions</strong> Kubelet proactively monitors the node for memory-pressure and terminates pods to reclaim the resources. Improper tuning can lead to OOM kills before kubelet has a chance to evict pods gracefully. A properly configured  is essential to ensure kubelet's eviction mechanism remains effective.</p></li></ul><p>Together, the kernel watermarks and kubelet eviction threshold create a series of memory pressure zones on a node. The eviction-threshold parameters need to be adjusted to configure Kubernetes managed evictions occur before the OOM kills.</p><p>As the diagram shows, an ideal configuration will be to create a large enough 'swapping zone' (between  and  watermarks) so that the kernel can handle memory pressure by swapping before available memory drops into the Eviction/Direct Reclaim zone.</p><p>Based on these findings, I recommend the following as a starting point for Linux nodes with swap enabled. You should benchmark this with your own workloads.</p><ul><li>: Linux default is a good starting point for general-purpose workloads. However, the ideal value is workload-dependent, and swap-sensitive applications may need more careful tuning.</li><li><code>vm.min_free_kbytes=500000</code> (500MB): Set this to a reasonably high value (e.g., 2-3% of total node memory) to give the node a reasonable safety buffer.</li><li><code>vm.watermark_scale_factor=2000</code>: Create a larger window for  to work with, preventing OOM kills during sudden memory allocation spikes.</li></ul><p>I encourage running benchmark tests with your own workloads in test-environments, when setting up swap for the first time in your Kubernetes cluster. Swap performance can be sensitive to different environment differences such as CPU load, disk type (SSD vs HDD) and I/O patterns.</p>","contentLength":11722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Bringing Agentic AI to Cloud Native with kagent & Kyverno","url":"https://www.youtube.com/watch?v=C0y8bDvL47M","date":1755623036,"author":"CNCF [Cloud Native Computing Foundation]","guid":233416,"unread":true,"content":"<article>Let‚Äôs stop babysitting clusters and build AI agents to operate and govern for us! Kagent, a new CNCF project, enables simple declarative development of Kubernetes-native AI agents for any cloud native projects. This session will show you how to rapidly construct custom AI agents for essential operational and management tasks. Learn how Kyverno, the Kubernetes-native policy engine for managing validation, mutation, generation, and cleanup, can be extended with AI capabilities through kagent using the emerging Model Context Protocol (MCP) to enable intelligent policy recommendations and automation.</article>","contentLength":605,"flags":null,"enclosureUrl":"https://www.youtube.com/v/C0y8bDvL47M?version=3","enclosureMime":"","commentsUrl":null},{"title":"Streamline NGINX Configuration with Docker Desktop Extension","url":"https://www.docker.com/blog/streamline-nginx-configuration-with-docker-desktop-extension/","date":1755620016,"author":"Dylen Turnbull","guid":233404,"unread":true,"content":"<p><em>Docker periodically highlights blog posts featuring use cases and success stories from Docker partners and practitioners. This story was contributed by Dylen Turnbull and Timo Stark. With over 29 years in enterprise and open-source software development, Dylen Turnbull has held roles at Symantec, Veritas, F5 Networks, and most recently as a Developer Advocate for NGINX. Timo is a Docker Captain, Head of IT at DoHo Engineering, and was formerly a Principal Technical Product Manager at NGINX.</em></p><p>Modern Application developers face challenges in managing dependencies, ensuring consistent environments, and scaling applications. Docker Desktop simplifies these tasks with intuitive containerization, delivering reliable environments, easy deployments, and scalable architectures. NGINX server management in containers offers opportunities for enhancement, which the NGINX Development Center addresses with user-friendly tools to optimize configuration, performance, and web server management.</p><h2>Opportunities for Increased Workflow Efficiency</h2><p>Docker Desktop streamlines container workflows, but NGINX configuration can be further improved with the NGINX Development Center:</p><ul><li>: NGINX setup often requires command-line expertise. The NGINX Development Center offers intuitive interfaces to simplify the process.</li><li><strong>Simplified Multi-Server Management</strong>: Managing multiple configurations involves complex volume mounting. The NGINX Development Center centralizes and streamlines configuration handling.</li><li>: Debugging requires manual log access and container inspection. The NGINX Development Center provides clear diagnostic tools for faster resolution.</li><li>: Reverse proxy updates need frequent restarts. The NGINX Development Center enables quick configuration changes with minimal downtime.</li></ul><p>By integrating Docker Desktop‚Äôs seamless containerization with the NGINX Development Center‚Äôs tools, developers can achieve a more efficient workflow for modern applications.</p><p>The <a href=\"https://hub.docker.com/extensions/nginx/docker-extension\" rel=\"nofollow noopener\" target=\"_blank\">NGINX Development Center</a>, available in the Docker Extensions Marketplace with over 51,000 downloads, addresses these frictions, streamlining NGINX configuration management for developers.</p><h2>The advantage for App/Web Server Development</h2><p>The NGINX Development Center enhances app and web server development by offering an intuitive GUI-based interface integrated into Docker Desktop, simplifying server configuration file management without requiring command-line expertise. It provides streamlined access to runtime configuration previews, minimizing manual container inspection, and enables rapid iteration without container restarts for faster development and testing cycles.</p><p>Centralized configuration management ensures consistency across development, testing, and production environments. Seamlessly integrated with Docker Desktop, the extension reduces the complexity of traditional NGINX workflows, allowing developers to focus on application development rather than infrastructure management.</p><h2>Overview of the NGINX Development Center</h2><p>The NGINX Development Center, developed by Timo Stark, is designed to enhance the developer experience for NGINX server configuration in containerized environments. Available in the Docker Extensions Marketplace, the extension leverages Docker Desktop‚Äôs extensibility to provide a dedicated NGINX Development Center. Key features include:</p><p><strong>Graphical Configuration Interface</strong></p><p>A user-friendly UI for creating and editing NGINX server blocks, routing rules, and SSL configurations.</p><p><strong>Run-Time Configuration Updates</strong></p><p>Apply changes to NGINX instances without container restarts, supporting rapid iteration.</p><p><strong>Integrated Debugging Tools</strong></p><p>Validate configurations, and troubleshoot issues directly within Docker Desktop.</p><h2>How Does the NGINX Development Center Work?</h2><p>The NGINX Development Center Docker extension, based on the <a href=\"https://github.com/nginx/docker-extension\" rel=\"nofollow noopener\" target=\"_blank\">NGINX Docker Desktop Extension public repository</a>, simplifies NGINX configuration and management within Docker Desktop. It operates as a containerized application with a React-based user interface and a Node.js backend, integrated into Docker Desktop via the Extensions Marketplace and Docker API.</p><p>Here‚Äôs how it works in simplified terms:</p><ol><li>: The extension is installed from the Docker Extensions Marketplace or built locally using a Dockerfile that compiles the UI and backend components. It runs as a container within Docker Desktop, pulling the image nginx/nginx-docker-extension:latest.</li><li>: The React-based UI, accessible through the NGINX Development Center tab in Docker Desktop, allows developers to create and edit NGINX configurations, such as server blocks, routing rules, and SSL settings.</li><li>: The Node.js backend processes user inputs from the UI, generates NGINX configuration files, and applies them to a managed NGINX container. Changes are deployed dynamically using NGINX‚Äôs reload mechanism, avoiding container restarts.</li><li>: The extension communicates with Docker Desktop‚Äôs API to manage NGINX containers and uses Docker volumes to store configuration files and logs, ensuring seamless interaction with the Docker ecosystem.</li><li>: While it doesn‚Äôt provide direct log access, the extension supports debugging by validating configurations in real-time and leveraging Docker Desktop‚Äôs native tools for indirect log viewing.</li></ol><p>The extension‚Äôs backend, built with Node.js, handles configuration generation and NGINX instance management, while the React-based frontend provides an intuitive user experience. For development, the extension supports hot reloading, allowing developers to test changes without rebuilding the image.</p><p>Below is a simplified architecture diagram illustrating how the NGINX Development Center integrates with Docker Desktop:</p><p>NGINX Development Center architecture showing integration with Docker Desktop, featuring a Node.js backend and React UI, managing NGINX containers and configuration files.</p><ul><li>: Hosts the extension and provides access to the Docker API and Extensions Marketplace.</li><li>: Runs as a container, with a Node.js backend for configuration management and a React UI for user interaction.</li><li>: The managed NGINX instance, configured dynamically by the extension.</li><li>: Generated and monitored by the extension, stored in Docker volumes for persistence.</li></ul><h2>Why run NGINX configuration management as a Docker Desktop Extension?</h2><p>Running NGINX configuration management as a Docker Desktop Extension provides a unified, streamlined experience for developers already working within the Docker ecosystem. By integrating directly into Docker Desktop‚Äôs interface, the extension eliminates the friction of switching between multiple tools and command-line interfaces, allowing developers to manage NGINX configurations alongside their containerized applications in a single, familiar environment.</p><p>The extension approach leverages Docker‚Äôs inherent benefits of isolation and consistency, ensuring that NGINX configuration management operates reliably across different development machines and operating systems. This containerized approach prevents conflicts with local system configurations and removes the complexity of installing and maintaining separate NGINX management tools.</p><p>Furthermore, Docker Desktop serves as the only prerequisite for the NGINX Development Center. Once Docker Desktop is installed, developers can immediately access sophisticated NGINX configuration capabilities without additional software installations, complex environment setup, or specialized NGINX expertise. The extension transforms what traditionally requires command-line proficiency into an intuitive, graphical workflow that integrates seamlessly with existing Docker-based development practices.</p><p>Follow these steps to set up and use the Docker Extension: NGINX Development Center<p>Prerequisites: Docker Desktop, 1 running NGINX container.</p></p><p><strong>NGINX Development Center Setup in Docker Desktop</strong>:</p><ul><li>Ensure Docker Desktop is installed and running on your machine (Windows, macOS, or Linux).</li></ul><p><strong>Installing the NGINX Development Center</strong>:</p><ul><li>Open Docker Desktop and navigate to the Extensions Marketplace (left-hand menu).</li><li>Search for ‚ÄúNGINX‚Äù or ‚ÄúNGINX Development Center‚Äù.</li><li>Click ‚ÄúInstall‚Äù to pull and install the NGINX Development Center image&nbsp;</li></ul><p><strong>Accessing the NGINX Development Center</strong>:</p><ul><li>After installation, a new ‚ÄúNGINX‚Äù tab appears in Docker Desktop‚Äôs left-hand menu.</li><li>Click the tab to open the NGINX Development Center, where you can manage configurations and monitor NGINX instances.</li></ul><p><strong>Configuration Management with the NGINX Development Center</strong>:</p><ul><li>Use the GUI configuration editor to create new NGINX config files.</li><li>Configure existing nginx configuration files.</li><li>Preview and validate configurations before applying them.</li><li>Save changes, which are applied dynamically via hot reloading without restarting the NGINX container.</li></ul><h2>Real-world use case example: <strong>Development Proxy for Local Services</strong></h2><p>In modern application development, NGINX serves as a reverse proxy that‚Äôs useful for developers on full-stack or microservices projects. It manages traffic routing between components, mitigates CORS issues in browser-based testing, enables secure local HTTPS setups, and supports efficient workflows by letting multiple services share a single entry point without direct port exposure. This aids local environments for simulating production setups, testing API integrations, or handling real-time features like WebSockets, while avoiding manual restarts and complex configurations. NGINX can proxy diverse local services, including frontend frameworks (e.g., React or Angular apps), backend APIs (e.g., Node.js/Express servers), databases with web interfaces (e.g., phpMyAdmin), static file servers, or third-party tools like mock services and caching layers.</p><p>Developers often require a local proxy to route traffic between services (e.g., frontend on port 3000 and backend API) and avoid CORS issues, but manual NGINX setup demands file edits and restarts.</p><p>With the Docker Extension: NGINX Development Center</p><ul><li>: Install the NGINX Development Center via Docker Extensions Marketplace in Docker Desktop. Ensure local services (e.g., Node.js backend on port 3000) run in separate containers. Open the NGINX Development Center tab.</li></ul><p>Containers run separately.</p><p>: In the UI, create a new server. Set upstream to server the frontend at localhost. Add proxy for /api/* to http://backend:3000. Publish via the graphical options.</p><p>Server config editing via the Docker Desktop UI</p><ul><li>: Preview the config in the NGINX Development Center UI to check for errors. Test by accessing http://localhost/ and http://localhost/api in a browser; confirm routing to backend.</li><li>: Save and apply changes dynamically (no restart needed). Export config for reuse in a Docker Compose file to orchestrate services.</li></ul><p>This use case utilizes the NGINX Development Center‚Äôs React UI for proxy configuration, Node.js backend for config generation, and Docker API for seamless networking. Try setting up your own local proxy today by installing the extension and exploring the NGINX Development Center.</p><h2>Try it out and come visit us</h2><p>This post has examined how the NGINX Development Center, integrated into Docker Desktop via the NGINX Development Center, tackles developer challenges in managing NGINX configurations for containerized web applications. It provides a UI and backend to simplify dependency management, ensure consistent environments, and support scalable setups. The graphical interface reduces the need for command-line expertise, managing server blocks, routing, and SSL settings, while dynamic updates and real-time previews aid iteration and debugging. Docker volumes help maintain consistency across development, testing, and production.</p><p>We‚Äôve highlighted a practical use case with <strong>Development Proxy for Local Services</strong> feasible within Docker Desktop using the extension. The architecture leverages Docker Desktop‚Äôs API and a containerized design to support the workflow.If you‚Äôre a developer interested in improving NGINX management, try installing the NGINX Development Center from the Docker Extensions Marketplace and explore the NGINX Development Center. For deeper engagement, <a href=\"https://github.com/nginx/docker-extension\" rel=\"nofollow noopener\" target=\"_blank\">visit the GitHub repository</a> to review the codebase, suggest features, or contribute to its development, and consider <a href=\"https://community.nginx.org/\" rel=\"nofollow noopener\" target=\"_blank\">joining discussions </a>to connect with others.</p>","contentLength":12158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building AI Agents with Docker MCP Toolkit: A Developer‚Äôs Real-World Setup","url":"https://www.docker.com/blog/docker-mcp-ai-agent-developer-setup/","date":1755615586,"author":"Rajesh Padmakumaran","guid":233356,"unread":true,"content":"<p>Building AI agents in the real world often involves more than just making model calls ‚Äî it requires integrating with external tools, handling complex workflows, and ensuring the solution can scale in production.</p><p>In this post, we‚Äôll walk through a real-world developer setup for creating an agent using the Docker MCP Toolkit.</p><p>To make things concrete, I‚Äôve built an agent that takes a Git repository as input and can answer questions about its contents ‚Äî whether it‚Äôs explaining the purpose of a function, summarizing a module, or finding where a specific API call is made. This simple but practical use case serves as a foundation for exploring how agents can interact with real-world data sources and respond intelligently.</p><p>I built and ran it using the Docker MCP Toolkit, which made setup and integration fast, portable, and repeatable. This blog walks you through that developer setup and explains why Docker MCP is a game changer for building and running agents.</p><h2>Use Case: GitHub Repo Question-Answering Agent</h2><p>The goal: Build an AI agent that can connect to a GitHub repository, retrieve relevant code or metadata, and answer developer questions in plain language.</p><ul><li>‚ÄúSummarize this repo: <code>https://github.com/owner/repo</code>‚Äù</li><li>‚ÄúWhere is the authentication logic implemented?‚Äù</li><li>‚ÄúList main modules and their purpose.‚Äù</li><li>‚ÄúExplain the function  and show where it‚Äôs used.‚Äù</li></ul><p><strong>This goes beyond a simple code demo ‚Äî it reflects how developers work in real-world environments</strong></p><ul><li>The agent acts like a code-aware teammate you can query anytime.</li><li>The MCP Gateway handles tooling integration (GitHub API) without bloating the agent code.</li><li>Docker Compose ties the environment together so it runs the same in dev, staging, or production.</li></ul><h2>Role of Docker MCP Toolkit</h2><p>Without MCP Toolkit, you‚Äôd spend hours wiring up API SDKs, managing auth tokens, and troubleshooting environment differences.</p><ol><li>Containerized connectors ‚Äì Run the GitHub MCP Gateway as a ready-made service (<code>docker/mcp-gateway:latest</code>), no SDK setup required.</li><li>Consistent environments ‚Äì The container image has fixed dependencies, so the setup works identically for every team member.</li><li>Rapid integration ‚Äì The agent connects to the gateway over HTTP; adding a new tool is as simple as adding a new container.</li><li>Iterate faster ‚Äì Restart or swap services in seconds using .</li><li>Focus on logic, not plumbing ‚Äì The gateway handles the GitHub-specific heavy lifting while you focus on prompt design, reasoning, and multi-agent orchestration.</li></ol><p>Running everything via Docker Compose means you treat the entire agent environment as a single deployable unit:</p><ul><li>One-command startup ‚Äì  brings up the MCP Gateway (and your agent, if containerized) together.</li><li>Service orchestration ‚Äì Compose ensures dependencies start in the right order.</li><li>Internal networking ‚Äì Services talk to each other by name (<code>http://mcp-gateway-github:8080</code>) without manual port wrangling.</li><li>Scaling ‚Äì Run multiple agent instances for concurrent requests.</li><li>Unified logging ‚Äì View all logs in one place for easier debugging.</li></ul><p>This setup connects a developer‚Äôs local agent to GitHub through a Dockerized MCP Gateway, with Docker Compose orchestrating the environment. Here‚Äôs how it works step-by-step:</p><ul><li>The developer runs the agent from a CLI or terminal.</li><li>They type a question about a GitHub repository ‚Äî e.g., ‚ÄúWhere is the authentication logic implemented?‚Äù</li></ul><ul><li>The Agent (LLM + MCPTools) receives the question.</li><li>The agent determines that it needs repository data and issues a tool call via MCPTools.</li></ul><ul><li>&nbsp;MCPTools sends the request using  to the MCP Gateway running in Docker.</li><li>This gateway is defined in  and configured for the GitHub server (<code>--servers=github --port=8080</code>).</li></ul><ul><li>The MCP Gateway handles all GitHub API interactions ‚Äî listing files, retrieving content, searching code ‚Äî and returns structured results to the agent.</li></ul><ul><li>The agent sends the retrieved GitHub context to OpenAI GPT-4o as part of a prompt.</li><li>&nbsp;The LLM reasons over the data and generates a clear, context-rich answer.</li></ul><ul><li>The agent prints the final answer back to the CLI, often with file names and line references.</li></ul><h2>Code Reference &amp; File Roles</h2><p>The detailed source code for this setup is available at this <a href=\"https://github.com/rajeshsgr/mcp-demo-agents/tree/main\" rel=\"nofollow noopener\" target=\"_blank\">link</a>.&nbsp;</p><p>Rather than walk through it line-by-line, here‚Äôs what each file does in the real-world developer setup:</p><ul><li>Defines the MCP Gateway service for GitHub.</li><li>Runs the <code>docker/mcp-gateway:latest</code> container with GitHub as the configured server.</li><li>Exposes the gateway on port .</li><li>Can be extended to run the agent and additional connectors as separate services in the same network.</li></ul><ul><li>Implements the GitHub Repo Summarizer Agent.</li><li>Uses  to connect to the MCP Gateway over .</li><li>Sends queries to GitHub via the gateway, retrieves results, and passes them to GPT-4o for reasoning.</li><li>Handles the interactive CLI loop so you can type questions and get real-time responses.</li></ul><p>In short: the Compose file manages <em>infrastructure and orchestration</em>, while the Python script handles <em>intelligence and conversation</em>.</p><p>git clone https://github.com/rajeshsgr/mcp-demo-agents/tree/main</p><p>Create a .env file in the root directory and add your OpenAI API key:</p><div><pre>OPEN_AI_KEY = &lt;&lt;Insert your Open AI Key&gt;&gt;\n</pre></div><p>To allow the MCP Gateway to access GitHub repositories, set your GitHub personal access token:</p><div><pre>docker mcp secret set github.personal_access_token=&lt;YOUR_GITHUB_TOKEN&gt;\n</pre></div><p>Bring up the GitHub MCP Gateway container using Docker Compose:</p><ol start=\"5\"><li><strong>Install Dependencies &amp; Run Agent</strong></li></ol><div><pre>python -m venv .venv &amp;&amp; source .venv/bin/activate\npip install -r requirements.txt\npython app.py\n</pre></div><p>Enter your query: <code>Summarize https://github.com/owner/repo</code></p><h2><strong>Real-World Agent Development with Docker, MCP, and Compose</strong></h2><p>This setup is built with production realities in mind ‚Äî</p><ul><li> ensures each integration (GitHub, databases, APIs) runs in its own isolated container with all dependencies preconfigured.</li><li> acts as the bridge between your agent and real-world tools, abstracting away API complexity so your agent code stays clean and focused on reasoning.</li><li> orchestrates all these moving parts, managing startup order, networking, scaling, and environment parity between development, staging, and production.</li></ul><p><strong>From here, it‚Äôs easy to add:</strong></p><ul><li>More MCP connectors (Jira, Slack, internal APIs).</li><li>Multiple agents specializing in different tasks.</li><li>CI/CD pipelines that spin up this environment for automated testing</li></ul><p>By combining  for isolation,  for seamless tool integration, and  for orchestration, we‚Äôve built more than just a working AI agent ‚Äî we‚Äôve created a repeatable, production-ready development pattern. This approach removes environment drift, accelerates iteration, and makes it simple to add new capabilities without disrupting existing workflows. Whether you‚Äôre experimenting locally or deploying at scale, this setup ensures your agents are reliable, maintainable, and ready to handle real-world demands from day one.</p><h2><strong>Before vs. After: The Developer Experience</strong></h2><div><table><tbody><tr><td><p>Manual SDK installs, dependency conflicts, ‚Äúworks on my machine‚Äù issues.</p></td><td><p>Prebuilt container images with fixed dependencies ensure identical environments everywhere.</p></td></tr><tr><td><p><strong>Integration with Tools (GitHub, Jira, etc.)</strong></p></td><td><p>Custom API wiring in the agent code; high maintenance overhead.</p></td><td><p>MCP handles integrations in separate containers; agent code stays clean and focused.</p></td></tr><tr><td><p>Multiple scripts/terminals; manual service ordering.</p></td><td><p> launches and orchestrates all services in the right order.</p></td></tr><tr><td><p>Manually configuring ports and URLs; prone to errors.</p></td><td><p>Internal Docker network with service name resolution (e.g., <code>http://mcp-gateway-github:8080</code>).</p></td></tr><tr><td><p>Scaling services requires custom scripts and reconfigurations.</p></td><td><p>Scale any service instantly with <code>docker compose up --scale</code>.</p></td></tr><tr><td><p>Adding a new integration means changing the agent‚Äôs code and redeploying.</p></td><td><p>Add new MCP containers to  without modifying the agent.</p></td></tr><tr><td><p>Hard to replicate environments in pipelines; brittle builds.</p></td><td><p>Same Compose file works locally, in staging, and in CI/CD pipelines.</p></td></tr><tr><td><p>Restarting services or switching configs is slow and error-prone.</p></td><td><p>Containers can be stopped, replaced, and restarted in seconds.</p></td></tr></tbody></table></div>","contentLength":7936,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNCF Deaf and Hard of Hearing Working Group at KubeCon + CloudNativeCon EU 2025","url":"https://www.youtube.com/watch?v=CLhA-du1Yy8","date":1755614998,"author":"CNCF [Cloud Native Computing Foundation]","guid":233363,"unread":true,"content":"<article>Huge shout-out to the CNCF Deaf and Hard of Hearing Working Group for an incredible week at KubeCon + CloudNativeCon in London earlier this year! Check out this latest video to get a glimpse of what they were up to and what to expect in Atlanta!\n\nThey brought so much energy and had lots of activities, including a keynote, several talks, and a fun sign language crash course. Their kiosk in the Project Pavilion was a hub of activity, creating meaningful connections and fostering a more inclusive community.\n\nThis group is full of passion and brilliant people. We're so proud of what they've accomplished, and look forward to seeing the team in Atlanta! And for a fun challenge, watch the video and see if you can spot the new sign they created for \"CNCF.\" ‚ú®</article>","contentLength":762,"flags":null,"enclosureUrl":"https://www.youtube.com/v/CLhA-du1Yy8?version=3","enclosureMime":"","commentsUrl":null},{"title":"Context Engineering is the Key to Unlocking AI Agents in DevOps","url":"https://devops.com/context-engineering-is-the-key-to-unlocking-ai-agents-in-devops/?utm_source=rss&utm_medium=rss&utm_campaign=context-engineering-is-the-key-to-unlocking-ai-agents-in-devops","date":1755599937,"author":"Harshil Shah","guid":233241,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FinOps as Code ‚Äì Unlocking Cloud Cost Optimization","url":"https://devops.com/finops-as-code-unlocking-cloud-cost-optimization/?utm_source=rss&utm_medium=rss&utm_campaign=finops-as-code-unlocking-cloud-cost-optimization","date":1755598537,"author":"Joydip Kanjilal","guid":233240,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Really Matters When Picking a Cross-Platform Stack Today","url":"https://devops.com/what-really-matters-when-picking-a-cross-platform-stack-today/?utm_source=rss&utm_medium=rss&utm_campaign=what-really-matters-when-picking-a-cross-platform-stack-today","date":1755597088,"author":"Oleksii Kyslenko","guid":233217,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Weekly Roundup: Single GPU P5 instances, Advanced Go Driver, Amazon SageMaker HyperPod and more (August 18, 2025)","url":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-single-gpu-p5-instances-advanced-go-driver-amazon-sagemaker-hyperpod-and-more-august-18-2025/","date":1755531550,"author":"Prasad Rao","guid":231646,"unread":true,"content":"<p>Let me start this week‚Äôs update with something I‚Äôm especially excited about ‚Äì the upcoming BeSA (Become a Solutions Architect) cohort. BeSA is a free mentoring program that I host along with a few other AWS employees on a volunteer basis to help people excel in their cloud careers. Last week, the instructors‚Äô lineup was finalized for the 6-week cohort starting September 6. The cohort will focus on migration and modernization on AWS. Visit the <a href=\"https://besaprogram.com/\">BeSA website</a> to learn more.</p><p>Another highlight for me last week was the announcement of six new AWS Heroes for their technical leadership and exceptional contributions to the AWS community. Read the <a href=\"https://aws.amazon.com/blogs/aws/meet-our-newest-aws-heroes-august-2025/\">full announcement</a> to learn more about these community leaders.</p><p> Here are some launches from last week that got my attention:</p><ul><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/08/aws-advanced-go-driver-generally-available/\">AWS Advanced Go Driver is generally available</a> ‚Äî You can now use the AWS Advanced Go Driver with Amazon Relational Database Service (Amazon RDS) and Amazon Aurora PostgreSQL-Compatible and MySQL-Compatible database clusters for faster switchover and failover times, Federated Authentication, and authentication with AWS Secrets Manager or AWS Identity and Access Management (IAM). You can install the PostgreSQL and MySQL packages for Windows, Mac, or Linux, by following the installation guides in <a href=\"https://github.com/aws/aws-advanced-go-wrapper\">GitHub</a>.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/08/expanded-support-cilium-amazon-eks-hybrid-nodes/\">Expanded support for Cilium with Amazon EKS Hybrid Nodes</a> ‚Äî Cilium is a Cloud Native Computing Foundation (CNCF) graduated project that provides core networking capabilities for Kubernetes workloads. Now, you can receive support from AWS for a broader set of Cilium features when using Cilium with Amazon EKS Hybrid Nodes including application ingress, in-cluster load balancing, Kubernetes network policies, and kube-proxy replacement mode.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/08/sagemaker-p6e-gb200-ultraservers/\">Amazon SageMaker AI now supports P6e-GB200 UltraServers</a> ‚Äî You can accelerate training and deployment of foundational models (FMs) at trillion-parameter scale by using up to 72 NVIDIA Blackwell GPUs under one NVLink domain with the new P6e-GB200 UltraServer support in Amazon SageMaker HyperPod and Model Training.</li><li>Amazon SageMaker HyperPod now supports <a href=\"https://aws.amazon.com/about-aws/whats-new/2025/08/sagemaker-hyperpod-fined-grained-quota-allocation-compute-resources/\">fine-grained quota allocation of compute resources</a>, <a href=\"https://aws.amazon.com/about-aws/whats-new/2025/08/sagemaker-hyperpod-topology-aware-scheduling-llm-tasks/\">topology-aware-scheduling of LLM tasks</a> and <a href=\"https://aws.amazon.com/about-aws/whats-new/2025/08/sagemaker-hyperpod-support-custom-ami/\">custom Amazon Machine Images (AMIs)</a> ‚Äî You can allocate fine-grained compute quota for GPU, Trainium accelerator, vCPU, and vCPU memory within an instance to optimize compute resource distribution. With topology-aware scheduling, you can schedule your large language model (LLM) tasks on an optimal network topology to minimize network communication and enhance training efficiency. Using custom AMIs, you can deploy clusters with pre-configured, security-hardened environments that meet your specific organizational requirements.</li></ul><p> Here are some additional news items and blog posts that I found interesting:</p><p> Check your calendars and sign up for upcoming AWS and AWS Community events:</p><ul><li><a href=\"https://reinvent.awsevents.com/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">AWS re:Invent 2025</a> (December 1-5, 2025, Las Vegas) ‚Äî The AWS flagship annual conference offering collaborative innovation through peer-to-peer learning, expert-led discussions, and invaluable networking opportunities.</li><li><a href=\"https://aws.amazon.com/events/summits/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">AWS Summits</a> ‚Äî Join free online and in-person events that bring the cloud computing community together to connect, collaborate, and learn about AWS. Coming up soon are summits in <a href=\"https://aws.amazon.com/events/summits/johannesburg/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">Johannesburg</a> (August 20) and <a href=\"https://aws.amazon.com/events/summits/toronto/?trk=a0051fdc-5b89-46a6-acfd-4881a84205f8&amp;utm_custom=a0051fdc-5b89-46a6-acfd-4881a84205f8&amp;sc_channel=el\">Toronto</a> (September 4).</li><li><a href=\"https://aws.amazon.com/events/community-day/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">AWS Community Days</a> ‚Äî Join community-led conferences that feature technical discussions, workshops, and hands-on labs led by expert AWS users and industry leaders from around the world: <a href=\"https://awscommunityadria.com/\">Adria</a> (September 5), <a href=\"https://awsbaltic.eu/\">Baltic</a> (September 10), <a href=\"https://aws-community-day.nz/\">Aotearoa</a> (September 18), and <a href=\"https://www.awscommunityday.co.za/\">South Africa</a> (September 20).</li></ul><p>That‚Äôs all for this week. Check back next Monday for another <a href=\"https://aws.amazon.com/blogs/aws/tag/week-in-review/?trk=7c8639c6-87c6-47d6-9bd0-a5812eecb848&amp;sc_channel=el\">Weekly Roundup</a>!</p>","contentLength":3673,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Network-level and Identity-based Observability with Calico Open Source","url":"https://www.youtube.com/watch?v=P7RUzvXr7Vg","date":1755525083,"author":"CNCF [Cloud Native Computing Foundation]","guid":231623,"unread":true,"content":"<article>Don't let your Kubernetes environment be a mystery! Gain the visibility you need to keep things running smoothly. This session dives into why network observability is key in Kubernetes, and includes a live demo showing how to identify misconfigurations, simplify troubleshooting, streamline overall network management, and safely implement and test network security policies across any Kubernetes environment.\n\nHere's what we'll explore:\n\n* Why network observability is important (hint: Kubernetes can be a bit chaotic! )\n* How observability is the foundation for zero trust security\n* Common customer pain points: Troubleshooting, incorrect policies, compliance, and audits\n* Calico Whisker: What it is and how it provides that crucial bird's-eye view\n* LIVE DEMO: Cluster-wide network observability using Calico Whisker</article>","contentLength":821,"flags":null,"enclosureUrl":"https://www.youtube.com/v/P7RUzvXr7Vg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Morphs Fusion Developers To Full Stack Builders","url":"https://devops.com/microsoft-morphs-fusion-developers-to-full-stack-builders/?utm_source=rss&utm_medium=rss&utm_campaign=microsoft-morphs-fusion-developers-to-full-stack-builders","date":1755517675,"author":"Adrian Bridgwater","guid":231552,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Keeping Humans in the Loop: Why Human Oversight Still Matters in an AI-Driven DevOps Future","url":"https://devops.com/keeping-humans-in-the-loop-why-human-oversight-still-matters-in-an-ai-driven-devops-future/?utm_source=rss&utm_medium=rss&utm_campaign=keeping-humans-in-the-loop-why-human-oversight-still-matters-in-an-ai-driven-devops-future","date":1755503841,"author":"Alan Shimel","guid":231460,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-152/?utm_source=rss&utm_medium=rss&utm_campaign=five-great-devops-job-opportunities-152","date":1755493465,"author":"Mike Vizard","guid":231427,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}