{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":50,"items":[{"title":"Career transition in to Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/","date":1739626865,"author":"/u/Similar-Secretary-86","guid":646,"unread":true,"content":"<p>\"I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated </p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My new blog post comparing networking in EKS vs. GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/","date":1739617569,"author":"/u/jumiker","guid":647,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Dive into VPA Recommender","url":"https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/","date":1739615164,"author":"/u/erik_zilinsky","guid":648,"unread":true,"content":"<p>I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.</p><p>Based on my findings, I wrote a <a href=\"https://erikzilinsky.com/posts/vpa1.html\">blog post</a> about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.</p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container Networking - Kubernetes with Calico","url":"https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/","date":1739604355,"author":"/u/tkr_2020","guid":645,"unread":true,"content":"<ul><li>: VLAN 10</li><li>: VLAN 20</li></ul><p>When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:</p><p>The inner IP header reflects:</p><p>The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.</p><p>Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?</p>","contentLength":502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tech Leaders Reveal New Approaches to Corporate Sustainability","url":"https://devops.com/executive-strategies-driving-corporate-sustainability/","date":1739598887,"author":"Bonnie Schneider","guid":599,"unread":true,"content":"<article>Over the past two years, I’ve interviewed more than 100 executives on tech innovation. Key insights emerged. But one stood out: sustainability is no longer a “nice to have.” It’s now a core business strategy. That’s the focus of my inaugural, exclusive report: Decisions That Define: Executive Strategies Driving Corporate Sustainability. Why 2025 is a […]</article>","contentLength":368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do I configure Minikube to use my local IP address instead of the cluster IP?","url":"https://www.reddit.com/r/kubernetes/comments/1ipr3i1/how_do_i_configure_minikube_to_use_my_local_ip/","date":1739584978,"author":"/u/Own_Appointment5630","guid":644,"unread":true,"content":"<p>Hi there!! How can I configure Minikube on Windows (using Docker) to allow my Spring Boot pods to connect to a remote database on the same network as my local machine? When I create the deployment, the pods use the same IP as the Minikube cluster which gets rejected by the database. Is there any way that Minikube uses my local IP in order to connect correctly?.</p>","contentLength":363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/","date":1739555053,"author":"/u/trouphaz","guid":649,"unread":true,"content":"<p>I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.</p><p>So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.</p><pre><code>sc() { CLUSTER_NAME=\"${1}\" export KUBECONFIG=\"~/.kube/${CLUSTER_NAME}\" } </code></pre>","contentLength":592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DataRobot Acquires Agnostic to Gain Distributed Covalent Platform for AI Apps","url":"https://devops.com/datarobot-acquires-agnostic-to-gain-distributed-covalent-platform-for-ai-apps/","date":1739546554,"author":"Mike Vizard","guid":598,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11","url":"https://www.youtube.com/watch?v=eHa6GhK7L0I","date":1739541570,"author":"CNCF [Cloud Native Computing Foundation]","guid":571,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eHa6GhK7L0I?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 46 (Dragonfly)","url":"https://www.youtube.com/watch?v=gd6HRgr8KcA","date":1739512616,"author":"CNCF [Cloud Native Computing Foundation]","guid":570,"unread":true,"content":"<article>Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. \n\nThis CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.</article>","contentLength":576,"flags":null,"enclosureUrl":"https://www.youtube.com/v/gd6HRgr8KcA?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1739491200,"author":"","guid":772,"unread":true,"content":"<p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the \nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial  is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><div>sequenceDiagram\nautonumber\nrect rgb(191, 223, 255)\nKubelet-&gt;&gt;+Kube-apiserver: Create Node\nNote over Kubelet: Taint: node.cloudprovider.kubernetes.io\nKube-apiserver-&gt;&gt;-Kubelet: Node Created\nend\nNote over Kube-apiserver: Node is Not Ready<p> Tainted, Missing Node Addresses*, ...\nNote over Kube-apiserver: Send Updates\nrect rgb(200, 150, 255)\nKube-apiserver-&gt;&gt;+Cloud-controller-manager: Watch: New Node Created\nNote over Cloud-controller-manager: Initialize Node:</p>Cloud Provider Labels, Node Addresses, ...\nCloud-controller-manager-&gt;&gt;-Kube-apiserver: Update Node\nend\nNote over Kube-apiserver: Node is Ready\n</div><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a  resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a  resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one “correct way” to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting “hostNetwork” to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider’s instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure’s node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.\n<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider’s documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nname: cloud-controller-manager\nnamespace: kube-system\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nannotations:\nkubernetes.io/description: Cloud controller manager for my infrastructure\nspec:\ncontainers: # the container details will depend on your specific cloud controller manager\n- name: cloud-controller-manager\ncommand:\n- /bin/my-infrastructure-cloud-controller-manager\n- --leader-elect=true\n- -v=1\nimage: registry/my-infrastructure-cloud-controller-manager@latest\nresources:\nrequests:\ncpu: 200m\nmemory: 50Mi\nhostNetwork: true # these Pods are part of the control plane\nnodeSelector:\nnode-role.kubernetes.io/control-plane: \"\"\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- topologyKey: \"kubernetes.io/hostname\"\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\ntolerations:\n- effect: NoSchedule\nkey: node-role.kubernetes.io/master\noperator: Exists\n- effect: NoExecute\nkey: node.kubernetes.io/unreachable\noperator: Exists\ntolerationSeconds: 120\n- effect: NoExecute\nkey: node.kubernetes.io/not-ready\noperator: Exists\ntolerationSeconds: 120\n- effect: NoSchedule\nkey: node.cloudprovider.kubernetes.io/uninitialized\noperator: Exists\n- effect: NoSchedule\nkey: node.kubernetes.io/not-ready\noperator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Terraform Architecture Explained , Terraform Core, State, and Plugins: How Terraform Works Under…","url":"https://blog.devops.dev/terraform-architecture-explained-terraform-core-state-and-plugins-how-terraform-works-under-a19e4d4dbb09?source=rss----33f8b2d9a328---4","date":1739465498,"author":"Kuseh Simon Wewoliamo","guid":799,"unread":true,"content":"<h3>Terraform Architecture Explained&nbsp;, Terraform Core, State, and Plugins: How Terraform Works Under the&nbsp;Hood.</h3><p><em>1. Introduction 2. Terraform Architecture4. Terraform Best Practices6. References</em></p><p>Infrastructure as Code (IaC), is an approach to managing and provisioning infrastructure by writing code instead of the manual processes&nbsp;, “ClickOps”. IaC can be described as a mindset where you treat all aspects of operations (servers, databases, networks) as software. When you define your infrastructure using code&nbsp;, it enables you to automate and use all the best practices of software development. IaC eliminates human errors&nbsp;, speeds up infrastructure deployments and ensures infrastructure is version-controlled, just like software&nbsp;code.</p><p>Terraform is an open-source tool developed by HashiCorp and the most popular and widely used IaC tool used by DevOps, SREs and cloud architects. Terraform is widely used because of it’s declarative syntax, platform agnostic and its simplicity. Understanding how terraform works behind the hood will go along way to help you in write better terraform code.</p><p>In this article, we will explore Terraform architecture, its core components, and how it orchestrates infrastructure provisioning efficiently.</p><h3><em>2. Terraform Architecture</em></h3><p>Terraform follows a standard architecture to fulfill the necessary IaC tasks. Terraform architecture mainly consists of the following components:<em> 1 Terraform core 2 Plugins (Providers and Provisioners) </em></p><p>Terraform core is the engine/brain behind how terraform works. It is responsible for reading configurations files&nbsp;, building the dependency graphs from resources and data sources, managing state and applying changes. Terraform Core does not directly interact with cloud providers but communicates with plugins via remote procedure calls (RPCs) and the plugins in turn communicates with their corresponding platforms via&nbsp;HTTPs.</p><h4>Plugins (Providers and Provisioners)</h4><p>Terraform ability is enhance by plugins, which enable terraform to interact with cloud services and configure resources dynamically. Plugins acts as connectors or the glue between terraform and external APIs such as AWS, Azure, GCP, Kubernetes, Docker etc. Each plugin is written in the Go programming language and implements a specific interface. Terraform core knows how to install and execute plugins. Provisioners in Terraform are used to execute scripts or commands on a resource after it has been created or modified.</p><p>State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources it created. It is a costumed JSON file that terraform uses to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By default, state is stored in a local file named “terraform.tfstate”. You can read more about terraform state <a href=\"http://State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources  it created. It is a customed  JSON file that terraform uses to  map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By  default, state is stored  in a local file named &quot;terraform.tfstate&quot;. You can read more about terraform state [here](https://developer.hashicorp.com/terraform/language/state)    There are two ways to manage state:  - 1. Local State:  Local State refers to the default way by which Terraform stores state files (terraform.tfstate).  It is suitable for small-scale projects or development environments and single person is managing Terraform.       - 2. Remote State: Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. [Remote State](https://developer.hashicorp.com/terraform/language/state/remote)\">here</a><p> There are two ways to manage state:</p> Local State refers to the default way by which Terraform stores state files (terraform.tfstate). It is suitable for small-scale projects or development environments and single person managing Terraform.</p><p>Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. More on <a href=\"https://developer.hashicorp.com/terraform/language/state/remote\">Remote&nbsp;State</a></p><p>Terraform follows a structured execution flow to provision, update, and manage infrastructure. This process ensures that infrastructure is deployed in a controlled and predictable manner. Terraform workflow consist of mainly three&nbsp;steps:</p><p> The first step is to write your terraform configuration just like any other code using any editor of your&nbsp;choice.</p><p> This is the step where you review your configurations. Terraform plan will define the infrastructure to be created, modified, or destroyed depending on the current configuration and infrastructure.</p><p> The final step in the workflow is Apply, where you are ready to provision real infrastructure. Once you approve of the changes&nbsp;,terraform will go ahead perform the desired actions as defined execution.</p><h3>4. Terraform Best Practices</h3><p>1. You should never edit the Terraform state files by hand or write code that reads them directly. If for some reason you need to manipulate the state file which should be a relatively rare occurrence, use the terraform import or terraform state commands.</p><p>2. It’s a good practice to store your work in a version controlled repository even when you’re just operating as an individual.</p><p>3. When working as a team, it’s important to delegate ownership of infrastructure across these teams and empower them to work in parallel without conflicts.</p><p>4. Never Store your state file in a version controlled repository.</p><p>5. Always use state locking on your state files to prevent data loss, conflicts and state file corruption.</p><p>6. Integrate Terraform to your CI/CD pipelines to make your DevOps pipeline efficient.</p><p>Well well, we have come to the end of this deep dive into terraform Architecture. To learn more about Terraform visit the <a href=\"https://developer.hashicorp.com\">official Terraform page</a>. Don’t forget to add your comments&nbsp;, till then keep&nbsp;coding.</p><p>6. Terraform:Up &amp; Running&nbsp;, Third Edition by Yevgeniy&nbsp;Brikman</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a19e4d4dbb09\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Micro Frontend Revolution","url":"https://blog.devops.dev/the-micro-frontend-revolution-29b6eedc8783?source=rss----33f8b2d9a328---4","date":1739465492,"author":"Adem KORKMAZ","guid":798,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Free AI models: Running Local LLMS with Llama 3.3,","url":"https://blog.devops.dev/running-local-llms-with-llama-3-3-deepseek-r1-and-other-large-language-models-using-ollama-5d0dc2d09358?source=rss----33f8b2d9a328---4","date":1739465463,"author":"Joel Wembo","guid":797,"unread":true,"content":"<h4>Part 4 of 10 Part series on DeepSeek&nbsp;MLOps</h4><h3>Free AI models: Running Local LLMS with Llama 3.3, DeepSeek-R1, and other Large Language Models using&nbsp;Ollama</h3><h4>Step-by-Step Guide: Installing a Web UI for Local LLMs on&nbsp;Windows</h4><p>With the rise of powerful open-source large language models (LLMs) like , , Phi-4, and Gemma 2, many users want to run these models locally for privacy, performance, and customization. However, interacting with these models via the command line can be limiting. <strong><em>The solution? A web-based user interface (UI) that allows easy interaction with your local&nbsp;LLMs.</em></strong></p><p>In this article, we will explore the best web UIs for running LLMs locally on Windows and guide you through the installation process.</p><p> is a lightweight, high-performance framework designed for running large language models (LLMs) locally with optimized execution. It works by leveraging <strong>GGUF (GGML Unified Format)</strong>, an efficient model storage format that supports quantization, allowing models to run smoothly even on consumer hardware.</p><h3>Why Use a Web UI for Local&nbsp;LLMs?</h3><p>Using a web UI for local LLMs offers several advantages:</p><ul><li>: No need to work with command-line tools.</li><li>: Manage multiple models in one&nbsp;place.</li><li>: Chat history, prompt engineering, and adjustable settings.</li><li>: Access your models remotely via a web&nbsp;browser</li></ul><h4>Step 1&nbsp;: Download and Install&nbsp;Ollama</h4><p>Download Ollama from <a href=\"https://ollama.com/download/windows\">https://ollama.com/download/windows</a>, then right click on the downloaded OllamaSetup.exe file and run the installer as administrator. Once the installation is complete, Ollama is ready to use on your Windows system. An Ollama icon will be added to the tray area at the bottom of the&nbsp;desktop.</p><p>To run Ollama and start utilizing its AI models, you’ll need to use a terminal on Windows. We’ll skip it here and let’s see how to install WebUI for a better experience.</p><p>Now open the browser and type localhost:11434 to check is Ollama is up and&nbsp;running</p><p>Also, Check in your system&nbsp;Tray</p><p>Next, Open your CMD to pull some free AI&nbsp;models</p><h4>Step 2 — Install Ollama&nbsp;WebUI</h4><p>Run the below docker command to deploy ollama-webui docker container on your local machine. If Ollama is on your computer, use this&nbsp;command:</p><pre>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</pre><p>To connect to Ollama on another server, change the OLLAMA_BASE_URL to the server’s URL. So if Ollama is on a Different Server, use this&nbsp;command:</p><pre>docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</pre><p>Next, Open your browser and type localhost:3000</p><p>Ollama utilizes <strong>Metal on macOS and CUDA on Windows/Linux</strong> for hardware acceleration, enabling faster inference by directly leveraging GPU tensor operations. It runs a <strong>persistent server in the background</strong>, managing requests via an  that communicates with models using optimized token streaming.</p><p>Internally, it uses <strong>low-level memory-efficient inference kernels</strong>, minimizing VRAM and RAM usage while maintaining performance. It also supports <strong>LoRA (Low-Rank Adaptation) fine-tuning</strong>, allowing users to personalize models on their local machine with minimal compute overhead.</p><p>Run the following command:</p><pre>ollama run deepseek-r1:671b</pre><h3>Choosing the Right Web UI for Your&nbsp;Needs</h3><ul><li>: LM Studio (Simple setup, user-friendly UI)</li><li>: Oobabooga (More features, customization options)</li><li>: Gradio (Custom interface, lightweight solution)</li><li>: Open WebUI (Accessible over the internet)</li></ul><p>Setting up a web UI for local LLMs on Windows significantly enhances your experience, making it easier to interact with AI models without complex command-line operations. Whether you’re a beginner or an advanced user, the right UI can streamline your workflow and unlock new possibilities with local AI&nbsp;models.</p><p>Start today with one of these web UIs and bring AI power to your local machine!&nbsp;🚀</p><p>Thank you for Reading&nbsp;!! 🙌🏻, don’t forget to subscribe and give it a&nbsp;CLAP</p><p><em>, cloud Solutions architect, Back-end developer, and AWS Community Builder, currently working at prodxcloud as a DevOps &amp; Cloud Architect. I bring a powerful combination of expertise in cloud architecture, DevOps practices, and a deep understanding of high availability (HA) principles. For more information about the author ( </em><a href=\"https://joelwembo.com/\"></a><a href=\"https://www.linkedin.com/in/joelotepawembo/\"></a><a href=\"https://github.com/joelwembo\"></a><a href=\"https://twitter.com/joelwembo1\"></a><a href=\"http://joelwembo.github.io/\"></a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d0dc2d09358\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Container Orchestration (AWS ECS, AWS EKS & Kubernetes)","url":"https://blog.devops.dev/understanding-container-orchestration-aws-ecs-aws-eks-kubernetes-baee401db009?source=rss----33f8b2d9a328---4","date":1739465455,"author":"Althaf Hussain","guid":796,"unread":true,"content":"<h3>Why Do We Need Container Orchestration?</h3><p>1️⃣ <strong>We use Docker to create and run containers</strong></p><ul><li>Docker  using a Dockerfile with :</li><li>: Packages and compiles the&nbsp;app.</li><li>: Runs the app and exposes&nbsp;ports.</li></ul><pre>docker run -p 80:80 my-app</pre><ul><li>Now the app is running inside a . 🎉</li></ul><p>2️⃣ <strong>But what if the app crashes due to high&nbsp;traffic?</strong></p><ul><li><strong>Docker cannot restart or scale the&nbsp;app</strong>.</li><li>If there’s high traffic (e.g., festive season sales), .</li></ul><p>3️⃣ <strong>Solution? We need a tool to manage containers automatically!</strong></p><ul><li>This is where <strong>Container Orchestration Tools</strong> come&nbsp;in!</li><li>Examples: <strong>Kubernetes, AWS ECS, AWS EKS, Azure AKS, Google GKE, OpenShift</strong>.</li></ul><h3>🚀 Kubernetes — Full Control but Complex&nbsp;Setup</h3><p>✅  When you want  over your cluster.✅ </p><ul><li><strong>Manages multiple containers</strong> (Docker is just for one container).</li><li> (if traffic increases, it adds more containers).</li><li> (if an app crashes, Kubernetes restarts&nbsp;it).</li></ul><h3>What is Kubernetes (Self-Managed)?</h3><p>If you want  over your cluster, you can <strong>set up Kubernetes manually</strong>.</p><h4>How Kubernetes Works (Practical Steps)</h4><p>1️⃣ Create a <strong>VM or server (EC2, Azure VM, GCP VM, on-premise server, etc.).</strong>2️⃣ Install <strong>Kubernetes, kubeadm, kubectl, networking, storage, etc.</strong>3️⃣ Set up a  and .4️⃣ Deploy your app using a .5️⃣ Manage <strong>scaling, auto-healing, networking, etc.</strong> manually.</p><h4>🛠️ Steps to Deploy an App Using Kubernetes:</h4><p>1️⃣ <strong>Set up a server (EC2 instance or&nbsp;VM)</strong></p><pre>sudo apt update &amp;&amp; sudo apt install -y curl apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -<p>echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list</p>sudo apt update<p>sudo apt install -y kubelet kubeadm kubectl</p></pre><p>2️⃣ <strong>Initialize Kubernetes cluster</strong></p><pre>mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<p>sudo chown $(id -u):$(id -g) $HOME/.kube/config</p></pre><pre>apiVersion: apps/v1kind: Deployment  name: my-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-app<p>        image: my-docker-image:latest</p>        ports:</pre><pre>kubectl apply -f deployment.yaml</pre><pre>apiVersion: v1kind: Service  name: my-app-service  type: LoadBalancer    app: my-app    - protocol: TCP      targetPort: 80</pre><pre>kubectl apply -f service.yaml</pre><p>🎯 <strong>Your app is now running inside Kubernetes!</strong> 🚀</p><h4>✅ Advantages of Self-Managed Kubernetes</h4><p>✔  → You can configure every part of the cluster.✔  → On-premise, AWS, Azure, GCP, or hybrid cloud.✔  → You’re not tied to AWS, Azure, or any provider.✔  → Most companies use  for flexibility.</p><h4>❌ Disadvantages of Self-Managed Kubernetes</h4><p>✖  → You need to manually configure <strong>networking, storage, security, etc.</strong>✖  → You have to <strong>patch, upgrade, and secure</strong> the cluster yourself.✖  → Setting up and managing Kubernetes is .</p><h3>🚀 AWS ECS — AWS Manages Everything</h3><p>✅  Running containers <strong>without managing Kubernetes</strong>.✅ </p><ul><li>You <strong>don’t need to set up Kubernetes</strong>.</li><li>Just  what app you want to run, and it does everything.</li><li> over cluster management.</li></ul><h3>🛠️ Steps to Deploy an App in AWS&nbsp;ECS</h3><p>1️⃣  in the AWS Console.2️⃣  (Choose Fargate or EC2).3️⃣ :</p><ul><li>Go to <strong>ECS &gt; Task Definitions &gt; Create new task definition</strong>.</li><li>Choose  (serverless) or  (self-managed).</li><li>Define  (Docker image, ports, CPU, memory).4️⃣ :</li><li>Go to <strong>ECS &gt; Services &gt; Create&nbsp;Service</strong>.</li><li>Choose the <strong>cluster and task definition</strong> you&nbsp;created.</li><li>Define  (number of tasks).5️⃣ &nbsp;🎉</li></ul><p>🎯 <strong>Your app is running inside AWS ECS without managing infrastructure!</strong> 🚀</p><p>✔  → AWS takes care of the infrastructure.✔  → No need to set up Kubernetes manually.✔ <strong>Tightly integrated with AWS</strong> → Works great with AWS services like ALB, IAM, CloudWatch, etc.✔ <strong>Less operational overhead</strong> → No need to worry about maintaining a&nbsp;cluster.</p><h4>❌ Disadvantages of AWS&nbsp;ECS</h4><p>✖  → If you want to move your app from AWS to , or , you have to <strong>set up everything from scratch</strong>.✖  → You don’t have full control over how the cluster is managed.✖  → Most companies prefer  over ECS for multi-cloud strategies.</p><h3>🚀 AWS EKS — AWS Manages Kubernetes for&nbsp;You</h3><p>✅  When you want <strong>Kubernetes but don’t want to install it manually</strong>.✅ </p><ul><li>AWS  (no need to install manually).</li><li>You  to deploy&nbsp;apps.</li><li> than ECS but  than DIY Kubernetes.</li></ul><h3>🛠️ Steps to Deploy an App in AWS&nbsp;EKS</h3><p>1️⃣  in the AWS Console.2️⃣ :</p><ul><li>Set <strong>Cluster name, VPC, IAM&nbsp;role</strong>.</li><li>AWS will create &amp; configure the Kubernetes control plane.3️⃣ </li></ul><pre>aws eks update-kubeconfig --region your-region --name your-cluster-name</pre><p>4️⃣  (same as Kubernetes DIY)</p><pre>kubectl apply -f deployment.yaml</pre><p>5️⃣ <strong>Expose the app using a Kubernetes service</strong> (same as&nbsp;before).</p><p>🎯 <strong>Your app is running in AWS EKS with Kubernetes, but AWS helps with the setup!</strong>&nbsp;🚀</p><p>✔  → Works exactly like Kubernetes, so it’s <strong>easy to move to another cloud</strong> (Azure AKS, Google GKE, etc.).✔ <strong>Fully managed control plane</strong> → AWS handles the  (setting up Kubernetes).✔  than ECS → You can tweak networking, security, and scaling.✔  → You can run Kubernetes anywhere (AWS, Azure, GCP, or on-premise).</p><h4>❌ Disadvantages of AWS&nbsp;EKS</h4><p>✖  → You still need to understand Kubernetes concepts.✖ <strong>More operational overhead</strong> → Though AWS sets up Kubernetes, you still .✖  → You  for the Kubernetes control&nbsp;plane.</p><h3>🎯 Real-World Example of How These Work&nbsp;Together</h3><p>Imagine you’re running an :1️⃣ You  to package your app into a container.2️⃣ You deploy it to <strong>Kubernetes (DIY) if you want full control</strong>.3️⃣ If you <strong>don’t want to manage Kubernetes</strong>, you use  (simplest).4️⃣ If you <strong>need Kubernetes but don’t want manual setup</strong>, you use .</p><p>📌 <strong>Think of Kubernetes as a powerful machine where you control everything.</strong>📌 <strong>Think of AWS ECS as a service where AWS does the heavy lifting for you.</strong>📌 <strong>Think of AWS EKS as Kubernetes, but AWS helps with&nbsp;setup.</strong></p><h3>Conclusion: Which One Should You&nbsp;Use?</h3><p>👉  if you  and don’t care about moving to another cloud.👉  if you  but don’t want to set it up manually.👉 <strong>Use Self-Managed Kubernetes</strong> if you  and <strong>plan to run across multiple clouds (AWS, Azure, GCP, on-premise, etc.).</strong></p><p>💡 If you’re , start with .If you’re building , go for .If you want , use .</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=baee401db009\" width=\"1\" height=\"1\" alt=\"\">","contentLength":6091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding APIs: A Developer’s Guide to Building and Using APIs","url":"https://blog.devops.dev/understanding-apis-a-developers-guide-to-building-and-using-apis-4253418d18ba?source=rss----33f8b2d9a328---4","date":1739465421,"author":"Subbareddysangham","guid":795,"unread":true,"content":"<p>An Application Programming Interface (API) acts as a bridge between different software applications, allowing them to communicate with each other. Think of an API like a waiter in a restaurant — customers (the client application) don’t need to know how the kitchen (the server) prepares their food; they need to know how to place their order through the waiter (the&nbsp;API).</p><p>I designed and developed an e-commerce web application with <strong>HTML, CSS, and JavaScript</strong> for the front end,  for the back end, and  for the database. I will use this as an example to explain the core concepts of&nbsp;APIs.</p><h3>E-commerce Web Application API Flow&nbsp;Chart:</h3><p><strong><em>To check the complete source&nbsp;code:</em></strong></p><h3>APIs in the E-Commerce Application</h3><p><strong>This E-Commerce application</strong> consists of the following API endpoints:</p><h4>1. Authentication API (auth_routes)</h4><ul><li>: /api/auth/login (POST) → Authenticates users and starts a&nbsp;session.</li><li>: /api/auth/logout (POST) → Clears session and logs out&nbsp;users.</li></ul><h4>2. Product API (product_routes)</h4><ul><li>: /api/products (GET) → Returns a list of products.</li><li>: /api/products/&lt;int:product_id&gt; (GET) → Fetches details of a specific&nbsp;product.</li></ul><h4>3. Cart API (cart_routes)</h4><ul><li>: /api/cart (GET) → Returns the current user's&nbsp;cart.</li><li>: /api/cart (POST) → Adds a product to the&nbsp;cart.</li><li>: /api/cart/&lt;int:item_id&gt; (DELETE) → Removes an item from the&nbsp;cart.</li></ul><h4>4. Order API (order_routes)</h4><ul><li>: /api/orders (POST) → Places an order with the items in the&nbsp;cart.</li><li>: /api/orders/&lt;int:order_id&gt; (GET) → Fetches details of a specific&nbsp;order.</li></ul><ul><li>: /api/health (GET) → Provides API uptime, session data, and frontend&nbsp;path.</li></ul><ul><li>: /&lt;path:filename&gt; (GET) → Serves frontend&nbsp;files.</li><li>: / (GET) → Serves index.html or API running&nbsp;message.</li></ul><ul><li>: Handles missing resources.</li><li><strong>500 Internal Server Error</strong>: Handles unexpected issues.</li><li>: Handles invalid requests.</li></ul><p>An API consists of several key components that work together:</p><ol><li>These are the URLs where the  can be accessed. Similar to a , each endpoint serves a specific purpose. For example,📌 <strong>https://api.ecommerce.com/products</strong> → Retrieves a list of available products.📌 <strong>https://api.ecommerce.com/cart</strong> → Fetches the current user's shopping cart details.📌 <strong>https://api.ecommerce.com/orders</strong> → Handles order-related operations.</li></ol><p>These actions can be performed on the allowed endpoints. They’re like verbs telling the API what to do with the&nbsp;data.</p><ul><li> → Read data (<strong>View products, orders, cart&nbsp;items</strong>).</li><li> → Create new data (<strong>Add product, register user, place&nbsp;order</strong>).</li><li> → Update existing data (<strong>Update profile, modify cart quantity</strong>).</li><li> → Remove data (<strong>Delete cart item, cancel&nbsp;order</strong>).</li></ul><p> Additional data is sent to fine-tune the API request, such as specifying which page of results you want to&nbsp;see.</p><ul><li>: Parameters are extra details added to an API request to filter or refine the&nbsp;results.</li><li>Fetch  of products:</li></ul><pre>GET /api/products?category=laptops</pre><p>Security measures ensure that only authorized users can access the&nbsp;API.</p><ul><li>: Ensures that only authorized users can access the&nbsp;API.</li></ul><p>When a user logs in, the API gives a&nbsp;:</p><pre>{  \"message\": \"Login successful\",<p>  \"token\": \"eyJhbGciOiJIUz...\"</p>}</pre><p>To <strong>add a product to the cart</strong>, the request must include this&nbsp;:</p><pre>POST /api/cart/addAuthorization: Bearer eyJhbGciOiJIUz...</pre><p> Prevents unauthorized access and protects user&nbsp;data.</p><p> The structure of the data returned by the API, commonly in formats like JSON or&nbsp;XML.</p><ul><li>: The structure of the data sent back by the&nbsp;API.</li><li> (because it’s easy to read and&nbsp;use).</li></ul><pre>{  \"id\": 1,  \"price\": 799.99,}</pre><p> Frontend uses this data to display products to&nbsp;users.</p><p>APIs are classified according to their usage patterns and architectures.</p><h3>API Types According to Purposes of&nbsp;Use</h3><p>🔹  — Used within a company, hidden from public access. Helps teams share data securely.</p><p>🔹  — Available to everyone, can be free or paid. Example: Google Maps&nbsp;API.</p><p>🔹  — Used between business partners for secure data exchange. Example: E-commerce &amp; shipping company integration.</p><p>🔹  — Combines multiple APIs into one request for efficiency. Example: Fetching account balance + transaction history in one&nbsp;call.</p><h3>API Types According to Architectural Structure:</h3><h3>1. Web APIs (HTTP/HTTPS APIs)</h3><p>These are the most common APIs, operating over the internet using HTTP protocols. They come in several varieties:</p><h3>1.1. REST (Representational State Transfer):</h3><p>The most popular type of web API today. REST APIs follow these principles:</p><ul><li>Stateless: Each request contains all the information needed</li><li>Resource-based: Everything is treated as a resource with a unique&nbsp;URL</li><li>Uses standard HTTP methods (GET, POST, PUT,&nbsp;DELETE)</li><li>Supports multiple data formats (usually&nbsp;JSON)</li></ul><h3><strong><em>Example REST API Request in an E-Commerce Web Application:</em></strong></h3><p>This request <strong>fetches all available products</strong> from the online&nbsp;store.</p><p>✅ <strong>Request (Client →&nbsp;Server)</strong></p><pre>GET /api/products HTTP/1.1Host: api.ecommerce.com<p>Authorization: Bearer &lt;User_Token&gt;</p>Content-Type: application/json</pre><p>✅ <strong>Response (Server →&nbsp;Client)</strong></p><pre>[    {\"id\": 1, \"name\": \"iPhone 15\", \"price\": 999.99, \"stock\": 20},<p>    {\"id\": 2, \"name\": \"Samsung Galaxy S24\", \"price\": 899.99, \"stock\": 15}</p>]</pre><h3><strong><em>catalog.html Fetches and Displays&nbsp;Products</em></strong></h3><h4>1️⃣ User Visits catalog.html</h4><ul><li>The user opens the  page in their browser (http://52.90.222.178:5000/catalog.html).</li><li>The browser  to fetch product&nbsp;data.</li></ul><h4>2️⃣ Frontend (JavaScript) Sends an API&nbsp;Request</h4><ul><li>JavaScript code in catalog.html makes a GET request to the  /api/products.</li></ul><h4>3️⃣ Backend API (GET /api/products) Fetches&nbsp;Data</h4><ul><li>The get_all_products() function runs when the frontend calls /api/products.</li></ul><h4>4️⃣ Database Retrieves Product Information</h4><ul><li>The backend queries the  in the&nbsp;database</li><li>Example database response:</li></ul><pre>[    {\"id\": 1, \"name\": \"Laptop\", \"price\": 799.99},<p>    {\"id\": 2, \"name\": \"Smartphone\", \"price\": 499.99}</p>]</pre><h4>5️⃣ Frontend Renders Product Data in catalog.html</h4><ul><li>The JavaScript loops through the  and dynamically  to display products.</li></ul><pre>&lt;div class=\"product-card\"&gt;  &lt;h3&gt;Laptop&lt;/h3&gt;  &lt;button onclick=\"addToCart(1)\"&gt;Add to Cart&lt;/button&gt;&lt;div class=\"product-card\"&gt;  &lt;p&gt;Price: $499.99&lt;/p&gt;<p>  &lt;button onclick=\"addToCart(2)\"&gt;Add to Cart&lt;/button&gt;</p>&lt;/div&gt;</pre><h3>1.2. SOAP (Simple Object Access Protocol):</h3><p>SOAP has strict rules and rigid messaging standards that can make it more secure than protocols such as REST. These types of APIs are frequently used in enterprise applications, particularly for payment processing and customer management, as they are highly safe in&nbsp;nature.</p><p>A more rigid, protocol-specific API style used in enterprise environments:</p><ul><li>Highly structured messaging</li></ul><pre>&lt;soap:Envelope&gt;  &lt;soap:Header&gt;<p>    &lt;Authorization&gt;Bearer abc123&lt;/Authorization&gt;</p>  &lt;/soap:Header&gt;    &lt;GetUser&gt;    &lt;/GetUser&gt;&lt;/soap:Envelope&gt;</pre><p>A modern API query language that gives clients more&nbsp;control:</p><ul><li>Clients specify precisely what data they&nbsp;need</li><li>Single endpoint for all&nbsp;requests</li><li>Reduces over-fetching and under-fetching of&nbsp;data</li></ul><p>Facebook initially developed GraphQL to simplify endpoint management for REST-based APIs. Instead of maintaining multiple endpoints with small amounts of disjointed data, GraphQL provides a single endpoint that inputs complex queries and outputs only as much information as is needed for the&nbsp;query.</p><pre>query {  user(id: \"123\") {    email      title  }</pre><p>These are programming interfaces provided by software libraries or frameworks:</p><ul><li>Used directly in your&nbsp;code</li><li>No network requests are&nbsp;needed</li><li>Usually specific to a programming language</li></ul><p>Example using a Python library&nbsp;API:</p><pre>import pandas as pd<p># Using pandas API to read a CSV file</p>df = pd.read_csv('data.csv')</pre><p>These allow applications to interact with the operating system:</p><ul></ul><p>Example using Python’s OS&nbsp;API:</p><pre>import os<p># Using OS API to create a directory</p>os.mkdir('new_folder')</pre><p>The foundation of web APIs, using well-defined methods and status&nbsp;codes:</p><ul><li>PUT: Update existing&nbsp;data</li><li>PATCH: Partially update&nbsp;data</li></ul><ul><li>2xx: Success (e.g., 200&nbsp;OK)</li></ul><p>Enables real-time, two-way communication:</p><ul><li>Ideal for chat apps and live&nbsp;updates</li></ul><p><strong><em>Example WebSocket connection:</em></strong></p><pre>const ws = new WebSocket('wss://api.example.com/chat');ws.onmessage = (event) =&gt; {<p>    console.log('Received:', event.data);</p>};</pre><p><strong>gRPC (Google Remote Procedure Call)</strong> is a  framework for <strong>inter-service communication</strong> in <strong>microservices architecture</strong>. Unlike REST APIs that use , gRPC uses <strong>Protocol Buffers (Protobuf)</strong>, making it <strong>faster and more efficient</strong>.</p><p>Google’s high-performance RPC framework:</p><ul><li>Excellent for microservices</li></ul><p><strong><em>Example Protocol Buffer definition:</em></strong></p><h3>📌 How gRPC Works in a Web Application</h3><p>In an , gRPC can be used for <strong>fast communication between microservices</strong>.</p><p>A  must fetch a  from the backend .</p><h4>1️⃣ Defining gRPC Service (product.proto)</h4><p>gRPC services use <strong>Protocol Buffers (Protobuf)</strong> to define API contracts.</p><ul><li>GetAllProducts(): Returns a list of products.</li><li>GetProductById(): Fetches a single product by&nbsp;ID.</li><li>Product: Defines the product structure.</li></ul><h4>2️⃣ Implementing gRPC Server (product_server.py)</h4><p>The gRPC server <strong>implements the service&nbsp;logic</strong>.</p><ul><li>Implements ProductService methods (GetAllProducts, GetProductById).</li></ul><h4>3️⃣ Implementing gRPC Client (product_client.py)</h4><p>The client  to fetch&nbsp;data.</p><ul><li>Calls GetAllProducts() to fetch all products.</li><li>Calls GetProductById() to fetch a single&nbsp;product.</li></ul><p>4️⃣ Running gRPC Server &amp;&nbsp;Client</p><pre># 1. Generate Python code from Protobufpython -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. product.protopython product_server.pypython product_client.py</pre><pre>ProductService gRPC Server is running on port 50051...Product List: products {  name: \"Laptop\"}  id: 2  price: 499.99</pre><h3>1. GET: Used to retrieve&nbsp;data</h3><p>It is a request used to retrieve data. Never used to delete, update or insert&nbsp;data.</p><ul></ul><h4>Product API in E-Commerce Web Application</h4><pre>curl -X GET http://52.90.222.178:5000/api/products</pre><pre>[   {\"id\": 1, \"name\": \"iPhone 15\", \"price\": 999.99},<p>   {\"id\": 2, \"name\": \"Samsung Galaxy S24\", \"price\": 899.99}</p>]</pre><p>If the API returns JSON, you can format the response using&nbsp;</p><pre>curl -X GET http://52.90.222.178:5000/api/products | jq</pre><h3>Debugging &amp; Troubleshooting</h3><p>If you’re not getting the expected response, check:</p><p><strong>Is the Flask server&nbsp;running?</strong></p><pre>curl -X GET http://52.90.222.178:5000/api/health</pre><h3>How the /api/health Endpoint Works in&nbsp;Flask</h3><p>The endpoint is a  that provides the current <strong>status of the application</strong>, including . It helps in monitoring the system and ensuring that the API is .</p><p>I have configured this /api/health endpoint in my E-Commerce Web Application.</p><pre>@app.route(\"/api/health\", methods=[\"GET\"])def health_check():<p>    uptime = time.time() - start_time</p>    return jsonify({        \"uptime\": f\"{uptime:.2f} seconds\",<p>        \"session_active\": \"username\" in session</p>    }), 200</pre><h3>2. POST: Creates new resources</h3><p>The  method is a request used to insert data. Posted data type — JSON.</p><ul></ul><p>✅ Authentication Endpoints</p><p>This is handled in auth_routes.py, prefixed with /api/auth.</p><p> → The frontend sends a request to the backend API (POST /api/auth/login). This is handled in auth_routes.py, prefixed with /api/auth.</p><ol><li>User sends a  with username &amp; password.</li><li>If credentials are&nbsp;valid:</li></ol><ul><li>The user session is&nbsp;stored.</li><li>API returns a success&nbsp;message.</li></ul><p>3. If credentials are&nbsp;invalid:</p><ul><li>API returns </li></ul><h3>3. PUT&nbsp;: Updates existing resources</h3><p>PUT method is used to create or update (replace) a resource. Useful for syncing&nbsp;data.</p><ul></ul><p>Ex: We can <strong>add a “Change Password” feature</strong> for an existing user using a <strong>PUT /api/auth/change-password</strong> API endpoint.</p><h3>Steps to Implement “Change Password” API</h3><ol><li><strong>PUT request with their old and new password.</strong></li></ol><pre>curl -X PUT http://localhost:5000/api/auth/change-password      -H \"Content-Type: app<p>     -d '{\"old_password\": \"currentPass123\", \"new_password\": \"newPass456\"}'</p></pre><p><strong>2. API verifies the old password</strong>:</p><ul><li>If incorrect, return an error (401 Unauthorized).</li></ul><p><strong>3. If correct, update the password in the database</strong>.</p><p><strong>4. Save the new password (after hashing it for security).</strong></p><p><strong>5. Return a success&nbsp;message.</strong></p><pre>{\"message\": \"Password changed successfully\"}</pre><h3><strong><em>In an e-commerce application like yours, a </em></strong><strong><em>PUT method would typically be used&nbsp;in:</em></strong></h3><ol><li> → PUT /api/auth/update-profile</li><li><strong>Updating Product Information (Admin)</strong> → PUT /api/products/&lt;product_id&gt;</li><li> → PUT /api/cart/&lt;cart_id&gt;</li><li> → PUT /api/orders/&lt;order_id&gt;</li></ol><h3><strong>4. DELETE: Removes resources</strong></h3><p>The DELETE method deletes the specified resource.</p><ul></ul><h4><strong><em>/api/cart API to Remove a Product from the&nbsp;Cart</em></strong></h4><p>1️⃣ Endpoint Definition (Flask&nbsp;API):</p><pre>@cart_bp.route('/cart', methods=['DELETE'])def remove_from_cart():    Remove a product from the cart for the logged-in user.<p>    Expects JSON payload: { product_id }.</p>    \"\"\"        user_id = session.get('user_id')            return jsonify({\"message\": \"User not authenticated\"}), 401        product_id = data.get('product_id')            return jsonify({\"message\": \"'product_id' is required\"}), 400<p>        connection = get_db_connection()</p>        cursor = connection.cursor()<p>        delete_query = \"DELETE FROM cart_items WHERE user_id = %s AND product_id = %s\"</p>        cursor.execute(delete_query, (user_id, product_id))            return jsonify({\"message\": \"Product not found in cart\"}), 404        return jsonify({\"message\": \"Product removed from cart successfully\"}), 200        return jsonify({\"message\": \"Failed to remove product from cart\", \"error\": str(e)}), 500        close_db_connection(connection)</pre><ol><li><strong>User sends a DELETE request</strong> with the product_id they want to&nbsp;remove.</li><li><strong>API verifies if the user is logged in</strong> (checks session['user_id']).</li><li> (if missing, returns 400 Bad Request).</li><li> to remove the product from the cart_items table.</li><li><strong>If the product does not exist</strong>, it returns 404 Not&nbsp;Found.</li><li>, it commits the transaction and returns a success message (200&nbsp;OK).</li><li><strong>Handles database errors and ensures the connection is&nbsp;closed.</strong></li></ol><p>3️⃣ Example API Request &amp; Response:</p><p>Now User wanted to delete iPhone 15 Pro from the&nbsp;cart:</p><ul><li>Click the  button under “iPhone 15&nbsp;Pro”.</li><li>The item should disappear, and the cart total should&nbsp;update.</li></ul><p>Run this command in the terminal:</p><pre>curl -X DELETE http://52.90.222.178:5000/api/cart      -H \"Content-Type: application/json\" <p>     -H \"Cookie: session=00068d4c-4b41-4e3e-8884-7389cabbb9b0\"</p>     -d '{\"product_id\": 4}'</pre><pre>{    \"message\": \"Product removed from cart successfully\"</pre><p>After deletion of that&nbsp;item:</p><h3>5. PATCH: Partially updates resources</h3><p>PATCH method is to request used to update data. Only passed data will be updated. You don’t need to provide all the data&nbsp;set.</p><ul></ul><p>The  method is used to  a resource. Instead of sending the entire data set, we <strong>only send the fields that need to be&nbsp;updated</strong>.</p><h4>Use Case: Updating a User’s Profile (PATCH /api/auth/update-profile)</h4><p>Imagine a user wants to update  or  without changing their username.</p><p>1️⃣ PATCH Endpoint: PATCH /api/auth/update-profile</p><p>2️⃣ Sending a PATCH&nbsp;Request</p><p>If the user wants to update </p><pre>curl -X PATCH http://52.90.222.178:5000/api/auth/update-profile \\     -H \"Content-Type: application/json\" \\<p>     -H \"Cookie: session=your_valid_session_id\" \\</p>     -d '{\"email\": \"newemail@example.com\"}'</pre><p>🔹 Only the  field will be&nbsp;updated.</p><p>✅ </p><pre>{    \"message\": \"Profile updated successfully\"</pre><p>❌ <strong>If no fields are provided:</strong></p><pre>{    \"message\": \"No valid fields provided for update\"</pre><pre>{    \"message\": \"User not authenticated\"</pre><h4>4️⃣ Why Use PATCH Instead of&nbsp;PUT?</h4><h3>Conclusion: Understanding APIs, Endpoints, and Methods in Web Development</h3><p>APIs (Application Programming Interfaces) allow different systems to  with each other. They define how requests and responses are exchanged between a client (browser, app) and a&nbsp;server.</p><ul><li>An  acts as a bridge between two applications, enabling data exchange.</li><li>Example: A shopping website uses an API to fetch product details from a database.</li></ul><ul><li>An  is a URL that clients use to request or send&nbsp;data.</li><li>Example: GET /api/products retrieves all products.</li></ul><ul><li> → Uses HTTP methods (GET, POST, PUT, DELETE) to manage&nbsp;data.</li><li> → Lets clients request specific data fields, reducing unnecessary data transfer.</li><li> → Uses XML messaging, mainly in enterprise applications.</li><li> → Maintains a continuous connection for real-time updates (e.g., live&nbsp;chat).</li></ul><ul><li>Use  (JWT, API Keys, OAuth) to restrict&nbsp;access.</li><li>Protect sensitive data with .</li><li>Implement  to prevent&nbsp;abuse.</li></ul><p>APIs are the backbone of modern applications, enabling data sharing between different services. Developers create smooth and efficient digital experiences by designing well-structured and secure&nbsp;APIs.</p><p><em>I’d love to hear what you think about this article — feel free to share your opinions in the comments below (or above, depending on your device!). If you found this helpful or enjoyable, a clap, a comment, or a highlight of your favourite sections would mean a&nbsp;lot.</em></p><p><em>For more insights into the world of technology and data, visit </em><a href=\"http://www.subbutechops.com/\"></a><em> There’s plenty of exciting content waiting for you to&nbsp;explore!</em></p><p><em>Thank you for reading, and happy learning! 🚀</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4253418d18ba\" width=\"1\" height=\"1\" alt=\"\">","contentLength":16408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Most Developers Get This Wrong in Docker Networking!","url":"https://blog.devops.dev/most-developers-get-this-wrong-in-docker-networking-359dbb3eac16?source=rss----33f8b2d9a328---4","date":1739465415,"author":"Gaddam.Naveen","guid":794,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jenkins in Kubernetes: Deployment and Persistent Storage(volume) Setup","url":"https://blog.devops.dev/jenkins-in-kubernetes-deployment-and-persistent-storage-volume-setup-a70fe0579ac8?source=rss----33f8b2d9a328---4","date":1739465410,"author":"th@n@n","guid":793,"unread":true,"content":"<p>Jenkins, a popular automation server, becomes even more powerful when deployed in Kubernetes. Ensuring its availability and data persistence is crucial for uninterrupted CI/CD pipelines. In this guide, we’ll walk through deploying Jenkins in Kubernetes, configuring its resources, and setting up persistent storage to safeguard critical&nbsp;data.</p><p>In this configuration, we have a Deployment resource for deploying Jenkins in Kubernetes, along with associated PersistentVolumeClaim (PVC), PersistentVolume (PV), Service, and StorageClass resources. Let’s break down each&nbsp;part</p><pre>kind: StorageClassapiVersion: storage.k8s.io/v1  name: localstorage<p>provisioner: kubernetes.io/no-provisioner</p>volumeBindingMode: WaitForFirstConsumer</pre><ul><li>This StorageClass resource defines storage provisioning and management policies.</li><li>Since provisioner is set to kubernetes.io/no-provisioner, it indicates that no dynamic provisioning is performed by Kubernetes.</li><li>volumeBindingMode: WaitForFirstConsumer ensures that volume binding waits for the first Pod using the PersistentVolumeClaim to be&nbsp;created.</li></ul><h3>PersistentVolumeClaim (PVC) Resource:</h3><pre>apiVersion: v1kind: PersistentVolumeClaim  name: pvc-jenkinsspec:<p>  storageClassName: localstorage</p>  accessModes:  resources:      storage: 2Gi</pre><ul><li>This PVC resource requests storage from a PersistentVolume using the localstorage StorageClass.</li><li>It requests 2Gi of storage with access mode ReadWriteOnce, meaning it can be mounted as read-write by a single&nbsp;node.</li></ul><h3>PersistentVolume (PV) Resource:</h3><pre>apiVersion: v1kind: PersistentVolume  name: pv-jenkins    type: local  claimRef:    namespace: jenkins    storage: 3Gi    - ReadWriteOnce    path: /mnt<p>  storageClassName: localstorage</p></pre><ul><li>This PV resource represents the actual storage volume available for use by the&nbsp;PVC.</li><li>It is bound to the PVC pvc-jenkins within the jenkins namespace.</li><li>The PV has a capacity of 3Gi and is accessible in ReadWriteOnce mode.</li><li>The storage is provided by a hostPath /mnt on the host machine, with storage class localstorage.</li></ul><pre>apiVersion: apps/v1kind: Deployment  name: jenkins-deployment    name: jenkinsspec:    matchLabels:  replicas: 1    metadata:      labels:    spec:        - name: deployment-jenkins<p>          image: jenkins/jenkins:lts</p>          resources:              memory: \"0.5Gi\"            requests:              cpu: \"125m\"            - name: http-port            - name: jnlp-port          livenessProbe:              path: \"/login\"            initialDelaySeconds: 60            timeoutSeconds: 5          readinessProbe:              path: \"/login\"            initialDelaySeconds: 60            timeoutSeconds: 5          volumeMounts:              mountPath: /var/jenkins_home        - name: data-jenkins            claimName: pvc-jenkins        runAsUser: 0        fsGroup: 0</pre><ul><li>This Deployment resource defines how Jenkins is deployed.</li><li>It specifies a single replica (replicas: 1) of the Jenkins container.</li><li>The container is based on the jenkins/jenkins:lts image.</li><li>Resource limits and requests for CPU and memory are set to ensure resource allocation.</li><li>Ports 8080 and 50000 are exposed for HTTP and JNLP respectively.</li><li>Liveness and readiness probes are configured to check the health of the container.</li><li>The Jenkins home directory (/var/jenkins_home) is mounted to a PersistentVolumeClaim (pvc-jenkins) named data-jenkins.</li><li>SecurityContext ensures that Jenkins runs with the appropriate user and group permissions.</li></ul><pre> securityContext:        runAsUser: 0        fsGroup: 0</pre><p>When you deploy this deployment instance in kubernetes cluster, make sure the user have the right privileges to read and write the host volume For this demo purpose, I am using the root user to do this task, but this is not encouraged to do in real environment.</p><pre>apiVersion: v1kind: Service  name: jenkins-servicespec:    app: jenkins-pod  ports:      port: 8080      nodePort: 32000</pre><ul><li>This Service resource exposes the Jenkins deployment internally within the jenkins namespace.</li><li>It selects pods with the label app: jenkins-pod.</li><li>The service type is NodePort, making the service accessible from outside the cluster on each node's IP at a static port (nodePort: 32000).</li><li>Port 8080 is mapped to the targetPort 8080 where Jenkins is listening.</li></ul><p>Once you execute all the manifiest file in kubernetes cluster.</p><p>Check the host volume path ls -al&nbsp;/mnt</p><p>Execute the below command to see the whether the same files are present in the jenkins container</p><pre>kubectl exec -it POD_NAME /bin/bash -n jenkinsls -al /var/jenkins_home</pre><p>This configuration sets up Jenkins deployment in Kubernetes with persistence using a PersistentVolume and PersistentVolumeClaim. It ensures that Jenkins data stored in /var/jenkins_home persists across container restarts and pod rescheduling. Additionally, the Service resource exposes Jenkins for external access within the Kubernetes cluster.</p><p>For now, that’s it guys, If you like this article don’t forget to give a clap.&nbsp;👏</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a70fe0579ac8\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Build and Deploy a Simple Frontend App with Python Backend","url":"https://blog.devops.dev/how-to-build-and-deploy-a-simple-frontend-app-with-python-backend-108b505be2be?source=rss----33f8b2d9a328---4","date":1739465406,"author":"krth1k","guid":792,"unread":true,"content":"<p>Building a full-stack web application might seem daunting, especially if you’re primarily a backend developer. However, with the right approach, you can create a simple frontend and connect it to a Python backend with&nbsp;ease.</p><p>In this guide, we’ll walk through the process&nbsp;of:</p><ul><li>Setting up a basic Python backend with&nbsp;Flask</li><li>Creating a simple frontend with HTML, CSS, and JavaScript</li><li>Connecting the frontend to the backend using REST&nbsp;API</li><li>Deploying the app on a local Kubernetes cluster</li></ul><h3>1. Setting Up the Python&nbsp;Backend</h3><p>We’ll use , a lightweight Python web framework, to create a REST API that serves data to the frontend.</p><p>Ensure you have Python installed, then install&nbsp;Flask:</p><p>Create a new file called&nbsp;app.py:</p><pre>from flask import Flask, jsonify</pre><pre>@app.route('/api/message')def get_message():<p>    return jsonify({\"message\": \"Hello from the Python backend!\"})</p></pre><pre>if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)</pre><p>This API exposes a single endpoint /api/message that returns a JSON response.</p><p>For the frontend, we’ll use <strong>HTML, CSS, and JavaScript</strong> to display the data from our&nbsp;backend.</p><h3>Create an HTML File (index.html)</h3><pre>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;    &lt;meta charset=\"UTF-8\"&gt;<p>    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;</p>    &lt;title&gt;Frontend App&lt;/title&gt;        body {<p>            font-family: Arial, sans-serif;</p>            text-align: center;        }            padding: 10px 20px;        }&lt;/head&gt;    &lt;h1&gt;Simple Frontend App&lt;/h1&gt;<p>    &lt;button onclick=\"fetchMessage()\"&gt;Get Message&lt;/button&gt;</p>    &lt;p id=\"message\"&gt;&lt;/p&gt;        function fetchMessage() {<p>            fetch('http://127.0.0.1:5000/api/message')</p>                .then(response =&gt; response.json())                    document.getElementById(\"message\").innerText = data.message;                .catch(error =&gt; console.error('Error:', error));    &lt;/script&gt;&lt;/html&gt;</pre><p>This page has a button that fetches and displays a message from the Flask&nbsp;backend.</p><h3>3. Connecting the Frontend to the&nbsp;Backend</h3><p>Now, let’s serve the frontend using  itself so that both frontend and backend are accessible from the same&nbsp;origin.</p><h3>Update app.py to Serve&nbsp;HTML</h3><p>Modify app.py to serve the index.html file:</p><pre>from flask import Flask, jsonify, send_from_directory</pre><pre>app = Flask(__name__, static_folder='static')</pre><pre>@app.route('/api/message')def get_message():<p>    return jsonify({\"message\": \"Hello from the Python backend!\"})</p></pre><pre>@app.route('/')def serve_frontend():<p>    return send_from_directory('static', 'index.html')</p></pre><pre>if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)</pre><h3>Move index.html to a static&nbsp;Folder</h3><p>Your project structure should now look like&nbsp;this:</p><pre>/project-folder│-- app.py│   └── index.html</pre><p>Now, visit http://127.0.0.1:5000/ in your browser, and your frontend will be&nbsp;served!</p><p>Now, let’s deploy this app using .</p><pre># Use Python base imageFROM python:3.9</pre><pre># Set the working directoryWORKDIR /app</pre><pre># Copy application filesCOPY . .</pre><pre># Install dependenciesRUN pip install flask</pre><pre># Expose port 5000EXPOSE 5000</pre><pre># Run the appCMD [\"python\", \"app.py\"]</pre><h3>Build and Run the Docker Container</h3><pre>docker build -t myapp .docker run -p 5000:5000 myapp</pre><ol><li><strong>Create a Kubernetes Deployment YAML (</strong></li></ol><pre>apiVersion: apps/v1kind: Deployment  name: myapp  replicas: 1    matchLabels:  template:      labels:    spec:        - name: myapp          ports:---kind: Service  name: myapp-service  selector:  ports:      port: 80  type: NodePort</pre><pre>kubectl apply -f deployment.yaml</pre><pre>minikube service myapp-service --url</pre><p>Visit the displayed URL in your&nbsp;browser!</p><p>In this tutorial, we covered: ✅ Creating a Flask backend<p> ✅ Building a simple HTML/JavaScript frontend</p> ✅ Connecting the frontend to the backend<p> ✅ Deploying the app with Docker and Kubernetes</p></p><p>This is a basic example, but you can expand it&nbsp;by:</p><ul><li>Adding user authentication</li><li>Using React or Vue.js for a modern&nbsp;frontend</li><li>Storing and retrieving data from a&nbsp;database</li></ul><p>If you found this helpful, let me know in the comments! 🚀</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=108b505be2be\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Practices For Database Authorization In Multi-Tenant Systems","url":"https://blog.devops.dev/best-practices-for-database-authorization-in-multi-tenant-systems-001a1bcf2568?source=rss----33f8b2d9a328---4","date":1739465383,"author":"Noel","guid":791,"unread":true,"content":"<p>Multi-tenant databases allow multiple companies or organizations (tenants) to securely share the same database infrastructure while ensuring data isolation and integrity. However, this shared structure introduces complexities in managing access and authorization. A robust authorization strategy is essential to ensure that users can only access resources belonging to their tenant without compromising scalability or performance.</p><p>This article explores the best practices and technical solutions that we adopt at <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Best+Practices+For+Database+Authorization+In+Multi-Tenant+Systems\">Xinthe</a>, for implementing efficient authorization mechanisms in multi-tenant systems, with a focus on nested resource structures, such as companies, clients, projects, and tasks. After having read this entire article, you will be armed with actionable insights to build secure, efficient, and future-proof authorization strategies for multi-tenant applications.</p><h3><strong>Understanding Multi-Tenant Database Authorization</strong></h3><ul><li><strong><em>Definition Of Multi-Tenancy</em></strong></li></ul><p>Multi-tenancy refers to an architectural pattern where a single instance of a software application and its database serves multiple tenants (e.g., companies, organizations, or users). Each tenant’s data remains logically isolated, ensuring that no tenant can access another’s data, while sharing underlying resources for efficiency.</p><p><strong>Key Multi-Tenancy Models:</strong></p><p>Each tenant has a dedicated database or&nbsp;schema.</p><p>Offers strong isolation and security.</p><p>Higher costs and maintenance complexity due to multiple instances.</p><p>Multiple tenants share the same database.</p><p>Logical separation is maintained through identifiers (e.g., tenant_id or company_id).</p><p>Cost-effective and scalable but requires robust authorization mechanisms.</p><ul><li><strong><em>Challenges Of Authorization</em></strong></li></ul><p>Implementing authorization in multi-tenant systems is a non-trivial task, especially as the scale and complexity of resources grow. Common challenges include&nbsp;-</p><p><strong>Cross-Tenant Data&nbsp;Leakage:</strong></p><p>Risk: Improper queries or configurations can expose data to unauthorized tenants.</p><p>Example: A user from Company A inadvertently accessing tasks belonging to Company B due to a missing or incorrect WHERE&nbsp;clause.</p><p>Deeply nested resource structures often require joins across multiple&nbsp;tables.</p><p>Queries with extensive joins can degrade performance as data volume increases.</p><p><strong>Scalability &amp; Maintainability:</strong></p><p>The need to balance fast access controls with a maintainable schema.</p><p>Adding new authorization rules or resource types without overhauling the&nbsp;system.</p><p><strong>Data Localization &amp; Compliance:</strong></p><p>For multi-tenant systems spanning regions, ensuring that tenant data complies with regulations like GDPR can complicate authorization logic.</p><ul><li><strong><em>Importance Of Nested Resource Structures</em></strong></li></ul><p>In many applications, resources are interconnected in a hierarchical fashion. Consider the following nested structure -</p><p><strong>Company → Client → Project →&nbsp;Task</strong></p><p>A  has multiple&nbsp;.</p><p>Each  manages several .</p><p>Each  contains multiple&nbsp;.</p><p><strong>Why Nested Structures Matter:</strong></p><p><strong>Access Control Complexity:</strong> Permissions must flow through the hierarchy (e.g., a user’s access to a task must be verified against their company).</p><p> Hierarchical access often necessitates multiple joins, impacting query efficiency.</p><p> Hierarchical structures reflect real-world use cases like SaaS platforms, where users must operate within their organization’s boundaries.</p><p>A user from Company A should only edit tasks within their projects. Authorization must ensure that the task → project → client → company linkage is maintained without exposing data from Company&nbsp;B.</p><h3><strong>Comparing Authorization Approaches</strong></h3><p>When implementing authorization in a multi-tenant database, there are three common strategies to choose from: the , the , and <strong>Tenant-Specific Databases or Tables</strong>. Each comes with its own set of benefits and trade-offs. Let’s break them down&nbsp;-</p><ul><li><strong><em>1. Flat Model (Adding Tenant&nbsp;IDs)</em></strong></li></ul><p>In this approach, a tenant_id or company_id is added to every resource table (e.g., tasks, projects, clients), enabling direct filtering for authorization.</p><p> Queries can directly filter by tenant_id without traversing the hierarchy.</p><pre>SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;</pre><p> Reduces query complexity by avoiding multiple table joins to enforce&nbsp;access.</p><p> Straightforward implementation makes it easy to debug and maintain.</p><p> tenant_id is replicated across multiple tables, introducing redundancy.</p><p> Adding tenant_id and other metadata can lead to bloated schemas, especially as the number of attributes grows.</p><p> Schema updates (e.g., adding new relationships) might require extensive changes across multiple&nbsp;tables.</p><p>Ideal for systems where performance is critical and the schema is relatively stable, such as SaaS platforms with many small&nbsp;tenants.</p><ul><li><strong><em>2. Hierarchical Model (Enforcing Relationships)</em></strong></li></ul><p>In this approach, the relationships between resources (e.g., task → project → client → company) are strictly enforced through foreign keys. Authorization is achieved by traversing the hierarchy.</p><p> Avoids redundant fields by relying on inherent relationships.</p><pre>CREATE TABLE tasks (    id SERIAL PRIMARY KEY,<p>    project_id INT REFERENCES projects(id),</p>    ...    id SERIAL PRIMARY KEY,<p>    client_id INT REFERENCES clients(id),</p>    ...    id SERIAL PRIMARY KEY,<p>    company_id INT REFERENCES companies(id),</p>    ...</pre><p> Reduces duplication of metadata like tenant_id.</p><p><strong>Relationship-Centric Queries:</strong> Makes it easier to enforce hierarchical constraints and maintain referential integrity.</p><p> Queries require multiple joins to verify access, which can impact performance.</p><pre>SELECT t.*FROM tasks t<p>JOIN projects p ON t.project_id = p.id</p>JOIN clients c ON p.client_id = c.id<p>WHERE c.company_id = :tenant_id AND t.id = :task_id;</p></pre><p> Deep hierarchies with large datasets can significantly increase query execution time.</p><p> As the hierarchy grows, maintaining performance becomes challenging.</p><p>Suitable for applications where maintaining strict relationships between resources is essential, such as ERP systems or large enterprise applications.</p><ul><li><strong><em>3. Tenant-Specific Databases Or&nbsp;Tables</em></strong></li></ul><p>This approach creates separate databases or tables for each tenant, isolating their data entirely.</p><p> Each tenant’s data can be managed independently, making it easier to scale horizontally by distributing databases across&nbsp;servers.</p><p> Ensures complete data isolation, reducing the risk of cross-tenant data&nbsp;leakage.</p><p> Simplifies adherence to regulations like GDPR by enabling tenant-specific backups, retention policies, and deletions.</p><p> Managing multiple databases or schemas requires sophisticated deployment and CI/CD pipelines.</p><p> Schema updates need to be applied consistently across all tenant databases.</p><p> For tenants with small datasets, the resource consumption of separate databases might be inefficient.</p><p>Best for large organizations with high regulatory or security requirements, or when dealing with tenants that require dedicated resources (e.g., enterprise customers).</p><ul><li><strong><em>Summary Table — Comparing Approaches</em></strong></li></ul><h3><strong>Criteria For Choosing An Authorization Model</strong></h3><p>Selecting the right authorization model for a multi-tenant database is critical for ensuring scalability, performance, and compliance. The decision hinges on a combination of technical, regulatory, and operational factors. Below are the primary criteria to consider&nbsp;-</p><p>The level of traffic and query complexity your application handles directly impacts the choice of an authorization model.</p><p><strong>High-Traffic Applications</strong>:</p><p>Benefit from simpler and faster queries, such as those enabled by the .</p><pre>SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;</pre><p>Minimal joins mean lower query latency, ensuring the system performs well under heavy&nbsp;loads.</p><p>Suitable for SaaS platforms or e-commerce systems with a high volume of tenant interactions.</p><p>:</p><p>Can afford the  with more joins, as performance trade-offs are less significant.</p><p>Allows for cleaner schema designs and strict relational integrity.</p><p>Suitable for internal enterprise tools or smaller-scale applications.</p><ul><li><strong><em>2. Regulatory Requirements</em></strong></li></ul><p>Compliance with data protection and privacy regulations often dictates how data is stored and accessed.</p><p>Using <strong>Tenant-Specific Databases or Tables</strong> simplifies compliance for regulations like GDPR or&nbsp;HIPAA.</p><p>Tenant isolation reduces the risk of data leakage and ensures tenant-specific data retention and deletion policies.</p><p>An enterprise customer requires dedicated storage with separate backups and audit&nbsp;logs.</p><p>A  can still meet compliance needs with appropriate access controls and audit mechanisms.</p><p>Challenges arise in managing and enforcing tenant-specific data governance policies within shared infrastructure.</p><p>The ability to handle growth in the number of tenants and data volumes is a critical&nbsp;factor.</p><p><strong>Planning For Tenant&nbsp;Growth</strong>:</p><p>For a rapidly scaling user base, <strong>Tenant-Specific Databases or Tables</strong> provide the most flexibility -</p><p>Each tenant can be distributed across servers to balance&nbsp;load.</p><p>Tenant databases can be independently scaled based on specific&nbsp;needs.</p><p>A B2B SaaS platform serving both small businesses and large enterprises can allocate resources dynamically based on tenant&nbsp;size.</p><p>The  can handle larger datasets more efficiently as indexes on tenant_id make filtering faster.</p><p>The  may struggle as table sizes grow, requiring optimization for complex&nbsp;joins.</p><p>Ease of schema management and updates is essential for long-term maintainability.</p><p><strong>Simplified Schema&nbsp;Updates</strong>:</p><p>The  simplifies schema updates by centralizing data attributes like tenant_id.</p><p>However, redundant fields may increase the risk of errors during&nbsp;updates.</p><p>The  enforces relational integrity, ensuring data consistency.</p><p>Complex queries for nested structures may require more effort to maintain and optimize.</p><p><strong>Automated CI/CD Pipelines</strong>:</p><p>For <strong>Tenant-Specific Databases</strong>, CI/CD automation becomes critical to manage schema changes across multiple databases.</p><p>Tools like Octopus Deploy or Liquibase can help automate schema migrations and ensure consistency.</p><ul><li><strong><em>Key Considerations Summary</em></strong></li></ul><h3><strong>Designing Authorization Strategies For Multi-Tenancy</strong></h3><p>Designing robust authorization strategies for multi-tenant systems requires careful consideration of schema design, indexing, and data partitioning to ensure scalability, security, and performance. This section outlines best practices for implementing these strategies effectively.</p><p>The foundation of a successful multi-tenant authorization system lies in a well-thought-out schema.</p><p>Add a tenant_id column to all relevant tables (e.g., clients, projects, tasks) for direct tenant filtering.</p><pre>CREATE TABLE tasks (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    FOREIGN KEY (project_id) REFERENCES projects(id)</p>);</pre><p>Ensure tenant_id is a mandatory field in all write operations to enforce multi-tenancy constraints.</p><p><strong>Defining Relationships In Hierarchical Structures</strong>:</p><p>Maintain strict referential integrity between hierarchical entities.</p><p>Example for hierarchical relationships -</p><pre>CREATE TABLE projects (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    client_id BIGINT NOT NULL,    FOREIGN KEY (client_id) REFERENCES clients(id)</pre><p>Flat schema enables quick lookups for tenant-specific data.</p><p>Hierarchical relationships ensure data consistency and logical separation.</p><ul><li><strong><em>2. Indexing Best Practices</em></strong></li></ul><p>Indexes are essential for optimizing queries in multi-tenant systems. However, improper indexing can lead to inefficiencies.</p><p><strong>Compound Indexes For Tenant-Specific Queries</strong>:</p><p>Use composite indexes combining tenant_id with frequently queried&nbsp;columns.</p><pre>CREATE INDEX idx_tasks_tenant_projectON tasks (tenant_id, project_id, status);</pre><p>This enables efficient filtering and sorting within a tenant’s&nbsp;scope.</p><p><strong>Balancing Indexing Depth &amp; Query&nbsp;Speed</strong>:</p><p>Avoid over-indexing, which can slow down write operations.</p><p>Prioritize indexing columns involved in filtering, joining, and sorting operations.</p><p>Regularly analyze query performance using tools like  in PostgreSQL or  in&nbsp;MySQL.</p><p>Partitioning improves scalability by dividing data into smaller, more manageable segments, reducing query times for tenant-specific operations.</p><p><strong>Horizontal Partitioning By&nbsp;Tenants</strong>:</p><p>Partition data within a single database based on tenant_id.</p><pre>CREATE TABLE tasks_1 PARTITION OF tasksFOR VALUES IN (1); -- Partition for tenant_id 1</pre><p>Faster tenant-specific queries as partitions reduce the search&nbsp;space.</p><p>Simplifies maintenance for large datasets.</p><p><strong>Database Sharding For High-Scale Systems</strong>:</p><p>Distribute tenant data across multiple databases (shards).</p><p>Example Sharding Strategy&nbsp;-</p><p>Use tenant_id % shard_count to assign tenants to&nbsp;shards.</p><p>Tools like  or  can manage sharding in distributed database&nbsp;systems.</p><p>Eliminates contention in single-database systems.</p><p>Enhances fault isolation and scalability.</p><p><strong>Example Use Case — Applying These Strategies</strong></p><p>An application manages 100,000 tenants, each with thousands of projects and&nbsp;tasks.</p><p>Add tenant_id to all&nbsp;tables.</p><p>Use foreign keys to link tasks → projects →&nbsp;clients.</p><p>Create a compound index on tasks (tenant_id, project_id) for common queries like&nbsp;-</p><pre>SELECT * FROM tasks WHERE tenant_id = 123 AND project_id = 456;</pre><p>For smaller tenants, use horizontal partitioning -</p><pre>CREATE TABLE tasks_tenant_123 PARTITION OF tasks FOR VALUES IN (123);</pre><p>For larger tenants, shard data across multiple databases to&nbsp;scale.</p><p>A well-designed schema with tenant_id simplifies multi-tenant data filtering.</p><p>Proper indexing ensures efficient queries, even at&nbsp;scale.</p><p>Partitioning and sharding prepare the system for growth, reducing query times and enhancing reliability.</p><p>This section provides concrete examples of implementing different authorization models for multi-tenant systems, including schemas, queries, and tooling. Each approach demonstrates how to enforce tenant-specific access effectively.</p><ul><li><strong><em>1. Flat Model Implementation</em></strong></li></ul><p>The flat model relies on adding a tenant_id column to all relevant tables, ensuring that queries are scoped to the tenant directly.</p><pre>CREATE TABLE tasks (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,</p>    FOREIGN KEY (project_id) REFERENCES projects(id)    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    client_id BIGINT NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,<p>    FOREIGN KEY (client_id) REFERENCES clients(id)</p>);</pre><p>: Access tasks for a user’s company&nbsp;-</p><pre>SELECT * FROM tasks <p>WHERE tenant_id = 123 AND status = 'in_progress';</p></pre><p>Simplifies authorization logic with direct&nbsp;lookups.</p><p>Reduces query complexity by avoiding&nbsp;joins.</p><p>Potential schema bloat with additional tenant_id columns.</p><ul><li><strong><em>2. Hierarchical Model Implementation</em></strong></li></ul><p>In this model, tenant authorization is enforced by traversing relationships between resources (e.g., Company → Client → Project →&nbsp;Task).</p><pre>CREATE TABLE companies (    id BIGINT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT PRIMARY KEY,<p>    company_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    FOREIGN KEY (company_id) REFERENCES companies(id)</p>);    id BIGINT PRIMARY KEY,<p>    client_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    FOREIGN KEY (client_id) REFERENCES clients(id)</p>);    id BIGINT PRIMARY KEY,<p>    project_id BIGINT NOT NULL,</p>    description TEXT,    FOREIGN KEY (project_id) REFERENCES projects(id)</pre><p>: Check task access by traversing relationships -</p><pre>SELECT t.* FROM tasks t<p>INNER JOIN projects p ON t.project_id = p.id</p>INNER JOIN clients c ON p.client_id = c.id<p>INNER JOIN companies co ON c.company_id = co.id</p>WHERE co.id = 123 AND t.status = 'in_progress';</pre><p>Maintains normalized relationships.</p><p>Avoids redundant tenant_id columns.</p><p>Complex joins increase query&nbsp;costs.</p><p>Requires optimized indexes to maintain performance.</p><ul><li><strong><em>3. Tenant-Specific Database/Table Implementation</em></strong></li></ul><p>For scenarios requiring strict isolation, separate databases or tables for each tenant can be&nbsp;used.</p><p>Create a separate database or schema for each tenant&nbsp;-</p><pre>CREATE DATABASE company_123;CREATE TABLE company_123.tasks (    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><p>: Access tasks for a specific tenant&nbsp;-</p><pre>USE company_123;SELECT * WHERE status = 'in_progress';</pre><p>: Use CI/CD tools like  to manage multi-tenant databases:</p><p>Automate schema changes across databases.</p><p>Track versioning for each tenant’s database.</p><p>Complete tenant isolation for security and compliance (e.g.,&nbsp;GDPR).</p><p>Simplifies data archival and backup for individual tenants.</p><p>Deployment complexity increases with the number of&nbsp;tenants.</p><p>Resource-intensive for systems with many small&nbsp;tenants.</p><ul><li><strong><em>Choosing The Right Implementation</em></strong></li></ul><p>Use the flat model for simplicity in high-traffic environments.</p><p>Use the hierarchical model when data relationships must be preserved and redundancy minimized.</p><p>Opt for tenant-specific databases for strict isolation and compliance requirements.</p><p>Each implementation can be tailored based on application needs, tenant size, and regulatory requirements. Balancing performance, scalability, and maintainability is key to successful multi-tenant authorization systems.</p><h3><strong>Security Best Practices For Authorization</strong></h3><p>Ensuring robust security in multi-tenant systems is essential to prevent data breaches, maintain compliance, and build user trust. This section outlines key practices for implementing secure and reliable authorization mechanisms.</p><ul><li><strong><em>1. Strict Access&nbsp;Controls</em></strong></li></ul><p>Implementing strong access controls ensures that only authorized users can access or modify resources.</p><p><strong>Role-Based Access Control&nbsp;(RBAC)</strong>:</p><p>Assign roles (e.g., Admin, Manager, User) to users based on their responsibilities.</p><p>Enforce role-specific permissions at the application and database&nbsp;layers.</p><p>Example: Use database roles to restrict access to tenant-specific tables.</p><pre>CREATE ROLE company_admin;GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO company_admin;<p>REVOKE ALL ON ALL TABLES FROM PUBLIC; -- Restrict public access</p></pre><p>:</p><p>Enforce tenant-level data segregation directly at the database&nbsp;layer.</p><p>RLS ensures that queries automatically filter data based on the user’s&nbsp;tenant.</p><p><strong>PostgreSQL Example For&nbsp;RLS</strong>:</p><pre>CREATE POLICY tenant_policyON tasks<p>USING (tenant_id = current_setting('app.current_tenant')::BIGINT);</p><p>ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;</p>SET app.current_tenant = '123'; -- Simulate tenant context<p>SELECT * FROM tasks; -- Only tasks with tenant_id = 123 will be visible</p></pre><ul><li><strong><em>2. Preventing Cross-Tenant Data&nbsp;Leaks</em></strong></li></ul><p>Preventing accidental or intentional cross-tenant data leaks is critical in multi-tenant architectures.</p><p><strong>Multi-Layer Access&nbsp;Checks</strong>:</p><p>Enforce tenant isolation at both the database and application layers.</p><p>Validate all queries to ensure they are scoped to the user’s&nbsp;tenant.</p><p>Always include a tenant_id check in database&nbsp;queries.</p><p>Use database views or abstractions to simplify tenant-specific filtering.</p><p><strong>Application-Layer Validation</strong>:</p><p>Add additional validation at the application level as a guardrail.</p><p>Ensure that APIs restrict data access to the authenticated tenant&nbsp;context.</p><pre>def get_user_tasks(user):    if user.tenant_id != request.tenant_id:<p>        raise PermissionDenied(\"Cross-tenant access is not allowed.\")</p>    return db.query(Tasks).filter(Tasks.tenant_id == user.tenant_id).all()</pre><p>Audit logs are essential for monitoring, compliance, and debugging. They provide visibility into access patterns and help detect unauthorized access attempts.</p><p>User ID and tenant ID for all&nbsp;queries.</p><p>Access attempts (successful and&nbsp;failed).</p><p>Data modification operations (insert, update,&nbsp;delete).</p><p>Timestamps and IP addresses for requests.</p><p><strong>SQL Example For Logging&nbsp;Queries</strong>:</p><pre>CREATE TABLE audit_logs (    id SERIAL PRIMARY KEY,    tenant_id BIGINT,    table_name VARCHAR(255),    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP<p>INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)</p>VALUES (123, 456, 'SELECT', 'tasks', 'SELECT * FROM tasks WHERE tenant_id = 456');</pre><p><strong>Integrating Logging&nbsp;Tools</strong>:</p><p>Use database triggers to log operations automatically.</p><p>Combine with external tools like  (Elasticsearch, Logstash, Kibana) or  for advanced monitoring.</p><pre>CREATE OR REPLACE FUNCTION log_task_changes()RETURNS TRIGGER AS $$    INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)<p>    VALUES (current_user_id(), NEW.tenant_id, TG_OP, TG_TABLE_NAME, current_query());</p>    RETURN NEW;$$ LANGUAGE plpgsql;<p>CREATE TRIGGER audit_task_changes</p>AFTER INSERT OR UPDATE OR DELETE ON tasks<p>FOR EACH ROW EXECUTE FUNCTION log_task_changes();</p></pre><p>Combine RBAC, RLS, and application-level validation for comprehensive protection.</p><p>Use multi-layered access checks and robust query scoping to ensure tenant isolation.</p><p>Maintain detailed audit logs to track access and modifications for accountability and compliance.</p><p>These practices create a secure foundation for multi-tenant authorization systems, ensuring that each tenant’s data is isolated, protected, and auditable.</p><p>Designing a robust multi-tenant authorization system involves navigating a set of challenges and trade-offs. Each approach has its own set of complexities that must be carefully managed to ensure scalability, performance, and maintainability.</p><ul><li><strong><em>1. Balancing Performance &amp; Flexibility</em></strong></li></ul><p>Choosing between speed and schema cleanliness can significantly impact your database design and performance.</p><p><strong>Prioritizing Performance (Flat&nbsp;Model)</strong>:</p><p>Direct lookups using a tenant_id column ensure fast query execution.</p><p>Reduced join complexity leads to quicker response&nbsp;times.</p><p>: May result in data redundancy (e.g., repeating tenant IDs across multiple&nbsp;tables).</p><p><strong>SQL Example For Optimized Query</strong>:</p><pre>SELECT * FROM tasks <p>WHERE tenant_id = 123 AND status = 'pending';</p></pre><p><strong>Prioritizing Schema Cleanliness (Hierarchical Model)</strong>:</p><p>Using a normalized schema ensures a clean and consistent database structure.</p><p>: Requires more complex joins and increased query times, especially for deeply nested relationships.</p><p><strong>Hierarchical Query&nbsp;Example</strong>:</p><pre>SELECT tasks.*FROM tasks<p>JOIN projects ON tasks.project_id = projects.id</p>JOIN clients ON projects.client_id = clients.id<p>WHERE clients.tenant_id = 123;</p></pre><p>Managing tenant-specific setups adds complexity, particularly as the number of tenants&nbsp;grows.</p><p><strong>Tenant-Specific Databases</strong>:</p><p>Each tenant has its own database, simplifying compliance and data isolation.</p><p>: Maintaining consistency across databases for schema&nbsp;changes.</p><p>: Use automation tools like  or  to manage schema migrations across&nbsp;tenants.</p><pre># Liquibase command to apply migrations to multiple tenant databasesliquibase --url=\"jdbc:mysql://db_host/tenant1\" update<p>liquibase --url=\"jdbc:mysql://db_host/tenant2\" update</p></pre><p><strong>Single Multi-Tenant Database</strong>:</p><p>Shared schema reduces maintenance but requires more sophisticated query scoping and indexing.</p><p>: Tracking and isolating tenant data effectively without introducing query overhead.</p><ul><li><strong><em>3. Handling Schema Updates In Multi-Tenant Databases</em></strong></li></ul><p>Ensuring all tenants have consistent schemas while minimizing downtime is one of the most significant challenges in multi-tenant systems.</p><p>Use versioned migrations to apply incremental updates across all&nbsp;tenants.</p><p>Maintain backward compatibility to prevent disruptions during&nbsp;updates.</p><pre>CREATE TABLE schema_versions (    tenant_id BIGINT,    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><p>Apply updates to a subset of tenants, validate, and then roll out to the&nbsp;rest.</p><p>Use feature flags to selectively enable new schema features.</p><p><strong>Code Example For Rolling&nbsp;Updates</strong>:</p><pre>tenants = get_tenant_list()for tenant in tenants:<p>    apply_schema_update(tenant_id=tenant.id)</p></pre><p><strong>Testing &amp; CI/CD For Multi-Tenant Systems</strong>:</p><p>Test migrations on a staging environment with realistic tenant data before deploying.</p><p>Use CI/CD tools like  to automate and track updates across tenant databases.</p><p><strong>Performance Vs. Flexibility</strong>:</p><p>Use a flat model for high-speed queries or hierarchical models for cleaner schemas but expect performance trade-offs.</p><p>Tenant-specific databases simplify compliance but require robust automation for schema management.</p><p>Implement version control and rolling updates to ensure seamless schema changes across all&nbsp;tenants.</p><p>Addressing these challenges with well-defined strategies ensures a scalable and maintainable multi-tenant authorization system, capable of adapting to evolving application needs.</p><p>Real-world applications of multi-tenant database authorization vary depending on the complexity of the resource structure, performance requirements, and compliance needs. Below are three illustrative scenarios demonstrating how different authorization models can be applied effectively.</p><ul><li><strong><em>Scenario 1 — Flat Model For A SaaS CRM&nbsp;App</em></strong></li></ul><p>A SaaS customer relationship management (CRM) application needs to store and manage customer interactions for multiple companies, ensuring users can only access data associated with their organization.</p><p>: Each company has its own sales team, and users need quick access to customer records and sales&nbsp;data.</p><p>: Use a flat model by adding tenant_id to every table, such as customers, leads, and&nbsp;sales.</p><pre>CREATE TABLE customers (    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    name VARCHAR(255),    phone VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    customer_id BIGINT,    status VARCHAR(50),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><pre>SELECT * FROM customers </pre><p>Simple queries without joins for tenant-specific data.</p><p>High performance due to direct&nbsp;lookups.</p><p>Wider tables due to the inclusion of tenant_id.</p><p>Potential redundancy if relationships between entities are not properly normalized.</p><ul><li><strong><em>Scenario 2 — Hierarchical Model For A Project Management Tool</em></strong></li></ul><p>A project management tool with a nested structure: Company → Client → Project → Task. Users need to manage projects while maintaining strict access control based on their organization.</p><p>: Each company has multiple clients, each with its own projects and tasks. Users must only access tasks related to their&nbsp;company.</p><p>: Use a hierarchical model to enforce relationships and control access through&nbsp;joins.</p><pre>CREATE TABLE companies (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    company_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    client_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    project_id BIGINT NOT NULL,</p>    description TEXT,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><pre>SELECT tasks.*FROM tasks<p>JOIN projects ON tasks.project_id = projects.id</p>JOIN clients ON projects.client_id = clients.id<p>WHERE clients.company_id = 123;</p></pre><p>Clean, normalized schema without redundant data.</p><p>Access naturally follows the hierarchy.</p><p>Complex queries due to multi-level joins.</p><p>Slower query performance for deep hierarchies.</p><ul><li><strong><em>Scenario 3 — Single-Tenant Databases For An Enterprise App</em></strong></li></ul><p>An enterprise application handles sensitive data, requiring strict data isolation for compliance with GDPR and HIPAA regulations.</p><p>: Each tenant’s data must be completely isolated to ensure compliance and scalability.</p><p>: Use a single-tenant database model where each company has its own dedicated database.</p><pre>Database Names:  tenant_1_db  tenant_3_db</pre><p>: Use  to manage database updates across multiple&nbsp;tenants.</p><pre>deploy:  steps:<p>    - name: Update Tenant Databases</p>      script: |<p>        for db in $(list_databases); do</p>          apply_migrations $db</pre><p>Complete isolation ensures compliance with regulatory requirements.</p><p>Scalability: Large tenants can have dedicated resources (e.g., separate hardware).</p><p>Higher operational complexity in managing multiple databases.</p><p>Requires robust CI/CD pipelines for schema&nbsp;updates.</p><p>Each use case demonstrates how careful consideration of application requirements, data relationships, and compliance needs can guide the choice of the best authorization model for a multi-tenant database.</p><p>Efficient, scalable, and secure authorization in multi-tenant databases as we’ve found often at <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Best+Practices+For+Database+Authorization+In+Multi-Tenant+Systems\">Xinthe</a>, requires a well-thought-out approach tailored to the application’s needs.</p><p>Authorization in multi-tenant databases isn’t a one-size-fits-all challenge. Developers and database architects must carefully evaluate their application’s structure, expected growth, and regulatory needs to select the most effective approach. Armed with the insights and strategies outlined in this article, you can design multi-tenant database systems that are secure, scalable, and efficient, ensuring both developer productivity and a seamless user experience.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=001a1bcf2568\" width=\"1\" height=\"1\" alt=\"\">","contentLength":28256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker, Kubernetes, and NATS — The Backbone of Cloud-Native Apps","url":"https://blog.devops.dev/docker-kubernetes-and-nats-the-backbone-of-cloud-native-apps-af724f41c17d?source=rss----33f8b2d9a328---4","date":1739465289,"author":"Cristhian Ferrufino","guid":790,"unread":true,"content":"<h3><strong>Docker, Kubernetes, and NATS — The Backbone of Cloud-Native Apps</strong></h3><p>Welcome back to the ! In the <a href=\"https://medium.com/devops-dev/decoding-the-message-broker-kafka-vs-rabbitmq-vs-nats-a-tale-of-three-titans-a4f47127256b\">last article</a>, we explored the world of message brokers and why NATS is a standout choice for modern microservices. Now, it’s time to dive into the backbone of cloud-native applications:  and . If microservices are the chefs in our restaurant analogy, containers are the kitchen tools that keep everything running smoothly. And Kubernetes? That’s the head chef, making sure everyone works in&nbsp;harmony.</p><p>In this article, we’ll break down  and , explore how they work together, and even touch on how  fits into the mix. By the end, you’ll have a solid understanding of how to containerize your applications and orchestrate them like a pro. Let’s get&nbsp;cooking!</p><h3>What Is Containerization, and Why Is It Important?</h3><p>Imagine you’re shipping a fragile package across the world. You’d want to pack it in a sturdy container, right? That’s exactly what containerization does for your applications. It packages your app and all its dependencies (libraries, frameworks, etc.) into a lightweight, portable unit called a . This ensures that your app runs consistently across different environments — whether it’s your laptop, a testing server, or a production cluster.</p><p><strong>Why developers love containers:</strong>✅ : “Works on my machine” becomes “Works everywhere.”✅ : No more dependency hell — each app lives in its own bubble.✅ : Deploy to AWS, Azure, or your grandma’s PC (if she’s cool with Kubernetes).✅ : 10x lighter than VMs. Think EVs vs. a gas-guzzling truck</p><h3>Introduction to Docker: Building, Running, and Managing Containers</h3><p>Docker is the most popular tool for containerization, and for good reason. It’s simple, powerful, and widely supported. Let’s break it&nbsp;down:</p><p>To create a container, you start with a  — a text file that defines the steps to build your app’s environment. Here’s a simple&nbsp;example:</p><pre># Use a lightweight Python image (because nobody likes bloat)FROM python:3.9-slim<p># Set the stage for your app</p>WORKDIR /app<p># Install dependencies (Pro tip: Skip the cache to shrink your image)</p>COPY requirements.txt .<p>RUN pip install --no-cache-dir -r requirements.txt</p><p># Copy the rest of the code</p>COPY . .<p># Open the app’s “front door”</p>EXPOSE 8080CMD [\"python\", \"app.py\"]</pre><p>With this Dockerfile, you can build a container image using the ``&nbsp;command:</p><pre>🚀 Run this: docker build -t my-python-app .</pre><p>Once you’ve built your image, you can run it as a container:</p><pre>🎯 Pro tip: Map ports like a pirate mapping treasure.  docker run -p 8080:8080 my-python-app</pre><p>This command starts your app and maps port 8080 on your host to port 8080 in the container. Easy,&nbsp;right?</p><p>Docker also provides tools to manage your containers:</p><ul><li>: List running containers.</li><li><em>docker logs &lt;container_id&gt;</em>: View logs for a specific container</li><li><em>docker stop &lt;container_id&gt;</em>: Stop a running container.</li></ul><h3>Docker vs. Podman: A Detailed Comparison</h3><p>While Docker is the most popular containerization tool, it’s not the only one.  is a rising star in the container world, and it’s worth understanding how it compares to&nbsp;Docker.</p><pre>+----------------------+-------------------------------------+-----------------------------------+| Feature              | Docker 🐳                           | Podman 📦                         |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Daemon Requirement   | Requires a daemon (dockerd)         | Daemonless (runs containers       |<p>|                      |                                     | directly)                         |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Root vs. Rootless    | Runs as root by default              | Supports rootless containers out  |</p>|                      |                                     | of the box                        |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Compatibility        | Uses Docker CLI and Dockerfiles     | Fully compatible with Docker CLI  |<p>|                      |                                     | and Dockerfiles                   |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Security             | Requires root privileges, which can | Rootless mode reduces attack      |</p>|                      | be a security risk                  | surface                           |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Orchestration        | Requires Docker Swarm for           | Integrates with Kubernetes        |<p>|                      | orchestration                       | natively                          |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Community Support    | Larger community and ecosystem      | Growing community, backed by Red  |</p>|                      |                                     | Hat                               |<p>+----------------------+-------------------------------------+-----------------------------------+</p></pre><ul><li>You need a mature, widely supported tool with a large ecosystem.</li><li>You’re already using Docker Swarm for orchestration.</li><li>You’re okay with running containers as&nbsp;root.</li></ul><ul><li>You want a daemonless, more secure alternative to&nbsp;Docker.</li><li>You’re working in environments where root privileges are restricted.</li><li>You’re already using Kubernetes and want tighter integration.</li></ul><p>Both tools are excellent choices, so pick the one that best fits your&nbsp;needs.</p><h3>Kubernetes Overview: Orchestration, Scaling, and Self-Healing</h3><p>While Docker is great for running containers, managing them at scale can get tricky. Enter  (or K8s for short), the de facto standard for container orchestration. Think of Kubernetes as the conductor of an orchestra — it ensures all your containers play in&nbsp;harmony.</p><p><strong>Key Features of Kubernetes</strong></p><ul><li>: Automates deployment, scaling, and management of containers.</li><li>: Automatically adjusts the number of running containers based on&nbsp;demand.</li><li>: Restarts failed containers and replaces unhealthy ones.</li><li>: Automatically assigns IP addresses and DNS names to containers.</li></ul><p>Kubernetes organizes containers into , which are the smallest deployable units. A pod can contain one or more containers that share resources like storage and networking. Here’s a simple Kubernetes deployment file:</p><pre>apiVersion: apps/v1kind: Deployment  name: my-python-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-python-app<p>        image: my-python-app:latest</p>        ports:</pre><p>his file tells Kubernetes to run three replicas of your app and expose it on port 8080. You can apply it&nbsp;using:</p><pre>🔥 Run this: kubectl apply -f deployment.yaml</pre><h3>How NATS Shines in a Kubernetes Environment</h3><p>Now, let’s talk about . As a lightweight, high-performance messaging system, NATS plays well with Kubernetes. Here’s how it stands&nbsp;out:</p><p><strong>Use Cases for NATS in Kubernetes</strong></p><ol><li>Service-to-Service Communication: NATS excels at enabling fast, reliable communication between microservices. Its lightweight design makes it perfect for Kubernetes’ dynamic environment.</li><li>Event-Driven Architectures: NATS’s pub/sub, request-reply or streams patterns make it ideal for event-driven systems, where services need to react to events in real&nbsp;time.</li><li>Scalability: NATS can handle millions of messages per second, making it a great fit for high-throughput applications running on Kubernetes.</li><li>Resilience: NATS’s built-in fault tolerance ensures that your messaging system remains reliable, even in the face of node failures.</li></ol><p><strong>Deploying NATS on Kubernetes</strong></p><p>Deploying NATS on Kubernetes is straightforward. Here’s a basic NATS deployment file:</p><pre>apiVersion: apps/v1kind: Deployment  name: nats  replicas: 1    matchLabels:  template:      labels:    spec:      - name: nats        ports:<p>        - containerPort: 4222 # The messaging highway 🛣️</p></pre><p>Once deployed, NATS can be used by your microservices for seamless communication.</p><h3>Best Practices for Containerizing Microservices</h3><p>To wrap things up, here are some best practices for containerizing your microservices:</p><ol><li><strong>Keep Containers Lightweight</strong>: Use minimal base images (e.g.,  or  versions) to reduce size and improve performance.</li><li>: Separate the build and runtime environments to keep production images&nbsp;small.</li><li><strong>Leverage Kubernetes Features</strong>: Use ConfigMaps and Secrets to manage configuration and sensitive data.</li><li>: Integrate tools like Prometheus and Fluentd for monitoring and&nbsp;logging.</li><li>: Use CI/CD pipelines to automate building, testing, and deploying containers.</li></ol><p>In the next article, we’ll explore “<strong>NATS as a Service Mesh — The Lightweight Superhero Your Microservices Deserve</strong>” and how it simplifies communication between microservices. Spoiler alert: it’s like giving your microservices a supercharged walkie-talkie. Stay&nbsp;tuned!</p><p>Until then, feel free to drop a comment or share your thoughts. What’s your experience with Docker and Kubernetes? Any tips or tricks you’d like to share? Let’s keep the conversation going.</p><p> 💬 <em>What’s your #1 Kubernetes struggle? Scaling? Debugging? Share&nbsp;below!</em></p><p> ❤️ </p><p>Happy containerizing, and stay tuned for the next chapter in the !</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=af724f41c17d\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Coding Assistants are Not the Solution You Think","url":"https://devops.com/ai-coding-assistants-are-not-the-solution-you-think/","date":1739449895,"author":"Anish Dhar","guid":597,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes History Inspector, with Kakeru Ishii","url":"http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii","date":1739445780,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":675,"unread":true,"content":"<p dir=\"ltr\">Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week ","contentLength":341,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Sandbox environments: Creating efficient and isolated testing realms","url":"https://www.youtube.com/watch?v=fh7-lQVmX-o","date":1739426433,"author":"CNCF [Cloud Native Computing Foundation]","guid":569,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fh7-lQVmX-o?version=3","enclosureMime":"","commentsUrl":null},{"title":"KitOps: AI Model Packaging Standards","url":"https://www.youtube.com/watch?v=1TD-e_wVe4Q","date":1739426400,"author":"CNCF [Cloud Native Computing Foundation]","guid":568,"unread":true,"content":"<article>Chat with us on Discord:  https://discord.gg/Tapeh8agYy\n\nCheck out our repos:\nKitOps      https://github.com/jozu-ai/kitops\nPyKitOps Python Library  https://github.com/jozu-ai/pykitops\nKitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin</article>","contentLength":253,"flags":null,"enclosureUrl":"https://www.youtube.com/v/1TD-e_wVe4Q?version=3","enclosureMime":"","commentsUrl":null},{"title":"Training IT Teams for Multi-Cloud DevOps Environments","url":"https://devops.com/training-it-teams-for-multi-cloud-devops-environments/","date":1739362266,"author":"Anne Fernandez","guid":596,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StackGen’s New Migration Engine: A DevOps Game-Changer for Multi-Cloud Transitions","url":"https://devops.com/stackgens-new-migration-engine-a-devops-game-changer-for-multi-cloud-transitions/","date":1739269023,"author":"Tom Smith","guid":595,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gcore Radar report reveals 56% year-on-year increase in DDoS attacks","url":"https://devops.com/gcore-radar-report-reveals-56-year-on-year-increase-in-ddos-attacks/","date":1739257299,"author":"cybernewswire","guid":594,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harness Merges with Traceable to Provide Integrated DevSecOps Platform","url":"https://devops.com/harness-merges-with-traceable-to-provide-integrated-devsecops-platform/","date":1739216172,"author":"Mike Vizard","guid":593,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing Kyverno policy enforcement performance for large clusters","url":"https://www.youtube.com/watch?v=DWmCAUCs3bc","date":1739211188,"author":"CNCF [Cloud Native Computing Foundation]","guid":567,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DWmCAUCs3bc?version=3","enclosureMime":"","commentsUrl":null},{"title":"AWS Extends AI Agent Reach into the Realm of Testing Code","url":"https://devops.com/aws-extends-ai-agent-reach-into-the-realm-of-testing-code/","date":1739195146,"author":"Mike Vizard","guid":592,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-125/","date":1739179101,"author":"Mike Vizard","guid":591,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digest #159: SQLite vs. PostgreSQL, Kubernetes Cloud Savings, DeepSeek Data Leaks and Terraform Tools","url":"https://www.devopsbulletin.com/p/digest-159-sqlite-vs-postgresql-kubernetes","date":1739022077,"author":"Mohamed Labouardy","guid":198,"unread":true,"content":"<p><strong>Welcome to this week’s edition of the DevOps Bulletin!</strong></p><p>Ever wonder if SQLite or PostgreSQL is the better choice? This week, we share how Kubernetes cut our cloud spend, saved $3.9M a year, and revealed how the Wiz research team uncovered a leaking database. We look at a new CI/CD built for developers, see how Google keeps its cloud safe, and introduce Flox—a new tool to replace Docker with Kubernetes libraries.</p><p>In our podcast selection, listen to the wild tale of a penetration tester who accidentally robbed the wrong bank. Our tutorials cover topics like keeping your Git credentials safe, using AI tools as a staff engineer, and avoiding huge Terraform state files. We also highlight cool projects like Mathesar for managing Postgres data, a tool to explore SQLite pages, a web honeypot to fool attackers, an AI agent called Goose, and TerraConstructs, a new helper for Terraform.</p><p>All this and more in this week’s DevOps Bulletin—don’t miss out!</p><p>A penetration tester accidentally robs the wrong bank:</p><p>Highlighting cool DevOps projects to keep an eye on:</p><ul><li><p> is an intuitive spreadsheet-like interface that lets users of all technical skill levels view, edit, query, and collaborate on Postgres data directly.</p></li></ul><ul><li><p> is a visual tool that explores SQLite databases page-by-page, how they're stored on disk, and how SQLite sees them.</p></li><li><p>&nbsp;is a web honeypot library that creates vulnerable-looking endpoints to detect and mislead attackers.</p></li></ul><ul><li><p> is an open-source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM.</p></li><li><p> is a library of classes and interfaces inspired by AWS CDK but designed to leverage the power and flexibility of Terraform.</p></li></ul><div><p>If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":1818,"flags":null,"enclosureUrl":"https://i.scdn.co/image/ab6765630000ba8a9b07806763f18923cf69da89","enclosureMime":"","commentsUrl":null},{"title":"Digma Adds Ability to Predict Coding Issues to Observability Platform","url":"https://devops.com/digma-adds-ability-to-predict-coding-issues-to-observability-platform/","date":1738941060,"author":"Mike Vizard","guid":590,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 45 (CRI-O)","url":"https://www.youtube.com/watch?v=--eJZu3Zkbw","date":1738909199,"author":"CNCF [Cloud Native Computing Foundation]","guid":566,"unread":true,"content":"<article>CRI-O, a CNCF Graduated project, is designed to be lightweight, focusing solely on Kubernetes needs’, reducing resource overhead compared to general-purpose container runtimes. \n\nIt improves security by leveraging OCI (Open Container Initiative) standards, integrating seamlessly with tools like seccomp, SELinux, and AppArmor for enhanced isolation and compliance. With robust support for OCI-compliant image formats and runtimes like runc, CRI-O ensures compatibility with a wide range of container images. Join CNCF Ambassador Marvin Beckers test CRI-O’s efficiency, as it minimizes the feature set to what Kubernetes requires, leading to faster startup times and reduced complexity.</article>","contentLength":690,"flags":null,"enclosureUrl":"https://www.youtube.com/v/--eJZu3Zkbw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Cloud Native Live: What's latest in KubeArmor v1.5","url":"https://www.youtube.com/watch?v=OUNEu3h2V3c","date":1738821857,"author":"CNCF [Cloud Native Computing Foundation]","guid":565,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/OUNEu3h2V3c?version=3","enclosureMime":"","commentsUrl":null},{"title":"AI for observability","url":"https://www.youtube.com/watch?v=IIz8Xpyebug","date":1738821619,"author":"CNCF [Cloud Native Computing Foundation]","guid":564,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/IIz8Xpyebug?version=3","enclosureMime":"","commentsUrl":null},{"title":"Docker Bake is Now Generally Available in Docker Desktop 4.38!","url":"https://www.docker.com/blog/ga-launch-docker-bake/","date":1738792034,"author":"Colin Hemmings","guid":315,"unread":true,"content":"<p>We’re excited to announce the General Availability of  with <a href=\"https://www.docker.com/blog/docker-desktop-4-38/\">Docker Desktop 4.38</a>! This powerful build orchestration tool takes the hassle out of managing complex builds and offers simplicity, flexibility, and performance for teams of all sizes.</p><p>Docker Bake is an orchestration tool that streamlines Docker builds, similar to how Compose simplifies managing runtime environments. With Bake, you can define build stages and deployment environments in a declarative file, making complex builds easier to manage. It also leverages BuildKit’s parallelization and optimization features to speed up build times.</p><p>While Dockerfiles are excellent for defining image build steps, teams often need to build multiple images and execute helper tasks like testing, linting, and code generation. Traditionally, this meant juggling numerous  commands with their own options and arguments – a tedious and error-prone process.</p><p>Bake changes the game by introducing a declarative file format that encapsulates all options and image dependencies, referred to as <a href=\"https://docs.docker.com/build/bake/targets/\" rel=\"nofollow noopener\" target=\"_blank\"></a>. Additionally, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.</p><h3>Challenges with complex Docker Build configuration:</h3><ul><li>Managing long, complex build commands filled with countless flags and environment variables.</li><li>Tedious workflows for building multiple images.</li><li>Difficulty declaring builds for specific targets or environments.</li><li>Requires a script or 3rd-party tool to make things manageable</li></ul><p>Docker Bake tackles these challenges with a better way to manage complex builds with a simple, declarative approach.</p><h3>Key benefits of Docker Bake</h3><ul><li>: Replace complex chains of Docker build commands and scripts with a single  command while maintaining clear, version-controlled configuration files that are easy to understand and modify.</li><li>: Express sophisticated build logic through HCL syntax and matrix builds, enabling dynamic configurations that adapt to different environments and requirements while supporting custom functions for advanced use cases.</li><li>: Maintain standardized build configurations across teams and environments through version-controlled files and inheritance patterns, eliminating environment-specific build issues and reducing configuration drift.</li><li>: Automatically parallelize independent builds and eliminate redundant operations through context deduplication and intelligent caching, dramatically reducing build times for complex multi-image workflows.</li></ul><p>: One simple Docker buildx bake command to replace all the flags and environment variables.</p><h3>Use cases for Docker Bake</h3><h4>1. Monorepo and Image Bakery</h4><p>Docker Bake can help developers efficiently manage and build multiple related Docker images from a single source repository. Plus, they can leverage shared configurations and automated dependency handling to enforce organizational standards.</p><ul><li> Teams can maintain consistent build logic across dozens or hundreds of microservices in a single repository, reducing configuration drift and maintenance overhead.</li><li> Shared base images and contexts are automatically deduplicated, dramatically reducing build times and storage costs.</li><li> Enforce organizational standards through inherited configurations, ensuring all services follow security, tagging, and testing requirements.</li><li> A single source of truth for build configurations makes it easier to implement organization-wide changes like base image updates or security patches.</li></ul><p>Docker Bake provides seamless compatibility with existing docker-compose.yml files, allowing direct use of your current configurations. Existing Compose users are able to get started using Bake with minimal effort.</p><ul><li> Teams can incrementally adopt advanced build features while still leveraging their existing compose workflows and knowledge.</li><li> Use the same configuration for both local development (via compose) and production builds (via Bake), eliminating “works on my machine” issues.</li><li>: Access powerful features like matrix builds and HCL expressions while maintaining compatibility with familiar compose syntax.</li><li>: Seamlessly integrate with existing CI/CD pipelines that already understand compose files while adding Bake’s advanced build capabilities.</li></ul><h4>3. Complex build configurations</h4><ul><li><strong>Cross-Platform Compatibility:</strong> Matrix builds enable teams to efficiently manage builds across multiple architectures, OS versions, and dependency combinations from a single configuration.</li><li> HCL expressions allow builds to adapt to different environments, git branches, or CI variables without maintaining multiple configurations.</li><li> Custom functions enable sophisticated logic for things like version calculation, tag generation, and conditional builds based on git history.</li><li> Variable validation and inheritance ensure consistent configuration across complex build scenarios, reducing errors and maintenance burden.</li><li> Groups and targets help organize large-scale build systems with dozens or hundreds of permutations, making them manageable and maintainable.</li></ul><p>With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.</p><ul><li><strong>Enhanced Docker Build Cloud Performance:</strong> Instantly parallelize matrix builds across cloud infrastructure, turning hour-long build pipelines into minutes without managing build infrastructure.</li><li> Leverage Build Cloud’s distributed caching and deduplication to dramatically reduce bandwidth usage and build times, which is especially valuable for remote teams.</li><li> Save cost with DBC  Bake’s precise target definitions mean you only consume cloud resources for exactly what needs to be built.</li><li> Teams can run complex multi-architecture builds without powerful local machines, enabling development from any device while maintaining build performance.</li><li> Offload resource-intensive builds from CI runners to Build Cloud, reducing CI costs and queue times while improving reliability.</li></ul><h2>What’s New in Bake for GA?</h2><p>Docker Bake has been an experimental feature for several years, allowing us to refine and improve it based on user feedback. So, there is already a strong set of ingredients that users love, such as <a href=\"https://docs.docker.com/build/bake/targets/\" rel=\"nofollow noopener\" target=\"_blank\">targets</a> and <a href=\"https://docs.docker.com/build/bake/targets/#grouping-targets\" rel=\"nofollow noopener\" target=\"_blank\">groups</a>, <a href=\"https://docs.docker.com/build/bake/variables/\" rel=\"nofollow noopener\" target=\"_blank\">variables</a>, <a href=\"https://docs.docker.com/build/bake/funcs/\" rel=\"nofollow noopener\" target=\"_blank\">HCL Expression Support</a>, <a href=\"https://docs.docker.com/build/bake/inheritance/\" rel=\"nofollow noopener\" target=\"_blank\">inheritance</a> capabilities, <a href=\"https://docs.docker.com/build/bake/matrices/\" rel=\"nofollow noopener\" target=\"_blank\">matrix targets</a>, and additional <a href=\"https://docs.docker.com/build/bake/contexts/\" rel=\"nofollow noopener\" target=\"_blank\">contex</a>ts. With this GA release, Bake is now ready for production use, and we’ve added several enhancements to make it more efficient, secure, and easier to use:</p><ul><li><a href=\"https://docs.docker.com/build/bake/contexts/#deduplicate-context-transfer\" rel=\"nofollow noopener\" target=\"_blank\"><strong>Deduplicated Context Transfers</strong></a> Significantly speeds up build pipelines by eliminating redundant file transfers when multiple targets share the same build context.</li><li><a href=\"https://docs.docker.com/reference/cli/docker/buildx/bake/#allow\" rel=\"nofollow noopener\" target=\"_blank\"></a> Enhances security and resource management by providing fine-grained control over what capabilities and resources builders can access during the build process.</li><li><a href=\"https://docs.docker.com/build/bake/inheritance/\" rel=\"nofollow noopener\" target=\"_blank\"></a> Simplifies configuration management by allowing teams to define reusable attribute sets that can be mixed, matched, and overridden across different targets.</li><li><a href=\"https://docs.docker.com/build/bake/variables/#validating-variables\" rel=\"nofollow noopener\" target=\"_blank\"></a> Prevents wasted time and resources by catching configuration errors before the actual build process begins.</li></ul><h3>Deduplicate context transfers</h3><p>When you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This can significantly impact build time, depending on your build configuration.</p><p>Previously, the workaround required users to define a named context that loads the context files and then have each target reference the named context. But with Bake, this will be handled automatically now.</p><p>Bake can automatically deduplicate context transfers from targets sharing the same context. When you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This more efficient approach leads to much faster build time.&nbsp;</p><p>Read more about how to speed up your build time in our <a href=\"https://docs.docker.com/build/bake/contexts/#deduplicate-context-transfer\" rel=\"nofollow noopener\" target=\"_blank\">docs</a>.&nbsp;</p><p>Bake now includes entitlements to control access to privileged operations, aligning with Build. This prevents unintended side effects and security risks. If Bake detects a potential issue — like a privileged access request or an attempt to access files outside the current directory — the build will fail unless explicitly allowed.</p><p>To be consistent, the Bake command now supports the flag to grant access to additional entitlements. The following entitlements are <a href=\"https://docs.docker.com/reference/cli/docker/buildx/bake/#allow\" rel=\"nofollow noopener\" target=\"_blank\">currently supported for Bake</a>.</p><ul><li>Build equivalents\n<ul><li> Allows executions with host networking.</li><li><code>--allow security.insecure</code> Allows executions without sandbox. (i.e. —privileged)</li></ul></li><li>File system: Grant filesystem access for builds that need access files outside the working directory. This will impact <code>context, output, cache-from, cache-to, dockerfile, secret</code><ul><li> Grant read and write access to files outside the working directory.</li><li> Grant read access to files outside the working directory.</li><li><code>--allow fs.write=&lt;path|*&gt; </code> Grant write access to files outside the working directory.</li></ul></li><li>ssh\n<ul><li>– Allows exposing SSH agent.</li></ul></li></ul><p>Several attributes previously had to be defined in CSV (e.g. ). These were challenging to read and couldn’t be easily overridden. The following can now be defined as structured objects:</p><pre><code>target \"app\" {\n\t\tattest = [\n\t\t\t{ type = \"provenance\", mode = \"max\" },\n\t\t\t{ type = \"sbom\", disabled = true}\n\t\t]\n\n\t\tcache-from = [\n\t\t\t{ type = \"registry\", ref = \"user/app:cache\" },\n\t\t\t{ type = \"local\", src = \"path/to/cache\"}\n\t\t]\n\n\t\tcache-to = [\n\t\t\t{ type = \"local\", dest = \"path/to/cache\" },\n\t\t]\n\n\t\toutput = [\n\t\t\t{ type = \"oci\", dest = \"../out.tar\" },\n\t\t\t{ type = \"local\", dest=\"../out\"}\n\t\t]\n\n\t\tsecret = [\n\t\t\t{ id = \"mysecret\", src = \"path/to/secret\" },\n\t\t\t{ id = \"mysecret2\", env = \"TOKEN\" },\n\t\t]\n\n\t\tssh = [\n\t\t\t{ id = \"default\" },\n\t\t\t{ id = \"key\", paths = [\"path/to/key\"] },\n\t\t]\n}</code></pre><p>As such, the attributes are now composable. Teams can mix, match, and override attributes across different targets which simplifies configuration management.</p><pre><code> target \"app-dev\" {\n    attest = [\n\t\t\t{ type = \"provenance\", mode = \"min\" },\n\t\t\t{ type = \"sbom\", disabled = true}\n\t\t]\n  }\n\n  target \"app-prod\" {\n    inherits = [\"app-dev\"]\n\n    attest = [\n\t\t\t{ type = \"provenance\", mode = \"max\" },\n\t\t]\n  }\n</code></pre><p>Bake now supports validation for variables similar to<a href=\"https://developer.hashicorp.com/terraform/language/values/variables#custom-validation-rules\" rel=\"nofollow noopener\" target=\"_blank\"> Terraform</a> to help developers catch and resolve configuration errors early. The GA for Bake also supports the following use cases.</p><p>To verify that the value of a variable conforms to an expected type, value range, or other condition, you can define custom validation rules using the  block.</p><pre><code>variable \"FOO\" {\n  validation {\n    condition = FOO != \"\"\n    error_message = \"FOO is required.\"\n  }\n}\n\ntarget \"default\" {\n  args = {\n    FOO = FOO\n  }\n}\n</code></pre><p>To evaluate more than one condition, define multiple  blocks for the variable. All conditions must be true.</p><pre><code>variable \"FOO\" {\n  validation {\n    condition = FOO != \"\"\n    error_message = \"FOO is required.\"\n  }\n  validation {\n    condition = strlen(FOO) &gt; 4\n    error_message = \"FOO must be longer than 4 characters.\"\n  }\n}\n\ntarget \"default\" {\n  args = {\n    FOO = FOO\n  }\n}\n</code></pre><p><strong>Dependency on other variables</strong></p><p>You can reference other <a href=\"https://docs.docker.com/build/bake/variables/#validating-variables\" rel=\"nofollow noopener\" target=\"_blank\">Bake variables</a> in your condition expression, enabling validations that enforce dependencies between variables. This ensures that dependent variables are set correctly before proceeding.</p><pre><code>variable \"FOO\" {}\nvariable \"BAR\" {\n  validation {\n    condition = FOO != \"\"\n    error_message = \"BAR requires FOO to be set.\"\n  }\n}\n\ntarget \"default\" {\n  args = {\n    BAR = BAR\n  }\n}\n</code></pre><p>In addition to updating the Bake configuration, we’ve added a new –list option. Previously, if you were unfamiliar with a project or wanted a reminder of the supported targets and variables, you would have to read through the file. Now, the list option will allow you to quickly query a list of them. It also supports the JSON format option if you need programmatic access.</p><p>Quickly get a list of the targets available in your Bake configuration.</p><ul><li><code>docker buildx bake --list targets</code></li><li><code>docker buildx bake --list type=targets,format=json</code></li></ul><p>Get a list of variables available for your Bake configuration.</p><ul><li><code>docker buildx bake --list variables</code></li><li><code>docker buildx bake --list type=variables,format=json</code></li></ul><p>These improvements build on a powerful feature set, ensuring Bake is both reliable and future-ready.</p><h2>Get started with Docker Bake</h2><p>Ready to simplify your builds? Update to Docker Desktop 4.38 today and start using Bake. With its declarative syntax and advanced features, Docker Bake is here to help you build faster, more efficiently, and with less effort.</p><p>Explore the <a href=\"https://docs.docker.com/build/bake/\" rel=\"nofollow noopener\" target=\"_blank\">documentation</a> to learn how to create your first Bake file and experience the benefits of streamlined builds firsthand.</p><p>Let’s bake something amazing together!</p>","contentLength":12611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker Desktop 4.38: New AI Agent, Multi-Node Kubernetes, and Bake in GA","url":"https://www.docker.com/blog/docker-desktop-4-38/","date":1738791751,"author":"Yiwen Xu","guid":314,"unread":true,"content":"<p>At Docker, we’re committed to simplifying the developer experience and empowering enterprises to scale securely and efficiently. With the <a href=\"https://docs.docker.com/desktop/release-notes/#4380\" rel=\"nofollow noopener\" target=\"_blank\">Docker Desktop 4.38</a> release, teams can look forward to improved developer productivity and enterprise governance.&nbsp;</p><p>We’re excited to announce the General Availability of Bake, a powerful feature for optimizing build performance and multi-node Kubernetes testing to help teams “shift left.” We’re also expanding availability for several enterprise features designed to boost operational efficiency. And last but not least, Docker AI Agent (formerly Project: Agent Gordon) is now in Beta, delivering intelligent, real-time Docker-related suggestions across Docker CLI, Desktop, and Hub. It’s here to help developers navigate Docker concepts, fix errors, and boost productivity.</p><h3>Docker’s AI Agent boosts developer productivity&nbsp;&nbsp;</h3><p>We’re thrilled to introduce Docker AI Agent (also known as Project: Agent Gordon) — an embedded, context-aware assistant seamlessly integrated into the Docker suite. Available within Docker Desktop and CLI, this innovative agent delivers real-time, tailored guidance for tasks like container management and Docker-specific troubleshooting — eliminating disruptive context-switching. Docker AI agent can be used for every Docker-related concept and technology, whether you’re getting started, optimizing an existing Dockerfile or Compose file, or understanding Docker technologies in general. By addressing challenges precisely when and where developers encounter them, Docker AI Agent ensures a smoother, more productive workflow.&nbsp;</p><p>The first iteration of Docker’s AI Agent is now available in Beta for all signed-in usersThe agent is disabled by default, so user activation is required. Read more about Docker’s New AI Agent and how to use it to accelerate developer velocity <a href=\"https://www.docker.com/blog/beta-launch-docker-ai-agent/\">here</a>.&nbsp;</p><p><strong>Figure 1: Asking questions to Docker AI Agent in Docker Desktop</strong></p><h3>Simplify build configurations and boost performance with Docker Bake</h3><p>Docker Bake is an orchestration tool that simplifies and speeds up Docker builds. After launching as an experimental feature, we’re thrilled to make it generally available with exciting new enhancements.</p><p>While Dockerfiles are great for defining build steps, teams often juggle docker build commands with various options and arguments — a tedious and error-prone process. Bake changes the game by introducing a declarative file format that consolidates all options and image dependencies (also known as targets) in one place. No more passing flags to every build command! Plus, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.</p><p>Key benefits of Docker Bake</p><ul><li> Abstract complex build configurations into one simple command.</li><li> Write build configurations in a declarative syntax, with support for custom functions, matrices, and more.</li><li> Share and maintain build configurations effortlessly across your team.</li><li> Bake parallelizes multi-image workflows, enabling faster and more efficient builds.</li></ul><p>Developers can simplify multi-service builds by integrating Bake directly into their Compose files — Bake supports Compose files natively. It enables easy, efficient building of multiple images from a single repository with shared configurations. Plus, it works seamlessly with <a href=\"https://www.docker.com/products/build-cloud/\">Docker Build Cloud</a> locally and in CI. With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.</p><h3>Shift Left with Multi-Node Kubernetes testing in Docker Desktop</h3><p>In today’s complex production environments, “shifting left”&nbsp; is more essential than ever. By addressing concerns earlier in the development cycle, teams reduce costs and simplify fixes, leading to more efficient workflows and better outcomes. That’s why we continue to bring new features and enhancements to integrate feedback directly into the developer’s inner loop</p><p>Docker Desktop now includes Multi-Node Kubernetes integration, enabling easier and extensive testing directly on developers’ machines. While single-node clusters allow for quick verification of app deployments, they fall short when it comes to testing resilience and handling the complex, unpredictable issues of distributed systems. To tackle this, we’re updating our Kubernetes distribution with <a href=\"https://kind.sigs.k8s.io/\" rel=\"nofollow noopener\" target=\"_blank\"></a> — a lightweight, fast, and user-friendly solution for local test and multi-node cluster simulations.</p><p><strong>Figure 2: Selecting Kubernetes version and cluster number for testing</strong></p><ul><li><strong>Multi-node cluster support:</strong> Replicate a more realistic production environment to test critical features like node affinity, failover, and networking configurations.</li><li><strong>Multiple Kubernetes versions:</strong> Easily test across different Kubernetes versions, which is a must for validating migration paths.</li><li> Since  is an actively maintained open-source project, developers can update to the latest version on demand without waiting for the next Docker Desktop release.</li></ul><p>Head over to our <a href=\"https://docs.docker.com/desktop/features/kubernetes/\" rel=\"nofollow noopener\" target=\"_blank\">documentation</a> to discover how to use multi-node Kubernetes clusters for local testing and simulation.</p><h3>General availability of administration features for Docker Business subscription</h3><p>With the <a href=\"https://www.docker.com/blog/docker-desktop-4-36/\">Docker Desktop 4.36 release</a>, we introduced Beta enterprise admin tools to streamline administration, improve security, and enhance operational efficiency. And the feedback from our Early Access Program customers has been overwhelmingly positive.&nbsp;</p><p>For instance, enforcing sign-in with macOS configuration files and across multiple organizations makes deployment easier and more flexible for large enterprises. Also, the PKG installer simplifies managing large-scale Docker Desktop deployments on macOS by eliminating the need to convert DMG files into PKG first.</p><p>Today, the features below are now available to all <a href=\"https://www.docker.com/products/business/\">Docker Business</a> customers.&nbsp;&nbsp;</p><p>Looking ahead, Docker is dedicated to continue expanding enterprise administration capabilities. <a href=\"https://www.docker.com/newsletter-subscription/\">Stay tuned</a> for more announcements!</p><p>Docker Desktop 4.38 reinforces our commitment to simplifying the developer experience while equipping enterprises with robust tools.&nbsp;</p><p>With Bake now in GA, developers can streamline complex build configurations into a single command. The new Docker AI Agent offers real-time, on-demand guidance within their preferred Docker tools. Plus, with Multi-node Kubernetes testing in Docker Desktop, they can replicate realistic production environments and address issues earlier in the development cycle. Finally, we made a few new admin tools available to all our <a href=\"https://www.docker.com/products/business/\">Business</a> customers, simplifying deployment, management, and monitoring.&nbsp;</p><p>We look forward to how these innovations accelerate your workflows and supercharge your operations!&nbsp;</p>","contentLength":6658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing the Beta Launch of Docker’s AI Agent, Transforming Development Experiences","url":"https://www.docker.com/blog/beta-launch-docker-ai-agent/","date":1738791389,"author":"Jean-Laurent de Morlhon","guid":313,"unread":true,"content":"<p>For years, Docker has been an essential partner for developers, empowering everyone from small startups to the world’s largest enterprises. Today, AI is transforming organizations across industries, creating opportunities for those who embrace it to gain a competitive edge. Yet, for many teams, the question of where to start and how to effectively integrate AI into daily workflows remains a challenge. True to its developer-first philosophy, Docker is here to bridge that gap.</p><p>We’re thrilled to introduce the beta launch of Docker AI Agent (also known as Project: Gordon)—an embedded, context-aware assistant seamlessly integrated into the Docker suite. Available within Docker Desktop and CLI, this innovative agent delivers tailored guidance for tasks like building and running containers, authoring Dockerfiles and Docker-specific troubleshooting—eliminating disruptive context-switching. By addressing challenges precisely when and where developers encounter them, Docker AI Agent ensures a smoother, more productive workflow.</p><p>As the AI Agent evolves, enterprise teams will unlock even greater capabilities, including customizable features that streamline collaboration, enhance security, and help developers work smarter. With the Docker AI Agent, we’re making Docker even easier and more effective to use than it has ever been — AI accessible, actionable, and indispensable for developers everywhere.</p><h3>How Docker’s AI Agent Simplifies Development Challenges&nbsp;&nbsp;</h3><p>Developing in today’s fast-paced tech landscape is increasingly complex, with developers having to learn an ever growing number of tools, libraries and technologies.</p><p>By integrating a GenAI Agent into Docker’s ecosystem, we aim to provide developers with a powerful assistant that can help them navigate these complexities.&nbsp;</p><p>The Docker AI Agent helps developers accelerate their work, providing real-time assistance, actionable suggestions, and automations that remove many of the manual tasks associated with containerized application development. Delivering the most helpful, expert-level guidance on Docker-related questions and technologies, Gordon serves as a powerful support system for developers, meeting them exactly where they are in their workflow.&nbsp;</p><p>If you’re a developer who favors graphical interfaces, Docker Desktop AI UI will help you navigate container running issues, image size management and more generic Dockerfile oriented questions. If you’re a command line interface user, you can call, and share context with the agent directly in your favorite terminal.</p><h3>So what can Docker’s AI Agent do today?&nbsp;</h3><p>We’re delivering an expert assistant for every Docker-related concept and technology, whether it’s getting started, optimizing an existing Dockerfile or Compose file, or understanding Docker technologies in general. With Docker AI Agent, you also have the ability to delegate actions while maintaining full control and review over the process.</p><p>A first example, if you want to run a container from an image, our agent can suggest the most appropriate  command tailored to your needs. This eliminates the guesswork or the need to search Docker Hub, saving you time and effort. The result combines a custom prompt, live data from Docker Hub, Docker container expertise and private usage insights, unique to Docker Inc.</p><p>We’ve intentionally designed the output to be concise and actionable, avoiding the overwhelming verbosity often associated with AI-generated commands. We also provide sources for most of the AI agent recommendations, pointing directly to our <a href=\"https://docs.docker.com/engine/\" rel=\"nofollow noopener\" target=\"_blank\">documentation</a> website. Our goal is to continuously refine this experience, ensuring that Docker’s AI Agent always provides the best possible command based on your specific local context.</p><p>Beside helping you run containers, the Docker AI Agent can today:</p><ul><li>Explain, Rate and optimize Dockerfile leveraging the latest version of Docker.</li><li>Help you run containers in an effective, concise way, leveraging the local context (checking port already used or volumes).</li><li>Answers any docker related questions with the latest version of our documentations for our whole tool suite, and as such is able to answer any kind of questions on Docker tools and technologies.</li><li>Containerize a software project helping you run your software in containers.</li><li>Helps on Docker related Github Actions.</li><li>Suggest fix when a container is failing to start in Docker Desktop.</li><li>Provides contextual help for containers, images and volumes.</li><li>Can augment its answer with per directory MCP servers (see <a href=\"https://docs.docker.com/desktop/features/gordon/mcp\" rel=\"nofollow noopener\" target=\"_blank\">doc</a>).</li></ul><p>For the node expert, in the above screenshot the AI is recommending node 20.12 which is not the latest version but the one the AI found in the .</p><p>With every future version of Docker Desktop and thanks to the feedback that you provide, the agent will be able to do so much more in the future.</p><h3>How can you try Docker AI Agent?&nbsp;</h3><p>This first beta release of Docker AI Agent is now progressively available for By default, the Docker AI agent is disabled. To enable it you will need to follow the steps below. Here’s how to get started:</p><ol><li><a href=\"https://www.docker.com/products/docker-desktop/\">Install</a> or update to the latest release of Docker Desktop 4.38</li><li>Enable Docker AI into Docker Desktop Settings -&gt; Features in Development</li><li>For the best experience, ensure the Docker terminal is enabled by going to Settings → General</li></ol><p>* If you’re a business subscriber, your Administrator needs to enable the Docker AI Agent for the organization first. This can be done through the <a href=\"https://docs.docker.com/security/for-admins/hardened-desktop/settings-management/\" rel=\"nofollow noopener\" target=\"_blank\">Settings Management</a>. If this is your case, feel free to contact us through the support&nbsp; for further information.</p><h2>Docker Agent’s Vision for 2025</h2><p>By 2025, we aim to expand the agent’s capabilities with features like customizing your experience with more context from your registry, enhanced GitHub Copilot integrations, and deeper presence across the development tools you already use. With regular updates and your feedback, Docker AI Agent is being built to become an indispensable part of your development process.</p><p>For now this beta is the start of an exciting evolution in how we approach developer productivity. Stay tuned for more updates as we continue to shape a smarter, more streamlined way to build, secure, and ship applications. We want to hear from you, if you like or want more information you can contact us.</p>","contentLength":6254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TOC Meeting 2025-02-04","url":"https://www.youtube.com/watch?v=R3H_ceqR6Us","date":1738707999,"author":"CNCF [Cloud Native Computing Foundation]","guid":563,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/R3H_ceqR6Us?version=3","enclosureMime":"","commentsUrl":null},{"title":"Digest #158: Kubernetes Security, PostgreSQL Upgrades, AWS Tagging, FinOps vs. DevOps, and Helm’s Future","url":"https://www.devopsbulletin.com/p/digest-158-kubernetes-security-postgresql","date":1738342111,"author":"Mohamed Labouardy","guid":197,"unread":true,"content":"<p><strong>Welcome to this week’s edition of the DevOps Bulletin!</strong></p><p>Ever wondered what Uber’s MySQL architecture looks like at scale? Or how the EU’s new DORA regulation will impact DevOps and cloud security? This week’s edition explores resilience engineering, Kubernetes controllers, and the next era of Helm.</p><p>This edition also includes hands-on tutorials on securing Maven proxy repositories, the risks of running Kubernetes containers as root, upgrading PostgreSQL with zero downtime, building a client VPN on AWS with Terraform, and automating Git Bisect with ephemeral environments. There’s also a closer look at AWS tagging best practices, Kubernetes API server proxies, and heap exploitation mechanisms.</p><p>On the open-source front, we’re featuring a browser extension analysis tool for security, a graph database designed for testing new techniques, a CLI to clean up Terraform configurations, and a project that simplifies AI model deployment using OCI containers. Plus, a discussion on the ongoing debate between DevOps and FinOps, and whether these two disciplines can truly coexist or if their priorities will always clash.</p><p>All this and more in this week’s DevOps Bulletin—don’t miss out!</p><p>Highlighting cool DevOps projects to keep an eye on:</p><ul><li><p> is a Python tool for analyzing browser extensions through a risk management lens.</p></li></ul><ul><li><p> is a graph database system for testing new database and graph techniques, supporting RDF/SPARQL and Property Graphs.</p></li><li><p> simplifies AI model deployment using OCI containers, automatically managing dependencies and GPU/CPU execution.</p></li><li><p> is a CLI tool that alphabetically sorts Terraform variables and outputs while fixing spacing and formatting issues</p></li><li><p> is a versatile tool manager that handles dev tools, environment variables, and task automation across projects</p></li></ul><div><p>If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":1926,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ff08844-2219-4b9e-8c15-7c85207f0c1d_1024x605.jpeg","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 44 (k3s)","url":"https://www.youtube.com/watch?v=6vYfJ6MM9_o","date":1738302651,"author":"CNCF [Cloud Native Computing Foundation]","guid":562,"unread":true,"content":"<article>K3s, a CNCF Sandbox project, is a lightweight, certified Kubernetes distribution designed for simplicity, minimal resource consumption, and ease of use. Its small binary size, pre-bundled dependencies, and single-command installation make it ideal for edge computing, IoT, and development environments. \n\nK3s feature low hardware requirements, support for ARM devices, and optimized performance, so it runs seamlessly on resource-constrained systems. Join CNCF Ambassador Aditya Soni while he explores how K3s ensures compatibility with Kubernetes tools while lowering the complexity and cost of deployment.</article>","contentLength":607,"flags":null,"enclosureUrl":"https://www.youtube.com/v/6vYfJ6MM9_o?version=3","enclosureMime":"","commentsUrl":null},{"title":"Deaf and Hard of Hearing WG Meeting - 2025-01-28","url":"https://www.youtube.com/watch?v=Rndt_zS6v_w","date":1738249798,"author":"CNCF [Cloud Native Computing Foundation]","guid":561,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Rndt_zS6v_w?version=3","enclosureMime":"","commentsUrl":null},{"title":"Linkerd, with William Morgan","url":"http://sites.libsyn.com/419861/linkerd-with-william-morgan","date":1738110300,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":674,"unread":true,"content":"<p dir=\"ltr\"><a href=\"https://www.linkedin.com/in/wmorgan/\">William Morgan</a> is the CEO of <a href=\"https://buoyant.io/\">Buoyant</a>, the company behind Linkerd. You worked at Twitter before as a software engineer and engineering manager and you have a long experience in the field.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week  Links from the interview ","contentLength":295,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPod246.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"ASL Container","url":"https://www.youtube.com/watch?v=meUtsFU7ndo","date":1737998838,"author":"CNCF [Cloud Native Computing Foundation]","guid":560,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/meUtsFU7ndo?version=3","enclosureMime":"","commentsUrl":null},{"title":"ASL Autoscaling 2","url":"https://www.youtube.com/watch?v=cGONmC1smaM","date":1737997887,"author":"CNCF [Cloud Native Computing Foundation]","guid":559,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/cGONmC1smaM?version=3","enclosureMime":"","commentsUrl":null},{"title":"ASL Reliability","url":"https://www.youtube.com/watch?v=pQluo2FG2eA","date":1737997855,"author":"CNCF [Cloud Native Computing Foundation]","guid":558,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/pQluo2FG2eA?version=3","enclosureMime":"","commentsUrl":null},{"title":"ASL Serverless 2","url":"https://www.youtube.com/watch?v=rbyBgXqCN2k","date":1737997826,"author":"CNCF [Cloud Native Computing Foundation]","guid":557,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/rbyBgXqCN2k?version=3","enclosureMime":"","commentsUrl":null},{"title":"Digest #157: GitHub Actions Risks, FinOps Shift, Kubernetes Security, and Terraform Best Practices","url":"https://www.devopsbulletin.com/p/digest-157-github-actions-risks-finops","date":1737721866,"author":"Mohamed Labouardy","guid":196,"unread":true,"content":"<p><strong>Welcome to this week’s edition of the DevOps Bulletin!</strong></p><p>Ever stumbled upon a bizarre root cause that left you scratching your head? Or questioned whether GitHub Actions is still the best CI/CD option? This week’s edition explores unexpected failures, lessons from building observability with GCP, and the secret weapon every DevOps engineer needs. There’s also a breakdown of the Flexera-NetApp FinOps acquisition, along with tutorials on securing Grafana, detecting honeypots in AWS, and mastering IAM policy checks.</p><p>On the open-source front, we’re highlighting some fascinating projects: a Bash-to-Go transpiler for faster and more secure scripts, Klarna’s internal threat modeling tool for system security, and a distributed key-value NoSQL database built on RocksDB with full Redis compatibility. Plus, an open-source Terraform pre-processor that helps you write less code without losing coverage and an Apache project redefining data lakehouse efficiency.</p><p>All this and more in this week’s DevOps Bulletin—don’t miss out!</p><p>Highlighting cool DevOps projects to keep an eye on:</p><ul><li><p> transpiles Bash scripts into Go, compiling them into fast, portable binaries for better performance and security.</p></li><li><p> is Klarna's internal threat modeling tool that helps engineers collaboratively create system dataflow diagrams with attached threats and controls.</p></li></ul><ul><li><p> is a native implementation of a document-oriented NoSQL database, enabling seamless CRUD operations on BSON data types within a PostgreSQL framework.</p></li><li><p>&nbsp;is a distributed key-value NoSQL database that uses RocksDB as a storage engine and is compatible with Redis protocol.</p></li><li><p> is a Terraform code pre-processor. Its primary goal is to minimize your total Terraform codebase without giving up on coverage. To do more with less.</p></li><li><p> Hudi is an open data lakehouse platform for efficient data ingestion, storage, indexing, and management across clouds.</p></li></ul><div><p>If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":2027,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F388278dc-7596-4322-b679-ddd0aee5d8e0_2029x1021.png","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}