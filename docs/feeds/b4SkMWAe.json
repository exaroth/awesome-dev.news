{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":50,"items":[{"title":"Navigating Failures in Pods With Devices","url":"https://kubernetes.io/blog/2025/07/03/navigating-failures-in-pods-with-devices/","date":1751500800,"author":"","guid":181564,"unread":true,"content":"<p>Kubernetes is the de facto standard for container orchestration, but when it\ncomes to handling specialized hardware like GPUs and other accelerators, things\nget a bit complicated. This blog post dives into the challenges of managing\nfailure modes when operating pods with devices in Kubernetes, based on insights\nfrom <a href=\"https://sched.co/1i7pT\">Sergey Kanzhelev and Mrunal Patel's talk at KubeCon NA\n2024</a>. You can follow the links to\n<a href=\"https://static.sched.com/hosted_files/kccncna2024/b9/KubeCon%20NA%202024_%20Navigating%20Failures%20in%20Pods%20With%20Devices_%20Challenges%20and%20Solutions.pptx.pdf?_gl=1*191m4j5*_gcl_au*MTU1MDM0MTM1My4xNzMwOTE4ODY5LjIxNDI4Nzk1NDIuMTczMTY0ODgyMC4xNzMxNjQ4ODIy*FPAU*MTU1MDM0MTM1My4xNzMwOTE4ODY5\">slides</a>\nand\n<a href=\"https://www.youtube.com/watch?v=-YCnOYTtVO8&amp;list=PLj6h78yzYM2Pw4mRw4S-1p_xLARMqPkA7&amp;index=150\">recording</a>.</p><h2>The AI/ML boom and its impact on Kubernetes</h2><p>The rise of AI/ML workloads has brought new challenges to Kubernetes. These\nworkloads often rely heavily on specialized hardware, and any device failure can\nsignificantly impact performance and lead to frustrating interruptions. As\nhighlighted in the 2024 <a href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\">Llama\npaper</a>,\nhardware issues, particularly GPU failures, are a major cause of disruption in\nAI/ML training. You can also learn how much effort NVIDIA spends on handling\ndevices failures and maintenance in the KubeCon talk by <a href=\"https://kccncna2024.sched.com/event/1i7kJ/all-your-gpus-are-belong-to-us-an-inside-look-at-nvidias-self-healing-geforce-now-infrastructure-ryan-hallisey-piotr-prokop-pl-nvidia\">Ryan Hallisey and Piotr\nProkop All-Your-GPUs-Are-Belong-to-Us: An Inside Look at NVIDIA's Self-Healing\nGeForce NOW\nInfrastructure</a>\n(<a href=\"https://www.youtube.com/watch?v=iLnHtKwmu2I\">recording</a>) as they see 19\nremediation requests per 1000 nodes a day!\nWe also see data centers offering spot consumption models and overcommit on\npower, making device failures commonplace and a part of the business model.</p><p>However, Kubernetes’s view on resources is still very static. The resource is\neither there or not. And if it is there, the assumption is that it will stay\nthere fully functional - Kubernetes lacks good support for handling full or partial\nhardware failures. These long-existing assumptions combined with the overall complexity of a setup lead\nto a variety of failure modes, which we discuss here.</p><h3>Understanding AI/ML workloads</h3><p>Generally, all AI/ML workloads require specialized hardware, have challenging\nscheduling requirements, and are expensive when idle. AI/ML workloads typically\nfall into two categories - training and inference. Here is an oversimplified\nview of those categories’ characteristics, which are different from traditional workloads\nlike web services:</p><dl><dd>These workloads are resource-intensive, often consuming entire\nmachines and running as gangs of pods. Training jobs are usually \"run to\ncompletion\" - but that could be days, weeks or even months. Any failure in a\nsingle pod can necessitate restarting the entire step across all the pods.</dd><dd>These workloads are usually long-running or run indefinitely,\nand can be small enough to consume a subset of a Node’s devices or large enough to span\nmultiple nodes. They often require downloading huge files with the model\nweights.</dd></dl><p>These workload types specifically break many past assumptions:</p><table><caption>Workload assumptions before and now</caption><tbody><tr><td>Can get a better CPU and the app will work faster.</td><td>Require a  device (or ) to run.</td></tr><tr><td>When something doesn’t work, just recreate it.</td><td>Allocation or reallocation is expensive.</td></tr><tr><td>Any node will work. No need to coordinate between Pods.</td><td>Scheduled in a special way - devices often connected in a cross-node topology.</td></tr><tr><td>Each Pod can be plug-and-play replaced if failed.</td><td>Pods are a part of a larger task. Lifecycle of an entire task depends on each Pod.</td></tr><tr><td>Container images are slim and easily available.</td><td>Container images may be so big that they require special handling.</td></tr><tr><td>Long initialization can be offset by slow rollout.</td><td>Initialization may be long and should be optimized, sometimes across many Pods together.</td></tr><tr><td>Compute nodes are commoditized and relatively inexpensive, so some idle time is acceptable.</td><td>Nodes with specialized hardware can be an order of magnitude more expensive than those without, so idle time is very wasteful.</td></tr></tbody></table><p>The existing failure model was relying on old assumptions. It may still work for\nthe new workload types, but it has limited knowledge about devices and is very\nexpensive for them. In some cases, even prohibitively expensive. You will see\nmore examples later in this article.</p><h3>Why Kubernetes still reigns supreme</h3><p>This article is not going deeper into the question: why not start fresh for\nAI/ML workloads since they are so different from the traditional Kubernetes\nworkloads. Despite many challenges, Kubernetes remains the platform of choice\nfor AI/ML workloads. Its maturity, security, and rich ecosystem of tools make it\na compelling option. While alternatives exist, they often lack the years of\ndevelopment and refinement that Kubernetes offers. And the Kubernetes developers\nare actively addressing the gaps identified in this article and beyond.</p><h2>The current state of device failure handling</h2><p>This section outlines different failure modes and the best practices and DIY\n(Do-It-Yourself) solutions used today. The next session will describe a roadmap\nof improving things for those failure modes.</p><h3>Failure modes: K8s infrastructure</h3><p>In order to understand the failures related to the Kubernetes infrastructure,\nyou need to understand how many moving parts are involved in scheduling a Pod on\nthe node. The sequence of events when the Pod is scheduled in the Node is as\nfollows:</p><ol><li> is scheduled on the Node</li><li> is registered with the  via local gRPC</li><li> uses  to watch for devices and updates capacity of\nthe node</li><li> places a  on a Node based on the updated capacity</li><li> asks  to  devices for a </li><li> creates a  with the allocated devices attached to it</li></ol><p>This diagram shows some of those actors involved:</p><p>As there are so many actors interconnected, every one of them and every\nconnection may experience interruptions. This leads to many exceptional\nsituations that are often considered failures, and may cause serious workload\ninterruptions:</p><ul><li>Pods failing admission at various stages of its lifecycle</li><li>Pods unable to run on perfectly fine hardware</li><li>Scheduling taking unexpectedly long time</li></ul><p>The goal for Kubernetes is to make the interruption between these components as\nreliable as possible. Kubelet already implements retries, grace periods, and\nother techniques to improve it. The roadmap section goes into details on other\nedge cases that the Kubernetes project tracks. However, all these improvements\nonly work when these best practices are followed:</p><ul><li>Configure and restart kubelet and the container runtime (such as containerd or CRI-O)\nas early as possible to not interrupt the workload.</li><li>Monitor device plugin health and carefully plan for upgrades.</li><li>Do not overload the node with less-important workloads to prevent interruption\nof device plugin and other components.</li><li>Configure user pods tolerations to handle node readiness flakes.</li><li>Configure and code graceful termination logic carefully to not block devices\nfor too long.</li></ul><p>Another class of Kubernetes infra-related issues is driver-related. With\ntraditional resources like CPU and memory, no compatibility checks between the\napplication and hardware were needed. With special devices like hardware\naccelerators, there are new failure modes. Device drivers installed on the node:</p><ul><li>Be compatible with an app</li><li>Must work with other drivers (like <a href=\"https://developer.nvidia.com/nccl\">nccl</a>,\netc.)</li></ul><p>Best practices for handling driver versions:</p><ul><li>Monitor driver installer health</li><li>Plan upgrades of infrastructure and Pods to match the version</li><li>Have canary deployments whenever possible</li></ul><p>Following the best practices in this section and using device plugins and device\ndriver installers from trusted and reliable sources generally eliminate this\nclass of failures. Kubernetes is tracking work to make this space even better.</p><h3>Failure modes: device failed</h3><p>There is very little handling of device failure in Kubernetes today. Device\nplugins report the device failure only by changing the count of allocatable\ndevices. And Kubernetes relies on standard mechanisms like liveness probes or\ncontainer failures to allow Pods to communicate the failure condition to the\nkubelet. However, Kubernetes does not correlate device failures with container\ncrashes and does not offer any mitigation beyond restarting the container while\nbeing attached to the same device.</p><p>This is why many plugins and DIY solutions exist to handle device failures based\non various signals.</p><p>In many cases a failed device will result in unrecoverable and very expensive\nnodes doing nothing. A simple DIY solution is a . The\ncontroller could compare the device allocatable count with the capacity and if\nthe capacity is greater, it starts a timer. Once the timer reaches a threshold,\nthe health controller kills and recreates a node.</p><p>There are problems with the  approach:</p><ul><li>Root cause of the device failure is typically not known</li><li>The controller is not workload aware</li><li>Failed device might not be in use and you want to keep other devices running</li><li>The detection may be too slow as it is very generic</li><li>The node may be part of a bigger set of nodes and simply cannot be deleted in\nisolation without other nodes</li></ul><p>There are variations of the health controller solving some of the problems\nabove. The overall theme here though is that to best handle failed devices, you\nneed customized handling for the specific workload. Kubernetes doesn’t yet offer\nenough abstraction to express how critical the device is for a node, for the\ncluster, and for the Pod it is assigned to.</p><p>Another DIY approach for device failure handling is a per-pod reaction on a\nfailed device. This approach is applicable for  workloads that are\nimplemented as Jobs.</p><p>There are some problems with the  approach for Jobs:</p><ul><li>There is no well-known  condition, so this approach does not work for the\ngeneric Pod case</li><li>Error codes must be coded carefully and in some cases are hard to guarantee.</li><li>Only works with Jobs with , due to the limitation of a pod\nfailure policy feature.</li></ul><p>So, this solution has limited applicability.</p><p>A little more generic approach is to implement the Pod watcher as a DIY solution\nor use some third party tools offering this functionality. The pod watcher is\nmost often used to handle device failures for inference workloads.</p><p>Since Kubernetes just keeps a pod assigned to a device, even if the device is\nreportedly unhealthy, the idea is to detect this situation with the pod watcher\nand apply some remediation. It often involves obtaining device health status and\nits mapping to the Pod using Pod Resources API on the node. If a device fails,\nit can then delete the attached Pod as a remediation. The replica set will\nhandle the Pod recreation on a healthy device.</p><p>The other reasons to implement this watcher:</p><ul><li>Without it, the Pod will keep being assigned to the failed device forever.</li><li>There is no  for a pod with .</li><li>There are no built-in controllers that delete Pods in CrashLoopBackoff.</li></ul><p>Problems with the :</p><ul><li>The signal for the pod watcher is expensive to get, and involves some\nprivileged actions.</li><li>It is a custom solution and it assumes the importance of a device for a Pod.</li><li>The pod watcher relies on external controllers to reschedule a Pod.</li></ul><p>There are more variations of DIY solutions for handling device failures or\nupcoming maintenance. Overall, Kubernetes has enough extension points to\nimplement these solutions. However, some extension points require higher\nprivilege than users may be comfortable with or are too disruptive. The roadmap\nsection goes into more details on specific improvements in handling the device\nfailures.</p><h3>Failure modes: container code failed</h3><p>When the container code fails or something bad happens with it, like out of\nmemory conditions, Kubernetes knows how to handle those cases. There is either\nthe restart of a container, or a crash of a Pod if it has \nand scheduling it on another node. Kubernetes has limited expressiveness on what\nis a failure (for example, non-zero exit code or liveness probe failure) and how\nto react on such a failure (mostly either Always restart or immediately fail the\nPod).</p><p>This level of expressiveness is often not enough for the complicated AI/ML\nworkloads. AI/ML pods are better rescheduled locally or even in-place as that\nwould save on image pulling time and device allocation. AI/ML pods are often\ninterconnected and need to be restarted together. This adds another level of\ncomplexity and optimizing it often brings major savings in running AI/ML\nworkloads.</p><p>There are various DIY solutions to handle Pod failures orchestration. The most\ntypical one is to wrap a main executable in a container by some orchestrator.\nAnd this orchestrator will be able to restart the main executable whenever the\njob needs to be restarted because some other pod has failed.</p><p>Solutions like this are very fragile and elaborate. They are often worth the\nmoney saved comparing to a regular JobSet delete/recreate cycle when used in\nlarge training jobs. Making these solutions less fragile and more streamlined\nby developing new hooks and extension points in Kubernetes will make it\neasy to apply to smaller jobs, benefiting everybody.</p><h3>Failure modes: device degradation</h3><p>Not all device failures are terminal for the overall workload or batch job.\nAs the hardware stack gets more and more\ncomplex, misconfiguration on one of the hardware stack layers, or driver\nfailures, may result in devices that are functional, but lagging on performance.\nOne device that is lagging behind can slow down the whole training job.</p><p>We see reports of such cases more and more often. Kubernetes has no way to\nexpress this type of failures today and since it is the newest type of failure\nmode, there is not much of a best practice offered by hardware vendors for\ndetection and third party tooling for remediation of these situations.</p><p>Typically, these failures are detected based on observed workload\ncharacteristics. For example, the expected speed of AI/ML training steps on\nparticular hardware. Remediation for those issues is highly depend on a workload needs.</p><p>As outlined in a section above, Kubernetes offers a lot of extension points\nwhich are used to implement various DIY solutions. The space of AI/ML is\ndeveloping very fast, with changing requirements and usage patterns. SIG Node is\ntaking a measured approach of enabling more extension points to implement the\nworkload-specific scenarios over introduction of new semantics to support\nspecific scenarios. This means prioritizing making information about failures\nreadily available over implementing automatic remediations for those failures\nthat might only be suitable for a subset of workloads.</p><p>This approach ensures there are no drastic changes for workload handling which\nmay break existing, well-oiled DIY solutions or experiences with the existing\nmore traditional workloads.</p><p>Many error handling techniques used today work for AI/ML, but are very\nexpensive. SIG Node will invest in extension points to make those cheaper, with\nthe understanding that the price cutting for AI/ML is critical.</p><p>The following is the set of specific investments we envision for various failure\nmodes.</p><h3>Roadmap for failure modes: K8s infrastructure</h3><p>The area of Kubernetes infrastructure is the easiest to understand and very\nimportant to make right for the upcoming transition from Device Plugins to DRA.\nSIG Node is tracking many work items in this area, most notably the following:</p><p>Basically, every interaction of Kubernetes components must be reliable via\neither the kubelet improvements or the best practices in plugins development\nand deployment.</p><h3>Roadmap for failure modes: device failed</h3><p>For the device failures some patterns are already emerging in common scenarios\nthat Kubernetes can support. However, the very first step is to make information\nabout failed devices available easier. The very first step here is the work in\n<a href=\"https://kep.k8s.io/4680\">KEP 4680</a> (Add Resource Health Status to the Pod Status for\nDevice Plugin and DRA).</p><p>Longer term ideas include to be tested:</p><ul><li>Integrate device failures into Pod Failure Policy.</li><li>Node-local retry policies, enabling pod failure policies for Pods with\nrestartPolicy=OnFailure and possibly beyond that.</li><li>Ability to  pod, including with the , so it can\nget a new device allocated.</li><li>Add device health to the ResourceSlice used to represent devices in DRA,\nrather than simply withdrawing an unhealthy device from the ResourceSlice.</li></ul><h3>Roadmap for failure modes: container code failed</h3><p>The main improvements to handle container code failures for AI/ML workloads are\nall targeting cheaper error handling and recovery. The cheapness is mostly\ncoming from reuse of pre-allocated resources as much as possible. From reusing\nthe Pods by restarting containers in-place, to node local restart of containers\ninstead of rescheduling whenever possible, to snapshotting support, and\nre-scheduling prioritizing the same node to save on image pulls.</p><p>Consider this scenario: A big training job needs 512 Pods to run. And one of the\npods failed. It means that all Pods need to be interrupted and synced up to\nrestart the failed step. The most efficient way to achieve this generally is to\nreuse as many Pods as possible by restarting them in-place, while replacing the\nfailed pod to clear up the error from it. Like demonstrated in this picture:</p><p>It is possible to implement this scenario, but all solutions implementing it are\nfragile due to lack of certain extension points in Kubernetes. Adding these\nextension points to implement this scenario is on the Kubernetes roadmap.</p><h3>Roadmap for failure modes: device degradation</h3><p>There is very little done in this area - there is no clear detection signal,\nvery limited troubleshooting tooling, and no built-in semantics to express the\n\"degraded\" device on Kubernetes. There has been discussion of adding data on\ndevice performance or degradation in the ResourceSlice used by DRA to represent\ndevices, but it is not yet clearly defined. There are also projects like\n<a href=\"https://github.com/medik8s/node-healthcheck-operator\">node-healthcheck-operator</a>\nthat can be used for some scenarios.</p><p>We expect developments in this area from hardware vendors and cloud providers, and we expect to see mostly DIY\nsolutions in the near future. As more users get exposed to AI/ML workloads, this\nis a space needing feedback on patterns used here.</p><p>The Kubernetes community encourages feedback and participation in shaping the\nfuture of device failure handling. Join SIG Node and contribute to the ongoing\ndiscussions!</p><p>This blog post provides a high-level overview of the challenges and future\ndirections for device failure management in Kubernetes. By addressing these\nissues, Kubernetes can solidify its position as the leading platform for AI/ML\nworkloads, ensuring resilience and reliability for applications that depend on\nspecialized hardware.</p>","contentLength":18095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: Optimizing Web Applications by Offloading Heavy Processing To Kuberne... Asami Okina","url":"https://www.youtube.com/watch?v=8pVt9dhHEVc","date":1751497305,"author":"CNCF [Cloud Native Computing Foundation]","guid":181437,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: Optimizing Web Applications by Offloading Heavy Processing To Kubernetes Jobs - Asami Okina, Craftsman Software, Inc.\n\nWhile typical web applications do not require large amounts of resources constantly, there are cases where specific processes consume significant CPU and memory.\n\nIn this session, we will introduce an architecture that offloads such resource-intensive processes to Kubernetes Jobs.\n\nWe will explain specific methods for Job management, how to integrate web applications (Next.js, @kubernetes/client-node) with the Kubernetes API, methods for data integration between Jobs and web applications, and real-time tracking of Job progress in the UI, all while sharing practical examples. Furthermore, we will provide a detailed introduction to a pattern where Kubernetes Job definitions generated from applications are managed using ConfigMaps, enabling quick configuration switching between environments, and offer hints to optimize your applications in terms of cost, performance, and management.</article>","contentLength":1410,"flags":null,"enclosureUrl":"https://www.youtube.com/v/8pVt9dhHEVc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Bitrise to Invest $3M in DevOps Cloud for Mobile Apps in EU","url":"https://devops.com/bitrise-to-invest-3m-in-devops-cloud-for-mobile-apps-in-eu/?utm_source=rss&utm_medium=rss&utm_campaign=bitrise-to-invest-3m-in-devops-cloud-for-mobile-apps-in-eu","date":1751482919,"author":"Mike Vizard","guid":181007,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon Nova Canvas update: Virtual try-on and style options now available","url":"https://aws.amazon.com/blogs/aws/amazon-nova-canvas-update-virtual-try-on-and-style-options-now-available/","date":1751481703,"author":"Matheus Guimaraes","guid":180954,"unread":true,"content":"<p>Have you ever wished you could quickly visualize how a new outfit might look on you before making a purchase? Or how a piece of furniture would look in your living room? Today, we’re excited to introduce a new virtual try-on capability in <a href=\"https://aws.amazon.com/ai/generative-ai/nova/creative??trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">Amazon Nova Canvas</a> that makes this possible. In addition, we are adding eight new style options for improved style consistency for text-to-image based style prompting. These features expand Nova Canvas AI-powered image generation capabilities making it easier than ever to create realistic product visualizations and stylized images that can enhance the experience of your customers.</p><p>Let’s take a quick look at how you can start using these today.</p><p> The first thing is to make sure that you have access to the Nova Canvas model through the usual means. Head to the <a href=\"https://console.aws.amazon.com/bedrock?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">Amazon Bedrock console</a>, choose  and <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/getting-started-console.html#getting-started-access?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">enable Amazon Nova Canvas for your account</a> making sure that you select the appropriate regions for your workloads. If you already have access and have been using Nova Canvas, you can start using the new features immediately as they’re automatically available to you.</p><p>The first exciting new feature is . With this, you can upload two pictures and ask Amazon Nova Canvas to put them together with realistic results. These could be pictures of apparel, accessories, home furnishings, and any other products including clothing. For example, you can provide the picture of a human as the source image and the picture of a garment as the reference image, and Amazon Nova Canvas will create a new image with that same person wearing the garment. Let’s try this out!</p><p>My starting point is to select two images. I picked one of myself in a pose that I think would work well for a clothes swap and a picture of an AWS-branded hoodie.</p><p>Note that Nova Canvas accepts images containing a maximum of 4.1M pixels – the equivalent of 2,048 x 2,048 – so be sure to scale your images to fit these constraints if necessary. Also, if you’d like to run the Python code featured in this article, ensure you have Python 3.9 or later installed as well as the Python packages boto3 and pillow.</p><p>To apply the hoodie to my photo, I use the Amazon Bedrock Runtime invoke API. You can find full details on the request and response structures for this API in the <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/image-generation.html\">Amazon Nova User Guide</a>. The code is straightforward, requiring only a few inference parameters. I use the new  of . I then specify the desired settings, including both the source image and reference image, using the  object to set a few required parameters. Note that both images must be converted to Base64 strings.</p><pre><code>import base64\n\n\ndef load_image_as_base64(image_path): \n   \"\"\"Helper function for preparing image data.\"\"\"\n   with open(image_path, \"rb\") as image_file:\n      return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\ninference_params = {\n   \"taskType\": \"VIRTUAL_TRY_ON\",\n   \"virtualTryOnParams\": {\n      \"sourceImage\": load_image_as_base64(\"person.png\"),\n      \"referenceImage\": load_image_as_base64(\"aws-hoodie.jpg\"),\n      \"maskType\": \"GARMENT\",\n      \"garmentBasedMask\": {\"garmentClass\": \"UPPER_BODY\"}\n   }\n}</code></pre><p>Nova Canvas uses masking to manipulate images. This&nbsp;is a technique that allows AI image generation to focus on specific areas or regions of an image while preserving others, similar to using painter’s tape to protect areas you don’t want to paint.</p><p>You can use three different masking modes, which you can choose by setting  to the correct value. In this case, I’m using , which requires me to specify which part of the body I want to be masked. I’m using  , but you can use others such as , , or  if you want to specifically target the feet. <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/image-generation.html\">Refer to the documentation</a>&nbsp;for a full list of options.</p><p>I then call the invoke API, passing in these inference arguments and saving the generated image to disk.</p><pre><code># Note: The inference_params variable from above is referenced below.\n\nimport base64\nimport io\nimport json\n\nimport boto3\nfrom PIL import Image\n\n# Create the Bedrock Runtime client.\nbedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n\n# Prepare the invocation payload.\nbody_json = json.dumps(inference_params, indent=2)\n\n# Invoke Nova Canvas.\nresponse = bedrock.invoke_model(\n   body=body_json,\n   modelId=\"amazon.nova-canvas-v1:0\",\n   accept=\"application/json\",\n   contentType=\"application/json\"\n)\n\n# Extract the images from the response.\nresponse_body_json = json.loads(response.get(\"body\").read())\nimages = response_body_json.get(\"images\", [])\n\n# Check for errors.\nif response_body_json.get(\"error\"):\n   print(response_body_json.get(\"error\"))\n\n# Decode each image from Base64 and save as a PNG file.\nfor index, image_base64 in enumerate(images):\n   image_bytes = base64.b64decode(image_base64)\n   image_buffer = io.BytesIO(image_bytes)\n   image = Image.open(image_buffer)\n   image.save(f\"image_{index}.png\")\n</code></pre><p>I get a very exciting result!</p><p>And just like that, I’m the proud wearer of an AWS-branded hoodie!</p><p>In addition to the  mask type, you can also use the  or  masks. With , you also provide the source and reference images, however, you provide a natural language prompt to specify which part of the source image you’d like to be replaced. This is similar to how the  and  tasks work in Nova Canvas. If you want to use your own image mask, then you choose the  mask type and provide a black-and-white image to be used as mask, where black indicates the pixels that you want to be replaced on the source image, and white the ones you want to preserve.</p><p>This capability is specifically useful for retailers. They can use it to help their customers make better purchasing decisions by seeing how products look before buying.</p><p> I’ve always wondered what I would look like as an anime superhero. Previously, I could use Nova Canvas to manipulate an image of myself, but I would have to rely on my good prompt engineering skills to get it right. Now, Nova Canvas comes with pre-trained styles that you can apply to your images to get high-quality results that follow the artistic style of your choice. There are eight available styles including 3D animated family film, design sketch, flat vector illustration, graphic novel, maximalism, midcentury retro, photorealism, and soft digital painting.</p><p>Applying them is as straightforward as passing in an extra parameter to the Nova Canvas API. Let’s try an example.</p><p>I want to generate an image of an AWS superhero using the 3D animated family film style. To do this, I specify a  of &nbsp;and a  object containing two parameters:  and . The  parameter contains the prompt describing the image I want to create which in this case is “a superhero in a yellow outfit with a big AWS logo and a cape.” The  parameter specifies one of the predefined style values. I’m using <code>\"3D_ANIMATED_FAMILY_FILM\"</code> here, but you can find the full list in the <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/image-generation.html\">Nova Canvas User Guide</a>.</p><pre><code>inference_params = {\n   \"taskType\": \"TEXT_IMAGE\",\n   \"textToImageParams\": {\n      \"text\": \"a superhero in a yellow outfit with a big AWS logo and a cape.\",\n      \"style\": \"3D_ANIMATED_FAMILY_FILM\",\n   },\n   \"imageGenerationConfig\": {\n      \"width\": 1280,\n      \"height\": 720,\n      \"seed\": 321\n   }\n}</code></pre><p>Then, I call the invoke API just as I did in the previous example. (The code has been omitted here for brevity.) And the result? Well, I’ll let you judge for yourself, but I have to say I’m quite pleased with the AWS superhero wearing my favorite color following the 3D animated family film style exactly as I envisioned.</p><p>What’s really cool is that I can keep my code and prompt exactly the same and only change the value of the style attribute to generate an image in a completely different style. Let’s try this out. I set  to .</p><pre><code>inference_params = { \n   \"taskType\": \"TEXT_IMAGE\", \n   \"textToImageParams\": { \n      \"text\": \"a superhero in a yellow outfit with a big AWS logo and a cape.\",\n      \"style\": \"PHOTOREALISM\",\n   },\n   \"imageGenerationConfig\": {\n      \"width\": 1280,\n      \"height\": 720,\n      \"seed\": 7\n   }\n}</code></pre><p>And the result is impressive! A photorealistic superhero exactly as I described, which is a far departure from the previous generated cartoon and all it took was changing one line of code.</p><p> Availability – Virtual try-on and style options are available in Amazon Nova Canvas in the US East (N. Virginia), Asia Pacific (Tokyo), and Europe (Ireland). Current users of Amazon Nova Canvas can immediately use these capabilities without migrating to a new model.</p><p>For a preview of virtual try-on of garments, you can visit <a href=\"https://nova.amazon.com/\">nova.amazon.com</a> where you can upload an image of a person and a garment to visualize different clothing combinations.</p><a href=\"https://www.linkedin.com/in/codingmatheus/\">Matheus Guimaraes | @codingmatheus</a>","contentLength":8644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using Gordon to Containerize Your Apps and Work with Containers","url":"https://www.docker.com/blog/containerize-your-apps-with-ask-gordon/","date":1751481281,"author":"Steve Buchanan","guid":180970,"unread":true,"content":"<p>These days, almost every tech company is looking for ways to integrate AI into their apps and workflows, and Docker is no exception. They’ve been rolling out some impressive AI capabilities across their products. This is my first post as a Docker Captain and in this post, I want to shine a spotlight on a feature that hasn’t gotten nearly enough attention in my opinion:  (also known as Docker AI), which is built into Docker Desktop and CLI.</p><p>Gordon is really helpful when it comes to containerizing applications. Not only does it help you understand how to package your app as a container, but it also reduces the overhead of figuring out dependencies, runtime configs, and other pieces that add to a developer’s daily cognitive load. The best part? Gordon doesn’t just guide you with responses; it can also generate or updatethe necessary files for you.</p><h2>The Problem: Containerizing apps and optimizing containers isn’t always easy</h2><p>Containerizing apps can range from super simple to a bit tricky, depending on what you’re working with. If your app has a  like Node.js, Python, or .NET Core, with <strong>clearly defined dependencies</strong> and , it will be straightforward.</p><p>A basic Dockerfile will usually get you up and running without much effort. But once you start adding more complexity, like a <strong>backend, frontend, database, and caching layer</strong>, you now have the need for a multi-container app. At this point, you might be dealing with additional Dockerfile configurations and potentially a Docker Compose setup. That’s where things can start to be challenging to get going.</p><p>This is where Gordon shines. It’s helpful in containerizing apps and can even handle multi-service container app setups, guiding you through what’s needed and even generating the supporting config files, such as Dockerfiles and docker-compose, to get you going.</p><h3>Optimizing containers can be a headache too</h3><p>Beyond just containerizing, there’s also the need to  for performance, security, and image size. And let’s face it, optimizing can be tedious. You need to know what base images to use, how to slim them down, how to avoid unnecessary layers, and more.</p><p>Gordon can help here too. It provides optimization suggestions, shows you how to apply best practices like multi-stage builds or removing dev dependencies, and helps you create leaner, more secure images.</p><h3>Why not just use general-purpose Generative AI?</h3><p>Sure, general-purpose AI tools like ChatGPT, Claude, Gemini, etc. are great and I use them regularly. But when it comes to containers, they can  needed for accurate and efficient help. Gordon, on the other hand, is . It has access to Docker’s ecosystem and has been trained on Docker documentation, best practices, and the nuances of Docker tooling. That means its recommendations are more likely to be precise and aligned with the latest standards.</p><p>Gordon can help with containerizing applications, optimizing your containers and more. Gordon is still a Beta feature. To start using Gordon, you need Docker Desktop version 4.38 or later. Gordon is powered by Large Language Models (LLMs), and it goes beyond prompt and response: it can perform certain tasks for you as an AI agent. Gordon can have access to your local files and local images when you give it permission. It will prompt you for access if needed for a task.</p><p>Please note, the examples I will show in this post are based on a single working session. Now, let’s dive in and start to explore Gordon.</p><h3>Enabling Gordon / Docker AI</h3><p>In order to turn Gordon on, go to  check the  box as shown in the following screenshot.&nbsp;</p><p><em>Figure 1: screenshot of where to enable Docker AI in beta features</em></p><p>Accept the terms. The AI in Docker Desktop is in two forms. The first one is through the Docker Desktop UI and is known as Gordon. The second option is Docker AI. Docker AI is accessed through the Docker CLI. The way you activate it is by typing Docker AI in the CLI. I will demonstrate this later on in this blog post.&nbsp;&nbsp;</p><p><em>Figure 2: screenshot of Docker AI terms acceptance dialog box</em></p><h3>Exploring Gordon in Docker Desktop</h3><p>Now Gordon will appear in your Docker Desktop UI. Here you can prompt it just like any Generative AI tool. Gordon will also have examples that you can use to get started working with it.</p><p>You can access Gordon throughout Docker Desktop by clicking on the AI icon as shown in the following screenshot.</p><p><em>Figure 3: screenshot of Docker Desktop interface showing the AI icon for Gordon</em></p><p>When you click on the AI icon a Gordon prompt box appears along with suggested prompts as shown in the following screenshot. The suggestions will change based on the object the AI is next to, and are context-aware.</p><p><em>Figure 4: Screenshot showing Gordon’s suggestion prompt box in Docker Desktop UI</em></p><p>Here is another example of Docker AI suggestions being context-aware based on what area of Docker Desktop you are in.&nbsp;</p><p><em>&nbsp;Figure 5: Screenshot showing Docker AI context- specific suggestions</em></p><p>Another common use case for Gordon is listing local images and using AI to work with them. You can see this in the following set of screenshots. Notice that Gordon will prompt you for permission before showing your local images.</p><p><em>Figure 6: Screenshot showing Gordon referencing local images</em></p><p>You can also prompt Gordon to take action. As shown in the following screenshot, I asked Gordon to run one of my images.</p><p><em>Figure 7: Screenshot showing Gordon prompts</em></p><p>If it can’t perform the action, it will attempt to help you.&nbsp;</p><p><em>Figure 8: Screenshot showing Gordon prompt response to failed request</em></p><p>Another cool use of Gordon is to explain a container image to you. When you ask this, Gordon will ask you to select the directory where the Dockerfile is and permission to access it as shown in the following screenshot.</p><p><em>Figure 9: Screenshot showing Gordon’s request for particular directory access</em></p><p>After you give it access to the directory where the Dockerfile is, it will then breakdown what’s in the Dockerfile.&nbsp;</p><p><em>Figure 10: Screenshot showing Gordon’s response to explaining a Dockerfile</em></p><p>As shown in the following screenshot, I followed up with a prompt asking Gordon to display what’s in the Dockerfile. It did a good job of explaining its contents, as shown in the following screenshot.</p><p><em>Figure 11: Screenshot showing Gordon’s response regarding Dockerfile contents</em></p><h3>Exploring Gordon in the Docker Desktop CLI</h3><p>Let’s take a quick tour through Gordon in the CLI. Gordon is referred to as Docker AI in the CLI. To work with Docker AI, you need to launch the Docker CLI as shown in the following screenshot.&nbsp;</p><p><em>Figure 12: Screenshot showing how to launch Docker AI from the CLI</em></p><p>Once in the CLI you can type “docker ai” and it will bring you into the chat experience so you can prompt Gordon. In my example, I asked Gordon about one of my local images. You can see that it asked me for permission.&nbsp;</p><p><em>Figure 13: Screenshot showing Docker CLI request for access</em></p><p>Next, I asked Docker AI to list all of my local images as shown in the following screenshot.&nbsp;</p><p><em>Figure 14: Screenshot showing Docker CLI response to display local images</em></p><p>I then tested pulling an image using Docker AI. As you can see in the following screenshot, Gordon pulled a nodeJS image for me!</p><p><em>Figure 15: Screenshot showing Docker CLI pulling nodeJS image</em></p><h3>Containerizing an application with Gordon</h3><p>Now let’s explore the experience of containerizing an application using Gordon.</p><p>I started by clicking on the example for containerizing an application. Gordon then prompted me for the directory where my application code is.&nbsp;</p><p><em>Figure 16: Screenshot showing where to enable access to directory for containerizing an application</em></p><p>I pointed it to my apps directory and gave it permission. It then started to analyze and containerize my app. It picked up the language and started to read through my app’s README file.</p><p><em>Figure 17: Screenshot showing Gordon starting to analyze and containerize app</em></p><p>You can see it understand the app was written in JavaScript and worked through the packages and dependencies.</p><p><em>Figure 18: Screenshot showing final steps of Gordon processing</em></p><p>Gordon understands that my app has a backend, frontend, and a database, knowing from this that I would need a Docker compose file.</p><p><em>Figure 19: Screenshot showing successful completion of steps to complete the Dockerfiles</em></p><p>From the following screenshot you can see the Docker related files needed for my app. Gordon created all of these.</p><p><em>Figure 20: Screenshot showing files produced from Gordon</em></p><p>Gordon created the Dockerfile (on the left) and a Compose yaml file (on the right) even picking up that I needed a Postgres DB for this application.</p><p><em>Figure 21: Screenshot showing Dockerfile and Compose yaml file produced from Gordon</em></p><p>I then took it a step further and asked Gordon to build and run the container for my application using the prompt “Can you build and run this application with compose?” It created the Docker Compose file, built the images, and ran the containers!</p><p><em>Figure 22: Screenshot showing completed containers from Gordon</em></p><p>I hope you picked up some useful insights about Docker and discovered one of its lesser-known AI features in Docker Desktop. We explored what Gordon is, how it compares to general-purpose generative AI tools like ChatGPT, Claude, and Gemini, and walked through use cases such as containerizing an application and working with local images. We also touched on how Gordon can support developers and IT professionals who work with containers. If you haven’t already, I encourage you to enable Gordon and take it for a test run. Thanks for reading and stay tuned for more blog posts coming soon.</p>","contentLength":9538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker Container Exits Immediately? Here’s How to Fix ENTRYPOINT & CMD Issues Fast","url":"https://blog.devops.dev/docker-container-exits-immediately-heres-how-to-fix-entrypoint-cmd-issues-fast-24903c44530a?source=rss----33f8b2d9a328---4","date":1751473563,"author":"Ali Hamza","guid":180839,"unread":true,"content":"<h4>Fix Docker containers that exit immediately by understanding and correcting ENTRYPOINT and CMD issues with real examples and step-by-step solutions.</h4><p>When you’re working with Docker, few things are more frustrating than building an image, running a container, and — <strong>boom — it exits immediately</strong>.</p><p>No error messages. No clear reason. Just&nbsp;silence.</p><p>This is a , especially for those just starting out. The culprit? It’s often related to how you define  and  in your Dockerfile.</p><p>In this guide, I’ll explain <strong>why your Docker container exits immediately</strong>, how to , and how to <strong>properly configure ENTRYPOINT and CMD</strong> so your container behaves as expected.</p><h3>Understanding the Problem: Why Docker Containers Exit Immediately</h3><p>When a Docker container exits right after starting, it usually means the container <strong>ran the default command and completed</strong>. Unlike virtual machines, Docker containers are designed to run a single process — when that process ends, the container exits&nbsp;too.</p><h4>How to Check What&nbsp;Happened</h4><p>You can use the following command to check the container’s status:</p><p>This shows all containers (even the stopped ones). Look at the  column to see if it exited immediately.</p><pre>docker logs &lt;container_id&gt;</pre><p>This often reveals errors or termination messages from your containerized app.</p><h3>ENTRYPOINT vs CMD: What’s the Difference?</h3><p>Both ENTRYPOINT and CMD are Dockerfile instructions used to define  when a container starts — but they work differently.</p><pre>ENTRYPOINT [\"python3\"]CMD [\"app.py\"]</pre><p>If you forget one or combine them wrong, the container may .</p><h3>Common ENTRYPOINT &amp; CMD Mistakes That Cause Immediate Exit</h3><p>Here are frequent issues that result in containers exiting:</p><ul><li><strong>CMD runs a short-lived command</strong>, like echo \"Done\" — it ends, so the container exits.</li><li><strong>Missing or misused ENTRYPOINT/CMD</strong> — container has no valid process to&nbsp;run.</li><li> (string vs exec&nbsp;format).</li><li>, so it fails silently.</li></ul><h3>Step-by-Step Guide to Debug the Exit&nbsp;Issue</h3><pre>docker logs &lt;container_id&gt;</pre><p>You might see errors&nbsp;like:</p><pre>bash: ./entrypoint.sh: Permission denied</pre><h4>2. Inspect the Dockerfile</h4><p>Look at the CMD and ENTRYPOINT lines. Are they defined properly?</p><h4>3. Run the Container Interactively</h4><p>Start with a shell to look&nbsp;inside:</p><pre>docker run -it &lt;image_name&gt; /bin/bash</pre><p>Then try running your script or command manually.</p><h3>How to Fix ENTRYPOINT and CMD Issues (with Examples)</h3><h4>Fix 1: Use CMD for Simple&nbsp;Apps</h4><pre>FROM python:3.10-slimCOPY app.py .<p>CMD [\"python3\", \"app.py\"]</p></pre><h4>Fix 2: Use ENTRYPOINT with CMD for Flexible Arguments</h4><pre>FROM ubuntuCOPY entrypoint.sh /entrypoint.sh<p>RUN chmod +x /entrypoint.sh</p>ENTRYPOINT [\"/entrypoint.sh\"]</pre><h4>Fix 3: Ensure Script is Executable</h4><h4>Fix 4: Avoid Shell Form (Unless&nbsp;Needed)</h4><pre>CMD [\"nginx\", \"-g\", \"daemon off;\"]</pre><pre>CMD nginx -g \"daemon off;\"</pre><p>Shell form can break argument parsing and&nbsp;signals.</p><h3>Working Dockerfile Examples</h3><pre>FROM python:3.10-slimCOPY app.py .<p>CMD [\"python3\", \"app.py\"]</p></pre><pre>FROM ubuntuCOPY entrypoint.sh /entrypoint.sh<p>RUN chmod +x /entrypoint.sh</p>ENTRYPOINT [\"/entrypoint.sh\"]</pre><p>If entrypoint.sh contains something like:</p><pre>#!/bin/bashecho \"Starting my app...\"</pre><p>Your container will now stay alive for 5 minutes (good for testing).</p><h3>Pro Tips to Avoid Exit Issues in the&nbsp;Future</h3><ul><li>Always test your image interactively:</li></ul><pre>docker run -it &lt;image&gt; bash</pre><ul><li>Stick to  ([\"cmd\", \"arg\"]) for ENTRYPOINT and&nbsp;CMD.</li><li>Check logs immediately using:</li></ul><pre>docker logs &lt;container_id&gt;</pre><ul><li>Ensure the main process runs , or your container will&nbsp;exit.</li><li>Add health checks for production deployments to monitor container status.</li></ul><p>A Docker container that  is almost always the result of an incorrect or short-lived command — often tied to  and&nbsp;.</p><ul><li>Understanding how Docker runs containers,</li><li>Debugging with logs and interactive shells,</li><li>And configuring your Dockerfile properly,</li></ul><p>…you can  these issues and get your container running the way you&nbsp;want.</p><p><strong>Have you faced a similar problem? Share your debugging tips in the comments below — let’s help each other&nbsp;grow!</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=24903c44530a\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"10 Steps To Ensure — Zero Trust Architecture in Kubernetes","url":"https://blog.devops.dev/10-steps-to-ensure-zero-trust-architecture-in-kubernetes-615ea58146de?source=rss----33f8b2d9a328---4","date":1751473558,"author":"Devops Diaries","guid":180838,"unread":true,"content":"<div><p>Implementing Zero Trust Architecture (ZTA) in Kubernetes involves designing security with the principle: “Never trust, always verify” —…</p></div>","contentLength":144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modern Observability in .NET: OpenTelemetry & Grafana","url":"https://blog.devops.dev/modern-observability-in-net-opentelemetry-grafana-4c08b9d74240?source=rss----33f8b2d9a328---4","date":1751473553,"author":"Adem KORKMAZ","guid":180837,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bash+Telegram : Simple Server Resource Alerting System","url":"https://blog.devops.dev/bash-telegram-simple-server-resource-alerting-system-833fcb534a33?source=rss----33f8b2d9a328---4","date":1751473551,"author":"bektiaw","guid":180836,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"11 Scripts to Transform Server Metrics into Insights","url":"https://blog.devops.dev/11-scripts-to-transform-server-metrics-into-insights-e461d72276eb?source=rss----33f8b2d9a328---4","date":1751473504,"author":"Obafemi","guid":180835,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker 101: The Complete Cheat Sheet for Devs & Ops","url":"https://blog.devops.dev/docker-101-the-complete-cheat-sheet-for-devs-ops-01ba7b13193a?source=rss----33f8b2d9a328---4","date":1751473496,"author":"Ashish Singh","guid":180834,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OSI Model Layer — Neworking Basic & Easiest Concept","url":"https://blog.devops.dev/osi-model-layer-neworking-basic-easiest-concept-f3cb2f15cd8f?source=rss----33f8b2d9a328---4","date":1751473472,"author":"Kamalpreet KAUR","guid":180833,"unread":true,"content":"<p>Let’s Deep Dive in OSI (Open System Interconnection Model)</p><p>The Open System Interconnection model is one of the crucial concepts in networking. It’s always made us stuck, and it’s one of the repetitive concepts. The main aim of the OSI Model is to understand the network flow. The basic strategy is to fully understand how the data streams flow up and down within the model -&gt; In other words, to understand the data flow that passes from one device to&nbsp;another.</p><h4>Let’s Begin with the Foundation</h4><blockquote>The OSI model was developed by the <strong><em>ISO (International Organization for Standardization) in the late 1970s and early 1980s</em></strong>. OSI is a<strong><em> as a standard or protocol itself</em></strong>it’s the reference to <strong>think about and describe the interactions between different layers of a computer&nbsp;network.</strong></blockquote><p>Imagine you are at home, and each room represents a layer (living room, study room&nbsp;…). These layers are interconnected with each other, which helps us to provide direction to move from one room to another. OSI reference model helps us to indicate the shared information between each&nbsp;other.</p><p><strong>The OSI model is like a blueprint of all layers </strong>that helps to understand <strong>each layer and its specifications (how different layers work with each&nbsp;other)</strong></p><h3>Every Layer Functionalities&nbsp;:</h3><h4>Functionality Explaination&nbsp;:</h4><blockquote>Application Layer — User Interface [GUI] Activities Presentation Layer — HTML pages / Readable webpages<p> Session Layer — Establish Communication Sessions </p>Transport layer — Data must arrive in order along with acknowledgement <p>Network Layer — Establish communication with distant devices</p>Data Link Layer — Data must be delivered to its correct destination<p>Physical Layer — Physical connectivity between devices involving cables and&nbsp;wires.</p></blockquote><h3>Difference between OSI Model and TCP/ IP&nbsp;Model</h3><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f3cb2f15cd8f\" width=\"1\" height=\"1\" alt=\"\">","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Successfully Deploy a Three-Tier application on AWS EKS using Terraform","url":"https://blog.devops.dev/how-to-successfully-deploy-a-three-tier-application-on-aws-eks-using-terraform-30fe80e82f88?source=rss----33f8b2d9a328---4","date":1751473467,"author":"Pravesh Sudha","guid":180832,"unread":true,"content":"<blockquote>Quick guide to provisioning and deploying a Three-Tier application on AWS EKS and Terraform</blockquote><p>Welcome to the world of cloud computing and automation. In this blog, we’re going to walk through an exciting real-world project — deploying a <strong>three-tier Todo List application</strong> on <strong>Amazon EKS (Elastic Kubernetes Service)</strong> using .</p><p>This project is perfect if you’re looking to get hands-on experience with:</p><ul><li><strong>Provisioning infrastructure using Terraform</strong></li><li> to containerize services</li><li><strong>Deploying applications on AWS using EKS, ECR, IAM</strong>, and&nbsp;more</li></ul><p>We’ll break it down step-by-step — from writing Terraform code to spinning up your Kubernetes cluster, containerizing the frontend, backend, and MongoDB services, and deploying everything seamlessly.</p><p>Whether you’re new to DevOps or brushing up on your cloud skills, this guide will help you understand how everything connects in a modern microservices-based deployment.</p><p>So without further ado, let’s get started and bring our infrastructure to life!&nbsp;🌐🛠️</p><h3>🔧 Prerequisites: What You’ll Need Before We&nbsp;Start</h3><p>Before we dive into the fun part — building and deploying — let’s quickly make sure your system is ready for action. Here’s what you’ll&nbsp;need:</p><p>✅ If you don’t already have one, head over to <a href=\"https://aws.amazon.com/\">aws.amazon.com</a> and sign up. We’ll be using AWS services like EKS (Elastic Kubernetes Service), ECR (Elastic Container Registry), and IAM (Identity and Access Management), so having an account is essential.</p><p>✅ We’ll use Docker to containerise the three components of our app: the frontend, backend, and MongoDB database. You can download Docker Desktop from the official Docker website and install it like any other&nbsp;app.</p><p>✅ Terraform will be our tool of choice for provisioning the infrastructure on AWS. You can download Terraform from <a href=\"https://developer.hashicorp.com/terraform/install\">terraform.io</a>. Just install it — no need to configure anything&nbsp;yet.</p><p>That’s it! Once you have these basics set up, you’re good to go. Let’s start building!</p><h3>🔐 Step 1: Set Up AWS CLI and IAM&nbsp;User</h3><p>Before Terraform can talk to AWS and spin up resources, we need to set up the  and create an  with the right permissions. Let’s walk through it step-by-step.</p><ol><li> to your AWS account as the root user (the one you used to sign&nbsp;up).</li></ol><ul><li>In the AWS Management Console, go to  and click on .</li></ul><ul><li>Give the user a name — something like three-tier-user works great — and click&nbsp;.</li><li>On the  page, attach the policy named .</li></ul><blockquote><em>: We’re giving full admin access here just to avoid permission issues during learning and experimentation. </em><strong><em>Never use this approach in production</em></strong><strong><em>Principle of Least Privilege</em></strong></blockquote><ul><li>Click  and then . You’re done with the IAM&nbsp;part!</li></ul><h3>📦 Install AWS CLI (Ubuntu/Linux)</h3><p>If you’re using , you can install the AWS CLI by running these commands in your terminal:</p><pre>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"unzip awscliv2.zip</pre><p>If you’re using a different operating system (like macOS or Windows), just head over to the official install guide here:👉 <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\">AWS CLI Installation Guide</a></p><h3>🔑 Generate Access Keys &amp; Configure AWS&nbsp;CLI</h3><ul><li>Go back to the  and click on your new user (three-tier-user).</li><li>Under the  tab, click on .</li></ul><ul><li>Choose <strong>Command Line Interface (CLI)</strong> as the use case, agree to the terms, and&nbsp;proceed.</li></ul><ul><li>Once the keys are generated, <strong>copy the Access Key ID and Secret Access Key</strong> (you’ll need them right&nbsp;away!).</li></ul><p>Now, go to your terminal and configure the AWS&nbsp;CLI:</p><p>It will prompt you to&nbsp;enter:</p><ul><li>: You can use us-east-1 for this&nbsp;demo</li><li>: Enter&nbsp;json</li></ul><p>That’s it! Your AWS CLI is now set up and ready to communicate with your AWS account&nbsp;🚀</p><h3>🛠️ Step 2: Install Terraform and Set Up Remote&nbsp;Backend</h3><p>Now that our AWS CLI is ready and configured, let’s install , our Infrastructure as Code (IaC) tool of choice for this project. We’ll also set up a secure and scalable way to store our Terraform state using an .</p><h3>📥 Installing Terraform on Ubuntu&nbsp;(amd64)</h3><p>If you’re using <strong>Ubuntu on an amd64 system</strong>, follow these commands to install Terraform:</p><pre>sudo apt-get update &amp;&amp; sudo apt-get install -y gnupg software-properties-common</pre><pre>echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \\https://apt.releases.hashicorp.com $(grep -oP '(?&lt;=UBUNTU_CODENAME=).*' /etc/os-release || lsb_release -cs) main\" | \\<p>sudo tee /etc/apt/sources.list.d/hashicorp.list</p></pre><pre>sudo apt updatesudo apt-get install terraform</pre><p>✅ After this, you can verify the installation with:</p><p>🖥️ If you’re on a different operating system or architecture, follow the official installation guide here:👉 <a href=\"https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli\">Terraform Install&nbsp;Guide</a></p><h3>🔐 AWS CLI + Terraform: Working&nbsp;Together</h3><p>Since we’ve already configured the AWS CLI, Terraform will automatically use the credentials (access key &amp; secret key) stored by aws configure. This means you’re ready to provision AWS resources securely and seamlessly.</p><h3>☁️ Best Practice: Use Remote Backend for Terraform State</h3><p>Terraform tracks the state of your infrastructure in a file called terraform.tfstate. By default, it’s stored locally, but that’s risky and not scalable. So, we’ll follow best practices and store this file remotely in an .</p><p>Here’s how to create an S3 bucket to act as your Terraform :</p><h4>🪣 Create an S3 Bucket for State&nbsp;Storage</h4><pre>aws s3api create-bucket \\  --bucket pravesh-terra-state-bucket \\</pre><h4>📜 Enable Versioning for State&nbsp;History</h4><pre>aws s3api put-bucket-versioning \\  --bucket pravesh-terra-state-bucket \\<p>  --versioning-configuration Status=Enabled</p></pre><h4>🔐 Enable Default Encryption</h4><pre>aws s3api put-bucket-encryption \\  --bucket pravesh-terra-state-bucket \\<p>  --server-side-encryption-configuration '{</p>    \"Rules\": [{<p>      \"ApplyServerSideEncryptionByDefault\": {</p>        \"SSEAlgorithm\": \"AES256\"    }]</pre><p>And that’s it! You now have a secure, versioned, and encrypted S3 bucket ready to store your Terraform state files — a key step toward building a production-grade infrastructure.</p><h3>📦 Step 3: Clone the Project and Provision Infrastructure with Terraform</h3><p>With all the groundwork done — AWS CLI set up, Terraform installed, and the backend ready — it’s time to move on to the actual&nbsp;project!</p><p>The codebase for our  is available on my GitHub repository:</p><p>To get started, open your terminal and run the following commands:</p><pre>git clone https://github.com/Pravesh-Sudha/3-tier-app-Deploymentcd 3-tier-app-Deployment/</pre><p>Inside the cloned repo, you’ll find a folder named terra-config/. That’s where all the Terraform magic happens. Navigate into that directory:</p><p>Now initialize the Terraform backend (which we configured to use your S3 bucket earlier):</p><p>This will configure Terraform to use the remote backend for storing the state file. If your bucket name is different from mine (pravesh-terra-state-bucket), make sure to update the name in backend.tf.</p><h3>📁 Understanding the Terraform Code Structure</h3><p>Instead of dumping everything into a single main.tf file, I’ve broken the configuration into logical modules for clarity and scalability. Here’s a quick overview:</p><ul><li>provider.tf: Specifies the cloud provider. In our case, it’s AWS (no surprise&nbsp;there!).</li><li>backend.tf: Configures Terraform to store state remotely in our S3&nbsp;bucket.</li><li>ecr.tf: Creates two public repositories in ECR: 3-tier-frontend and 3-tier-backend for storing Docker&nbsp;images.</li><li>vpc.tf: Fetches the default VPC and subnet&nbsp;details.</li><li>role.tf: Defines IAM&nbsp;roles:</li></ul><p>— One for the EKS cluster (includes AmazonEKSClusterPolicy)</p><p>— One for the Node Group (includes policies like AmazonEKSWorkerNodePolicy, AmazonEC2ContainerRegistryReadOnly, and AmazonEKS_CNI_Policy)</p><ul><li>eks.tf: Provisions the EKS cluster named Three-tier-cloud.</li><li>node_group.tf: Creates the worker node group for the cluster with one t2.medium EC2 instance.</li></ul><h3>⏳ Apply the Terraform Configuration</h3><p>Now we’re ready to provision the infrastructure! Run the following command:</p><pre>terraform apply --auto-approve</pre><p>⏱️ This might take , especially since provisioning EKS clusters and node groups can take some time. Be patient — AWS is building your cloud infrastructure behind the&nbsp;scenes.</p><h3>🐳 Push Docker Images to&nbsp;ECR</h3><p>Once the infrastructure is up, it’s time to push our Docker images for the frontend and backend to AWS&nbsp;ECR.</p><ul><li>Go to your <strong>AWS Console &gt; ECR &gt; Repositories</strong></li></ul><ul><li>Click on the 3-tier-frontend repository</li><li>Click on  — AWS will show you four CLI&nbsp;commands</li></ul><p>Now, go to the frontend/ folder in your project directory:</p><p>Run each of the four commands one by one to build the image and push it to&nbsp;ECR.</p><p>Repeat the same steps for the 3-tier-backend repository:</p><ul><li>Go back to </li><li>Select 3-tier-backend and click </li></ul><ul><li>Navigate to the backend directory:</li></ul><p>Run the ECR commands provided to push the backend Docker&nbsp;image.</p><p>🎉 Once done, your container images will be hosted in your private AWS ECR repositories — ready to be deployed to your EKS&nbsp;cluster!</p><h3>🌐 Step 4: Deploy to EKS with kubectl and Set Up Ingress via&nbsp;ALB</h3><p>Now that your EKS cluster and ECR repositories are ready, it’s time to interact with the cluster, deploy your workloads, and expose your application to the internet. We’ll use  for that — the command-line tool to manage Kubernetes clusters.</p><p>If you’re using , run the following to install&nbsp;kubectl:</p><pre>curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl  chmod +x ./kubectl  <p>sudo mv ./kubectl /usr/local/bin  </p>kubectl version --short --client</pre><p>If you’re using a different OS/architecture, install it using the official instructions:👉 <a href=\"https://kubernetes.io/docs/tasks/tools/\">kubectl Install&nbsp;Guide</a></p><h3>🔧 Connect kubectl to Your EKS&nbsp;Cluster</h3><p>Now configure kubectl to use your EKS&nbsp;cluster:</p><pre>aws eks update-kubeconfig --region us-east-1 --name Three-tier-cloud</pre><p>This updates your ~/.kube/config file so that you can interact with your new EKS cluster using&nbsp;kubectl.</p><h3>📁 Update Kubernetes Manifests</h3><p>Inside the repo directory 3-tier-app-Deployment/k8s_manifests/, you’ll find the Kubernetes manifests for deploying the , , and  services.</p><p>Before applying them, update the image URIs in both deployment files with the correct values from&nbsp;ECR.</p><h4>🔄 Update backend_deployment.yml:</h4><pre>spec:  containers:    image: &lt;YOUR_IMAGE_URI&gt;</pre><p>Replace &lt;YOUR_IMAGE_URI&gt; with the full image URL from your  ECR repo (latest&nbsp;tag).</p><h4>🔄 Update frontend_deployment.yml:</h4><p>Do the same in the frontend manifest with the image URI from the  ECR&nbsp;repo.</p><h3>🧱 Create a Namespace for the&nbsp;App</h3><p>Let’s keep things clean by isolating our app into a dedicated Kubernetes namespace:</p><pre>kubectl create namespace workshopkubectl config set-context --current --namespace workshop</pre><h3>🚀 Deploy the App Components</h3><p>Apply the deployment and service files for each component:</p><pre>kubectl apply -f frontend-deployment.yaml -f frontend-service.yamlkubectl apply -f backend-deployment.yaml -f backend-service.yaml</pre><pre># Deploy MongoDBcd mongo/</pre><p>At this point, your services are up and running within the cluster — but we still need a way to expose them to the outside&nbsp;world.</p><h3>🌍 Set Up Application Load Balancer (ALB) and&nbsp;Ingress</h3><p>To route external traffic into your Kubernetes services, we’ll use an <strong>AWS Application Load Balancer</strong> along with an .</p><h4>📜 Create an IAM Policy for the Load&nbsp;Balancer</h4><p>The IAM policy json is present inside the kubernetes manifests dir:</p><p>Create the IAM policy in&nbsp;AWS:</p><pre>aws iam create-policy \\  --policy-name AWSLoadBalancerControllerIAMPolicy \\<p>  --policy-document file://iam_policy.json</p></pre><h3>🔒 Associate OIDC Provider with&nbsp;EKS</h3><p>To enable IAM roles for Kubernetes service accounts, associate an OIDC provider with your EKS&nbsp;cluster.</p><pre>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp  sudo mv /tmp/eksctl /usr/local/bin  </pre><p>Then associate the OIDC provider:</p><pre>eksctl utils associate-iam-oidc-provider \\  --region=us-east-1 \\<p>  --cluster=Three-tier-cloud \\</p>  --approve</pre><h3>🔗 Create a Service Account for the Load&nbsp;Balancer</h3><p>Replace &lt;Your-Account-Number&gt; with your actual AWS account ID and&nbsp;run:</p><pre>eksctl create iamserviceaccount \\  --cluster=Three-tier-cloud \\<p>  --namespace=kube-system \\</p>  --name=aws-load-balancer-controller \\<p>  --role-name AmazonEKSLoadBalancerControllerRole \\</p>  --attach-policy-arn=arn:aws:iam::&lt;Your-Account-Number&gt;:policy/AWSLoadBalancerControllerIAMPolicy \\  --region=us-east-1</pre><h3>🧰 Install Helm and Deploy the Load Balancer Controller</h3><p>We’ll use Helm to install the AWS Load Balancer Controller:</p><pre>sudo snap install helm --classic</pre><pre>helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\  -n kube-system \\<p>  --set clusterName=Three-tier-cloud \\</p>  --set serviceAccount.create=false \\<p>  --set serviceAccount.name=aws-load-balancer-controller</p></pre><pre>kubectl get deployment -n kube-system aws-load-balancer-controller</pre><h3>🛣️ Apply Ingress Configuration</h3><p>Now go back to the k8s_manifests/ directory and apply the ingress resource:</p><pre>kubectl apply -f full_stack_lb.yaml</pre><p>Wait for 5–7 minutes to allow the ingress and ALB to be fully provisioned.</p><h3>🌐 Access Your Application</h3><pre>kubectl get ing -n workshop</pre><p>You’ll see an  field in the output. Copy that URL, paste it in your browser, and voilà 🎉 — your <strong>three-tier application is live on&nbsp;AWS!</strong></p><h3>🧹 Step 5: Clean Up AWS Resources</h3><p>Congratulations on successfully deploying your three-tier application on AWS EKS using Terraform! 🎉</p><p>Before we wrap things up, it’s important to  the resources we created — to avoid any unexpected AWS&nbsp;charges.</p><h3>🗑️ Delete Docker Images from&nbsp;ECR</h3><ol><li>Head over to the  in the AWS&nbsp;Console.</li><li>Under , select both three-tier-backend and three-tier-frontend.</li><li>Delete the images from each repository.</li></ol><h3>💣 Destroy Infrastructure with Terraform</h3><p>Now let’s destroy the entire infrastructure from your terminal. Navigate to the terra-config/ directory and&nbsp;run:</p><pre>terraform destroy --auto-approve</pre><p>Terraform will tear down the EKS cluster, node group, IAM roles, VPC config, ECR repositories, and&nbsp;more.</p><h3>🧽 Delete Terraform State File and S3&nbsp;Bucket</h3><p>After destroying your resources, don’t forget to remove the Terraform state file and the bucket&nbsp;itself:</p><pre>aws s3 rm s3://pravesh-terra-state-bucket/eks/terraform.tfstate</pre><p>Then go to the , empty the bucket manually (if needed), and delete the bucket to finish the cleanup&nbsp;process.</p><blockquote><em>⚠️ Make sure to delete the bucket, otherwise it will incur unwanted&nbsp;charges.</em></blockquote><h3>✅ Conclusion: What You’ve&nbsp;Learned</h3><p>In this project, you’ve gone through the complete lifecycle of deploying a real-world  using modern DevOps tools and cloud infrastructure:</p><ul><li>You learned how to use  to provision infrastructure as&nbsp;code.</li><li>You created and managed AWS resources like , , , and&nbsp;.</li><li>You containerized applications and deployed them with .</li><li>You exposed your app to the internet using an <strong>Application Load Balancer</strong> and&nbsp;.</li><li>And finally, you followed best practices like remote state management and safe resource&nbsp;cleanup.</li></ul><p>This project isn’t just a demo — it’s a  you can build on for production-grade cloud-native applications.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=30fe80e82f88\" width=\"1\" height=\"1\" alt=\"\">","contentLength":14782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Installing PHP 8 with phpenv — The Hard Way (But Right)","url":"https://blog.devops.dev/installing-php-8-with-phpenv-the-hard-way-but-right-920a0a8ea1e5?source=rss----33f8b2d9a328---4","date":1751473465,"author":"Gwang-Jin","guid":180831,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automation Evolution: Is Your DevOps Ready for Tomorrow’s Innovations?","url":"https://devops.com/automation-evolution-is-your-devops-ready-for-tomorrows-innovations/?utm_source=rss&utm_medium=rss&utm_campaign=automation-evolution-is-your-devops-ready-for-tomorrows-innovations","date":1751467624,"author":"Christopher Haggan","guid":180655,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker State of App Dev: Dev Ex & Productivity","url":"https://www.docker.com/blog/docker-state-of-app-dev-dev-ex-productivity/","date":1751462443,"author":"Olga Diachkova","guid":180463,"unread":true,"content":"<p><strong>Report: What’s helping devs thrive — and what’s still holding them back?&nbsp;</strong></p><p><em>A look at how culture, tooling, and habits are shaping the developer experience today, per Docker’s 2025 State of Application Development Survey.</em></p><p>Great culture, better tools — but developers often still feel stuck. From pull requests stuck in review to tasks without clear estimates, the inner loop remains cluttered with surprisingly persistent friction points. This year’s data maps the disconnect between what developers need, where they’re blocked, and how better tooling and cultural support can keep velocity on track.</p><p>Here are six key insights into developer experience and productivity from Docker’s annual <strong><em>State of Application Development Survey</em></strong>, based on responses from over 4,500 industry professionals.</p><p><strong>1. How devs learn — and what’s changing</strong></p><p><strong>Self-guided learning is on the upswing</strong>. Across all industries, fully  of respondents turn to online courses or certifications, far outpacing traditional sources like school (), books (), or on-the-job training ().&nbsp;</p><p>Among IT folks, the picture is more nuanced. <strong>School is still the top venue</strong> for learning to code (, up from 57% in our 2024 survey), but online resources are also trending upward. Some  of IT pros learned coding skills via online resources (up from 54% in our 2024 survey) and  favored online courses or certifications (up from 45% in 2024).</p><p>Note: For this year’s report, we surveyed over three times more users across a broader spectrum of industries than for our more IT-focused 2024 report.</p><p>As for  devs prefer to learn, <strong>documentation tops the list</strong>, as in last year’s report — that despite the rise in new and interactive forms of learning. Some  say they lean on documentation, edging out videos and side projects () and slightly ahead of structured online training ().&nbsp;</p><p>AI tools play a relatively minor role in how respondents learn, with GitHub Copilot cited by just  overall — and only among IT pros. It’s also cited by  as a preferred learning method.</p><p><strong>2. Containers: the great divide?</strong></p><p>Among IT pros, container usage soared to  — up from 80% in our 2024 survey. Zoom out to a broader view across industries, however, and adoption appears considerably lower. Just  of developers say they use containers in any part of their workflow.&nbsp;</p><p>Why the gap? Differences in app structure may offer an explanation: IT industry respondents work with microservice-based architectures more often than those in other industries ( versus ). So the higher container adoption may stem from IT pros’ need for modularity and scalability — which containers provide in spades. </p><p>And among container users, needs are evolving. They want better tools for , , and — stubborn pain points across the software lifecycle.</p><p><strong>3. An equal-opportunity headache: estimating time</strong></p><p>No matter the role, <strong>estimating how long a task will take is the most consistent pain point</strong> across the board. Whether you’re a front-end developer (), data scientist (), or a software decision-maker (), precision in time planning remains elusive.</p><p>Other top roadblocks?  and <strong>pull-request review (25%)</strong> are slowing teams down. Interestingly, where people say they need  doesn’t always match where they’re getting stuck. Case in point, <strong>testing solutions and Continuous Delivery (CD)</strong> come up often when devs talk about tooling gaps — even though they’re not always flagged as blockers.</p><p><strong>4. Productivity by persona: different hats, same struggles</strong></p><p>When you break it down by role, some unique themes emerge:</p><ul><li> struggle most with time estimation ().</li><li> face a three-way tie: <strong>planning, time estimation, and designing from scratch (28% each)</strong>.</li><li> are especially challenged by — a task not traditionally in their wheelhouse.</li><li>, surprisingly, list  as a challenge, closely followed by .</li></ul><p>Across personas, a common thread stands out: even seasoned professionals are grappling with foundational coordination tasks — not the “hard” tech itself, but the orchestration around it.</p><p><strong>5. Tools vs. culture: two sides of the experience equation</strong></p><p>On the tooling side, the biggest callouts for improvement include:</p><ul><li><strong>Designing solutions from scratch (17%)</strong></li></ul><p>But productivity isn’t just about tools — it’s deeply cultural. When asked what’s working well, developers pointed to , <strong>location flexibility such as work from home policies (38%)</strong>, and  as top cultural strengths.</p><p>The weak spots? , , and . In other words: developers like where, when, and how they work, but not always .</p><p><strong>6. What’s easy? What’s not?</strong></p><p>While the dev world is full of moving parts, a few areas are surprisingly  challenging:</p><ul><li><strong>Editing config files (8%)</strong></li><li><strong>Writing config files (7%)</strong></li></ul><p>Contrast that with the most taxing areas:</p><ul><li><strong>Troubleshooting in production (9%)</strong></li><li><strong>Debugging in production (9%)</strong></li><li><strong>Security-related tasks (8%)</strong></li></ul><p>It’s a reminder that production is still where the stress — and the stakes — are highest.</p><p>Developer productivity isn’t about just one thing. It’s the compound effect of better tools, smarter learning, sharper planning — and yes, a healthy team culture. For orgs to excel, they need to invest not just in platforms, but also in people. Because when you improve the , you unlock the performance.</p>","contentLength":5167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VS Code’s Open Source AI Revolution: A New Chapter for Developers","url":"https://devops.com/vs-codes-open-source-ai-revolution-a-new-chapter-for-developers/?utm_source=rss&utm_medium=rss&utm_campaign=vs-codes-open-source-ai-revolution-a-new-chapter-for-developers","date":1751457835,"author":"Tom Smith","guid":180473,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Platform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platfor... Puja Abbassi","url":"https://www.youtube.com/watch?v=Xr0Eb-ybvck","date":1751401964,"author":"CNCF [Cloud Native Computing Foundation]","guid":179271,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPlatform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platforms - Puja Abbassi, Giant Swarm\n\nEveryone is talking about platform engineering. You see smooth demos of golden paths and self-service platforms. However, there’s a significant area of challenges that is less talked about and thus often neglected when designing developer platforms.\n\nIn this talk, we’ll explore the often-overlooked day 2 challenges that platform teams face. We’ll dissect the area of day 2 into the many sub-areas and challenges they pose. Drawing on real-world experiences, including notable migrations that many in this community have faced, we'll shed light on the pain behind developer platforms and discuss solutions to these issues. Among others, we’ll delve into practical strategies for managing versioning and rollouts, and highlight the significant hurdles encountered, such as dependencies on end user teams or GitOps.\n\nJoin us for insights, strategies, and stories from the trenches that will help you navigate the complexities of service iteration in developer platforms.</article>","contentLength":1476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Xr0Eb-ybvck?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=hERRANApN5c","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179127,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb</article>","contentLength":476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/hERRANApN5c?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Con... Gordon Radlein","url":"https://www.youtube.com/watch?v=dlDTX-aDNzg","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179128,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Contribution - Gordon Radlein, Datadog\n\nOpenTelemetry has accelerated the commoditization of instrumentation. Telemetry generation is becoming a solved problem, an implementation detail. But this has created a new challenge: a wealth of standardized signals with no standard meaning. Different systems instrumented with different semantics generating telemetry in their own unique language. And while signal correlation connects specific workloads, it fails when we need to understand our systems at a macro scale by joining disparate datasets.\nThat is, until we all agreed to speak the same language.\n\nJust as English as a lingua franca fueled progress across the internet, OpenTelemetry Semantic Conventions are providing a shared language for our systems. In this talk we’ll discuss why semantic interoperability is the real connective tissue, how it’s fueling deeper insights into our production environments, and the key role it plays in enabling the AI systems that are rapidly ushering in the next revolution of our industry.</article>","contentLength":1500,"flags":null,"enclosureUrl":"https://www.youtube.com/v/dlDTX-aDNzg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Welcome + Opening Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=_rqgWHaEvgc","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179129,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nWelcome + Opening Remarks - Austin Parker, Honeycomb</article>","contentLength":435,"flags":null,"enclosureUrl":"https://www.youtube.com/v/_rqgWHaEvgc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere","url":"https://www.youtube.com/watch?v=Z4umnlRdLtA","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179130,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere\n\nLogs can get very expensive and often how useful all those logs are is unknown, some are but many are not. It is very difficult to know which logs are useful and how exactly they are used. With Chronosphere's Control plane for logs users can now get a comprehensive analysis of value and usage patterns, along with sophisticated recommendations and control actions that allow some or most of the value derived from those logs to be preserved. In order to achieve our goals we have enhanced Fluent Bit to be more flexible in which logs are actioned upon and will share useful future additions to it.</article>","contentLength":1072,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Z4umnlRdLtA?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson","url":"https://www.youtube.com/watch?v=J_hHiwa_3QU","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179131,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson, Chronosphere\n\nHybrid cloud isn’t a stepping stone—it’s a destination. With 39% of CNCF survey respondents already operating in hybrid environments, this model is here to stay. But as teams pursue cloud-native architectures, many skip a critical step: developing a clear cloud strategy and an observability approach to match.\nThe result is predictable— widening visibility gaps, redundant tooling and data, and spiraling costs as teams try to stitch together disconnected, vendor-specific systems never meant to work in concert. Hybrid environments expose these issues quickly, especially when workloads span multiple platforms without a unified way to observe and understand them.\nModernization efforts demand open observability from the start—not as an add-on. Technologies like OpenTelemetry, Fluent Bit, and Prometheus act as connective tissue across clouds, clusters, and on-prem infrastructure, enabling standardization where it’s needed most.\nThis talk outlines how to center open observability in your modernization journey: where to standardize architectural layers, how to maintain a more open approach, and why these decisions have long-term payoff. \nHybrid complexity is inevitable. Leading with open observability is how you stay in control—now and in the future.</article>","contentLength":1761,"flags":null,"enclosureUrl":"https://www.youtube.com/v/J_hHiwa_3QU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data I... Dotan Horovits","url":"https://www.youtube.com/watch?v=C5Y3qnEJSY8","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179132,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data Insights - Dotan Horovits, AWS OpenSearch</article>","contentLength":505,"flags":null,"enclosureUrl":"https://www.youtube.com/v/C5Y3qnEJSY8?version=3","enclosureMime":"","commentsUrl":null},{"title":"Building Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Per... Denton Krietz","url":"https://www.youtube.com/watch?v=zgnY8szpKUw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179118,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nBuilding Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Persistent Queue - Denton Krietz, Bindplane\n\nThe OpenTelemetry Collector’s persistent queue provides a robust mechanism for handling data bursts, destination outages, and processing delays, ensuring no telemetry data is lost—but from experience, it’s consistently one of the collector's least understood features.\n\nIn this talk, we’ll explore the inner workings of the OTel Collector’s persistent queue, including how it buffers data, ensures durability, and enables replay after failures. Attendees will learn how to configure persistent queues for their unique workloads, optimize their telemetry pipeline performance, and troubleshoot common pitfalls.\n\nWhether you’re a site reliability engineer, developer, or observability enthusiast, this talk will equip you with the knowledge to deeply understand persistent queues to optimize your telemetry pipeline in production.</article>","contentLength":1348,"flags":null,"enclosureUrl":"https://www.youtube.com/v/zgnY8szpKUw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Introducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner & Ray Jenkins, Streamfold","url":"https://www.youtube.com/watch?v=xeQnP8Ct7qY","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179119,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nIntroducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner &amp; Ray Jenkins, Streamfold\n\nIn this talk, we'll introduce Rotel—an open-source OpenTelemetry collector built in Rust. Rotel is lightweight and resource-efficient, integrating seamlessly into your development workflow. Its compact design lets you package it with your Python or NodeJS projects, so telemetry collection runs alongside your code without needing additional sidecars.\n\nWe'll explore how rethinking telemetry collection at the edge can empower developers right from the early stages of development, paving the way for broader OpenTelemetry adoption. You’ll learn how Rust’s low-overhead FFI enables native extensions for telemetry filtering, transformation, and enrichment using Python and Typescript.\n\nBy leveraging Rust’s performance strengths, Rotel avoids the overhead of garbage collection, resulting in lower memory usage and reduced latency. Its quick cold start times make it a natural fit for modern cloud-native, serverless, and edge computing environments. Join us to discover how moving telemetry collection closer to the source can help you analyze high-volume, high-fidelity signals more effectively.</article>","contentLength":1585,"flags":null,"enclosureUrl":"https://www.youtube.com/v/xeQnP8Ct7qY?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTele... Diana Todea","url":"https://www.youtube.com/watch?v=wWON2NT41lE","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179120,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTelemetry - Diana Todea, Aircall\n\nBecoming a contributor to an open-source project is a transformative step in any developer's career. This session explores the journey from first-time contributor to active developer, covering best practices for navigating project communities, understanding codebases, and making meaningful contributions. Learn strategies for selecting the right project, mastering collaboration tools, and embracing the culture of open-source development. The audience will be inspired about my one year journey with the open source project OpenTelemetry and how I have built a proof of concept for it and achieved developer status for this project. By the end of this talk, the public will gain insights into the tools to become a better developer and how to build more engagement with the community.</article>","contentLength":1284,"flags":null,"enclosureUrl":"https://www.youtube.com/v/wWON2NT41lE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Telemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchma... Henrik Rexed","url":"https://www.youtube.com/watch?v=tZho5W9L_Z8","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179121,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nTelemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchmark Analysis - Henrik Rexed, Dynatrace\n\nIn a push to standardize observability practices, the cloud-native community has embraced OpenTelemetry, offering a unified framework for metrics, logs, and traces. Prior to this, log processing relied on agents like fluent, evolving into fluentbit. With fluentbit's recent expansion to support additional signals and the OpenTelemetry Collector's emergence, a pertinent question arises: Which is the superior choice for performance?\n\nThis session delves into:\n- Unveiling the distinctions between Fluent Bit and the OpenTelemetry Collector.\n- Sharing the findings derived from a series of benchmark tests.\n- Providing valuable insights to empower the community in selecting the most fitting agent for their cloud-native environments.</article>","contentLength":1240,"flags":null,"enclosureUrl":"https://www.youtube.com/v/tZho5W9L_Z8?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Spec-tacular Game Show - Liudmila Molkova, Ted Young, Tyler Helmuth, Jamie Danielson, Alex Boten","url":"https://www.youtube.com/watch?v=ipFVu0dl5Bw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179122,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPanel: The Spec-tacular Game Show - Liudmila Molkova, Microsoft; Ted Young, Grafana Labs; Tyler Helmuth, Jamie Danielson &amp; Alex Boten, Honeycomb\n\nFrom OTLP to OTTL, engineers are excited about a lot of things. But there is one thing that excites them above all else and that is correcting people. Welcome to “The Spec-tacular Game Show”.\n\nIn this fun game show our panelists will be given incorrect statements about the OpenTelemetry Specification or Semantic Convention. The panelists will buzz in, identify what’s wrong, and state the correction. If none of the panelists know the answer the audience will get a chance to answer to steal the point. The panelist (or audience) with the most points wins!\n\nAfter each question we’ll spend a time explaining why the Spec and Semconv is the way it is and highlight how it produces the production-quality telemetry you know and love. Join us for a fun, relaxing, (snarky) panel about everyone’s favorite part of Otel!</article>","contentLength":1356,"flags":null,"enclosureUrl":"https://www.youtube.com/v/ipFVu0dl5Bw?version=3","enclosureMime":"","commentsUrl":null},{"title":"How To Think About Instrumentation Overhead - Jason Plumb, Splunk","url":"https://www.youtube.com/watch?v=fvmzAX_ZyvM","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179123,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nHow To Think About Instrumentation Overhead - Jason Plumb, Splunk\n\nNovice observability practitioners are often overly obsessed with performance. They might approach instrumentation with skepticism and have concerns about latency degradation or resource consumption. This talk is a primer on the topic of instrumentation overhead, and it will teach you how to think about overhead in an observability context. We will cover the causes of overhead and why overhead is so hard to measure and even harder to predict reliably. Lastly, we will present some practical techniques for understanding overhead in your environment and some strategies for coping with it.</article>","contentLength":1042,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fvmzAX_ZyvM?version=3","enclosureMime":"","commentsUrl":null},{"title":"No Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft","url":"https://www.youtube.com/watch?v=fU6jsw0yaVU","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179124,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNo Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft\n\nThe best telemetry starts at the source—inside the client libraries.\nBut in most cases, that means taking a dependency on the OpenTelemetry API from your library. And while it’s stable, minimal, reliable, and safely no-op unless configured—transitive dependencies are still the bane of any library developer’s existence, and most of us try to avoid them.\n\nTo work around this, people reach for abstractions, plugins, bridges, or even OTel forks that break context propagation. The result? A poor user experience. Users must find the right plugin, install it, wire it up—and still hit the diamond dependency problem, now it just affects a subset of users.\n\nBut what if you could take a truly optional dependency? If OpenTelemetry is on the classpath, instrumentation kicks in. If it’s not, no harm done.\nHow hard is that to pull off? How reliable? How performant?\n\nLet’s explore that—through the lens of the next generation of Azure SDKs for Java. Spoiler: it’s easy and fast, and as a side-bonus, we can fall back to logs-based tracing if OTel is not found.</article>","contentLength":1544,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fU6jsw0yaVU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Closing Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=eDbQfZ9eoNI","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179125,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nClosing Remarks - Austin Parker, Honeycomb</article>","contentLength":425,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eDbQfZ9eoNI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace","url":"https://www.youtube.com/watch?v=di5nhYvUh6w","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179126,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace\n\nThe OTel Java API, SDK, and ecosystem are perfectly adequate for Android developer to get OTel instrumentation into their apps. But for a host of reasons, the match is not perfect, especially for developers who only write in Kotlin, which is the recommended development language for Android by Google, not the least of which is the emergence of Kotlin Multiple Platform (KMP) as a means to share code between Android, iOS, and many other platforms.\n\nThis session will outline the reasons why we at Embrace is trying to kick-start the development of a pure Kotlin ecosystem for OTel, starting with an API and SDK implementation, and how we are doing it in a way where mobile developers can get value incrementally without having to wait until every aspect is fully built out.\n\nWe want OTel to feel natural and idiomatic for Android developers, and this is the first step towards that end.</article>","contentLength":1361,"flags":null,"enclosureUrl":"https://www.youtube.com/v/di5nhYvUh6w?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Docker MCP Catalog: the Secure Way to Discover and Run MCP Servers","url":"https://www.docker.com/blog/docker-mcp-catalog-secure-way-to-discover-and-run-mcp-servers/","date":1751375060,"author":"Nuno Coracao","guid":178895,"unread":true,"content":"<p>The Model Context Protocol (MCP) ecosystem is exploding. In just weeks, our Docker MCP Catalog has surpassed , validating that developers are hungry for a <a href=\"https://www.docker.com/products/mcp-catalog-and-toolkit/\">secure way to run MCP servers</a>. Today, we’re excited to share major updates to the Docker MCP Catalog, including enhanced discovery features and our new open submission process. With hundreds of developers already requesting to publish their MCP servers through Docker, we’re accelerating our mission to make <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">containerized MCP servers</a> the standard for secure AI tool distribution.</p><p>The rapid adoption of MCP servers also highlights a critical problem — the current practice of running them via npx or uvx commands exposes systems to unverified code with full host access, not to mention dependency management friction. In this post, we’ll explain why Docker is investing in the MCP ecosystem, showcase the new catalog capabilities, and share how you can contribute to building a more secure foundation for AI applications.</p><p><strong>Figure 1: The new Docker MCP Catalog, built for easier discovery.</strong></p><h2>Why Docker is building the MCP Catalog</h2><h3>The security issues in MCP distribution</h3><p>Every time a developer runs npx -y @untrusted/mcp-server or uvx some-mcp-tool, they’re making a dangerous trade-off: convenience over security. These commands execute arbitrary code directly on the host system with full access to:</p><ul><li>Environment variables and secrets</li></ul><p>Some MCP clients limit environment variable access, but even that is not a universal practice. This isn’t sustainable. As MCP moves from experimentation to production, we need a fundamentally different approach.</p><p>Docker has spent over a decade solving exactly these problems for cloud-native applications. We’ve built the infrastructure, tools, and trust that developers rely on to run billions of containers in production. Now, we’re applying these same principles to the MCP ecosystem.</p><p>When you run an MCP server from our Catalog, you get:</p><ul><li> verifying the image hasn’t been tampered with</li><li><strong>Software Bill of Materials (SBOMs)</strong> documenting every component</li><li> from your host system</li><li> to only what the server actually needs</li></ul><p>This isn’t about making life harder for developers—it’s about making security the path of least resistance.</p><h2>Introducing the enhanced MCP Catalog</h2><p>We’ve reimagined the MCP Catalog to make it more accessible and easier to navigate. You can still access the MCP Catalog from Docker Hub and the MCP Toolkit in Docker Desktop just like before, or <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">go straight to the MCP catalog</a>. We’ve gone beyond generic container image listings by building features that help you quickly find the right MCP servers for your AI applications.&nbsp;&nbsp;</p><p>: MCP servers are organized by what they actually do:</p><ul><li>Data Integration (databases, APIs, file systems)</li><li>Development Tools (IDEs, code analysis, testing)</li><li>Communication (email, Slack, messaging platforms)</li><li>Productivity (task management, calendars, note-taking)</li><li>Analytics (data processing, visualization, reporting)</li></ul><p>: Find servers by capability, tools, GitHub tags, and categories — not just by name.</p><p>: Every catalog entry clearly shows whether it’s Docker-built (with transparent build signing and verification) or community-built (containerized and maintained by the publisher).</p><p><strong>Figure 2: Discover MCP servers by use cases.</strong></p><h3>How we classify MCP Servers: Built by Docker vs. community-built</h3><p>: When you see “Built by Docker,” you’re getting our complete security treatment. We control the entire build pipeline, providing cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning.</p><p>: These servers are packaged as Docker images by their developers. While we don’t control their build process, they still benefit from container isolation, which is a massive security improvement over direct execution.</p><p><strong>Tiers serve important roles</strong>: Docker-built servers demonstrate the gold standard for security, while community-built servers ensure we can scale rapidly to meet developer demand. Developers can change their mind after submitting a community-built server and opt to resubmit it as a Docker-built server.</p><p><strong>Figure 3: An example of Built by Docker MCP Server.</strong></p><h2>Open for MCP server submission: Join the secure MCP movement</h2><p>Starting today, we’re opening our submission process to the community. Whether you’re an individual developer or an enterprise team, you can feature your MCP servers on the Docker MCP Catalog. By publishing through our catalog, you’re not just distributing your MCP server — you’re helping establish a new security standard for the entire ecosystem while getting your MCP tools available to millions of developers already using Docker via Docker Hub and Docker Desktop. Your containerized server becomes part of the solution, demonstrating that production-ready AI tools don’t require compromising on security.&nbsp;</p><h3>How to submit your MCP server</h3><ol><li> – Package your MCP server as a Docker image</li><li> – Opt for Docker-built (we handle the build) or community-built (you build and maintain it)</li></ol><p>We’re committed to a fast, transparent review process. Quality MCP servers that follow our security guidelines will be published quickly, helping you reach Docker’s 20+ million developer community.</p><p>ClickHouse is one of the first companies to take advantage of Docker’s MCP Catalog, and they opted for the Docker-built tier to ensure maximum security. Here’s why they chose to partner with Docker:</p><p><a href=\"https://clickhouse.com/\" rel=\"nofollow noopener\" target=\"_blank\"></a><em>, we deliver the fastest analytics database – open-source, and designed for real-time data processing and analytics at scale. As agentic AI becomes more embedded in modern applications, developers are using the ClickHouse MCP server to support intelligent, data-driven workflows that demand low latency, high concurrency, and cost efficiency.</em><em>To make it easier for developers to deploy these workloads, we’re featuring </em><a href=\"https://hub.docker.com/mcp/server/clickhouse/overview\" rel=\"nofollow noopener\" target=\"_blank\"></a><em> on Docker’s MCP Catalog, which provides </em><strong><em>a powerful way to reach 20M+ developers</em></strong><em> and makes it easier for Docker users to discover and use our solution. </em><strong><em>We opted for “Built by Docker” with the highest security standard</em></strong><em>, including cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning. Together with Docker, developers can run ClickHouse MCP Server with confidence, knowing it’s secured, verified, and ready for their agentic applications.” – </em>Tanya Bragin, VP of Product and Marketing Clickhouse</p><p>We’re preparing for the future of cloud-native AI applications. Remote MCP servers will enable:</p><ul><li>Managed MCP services that scale automatically</li><li>Shared capabilities across teams without distributing code</li><li>Stricter security boundaries for sensitive operations</li></ul><h3>Integration with the official MCP registry</h3><p>We’re actively collaborating with the MCP community on the upcoming official registry. Our vision is complementary:</p><ul><li>The official registry provides centralized discovery – the “yellow pages” of available MCP servers</li><li>Docker provides the secure runtime and distribution for those listings</li><li>Together, we create a complete ecosystem where discovery and security work hand-in-hand</li></ul><p>The explosive growth of our MCP Catalog, 1 million pulls and hundreds of publisher requests, tells us developers are ready for change. They want the power of MCP, but they need it delivered securely.</p><p>By establishing containers as the standard for MCP server distribution, we’re not trying to own the ecosystem — we’re trying to secure it. Every MCP server that moves from npx execution to containerized deployment is a win for the entire community.</p><ul><li><strong>Explore the enhanced MCP Catalog</strong>: <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">Visit the MCP Catalog</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to discover MCP servers that solve your specific needs securely.</li><li><strong>Use and test hundreds of MCP Servers</strong>: <a href=\"https://www.docker.com/products/docker-desktop/\">Download Docker Desktop</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to download and use any MCP server in our catalog with your favorite clients: Gordon, Claude, Cursor, VSCode, etc</li><li>: Star our repository and watch for updates on the MCP Gateway release and remote server capabilities.</li></ul><p>Together, we’re building more than a catalog — we’re establishing the secure foundation that the MCP ecosystem needs to grow from experimental tool to production-ready platform. Because when it comes to AI applications, security isn’t optional. It’s fundamental.</p>","contentLength":8137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build the highest resilience apps with multi-Region strong consistency in Amazon DynamoDB global tables","url":"https://aws.amazon.com/blogs/aws/build-the-highest-resilience-apps-with-multi-region-strong-consistency-in-amazon-dynamodb-global-tables/","date":1751315448,"author":"Donnie Prakoso","guid":176903,"unread":true,"content":"<p>While tens of thousands of customers are successfully using <a href=\"https://aws.amazon.com/dynamodb/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon DynamoDB</a><a href=\"https://aws.amazon.com/dynamodb/global-tables/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">global tables</a> with eventual consistency, we’re seeing emerging needs for even stronger resilience. Many organizations find that the DynamoDB multi-Availability Zone architecture and eventually consistent global tables meet their requirements, but critical applications like payment processing systems and financial services demand more.</p><p>For these applications, customers require a zero Recovery Point Objective (RPO) during rare Region-wide events, meaning you can direct your app to read the latest data from any Region. Your multi-Region applications always need to access the same data regardless of location.</p><p>Starting today, you can use a new Amazon DynamoDB global tables capability that provides multi-Region strong consistency (MRSC), enabling zero RPO. This capability, first announced as a <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-dynamodb-global-tables-previews-multi-region-strong-consistency/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">preview at AWS re:Invent 2024</a>, simplifies building highly resilient global applications.</p><p>Here’s how you can enable MRSC starting with an existing empty DynamoDB table:</p><p>With MRSC, DynamoDB now offers you the highest level of resilience for your applications. If application processing is interrupted in a Region, you can redirect traffic to another Region containing a MRSC replica and know that you’ll be processing the latest data.</p><p>Let me walk you through how you can use this new capability.</p><p>To start using MRSC, I need to create a global table from an existing DynamoDB table that doesn’t contain any data. I navigate to my existing table, select the  tab and select .</p><p>The MRSC availability architecture requires three AWS Regions. I can configure MRSC with either three full replicas or with two replicas and a witness. A witness contains only replicated change data to provide the required availability without maintaining a full copy of my table data.</p><p>The following screenshot shows how I configure MRSC with two replicas and a witness. To configure MRSC with three full replicas instead, I can clear <strong>Configure Region 2 as Witness</strong>.</p><p>If I need to update my existing table programmatically, I can use the <a href=\"https://aws.amazon.com/developer/learning/q-developer-cli/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon Q CLI</a> to generate the required command with the following prompt:</p><p><code>&gt; Hey Q! Update my existing DynamoDB table called \"demo-mrsc\" in us-east-1 with multi-Region strong consistency across us-east-2 with witness in us-west-2 Regions</code></p><p>Shortly after, Q CLI will respond back with following command:</p><pre><code>&gt; To update your DynamoDB table with multi-Region strong consistency, you'll need to use the update-table command with the appropriate parameters. Here's how you would do it:\n\naws dynamodb update-table \\ \n    --table-name demo-mrsc \\ \n    --replica-updates '[{\"Create\": {\"RegionName\": \"us-east-2\"}}]' \\ \n    --global-table-witness-updates '[{\"Create\": {\"RegionName\": \"us-west-2\"}}]' \\ \n    --multi-region-consistency STRONG \\ \n    --region us-east-1\n</code></pre><p>After it’s finished processing, I can check the status of my MRSC global table. I can see I have a  configured for my DynamoDB global table. A witness reduces costs while still providing the resilience benefits of multi-Region strong consistency.</p><p>Then, in my application, I can use  to read data with strong consistency. Here’s a Python example:</p><pre><code>import boto3\n\n# Configure the DynamoDB client for your region\ndynamodb = boto3.resource('dynamodb', region_name='us-east-2')\ntable = dynamodb.Table('demo-mrsc')\n\npk_id = \"demo#test123\"\n\n# Read with strong consistency across regions\nresponse = table.get_item(\n    Key={\n        'PK': pk_id\n    },\n    ConsistentRead=True\n)\n\nprint(response)\n</code></pre><p>For operations that require the strongest resilience, I can use . For less critical operations where eventual consistency is acceptable, I can omit this parameter to improve performance and reduce costs.</p><p>Here are a couple of things to note:</p><ul><li> – The Amazon DynamoDB multi-Region strong consistency capability is available in following AWS Regions: US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Osaka, Seoul, Tokyo), and Europe (Frankfurt, Ireland, London, Paris)</li></ul><p>Learn more about how you can achieve the highest level of application resilience, enable your applications to be always available and always read the latest data regardless of the Region by visiting <a href=\"https://aws.amazon.com/dynamodb/global-tables/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon DynamoDB global tables</a>.</p>","contentLength":4218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Survey: Pace of Increased Adoption of GitOps Varies Widely","url":"https://devops.com/survey-pace-of-increased-adoption-of-gitops-varies-widely/?utm_source=rss&utm_medium=rss&utm_campaign=survey-pace-of-increased-adoption-of-gitops-varies-widely","date":1751313103,"author":"Mike Vizard","guid":176880,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kusari Adds AI Security Tool to Inspect Code as Pull Requests Are Made","url":"https://devops.com/kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made/?utm_source=rss&utm_medium=rss&utm_campaign=kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made","date":1751310779,"author":"Mike Vizard","guid":176879,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond a RHEL Clone: How Rocky Linux Is Evolving Into Something More","url":"https://devops.com/beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more/?utm_source=rss&utm_medium=rss&utm_campaign=beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more","date":1751308370,"author":"Nathan Blackham","guid":176852,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Amazon EC2 C8gn instances powered by AWS Graviton4 offering up to 600Gbps network bandwidth","url":"https://aws.amazon.com/blogs/aws/new-amazon-ec2-c8gn-instances-powered-by-aws-graviton4-offering-up-to-600gbps-network-bandwidth/","date":1751306492,"author":"Channy Yun (윤석찬)","guid":176811,"unread":true,"content":"<p>You can use C8gn instances to run the most demanding network intensive workloads, such as security and network virtual appliances (virtual ﬁrewalls, routers, load balancers, proxy servers, DDoS appliances), data analytics, and tightly-coupled cluster computing jobs.</p><p><strong><u>EC2 C8gn instances specifications</u></strong> C8gn instances provide up to 192 vCPUs and 384 GiB memory, and offer up to 30 percent higher compute performance compared Graviton3-based <a href=\"https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7gn-instances-graviton3e-processors-and-up-to-200-gbps-network-bandwidth/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">EC2 C7gn instances</a>.</p><p>Here are the specs for C8gn instances:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>If you’re using C7gn instances now, you will have straightforward experience migrating network intensive workloads to C8gn instances because the new instances offer similar vCPU and memory ratios. To learn more, check out the collection of <a href=\"https://aws.amazon.com/ec2/graviton/resources/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Graviton resources</a> to help you start migrating your applications to Graviton instance types.</p>","contentLength":831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Weekly Roundup: Project Rainier, Amazon CloudWatch investigations, AWS MCP servers, and more (June 30, 2025)","url":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-project-rainier-amazon-cloudwatch-investigations-aws-mcp-servers-and-more-june-30-2025/","date":1751301557,"author":"Channy Yun (윤석찬)","guid":176779,"unread":true,"content":"<p>Every time I visit Seattle, the first thing that greets me at the airport is <a href=\"https://en.wikipedia.org/wiki/Mount_Rainier\">Mount Rainier</a>. Did you know that the most innovative project at <a href=\"https://aws.amazon.com/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Web Services (AWS)</a> is named after this mountain?</p><p>Project Rainier is a new project to create what is expected to be the world’s most powerful computer for training AI models across multiple data centers in the United Stages. Anthropic will develop the advanced versions of its <a href=\"https://aws.amazon.com/bedrock/anthropic/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Claude models</a> with five times more computing power than its current largest training cluster.</p><p>The key technology powering Project Rainier is <a href=\"https://aws.amazon.com/ai/machine-learning/trainium/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS custom-designed Trainium2 chips</a>, which are specialized for the immense data processing required to train complex AI models. Thousands of these Trainium2 chips will be connected in a new type of <a href=\"https://aws.amazon.com/ec2/ultraservers/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon EC2 UltraServer</a> and <a href=\"https://aws.amazon.com/ec2/ultraclusters/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">EC2 UltraCluster</a> architecture that allows ultra-fast communication and data sharing across the massive system.</p><p>Learn about the <a href=\"https://www.aboutamazon.com/news/aws/aws-project-rainier-ai-trainium-chips-compute-cluster\">AWS vertical integration of Project Rainer</a>, where it designs every component of the technology stack from chips to software, allows it to optimize the entire system for maximum efficiency and reliability.</p><p> Here are some launches that got my attention:</p><ul><li><a href=\"https://aws.amazon.com/blogs/aws/amazon-fsx-for-openzfs-now-supports-amazon-s3-access-without-any-data-movement/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon S3 access for Amazon FSx for OpenZFS</a> – You can access and analyze your FSx for OpenZFS file data through Amazon S3 Access Points, enabling seamless integration with AWS AI/ML, and analytics services without moving your data out of the file system. You can treat your FSx for OpenZFS data as if it were stored in S3, making it accessible through the S3 API for various applications including Amazon Bedrock, Amazon SageMaker, AWS Glue, and other S3 based cloud-native applications.</li><li><a href=\"https://aws.amazon.com/blogs/aws/new-improve-apache-iceberg-query-performance-in-amazon-s3-with-sort-and-z-order-compaction/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon S3 with sort and z-order compaction for Apache Iceberg tables</a> – You can optimize query performance and reduce costs with new sort and z-order compaction. With S3 Tables, sort compaction automatically organizes data files based on defined column orders, while z-order compaction can be enabled through the maintenance API for efficient multicolumn queries.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/ga-accelerate-troubleshooting-amazon-cloudwatch-investigations/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon CloudWatch investigations</a> – You can accelerate your operational troubleshooting in AWS environments using the Amazon CloudWatch AI-powered investigation feature, which helps identify anomalies, surface related signals, and suggest remediation steps. This capability can be initiated through CloudWatch data widgets, multiple AWS consoles, CloudWatch alarm actions, or Amazon Q chat and enables team collaboration and integration with Slack and Microsoft Teams.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-bedrock-guardrails-tiers-content-filters-denied-topics/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Bedrock Guardrails Standard tier</a> – You can enhance your AI content safety measures using the new Standard tier. It offers improved content filtering and topic denial capabilities across up to 60 languages, better detection of variations including typos, and stronger protection against prompt attacks. This feature lets you configure safeguards to block harmful content, prevent model hallucinations, redact personally identifiable information (PII), and verify factual claims through automated reasoning checks.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-route-53-resolver-endpoints-dns-delegation-private-hosted-zones/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Route 53 Resolver endpoints for private hosted zone</a> – You can simplify DNS management across AWS and on-premises infrastructure using the new Route 53 DNS delegation feature for private hosted zone subdomains, which works with both inbound and outbound Resolver endpoints. You can delegate subdomain authority between your on-premises infrastructure and Route 53 Resolver cloud service using name server records, eliminating the need for complex conditional forwarding rules.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-q-developer-java-upgrade-transformation-cli-generally-available/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Q Developer CLI for Java transformation</a> – You can automate and scale Java application upgrades using the new Amazon Q Developer Java transformation command line interface (CLI). This feature perform upgrades from Java versions 8, 11, 17, or 21 to versions 17 or 21 directly from the command line. This tool offers selective transformation options so you can choose specific steps from transformation plans and customize library upgrades.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/managed-integrations-aws-iot-device-management/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">New AWS IoT Device Management managed integrations</a> – You can simplify Internet of Things (IoT) device management across multiple manufacturers and protocols using the new managed integrations feature, which provides a unified interface for controlling devices whether they connect directly, through hubs or third-party clouds. The feature includes pre-built cloud-to-cloud (C2C) connectors, device data model templates, and SDKs that support ZigBee, Z-Wave, and Wi-Fi protocols, while you can still create custom connectors and data models.</li></ul><p> Check your calendars and sign up for these upcoming AWS events:</p><ul><li><a href=\"https://reinvent.awsevents.com/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS re:Invent</a> – Register now to get a head start on choosing your best learning path, booking travel and accommodations, and bringing your team to learn, connect, and have fun. If you’re an early-career professional, you can apply to the <a href=\"https://reinvent.awsevents.com/all-builders-welcome/\">All Builders Welcome Grant program</a>, which is designed to remove financial barriers and create diverse pathways into cloud technology.</li><li><a href=\"https://aws.amazon.com/events/builders-online-series/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS Builders Online Series</a> – If you’re based in one of the Asia Pacific time zones, join and learn fundamental AWS concepts, architectural best practices, and hands-on demonstrations to help you build, migrate, and deploy your workloads on AWS.</li></ul><p>That’s all for this week. Check back next Monday for another Weekly Roundup!</p>","contentLength":5246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast Code, Real Risks: Guardrails for AI-Generated Software","url":"https://devops.com/fast-code-real-risks-guardrails-for-ai-generated-software/?utm_source=rss&utm_medium=rss&utm_campaign=fast-code-real-risks-guardrails-for-ai-generated-software","date":1751298670,"author":"Mike Vizard","guid":176743,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Next Version of Grok Includes Advanced Coding Assistance: Reports","url":"https://devops.com/next-version-of-grok-includes-advanced-coding-assistance-reports/?utm_source=rss&utm_medium=rss&utm_campaign=next-version-of-grok-includes-advanced-coding-assistance-reports","date":1751298224,"author":"Jon Swartz","guid":176742,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tool Calling with Local LLMs: A Practical Evaluation","url":"https://www.docker.com/blog/local-llm-tool-calling-a-practical-evaluation/","date":1751291322,"author":"Ignasi Lopez Luna","guid":176646,"unread":true,"content":"<h2>Which local model should I use for tool calling?</h2><p>When building GenAI and agentic applications, one of the most pressing and persistent questions is: <em>“Which local model should I use for tool calling?”</em>&nbsp; We kept hearing again and again, from colleagues within Docker and the developer community, ever since we started working on <a href=\"https://www.docker.com/blog/introducing-docker-model-runner/\"></a>, a local inference engine that helps developers run and experiment with local models.&nbsp;</p><p>It’s a deceptively simple question with a surprisingly nuanced answer. Even when we tried to answer it for a very specific case: <em>“What if I just expose 5 simple tools to the model?”</em>We realized we had no definite answer forthat. <a href=\"https://www.docker.com/blog/run-llms-locally/\">Local LLM models</a> offer control, cost-efficiency, and privacy, but when it comes to structured tool use, deciding when and how to act, they can behave very differently. We decided to dig deep and test this properly. We started with manual experimentation, then built a framework to scale our testing. This blog documents that journey and shares which models ranked highest on our tool-calling leaderboard.</p><h2>The first attempt: Manual testing</h2><p>Our first instinct was to build something quickly and try it out manually.</p><p>So we created<a href=\"https://github.com/ilopezluna/chat2cart\" rel=\"nofollow noopener\" target=\"_blank\"></a>, an AI-powered shopping assistant that lets users interact via chat to build, modify, and check out a shopping cart. Through a natural conversation, users can discover products, add or remove items, and complete or cancel their purchase, all from the chat interface.</p><p>To support testing across different LLMs, we added a model selector that makes it easy to switch between local models (via Docker Model Runner or Ollama) and hosted models using the OpenAI API.</p><p>OpenAI’s GPT-4 or GPT-3.5 worked as expected, and the experience was fairly smooth.&nbsp;</p><ul><li>Called tools when they were needed</li><li>Avoided unnecessary tool usage</li><li>Handled tool responses naturally</li></ul><p>But the local models? That’s where the challenges started to surface.</p><h2>What went wrong with local models</h2><p>We started experimenting with some of the local models listed on the<a href=\"https://huggingface.co/spaces/gorilla-llm/berkeley-function-calling-leaderboard\" rel=\"nofollow noopener\" target=\"_blank\"> Berkeley Function-Calling Leaderboard</a>. Our goal was to find smaller models, ideally with fewer than 10 billion parameters, so we tested xLAM-2-8b-fc-r and watt-tool-8B. We quickly ran into several recurring issues:</p><ul><li>: Tools were being called even for greeting messages like “Hi there!”</li><li>: The model would search when it should have added, or tried to remove when the cart was empty</li><li>: Parameters like product_name or quantity were missing or malformed</li><li>: The model often failed to respond to tool output, leading to awkward or incomplete conversations</li></ul><p>At this point, it was clear that manual testing wouldn’t scale. Different models failed in different ways, some struggled with invocation logic, while others mishandled tool arguments or responses.&nbsp; Testing was not only slow, but also unreliable. Because these models are non-deterministic, we had to run each scenario multiple times just to get a reliable read on behavior.</p><p>We needed a testing setup that was repeatable, measurable, and fast.</p><h2>Our second attempt: A scalable testing tool</h2><p>Our goal wasn’t academic rigor.It was: <em>“Give us good-enough answers in 2–3 days, not weeks.”</em></p><p>In a couple of days, we created<a href=\"https://github.com/docker/model-test\" rel=\"nofollow noopener\" target=\"_blank\"></a>, This is a flexible project with the following capabilities</p><ul><li>Define real-world  with multiple valid tool call sequences</li><li>Run them against many models (local &amp; hosted)</li><li>Track , , and </li><li>Log  for analysis (or eventual fine-tuning)</li></ul><p>The core idea behind model-test is simple: simulate realistic tool-using conversations, give the model room to reason and act, and check whether its behavior makes sense.</p><ul><li>A  (e.g. “Add iPhone to cart”)</li><li>The  (optional)</li><li>One or more , because there’s often more than one right answer</li></ul><div><pre>{\n&nbsp;&nbsp;\"prompt\": \"Add iPhone to cart\",\n&nbsp;&nbsp;\"expected_tools_variants\": [\n&nbsp;&nbsp;&nbsp;&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\": \"direct_add\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"tools\": [{ \"name\": \"add_to_cart\", \"arguments\": { \"product_name\": \"iPhone\" } }]\n&nbsp;&nbsp;&nbsp;&nbsp;},\n&nbsp;&nbsp;&nbsp;&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\": \"search_then_add\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"tools\": [\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{ \"name\": \"search_products\", \"arguments\": { \"query\": \"iPhone\" } },\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{ \"name\": \"add_to_cart\", \"arguments\": { \"product_name\": \"iPhone 15\" } }\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]\n&nbsp;&nbsp;&nbsp;&nbsp;}\n&nbsp;&nbsp;]\n}\n</pre></div><p>In this case, we consider both  and <strong>“search first, then add the result”</strong> as acceptable. Even though “iPhone” isn’t a real product name, we’re fine with it. We weren’t aiming for overly strict precision, just realistic behavior.</p><p>Each test case belongs to a test suite. We provide two built-in suites. However, you can run an entire suite, individual test cases, or a selection of multiple test cases. Additionally, you can create your own custom suites to group tests as needed.&nbsp;</p><ul><li>: Greetings, single-step actions</li><li>: Multi-step reasoning and tool chaining</li></ul><p>To make tests feel closer to how real agents behave, we simulate an agent loop up to .</p><p>User: </p><ol><li>Model: <em>“Let me search for iPhone 5…”</em><ol><li>Tool: </li></ol></li><li>Model: <em>“Adding product X to cart…”</em></li><li>Model:  → Great, test passed!</li></ol><p>But if the model still wants to keep going after round 5?</p><p>That’s it, my friend,&nbsp; . Time’s up.</p><p>We deliberately avoided designing tests that require perfect predictions.</p><ul><li>We didn’t demand that the model always know the exact product name.</li><li>What mattered was: <strong>did the tool sequence make sense</strong> for the intent?</li></ul><p>This helped us focus on the kind of reasoning and behavior we actually want in agents, not just perfect token matches.</p><p>Our test outputs distilled down to a final F1 score, encapsulating three core dimensions:</p><div><table><tbody><tr><td><p>Did the model realize a tool was needed?</p></td></tr><tr><td><p>Did it choose the right tool(s) and use them correctly?</p></td></tr><tr><td><p>Whether the tool call arguments were correct?</p></td></tr></tbody></table></div><p>The F1 score is the harmonic mean of two things: precision (how often the model made valid tool calls) and recall (how often it made the tool calls it was supposed to).</p><p>We also tracked latency, the average runtime in seconds, but that wasn’t part of the F1 calculation; it simply helped us evaluate speed and user experience.</p><h2>21 models and 3,570 tests later: Which models nailed tool calling?</h2><p>We tested 21 models across  using 210 batch runs.</p><h3>Overall Rankings (by Tool Selection F1):</h3><div><table><tbody><tr></tr><tr><td><p>claude-3-5-sonnet-20241022</p></td></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>Among all models, OpenAI’s GPT-4 came out on top with a tool selection F1 score of 0.974, completing responses in just under 5 seconds on average. While hosted and not the focus of our local model exploration, it served as a reliable benchmark and provided some ground truths.</p><p>On the local side, Qwen 3 (14B) delivered outstanding results, nearly matching GPT-4 with a 0.971 F1 score, though with significantly higher latency (~142 seconds per interaction).</p><p>If you’re looking for something faster, Qwen 3 (8B) also achieved an F1 score of 0.933, while cutting latency nearly in half (~84 seconds), making it a compelling balance between speed and tool-use accuracy.</p><p>Hosted models like Claude 3 Haiku also performed very well, hitting 0.933 F1 with exceptional speed (3.56 seconds average latency), further illustrating the high bar set by cloud-based offerings.</p><p>Not all models handled tool calling well. The quantized Watt 8B model struggled with parameter accuracy and ended up with a tool selection F1 score of just 0.484. Similarly, the LLaMA-based XLam 8B variant often missed the correct tool path altogether, finishing with an F1 score of 0.570. These models may be suitable for other tasks, but for our structured tool use test, they underdeliver.</p><p>We also experimented with both  and  variants for some models, and in all cases observed <strong>no significant difference</strong> in tool-calling behavior or performance. This suggests that quantization is beneficial for reducing resource usage without negatively impacting accuracy or reasoning quality, at least for the models and scenarios we tested.</p><p>If your goal is maximum tool-calling accuracy, then Qwen 3 (14B) or Qwen 3 (8B) are your best bets, both local, both precise, with the 8B variant being notably faster.</p><p>For a good trade-off between speed and performance, Qwen 2.5 stood out as a solid option. It’s fast enough to support real-time experiences, while still maintaining decent tool selection accuracy.</p><p>If you need something more lightweight, especially for resource-constrained environments, the <a href=\"https://groq.com/introducing-llama-3-groq-tool-use-models/\" rel=\"nofollow noopener\" target=\"_blank\">LLaMA 3 Groq 7B</a> variant offers modest performance at a much lower compute footprint.</p><h2>What we learned and why this matters</h2><p>Our testing confirmed that the Qwen family of models leads the pack among open-source options for tool calling. But as always, there’s a trade-off; you’ll need to balance between accuracy and latency when designing your application</p><ul><li>: Even the 8B version of Qwen3 outperformed any other local model</li><li>: Higher-accuracy models take longer, often significantly.</li></ul><p>Tool calling is core to almost every real-world GenAI application. Whether you’re building agents or creating agentic workflows, your LLM must know when to act and how. Thanks to this simple framework, “We don’t know which model to pick” became “We’ve narrowed it down to three great options, each with clear pros and cons.”</p>","contentLength":8955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Still Running Vulnerable Log4j Instances?","url":"https://devops.com/still-running-vulnerable-log4j-instances/?utm_source=rss&utm_medium=rss&utm_campaign=still-running-vulnerable-log4j-instances","date":1751281674,"author":"Ofer Regev","guid":176565,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serverless CI/CD: Redefining Continuous Delivery in the Modern DevOps Era","url":"https://devops.com/serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era/?utm_source=rss&utm_medium=rss&utm_campaign=serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era","date":1751280883,"author":"Harikrishna Kundariya","guid":176527,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How DevOps Services Improve Software Delivery and Quality","url":"https://devops.com/how-devops-services-improve-software-delivery-and-quality/?utm_source=rss&utm_medium=rss&utm_campaign=how-devops-services-improve-software-delivery-and-quality","date":1751277346,"author":"Vinay Pasilkar","guid":176468,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revolutionizing CI/CD: A Framework for Integrating Generative AI Across the Software Delivery Lifecycle","url":"https://devops.com/revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle/?utm_source=rss&utm_medium=rss&utm_campaign=revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle","date":1751275960,"author":"Anirban Biswas","guid":176467,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Software Migrations Fail: It’s Not the Code","url":"https://devops.com/why-software-migrations-fail-its-not-the-code/?utm_source=rss&utm_medium=rss&utm_campaign=why-software-migrations-fail-its-not-the-code","date":1751273613,"author":"Nishil Macwan","guid":176426,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harnessing AI and Automation for the Future of Innovation in DevOps","url":"https://devops.com/harnessing-ai-and-automation-for-the-future-of-innovation-in-devops/?utm_source=rss&utm_medium=rss&utm_campaign=harnessing-ai-and-automation-for-the-future-of-innovation-in-devops","date":1751268502,"author":"Tony Barbagallo","guid":176391,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Self-Driving Help Desk: Agentic AI’s Role in the Next DevOps Era","url":"https://devops.com/the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era/?utm_source=rss&utm_medium=rss&utm_campaign=the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era","date":1751267594,"author":"Venkat Thiruvengadam","guid":176390,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}