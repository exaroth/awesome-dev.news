{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":50,"items":[{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-25/","date":1762011154,"author":"","guid":324098,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-24/","date":1762011153,"author":"","guid":324097,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-23/","date":1762011147,"author":"","guid":324096,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-22/","date":1762011146,"author":"","guid":324095,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-21/","date":1762011143,"author":"","guid":324094,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-20/","date":1762011139,"author":"","guid":324093,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-19/","date":1762011138,"author":"","guid":324092,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-18/","date":1762011132,"author":"","guid":324091,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-17/","date":1762011131,"author":"","guid":324090,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-16/","date":1762011130,"author":"","guid":324089,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Story Of an App That Was Slow","url":"https://blog.devops.dev/the-story-of-an-app-that-was-slow-3844f1af9250?source=rss----33f8b2d9a328---4","date":1762006322,"author":"Amita Pal Singh","guid":324086,"unread":true,"content":"<p>Once upon a time, there was an app — , it had beautiful forms, users loved it. Soon hundreds of people started visiting it. But then it started struggling. With 1500 daily users… slow load times and outages started causing&nbsp;chaos!</p><p>Bandwidth is the maximum amount of data that can pass through the network at any given&nbsp;time.</p><p>Throughput is the average amount of data that actually passes through over a given period of&nbsp;time.</p><p>It is measured in bps = bits per&nbsp;second</p><p>Network latency is measured in milliseconds by calculating the time interval between the initiation of a send operation from a source system and the completion of the matching receive operation by the target&nbsp;system.</p><p>This measurement helps developers understand how quickly a webpage or application will load for&nbsp;users.</p><p>A ping rate of less than 100ms is considered acceptable but latency in the range of 30–40ms is desirable.</p><p>The amount of time it takes for a response to reach a client device after a client request. It is double the Latency, plus processing time at&nbsp;server.</p><p>Expected Daily users =&nbsp;1500</p><p>Expected Peak traffic = 100–200 form submissions per&nbsp;hour</p><p>Required Bandwidth ≥300 Mbps for 200–300 RPS (~1 MB&nbsp;each).</p><p>Throughput ≥250 Mbps (80% of Bandwidth) for smooth data&nbsp;flow.</p><p>Required Latency &lt;100ms for snappy form rendering.</p><p>RTT &lt;200ms for quick submissions.</p><p>Upgrade Bandwidth to 500 Mbps, enough for 200–300 simultaneous 1 MB form submissions</p><p>Use QoS rules to prioritize form traffic and upgrade load balancers</p><p>Deploy a CDN to cache form assets closer to users and optimize server&nbsp;code</p><p>Optimize Db Queries, Add&nbsp;Indexes</p><p>Want to create your own networking adventure? Track Bandwidth, Latency, Throughput, and RTT. Use observability tools, scale smart, and aim for these metrics to keep your users&nbsp;happy!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3844f1af9250\" width=\"1\" height=\"1\" alt=\"\">","contentLength":1788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digest #186: Inside the AWS Outage, Docker Compose in Production, F1 Hacks and 86,000 npm Packages Attacks","url":"https://www.devopsbulletin.com/p/digest-186-inside-the-aws-outage","date":1761989524,"author":"Mohamed Labouardy","guid":323960,"unread":true,"content":"<p>Welcome to this week’s edition of the DevOps Bulletin!</p><p>A recent 14-hour AWS us-east-1 outage took down 140 services after a DNS race condition in DynamoDB spiraled out of control. Palo Alto’s Unit42 uncovered a cloud-based gift card fraud campaign, and researchers exploited bugs in the FIA portal to access F1 driver data. Meanwhile, npm faced another supply-chain attack, with over 86,000 malicious packages downloaded.</p><p>Cloudflare detailed how it’s escaping the Linux networking stack, AWS quietly deprecated two dozen services, and Netflix revealed how Tudum supports 20M+ users using CQRS.</p><p>On the hands-on side: Docker Compose in production, ArgoCD for multi-cluster deployments, detecting bad images in S3 with Rekognition, and TDD with Terraform. Plus, why for some workloads, Postgres can beat Kafka.</p><p>Tools of the week: WhoDB (chat-based DB explorer), LME (CISA’s free SIEM), Grype (vulnerability scanner), Kanchi (Celery monitor), Bruin (data pipeline), and Nyno (multi-language workflow engine).</p><p>All this and more in this week’s DevOps Bulletin, don’t miss out!</p><div><p> Consider supporting it with a <a href=\"https://www.devopsbulletin.com/subscribe\">paid subscription</a>. You’ll keep the free Friday issues  get extras like bonus deep-dives, templates, and the full archive.</p></div><p>Ever feel like your cloud bill keeps growing, but you’re not sure  the money’s going? Start with an .</p><p><em>Listing all your resources — EC2 instances, S3 buckets, Lambdas, and more — often reveals idle or forgotten assets quietly adding to your bill. You can script it yourself with the AWS CLI or use tools like AWS Config or CloudQuery for a more automated setup.</em></p><p><em>If you want more hands-on tips like this, check out my latest book, “.</em></p><ul><li><p>A lightweight  - Postgres, MySQL, SQLite, MongoDB, Redis, MariaDB, Elastic Search, and Clickhouse with Chat interface.</p></li></ul><ul><li><p> is a no-cost, open-source platform that centralizes log collection, enhances threat detection, and enables real-time alerting.</p></li><li><p> is a vulnerability scanner for container images and filesystems.</p></li><li><p> is a real-time Celery task monitoring (and management) system with an enjoyable user interface.</p></li></ul><ul><li><p> is a data pipeline tool that brings together data ingestion, data transformation with SQL &amp; Python, and data quality into a single framework.</p></li><li><p> is an open-source multi-language workflow engine that lets you build, extend, and connect automation in the languages you already know.</p></li></ul><div><p> If you have feedback to share or are interested in <a href=\"https://www.devopsbulletin.com/p/sponsorships\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email. </p></div>","contentLength":2494,"flags":null,"enclosureUrl":"https://substackcdn.com/image/youtube/w_728,c_limit/2p_huDMN8XI","enclosureMime":"","commentsUrl":null},{"title":"How Data, Empathy and Visibility Are Redefining DevOps Maturity","url":"https://devops.com/how-data-empathy-and-visibility-are-redefining-devops-maturity/","date":1761923338,"author":"Alan Shimel","guid":323599,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Developer Discipline Matters More Than Ever in the AI Era","url":"https://devops.com/why-developer-discipline-matters-more-than-ever-in-the-ai-era/","date":1761922400,"author":"Mike Vizard","guid":323570,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 5 Open Source Tools to Automate Database Deployments in CI/CD Pipelines — Setup Guide","url":"https://blog.devops.dev/top-5-open-source-tools-to-automate-database-deployments-in-ci-cd-pipelines-setup-guide-4703395bde4c?source=rss----33f8b2d9a328---4","date":1761919861,"author":"Virinchi T","guid":323593,"unread":true,"content":"<h3>Top 5 Open Source Tools to Automate Database Deployments in CI/CD Pipelines — Setup&nbsp;Guide</h3><p>Database migrations are critical for modern application development, yet they remain one of the riskiest aspects of the deployment process. Whether you’re upgrading legacy systems, moving to cloud environments, or maintaining consistency across development, testing, and production, reliable database migration tools integrated into your CI/CD pipeline are essential.</p><p>In this comprehensive guide, we’ll explore the top 5 open-source database migration CI/CD tools, their key features, and step-by-step setup instructions to help you automate your database change management.</p><h3>Why Database Migration Tools&nbsp;Matter</h3><p>Before diving into the tools, let’s understand why they’re&nbsp;crucial:</p><ul><li>: Track database changes just like application code</li><li>: Ensure all environments have the same&nbsp;schema</li><li>: Eliminate manual errors and reduce deployment time</li><li>: Safely revert changes if something goes&nbsp;wrong</li><li>: Maintain compliance with detailed change&nbsp;logs</li><li>: Enable multiple developers to work on database changes simultaneously</li></ul><h3>1. Flyway — The Simple Yet Powerful&nbsp;Choice</h3><p>Flyway is a lightweight, open-source database migration tool that has gained massive popularity due to its simplicity and reliability. Created by Redgate, it treats database migrations like code using simple SQL or Java-based migration scripts.</p><ul><li><strong>Simple SQL-based migrations</strong>: Write migrations in plain SQL that are easy to understand</li><li><strong>Support for 50+ databases</strong>: Including PostgreSQL, MySQL, Oracle, SQL Server, MariaDB, and&nbsp;more</li><li><strong>Version control integration</strong>: Seamlessly integrates with Git and other&nbsp;VCS</li><li>: Support for views, stored procedures, and functions</li><li>: Easy integration into existing databases</li><li>: Prevents duplicate migrations</li><li>: CLI, Maven/Gradle plugins, and API&nbsp;support</li><li>: Minimal overhead and fast execution</li></ul><p>PostgreSQL, MySQL, MariaDB, Oracle, SQL Server, DB2, H2, SQLite, Redshift, CockroachDB, Firebird, and many&nbsp;more.</p><p>Flyway uses a migration naming convention and maintains a flyway_schema_history table to track which migrations have been&nbsp;applied:</p><p><strong>Migration Naming Convention:</strong></p><pre>V&lt;Version&gt;__&lt;Description&gt;.sql</pre><ul><li>V1__create_users_table.sql</li><li>V1.1__add_email_column.sql</li></ul><pre># Download and extractwget -qO- https://download.red-gate.com/maven/release/com/redgate/flyway/flyway-commandline/10.8.1/flyway-commandline-10.8.1-linux-x64.tar.gz | tar -xvz</pre><pre># Create symbolic linksudo ln -s `pwd`/flyway-10.8.1/flyway /usr/local/bin</pre><pre>docker pull flyway/flyway</pre><pre>&lt;dependency&gt;    &lt;groupId&gt;org.flywaydb&lt;/groupId&gt;<p>    &lt;artifactId&gt;flyway-core&lt;/artifactId&gt;</p>    &lt;version&gt;10.8.1&lt;/version&gt;</pre><h4>Step 2: Create Migration Files</h4><p>Create a directory structure:</p><pre>/project  /migrations    V2__add_tables.sql</pre><p>Example migration file (V1__create_users_table.sql):</p><pre>CREATE TABLE users (    id INT PRIMARY KEY AUTO_INCREMENT,<p>    username VARCHAR(255) NOT NULL,</p>    email VARCHAR(255) NOT NULL,<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><p>Create a flyway.conf file:</p><pre>flyway.url=jdbc:postgresql://localhost:5432/mydbflyway.user=dbuser<p>flyway.password=dbpassword</p>flyway.locations=filesystem:./migrations<p>flyway.baselineOnMigrate=true</p></pre><pre># Check migration statusflyway info</pre><pre># Run migrationsflyway migrate</pre><pre># Rollback (requires Flyway Teams)flyway undo</pre><p>Create&nbsp;.github/workflows/database-migration.yml:</p><pre>on:  push:</pre><pre>jobs:  test:      postgres:        env:<p>          POSTGRES_DB: ${{ secrets.DB_TEST }}</p>          POSTGRES_USER: ${{ secrets.USER_TEST }}<p>          POSTGRES_PASSWORD: ${{ secrets.PASSWORD_TEST }}</p>        options: &gt;-          --health-interval 10s          --health-retries 5      - uses: actions/checkout@v2<p>      - name: Run Flyway Migrations</p>        uses: joshuaavalon/flyway-action@v3.0.0          url: ${{ secrets.URL_TEST }}<p>          user: ${{ secrets.USER_TEST }}</p>          password: ${{ secrets.PASSWORD_TEST }}<p>          locations: filesystem:./migrations</p><p>      - run: echo 'Testing complete'</p></pre><pre>  deploy-to-prod:    needs: test      - uses: actions/checkout@v2<p>      - name: Deploy to Production</p>        uses: joshuaavalon/flyway-action@v3.0.0          url: ${{ secrets.URL_PROD }}<p>          user: ${{ secrets.USER_PROD }}</p>          password: ${{ secrets.PASSWORD_PROD }}<p>          locations: filesystem:./migrations</p><p>      - run: echo 'Deployment complete'</p></pre><pre>database_migration:  stage: migrate  script:<p>    - flyway -url=\"jdbc:postgresql://$DB_HOST:5432/$DB_NAME\" </p>            -user=$DB_USER             -locations=filesystem:./migrations   only:</pre><pre>pipeline {    agent any        FLYWAY_URL = credentials('flyway-url')<p>        FLYWAY_USER = credentials('flyway-user')</p>        FLYWAY_PASSWORD = credentials('flyway-password')        stage('Checkout') {                git 'https://github.com/yourorg/your-repo.git'        }<p>        stage('Flyway Migration') {</p>            steps {                    docker run --rm \\<p>                        -v $(pwd)/migrations:/flyway/sql \\</p>                        flyway/flyway \\                        -user=${FLYWAY_USER} \\<p>                        -password=${FLYWAY_PASSWORD} \\</p>                        migrate            }    }</pre><ol><li><strong>Never modify applied migrations</strong>: Always create new migration files</li><li>: Make migration purpose clear from&nbsp;filename</li><li>: One logical change per migration</li><li><strong>Test on non-production first</strong>: Always test migrations in&nbsp;staging</li><li><strong>Store credentials securely</strong>: Use secrets management tools</li><li><strong>Version control everything</strong>: Include migrations in your repository</li></ol><h3>2. Liquibase — The Enterprise-Grade Solution</h3><p>Liquibase is arguably the most well-known database migration tool, having been in the market since 2006. It’s a Java-based CLI tool that provides sophisticated tracking of database changes through XML, YAML, JSON, or SQL&nbsp;scripts.</p><ul><li><strong>Multiple change log formats</strong>: XML, YAML, JSON, and&nbsp;SQL</li><li><strong>Support for 60+ databases</strong>: Vendor-independent solution</li><li><strong>Advanced rollback capabilities</strong>: Targeted rollback for specific changesets</li><li>: Apply changes based on database&nbsp;state</li><li><strong>Diff and sync capabilities</strong>: Compare schemas across environments</li><li><strong>Automated drift detection</strong>: Identify untracked database&nbsp;changes</li><li>: Enforce compliance and governance</li><li><strong>Extensive CI/CD integrations</strong>: Jenkins, GitLab, GitHub Actions, Azure DevOps, and&nbsp;more</li></ul><p>Oracle, SQL Server, PostgreSQL, MySQL, MariaDB, MongoDB, Snowflake, DB2, Redshift, CockroachDB, and many&nbsp;others.</p><p>Liquibase organizes changes as  within . It maintains a DATABASECHANGELOG table to track applied&nbsp;changes.</p><ul><li>: Master file listing all changesets</li><li>: Unit of change with unique ID and&nbsp;author</li><li>: Create table, add column, insert data,&nbsp;etc.</li></ul><pre># Download from liquibase.orgwget https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gz<p>tar -xzf liquibase-4.24.0.tar.gz</p>sudo mv liquibase /usr/local/bin/</pre><pre>&lt;dependency&gt;    &lt;groupId&gt;org.liquibase&lt;/groupId&gt;<p>    &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt;</p>    &lt;version&gt;4.24.0&lt;/version&gt;</pre><pre>&lt;plugin&gt;    &lt;groupId&gt;org.liquibase&lt;/groupId&gt;<p>    &lt;artifactId&gt;liquibase-maven-plugin&lt;/artifactId&gt;</p>    &lt;version&gt;4.24.0&lt;/version&gt;</pre><h4>Step 2: Create Changelog Files</h4><p>Create a master changelog (db.changelog-master.xml):</p><pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;databaseChangeLog<p>    xmlns=\"http://www.liquibase.org/xml/ns/dbchangelog\"</p>    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"<p>    xsi:schemaLocation=\"http://www.liquibase.org/xml/ns/dbchangelog</p>        http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.0.xsd\"&gt;</pre><pre>    &lt;include file=\"db/changelog/v1.0/create-users-table.xml\"/&gt;    &lt;include file=\"db/changelog/v1.1/add-email-column.xml\"/&gt;</pre><p>Create individual changeset (create-users-table.xml):</p><pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;databaseChangeLog<p>    xmlns=\"http://www.liquibase.org/xml/ns/dbchangelog\"</p>    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"<p>    xsi:schemaLocation=\"http://www.liquibase.org/xml/ns/dbchangelog</p>        http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.0.xsd\"&gt;</pre><pre>    &lt;changeSet id=\"1\" author=\"john.doe\"&gt;        &lt;createTable tableName=\"users\"&gt;<p>            &lt;column name=\"id\" type=\"int\" autoIncrement=\"true\"&gt;</p>                &lt;constraints primaryKey=\"true\"/&gt;            &lt;column name=\"username\" type=\"varchar(255)\"&gt;<p>                &lt;constraints nullable=\"false\"/&gt;</p>            &lt;/column&gt;<p>            &lt;column name=\"email\" type=\"varchar(255)\"&gt;</p>                &lt;constraints nullable=\"false\"/&gt;            &lt;column name=\"created_at\" type=\"timestamp\" defaultValueComputed=\"CURRENT_TIMESTAMP\"/&gt;    &lt;/changeSet&gt;</pre><pre>databaseChangeLog:  - changeSet:      author: john.doe        - createTable:            columns:                  name: id                  autoIncrement: true                    primaryKey: true                  name: username                  constraints:</pre><pre>--liquibase formatted sql</pre><pre>--changeset john.doe:1CREATE TABLE users (<p>    id INT PRIMARY KEY AUTO_INCREMENT,</p>    username VARCHAR(255) NOT NULL,<p>    email VARCHAR(255) NOT NULL,</p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><p>Create liquibase.properties:</p><pre>driver: org.postgresql.Driverurl: jdbc:postgresql://localhost:5432/mydbpassword: dbpassword<p>changeLogFile: db.changelog-master.xml</p>liquibase.hub.mode: off</pre><pre># Check statusliquibase status</pre><pre># Update databaseliquibase update</pre><pre># Rollback last changeliquibase rollback-count 1</pre><pre># Rollback to specific tagliquibase rollback tagName</pre><pre># Generate documentationliquibase dbDoc ./output</pre><pre># Diff two databasesliquibase diff</pre><h4>Jenkins Pipeline with&nbsp;Docker</h4><pre>pipeline {    agent {            image 'liquibase/liquibase:4.24.0'    }        DB_HOST = 'localhost'        DB_NAME = 'mydb'<p>        DB_USER = credentials('db-username')</p>        DB_PASS = credentials('db-password')<p>        CHANGELOG_FILE = 'db.changelog-master.xml'</p>        ROLLBACK_COUNT = 2        stage('Checkout') {                git 'https://github.com/yourorg/database-repo.git'        }            steps {                    liquibase status \\<p>                        --url=\"jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}\" \\</p>                        --changeLogFile=${CHANGELOG_FILE} \\                        --password=${DB_PASS}            }<p>        stage('Update Database') {</p>            steps {                    liquibase update \\<p>                        --url=\"jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}\" \\</p>                        --changeLogFile=${CHANGELOG_FILE} \\                        --password=${DB_PASS}            }                failure {                        liquibase rollback-count ${ROLLBACK_COUNT} \\<p>                            --url=\"jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}\" \\</p>                            --changeLogFile=${CHANGELOG_FILE} \\                            --password=${DB_PASS}                }        }}</pre><p>Create&nbsp;.github/workflows/liquibase.yml:</p><pre>on:  push:  pull_request:</pre><pre>jobs:  database-update:      - name: Checkout code<p>        uses: actions/checkout@v2</p>        uses: actions/setup-java@v2          java-version: '11'<p>      - name: Install Liquibase</p>        run: |<a href=\"https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gz\">https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gz</a>          tar -xzf liquibase-4.24.0.tar.gz<p>          sudo mv liquibase /usr/local/bin/</p><p>      - name: Run Liquibase Update</p>        env:<p>          DB_URL: ${{ secrets.DB_URL }}</p>          DB_USER: ${{ secrets.DB_USER }}<p>          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}</p>        run: |<p>          liquibase --url=\"${DB_URL}\" \\</p>                    --username=\"${DB_USER}\" \\<p>                    --password=\"${DB_PASSWORD}\" \\</p>                    --changeLogFile=db.changelog-master.xml \\</pre><pre>stages:  - validate</pre><pre>variables:  LIQUIBASE_VERSION: \"4.24.0\"</pre><pre>validate:  stage: validate<p>  image: liquibase/liquibase:${LIQUIBASE_VERSION}</p>  script:<p>    - liquibase --changeLogFile=db.changelog-master.xml validate</p>  only:</pre><pre>deploy-staging:  stage: deploy<p>  image: liquibase/liquibase:${LIQUIBASE_VERSION}</p>  script:      liquibase --url=\"${STAGING_DB_URL}\" \\<p>                --username=\"${STAGING_DB_USER}\" \\</p>                --password=\"${STAGING_DB_PASSWORD}\" \\<p>                --changeLogFile=db.changelog-master.xml \\</p>                update    name: staging    - develop</pre><pre>deploy-production:  stage: deploy<p>  image: liquibase/liquibase:${LIQUIBASE_VERSION}</p>  script:      liquibase --url=\"${PROD_DB_URL}\" \\<p>                --username=\"${PROD_DB_USER}\" \\</p>                --password=\"${PROD_DB_PASSWORD}\" \\<p>                --changeLogFile=db.changelog-master.xml \\</p>                update    name: production  only:</pre><ol><li>: Combine author name with timestamp</li><li>: Apply different changes for different environments</li><li>: Ensure database is in expected&nbsp;state</li><li>: Enable easy rollback to known-good states</li><li>: Organize changesets logically</li><li><strong>Never modify deployed changesets</strong>: Liquibase tracks checksums</li></ol><h3>3. Atlas — The Modern Schema-as-Code Tool</h3><p>Atlas is a modern, open-source database CI/CD tool that promotes “database schema-as-code” philosophy. Built with Go, it draws inspiration from Terraform and provides a declarative approach to database schema management.</p><ul><li>: Define schemas in HCL, SQL, or via&nbsp;ORMs</li><li>: Plan, apply, and manage migrations</li><li><strong>Automated drift detection</strong>: Identify untracked schema&nbsp;changes</li><li>: Traditional migration files also supported</li><li>: Built-in quality&nbsp;checks</li><li><strong>Multiple database support</strong>: PostgreSQL, MySQL, MariaDB, SQLite, SQL Server, ClickHouse</li><li>: Atlas Cloud for team collaboration</li><li>: Enterprise-ready security</li></ul><p>PostgreSQL, MySQL, MariaDB, SQLite, SQL Server, ClickHouse, Redshift.</p><p>Atlas supports two primary workflows:</p><ol><li><strong>Declarative (Schema-as-Code)</strong>: Define desired state, Atlas calculates migration</li><li>: Traditional migration files with Atlas enhancements</li></ol><pre>curl -sSf https://atlasgo.sh | sh</pre><pre>docker pull arigaio/atlas</pre><pre>brew install ariga/tap/atlas</pre><h4>Step 2: Initialize Project</h4><pre># Create project directorymkdir my-database</pre><pre># Initialize Atlas projectatlas init</pre><h4>Step 3: Define Schema (Declarative Approach)</h4><pre>table \"users\" {  schema = schema.public    type = int  }    type = varchar(255)  }    type = varchar(255)  }    type = timestamp<p>    default = sql(\"CURRENT_TIMESTAMP\")</p>  }    columns = [column.id]    columns = [column.email]  }</pre><pre>table \"posts\" {  schema = schema.public    type = int  }    type = int  }    type = varchar(255)  }    type = text    columns = [column.id]    columns = [column.user_id]<p>    ref_columns = [table.users.column.id]</p>    on_delete = CASCADE}</pre><h4>Step 4: Generate and Apply Migrations</h4><p><strong>Inspect current database:</strong></p><pre>atlas schema inspect \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\</pre><p><strong>Plan migration (dry&nbsp;run):</strong></p><pre>atlas schema apply \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\<p>  --to \"file://schema.hcl\" \\</p>  --dry-run</pre><pre>atlas schema apply \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\</pre><h4>Step 5: Using Versioned Migrations</h4><p>Create migration directory:</p><p>Generate migration from&nbsp;schema:</p><pre>atlas migrate diff create_users \\  --dir \"file://migrations\" \\<p>  --to \"file://schema.hcl\" \\</p>  --dev-url \"docker://postgres/15/dev\"</pre><pre>atlas migrate apply \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\<p>  --dir \"file://migrations\"</p></pre><p>Create&nbsp;.github/workflows/atlas.yml:</p><pre>on:  push:  pull_request:</pre><pre>jobs:  lint:    steps:<p>      - uses: actions/checkout@v3</p>        uses: ariga/setup-atlas@v0        run: |            --dir \"file://migrations\" \\<p>            --dev-url \"docker://postgres/15/dev\" \\</p>            --latest 1</pre><pre>  deploy:    needs: lint    if: github.ref == 'refs/heads/main'      - uses: actions/checkout@v3        uses: ariga/setup-atlas@v0        env:<p>          DATABASE_URL: ${{ secrets.DATABASE_URL }}</p>        run: |            --url \"${DATABASE_URL}\" \\<p>            --dir \"file://migrations\"</p></pre><pre>stages:  - validate</pre><pre>variables:  ATLAS_VERSION: \"latest\"</pre><pre>validate:  stage: validate<p>  image: arigaio/atlas:${ATLAS_VERSION}</p>  script:<p>    - atlas migrate lint --dir \"file://migrations\" --dev-url \"docker://postgres/15/dev\"</p>  only:</pre><pre>deploy-staging:  stage: deploy<p>  image: arigaio/atlas:${ATLAS_VERSION}</p>  script:      atlas migrate apply \\<p>        --url \"${STAGING_DB_URL}\" \\</p>        --dir \"file://migrations\"    name: staging    - develop</pre><pre>deploy-production:  stage: deploy<p>  image: arigaio/atlas:${ATLAS_VERSION}</p>  script:      atlas migrate apply \\        --dir \"file://migrations\"    name: production  only:</pre><ol><li>: Always specify dev-url for schema calculations</li><li>: Commit both HCL files and generated migrations</li><li>: Catch issues before production deployment</li><li>: Validate migrations in non-production first</li><li>: For team collaboration and enhanced&nbsp;features</li><li>: Regularly check for schema&nbsp;drift</li></ol><h3>4. Bytebase — The All-in-One Database DevOps&nbsp;Platform</h3><p>Bytebase is an all-in-one database DevOps and CI/CD solution that provides a GUI workspace for developers and DBAs to collaborate on database changes. It’s like GitHub/GitLab but specifically designed for database management.</p><ul><li>: User-friendly interface for managing database&nbsp;changes</li><li>: Database-as-code with Git integration</li><li>: Built-in review system for database&nbsp;changes</li><li>: Detect SQL anti-patterns automatically</li><li>: Protect sensitive data</li><li>: Complete audit trail of all database activities</li><li>: Fine-grained permission management</li><li>: Integrated SQL editor with data access&nbsp;control</li><li><strong>Multiple database support</strong>: MySQL, PostgreSQL, Oracle, SQL Server, MongoDB, Redis, and&nbsp;more</li><li>: Visual timeline of all database&nbsp;changes</li></ul><p>PostgreSQL, MySQL, Oracle, SQL Server, MongoDB, Redis, MariaDB, TiDB, ClickHouse, Snowflake, and&nbsp;more.</p><p>Bytebase uses a database-as-code approach&nbsp;with:</p><ul><li>: Organize databases by application or&nbsp;team</li><li>: Track database change&nbsp;requests</li><li>: Manage Dev, Staging, Production separately</li><li>: Sync database schemas from Git repositories</li><li>: Approval workflow for sensitive changes</li></ul><p><strong>Using Docker (Recommended):</strong></p><pre>docker run --init \\  --name bytebase \\  --add-host host.docker.internal:host-gateway \\  --volume ~/.bytebase/data:/var/opt/bytebase \\<p>  bytebase/bytebase:latest \\</p>  --data /var/opt/bytebase \\</pre><p>Create docker-compose.yml:</p><pre>services:  bytebase:<p>    image: bytebase/bytebase:latest</p>    container_name: bytebase    ports:    volumes:<p>      - ./bytebase-data:/var/opt/bytebase</p>    command:      - /var/opt/bytebase      - \"8080\"      - \"host.docker.internal:host-gateway\"</pre><p>Create bytebase-deployment.yaml:</p><pre>apiVersion: apps/v1kind: Deployment  name: bytebasespec:  selector:      app: bytebase    metadata:        app: bytebase      containers:        image: bytebase/bytebase:latest        - containerPort: 8080        - name: data<p>          mountPath: /var/opt/bytebase</p>        command:          - --data          - --port      volumes:        persistentVolumeClaim:---kind: Service  name: bytebasespec:  ports:    targetPort: 8080    app: bytebase</pre><h4>Step 3: Add Database Instances</h4><ol><li>Go to  → </li><li>Select database type (PostgreSQL, MySQL,&nbsp;etc.)</li><li>Enter connection details:</li></ol><ul></ul><ol><li>Go to  → </li><li>Enter project name and description</li></ol><ul><li>: Traditional approval&nbsp;workflow</li><li>: Sync from Git repository</li></ul><ol><li>Assign team members with&nbsp;roles</li></ol><h4>Step 5: Setup GitOps (Optional)</h4><ol><li>Connect Git provider (GitHub, GitLab, Bitbucket)</li><li>Configure repository and&nbsp;branch</li><li>Set up directory structure:</li></ol><ul><li>/bytebase /migrations /prod 001_create_users.sql 002_add_email_column.sql /staging 001_create_users.sql</li></ul><h4>Step 6: Create Database&nbsp;Change</h4><ol></ol><ol><li>Create SQL file in Git repository</li><li>Bytebase automatically detects and creates&nbsp;issue</li></ol><p>Bytebase provides native API for CI/CD integration.</p><pre># Get access tokencurl -X POST http://localhost:8080/v1/auth/login \\<p>  -H \"Content-Type: application/json\" \\</p>  -d '{<p>    \"email\": \"admin@example.com\",</p>    \"password\": \"yourpassword\"</pre><pre># Create issuecurl -X POST http://localhost:8080/v1/projects/project-1/issues \\<p>  -H \"Authorization: Bearer YOUR_TOKEN\" \\</p>  -H \"Content-Type: application/json\" \\    \"type\": \"DATABASE_CHANGE\",<p>    \"title\": \"Add users table\",</p>    \"description\": \"Create users table for authentication\",      \"statement\": \"CREATE TABLE users (id INT PRIMARY KEY, username VARCHAR(255));\"  }'</pre><p>Create&nbsp;.github/workflows/bytebase.yml:</p><pre>on:  push:      - 'migrations/**'</pre><pre>jobs:  deploy:      - uses: actions/checkout@v2<p>      - name: Authenticate with Bytebase</p>        id: auth          TOKEN=$(curl -X POST ${{ secrets.BYTEBASE_URL }}/v1/auth/login \\<p>            -H \"Content-Type: application/json\" \\</p>            -d '{<p>              \"email\": \"${{ secrets.BYTEBASE_EMAIL }}\",</p>              \"password\": \"${{ secrets.BYTEBASE_PASSWORD }}\"          echo \"::set-output name=token::${TOKEN}\"<p>      - name: Create Migration Issue</p>        run: |<p>          MIGRATION_SQL=$(cat migrations/latest.sql)</p>          curl -X POST ${{ secrets.BYTEBASE_URL }}/v1/projects/my-project/issues \\<p>            -H \"Authorization: Bearer ${{ steps.auth.outputs.token }}\" \\</p>            -H \"Content-Type: application/json\" \\              \\\"type\\\": \\\"DATABASE_CHANGE\\\",<p>              \\\"title\\\": \\\"Auto migration from CI\\\",</p>              \\\"payload\\\": {<p>                \\\"statement\\\": \\\"${MIGRATION_SQL}\\\"</p>              }</pre><ol><li>: Separate Dev, Staging, Production</li><li>: Require review for production changes</li><li>: Catch issues&nbsp;early</li><li>: Maintain single source of&nbsp;truth</li><li>: Track all database activities</li><li>: Protect sensitive data</li><li>: Integrate with Slack or&nbsp;email</li></ol><h3>5. Sqitch — The Git-like Database Change Management</h3><p>Sqitch is a purely open-source database change management system with no commercial offerings. Built with Perl, it has been on the market since 2012 and takes a unique, Git-inspired approach to managing database&nbsp;changes.</p><ul><li>: Works with Git, Mercurial, SVN, or no&nbsp;VCS</li><li>: Support for PostgreSQL, MySQL, Oracle, SQLite, Firebird, Vertica, Exasol, Snowflake</li><li>: Explicit dependency management</li><li><strong>No file naming conventions</strong>: Changes managed via sqitch&nbsp;plan</li><li>: Mark important milestones</li><li><strong>Powerful revert capability</strong>: Built-in rollback&nbsp;support</li><li>: Specify prerequisites for&nbsp;changes</li><li>: No GUI, fully command-line driven</li><li>: Deploy with all dependencies</li></ul><p>PostgreSQL, MySQL, Oracle, SQLite, Firebird, Vertica, Exasol, Snowflake, and&nbsp;more.</p><p>Unlike Liquibase and Flyway which use file naming conventions, Sqitch uses an explicit plan file (sqitch.plan) to specify the order and dependencies of&nbsp;changes:</p><ul><li>: Apply&nbsp;changes</li><li>: Undo&nbsp;changes</li><li>: Test that changes&nbsp;worked</li></ul><pre>docker pull sqitch/sqitch</pre><pre># Install dependenciessudo apt-get install libdbd-pg-perl libdbd-mysql-perl</pre><pre># Install from CPANcpan App::Sqitch</pre><h4>Step 2: Initialize Project</h4><pre># Create new Sqitch projectsqitch init myapp --uri https://github.com/myorg/myapp --engine pg</pre><pre># This creates:# - sqitch.conf (configuration)<p># - sqitch.plan (change plan)</p># - deploy/, revert/, verify/ directories</pre><p>Configuration file (sqitch.conf):</p><pre>[core]    engine = pg    plan_file = sqitch.plan</pre><pre>[engine \"pg\"]    target = db:pg://user:pass@localhost/mydb    client = psql</pre><pre># Add a new changesqitch add create_users -n \"Create users table\"</pre><pre># This creates three files:# - deploy/create_users.sql<p># - revert/create_users.sql</p># - verify/create_users.sql</pre><p>Edit deploy/create_users.sql:</p><pre>-- Deploy myapp:create_users to pg</pre><pre>CREATE TABLE users (    id SERIAL PRIMARY KEY,<p>    username VARCHAR(255) NOT NULL UNIQUE,</p>    email VARCHAR(255) NOT NULL UNIQUE,<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><p>Edit revert/create_users.sql:</p><pre>-- Revert myapp:create_users from pg</pre><p>Edit verify/create_users.sql:</p><pre>-- Verify myapp:create_users on pg</pre><pre>SELECT id, username, email, created_atFROM users</pre><h4>Step 4: Add Change with Dependencies</h4><pre># Add change that depends on create_userssqitch add add_posts --requires create_users -n \"Create posts table\"</pre><p>The sqitch.plan file will look&nbsp;like:</p><pre>%syntax-version=1.0.0%project=myapp<p>%uri=https://github.com/myorg/myapp</p></pre><pre>create_users 2025-01-15T10:30:00Z User Name &lt;user@example.com&gt; # Create users tableadd_posts [create_users] 2025-01-15T11:00:00Z User Name &lt;user@example.com&gt; # Create posts table</pre><pre># Check statussqitch status</pre><pre># Deploy all changessqitch deploy</pre><pre># Deploy to specific changesqitch deploy add_posts</pre><pre># Verify deploymentsqitch verify</pre><pre># Revert last changesqitch revert</pre><pre># Revert to specific changesqitch revert create_users</pre><pre># Revert all changessqitch revert --to @ROOT</pre><p>Create&nbsp;.github/workflows/sqitch.yml:</p><pre>name: Sqitch Database Migration</pre><pre>on:  push:    paths:      - 'revert/**'      - 'sqitch.plan'</pre><pre>jobs:  deploy:      postgres:        env:<p>          POSTGRES_PASSWORD: postgres</p>          POSTGRES_DB: testdb          --health-cmd pg_isready          --health-timeout 5s        ports:      - uses: actions/checkout@v2        run: |          sudo apt-get install -y sqitch libdbd-pg-perl postgresql-client<p>      - name: Run Sqitch Deploy</p>        env:<p>          SQITCH_TARGET: db:pg://postgres:postgres@localhost:5432/testdb</p>        run: |<p>      - name: Verify Deployment</p>        env:<p>          SQITCH_TARGET: db:pg://postgres:postgres@localhost:5432/testdb</p>        run: |</pre><pre>  deploy-production:    needs: deploy    if: github.ref == 'refs/heads/main'      - uses: actions/checkout@v2        run: |          sudo apt-get install -y sqitch libdbd-pg-perl<p>      - name: Deploy to Production</p>        env:<p>          SQITCH_TARGET: ${{ secrets.PROD_DATABASE_URL }}</p>        run: |          sqitch verify</pre><pre>variables:  POSTGRES_DB: testdb  POSTGRES_PASSWORD: postgres</pre><pre>test:  stage: test<p>  image: sqitch/sqitch:latest</p>  services:  variables:<p>    SQITCH_TARGET: \"db:pg://$POSTGRES_USER:$POSTGRES_PASSWORD@postgres/$POSTGRES_DB\"</p>  script:    - sqitch verify    - merge_requests</pre><pre>deploy-staging:  stage: deploy<p>  image: sqitch/sqitch:latest</p>  variables:<p>    SQITCH_TARGET: $STAGING_DATABASE_URL</p>  script:    - sqitch verify    name: staging    - develop</pre><pre>deploy-production:  stage: deploy<p>  image: sqitch/sqitch:latest</p>  variables:<p>    SQITCH_TARGET: $PROD_DATABASE_URL</p>  script:    - sqitch verify    name: production  only:</pre><pre>pipeline {    agent any        STAGING_DB = credentials('staging-db-url')<p>        PROD_DB = credentials('prod-db-url')</p>    }        stage('Checkout') {                git 'https://github.com/yourorg/database-repo.git'        }            steps {                    docker run --rm \\                        -w /repo \\                        sqitch deploy --target ${STAGING_DB}                        -v $(pwd):/repo \\                        sqitch/sqitch:latest \\<p>                        sqitch verify --target ${STAGING_DB}</p>                '''        }<p>        stage('Deploy to Production') {</p>            when {            }                input message: 'Deploy to production?', ok: 'Deploy'                    docker run --rm \\                        -w /repo \\                        sqitch deploy --target ${PROD_DB}                        -v $(pwd):/repo \\                        sqitch/sqitch:latest \\<p>                        sqitch verify --target ${PROD_DB}</p>                '''        }        failure {                docker run --rm \\                    -w /repo \\                    sqitch revert --target ${PROD_DB} -y        }}</pre><ol><li><strong>Always write verify scripts</strong>: Test that changes applied correctly</li><li><strong>Use explicit dependencies</strong>: Define prerequisites in sqitch.plan</li><li>: Mark important milestones</li><li>: Ensure rollback works before production</li><li>: Use sqitch bundle for offline deployments</li><li>: Write clear commit&nbsp;messages</li><li>: Define environments in sqitch.conf</li></ol><p>Feature Flyway Liquibase Atlas Bytebase Sqitch  Java Java Go Go/TypeScript Perl  Open Source + Commercial Open Source + Commercial Open Source + Cloud Open Source + Enterprise Pure Open Source  CLI, API, Maven/Gradle CLI, API, Maven/Gradle CLI GUI + CLI + API CLI  SQL, Java XML, YAML, JSON, SQL HCL, SQL SQL SQL  50+ 60+ 7+ 15+ 10+  Limited (Teams only) Yes Yes Yes Yes  Easy Moderate Moderate Easy Moderate  Via CI/CD Via CI/CD Native Native Native  No No (separate tool) No (Cloud has GUI) Yes No  Sequential Limited Yes Limited Explicit  Simple migrations, CI/CD Enterprise, Complex changes Modern teams, IaC fans Teams needing GUI, collaboration Git-like workflow, explicit&nbsp;control</p><ul><li>You want simplicity and ease of&nbsp;use</li><li>Your team prefers SQL-based migrations</li><li>You need fast integration into CI/CD pipelines</li><li>You’re working with standard relational databases</li><li>You don’t require complex rollback scenarios</li></ul><ul><li>You need enterprise-grade features</li><li>You require support for many database&nbsp;types</li><li>You need flexible change log&nbsp;formats</li><li>Compliance and audit trails are&nbsp;critical</li><li>You want precondition support</li></ul><ul><li>You embrace infrastructure-as-code principles</li><li>You like Terraform’s workflow</li><li>You want modern, declarative schema management</li><li>You’re building cloud-native applications</li></ul><ul><li>You want an all-in-one GUI&nbsp;solution</li><li>Your team includes non-technical stakeholders</li><li>You need built-in collaboration features</li><li>SQL review and approval workflows are important</li><li>You want integrated access control and data&nbsp;masking</li></ul><ul><li>You want Git-like change management</li><li>You need explicit dependency control</li><li>You prefer pure open-source tools</li><li>You’re comfortable with command-line tools</li><li>You want bundled offline deployments</li></ul><h3>General Best Practices for Database Migrations</h3><p>Regardless of which tool you choose, follow these universal best practices:</p><h3>1. Version Control Everything</h3><ul><li>Store all migration scripts in&nbsp;Git</li><li>Treat database changes like&nbsp;code</li><li>Use branches for feature development</li><li>Review changes via pull&nbsp;requests</li></ul><h3>2. Test in Non-Production First</h3><ul><li>Always test migrations in&nbsp;staging</li><li>Use production-like data&nbsp;volumes</li><li>Verify rollback procedures</li></ul><h3>3. Maintain Backward Compatibility</h3><ul><li>Make changes backward-compatible when&nbsp;possible</li><li>Use blue-green deployments for breaking&nbsp;changes</li><li>Keep old columns/tables temporarily</li><li>Coordinate with application deployments</li></ul><h3>4. Use Secure Credential Management</h3><ul><li>Never commit passwords to&nbsp;Git</li><li>Use secret management tools (HashiCorp Vault, AWS Secrets&nbsp;Manager)</li><li>Rotate credentials regularly</li><li>Use least-privilege database&nbsp;users</li></ul><h3>5. Implement Proper CI/CD Integration</h3><ul><li>Automate testing of migrations</li><li>Run migrations in deployment pipeline</li><li>Use separate databases for each environment</li><li>Implement approval gates for production</li></ul><ul><li>Log all migration activities</li><li>Set up alerts for&nbsp;failures</li><li>Track migration execution time</li><li>Maintain audit trails for compliance</li></ul><ul><li>Always write and test rollback&nbsp;scripts</li><li>Practice rollback procedures</li><li>Keep rollback windows&nbsp;minimal</li></ul><ul><li>Document migration purpose</li><li>Include context in commit&nbsp;messages</li><li>Maintain runbooks for complex migrations</li><li>Share knowledge with&nbsp;team</li></ul><p>Database migrations are a critical yet challenging aspect of modern software development. The right tool can make the difference between smooth, reliable deployments and risky, error-prone manual processes.</p><p>Each of the five tools we’ve covered has its strengths:</p><ul><li> offers simplicity and reliability for teams wanting straightforward SQL migrations</li><li> provides enterprise-grade features for complex, compliance-driven environments</li><li> brings modern infrastructure-as-code principles to database management</li><li> delivers an all-in-one GUI platform for team collaboration</li><li> offers Git-like control for teams wanting explicit change management</li></ul><p>The key is choosing the tool that best fits your team’s needs, technical stack, and workflow preferences. Start with one tool, integrate it into your CI/CD pipeline, and iterate based on your experience.</p><p>Remember: the best database migration tool is the one your team will actually use consistently. Pick one, set it up properly, and make database changes as reliable and traceable as your application code deployments.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4703395bde4c\" width=\"1\" height=\"1\" alt=\"\">","contentLength":30476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Doesn’t Have to Hurt","url":"https://www.docker.com/blog/security-shadow-it-collaboration/","date":1761915600,"author":"Simeon Ratliff","guid":323517,"unread":true,"content":"<p>Do you ever wish security would stop blocking the tools you need to do your job? Surprise: your security team wants the same.</p><p>There you are, just trying to get your work done, when…</p><ul><li>You need an AI to translate documentation, but all the AI services are blocked by a security web monitoring tool.</li><li>You finish coding and QA for a new software version just under the wire, but the release is late because security has not reviewed the open source software and libraries included.</li><li>Your new database works perfectly in dev/test, but it does not work in production because of a port configuration, and you do not have permissions. Changes to production permissions all require security approval</li></ul><p>Shadow IT is a spy-movie name for a phenomenon that is either a frustrating necessity or a game of whack-a-mole, depending on your responsibilities.</p><p>If you’re an engineer creating the next best product, shadow IT is a necessity.&nbsp;</p><p>Company-supplied information technology does not change fast enough to keep up with the market, let alone allow you to innovate. Despite that, your security team will come down hard on anyone who tries to go outside the allowed vendors and products. Data storage has to be squared away in encrypted, protected spaces, and you have to jump like a show pony to get access. And you have no flexibility in the tools you’re allowed to use, even if you could produce faster and better with other options.</p><p>So you stop playing by the rules, and you find tools and tech that work.</p><p>That is, until someone protests the cloud hosting bill, finds the wifi access point, or notices the unofficial software repository. Security takes away your tools or cuts off access. And then you are upset, your team feels attacked, and security is up in arms.</p><p>If you are on a security team, shadow IT is a game of whack-a-mole. Company-supplied information technology changes without review. You know they’re trying to enable innovation, but they’re negating all the IT compliance certifications that allow you to sell your services and products. You have to investigate, prove, and argue about policies and regulations just to stop people from storing client secrets in their personal cloud storage.</p><p>Whether you are a new hire in the Security Operations Center or the unlucky CISO who reports to the CTO, this is a familiar refrain.</p><p>Yet no one wants this. Not you, not your boss, and not security.</p><h2>If It Cannot Be Fixed, Break It</h2><p>It’s time we change the ground rules of security to focus on compromise rather than stringency.&nbsp;</p><p>Most security teams want to change their operations to concentrate on the capabilities they are trained for: threat intelligence, risk management, forensic analysis, and security engineering. I have never met a security professional who wants to spend their time arguing over a port configuration. It’s tiresome, and that friction inspires lasting antagonism on both sides.</p><p>Imagine working in a place where you can use innovative new tools, release products without a security delay, and change configurations so that your deployment works smoothly.</p><p>But there is a subtle change that must happen to enable this security-IT paradise: non-security teams would have to understand and implement all the requirements security departments would check. And everyone who is part of the change would need to understand the implications of their actions and take sole responsibility for the security outcomes.</p><p>My non-IT colleagues are shocked when I explain the scope of work for a security department in preparation for any release or product launch:</p><ul><li>Weaknesses and exploits for custom and third-party code</li><li>Scope and adequacy of vendor security</li><li>Data encryption, transmission, and storage, especially across borders</li><li>Compliance with regulation and data protection laws</li></ul><p>In many industries, we legally cannot remove security practices from IT processes. But we can change who takes responsibility for which parts of the work&nbsp;</p><p>Security requirements are not a secret. A developer with integrated code scanners can avoid OWASP Top 10 flaws and vulnerable libraries and remove hard-coded accounts. Infrastructure admins with access to network security tools can run tidy networks, servers, and containers with precise configurations.</p><p>The result? The security team can let go of their rigid deployment rules.</p><p>If developers use code security tools and incorporate good practices, security team approval should take hours rather than days or weeks. Security can also approve the standard container configuration rather than each separate container in an architecture. They can define the requirements, offer you tools to review your work, and help you integrate good practices into your workflow.</p><p>“Trust but verify” would become a daily pattern instead of lip service to good interdepartmental relationships. Security will continue to monitor the environment and the application after release. They will keep an eye on vendor assertions and audits, watching threat intelligence streams for notifications that demonstrate risk. Security teams will have time to do the job they signed up for, which is much more interesting than policing other departments.</p><p>This change would also require that the security team be allowed to let go. When trust is broken—if vendors are not properly assessed, or software is introduced but not reported—the fault should not lie with the security team. If insecure coding causes a compromise, the development team must be accountable, and if an inadequately configured network causes a data leak, the network and hosting team must be called on the carpet. If the requirements are in place but not met, the responsible parties must be those that agreed to them but neglected to enact them.</p><h2>Freedom to Choose Comes with a Catch</h2><p>This new freedom makes shadow IT unnecessary. Teams do not need to hide the choices they make. However, the freedom to choose comes with a catch: full responsibility for your choices.</p><p>Consider the company charge card: Finance teams create the policy for how to use company charge cards and provide the tools for reimbursement. They do not scrutinize every charge in real time, but they review usage and payments.</p><p>If the tool is abused and the agreed-upon care is ignored, the card user is held responsible. Any lack of knowledge does not exempt you from the consequences. For minor infractions, you may get a written notice. For severe infractions, you can expect to be terminated for cause.</p><p>The finance requirements, your agreement, regular review, and enacted consequences minimize fraud internally. More importantly, though, this combination protects the company against accusations of negligence.</p><p>Security responsibility could work the same. Security teams can set requirements that IT workers agree to individually. IT teams are then free to deploy and make changes as appropriate for their work. IT secures assets before they are put into production, and security continues with the best practice of reviewing assets continuously after the fact. Delays in getting the tools you need are reduced, and you control the deployment of your work with much more assurance. The incentive for shadow IT is much lower, and the personal risk of choosing it is higher.</p><p>That last bit is the catch, though—when you take control, you take responsibility for the result. Instead of committing to a patch, you back out insecure code and redeploy when it is corrected. When your department contracts with a squirrelly vendor, your manager’s budget takes the hit for breaking the contract. When the network is compromised, the CIO, not the CISO, gets fired.</p><p>Right now, the security team carries this responsibility and shoulders these risks. But the result is an enterprise held hostage by risk aversion, with no understanding or control over the outcomes.</p><p>So far, I’ve mostly addressed IT, but I also want to bring this argument back home: Security professionals, let’s stop taking control of everyone else’s work. When we make hard requirements that do not meet tech realities, our IT teams get better at hiding their tracks. You will make more progress if you invest in mutual success and reward people who step up to exceed your expectations.</p><h2>When Security and IT Make Peace, Shadow IT Becomes Unnecessary</h2><p>I once worked with a development team that wanted to store proprietary code in a hosted code repository. The repository was great for their needs: versioning automation, fine-grained access management, easy branching, access from anywhere, and centralized storage. Instead of waiting six months for the new vendor security investigation process, the developer team gathered the vendor’s audit certificates, data handling guarantees, and standard contract language about security and data mining. The devs proactively researched the third-party security scanning policies and asked for their incident response and notification policies.</p><p>Our security team would have struggled to locate this repository if the developers had simply chosen to use it. Instead, they circumvented our process in the best way—by providing every necessary answer to our security questions.</p><p>The reward was an instant yes from me, the security leader, without having to wait for my overworked team to schedule yet another vendor review.</p><p>My reward? No shadow IT plus a very happy IT team.</p><p>Security should go beyond allowing compromises like this: we should seek them out. Convince the CISO to work toward giving your IT teams both control and responsibility, find a compromise with the teams that will take security seriously—and save your energy for wrangling teams that don’t.</p><p>For admins and developers: Provide the ISO audit documents for that vendor you want to use. Be the first dev team to learn the org’s code scanning tool. Read the latest risk assessments from your cloud environment and don’t repeat vulnerable configurations. These small changes make your work faster, simpler, and less expensive than finding your own solutions.</p>","contentLength":9967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mr. Bones: A Pirate-Voiced Halloween Chatbot Powered by Docker Model Runner","url":"https://www.docker.com/blog/talking-skeleton-docker-llm/","date":1761913041,"author":"Mike Coleman","guid":323516,"unread":true,"content":"<p>My name is Mike Coleman, a staff solution architect at Docker. This year I decided to turn a Home Depot animatronic skeleton into an AI-powered,&nbsp; live, interactive Halloween chatbot. The result: kids walk up to Mr. Bones, a spooky skeleton in my yard, ask it questions, and it answers back — in full pirate voice — with actual conversational responses, thanks to a local LLM powered by <a href=\"https://docs.docker.com/ai/model-runner/\" rel=\"nofollow noopener\" target=\"_blank\">Docker Model Runner</a>.</p><p><a href=\"https://docs.docker.com/ai/model-runner/?utm_source=chatgpt.com\" rel=\"nofollow noopener\" target=\"_blank\"></a> is a tool from Docker that makes it dead simple to run open-source LLMs locally using standard Docker workflows. I pulled the model like I’d pull any image, and it exposed an OpenAI-compatible API I could call from my app. Under the hood, it handled model loading, inference, and optimization.</p><p>For this project, Docker Model Runner offered a few key benefits:</p><ul><li> for LLM inference — unlike OpenAI or Anthropic</li><li> because the model runs on local hardware</li><li> over model selection, prompts, and scaffolding</li><li><strong>API-compatible with OpenAI</strong> — switching providers is as simple as changing an environment variable and restarting the service</li></ul><p>That last point matters: if I ever needed to switch to OpenAI or Anthropic for a particular use case, the change would take seconds.</p><p>Figure 1: System overview of Mr. Bones answering questions in pirate language</p><ol><li> records audio</li><li> transcribes speech to text</li><li><strong>API call to a Windows gaming PC</strong> with an RTX 5070 GPU</li><li> runs a local LLaMA 3.1 8B (Q4 quant) model</li><li><strong>LLM returns a text response</strong></li><li> converts the text to speech (pirate voice)</li><li><strong>Pi sends audio to skeleton via Bluetooth</strong>, which moves the jaw in sync</li></ol><p>Figure 2: The controller box that holds the Raspberry Pi that drives the pirate</p><p>That Windows machine isn’t a dedicated inference server — it’s my gaming rig. Just a regular setup running a quantized model locally.</p><p>The biggest challenge with this project was balancing response quality (in character and age appropriate) and response time. With that in mind, there were four key areas that needed a little extra emphasis: model selection, how to do text to speech (TTS) processing efficiently, fault tolerance, and setting up guardrails.&nbsp;</p><h2><strong>Consideration 1: Model Choice and Local LLM Performance</strong></h2><p>I tested several open models and found <strong>LLaMA 3.1 8B (Q4 quantized)</strong> to be the best mix of performance, fluency, and personality. On my RTX 5070, it handled real-time inference fast enough for the interaction to feel responsive.</p><p>At one point I was struggling to keep Mr. Bones in character, so I&nbsp; tried OpenAI’s ChatGPT API, but response times averaged .</p><p>By revising the prompt and Docker Model Runner serving the right model, I got that down to . That’s a huge difference when a kid is standing there waiting for the skeleton to talk.</p><p>In the end, GPT-4 was only  at staying in character and avoiding inappropriate replies. With a solid prompt scaffold and some guardrails, the local model held up just fine.</p><h2><strong>Consideration 2: TTS Pipeline: Kokoro to ElevenLabs Flash</strong></h2><p>I first tried using , a local TTS engine. It worked, but the voices were too generic. I wanted something more pirate-y, without adding custom audio effects.</p><p>So I moved to , starting with their multilingual model. The voice quality was excellent, but latency was painful — especially when combined with LLM processing. Full responses could take up to , which is way too long.</p><p>Eventually I found , a much faster model. That helped a lot. I also changed the logic so that instead of waiting for the entire LLM response, I  the output and sent it to ElevenLabs in parts. Not true streaming, but it allowed the Pi to start playing the audio as each chunk came back.</p><p>This turned the skeleton from slow and laggy into something that felt snappy and responsive.</p><h2><strong>Consideration 3: Weak Points and Fallback Ideas</strong></h2><p>While the LLM runs locally, the system still depends on the internet for ElevenLabs. If the network goes down, the skeleton stops talking.</p><p>One fallback idea I’m exploring: creating a set of common Q&amp;A pairs (e.g., “What’s your name?”, “Are you a real skeleton?”), embedding them in a local , and having the Pi serve those in case the TTS call fails.</p><p>But the deeper truth is: this is a . If the Pi loses its connection to the Windows machine, the whole thing is toast. There’s no skeleton-on-a-chip mode yet.</p><h2><strong>Consideration 4: Guardrails and Prompt Engineering</strong></h2><p>Because kids will say anything, I put some safeguards in place via my system prompt.&nbsp;</p><div><pre>You are \"Mr. Bones,\" a friendly pirate who loves chatting with kids in a playful pirate voice.\n\nIMPORTANT RULES:\n- Never break character or speak as anyone but Mr. Bones\n- Never mention or repeat alcohol (rum, grog, drink), drugs, weapons (sword, cannon, gunpowder), violence (stab, destroy), or real-world safety/danger\n- If asked about forbidden topics, do not restate the topic; give a kind, playful redirection without naming it\n- Never discuss inappropriate content or give medical/legal advice\n- Always be kind, curious, and age-appropriate\n\nBEHAVIOR:\n- Speak in a warm, playful pirate voice using words like \"matey,\" \"arr,\" \"aye,\" \"shiver me timbers\"\n- Be imaginative and whimsical - talk about treasure, ships, islands, sea creatures, maps\n- Keep responses conversational and engaging for voice interaction\n- If interrupted or confused, ask for clarification in character\n- If asked about technology, identity, or training, stay fully in character; respond with whimsical pirate metaphors about maps/compasses instead of tech explanations\n\nFORMAT:\n- Target 30 words; must be 10-50 words. If you exceed 50 words, stop early\n- Use normal punctuation only (no emojis or asterisks)\n- Do not use contractions. Always write \"Mister\" (not \"Mr.\"), \"Do Not\" (not \"Don't\"), \"I Am\" (not \"I'm\")\n- End responses naturally to encourage continued conversation\n\n</pre></div><p>The prompt is designed to deal with a few different issues. First and foremost, keeping things appropriate for the intended audience. This includes not discussing sensitive topics, but also staying in character at all times.&nbsp; Next I added some instructions to deal with pesky parents trying to trick Mr. Bones into revealing his true identity. Finally, there is some guidance on response format to help keep things conversational – for instance, it turns out that some STT engines can have problems with things like contractions.&nbsp;</p><p>Instead of just refusing to respond, the prompt redirects sensitive or inappropriate inputs in-character. For example, if a kid says “I wanna drink rum with you,” the skeleton might respond, “Arr, matey, seems we have steered a bit off course. How about we sail to smoother waters?”</p><p>This approach keeps the interaction playful while subtly correcting the topic. So far, it’s been enough to keep Mr. Bones spooky-but-family-friendly.</p><p>Figure 3: Mr. Bones is powered by AI and talks to kids in pirate-speak with built-in safety guardrails.</p><p>This project started as a Halloween goof, but it’s turned into a surprisingly functional proof-of-concept for real-time, local voice assistants.</p><p>Using  for LLMs gave me speed, cost control, and flexibility. ElevenLabs Flash handled voice. A Pi 5 managed the input and playback. And a Home Depot skeleton brought it all to life.</p><p>Could you build a more robust version with better failover and smarter motion control? Absolutely. But even as he stands today, Mr. Bones has already made a bunch of kids smile — and probably a few grown-up engineers think, “Wait, I could build one of those.”&nbsp;</p><p>Figure 4: Aye aye! Ye can build a Mr. Bones too and bring smiles to all the young mateys in the neighborhood!</p>","contentLength":7471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cursor 2.0 Brings Faster AI Coding and Multi-Agent Workflows","url":"https://devops.com/cursor-2-0-brings-faster-ai-coding-and-multi-agent-workflows/","date":1761901689,"author":"Tom Smith","guid":323431,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"8 Pipeline Caching Tricks That Cut CI Time in Half","url":"https://blog.devops.dev/8-pipeline-caching-tricks-that-cut-ci-time-in-half-3af50fae7db9?source=rss----33f8b2d9a328---4","date":1761833522,"author":"Obafemi","guid":322183,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aembit Introduces Identity and Access Management for Agentic AI","url":"https://devops.com/aembit-introduces-identity-and-access-management-for-agentic-ai/","date":1761825724,"author":"cybernewswire","guid":322090,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Integrate Quantum-Safe Security into Your DevOps Workflow","url":"https://devops.com/how-to-integrate-quantum-safe-security-into-your-devops-workflow/","date":1761825054,"author":"Carl Torrence","guid":322089,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"theCUBE Research economic validation of Docker’s development platform","url":"https://www.docker.com/blog/thecube-research-economic-validation-of-docker-development-platform/","date":1761824788,"author":"John Ayub","guid":322088,"unread":true,"content":"<h3><strong>Docker’s ROI and impact on agentic AI, security, and developer productivity</strong>.</h3><p> surveyed ~400 IT and AppDev professionals at leading global enterprises to investigate <strong>Docker’s ROI and impact on agentic AI development, software supply chain security, and developer productivity</strong>.&nbsp; The industry context is that enterprise developers face mounting pressure to rapidly ship features, build agentic AI applications, and maintain security, all while navigating a fragmented array of development tools and open source code that require engineering cycles and introduce security risks. Docker transformed software development through containers and DevSecOps workflows, and is now doing the same for agentic AI development and software supply chain security. <strong>&nbsp;theCUBE Research quantified Docker’s impact:</strong> teams build agentic AI apps faster, achieve near-zero CVEs, remediate vulnerabilities before exploits, ship modern cloud-native applications, save developer hours, and generate financial returns.</p><p> for key highlights and analysis.  theCUBE Research <a href=\"https://www.docker.com/resources/thecube-research-docker-economic-validation-analyst-study/\"></a>and <a href=\"https://www.docker.com/resources/thecube-research-docker-economic-validation-ebook/\"></a>to take a deep dive.</p><h3>Agentic AI development streamlined using familiar technologies</h3><p>Developers can build, run, and share agents and compose agentic systems using familiar Docker container workflows. To do this, developers can build agents safely using Docker MCP Gateway, Catalog, and Toolkit; run agents securely with Docker Sandboxes; and run models with Docker Model Runner. These capabilities align with theCUBE Research findings that <strong>87% of organizations reduced AI setup time by over 25%</strong> and <strong>80% report accelerating AI time-to-market by at least 26%</strong>.&nbsp; Using Docker’s modern and secure software delivery practices, development teams can implement AI feature experiments faster and in days test agentic AI capabilities that previously took months. <strong>Nearly 78% of developers experienced significant improvement in the standardization and streamlining of AI development workflows</strong>, enabling better testing and validation of AI models. Docker helps enterprises generate business advantages through deploying new customer experiences that leverage agentic AI applications. This is phenomenal, given the nascent stage of agentic AI development in enterprises.</p><h3>Software supply chain security and innovation can move in lockstep</h3><p>Security engineering and vulnerability remediation can slow development to a crawl. Furthermore, checkpoints or controls may be applied too late in the software development cycle, or after dangerous exploits, creating compounded friction between security teams seeking to mitigate vulnerabilities and developers seeking to rapidly ship features. Docker embeds security directly into development workflows through vulnerability analysis and continuously-patched certified container images. theCUBE Research analysis supports these Docker security capabilities: <strong>79% of organizations find Docker extremely or very effective at maintaining security &amp; compliance</strong>, while <strong>95% of respondents reported that Docker improved their ability to identify and remediate vulnerabilities.</strong> By making it very simple for developers to use secure images as a default, Docker enables engineering teams to plan, build, and deploy securely without sacrificing feature velocity or creating deployment bottlenecks. Security and innovation can move in lockstep because Docker concurrently secures software supply chains and eliminates vulnerabilities.</p><h3>Developer productivity becomes a competitive advantage</h3><p>Consistent container environments eliminate friction, accelerate software delivery cycles, and enable teams to focus on building features rather than overcoming infrastructure challenges. When developers spend less time on environment setup and troubleshooting, they ship more features. Application features that previously took months now reach customers in weeks. The research demonstrates Docker’s ability to increase developer productivity. <strong>72% of organizations reported significant productivity gains in development workflows</strong>, while <strong>75% have transformed or adopted DevOps practices when using Docker.</strong> Furthermore, when it comes to AI and supply chain security, the findings mentioned above further support how Docker unlocks developer productivity.</p><h3>Financial returns exceed expectations</h3><p>CFOs demand quantifiable returns for technology investments, and Docker delivers them.  reported substantial , <strong>with 43% reporting $50,000-$250,000 in cost reductions</strong> from infrastructure efficiency, reduced rework, and faster time-to-market. The ROI story is equally compelling: <strong>69% of organizations report ROI exceeding 101%, with many achieving ROI above 500%</strong>. When factoring in faster feature delivery, improved developer satisfaction, and reduced security incidents, the business case for Docker becomes even more tangible. <strong>The direct costs of a security breach can surpass $500 million</strong>, so mitigating even a fraction of this cost provides a compelling financial justification for enterprises to deploy Docker to every developer.</p><h3>Modernization and cloud native apps remain top of mind</h3><p>For enterprises who maintain extensive legacy systems, Docker serves as a proven catalyst for cloud-native transformation at scale. Results show that nearly <strong>nine in ten (88%) of organizations report Docker has enabled modernization of at least 10%</strong> of their applications, with half achieving modernization across 31-60% of workloads and another 20% modernizing over 60%. Docker accelerates the shift from monolithic architectures to modern containerized cloud-native environments while also delivering substantial business value.&nbsp; For example, <strong>37% of organizations report 26% to &gt;50% faster product time-to-market</strong>, and <strong>72% report annual cost savings ranging from $50,000 to over $1 million</strong>.</p><h3><strong>Learn more about Docker’s impact on enterprise software development</strong></h3><p>Docker has evolved from a containerization suite into a development platform for testing, building, securing, and deploying modern software, including agentic AI applications. Docker enables enterprises to apply proven containerization and DevSecOps practices to agentic AI development and software supply chain security.</p><p><strong>Download (below) the full report and the ebook from theCUBE Research analysis </strong>to learn Docker’s impact on developer productivity, software supply chain security, agentic AI application development, CI/CD and DevSecOps, modernization, cost savings, and ROI.&nbsp; Learn how enterprises leverage Docker to transform application development and win in markets where speed and innovation determine success.</p><p><strong>theCUBE Research economic validation of Docker’s development platform</strong></p>","contentLength":6581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anatomy of an Outage: Our AWS AutoScaling Group “Helping” Hand Pushed us off the Cliff","url":"https://devops.com/anatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff/","date":1761820328,"author":"Muhammad Yawar Malik","guid":322048,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Cloud Key Management Options","url":"https://devops.com/exploring-cloud-key-management-options/","date":1761818865,"author":"Alexander Williams","guid":322047,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GKE 10 Year Anniversary, with Gari Singh","url":"https://e780d51f-f115-44a6-8252-aed9216bb521.libsyn.com/gke-10-year-anniversary-with-gari-singh","date":1761780840,"author":"","guid":320999,"unread":true,"content":"<p dir=\"ltr\">GKE turned 10 in 2025! In this episode, we talk with GKE PM <a href=\"https://www.linkedin.com/in/garisingh/\">Gari Singh</a> about GKE's journey from early container orchestration to AI-driven ops. Discover Autopilot, IPPR, and a bold vision for the future of Kubernetes.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p>","contentLength":282,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/kpod262.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Deaf and Hard of Hearing WG Meeting - October 2025","url":"https://www.youtube.com/watch?v=Ngjrv07mdDI","date":1761768785,"author":"CNCF [Cloud Native Computing Foundation]","guid":320946,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":319,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Ngjrv07mdDI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Survey Surfaces Rising Tide of Vulnerabilities in Code Generated by AI","url":"https://devops.com/survey-surfaces-rising-tide-of-vulnerabilities-in-code-generated-by-ai/","date":1761762418,"author":"Mike Vizard","guid":320876,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AppOmni Open Sources Heisenberg Tool to Scan Pull Requests for Dependencies","url":"https://devops.com/appomni-open-sources-heisenberg-tool-to-scan-pull-requests-for-dependencies/","date":1761754105,"author":"Mike Vizard","guid":320812,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spring Boot Java App on Azure VM","url":"https://blog.devops.dev/spring-boot-java-app-on-azure-vm-e36f5ca7ce86?source=rss----33f8b2d9a328---4","date":1761746761,"author":"Amita Pal Singh","guid":320733,"unread":true,"content":"<p>The classic developer dilemma: Your code is a masterpiece, but it’s trapped on .</p><p>To make it accessible to the world, you need a robust deployment platform. For critical applications, like core banking systems or high-throughput financial services, deployment isn’t just a technical task; it’s a cornerstone of digital transformation. Large FinTech companies are increasingly turning to cloud infrastructure, even for their complex, legacy monolith applications, to accelerate deployment, ensure scalability, enhance disaster recovery, and deliver insights faster. Azure Virtual Machines (VMs) provide a powerful, flexible solution for hosting these applications with enterprise-grade reliability.</p><p>This guide serves as your roadmap to deploying a Spring Boot Java application on an Azure VM. We’ll walk through the essential steps to take your compiled application from local development to a production-ready cloud environment</p><h3>Prerequisites and Environment Setup</h3><h4>Sign in to the Azure&nbsp;portal</h4><p>Enter virtual machines in the search. Under Services, select Virtual machines.</p><ul><li>In the Basics tab, under the Project details, select an appropriate subscription. Then choose an existing resource group or create a new one to&nbsp;use.</li><li>Provide a VM name, select the region nearest to you, and fill in mandatory fields.</li><li>Select OS Image for&nbsp;Linux</li><li>Under Authentication type, select password, provide user name and password, and save the credentials somewhere. We would need the credentials to login to the&nbsp;VM.</li><li>Under Inbound port rules &gt; Public inbound ports, choose Allow selected ports and then select SSH (22) and HTTP (80) from the drop-down.</li></ul><ul><li>Select the Review + create button at the bottom of the&nbsp;page.</li><li>On the Create a virtual machine page, you can see the details about the VM you are about to create. Select&nbsp;Create.</li><li>When the deployment is finished, select Go to resource.</li><li>On the page for your new VM, select the public IP address and copy&nbsp;it.</li></ul><p>Select BashExecute&nbsp;command</p><pre>ssh &lt;username&gt;@&lt;vm public ip&gt;</pre><p>Enter the password to&nbsp;connect.</p><p>If you are deploying to an Azure Virtual Machine (VM) running a Linux distribution like Ubuntu, you may need to ensure the correct Java version is installed. The following commands are specific to Debian/Ubuntu-based systems to install .</p><p><strong>Step 1: Search for Available OpenJDK&nbsp;Packages</strong></p><p>This command verifies the existence of OpenJDK 17 packages in the repository cache.</p><pre>apt-cache search openjdk | grep openjdk-17</pre><p><strong>Step 2: Install OpenJDK 17&nbsp;JRE</strong></p><pre>sudo apt install openjdk-17-jre</pre><p>Confirm that the correct version is set as the default Java&nbsp;runtime.</p><p> The output should clearly display the installed Java 17 version&nbsp;details.</p><p>Tomcat server is used for deploying Java-based web applications.</p><p>Instructions for setup and configuration of Tomcat server are available in&nbsp;<a href=\"https://amitasinghtech.medium.com/tomcat-server-setup-dde5f42dd4c6\">article</a></p><p>In an Azure VM environment, you must also ensure the VM’s <strong>Network Security Group (NSG)</strong> has an inbound rule to allow traffic on the Tomcat port (default ) from your desired source IP or&nbsp;range.</p><p>Details on how to create a WAR file for Spring Boot application are available in the&nbsp;<a href=\"https://medium.com/@amitasinghtech/deploying-a-spring-boot-application-as-a-war-file-on-apache-tomcat-9a48cd04f3ae\">article</a></p><p>Navigate to the Tomcat Manager GUI -&gt; “WAR file to deploy” section, and&nbsp;deploy</p><p><strong>Check for Issues and Exceptions</strong></p><p>Deployment is rarely smooth on the first attempt. To troubleshoot, you must monitor the logs at /opt/tomcat/logs directory.</p><p><strong>Checking Logs using the Command&nbsp;Line:</strong></p><p>Log files are typically huge in size. To view the end of a log file in real-time or view a specific number of lines, use following Bash commands:</p><p>View the last 10 lines of the host-manager log</p><pre>/opt/tomcat/logs tail host-manager.*.log</pre><p>View the last N lines of any&nbsp;file</p><pre>tail -n &lt;number of lines&gt; &lt;filename&gt;</pre><p>Continuously stream the main application log (Ctrl+C to&nbsp;stop)</p><ol><li>The application will be accessible at:</li></ol><pre>http://&lt;public-IP&gt;:8080/your-war-file-name/</pre><p>Where your-war-file-name is the name of your WAR file without the&nbsp;.war extension (e.g. BasicLOS). This is also known as the Context&nbsp;Path.</p><pre>curl http://&lt;privateip&gt;:8080</pre><h3>Change the port to 80 (Optional)</h3><p>For improved user experience it is advisable to change the port to&nbsp;80.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e36f5ca7ce86\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum‑Ready Cloud DevOps – Getting ready for Quantum Computing Integration","url":"https://devops.com/quantum%e2%80%91ready-cloud-devops-getting-ready-for-quantum-computing-integration/","date":1761736788,"author":"Joydip Kanjilal","guid":320663,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Experience-Based Guide to Choosing the Right DevOps Provider in 2026","url":"https://devops.com/an-experience-based-guide-to-choosing-the-right-devops-provider-in-2026/","date":1761732552,"author":"Alex Vakulov","guid":320635,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build more accurate AI applications with Amazon Nova Web Grounding","url":"https://aws.amazon.com/blogs/aws/build-more-accurate-ai-applications-with-amazon-nova-web-grounding/","date":1761695957,"author":"Matheus Guimaraes","guid":319489,"unread":true,"content":"<p>Imagine building AI applications that deliver accurate, current information without the complexity of developing intricate data retrieval systems. Today, we’re excited to announce the general availability of Web Grounding, a new built-in tool for Nova models on <a href=\"https://aws.amazon.com/bedrock/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">Amazon Bedrock</a>.</p><p>Web Grounding provides developers with a turnkey <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">Retrieval Augmented Generation (RAG)</a> option that allows the Amazon Nova <a href=\"https://aws.amazon.com/what-is/foundation-models/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">foundation models</a> to intelligently decide when to retrieve and incorporate relevant up-to-date information based on the context of the prompt. This helps to ground the model output by incorporating cited public sources as context, aiming to reduce hallucinations and improve accuracy.</p><p>Developers should consider using Web Grounding when building applications that require access to current, factual information or need to provide well-cited responses. The capability is particularly valuable across a range of applications, from knowledge-based chat assistants providing up-to-date information about products and services, to content generation tools requiring fact-checking and source verification. It’s also ideal for research assistants that need to synthesize information from multiple current sources, as well as customer support applications where accuracy and verifiability are crucial.</p><p>Web Grounding is especially useful when you need to reduce hallucinations in your AI applications or when your use case requires transparent source attribution. Because it automatically handles the retrieval and integration of information, it’s an efficient solution for developers who want to focus on building their applications rather than managing complex RAG implementations.</p><p> Web Grounding seamlessly integrates with supported Amazon Nova models to handle information retrieval and processing during inference. This eliminates the need to build and maintain complex RAG pipelines, while also providing source attributions that verify the origin of information.</p><p>Let’s see an example of asking a question to Nova Premier using Python to call the Amazon Bedrock Converse API with Web Grounding enabled.</p><p>First, I created an Amazon Bedrock client using <a href=\"https://aws.amazon.com/sdk-for-python/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">AWS SDK for Python (Boto3)</a> in the usual way. For good practice, I’m using a session, which helps to group configurations and make them reusable. I then create a BedrockRuntimeClient.</p><pre><code>try:\n    session = boto3.Session(region_name='us-east-1')\n    client = session.client(\n        'bedrock-runtime')</code></pre><p>I then prepare the Amazon Bedrock Converse API payload. It includes a “role” parameter set to “user”, indicating that the message comes from our application’s user (compared to “assistant” for AI-generated responses).</p><p>For this demo, I chose the question “What are the current AWS Regions and their locations?” This was selected intentionally because it requires current information, making it useful to demonstrate how Amazon Nova can automatically invoke searches using Web Grounding when it determines that up-to-date knowledge is needed.</p><pre><code># Prepare the conversation in the format expected by Bedrock\nquestion = \"What are the current AWS regions and their locations?\"\nconversation = [\n   {\n     \"role\": \"user\",  # Indicates this message is from the user\n     \"content\": [{\"text\": question}],  # The actual question text\n      }\n    ]</code></pre><p>First, let’s see what the output is without Web Grounding. I make a call to Amazon Bedrock Converse API.</p><pre><code># Make the API call to Bedrock \nmodel_id = \"us.amazon.nova-premier-v1:0\" \nresponse = client.converse( \n    modelId=model_id, # Which AI model to use \n    messages=conversation, # The conversation history (just our question in this case) \n    )\nprint(response['output']['message']['content'][0]['text'])</code></pre><p>I get a list of all the current <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">AWS Regions</a> and their locations.</p><p>Now let’s use Web Grounding. I make a similar call to the Amazon Bedrock Converse API, but declare  as one of the tools available to the model.</p><pre><code>model_id = \"us.amazon.nova-premier-v1:0\" \nresponse = client.converse( \n    modelId=model_id, \n    messages=conversation, \n    toolConfig= {\n          \"tools\":[ \n              {\n                \"systemTool\": {\n                   \"name\": \"nova_grounding\" # Enables the model to search real-time information\n                 }\n              }\n          ]\n     }\n)</code></pre><p>After processing the response, I can see that the model used Web Grounding to access up-to-date information. The output includes reasoning traces that I can use to follow its thought process and see where it automatically queried external sources. The content of the responses from these external calls appear as  – a standard practice in AI systems that both protects sensitive information and helps manage output size.</p><p>Additionally, the output also includes  objects containing information about the sources queried by Web Grounding.</p><p>Finally, I can see the list of AWS Regions. It finishes with a message right at the end stating that “These are the most current and active AWS regions globally.”</p><p>Web Grounding represents a significant step forward in making AI applications more reliable and current with minimum effort. Whether you’re building customer service chat assistants that need to provide up-to-date accurate information, developing research applications that analyze and synthesize information from multiple sources, or creating travel applications that deliver the latest details about destinations and accommodations, Web Grounding can help you deliver more accurate and relevant responses to your users with a convenient turnkey solution that is straightforward to configure and use.</p><p>Amazon Nova Web Grounding is available now in US East (N. Virginia), US East (Ohio), and US West (Oregon).</p><p>Currently, you can only use Web Grounding with Nova Premier but support for other Nova models will be added soon.</p><p>10/30/25: Updated to all available regions. Original launch only in US East (N. Virginia).</p><a href=\"https://link.codingmatheus.com/linkedin\">Matheus Guimaraes | @codingmatheus</a>","contentLength":5921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TAP Developer Portals in Practice: A Deep Dive into Cloud-Native Productivity with Backstage","url":"https://blog.devops.dev/tap-developer-portals-in-practice-a-deep-dive-into-cloud-native-productivity-with-backstage-bdb6d20bee32?source=rss----33f8b2d9a328---4","date":1761681104,"author":"JIN","guid":319404,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CORS Busters: Quick Hacks for Local Dev (Chrome, Edge, Firefox, Safari)","url":"https://blog.devops.dev/cors-busters-quick-hacks-for-local-dev-chrome-edge-firefox-safari-bd06bd93dc3a?source=rss----33f8b2d9a328---4","date":1761681094,"author":"Raja Sekar Durairaj","guid":319403,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automating Azure Infrastructure and Deployment with Terraform & Ansible: Step-by-Step Guide","url":"https://blog.devops.dev/automating-azure-infrastructure-and-deployment-with-terraform-ansible-step-by-step-guide-88bbe020418c?source=rss----33f8b2d9a328---4","date":1761681085,"author":"Egwu Oko","guid":319402,"unread":true,"content":"<p>Automation is at the heart of modern DevOps. Manually creating cloud resources and configuring servers is time-consuming, error-prone, and non-repeatable.</p><p>In this project, I built — a complete automated pipeline to deploy a :</p><ul><li> provisions the Azure infrastructure.</li><li> installs and configures Nginx, builds the frontend app, deploys static files, and reloads the service automatically.</li></ul><p>This project demonstrates <strong>Infrastructure as Code (IaC)</strong>, , and real-world automation practices.</p><h3>Step 1: Prepare the Project&nbsp;Folder</h3><ul><li>Create the project root directory and change to&nbsp;it.</li></ul><pre>mkdir mini-finance &amp;&amp; cd mini-finance</pre><ul><li>Create the Terraform and Ansible directories</li></ul><ul><li>Change directory to the Terraform folder and create the Terraform files</li></ul><pre>cd terraformtouch main.tf variables.tf outputs.tf</pre><ul><li>Change to the ansible directory and create the ansible&nbsp;files</li></ul><pre>cd ansibletouch inventory.ini site.yml</pre><h3>Step 2: Provision Azure Infrastructure with Terraform</h3><p>Create a secure VM with networking and firewall rules, ready for deployment.</p><h3>Terraform Implementation Steps</h3><ol><li><strong>Define main.tf file for resources</strong></li></ol><pre>provider \"azurerm\" {  features {}resource \"azurerm_resource_group\" \"rg\" {<p>  name     = var.resource_group_name</p>  location = var.location<p>resource \"azurerm_virtual_network\" \"vnet\" {</p>  name                = \"${var.resource_group_name}-vnet\"<p>  address_space       = [\"10.0.0.0/16\"]</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>}<p>resource \"azurerm_subnet\" \"subnet\" {</p>  name                 = \"${var.resource_group_name}-subnet\"<p>  resource_group_name  = azurerm_resource_group.rg.name</p>  virtual_network_name = azurerm_virtual_network.vnet.name<p>  address_prefixes     = [\"10.0.1.0/24\"]</p>}<p># NSG with rules for SSH and HTTP only</p>resource \"azurerm_network_security_group\" \"nsg\" {<p>  name                = \"${var.resource_group_name}-nsg\"</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>}<p>resource \"azurerm_network_security_rule\" \"ssh\" {</p>  name                        = \"Allow-SSH\"<p>  priority                    = 1001</p>  direction                   = \"Inbound\"<p>  access                      = \"Allow\"</p>  protocol                    = \"Tcp\"<p>  source_port_range           = \"*\"</p>  destination_port_range      = \"22\"<p>  source_address_prefix       = \"*\"</p>  destination_address_prefix  = \"*\"<p>  resource_group_name         = azurerm_resource_group.rg.name</p>  network_security_group_name = azurerm_network_security_group.nsg.name<p>resource \"azurerm_network_security_rule\" \"http\" {</p>  name                        = \"Allow-HTTP\"<p>  priority                    = 1010</p>  direction                   = \"Inbound\"<p>  access                      = \"Allow\"</p>  protocol                    = \"Tcp\"<p>  source_port_range           = \"*\"</p>  destination_port_range      = \"80\"<p>  source_address_prefix       = \"*\"</p>  destination_address_prefix  = \"*\"<p>  resource_group_name         = azurerm_resource_group.rg.name</p>  network_security_group_name = azurerm_network_security_group.nsg.nameresource \"azurerm_public_ip\" \"pip\" {<p>  name                = \"${var.resource_group_name}-pip\"</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>  allocation_method   = \"Static\"<p>  #sku                 = \"Basic\"</p>}resource \"azurerm_network_interface\" \"nic\" {<p>  name                = \"${var.resource_group_name}-nic\"</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>    name                          = \"ipconfig1\"<p>    subnet_id                     = azurerm_subnet.subnet.id</p>    private_ip_address_allocation = \"Dynamic\"<p>    public_ip_address_id          = azurerm_public_ip.pip.id</p>  }<p>resource \"azurerm_network_interface_security_group_association\" \"nsg_association\" {</p>  network_interface_id       = azurerm_network_interface.nic.id<p>  network_security_group_id  = azurerm_network_security_group.nsg.id</p>}<p># Ubuntu VM (azurerm_linux_virtual_machine)</p>resource \"azurerm_linux_virtual_machine\" \"vm\" {<p>  name                = \"${var.resource_group_name}-vm\"</p>  resource_group_name = azurerm_resource_group.rg.name<p>  location            = azurerm_resource_group.rg.location</p>  size                = var.vm_size<p>  admin_username      = var.admin_username</p>  network_interface_ids = [azurerm_network_interface.nic.id]<p>  disable_password_authentication = true</p>    username   = var.admin_username<p>    public_key = file(var.ssh_public_key)</p>  }    caching              = \"ReadWrite\"<p>    storage_account_type = \"Standard_LRS\"</p>  }    publisher = \"Canonical\"<p>    offer     = \"0001-com-ubuntu-server-jammy\"</p>    sku       = \"22_04-lts\"  }    project = \"mini-finance\"}</pre><p>2. Define variables in variables.tf</p><pre>variable \"resource_group_name\" {  description = \"Name of the Azure Resource Group\"}  description = \"Azure region for resources\"}<p>variable \"admin_username\" {</p>  description = \"Admin username for Linux VMs\"}<p>variable \"ssh_public_key\" {</p>  description = \"Path to your SSH public key\"}  description = \"VM size\"}</pre><p>3. Define the output in outputs.tf</p><pre># Outputsoutput \"public_ip\" {<p>  value = azurerm_public_ip.pip.ip_address</p>}  value = var.admin_username  description = \"SSH commands to access each VM\"<p>  value = \"ssh ${var.admin_username}@${azurerm_public_ip.pip.ip_address}\"</p>}</pre><p>4. Define Terraform variables in terraform.tfvars</p><pre>location = \"eastus\"resource_group_name = \"rg-mini-finance\"<p>admin_username = \"azureuser\"</p>vm_size = \"Standard_B1s\"<p>ssh_public_key = \"~/.ssh/id_ed25519.pub\"</p></pre><p>5. Provision the Azure Infrastructure by running Terraform commands:</p><pre>terraform initterraform plan<p>terraform apply -auto-approve</p></pre><h3>Step 3: Configure Ansible Inventory</h3><ul><li>Paste the following code into the inventory.ini</li></ul><pre>[web]20.169.254.159 # Use the actual Public IP of your serveransible_user=azureuser<p>ansible_ssh_private_key_file=~/.ssh/id_ed25519</p></pre><ul><li>Test connection to the&nbsp;server</li></ul><p>- Change directory to the Ansible&nbsp;folder</p><p>- Run the following command to ping the&nbsp;server</p><pre>ansible all -i inventory -m ping</pre><h3>Step 4: Prepare the Ansible multi-Playbook</h3><p>Open the site.yml and add the different playbooks</p><ul><li>Install and configure nginx&nbsp;playbook</li></ul><pre>---- name: Install and configure Nginx  become: yes    nginx_conf: /etc/nginx/sites-available/default    - name: Update apt cache        update_cache: yes<p>    - name: Install required packages</p>      apt:          - nginx        state: present<p>    - name: Ensure Nginx is enabled and running</p>      service:        state: started</pre><ul><li>Installs required packages.</li><li>Ensures Nginx is enabled and&nbsp;running.</li></ul><p>Deploy the static mini finance website&nbsp;playbook</p><pre>- name: Deploy static mini finance website  hosts: web  vars:<p>    site_repo: \"https://github.com/vincegwu/mini-finance-app.git\"</p>    tmp_dir: \"/home/azureuser/mini-finance-app\"<p>    nginx_root: \"/var/www/html\"</p>    - name: Remove default Nginx index        path: \"{{ nginx_root }}/index.nginx-debian.html\"      ignore_errors: yes<p>    - name: Install Node.js and npm</p>      apt:          - nodejs        state: present<p>    - name: Clone static site into temporary directory (user-owned)</p>      git:        dest: \"{{ tmp_dir }}\"<p>    - name: Install npm dependencies</p>      npm:        production: no<p>    - name: Build the static app</p>      command: npm run build        chdir: \"{{ tmp_dir }}\"<p>    - name: Deploy built app to Nginx root</p>      copy:<p>        src: \"{{ tmp_dir }}/build/\"</p>        dest: \"{{ nginx_root }}/\"        group: www-data        remote_src: yes<p>    - name: Ensure proper ownership for Nginx root</p>      file:        owner: www-data        recurse: yes      service:        state: reloaded</pre><ul><li>Removes default nginx&nbsp;index</li><li>Clones static site into temporary directory</li><li>Installs npm dependencies</li><li>Deploys built app to Nginx&nbsp;root</li><li>Ensures proper ownership for Nginx&nbsp;root</li></ul><p>Verify Nginx and Deployment Playbook</p><pre>- name: Verify Nginx and deployment  hosts: web    - name: Check Nginx service status<p>    - name: Display Nginx status</p>      debug:<p>        msg: \"Nginx is {{ 'running' if services['nginx'].state == 'running' else 'not running' }}\"</p><p>    - name: Check homepage availability</p>      uri:        return_content: yes<p>    - name: Show homepage preview</p>      debug:<p>        msg: \"{{ homepage.content | truncate(200) }}\"</p></pre><ul><li>Checks the Nginx service&nbsp;status</li><li>Checks Homepage availability</li></ul><p>Run the automation from the control&nbsp;node</p><pre>ansible-playbook -i inventory.ini site.yml</pre><p>Visit http://&lt;VM Public_ip&gt; — the <strong>Mini-Finance App home page</strong> should render&nbsp;fully.</p><p>Pull down the infrastructure once done using Terraform</p><pre>cd terraformterraform destroy --auto-approve</pre><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=88bbe020418c\" width=\"1\" height=\"1\" alt=\"\">","contentLength":8375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We Lost Events in Production — Then I Discovered Kafka Transactions","url":"https://blog.devops.dev/we-lost-events-in-production-then-i-discovered-kafka-transactions-db4851f12684?source=rss----33f8b2d9a328---4","date":1761681082,"author":"Gaddam.Naveen","guid":319401,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Minimal Feast Tutorial: Turning the California Housing Dataset into Features","url":"https://blog.devops.dev/a-minimal-feast-tutorial-turning-the-california-housing-dataset-into-features-dbda10a0507b?source=rss----33f8b2d9a328---4","date":1761681080,"author":"Okan Yenigün","guid":319400,"unread":true,"content":"<h4>From Raw Data to Real-Time ML: A Hands-On Guide with&nbsp;Feast</h4><p>Feast (short for Feature Store) is an open-source feature store for machine learning, originally developed by Google Cloud and&nbsp;GO-JEK.</p><p>It acts as the bridge between models and&nbsp;data.</p><p>In this post, we’ll briefly explore what Feast is, and later, we’ll use it to build projects.</p><p><em>Before reading this post, you may want to check out my previous blog entry, where I introduced the concept of feature&nbsp;stores:</em></p><p>To ground the concepts of a feature store without getting lost in infrastructure, let’s build a tiny, local Feast project around the California housing&nbsp;dataset.</p><p>First, install the&nbsp;library:</p><p>Let’s pull in the California housing dataset from scikit-learn and take a quick inventory of its shape and&nbsp;columns.</p><pre>from sklearn.datasets import fetch_california_housing<p>df = fetch_california_housing(as_frame=True).frame</p><p>print(f\"Shape of the dataset: {df.shape}\")</p>print(f\"Columns in the dataset: {df.columns.tolist()}\")Shape of the dataset: (20640, 9)<p>Columns in the dataset: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'MedHouseVal']</p>\"\"\"</pre><p>Before we can register features in a Feast feature store, each record must be associated with a . Feast relies on these event timestamps to determine when a feature value was observed — crucial for maintaining point-in-time correctness during both training and&nbsp;serving.</p><p>To prepare our dataset for that, let’s separate predictors and targets, then attach synthetic timestamps to each&nbsp;record.</p><pre>import pandas as pd<p>df_predictors = df.drop(columns=[\"MedHouseVal\"])</p>df_target = df[\"MedHouseVal\"]<p>timestamps = pd.date_range(</p>    end=pd.Timestamp.now(),     freq=\"D\"<p>).to_frame(name=\"event_timestamp\", index=False)</p><p>df_predictors = pd.concat([timestamps, df_predictors], axis=1)</p>df_target = pd.concat([timestamps, df_target], axis=1)</pre><p>To make our dataset fully compatible with Feast, we need an , a column that uniquely identifies each record (or entity) in the feature&nbsp;store.</p><p>In a production setting, this could be something meaningful like a user_id, customer_id, or property_id. Since our California housing dataset doesn’t include a natural unique identifier, we can generate one ourselves using the DataFrame index.</p><pre>df_predictors.reset_index(drop=False, inplace=True)df_predictors.rename(columns={\"index\": \"id\"}, inplace=True)<p>df_target.reset_index(drop=False, inplace=True)</p>df_target.rename(columns={\"index\": \"id\"}, inplace=True)</pre><p>At this stage, we’re setting up the Feast feature repository, which will serve as the central configuration and management hub for all our features, entities, and data&nbsp;sources.</p><p>This command creates a brand-new Feast project directory structure — essentially scaffolding the environment where you’ll define your feature store’s schema and operational logic.</p><p>Feast automatically generates a folder (here named feature_repo) that contains the core components of a feature repository.</p><ol><li>feature_store.yaml: This is the global configuration file that tells Feast where to find your data and how to&nbsp;operate.</li><li>example_repo.py: A starter example containing a sample Entity, FeatureView, and DataSource.</li><li>data/ folder: A placeholder directory where you can store or point to your offline feature data (e.g., Parquet or CSV&nbsp;files).</li></ol><p>Feast’s design philosophy separates data engineering (how features are created and stored) from modeling (how features are consumed). The feature repository is where you define that boundary.</p><p>Now that our Feast repository has been initialized, the next step is to provide it with offline data sources — datasets that define the historical values of our features.</p><pre>df_predictors.to_parquet(\"./feature_repo/feature_repo/data/predictors.parquet\")df_target.to_parquet(\"./feature_repo/feature_repo/data/target.parquet\")</pre><p>Now that our data is organized and stored in Parquet format, the next step is defining feature metadata — the schema, sources, and entities that Feast uses to understand and serve our features.</p><p>Remove the code inside example_repo.py, and&nbsp;then:</p><pre># feature_repo/example_repo.py    Project,    ValueType,    Field,)<p>from feast.types import Float64, Int64</p>from datetime import timedelta<p>project = Project(name=\"feature_repo\",</p>                  description=\"A project for house prices prediction\")<p>house = Entity(name=\"id\", value_type=ValueType.INT64,</p>               description=\"house id\")<p>predictors_fv = FeatureView(</p>    name=\"predictors\",<p>    ttl=timedelta(seconds=3600 * 1),</p>    entities=[house],        Field(name=\"MedInc\", dtype=Float64),<p>        Field(name=\"HouseAge\", dtype=Float64),</p>        Field(name=\"AveRooms\", dtype=Float64),<p>        Field(name=\"AveBedrms\", dtype=Float64),</p>        Field(name=\"Population\", dtype=Int64),<p>        Field(name=\"AveOccup\", dtype=Float64),</p>        Field(name=\"Latitude\", dtype=Float64),<p>        Field(name=\"Longitude\", dtype=Float64),</p>    ],<p>    source=FileSource(path=r\"data/predictors.parquet\",</p>                      timestamp_field=\"event_timestamp\"),    tags={\"team\": \"house_price_prediction\"},    name=\"target\",<p>    ttl=timedelta(seconds=3600 * 1),</p>    entities=[house],        Field(name=\"MedHouseVal\", dtype=Float64),    source=FileSource(path=r\"data/target.parquet\",<p>                      timestamp_field=\"event_timestamp\"),</p>    online=True,<p>    tags={\"team\": \"house_price_prediction\"},</p>)</pre><p>First, we declared a Feast project — a logical grouping of entities, features, and&nbsp;sources.</p><pre>house = Entity(name=\"id\", value_type=ValueType.INT64, description=\"house id\")</pre><p>An Entity represents the unique key around which features are organized. In our case, each house record has a unique id (we created it earlier using reset_index()). Feast uses this to join feature tables during offline training and online&nbsp;serving.</p><p>A Feature View tells&nbsp;Feast:</p><ul><li>What data to treat as&nbsp;features</li><li>How to link it to&nbsp;entities</li><li>How long those features remain fresh&nbsp;TTL</li></ul><pre>predictors_fv = FeatureView(    name=\"predictors\",<p>    ttl=timedelta(seconds=3600 * 1),</p>    entities=[house],        Field(name=\"MedInc\", dtype=Float64),<p>        Field(name=\"HouseAge\", dtype=Float64),</p>        ...    source=FileSource(...),    tags={\"team\": \"house_price_prediction\"},</pre><ul><li>schema: explicitly lists each feature and its data&nbsp;type.</li><li>ttl: sets how long a feature remains&nbsp;valid.</li><li>online=True: ensures this view can be materialized to an online store for low-latency inference.</li><li>tags: metadata for discovery.</li></ul><p>Each FeatureView points to a FileSource, which specifies where Feast should read offline data&nbsp;from.</p><p>Similarly, we define a smaller view for the&nbsp;target.</p><p>After saving the file, apply the definitions to register them in&nbsp;Feast:</p><p>This command takes Python definitions — the Entity, FeatureView, and FileSource objects we declared in example_repo.py — and materializes them into a structured, version-controlled registry.</p><pre>Applying changes for project feature_repo Created project feature_repo Created feature view predictors <p>Created feature view target </p>Created sqlite table feature_repo_predictors <p>Created sqlite table feature_repo_target</p></pre><p>The message we saw means Feast&nbsp;has:</p><ul><li>Created a new project feature_repo. A logical namespace where all our entities, features, and sources&nbsp;live.</li><li>Registered the entity id. This entity now acts as the unique key that binds our predictors and targets together across data&nbsp;sources.</li><li>Created two FeatureViews, predictors and target. Each view has been added to Feast’s internal registry (stored under registry.db).</li><li>Initialized an SQLite online store. These tables will serve as our online store — a lightweight, fast-access database for serving features during real-time predictions. Since our feature_store.yaml specifies SQLite as the default online store, Feast automatically creates the necessary tables.</li></ul><p>Now, we’ve reached the practical stage in our Feast workflow: generating and saving a training dataset from our feature&nbsp;store.</p><p>Up until now, we’ve defined features, entities, and sources and registered them in the Feast registry.</p><pre>from feast import FeatureStorefrom feast.infra.offline_stores.file_source import SavedDatasetFileStorage<p>store = FeatureStore(repo_path=\"./feature_repo/feature_repo\")</p><p>entity_df = pd.read_parquet(path=\"./feature_repo/feature_repo/data/target.parquet\")</p><p>training_data = store.get_historical_features(</p>    entity_df=entity_df,        \"predictors:MedInc\",        \"predictors:AveRooms\",        \"predictors:Population\",        \"predictors:Latitude\",    ]<p>dataset = store.create_saved_dataset(</p>    from_=training_data,<p>    name=\"house_price_prediction_dataset\",</p>    storage=SavedDatasetFileStorage(path=\"./data/house_price_prediction_dataset.parquet\"),</pre><p>First, we initialize the FeatureStore object, which gives us access to both offline (historical data) and online (real-time) retrieval capabilities.</p><p>Feast needs an entity_df which acts as a query template that tells&nbsp;it:</p><ul><li>Which entities (via the id column) we want features&nbsp;for.</li><li>When (via the event_timestamp column) those features should be&nbsp;valid.</li></ul><p>Each row provides context for point-in-time feature&nbsp;lookup.</p><p>Feast will automatically ensure that no future data leakage occurs — it only pulls feature values that existed before the timestamp in each&nbsp;row.</p><pre>training_data = store.get_historical_features(    entity_df=entity_df,<p>    features=[ ... list of predictor features ... ]</p>)</pre><p>Our target.parquet includes entity key and timestamp, but also the target value. Fest doesn’t care that entity_df has extra columns. All it needs&nbsp;are:</p><ul><li>event_timestamp (the event&nbsp;time)</li></ul><p>So, using the target DataFrame as entity_df is totally valid — it’s a convenient shortcut.</p><p>What happens behind the&nbsp;scenes:</p><ul><li>Feast looks up the entity IDs id in the entity_df.</li><li>It finds all matching feature records in the predictors FeatureView.</li><li>It ensures each feature value’s event_timestamp is before the entity’s timestamp.</li><li>It returns a merged dataset ready for model training.</li></ul><p>The result training_data is a Feast retrieval job object, which you can turn into a DataFrame by calling&nbsp;.to_df().</p><p>Then, we convert the retrieval job result into a reusable saved dataset managed by&nbsp;Feast.</p><pre>dataset = store.create_saved_dataset(    from_=training_data,<p>    name=\"house_price_prediction_dataset\",</p>    storage=SavedDatasetFileStorage(path=\"./data/house_price_prediction_dataset.parquet\"),</pre><p>This stores it at the specified Parquet path. It also registers metadata about the dataset in the Feast registry.</p><p>Now, let’s train the&nbsp;model.</p><p>First, we will reopen the Feast project by pointing to our&nbsp;repo.</p><pre>store = FeatureStore(repo_path=\"./feature_repo/feature_repo\")<p>training_df = store.get_saved_dataset(\"house_price_prediction_dataset\").to_df()</p></pre><p>That dataset already contains:</p><ul></ul><p>We split the predictors and the target, and split again for evaluation.</p><pre>y = training_df[\"MedHouseVal\"]X = training_df.drop(columns=[\"MedHouseVal\", \"event_timestamp\", \"id\"])</pre><pre>from sklearn.model_selection import train_test_split<p>X_train, X_test, y_train, y_test = train_test_split(</p>    X, y, test_size=0.2, random_state=42</pre><pre>from sklearn.ensemble import RandomForestRegressor<p>model = RandomForestRegressor()</p>model.fit(X_train, y_train)<p>dump(model, \"./feature_repo/feature_repo/data/model.joblib\")</p></pre><p>Lastly, we serialized our model using joblib and saved&nbsp;it.</p><p>Let’s move from “features on disk” to “features ready for real-time use.” To serve features at low latency, we’ll materialize our offline data (the Parquet files) into Feast’s online store (SQLite in this project).</p><p>We’ll define a time window so Feast only loads rows whose event_timestamp falls within that&nbsp;range.</p><pre>from datetime import datetime<p>store = FeatureStore(repo_path=\"./feature_repo/feature_repo\")</p><p>store.materialize(start_date=datetime(2010, 1, 1), end_date=datetime.now())</p># Materializing 2 feature views from 2010-01-01 00:00:00+00:00 to 2025-10-21 19:57:54+00:00 into the sqlite online store.# predictors:</pre><p> is actually copying for&nbsp;serving.</p><p>Feast scans our sources (our two Parquet files), picks all rows whose event_timestamp is between start_date and end_date, and upserts them into the online store. After this, those features are available for low-latency lookup by&nbsp;id.</p><p>To wrap the workflow with real-time inference, let’s fetch the latest features for a specific house id from Feast’s online store and pass that feature vector to our trained&nbsp;model.</p><pre>feast_features = [    \"predictors:MedInc\",    \"predictors:AveRooms\",    \"predictors:Population\",    \"predictors:Latitude\",]<p>df_features = store.get_online_features(</p>    features=feast_features,<p>    entity_rows=[{\"id\": 20637}],</p>).to_df()<p>#       id  Population  MedInc  AveOccup  AveRooms  Longitude  Latitude  \\</p># 0  20637        1007     1.7  2.325635  5.205543    -121.22     39.43#    HouseAge  AveBedrms</pre><ul><li>entity_rows=[{\"id\": 20637}] asks Feast for the latest materialized features for that single&nbsp;entity.</li><li>.to_df() returns a tidy, single-row DataFrame that includes the id column plus all requested features.</li></ul><pre>from joblib import load<p>model = load(\"./feature_repo/feature_repo/data/model.joblib\")</p><p>X = df_features.reindex(columns=model.feature_names_in_)</p>print(preds)</pre><p>model.predict produces one scalar—the estimated median house value—for id=20637.</p><p>We built a complete mini feature store using Feast to manage features for a house price prediction model — from raw data to real-time predictions.</p><p>Starting with the California housing dataset, we split predictors and targets, added synthetic timestamps, and an id entity, and saved them as Parquet files. After initializing a Feast repo, we defined an Entity id and two Feature Views predictors and target, then applied them to register our features.</p><p>Using Feast’s offline store, we created a historical training dataset with point-in-time correctness, trained a Random Forest Regressor, and saved the&nbsp;model.</p><p>We then materialized the features into the online store and retrieved them by entity ID for real-time inference.</p><p>The key learnings: every record needs an entity and timestamp, Feast guarantees no data leakage, and the same feature definitions power both training and serving — ensuring consistent, reproducible ML workflows from offline to&nbsp;online.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dbda10a0507b\" width=\"1\" height=\"1\" alt=\"\">","contentLength":14055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VoxScribe: A platform to test Opensource Speech-to-Text models","url":"https://blog.devops.dev/voxscribe-a-platform-to-test-opensource-speech-to-text-models-70474a05c513?source=rss----33f8b2d9a328---4","date":1761681077,"author":"Fraser sequeira","guid":319399,"unread":true,"content":"<p><em>The views and opinions expressed in this blog post are my own and do not reflect the official position or views of Amazon Web Services&nbsp;(AWS)</em></p><p>As a Solutions Architect at AWS, I’ve worked with customers across healthcare, contact centers, and enterprise applications who all share a common challenge:- speech-to-text(STT) at scale is expensive. Whether it’s HealthScribe for medical documentation, analyzing millions of call center recordings, or building voice-enabled applications, proprietary STT solutions quickly become a significant line item in the&nbsp;budget.</p><p>I’ve watched customers hit cost ceilings with transcription services, because at scale, the per-minute pricing becomes prohibitive. A contact center processing 100,000 hours of calls monthly can easily spend $150,000+ on transcription alone.</p><p>The open-source STT landscape has matured significantly. Models like <a href=\"https://openai.com/index/whisper/\">Whisper</a>, <a href=\"https://mistral.ai/news/voxtral\">Voxtral</a>, <a href=\"https://developer.nvidia.com/blog/nvidia-speech-ai-models-deliver-industry-leading-accuracy-and-performance/\">Parakeet</a>, and <a href=\"https://huggingface.co/nvidia/canary-qwen-2.5b\">Canary-Qwen</a> now rival or exceed proprietary solutions in accuracy. But here’s the problem I kept seeing: customers wanted to evaluate these models for their specific use cases, but each engine has different dependencies, APIs, and setup requirements. Comparing models meant building custom infrastructure, managing version conflicts, and writing integration code, a weeks-long project before you even see&nbsp;results.</p><p>So here is a lightweight platform <a href=\"https://github.com/Fraser27/VoxScribe\"></a>😊 a platform to test out opensource STT models. It’s a FastAPI backend with a lightweight HTML/JS frontend that lets you test multiple open-source STT models through a single interface. Upload your audio, select models, and compare transcriptions side-by-side. The platform handles dependency conflicts (yes, even the transformers version nightmare between Voxtral and NeMo models), manages model caching, and provides a clean API for integration.</p><h3>The Problem: Fragmented STT Ecosystem</h3><p>If you’ve ever worked with multiple speech recognition models, you know the&nbsp;pain:</p><ul><li>: Different models require conflicting library versions. Mistral’s Voxtral works great with the latest transformers, but then you try to add Parakeet which depends on NeMo that relies of a lower transformers model. Or even the latest Canary-Qwen-2.5B for which you need to directly build NeMo-toolkit from their GIT&nbsp;repo.</li><li>: Each model has its own interface, making it difficult to compare results or switch between&nbsp;engines.</li><li>: Installing CUDA drivers, managing Python environments, and debugging version conflicts can take hours or days. I have combined notes from multiple sources to make this process easier😏.</li><li><strong>Limited Comparison Tools*</strong>: Evaluating which model works best for your use case means building custom testing infrastructure.</li></ul><p>This OpenSource STT platform solves these challenges by managing version conflicts, providing you a playground to test out &gt;5 STT models. Let get straight into deploying this solution. As they say the only way to learn something is to do it yourself. No worries if you aren’t a CUDA freak we’ve got you&nbsp;covered</p><ol><li>We started out with a G6.xlarge EC2 on AWS. This instance comprises of 4 VCPUs/16GB RAM and a single L4 Tensor core GPU with 16GB GPU memory. The latest pricing details for this instance can be found <a href=\"https://instances.vantage.sh/aws/ec2/g6.xlarge?currency=USD&amp;region=us-east-1\">here</a>. Select sufficient GP3 storage, in our case we went ahead with 100 GB GP3&nbsp;storage.</li></ol><p>2. This instance should be launched in a public subnet with a security groups that allows inbound access on port 8000 and SSH access on port&nbsp;22.</p><p>3. Once you’ve launched your instance and its in running state, lets SSH into the system using the below command or you could use <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-methods.html\">EC2 Instance&nbsp;Connect</a>.</p><ul><li>The SSH approach: SSH from your local machine using the PEM key used while launching the EC2 instance.</li></ul><pre>ssh -i &lt;your-pem-file&gt; -o \"StrictHostKeyChecking=no\" ec2-user@&lt;your-ec2-public-ip&gt;</pre><ul><li>The EC2 Instance Connect approach: You could also login via the AWS console. However for the remainder of this blog we shall go ahead with the SSH approach as I’ve found the SSH approach to be more stable especially when our platform downloads larger models from the&nbsp;hub.</li></ul><h3>NVIDIA GRID DRIVERS Installation</h3><p>4. Once within the EC2, we need to install the GRID drivers that gives us access to CUDA. Here are the installation steps the details of which can be found in this <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html#nvidia-GRID-driver\">AWS documentation</a> (Head to Option3).</p><pre>sudo dnf update -ysudo dnf install gcc make</pre><p>5. Once you hit reboot you will lose connectivity to your instance. Hence you would again need to SSH into the EC2 machine once its rebooted (around 30 seconds&nbsp;later).</p><p>6. Lets continue with the GRID installation process</p><pre>sudo dnf install -y kernel-devel kernel-modules-extraaws s3 cp --recursive s3://ec2-linux-nvidia-drivers/latest/ .<p>chmod +x NVIDIA-Linux-x86_64*.run</p>sudo /bin/sh ./NVIDIA-Linux-x86_64*.run</pre><p>7. Verify whether the driver is functional. Here you should see the CUDA version. We are on version&nbsp;13.</p><p>8. Conda is an open-source package and environment manager for Python and other software that helps users create isolated environments with different package versions to avoid conflicts. It comes pre-compiled with binaries such as <a href=\"https://pypi.org/project/cdifflib/\">cdifflib</a> required for <a href=\"https://github.com/NVIDIA-NeMo/NeMo\">Nemo</a> on which Parakeet-v2 and Canary-Qwen-2.5B rely. It just makes dependency management way easier compared to&nbsp;PIP.</p><p>9. Lets install CONDA through the following commands</p><pre>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.sh</pre><ul><li>Accept the license agreement (type&nbsp;`yes`)</li><li>Confirm installation location (default is&nbsp;fine)</li><li>Initialize Conda (type `yes` when prompted)</li></ul><p>10. Restart your shell(SSH Terminal, just as in Step 3) our source&nbsp;bashrc.</p><p>11. You should now be able to verify the conda installation by firing the below&nbsp;command</p><p>12. Now lets create a conda environment with python3.12. Conda will automatically install python3.12 for us. Lets also activate the environment</p><pre>conda create -n stthub python=3.12conda activate stthub</pre><p>13. Lets install ffmpeg in our conda environment. ffmpeg is implicitly used by STT libraries to convert our audio files into compatible audio formats for the STT engine. It can also efficiently extract the audio track from a video file, isolating it for STT processing.</p><h3>GIT CLONE the STT&nbsp;Platform</h3><p>15. git clone into our VoxScribe project</p><pre>git clone https://github.com/Fraser27/VoxScribe.gitcd VoxScribe</pre><p>16. Lets install the project dependencies</p><pre>pip install -r requirements.txt</pre><p>17. Now lets start the VoxScribe app</p><p>18. The app is now running on port 8000 and can be accessed from your public&nbsp;IP</p><pre>http://&lt;your-public-ip&gt;:8000</pre><p>The platform allows you to compare various STT models. The comparisons happen sequentially on the GPU-based instance.</p><ul><li>Solves our number one problem of cost. This self-hosted solution offers a handle on your costs which is crucial as your STT requirements scale.</li><li>You can test multiple models in minutes instead of&nbsp;days.</li><li>Make data-driven choices about which model to use in production.</li><li>One codebase to update, one set of dependencies to&nbsp;manage.</li><li>New models can be added without disrupting existing functionality.</li></ul><p>This is just the beginning. Future enhancements could&nbsp;include:</p><ul><li>Chunking large audio&nbsp;files</li><li>Real-time streaming transcription for live&nbsp;audio</li><li>Speaker diarization to identify different voices</li><li>Language detection and automatic model selection</li><li>Integration with cloud storage (S3, GCS) for seamless workflows</li></ul><p>The <a href=\"https://github.com/Fraser27/VoxScribe\">platform</a> is open source and ready to test. If you’re working with speech recognition models and want a cleaner way to evaluate them, give it a spin. I’m actively working on it and would value your feedback — especially around edge cases, performance bottlenecks, or additional models you’d like to see supported.</p><ul><li><strong>Interested in testing it?</strong> Clone the repo, follow the setup instructions, and let me know what breaks or what could be better. PRs and issues&nbsp;welcome.</li><li>Looking for beta testers and contributors. If you have audio samples that challenge STT models or ideas for improving the comparison workflow, I’d love to hear from&nbsp;you.*</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=70474a05c513\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7914,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complete Guide: GitHub, Git, and Jenkins Integration on Windows","url":"https://blog.devops.dev/complete-guide-github-git-and-jenkins-integration-on-windows-8822a83d4165?source=rss----33f8b2d9a328---4","date":1761681071,"author":"Aravindcsebe","guid":319398,"unread":true,"content":"<p>This comprehensive guide will walk you through setting up a complete CI/CD pipeline on Windows, from creating your first GitHub repository to automating builds with Jenkins. By the end of this tutorial, you’ll have a fully functional setup where pushing code to GitHub automatically triggers builds in&nbsp;Jenkins.</p><h3>Part 1: Installing Git on&nbsp;Windows</h3><ol><li>The download should start automatically. If not, click on the appropriate version (64-bit or&nbsp;32-bit)</li><li>Save the installer file to your&nbsp;computer</li></ol><ol><li>Run the downloaded&nbsp;.exe&nbsp;file</li><li>Click  through the license agreement</li><li>Choose the installation location (default is usually fine: C:\\Program Files\\Git)</li><li>Select components — keep the defaults selected:</li></ol><ul><li>Windows Explorer integration</li></ul><ol><li>Choose the default editor (select your preferred editor or leave as&nbsp;Vim)</li><li>Adjust your PATH environment — select <strong>“Git from the command line and also from 3rd-party software”</strong></li><li>Choose HTTPS transport backend — select <strong>“Use the OpenSSL&nbsp;library”</strong></li><li>Configure line ending conversions — select <strong>“Checkout Windows-style, commit Unix-style line&nbsp;endings”</strong></li><li>Configure terminal emulator — select </li><li>Choose default behavior of git pull - select <strong>\"Default (fast-forward or&nbsp;merge)\"</strong></li><li>Choose credential helper — select </li><li>Enable file system caching — check this&nbsp;option</li></ol><h3>Step 3: Verify Installation</h3><p>Open Command Prompt or PowerShell and&nbsp;type:</p><p>You should see output like: git version 2.x.x.windows.x</p><p>Set up your identity (this will be associated with your commits):</p><pre>git config --global user.name \"Your Name\"git config --global user.email \"your.email@example.com\"</pre><p>Verify your configuration:</p><h3>Part 2: Creating a GitHub&nbsp;Account</h3><ol><li>Click on  in the top right&nbsp;corner</li><li>Enter your email address and click&nbsp;</li><li>Create a strong password and click&nbsp;</li><li>Enter a username (this will be your GitHub handle) and click&nbsp;</li><li>Choose whether you want to receive updates and click&nbsp;</li><li>Solve the verification puzzle</li><li>Check your email for the verification code</li><li>Enter the 6-digit code sent to your&nbsp;email</li></ol><h3>Step 2: Personalize Your&nbsp;Account</h3><ol><li>Answer the questions about your experience (or&nbsp;skip)</li><li>Complete your profile by adding a profile picture and bio (optional)</li></ol><h3>Part 3: Creating Your First Repository</h3><h3>Step 1: Create a New Repository</h3><ol><li>Log in to your GitHub&nbsp;account</li><li>Click the  icon in the top right&nbsp;corner</li><li>Fill in the repository details:</li></ol><ul><li>: my-jenkins-project</li><li>: “Testing GitHub and Jenkins integration”</li><li>: Select  (this is important for Jenkins integration without authentication)</li><li> initialize with README,&nbsp;.gitignore, or license (we’ll add files&nbsp;later)</li></ul><h3>Step 2: Note Your Repository URL</h3><p>After creation, you’ll see a page with setup instructions. Copy the repository URL, which looks&nbsp;like:</p><pre>https://github.com/YOUR-USERNAME/my-jenkins-project.git</pre><h3>Part 4: Creating a&nbsp;Branch</h3><h3>Step 1: Clone the Repository Locally</h3><p>Open Command Prompt or PowerShell and navigate to where you want to store your&nbsp;project:</p><pre>cd C:\\Users\\YourUsername\\Documentsgit clone https://github.com/YOUR-USERNAME/my-jenkins-project.git</pre><h3>Step 2: Create a New&nbsp;Branch</h3><h3>Step 3: Switch to the New&nbsp;Branch</h3><p>Or create and switch in one&nbsp;command:</p><pre>git checkout -b development</pre><h3>Step 4: Verify Your Current&nbsp;Branch</h3><p>The active branch will be marked with an asterisk&nbsp;(*).</p><h3>Step 5: Push the Branch to&nbsp;GitHub</h3><pre>git push -u origin development</pre><p>This creates the branch on GitHub and sets up tracking.</p><h3>Part 5: Creating a Shell Script (.sh&nbsp;File)</h3><h3>Step 1: Create the Script&nbsp;File</h3><p>In your project directory, create a new file called build.sh:</p><p>Open build.sh with any text editor (Notepad, VS Code, etc.) and add the following content:</p><pre>echo \"==================================\"echo \"Starting Build Process\"<p>echo \"==================================\"</p>echo \"Build Date: $(date)\"<p>echo \"Build Number: ${BUILD_NUMBER:-Manual}\"</p>echo \"==================================\"<p>echo \"Hello from the build script!\"</p>echo \"This is version 1.0\"<p>echo \"==================================\"</p>echo \"Build completed successfully!\"<p>echo \"==================================\"</p></pre><p>: Even though Windows uses PowerShell/CMD, the&nbsp;.sh file will be executed by Jenkins, which can run bash&nbsp;scripts.</p><p>Save and close the&nbsp;editor.</p><h3>Part 6: Git Operations — Add, Commit, and&nbsp;Push</h3><p>See what files have&nbsp;changed:</p><p>You should see build.sh as an untracked file.</p><h3>Step 2: Add Files to&nbsp;Staging</h3><pre>git commit -m \"Add initial build script\"</pre><p>The -m flag adds a commit message describing your&nbsp;changes.</p><p>Push your changes to the remote repository:</p><pre>git push origin development</pre><p>This pushes the development branch to&nbsp;GitHub.</p><p>Go to your GitHub repository in a browser and switch to the development branch using the branch dropdown. You should see your build.sh&nbsp;file.</p><h3>Part 7: Git Pull Operations</h3><h3>Pulling Changes from&nbsp;Remote</h3><p>When others make changes or you make changes from another location, you’ll need to pull&nbsp;updates:</p><pre>git pull origin development</pre><p>This fetches and merges changes from the remote development branch.</p><p>Before making changes, always pull&nbsp;first:</p><pre>git pull origin development# Make your changesgit commit -m \"Your commit message\"<p>git push origin development</p></pre><h3>Part 8: Installing and Setting Up Jenkins on&nbsp;Windows</h3><h4>Using the Windows Installer:</h4><ol><li>Run the downloaded&nbsp;.msi&nbsp;file</li><li>Follow the installation wizard</li><li>Jenkins will install as a Windows&nbsp;service</li></ol><ol><li>The installer will show you a path to the initial admin password, typically:(C:\\Program Files\\Jenkins\\secrets\\initialAdminPassword)</li><li>Open this file with&nbsp;Notepad</li><li>Copy the password and paste it into the web interface</li></ol><ol><li>Choose <strong>Install suggested plugins</strong></li><li>Wait for the installation to complete (this may take several&nbsp;minutes)</li></ol><h3>Step 6: Create First Admin&nbsp;User</h3><ol><li>Fill in the form with your&nbsp;details:</li></ol><ul></ul><h3>Step 7: Configure Instance</h3><ol><li>Keep the default Jenkins URL (http://localhost:8080)</li></ol><h3>Part 9: Installing Required Jenkins&nbsp;Plugins</h3><h3>Step 1: Access Plugin&nbsp;Manager</h3><ol><li>From the Jenkins dashboard, click </li><li>Click the &nbsp;tab</li></ol><h3>Step 2: Install Git&nbsp;Plugin</h3><ol><li>In the search box, type </li><li>Check the box next to  (it might already be installed)</li><li>Also search for and&nbsp;install:</li></ol><ul></ul><ol><li>Click  (or <strong>Download now and install after&nbsp;restart</strong>)</li><li>Wait for installation to&nbsp;complete</li></ol><h3>Part 10: Configuring Git in&nbsp;Jenkins</h3><h3>Step 1: Configure Git&nbsp;Path</h3><ol><li>Go to  → <strong>Global Tool Configuration</strong></li><li>Path to Git executable: C:\\Program Files\\Git\\bin\\git.exe</li></ol><h3>Part 11: Linking GitHub and&nbsp;Jenkins</h3><p>Since your repository is , you don’t need to set up credentials. Jenkins can access public repositories directly.</p><ol><li>Make sure your repository is set to  on&nbsp;GitHub:</li></ol><ul><li>Under , verify visibility is&nbsp;“Public”</li></ul><h3>Part 12: Creating a Freestyle Job in&nbsp;Jenkins</h3><ol><li>From Jenkins dashboard, click </li><li>Enter job name: GitHub-Build-Job</li></ol><h3>Step 2: Configure Source Code Management</h3><ol><li>In the  section, select&nbsp;</li><li>In , paste your GitHub repository URL:</li></ol><pre>https://github.com/YOUR-USERNAME/my-jenkins-project.git</pre><p>: Leave as — (since the repo is&nbsp;public)</p><p>: Change from */master to */development</p><p><strong>Do NOT check any build triggers</strong> since you want to build manually or trigger manually.</p><h3>Step 4: Configure Build&nbsp;Steps</h3><ol><li>Scroll down to &nbsp;section</li><li>Select <strong>Execute Windows batch&nbsp;command</strong></li><li>In the command box,&nbsp;enter:</li></ol><pre>@echo offecho Running build script...echo.<p>echo ================================</p>echo Executing script contents:<p>echo ================================</p>bash build.sh</pre><p><strong>Alternative if Git Bash is in&nbsp;PATH:</strong></p><pre>\"C:\\Program Files\\Git\\bin\\bash.exe\" build.sh</pre><h3>Step 5: Save Configuration</h3><p>Click  at the bottom of the&nbsp;page.</p><h3>Part 13: Testing the&nbsp;Pipeline</h3><ol><li>On your job page, click </li><li>You’ll see a build appear in the </li><li>Click on the build number (e.g.,&nbsp;)</li><li>You should see the output from your build.sh&nbsp;script</li></ol><h3>Step 2: Make Changes and&nbsp;Push</h3><ol><li>Open build.sh on your local&nbsp;machine</li><li>Modify the script (e.g., change the version&nbsp;number):</li></ol><pre>echo \"==================================\"echo \"Starting Build Process\"<p>echo \"==================================\"</p>echo \"Build Date: $(date)\"<p>echo \"Build Number: ${BUILD_NUMBER:-Manual}\"</p>echo \"==================================\"<p>echo \"Hello from the build script!\"</p>echo \"This is version 2.0 - NEW UPDATE!\"<p>echo \"==================================\"</p>echo \"Build completed successfully!\"<p>echo \"==================================\"</p></pre><ol></ol><pre>git add build.shgit commit -m \"Update build script to version 2.0\"<p>git push origin development</p></pre><h3>Step 3: Trigger New Build in&nbsp;Jenkins</h3><ol><li>Click on your job </li><li>Click on the new build&nbsp;number</li><li><strong>You should see the NEW output</strong> with “version 2.0 — NEW&nbsp;UPDATE!”</li></ol><ol><li>Commit and push to&nbsp;GitHub</li></ol><p>The console output will reflect your latest&nbsp;changes!</p><h3>Part 14: Understanding the Console&nbsp;Output</h3><p>When you check the console output in Jenkins, you’ll&nbsp;see:</p><ol><li>: Jenkins pulling the latest code from your repository</li><li>: Your script&nbsp;running</li><li>: Everything echoed from your build.sh&nbsp;file</li><li>: Success or failure&nbsp;status</li></ol><pre>Started by user adminRunning as SYSTEM<p>Building in workspace C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job</p>The recommended git tool is: NONECloning the remote Git repository<p>Cloning repository https://github.com/YOUR-USERNAME/my-jenkins-project.git</p> &gt; git init C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job<p> &gt; git fetch --tags --force --progress -- https://github.com/YOUR-USERNAME/my-jenkins-project.git +refs/heads/*:refs/remotes/origin/*</p> &gt; git config remote.origin.url https://github.com/YOUR-USERNAME/my-jenkins-project.git<p> &gt; git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/*</p> &gt; git rev-parse refs/remotes/origin/development^{commit}<p>Checking out Revision abc123... (refs/remotes/origin/development)</p> &gt; git config core.sparsecheckout<p> &gt; git checkout -f abc123...</p>Commit message: \"Update build script to version 2.0\"<p>[GitHub-Build-Job] $ cmd /c call C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jenkins123.bat</p></pre><pre>C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job&gt;@echo off</pre><pre>C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job&gt;echo Running build script...Running build script...</pre><pre>==================================Starting Build Process<p>==================================</p>Build Date: Wed Oct 15 10:30:45 2025==================================<p>Hello from the build script!</p>This is version 2.0 - NEW UPDATE!<p>==================================</p>Build completed successfully!<p>==================================</p></pre><h3>Part 15: Common Workflows and&nbsp;Tips</h3><pre>cd my-jenkins-project # Edit your files</pre><ul><li>git add&nbsp;. git commit -m \"Descriptive message about&nbsp;changes\"</li></ul><ul><li>git push origin development</li></ul><p>: Click “Build Now” and check console&nbsp;output</p><h3>Switching Between&nbsp;Branches</h3><pre># View all branchesgit branch -a</pre><pre># Switch to a different branchgit checkout branch-name</pre><pre># Create and switch to new branchgit checkout -b new-feature-branch</pre><pre># Update your local repositorygit pull origin development</pre><pre>git log# or for a compact view</pre><p>: Git commands not recognized</p><ul><li>: Ensure Git is added to PATH during installation. Restart Command Prompt after installation.</li></ul><p>: Permission denied when&nbsp;pushing</p><ul><li>: Check your Git credentials. Run git config --list to&nbsp;verify.</li></ul><p>: Cannot access Jenkins at localhost:8080</p><ul><li>: Check if Jenkins service is running. Go to Services (services.msc) and look for&nbsp;Jenkins.</li></ul><p>: Build fails with “git command not&nbsp;found”</p><ul><li>: Configure Git path in Jenkins: Manage Jenkins → Global Tool Configuration →&nbsp;Git</li></ul><p>: Script doesn’t&nbsp;execute</p><ul><li>: Verify the bash path in your build step. Use full path: \"C:\\Program Files\\Git\\bin\\bash.exe\"</li></ul><p>: Cannot push to repository</p><ul><li>: Verify repository URL and that you have permissions. Use git remote -v to check configured remotes.</li></ul><p>Congratulations! You’ve successfully:</p><p>✅ Installed Git on Windows ✅ Created a GitHub account and repository<p> ✅ Created and managed branches</p> ✅ Created and edited shell scripts<p> ✅ Performed Git operations (add, commit, push, pull)</p> ✅ Installed and configured Jenkins<p> ✅ Linked GitHub with Jenkins</p> ✅ Created a Freestyle job without triggers<p> ✅ Built your project and viewed console&nbsp;output</p></p><p>Every time you push changes to your development branch and manually trigger a build in Jenkins, you'll see the updated output in the console. This forms the foundation of CI/CD practices!</p><p>To enhance your setup, consider:</p><ol><li>: Learn to merge your development branch to&nbsp;main</li><li>: Set up automatic builds when you push (requires webhook configuration)</li><li>: Explore Jenkins Pipeline for more complex workflows</li><li>: Configure email or Slack notifications for build&nbsp;results</li><li>: Add automated tests to your build&nbsp;process</li></ol><p>Happy coding and building! 🚀</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8822a83d4165\" width=\"1\" height=\"1\" alt=\"\">","contentLength":12069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Connect Spring Boot with PostgreSQL in Docker","url":"https://blog.devops.dev/how-to-connect-spring-boot-with-postgresql-in-docker-a654fffdd717?source=rss----33f8b2d9a328---4","date":1761681063,"author":"Aravindcsebe","guid":319397,"unread":true,"content":"<blockquote><em>Run PostgreSQL in Docker and integrate it seamlessly with a Spring Boot application</em></blockquote><p>PostgreSQL is one of the most popular open-source databases — and pairing it with Spring Boot is a common choice for modern backend applications.</p><p>In this guide, I’ll show you how&nbsp;to:</p><ul><li>Run PostgreSQL using&nbsp;Docker</li><li>Connect a Spring Boot application (built with Maven) to the containerized database</li><li>Set up everything for quick and clean local development</li></ul><h3>🐳 Step 1: Pull and Run PostgreSQL with&nbsp;Docker</h3><p>We’ll use the official PostgreSQL image from Docker Hub. If you don’t already have it locally, Docker will  it the first time you run the container.</p><p>🔧 One-liner to pull and&nbsp;run:</p><pre>docker run --name my-postgres -e POSTGRES_USER=testuser -e POSTGRES_PASSWORD=testpass -e POSTGRES_DB=testdb -p 5433:5432 -d postgres</pre><ul><li><strong>Image pulled automatically</strong>: If the postgres image isn’t found locally, Docker will pull the latest version from Docker&nbsp;Hub</li><li>: Named my-postgres</li><li>: With user testuser, password testpass, and database&nbsp;testdb</li><li>: Maps container’s 5432 to your machine's 5433</li></ul><p>You can verify the container is&nbsp;running:</p><p>And if you want to pull the image manually beforehand:</p><h3>⚙️ Step 2: Configure Spring Boot (application.properties)</h3><p>Add the following to your src/main/resources/application.properties:</p><pre>spring.application.name=PostgresDockerDemo<p>spring.datasource.url=jdbc:postgresql://localhost:5433/testdb</p>spring.datasource.username=testuser<p>spring.datasource.password=testpass</p>spring.datasource.driver-class-name=org.postgresql.Driver<p>spring.jpa.hibernate.ddl-auto=update</p>spring.jpa.show-sql=true<p>spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect</p></pre><p>This configures Spring Boot to connect to the PostgreSQL container using the same credentials.</p><h3>📦 Step 3: Add PostgreSQL Driver to&nbsp;Maven</h3><p>In your pom.xml,&nbsp;include:</p><pre> &lt;dependencies&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</p>   &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</p>   &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.postgresql&lt;/groupId&gt;</p>   &lt;artifactId&gt;postgresql&lt;/artifactId&gt;  &lt;/dependency&gt;   &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;<p>   &lt;artifactId&gt;lombok&lt;/artifactId&gt;</p>   &lt;optional&gt;true&lt;/optional&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</p>   &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;  &lt;/dependency&gt;</pre><p>🧪 Step 4: Create a Sample Entity and Repository</p><pre>@Entity@Datapublic class User    @Id<p>    @GeneratedValue(strategy = GenerationType.IDENTITY)</p>    private Long id;    private String email;</pre><pre>public interface UserRepository extends JpaRepository&lt;User, Long&gt; {}</pre><p>🌐 Step 5: Simple REST Controller</p><pre>@RestController@RequestMapping(\"/users\")<p>public class UserController {</p>  private UserRepository userRepository;  public User create(@RequestBody User user) {<p>    return userRepository.save(user);</p>  }  public List&lt;User&gt; all() {<p>    return userRepository.findAll();</p>  }</pre><p>When you run the spring application you could see the below in&nbsp;console.</p><pre>Hibernate: create table users (id bigint generated by default as identity, email varchar(255), name varchar(255), primary key (id))</pre><p>Access PostgreSQL Inside the Docker Container (Optional)</p><p>After your PostgreSQL container is up and running, you can <strong>connect to the database shell</strong> to inspect or insert data manually.</p><p>🧩 Command to enter the container:</p><pre>docker exec -it my-postgres psql -U testuser -d testdb</pre><ul><li>docker exec -it runs a command inside your running container</li><li>my-postgres is the name of your container</li><li>psql is the PostgreSQL CLI</li><li>-U testuser connects with the user you set&nbsp;up</li><li>-d testdb connects to the database you&nbsp;created</li></ul><p>let’s hit the endpoint and&nbsp;check.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a654fffdd717\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitOps Observability: From “It Works on My Machine” to “We Can Prove It Works Everywhere”","url":"https://blog.devops.dev/gitops-observability-from-it-works-on-my-machine-to-we-can-prove-it-works-everywhere-a7896b8f0e08?source=rss----33f8b2d9a328---4","date":1761681007,"author":"Salwan Mohamed","guid":319396,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Host your Helm repo using GitHub","url":"https://blog.devops.dev/host-your-helm-repo-using-github-212ee44466cd?source=rss----33f8b2d9a328---4","date":1761681003,"author":"Dejanu Alex","guid":319395,"unread":true,"content":"<p>When you’re ready to share your charts, the preferred way to do so is by uploading them to a chart repository.</p><p><em>TL;DR: A Helm Repository is HTTP server where packaged charts can be stored and shared. The repository consists of packaged charts and a special file called </em><em>index.yaml (which contains an index of all of the charts in the repository).</em></p><ol><li>Create the repository structure in the root directory: mkdir {charts,packages}&nbsp;:</li></ol><pre>GIT_REPO/├── charts/      # For your source chart directories<p>└── packages/    # For packaged .tgz files and index.yaml</p></pre><p>3. Create a chart i.e. mychart: helm create charts/mychart (<a href=\"https://helm.sh/docs/helm/helm_create/\">helm create</a> creates a chart directory along with the common files and directories used in a&nbsp;chart).</p><p>4. Package the mychart into a versioned chart archive file (basically creating the&nbsp;.tgz file for the chart under packages directory): helm package charts/mychart -d&nbsp;packages</p><p>helm repo indexwill read the packagesdirectory, and generate an index file based on the charts found and write the result to index.yaml (that lists all the charts in your repository along with their metadata).</p><p>6. Configure your GitHub repo to serve static web pages (via <a href=\"https://docs.github.com/en/pages\">GitHubPages</a>) by configuring it to serve a particular branch (main in this&nbsp;case)</p><p>To verify everything works,&nbsp;simply:</p><pre># Add your repo (point to the location of index.yaml file)helm repo add helm_reponame https://your-username.github.io/gitrepo-name/packages<p># Update to fetch the latest index</p>helm repo updatehelm repo list<p># Search for charts in your repo</p>helm search repo helm_reponame</pre><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=212ee44466cd\" width=\"1\" height=\"1\" alt=\"\">","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub Adds Platform for Managing AI Agents Embedded in DevOps Workflows","url":"https://devops.com/github-adds-platform-for-managing-ai-agents-embedded-in-devops-workflows/","date":1761677781,"author":"Mike Vizard","guid":319344,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Securing the AI Era: How Development, Security, and Compliance Must Evolve","url":"https://devops.com/securing-the-ai-era-how-development-security-and-compliance-must-evolve/","date":1761676154,"author":"Sumeet Singh","guid":319343,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ASL pod","url":"https://www.youtube.com/watch?v=A8o99is_L-k","date":1761673770,"author":"CNCF [Cloud Native Computing Foundation]","guid":319313,"unread":true,"content":"<article>Here is the ASL sign for pod.</article>","contentLength":29,"flags":null,"enclosureUrl":"https://www.youtube.com/v/A8o99is_L-k?version=3","enclosureMime":"","commentsUrl":null},{"title":"Survey Surfaces Impact AI Coding Tools Are Having on DevOps Workflows","url":"https://devops.com/survey-surfaces-impact-ai-coding-tools-are-having-on-devops-workflows/","date":1761664864,"author":"Mike Vizard","guid":319237,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon Nova Multimodal Embeddings: State-of-the-art embedding model for agentic RAG and semantic search","url":"https://aws.amazon.com/blogs/aws/amazon-nova-multimodal-embeddings-now-available-in-amazon-bedrock/","date":1761664358,"author":"Danilo Poccia","guid":319230,"unread":true,"content":"<p>Embedding models convert textual, visual, and audio inputs into numerical representations called <a href=\"https://aws.amazon.com/what-is/embeddings-in-machine-learning/\">embeddings</a>. These embeddings capture the meaning of the input in a way that AI systems can compare, search, and analyze, powering use cases such as semantic search and RAG.</p><p>Organizations are increasingly seeking solutions to unlock insights from the growing volume of unstructured data that is spread across text, image, document, video, and audio content. For example, an organization might have product images, brochures that contain infographics and text, and user-uploaded video clips. Embedding models are able to unlock value from unstructured data, however traditional models are typically specialized to handle one content type. This limitation drives customers to either build complex crossmodal embedding solutions or restrict themselves to use cases focused on a single content type. The problem also applies to mixed-modality content types such as documents with interleaved text and images or video with visual, audio, and textual elements where existing models struggle to capture crossmodal relationships eﬀectively.</p><p>Nova Multimodal Embeddings supports a unified semantic space for text, documents, images, video, and audio for use cases such as crossmodal search across mixed-modality content, searching with a reference image, and retrieving visual documents.</p><p>We evaluated the model on a broad range of benchmarks, and it delivers leading accuracy out-of-the-box as described in the following table.</p><p>Nova Multimodal Embeddings supports a context length of up to 8K tokens, text in up to 200 languages, and accepts inputs via synchronous and asynchronous APIs. Additionally, it supports segmentation (also known as “chunking”) to partition long-form text, video, or audio content into manageable segments, generating embeddings for each portion. Lastly, the model oﬀers four output embedding dimensions, trained using <a href=\"https://arxiv.org/abs/2205.13147\">Matryoshka Representation Learning (MRL)</a> that enables low-latency end-to-end retrieval with minimal accuracy changes.</p><p>Let’s see how the new model can be used in practice.</p><p> Getting started with Nova Multimodal Embeddings follows the same pattern as <a href=\"https://aws.amazon.com/bedrock/model-choice/\">other models in Amazon Bedrock</a>. The model accepts text, documents, images, video, or audio as input and returns numerical embeddings that you can use for semantic search, similarity comparison, or RAG.</p><p>Here’s a practical example using the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> that shows how to create embeddings from different content types and store them for later retrieval. For simplicity, I’ll use <a href=\"https://aws.amazon.com/s3/features/vectors/\">Amazon S3 Vectors</a>, a cost-optimized storage with native support for storing and querying vectors at any scale, to store and search the embeddings.</p><p>Let’s start with the fundamentals: converting text into embeddings. This example shows how to transform a simple text description into a numerical representation that captures its semantic meaning. These embeddings can later be compared with embeddings from documents, images, videos, or audio to find related content.</p><p>To make the code easy to follow, I’ll show a section of the script at a time. The full script is included at the end of this walkthrough.</p><pre><code>import json\nimport base64\nimport time\nimport boto3\n\nMODEL_ID = \"amazon.nova-2-multimodal-embeddings-v1:0\"\nEMBEDDING_DIMENSION = 3072\n\n# Initialize Amazon Bedrock Runtime client\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n\nprint(f\"Generating text embedding with {MODEL_ID} ...\")\n\n# Text to embed\ntext = \"Amazon Nova is a multimodal foundation model\"\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"text\": {\"truncationMode\": \"END\", \"value\": text},\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")</code></pre><p>Now we’ll process visual content using the same embedding space using a  file in the same folder as the script. This demonstrates the power of multimodality: Nova Multimodal Embeddings is able to capture both textual and visual context into a single embedding that provides enhanced understanding of the document.</p><p>Nova Multimodal Embeddings can generate embeddings that are optimized for how they are being used. When indexing for a search or retrieval use case,  can be set to . For the query step,  can be set depending on the type of item to be retrieved. For example, when retrieving documents,  can be set to .</p><pre><code># Read and encode image\nprint(f\"Generating image embedding with {MODEL_ID} ...\")\n\nwith open(\"photo.jpg\", \"rb\") as f:\n    image_bytes = base64.b64encode(f.read()).decode(\"utf-8\")\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"image\": {\n            \"format\": \"jpeg\",\n            \"source\": {\"bytes\": image_bytes}\n        },\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")</code></pre><p>To process video content, I use the asynchronous API. That’s a requirement for videos that are larger than 25MB when encoded as <a href=\"https://en.wikipedia.org/wiki/Base64\">Base64</a>. First, I upload a local video to an S3 bucket in the same <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Region</a>.</p><pre><code>aws s3 cp presentation.mp4 s3://my-video-bucket/videos/</code></pre><p>This example shows how to extract embeddings from both visual and audio components of a video file. The segmentation feature breaks longer videos into manageable chunks, making it practical to search through hours of content efficiently.</p><pre><code># Initialize Amazon S3 client\ns3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n\nprint(f\"Generating video embedding with {MODEL_ID} ...\")\n\n# Amazon S3 URIs\nS3_VIDEO_URI = \"s3://my-video-bucket/videos/presentation.mp4\"\nS3_EMBEDDING_DESTINATION_URI = \"s3://my-embedding-destination-bucket/embeddings-output/\"\n\n# Create async embedding job for video with audio\nmodel_input = {\n    \"taskType\": \"SEGMENTED_EMBEDDING\",\n    \"segmentedEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"video\": {\n            \"format\": \"mp4\",\n            \"embeddingMode\": \"AUDIO_VIDEO_COMBINED\",\n            \"source\": {\n                \"s3Location\": {\"uri\": S3_VIDEO_URI}\n            },\n            \"segmentationConfig\": {\n                \"durationSeconds\": 15  # Segment into 15-second chunks\n            },\n        },\n    },\n}\n\nresponse = bedrock_runtime.start_async_invoke(\n    modelId=MODEL_ID,\n    modelInput=model_input,\n    outputDataConfig={\n        \"s3OutputDataConfig\": {\n            \"s3Uri\": S3_EMBEDDING_DESTINATION_URI\n        }\n    },\n)\n\ninvocation_arn = response[\"invocationArn\"]\nprint(f\"Async job started: {invocation_arn}\")\n\n# Poll until job completes\nprint(\"\\nPolling for job completion...\")\nwhile True:\n    job = bedrock_runtime.get_async_invoke(invocationArn=invocation_arn)\n    status = job[\"status\"]\n    print(f\"Status: {status}\")\n\n    if status != \"InProgress\":\n        break\n    time.sleep(15)\n\n# Check if job completed successfully\nif status == \"Completed\":\n    output_s3_uri = job[\"outputDataConfig\"][\"s3OutputDataConfig\"][\"s3Uri\"]\n    print(f\"\\nSuccess! Embeddings at: {output_s3_uri}\")\n\n    # Parse S3 URI to get bucket and prefix\n    s3_uri_parts = output_s3_uri[5:].split(\"/\", 1)  # Remove \"s3://\" prefix\n    bucket = s3_uri_parts[0]\n    prefix = s3_uri_parts[1] if len(s3_uri_parts) &gt; 1 else \"\"\n\n    # AUDIO_VIDEO_COMBINED mode outputs to embedding-audio-video.jsonl\n    # The output_s3_uri already includes the job ID, so just append the filename\n    embeddings_key = f\"{prefix}/embedding-audio-video.jsonl\".lstrip(\"/\")\n\n    print(f\"Reading embeddings from: s3://{bucket}/{embeddings_key}\")\n\n    # Read and parse JSONL file\n    response = s3.get_object(Bucket=bucket, Key=embeddings_key)\n    content = response['Body'].read().decode('utf-8')\n\n    embeddings = []\n    for line in content.strip().split('\\n'):\n        if line:\n            embeddings.append(json.loads(line))\n\n    print(f\"\\nFound {len(embeddings)} video segments:\")\n    for i, segment in enumerate(embeddings):\n        print(f\"  Segment {i}: {segment.get('startTime', 0):.1f}s - {segment.get('endTime', 0):.1f}s\")\n        print(f\"    Embedding dimension: {len(segment.get('embedding', []))}\")\nelse:\n    print(f\"\\nJob failed: {job.get('failureMessage', 'Unknown error')}\")</code></pre><p>With our embeddings generated, we need a place to store and search them efficiently. This example demonstrates setting up a vector store using Amazon S3 Vectors, which provides the infrastructure needed for similarity search at scale. Think of this as creating a searchable index where semantically similar content naturally clusters together. When adding an embedding to the index, I use the metadata to specify the original format and the content being indexed.</p><pre><code># Initialize Amazon S3 Vectors client\ns3vectors = boto3.client(\"s3vectors\", region_name=\"us-east-1\")\n\n# Configuration\nVECTOR_BUCKET = \"my-vector-store\"\nINDEX_NAME = \"embeddings\"\n\n# Create vector bucket and index (if they don't exist)\ntry:\n    s3vectors.get_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Vector bucket {VECTOR_BUCKET} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Created vector bucket: {VECTOR_BUCKET}\")\n\ntry:\n    s3vectors.get_index(vectorBucketName=VECTOR_BUCKET, indexName=INDEX_NAME)\n    print(f\"Vector index {INDEX_NAME} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_index(\n        vectorBucketName=VECTOR_BUCKET,\n        indexName=INDEX_NAME,\n        dimension=EMBEDDING_DIMENSION,\n        dataType=\"float32\",\n        distanceMetric=\"cosine\"\n    )\n    print(f\"Created index: {INDEX_NAME}\")\n\ntexts = [\n    \"Machine learning on AWS\",\n    \"Amazon Bedrock provides foundation models\",\n    \"S3 Vectors enables semantic search\"\n]\n\nprint(f\"\\nGenerating embeddings for {len(texts)} texts...\")\n\n# Generate embeddings using Amazon Nova for each text\nvectors = []\nfor text in texts:\n    response = bedrock_runtime.invoke_model(\n        body=json.dumps({\n            \"taskType\": \"SINGLE_EMBEDDING\",\n            \"singleEmbeddingParams\": {\n                \"embeddingDimension\": EMBEDDING_DIMENSION,\n                \"text\": {\"truncationMode\": \"END\", \"value\": text}\n            }\n        }),\n        modelId=MODEL_ID,\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n\n    response_body = json.loads(response[\"body\"].read())\n    embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\n    vectors.append({\n        \"key\": f\"text:{text[:50]}\",  # Unique identifier\n        \"data\": {\"float32\": embedding},\n        \"metadata\": {\"type\": \"text\", \"content\": text}\n    })\n    print(f\"  ✓ Generated embedding for: {text}\")\n\n# Add all vectors to store in a single call\ns3vectors.put_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    vectors=vectors\n)\n\nprint(f\"\\nSuccessfully added {len(vectors)} vectors to the store in one put_vectors call!\")</code></pre><p>This final example demonstrates the capability of searching across different content types with a single query, finding the most similar content regardless of whether it originated from text, images, videos, or audio. The distance scores help you understand how closely related the results are to your original query.</p><pre><code># Text to query\nquery_text = \"foundation models\"  \n\nprint(f\"\\nGenerating embeddings for query '{query_text}' ...\")\n\n# Generate embeddings\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps({\n        \"taskType\": \"SINGLE_EMBEDDING\",\n        \"singleEmbeddingParams\": {\n            \"embeddingPurpose\": \"GENERIC_RETRIEVAL\",\n            \"embeddingDimension\": EMBEDDING_DIMENSION,\n            \"text\": {\"truncationMode\": \"END\", \"value\": query_text}\n        }\n    }),\n    modelId=MODEL_ID,\n    accept=\"application/json\",\n    contentType=\"application/json\"\n)\n\nresponse_body = json.loads(response[\"body\"].read())\nquery_embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Searching for similar embeddings...\\n\")\n\n# Search for top 5 most similar vectors\nresponse = s3vectors.query_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    queryVector={\"float32\": query_embedding},\n    topK=5,\n    returnDistance=True,\n    returnMetadata=True\n)\n\n# Display results\nprint(f\"Found {len(response['vectors'])} results:\\n\")\nfor i, result in enumerate(response[\"vectors\"], 1):\n    print(f\"{i}. {result['key']}\")\n    print(f\"   Distance: {result['distance']:.4f}\")\n    if result.get(\"metadata\"):\n        print(f\"   Metadata: {result['metadata']}\")\n    print()</code></pre><p>Crossmodal search is one of the key advantages of multimodal embeddings. With crossmodal search, you can query with text and find relevant images. You can also search for videos using text descriptions, find audio clips that match certain topics, or discover documents based on their visual and textual content. For your reference, the full script with all previous examples merged together is here:</p><pre><code>import json\nimport base64\nimport time\nimport boto3\n\nMODEL_ID = \"amazon.nova-2-multimodal-embeddings-v1:0\"\nEMBEDDING_DIMENSION = 3072\n\n# Initialize Amazon Bedrock Runtime client\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n\nprint(f\"Generating text embedding with {MODEL_ID} ...\")\n\n# Text to embed\ntext = \"Amazon Nova is a multimodal foundation model\"\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"text\": {\"truncationMode\": \"END\", \"value\": text},\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")\n# Read and encode image\nprint(f\"Generating image embedding with {MODEL_ID} ...\")\n\nwith open(\"photo.jpg\", \"rb\") as f:\n    image_bytes = base64.b64encode(f.read()).decode(\"utf-8\")\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"image\": {\n            \"format\": \"jpeg\",\n            \"source\": {\"bytes\": image_bytes}\n        },\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")\n# Initialize Amazon S3 client\ns3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n\nprint(f\"Generating video embedding with {MODEL_ID} ...\")\n\n# Amazon S3 URIs\nS3_VIDEO_URI = \"s3://my-video-bucket/videos/presentation.mp4\"\n\n# Amazon S3 output bucket and location\nS3_EMBEDDING_DESTINATION_URI = \"s3://my-video-bucket/embeddings-output/\"\n\n# Create async embedding job for video with audio\nmodel_input = {\n    \"taskType\": \"SEGMENTED_EMBEDDING\",\n    \"segmentedEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"video\": {\n            \"format\": \"mp4\",\n            \"embeddingMode\": \"AUDIO_VIDEO_COMBINED\",\n            \"source\": {\n                \"s3Location\": {\"uri\": S3_VIDEO_URI}\n            },\n            \"segmentationConfig\": {\n                \"durationSeconds\": 15  # Segment into 15-second chunks\n            },\n        },\n    },\n}\n\nresponse = bedrock_runtime.start_async_invoke(\n    modelId=MODEL_ID,\n    modelInput=model_input,\n    outputDataConfig={\n        \"s3OutputDataConfig\": {\n            \"s3Uri\": S3_EMBEDDING_DESTINATION_URI\n        }\n    },\n)\n\ninvocation_arn = response[\"invocationArn\"]\nprint(f\"Async job started: {invocation_arn}\")\n\n# Poll until job completes\nprint(\"\\nPolling for job completion...\")\nwhile True:\n    job = bedrock_runtime.get_async_invoke(invocationArn=invocation_arn)\n    status = job[\"status\"]\n    print(f\"Status: {status}\")\n\n    if status != \"InProgress\":\n        break\n    time.sleep(15)\n\n# Check if job completed successfully\nif status == \"Completed\":\n    output_s3_uri = job[\"outputDataConfig\"][\"s3OutputDataConfig\"][\"s3Uri\"]\n    print(f\"\\nSuccess! Embeddings at: {output_s3_uri}\")\n\n    # Parse S3 URI to get bucket and prefix\n    s3_uri_parts = output_s3_uri[5:].split(\"/\", 1)  # Remove \"s3://\" prefix\n    bucket = s3_uri_parts[0]\n    prefix = s3_uri_parts[1] if len(s3_uri_parts) &gt; 1 else \"\"\n\n    # AUDIO_VIDEO_COMBINED mode outputs to embedding-audio-video.jsonl\n    # The output_s3_uri already includes the job ID, so just append the filename\n    embeddings_key = f\"{prefix}/embedding-audio-video.jsonl\".lstrip(\"/\")\n\n    print(f\"Reading embeddings from: s3://{bucket}/{embeddings_key}\")\n\n    # Read and parse JSONL file\n    response = s3.get_object(Bucket=bucket, Key=embeddings_key)\n    content = response['Body'].read().decode('utf-8')\n\n    embeddings = []\n    for line in content.strip().split('\\n'):\n        if line:\n            embeddings.append(json.loads(line))\n\n    print(f\"\\nFound {len(embeddings)} video segments:\")\n    for i, segment in enumerate(embeddings):\n        print(f\"  Segment {i}: {segment.get('startTime', 0):.1f}s - {segment.get('endTime', 0):.1f}s\")\n        print(f\"    Embedding dimension: {len(segment.get('embedding', []))}\")\nelse:\n    print(f\"\\nJob failed: {job.get('failureMessage', 'Unknown error')}\")\n# Initialize Amazon S3 Vectors client\ns3vectors = boto3.client(\"s3vectors\", region_name=\"us-east-1\")\n\n# Configuration\nVECTOR_BUCKET = \"my-vector-store\"\nINDEX_NAME = \"embeddings\"\n\n# Create vector bucket and index (if they don't exist)\ntry:\n    s3vectors.get_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Vector bucket {VECTOR_BUCKET} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Created vector bucket: {VECTOR_BUCKET}\")\n\ntry:\n    s3vectors.get_index(vectorBucketName=VECTOR_BUCKET, indexName=INDEX_NAME)\n    print(f\"Vector index {INDEX_NAME} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_index(\n        vectorBucketName=VECTOR_BUCKET,\n        indexName=INDEX_NAME,\n        dimension=EMBEDDING_DIMENSION,\n        dataType=\"float32\",\n        distanceMetric=\"cosine\"\n    )\n    print(f\"Created index: {INDEX_NAME}\")\n\ntexts = [\n    \"Machine learning on AWS\",\n    \"Amazon Bedrock provides foundation models\",\n    \"S3 Vectors enables semantic search\"\n]\n\nprint(f\"\\nGenerating embeddings for {len(texts)} texts...\")\n\n# Generate embeddings using Amazon Nova for each text\nvectors = []\nfor text in texts:\n    response = bedrock_runtime.invoke_model(\n        body=json.dumps({\n            \"taskType\": \"SINGLE_EMBEDDING\",\n            \"singleEmbeddingParams\": {\n                \"embeddingPurpose\": \"GENERIC_INDEX\",\n                \"embeddingDimension\": EMBEDDING_DIMENSION,\n                \"text\": {\"truncationMode\": \"END\", \"value\": text}\n            }\n        }),\n        modelId=MODEL_ID,\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n\n    response_body = json.loads(response[\"body\"].read())\n    embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\n    vectors.append({\n        \"key\": f\"text:{text[:50]}\",  # Unique identifier\n        \"data\": {\"float32\": embedding},\n        \"metadata\": {\"type\": \"text\", \"content\": text}\n    })\n    print(f\"  ✓ Generated embedding for: {text}\")\n\n# Add all vectors to store in a single call\ns3vectors.put_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    vectors=vectors\n)\n\nprint(f\"\\nSuccessfully added {len(vectors)} vectors to the store in one put_vectors call!\")\n# Text to query\nquery_text = \"foundation models\"  \n\nprint(f\"\\nGenerating embeddings for query '{query_text}' ...\")\n\n# Generate embeddings\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps({\n        \"taskType\": \"SINGLE_EMBEDDING\",\n        \"singleEmbeddingParams\": {\n            \"embeddingPurpose\": \"GENERIC_RETRIEVAL\",\n            \"embeddingDimension\": EMBEDDING_DIMENSION,\n            \"text\": {\"truncationMode\": \"END\", \"value\": query_text}\n        }\n    }),\n    modelId=MODEL_ID,\n    accept=\"application/json\",\n    contentType=\"application/json\"\n)\n\nresponse_body = json.loads(response[\"body\"].read())\nquery_embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Searching for similar embeddings...\\n\")\n\n# Search for top 5 most similar vectors\nresponse = s3vectors.query_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    queryVector={\"float32\": query_embedding},\n    topK=5,\n    returnDistance=True,\n    returnMetadata=True\n)\n\n# Display results\nprint(f\"Found {len(response['vectors'])} results:\\n\")\nfor i, result in enumerate(response[\"vectors\"], 1):\n    print(f\"{i}. {result['key']}\")\n    print(f\"   Distance: {result['distance']:.4f}\")\n    if result.get(\"metadata\"):\n        print(f\"   Metadata: {result['metadata']}\")\n    print()</code></pre><p>For production applications, embeddings can be stored in any vector database. <a href=\"https://aws.amazon.com/opensearch-service/\">Amazon OpenSearch Service</a> offers native integration with Nova Multimodal Embeddings at launch, making it straightforward to build scalable search applications. As shown in the examples before, <a href=\"https://aws.amazon.com/s3/features/vectors/\">Amazon S3 Vectors</a> provides a simple way to store and query embeddings with your application data.</p><p> Nova Multimodal Embeddings offers four output dimension options: 3,072, 1,024, 384, and 256. Larger dimensions provide more detailed representations but require more storage and computation. Smaller dimensions offer a practical balance between retrieval performance and resource efficiency. This flexibility helps you optimize for your specific application and cost requirements.</p><p>The model handles substantial context lengths. For text inputs, it can process up to 8,192 tokens at once. Video and audio inputs support segments of up to 30 seconds, and the model can segment longer files. This segmentation capability is particularly useful when working with large media files—the model splits them into manageable pieces and creates embeddings for each segment.</p><p>The model includes responsible AI features built into Amazon Bedrock. Content submitted for embedding goes through Amazon Bedrock content safety filters, and the model includes fairness measures to reduce bias.</p><p>As described in the code examples, the model can be invoked through both synchronous and asynchronous APIs. The synchronous API works well for real-time applications where you need immediate responses, such as processing user queries in a search interface. The asynchronous API handles latency insensitive workloads more efficiently, making it suitable for processing large content such as videos.</p><p>If you’re using an AI–powered assistant for software development such as <a href=\"https://aws.amazon.com/q/developer/\">Amazon Q Developer</a> or <a href=\"https://kiro.dev/\">Kiro</a>, you can set up the <a href=\"https://awslabs.github.io/mcp/servers/aws-api-mcp-server\">AWS API MCP Server</a> to help the AI assistants interact with AWS services and resources and the <a href=\"https://awslabs.github.io/mcp/servers/aws-knowledge-mcp-server\">AWS Knowledge MCP Server</a> to provide up-to-date documentation, code samples, knowledge about the regional availability of AWS APIs and CloudFormation resources.</p><p>Start building multimodal AI-powered applications with Nova Multimodal Embeddings today, and share your feedback through <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a> or your usual AWS Support contacts.</p>","contentLength":24126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Before learning Kubernetes, understand what a service architecture is.","url":"https://blog.devops.dev/antes-de-aprender-kubernetes-entenda-o-que-e-uma-arquitetura-de-servicos-f7ad9160f4fc?source=rss----33f8b2d9a328---4","date":1761660738,"author":"Prata","guid":319224,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"7 Proven Benefits of DevOps Implementation in Modern Software Development","url":"https://devops.com/7-proven-benefits-of-devops-implementation-in-modern-software-development/","date":1761653342,"author":"Albert Hilton","guid":319133,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"End-to-End Visibility: The Role of Observability in Frontend and Backend Systems","url":"https://devops.com/end-to-end-visibility-observability-in-frontend-backend-systems/","date":1761587337,"author":"Neel Shah","guid":316905,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}