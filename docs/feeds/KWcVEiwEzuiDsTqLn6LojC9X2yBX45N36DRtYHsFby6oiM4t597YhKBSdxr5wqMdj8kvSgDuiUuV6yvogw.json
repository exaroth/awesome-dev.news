{"id":"KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw","title":"GitHub All Languages Daily Trending","displayTitle":"Github Trending","url":"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml","feedLink":"http://mshibanami.github.io/GitHubTrendingRSS","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":50,"items":[{"title":"The-Cool-Coders/Project-Ideas-And-Resources","url":"https://github.com/The-Cool-Coders/Project-Ideas-And-Resources","date":1751423796,"author":"","guid":179513,"unread":true,"content":"<p>A Collection of application ideas that can be used to improve your coding skills ‚ù§.</p><img src=\"https://user-images.githubusercontent.com/61475220/97093903-6d8aee80-166d-11eb-8799-13e119380d2b.jpg\" width=\"50%\" align=\"right\"><p>Have you ever wanted to build something but you had no idea what to do? Just as authors sometimes have \"writers block\" it's also true for developers. This list is intended to solve this issue once and for all! üëç</p><ul><li>Great for improving your coding skills ;</li><li>Great for experimenting with new technologies üÜï;</li><li>Great for adding to your portfolio to impress your next employer/client ;</li><li>Great for using as examples in tutorials (articles or videos) ;</li><li>Easy to complete and also easily extendable with new features ;</li></ul><p>This is not just a simple list of projects, but a collection that describes each project in enough detail so that you can develop it from the ground up!</p><p>Each project has the following :</p><ol><li>A clear and descriptive objective;</li><li>A list of  which should be implemented (these stories act more as a guideline than a forced list of . Feel free to add your own stories if you want);</li><li>A list of bonus features that not only improve the base project, but also your skills at the same time (these are optional and should be attempted only after you have completed the required user stories)</li><li>All the resources and links to help you find what you need to complete the project</li></ol><p>Projects are divided into three tiers based on the knowledge and experience required to complete them.</p><table><tbody><tr><td>Developers in the early stages of their learning journey. Those who are typically focused on creating user-facing applications.</td></tr><tr><td>Developers at an intermediate stage of learning and experience. They are comfortable in UI/UX, using development tools, and building apps that use API services.</td></tr><tr><td>Developers who have all of the above, and are learning more advanced techniques like implementing backend applications and database services.</td></tr></tbody></table><h3>Tier-1: Beginner Projects</h3><h3>Tier-2: Intermediate Projects</h3><h3>Tier-3: Advanced Projects</h3><h3>DApp and Blockchain Projects -</h3><p>We are planning to add more and more projects to this repository. For this, we need your help! Find out how to contribute below. üëá</p><p>We are also planning to create a website where you can easily browse through all of the projects.</p><p>Any contributions are highly appreciated.  You can contribute in two ways:</p><ol><li>create an issue and tell us your idea . Make sure that you use the  label in this case;</li><li>fork the project and submit a PR with your new idea. Before doing that, please make sure that you read and follow the <a href=\"https://raw.githubusercontent.com/The-Cool-Coders/Project-Ideas-And-Resources/main/CONTRIBUTION.md\">Contribution Guide</a>;</li></ol><p>You can also add your own examples to the projects after you have completed them. I highly encourage you to do this as it will show others what amazing things were built! üëç</p><h4>Awesome APIS for Front-End Developers</h4><p>Thanks goes to all these Wonderful People. Contributions of any kind are welcome!üöÄ</p><a href=\"https://github.com/The-Cool-Coders/Project-Ideas-And-Resources/contributors\"><img src=\"https://contrib.rocks/image?repo=The-Cool-Coders/Project-Ideas-And-Resources\"></a><p>Looking for Great Contributions from all the open source enthusiasts for making this repository even bigger! Do follow the steps mentioned in Contributing.md file for succesful PRs</p><p>If the information from this repo was useful to you in any way, make sure you give it a star üåü, this way others can find it and benefit too! Together we can grow and make our community better! </p><p align=\"center\" width=\"60%\"><b>Do you have any suggestions on how we could improve this project overall? Let us know! We'd love to hear your feedback ‚ù§!</b></p>","contentLength":3204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"confident-ai/deepeval","url":"https://github.com/confident-ai/deepeval","date":1751423796,"author":"","guid":179514,"unread":true,"content":"<p>The LLM Evaluation Framework</p><p> is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs  for evaluation.</p><p>Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.</p><blockquote><p>[!IMPORTANT] Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? <a href=\"https://confident-ai.com?utm_source=GitHub\">Sign up to the DeepEval platform</a> to compare iterations of your LLM app, generate &amp; share testing reports, and more.</p></blockquote><blockquote><p>ü•≥ You can now share DeepEval's test results on the cloud directly on <a href=\"https://confident-ai.com?utm_source=GitHub\">Confident AI</a>'s infrastructure</p></blockquote><ul><li>Supports both end-to-end and component-level LLM evaluation.</li><li>Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by  LLM of your choice, statistical methods, or NLP models that runs : \n  <ul><li><ul></ul></li><li><ul></ul></li><li><ul></ul></li><li><ul><li>Conversation Completeness</li></ul></li></ul></li><li>Build your own custom metrics that are automatically integrated with DeepEval's ecosystem.</li><li>Generate synthetic datasets for evaluation.</li><li>Integrates seamlessly with  CI/CD environment.</li><li><a href=\"https://deepeval.com/docs/red-teaming-introduction\">Red team your LLM application</a> for 40+ safety vulnerabilities in a few lines of code, including: \n  <ul><li>etc., using advanced 10+ attack enhancement strategies such as prompt injections.</li></ul></li><li>Easily benchmark  LLM on popular LLM benchmarks in <a href=\"https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub\">under 10 lines of code.</a>, which includes: \n  <ul></ul></li><li><a href=\"https://confident-ai.com?utm_source=GitHub\">100% integrated with Confident AI</a> for the full evaluation lifecycle: \n  <ul><li>Curate/annotate evaluation datasets on the cloud</li><li>Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best</li><li>Fine-tune metrics for custom results</li><li>Debug evaluation results via LLM traces</li><li>Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data</li></ul></li></ul><blockquote><p>[!NOTE] Confident AI is the DeepEval platform. Create an account <a href=\"https://app.confident-ai.com?utm_source=GitHub\">here.</a></p></blockquote><p>Let's pretend your LLM application is a RAG based customer support chatbot; here's how DeepEval can help test what you've built.</p><h2>Create an account (highly recommended)</h2><p>Using the  platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.</p><p>Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy <a href=\"https://deepeval.com/docs/data-privacy?utm_source=GitHub\">here</a>).</p><h2>Writing your first test case</h2><p>Open  and write your first test case to run an  evaluation using DeepEval, which treats your LLM app as a black-box:</p><pre><code>import pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\ndef test_case():\n    correctness_metric = GEval(\n        name=\"Correctness\",\n        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n        threshold=0.5\n    )\n    test_case = LLMTestCase(\n        input=\"What if these shoes don't fit?\",\n        # Replace this with the actual output from your LLM application\n        actual_output=\"You have 30 days to get a full refund at no extra cost.\",\n        expected_output=\"We offer a 30-day full refund at no extra costs.\",\n        retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n    )\n    assert_test(test_case, [correctness_metric])\n</code></pre><p>Set your  as an environment variable (you can also evaluate using your own custom model, for more details visit <a href=\"https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub\">this part of our docs</a>):</p><pre><code>export OPENAI_API_KEY=\"...\"\n</code></pre><p>And finally, run  in the CLI:</p><pre><code>deepeval test run test_chatbot.py\n</code></pre><p><strong>Congratulations! Your test case should have passed ‚úÖ</strong> Let's breakdown what happened.</p><ul><li>The variable  mimics a user input, and  is a placeholder for what your application's supposed to output based on this input.</li><li>The variable  represents the ideal answer for a given , and <a href=\"https://deepeval.com/docs/metrics-llm-evals\"></a> is a research-backed metric provided by  for you to evaluate your LLM output's on any custom custom with human-like accuracy.</li><li>In this example, the metric  is correctness of the  based on the provided .</li><li>All metric scores range from 0 - 1, which the  threshold ultimately determines if your test have passed or not.</li></ul><p><a href=\"https://deepeval.com/docs/getting-started?utm_source=GitHub\">Read our documentation</a> for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.</p><h2>Evaluating Nested Components</h2><p>If you wish to evaluate individual components within your LLM app, you need to run  evals - a powerful way to evaluate any component within an LLM system.</p><p>Simply trace \"components\" such as LLM calls, retrievers, tool calls, and agents within your LLM application using the  decorator to apply metrics on a component-level. Tracing with  is non-instrusive (learn more <a href=\"https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing\">here</a>) and helps you avoid rewriting your codebase just for evals:</p><pre><code>from deepeval.tracing import observe, update_current_span\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.dataset import Golden\nfrom deepeval.metrics import GEval\nfrom deepeval import evaluate\n\ncorrectness = GEval(name=\"Correctness\", criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])\n\n@observe(metrics=[correctness])\ndef inner_component():\n    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.\n    update_current_span(test_case=LLMTestCase(input=\"...\", actual_output=\"...\"))\n    return\n\n@observe\ndef llm_app(input: str):\n    inner_component()\n    return\n\nevaluate(observed_callback=llm_app, goldens=[Golden(input=\"Hi!\")])\n</code></pre><p>You can learn everything about component-level evaluations <a href=\"https://www.deepeval.com/docs/evaluation-component-level-llm-evals\">here.</a></p><h2>Evaluating Without Pytest Integration</h2><p>Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.</p><pre><code>from deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\nanswer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\ntest_case = LLMTestCase(\n    input=\"What if these shoes don't fit?\",\n    # Replace this with the actual output from your LLM application\n    actual_output=\"We offer a 30-day full refund at no extra costs.\",\n    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n)\nevaluate([test_case], [answer_relevancy_metric])\n</code></pre><p>DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:</p><pre><code>from deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\nanswer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\ntest_case = LLMTestCase(\n    input=\"What if these shoes don't fit?\",\n    # Replace this with the actual output from your LLM application\n    actual_output=\"We offer a 30-day full refund at no extra costs.\",\n    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n)\n\nanswer_relevancy_metric.measure(test_case)\nprint(answer_relevancy_metric.score)\n# All metrics also offer an explanation\nprint(answer_relevancy_metric.reason)\n</code></pre><p>Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.</p><h2>Evaluating a Dataset / Test Cases in Bulk</h2><p>In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:</p><pre><code>import pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.dataset import EvaluationDataset\n\nfirst_test_case = LLMTestCase(input=\"...\", actual_output=\"...\", context=[\"...\"])\nsecond_test_case = LLMTestCase(input=\"...\", actual_output=\"...\", context=[\"...\"])\n\ndataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])\n\n@pytest.mark.parametrize(\n    \"test_case\",\n    dataset,\n)\ndef test_customer_chatbot(test_case: LLMTestCase):\n    hallucination_metric = HallucinationMetric(threshold=0.3)\n    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n    assert_test(test_case, [hallucination_metric, answer_relevancy_metric])\n</code></pre><pre><code># Run this in the CLI, you can also add an optional -n flag to run tests in parallel\ndeepeval test run test_&lt;filename&gt;.py -n 4\n</code></pre><p>Alternatively, although we recommend using , you can evaluate a dataset/test cases without using our Pytest integration:</p><pre><code>from deepeval import evaluate\n...\n\nevaluate(dataset, [answer_relevancy_metric])\n# or\ndataset.evaluate([answer_relevancy_metric])\n</code></pre><ol><li>Curate/annotate evaluation datasets on the cloud</li><li>Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best</li><li>Fine-tune metrics for custom results</li><li>Debug evaluation results via LLM traces</li><li>Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data</li></ol><p>Everything on Confident AI, including how to use Confident is available <a href=\"https://documentation.confident-ai.com/docs?utm_source=GitHub\">here</a>.</p><p>To begin, login from the CLI:</p><p>Follow the instructions to log in, create your account, and paste your API key into the CLI.</p><p>Now, run your test file again:</p><pre><code>deepeval test run test_chatbot.py\n</code></pre><p>You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!</p><p>Please read <a href=\"https://github.com/confident-ai/deepeval/raw/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for details on our code of conduct, and the process for submitting pull requests to us.</p><p>DeepEval is licensed under Apache 2.0 - see the <a href=\"https://github.com/confident-ai/deepeval/raw/main/LICENSE.md\">LICENSE.md</a> file for details.</p>","contentLength":9937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TapXWorld/ChinaTextbook","url":"https://github.com/TapXWorld/ChinaTextbook","date":1751423796,"author":"","guid":179515,"unread":true,"content":"<p>ËôΩÁÑ∂ÂõΩÂÜÖÊïôËÇ≤ÁΩëÁ´ôÂ∑≤Êèê‰æõÂÖçË¥πËµÑÊ∫êÔºå‰ΩÜÂ§ßÂ§öÊï∞ÊôÆÈÄö‰∫∫Ëé∑Âèñ‰ø°ÊÅØÁöÑÈÄîÂæÑ‰æùÁÑ∂ÂèóÈôê„ÄÇÊúâ‰∫õ‰∫∫Âà©Áî®Ëøô‰∏ÄÁÇπÔºåÂú®ÊüêÁ´ô‰∏äÈîÄÂîÆËøô‰∫õÂ∏¶ÊúâÁßÅ‰∫∫Ê∞¥Âç∞ÁöÑËµÑÊ∫ê„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàëËÆ°ÂàíÂ∞ÜËøô‰∫õËµÑÊ∫êÈõÜ‰∏≠Âπ∂ÂºÄÊ∫êÔºå‰ª•‰øÉËøõ‰πâÂä°ÊïôËÇ≤ÁöÑÊôÆÂèäÂíåÊ∂àÈô§Âú∞Âå∫Èó¥ÁöÑÊïôËÇ≤Ë¥´Âõ∞„ÄÇ</p><p>ËøòÊúâ‰∏Ä‰∏™ÊúÄÈáçË¶ÅÁöÑÂéüÂõ†ÊòØÔºåÂ∏åÊúõÊµ∑Â§ñÂçé‰∫∫ËÉΩÂ§üËÆ©Ëá™Â∑±ÁöÑÂ≠©Â≠êÁªßÁª≠‰∫ÜËß£ÂõΩÂÜÖÊïôËÇ≤„ÄÇ</p><p>Áî±‰∫é GitHub ÂØπÂçï‰∏™Êñá‰ª∂ÁöÑ‰∏ä‰º†ÊúâÊúÄÂ§ßÈôêÂà∂ÔºåË∂ÖËøá 100MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãíÁªù‰∏ä‰º†ÔºåË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰∏ä‰º†Êó∂‰ºöÊî∂Âà∞Ë≠¶Âëä„ÄÇÂõ†Ê≠§ÔºåÊñá‰ª∂Â§ßÂ∞èË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãÜÂàÜÊàêÊØè‰∏™ 35MB ÁöÑÂ§ö‰∏™Êñá‰ª∂„ÄÇ</p><ul></ul><p>Ë¶ÅÂêàÂπ∂Ëøô‰∫õË¢´ÊãÜÂàÜÁöÑÊñá‰ª∂ÔºåÊÇ®Âè™ÈúÄÊâßË°å‰ª•‰∏ãÊ≠•È™§(ÂÖ∂‰ªñÊìç‰ΩúÁ≥ªÁªüÂêåÁêÜ)Ôºö</p><ol><li>Â∞ÜÂêàÂπ∂Á®ãÂ∫è <code>mergePDFs-windows-amd64.exe</code> ‰∏ãËΩΩÂà∞ÂåÖÂê´ PDF Êñá‰ª∂ÁöÑÊñá‰ª∂Â§π‰∏≠„ÄÇ</li><li>Á°Æ‰øù <code>mergePDFs-windows-amd64.exe</code> ÂíåË¢´ÊãÜÂàÜÁöÑ PDF Êñá‰ª∂Âú®Âêå‰∏ÄÁõÆÂΩï‰∏ã„ÄÇ</li><li>ÂèåÂáª <code>mergePDFs-windows-amd64.exe</code> Á®ãÂ∫èÂç≥ÂèØËá™Âä®ÂÆåÊàêÊñá‰ª∂ÂêàÂπ∂„ÄÇ</li></ol><ul><li>mergePDFs-windows-amd64.exe</li></ul><ul><li>Â¶ÇÊûúÊÇ®‰Ωç‰∫éÂõΩÂ§ñÔºåÂíåÂÜÖÂú∞ÁΩëÁªúÈÄö‰ø°ÈÄüÂ∫¶ËæÉÊÖ¢ÔºåÂª∫ËÆÆ‰ΩøÁî®Êú¨Â≠òÂÇ®Â∫ìËøõË°åÁ≠æÂá∫„ÄÇ</li></ul><p>Â¶ÇÊûúËøô‰∏™È°πÁõÆÂ∏ÆÂä©ÊÇ®ÂÖçË¥πËé∑ÂèñÊïôËÇ≤ËµÑÊ∫êÔºåËØ∑ËÄÉËôëÊîØÊåÅÊàë‰ª¨Êé®ÂπøÂºÄÊîæÊïôËÇ≤ÁöÑÂä™ÂäõÔºÅÊÇ®ÁöÑÊçêÁåÆÂ∞ÜÂ∏ÆÂä©Êàë‰ª¨Áª¥Êä§ÂíåÊâ©Â±ïËøô‰∏™ËµÑÊ∫êÂ∫ì„ÄÇ</p><p>Â¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÊÇ®ÂèØ‰ª•Êâ´Êèè‰ª•‰∏ã‰∫åÁª¥Á†ÅËøõË°åÊçêËµ†Ôºö</p>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"onlook-dev/onlook","url":"https://github.com/onlook-dev/onlook","date":1751423796,"author":"","guid":179516,"unread":true,"content":"<p>The Cursor for Designers ‚Ä¢ An Open-Source Visual Vibecoding Editor ‚Ä¢ Visually build, style, and edit your React App with AI</p><p>Craft websites, prototypes, and designs with AI in Next.js + TailwindCSS. Make edits directly in the browser DOM with a visual editor. Design in realtime with code. An open-source alternative to Bolt.new, Lovable, V0, Replit Agent, Figma Make, Webflow, etc.</p><h3>üöß üöß üöß Onlook for Web is still under development üöß üöß üöß</h3><p>We're actively looking for contributors to help make Onlook for Web an incredible prompt-to-build experience. Check the <a href=\"https://github.com/onlook-dev/onlook/issues\">open issues</a> for a full list of proposed features (and known issues), and join our <a href=\"https://discord.gg/hERDfFZCsH\">Discord</a> to collaborate with hundreds of other builders.</p><h2>What you can do with Onlook:</h2><h3>Onlook for Desktop (aka Onlook Alpha)</h3><p>We're in early preview for Onlook Web. If you're looking for the downloadable desktop electron app, it's moved to <a href=\"https://github.com/onlook-dev/desktop\">Onlook Desktop</a>.</p><p>Onlook will run on any Next.js + TailwindCSS project, import your project into Onlook or start from scratch within the editor.</p><p>Use the AI chat to create or edit a project you're working on. At any time, you can always right-click an element to open up the exact location of the element in code.</p><img width=\"600\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4ad9f411-b172-4430-81ef-650f4f314666\"><p>Draw-in new divs and re-arrange them within their parent containers by dragging-and-dropping.</p><img width=\"600\" alt=\"image\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/insert-div.png\"><p>Preview the code side-by-side with your site design.</p><img width=\"600\" alt=\"image\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/code-connect.png\"><p>Use Onlook's editor toolbar to adjust Tailwind styles, directly manipulate objects, and experiment with layouts.</p><img width=\"600\" alt=\"image\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/text-styling.png\"><img width=\"676\" alt=\"architecture\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/architecture.png\"><ol><li>When you create an app, we load the code into a web container</li><li>The container runs and serves the code</li><li>Our editor receives the preview link and displays it in an iFrame</li><li>Our editor reads and indexes the code from the container</li><li>We instrument the code in order to map elements to their place in code</li><li>When the element is edited, we edit the element in our iFrame, then in code</li><li>Our AI chat also has code access and tools to understand and edit the code</li></ol><p>This architecture can theoretically scale to any language or framework that displays DOM elements declaratively (e.g. jsx/tsx/html). We are focused on making it work well with Next.js and TailwindCSS for now.</p><ul><li><a href=\"https://bun.sh/\">Bun</a> - Monorepo, runtime, bundler</li></ul><p>If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also <a href=\"https://github.com/onlook-dev/onlook/issues\">open issues</a>.</p><a href=\"https://github.com/onlook-dev/onlook/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=onlook-dev/onlook\"></a><p>Distributed under the Apache 2.0 License. See <a href=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/LICENSE.md\">LICENSE.md</a> for more information.</p>","contentLength":2340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NanmiCoder/MediaCrawler","url":"https://github.com/NanmiCoder/MediaCrawler","date":1751423796,"author":"","guid":179517,"unread":true,"content":"<p>Â∞èÁ∫¢‰π¶Á¨îËÆ∞ | ËØÑËÆ∫Áà¨Ëô´„ÄÅÊäñÈü≥ËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅÂø´ÊâãËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅB Á´ôËßÜÈ¢ë ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÂæÆÂçöÂ∏ñÂ≠ê ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÁôæÂ∫¶Ë¥¥ÂêßÂ∏ñÂ≠ê ÔΩú ÁôæÂ∫¶Ë¥¥ÂêßËØÑËÆ∫ÂõûÂ§çÁà¨Ëô´ | Áü•‰πéÈóÆÁ≠îÊñáÁ´†ÔΩúËØÑËÆ∫Áà¨Ëô´</p><blockquote><p>Êú¨‰ªìÂ∫ìÁöÑÊâÄÊúâÂÜÖÂÆπ‰ªÖ‰æõÂ≠¶‰π†ÂíåÂèÇËÄÉ‰πãÁî®ÔºåÁ¶ÅÊ≠¢Áî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇ‰ªª‰Ωï‰∫∫ÊàñÁªÑÁªá‰∏çÂæóÂ∞ÜÊú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÁî®‰∫éÈùûÊ≥ïÁî®ÈÄîÊàñ‰æµÁäØ‰ªñ‰∫∫ÂêàÊ≥ïÊùÉÁõä„ÄÇÊú¨‰ªìÂ∫ìÊâÄÊ∂âÂèäÁöÑÁà¨Ëô´ÊäÄÊúØ‰ªÖÁî®‰∫éÂ≠¶‰π†ÂíåÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫éÂØπÂÖ∂‰ªñÂπ≥Âè∞ËøõË°åÂ§ßËßÑÊ®°Áà¨Ëô´ÊàñÂÖ∂‰ªñÈùûÊ≥ïË°å‰∏∫„ÄÇÂØπ‰∫éÂõ†‰ΩøÁî®Êú¨‰ªìÂ∫ìÂÜÖÂÆπËÄåÂºïËµ∑ÁöÑ‰ªª‰ΩïÊ≥ïÂæãË¥£‰ªªÔºåÊú¨‰ªìÂ∫ì‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ‰ΩøÁî®Êú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÂç≥Ë°®Á§∫ÊÇ®ÂêåÊÑèÊú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊâÄÊúâÊù°Ê¨æÂíåÊù°‰ª∂„ÄÇ</p></blockquote><p>‰∏Ä‰∏™ÂäüËÉΩÂº∫Â§ßÁöÑÔºåÊîØÊåÅÂ∞èÁ∫¢‰π¶„ÄÅÊäñÈü≥„ÄÅÂø´Êâã„ÄÅBÁ´ô„ÄÅÂæÆÂçö„ÄÅË¥¥Âêß„ÄÅÁü•‰πéÁ≠â‰∏ªÊµÅÂπ≥Âè∞ÁöÑÂÖ¨ÂºÄ‰ø°ÊÅØÊäìÂèñ„ÄÇ</p><ul><li>ÔºöÂà©Áî®‰øùÁïôÁôªÂΩïÊÄÅÁöÑÊµèËßàÂô®‰∏ä‰∏ãÊñáÁéØÂ¢ÉÔºåÈÄöËøá JS Ë°®ËææÂºèËé∑ÂèñÁ≠æÂêçÂèÇÊï∞</li><li>ÔºöÊó†ÈúÄÈÄÜÂêëÂ§çÊùÇÁöÑÂä†ÂØÜÁÆóÊ≥ïÔºåÂ§ßÂπÖÈôç‰ΩéÊäÄÊúØÈó®Êßõ</li></ul><table><thead><tr></tr></thead><tbody></tbody></table><blockquote><p>üí° <strong>ÂºÄÊ∫ê‰∏çÊòìÔºåÂ¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ</strong></p></blockquote><p>Âú®ËøõË°å‰∏ã‰∏ÄÊ≠•Êìç‰Ωú‰πãÂâçÔºåËØ∑Á°Æ‰øùÁîµËÑë‰∏äÂ∑≤ÁªèÂÆâË£Ö‰∫Ü uvÔºö</p><ul><li>ÔºöÁªàÁ´ØËæìÂÖ•ÂëΩ‰ª§ ÔºåÂ¶ÇÊûúÊ≠£Â∏∏ÊòæÁ§∫ÁâàÊú¨Âè∑ÔºåËØÅÊòéÂ∑≤ÁªèÂÆâË£ÖÊàêÂäü</li><li>Ôºöuv ÊòØÁõÆÂâçÊúÄÂº∫ÁöÑ Python ÂåÖÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÈÄüÂ∫¶Âø´„ÄÅ‰æùËµñËß£ÊûêÂáÜÁ°Æ</li></ul><pre><code># ËøõÂÖ•È°πÁõÆÁõÆÂΩï\ncd MediaCrawler\n\n# ‰ΩøÁî® uv sync ÂëΩ‰ª§Êù•‰øùËØÅ python ÁâàÊú¨ÂíåÁõ∏ÂÖ≥‰æùËµñÂåÖÁöÑ‰∏ÄËá¥ÊÄß\nuv sync\n</code></pre><pre><code># ÂÆâË£ÖÊµèËßàÂô®È©±Âä®\nuv run playwright install\n</code></pre><blockquote><p>ÔºöMediaCrawler ÁõÆÂâçÂ∑≤ÁªèÊîØÊåÅ‰ΩøÁî® playwright ËøûÊé•‰Ω†Êú¨Âú∞ÁöÑ Chrome ÊµèËßàÂô®‰∫ÜÔºå‰∏Ä‰∫õÂõ†‰∏∫ Webdriver ÂØºËá¥ÁöÑÈóÆÈ¢òËøéÂàÉËÄåËß£‰∫Ü„ÄÇ</p><p>ÁõÆÂâçÂºÄÊîæ‰∫Ü  Âíå  Ëøô‰∏§‰∏™‰ΩøÁî® CDP ÁöÑÊñπÂºèËøûÊé•Êú¨Âú∞ÊµèËßàÂô®ÔºåÂ¶ÇÊúâÈúÄË¶ÅÔºåÊü•Áúã  ‰∏≠ÁöÑÈÖçÁΩÆÈ°π„ÄÇ</p></blockquote><pre><code># È°πÁõÆÈªòËÆ§ÊòØÊ≤°ÊúâÂºÄÂêØËØÑËÆ∫Áà¨ÂèñÊ®°ÂºèÔºåÂ¶ÇÈúÄËØÑËÆ∫ËØ∑Âú® config/base_config.py ‰∏≠ÁöÑ ENABLE_GET_COMMENTS ÂèòÈáè‰øÆÊîπ\n# ‰∏Ä‰∫õÂÖ∂‰ªñÊîØÊåÅÈ°πÔºå‰πüÂèØ‰ª•Âú® config/base_config.py Êü•ÁúãÂäüËÉΩÔºåÂÜôÁöÑÊúâ‰∏≠ÊñáÊ≥®Èáä\n\n# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÂÖ≥ÈîÆËØçÊêúÁ¥¢Áõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠êÂπ∂Áà¨ÂèñÂ∏ñÂ≠ê‰ø°ÊÅØ‰∏éËØÑËÆ∫\nuv run main.py --platform xhs --lt qrcode --type search\n\n# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÊåáÂÆöÁöÑÂ∏ñÂ≠êIDÂàóË°®Ëé∑ÂèñÊåáÂÆöÂ∏ñÂ≠êÁöÑ‰ø°ÊÅØ‰∏éËØÑËÆ∫‰ø°ÊÅØ\nuv run main.py --platform xhs --lt qrcode --type detail\n\n# ÊâìÂºÄÂØπÂ∫îAPPÊâ´‰∫åÁª¥Á†ÅÁôªÂΩï\n\n# ÂÖ∂‰ªñÂπ≥Âè∞Áà¨Ëô´‰ΩøÁî®Á§∫‰æãÔºåÊâßË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§Êü•Áúã\nuv run main.py --help\n</code></pre><ul><li>ÔºöÊîØÊåÅÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ì MySQL ‰∏≠‰øùÂ≠òÔºàÈúÄË¶ÅÊèêÂâçÂàõÂª∫Êï∞ÊçÆÂ∫ìÔºâ \n  <ul><li>ÊâßË°å  ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìË°®ÁªìÊûÑÔºàÂè™Âú®È¶ñÊ¨°ÊâßË°åÔºâ</li></ul></li><li>ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ CSV ‰∏≠Ôºà ÁõÆÂΩï‰∏ãÔºâ</li><li>ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ JSON ‰∏≠Ôºà ÁõÆÂΩï‰∏ãÔºâ</li></ul><blockquote><p>Â¶ÇÊûúÊÉ≥Âø´ÈÄüÂÖ•Èó®ÂíåÂ≠¶‰π†ËØ•È°πÁõÆÁöÑ‰ΩøÁî®„ÄÅÊ∫êÁ†ÅÊû∂ÊûÑËÆæËÆ°Á≠â„ÄÅÂ≠¶‰π†ÁºñÁ®ãÊäÄÊúØ„ÄÅ‰∫¶ÊàñËÄÖÊÉ≥‰∫ÜËß£MediaCrawlerProÁöÑÊ∫ê‰ª£Á†ÅËÆæËÆ°ÂèØ‰ª•Áúã‰∏ãÊàëÁöÑÁü•ËØÜ‰ªòË¥πÊ†èÁõÆ„ÄÇ</p></blockquote><p>Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºåËÆ©Êõ¥Â§öÁöÑ‰∫∫ÁúãÂà∞ MediaCrawlerÔºÅ</p><a href=\"https://www.swiftproxy.net/?ref=nanmi\"><img src=\"https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_5.png\"> **Swiftproxy** - 90M+ ÂÖ®ÁêÉÈ´òË¥®ÈáèÁ∫ØÂáÄ‰ΩèÂÆÖIPÔºåÊ≥®ÂÜåÂèØÈ¢ÜÂÖçË¥π 500MB ÊµãËØïÊµÅÈáèÔºåÂä®ÊÄÅÊµÅÈáè‰∏çËøáÊúüÔºÅ &gt; ‰∏ìÂ±ûÊäòÊâ£Á†ÅÔºö**GHB5** Á´ã‰∫´‰πùÊäò‰ºòÊÉ†ÔºÅ </a><p>Êàê‰∏∫ËµûÂä©ËÄÖÔºåÂèØ‰ª•Â∞ÜÊÇ®ÁöÑ‰∫ßÂìÅÂ±ïÁ§∫Âú®ËøôÈáåÔºåÊØèÂ§©Ëé∑ÂæóÂ§ßÈáèÊõùÂÖâÔºÅ</p><ul></ul><div><p>Êú¨È°πÁõÆÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÊú¨È°πÁõÆ‚ÄùÔºâÊòØ‰Ωú‰∏∫‰∏Ä‰∏™ÊäÄÊúØÁ†îÁ©∂‰∏éÂ≠¶‰π†Â∑•ÂÖ∑ËÄåÂàõÂª∫ÁöÑÔºåÊó®Âú®Êé¢Á¥¢ÂíåÂ≠¶‰π†ÁΩëÁªúÊï∞ÊçÆÈááÈõÜÊäÄÊúØ„ÄÇÊú¨È°πÁõÆ‰∏ìÊ≥®‰∫éËá™Â™í‰ΩìÂπ≥Âè∞ÁöÑÊï∞ÊçÆÁà¨ÂèñÊäÄÊúØÁ†îÁ©∂ÔºåÊó®Âú®Êèê‰æõÁªôÂ≠¶‰π†ËÄÖÂíåÁ†îÁ©∂ËÄÖ‰Ωú‰∏∫ÊäÄÊúØ‰∫§ÊµÅ‰πãÁî®„ÄÇ</p><p>Êú¨È°πÁõÆÂºÄÂèëËÄÖÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÂºÄÂèëËÄÖ‚ÄùÔºâÈÉëÈáçÊèêÈÜíÁî®Êà∑Âú®‰∏ãËΩΩ„ÄÅÂÆâË£ÖÂíå‰ΩøÁî®Êú¨È°πÁõÆÊó∂Ôºå‰∏•Ê†ºÈÅµÂÆà‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁΩëÁªúÂÆâÂÖ®Ê≥ï„Äã„ÄÅ„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÂèçÈó¥Ë∞çÊ≥ï„ÄãÁ≠âÊâÄÊúâÈÄÇÁî®ÁöÑÂõΩÂÆ∂Ê≥ïÂæãÂíåÊîøÁ≠ñ„ÄÇÁî®Êà∑Â∫îËá™Ë°åÊâøÊãÖ‰∏ÄÂàáÂõ†‰ΩøÁî®Êú¨È°πÁõÆËÄåÂèØËÉΩÂºïËµ∑ÁöÑÊ≥ïÂæãË¥£‰ªª„ÄÇ</p><p>Êú¨È°πÁõÆ‰∏•Á¶ÅÁî®‰∫é‰ªª‰ΩïÈùûÊ≥ïÁõÆÁöÑÊàñÈùûÂ≠¶‰π†„ÄÅÈùûÁ†îÁ©∂ÁöÑÂïÜ‰∏öË°å‰∏∫„ÄÇÊú¨È°πÁõÆ‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ï‰æµÂÖ•‰ªñ‰∫∫ËÆ°ÁÆóÊú∫Á≥ªÁªüÔºå‰∏çÂæóÁî®‰∫é‰ªª‰Ωï‰æµÁäØ‰ªñ‰∫∫Áü•ËØÜ‰∫ßÊùÉÊàñÂÖ∂‰ªñÂêàÊ≥ïÊùÉÁõäÁöÑË°å‰∏∫„ÄÇÁî®Êà∑Â∫î‰øùËØÅÂÖ∂‰ΩøÁî®Êú¨È°πÁõÆÁöÑÁõÆÁöÑÁ∫ØÂ±û‰∏™‰∫∫Â≠¶‰π†ÂíåÊäÄÊúØÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ïÊ¥ªÂä®„ÄÇ</p><p>ÂºÄÂèëËÄÖÂ∑≤Â∞ΩÊúÄÂ§ßÂä™ÂäõÁ°Æ‰øùÊú¨È°πÁõÆÁöÑÊ≠£ÂΩìÊÄßÂèäÂÆâÂÖ®ÊÄßÔºå‰ΩÜ‰∏çÂØπÁî®Êà∑‰ΩøÁî®Êú¨È°πÁõÆÂèØËÉΩÂºïËµ∑ÁöÑ‰ªª‰ΩïÂΩ¢ÂºèÁöÑÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±ÊâøÊãÖË¥£‰ªª„ÄÇÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÁî±‰∫é‰ΩøÁî®Êú¨È°πÁõÆËÄåÂØºËá¥ÁöÑ‰ªª‰ΩïÊï∞ÊçÆ‰∏¢Â§±„ÄÅËÆæÂ§áÊçüÂùè„ÄÅÊ≥ïÂæãËØâËÆºÁ≠â„ÄÇ</p><p>Êú¨È°πÁõÆÁöÑÁü•ËØÜ‰∫ßÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÊú¨È°πÁõÆÂèóÂà∞Ëëó‰ΩúÊùÉÊ≥ïÂíåÂõΩÈôÖËëó‰ΩúÊùÉÊù°Á∫¶‰ª•ÂèäÂÖ∂‰ªñÁü•ËØÜ‰∫ßÊùÉÊ≥ïÂæãÂíåÊù°Á∫¶ÁöÑ‰øùÊä§„ÄÇÁî®Êà∑Âú®ÈÅµÂÆàÊú¨Â£∞ÊòéÂèäÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÁöÑÂâçÊèê‰∏ãÔºåÂèØ‰ª•‰∏ãËΩΩÂíå‰ΩøÁî®Êú¨È°πÁõÆ„ÄÇ</p><p>ÂÖ≥‰∫éÊú¨È°πÁõÆÁöÑÊúÄÁªàËß£ÈáäÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÂºÄÂèëËÄÖ‰øùÁïôÈöèÊó∂Êõ¥ÊîπÊàñÊõ¥Êñ∞Êú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊùÉÂà©ÔºåÊÅï‰∏çÂè¶Ë°åÈÄöÁü•„ÄÇ</p></div><p>ÊÑüË∞¢ JetBrains ‰∏∫Êú¨È°πÁõÆÊèê‰æõÂÖçË¥πÁöÑÂºÄÊ∫êËÆ∏ÂèØËØÅÊîØÊåÅÔºÅ</p><a href=\"https://www.jetbrains.com/?from=MediaCrawler\"><img src=\"https://www.jetbrains.com/company/brand/img/jetbrains_logo.png\" width=\"100\" alt=\"JetBrains\"></a>","contentLength":4915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ColorlibHQ/AdminLTE","url":"https://github.com/ColorlibHQ/AdminLTE","date":1751423796,"author":"","guid":179518,"unread":true,"content":"<p>AdminLTE - Free admin dashboard template based on Bootstrap 5</p><p> is a fully responsive administration template. Based on  framework and also the JavaScript plugins. Highly customizable and easy to use. Fits many screen resolutions from small mobile devices to large desktops.</p><p><strong>Production Deployment &amp; Cross-Platform Compatibility</strong> - This release resolves critical production deployment issues:</p><ul><li> - Resolved CSS/JS path issues, sidebar navigation, and image loading in all deployment scenarios</li><li> - Automatic relative path calculation works for root deployment, sub-folders, and CDN hosting</li><li> - Eliminated rtlcss interference with standard LTR production builds</li><li> - Bootstrap 5.3.7, Bootstrap Icons 1.13.1, OverlayScrollbars 2.11.0</li><li> - Fixed all CDN integrity mismatches and runtime issues</li><li> - Perfect compatibility with traditional hosting and modern static platforms</li></ul><ul><li>‚úÖ Development and production environments now behave identically</li><li>‚úÖ Images, CSS, and JavaScript load correctly in any deployment structure</li><li>‚úÖ Sidebar navigation displays properly with badges and arrow indicators</li><li>‚úÖ All CDN resources load without console errors</li><li>‚úÖ Complete production build included in repository for easy deployment</li></ul><h2>Looking for Premium Templates?</h2><p>AdminLTE.io just opened a new premium templates page. Hand picked to ensure the best quality and the most affordable prices. Visit <a href=\"https://adminlte.io/premium\">https://adminlte.io/premium</a> for more information.</p><p> has been carefully coded with clear comments in all of its JS, SCSS and HTML files. SCSS has been used to increase code customizability.</p><p>To start developing with AdminLTE:</p><ol><li> Files auto-compile and refresh on changes</li></ol><ol><li><em>(includes linting and optimization)</em></li><li><em>(faster for development/testing)</em></li></ol><ul><li> - Start development server with file watching</li><li> - Build all assets for development</li><li> - Full production build with linting and bundlewatch</li><li> - Run all linters (JS, CSS, docs, lockfile)</li><li> - Build CSS only</li><li> - Build JavaScript only</li></ul><p>AdminLTE supports all modern browsers with the latest Bootstrap 5.3.7:</p><ul></ul><ul><li>First thing first, you should have bit knowledge about NodeJS.</li><li>Install NodeJS LTS version.</li><li>Clone this Repository to your machine and change to  branch.</li><li>In cli/bash run  it will install dependency from .</li><li>After installation completes, run </li><li>Cool, Send your changes in PR to  branch.</li></ul><p>AdminLTE is an open source project by <a href=\"https://adminlte.io\">AdminLTE.io</a> that is licensed under <a href=\"https://opensource.org/licenses/MIT\">MIT</a>. AdminLTE.io reserves the right to change the license of future releases.</p>","contentLength":2385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"actualbudget/actual","url":"https://github.com/actualbudget/actual","date":1751337922,"author":"","guid":177070,"unread":true,"content":"<p>A local-first personal finance app</p><p>Actual is a local-first personal finance tool. It is 100% free and open-source, written in NodeJS, it has a synchronization element so that all your changes can move between devices without any heavy lifting.</p><p>If you are interested in contributing, or want to know how development works, see our <a href=\"https://actualbudget.org/docs/contributing/\">contributing</a> document we would love to have you.</p><p>Want to say thanks? Click the ‚≠ê at the top of the page.</p><p>There are four ways to deploy Actual:</p><h2>Ready to Start Budgeting?</h2><h3>Are you new to budgeting or want to start fresh?</h3><p>Check out the community's <a href=\"https://actualbudget.org/docs/getting-started/starting-fresh\">Starting Fresh</a> guide so you can quickly get up and running!</p><h3>Are you migrating from other budgeting apps?</h3><p>Check out the community's <a href=\"https://actualbudget.org/docs/migration/\">Migration</a> guide to start jumping on the Actual Budget train!</p><p>We have a wide range of documentation on how to use Actual, this is all available in our <a href=\"https://actualbudget.org/docs\">Community Documentation</a>, this includes topics on Budgeting, Account Management, Tips &amp; Tricks and some documentation for developers.</p><p>The Actual app is split up into a few packages:</p><ul><li>loot-core - The core application that runs on any platform</li><li>desktop-client - The desktop UI</li><li>desktop-electron - The desktop app</li></ul><p>Current feature requests can be seen <a href=\"https://github.com/actualbudget/actual/issues?q=is%3Aissue+label%3A%22needs+votes%22+sort%3Areactions-%2B1-desc\">here</a>. Vote for your favorite requests by reacting  to the top comment of the request.</p><p>To add new feature requests, open a new Issue of the \"Feature Request\" type.</p><p>Make Actual Budget accessible to more people by helping with the <a href=\"https://actualbudget.org/docs/contributing/i18n/\">Internationalization</a> of Actual. We are using a crowd sourcing tool to manage the translations, see our <a href=\"https://hosted.weblate.org/projects/actualbudget/\">Weblate Project</a>. Weblate proudly supports open-source software projects through their <a href=\"https://weblate.org/en/hosting/#libre\">Libre plan</a>.</p><a href=\"https://hosted.weblate.org/engage/actualbudget/\"><img src=\"https://hosted.weblate.org/widget/actualbudget/actual/287x66-grey.png\" alt=\"Translation status\"></a><p>Thanks to our wonderful sponsors who make Actual Budget possible!</p>","contentLength":1677,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nextcloud/all-in-one","url":"https://github.com/nextcloud/all-in-one","date":1751337922,"author":"","guid":177071,"unread":true,"content":"<p>üì¶ The official Nextcloud installation method. Provides easy deployment and maintenance with most features included in this one Nextcloud instance.</p><p>The official Nextcloud installation method. Nextcloud AIO provides easy deployment and maintenance with most features included in this one Nextcloud instance.</p><ul><li>High performance backend for Nextcloud Files</li><li>Nextcloud Office (optional)</li><li>High performance backend for Nextcloud Talk and TURN-server (optional)</li><li>Nextcloud Talk Recording-server (optional)</li><li>Imaginary (optional, for previews of heic, heif, illustrator, pdf, svg, tiff and webp)</li><li>ClamAV (optional, Antivirus backend for Nextcloud)</li><li>Fulltextsearch (optional)</li></ul><blockquote><p>[!WARNING] You should first make sure that you are not using docker installed via snap. You can check this by running <code>sudo docker info | grep \"Docker Root Dir\" | grep \"/var/snap/docker/\"</code>. If the output should contain the mentioned string , you should first uninstall docker snap via  and then follow the instructions below. ‚ö†Ô∏è Attention: only run the command if this is a clean new docker installation and you are not running any service already using this.</p></blockquote><blockquote><p>[!NOTE] The following instructions are meant for installations without a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else) already being in place. If you want to run AIO behind a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else), see the <a href=\"https://github.com/nextcloud/all-in-one/raw/main/reverse-proxy.md\">reverse proxy documentation</a>. Also, the instructions below are especially meant for Linux. For macOS see <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-run-aio-on-macos\">this</a>, for Windows see <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-run-aio-on-windows\">this</a> and for Synology see <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-run-aio-on-synology-dsm\">this</a>.</p></blockquote><blockquote><p>[!WARNING] You could use the convenience script below to install docker. However we recommend to not blindly download and execute scripts as sudo. But if you feel like it, you can of course use it. See below:</p></blockquote><ol start=\"2\"><li><p>Run the command below in order to start the container on Linux and without a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else) already in place:</p><pre><code># For Linux and without a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else) already in place:\nsudo docker run \\\n--init \\\n--sig-proxy=false \\\n--name nextcloud-aio-mastercontainer \\\n--restart always \\\n--publish 80:80 \\\n--publish 8080:8080 \\\n--publish 8443:8443 \\\n--volume nextcloud_aio_mastercontainer:/mnt/docker-aio-config \\\n--volume /var/run/docker.sock:/var/run/docker.sock:ro \\\nghcr.io/nextcloud-releases/all-in-one:latest\n</code></pre><p>Note: You may be interested in adjusting Nextcloud‚Äôs datadir to store the files in a different location than the default docker volume. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p></li><li><p>After the initial startup, you should be able to open the Nextcloud AIO Interface now on port 8080 of this server. E.g. <code>https://ip.address.of.this.server:8080</code> ‚ö†Ô∏è  do always use an ip-address if you access this port and not a domain as HSTS might block access to it later! (It is also expected that this port uses a self-signed certificate due to security concerns which you need to accept in your browser)<p> If your firewall/router has port 80 and 8443 open/forwarded and you point a domain to your server, you can get a valid certificate automatically by opening the Nextcloud AIO Interface via:</p><code>https://your-domain-that-points-to-this-server.tld:8443</code></p></li><li><p>Please do not forget to open port  and  in your firewall/router for the Talk container!</p></li></ol><h3>Where can I find additional documentation?</h3><p>Nextcloud AIO is inspired by projects like Portainer that manage the docker daemon by talking to it through the docker socket directly. This concept allows a user to install only one container with a single command that does the heavy lifting of creating and managing all containers that are needed in order to provide a Nextcloud installation with most features included. It also makes updating a breeze and is not bound to the host system (and its slow updates) anymore as everything is in containers. Additionally, it is very easy to handle from a user perspective because a simple interface for managing your Nextcloud AIO installation is provided.</p><p>See <a href=\"https://github.com/nextcloud/all-in-one/issues/5251\">this issue</a> for a list of feature requests that need help by contributors.</p><h3>How many users are possible?</h3><h3>Are reverse proxies supported?</h3><h3>Which ports are mandatory to be open in your firewall/router?</h3><p>Only those (if you access the Mastercontainer Interface internally via port 8080):</p><ul><li> for the Apache container</li><li> if you want to enable http3 for the Apache container</li><li> and  for the Talk container</li></ul><h3>Explanation of used ports</h3><ul><li>: Mastercontainer Interface with self-signed certificate (works always, also if only access via IP-address is possible, e.g. <code>https://ip.address.of.this.server:8080/</code>) ‚ö†Ô∏è  do always use an ip-address if you access this port and not a domain as HSTS might block access to it later! (It is also expected that this port uses a self-signed certificate due to security concerns which you need to accept in your browser)</li><li>: redirects to Nextcloud (is used for getting the certificate via ACME http-challenge for the Mastercontainer)</li><li>: Mastercontainer Interface with valid certificate (only works if port 80 and 8443 are open/forwarded in your firewall/router and you point a domain to your server. It generates a valid certificate then automatically and access via e.g. <code>https://public.domain.com:8443/</code> is possible.)</li><li>: will be used by the Apache container later on and needs to be open/forwarded in your firewall/router</li><li>: will be used by the Apache container later on and needs to be open/forwarded in your firewall/router if you want to enable http3</li><li> and : will be used by the Turnserver inside the Talk container and needs to be open/forwarded in your firewall/router</li></ul><h3>Notes on Cloudflare (proxy/tunnel)</h3><p>Since Cloudflare Proxy/Tunnel comes with a lot of limitations which are listed below, it is rather recommended to switch to <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a> if possible.</p><ul><li>Cloudflare Proxy and Cloudflare Tunnel both require Cloudflare to perform TLS termination on their side and thus decrypt all the traffic on their infrastructure. This is a privacy concern and you will need to look for other solutions if it's unacceptable for you.</li><li>Using Cloudflare Tunnel might potentially slow down Nextcloud since local access via the configured domain is not possible because TLS termination is in that case offloaded to Cloudflare's infrastructure. There is no way to disable this behavior in Cloudflare Tunnel.</li><li>Cloudflare only supports uploading files up to 100&nbsp;MB in the free plan, if you try to upload bigger files you will get an error (413 - Payload Too Large) if no chunking is used (e.g. for public uploads in the web, or if chunks are configured to be bigger than 100 MB in the clients or the web). If you need to upload bigger files, you need to disable the proxy option in your DNS settings. Note that this will both disable Cloudflare DDoS protection and Cloudflare Tunnel as these services require the proxy option to be enabled.</li><li>Cloudflare only allows a max timeout of 100s for requests which is not configurable. This means that any server-side processing e.g. for assembling chunks for big files during upload that take longer than 100s will simply not work. See <a href=\"https://github.com/nextcloud/server/issues/19223\">https://github.com/nextcloud/server/issues/19223</a>. If you need to upload big files reliably, you need to disable the proxy option in your DNS settings. Note that this will both disable Cloudflare DDoS protection and Cloudflare Tunnel as these services require the proxy option to be enabled.</li><li>It is known that the in AIO included collabora (Nextcloud Office) does not work out of the box behind Cloudflare. To make it work, you need to add all <a href=\"https://www.cloudflare.com/ips/\">Cloudflare IP-ranges</a> to the wopi-allowlist in <code>https://yourdomain.com/settings/admin/richdocuments</code></li><li>The built-in turn-server for Nextcloud Talk will not work behind Cloudflare Tunnel since it needs a separate port (by default 3478 or as chosen) available on the same domain. If you still want to use the feature, you will need to install your own turnserver or use a publicly available one and adjust and test your stun and turn settings in <code>https://yourdomain.com/settings/admin/talk</code>.</li><li>If you get an error in Nextcloud's admin overview that the HSTS header is not set correctly, you might need to enable it in Cloudflare manually.</li><li>If you are using AIO's built-in Reverse Proxy and don't use your own, then the certificate issuing may possibly not work out-of-the-box because Cloudflare might block the attempt. In that case you need to disable the Proxy feature at least temporarily in order to make it work. Note that this isn't an option if you need Cloudflare Tunnel as disabling the proxy would also disable Cloudflare Tunnel which would in turn make your server unreachable for the verification. See <a href=\"https://github.com/nextcloud/all-in-one/discussions/1101\">https://github.com/nextcloud/all-in-one/discussions/1101</a>.</li></ul><h3>How to run Nextcloud behind a Cloudflare Tunnel?</h3><p>Although it does not seems like it is the case but from AIO perspective a Cloudflare Tunnel works like a reverse proxy. So please follow the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/reverse-proxy.md\">reverse proxy documentation</a> where is documented how to make it run behind a Cloudflare Tunnel. However please see the <a href=\"https://github.com/nextcloud/all-in-one#notes-on-cloudflare-proxytunnel\">caveats</a> before proceeding.</p><h3>How to run Nextcloud via Tailscale?</h3><h3>How to get Nextcloud running using the ACME DNS-challenge?</h3><h3>How to run Nextcloud locally? No domain wanted, or wanting intranet access within your LAN.</h3><p>If you do not want to open Nextcloud to the public internet, you may have a look at the following documentation on how to set it up locally: <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/local-instance.md\">local-instance.md</a>, but keep in mind you're still required to have https working properly.</p><h3>Can I use an ip-address for Nextcloud instead of a domain?</h3><p>No and it will not be added. If you only want to run it locally, you may have a look at the following documentation: <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/local-instance.md\">local-instance.md</a>. Recommended is to use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a>.</p><h3>Can I run AIO offline or in an airgapped system?</h3><p>No. This is not possible and will not be added due to multiple reasons: update checks, app installs via app-store, downloading additional docker images on demand and more.</p><h3>Are self-signed certificates supported for Nextcloud?</h3><p>No and they will not be. If you want to run it locally, without opening Nextcloud to the public internet, please have a look at the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/local-instance.md\">local instance documentation</a>. Recommended is to use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a>.</p><h3>Can I use AIO with multiple domains?</h3><p>No and it will not be added. However you can use <a href=\"https://github.com/nextcloud/all-in-one/raw/main/multiple-instances.md\">this feature</a> in order to create multiple AIO instances, one for each domain.</p><h3>Are other ports than the default 443 for Nextcloud supported?</h3><p>No and they will not be. If port 443 and/or 80 is blocked for you, you may use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a> if you want to publish it online. If you already run a different service on port 443, please use a dedicated domain for Nextcloud and set it up correctly by following the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/reverse-proxy.md\">reverse proxy documentation</a>. However in all cases the Nextcloud interface will redirect you to port 443.</p><h3>Can I run Nextcloud in a subdirectory on my domain?</h3><p>No and it will not be added. Please use a dedicated (sub-)domain for Nextcloud and set it up correctly by following the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/reverse-proxy.md\">reverse proxy documentation</a>. Alternatively, you may use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a> if you want to publish it online.</p><h3>How can I access Nextcloud locally?</h3><p>Please note that local access is not possible if you are running AIO behind Cloudflare Tunnel since TLS proxying is in that case offloaded to Cloudflares infrastructure. You can fix this by setting up your own reverse proxy that handles TLS proxying locally and will make the steps below work.</p><p>Please make sure that if you are running AIO behind a reverse proxy, that the reverse proxy is configured to use port 443 on the server that runs it. Otherwise the steps below will not work.</p><p>Now that this is out of the way, the recommended way how to access Nextcloud locally, is to set up a local dns-server like a pi-hole and set up a custom dns-record for that domain that points to the internal ip-adddress of your server that runs Nextcloud AIO. Below are some guides:</p><h3>How to skip the domain validation?</h3><p>If you are completely sure that you've configured everything correctly and are not able to pass the domain validation, you may skip the domain validation by adding <code>--env SKIP_DOMAIN_VALIDATION=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used).</p><h3>How to resolve firewall problems with Fedora Linux, RHEL OS, CentOS, SUSE Linux and others?</h3><p>It is known that Linux distros that use <a href=\"https://firewalld.org\">firewalld</a> as their firewall daemon have problems with docker networks. In case the containers are not able to communicate with each other, you may change your firewalld to use the iptables backend by running:</p><pre><code>sudo sed -i 's/FirewallBackend=nftables/FirewallBackend=iptables/g' /etc/firewalld/firewalld.conf\nsudo systemctl restart firewalld docker\n</code></pre><p>Afterwards it should work.</p><h3>What can I do to fix the internal or reserved ip-address error?</h3><p>If you get an error during the domain validation which states that your ip-address is an internal or reserved ip-address, you can fix this by first making sure that your domain indeed has the correct public ip-address that points to the server and then adding <code>--add-host yourdomain.com:&lt;public-ip-address&gt;</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) which will allow the domain validation to work correctly. And so that you know: even if the  record of your domain should change over time, this is no problem since the mastercontainer will not make any attempt to access the chosen domain after the initial domain validation.</p><h3>Which CPU architectures are supported?</h3><p>You can check this on Linux by running: </p><ul></ul><h3>Disrecommended VPS providers</h3><ul><li> Strato VPS using Virtuozzo caused problems though ones from Q3 2023 and later should work. If your VPS has a  file and a low  limit set in it your server will likely misbehave once it reaches this limit which is very quickly reached by AIO, see <a href=\"https://github.com/nextcloud/all-in-one/discussions/1747#discussioncomment-4716164\">here</a>.</li><li>Hostingers VPS seem to miss a specific Kernel feature which is required for AIO to run correctly. See <a href=\"https://help.nextcloud.com/t/help-installing-nc-via-aio-on-vps/153956\">here</a>.</li></ul><p>In general recommended VPS are those that are KVM/non-virtualized as Docker should work best on them.</p><ul><li>SD-cards are disrecommended for AIO since they cripple the performance and they are not meant for many write operations which is needed for the database and other parts</li><li>SSD storage is recommended</li><li>HDD storage should work as well but is of course much slower than SSD storage</li></ul><h3>Are there known problems when SELinux is enabled?</h3><p>Yes. If SELinux is enabled, you might need to add the <code>--security-opt label:disable</code> option to the docker run command of the mastercontainer in order to allow it to access the docker socket (or <code>security_opt: [\"label:disable\"]</code> in compose.yaml). See <a href=\"https://github.com/nextcloud/all-in-one/discussions/485\">https://github.com/nextcloud/all-in-one/discussions/485</a></p><h3>How to change the default location of Nextcloud's Datadir?</h3><blockquote><p>[!WARNING] Do not set or adjust this value after the initial Nextcloud installation is done! If you still want to do it afterwards, see <a href=\"https://github.com/nextcloud/all-in-one/discussions/890#discussioncomment-3089903\">this</a> on how to do it.</p></blockquote><p>You can configure the Nextcloud container to use a specific directory on your host as data directory. You can do so by adding the environmental variable  to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used). Allowed values for that variable are strings that start with  and are not equal to . The chosen directory or volume will then be mounted to  inside the container.</p><ul><li>An example for Linux is <code>--env NEXTCLOUD_DATADIR=\"/mnt/ncdata\"</code>. ‚ö†Ô∏è Please note: If you should be using an external BTRFS drive that is mounted to , make sure to choose a subfolder like e.g.  as datadir, since the root folder is not suited as datadir in that case. See <a href=\"https://github.com/nextcloud/all-in-one/discussions/2696\">https://github.com/nextcloud/all-in-one/discussions/2696</a>.</li><li>On macOS it might be <code>--env NEXTCLOUD_DATADIR=\"/var/nextcloud-data\"</code></li><li>For Synology it may be <code>--env NEXTCLOUD_DATADIR=\"/volume1/docker/nextcloud/data\"</code>.</li><li>On Windows it might be <code>--env NEXTCLOUD_DATADIR=\"/run/desktop/mnt/host/c/ncdata\"</code>. (This path is equivalent to  on your Windows host so you need to translate the path accordingly. Hint: the path that you enter needs to start with . Append to that the exact location on your windows host, e.g.  which is equivalent to .) ‚ö†Ô∏è : This does not work with external drives like USB or network drives and only with internal drives like SATA or NVME drives.</li><li>Another option is to provide a specific volume name here with: <code>--env NEXTCLOUD_DATADIR=\"nextcloud_aio_nextcloud_datadir\"</code>. This volume needs to be created beforehand manually by you in order to be able to use it. e.g. on Windows with: <pre><code>docker volume create ^\n--driver local ^\n--name nextcloud_aio_nextcloud_datadir ^\n-o device=\"/host_mnt/e/your/data/path\" ^\n-o type=\"none\" ^\n-o o=\"bind\"\n</code></pre> In this example, it would mount  into the volume so for a different location you need to adjust <code>/host_mnt/e/your/data/path</code> accordingly.</li></ul><h3>How to store the files/installation on a separate drive?</h3><p>‚ö†Ô∏è If you encounter errors from richdocuments in your Nextcloud logs, check in your Collabora container if the message \"Capabilities are not set for the coolforkit program.\" appears. If so, follow these steps:</p><ol><li>Stop all the containers from the AIO Interface.</li><li>Go to your terminal and delete the Collabora container (<code>docker rm nextcloud-aio-collabora</code>) AND the Collabora image (<code>docker image rm nextcloud/aio-collabora</code>).</li><li>You might also want to prune your Docker () (no data will be lost).</li><li>Restart your containers from the AIO Interface.</li></ol><p>This should solve the problem.</p><h3>How to allow the Nextcloud container to access directories on the host?</h3><p>By default, the Nextcloud container is confined and cannot access directories on the host OS. You might want to change this when you are planning to use local external storage in Nextcloud to store some files outside the data directory and can do so by adding the environmental variable  to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used). Allowed values for that variable are strings that start with  and are not equal to .</p><ul><li>Two examples for Linux are <code>--env NEXTCLOUD_MOUNT=\"/mnt/\"</code> and <code>--env NEXTCLOUD_MOUNT=\"/media/\"</code>.</li><li>On macOS it might be <code>--env NEXTCLOUD_MOUNT=\"/Volumes/your_drive/\"</code></li><li>For Synology it may be <code>--env NEXTCLOUD_MOUNT=\"/volume1/\"</code>.</li><li>On Windows it might be <code>--env NEXTCLOUD_MOUNT=\"/run/desktop/mnt/host/d/your-folder/\"</code>. (This path is equivalent to  on your Windows host so you need to translate the path accordingly. Hint: the path that you enter needs to start with . Append to that the exact location on your windows host, e.g.  which is equivalent to .) ‚ö†Ô∏è : This does not work with external drives like USB or network drives and only with internal drives like SATA or NVME drives.</li></ul><p>After using this option, please make sure to apply the correct permissions to the directories that you want to use in Nextcloud. E.g. <code>sudo chown -R 33:0 /mnt/your-drive-mountpoint</code> and <code>sudo chmod -R 750 /mnt/your-drive-mountpoint</code> should make it work on Linux when you have used <code>--env NEXTCLOUD_MOUNT=\"/mnt/\"</code>. On Windows you could do this e.g. with <code>docker exec -it nextcloud-aio-nextcloud chown -R 33:0 /run/desktop/mnt/host/d/your-folder/</code> and <code>docker exec -it nextcloud-aio-nextcloud chmod -R 750 /run/desktop/mnt/host/d/your-folder/</code>.</p><p>You can then navigate to <code>https://your-nc-domain.com/settings/apps/disabled</code>, activate the external storage app, navigate to <code>https://your-nc-domain.com/settings/admin/externalstorages</code> and add a local external storage directory that will be accessible inside the container at the same place that you've entered. E.g. <code>/mnt/your-drive-mountpoint</code> will be mounted to <code>/mnt/your-drive-mountpoint</code> inside the container, etc.</p><p>Be aware though that these locations will not be covered by the built-in backup solution - but you can add further Docker volumes and host paths that you want to back up after the initial backup is done.</p><blockquote><p>[!NOTE] If you can't see the type \"local storage\" in the external storage admin options, a restart of the containers from the AIO interface may be required.</p></blockquote><h3>How to adjust the Talk port?</h3><p>By default will the talk container use port  and  for connections. This should be set to something higher than 1024! You can adjust the port by adding e.g.  to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and adjusting the port to your desired value. Best is to use a port over 1024, so e.g. 3479 to not run into this: <a href=\"https://github.com/nextcloud/all-in-one/discussions/2517\">https://github.com/nextcloud/all-in-one/discussions/2517</a></p><h3>How to adjust the upload limit for Nextcloud?</h3><p>By default, public uploads to Nextcloud are limited to a max of 16G (logged in users can upload much bigger files using the webinterface or the mobile/desktop clients, since chunking is used in that case). You can adjust the upload limit by providing <code>--env NEXTCLOUD_UPLOAD_LIMIT=16G</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must start with a number and end with  e.g. .</p><h3>How to adjust the max execution time for Nextcloud?</h3><p>By default, uploads to Nextcloud are limited to a max of 3600s. You can adjust the upload time limit by providing <code>--env NEXTCLOUD_MAX_TIME=3600</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a number e.g. .</p><h3>How to adjust the PHP memory limit for Nextcloud?</h3><p>By default, each PHP process in the Nextcloud container is limited to a max of 512 MB. You can adjust the memory limit by providing <code>--env NEXTCLOUD_MEMORY_LIMIT=512M</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must start with a number and end with  e.g. .</p><h3>How to change the Nextcloud apps that are installed on the first startup?</h3><p>You might want to adjust the Nextcloud apps that are installed upon the first startup of the Nextcloud container. You can do so by adding <code>--env NEXTCLOUD_STARTUP_APPS=\"deck twofactor_totp tasks calendar contacts notes\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a string with small letters a-z, 0-9, spaces and hyphens or '_'. You can disable shipped and by default enabled apps by adding a hyphen in front of the appid. E.g. .</p><h3>How to add OS packages permanently to the Nextcloud container?</h3><p>Some Nextcloud apps require additional external dependencies that must be bundled within Nextcloud container in order to work correctly. As we cannot put each and every dependency for all apps into the container - as this would make the project quickly unmaintainable - there is an official way in which you can add additional dependencies into the Nextcloud container. However note that doing this is disrecommended since we do not test Nextcloud apps that require external dependencies.</p><p>You can do so by adding <code>--env NEXTCLOUD_ADDITIONAL_APKS=\"imagemagick dependency2 dependency3\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a string with small letters a-z, digits 0-9, spaces, dots and hyphens or '_'. You can find available packages here: <a href=\"https://pkgs.alpinelinux.org/packages?branch=v3.21\">https://pkgs.alpinelinux.org/packages?branch=v3.21</a>. By default  is added. If you want to keep it, you need to specify it as well.</p><h3>How to add PHP extensions permanently to the Nextcloud container?</h3><p>Some Nextcloud apps require additional php extensions that must be bundled within Nextcloud container in order to work correctly. As we cannot put each and every dependency for all apps into the container - as this would make the project quickly unmaintainable - there is an official way in which you can add additional php extensions into the Nextcloud container. However note that doing this is disrecommended since we do not test Nextcloud apps that require additional php extensions.</p><p>You can do so by adding <code>--env NEXTCLOUD_ADDITIONAL_PHP_EXTENSIONS=\"imagick extension1 extension2\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a string with small letters a-z, digits 0-9, spaces, dots and hyphens or '_'. You can find available extensions here: <a href=\"https://pecl.php.net/packages.php\">https://pecl.php.net/packages.php</a>. By default  is added. If you want to keep it, you need to specify it as well.</p><h3>What about the pdlib PHP extension for the facerecognition app?</h3><p>The <a href=\"https://apps.nextcloud.com/apps/facerecognition\">facerecognition app</a> requires the pdlib PHP extension to be installed. Unfortunately, it is not available on PECL nor via PHP core, so there is no way to add this into AIO currently. However you can use <a href=\"https://github.com/nextcloud/all-in-one/tree/main/community-containers/facerecognition\">this community container</a> in order to run facerecognition.</p><h3>How to enable hardware acceleration for Nextcloud?</h3><p>Some container can use GPU acceleration to increase performance like <a href=\"https://apps.nextcloud.com/apps/memories\">memories app</a> allows to enable hardware transcoding for videos.</p><h4>With open source drivers MESA for AMD, Intel and  drivers  for Nvidia</h4><blockquote><p>[!WARNING] This only works if the  device is present on the host! If it does not exist on your host, don't proceed as otherwise the Nextcloud container will fail to start! If you are unsure about this, better do not proceed with the instructions below. Make sure that your driver is correctly configured on the host.</p></blockquote><p>In order to use that, you need to add <code>--env NEXTCLOUD_ENABLE_DRI_DEVICE=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) which will mount the  device into the container.</p><h4>With proprietary drivers for Nvidia  BETA</h4><blockquote><p>[!WARNING] This only works if the Nvidia Toolkit is installed on the host and an NVIDIA GPU is enabled! Make sure that it is correctly configured on the host. If it does not exist on your host, don't proceed as otherwise the Nextcloud container will fail to start! If you are unsure about this, better do not proceed with the instructions below.</p><p>This feature is in beta. Since the proprietary, we haven't a lot of user using proprietary drivers, we can't guarantee the stability of this feature. Your feedback is welcome.</p></blockquote><p>In order to use that, you need to add <code>--env NEXTCLOUD_ENABLE_NVIDIA_GPU=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) which will enable the nvidia runtime.</p><h3>How to keep disabled apps?</h3><p>In certain situations you might want to keep Nextcloud apps that are disabled in the AIO interface and not uninstall them if they should be installed in Nextcloud. You can do so by adding <code>--env NEXTCLOUD_KEEP_DISABLED_APPS=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used).</p><blockquote><p>[!WARNING] Doing this might cause unintended problems in Nextcloud if an app that requires an external dependency is still installed but the external dependency not for example.</p></blockquote><h3>How to trust user-defined Certification Authorities (CA)?</h3><blockquote><p>[!NOTE] Please note, that this feature is only intended to make LDAPS connections with self-signed certificates work. It will not make other interconnectivity between the different containers work, as they expect a valid publicly trusted certificate like one from Let's Encrypt.</p></blockquote><p>For some applications it might be necessary to establish a secure connection to another host/server which is using a certificate issued by a Certification Authority that is not trusted out of the box. An example could be configuring LDAPS against a domain controller (Active Directory or Samba-based) of an organization.</p><p>You can make the Nextcloud container trust any Certification Authority by providing the environmental variable <code>NEXTCLOUD_TRUSTED_CACERTS_DIR</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used). The value of the variables should be set to the absolute paths of the directory on the host, which contains one or more Certification Authorities certificates. You should use X.509 certificates, Base64 encoded. (Other formats may work but have not been tested!) All the certificates in the directory will be trusted.</p><p>When using , the environmental variable can be set with <code>--env NEXTCLOUD_TRUSTED_CACERTS_DIR=/path/to/my/cacerts</code>.</p><p>In order for the value to be valid, the path should start with  and not end with  and point to an existing . Pointing the variable directly to a certificate  will not work and may also break things.</p><h3>How to disable Collabora's Seccomp feature?</h3><p>The Collabora container enables Seccomp by default, which is a security feature of the Linux kernel. On systems without this kernel feature enabled, you need to provide <code>--env COLLABORA_SECCOMP_DISABLED=true</code> to the initial docker run command in order to make it work. If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used.</p><h3>How to adjust the Fulltextsearch Java options?</h3><p>The Fulltextsearch Java options are by default set to  which might not be enough on some systems. You can adjust this by adding e.g. <code>--env FULLTEXTSEARCH_JAVA_OPTIONS=\"-Xms1024M -Xmx1024M\"</code> to the initial docker run command. If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used.</p><p>On macOS, there is only one thing different in comparison to Linux: instead of using <code>--volume /var/run/docker.sock:/var/run/docker.sock:ro</code>, you need to use <code>--volume /var/run/docker.sock.raw:/var/run/docker.sock:ro</code> to run it after you installed <a href=\"https://www.docker.com/products/docker-desktop/\">Docker Desktop</a> (and don't forget to <a href=\"https://github.com/nextcloud/all-in-one/raw/main/docker-ipv6-support.md\">enable ipv6</a> if you should need that). Apart from that it should work and behave the same like on Linux.</p><p>Also, you may be interested in adjusting Nextcloud's Datadir to store the files on the host system. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p><h3>How to run AIO on Windows?</h3><p>On Windows, install <a href=\"https://www.docker.com/products/docker-desktop/\">Docker Desktop</a> (and don't forget to <a href=\"https://github.com/nextcloud/all-in-one/raw/main/docker-ipv6-support.md\">enable ipv6</a> if you should need that) and run the following command in the command prompt:</p><pre><code>docker run ^\n--init ^\n--sig-proxy=false ^\n--name nextcloud-aio-mastercontainer ^\n--restart always ^\n--publish 80:80 ^\n--publish 8080:8080 ^\n--publish 8443:8443 ^\n--volume nextcloud_aio_mastercontainer:/mnt/docker-aio-config ^\n--volume //var/run/docker.sock:/var/run/docker.sock:ro ^\nghcr.io/nextcloud-releases/all-in-one:latest\n</code></pre><p>Also, you may be interested in adjusting Nextcloud's Datadir to store the files on the host system. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p><blockquote><p>[!NOTE] Almost all commands in this project's documentation use . Since  is not available on Windows, you simply remove  from the commands and they should work.</p></blockquote><h3>How to run AIO on Synology DSM</h3><p>On Synology, there are two things different in comparison to Linux: instead of using <code>--volume /var/run/docker.sock:/var/run/docker.sock:ro</code>, you need to use <code>--volume /volume1/docker/docker.sock:/var/run/docker.sock:ro</code> to run it. You also need to add <code>--env WATCHTOWER_DOCKER_SOCKET_PATH=\"/volume1/docker/docker.sock\"</code>to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>). Apart from that it should work and behave the same like on Linux. Obviously the Synology Docker GUI will not work with that so you will need to either use SSH or create a user-defined script task in the task scheduler as the user 'root' in order to run the command.</p><blockquote><p>[!NOTE] It is possible that the docker socket on your Synology is located in  like the default on Linux. Then you can just use the Linux command without having to change anything - you will notice this when you try to start the container and it says that the bind mount failed. E.g. <code>docker: Error response from daemon: Bind mount failed: '/volume1/docker/docker.sock' does not exists.</code></p></blockquote><p>Also, you may be interested in adjusting Nextcloud's Datadir to store the files on the host system. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p><p>You'll also need to adjust Synology's firewall, see below:</p><h3>How to run AIO with Portainer?</h3><p>The easiest way to run it with Portainer on Linux is to use Portainer's stacks feature and use <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/compose.yaml\">this docker-compose file</a> in order to start AIO correctly.</p><h3>Can I run AIO on TrueNAS SCALE?</h3><p>With the Truenas Scale Release 24.10.0 (which was officially released on October 29th 2024 as a stable release) IX Systems ditched the Kubernetes integration and implemented a fully working docker environment.</p><p>On older TrueNAS SCALE releases with Kubernetes environment, there are two ways to run AIO. The preferred one is to run AIO inside a VM. This is necessary since they do not expose the docker socket for containers on the host, you also cannot use docker-compose on it thus and it is also not possible to run custom helm-charts that are not explicitly written for TrueNAS SCALE.</p><p>Simply run the following: <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ your-command</code>. Of course  needs to be exchanged with the command that you want to run.</p><h3>How to resolve <code>Security &amp; setup warnings displays the \"missing default phone region\" after initial install</code>?</h3><h3>How to run multiple AIO instances on one server?</h3><h3>Bruteforce protection FAQ</h3><p>Nextcloud features a built-in bruteforce protection which may get triggered and will block an ip-address or disable a user. You can unblock an ip-address by running <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ security:bruteforce:reset &lt;ip-address&gt;</code> and enable a disabled user by running <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ user:enable &lt;name of user&gt;</code>. See <a href=\"https://docs.nextcloud.com/server/latest/admin_manual/configuration_server/occ_command.html#security\">https://docs.nextcloud.com/server/latest/admin_manual/configuration_server/occ_command.html#security</a> for further information.</p><h3>How to switch the channel?</h3><p>You can switch to a different channel like e.g. the beta channel or from the beta channel back to the latest channel by stopping the mastercontainer, removing it (no data will be lost) and recreating the container using the same command that you used initially to create the mastercontainer. You simply need to change the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code> to <code>ghcr.io/nextcloud-releases/all-in-one:beta</code> and vice versa.</p><h3>How to update the containers?</h3><p>If we push new containers to , you will see in the AIO interface below the  section that new container updates were found. In this case, just press  and <code>Start and update containers</code> in order to update the containers. The mastercontainer has its own update procedure though. See below. And don't forget to back up the current state of your instance using the built-in backup solution before starting the containers again! Otherwise you won't be able to restore your instance easily if something should break during the update.</p><p>If a new  update was found, you'll see a note below the  button that allows to show the changelog. If you click that button and the containers are stopped, you will see a new button that allows to update the mastercontainer. After doing so and after the update is gone through, you will have the option again to <code>Start and update containers</code>. It is recommended to create a backup before clicking the <code>Start and update containers</code> button.</p><p>Additionally, there is a cronjob that runs once a day that checks for container and mastercontainer updates and sends a notification to all Nextcloud admins if a new update was found.</p><h3>How to easily log in to the AIO interface?</h3><p>If your Nextcloud is running and you are logged in as admin in your Nextcloud, you can easily log in to the AIO interface by opening <code>https://yourdomain.tld/settings/admin/overview</code> which will show a button on top that enables you to log in to the AIO interface by just clicking on this button.</p><blockquote><p>[!Note] You can change the domain/ip-address/port of the button by simply stopping the containers, visiting the AIO interface from the correct and desired domain/ip-address/port and clicking once on .</p></blockquote><h3>How to change the domain?</h3><blockquote><p>[!NOTE] Editing the configuration.json manually and making a mistake may break your instance so please create a backup first!</p></blockquote><p>If you set up a new AIO instance, you need to enter a domain. Currently there is no way to change this domain afterwards from the AIO interface. So in order to change it, you need to edit the configuration.json manually using <code>sudo docker run -it --rm --volume nextcloud_aio_mastercontainer:/mnt/docker-aio-config:rw alpine sh -c \"apk add --no-cache nano &amp;&amp; nano /mnt/docker-aio-config/data/configuration.json\"</code>, substitute each occurrence of your old domain with your new domain and save and write out the file. Afterwards restart your containers from the AIO interface and everything should work as expected if the new domain is correctly configured. If you are running AIO behind a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else), you need to obviously also change the domain in your reverse proxy config.</p><p>Additionally, after restarting the containers, you need to open the admin settings and update some values manually that cannot be changed automatically. Here is a list of some known places:</p><ul><li><code>https://your-nc-domain.com/settings/admin/talk</code> for Turn/Stun server and Signaling Server if you enabled Talk via the AIO interface</li><li><code>https://your-nc-domain.com/settings/admin/theming</code> for the theming URL</li><li><code>https://your-nc-domain.com/settings/admin/app_api</code> for the deploy daemon if you enabled the App API via the AIO interface</li></ul><h3>How to properly reset the instance?</h3><p>If something goes unexpected routes during the initial installation, you might want to reset the AIO installation to be able to start from scratch.</p><blockquote><p>[!NOTE] If you already have it running and have data on your instance, you should not follow these instructions as it will delete all data that is coupled to your AIO instance.</p></blockquote><p>Here is how to reset the AIO instance properly:</p><ol><li>Stop all containers if they are running from the AIO interface</li><li>Stop the mastercontainer with <code>sudo docker stop nextcloud-aio-mastercontainer</code></li><li>If the domaincheck container is still running, stop it with <code>sudo docker stop nextcloud-aio-domaincheck</code></li><li>Check that no AIO containers are running anymore by running <code>sudo docker ps --format {{.Names}}</code>. If no  containers are listed, you can proceed with the steps below. If there should be some, you will need to stop them with <code>sudo docker stop &lt;container_name&gt;</code> until no one is listed anymore.</li><li>Check which containers are stopped: <code>sudo docker ps --filter \"status=exited\"</code></li><li>Now remove all these stopped containers with <code>sudo docker container prune</code></li><li>Delete the docker network with <code>sudo docker network rm nextcloud-aio</code></li><li>Check which volumes are dangling with <code>sudo docker volume ls --filter \"dangling=true\"</code></li><li>Now remove all these dangling volumes: <code>sudo docker volume prune --filter all=1</code> (on Windows you might need to remove some volumes afterwards manually with <code>docker volume rm nextcloud_aio_backupdir</code>, <code>docker volume rm nextcloud_aio_nextcloud_datadir</code>).</li><li>If you've configured  to a path on your host instead of the default volume, you need to clean that up as well. (E.g. by simply deleting the directory).</li><li>Make sure that no volumes are remaining with <code>sudo docker volume ls --format {{.Name}}</code>. If no  volumes are listed, you can proceed with the steps below. If there should be some, you will need to remove them with <code>sudo docker volume rm &lt;volume_name&gt;</code> until no one is listed anymore.</li><li>Optional: You can remove all docker images with <code>sudo docker image prune -a</code>.</li><li>And you are done! Now feel free to start over with the recommended docker run command!</li></ol><h3>Can I use a CIFS/SMB share as Nextcloud's datadir?</h3><p>Sure. Add this to the  file on the host system: <code>&lt;your-storage-host-and-subpath&gt; &lt;your-mount-dir&gt; cifs rw,mfsymlinks,seal,credentials=&lt;your-credentials-file&gt;,uid=33,gid=0,file_mode=0770,dir_mode=0770 0 0</code> (Of course you need to modify <code>&lt;your-storage-host-and-subpath&gt;</code>,  and  for your specific case.)</p><p>One example could look like this:<code>//your-storage-host/subpath /mnt/storagebox cifs rw,mfsymlinks,seal,credentials=/etc/storage-credentials,uid=33,gid=0,file_mode=0770,dir_mode=0770 0 0</code> and add into :</p><pre><code>username=&lt;smb/cifs username&gt;\npassword=&lt;password&gt;\n</code></pre><p>(Of course you need to modify  and  for your specific case.)</p><p>Now you can use  as Nextcloud's datadir like described in the section above this one.</p><h3>Can I run this with Docker swarm?</h3><h3>Can I run this with Kubernetes?</h3><h3>How to run this with Docker rootless?</h3><p>You can run AIO also with docker rootless. How to do this is documented here: <a href=\"https://github.com/nextcloud/all-in-one/raw/main/docker-rootless.md\">docker-rootless.md</a></p><h3>Can I run this with Podman instead of Docker?</h3><h3>Access/Edit Nextcloud files/folders manually</h3><p>The files and folders that you add to Nextcloud are by default stored in the following docker directory: <code>nextcloud_aio_nextcloud:/mnt/ncdata/</code> (usually <code>/var/lib/docker/volumes/nextcloud_aio_nextcloud_data/_data/</code> on linux host systems). If needed, you can modify/add/delete files/folders there but : be very careful when doing so because you might corrupt your AIO installation! Best is to create a backup using the built-in backup solution before editing/changing files/folders in there because you will then be able to restore your instance to the backed up state.</p><p>After you are done modifying/adding/deleting files/folders, don't forget to apply the correct permissions by running: <code>sudo docker exec nextcloud-aio-nextcloud chown -R 33:0 /mnt/ncdata/</code> and <code>sudo docker exec nextcloud-aio-nextcloud chmod -R 750 /mnt/ncdata/</code> and rescan the files with <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ files:scan --all</code>.</p><h3>How to edit Nextclouds config.php file with a texteditor?</h3><p>You can edit Nextclouds config.php file directly from the host with your favorite text editor. E.g. like this: <code>sudo docker run -it --rm --volume nextcloud_aio_nextcloud:/var/www/html:rw alpine sh -c \"apk add --no-cache nano &amp;&amp; nano /var/www/html/config/config.php\"</code>. Make sure to not break the file though which might corrupt your Nextcloud instance otherwise. In best case, create a backup using the built-in backup solution before editing the file.</p><h3>How to change default files by creating a custom skeleton directory?</h3><p>All users see a set of <a href=\"https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/default_files_configuration.html\">default files and folders</a> as dictated by Nextcloud's configuration. To change these default files and folders a custom skeleton directory must first be created; this can be accomplished by copying your skeleton files <code>sudo docker cp --follow-link /path/to/nextcloud/skeleton/ nextcloud-aio-nextcloud:/mnt/ncdata/skeleton/</code>, applying the correct permissions with <code>sudo docker exec nextcloud-aio-nextcloud chown -R 33:0 /mnt/ncdata/skeleton/</code> and <code>sudo docker exec nextcloud-aio-nextcloud chmod -R 750 /mnt/ncdata/skeleton/</code> and setting the skeleton directory option with <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ config:system:set skeletondirectory --value=\"/mnt/ncdata/skeleton\"</code>. Further information is available in the Nextcloud documentation on <a href=\"https://docs.nextcloud.com/server/stable/admin_manual/configuration_server/config_sample_php_parameters.html#skeletondirectory\">configuration parameters for the skeleton directory</a>.</p><h3>How to adjust the version retention policy and trashbin retention policy?</h3><h3>How to enable automatic updates without creating a backup beforehand?</h3><p>If you have an external backup solution, you might want to enable automatic updates without creating a backup first. However note that doing this is disrecommended since you will not be able to easily create and restore a backup from the AIO interface anymore and you need to make sure to shut down all the containers properly before creating the backup, e.g. by stopping them from the AIO interface first.</p><p>But anyhow, is here a guide that helps you automate the whole procedure:</p><p>You can simply copy and paste the script into a file e.g. named  e.g. here: .</p><p>Afterwards apply the correct permissions with <code>sudo chown root:root /root/shutdown-script.sh</code> and <code>sudo chmod 700 /root/shutdown-script.sh</code>. Then you can create a cronjob that runs it on a schedule e.g. runs the script at  each day like this:</p><ol><li>Open the cronjob with  (and choose your editor of choice if not already done. I'd recommend nano).</li><li>Add the following new line to the crontab if not already present: <code>0 4 * * * /root/shutdown-script.sh</code> which will run the script at 04:00 each day.</li><li>save and close the crontab (when using nano the shortcuts for this are  and then  to save, and close the editor with ).</li></ol><p><strong>After that is in place, you should schedule a backup from your backup solution that creates a backup after AIO is shut down properly. Hint: If your backup runs on the same host, make sure to at least back up all docker volumes and additionally Nextcloud's datadir if it is not stored in a docker volume.</strong></p><p><strong>Afterwards, you can create a second script that automatically updates the containers:</strong></p><p>You can simply copy and paste the script into a file e.g. named  e.g. here: <code>/root/automatic-updates.sh</code>.</p><p>Afterwards apply the correct permissions with <code>sudo chown root:root /root/automatic-updates.sh</code> and <code>sudo chmod 700 /root/automatic-updates.sh</code>. Then you can create a cronjob that runs e.g. at  each day like this:</p><ol><li>Open the cronjob with  (and choose your editor of choice if not already done. I'd recommend nano).</li><li>Add the following new line to the crontab if not already present: <code>0 5 * * * /root/automatic-updates.sh</code> which will run the script at 05:00 each day.</li><li>save and close the crontab (when using nano the shortcuts for this are  then  to save, and close the editor with ).</li></ol><h3>Securing the AIO interface from unauthorized ACME challenges</h3><p><a href=\"https://github.com/nextcloud/all-in-one/discussions/4882#discussioncomment-9858384\">By design</a>, Caddy that runs inside the mastercontainer, which handles automatic TLS certificate generation for the AIO interface on port 8443, is configured to accept traffic on any valid domain in order to make the AIO interface as convenient to use as possible. However due to this, it is vulnerable to receiving DNS challenges for arbitrary hostnames from anyone on the internet. While this does not compromise your server's security, it can result in cluttered logs and rejected certificate renewal attempts due to rate limit abuse. To mitigate this issue, it is recommended to place the AIO interface behind a VPN and/or limit its public exposure.</p><h3>How to migrate from an already existing Nextcloud installation to Nextcloud AIO?</h3><p>Nextcloud AIO provides a backup solution based on <a href=\"https://github.com/borgbackup/borg#what-is-borgbackup\">BorgBackup</a>. These backups act as a restore point in case the installation gets corrupted. By using this tool, backups are incremental, differential, compressed and encrypted ‚Äì so only the first backup will take a while. Further backups should be fast as only changes are taken into account.</p><p>It is recommended to create a backup before any container update. By doing this, you will be safe regarding any possible complication during updates because you will be able to restore the whole instance with basically one click.</p><p>For local backups, the restore process should be pretty fast as rsync is used to restore the chosen backup which only transfers changed files and deletes additional ones. For remote borg backups, the whole backup archive is extracted from the remote, which depending on how clever  is, may require downloading the whole archive.</p><p>If you connect an external drive to your host, and choose the backup directory to be on that drive, you are also kind of safe against drive failures of the drive where the docker volumes are stored on.</p><p>If you want to back up directly to a remote borg repository:</p><p>Backups can be created and restored in the AIO interface using the buttons  and . Additionally, a backup check is provided that checks the integrity of your backups but it shouldn't be needed in most situations.</p><p>The backups themselves get encrypted with an encryption key that gets shown to you in the AIO interface. Please save that at a safe place as you will not be able to restore from backup without this key.</p><p>Daily backups can get enabled after the initial backup is done. Enabling this also allows to enable an option that allows to automatically update all containers, Nextcloud and its apps.</p><p>Be aware that this solution does not back up files and folders that are mounted into Nextcloud using the external storage app - but you can add further Docker volumes and host paths that you want to back up after the initial backup is done.</p><h3>What is getting backed up by AIO's backup solution?</h3><p>Backed up will get all important data of your Nextcloud AIO instance required to restore the instance, like the database, your files and configuration files of the mastercontainer and else. Files and folders that are mounted into Nextcloud using the external storage app are not getting backed up. There is currently no way to exclude the data directory because it would require hacks like running files:scan and would make the backup solution much more unreliable (since the database and your files/folders need to stay in sync). If you still don't want your datadirectory to be backed up, see <a href=\"https://github.com/nextcloud/all-in-one#how-to-enable-automatic-updates-without-creating-a-backup-beforehand\">https://github.com/nextcloud/all-in-one#how-to-enable-automatic-updates-without-creating-a-backup-beforehand</a> for options (there is a hint what needs to be backed up in which order).</p><h3>How to adjust borgs retention policy?</h3><p>The built-in borg-based backup solution has by default a retention policy of <code>--keep-within=7d --keep-weekly=4 --keep-monthly=6</code>. See <a href=\"https://borgbackup.readthedocs.io/en/stable/usage/prune.html\">https://borgbackup.readthedocs.io/en/stable/usage/prune.html</a> for what these values mean. You can adjust the retention policy by providing <code>--env BORG_RETENTION_POLICY=\"--keep-within=7d --keep-weekly=4 --keep-monthly=6\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. ‚ö†Ô∏è Please make sure that this value is valid, otherwise backup pruning will bug out!</p><h3>How to migrate from AIO to AIO?</h3><p>If you have the borg backup feature enabled, you can copy it over to the new host and restore from the backup. This guide assumes the new installation data dir will be on , you can adjust the steps if it's elsewhere.</p><ol><li>Set the DNS entry to 60 seconds TTL if applicable</li><li>On your current installation, use the AIO interface to: \n  <ol><li>Update AIO and all containers</li><li>Stop all containers (from now on, your cloud is down)</li><li>Create a current borg backup</li><li>Note the path where the backups are stored and the encryption password</li></ol></li><li>Navigate to the backup folder</li><li>Create archive of the backup so it's easier to copy: <code>tar -czvf borg.tar.gz borg</code></li><li>Copy the archive over to the new host: <code>scp borg.tar.gz user@new.host:/mnt</code>. Make sure to replace  with your actual user and  with the IP or domain of the actual host. You can also use another way to copy the archive.</li><li>Go to the folder you put the backup archive and extract it with </li><li>Follow the installation guide to create a new aio instance, but do not start the containers yet (the  or  command)</li><li>Change the DNS entry to the new host's IP</li><li>Configure your reverse proxy if you use one</li><li>Start the AIO container and open the new AIO interface in your browser</li><li>Make sure to save the newly generated passphrase and enter it in the next step</li><li>Select the \"Restore former AIO instance from backup\" option and enter the encryption password from the old backup and the path in which the extracted  folder lies in (without the borg part) and hit <code>Submit location and password</code></li><li>Choose the latest backup in the dropdown and hit </li><li>Wait until the backup is restored</li><li>Start the containers in the AIO interface</li></ol><h3>Are remote borg backups supported?</h3><p>Backing up directly to a remote borg repository is supported. This avoids having to store a local copy of your backups, supports append-only borg keys to counter ransomware and allows using the AIO interface to manage your backups.</p><p>Some alternatives, which do not have all the above benefits:</p><h3>Failure of the backup container in LXC containers</h3><p>If you are running AIO in a LXC container, you need to make sure that FUSE is enabled in the LXC container settings. Also, if using Alpine Linux as host OS, make sure to add fuse via . Otherwise the backup container will not be able to start as FUSE is required for it to work.</p><h3>How to create the backup volume on Windows?</h3><p>As stated in the AIO interface, it is possible to use a docker volume as backup target. Before you can use that, you need to create it first. Here is an example how to create one on Windows:</p><pre><code>docker volume create ^\n--driver local ^\n--name nextcloud_aio_backupdir ^\n-o device=\"/host_mnt/e/your/backup/path\" ^\n-o type=\"none\" ^\n-o o=\"bind\"\n</code></pre><p>In this example, it would mount  into the volume so for a different location you need to adjust <code>/host_mnt/e/your/backup/path</code> accordingly. Afterwards enter  in the AIO interface as backup location.</p><h3>Pro-tip: Backup archives access</h3><p>You can open the BorgBackup archives on your host by following these steps: (instructions for Ubuntu Desktop)</p><pre><code># Install borgbackup on the host\nsudo apt update &amp;&amp; sudo apt install borgbackup\n\n# In any shell where you use borg, you must first export this variable\n# If you are using the default backup location /mnt/backup/borg\nexport BORG_REPO='/mnt/backup/borg'\n# or if you are using a remote repository\nexport BORG_REPO='user@host:/path/to/repo'\n\n# Mount the archives to /tmp/borg\nsudo mkdir -p /tmp/borg &amp;&amp; sudo borg mount \"$BORG_REPO\" /tmp/borg\n\n# After entering your repository key successfully, you should be able to access all archives in /tmp/borg\n# You can now do whatever you want by syncing them to a different place using rsync or doing other things\n# E.g. you can open the file manager on that location by running:\nxhost +si:localuser:root &amp;&amp; sudo nautilus /tmp/borg\n\n# When you are done, simply close the file manager and run the following command to unmount the backup archives:\nsudo umount /tmp/borg\n</code></pre><h3>Delete backup archives manually</h3><p>You can delete BorgBackup archives on your host manually by following these steps: (instructions for Debian based OS' like Ubuntu)</p><pre><code># Install borgbackup on the host\nsudo apt update &amp;&amp; sudo apt install borgbackup\n\n# In any shell where you use borg, you must first export this variable\n# If you are using the default backup location /mnt/backup/borg\nexport BORG_REPO='/mnt/backup/borg'\n# or if you are using a remote repository\nexport BORG_REPO='user@host:/path/to/repo'\n\n# List all archives (if you are using the default backup location /mnt/backup/borg)\nsudo borg list\n\n# After entering your repository key successfully, you should now see a list of all backup archives\n# An example backup archive might be called 20220223_174237-nextcloud-aio\n# Then you can simply delete the archive with:\nsudo borg delete --stats --progress \"::20220223_174237-nextcloud-aio\"\n\n# If borg 1.2.0 or higher is installed, you then need to run borg compact in order to clean up the freed space\nsudo borg --version\n# If version number of the command above is higher than 1.2.0 you need to run the command below:\nsudo borg compact\n\n</code></pre><p>After doing so, make sure to update the backup archives list in the AIO interface! You can do so by clicking on the  button or  button.</p><h3>Sync local backups regularly to another drive</h3><p>For increased backup security, you might consider syncing the local backup repository regularly to another drive.</p><p>To do that, first add the drive to  so that it is able to get automatically mounted and then create a script that does all the things automatically. Here is an example for such a script:</p><p>You can simply copy and paste the script into a file e.g. named  e.g. here: . Do not forget to modify the variables to your requirements!</p><p>Afterwards apply the correct permissions with <code>sudo chown root:root /root/backup-script.sh</code> and <code>sudo chmod 700 /root/backup-script.sh</code>. Then you can create a cronjob that runs e.g. at  each week on Sundays like this:</p><ol><li>Open the cronjob with  (and choose your editor of choice if not already done. I'd recommend nano).</li><li>Add the following new line to the crontab if not already present: <code>0 20 * * 7 /root/backup-script.sh</code> which will run the script at 20:00 on Sundays each week.</li><li>save and close the crontab (when using nano are the shortcuts for this  -&gt;  and close the editor with ).</li></ol><h3>How to exclude Nextcloud's data directory or the preview folder from backup?</h3><p>In order to speed up the backups and to keep the backup archives small, you might want to exclude Nextcloud's data directory or its preview folder from backup.</p><blockquote><p>[!WARNING] However please note that you will run into problems if the database and the data directory or preview folder get out of sync. <strong>So please only read further, if you have an additional external backup of the data directory!</strong> See <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-enable-automatic-updates-without-creating-a-backup-beforehand\">this guide</a> for example.</p></blockquote><blockquote><p>[!TIP] A better option is to use the external storage app inside Nextcloud as the data connected via the external storage app is not backed up by AIO's backup solution. See <a href=\"https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/external_storage_configuration_gui.html\">this documentation</a> on how to configure the app.</p></blockquote><p>If you still want to proceed, you can exclude the data directory by simply creating a  file in the root directory of the specified  target. The same logic is implemented for the preview folder that is located inside the data directory, inside the  folder. So simply create a  file in there if you want to exclude the preview folder.</p><p>After doing a restore via the AIO interface, you might run into problems due to the data directory and database being out of sync. You might be able to fix this by running  and  and . See <a href=\"https://github.com/nextcloud/all-in-one#how-to-run-occ-commands\">https://github.com/nextcloud/all-in-one#how-to-run-occ-commands</a>. If only the preview folder is excluded, the command <code>occ files:scan-app-data preview</code> should be used.</p><h3>How to stop/start/update containers or trigger the daily backup from a script externally?</h3><blockquote><p>[!WARNING] The below script will only work after the initial setup of AIO. So you will always need to first visit the AIO interface, type in your domain and start the containers the first time or restore an older AIO instance from its borg backup before you can use the script.</p></blockquote><p>You can do so by running the  script that is stored in the mastercontainer. It accepts the following environment variables:</p><ul><li> if set to , it will automatically stop the containers, update them and start them including the mastercontainer. If the mastercontainer gets updated, this script's execution will stop as soon as the mastercontainer gets stopped. You can then wait until it is started again and run the script with this flag again in order to update all containers correctly afterwards.</li><li> if set to , it will automatically stop the containers and create a backup. If you want to start them again afterwards, you may have a look at the  option.</li><li> if set to , it will automatically start the containers without updating them.</li><li> if set to , it will automatically stop the containers.</li><li> if set to , it will start the backup check. This is not allowed to be enabled at the same time like . Please be aware that this option is non-blocking which means that the backup check is not done when the process is finished since it only start the borgbackup container with the correct configuration.</li></ul><p>One example for this would be <code>sudo docker exec -it --env DAILY_BACKUP=1 nextcloud-aio-mastercontainer /daily-backup.sh</code>, which you can run via a cronjob or put it in a script.</p><blockquote><p>[!NOTE] None of the option returns error codes. So you need to check for the correct result yourself.</p></blockquote><h3>How to disable the backup section?</h3><p>If you already have a backup solution in place, you may want to hide the backup section. You can do so by adding <code>--env AIO_DISABLE_BACKUP_SECTION=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used).</p><p>It is possible to connect to an existing LDAP server. You need to make sure that the LDAP server is reachable from the Nextcloud container. Then you can enable the LDAP app and configure LDAP in Nextcloud manually. If you don't have a LDAP server yet, recommended is to use this docker container: <a href=\"https://hub.docker.com/r/nitnelave/lldap\">https://hub.docker.com/r/nitnelave/lldap</a>. Make sure here as well that Nextcloud can talk to the LDAP server. The easiest way is by adding the LDAP docker container to the docker network . Then you can connect to the LDAP container by its name from the Nextcloud container. There is now a community container which allows to easily add LLDAP to AIO: <a href=\"https://github.com/nextcloud/all-in-one/tree/main/community-containers/lldap\">https://github.com/nextcloud/all-in-one/tree/main/community-containers/lldap</a></p><p>If you want to use the user_sql app, the easiest way is to create an additional database container and add it to the docker network . Then the Nextcloud container should be able to talk to the database container using its name.</p><h3>phpMyAdmin, Adminer or pgAdmin</h3><h3>Requirements for integrating new containers</h3><p>What are the requirements?</p><ol><li>New containers must be related to Nextcloud. Related means that there must be a feature in Nextcloud that gets added by adding this container.</li><li>It must be optionally installable. Disabling and enabling the container from the AIO interface must work and must not produce any unexpected side-effects.</li><li>The feature that gets added into Nextcloud by adding the container must be maintained by the Nextcloud GmbH.</li><li>It must be possible to run the container without big quirks inside docker containers. Big quirks means e.g. needing to change the capabilities or security options.</li><li>The container should not mount directories from the host into the container: only docker volumes should be used.</li><li>The container must be usable by more than 90% of the users (e.g. not too high system requirements and such)</li><li>No additional setup should be needed after adding the container - it should work completely out of the box.</li><li>If the container requires being exposed, only subfolders are supported. So the container should not require its own (sub-)domain and must be able to run in a subfolder.</li></ol><p>This project values stability over new features. That means that when a new major Nextcloud update gets introduced, we will wait at least until the first patch release, e.g.  is out before upgrading to it. Also we will wait with the upgrade until all important apps are compatible with the new major version. Minor or patch releases for Nextcloud and all dependencies as well as all containers will be updated to new versions as soon as possible but we try to give all updates first a good test round before pushing them. That means that it can take around 2 weeks before new updates reach the  channel. If you want to help testing, you can switch to the  channel by following <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-switch-the-channel\">this documentation</a> which will also give you the updates earlier.</p><h3>How often are update notifications sent?</h3><p>AIO ships its own update notifications implementation. It checks if container updates are available. If so, it sends a notification with the title <code>Container updates available!</code> on saturdays to Nextcloud users that are part of the  group. If the Nextcloud container image should be older than 90 days (~3 months) and thus badly outdated, AIO sends a notification to all Nextcloud users with the title . Thus admins should make sure to update the container images at least once every 3 months in order to make sure that the instance gets all security bugfixes as soon as possible.</p><p>If you should run into issues with huge docker logs, you can adjust the log size by following <a href=\"https://docs.docker.com/config/containers/logging/local/#usage\">https://docs.docker.com/config/containers/logging/local/#usage</a>. However for the included AIO containers, this should usually not be needed because almost all of them have the log level set to warn so they should not produce many logs.</p>","contentLength":65765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"visgl/deck.gl","url":"https://github.com/visgl/deck.gl","date":1751337922,"author":"","guid":177072,"unread":true,"content":"<p>WebGL2 powered visualization framework</p><h5 align=\"center\"> GPU-powered, highly performant large-scale data visualization</h5><p>deck.gl is designed to simplify high-performance, WebGL2/WebGPU based visualization of large data sets. Users can quickly get impressive visual results with minimal effort by composing existing layers, or leverage deck.gl's extensible architecture to address custom needs.</p><p>deck.gl maps  (usually an array of JSON objects) into a stack of visual  - e.g. icons, polygons, texts; and look at them with : e.g. map, first-person, orthographic.</p><p>deck.gl handles a number of challenges out of the box:</p><ul><li>Performant rendering and updating of large data sets</li><li>Interactive event handling such as picking, highlighting and filtering</li><li>Cartographic projections and integration with major basemap providers</li><li>A catalog of proven, well-tested layers</li></ul><p>Deck.gl is designed to be highly customizable. All layers come with flexible APIs to allow programmatic control of each aspect of the rendering. All core classes such are easily extendable by the users to address custom use cases.</p><pre><code>&lt;script src=\"https://unpkg.com/deck.gl@latest/dist.min.js\"&gt;&lt;/script&gt;\n</code></pre><p>Data sources are listed in each example.</p><h4>The deck.gl project is supported by</h4>","contentLength":1195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"vanshb03/Summer2026-Internships","url":"https://github.com/vanshb03/Summer2026-Internships","date":1751337922,"author":"","guid":177073,"unread":true,"content":"<p>Collection of Summer 2026 tech internships!</p><p>The torch has been passed. Use this repo to share and keep track of software, tech, CS, PM, quant internships for . The list is maintained collaboratively by Vansh and <a href=\"https://discord.gg/cscareers\">CSCareers</a>!</p><p> Please note that this repository is exclusively for internships/co-ops in the United States, Canada, or Remote positions </p><p>üôè <strong>Contribute by submitting an <a href=\"https://github.com/vanshb03/Summer2026-Internships/issues/new/choose\">issue</a>! See the contribution guidelines <a href=\"https://raw.githubusercontent.com/vanshb03/Summer2026-Internships/dev/CONTRIBUTING.md\">here</a>!</strong> üôè</p><div align=\"center\"><h3>Want notifications when new internships open? </h3><p> Join the ‚¨áÔ∏è <strong> discord  ‚¨áÔ∏è and get your internship applications in right when they open! <a href=\"https://redirect.cvrve.me/discord\"></a></strong></p><strong><sub><i>Join the Discord to connect with fellow peers and streamline your internship search.</i></sub></strong></div>","contentLength":651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"paperless-ngx/paperless-ngx","url":"https://github.com/paperless-ngx/paperless-ngx","date":1751337922,"author":"","guid":177074,"unread":true,"content":"<p>A community-supported supercharged document management system: scan, index and archive all your documents</p><p>Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, .</p><p>Paperless-ngx is the official successor to the original <a href=\"https://github.com/the-paperless-project/paperless\">Paperless</a> &amp; <a href=\"https://github.com/jonaswinkler/paperless-ng\">Paperless-ng</a> projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. <a href=\"https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support\">Consider joining us!</a></p><p>Thanks to the generous folks at <a href=\"https://m.do.co/c/8d70b916d462\">DigitalOcean</a>, a demo is available at <a href=\"https://demo.paperless-ngx.com\">demo.paperless-ngx.com</a> using login  / . <em>Note: demo content is reset frequently and confidential information should not be uploaded.</em></p><p align=\"right\">This project is supported by:<a href=\"https://m.do.co/c/8d70b916d462\"></a></p><p>The easiest way to deploy paperless is . The files in the <a href=\"https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose\"> directory</a> are configured to pull the image from the GitHub container registry.</p><p>If you'd like to jump right in, you can configure a  environment with our install script:</p><pre><code>bash -c \"$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)\"\n</code></pre><p>More details and step-by-step guides for alternative installation methods can be found in <a href=\"https://docs.paperless-ngx.com/setup/#installation\">the documentation</a>.</p><p>If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The <a href=\"https://docs.paperless-ngx.com/development/\">documentation</a> has some basic information on how to get started.</p><p>People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the <a href=\"https://matrix.to/#/%23paperless:matrix.org\">Matrix Room</a>. If you would like to contribute to the project on an ongoing basis there are multiple <a href=\"https://github.com/orgs/paperless-ngx/people\">teams</a> (frontend, ci/cd, etc) that could use your help so please reach out!</p><p>Feature requests can be submitted via <a href=\"https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests\">GitHub Discussions</a>, you can search for existing ideas, add your own and vote for the ones you care about.</p><p>Please see <a href=\"https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects\">the wiki</a> for a user-maintained list of related projects and software that is compatible with Paperless-ngx.</p><blockquote><p>Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. <strong>Paperless-ngx should never be run on an untrusted host</strong> because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk. <strong>The safest way to run Paperless-ngx is on a local server in your own home with backups in place</strong>.</p></blockquote>","contentLength":2398,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"007revad/Synology_HDD_db","url":"https://github.com/007revad/Synology_HDD_db","date":1751337922,"author":"","guid":177075,"unread":true,"content":"<p>Add your HDD, SSD and NVMe drives to your Synology's compatible drive database and a lot more</p><p>Add your SATA or SAS HDDs and SSDs plus SATA and NVMe M.2 drives to your Synology's compatible drive databases, including your Synology M.2 PCIe card and Expansion Unit databases.</p><p>The script works in DSM 7, including DSM 7.2, and DSM 6.</p><p>It also has a restore option to undo all the changes made by the script.</p><ul><li>Gets the Synology NAS model and DSM version (so it knows which db files to edit).</li><li>Gets a list of the HDD, SSD, SAS and NVMe drives installed in your Synology NAS.</li><li>Gets each drive's model number and firmware version.</li><li>Backs up the database files if there is no backup already.</li><li>Checks if each drive is already in the Synology's compatible-drive database.</li><li>Adds any missing drives to the Synology's compatible-drive database.</li><li>Optionally prevents DSM auto updating the drive database.</li><li>Optionally disable DSM's \"support_disk_compatibility\".</li><li>Optionally edits max supported memory to match the amount of memory installed, if installed memory is greater than the current max memory setting. \n  <ul><li>DSM only uses the max memory setting when calculating the reserved RAM area size for SSD caches.</li></ul></li><li>Optionally set write_mostly for your internal HDDs so DSM will normally read from your faster internal SSD(s). \n  <ul><li>It can automatically set DSM to read from your internal SSDs.</li><li>Or you can tell the script which internal drive(s) DSM should read from.</li></ul></li><li>Enables M2D20, M2D18, M2D17 and E10M20-T1 if present on Synology NAS that don't officially support them. \n  </li><li>Checks that M.2 volume support is enabled (on models that have M.2 slots or PCIe slots).</li><li>Enables creating M.2 storage pools and volumes from within Storage Manager in DSM 7.2 and later . \n  <ul><li>Including M.2 drives in PCIe adaptor cards like M2D20, M2D18, M2D17 and E10M20-T1 for DSM 7.2 and above <strong>(schedule the script to run boot)</strong>.</li></ul></li><li>Optionally update IronWolf Health Monitor to v2.5.1 to support recent model IronWolf and IronWolf Pro drives. <strong>(NAS with x86_64 CPUs only)</strong>. \n  <ul><li>Also installs IronWolf Health Management on '22 series and newer models that don't have IronWolf Health Management .</li></ul></li><li>Makes DSM recheck disk compatibility so rebooting is not needed if you don't have M.2 drives (DSM 7 only). \n  <ul><li><strong>If you have M.2 drives you may need to reboot.</strong></li><li>Reminds you that you may need to reboot the Synology after running the script.</li></ul></li><li>Checks if there is a newer version of this script and offers to download it for you. \n  <ul><li>The new version available messages time out so they don't prevent the script running if it is scheduled to run unattended.</li></ul></li></ul><ol><li>Save the download zip file to a folder on the Synology. \n  <ul><li>Do  save the script to a M.2 volume. After a DSM or Storage Manager update the M.2 volume won't be available until after the script has run.</li></ul></li></ol><p>Or via SSH as your regular user:</p><pre><code>cd $HOME\nwget https://github.com/007revad/Synology_HDD_db/archive/refs/heads/main.zip -O syno_hdd_db.zip\n7z x syno_hdd_db.zip\ncd Synology_HDD_db-main &amp;&amp; ls -ali\n</code></pre><p>The following files from the downloaded zip file must be in the same folder:</p><ol><li>dtc or the bin folder containing dtc (only required if you have a E10M20-T1, M2D20 or M2D18 in a NAS that does not support them).</li></ol><p>You would need to re-run the script after a DSM update. If you have DSM set to auto update the best option is to run the script every time the Synology boots, and the best way to do that is to <a href=\"https://raw.githubusercontent.com/007revad/Synology_HDD_db/main/how_to_schedule.md/\">setup a scheduled task</a> to run the the script at boot-up.</p><p> After you first run the script you may need to reboot the Synology to see the effect of the changes.</p><h3>Options when running the script </h3><p>There are optional flags you can use when running the script:</p><pre><code>  -s, --showedits       Show edits made to &lt;model&gt;_host db and db.new file(s)\n  -n, --noupdate        Prevent DSM updating the compatible drive databases\n  -r, --ram             Disable memory compatibility checking (DSM 7.x only)\n                        and sets max memory to the amount of installed memory\n  -f, --force           Force DSM to not check drive compatibility\n                        Do not use this option unless absolutely needed\n  -i, --incompatible    Change incompatible drives to supported\n                        Do not use this option unless absolutely needed\n  -w, --wdda            Disable WD Device Analytics to prevent DSM showing\n                        a false warning for WD drives that are 3 years old\n                          DSM 7.2.1 and later already has WDDA disabled\n  -p, --pcie            Enable creating volumes on M2 in unknown PCIe adaptor\n  -e, --email           Disable colored text in output scheduler emails\n  -S, --ssd=DRIVE       Enable write_mostly on internal HDDs so DSM primarily \n                        reads from internal SSDs or your specified drives\n                          -S automatically sets internal SSDs as DSM preferred\n                          --ssd=DRIVE requires the fast drive(s) as argument,\n                          or restore as the argument to reset drives to default\n                          --ssd=sata1 or --ssd=sata1,sata2 or --ssd=sda etc\n                          --ssd=restore\n      --restore         Undo all changes made by the script (except -S --ssd)\n                        To restore all changes including write_mostly use\n                          --restore --ssd=restore\n      --autoupdate=AGE  Auto update script (useful when script is scheduled)\n                          AGE is how many days old a release must be before\n                          auto-updating. AGE must be a number: 0 or greater\n  -I, --ihm             Update IronWolf Health Management to 2.5.1 to support\n                        recent model IronWolf and IronWolf Pro drives.\n                        For NAS with x86_64 CPUs only.\n                        Also installs IHM on '22 series and newer models (untested)\n  -h, --help            Show this help message\n  -v, --version         Show the script version\n</code></pre><ul><li>The -f or --force option is only needed if for some reason your drives still show as unsupported in storage manager. \n  <ul><li>Only use this option as last resort.</li><li>Using this option will prevent data deduplication from being available, and prevent firmware updates on Synology brand drives.</li></ul></li><li>If you have some Synology drives and want to update their firmware run the script  --noupdate or -n then do the drive database update from Storage Manager and finally run the script again with your preferred options.</li></ul><h3>Scheduling the script in Synology's Task Scheduler</h3><h3>Running the script via SSH</h3><p>You run the script in a shell with sudo -s or as root.</p><pre><code>sudo -s /path-to-script/syno_hdd_db.sh -nr\n</code></pre><p> Replace /path-to-script/ with the actual path to the script on your Synology.</p><p>If you run the script with the --showedits flag it will show you the changes it made to the Synology's compatible-drive database. Obviously this is only useful if you run the script in a shell.</p><pre><code>sudo -s /path-to-script/syno_hdd_db.sh -nr --showedits\n</code></pre><p> Replace /path-to-script/ with the actual path to the script on your Synology.</p><p>If you get a \"No such file or directory\" error check the following:</p><ol><li>Make sure you downloaded the zip or rar file to a folder on your Synology (not on your computer).</li><li>Make sure you unpacked the zip or rar file that you downloaded and are trying to run the syno_hdd_db.sh file.</li><li>If the path to the script contains any spaces you need to enclose the path/scriptname in double quotes: <pre><code>sudo -s \"/volume1/my scripts/syno_hdd_db.sh -n\"\n</code></pre></li><li>Set the script file as executable: <pre><code>sudo chmod +x \"/volume1/scripts/syno_hdd_db.sh\"\n</code></pre></li></ol><p>You only need to edit syno_hdd_vendor_ids.txt if the script warns you about a missing vendor id.</p><p>If DSM doesn't know the brand of your NVMe drives they will show up in Storage Manager as Unknown brand, and Unrecognised firmware version.</p><p>In this case the script will show you the vendor ID and advise you to add it to the syno_hdd_vendor_ids.txt file.</p><p>Ironwolf Health working with the latest version of Ironwolf Health Monitor.</p><ul><li>The idea for this script came from a comment made by Empyrealist on the Synology subreddit.</li><li>Thanks for the assistance from Alex_of_Chaos on the Synology subreddit.</li><li>Thanks to dwabraxus and aferende for help detecting connected expansion units.</li><li>Thanks to bartoque on the Synology subreddit for the tip on making the script download the latest release from GitHub.</li><li>Thanks to nicolerenee for pointing out the easiest way to enable creating M.2 storage pools and volumes in Storage Manager.</li></ul><p>Thank you to the PayPal and Buy Me a Coffee donators, GitHub sponsors and hardware donators</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table>","contentLength":8450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"aldinokemal/go-whatsapp-web-multidevice","url":"https://github.com/aldinokemal/go-whatsapp-web-multidevice","date":1751337922,"author":"","guid":177076,"unread":true,"content":"<p>API for Whatsapp Web Multi Device Version, Support UI, Webhook &amp; MCP</p><p><a href=\"https://www.patreon.com/c/aldinokemal\"><img src=\"https://img.shields.io/badge/Support%20on-Patreon-orange.svg?sanitize=true\" alt=\"Patreon\"></a><strong>If you're using this tools to generate income, consider supporting its development by becoming a Patreon member!</strong> Your support helps ensure the library stays maintained and receives regular updates!</p><h2>Support for  &amp;  Architecture along with  Support</h2><ul><li><ul><li>For REST mode, you need to run  instead of <ul><li>for example:  instead of </li></ul></li><li>For MCP mode, you need to run <ul><li>for example: </li></ul></li></ul></li></ul><ul><li><p><strong>MCP (Model Context Protocol) Server Support</strong> - Integrate with AI agents and tools using standardized protocol</p></li><li><ul><li>example: <code>Hello @628974812XXXX, @628974812XXXX</code></li></ul></li><li><p>Compress image before send</p></li><li><p>Compress video before send</p></li><li><p>Change OS name become your app (it's the device name when connect via mobile)</p><ul><li> or </li></ul></li><li><p>Basic Auth (able to add multi credentials)</p><ul><li><code>--basic-auth=kemal:secret,toni:password,userName:secretPassword</code>, or you can simplify</li><li><code>-b=kemal:secret,toni:password,userName:secretPassword</code></li></ul></li><li><p>Customizable port and debug mode</p></li><li><ul><li><code>--autoreply=\"Don't reply this message\"</code></li></ul></li><li><p>Webhook for received message</p><ul><li><code>--webhook=\"http://yourwebhook.site/handler\"</code>, or you can simplify</li><li><code>-w=\"http://yourwebhook.site/handler\"</code></li></ul></li><li><p>Webhook Secret Our webhook will be sent to you with an HMAC header and a sha256 default key .</p><p>You may modify this by using the option below:</p><ul><li><code>--webhook-secret=\"secret\"</code></li></ul></li></ul><p>You can configure the application using either command-line flags (shown above) or environment variables. Configuration can be set in three ways (in order of priority):</p><ol><li>Command-line flags (highest priority)</li><li> file (lowest priority)</li></ol><p>You can configure the application using environment variables. Configuration can be set in three ways (in order of priority):</p><ol><li>Command-line flags (highest priority)</li><li> file (lowest priority)</li></ol><p>To use environment variables:</p><ol><li>Copy  to  in your project root (<code>cp src/.env.example src/.env</code>)</li><li>Modify the values in  according to your needs</li><li>Or set the same variables as system environment variables</li></ol><h4>Available Environment Variables</h4><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td>OS name (device name in WhatsApp)</td></tr><tr><td>Basic authentication credentials</td><td><code>APP_BASIC_AUTH=user1:pass1,user2:pass2</code></td></tr><tr><td>Chat flush interval in days</td><td><code>APP_CHAT_FLUSH_INTERVAL=30</code></td></tr><tr><td><code>file:storages/whatsapp.db?_foreign_keys=on</code></td><td><code>DB_URI=postgres://user:pass@host/db</code></td></tr><tr><td><code>WHATSAPP_AUTO_REPLY=\"Auto reply message\"</code></td></tr><tr><td>Webhook URL(s) for events (comma-separated)</td><td><code>WHATSAPP_WEBHOOK=https://webhook.site/xxx</code></td></tr><tr><td>Webhook secret for validation</td><td><code>WHATSAPP_WEBHOOK_SECRET=super-secret-key</code></td></tr><tr><td><code>WHATSAPP_ACCOUNT_VALIDATION</code></td><td>Enable account validation</td><td><code>WHATSAPP_ACCOUNT_VALIDATION=false</code></td></tr><tr><td><code>WHATSAPP_CHAT_STORAGE=false</code></td></tr></tbody></table><p>Note: Command-line flags will override any values set in environment variables or  file.</p><ul><li>For more command </li></ul><ul><li> (for building from source)</li><li> (for media processing)</li></ul><ul><li>macOS (Intel, Apple Silicon)</li><li>Windows (x86_64) - WSL recommended</li></ul><h3>Dependencies (without docker)</h3><ul><li>Mac OS: \n  <ul><li><code>export CGO_CFLAGS_ALLOW=\"-Xpreprocessor\"</code></li></ul></li><li>Linux: \n  <ul></ul></li></ul><ol><li>Clone this repo: <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run  (for REST API mode)</li><li>Open </li></ol><h3>Docker (you don't need to install in required)</h3><ol><li>Clone this repo: <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run <code>docker-compose up -d --build</code></li><li>open </li></ol><ol><li>Clone this repo <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run \n  <ol><li>Linux &amp; MacOS: </li><li>Windows (CMD / PowerShell): </li></ol></li><li>run \n  <ol><li>Linux &amp; MacOS:  (for REST API mode) \n    <ol><li>run  for more detail flags</li></ol></li><li>Windows:  (for REST API mode) \n    <ol><li>run  for more detail flags</li></ol></li></ol></li><li>open  in browser</li></ol><h3>MCP Server (Model Context Protocol)</h3><p>This application can also run as an MCP server, allowing AI agents and tools to interact with WhatsApp through a standardized protocol.</p><ol><li>Clone this repo <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run  or build the binary and run </li><li>The MCP server will start on  by default</li></ol><ul><li> - Set the host for MCP server (default: localhost)</li><li> - Set the port for MCP server (default: 8080)</li></ul><ul><li> - Send text messages</li><li> - Send contact cards</li><li> - Send links with captions</li><li> - Send location coordinates</li></ul><ul><li>SSE endpoint: <code>http://localhost:8080/sse</code></li><li>Message endpoint: <code>http://localhost:8080/message</code></li></ul><p>Make sure you have the MCP server running: </p><p>For AI tools that support MCP with SSE (like Cursor), add this configuration:</p><pre><code>{\n  \"mcpServers\": {\n    \"whatsapp\": {\n      \"url\": \"http://localhost:8080/sse\"\n    }\n  }\n}\n</code></pre><h3>Production Mode REST (docker)</h3><pre><code>docker run --detach --publish=3000:3000 --name=whatsapp --restart=always --volume=$(docker volume create --name=whatsapp):/app/storages aldinokemal2104/go-whatsapp-web-multidevice rest --autoreply=\"Dont't reply this message please\"\n</code></pre><h3>Production Mode REST (docker compose)</h3><p>create  file with the following configuration:</p><pre><code>services:\n  whatsapp:\n    image: aldinokemal2104/go-whatsapp-web-multidevice\n    container_name: whatsapp\n    restart: always\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - whatsapp:/app/storages\n    command:\n      - rest\n      - --basic-auth=admin:admin\n      - --port=3000\n      - --debug=true\n      - --os=Chrome\n      - --account-validation=false\n\nvolumes:\n  whatsapp:\n</code></pre><pre><code>services:\n  whatsapp:\n    image: aldinokemal2104/go-whatsapp-web-multidevice\n    container_name: whatsapp\n    restart: always\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - whatsapp:/app/storages\n    environment:\n      - APP_BASIC_AUTH=admin:admin\n      - APP_PORT=3000\n      - APP_DEBUG=true\n      - APP_OS=Chrome\n      - APP_ACCOUNT_VALIDATION=false\n\nvolumes:\n  whatsapp:\n</code></pre><p>You can fork or edit this source code !</p><h3>MCP (Model Context Protocol) API</h3><ul><li>MCP server provides standardized tools for AI agents to interact with WhatsApp</li><li>Supports Server-Sent Events (SSE) transport</li><li>Available tools: , , , </li><li>Compatible with MCP-enabled AI tools and agents</li></ul><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>/message/:message_id/revoke</td></tr><tr><td>/message/:message_id/reaction</td></tr><tr><td>/message/:message_id/delete</td></tr><tr><td>/message/:message_id/update</td></tr><tr><td>/message/:message_id/read</td></tr><tr><td>/message/:message_id/star</td></tr><tr><td>/message/:message_id/unstar</td></tr><tr></tr><tr></tr><tr><td>Add Participants in Group</td></tr><tr><td>Remove Participant in Group</td><td>/group/participants/remove</td></tr><tr><td>Promote Participant in Group</td><td>/group/participants/promote</td></tr><tr><td>Demote Participant in Group</td><td>/group/participants/demote</td></tr><tr><td>List Requested Participants in Group</td><td>/group/participant-requests</td></tr><tr><td>Approve Requested Participant in Group</td><td>/group/participant-requests/approve</td></tr><tr><td>Reject Requested Participant in Group</td><td>/group/participant-requests/reject</td></tr><tr></tr></tbody></table><pre><code>‚úÖ = Available\n‚ùå = Not Available Yet\n</code></pre><ul><li>Please do this if you have an error (invalid flag in pkg-config --cflags: -Xpreprocessor) <code>export CGO_CFLAGS_ALLOW=\"-Xpreprocessor\"</code></li></ul><ul><li>This project is unofficial and not affiliated with WhatsApp.</li><li>Please use official WhatsApp API to avoid any issues.</li><li>We only able to run MCP or REST API, this is limitation from whatsmeow library. independent MCP will be available in the future.</li></ul>","contentLength":6667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"snailyp/gemini-balance","url":"https://github.com/snailyp/gemini-balance","date":1751337922,"author":"","guid":177077,"unread":true,"content":"<li><p>: Supports configuring multiple Gemini API Keys () for automatic sequential polling, improving availability and concurrency.</p></li><li><p><strong>Visual Configuration Takes Effect Immediately</strong>: Configurations modified through the admin backend take effect without restarting the service. Remember to click save for changes to apply. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image4.png\" alt=\"Configuration Panel\"></p></li><li><p><strong>Dual Protocol API Compatibility</strong>: Supports forwarding CHAT API requests in both Gemini and OpenAI formats.</p><pre><code>openai baseurl `http://localhost:8000(/hf)/v1`\ngemini baseurl `http://localhost:8000(/gemini)/v1beta`\n</code></pre></li><li><p>: Supports web search.  configures which models can perform web searches. When actually calling, use the  model name to use this feature. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image8.png\" alt=\"Web Search\"></p></li><li><p>: Provides a  page (requires authentication) to view the status and usage of each Key in real-time. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image.png\" alt=\"Monitoring Panel\"></p></li><li><p><strong>Support for Custom Gemini Proxy</strong>: Supports custom Gemini proxies, such as those built on Deno or Cloudflare.</p></li><li><p><strong>OpenAI Image Generation API Compatibility</strong>: Adapts the  model interface to be compatible with the OpenAI image generation API, supporting client calls.</p></li><li><p>: Flexible way to add keys using regex matching for , with key deduplication. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image5.png\" alt=\"Add Key\"></p></li><li><p><strong>OpenAI Format Embeddings API Compatibility</strong>: Perfectly adapts to the OpenAI format  interface, usable for local document vectorization.</p></li><li><p><strong>Streamlined Response Optimization</strong>: Optional stream output optimizer () to improve the experience of long-text stream responses.</p></li><li><p><strong>Failure Retry and Key Management</strong>: Automatically handles API request failures, retries (), automatically disables Keys after too many failures (), and periodically checks for recovery ().</p></li><li><p>: Supports AMD and ARM architecture Docker deployments. You can also build your own Docker image.</p><blockquote><p>Image address: docker pull ghcr.io/snailyp/gemini-balance:latest</p></blockquote></li><li><p><strong>Automatic Model List Maintenance</strong>: Supports fetching OpenAI and Gemini model lists, perfectly compatible with NewAPI's automatic model list fetching, no manual entry required.</p></li><li><p><strong>Support for Removing Unused Models</strong>: Too many default models are provided, many of which are not used. You can filter them out using .</p></li><li><p>: Supports configuring HTTP/SOCKS5 proxy servers () for accessing the Gemini API, convenient for use in special network environments. Supports batch adding proxies.</p></li>","contentLength":2169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mendableai/firecrawl","url":"https://github.com/mendableai/firecrawl","date":1751251237,"author":"","guid":175482,"unread":true,"content":"<p>üî• Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.</p><p>Empower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.</p><p><em>This repository is in development, and we‚Äôre still integrating custom modules into the mono repo. It's not fully ready for self-hosted deployment yet, but you can run it locally.</em></p><p><a href=\"https://firecrawl.dev?ref=github\">Firecrawl</a> is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our <a href=\"https://docs.firecrawl.dev\">documentation</a>.</p><p><em>Pst. hey, you, join our stargazers :)</em></p><a href=\"https://github.com/mendableai/firecrawl\"><img src=\"https://img.shields.io/github/stars/mendableai/firecrawl.svg?style=social&amp;label=Star&amp;maxAge=2592000\" alt=\"GitHub stars\"></a><p>We provide an easy to use API with our hosted version. You can find the playground and documentation <a href=\"https://firecrawl.dev/playground\">here</a>. You can also self host the backend if you'd like.</p><p>Check out the following resources to get started:</p><p>To run locally, refer to guide <a href=\"https://github.com/mendableai/firecrawl/raw/main/CONTRIBUTING.md\">here</a>.</p><p>To use the API, you need to sign up on <a href=\"https://firecrawl.dev\">Firecrawl</a> and get an API key.</p><ul><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#scraping\"></a>: scrapes a URL and get its content in LLM-ready format (markdown, structured data via <a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#llm-extraction-beta\">LLM Extract</a>, screenshot, html)</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#crawling\"></a>: scrapes all the URLs of a web page and return content in LLM-ready format</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#map-alpha\"></a>: input a website and get all the website urls - extremely fast</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#search\"></a>: search the web and get full content from results</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#extract\"></a>: get structured data from single page, multiple pages or entire websites with AI.</li></ul><ul><li>: markdown, structured data, screenshot, HTML, links, metadata</li><li>: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration</li><li>: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...</li><li>: pdfs, docx, images</li><li>: designed to get the data you need - no matter how hard it is</li><li>: click, scroll, input, wait and more before extracting data</li><li>: scrape thousands of URLs at the same time with a new async endpoint.</li></ul><p>You can find all of Firecrawl's capabilities and how to use them in our <a href=\"https://docs.firecrawl.dev\">documentation</a></p><p>Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/crawl \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer fc-YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"limit\": 10,\n      \"scrapeOptions\": {\n        \"formats\": [\"markdown\", \"html\"]\n      }\n    }'\n</code></pre><p>Returns a crawl job id and the url to check the status of the crawl.</p><pre><code>{\n  \"success\": true,\n  \"id\": \"123-456-789\",\n  \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n</code></pre><p>Used to check the status of a crawl job and get its result.</p><pre><code>curl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY'\n</code></pre><pre><code>{\n  \"status\": \"completed\",\n  \"total\": 36,\n  \"creditsUsed\": 36,\n  \"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n  \"data\": [\n    {\n      \"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\n      \"html\": \"&lt;!DOCTYPE html&gt;&lt;html lang=\\\"en\\\" class=\\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\\" data-js-focus-visible=\\\"\\\"&gt;...\",\n      \"metadata\": {\n        \"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\n        \"language\": \"en\",\n        \"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\n        \"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\n        \"ogLocaleAlternate\": [],\n        \"statusCode\": 200\n      }\n    }\n  ]\n}\n</code></pre><p>Used to scrape a URL and get its content in the specified formats.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"markdown\": \"Launch Week I is here! [See our Day 2 Release üöÄ](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[üí• Get 2 months free...\",\n    \"html\": \"&lt;!DOCTYPE html&gt;&lt;html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"&gt;&lt;body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\n    \"metadata\": {\n      \"title\": \"Home - Firecrawl\",\n      \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n      \"language\": \"en\",\n      \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Firecrawl\",\n      \"ogDescription\": \"Turn any website into LLM-ready data.\",\n      \"ogUrl\": \"https://www.firecrawl.dev/\",\n      \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Firecrawl\",\n      \"sourceURL\": \"https://firecrawl.dev\",\n      \"statusCode\": 200\n    }\n  }\n}\n</code></pre><p>Used to map a URL and get urls of the website. This returns most links present on the website.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\"\n    }'\n</code></pre><pre><code>{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://firecrawl.dev\",\n    \"https://www.firecrawl.dev/pricing\",\n    \"https://www.firecrawl.dev/blog\",\n    \"https://www.firecrawl.dev/playground\",\n    \"https://www.firecrawl.dev/smart-crawl\",\n  ]\n}\n</code></pre><p>Map with  param allows you to search for specific urls inside a website.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\",\n      \"search\": \"docs\"\n    }'\n</code></pre><p>Response will be an ordered list from the most relevant to the least relevant.</p><pre><code>{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://docs.firecrawl.dev\",\n    \"https://docs.firecrawl.dev/sdks/python\",\n    \"https://docs.firecrawl.dev/learn/rag-llama3\",\n  ]\n}\n</code></pre><p>Search the web and get full content from results</p><p>Firecrawl‚Äôs search API allows you to perform web searches and optionally scrape the search results in one operation.</p><ul><li>Choose specific output formats (markdown, HTML, links, screenshots)</li><li>Search the web with customizable parameters (language, country, etc.)</li><li>Optionally retrieve content from search results in various formats</li><li>Control the number of results and set timeouts</li></ul><pre><code>curl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5\n  }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"data\": [\n    {\n      \"url\": \"https://firecrawl.dev\",\n      \"title\": \"Firecrawl | Home Page\",\n      \"description\": \"Turn websites into LLM-ready data with Firecrawl\"\n    },\n    {\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"title\": \"Documentation | Firecrawl\",\n      \"description\": \"Learn how to use Firecrawl in your own applications\"\n    }\n  ]\n}\n</code></pre><pre><code>curl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5,\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\", \"links\"]\n    }\n  }'\n</code></pre><p>Get structured data from entire websites with a prompt and/or a schema.</p><p>You can extract structured data from one or multiple URLs, including wildcards:</p><p>When you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/extract \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\n        \"https://firecrawl.dev/*\", \n        \"https://docs.firecrawl.dev/\", \n        \"https://www.ycombinator.com/companies\"\n      ],\n      \"prompt\": \"Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"company_mission\": {\n            \"type\": \"string\"\n          },\n          \"is_open_source\": {\n            \"type\": \"boolean\"\n          },\n          \"is_in_yc\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"required\": [\n          \"company_mission\",\n          \"is_open_source\",\n          \"is_in_yc\"\n        ]\n      }\n    }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"id\": \"44aa536d-f1cb-4706-ab87-ed0386685740\",\n  \"urlTrace\": []\n}\n</code></pre><p>If you are using the sdks, it will auto pull the response for you:</p><pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n    \"supports_sso\": false,\n    \"is_open_source\": true,\n    \"is_in_yc\": true\n  }\n}\n</code></pre><p>Used to extract structured data from scraped pages.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://www.mendable.ai/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"company_mission\": {\n                      \"type\": \"string\"\n            },\n            \"supports_sso\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_open_source\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_in_yc\": {\n                      \"type\": \"boolean\"\n            }\n          },\n          \"required\": [\n            \"company_mission\",\n            \"supports_sso\",\n            \"is_open_source\",\n            \"is_in_yc\"\n          ]\n        }\n      }\n    }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"content\": \"Raw Content\",\n    \"metadata\": {\n      \"title\": \"Mendable\",\n      \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Mendable\",\n      \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"ogUrl\": \"https://mendable.ai/\",\n      \"ogImage\": \"https://mendable.ai/mendable_new_og1.png\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Mendable\",\n      \"sourceURL\": \"https://mendable.ai/\"\n    },\n    \"json\": {\n      \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n      \"supports_sso\": true,\n      \"is_open_source\": false,\n      \"is_in_yc\": true\n    }\n  }\n}\n</code></pre><h3>Extracting without a schema (New)</h3><p>You can now extract without a schema by just passing a  to the endpoint. The llm chooses the structure of the data.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"prompt\": \"Extract the company mission from the page.\"\n      }\n    }'\n</code></pre><h3>Interacting with the page with Actions (Cloud-only)</h3><p>Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.</p><p>Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n        \"url\": \"google.com\",\n        \"formats\": [\"markdown\"],\n        \"actions\": [\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"click\", \"selector\": \"textarea[title=\\\"Search\\\"]\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"write\", \"text\": \"firecrawl\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"press\", \"key\": \"ENTER\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"click\", \"selector\": \"h3\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"screenshot\"}\n        ]\n    }'\n</code></pre><h3>Batch Scraping Multiple URLs (New)</h3><p>You can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n</code></pre><pre><code>from firecrawl.firecrawl import FirecrawlApp\nfrom firecrawl.firecrawl import ScrapeOptions\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n  'https://firecrawl.dev', \n  formats=[\"markdown\", \"html\"]\n)\nprint(scrape_status)\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev',\n  limit=100,\n  scrape_options=ScrapeOptions(\n    formats=[\"markdown\", \"html\"],),\n  poll_interval=30\n)\nprint(crawl_status)\n</code></pre><h3>Extracting structured data from a URL</h3><p>With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:</p><pre><code>class ArticleSchema(BaseModel):\n    title: str\n    points: int \n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., description=\"Top 5 stories\")\n\njson_config = JsonConfig(schema=TopArticlesSchema.model_json_schema())\n\nllm_extraction_result = app.scrape_url('https://news.ycombinator.com', formats=[\"json\"], json=json_config)\n\nprint(llm_extraction_result.json)\n</code></pre><p>To install the Firecrawl Node SDK, you can use npm:</p><pre><code>npm install @mendable/firecrawl-js\n</code></pre><ol><li>Set the API key as an environment variable named  or pass it as a parameter to the  class.</li></ol><pre><code>import FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n  formats: ['markdown', 'html'],\n});\n\nif (scrapeResponse) {\n  console.log(scrapeResponse)\n}\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;\n\nif (crawlResponse) {\n  console.log(crawlResponse)\n}\n</code></pre><h3>Extracting structured data from a URL</h3><p>With LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:</p><pre><code>import FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\n  apiKey: \"fc-YOUR_API_KEY\"\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://news.ycombinator.com\", {\n  jsonOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"json\"]);\n</code></pre><h2>Open Source vs Cloud Offering</h2><p>Firecrawl is open source available under the AGPL-3.0 license.</p><p>To deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.</p><p>Firecrawl Cloud is available at <a href=\"https://firecrawl.dev\">firecrawl.dev</a> and offers a range of features that are not available in the open source version:</p><p><em>It is the sole responsibility of the end users to respect websites' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions.</em></p><a href=\"https://github.com/mendableai/firecrawl/graphs/contributors\"><img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=mendableai/firecrawl\"></a><p>This project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.</p><ul><li>The AGPL-3.0 license applies to all parts of the project unless otherwise specified.</li><li>The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.</li><li>When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.</li></ul><p>For more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.</p>","contentLength":17168,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"jnsahaj/tweakcn","url":"https://github.com/jnsahaj/tweakcn","date":1751251237,"author":"","guid":175483,"unread":true,"content":"<p>A visual no-code theme editor for shadcn/ui components</p><p> is a powerful Visual Theme Editor for tailwind CSS &amp; shadcn/ui components. It comes with Beautiful theme presets to get started, while aiming to offer advanced customisation for each aspect of your UI</p><p>Websites made with shadcn/ui famously look the same. tweakcn is a tool that helps you customize shadcn/ui components visually, to make your components stand-out. The goal is to build a platform where a user can discover endless customization options and then have the ability to put their own twist on it. Check our roadmap for more information</p><ul></ul><pre><code>git clone https://github.com/jnsahaj/tweakcn.git\ncd tweakcn\n</code></pre><ol start=\"3\"><li>Start the development server:</li></ol><a href=\"https://github.com/jnsahaj/tweakcn/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=jnsahaj/tweakcn\"></a><h3>Interested in Contributing?</h3><p>Contributions are welcome! Please feel free to submit a Pull Request.</p>","contentLength":784,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LMCache/LMCache","url":"https://github.com/LMCache/LMCache","date":1751251237,"author":"","guid":175484,"unread":true,"content":"<p>Supercharge Your LLM with the Fastest KV Cache Layer</p><h3 align=\"center\"> Redis for LLMs - Infinite and Ultra-Fast </h3><p>LMCache is an  serving engine extension to  and , especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of  reused text (not necessarily prefix) in  serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.</p><p>By combining LMCache with vLLM, LMCache achieves 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.</p><p>Try LMCache with pre-built vllm docker images <a href=\"https://docs.lmcache.ai/developer_guide/docker_file.html\">here</a>.</p><p>The community meeting for LMCache is hosted weekly. Meeting Details:</p><p>Meetings  between the two times. All are welcome to join!</p><p>We welcome and value any contributions and collaborations. Please check out <a href=\"https://raw.githubusercontent.com/LMCache/LMCache/dev/CONTRIBUTING.md\">CONTRIBUTING.md</a> for how to get involved.</p><p>If you use LMCache for your research, please cite our papers:</p><pre><code>@inproceedings{liu2024cachegen,\n  title={Cachegen: Kv cache compression and streaming for fast large language model serving},\n  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},\n  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},\n  pages={38--56},\n  year={2024}\n}\n\n@article{cheng2024large,\n  title={Do Large Language Models Need a Content Delivery Network?},\n  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},\n  journal={arXiv preprint arXiv:2409.13761},\n  year={2024}\n}\n\n@inproceedings{10.1145/3689031.3696098,\n  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},\n  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},\n  year = {2025},\n  url = {https://doi.org/10.1145/3689031.3696098},\n  doi = {10.1145/3689031.3696098},\n  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},\n  pages = {94‚Äì109},\n}\n\n  \n</code></pre><p>This project is licensed under Apache License 2.0. See the <a href=\"https://raw.githubusercontent.com/LMCache/LMCache/dev/LICENSE\">LICENSE</a> file for details.</p>","contentLength":2207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"octra-labs/wallet-gen","url":"https://github.com/octra-labs/wallet-gen","date":1751251237,"author":"","guid":175485,"unread":true,"content":"<p><strong>download and start wallet generator web UI with a single command:</strong></p><pre><code>curl -fsSL https://octra.org/wallet-generator.sh | bash\n</code></pre><pre><code>powershell -c \"irm octra.org/wallet-generator.ps1 | iex\"\n</code></pre><ul><li>download the latest source code and build the wallet generator</li><li>start the server and open the generator web UI page in your browser</li><li>install to <code>~/.octra/wallet-generator</code> for future use</li></ul>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"m1k1o/neko","url":"https://github.com/m1k1o/neko","date":1751251237,"author":"","guid":175486,"unread":true,"content":"<p>A self hosted virtual browser that runs in docker and uses WebRTC.</p><p>Welcome to Neko, a self-hosted virtual browser that runs in Docker and uses WebRTC technology. Neko is a powerful tool that allows you to <strong>run a fully-functional browser in a virtual environment</strong>, giving you the ability to <strong>access the internet securely and privately from anywhere</strong>. With Neko, you can browse the web, , and perform other tasks just as you would on a regular browser, all within a <strong>secure and isolated environment</strong>. Whether you are a developer looking to test web applications, a <strong>privacy-conscious user seeking a secure browsing experience</strong>, or simply someone who wants to take advantage of the <strong>convenience and flexibility of a virtual browser</strong>, Neko is the perfect solution.</p><p>In addition to its security and privacy features, Neko offers the <strong>ability for multiple users to access it simultaneously</strong>. This makes it an ideal solution for teams or organizations that need to share access to a browser, as well as for individuals who want to use <strong>multiple devices to access the same virtual environment</strong>. With Neko, you can <strong>easily and securely share access to a browser with others</strong>, without having to worry about maintaining separate configurations or settings. Whether you need to , access shared resources, or simply want to <strong>share access to a browser with friends or family</strong>, Neko makes it easy to do so.</p><p>Neko is also a great tool for  and interactive presentations. With its virtual browser capabilities, Neko allows you to host watch parties and presentations that are , without the need for in-person gatherings. This makes it easy to <strong>stay connected with friends and colleagues</strong>, even when you are unable to meet in person. With Neko, you can easily host a watch party or give an , whether it's for leisure or work. Simply invite your guests to join the virtual environment, and you can share the screen and <strong>interact with them in real-time</strong>.</p><p>This app uses WebRTC to stream a desktop inside of a docker container, original author made this because <a href=\"https://en.wikipedia.org/wiki/Rabb.it\">rabb.it</a> went under and his internet could not handle streaming and discord kept crashing when his friend attempted to. He just wanted to watch anime with his friends ·Éö(‡≤†Áõä‡≤†·Éö) so he started digging throughout the internet and found a few  clones, but none of them had the virtual browser, then he found <a href=\"https://github.com/Khauri/Turtus\">Turtus</a> and he was able to figure out the rest.</p><p>Then I found <a href=\"https://github.com/nurdism/neko\">this</a> project and started to dig into it. I really liked the idea of having collaborative browser browsing together with multiple people, so I created a fork. Initially, I wanted to merge my changes to the upstream repository, but the original author did not have time for this project anymore and it got eventually archived.</p><p>Neko started as a virtual browser that is streamed using WebRTC to multiple users.</p><ul><li>It is <strong>not only limited to a browser</strong>; it can run anything that runs on linux (e.g. VLC). Browser only happens to be the most popular and widely used use-case.</li><li>In fact, it is not limited to a single program either; you can install a full desktop environment (e.g. XFCE, KDE).</li><li>Speaking of limits, it does not need to run in a container; you could install neko on your host, connect to your X server and control your whole VM.</li><li>Theoretically it is not limited to only X server, anything that can be controlled and scraped periodically for images could be used instead. \n  <ul><li>Like implementing RDP or VNC protocol, where neko would only act as WebRTC relay server. This is currently only future.</li></ul></li></ul><p>Primary use case is connecting with multiple people, leveraging real time synchronization and interactivity:</p><ul><li> - watching video content together with multiple people and reacting to it (chat, emotes) - open source alternative to <a href=\"https://giggl.app/\">giggl.app</a> or <a href=\"https://watch.hyperbeam.com\">hyperbeam</a>.</li><li> - not only screen sharing, but others can control the screen.</li><li> - brainstorming ideas, cobrowsing, code debugging together.</li><li> - interactively guiding people in controlled environment.</li><li> - embed virtual browser in your web app - open source alternative to <a href=\"https://hyperbeam.com/\">hyperbeam API</a>. \n  <ul><li>open any third-party website or application, synchronize audio and video flawlessly among multiple participants.</li></ul></li></ul><p>Other use cases that benefit from single-user:</p><ul><li> - streaming containerized apps and desktops to end-users - similar to <a href=\"https://www.kasmweb.com/\">kasm</a>.</li><li> - own browser with persistent cookies available anywhere - similar to <a href=\"https://www.mightyapp.com/\">mightyapp</a>. \n  <ul><li>no state is left on the host browser after terminating the connection.</li><li>sensitive data like cookies are not transferred - only video is shared.</li></ul></li><li> - a better solution for planning secret parties and buying birthday gifts off the internet. \n  <ul><li>use Tor Browser and <a href=\"https://github.com/m1k1o/neko-vpn\">VPN</a> for additional anonymity.</li><li>mitigates risk of OS fingerprinting and browser vulnerabilities by running in container.</li></ul></li><li> - broadcast room content using RTMP (to e.g. twitch or youtube...).</li><li> - broadcast RTMP can be saved to a file using e.g. <a href=\"https://www.nginx.com/products/nginx/modules/rtmp-media-streaming/\">nginx-rtmp</a><ul><li>have clean environment when recording tutorials.</li><li>no need to hide bookmarks or use incognito mode.</li></ul></li><li> - access your internal applications securely without the need for VPN.</li><li> - you can install <a href=\"https://playwright.dev/\">playwright</a> or <a href=\"https://pptr.dev/\">puppeteer</a> and automate tasks while being able to actively intercept them.</li></ul><ul><li> because it uses WebRTC and not images sent over WebSockets.</li><li> support, what is not part of Apache Guacamole or noVNC.</li><li><strong>Multi-participant control</strong>, what is not natively supported by Apache Guacamole or noVNC.</li></ul><p>I like cats üê± ( is the Japanese word for cat), I'm a weeb/nerd.</p><p> Because cats are , but you love them anyways.</p><p>For neko room management software, visit <a href=\"https://github.com/m1k1o/neko-rooms\">neko-rooms</a>.</p><p>Full documentation is available at <a href=\"https://neko.m1k1o.net/\">neko.m1k1o.net</a>. Key sections include:</p><p>If you find Neko useful, consider supporting the project via <a href=\"https://github.com/sponsors/m1k1o\">GitHub Sponsors</a>.</p>","contentLength":5601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"stanford-oval/storm","url":"https://github.com/stanford-oval/storm","date":1751251237,"author":"","guid":175487,"unread":true,"content":"<p>An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.</p> **Latest News** üî• \n<ul><li><p>[2025/01] We add <a href=\"https://github.com/BerriAI/litellm\">litellm</a> integration for language models and embedding models in  v1.1.0.</p></li><li><p>[2024/09] Co-STORM codebase is now released and integrated into  python package v1.0.0. Run <code>pip install knowledge-storm --upgrade</code> to check it out.</p></li><li><p>[2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! <a href=\"https://www.arxiv.org/abs/2408.15232\">Co-STORM Paper</a> has been accepted to EMNLP 2024 main conference.</p></li><li><p>[2024/07] You can now install our package with <code>pip install knowledge-storm</code>!</p></li><li><p>[2024/07] We add  to support grounding on user-provided documents, complementing existing support of search engines (, ). (check out <a href=\"https://github.com/stanford-oval/storm/pull/58\">#58</a>)</p></li><li><p>[2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout <a href=\"https://github.com/stanford-oval/storm/pull/54\">#54</a>)</p></li><li><p>[2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/assets/storm_naacl2024_slides.pdf\">presentation material</a>.</p></li><li><p>[2024/05] We add Bing Search support in <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/rm.py\">rm.py</a>. Test STORM with  - we now configure the article generation part in our demo using  model.</p></li><li><p>[2024/04] We release refactored version of STORM codebase! We define <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/interface.py\">interface</a> for STORM pipeline and reimplement STORM-wiki (check out <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/storm_wiki\"></a>) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.</p></li></ul> STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. Co-STORM further enhanced its feature by enabling human to collaborative LLM system to support more aligned and preferred information seeking and knowledge curation. \n<p>While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.</p><p><strong>More than 70,000 people have tried our <a href=\"https://storm.genie.stanford.edu/\">live research preview</a>. Try it out to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!</strong></p><h2>How STORM &amp; Co-STORM works</h2><p>STORM breaks down generating long articles with citations into two steps:</p><ol><li>: The system conducts Internet-based research to collect references and generates an outline.</li><li>: The system uses the outline and references to generate the full-length article with citations.</li></ol><p>STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:</p><ol><li><strong>Perspective-Guided Question Asking</strong>: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.</li><li>: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.</li></ol><p>Co-STORM proposes <strong>a collaborative discourse protocol</strong> which implements a turn management policy to support smooth collaboration among</p><ul><li>: This type of agent generates answers grounded on external knowledge sources and/or raises follow-up questions based on the discourse history.</li><li>: This agent generates thought-provoking questions inspired by information discovered by the retriever but not directly used in previous turns. Question generation can also be grounded!</li><li>: The human user will take the initiative to either (1) observe the discourse to gain deeper understanding of the topic, or (2) actively engage in the conversation by injecting utterances to steer the discussion focus.</li></ul><p>Co-STORM also maintains a dynamic updated , which organize collected information into a hierarchical concept structure, aiming to <strong>build a shared conceptual space between the human user and the system</strong>. The mind map has been proven to help reduce the mental load when the discourse goes long and in-depth.</p><p>Both STORM and Co-STORM are implemented in a highly modular way using <a href=\"https://github.com/stanfordnlp/dspy\">dspy</a>.</p><p>To install the knowledge storm library, use <code>pip install knowledge-storm</code>.</p><p>You could also install the source code which allows you to modify the behavior of STORM engine directly.</p><ol><li><p>Clone the git repository.</p><pre><code>git clone https://github.com/stanford-oval/storm.git\ncd storm\n</code></pre></li><li><p>Install the required packages.</p><pre><code>conda create -n storm python=3.11\nconda activate storm\npip install -r requirements.txt\n</code></pre></li></ol><p>Currently, our package support:</p><ul><li>Language model components: All language models supported by litellm as listed <a href=\"https://docs.litellm.ai/docs/providers\">here</a></li><li>Embedding model components: All embedding models supported by litellm as listed <a href=\"https://docs.litellm.ai/docs/embedding/supported_embedding\">here</a></li><li>retrieval module components: , , , , , , , , , and  as</li></ul><p>Both STORM and Co-STORM are working in the information curation layer, you need to set up the information retrieval module and language model module to create their  classes respectively.</p><p>The STORM knowledge curation engine is defined as a simple Python  class. Here is an example of using You.com search engine and OpenAI models.</p><pre><code>import os\nfrom knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs\nfrom knowledge_storm.lm import LitellmModel\nfrom knowledge_storm.rm import YouRM\n\nlm_configs = STORMWikiLMConfigs()\nopenai_kwargs = {\n    'api_key': os.getenv(\"OPENAI_API_KEY\"),\n    'temperature': 1.0,\n    'top_p': 0.9,\n}\n# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.\n# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.\n# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.\ngpt_35 = LitellmModel(model='gpt-3.5-turbo', max_tokens=500, **openai_kwargs)\ngpt_4 = LitellmModel(model='gpt-4o', max_tokens=3000, **openai_kwargs)\nlm_configs.set_conv_simulator_lm(gpt_35)\nlm_configs.set_question_asker_lm(gpt_35)\nlm_configs.set_outline_gen_lm(gpt_4)\nlm_configs.set_article_gen_lm(gpt_4)\nlm_configs.set_article_polish_lm(gpt_4)\n# Check out the STORMWikiRunnerArguments class for more configurations.\nengine_args = STORMWikiRunnerArguments(...)\nrm = YouRM(ydc_api_key=os.getenv('YDC_API_KEY'), k=engine_args.search_top_k)\nrunner = STORMWikiRunner(engine_args, lm_configs, rm)\n</code></pre><p>The  instance can be evoked with the simple  method:</p><pre><code>topic = input('Topic: ')\nrunner.run(\n    topic=topic,\n    do_research=True,\n    do_generate_outline=True,\n    do_generate_article=True,\n    do_polish_article=True,\n)\nrunner.post_run()\nrunner.summary()\n</code></pre><ul><li>: if True, simulate conversations with difference perspectives to collect information about the topic; otherwise, load the results.</li><li>: if True, generate an outline for the topic; otherwise, load the results.</li><li>: if True, generate an article for the topic based on the outline and the collected information; otherwise, load the results.</li><li>: if True, polish the article by adding a summarization section and (optionally) removing duplicate content; otherwise, load the results.</li></ul><p>The Co-STORM knowledge curation engine is defined as a simple Python  class. Here is an example of using Bing search engine and OpenAI models.</p><pre><code>from knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner\nfrom knowledge_storm.lm import LitellmModel\nfrom knowledge_storm.logging_wrapper import LoggingWrapper\nfrom knowledge_storm.rm import BingSearch\n\n# Co-STORM adopts the same multi LM system paradigm as STORM \nlm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()\nopenai_kwargs = {\n    \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    \"api_provider\": \"openai\",\n    \"temperature\": 1.0,\n    \"top_p\": 0.9,\n    \"api_base\": None,\n} \nquestion_answering_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\ndiscourse_manage_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\nutterance_polishing_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)\nwarmstart_outline_gen_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\nquestion_asking_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)\nknowledge_base_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\n\nlm_config.set_question_answering_lm(question_answering_lm)\nlm_config.set_discourse_manage_lm(discourse_manage_lm)\nlm_config.set_utterance_polishing_lm(utterance_polishing_lm)\nlm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)\nlm_config.set_question_asking_lm(question_asking_lm)\nlm_config.set_knowledge_base_lm(knowledge_base_lm)\n\n# Check out the Co-STORM's RunnerArguments class for more configurations.\ntopic = input('Topic: ')\nrunner_argument = RunnerArgument(topic=topic, ...)\nlogging_wrapper = LoggingWrapper(lm_config)\nbing_rm = BingSearch(bing_search_api_key=os.environ.get(\"BING_SEARCH_API_KEY\"),\n                     k=runner_argument.retrieve_top_k)\ncostorm_runner = CoStormRunner(lm_config=lm_config,\n                               runner_argument=runner_argument,\n                               logging_wrapper=logging_wrapper,\n                               rm=bing_rm)\n</code></pre><p>The  instance can be evoked with the  and  methods.</p><pre><code># Warm start the system to build shared conceptual space between Co-STORM and users\ncostorm_runner.warm_start()\n\n# Step through the collaborative discourse \n# Run either of the code snippets below in any order, as many times as you'd like\n# To observe the conversation:\nconv_turn = costorm_runner.step()\n# To inject your utterance to actively steer the conversation:\ncostorm_runner.step(user_utterance=\"YOUR UTTERANCE HERE\")\n\n# Generate report based on the collaborative discourse\ncostorm_runner.knowledge_base.reorganize()\narticle = costorm_runner.generate_report()\nprint(article)\n</code></pre><h2>Quick Start with Example Scripts</h2><p>We provide scripts in our <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/examples\">examples folder</a> as a quick start to run STORM and Co-STORM with different configurations.</p><p>We suggest using  to set up the API keys. Create a file  under the root directory and add the following content:</p><pre><code># ============ language model configurations ============ \n# Set up OpenAI API key.\nOPENAI_API_KEY=\"your_openai_api_key\"\n# If you are using the API service provided by OpenAI, include the following line:\nOPENAI_API_TYPE=\"openai\"\n# If you are using the API service provided by Microsoft Azure, include the following lines:\nOPENAI_API_TYPE=\"azure\"\nAZURE_API_BASE=\"your_azure_api_base_url\"\nAZURE_API_VERSION=\"your_azure_api_version\"\n# ============ retriever configurations ============ \nBING_SEARCH_API_KEY=\"your_bing_search_api_key\" # if using bing search\n# ============ encoder configurations ============ \nENCODER_API_TYPE=\"openai\" # if using openai encoder\n</code></pre><p><strong>To run STORM with  family models with default configurations:</strong></p><p>Run the following command.</p><pre><code>python examples/storm_examples/run_storm_wiki_gpt.py \\\n    --output-dir $OUTPUT_DIR \\\n    --retriever bing \\\n    --do-research \\\n    --do-generate-outline \\\n    --do-generate-article \\\n    --do-polish-article\n</code></pre><p>To run Co-STORM with  family models with default configurations,</p><ol><li>Add <code>BING_SEARCH_API_KEY=\"xxx\"</code> and  to </li><li>Run the following command</li></ol><pre><code>python examples/costorm_examples/run_costorm_gpt.py \\\n    --output-dir $OUTPUT_DIR \\\n    --retriever bing\n</code></pre><h2>Customization of the Pipeline</h2><p>If you have installed the source code, you can customize STORM based on your own use case. STORM engine consists of 4 modules:</p><ol><li>Knowledge Curation Module: Collects a broad coverage of information about the given topic.</li><li>Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.</li><li>Article Generation Module: Populates the generated outline with the collected information.</li><li>Article Polishing Module: Refines and enhances the written article for better presentation.</li></ol><p>The interface for each module is defined in <code>knowledge_storm/interface.py</code>, while their implementations are instantiated in <code>knowledge_storm/storm_wiki/modules/*</code>. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).</p><p>If you have installed the source code, you can customize Co-STORM based on your own use case</p><ol><li>Co-STORM introduces multiple LLM agent types (i.e. Co-STORM experts and Moderator). LLM agent interface is defined in <code>knowledge_storm/interface.py</code> , while its implementation is instantiated in <code>knowledge_storm/collaborative_storm/modules/co_storm_agents.py</code>. Different LLM agent policies can be customized.</li><li>Co-STORM introduces a collaborative discourse protocol, with its core function centered on turn policy management. We provide an example implementation of turn policy management through  in <code>knowledge_storm/collaborative_storm/engine.py</code>. It can be customized and further improved.</li></ol><p>To facilitate the study of automatic knowledge curation and complex information seeking, our project releases the following datasets:</p><p>The FreshWiki Dataset is a collection of 100 high-quality Wikipedia articles focusing on the most-edited pages from February 2022 to September 2023. See Section 2.1 in <a href=\"https://arxiv.org/abs/2402.14207\">STORM paper</a> for more details.</p><p>You can download the dataset from <a href=\"https://huggingface.co/datasets/EchoShao8899/FreshWiki\">huggingface</a> directly. To ease the data contamination issue, we archive the <a href=\"https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup/FreshWiki\">source code</a> for the data construction pipeline that can be repeated at future dates.</p><p>To study users‚Äô interests in complex information seeking tasks in the wild, we utilized data collected from the web research preview to create the WildSeek dataset. We downsampled the data to ensure the diversity of the topics and the quality of the data. Each data point is a pair comprising a topic and the user‚Äôs goal for conducting deep search on the topic. For more details, please refer to Section 2.2 and Appendix A of <a href=\"https://www.arxiv.org/abs/2408.15232\">Co-STORM paper</a>.</p><p>The WildSeek dataset is available <a href=\"https://huggingface.co/datasets/YuchengJiang/WildSeek\">here</a>.</p><h2>Replicate STORM &amp; Co-STORM paper result</h2><p>For STORM paper experiments, please switch to the branch <a href=\"https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup\">here</a>.</p><p>For Co-STORM paper experiments, please switch to the branch  (placeholder for now, will be updated soon).</p><p>Our team is actively working on:</p><ol><li>Human-in-the-Loop Functionalities: Supporting user participation in the knowledge curation process.</li><li>Information Abstraction: Developing abstractions for curated information to support presentation formats beyond the Wikipedia-style report.</li></ol><p>If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!</p><p>We would like to thank Wikipedia for its excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.</p><p>We are very grateful to <a href=\"https://michelle123lam.github.io/\">Michelle Lam</a> for designing the logo for this project and <a href=\"https://dekun.me\">Dekun Ma</a> for leading the UI development.</p><p>Please cite our paper if you use this code or part of it in your work:</p><pre><code>@inproceedings{jiang-etal-2024-unknown,\n    title = \"Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations\",\n    author = \"Jiang, Yucheng  and\n      Shao, Yijia  and\n      Ma, Dekun  and\n      Semnani, Sina  and\n      Lam, Monica\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.554/\",\n    doi = \"10.18653/v1/2024.emnlp-main.554\",\n    pages = \"9917--9955\",\n}\n\n@inproceedings{shao-etal-2024-assisting,\n    title = \"Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models\",\n    author = \"Shao, Yijia  and\n      Jiang, Yucheng  and\n      Kanell, Theodore  and\n      Xu, Peter  and\n      Khattab, Omar  and\n      Lam, Monica\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.347/\",\n    doi = \"10.18653/v1/2024.naacl-long.347\",\n    pages = \"6252--6278\",\n}\n</code></pre>","contentLength":16494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ItzCrazyKns/Perplexica","url":"https://github.com/ItzCrazyKns/Perplexica","date":1751251237,"author":"","guid":175488,"unread":true,"content":"<p>Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI</p><p>Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it's an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.</p><p>Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.</p><p>Want to know more about its architecture and how it works? You can read it <a href=\"https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md\">here</a>.</p><ul><li>: You can make use local LLMs such as Llama3 and Mixtral using Ollama.</li><li><ul><li> (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user's query directly from the page.</li><li> Processes your query and performs a web search.</li></ul></li><li> Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes: \n  <ul><li> Searches the entire web to find the best results.</li><li> Helpful for writing tasks that do not require searching the web.</li><li> Finds articles and papers, ideal for academic research.</li><li> Finds YouTube videos based on the search query.</li><li><strong>Wolfram Alpha Search Mode:</strong> Answers queries that need calculations or data analysis using Wolfram Alpha.</li><li> Searches Reddit for discussions and opinions related to the query.</li></ul></li><li> Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.</li><li>: Integrate Perplexica into your existing applications and make use of its capibilities.</li></ul><p>It has many more features like image and video search. Some of the planned features are mentioned in <a href=\"https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features\">upcoming features</a>.</p><p>There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.</p><h3>Getting Started with Docker (Recommended)</h3><ol><li><p>Ensure Docker is installed and running on your system.</p></li><li><p>Clone the Perplexica repository:</p><pre><code>git clone https://github.com/ItzCrazyKns/Perplexica.git\n</code></pre></li><li><p>After cloning, navigate to the directory containing the project files.</p></li><li><p>Rename the  file to . For Docker setups, you need only fill in the following fields:</p><ul><li><p>: Your OpenAI API key. <strong>You only need to fill this if you wish to use OpenAI's models</strong>.</p></li><li><p>: Your Ollama API URL. You should enter it as <code>http://host.docker.internal:PORT_NUMBER</code>. If you installed Ollama on port 11434, use <code>http://host.docker.internal:11434</code>. For other ports, adjust accordingly. <strong>You need to fill this if you wish to use Ollama's models instead of OpenAI's</strong>.</p></li><li><p>: Your Groq API key. <strong>You only need to fill this if you wish to use Groq's hosted models</strong>.</p></li><li><p>: Your Anthropic API key. <strong>You only need to fill this if you wish to use Anthropic models</strong>.</p></li><li><p>: Your Gemini API key. <strong>You only need to fill this if you wish to use Google's models</strong>.</p><p>: You can change these after starting Perplexica from the settings dialog.</p></li><li><p>: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)</p></li></ul></li><li><p>Ensure you are in the directory containing the  file and execute:</p></li><li><p>Wait a few minutes for the setup to complete. You can access Perplexica at <a href=\"http://localhost:3000\">http://localhost:3000</a> in your web browser.</p></li></ol><p>: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.</p><ol><li>Install SearXNG and allow  format in the SearXNG settings.</li><li>Clone the repository and rename the  file to  in the root directory. Ensure you complete all required fields in this file.</li><li>After populating the configuration run .</li><li>Install the dependencies and then execute .</li><li>Finally, start the app by running </li></ol><p>: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.</p><p>If you're encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama's API. To fix this issue you can:</p><ol><li><p><strong>Check your Ollama API URL:</strong> Ensure that the API URL is correctly set in the settings menu.</p></li><li><p><strong>Update API URL Based on OS:</strong></p><ul><li> Use <code>http://host.docker.internal:11434</code></li><li> Use <code>http://host.docker.internal:11434</code></li><li> Use <code>http://&lt;private_ip_of_host&gt;:11434</code></li></ul><p>Adjust the port number if you're using a different one.</p></li><li><p><strong>Linux Users - Expose Ollama to Network:</strong></p><ul><li><p>Inside <code>/etc/systemd/system/ollama.service</code>, you need to add <code>Environment=\"OLLAMA_HOST=0.0.0.0\"</code>. Then restart Ollama by . For more information see <a href=\"https://github.com/ollama/ollama/raw/main/docs/faq.md#setting-environment-variables-on-linux\">Ollama docs</a></p></li><li><p>Ensure that the port (default is 11434) is not blocked by your firewall.</p></li></ul></li></ol><p>If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser's search bar, follow these steps:</p><ol><li>Open your browser's settings.</li><li>Navigate to the 'Search Engines' section.</li><li>Add a new site search with the following URL: <code>http://localhost:3000/?q=%s</code>. Replace  with your IP address or domain name, and  with the port number if Perplexica is not hosted locally.</li><li>Click the add button. Now, you can use Perplexica directly from your browser's search bar.</li></ol><p>Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.</p><p>For more details, check out the full documentation <a href=\"https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md\">here</a>.</p><h2>Expose Perplexica to network</h2><p>Perplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.</p><p>If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.</p><p>We also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!</p><table><tbody><tr><td>Address: <code>0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD</code></td></tr></tbody></table><p>Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the <a href=\"https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/CONTRIBUTING.md\">CONTRIBUTING.md</a> file to learn more about Perplexica and how you can contribute to it.</p><p>If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. <a href=\"https://discord.gg/EFwsmQDgAu\">Click here</a> to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at .</p><p>Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don't forget to check back for updates and new features!</p>","contentLength":7277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"zyronon/typing-word","url":"https://github.com/zyronon/typing-word","date":1751251237,"author":"","guid":175489,"unread":true,"content":"<p>ÂèØ‰ª•ÈÄâÊã©ËÆ∞ÂøÜÊàñÈªòÂÜôÂçïËØçÔºåÊèê‰æõ‰∫ÜÈü≥Ê†áÊòæÁ§∫„ÄÅÂèëÈü≥ÂäüËÉΩÔºàÂùáÂèØÈÄâÁæéÈü≥„ÄÅËã±Èü≥Ôºâ„ÄÅÈîôËØØÁªüËÆ°</p><p>ÂÜÖÁΩÆÁªèÂÖ∏ÊïôÊùê‰π¶Á±çÔºåÂèØ‰ª•ÁªÉ‰π†ÂíåËÉåËØµÊñáÁ´†ÔºåÈÄêÂè•ËæìÂÖ•ÔºåËá™Âä®ÂèëÈü≥„ÄÇ‰πüÂèØ‰ª•Ëá™Ë°åÊ∑ªÂä†„ÄÅÂØºÂÖ•ÊñáÁ´†ÔºåÊèê‰æõ‰∏ÄÈîÆÁøªËØë„ÄÅËØëÊñáÂØπÁÖßÂäüËÉΩ</p><p>ÈªòÂÜôÂçïËØçÊó∂ËæìÂÖ•ÈîôËØØ‰ºöËá™Âä®Ê∑ªÂä†Âà∞ÈîôËØçÊú¨Ôºå‰ª•‰æøÂêéÁª≠Â§ç‰π†„ÄÇ‰πüÂèØ‰ª•Ê∑ªÂä†Âà∞ÁÆÄÂçïËØçÔºå‰πãÂêéÂÜçÈÅáÂà∞Ëøô‰∏™ËØç‰æø‰ºöËá™Âä®Ë∑≥ËøáÔºåÂêåÊó∂‰πüÂèØ‰ª•Â∞ÜÂÖ∂Ê∑ªÂä†Âà∞ÁîüËØçÊú¨‰∏≠Ôºå‰ª•‰æøÂ∑©Âõ∫Â§ç‰π†</p><p>Âú®Áî®Êà∑ÂÆåÊàê‰∏Ä‰∏™Á´†ËäÇÁöÑÁªÉ‰π†ÂêéÔºåÂ¶ÇÊûúÊúâÈîôËØØËØçÔºåÈÇ£‰πà‰ºöÈáçÂ§çÁªÉ‰π†ÈîôËØØËØçÔºåÁõ¥Âà∞Ê≤°ÊúâÈîôËØØËØç‰∏∫Ê≠¢„ÄÇÂÆåÊàê‰πãÂêéÂºπÂá∫ÈÄâÈ°πÂèØÈÄâÊã©ÈªòÂÜôÊú¨Á´†„ÄÅÈáçÂ§çÊú¨Á´†„ÄÅ‰∏ã‰∏ÄÁ´†</p><p>ÂÜÖÁΩÆ‰∫ÜÂ∏∏Áî®ÁöÑ CET-4 „ÄÅCET-6 „ÄÅGMAT „ÄÅGRE „ÄÅIELTS „ÄÅSAT „ÄÅTOEFL „ÄÅËÄÉÁ†îËã±ËØ≠„ÄÅ‰∏ì‰∏öÂõõÁ∫ßËã±ËØ≠„ÄÅ‰∏ì‰∏öÂÖ´Á∫ßËã±ËØ≠Ôºå‰πüÊúâÁ®ãÂ∫èÂëòÂ∏∏ËßÅËã±ËØ≠ÂçïËØç‰ª•ÂèäÂ§öÁßçÁºñÁ®ãËØ≠Ë®Ä API Á≠âËØçÂ∫ì„ÄÇ Â∞ΩÂèØËÉΩÊª°Ë∂≥Â§ßÈÉ®ÂàÜÁî®Êà∑ÂØπËÉåÂçïËØçÁöÑÈúÄÊ±ÇÔºå‰πüÈùûÂ∏∏Ê¨¢ËøéÁ§æÂå∫Ë¥°ÁåÆÊõ¥Â§öÁöÑËØçÂ∫ì„ÄÇ</p><p>Êú¨È°πÁõÆÊòØÂü∫‰∫éÂºÄÂèëÁöÑÔºåÈúÄË¶Å node ÁéØÂ¢ÉÊù•ËøêË°å„ÄÇ</p><p>ÂàõÂª∫ÈïúÂÉè docker build -t typing-word:001 .</p><p>ÂêØÂä®ÂÆπÂô® docker run --name typing-word -p 3000:3000 -d typing-word:001</p><ul><li>CET-4„ÄÅCET-6„ÄÅGMAT„ÄÅGRE„ÄÅIELTS„ÄÅSAT„ÄÅTOEFL„ÄÅBEC</li></ul><ul><li>JavaScript API„ÄÅNode.js API„ÄÅJava API„ÄÅLinux Command„ÄÅC#: List API</li></ul><p>Â¶ÇÊûúÊÇ®ÈúÄË¶ÅËÉåËØµÂÖ∂‰ªñËØçÂ∫ìÔºåÊ¨¢ËøéÂú® Issue ‰∏≠ÊèêÂá∫</p><p>ÁõÆÂâçÈ°πÁõÆÂ§Ñ‰∫éÂºÄÂèëÂàùÊúüÔºåÊñ∞ÂäüËÉΩÊ≠£Âú®ÊåÅÁª≠Ê∑ªÂä†‰∏≠ÔºåÂ¶ÇÊûú‰Ω†ÂØπËΩØ‰ª∂Êúâ‰ªª‰ΩïÂäüËÉΩ‰∏éÂª∫ËÆÆÔºåÊ¨¢ËøéÂú® Issues ‰∏≠ÊèêÂá∫ Â¶ÇÊûú‰Ω†‰πüÂñúÊ¨¢Êú¨ËΩØ‰ª∂ÁöÑËÆæËÆ°ÊÄùÊÉ≥ÔºåÊ¨¢ËøéÊèê‰∫§ prÔºåÈùûÂ∏∏ÊÑüË∞¢‰Ω†ÂØπÊàë‰ª¨ÁöÑÊîØÊåÅÔºÅ</p><p><a href=\"https://github.com/RealKai42/qwerty-learner/\">qwerty-learner</a> ÂæàÂñúÊ¨¢‰ΩúËÄÖÁöÑËøô‰∏™È°πÁõÆÔºå‰ΩÜÊòØÂÆÉÊ≤°ÊúâËÉåÂçïËØçÊâÄÂøÖÂ§áÁöÑ  ÁöÑÂäüËÉΩÔºåÂèØËÉΩÊòØ‰ΩúËÄÖÂèçÂ§çÂº∫Ë∞ÉÂíåÊèêÈÜíËøô‰∏™È°πÁõÆÊòØ‚Äú‚ÄùËÄå‰∏çÊòØ‰∏Ä‰∏™‚Äú‚ÄùÁöÑËΩØ‰ª∂ÂêßÔºåÂ∞ΩÁÆ°ÁªùÂ§ßÂ§öÊï∞Áî®Êà∑ÈÉΩÊòØÁî®ÂÆÉÊù•ËÉåÂçïËØçüòÇüòÇüòÇ„ÄÇ</p><p>Êú¨È°πÁõÆÂèÇËÄÉÂÖ∂ÊÄùË∑Ø‰ΩøÁî® Vue ÈáçÂÜô‰∫ÜÔºåÂπ∂Ê∑ªÂä†‰∫Ü  „ÄÅ  Á≠âÂäüËÉΩ</p>","contentLength":1935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"google-gemini/cookbook","url":"https://github.com/google-gemini/cookbook","date":1751165055,"author":"","guid":174813,"unread":true,"content":"<p>Examples and guides for using the Gemini API</p><p>This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.</p><p>This cookbook is organized into two main categories:</p><ol><li> Practical use cases demonstrating how to combine multiple features.</li></ol><p>We also showcase  in separate repositories, illustrating end-to-end applications of the Gemini API. </p><p>Here are the recent additions and updates to the Gemini API and the Cookbook:</p><ul><li> Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the <a href=\"https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb\">Get Started Guide</a> and the <a href=\"https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_thinking.ipynb\">thinking guide</a> as they'll all be thinking ones.</li><li>: Get started with podcast and music generation with the <a href=\"https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_TTS.ipynb\">TTS</a> and <a href=\"https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LyriaRealTime.ipynb\">Lyria RealTime</a> models.</li><li><ul><li><a href=\"https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Browser_as_a_tool.ipynb\">Browser as a tool</a>: Use a web browser for live and internal (intranet) web interactions</li><li><a href=\"https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Grounding.ipynb\">Grounding</a>: Discover different ways to ground Gemini's answer using different tools, from Google Search to Youtube and the new url context tool.</li></ul></li></ul><p>The <a href=\"https://github.com/google-gemini/cookbook/tree/main/quickstarts/\">quickstarts section</a> contains step-by-step tutorials to get you started with Gemini and learn about its specific features.</p><p>We recommend starting with the following:</p><ul><li><a href=\"https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb\"></a>: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input. </li></ul><p>Then, explore the other quickstarts tutorials to learn about individual features:</p><h2>2. Examples (Practical Use Cases)</h2><p>These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications.</p><h2>3. Demos (End-to-End Applications)</h2><p>These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.</p><p>The Gemini API is a REST API. You can call it directly using tools like  (see <a href=\"https://github.com/google-gemini/cookbook/tree/main/quickstarts/rest/\">REST examples</a> or the great <a href=\"https://www.postman.com/ai-on-postman/google-gemini-apis/overview\">Postman workspace</a>), or use one of our official SDKs:</p><p>The  package will continue to support the original Gemini models. It  also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.</p><h2>The Gemini API on Google Cloud Vertex AI</h2><p>For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See <a href=\"https://github.com/GoogleCloudPlatform/generative-ai\">this repo</a> for examples.</p><p>Thank you for developing with the Gemini API! We're excited to see what you create.</p>","contentLength":2178,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"pot-app/pot-desktop","url":"https://github.com/pot-app/pot-desktop","date":1751165055,"author":"","guid":174814,"unread":true,"content":"<p>üåà‰∏Ä‰∏™Ë∑®Âπ≥Âè∞ÁöÑÂàíËØçÁøªËØëÂíåOCRËΩØ‰ª∂ | A cross-platform software for text translation and recognition.</p><img width=\"200px\" src=\"https://raw.githubusercontent.com/pot-app/pot-desktop/master/public/icon.svg?sanitize=true\" align=\"left\"><p>ËΩØ‰ª∂ÂÜÖÁΩÆÊé•Âè£Êï∞ÈáèÊúâÈôêÔºå‰ΩÜÊòØÊÇ®ÂèØ‰ª•ÈÄöËøáÊèí‰ª∂Á≥ªÁªüÊù•Êâ©Â±ïËΩØ‰ª∂ÁöÑÂäüËÉΩ„ÄÇ</p><p>pot Êèí‰ª∂ÁöÑÊâ©Â±ïÂêç‰∏∫ , ‰∏ãËΩΩÂæóÂà∞Êñá‰ª∂‰πãÂêéÔºå Âú® ÂÅèÂ•ΩËÆæÁΩÆ-ÊúçÂä°ËÆæÁΩÆ-Ê∑ªÂä†Â§ñÈÉ®Êèí‰ª∂-ÂÆâË£ÖÂ§ñÈÉ®Êèí‰ª∂ ÈÄâÊã©ÂØπÂ∫îÁöÑ  Âç≥ÂèØÂÆâË£ÖÊàêÂäüÔºåÊ∑ªÂä†Âà∞ÊúçÂä°ÂàóË°®‰∏≠Âç≥ÂèØÂÉèÂÜÖÁΩÆÊúçÂä°‰∏ÄÊ†∑Ê≠£Â∏∏‰ΩøÁî®‰∫Ü„ÄÇ</p><ul><li><p>Âá∫Áé∞Á±ª‰ººËøôÊ†∑ÁöÑÊä•ÈîôÊòØÂõ†‰∏∫Á≥ªÁªüÁº∫Â∞ë C++Â∫ìÔºåÂâçÂæÄ<a href=\"https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170#visual-studio-2015-2017-2019-and-2022\">ËøôÈáå</a>ÂÆâË£ÖÂç≥ÂèØËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ</p></li><li><p>‰∏çÊòØÊúâÊïàÁöÑ Win32 Â∫îÁî®Á®ãÂ∫è (Windows)</p><p>Âá∫Áé∞Á±ª‰ººËøôÊ†∑ÁöÑÊä•ÈîôËØ¥Êòé‰Ω†Ê≤°Êúâ‰∏ãËΩΩÂØπÂ∫îÁ≥ªÁªüÊàñËÄÖÊû∂ÊûÑÁöÑÊèí‰ª∂ÔºåÂâçÂæÄÊèí‰ª∂‰ªìÂ∫ì‰∏ãËΩΩÊ≠£Á°ÆÁöÑÊèí‰ª∂Âç≥ÂèØËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ</p></li></ul><pre><code>winget install Pylogmon.pot\n</code></pre><ol><li><ul><li>64 ‰ΩçÊú∫Âô®‰∏ãËΩΩ <code>pot_{version}_x64-setup.exe</code></li><li>32 ‰ΩçÊú∫Âô®‰∏ãËΩΩ <code>pot_{version}_x86-setup.exe</code></li><li>arm64 Êú∫Âô®‰∏ãËΩΩ <code>pot_{version}_arm64-setup.exe</code></li></ul></li></ol><ul><li><p>Ê£ÄÊü•ÊòØÂê¶Âç∏ËΩΩ/Á¶ÅÁî®‰∫Ü WebView2ÔºåÂ¶ÇÊûúÂç∏ËΩΩ/Á¶ÅÁî®‰∫Ü WebView2ÔºåËØ∑ÊâãÂä®ÂÆâË£Ö WebView2 ÊàñÂ∞ÜÂÖ∂ÊÅ¢Â§ç„ÄÇ</p><p>Â¶ÇÊûúÊòØ‰ºÅ‰∏öÁâàÁ≥ªÁªü‰∏çÊñπ‰æøÂÆâË£ÖÊàñÊó†Ê≥ïÂÆâË£Ö WebView2ÔºåËØ∑Â∞ùËØïÂú® <a href=\"https://github.com/pot-app/pot-desktop/releases/latest\">Release</a> ‰∏ãËΩΩÂÜÖÁΩÆ WebView2 ÁöÑÁâàÊú¨ <code>pot_{version}_{arch}_fix_webview2_runtime-setup.exe</code></p><p>Ëã•ÈóÆÈ¢ò‰ªçÁÑ∂Â≠òÂú®ËØ∑Â∞ùËØï‰ΩøÁî® Windows7 ÂÖºÂÆπÊ®°ÂºèÂêØÂä®„ÄÇ</p></li></ul><pre><code>brew tap pot-app/homebrew-tap\n</code></pre><ol><li>‰ªé <a href=\"https://github.com/pot-app/pot-desktop/releases/latest\">Release</a> È°µÈù¢‰∏ãËΩΩÊúÄÊñ∞ÁöÑ  ÂÆâË£ÖÂåÖ„ÄÇÔºàÂ¶ÇÊûúÊÇ®‰ΩøÁî®ÁöÑÊòØ M1 ËäØÁâáÔºåËØ∑‰∏ãËΩΩÂêç‰∏∫<code>pot_{version}_aarch64.dmg</code>ÁöÑÂÆâË£ÖÂåÖÔºåÂê¶ÂàôËØ∑‰∏ãËΩΩÂêç‰∏∫ÁöÑÂÆâË£ÖÂåÖÔºâ</li><li>ÂèåÂáª‰∏ãËΩΩÁöÑÊñá‰ª∂ÂêéÂ∞Ü pot ÊãñÂÖ• Applications Êñá‰ª∂Â§πÂç≥ÂèØÂÆåÊàêÂÆâË£Ö„ÄÇ</li></ol><ul><li><p>ÁÇπÂáª ÂèñÊ∂à ÊåâÈíÆÔºåÁÑ∂ÂêéÂéª ËÆæÁΩÆ -&gt; ÈöêÁßÅ‰∏éÂÆâÂÖ®ÊÄß È°µÈù¢ÔºåÁÇπÂáª ‰ªçË¶ÅÊâìÂºÄ ÊåâÈíÆÔºåÁÑ∂ÂêéÂú®ÂºπÂá∫Á™óÂè£ÈáåÁÇπÂáª ÊâìÂºÄ ÊåâÈíÆÂç≥ÂèØÔºå‰ª•ÂêéÊâìÂºÄ pot Â∞±ÂÜç‰πü‰∏ç‰ºöÊúâ‰ªª‰ΩïÂºπÁ™óÂëäË≠¶‰∫Ü</p><p>Â¶ÇÊûúÂú® ÈöêÁßÅ‰∏éÂÆâÂÖ®ÊÄß ‰∏≠Êâæ‰∏çÂà∞‰ª•‰∏äÈÄâÈ°πÔºåÊàñÂêØÂä®Êó∂ÊèêÁ§∫Êñá‰ª∂ÊçüÂùè„ÄÇÊâìÂºÄ Terminal.appÔºåÂπ∂ËæìÂÖ•‰ª•‰∏ãÂëΩ‰ª§ÔºåÁÑ∂ÂêéÈáçÂêØ pot Âç≥ÂèØÔºö</p><pre><code>sudo xattr -d com.apple.quarantine /Applications/pot.app\n</code></pre></li><li><p>Â¶ÇÊûúÊØèÊ¨°ÊâìÂºÄÊó∂ÈÉΩÈÅáÂà∞ËæÖÂä©ÂäüËÉΩÊùÉÈôêÊèêÁ§∫ÔºåÊàñËÄÖÊó†Ê≥ïËøõË°åÂàíËØçÁøªËØëÔºåËØ∑ÂâçÂæÄËÆæÁΩÆ -&gt; ÈöêÁßÅ‰∏éÂÆâÂÖ® -&gt; ËæÖÂä©ÂäüËÉΩÔºåÁßªÈô§ ‚Äúpot‚ÄùÔºåÂπ∂ÈáçÊñ∞Ê∑ªÂä† ‚Äúpot‚Äù„ÄÇ</p></li></ul><ol><li><pre><code>sudo apt-get install ./pot_{version}_amd64.deb\n</code></pre></li></ol><blockquote><p>[!WARNING] Âú®ÊúÄÊñ∞ÁâàÊú¨ÁöÑ <a href=\"https://archlinux.org/packages/extra/x86_64/webkit2gtk\">Webkit2Gtk</a> (2.42.0) ‰∏≠ÔºåÁî±‰∫é Nvidia ‰∏ìÊúâÈ©±Âä®Êú™ÂÆåÂÖ®ÂÆûÁé∞ DMABUFÔºåÂ∞ÜÂØºËá¥Êó†Ê≥ïÂêØÂä®ÂíåÂ¥©Ê∫ÉÁöÑÊÉÖÂÜµÂèëÁîü„ÄÇ ËØ∑ÈôçÁ∫ßÊàñÂú®  ÔºàÊàñËÄÖÂÖ∂‰ªñËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÁöÑÂú∞ÊñπÔºâ‰∏≠Âä†ÂÖ• <code>WEBKIT_DISABLE_DMABUF_RENDERER=1</code> ÁéØÂ¢ÉÂèòÈáèÂÖ≥Èó≠ DMABUF ÁöÑ‰ΩøÁî®„ÄÇ</p></blockquote><pre><code>yay -S pot-translation # Êàñ pot-translation-bin\n\n# paru -S pot-translation # Êàñ pot-translation-bin\n</code></pre><ol start=\"2\"><li>Â¶ÇÊûú‰Ω†‰ΩøÁî®  Ê∫êÔºåÂèØ‰ª•Áõ¥Êé•‰ΩøÁî® pacman ÂÆâË£Ö</li></ol><pre><code>sudo pacman -S pot-translation\n</code></pre><blockquote><p>[!WARNING] Flatpak ÁâàÊú¨Áº∫Â§±ÊâòÁõòÂõæÊ†á„ÄÇ</p></blockquote><a href=\"https://flathub.org/apps/com.pot_app.pot\"><img width=\"240\" alt=\"Download on Flathub\" src=\"https://flathub.org/api/badge?locale=zh-Hans\"></a><p>Pot Êèê‰æõ‰∫ÜÂÆåÊï¥ÁöÑ HTTP Êé•Âè£Ôºå‰ª•‰æøÂèØ‰ª•Ë¢´ÂÖ∂‰ªñËΩØ‰ª∂Ë∞ÉÁî®„ÄÇÊÇ®ÂèØ‰ª•ÈÄöËøáÂêë  ÂèëÈÄÅ HTTP ËØ∑Ê±ÇÊù•Ë∞ÉÁî® potÔºåÂÖ∂‰∏≠ÁöÑÊòØ pot ÁõëÂê¨ÁöÑÁ´ØÂè£Âè∑ÔºåÈªòËÆ§‰∏∫,ÂèØ‰ª•Âú®ËΩØ‰ª∂ËÆæÁΩÆ‰∏≠ËøõË°åÊõ¥Êîπ„ÄÇ</p><pre><code>POST \"/\" =&gt; ÁøªËØëÊåáÂÆöÊñáÊú¨(body‰∏∫ÈúÄË¶ÅÁøªËØëÁöÑÊñáÊú¨),\nGET \"/config\" =&gt; ÊâìÂºÄËÆæÁΩÆ,\nPOST \"/translate\" =&gt; ÁøªËØëÊåáÂÆöÊñáÊú¨(Âêå\"/\"),\nGET \"/selection_translate\" =&gt; ÂàíËØçÁøªËØë,\nGET \"/input_translate\" =&gt; ËæìÂÖ•ÁøªËØë,\nGET \"/ocr_recognize\" =&gt; Êà™ÂõæOCR,\nGET \"/ocr_translate\" =&gt; Êà™ÂõæÁøªËØë,\nGET \"/ocr_recognize?screenshot=false\" =&gt; Êà™ÂõæOCR(‰∏ç‰ΩøÁî®ËΩØ‰ª∂ÂÜÖÊà™Âõæ),\nGET \"/ocr_translate?screenshot=false\" =&gt; Êà™ÂõæÁøªËØë(‰∏ç‰ΩøÁî®ËΩØ‰ª∂ÂÜÖÊà™Âõæ),\nGET \"/ocr_recognize?screenshot=true\" =&gt; Êà™ÂõæOCR,\nGET \"/ocr_translate?screenshot=true\" =&gt; Êà™ÂõæÁøªËØë,\n</code></pre><ul><li><p>Â¶ÇÊûúÊÉ≥Ë¶ÅË∞ÉÁî® pot ÂàíËØçÁøªËØëÔºåÂè™ÈúÄÂêëÂèëÈÄÅËØ∑Ê±ÇÂç≥ÂèØ„ÄÇ</p><pre><code>curl \"127.0.0.1:60828/selection_translate\"\n</code></pre></li></ul><p>Ëøô‰∏ÄÂäüËÉΩÂèØ‰ª•ËÆ©ÊÇ®Âú®‰∏ç‰ΩøÁî®ËΩØ‰ª∂ÂÜÖÊà™ÂõæÁöÑÊÉÖÂÜµ‰∏ãË∞ÉÁî®Êà™Âõæ OCR/Êà™ÂõæÁøªËØëÂäüËÉΩÔºåËøôÊ†∑ÊÇ®Â∞±ÂèØ‰ª•‰ΩøÁî®ÊÇ®ÂñúÊ¨¢ÁöÑÊà™ÂõæÂ∑•ÂÖ∑Êù•Êà™Âõæ‰∫ÜÔºå‰πüÂèØ‰ª•Ëß£ÂÜ≥Âú®Êüê‰∫õÂπ≥Âè∞‰∏ã pot Ëá™Â∏¶ÁöÑÊà™ÂõæÊó†Ê≥ï‰ΩøÁî®ÁöÑÈóÆÈ¢ò„ÄÇ</p><ol><li>Â∞ÜÊà™Âõæ‰øùÂ≠òÂú® <code>$CACHE/com.pot-app.desktop/pot_screenshot_cut.png</code></li><li>Âêë<code>127.0.0.1:port/ocr_recognize?screenshot=false</code>ÂèëÈÄÅËØ∑Ê±ÇÂç≥ÂèØË∞ÉÁî®ÊàêÂäü</li></ol><blockquote><p>‰∏∫Á≥ªÁªüÁºìÂ≠òÁõÆÂΩïÔºå‰æãÂ¶ÇÂú® Windows ‰∏ä‰∏∫<code>C:\\Users\\{Áî®Êà∑Âêç}\\AppData\\Local\\com.pot-app.desktop\\pot_screenshot_cut.png</code></p></blockquote><p>Âú® Linux ‰∏ãË∞ÉÁî® Flameshot ËøõË°åÊà™Âõæ OCR:</p><pre><code>rm ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; flameshot gui -s -p ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; curl \"127.0.0.1:60828/ocr_recognize?screenshot=false\"\n</code></pre><ol><li>‰ªé <a href=\"https://github.com/pot-app/pot-desktop/releases/latest\">Release</a> ‰∏ãËΩΩ pot ÁöÑ SnipDo Êâ©Â±ï (pot.pbar)</li><li>ÈÄâ‰∏≠ÊñáÂ≠óÔºåÂèØ‰ª•ÁúãÂà∞ÂºπÂá∫ÁöÑ SnipDo Â∑•ÂÖ∑Êù°ÔºåÁÇπÂáªÁøªËØëÊåâÈíÆÂç≥ÂèØÁøªËØë„ÄÇ</li></ol><ol><li>‰ªé <a href=\"https://github.com/pot-app/pot-desktop/releases/latest\">Release</a> ‰∏ãËΩΩ pot ÁöÑ PopClip Êâ©Â±ï (pot.popclipextz)</li><li>Âú® PopClip ÁöÑÊâ©Â±ï‰∏≠ÂêØÁî® pot Êâ©Â±ïÔºåÈÄâ‰∏≠ÊñáÊú¨Âç≥ÂèØÁÇπÂáªÁøªËØë„ÄÇ</li></ol><blockquote><p>Starry ÁõÆÂâç‰ªçÂ§Ñ‰∫éÂºÄÂèëÈò∂ÊÆµÔºåÂõ†Ê≠§ÊÇ®Âè™ËÉΩÊâãÂä®ÁºñËØëÂÆÉ„ÄÇ</p></blockquote><p>Áî±‰∫éÂêÑÂ§ßÂèëË°åÁâàÂØπ‰∫é Wayland ÁöÑÊîØÊåÅÁ®ãÂ∫¶‰∏çÂêåÔºåÊâÄ‰ª• pot Êú¨Ë∫´Ê≤°Ê≥ïÂÅöÂà∞ÁâπÂà´ÂÆåÁæéÁöÑÊîØÊåÅÔºåËøôÈáåÂèØ‰ª•Êèê‰æõ‰∏Ä‰∫õÂ∏∏ËßÅÈóÆÈ¢òÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÈÄöËøáÂêàÁêÜÁöÑËÆæÁΩÆ‰πãÂêéÔºåpot ‰πüÂèØ‰ª•Âú® Wayland ‰∏ãÂÆåÁæéËøêË°å„ÄÇ</p><p>Áî±‰∫é Tauri ÁöÑÂø´Êç∑ÈîÆÊñπÊ°àÂπ∂Ê≤°ÊúâÊîØÊåÅ WaylandÔºåÊâÄ‰ª• pot Â∫îÁî®ÂÜÖÁöÑÂø´Êç∑ÈîÆËÆæÁΩÆÂú® Wayland ‰∏ãÊó†Ê≥ï‰ΩøÁî®„ÄÇ ÊÇ®ÂèØ‰ª•ËÆæÁΩÆÁ≥ªÁªüÂø´Êç∑Áî® curl ÂèëÈÄÅËØ∑Ê±ÇÊù•Ëß¶Âèë potÔºåËØ¶ËßÅ<a href=\"https://raw.githubusercontent.com/pot-app/pot-desktop/master/#%E5%A4%96%E9%83%A8%E8%B0%83%E7%94%A8\">Â§ñÈÉ®Ë∞ÉÁî®</a></p><p>Âú®‰∏Ä‰∫õÁ∫Ø Wayland Ê°åÈù¢ÁéØÂ¢É/Á™óÂè£ÁÆ°ÁêÜÂô®(Â¶Ç Hyprland)‰∏äÔºåpot ÂÜÖÁΩÆÁöÑÊà™ÂõæÊó†Ê≥ï‰ΩøÁî®ÔºåËøôÊó∂ÂèØ‰ª•ÈÄöËøá‰ΩøÁî®ÂÖ∂‰ªñÊà™ÂõæÂ∑•ÂÖ∑‰ª£ÊõøÔºåËØ¶ËßÅ <a href=\"https://raw.githubusercontent.com/pot-app/pot-desktop/master/#%E4%B8%8D%E4%BD%BF%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%86%85%E6%88%AA%E5%9B%BE\">‰∏ç‰ΩøÁî®ËΩØ‰ª∂ÂÜÖÊà™Âõæ</a></p><p>‰∏ãÈù¢ÁªôÂá∫Âú® Hyprland ‰∏ãÁöÑÈÖçÁΩÆÁ§∫‰æã(ÈÄöËøá grim Âíå slurp ÂÆûÁé∞Êà™Âõæ)Ôºö</p><pre><code>bind = ALT, X, exec, grim -g \"$(slurp)\" ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; curl \"127.0.0.1:60828/ocr_recognize?screenshot=false\"\nbind = ALT, C, exec, grim -g \"$(slurp)\" ~/.cache/com.pot-app.desktop/pot_screenshot_cut.png &amp;&amp; curl \"127.0.0.1:60828/ocr_translate?screenshot=false\"\n</code></pre><p>Áî±‰∫éÁõÆÂâç pot Âú® Wayland ‰∏ãËøòÊó†Ê≥ïËé∑ÂèñÂà∞Ê≠£Á°ÆÁöÑÈº†Ê†áÂùêÊ†áÔºåÊâÄ‰ª•ÂÜÖÈÉ®ÁöÑÂÆûÁé∞Êó†Ê≥ïÂ∑•‰Ωú„ÄÇ ÂØπ‰∫éÊüê‰∫õÊ°åÈù¢ÁéØÂ¢É/Á™óÂè£ÁÆ°ÁêÜÂô®ÔºåÂèØ‰ª•ÈÄöËøáËÆæÁΩÆÁ™óÂè£ËßÑÂàôÊù•ÂÆûÁé∞Á™óÂè£Ë∑üÈöèÈº†Ê†á‰ΩçÁΩÆÔºåËøôÈáå‰ª• Hyprland ‰∏∫‰æãÔºö</p><pre><code>windowrulev2 = float, class:(pot), title:(Translator|OCR|PopClip|Screenshot Translate) # Translation window floating\nwindowrulev2 = move cursor 0 0, class:(pot), title:(Translator|PopClip|Screenshot Translate) # Translation window follows the mouse position.\n</code></pre><img src=\"https://github.com/pot-app/.github/raw/master/pot-desktop-contributions.svg?raw=true\" width=\"100%\"><ol><li><pre><code>git clone https://github.com/pot-app/pot-desktop.git\n</code></pre></li><li><pre><code>cd pot-desktop\npnpm install\n</code></pre></li><li><pre><code>sudo apt-get install -y libgtk-3-dev libwebkit2gtk-4.0-dev libayatana-appindicator3-dev librsvg2-dev patchelf libxdo-dev libxcb1 libxrandr2 libdbus-1-3\n</code></pre></li><li><pre><code>pnpm tauri dev # Run the app in development mode\n</code></pre></li><li><pre><code>pnpm tauri build # Build into installation package\n</code></pre></li></ol>","contentLength":6560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"actions/actions-runner-controller","url":"https://github.com/actions/actions-runner-controller","date":1751165055,"author":"","guid":174815,"unread":true,"content":"<p>Kubernetes controller for GitHub Actions self-hosted runners</p><p>Actions Runner Controller (ARC) is a Kubernetes operator that orchestrates and scales self-hosted runners for GitHub Actions.</p><p>With ARC, you can create runner scale sets that automatically scale based on the number of workflows running in your repository, organization, or enterprise. Because controlled runners can be ephemeral and based on containers, new runner instances can scale up or down rapidly and cleanly. For more information about autoscaling, see <a href=\"https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/autoscaling-with-self-hosted-runners\">\"Autoscaling with self-hosted runners.\"</a></p><p>Actions Runner Controller (ARC) is an open-source project currently developed and maintained in collaboration with the GitHub Actions team, external maintainers @mumoshu and @toast-gear, various <a href=\"https://github.com/actions/actions-runner-controller/graphs/contributors\">contributors</a>, and the <a href=\"https://github.com/actions/actions-runner-controller/discussions\">awesome community</a>.</p><p>If you think the project is awesome and is adding value to your business, please consider directly sponsoring <a href=\"https://github.com/sponsors/actions-runner-controller\">community maintainers</a> and individual contributors via GitHub Sponsors.</p><p>If you are already the employer of one of the contributors, sponsoring via GitHub Sponsors might not be an option. Just support them by other means!</p><p>To give ARC a try with just a handful of commands, please refer to the <a href=\"https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/quickstart-for-actions-runner-controller\">Quickstart guide</a>.</p><p>For an overview of ARC, please refer to <a href=\"https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/about-actions-runner-controller\">About ARC</a>.</p><p>The following documentation is for the legacy autoscaling modes that continue to be maintained by the community:</p><p>We welcome contributions from the community. For more details on contributing to the project (including requirements), please refer to \"<a href=\"https://github.com/actions/actions-runner-controller/raw/master/CONTRIBUTING.md\">Getting Started with Contributing</a>.\"</p><p>We are very happy to help you with any issues you have. Please refer to the \"<a href=\"https://github.com/actions/actions-runner-controller/raw/master/TROUBLESHOOTING.md\">Troubleshooting</a>\" section for common issues.</p>","contentLength":1662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rommapp/romm","url":"https://github.com/rommapp/romm","date":1751165055,"author":"","guid":174816,"unread":true,"content":"<p>A beautiful, powerful, self-hosted rom manager and player.</p><p>RomM (ROM Manager) allows you to scan, enrich, browse and play your game collection with a clean and responsive interface. With support for multiple platforms, various naming schemes, and custom tags, RomM is a must-have for anyone who plays on emulators.</p><ul><li>Share your library with friends with limited access and permissions</li><li>Supports multi-disk games, DLCs, mods, hacks, patches, and manuals</li><li>Parse and filter by <a href=\"https://docs.romm.app/latest/Getting-Started/Folder-Structure/#tag-support\">tags</a> in filenames</li><li>View, upload, update, and delete games from any modern web browser</li></ul><p>Here are a few projects maintained by members of our community. Please note that the RomM team does not regularly review their source code.</p><p>Join us on Discord, where you can ask questions, submit ideas, get help, showcase your collection, and discuss RomM with other users.</p><p>If you have any issues with RomM, please <a href=\"https://github.com/rommapp/romm/issues/new\">open an issue</a> in this repository.</p><p>Consider supporting the development of this project on Open Collective.</p><p>Here are a few projects that we think you might like:</p><ul><li><a href=\"https://github.com/gaseous-project/gaseous-server\">Gaseous</a>: Another ROM manager with web-based emulator</li><li><a href=\"https://github.com/JMBeresford/retrom\">Retrom</a>: A centralized game library/collection management service</li></ul>","contentLength":1135,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"swisskyrepo/PayloadsAllTheThings","url":"https://github.com/swisskyrepo/PayloadsAllTheThings","date":1751165055,"author":"","guid":174817,"unread":true,"content":"<p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p><p>A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques ! I  pull requests :)</p><p>You can also contribute with a  IRL, or using the sponsor button</p><p>Every section contains the following files, you can use the  folder to create a new chapter:</p><ul><li>README.md - vulnerability description and how to exploit it, including several payloads</li><li>Intruder - a set of files to give to Burp Intruder</li><li>Images - pictures for the README.md</li><li>Files - some files referenced in the README.md</li></ul><p>You might also like the other projects from the AllTheThings family :</p><p>Thanks again for your contribution! </p><p>This project is proudly sponsored by these companies:</p>","contentLength":758,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serial-Studio/Serial-Studio","url":"https://github.com/Serial-Studio/Serial-Studio","date":1751165055,"author":"","guid":174818,"unread":true,"content":"<p>Visualize embedded device data.</p><p> is an open-core, cross-platform telemetry dashboard and real-time data visualization tool. It supports input from serial ports, Bluetooth Low Energy (BLE), MQTT, and TCP/UDP sockets, allowing data acquisition from embedded devices, external software, and networked services.</p><p>Serial Studio runs on Windows, macOS, and Linux. It is suited for telemetry monitoring, sensor data analysis, and real-time debugging in educational, hobbyist, and professional environments.</p><p>Serial Studio is available as source code and official precompiled binaries for Windows, macOS, and Linux.</p><p>Distributed as a universal DMG. Open the DMG file and drag  into the  folder. Alternatively, you can try installing via Homebrew:</p><pre><code>brew install --cask serial-studio\n</code></pre><p> The Homebrew cask is community-maintained. It‚Äôs available, but not officially developed or tested by me.</p><p>The recommended way to install Serial Studio on Linux is via the official pre-built <a href=\"https://appimage.org/\">AppImage</a>. Make it executable and run it:</p><pre><code>chmod +x SerialStudio-3.1.7-Linux-x86_64.AppImage\n./SerialStudio-3.1.7-Linux-x86_64.AppImage\n</code></pre><p>If the AppImage fails to launch, your system may be missing :</p><pre><code>sudo apt install libfuse2\n</code></pre><p> For better desktop integration (menu entries, updates, icons), use <a href=\"https://github.com/TheAssassin/AppImageLauncher\">AppImageLauncher</a>.</p><p>Serial Studio is also available on <a href=\"https://flathub.org/apps/com.serial_studio.Serial-Studio\">Flathub</a>. This version receives regular updates and may offer better support for ARM64 systems. However, minor graphical glitches may occur on some desktop environments‚Äîespecially under Wayland (e.g., missing window shadows).</p><p>An ARM64 AppImage is available for Raspberry Pi and similar devices. Performance varies based on hardware and GPU drivers, since the UI depends on GPU acceleration. The ARM64 AppImage requires:</p><ul><li>A 64-bit Linux OS equivalent to or newer than  (due to a  dependency)</li></ul><p>Make sure your system meets these requirements before running the AppImage.</p><ul><li><strong>Project File Mode (recommended):</strong> Uses local JSON files created with the  to define the dashboard layout and data mapping.</li><li> Automatically plots comma-separated values with no configuration.</li><li> Dashboards are fully defined by incoming JSON data from the device.</li></ul><ul><li> Runs on Windows, macOS, and Linux.</li><li> Save received data for offline analysis or processing.</li><li> Supports serial ports, MQTT, BLE, and network sockets (TCP/UDP).</li><li><strong>Customizable visualization:</strong> Build dashboards using various widgets via the integrated project editor.</li><li> Use a custom JavaScript function to preprocess raw data or handle complex binary formats.</li><li> Publish and receive data over the internet for remote visualization.</li></ul><p>Refer to the <a href=\"https://github.com/Serial-Studio/Serial-Studio/wiki\">Wiki</a> for complete guides and examples:</p><ul><li> Instructions for Windows, macOS, and Linux.</li><li> Connect a device and visualize data in minutes.</li><li> Learn about data flow, frame parsing, and dashboard customization.</li><li> Sample code and projects to accelerate learning.</li></ul><p>The only required dependency to build Serial Studio from source is <a href=\"https://www.qt.io/download-open-source/\">Qt</a>, preferably with all modules and plugins installed. The project is built using .</p><h4>Additional Requirements for Linux</h4><p>If you‚Äôre compiling on Linux, install the following packages:</p><pre><code>sudo apt install libgl1-mesa-dev build-essential\n</code></pre><p>Once Qt is installed, you can compile the project by opening  in your preferred IDE or using the terminal:</p><pre><code>mkdir build\ncd build\ncmake ../ -DPRODUCTION_OPTIMIZATION=ON -DCMAKE_BUILD_TYPE=Release\ncmake --build . -j$(nproc)\n</code></pre><p>By default, the build system produces a fully GPLv3-compliant version of Serial Studio. This version includes most core features but excludes commercial modules such as MQTT, 3D visualization, XY plotting, and other advanced tools that depend on proprietary Qt components.</p><p>Serial Studio is developed and maintained by <a href=\"https://github.com/alex-spataru\">Alex Spataru</a>. It is open source and community-driven, with commercial options available for users who need advanced features or business-friendly licensing.</p><p>If Serial Studio is useful to you, consider supporting its development in one of the following ways:</p><p>Commercial licenses directly fund continued development, bug fixes, and new features.</p><p>Serial Studio uses a  that distinguishes between open-source usage and commercial distribution:</p><p>Source files are individually marked with SPDX headers indicating whether they are:</p><ul><li>Licensed under </li><li>Licensed under <code>LicenseRef-SerialStudio-Commercial</code></li><li>Or dual-licensed as <code>GPL-3.0-only OR LicenseRef-SerialStudio-Commercial</code></li></ul><p>This structure allows developers to build and distribute GPL-compliant versions while protecting commercial functionality.</p><h2>Choosing the Right Version of Serial Studio</h2><p>The table below outlines licensing, feature access, and obligations across each edition:</p><table><thead><tr><th>GPL Version </th><th>Trial Version </th><th>Pro Version <em>(Activated official binary)</em></th></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr><td>Must comply with GPL and Qt terms</td><td>14-day trial, no redistribution</td><td>Bound by commercial license terms</td></tr><tr><td>‚úÖ Provided for trial only</td></tr><tr><td>Requires GPL-compatible Qt</td><td>Qt licensing covered by vendor</td><td>Qt licensing covered by vendor</td></tr><tr><td>‚úÖ Trial disables after 14 days</td><td>‚úÖ Requires valid license key</td></tr><tr><td>‚úÖ If strictly GPL compliant</td></tr><tr><td>OSS devs, students, contributors</td><td>Hobbyists, personal evaluation</td><td>Businesses, teams, commercial products</td></tr></tbody></table><p> Pro features and official binaries are proprietary and require a commercial license for any use beyond personal evaluation. Visibility of source code does  imply GPL rights unless explicitly licensed.</p>","contentLength":5193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"midday-ai/midday","url":"https://github.com/midday-ai/midday","date":1751165055,"author":"","guid":174819,"unread":true,"content":"<p>Invoicing, Time tracking, File reconciliation, Storage, Financial Overview &amp; your own Assistant made for Freelancers</p><p>Midday is an all-in-one tool designed to help freelancers, contractors, consultants, and solo entrepreneurs manage their business operations more efficiently. It integrates various functions typically scattered across multiple platforms into a single, cohesive system.</p><p>: Allows for live time tracking of projects to boost productivity and collaboration, providing insightful project overviews.: An upcoming feature that will enable users to create web-based invoices, collaborate in real-time, and synchronize projects seamlessly.: Automatically matches incoming invoices or receipts to the correct transactions, simplifying financial tracking and organization.: Secure storage for important files like contracts and agreements, keeping everything in one place for easy access‚Äã.: Facilitates easy export of financial data, packaged neatly in CSV files for accountants.: Provides tailored insights into financial situations, helping users understand spending patterns, cut costs, and find documents.</p><a href=\"https://news.ycombinator.com/item?id=40737901\"><img width=\"250\" height=\"54\" alt=\"Featured on Hacker News\" src=\"https://hackernews-badge.vercel.app/api?id=40737901\"></a><ul></ul><ul><li>Supabase (database, storage, realtime, auth)</li><li>Vercel (Website, Dashboard)</li></ul><ul><li>Trigger.dev (background jobs)</li><li>Resend (Transactional &amp; Marketing)</li><li>GoCardLess (Bank connection EU)</li><li>Plaid (Bank connection in Canada and US)</li><li>Teller (Bank connection in the US)</li><li>OpenPanel (Events and Analytics)</li><li>Polar (Payment processing)</li></ul><p>This project is licensed under the  for non-commercial use.</p><p>For commercial use or deployments requiring a setup fee, please contact us for a commercial license at <a href=\"mailto:engineer@midday.ai\">engineer@midday.ai</a>.</p><p>By using this software, you agree to the terms of the license.</p>","contentLength":1655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GraphiteEditor/Graphite","url":"https://github.com/GraphiteEditor/Graphite","date":1751165055,"author":"","guid":174820,"unread":true,"content":"<p>An open source graphics editor for 2025: comprehensive 2D content creation tool for graphic design, digital art, and interactive real-time motion graphics ‚Äî featuring node-based procedural editing</p><a href=\"https://graphite.rs/\"></a><p><strong>Graphite is a free, open source vector and raster graphics engine, <a href=\"https://editor.graphite.rs\">available now</a> in alpha. Get creative with a fully nondestructive editing workflow that combines layer-based compositing with node-based generative design.</strong></p><p>Having begun life as a vector editor, Graphite continues evolving into a generalized, all-in-one graphics toolbox that's built more like a game engine than a conventional creative app. The editor's tools wrap its node graph core, providing user-friendly workflows for vector, raster, and beyond. Photo editing, motion graphics, digital painting, desktop publishing, and VFX compositing are additional competencies on the planned <a href=\"https://graphite.rs/features/#roadmap\">roadmap</a> making Graphite into a highly versatile content creation tool.</p><a href=\"https://discord.graphite.rs/\"></a><a href=\"https://www.reddit.com/r/graphite/\"></a><a href=\"https://bsky.app/profile/graphiteeditor.bsky.social\"></a><a href=\"https://twitter.com/graphiteeditor\"></a><a href=\"https://www.youtube.com/@GraphiteEditor\"></a><p>Graphite is 100% community built and funded. Please become a part of keeping the project alive and thriving with a <a href=\"https://graphite.rs/donate/\">donation</a> if you share a belief in our :</p><blockquote><p>Graphite strives to unshackle the creativity of every budding artist and seasoned professional by building the best comprehensive art and design tool that's accessible to all.</p><p>Mission success will come when Graphite is an industry standard. A cohesive product vision and focus on innovation over imitation is the strategy that will make that possible.</p></blockquote><h2>Contributing/building the code</h2><p>Are you a graphics programmer or Rust developer? Graphite aims to be one of the most approachable projects for putting your engineering skills to use in the world of open source. See <a href=\"https://graphite.rs/volunteer/guide/\">instructions here</a> for setting up the project and getting started.</p><p><em>By submitting code for inclusion in the project, you are agreeing to license your changes under the Apache 2.0 license, and that you have the authority to do so. Some directories may have other licenses, like dual-licensed MIT/Apache 2.0, and code submissions to those directories mean you agree to the applicable license(s).</em></p>","contentLength":2025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"fastapi/full-stack-fastapi-template","url":"https://github.com/fastapi/full-stack-fastapi-template","date":1751165055,"author":"","guid":174821,"unread":true,"content":"<p>Full stack, modern web application template. Using FastAPI, React, SQLModel, PostgreSQL, Docker, GitHub Actions, automatic HTTPS and more.</p><h2>Technology Stack and Features</h2><ul><li>‚ö° <a href=\"https://fastapi.tiangolo.com\"></a> for the Python backend API. \n  <ul><li>üß∞ <a href=\"https://sqlmodel.tiangolo.com\">SQLModel</a> for the Python SQL database interactions (ORM).</li><li>üîç <a href=\"https://docs.pydantic.dev\">Pydantic</a>, used by FastAPI, for the data validation and settings management.</li></ul></li><li>üöÄ <a href=\"https://react.dev\">React</a> for the frontend. \n  <ul><li>üíÉ Using TypeScript, hooks, Vite, and other parts of a modern frontend stack.</li><li>ü§ñ An automatically generated frontend client.</li></ul></li><li>üîí Secure password hashing by default.</li><li>üîë JWT (JSON Web Token) authentication.</li><li>üì´ Email based password recovery.</li><li>üìû <a href=\"https://traefik.io\">Traefik</a> as a reverse proxy / load balancer.</li><li>üö¢ Deployment instructions using Docker Compose, including how to set up a frontend Traefik proxy to handle automatic HTTPS certificates.</li><li>üè≠ CI (continuous integration) and CD (continuous deployment) based on GitHub Actions.</li></ul><h3>Dashboard - User Settings</h3><h3>Interactive API Documentation</h3><p>You can  this repository and use it as is.</p><h3>How to Use a Private Repository</h3><p>If you want to have a private repository, GitHub won't allow you to simply fork it as it doesn't allow changing the visibility of forks.</p><p>But you can do the following:</p><ul><li>Create a new GitHub repo, for example .</li><li>Clone this repository manually, set the name with the name of the project you want to use, for example :</li></ul><pre><code>git clone git@github.com:fastapi/full-stack-fastapi-template.git my-full-stack\n</code></pre><ul><li>Enter into the new directory:</li></ul><ul><li>Set the new origin to your new repository, copy it from the GitHub interface, for example:</li></ul><pre><code>git remote set-url origin git@github.com:octocat/my-full-stack.git\n</code></pre><ul><li>Add this repo as another \"remote\" to allow you to get updates later:</li></ul><pre><code>git remote add upstream git@github.com:fastapi/full-stack-fastapi-template.git\n</code></pre><ul><li>Push the code to your new repository:</li></ul><pre><code>git push -u origin master\n</code></pre><h3>Update From the Original Template</h3><p>After cloning the repository, and after doing changes, you might want to get the latest changes from this original template.</p><ul><li>Make sure you added the original repository as a remote, you can check it with:</li></ul><pre><code>git remote -v\n\norigin    git@github.com:octocat/my-full-stack.git (fetch)\norigin    git@github.com:octocat/my-full-stack.git (push)\nupstream    git@github.com:fastapi/full-stack-fastapi-template.git (fetch)\nupstream    git@github.com:fastapi/full-stack-fastapi-template.git (push)\n</code></pre><ul><li>Pull the latest changes without merging:</li></ul><pre><code>git pull --no-commit upstream master\n</code></pre><p>This will download the latest changes from this template without committing them, that way you can check everything is right before committing.</p><ul><li><p>If there are conflicts, solve them in your editor.</p></li><li><p>Once you are done, commit the changes:</p></li></ul><p>You can then update configs in the  files to customize your configurations.</p><p>Before deploying it, make sure you change at least the values for:</p><ul></ul><p>You can (and should) pass these as environment variables from secrets.</p><p>Some environment variables in the  file have a default value of .</p><p>You have to change them with a secret key, to generate secret keys you can run the following command:</p><pre><code>python -c \"import secrets; print(secrets.token_urlsafe(32))\"\n</code></pre><p>Copy the content and use that as password / secret key. And run that again to generate another secure key.</p><h2>How To Use It - Alternative With Copier</h2><p>This repository also supports generating a new project using <a href=\"https://copier.readthedocs.io\">Copier</a>.</p><p>It will copy all the files, ask you configuration questions, and update the  files with your answers.</p><p>You can install Copier with:</p><p>Or better, if you have <a href=\"https://pipx.pypa.io/\"></a>, you can run it with:</p><p>: If you have , installing copier is optional, you could run it directly.</p><h3>Generate a Project With Copier</h3><p>Decide a name for your new project's directory, you will use it below. For example, .</p><p>Go to the directory that will be the parent of your project, and run the command with your project's name:</p><pre><code>copier copy https://github.com/fastapi/full-stack-fastapi-template my-awesome-project --trust\n</code></pre><p>If you have  and you didn't install , you can run it directly:</p><pre><code>pipx run copier copy https://github.com/fastapi/full-stack-fastapi-template my-awesome-project --trust\n</code></pre><p> the  option is necessary to be able to execute a <a href=\"https://github.com/fastapi/full-stack-fastapi-template/raw/master/.copier/update_dotenv.py\">post-creation script</a> that updates your  files.</p><p>Copier will ask you for some data, you might want to have at hand before generating the project.</p><p>But don't worry, you can just update any of that in the  files afterwards.</p><p>The input variables, with their default values (some auto generated) are:</p><ul><li>: (default: ) The name of the project, shown to API users (in .env).</li><li>: (default: ) The name of the stack used for Docker Compose labels and project name (no spaces, no periods) (in .env).</li><li>: (default: ) The secret key for the project, used for security, stored in .env, you can generate one with the method above.</li><li>: (default: ) The email of the first superuser (in .env).</li><li>: (default: ) The password of the first superuser (in .env).</li><li>: (default: \"\") The SMTP server host to send emails, you can set it later in .env.</li><li>: (default: \"\") The SMTP server user to send emails, you can set it later in .env.</li><li>: (default: \"\") The SMTP server password to send emails, you can set it later in .env.</li><li>: (default: ) The email account to send emails from, you can set it later in .env.</li><li>: (default: ) The password for the PostgreSQL database, stored in .env, you can generate one with the method above.</li><li>: (default: \"\") The DSN for Sentry, if you are using it, you can set it later in .env.</li></ul><p>This includes using Docker Compose, custom local domains,  configurations, etc.</p><p>The Full Stack FastAPI Template is licensed under the terms of the MIT license.</p>","contentLength":5464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"adityachandelgit/BookLore","url":"https://github.com/adityachandelgit/BookLore","date":1751165055,"author":"","guid":174822,"unread":true,"content":"<p>BookLore is a web app for hosting, managing, and exploring books, with support for PDFs, eBooks, reading progress, metadata, and stats.</p><p>BookLore is a self-hosted web app for organizing and managing your personal book collection. It provides an intuitive interface to browse, read, and track your progress across PDFs and eBooks. With robust metadata management, multi-user support, and a sleek, modern UI, BookLore makes it easy to build and explore your personal library.</p><ul><li>üìö <strong>Organized Book Management</strong> - Categorize books with  and  for easy discovery and structured organization.</li><li>üß†  - Automatically fetch book details from , , and , or edit them manually with fine-grained control.</li><li>üë•  - Admins can create accounts, assign libraries, and manage permissions for metadata edits, uploads, and downloads.</li><li>üìñ <strong>Built-in PDF &amp; ePub Reader</strong> - A fast, feature-rich reader for PDFs and ePubs, with customizable reading settings and a clean UI.</li><li>üåê  - Browse and download books through the <strong>Open Publication Distribution System</strong> ‚Äì compatible with many reading apps.</li><li>üîê <strong>Optional OIDC Authentication</strong> - Secure access with , supporting both local JWT authentication and external providers like .</li><li>üì§  - Upload multiple books at once with metadata auto-detection and file organization.</li><li>üìß  - Share books directly with others by sending them via email ‚Äì quick and easy.</li><li>üöÄ  - Frequent updates with new features, performance enhancements, and UI improvements. BookLore is perfect for self-hosters who want complete control over their digital library. Stay tuned for updates!</li></ul><h2>üé• Video Guides &amp; Tutorials</h2><p>For a step-by-step walkthrough, check out the official BookLore video guides on YouTube:</p><p>These videos cover deployment, configuration, and feature highlights to help you get started quickly.</p><p>You can quickly set up and run BookLore using Docker.</p><h3>1Ô∏è‚É£ Install Docker &amp; Docker Compose</h3><h3>2Ô∏è‚É£ Create docker-compose.yml</h3><p>Create a  file with content:</p><pre><code>services:\n  booklore:\n    image: ghcr.io/adityachandelgit/booklore-app:latest\n    container_name: booklore\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Etc/UTC\n      - DATABASE_URL=jdbc:mariadb://mariadb:3306/booklore # Only modify this if you're familiar with JDBC and your database setup\n      - DATABASE_USERNAME=booklore # Must match MYSQL_USER defined in the mariadb container\n      - DATABASE_PASSWORD=your_secure_password # Use a strong password; must match MYSQL_PASSWORD defined in the mariadb container \n      - SWAGGER_ENABLED=false # Enable or disable Swagger UI (API docs). Set to 'true' to allow access; 'false' to block access (recommended for production).\n    depends_on:\n      mariadb:\n        condition: service_healthy\n    ports:\n      - \"6060:6060\"\n    volumes:\n      - /your/local/path/to/booklore/data:/app/data\n      - /your/local/path/to/booklore/books:/books\n    restart: unless-stopped\n\n  mariadb:\n    image: lscr.io/linuxserver/mariadb:11.4.5\n    container_name: mariadb\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Etc/UTC\n      - MYSQL_ROOT_PASSWORD=super_secure_password # Use a strong password for the database's root user, should be different from MYSQL_PASSWORD\n      - MYSQL_DATABASE=booklore\n      - MYSQL_USER=booklore # Must match DATABASE_USERNAME defined in the booklore container\n      - MYSQL_PASSWORD=your_secure_password # Use a strong password; must match DATABASE_PASSWORD defined in the booklore container\n    volumes:\n      - /your/local/path/to/mariadb/config:/config\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"mariadb-admin\", \"ping\", \"-h\", \"localhost\"]\n      interval: 5s\n      timeout: 5s\n      retries: 10\n</code></pre><p>Run the following command to start the services:</p><p>Once the containers are up, access BookLore in your browser at:</p><h2>üîë OIDC/OAuth2 Authentication (Authentik, Pocket ID, etc.)</h2><p>BookLore supports optional OIDC/OAuth2 authentication for secure access. This feature allows you to integrate external authentication providers for a seamless login experience.</p><p>While the integration has been tested with  and , it should work with other OIDC providers like  as well. The setup allows you to use either JWT-based local authentication or external providers, giving users the flexibility to choose their preferred method.</p><p>For detailed instructions on setting up OIDC authentication:</p><h2>üîê Remote Authentication (Trusted Header SSO, Forward Auth)</h2><p>If you run BookLore behind a reverse proxy with remote authentication (middleware), you can enable automatic login by setting  to .</p><p>This allows you to use your existing authentication system (e.g., OAuth, SAML) to log in to BookLore.</p><p>The following remote auth environment variables can be configured:</p><table><thead><tr></tr></thead><tbody><tr><td>Enable remote authentication</td></tr><tr><td>REMOTE_AUTH_CREATE_NEW_USERS</td><td>Auto-create users from remote auth</td></tr><tr><td>HTTP header containing user's name</td></tr><tr><td>HTTP header containing username</td></tr><tr><td>HTTP header containing user's email</td></tr><tr><td>REMOTE_AUTH_HEADER_GROUPS</td><td>HTTP header containing user's groups</td></tr><tr><td>Group name that grants admin privileges</td></tr></tbody></table><h2>üë®‚Äçüíª Contributors &amp; Developers</h2><p>Thanks to all the amazing people who contribute to Booklore.</p><p>If you find BookLore helpful, consider ‚≠ê starring the repo!</p><p>Or support the project via <a href=\"https://venmo.com/AdityaChandel\">Venmo</a>:</p>","contentLength":5156,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GyulyVGC/sniffnet","url":"https://github.com/GyulyVGC/sniffnet","date":1751165055,"author":"","guid":174823,"unread":true,"content":"<p>Comfortably monitor your Internet traffic üïµÔ∏è‚Äç‚ôÇÔ∏è</p><h2><em>Support Sniffnet's development</em> üíñ</h2><p><i>Sniffnet is completely free, open-source software which needs lots of effort and time to develop and maintain.</i></p><p><i>A special mention goes to these awesome organizations and folks who are sponsoring Sniffnet:</i></p><ul><li>üíª choose a  of your PC to inspect</li><li>üè∑Ô∏è select a set of  to apply to the observed traffic</li><li>üìñ view overall  about your Internet traffic</li><li>üìà view  about traffic intensity</li><li>üìå keep an eye on your network even when the application is </li><li>üìÅ  and  comprehensive capture reports as </li><li>üîé identify <strong>6000+ upper layer services</strong>, protocols, trojans, and worms</li><li>üåê find out  and  of the hosts you are exchanging traffic with</li><li>üè† identify connections in your </li><li>üåç discover the  of remote hosts</li><li>‚≠ê save your  network hosts</li><li>üïµÔ∏è‚Äç‚ôÇÔ∏è search and  each of your network connections in real time</li><li>üîâ set custom  to inform you when defined network events occur</li><li>üé® choose the  that fits you the most, including custom themes support</li></ul><p>Do you want to ?  Check out the <a href=\"https://github.com/GyulyVGC/sniffnet/wiki\"></a>, a comprehensive manual to help you thoroughly master the application from a basic setup to the most advanced functionalities.  The Wiki includes step-by-step guides, tips, examples of usage, and answers to frequent questions.</p><ul><li>The graphical user interface has been realized with <a href=\"https://github.com/iced-rs/iced\">iced</a>, a cross-platform GUI library for Rust focused on simplicity and type-safety</li><li>IP geolocation and ASN data are provided by <a href=\"https://www.maxmind.com\">MaxMind</a></li><li>Last but not least, thanks to <a href=\"https://github.com/GyulyVGC/sniffnet/stargazers\">every single stargazer</a>: all forms of support made it possible to keep improving Sniffnet!</li></ul><p>Wait... there's more!Sniffnet is rapidly evolving, and new features are added on a regular basis.<a href=\"https://sniffnet.net/news\"></a> and Sniffnet socials to never miss an update.</p>","contentLength":1720,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"coleam00/ottomator-agents","url":"https://github.com/coleam00/ottomator-agents","date":1751078139,"author":"","guid":174095,"unread":true,"content":"<p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p><p>The <a href=\"https://studio.ottomator.ai\">Live Agent Studio</a> is a community-driven platform developed by <a href=\"https://ottomator.ai\">oTTomator</a> for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.</p><p>The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that you‚Äôll want to use the agents just for the sake of what they can do for you!</p><p>This platform is still in beta ‚Äì expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medin‚Äôs YouTube channel!</p><p>This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!</p><p>Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!</p><p>As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, it‚Äôll be featured through agents on the platform. It‚Äôs a tall order, but we have big plans for the oTTomator community, and we‚Äôre confident we can grow to accomplish this!</p><h3>I want to build an agent to showcase in the Live Agent Studio! How do I do that?</h3><p>Head on over here to learn how to build an agent for the platform:</p><h3>How many tokens does it cost to use an agent?</h3><p>Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.</p><h3>Where can I go to talk about all these agents and get help implementing them myself?</h3><p>Head on over to our Think Tank community and feel free to make a post!</p><p>¬© 2024 Live Agent Studio. All rights reserved. Created by oTTomator</p>","contentLength":2461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rxi/microui","url":"https://github.com/rxi/microui","date":1751078139,"author":"","guid":174096,"unread":true,"content":"<p>A tiny immediate-mode UI library</p><p>A , portable, immediate-mode UI library written in ANSI C</p><ul><li>Tiny: around  of ANSI C</li><li>Works within a fixed-sized memory region: no additional memory is allocated</li><li>Built-in controls: window, scrollable panel, button, slider, textbox, label, checkbox, wordwrapped text</li><li>Works with any rendering system that can draw rectangles and text</li><li>Designed to allow the user to easily add custom controls</li></ul><pre><code>if (mu_begin_window(ctx, \"My Window\", mu_rect(10, 10, 140, 86))) {\n  mu_layout_row(ctx, 2, (int[]) { 60, -1 }, 0);\n\n  mu_label(ctx, \"First:\");\n  if (mu_button(ctx, \"Button1\")) {\n    printf(\"Button1 pressed\\n\");\n  }\n\n  mu_label(ctx, \"Second:\");\n  if (mu_button(ctx, \"Button2\")) {\n    mu_open_popup(ctx, \"My Popup\");\n  }\n\n  if (mu_begin_popup(ctx, \"My Popup\")) {\n    mu_label(ctx, \"Hello world!\");\n    mu_end_popup(ctx);\n  }\n\n  mu_end_window(ctx);\n}\n</code></pre><p>The library expects the user to provide input and handle the resultant drawing commands, it does not do any drawing itself.</p><p>The library is designed to be lightweight, providing a foundation to which you can easily add custom controls and UI elements; pull requests adding additional features will likely not be merged. Bug reports are welcome.</p><p>This library is free software; you can redistribute it and/or modify it under the terms of the MIT license. See <a href=\"https://raw.githubusercontent.com/rxi/microui/master/LICENSE\">LICENSE</a> for details.</p>","contentLength":1333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"automatisch/automatisch","url":"https://github.com/automatisch/automatisch","date":1751078139,"author":"","guid":174097,"unread":true,"content":"<p>The open source Zapier alternative. Build workflow automation without spending time and money.</p><p>üßê Automatisch is a business automation tool that lets you connect different services like Twitter, Slack, and more to automate your business processes.</p><p>üí∏ Automating your workflows doesn't have to be a difficult or expensive process. You also don't need any programming knowledge to use Automatisch.</p><p>There are other existing solutions in the market, like Zapier and Integromat, so you might be wondering why you should use Automatisch.</p><p>‚úÖ One of the main benefits of using Automatisch is that it allows you to store your data on your own servers, which is essential for businesses that handle sensitive user information and cannot risk sharing it with external cloud services. This is especially relevant for industries such as healthcare and finance, as well as for European companies that must adhere to the General Data Protection Regulation (GDPR).</p><p>ü§ì Your contributions are vital to the development of Automatisch. As an open-source software, anyone can have an impact on how it is being developed.</p><p>üíô No vendor lock-in. If you ever decide that Automatisch is no longer helpful for your business, you can switch to any other provider, which will be easier than switching from the one cloud provider to another since you have all data and flexibility.</p><pre><code># Clone the repository\ngit clone https://github.com/automatisch/automatisch.git\n\n# Go to the repository folder\ncd automatisch\n\n# Start\ndocker compose up\n</code></pre><p>You can use  email address and  password to login to Automatisch. Please do not forget to change your email and password from the settings page.</p><p>For other installation types, you can check the <a href=\"https://automatisch.io/docs/guide/installation\">installation</a> guide.</p><p>If you have any questions or problems, please visit our GitHub issues page, and we'll try to help you as soon as possible.</p><p>Automatisch Community Edition (Automatisch CE) is an open-source software with the <a href=\"https://raw.githubusercontent.com/automatisch/automatisch/main/LICENSE.agpl\">AGPL-3.0 license</a>.</p><p>Automatisch Enterprise Edition (Automatisch EE) is a commercial offering with the <a href=\"https://raw.githubusercontent.com/automatisch/automatisch/main/LICENSE.enterprise\">Enterprise license</a>.</p><p>The Automatisch repository contains both AGPL-licensed and Enterprise-licensed files. We maintain a single repository to make development easier.</p><p>See the <a href=\"https://raw.githubusercontent.com/automatisch/automatisch/main/LICENSE\">LICENSE</a> file for more information.</p>","contentLength":2228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"cline/cline","url":"https://github.com/cline/cline","date":1751078139,"author":"","guid":174098,"unread":true,"content":"<p>Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.</p><p>Meet Cline (pronounced /kla…™n/, like \"Klein\"), an AI assistant that can use your  ad ditor.</p><p>Thanks to&nbsp;<a href=\"https://www.anthropic.com/claude/sonnet\">Claude 3.7 Sonnet's agentic coding capabilities</a>,&nbsp;Cline can handle complex software development tasks step-by-step. With tools that let him create &amp; edit files, explore large projects, use the browser, and execute terminal commands (after you grant permission), he can assist you in ways that go beyond code completion or tech support. Cline can even use the Model Context Protocol (MCP) to create new tools and extend his own capabilities. While autonomous AI scripts traditionally run in sandboxed environments, this extension provides a human-in-the-loop GUI to approve every file change and terminal command, providing a safe and accessible way to explore the potential of agentic AI.</p><ol><li>Enter your task and add images to convert mockups into functional apps or fix bugs with screenshots.</li><li>Cline starts by analyzing your file structure &amp; source code ASTs, running regex searches, and reading relevant files to get up to speed in existing projects. By carefully managing what information is added to context, Cline can provide valuable assistance even for large, complex projects without overwhelming the context window.</li><li>Once Cline has the information he needs, he can: \n  <ul><li>Create and edit files + monitor linter/compiler errors along the way, letting him proactively fix issues like missing imports and syntax errors on his own.</li><li>Execute commands directly in your terminal and monitor their output as he works, letting him e.g., react to dev server issues after editing a file.</li><li>For web development tasks, Cline can launch the site in a headless browser, click, type, scroll, and capture screenshots + console logs, allowing him to fix runtime errors and visual bugs.</li></ul></li><li>When a task is completed, Cline will present the result to you with a terminal command like&nbsp;<code>open -a \"Google Chrome\" index.html</code>, which you run with a click of a button.</li></ol><blockquote><p>[!TIP] Use the&nbsp;&nbsp;shortcut to open the command palette and type&nbsp;\"Cline: Open In New Tab\"&nbsp;to open the extension as a tab in your editor. This lets you use Cline side-by-side with your file explorer, and see how he changes your workspace more clearly.</p></blockquote><img align=\"right\" width=\"340\" src=\"https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4\"><p>Cline supports API providers like OpenRouter, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure, GCP Vertex, and Cerebras. You can also configure any OpenAI compatible API, or use a local model through LM Studio/Ollama. If you're using OpenRouter, the extension fetches their latest model list, allowing you to use the newest models as soon as they're available.</p><p>The extension also keeps track of total tokens and API usage cost for the entire task loop and individual requests, keeping you informed of spend every step of the way.</p><img align=\"left\" width=\"370\" src=\"https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76\"><p>Thanks to the new <a href=\"https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api\">shell integration updates in VSCode v1.93</a>, Cline can execute commands directly in your terminal and receive the output. This allows him to perform a wide range of tasks, from installing packages and running build scripts to deploying applications, managing databases, and executing tests, all while adapting to your dev environment &amp; toolchain to get the job done right.</p><p>For long running processes like dev servers, use the \"Proceed While Running\" button to let Cline continue in the task while the command runs in the background. As Cline works he‚Äôll be notified of any new terminal output along the way, letting him react to issues that may come up, such as compile-time errors when editing files.</p><img align=\"right\" width=\"400\" src=\"https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588\"><p>Cline can create and edit files directly in your editor, presenting you a diff view of the changes. You can edit or revert Cline's changes directly in the diff view editor, or provide feedback in chat until you're satisfied with the result. Cline also monitors linter/compiler errors (missing imports, syntax errors, etc.) so he can fix issues that come up along the way on his own.</p><p>All changes made by Cline are recorded in your file's Timeline, providing an easy way to track and revert modifications if needed.</p><img align=\"left\" width=\"370\" src=\"https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5\"><p>With Claude 3.5 Sonnet's new <a href=\"https://www.anthropic.com/news/3-5-models-and-computer-use\">Computer Use</a> capability, Cline can launch a browser, click elements, type text, and scroll, capturing screenshots and console logs at each step. This allows for interactive debugging, end-to-end testing, and even general web use! This gives him autonomy to fixing visual bugs and runtime issues without you needing to handhold and copy-pasting error logs yourself.</p><p>Try asking Cline to \"test the app\", and watch as he runs a command like , launches your locally running dev server in a browser, and performs a series of tests to confirm that everything works. <a href=\"https://x.com/sdrzn/status/1850880547825823989\">See a demo here.</a></p><img align=\"right\" width=\"350\" src=\"https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd\"><p>Thanks to the <a href=\"https://github.com/modelcontextprotocol\">Model Context Protocol</a>, Cline can extend his capabilities through custom tools. While you can use <a href=\"https://github.com/modelcontextprotocol/servers\">community-made servers</a>, Cline can instead create and install tools tailored to your specific workflow. Just ask Cline to \"add a tool\" and he will handle everything, from creating a new MCP server to installing it into the extension. These custom tools then become part of Cline's toolkit, ready to use in future tasks.</p><ul><li>\"add a tool that fetches Jira tickets\": Retrieve ticket ACs and put Cline to work</li><li>\"add a tool that manages AWS EC2s\": Check server metrics and scale instances up or down</li><li>\"add a tool that pulls the latest PagerDuty incidents\": Fetch details and ask Cline to fix bugs</li></ul><img align=\"left\" width=\"360\" src=\"https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970\"><p>&nbsp;Paste in a URL for the extension to fetch and convert to markdown, useful when you want to give Cline the latest docs</p><p>&nbsp;Add workspace errors and warnings ('Problems' panel) for Cline to fix</p><p>&nbsp;Adds a file's contents so you don't have to waste API requests approving read file (+ type to search files)</p><p>&nbsp;Adds folder's files all at once to speed up your workflow even more</p><img align=\"right\" width=\"350\" src=\"https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb\"><h3>Checkpoints: Compare and Restore</h3><p>As Cline works through a task, the extension takes a snapshot of your workspace at each step. You can use the 'Compare' button to see a diff between the snapshot and your current workspace, and the 'Restore' button to roll back to that point.</p><p>For example, when working with a local web server, you can use 'Restore Workspace Only' to quickly test different versions of your app, then use 'Restore Task and Workspace' when you find the version you want to continue building from. This lets you safely explore different approaches without losing progress.</p><p>To contribute to the project, start with our <a href=\"https://raw.githubusercontent.com/cline/cline/main/CONTRIBUTING.md\">Contributing Guide</a> to learn the basics. You can also join our <a href=\"https://discord.gg/cline\">Discord</a> to chat with other contributors in the  channel. If you're looking for full-time work, check out our open positions on our <a href=\"https://cline.bot/join-us\">careers page</a>!</p>","contentLength":6612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"black-forest-labs/flux","url":"https://github.com/black-forest-labs/flux","date":1751078139,"author":"","guid":174099,"unread":true,"content":"<p>Official inference repo for FLUX.1 models</p><p>Documentation for our API can be found here: <a href=\"https://docs.bfl.ai/\">docs.bfl.ai</a>.</p><p>This repo contains minimal inference code to run image generation &amp; editing with our Flux open-weight models.</p><pre><code>cd $HOME &amp;&amp; git clone https://github.com/black-forest-labs/flux\ncd $HOME/flux\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -e \".[all]\"\n</code></pre><h3>Local installation with TensorRT support</h3><p>If you would like to install the repository with <a href=\"https://github.com/NVIDIA/TensorRT\">TensorRT</a> support, you currently need to install a PyTorch image from NVIDIA instead. First install <a href=\"https://github.com/NVIDIA/enroot\">enroot</a>, next follow the steps below:</p><pre><code>cd $HOME &amp;&amp; git clone https://github.com/black-forest-labs/flux\nenroot import 'docker://$oauthtoken@nvcr.io#nvidia/pytorch:25.01-py3'\nenroot create -n pti2501 nvidia+pytorch+25.01-py3.sqsh\nenroot start --rw -m ${PWD}/flux:/workspace/flux -r pti2501\ncd flux\npip install -e \".[tensorrt]\" --extra-index-url https://pypi.nvidia.com\n</code></pre><p>We are offering an extensive suite of open-weight models. For more information about the individual models, please refer to the link under .</p><p>The weights of the autoencoder are also released under <a href=\"https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md\">apache-2.0</a> and can be found in the HuggingFace repos above.</p><p>Our API offers access to all models including our Pro tier non-open weight models. Check out our API documentation <a href=\"https://docs.bfl.ai/\">docs.bfl.ai</a> to learn more.</p><h2>Licensing models for commercial use</h2><p>As the fee is based on a monthly usage, we provide code to automatically track your usage via the BFL API. To enable usage tracking please select  in the cli or click the corresponding checkmark in our provided demos.</p><h3>Example: Using FLUX.1 Kontext with usage tracking</h3><p>We provide a reference implementation for running FLUX.1 with usage tracking enabled for commercial licensing. This can be customized as needed as long as the usage reporting is accurate.</p><p>For the reporting logic to work you will need to set your API key as an environment variable before running:</p><pre><code>export BFL_API_KEY=\"your_api_key_here\"\n</code></pre><p>You can call  like this with tracking activated:</p><pre><code>python -m flux kontext --track_usage --loop\n</code></pre><pre><code>python -m flux kontext --track_usage --prompt \"replace the logo with the text 'Black Forest Labs'\"\n</code></pre><p>The above reporting logic works similarly for FLUX.1 [dev] and FLUX.1 Tools [dev].</p><p><strong>Note that this is only required when using one or more of our open weights models commercially. More information on the commercial licensing can be found at the <a href=\"https://help.bfl.ai/collections/6939000511-licensing\">BFL Helpdesk</a>.</strong></p><p>If you find the provided code or models useful for your research, consider citing them as:</p><pre><code>@misc{labs2025flux1kontextflowmatching,\n      title={FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space},\n      author={Black Forest Labs and Stephen Batifol and Andreas Blattmann and Frederic Boesel and Saksham Consul and Cyril Diagne and Tim Dockhorn and Jack English and Zion English and Patrick Esser and Sumith Kulal and Kyle Lacey and Yam Levi and Cheng Li and Dominik Lorenz and Jonas M√ºller and Dustin Podell and Robin Rombach and Harry Saini and Axel Sauer and Luke Smith},\n      year={2025},\n      eprint={2506.15742},\n      archivePrefix={arXiv},\n      primaryClass={cs.GR},\n      url={https://arxiv.org/abs/2506.15742},\n}\n\n@misc{flux2024,\n    author={Black Forest Labs},\n    title={FLUX},\n    year={2024},\n    howpublished={\\url{https://github.com/black-forest-labs/flux}},\n}\n</code></pre>","contentLength":3307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"gensyn-ai/rl-swarm","url":"https://github.com/gensyn-ai/rl-swarm","date":1751078139,"author":"","guid":174100,"unread":true,"content":"<p>A fully open source framework for creating RL training swarms over the internet.</p><p>RL Swarm is a peer-to-peer system for reinforcement learning. It allows you to train models collaboratively with others in the swarm, leveraging their collective intelligence. It is open source and permissionless, meaning you can run it on a consumer laptop at home or on a powerful GPU in the cloud. You can also connect your model to the Gensyn Testnet to receive an on-chain identity that tracks your progress over time.</p><p>Currently, we are running the <a href=\"https://github.com/open-thought/reasoning-gym/tree/main\">reasoning-gym</a> swarm on the Testnet. This swarm is designed to train models to solve a diverse set of reasoning tasks using the reasoning-gym dataset. The current list of default models includes:</p><ul><li>Gensyn/Qwen2.5-0.5B-Instruct</li><li>dnotitia/Smoothie-Qwen3-1.7B</li><li>Gensyn/Qwen2.5-1.5B-Instruct</li></ul><p>This iteration of rl-swarm is powered by the <a href=\"https://github.com/gensyn-ai/genrl-swarm\">GenRL-Swarm</a> library. It is a fully composable framework for decentralized reinforcement learning which enables users to create and customize their own swarms for reinforcement learning with multi-agent multi-stage environments.</p><p>Your hardware requirements will vary depending on a number of factors including model size and the accelerator platform you use. Users running large NVIDIA GPU will be assigned a model from the large model pool, while users running less powerful hardware will be assigned a model from the small model pool. This design decision is intended to allow users to advance at a similar rate regardless of the hardware they use, maximizing their utility to the swarm.</p><ul><li>arm64 or x86 CPU with minimum 32gb ram (note that if you run other applications during training it might crash training).</li></ul><ul><li>CUDA devices (officially supported): \n  <ul></ul></li></ul><p>With either configuration, you will need Python &gt;=3.10 (for Mac, you will likely need to upgrade).</p><h2>‚ö†Ô∏è Please read before continuing ‚ö†Ô∏è</h2><p>This software is  and provided as-is for users who are interested in using (or helping to develop) an early version of the Gensyn Protocol for training models.</p><p>If you encounter issues, please first check <a href=\"https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#troubleshooting\">Troubleshooting</a>. If you cannot find a solution there, please check if there is an open (or closed) <a href=\"https://raw.githubusercontent.com/gensyn-ai/issues\">Issue</a>. If there is no relevant issue, please file one and include 1) all relevant <a href=\"https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#troubleshooting\">logs</a>, 2) information about your device (e.g. which GPU, if relevant), and 3) your operating system information.</p><p>The easiest way to run RL Swarm is using Docker. This ensures a consistent setup across all operating systems with minimal dependencies.</p><pre><code>git clone https://github.com/gensyn-ai/rl-swarm\n</code></pre><p>Make sure you have Docker installed and the Docker daemon is running on your machine. To do that, follow <a href=\"https://docs.docker.com/get-started/get-docker/\">these instructions</a> according to your OS. Ensure you allot sufficient memory to the Docker containers. For example if using Docker Desktop, this can be done by going to Docker Desktop Settings &gt; Resources &gt; Advanced &gt; Memory Limit, and increasing it to the maximum possible value.</p><p>Run the following commands from the root of the repository.</p><p>If you‚Äôre using a Mac or if your machine has CPU-only support:</p><pre><code>docker-compose run --rm --build -Pit swarm-cpu\n</code></pre><p>If you're using a machine with an officially supported GPU:</p><pre><code>docker-compose run --rm --build -Pit swarm-gpu\n</code></pre><p>If  does not work when running the above commands, please try  (no hyphen) instead. I.e. <code> docker compose run --rm --build -Pit swarm-gpu</code>. This issue sometimes occurs on users running Ubuntu.</p><h3>Experimental (advanced) mode</h3><pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n./run_rl_swarm.sh\n</code></pre><ol><li>Login with your preferred method.</li></ol><p>If you would like to upload your model to Hugging Face, enter your Hugging Face access token when prompted. You can generate one from your Hugging Face account, under <a href=\"https://huggingface.co/docs/hub/en/security-tokens\">Access Tokens</a>.</p><h3>Initial peering and training</h3><p>From this stage onward your device will begin training. You should see your peer register and vote on-chain <a href=\"https://gensyn-testnet.explorer.alchemy.com/address/0xFaD7C5e93f28257429569B854151A1B8DCD404c2?tab=logs\">here</a>.</p><p>You can also track your training progress in real time:</p><p>On-chain identity is managed via an Alchemy modal sign-in screen. You need to supply an email address or login via a supported method (e.g. Google). This creates an EOA public/private key (which are stored by Alchemy). You will also receive local session keys in the . Note that these aren't your EOA public/private keys.</p><p>During the initial set-up process, you will also create a  file which maintains the identity of your peer. This is then registered on chain using the EOA wallet hosted in Alchemy, triggered using your local api keys. This links the  to the  (and corresponding EOA in Alchemy).</p><p><strong>If you want to link multiple nodes to a single EOA</strong>, simply sign up each node using the same email address. You will get a new peer ID for each node, however they will all be linked to the same EOA that your email is linked to.</p><p>: if you are using a fork of this repo, or a service organised by someone else (e.g. a 'one click deployment' provider) the identity management flow below is not guaranteed.</p><p>In the following two scenarios, everything will work (i.e. you will have an on-chain identity linked with your RL Swarm peer training):</p><ul><li>The very first time you run the node from scratch with a new email address. The smart account will be created fresh and linked with the swarm.pem that is also fresh.</li><li>If you run it again with a  AND login the original  used with that . Note: this will throw an error into the log on registration but will still be able to sign transactions.</li></ul><p>In the following two scenarios, it will not work (i.e. you won't have an on-chain identity linked with your RL Swarm peer training):</p><ul><li>If you keep your  and try to link it to an  distinct from the one with which it was first registered.</li></ul><p>Therefore, you should do these actions in the following scenarios</p><ul><li><strong>Signed up with , generated , BUT lost </strong> OR <strong>You want to run multiple nodes at once</strong>: run from scratch with the same email address and generate a new .</li><li><strong>Signed up with , generated , kept </strong> -&gt; you can re-run a single node using this pair if you've still got them both.</li></ul><ul><li><p> You can find them inside the  directory:</p><ul><li>: This file contains logs for the modal login server.</li><li>: This is the main log file for the RL Swarm application.</li><li>: This directory contains various logs related to your training runs, including a  file. These can be updated to Weights &amp; Biases (only available if you log_with wandb).</li></ul></li><li><p><strong>My peer 'skipped a round'</strong>: this occurs when your device isn't fast enough to keep up with the pace of the swarm. For example, if you start training at round 100 and by the time you finish training the rest of the swarm reaches round 102, you will skip round 101 and go straight to 102. This is because your peer is more valuable if it is participating in the active round.</p></li><li><p><strong>My model doesn't seem to be training?</strong></p><ul><li>If you're using a consumer device (e.g. a MacBook), it is likely just running slowly - check back in 20 minutes.</li></ul></li><li><p><strong>Logging in with a new account after previous login?</strong></p><ul><li>Make sure you click 'Logout' on the login screen before you leave your previous session</li><li>Make sure you delete  from the root directory (try ). If you don't do this, and you previously registered with the peer-id stored in this file, it will disrupt the training process.</li></ul></li><li><p><strong>Issues with the Login screen</strong></p><ul><li>: some users report issues with the  package. There are two fixes: \n    <ul><li>in the  update: </li><li>in the terminal <code>cd /root/rl-swarm/modal-login/ &amp;&amp; yarn upgrade &amp;&amp; yarn add next@latest &amp;&amp; yarn add viem@latest</code></li></ul></li></ul></li><li><p><strong>I'm getting lots of warnings</strong></p><ul><li>This is expected behaviour and usually the output of the package managers or other dependencies. The most common is the below Protobuf warning - which can be ignored <pre><code>WARNING: The candidate selected for download or install is a yanked version: 'protobuf' candidate...\n</code></pre></li></ul></li><li><ul><li><p><strong>How do I access the login screen if I'm running in a VM?</strong>: port forwarding. Add this SSH flag:  when connecting to your VM. E.g. <code>gcloud compute ssh --zone \"us-central1-a\" [your-vm] --project [your-project] -- -L 3000:localhost:3000</code>. Note, some VPSs may not work with . Check the Gensyn <a href=\"https://discord.gg/AdnyWNzXh5\">discord</a> for up-to-date information on this.</p></li><li><p><strong>Disconnection/general issues</strong>: If you are tunneling to a VM and suffer a broken pipe, you will likely encounter OOM or unexpected behaviour the first time you relaunch the script. If you  and kill the script it should spin down all background processes. Restart the script and everything should work normally.</p></li></ul></li><li><p><strong>Issues with npm/general installation?</strong></p><ul><li>Try <code>npm install -g node@latest</code></li></ul></li><li><ul><li>Try this (experimental) fix to increase memory: <pre><code>export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n</code></pre></li></ul></li><li><p><strong>I have a Windows machine, can I still train a model on the swarm?</strong>: Yes - but this is not very well tested and may require you to do some debugging to get it set up properly. Install WSL and Linux on your Windows machine using the following instructions: <a href=\"https://learn.microsoft.com/en-us/windows/wsl/install\">https://learn.microsoft.com/en-us/windows/wsl/install</a></p></li><li><p><strong>I want to move my to a different machine and/or restart with a fresh build of the repo, but I want my animal name/peer id to persist.</strong>: To achieve this simply backup the  file on your current machine and then put it in the corresponding location on your new machine/build of the repo.</p></li><li><p><strong>I have multiple GPUs on one machine, can I run multiple peers?</strong>: Yes - but you'll need to manually change things. You'll need to isolate each GPU, install this repo for each GPU, and expose each peer under a different port to pass the modal onboard.</p></li><li><p><strong>My round/stage is behind the smart contract/other peers?</strong>: This is expected behaviour given the different speeds of machines in the network. Once your machine completes it's current round, it will move to the the current round.</p></li><li><p><strong>I want to use a bigger and/or different model in the RL swarm, can I do that?</strong>: Yes - but we only recommend doing so if you are comfortable understanding what size model can reasonably run on your hardware. If you elect to bring a custom model, just paste the repo/model name into the command line when prompted.</p></li><li><p><strong>I am running a model in the swarm on my CPU, have received a python , and my training progress seems to have stopped.</strong>: There are several possible causes for this, but before trying anything please wait long enough to be sure your training actually is frozen and not just slow (e.g., wait longer than a single training iteration has previously taken on your machine). If you're sure training is actually frozen, then some things to try are:</p><ul><li>Set this (experimental) fix: <code>export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 &amp;&amp; ./run_rl_swarm.sh</code></li></ul></li></ul>","contentLength":10287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/generative-ai-for-beginners","url":"https://github.com/microsoft/generative-ai-for-beginners","date":1750991844,"author":"","guid":172652,"unread":true,"content":"<p>21 Lessons, Get Started Building with Generative AI üîó https://microsoft.github.io/generative-ai-for-beginners/</p><h3>21 Lessons teaching everything you need to know to start building Generative AI applications</h3><h4>Supported via GitHub Action (Automated &amp; Always Up-to-Date)</h4><p>Learn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.</p><p>This course has 21 lessons. Each lesson covers its own topic so start wherever you like!</p><p>Lessons are labeled either \"Learn\" lessons explaining a Generative AI concept or \"Build\" lessons that explain a concept and code examples in both  and  when possible.</p><p>Each lesson also includes a \"Keep Learning\" section with additional learning tools.</p><h3>To run the code of this course, you can use either:</h3><p>We have created a  lesson to help you with setting up your development environment.</p><h2>üó£Ô∏è Meet Other Learners, Get Support</h2><ul><li>A short video introduction to the topic</li><li>A written lesson located in the README</li><li>Python and TypeScript code samples supporting Azure OpenAI and OpenAI API</li><li>Links to extra resources to continue your learning</li></ul><p>Special thanks to <a href=\"https://www.linkedin.com/in/john0isaac/\"></a> for creating all of the GitHub Actions and workflows</p><p><a href=\"https://www.linkedin.com/in/bernhard-merkle-738b73/\"></a> for making key contributions to each lesson to improve the learner and code experience.</p><p>Our team produces other courses! Check out:</p>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"punkpeye/awesome-mcp-servers","url":"https://github.com/punkpeye/awesome-mcp-servers","date":1750991844,"author":"","guid":172653,"unread":true,"content":"<p>A collection of MCP servers.</p><p>A curated list of awesome Model Context Protocol (MCP) servers.</p><p><a href=\"https://modelcontextprotocol.io/\">MCP</a> is an open protocol that enables AI models to securely interact with local and remote resources through standardized server implementations. This list focuses on production-ready and experimental MCP servers that extend AI capabilities through file access, database connections, API integrations, and other contextual services.</p><ul><li>üéñÔ∏è ‚Äì official implementation</li><li>programming language \n  <ul><li>üìá ‚Äì TypeScript (or JavaScript) codebase</li></ul></li><li>scope \n  <ul></ul></li><li>operating system \n  <ul></ul></li></ul><blockquote><p>[!NOTE] Confused about Local üè† vs Cloud ‚òÅÔ∏è?</p><ul><li>Use local when MCP server is talking to a locally installed software, e.g. taking control over Chrome browser.</li><li>Use network when MCP server is talking to remote APIs, e.g. weather API.</li></ul></blockquote><p>Servers for accessing many apps and tools through a single MCP server.</p><ul><li><a href=\"https://github.com/julien040/anyquery\">julien040/anyquery</a> üèéÔ∏è üè† ‚òÅÔ∏è - Query more than 40 apps with one binary using SQL. It can also connect to your PostgreSQL, MySQL, or SQLite compatible database. Local-first and private by design.</li><li><a href=\"https://github.com/metatool-ai/metatool-app\">metatool-ai/metatool-app</a> üìá ‚òÅÔ∏è üè† üçé ü™ü üêß - MetaMCP is the one unified middleware MCP server that manages your MCP connections with GUI.</li><li><a href=\"https://github.com/glenngillen/mcpmcp-server\">glenngillen/mcpmcp-server</a> ‚òÅÔ∏è üìá üçé ü™ü üêß - A list of MCP servers so you can ask your client which servers you can use to improve your daily workflow.</li><li><a href=\"https://github.com/PipedreamHQ/pipedream/tree/master/modelcontextprotocol\">PipedreamHQ/pipedream</a> ‚òÅÔ∏è üè† - Connect with 2,500 APIs with 8,000+ prebuilt tools, and manage servers for your users, in your own app.</li><li><a href=\"https://github.com/VeriTeknik/pluggedin-mcp-proxy\">VeriTeknik/pluggedin-mcp-proxy</a> üìá üè† - A comprehensive proxy server that combines multiple MCP servers into a single interface with extensive visibility features. It provides discovery and management of tools, prompts, resources, and templates across servers, plus a playground for debugging when building MCP servers.</li><li><a href=\"https://github.com/tigranbs/mcgravity\">tigranbs/mcgravity</a> üìá üè† - A proxy tool for composing multiple MCP servers into one unified endpoint. Scale your AI tools by load balancing requests across multiple MCP servers, similar to how Nginx works for web servers.</li><li><a href=\"https://github.com/metatool-ai/metatool-app\">MetaMCP</a> üìá ‚òÅÔ∏è üè† üçé ü™ü üêß - MetaMCP is the one unified middleware MCP server that manages your MCP connections with GUI.</li><li><a href=\"https://github.com/waystation-ai/mcp\">WayStation-ai/mcp</a> ‚òÅÔ∏è üçé ü™ü - Seamlessly and securely connect Claude Desktop and other MCP hosts to your favorite apps (Notion, Slack, Monday, Airtable, etc.). Takes less than 90 secs.</li><li><a href=\"https://github.com/hamflx/imagen3-mcp\">hamflx/imagen3-mcp</a> üìá üè† ü™ü üçé üêß - A powerful image generation tool using Google's Imagen 3.0 API through MCP. Generate high-quality images from text prompts with advanced photography, artistic, and photorealistic controls.</li></ul><p>Access and explore art collections, cultural heritage, and museum databases. Enables AI models to search and analyze artistic and cultural content.</p><p>Web content access and automation capabilities. Enables searching, scraping, and processing web content in AI-friendly formats.</p><p>Cloud platform service integration. Enables management and interaction with cloud infrastructure and services.</p><ul><li><a href=\"https://github.com/awslabs/mcp\">awslabs/mcp</a> üéñÔ∏è ‚òÅÔ∏è - AWS MCP servers for seamless integration with AWS services and resources.</li><li><a href=\"https://github.com/qiniu/qiniu-mcp-server\">qiniu/qiniu-mcp-server</a> üêç ‚òÅÔ∏è - A MCP built on Qiniu Cloud products, supporting access to Qiniu Cloud Storage, media processing services, etc.</li><li><a href=\"https://github.com/reza-gholizade/k8s-mcp-server\">reza-gholizade/k8s-mcp-server</a> üèéÔ∏è ‚òÅÔ∏è/üè† - A Kubernetes Model Context Protocol (MCP) server that provides tools for interacting with Kubernetes clusters through a standardized interface, including API resource discovery, resource management, pod logs, metrics, and events.</li><li><a href=\"https://github.com/VmLia/books-mcp-server\">VmLia/books-mcp-server</a> üìá ‚òÅÔ∏è - This is an MCP server used for querying books, and it can be applied in common MCP clients, such as Cherry Studio.</li><li><a href=\"https://github.com/alexei-led/aws-mcp-server\">alexei-led/aws-mcp-server</a> üêç ‚òÅÔ∏è - A lightweight but powerful server that enables AI assistants to execute AWS CLI commands, use Unix pipes, and apply prompt templates for common AWS tasks in a safe Docker environment with multi-architecture support</li><li><a href=\"https://github.com/alexei-led/k8s-mcp-server\">alexei-led/k8s-mcp-server</a> üêç - A lightweight yet robust server that empowers AI assistants to securely execute Kubernetes CLI commands (, , , and ) using Unix pipes in a safe Docker environment with multi-architecture support.</li><li><a href=\"https://github.com/aliyun/alibaba-cloud-ops-mcp-server\">aliyun/alibaba-cloud-ops-mcp-server</a> üéñÔ∏è üêç ‚òÅÔ∏è - A MCP server that enables AI assistants to operation resources on Alibaba Cloud, supporting ECS, Cloud Monitor, OOS and widely used cloud products.</li><li><a href=\"https://github.com/bright8192/esxi-mcp-server\">bright8192/esxi-mcp-server</a> üêç ‚òÅÔ∏è - A VMware ESXi/vCenter management server based on MCP (Model Control Protocol), providing simple REST API interfaces for virtual machine management.</li><li><a href=\"https://github.com/cyclops-ui/mcp-cyclops\">cyclops-ui/mcp-cyclops</a> üéñÔ∏è üèéÔ∏è ‚òÅÔ∏è - An MCP server that allows AI agents to manage Kubernetes resources through Cyclops abstraction</li><li><a href=\"https://github.com/hardik-id/azure-resource-graph-mcp-server\">hardik-id/azure-resource-graph-mcp-server</a> üìá ‚òÅÔ∏è/üè† - A Model Context Protocol server for querying and analyzing Azure resources at scale using Azure Resource Graph, enabling AI assistants to explore and monitor Azure infrastructure.</li><li><a href=\"https://github.com/jdubois/azure-cli-mcp\">jdubois/azure-cli-mcp</a> - A wrapper around the Azure CLI command line that allows you to talk directly to Azure</li><li><a href=\"https://github.com/johnneerdael/netskope-mcp\">johnneerdael/netskope-mcp</a> üîí ‚òÅÔ∏è - An MCP to give access to all Netskope Private Access components within a Netskope Private Access environments including detailed setup information and LLM examples on usage.</li><li><a href=\"https://github.com/manusa/kubernetes-mcp-server\">manusa/Kubernetes MCP Server</a> üèéÔ∏è üè† A - powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for  Kubernetes resource, this server provides specialized tools to interact with your cluster.</li><li><a href=\"https://github.com/nwiizo/tfmcp\">nwiizo/tfmcp</a> ü¶Ä üè† - A Terraform MCP server allowing AI assistants to manage and operate Terraform environments, enabling reading configurations, analyzing plans, applying configurations, and managing Terraform state.</li><li><a href=\"https://github.com/pulumi/mcp-server\">pulumi/mcp-server</a> üéñÔ∏è üìá üè† - MCP server for interacting with Pulumi using the Pulumi Automation API and Pulumi Cloud API. Enables MCP clients to perform Pulumi operations like retrieving package information, previewing changes, deploying updates, and retrieving stack outputs programmatically.</li><li><a href=\"https://github.com/rohitg00/kubectl-mcp-server\">rohitg00/kubectl-mcp-server</a> üêç ‚òÅÔ∏è/üè† - A Model Context Protocol (MCP) server for Kubernetes that enables AI assistants like Claude, Cursor, and others to interact with Kubernetes clusters through natural language.</li><li><a href=\"https://github.com/trilogy-group/aws-pricing-mcp\">trilogy-group/aws-pricing-mcp</a> üèéÔ∏è ‚òÅÔ∏è/üè† - Get up-to-date EC2 pricing information with one call. Fast. Powered by a pre-parsed AWS pricing catalogue.</li><li><a href=\"https://github.com/weibaohui/k8m\">weibaohui/k8m</a> üèéÔ∏è ‚òÅÔ∏è/üè† - Provides MCP multi-cluster Kubernetes management and operations, featuring a management interface, logging, and nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.</li><li><a href=\"https://github.com/weibaohui/kom\">weibaohui/kom</a> üèéÔ∏è ‚òÅÔ∏è/üè† - Provides MCP multi-cluster Kubernetes management and operations. It can be integrated as an SDK into your own project and includes nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.</li><li><a href=\"https://github.com/wenhuwang/mcp-k8s-eye\">wenhuwang/mcp-k8s-eye</a> üèéÔ∏è ‚òÅÔ∏è/üè† - MCP Server for kubernetes management, and analyze your cluster, application health</li><li><a href=\"https://github.com/erikhoward/adls-mcp-server\">erikhoward/adls-mcp-server</a> üêç ‚òÅÔ∏è/üè† - MCP Server for Azure Data Lake Storage. It can perform manage containers, read/write/upload/download operations on container files and manage file metadata.</li><li><a href=\"https://github.com/silenceper/mcp-k8s\">silenceper/mcp-k8s</a> üèéÔ∏è ‚òÅÔ∏è/üè† - MCP-K8S is an AI-driven Kubernetes resource management tool that allows users to operate any resources in Kubernetes clusters through natural language interaction, including native resources (like Deployment, Service) and custom resources (CRD). No need to memorize complex commands - just describe your needs, and AI will accurately execute the corresponding cluster operations, greatly enhancing the usability of Kubernetes.</li><li><a href=\"https://github.com/redis/mcp-redis-cloud\">redis/mcp-redis-cloud</a> üìá ‚òÅÔ∏è - Manage your Redis Cloud resources effortlessly using natural language. Create databases, monitor subscriptions, and configure cloud deployments with simple commands.</li><li><a href=\"https://github.com/portainer/portainer-mcp\">portainer/portainer-mcp</a> üèéÔ∏è ‚òÅÔ∏è/üè† - A powerful MCP server that enables AI assistants to seamlessly interact with Portainer instances, providing natural language access to container management, deployment operations, and infrastructure monitoring capabilities.</li></ul><p>Code execution servers. Allow LLMs to execute code in a secure environment, e.g. for coding agents.</p><ul><li><a href=\"https://github.com/yepcode/mcp-server-js\">yepcode/mcp-server-js</a> üéñÔ∏è üìá ‚òÅÔ∏è - Execute any LLM-generated code in a secure and scalable sandbox environment and create your own MCP tools using JavaScript or Python, with full support for NPM and PyPI packages</li><li><a href=\"https://github.com/ckanthony/openapi-mcp\">ckanthony/openapi-mcp</a> üèéÔ∏è ‚òÅÔ∏è - OpenAPI-MCP: Dockerized MCP Server to allow your AI agent to access any API with existing api docs.</li><li><a href=\"https://github.com/alfonsograziano/node-code-sandbox-mcp\">alfonsograziano/node-code-sandbox-mcp</a> üìá üè† ‚Äì A Node.js MCP server that spins up isolated Docker-based sandboxes for executing JavaScript snippets with on-the-fly npm dependency installation and clean teardown</li><li><a href=\"https://github.com/r33drichards/mcp-js\">r33drichards/mcp-js</a> ü¶Ä üè† üêß üçé - A Javascript code execution sandbox that uses v8 to isolate code to run AI generated javascript locally without fear. Supports heap snapshotting for persistent sessions.</li></ul><p>Full coding agents that enable LLMs to read, edit, and execute code and solve general programming tasks completely autonomously.</p><ul><li><a href=\"https://github.com/oraios/serena\">oraios/serena</a>üêçüè† - A fully-featured coding agent that relies on symbolic code operations by using language servers.</li><li><a href=\"https://github.com/ezyang/codemcp\">ezyang/codemcp</a> üêçüè† - Coding agent with basic read, write and command line tools.</li><li><a href=\"https://github.com/doggybee/mcp-server-leetcode\">doggybee/mcp-server-leetcode</a> üìá ‚òÅÔ∏è - An MCP server that enables AI models to search, retrieve, and solve LeetCode problems. Supports metadata filtering, user profiles, submissions, and contest data access.</li><li><a href=\"https://github.com/jinzcdev/leetcode-mcp-server\">jinzcdev/leetcode-mcp-server</a> üìá ‚òÅÔ∏è - MCP server enabling automated access to 's programming problems, solutions, submissions and public data with optional authentication for user-specific features (e.g., notes), supporting both  (global) and  (China) sites.</li><li><a href=\"https://github.com/juehang/vscode-mcp-server\">juehang/vscode-mcp-server</a> üìá üè† - A MCP Server that allows AI such as Claude to read from the directory structure in a VS Code workspace, see problems picked up by linter(s) and the language server, read code files, and make edits.</li><li><a href=\"https://github.com/micl2e2/code-to-tree\">micl2e2/code-to-tree</a> üåä üè† üìü üêß ü™ü üçé - A single-binary MCP server that converts source code into AST, regardless of language.</li></ul><p>Run commands, capture output and otherwise interact with shells and command line tools.</p><p>Integration with communication platforms for message management and channel operations. Enables AI models to interact with team communication tools.</p><ul><li><a href=\"https://github.com/AbdelStark/nostr-mcp\">AbdelStark/nostr-mcp</a> ‚òÅÔ∏è - A Nostr MCP server that allows to interact with Nostr, enabling posting notes, and more.</li><li><a href=\"https://github.com/agentmail-to/agentmail-toolkit/tree/main/mcp\">agentmail-toolkit/mcp</a> üêç üí¨ - An MCP server to create inboxes on the fly to send, receive, and take actions on email. We aren't AI agents for email, but email for AI Agents.</li><li><a href=\"https://github.com/carterlasalle/mac_messages_mcp\">carterlasalle/mac_messages_mcp</a> üè† üçé üöÄ - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.</li><li><a href=\"https://github.com/chaindead/telegram-mcp\">chaindead/telegram-mcp</a> üèéÔ∏è üè† - Telegram API integration for accessing user data, managing dialogs (chats, channels, groups), retrieving messages, and handling read status</li><li><a href=\"https://github.com/chigwell/telegram-mcp\">chigwell/telegram-mcp</a> üêç üè† - Telegram API integration for accessing user data, managing dialogs (chats, channels, groups), retrieving messages, sending messages and handling read status.</li><li><a href=\"https://github.com/elie222/inbox-zero/tree/main/apps/mcp-server\">elie222/inbox-zero</a> üêç ‚òÅÔ∏è - An MCP server for Inbox Zero. Adds functionality on top of Gmail like finding out which emails you need to reply to or need to follow up on.</li><li><a href=\"https://github.com/gitmotion/ntfy-me-mcp\">gitmotion/ntfy-me-mcp</a> üìá ‚òÅÔ∏è üè† - An ntfy MCP server for sending/fetching ntfy notifications to your self-hosted ntfy server from AI Agents üì§ (supports secure token auth &amp; more - use with npx or docker!)</li><li><a href=\"https://github.com/gotoolkits/mcp-wecombot-server.git\">gotoolkits/wecombot</a> üöÄ ‚òÅÔ∏è - An MCP server application that sends various types of messages to the WeCom group robot.</li><li><a href=\"https://github.com/hannesrudolph/imessage-query-fastmcp-mcp-server\">hannesrudolph/imessage-query-fastmcp-mcp-server</a> üêç üè† üçé - An MCP server that provides safe access to your iMessage database through Model Context Protocol (MCP), enabling LLMs to query and analyze iMessage conversations with proper phone number validation and attachment handling</li><li><a href=\"https://github.com/i-am-bee/acp-mcp\">i-am-bee/acp-mcp</a> üêç üí¨ - An MCP server acting as an adapter into the <a href=\"https://agentcommunicationprotocol.dev\">ACP</a> ecosystem. Seamlessly exposes ACP agents to MCP clients, bridging the communication gap between the two protocols.</li><li><a href=\"https://github.com/jagan-shanmugam/mattermost-mcp-host\">jagan-shanmugam/mattermost-mcp-host</a> üêç üè† - A MCP server along with MCP host that provides access to Mattermost teams, channels and messages. MCP host is integrated as a bot in Mattermost with access to MCP servers that can be configured.</li><li><a href=\"https://github.com/lharries/whatsapp-mcp\">lharries/whatsapp-mcp</a> üêç üèéÔ∏è - An MCP server for searching your personal WhatsApp messages, contacts and sending messages to individuals or groups</li><li><a href=\"https://github.com/sawa-zen/vrchat-mcp\">sawa-zen/vrchat-mcp</a> - üìá üè† This is an MCP server for interacting with the VRChat API. You can retrieve information about friends, worlds, avatars, and more in VRChat.</li><li><a href=\"https://github.com/teddyzxcv/ntfy-mcp\">teddyzxcv/ntfy-mcp</a> - The MCP server that keeps you informed by sending the notification on phone using ntfy</li><li><a href=\"https://github.com/softeria/ms-365-mcp-server\">softeria/ms-365-mcp-server</a> üìá ‚òÅÔ∏è - MCP server that connects to the whole Microsoft 365 suite using Graph API (including mail, files, Excel, calendar)</li></ul><p>Provides access to customer profiles inside of customer data platforms</p><p>Secure database access with schema inspection capabilities. Enables querying and analyzing data with configurable security controls including read-only access.</p><ul><li><a href=\"https://github.com/bytebase/dbhub\">bytebase/dbhub</a> üìá üè† ‚Äì Universal database MCP server supporting mainstream databases.</li><li><a href=\"https://github.com/Canner/wren-engine\">Canner/wren-engine</a> üêç ü¶Ä üè† - The Semantic Engine for Model Context Protocol(MCP) Clients and AI Agents</li><li><a href=\"https://github.com/centralmind/gateway\">centralmind/gateway</a> üèéÔ∏è üè† üçé ü™ü - MCP and MCP SSE Server that automatically generate API based on database schema and data. Supports PostgreSQL, Clickhouse, MySQL, Snowflake, BigQuery, Supabase</li><li><a href=\"https://github.com/ChristianHinge/dicom-mcp\">ChristianHinge/dicom-mcp</a> üêç ‚òÅÔ∏è üè† - DICOM integration to query, read, and move medical images and reports from PACS and other DICOM compliant systems.</li><li><a href=\"https://github.com/chroma-core/chroma-mcp\">chroma-core/chroma-mcp</a> üéñÔ∏è üêç ‚òÅÔ∏è üè† - Chroma MCP server to access local and cloud Chroma instances for retrieval capabilities</li><li><a href=\"https://github.com/Couchbase-Ecosystem/mcp-server-couchbase\">Couchbase-Ecosystem/mcp-server-couchbase</a> üéñÔ∏è üêç ‚òÅÔ∏è üè† - Couchbase MCP server provides unfied access to both Capella cloud and self-managed clusters for document operations, SQL++ queries and natural language data analysis.</li><li><a href=\"https://github.com/crystaldba/postgres-mcp\">crystaldba/postgres-mcp</a> üêç üè† - All-in-one MCP server for Postgres development and operations, with tools for performance analysis, tuning, and health checks</li><li><a href=\"https://github.com/tuannvm/mcp-trino\">tuannvm/mcp-trino</a> üèéÔ∏è ‚òÅÔ∏è - A Go implementation of a Model Context Protocol (MCP) server for Trino</li><li><a href=\"https://github.com/designcomputer/mysql_mcp_server\">designcomputer/mysql_mcp_server</a> üêç üè† - MySQL database integration with configurable access controls, schema inspection, and comprehensive security guidelines</li><li><a href=\"https://github.com/wenb1n-dev/mysql_mcp_server_pro\">wenb1n-dev/mysql_mcp_server_pro</a> üêç üè† - Supports SSE, STDIO; not only limited to MySQL's CRUD functionality; also includes database exception analysis capabilities; controls database permissions based on roles; and makes it easy for developers to extend tools with customization</li><li><a href=\"https://github.com/ergut/mcp-bigquery-server\">ergut/mcp-bigquery-server</a> üìá ‚òÅÔ∏è - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities</li><li><a href=\"https://github.com/f4ww4z/mcp-mysql-server\">f4ww4z/mcp-mysql-server</a> üìá üè† - Node.js-based MySQL database integration that provides secure MySQL database operations</li><li><a href=\"https://github.com/FreePeak/db-mcp-server\">FreePeak/db-mcp-server</a> üèéÔ∏è üè† ‚Äì A high-performance multi-database MCP server built with Golang, supporting MySQL &amp; PostgreSQL (NoSQL coming soon). Includes built-in tools for query execution, transaction management, schema exploration, query building, and performance analysis, with seamless Cursor integration for enhanced database workflows.</li><li><a href=\"https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server\">hannesrudolph/sqlite-explorer-fastmcp-mcp-server</a> üêç üè† - An MCP server that provides safe, read-only access to SQLite databases through Model Context Protocol (MCP). This server is built with the FastMCP framework, which enables LLMs to explore and query SQLite databases with built-in safety features and query validation.</li><li><a href=\"https://github.com/jovezhong/mcp-timeplus\">jovezhong/mcp-timeplus</a> üêç ‚òÅÔ∏è - MCP server for Apache Kafka and Timeplus. Able to list Kafka topics, poll Kafka messages, save Kafka data locally and query streaming data with SQL via Timeplus</li><li><a href=\"https://github.com/jparkerweb/mcp-sqlite\">jparkerweb/mcp-sqlite</a> üìá üè† - Model Context Protocol (MCP) server that provides comprehensive SQLite database interaction capabilities.</li><li><a href=\"https://github.com/memgraph/mcp-memgraph\">memgraph/mcp-memgraph</a> üêç üè† - Memgraph MCP Server - includes a tool to run a query against Memgraph and a schema resource.</li><li><a href=\"https://github.com/neo4j-contrib/mcp-neo4j\">neo4j-contrib/mcp-neo4j</a> üêç üè† - Model Context Protocol with Neo4j (Run queries, Knowledge Graph Memory, Manaage Neo4j Aura Instances)</li><li><a href=\"https://github.com/OpenLinkSoftware/mcp-odbc-server\">openlink/mcp-server-odbc</a> üêç üè† - An MCP server for generic Database Management System (DBMS) Connectivity via the Open Database Connectivity (ODBC) protocol</li><li><a href=\"https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server\">openlink/mcp-server-sqlalchemy</a> üêç üè† - An MCP server for generic Database Management System (DBMS) Connectivity via SQLAlchemy using Python ODBC (pyodbc)</li><li><a href=\"https://github.com/prisma/prisma\">prisma/prisma</a> üêç üè† - Gives LLMs the ability to manage Prisma Postgres databases (e.g. spin up new database instances or run schema migrations).</li><li><a href=\"https://github.com/rashidazarang/airtable-mcp\">rashidazarang/airtable-mcp</a> üêç ‚òÅÔ∏è - Connect AI tools directly to Airtable. Query, create, update, and delete records using natural language. Features include base management, table operations, schema manipulation, record filtering, and data migration through a standardized MCP interface.</li><li><a href=\"https://github.com/redis/mcp-redis\">redis/mcp-redis</a> üêç üè† - The Redis official MCP Server offers an interface to manage and search data in Redis.</li><li><a href=\"https://github.com/runekaagaard/mcp-alchemy\">runekaagaard/mcp-alchemy</a> üêç üè† - Universal SQLAlchemy-based database integration supporting PostgreSQL, MySQL, MariaDB, SQLite, Oracle, MS SQL Server and many more databases. Features schema and relationship inspection, and large dataset analysis capabilities.</li><li><a href=\"https://github.com/skysqlinc/skysql-mcp\">skysqlinc/skysql-mcp</a> üéñÔ∏è ‚òÅÔ∏è - Serverless MariaDB Cloud DB MCP server. Tools to launch, delete, execute SQL and work with DB level AI agents for accurate text-2-sql and conversations.</li><li><a href=\"https://github.com/supabase-community/supabase-mcp\">supabase-community/supabase-mcp</a> üéñÔ∏è üìá ‚òÅÔ∏è - Official Supabase MCP server to connect AI assistants directly with your Supabase project and allows them to perform tasks like managing tables, fetching config, and querying data.</li><li><a href=\"https://github.com/TheRaLabs/legion-mcp\">TheRaLabs/legion-mcp</a> üêç üè† Universal database MCP server supporting multiple database types including PostgreSQL, Redshift, CockroachDB, MySQL, RDS MySQL, Microsoft SQL Server, BigQuery, Oracle DB, and SQLite.</li><li><a href=\"https://github.com/weaviate/mcp-server-weaviate\">weaviate/mcp-server-weaviate</a> üêç üìá ‚òÅÔ∏è - An MCP Server to connect to your Weaviate collections as a knowledge base as well as using Weaviate as a chat memory store.</li><li><a href=\"https://github.com/XGenerationLab/xiyan_mcp_server\">XGenerationLab/xiyan_mcp_server</a> üìá ‚òÅÔ∏è ‚Äî An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.</li><li><a href=\"https://github.com/xing5/mcp-google-sheets\">xing5/mcp-google-sheets</a> üêç ‚òÅÔ∏è - A Model Context Protocol server for interacting with Google Sheets. This server provides tools to create, read, update, and manage spreadsheets through the Google Sheets API.</li><li><a href=\"https://github.com/freema/mcp-gsheets\">freema/mcp-gsheets</a> üìá ‚òÅÔ∏è - MCP server for Google Sheets API integration with comprehensive reading, writing, formatting, and sheet management capabilities.</li><li><a href=\"https://github.com/Zhwt/go-mcp-mysql\">Zhwt/go-mcp-mysql</a> üèéÔ∏è üè† ‚Äì Easy to use, zero dependency MySQL MCP server built with Golang with configurable readonly mode and schema inspection.</li><li><a href=\"https://github.com/OpenLinkSoftware/mcp-jdbc-server\">openlink/mcp-server-jdbc</a> üêç üè† - An MCP server for generic Database Management System (DBMS) Connectivity via the Java Database Connectivity (JDBC) protocol</li><li><a href=\"https://github.com/hydrolix/mcp-hydrolix\">hydrolix/mcp-hydrolix</a> üéñÔ∏è üêç ‚òÅÔ∏è - Hydrolix time-series datalake integration providing schema exploration and query capabilities to LLM-based workflows.</li></ul><p>Data Platforms for data integration, transformation and pipeline orchestration.</p><ul><li><a href=\"https://github.com/flowcore-io/mcp-flowcore-platform\">flowcore/mcp-flowcore-platform</a> üéñÔ∏è üìá ‚òÅÔ∏è üè† - Interact with Flowcore to perform actions, ingest data, and analyse, cross reference and utilise any data in your data cores, or in public data cores; all with human language.</li><li><a href=\"https://github.com/yashshingvi/databricks-genie-MCP\">yashshingvi/databricks-genie-MCP</a> üêç ‚òÅÔ∏è - A server that connects to the Databricks Genie API, allowing LLMs to ask natural language questions, run SQL queries, and interact with Databricks conversational agents.</li><li><a href=\"https://github.com/jwaxman19/qlik-mcp\">jwaxman19/qlik-mcp</a> üìá ‚òÅÔ∏è - MCP Server for Qlik Cloud API that enables querying applications, sheets, and extracting data from visualizations with comprehensive authentication and rate limiting support.</li><li><a href=\"https://github.com/keboola/keboola-mcp-server\">keboola/keboola-mcp-server</a> üêç - interact with Keboola Connection Data Platform. This server provides tools for listing and accessing data from Keboola Storage API.</li><li><a href=\"https://github.com/dbt-labs/dbt-mcp\">dbt-labs/dbt-mcp</a> üéñÔ∏è üêç üè† ‚òÅÔ∏è - Official MCP server for <a href=\"https://www.getdbt.com/product/what-is-dbt\">dbt (data build tool)</a> providing integration with dbt Core/Cloud CLI, project metadata discovery, model information, and semantic layer querying capabilities.</li><li><a href=\"https://github.com/mattijsdp/dbt-docs-mcp\">mattijsdp/dbt-docs-mcp</a> üêç üè† - MCP server for dbt-core (OSS) users as the official dbt MCP only supports dbt Cloud. Supports project metadata, model and column-level lineage and dbt documentation.</li></ul><p>Tools and integrations that enhance the development workflow and environment management.</p><p>Integrations and tools designed to simplify data exploration, analysis and enhance data science workflows.</p><p>Provides access to documentation and shortcuts for working on embedded devices.</p><ul><li><a href=\"https://github.com/horw/esp-mcp\">horw/esp-mcp</a> üìü - Workflow for fixing build issues in ESP32 series chips using ESP-IDF.</li><li><a href=\"https://github.com/kukapay/modbus-mcp\">kukapay/modbus-mcp</a> üêç üìü - An MCP server that standardizes and contextualizes industrial Modbus data.</li><li><a href=\"https://github.com/kukapay/opcua-mcp\">kukapay/opcua-mcp</a> üêç üìü - An MCP server that connects to OPC UA-enabled industrial systems.</li><li><a href=\"https://github.com/yoelbassin/gnuradioMCP\">yoelbassin/gnuradioMCP</a> üêç üìü üè† - An MCP server for GNU Radio that enables LLMs to autonomously create and modify RF  flowcharts.</li></ul><p>Provides direct access to local file systems with configurable permissions. Enables AI models to read, write, and manage files within specified directories.</p><p>Financial data access and analysis tools. Enables AI models to work with market data, trading platforms, and financial information.</p><p>Integration with gaming related data, game engines, and services</p><ul><li><a href=\"https://github.com/Coding-Solo/godot-mcp\">Coding-Solo/godot-mcp</a> üìá üè† - A MCP server for interacting with the Godot game engine, providing tools for editing, running, debugging, and managing scenes in Godot projects.</li><li><a href=\"https://github.com/pab1it0/chess-mcp\">pab1ito/chess-mcp</a> üêç ‚òÅÔ∏è - Access Chess.com player data, game records, and other public information through standardized MCP interfaces, allowing AI assistants to search and analyze chess information.</li><li><a href=\"https://github.com/opgginc/opgg-mcp\">opgginc/opgg-mcp</a> üìá ‚òÅÔ∏è - Access real-time gaming data across popular titles like League of Legends, TFT, and Valorant, offering champion analytics, esports schedules, meta compositions, and character statistics.</li><li><a href=\"https://github.com/stefan-xyz/mcp-server-runescape\">stefan-xyz/mcp-server-runescape</a> üìá - An MCP server with tools for interacting with RuneScape (RS) and Old School RuneScape (OSRS) data, including item prices, player hiscores, and more.</li></ul><p>Persistent memory storage using knowledge graph structures. Enables AI models to maintain and query structured information across sessions.</p><ul><li><a href=\"https://github.com/graphlit/graphlit-mcp-server\">graphlit-mcp-server</a> üìá ‚òÅÔ∏è - Ingest anything from Slack, Discord, websites, Google Drive, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf or Cline.</li><li><a href=\"https://github.com/hannesrudolph/mcp-ragdocs\">hannesrudolph/mcp-ragdocs</a> üêç üè† - An MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context</li><li><a href=\"https://github.com/jinzcdev/markmap-mcp-server\">jinzcdev/markmap-mcp-server</a> üìá üè† - An MCP server built on <a href=\"https://github.com/markmap/markmap\">markmap</a> that converts  to interactive . Supports multi-format exports (PNG/JPG/SVG), live browser preview, one-click Markdown copy, and dynamic visualization features.</li><li><a href=\"https://github.com/kaliaboi/mcp-zotero\">kaliaboi/mcp-zotero</a> üìá ‚òÅÔ∏è - A connector for LLMs to work with collections and sources on your Zotero Cloud</li><li><a href=\"https://github.com/0xshellming/mcp-summarizer\">mcp-summarizer</a> üìï ‚òÅÔ∏è - AI Summarization MCP Server, Support for multiple content types: Plain text, Web pages, PDF documents, EPUB books, HTML content</li><li><a href=\"https://github.com/mem0ai/mem0-mcp\">mem0ai/mem0-mcp</a> üêç üè† - A Model Context Protocol server for Mem0 that helps manage coding preferences and patterns, providing tools for storing, retrieving and semantically handling code implementations, best practices and technical documentation in IDEs like Cursor and Windsurf</li><li><a href=\"https://github.com/pinecone-io/assistant-mcp\">pinecone-io/assistant-mcp</a> üéñÔ∏è ü¶Ä ‚òÅÔ∏è - Connects to your Pinecone Assistant and gives the agent context from its knowledge engine.</li><li><a href=\"https://github.com/ragieai/ragie-mcp-server\">@ragieai/mcp-server</a> üìá ‚òÅÔ∏è - Retrieve context from your <a href=\"https://www.ragie.ai\">Ragie</a> (RAG) knowledge base connected to integrations like Google Drive, Notion, JIRA and more.</li><li><a href=\"https://github.com/TechDocsStudio/biel-mcp\">TechDocsStudio/biel-mcp</a> üìá ‚òÅÔ∏è - Let AI tools like Cursor, VS Code, or Claude Desktop answer questions using your product docs. Biel.ai provides the RAG system and MCP server.</li><li><a href=\"https://github.com/topoteretes/cognee/tree/dev/cognee-mcp\">topoteretes/cognee</a> üìá üè† - Memory manager for AI apps and Agents using various graph and vector stores and allowing ingestion from 30+ data sources</li><li><a href=\"https://github.com/entanglr/zettelkasten-mcp\">entanglr/zettelkasten-mcp</a> üêç üè† - A Model Context Protocol (MCP) server that implements the Zettelkasten knowledge management methodology, allowing you to create, link, and search atomic notes through Claude and other MCP-compatible clients.</li></ul><p>Location-based services and mapping tools. Enables AI models to work with geographic data, weather information, and location-based analytics.</p><p>Tools for creating and editing marketing content, working with web meta data, product positioning, and editing guides.</p><ul><li><a href=\"https://github.com/nictuku/meta-ads-mcp\">nictuku/meta-ads-mcp</a> üêç ‚òÅÔ∏è üè† - Enables AI agents to monitor and optimize Meta ad performance, analyze campaign metrics, adjust audience targeting, manage creative assets, and make data-driven recommendations for ad spend and campaign settings through seamless Graph API integration.</li></ul><p>Access and analyze application monitoring data. Enables AI models to review error reports and performance metrics.</p><ul><li><a href=\"https://github.com/netdata/netdata/raw/master/src/web/mcp/README.md\">netdata/netdata#Netdata</a> üéñÔ∏è üè† ‚òÅÔ∏è üìü üçé ü™ü üêß - Discovery, exploration, reporting and root cause analysis using all observability data, including metrics, logs, systems, containers, processes, and network connections</li><li><a href=\"https://github.com/grafana/mcp-grafana\">grafana/mcp-grafana</a> üéñÔ∏è üêç üè† ‚òÅÔ∏è - Search dashboards, investigate incidents and query datasources in your Grafana instance</li><li><a href=\"https://github.com/hyperb1iss/lucidity-mcp\">hyperb1iss/lucidity-mcp</a> üêç üè† - Enhance AI-generated code quality through intelligent, prompt-based analysis across 10 critical dimensions from complexity to security vulnerabilities</li><li><a href=\"https://github.com/last9/last9-mcp-server\">last9/last9-mcp-server</a> - Seamlessly bring real-time production context‚Äîlogs, metrics, and traces‚Äîinto your local environment to auto-fix code faster</li><li><a href=\"https://github.com/seekrays/mcp-monitor\">seekrays/mcp-monitor</a> üèéÔ∏è üè† - A system monitoring tool that exposes system metrics via the Model Context Protocol (MCP). This tool allows LLMs to retrieve real-time system information through an MCP-compatible interface.Ôºàsupport CPU„ÄÅMemory„ÄÅDisk„ÄÅNetwork„ÄÅHost„ÄÅProcessÔºâ</li></ul><p>Provides the ability to handle multimedia, such as audio and video editing, playback, format conversion, also includes video filters, enhancements, and so on</p><ul><li><a href=\"https://github.com/video-creator/ffmpeg-mcp.git\">video-creator/ffmpeg-mcp</a> üé• üîä - Using ffmpeg command line to achieve an mcp server, can be very convenient, through the dialogue to achieve the local video search, tailoring, stitching, playback and other functions</li><li><a href=\"https://github.com/stass/exif-mcp\">stass/exif-mcp</a> üìá üè† üêß üçé ü™ü - A MCP server that allows one to examine image metadata like EXIF, XMP, JFIF and GPS. This provides foundation for LLM-powered search and analysis of photo librares and image collections.</li></ul><h3>üîé Search &amp; Data Extraction</h3><ul><li><a href=\"https://github.com/LaurieWired/GhidraMCP\">LaurieWired/GhidraMCP</a> ‚òï üè† - A Model Context Protocol server for Ghidra that enables LLMs to autonomously reverse engineer applications. Provides tools for decompiling binaries, renaming methods and data, and listing methods, classes, imports, and exports.</li><li><a href=\"https://github.com/13bm/GhidraMCP\">13bm/GhidraMCP</a> üêç ‚òï üè† - MCP server for integrating Ghidra with AI assistants. This plugin enables binary analysis, providing tools for function inspection, decompilation, memory exploration, and import/export analysis via the Model Context Protocol.</li><li><a href=\"https://github.com/BurtTheCoder/mcp-dnstwist\">BurtTheCoder/mcp-dnstwist</a> üìá ü™ü ‚òÅÔ∏è - MCP server for dnstwist, a powerful DNS fuzzing tool that helps detect typosquatting, phishing, and corporate espionage.</li><li><a href=\"https://github.com/BurtTheCoder/mcp-maigret\">BurtTheCoder/mcp-maigret</a> üìá ü™ü ‚òÅÔ∏è - MCP server for maigret, a powerful OSINT tool that collects user account information from various public sources. This server provides tools for searching usernames across social networks and analyzing URLs.</li><li><a href=\"https://github.com/BurtTheCoder/mcp-shodan\">BurtTheCoder/mcp-shodan</a> üìá ü™ü ‚òÅÔ∏è - MCP server for querying the Shodan API and Shodan CVEDB. This server provides tools for IP lookups, device searches, DNS lookups, vulnerability queries, CPE lookups, and more.</li><li><a href=\"https://github.com/BurtTheCoder/mcp-virustotal\">BurtTheCoder/mcp-virustotal</a> üìá ü™ü ‚òÅÔ∏è - MCP server for querying the VirusTotal API. This server provides tools for scanning URLs, analyzing file hashes, and retrieving IP address reports.</li><li><a href=\"https://github.com/fosdickio/binary_ninja_mcp\">fosdickio/binary_ninja_mcp</a> üêç üè† üçé ü™ü üêß - A Binary Ninja plugin, MCP server, and bridge that seamlessly integrates <a href=\"https://binary.ninja\">Binary Ninja</a> with your favorite MCP client. It enables you to automate the process of performing binary analysis and reverse engineering.</li><li><a href=\"https://github.com/fr0gger/MCP_Security\">fr0gger/MCP_Security</a> üìá ‚òÅÔ∏è - MCP server for querying the ORKL API. This server provides tools for fetching threat reports, analyzing threat actors, and retrieving intelligence sources.</li><li><a href=\"https://github.com/gbrigandi/mcp-server-cortex\">gbrigandi/mcp-server-cortex</a> ü¶Ä üè† üö® üçé ü™ü üêß - A Rust-based MCP server to integrate Cortex, enabling observable analysis and automated security responses through AI.</li><li><a href=\"https://github.com/gbrigandi/mcp-server-thehive\">gbrigandi/mcp-server-thehive</a> ü¶Ä üè† üö® üçé ü™ü üêß - A Rust-based MCP server to integrate TheHive, facilitating collaborative security incident response and case management via AI.</li><li><a href=\"https://github.com/gbrigandi/mcp-server-wazuh\">gbrigandi/mcp-server-wazuh</a> ü¶Ä üè† üö® üçé ü™ü üêß - A Rust-based MCP server bridging Wazuh SIEM with AI assistants, providing real-time security alerts and event data for enhanced contextual understanding.</li><li><a href=\"https://github.com/jyjune/mcp_vms\">jyjune/mcp_vms</a> üêç üè† ü™ü - A Model Context Protocol (MCP) server designed to connect to a CCTV recording program (VMS) to retrieve recorded and live video streams. It also provides tools to control the VMS software, such as showing live or playback dialogs for specific channels at specified times.</li><li><a href=\"https://github.com/qianniuspace/mcp-security-audit\">qianniuspace/mcp-security-audit</a> üìá ‚òÅÔ∏è A powerful MCP (Model Context Protocol) Server that audits npm package dependencies for security vulnerabilities. Built with remote npm registry integration for real-time security checks.</li><li><a href=\"https://github.com/slouchd/cyberchef-api-mcp-server\">slouchd/cyberchef-api-mcp-server</a> üêç ‚òÅÔ∏è - MCP server for interacting with the CyberChef server API which will allow an MCP client to utilise the CyberChef operations.</li><li><a href=\"https://github.com/mrexodia/ida-pro-mcp\">mrexodia/ida-pro-mcp</a> üêç üè† - MCP server for IDA Pro, allowing you to perform binary analysis with AI assistants. This plugin implement decompilation, disassembly and allows you to generate malware analysis reports automatically.</li><li><a href=\"https://github.com/rad-security/mcp-server\">rad-security/mcp-server</a> üìá ‚òÅÔ∏è - MCP server for RAD Security, providing AI-powered security insights for Kubernetes and cloud environments. This server provides tools for querying the Rad Security API and retrieving security findings, reports, runtime data and many more.</li><li><a href=\"https://github.com/securityfortech/secops-mcp\">securityfortech/secops-mcp</a> üêç üè† - All-in-one security testing toolbox that brings together popular open source tools through a single MCP interface. Connected to an AI agent, it enables tasks like pentesting, bug bounty hunting, threat hunting, and more.</li><li><a href=\"https://github.com/roadwy/cve-search_mcp\">roadwy/cve-search_mcp</a> üêç üè† - A Model Context Protocol (MCP) server for querying the CVE-Search API. This server provides comprehensive access to CVE-Search, browse vendor and product„ÄÅget CVE per CVE-ID„ÄÅget the last updated CVEs.</li><li><a href=\"https://github.com/StacklokLabs/osv-mcp\">StacklokLabs/osv-mcp</a> üèéÔ∏è ‚òÅÔ∏è - Access the OSV (Open Source Vulnerabilities) database for vulnerability information. Query vulnerabilities by package version or commit, batch query multiple packages, and get detailed vulnerability information by ID.</li><li><a href=\"https://github.com/nickpending/mcp-recon\">nickpending/mcp-recon</a> üèéÔ∏è üè† - Conversational recon interface and MCP server powered by httpx and asnmap. Supports various reconnaissance levels for domain analysis, security header inspection, certificate analysis, and ASN lookup.</li><li><a href=\"https://github.com/Gaffx/volatility-mcp\">Gaffx/volatility-mcp</a> - MCP server for Volatility 3.x, allowing you to perform memory forensics analysis with AI assistant. Experience memory forensics without barriers as plugins like pslist and netscan become accessible through clean REST APIs and LLMs.</li><li><a href=\"https://github.com/co-browser/attestable-mcp-server\">co-browser/attestable-mcp-server</a> üêç üè† ‚òÅÔ∏è üêß - An MCP server running inside a trusted execution environment (TEE) via Gramine, showcasing remote attestation using <a href=\"https://gramine.readthedocs.io/en/stable/attestation.html\">RA-TLS</a>. This allows an MCP client to verify the server before conencting.</li><li><a href=\"https://github.com/zinja-coder/jadx-ai-mcp\">zinja-coder/jadx-ai-mcp</a> ‚òï üè† - JADX-AI-MCP is a plugin and MCP Server for the JADX decompiler that integrates directly with Model Context Protocol (MCP) to provide live reverse engineering support with LLMs like Claude.</li></ul><p>Integration with social media platforms to allow posting, analytics, and interaction management. Enables AI-driven automation for social presence.</p><ul><li><a href=\"https://github.com/macrocosm-os/macrocosmos-mcp\">macrocosm-os/macrocosmos-mcp</a> - üéñÔ∏è üêç ‚òÅÔ∏è Access real-time X/Reddit/YouTube data directly in your LLM applications with search phrases, users, and date filtering.</li><li><a href=\"https://github.com/LuniaKunal/mcp-twitter\">kunallunia/twitter-mcp</a> üêç üè† - All-in-one Twitter management solution providing timeline access, user tweet retrieval, hashtag monitoring, conversation analysis, direct messaging, sentiment analysis of a post, and complete post lifecycle control - all through a streamlined API.</li><li><a href=\"https://github.com/HagaiHen/facebook-mcp-server\">HagaiHen/facebook-mcp-server</a> üêç ‚òÅÔ∏è - Integrates with Facebook Pages to enable direct management of posts, comments, and engagement metrics through the Graph API for streamlined social media management.</li></ul><p>Tools for accessing sports-related data, results, and statistics.</p><ul><li><a href=\"https://github.com/mikechao/balldontlie-mcp\">mikechao/balldontlie-mcp</a> üìá - MCP server that integrates balldontlie api to provide information about players, teams and games for the NBA, NFL and MLB</li><li><a href=\"https://github.com/r-huijts/firstcycling-mcp\">r-huijts/firstcycling-mcp</a> üìá ‚òÅÔ∏è - Access cycling race data, results, and statistics through natural language. Features include retrieving start lists, race results, and rider information from firstcycling.com.</li><li><a href=\"https://github.com/r-huijts/strava-mcp\">r-huijts/strava-mcp</a> üìá ‚òÅÔ∏è - A Model Context Protocol (MCP) server that connects to Strava API, providing tools to access Strava data through LLMs</li><li><a href=\"https://github.com/willvelida/mcp-afl-server\">willvelida/mcp-afl-server</a> ‚òÅÔ∏è - MCP server that integrates with the Squiggle API to provide information on Australian Football League teams, ladder standings, results, tips, and power rankings.</li><li><a href=\"https://github.com/guillochon/mlb-api-mcp\">guillochon/mlb-api-mcp</a> üêç üè† - MCP server that acts as a proxy to the freely available MLB API, which provides player info, stats, and game information.</li></ul><h3>üéß Support &amp; Service Management</h3><p>Tools for managing customer support, IT service management, and helpdesk operations.</p><ul><li><a href=\"https://github.com/effytech/freshdesk_mcp\">effytech/freshdesk-mcp</a> üêç ‚òÅÔ∏è - MCP server that integrates with Freshdesk, enabling AI models to interact with Freshdesk modules and perform various support operations.</li><li><a href=\"https://github.com/nguyenvanduocit/jira-mcp\">nguyenvanduocit/jira-mcp</a> üèéÔ∏è ‚òÅÔ∏è - A Go-based MCP connector for Jira that enables AI assistants like Claude to interact with Atlassian Jira. This tool provides a seamless interface for AI models to perform common Jira operations including issue management, sprint planning, and workflow transitions.</li><li><a href=\"https://github.com/sooperset/mcp-atlassian\">sooperset/mcp-atlassian</a> üêç ‚òÅÔ∏è - MCP server for Atlassian products (Confluence and Jira). Supports Confluence Cloud, Jira Cloud, and Jira Server/Data Center. Provides comprehensive tools for searching, reading, creating, and managing content across Atlassian workspaces.</li></ul><p>Translation tools and services to enable AI assistants to translate content between different languages.</p><ul><li><a href=\"https://github.com/translated/lara-mcp\">translated/lara-mcp</a> üéñÔ∏è üìá ‚òÅÔ∏è - MCP Server for Lara Translate API, enabling powerful translation capabilities with support for language detection and context-aware translations.</li><li><a href=\"https://github.com/mmntm/weblate-mcp\">mmntm/weblate-mcp</a> üìá ‚òÅÔ∏è - Comprehensive Model Context Protocol server for Weblate translation management, enabling AI assistants to perform translation tasks, project management, and content discovery with smart format transformations.</li></ul><p>Tools for converting text-to-speech and vice-versa</p><ul><li><a href=\"https://github.com/mberg/kokoro-tts-mcp\">mberg/kokoro-tts-mcp</a> üêç üè† - MCP Server that uses the open weight Kokoro TTS models to convert text-to-speech. Can convert text to MP3 on a local driver or auto-upload to an S3 bucket.</li><li><a href=\"https://github.com/mbailey/voice-mcp\">mbailey/voice-mcp</a> üêç üè† - Complete voice interaction server supporting speech-to-text, text-to-speech, and real-time voice conversations through local microphone, OpenAI-compatible APIs, and LiveKit integration</li></ul><p>Access to travel and transportation information. Enables querying schedules, routes, and real-time travel data.</p><ul><li><a href=\"https://github.com/KyrieTangSheng/mcp-server-nationalparks\">KyrieTangSheng/mcp-server-nationalparks</a> üìá ‚òÅÔ∏è - National Park Service API integration providing latest information of park details, alerts, visitor centers, campgrounds, and events for U.S. National Parks</li><li><a href=\"https://github.com/pab1it0/tripadvisor-mcp\">pab1it0/tripadvisor-mcp</a> üìá üêç - A MCP server that enables LLMs to interact with Tripadvisor API, supporting location data, reviews, and photos through standardized MCP interfaces</li><li><a href=\"https://github.com/lucygoodchild/mcp-national-rail\">lucygoodchild/mcp-national-rail</a> üìá ‚òÅÔ∏è - An MCP server for UK National Rail trains service, providing train schedules and live travel information, intergrating the Realtime Trains API</li></ul><p>Interact with Git repositories and version control platforms. Enables repository management, code analysis, pull request handling, issue tracking, and other version control operations through standardized APIs.</p><h3>üõ†Ô∏è Other Tools and Integrations</h3><ul><li><a href=\"https://github.com/jlowin/fastmcp\">FastMCP</a> üêç - A high-level framework for building MCP servers in Python</li><li><a href=\"https://github.com/punkpeye/fastmcp\">FastMCP</a> üìá - A high-level framework for building MCP servers in TypeScript</li></ul><h3>Official prompt to inform LLMs how to use MCP</h3><p>Want to ask Claude about Model Context Protocol?</p><p>Create a Project, then add this file to it:</p><p>Now Claude can answer questions about writing MCP servers and how they work</p><a href=\"https://star-history.com/#punkpeye/awesome-mcp-servers&amp;Date\"></a>","contentLength":37682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Portkey-AI/gateway","url":"https://github.com/Portkey-AI/gateway","date":1750991844,"author":"","guid":172654,"unread":true,"content":"<p>A blazing fast AI Gateway with integrated guardrails. Route to 200+ LLMs, 50+ AI Guardrails with 1 fast &amp; friendly API.</p><p>The <a href=\"https://portkey.wiki/gh-10\"></a> is designed for fast, reliable &amp; secure routing to 1600+ language, vision, audio, and image models. It is a lightweight, open-source, and enterprise-ready solution that allows you to integrate with any language model in under 2 minutes.</p><h4>What can you do with the AI Gateway?</h4><blockquote><p>[!TIP] Starring this repo helps more developers discover the AI Gateway üôèüèª</p></blockquote><pre><code># Run the gateway locally (needs Node.js and npm)\nnpx @portkey-ai/gateway\n</code></pre><blockquote><p>The Gateway is running on </p><p>The Gateway Console is running on <code>http://localhost:8787/public/</code></p></blockquote><h3>2. Make your first request</h3><pre><code># pip install -qU portkey-ai\n\nfrom portkey_ai import Portkey\n\n# OpenAI compatible client\nclient = Portkey(\n    provider=\"openai\", # or 'anthropic', 'bedrock', 'groq', etc\n    Authorization=\"sk-***\" # the provider API key\n)\n\n# Make a request through your AI Gateway\nclient.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather like?\"}],\n    model=\"gpt-4o-mini\"\n)\n</code></pre><p>On the Gateway Console (<code>http://localhost:8787/public/</code>) you can see all of your local logs in one place.</p><img src=\"https://github.com/user-attachments/assets/362bc916-0fc9-43f1-a39e-4bd71aac4a3a\" width=\"400\"><p> in the LLM gateway allow you to create routing rules, add reliability and setup guardrails.</p><pre><code>config = {\n  \"retry\": {\"attempts\": 5},\n\n  \"output_guardrails\": [{\n    \"default.contains\": {\"operator\": \"none\", \"words\": [\"Apple\"]},\n    \"deny\": True\n  }]\n}\n\n# Attach the config to the client\nclient = client.with_options(config=config)\n\nclient.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Reply randomly with Apple or Bat\"}]\n)\n\n# This would always response with \"Bat\" as the guardrail denies all replies containing \"Apple\". The retry config would retry 5 times before giving up.\n</code></pre><h3>Enterprise Version (Private deployments)</h3><p>Join weekly community calls every Friday (8 AM PT) to kickstart your AI Gateway implementation! <a href=\"https://portkey.wiki/gh-35\">Happening every Friday</a></p><p>Insights from analyzing 2 trillion+ tokens, across 90+ regions and 650+ teams in production. What to expect from this report:</p><ul><li>Trends shaping AI adoption and LLM provider growth.</li><li>Benchmarks to optimize speed, cost and reliability.</li><li>Strategies to scale production-grade AI systems.</li></ul><ul><li><a href=\"https://portkey.wiki/gh-37\"></a>: Fallback to another provider or model on failed requests using the LLM gateway. You can specify the errors on which to trigger the fallback. Improves reliability of your application.</li><li><a href=\"https://portkey.wiki/gh-38\"></a>: Automatically retry failed requests up to 5 times. An exponential backoff strategy spaces out retry attempts to prevent network overload.</li><li><a href=\"https://portkey.wiki/gh-39\"></a>: Distribute LLM requests across multiple API keys or AI providers with weights to ensure high availability and optimal performance.</li><li><a href=\"https://portkey.wiki/gh-40\"></a>: Manage unruly LLMs &amp; latencies by setting up granular request timeouts, allowing automatic termination of requests that exceed a specified duration.</li><li><a href=\"https://portkey.wiki/gh-41\"></a>: Call vision, audio (text-to-speech &amp; speech-to-text), and image generation models from multiple providers ‚Äî all using the familiar OpenAI signature</li><li><a href=\"https://portkey.wiki/gh-42\"></a>: Call realtime APIs launched by OpenAI through the integrate websockets server.</li></ul><ul><li><a href=\"https://portkey.wiki/gh-48\"></a>: Cache responses from LLMs to reduce costs and improve latency. Supports simple and semantic* caching.</li><li><a href=\"https://portkey.wiki/gh-49\"></a>: Monitor and analyze your AI and LLM usage, including request volume, latency, costs and error rates.</li><li><a href=\"https://portkey.wiki/gh-89\">*</a>: Automatically switch to the most cost-effective provider based on usage patterns and pricing models.</li></ul><h3>Collaboration &amp; Workflows</h3><sup> *&nbsp;Available in hosted and enterprise versions </sup><h2>Gateway Enterprise Version</h2><p>Make your AI app more  and , while ensuring complete  and .</p><p>‚úÖ&nbsp; Secure Key Management - for role-based access control and tracking  ‚úÖ&nbsp; Simple &amp; Semantic Caching - to serve repeat queries faster &amp; save costs <p> ‚úÖ&nbsp; Access Control &amp; Inbound Rules - to control which IPs and Geos can connect to your deployments </p> ‚úÖ&nbsp; PII Redaction - to automatically remove sensitive data from your requests to prevent indavertent exposure <p> ‚úÖ&nbsp; SOC2, ISO, HIPAA, GDPR Compliances - for best security practices </p> ‚úÖ&nbsp; Professional Support - along with feature prioritization </p><p>The easiest way to contribute is to pick an issue with the  tag üí™. Read the contribution guidelines <a href=\"https://raw.githubusercontent.com/Portkey-AI/gateway/main/.github/CONTRIBUTING.md\">here</a>.</p><h3>Getting Started with the Community</h3><p>Join our weekly AI Engineering Hours every Friday (8 AM PT) to:</p><ul><li>Meet other contributors and community members</li><li>Learn advanced Gateway features and implementation patterns</li><li>Share your experiences and get help</li><li>Stay updated with the latest development priorities</li></ul><p>Join our growing community around the world, for help, ideas, and discussions on AI.</p>","contentLength":4498,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"twentyhq/twenty","url":"https://github.com/twentyhq/twenty","date":1750991844,"author":"","guid":172655,"unread":true,"content":"<p>Building a modern alternative to Salesforce, powered by the community.</p><p>We built Twenty for three reasons:</p><p><strong>CRMs are too expensive, and users are trapped.</strong> Companies use locked-in customer data to hike prices. It shouldn't be that way.</p><p><strong>A fresh start is required to build a better experience.</strong> We can learn from past mistakes and craft a cohesive experience inspired by new UX patterns from tools like Notion, Airtable or Linear.</p><p><strong>We believe in Open-source and community.</strong> Hundreds of developers are already building Twenty together. Once we have plugin capabilities, a whole ecosystem will grow around it.</p><p>Please feel free to flag any specific needs you have by creating an issue.</p><p>Below are a few features we have implemented to date:</p><h2>Personalize layouts with filters, sort, group by, kanban and table views</h2><h2>Customize your objects and fields</h2><h2>Create and manage permissions with custom roles</h2><h2>Automate workflow with triggers and actions</h2><h2>Emails, calendar events, files, and more</h2><p>Thanks to these amazing services that we use and recommend for UI testing (Chromatic), code review (Greptile), catching bugs (Sentry) and translating (Crowdin).</p>","contentLength":1117,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/ML-For-Beginners","url":"https://github.com/microsoft/ML-For-Beginners","date":1750991844,"author":"","guid":172656,"unread":true,"content":"<p>12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all</p><blockquote><p>üåç Travel around the world as we explore Machine Learning by means of world cultures üåç</p></blockquote><p>Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about . In this curriculum, you will learn about what is sometimes called , using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our <a href=\"https://aka.ms/ai4beginners\">AI for Beginners' curriculum</a>. Pair these lessons with our <a href=\"https://aka.ms/ds4beginners\">'Data Science for Beginners' curriculum</a>, as well!</p><p>Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.</p><p><strong>‚úçÔ∏è Hearty thanks to our authors</strong> Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, Ruth Yakubu and Amy Boyd</p><p><strong>üé® Thanks as well to our illustrators</strong> Tomomi Imura, Dasani Madipalli, and Jen Looper</p><p><strong>üôè Special thanks üôè to our Microsoft Student Ambassador authors, reviewers, and content contributors</strong>, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal</p><p><strong>ü§© Extra gratitude to Microsoft Student Ambassadors Eric Wanjau, Jasleen Sondhi, and Vidushi Gupta for our R lessons!</strong></p><ol><li>: Click on the \"Fork\" button at the top-right corner of this page.</li><li>: <code>git clone https://github.com/microsoft/ML-For-Beginners.git</code></li></ol><p>, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:</p><ul><li>Start with a pre-lecture quiz.</li><li>Read the lecture and complete the activities, pausing and reflecting at each knowledge check.</li><li>Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the  folders in each project-oriented lesson.</li><li>Take the post-lecture quiz.</li><li>After completing a lesson group, visit the <a href=\"https://github.com/microsoft/ML-For-Beginners/discussions\">Discussion Board</a> and \"learn out loud\" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.</li></ul><blockquote><p>For further study, we recommend following these <a href=\"https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-77952-leestott\">Microsoft Learn</a> modules and learning paths.</p></blockquote><blockquote><p>üé• Click the image above for a video about the project and the folks who created it!</p></blockquote><p>We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on  and that it includes . In addition, this curriculum has a common  to give it cohesion.</p><p>By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.</p><ul><li>optional supplemental video</li><li>video walkthrough (some lessons only)</li><li>for project-based lessons, step-by-step guides on how to build the project</li></ul><blockquote><p>: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the  folder and look for R lessons. They include an .rmd extension that represents an  file which can be simply defined as an embedding of  (of R or other languages) and a  (that guides how to format outputs such as PDF) in a . As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.</p></blockquote><blockquote><p>: All quizzes are contained in <a href=\"https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/quiz-app/\">Quiz App folder</a>, for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the  folder to locally host or deploy to Azure.</p></blockquote><table><thead><tr></tr></thead><tbody><tr><td align=\"center\">Introduction to machine learning</td><td>Learn the basic concepts behind machine learning</td></tr><tr><td align=\"center\">The History of machine learning</td><td>Learn the history underlying this field</td></tr><tr><td align=\"center\">Fairness and machine learning</td><td>What are the important philosophical issues around fairness that students should consider when building and applying ML models?</td></tr><tr><td align=\"center\">Techniques for machine learning</td><td>What techniques do ML researchers use to build ML models?</td></tr><tr><td align=\"center\">Introduction to regression</td><td>Get started with Python and Scikit-learn for regression models</td></tr><tr><td align=\"center\">North American pumpkin prices üéÉ</td><td>Visualize and clean data in preparation for ML</td></tr><tr><td align=\"center\">North American pumpkin prices üéÉ</td><td>Build linear and polynomial regression models</td><td align=\"center\"><ul></ul></td></tr><tr><td align=\"center\">North American pumpkin prices üéÉ</td><td>Build a logistic regression model</td></tr><tr><td>Build a web app to use your trained model</td></tr><tr><td align=\"center\">Introduction to classification</td><td>Clean, prep, and visualize your data; introduction to classification</td><td align=\"center\"><ul></ul></td></tr><tr><td align=\"center\">Delicious Asian and Indian cuisines üçú</td><td>Introduction to classifiers</td><td align=\"center\"><ul></ul></td></tr><tr><td align=\"center\">Delicious Asian and Indian cuisines üçú</td><td>Build a recommender web app using your model</td></tr><tr><td align=\"center\">Introduction to clustering</td><td>Clean, prep, and visualize your data; Introduction to clustering</td></tr><tr><td align=\"center\">Exploring Nigerian Musical Tastes üéß</td><td>Explore the K-Means clustering method</td></tr><tr><td align=\"center\">Introduction to time series forecasting</td><td>Introduction to time series forecasting</td></tr><tr><td align=\"center\">‚ö°Ô∏è World Power Usage ‚ö°Ô∏è - time series forecasting with ARIMA</td><td>Time series forecasting with ARIMA</td></tr><tr><td align=\"center\">‚ö°Ô∏è World Power Usage ‚ö°Ô∏è - time series forecasting with SVR</td><td>Time series forecasting with Support Vector Regressor</td></tr><tr><td align=\"center\">Real-World ML scenarios and applications</td><td>Interesting and revealing real-world applications of classical ML</td></tr><tr><td align=\"center\">Model Debugging in ML using RAI dashboard</td><td>Model Debugging in Machine Learning using Responsible AI dashboard components</td></tr></tbody></table><p>You can run this documentation offline by using <a href=\"https://docsify.js.org/#/\">Docsify</a>. Fork this repo, <a href=\"https://docsify.js.org/#/quickstart\">install Docsify</a> on your local machine, and then in the root folder of this repo, type . The website will be served on port 3000 on your localhost: .</p><p>Find a pdf of the curriculum with links <a href=\"https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf\">here</a>.</p><p>Would you like to contribute a translation? Please read our <a href=\"https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md\">translation guidelines</a> and add a templated issue to manage the workload <a href=\"https://github.com/microsoft/ML-For-Beginners/issues\">here</a>.</p><p>Our team produces other courses! Check out:</p>","contentLength":6512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"modelcontextprotocol/registry","url":"https://github.com/modelcontextprotocol/registry","date":1750991844,"author":"","guid":172657,"unread":true,"content":"<p>A community driven registry service for Model Context Protocol (MCP) servers.</p><p>A community driven registry service for Model Context Protocol (MCP) servers.</p><p>This project is being built in the open and is currently in the early stages of development. Please see the <a href=\"https://github.com/modelcontextprotocol/registry/discussions/11\">overview discussion</a> for the project scope and goals. If you would like to contribute, please check out the <a href=\"https://raw.githubusercontent.com/modelcontextprotocol/registry/main/CONTRIBUTING.md\">contributing guidelines</a>.</p><p>The MCP Registry service provides a centralized repository for MCP server entries. It allows discovery and management of various MCP implementations with their associated metadata, configurations, and capabilities.</p><ul><li>RESTful API for managing MCP registry entries (list, get, create, update, delete)</li><li>Health check endpoint for service monitoring</li><li>Support for various environment configurations</li><li>Graceful shutdown handling</li><li>MongoDB and in-memory database support</li><li>Comprehensive API documentation</li><li>Pagination support for listing registry entries</li></ul><ul><li>Docker (optional, but recommended for development)</li></ul><p>The easiest way to get the registry running is to use . This will setup the MCP Registry service, import the seed data and run MongoDB in a local Docker environment.</p><pre><code># Build the Docker image\ndocker build -t registry .\n\n# Run the registry and MongoDB with docker compose\ndocker compose up\n</code></pre><p>This will start the MCP Registry service and MongoDB with Docker, exposing it on port 8080.</p><p>If you prefer to run the service locally without Docker, you can build and run it directly using Go.</p><pre><code># Build a registry executable\ngo build ./cmd/registry\n</code></pre><p>This will create the  binary in the current directory. You'll need to have MongoDB running locally or with Docker.</p><p>By default, the service will run on .</p><pre><code>‚îú‚îÄ‚îÄ api/           # OpenApi specification\n‚îú‚îÄ‚îÄ cmd/           # Application entry points\n‚îú‚îÄ‚îÄ config/        # Configuration files\n‚îú‚îÄ‚îÄ internal/      # Private application code\n‚îÇ   ‚îú‚îÄ‚îÄ api/       # HTTP server and request handlers\n‚îÇ   ‚îú‚îÄ‚îÄ config/    # Configuration management\n‚îÇ   ‚îú‚îÄ‚îÄ model/     # Data models\n‚îÇ   ‚îî‚îÄ‚îÄ service/   # Business logic\n‚îú‚îÄ‚îÄ pkg/           # Public libraries\n‚îú‚îÄ‚îÄ scripts/       # Utility scripts\n‚îî‚îÄ‚îÄ tools/         # Command line tools\n    ‚îî‚îÄ‚îÄ publisher/ # Tool to publish MCP servers to the registry\n</code></pre><p>The API is documented using Swagger/OpenAPI. You can access the interactive Swagger UI at:</p><p>This provides a complete reference of all endpoints with request/response schemas and allows you to test the API directly from your browser.</p><p>Returns the health status of the service:</p><h4>List Registry Server Entries</h4><p>Lists MCP registry server entries with pagination support.</p><ul><li>: Maximum number of entries to return (default: 30, max: 100)</li><li>: Pagination cursor for retrieving next set of results</li></ul><pre><code>{\n  \"servers\": [\n    {\n      \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n      \"name\": \"Example MCP Server\",\n      \"url\": \"https://example.com/mcp\",\n      \"description\": \"An example MCP server\",\n      \"created_at\": \"2025-05-17T17:34:22.912Z\",\n      \"updated_at\": \"2025-05-17T17:34:22.912Z\"\n    }\n  ],\n  \"metadata\": {\n    \"next_cursor\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"count\": 30\n  }\n}\n</code></pre><p>Retrieves detailed information about a specific MCP server entry.</p><ul><li>: Unique identifier of the server entry</li></ul><pre><code>{\n  \"id\": \"01129bff-3d65-4e3d-8e82-6f2f269f818c\",\n  \"name\": \"io.github.gongrzhe/redis-mcp-server\",\n  \"description\": \"A Redis MCP server (pushed to https://github.com/modelcontextprotocol/servers/tree/main/src/redis) implementation for interacting with Redis databases. This server enables LLMs to interact with Redis key-value stores through a set of standardized tools.\",\n  \"repository\": {\n    \"url\": \"https://github.com/GongRzhe/REDIS-MCP-Server\",\n    \"source\": \"github\",\n    \"id\": \"907849235\"\n  },\n  \"version_detail\": {\n    \"version\": \"0.0.1-seed\",\n    \"release_date\": \"2025-05-16T19:13:21Z\",\n    \"is_latest\": true\n  },\n  \"packages\": [\n    {\n      \"registry_name\": \"docker\",\n      \"name\": \"@gongrzhe/server-redis-mcp\",\n      \"version\": \"1.0.0\",\n      \"package_arguments\": [\n        {\n          \"description\": \"Docker image to run\",\n          \"is_required\": true,\n          \"format\": \"string\",\n          \"value\": \"mcp/redis\",\n          \"default\": \"mcp/redis\",\n          \"type\": \"positional\",\n          \"value_hint\": \"mcp/redis\"\n        },\n        {\n          \"description\": \"Redis server connection string\",\n          \"is_required\": true,\n          \"format\": \"string\",\n          \"value\": \"redis://host.docker.internal:6379\",\n          \"default\": \"redis://host.docker.internal:6379\",\n          \"type\": \"positional\",\n          \"value_hint\": \"host.docker.internal:6379\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre><p>Publishes a new MCP server entry to the registry. Authentication is required via Bearer token in the Authorization header.</p><ul><li>: Bearer token for authentication (e.g., )</li><li>: application/json</li></ul><pre><code>{\n    \"description\": \"&lt;your description here&gt;\",\n    \"name\": \"io.github.&lt;owner&gt;/&lt;server-name&gt;\",\n    \"packages\": [\n        {\n            \"registry_name\": \"npm\",\n            \"name\": \"@&lt;owner&gt;/&lt;server-name&gt;\",\n            \"version\": \"0.2.23\",\n            \"package_arguments\": [\n                {\n                    \"description\": \"Specify services and permissions.\",\n                    \"is_required\": true,\n                    \"format\": \"string\",\n                    \"value\": \"-s\",\n                    \"default\": \"-s\",\n                    \"type\": \"positional\",\n                    \"value_hint\": \"-s\"\n                }\n            ],\n            \"environment_variables\": [\n                {\n                    \"description\": \"API Key to access the server\",\n                    \"name\": \"API_KEY\"\n                }\n            ]\n        },{\n            \"registry_name\": \"docker\",\n            \"name\": \"@&lt;owner&gt;/&lt;server-name&gt;-cli\",\n            \"version\": \"0.123.223\",\n            \"runtime_hint\": \"docker\",\n            \"runtime_arguments\": [\n                {\n                    \"description\": \"Specify services and permissions.\",\n                    \"is_required\": true,\n                    \"format\": \"string\",\n                    \"value\": \"--mount\",\n                    \"default\": \"--mount\",\n                    \"type\": \"positional\",\n                    \"value_hint\": \"--mount\"\n                }\n            ],\n            \"environment_variables\": [\n                {\n                    \"description\": \"API Key to access the server\",\n                    \"name\": \"API_KEY\"\n                }\n            ]\n        }\n    ],\n    \"repository\": {\n        \"url\": \"https://github.com/&lt;owner&gt;/&lt;server-name&gt;\",\n        \"source\": \"github\"\n    },\n    \"version_detail\": {\n        \"version\": \"0.0.1-&lt;publisher_version&gt;\"\n    }\n}\n</code></pre><pre><code>{\n  \"message\": \"Server publication successful\",\n  \"id\": \"1234567890abcdef12345678\"\n}\n</code></pre><p>Simple ping endpoint that returns environment configuration information:</p><pre><code>{\n  \"environment\": \"dev\",\n  \"version\": \"registry-&lt;sha&gt;\"\n}\n</code></pre><p>The service can be configured using environment variables:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr><td><code>MCP_REGISTRY_DATABASE_TYPE</code></td></tr><tr><td><code>MCP_REGISTRY_COLLECTION_NAME</code></td></tr><tr><td><code>MCP_REGISTRY_DATABASE_NAME</code></td></tr><tr><td><code>MCP_REGISTRY_DATABASE_URL</code></td><td>MongoDB connection string</td><td><code>mongodb://localhost:27017</code></td></tr><tr><td><code>MCP_REGISTRY_GITHUB_CLIENT_ID</code></td></tr><tr><td><code>MCP_REGISTRY_GITHUB_CLIENT_SECRET</code></td></tr><tr></tr><tr><td><code>MCP_REGISTRY_SEED_FILE_PATH</code></td></tr><tr><td>Import  on first run</td></tr><tr><td><code>MCP_REGISTRY_SERVER_ADDRESS</code></td><td>Listen address for the server</td></tr></tbody></table><p>Run the test script to validate API endpoints:</p><pre><code>./scripts/test_endpoints.sh\n</code></pre><p>You can specify specific endpoints to test:</p><pre><code>./scripts/test_endpoints.sh --endpoint health\n./scripts/test_endpoints.sh --endpoint servers\n</code></pre>","contentLength":7427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nexus-xyz/nexus-cli","url":"https://github.com/nexus-xyz/nexus-cli","date":1750991844,"author":"","guid":172658,"unread":true,"content":"<p>Command line interface for supplying proofs to the Nexus network.</p><p>A high-performance command-line interface for contributing proofs to the Nexus network.</p><p><a href=\"https://nexus.xyz/\">Nexus</a> is a global distributed prover network that unites the world's computers to power a new and better Internet: the Verifiable Internet.</p><p>There have been several testnets so far:</p><h4>Precompiled Binary (Recommended)</h4><p>For the simplest and most reliable installation:</p><pre><code>curl https://cli.nexus.xyz/ | sh\n</code></pre><ol><li>Download and install the latest precompiled binary for your platform.</li><li>Prompt you to accept the Terms of Use.</li><li>Start the CLI in interactive mode.</li></ol><p>The exact installation script is viewable <a href=\"https://raw.githubusercontent.com/nexus-xyz/nexus-cli/main/public/install.sh\">here</a>.</p><h4>Non-Interactive Installation</h4><p>For automated installations (e.g., in CI):</p><pre><code>curl -sSf https://cli.nexus.xyz/ -o install.sh\nchmod +x install.sh\nNONINTERACTIVE=1 ./install.sh\n</code></pre><p>Proving with the CLI is documented <a href=\"https://docs.nexus.xyz/layer-1/testnet/cli-node\">here</a>.</p><p>To start with an existing node ID, run:</p><pre><code>nexus-cli start --node-id &lt;your-node-id&gt;\n</code></pre><p>Alternatively, you can register your wallet address and create a node ID with the CLI, or at at <a href=\"https://app.nexus.xyz\">app.nexus.xyz</a>.</p><pre><code>nexus-cli register-user --wallet-address &lt;your-wallet-address&gt;\nnexus-cli register-node\nnexus-cli start\n</code></pre><p>The  and  commands will save your credentials to . To clear credentials, run:</p><p>For troubleshooting or to see available command line options, run:</p><p>Use of the CLI is subject to the <a href=\"https://nexus.xyz/terms-of-use\">Terms of Use</a>. First-time users running interactively will be prompted to accept these terms.</p><p>During the CLI's startup, you'll be asked for your node ID. To skip prompts in a non-interactive environment, manually create a  in the following format:</p><pre><code>{\n   \"node_id\": \"&lt;YOUR NODE ID&gt;\"\n}\n</code></pre><p>Interested in contributing to the Nexus Network CLI? Check out our <a href=\"https://raw.githubusercontent.com/nexus-xyz/nexus-cli/main/CONTRIBUTING.md\">Contributor Guide</a> for:</p><ul><li>Development setup instructions</li><li>How to report issues and submit pull requests</li><li>Our code of conduct and community guidelines</li><li>Tips for working with the codebase</li></ul><p>For most users, we recommend using the precompiled binaries as described above. The contributor guide is intended for those who want to modify or improve the CLI itself.</p><p>The following steps may be required in order to setup a development environment for contributing to the project:</p><pre><code>sudo apt update\nsudo apt upgrade\nsudo apt install build-essential pkg-config libssl-dev git-all\nsudo apt install protobuf-compiler\n</code></pre><pre><code># Install using Homebrew\nbrew install protobuf\n\n# Verify installation\nprotoc --version\n</code></pre><pre><code># Install using Chocolatey\nchoco install protobuf\n</code></pre><p>To build the ProtoBuf files, run the following command in the  directory:</p><pre><code>cargo build --features build_proto\n</code></pre><p>To create a release, update the package version in , then create and push a new (annotated) tag, e.g.:</p><pre><code>git tag -a v0.1.2 -m \"Release v0.1.2\"\ngit push origin v0.1.2\n</code></pre><p>This will trigger the GitHub Actions release workflow that compiles binaries and pushes the Docker image, in addition to creating release.</p><p>: Creating a release through the GitHub UI creates a new release but does  trigger the workflow. This leads to a release without a Docker image or binaries, which breaks the installation script.</p>","contentLength":2982,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mui/base-ui","url":"https://github.com/mui/base-ui","date":1750991844,"author":"","guid":172659,"unread":true,"content":"<p>Unstyled UI components for building accessible web apps and design systems. From the creators of Radix, Floating UI, and Material UI.</p><p>From the creators of Radix, Floating&nbsp;UI, and Material&nbsp;UI, Base&nbsp;UI is an unstyled UI component library for building accessible user interfaces.</p><p>Read our <a href=\"https://raw.githubusercontent.com/mui/base-ui/master/CONTRIBUTING.md\">contributing guide</a> to learn about our development process, how to propose bug fixes and improvements, and how to build and test your changes.</p><p>To see the latest updates, check out the <a href=\"https://base-ui.com/react/overview/releases\">releases</a>.</p><ul><li> For community support, questions, and tips, join our <a href=\"https://discord.gg/g6C3hUtuxz\">Discord</a>.</li><li> To stay up-to-date on new releases and announcements follow <a href=\"https://x.com/base_ui\">Base&nbsp;UI on X</a>.</li></ul><p>This project is licensed under the terms of the <a href=\"https://raw.githubusercontent.com/mui/base-ui/master/LICENSE\">MIT license</a>.</p>","contentLength":675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"sindresorhus/awesome","url":"https://github.com/sindresorhus/awesome","date":1750905384,"author":"","guid":171153,"unread":true,"content":"<p>üòé Awesome lists about all kinds of interesting topics</p><ul><li><a href=\"https://github.com/sindresorhus/awesome-nodejs#readme\">Node.js</a> - Async non-blocking event-driven JavaScript runtime built on Chrome's V8 JavaScript engine. \n  </li><li><a href=\"https://github.com/vsouza/awesome-ios#readme\">iOS</a> - Mobile operating system for Apple phones and tablets.</li><li><a href=\"https://github.com/JStumpp/awesome-android#readme\">Android</a> - Mobile operating system developed by Google.</li><li><a href=\"https://github.com/sindresorhus/awesome-electron#readme\">Electron</a> - Cross-platform native desktop apps using JavaScript/HTML/CSS.</li><li><a href=\"https://github.com/busterc/awesome-cordova#readme\">Cordova</a> - JavaScript API for hybrid apps.</li><li><a href=\"https://github.com/jondot/awesome-react-native#readme\">React Native</a> - JavaScript framework for writing natively rendering mobile apps for iOS and Android.</li><li><a href=\"https://github.com/XamSome/awesome-xamarin#readme\">Xamarin</a> - Mobile app development IDE, testing, and distribution.</li><li><a href=\"https://github.com/inputsh/awesome-linux#readme\">Linux</a><ul><li><a href=\"https://github.com/zoidbergwill/awesome-ebpf#readme\">eBPF</a> - Virtual machine that allows you to write more efficient and powerful tracing and monitoring for Linux systems.</li><li><a href=\"https://github.com/AppImage/awesome-appimage#readme\">AppImage</a> - Package apps in a single file that works on various mainstream Linux distributions.</li></ul></li><li><a href=\"https://github.com/yenchenlin/awesome-watchos#readme\">watchOS</a> - Operating system for the Apple Watch.</li><li><a href=\"https://github.com/ipfs/awesome-ipfs#readme\">IPFS</a> - P2P hypermedia protocol.</li><li><a href=\"https://github.com/fuse-compound/awesome-fuse#readme\">Fuse</a> - Mobile development tools.</li><li><a href=\"https://github.com/ianstormtaylor/awesome-heroku#readme\">Heroku</a> - Cloud platform as a service.</li><li><a href=\"https://github.com/thibmaek/awesome-raspberry-pi#readme\">Raspberry Pi</a> - Credit card-sized computer aimed at teaching kids programming, but capable of a lot more.</li><li><a href=\"https://github.com/JesseTG/awesome-qt#readme\">Qt</a> - Cross-platform GUI app framework.</li><li><a href=\"https://github.com/vitalets/awesome-smart-tv#readme\">Smart TV</a> - Create apps for different TV platforms.</li><li><a href=\"https://github.com/Kazhnuz/awesome-gnome#readme\">GNOME</a> - Simple and distraction-free desktop environment for Linux.</li><li><a href=\"https://github.com/francoism90/awesome-kde#readme\">KDE</a> - A free software community dedicated to creating an open and user-friendly computing experience.</li><li><a href=\"https://github.com/quozd/awesome-dotnet#readme\">.NET</a><ul><li><a href=\"https://github.com/ironcev/awesome-roslyn#readme\">Roslyn</a> - Open-source compilers and code analysis APIs for C# and VB.NET languages.</li></ul></li><li><a href=\"https://github.com/jonleibowitz/awesome-digitalocean#readme\">DigitalOcean</a> - Cloud computing platform designed for developers.</li><li><a href=\"https://github.com/Solido/awesome-flutter#readme\">Flutter</a> - Google's mobile SDK for building native iOS and Android apps from a single codebase written in Dart.</li><li><a href=\"https://github.com/frenck/awesome-home-assistant#readme\">Home Assistant</a> - Open source home automation that puts local control and privacy first.</li><li><a href=\"https://github.com/victorshinya/awesome-ibmcloud#readme\">IBM Cloud</a> - Cloud platform for developers and companies.</li><li><a href=\"https://github.com/jthegedus/awesome-firebase#readme\">Firebase</a> - App development platform built on Google Cloud.</li><li><a href=\"https://github.com/irazasyed/awesome-cloudflare#readme\">Cloudflare</a> - CDN, DNS, DDoS protection, and security for your site.</li><li><a href=\"https://github.com/agucova/awesome-esp#readme\">ESP</a> - Low-cost microcontrollers with WiFi and broad IoT applications.</li><li><a href=\"https://github.com/denolib/awesome-deno#readme\">Deno</a> - A secure runtime for JavaScript and TypeScript that uses V8 and is built in Rust.</li><li><a href=\"https://github.com/balintkissdev/awesome-dos#readme\">DOS</a> - Operating system for x86-based personal computers that was popular during the 1980s and early 1990s.</li><li><a href=\"https://github.com/nix-community/awesome-nix#readme\">Nix</a> - Package manager for Linux and other Unix systems that makes package management reliable and reproducible.</li><li><a href=\"https://github.com/stn1slv/awesome-integration#readme\">Integration</a> - Linking together different IT systems (components) to functionally cooperate as a whole.</li><li><a href=\"https://github.com/naimo84/awesome-nodered#readme\">Node-RED</a> - A programming tool for wiring together hardware devices, APIs, and online services.</li><li><a href=\"https://github.com/zenitysec/awesome-low-code#readme\">Low Code</a> - Allowing business professionals to address their needs on their own with little to no coding skills.</li><li><a href=\"https://github.com/riderx/awesome-capacitor#readme\">Capacitor</a> - Cross-platform open source runtime for building Web Native apps.</li><li><a href=\"https://github.com/dotintent/awesome-ble#readme\">Bluetooth Low Energy</a> - Low-power wireless communication protocol ideal for IoT, wearables, and other battery-powered applications.</li><li><a href=\"https://github.com/MartinZikmund/awesome-uno-platform#readme\">Uno Platform</a> - Open-source .NET UI platform for building cross-platform apps.</li><li><a href=\"https://github.com/xavidop/awesome-firebase-genkit#readme\">Firebase Genkit</a> - An open-source framework for building AI-powered apps and features.</li></ul><ul><li><a href=\"https://github.com/vinta/awesome-python#readme\">Python</a> - General-purpose programming language designed for readability. \n  </li><li><a href=\"https://github.com/svaksha/Julia.jl#readme\">Julia</a> - High-level dynamic programming language designed to address the needs of high-performance numerical analysis and computational science.</li><li><a href=\"https://github.com/fffaraz/awesome-cpp#readme\">C/C++</a> - General-purpose language with a bias toward system programming and embedded, resource-constrained software.</li><li><a href=\"https://github.com/qinwf/awesome-R#readme\">R</a> - Functional programming language and environment for statistical computing and graphics. \n  </li><li><a href=\"https://github.com/CodyReichert/awesome-cl#readme\">Common Lisp</a> - Powerful dynamic multiparadigm language that facilitates iterative and interactive development. \n  </li><li><a href=\"https://github.com/akullpp/awesome-java#readme\">Java</a> - Popular secure object-oriented language designed for flexibility to \"write once, run anywhere\". \n  <ul><li><a href=\"https://github.com/hstsethi/awesome-j2me#readme\">J2ME</a> - Java specification designed for old keypad phones and PDAs.</li></ul></li><li><a href=\"https://github.com/ziadoz/awesome-php#readme\">PHP</a> - Server-side scripting language. \n  </li><li><a href=\"https://github.com/sfischer13/awesome-frege#readme\">Frege</a> - Haskell for the JVM.</li><li><a href=\"https://github.com/onqtam/awesome-cmake#readme\">CMake</a> - Build, test, and package software.</li><li><a href=\"https://github.com/sfischer13/awesome-eta#readme\">Eta</a> - Functional programming language for the JVM.</li><li><a href=\"https://github.com/joaomilho/awesome-idris#readme\">Idris</a> - General purpose pure functional programming language with dependent types influenced by Haskell and ML.</li><li><a href=\"https://github.com/ohenley/awesome-ada#readme\">Ada/SPARK</a> - Modern programming language designed for large, long-lived apps where reliability and efficiency are essential.</li><li><a href=\"https://github.com/ebraminio/awesome-qsharp#readme\">Q#</a> - Domain-specific programming language used for expressing quantum algorithms.</li><li><a href=\"https://github.com/koolamusic/awesome-imba#readme\">Imba</a> - Programming language inspired by Ruby and Python and compiles to performant JavaScript.</li><li><a href=\"https://github.com/desiderantes/awesome-vala#readme\">Vala</a> - Programming language designed to take full advantage of the GLib and GNOME ecosystems, while preserving the speed of C code.</li><li><a href=\"https://github.com/coq-community/awesome-coq#readme\">Coq</a> - Formal language and environment for programming and specification which facilitates interactive development of machine-checked proofs.</li><li><a href=\"https://github.com/vlang/awesome-v#readme\">V</a> - Simple, fast, safe, compiled language for developing maintainable software.</li><li><a href=\"https://github.com/catdevnull/awesome-zig#readme\">Zig</a> - General-purpose programming language and toolchain for maintaining robust, optimal, and reusable software.</li><li><a href=\"https://github.com/MystenLabs/awesome-move#readme\">Move</a> - Domain-specific programming language for writing safe smart contracts.</li><li><a href=\"https://github.com/angrykoala/awesome-esolangs#readme\">Esolangs</a> - Programming languages designed for experimentation or as jokes rather than actual use.</li><li><a href=\"https://github.com/sancarn/awesome-vba#readme\">VBA</a> - An event-driven version of Visual Basic 6.0 built into most Microsoft Office apps for automation and scripting.</li></ul><a href=\"https://vshymanskyy.github.io/StandWithUkraine\"><img src=\"https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner2-direct.svg?sanitize=true\"></a><ul><li><a href=\"https://github.com/iJackUA/awesome-vagrant#readme\">Vagrant</a> - Automation virtual machine environment.</li><li><a href=\"https://github.com/gramantin/awesome-rails#readme\">Rails</a> - Web app framework for Ruby. \n  </li><li><a href=\"https://github.com/ramitsurana/awesome-kubernetes#readme\">Kubernetes</a> - Open-source platform that automates Linux container operations.</li><li><a href=\"https://github.com/unicodeveloper/awesome-lumen#readme\">Lumen</a> - PHP micro-framework.</li><li><a href=\"https://github.com/vert-x3/vertx-awesome#readme\">Vert.x</a> - Toolkit for building reactive apps on the JVM.</li><li><a href=\"https://github.com/shuaibiyy/awesome-terraform#readme\">Terraform</a> - Tool for building, changing, and versioning infrastructure.</li><li><a href=\"https://github.com/vapor-community/awesome-vapor#readme\">Vapor</a> - Server-side development in Swift.</li><li><a href=\"https://github.com/ucg8j/awesome-dash#readme\">Dash</a> - Python web app framework.</li><li><a href=\"https://github.com/kolomied/awesome-cdk#readme\">CDK</a> - Open-source software development framework for defining cloud infrastructure in code.</li><li><a href=\"https://github.com/kdeldycke/awesome-iam#readme\">IAM</a> - User accounts, authentication and authorization.</li><li><a href=\"https://github.com/gofiber/awesome-fiber#readme\">Fiber</a> - Web framework built on top of Fasthttp, the fastest HTTP engine for Go.</li><li><a href=\"https://github.com/DevOpsHiveHQ/awesome-kustomize#readme\">Kustomize</a> - Kubernetes native declarative configuration management tool.</li><li><a href=\"https://github.com/virtualroot/awesome-opentofu#readme\">OpenTofu</a> - Open-source infrastructure as code tool.</li><li><a href=\"https://github.com/reflex-dev/awesome-reflex#readme\">Reflex</a> - Python web framework for building both your frontend and backend with no JavaScript.</li></ul><ul><li><a href=\"https://github.com/quangv/awesome-couchdb#readme\">CouchDB</a> - Document-oriented NoSQL database.</li><li><a href=\"https://github.com/rayokota/awesome-hbase#readme\">HBase</a> - Distributed, scalable, big data store.</li><li><a href=\"https://github.com/erictleung/awesome-nosql-guides#readme\">NoSQL Guides</a> - Help on using non-relational, distributed, open-source, and horizontally scalable databases.</li><li><a href=\"https://github.com/vaticle/typedb-awesome#readme\">TypeDB</a> - Logical database to organize large and complex networks of data as one body of knowledge.</li><li><a href=\"https://github.com/Anant/awesome-cassandra#readme\">Cassandra</a> - Open-source, distributed, wide column store, NoSQL database management system.</li><li><a href=\"https://github.com/taosdata/awesome-tdengine#readme\">TDengine</a> - An open-source time-series database with high-performance, scalability, and SQL support.</li><li><a href=\"https://github.com/lyqht/awesome-supabase#readme\">Supabase</a> - An open-source alternative to Firebase.</li><li><a href=\"https://github.com/benallfree/awesome-pocketbase#readme\">PocketBase</a> - An open-source, Go-based backend in one file.</li><li><a href=\"https://github.com/tyaga001/awesome-neon#readme\">Neon</a> - An open-source alternative to AWS Aurora.</li></ul><h2>Content Management Systems</h2><ul><li><a href=\"https://github.com/springload/awesome-wagtail#readme\">Wagtail</a> - Django CMS focused on flexibility and user experience.</li><li><a href=\"https://github.com/nirgn975/awesome-drupal#readme\">Drupal</a> - Extensible PHP-based CMS.</li><li><a href=\"https://github.com/MartinMiles/Awesome-Sitecore#readme\">Sitecore</a> - .NET digital marketing platform that combines CMS with tools for managing multiple websites.</li><li><a href=\"https://github.com/directus-community/awesome-directus#readme\">Directus</a> - A real-time API and app dashboard for managing SQL database content.</li><li><a href=\"https://github.com/collective/awesome-plone#readme\">Plone</a> - Open source Python CMS.</li><li><a href=\"https://github.com/DanailMinchev/awesome-payload#readme\">Payload</a> - Next.js native and open source headless CMS.</li></ul><ul><li><a href=\"https://github.com/eozer/awesome-snmp#readme\">SNMP</a> - A protocol for collecting, modifying, and organizing information about managed devices on IP networks.</li><li><a href=\"https://github.com/secdev/awesome-scapy#readme\">Scapy</a> - Python-based interactive packet manipulation.</li><li><a href=\"https://github.com/seifrajhi/awesome-cilium#readme\">Cilium</a> - Provides networking and security capabilities for containerized apps, microservices, and virtual machines.</li></ul><ul><li><a href=\"https://github.com/igorbarinov/awesome-bitcoin#readme\">Bitcoin</a> - Bitcoin services and tools for software developers.</li><li><a href=\"https://github.com/vhpoet/awesome-ripple#readme\">Ripple</a> - Open source distributed settlement network.</li><li><a href=\"https://github.com/hyperupcall/awesome-mastodon#readme\">Mastodon</a> - Open source decentralized microblogging network.</li><li><a href=\"https://github.com/ttumiel/Awesome-Ethereum#readme\">Ethereum</a> - Distributed computing platform for smart contract development.</li><li><a href=\"https://github.com/steven2358/awesome-blockchain-ai#readme\">Blockchain AI</a> - Blockchain projects for artificial intelligence and machine learning.</li><li><a href=\"https://github.com/DanailMinchev/awesome-eosio#readme\">EOSIO</a> - A decentralized operating system supporting industrial-scale apps.</li><li><a href=\"https://github.com/chainstack/awesome-corda#readme\">Corda</a> - Open source blockchain platform designed for business.</li><li><a href=\"https://github.com/msmolyakov/awesome-waves#readme\">Waves</a> - Open source blockchain platform and development toolset for Web 3.0 apps and decentralized solutions.</li><li><a href=\"https://github.com/substrate-developer-hub/awesome-substrate#readme\">Substrate</a> - Framework for writing scalable, upgradeable blockchains in Rust.</li><li><a href=\"https://github.com/golemfactory/awesome-golem#readme\">Golem</a> - Open source peer-to-peer marketplace for computing resources.</li><li><a href=\"https://github.com/friedger/awesome-stacks-chain#readme\">Stacks</a> - A smart contract platform secured by Bitcoin.</li><li><a href=\"https://github.com/aorumbayev/awesome-algorand#readme\">Algorand</a> - An open-source, proof of stake blockchain and smart contract computing platform.</li><li><a href=\"https://github.com/zolagonano/awesome-zeronet#readme\">ZeroNet</a> - A decentralized web-like network of peer-to-peer users.</li><li><a href=\"https://github.com/cosmos/awesome-cosmos#readme\">Cosmos SDK</a> - Modular framework for building app-specific blockchains in Go.</li><li><a href=\"https://github.com/polycarbohydrate/awesome-tor#readme\">Tor</a> - A free overlay network for enabling anonymous communication.</li><li><a href=\"https://github.com/atblueprints/awesome-atproto#readme\">ATProto</a> - Open, decentralized network for building social apps.</li></ul><h2>Health and Social Science</h2><ul><li><a href=\"https://github.com/christian-bromann/awesome-selenium#readme\">Selenium</a> - Open-source browser automation framework and ecosystem.</li><li><a href=\"https://github.com/SrinivasanTarget/awesome-appium#readme\">Appium</a> - Test automation tool for apps.</li><li><a href=\"https://github.com/sindresorhus/awesome-tap#readme\">TAP</a> - Test Anything Protocol.</li><li><a href=\"https://github.com/aliesbelik/awesome-jmeter#readme\">JMeter</a> - Load testing and performance measurement tool.</li><li><a href=\"https://github.com/grafana/awesome-k6#readme\">k6</a> - Open-source, developer-centric performance monitoring and load testing solution.</li><li><a href=\"https://github.com/mxschmitt/awesome-playwright#readme\">Playwright</a> - Node.js library to automate Chromium, Firefox and WebKit with a single API.</li><li><a href=\"https://github.com/aliesbelik/awesome-gatling#readme\">Gatling</a> - Open-source load and performance testing framework based on Scala, Akka, and Netty.</li></ul>","contentLength":8636,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"gitleaks/gitleaks","url":"https://github.com/gitleaks/gitleaks","date":1750905384,"author":"","guid":171154,"unread":true,"content":"<p>Find secrets with Gitleaks üîë</p><pre><code>‚îå‚îÄ‚óã‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ ‚îÇ‚ï≤  ‚îÇ\n‚îÇ ‚îÇ ‚óã ‚îÇ\n‚îÇ ‚óã ‚ñë ‚îÇ\n‚îî‚îÄ‚ñë‚îÄ‚îÄ‚îÄ‚îò\n</code></pre><p>Gitleaks is a tool for  secrets like passwords, API keys, and tokens in git repos, files, and whatever else you wanna throw at it via . If you wanna learn more about how the detection engine works check out this blog: <a href=\"https://lookingatcomputer.substack.com/p/regex-is-almost-all-you-need\">Regex is (almost) all you need</a>.</p><pre><code>‚ûú  ~/code(master) gitleaks git -v\n\n    ‚óã\n    ‚îÇ‚ï≤\n    ‚îÇ ‚óã\n    ‚óã ‚ñë\n    ‚ñë    gitleaks\n\n\nFinding:     \"export BUNDLE_ENTERPRISE__CONTRIBSYS__COM=cafebabe:deadbeef\",\nSecret:      cafebabe:deadbeef\nRuleID:      sidekiq-secret\nEntropy:     2.609850\nFile:        cmd/generate/config/rules/sidekiq.go\nLine:        23\nCommit:      cd5226711335c68be1e720b318b7bc3135a30eb2\nAuthor:      John\nEmail:       john@users.noreply.github.com\nDate:        2022-08-03T12:31:40Z\nFingerprint: cd5226711335c68be1e720b318b7bc3135a30eb2:cmd/generate/config/rules/sidekiq.go:sidekiq-secret:23\n</code></pre><p>Gitleaks can be installed using Homebrew, Docker, or Go. Gitleaks is also available in binary form for many popular platforms and OS types on the <a href=\"https://github.com/gitleaks/gitleaks/releases\">releases page</a>. In addition, Gitleaks can be implemented as a pre-commit hook directly in your repo or as a GitHub action using <a href=\"https://github.com/gitleaks/gitleaks-action\">Gitleaks-Action</a>.</p><pre><code># MacOS\nbrew install gitleaks\n\n# Docker (DockerHub)\ndocker pull zricethezav/gitleaks:latest\ndocker run -v ${path_to_host_folder_to_scan}:/path zricethezav/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]\n\n# Docker (ghcr.io)\ndocker pull ghcr.io/gitleaks/gitleaks:latest\ndocker run -v ${path_to_host_folder_to_scan}:/path ghcr.io/gitleaks/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]\n\n# From Source (make sure `go` is installed)\ngit clone https://github.com/gitleaks/gitleaks.git\ncd gitleaks\nmake build\n</code></pre><pre><code>name: gitleaks\non: [pull_request, push, workflow_dispatch]\njobs:\n  scan:\n    name: gitleaks\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - uses: gitleaks/gitleaks-action@v2\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          GITLEAKS_LICENSE: ${{ secrets.GITLEAKS_LICENSE}} # Only required for Organizations, not personal accounts.\n</code></pre><pre><code>‚ûú git commit -m \"this commit contains a secret\"\nDetect hardcoded secrets.................................................Failed\n</code></pre><p>Note: to disable the gitleaks pre-commit hook you can prepend  to the commit command and it will skip running gitleaks</p><pre><code>‚ûú SKIP=gitleaks git commit -m \"skip gitleaks check\"\nDetect hardcoded secrets................................................Skipped\n</code></pre><pre><code>Usage:\n  gitleaks [command]\n\nAvailable Commands:\n  dir         scan directories or files for secrets\n  git         scan git repositories for secrets\n  help        Help about any command\n  stdin       detect secrets from stdin\n  version     display gitleaks version\n\nFlags:\n  -b, --baseline-path string          path to baseline with issues that can be ignored\n  -c, --config string                 config file path\n                                      order of precedence:\n                                      1. --config/-c\n                                      2. env var GITLEAKS_CONFIG\n                                      3. env var GITLEAKS_CONFIG_TOML with the file content\n                                      4. (target path)/.gitleaks.toml\n                                      If none of the four options are used, then gitleaks will use the default config\n      --diagnostics string            enable diagnostics (comma-separated list: cpu,mem,trace). cpu=CPU profiling, mem=memory profiling, trace=execution tracing\n      --diagnostics-dir string        directory to store diagnostics output files (defaults to current directory)\n      --enable-rule strings           only enable specific rules by id\n      --exit-code int                 exit code when leaks have been encountered (default 1)\n  -i, --gitleaks-ignore-path string   path to .gitleaksignore file or folder containing one (default \".\")\n  -h, --help                          help for gitleaks\n      --ignore-gitleaks-allow         ignore gitleaks:allow comments\n  -l, --log-level string              log level (trace, debug, info, warn, error, fatal) (default \"info\")\n      --max-decode-depth int          allow recursive decoding up to this depth (default \"0\", no decoding is done)\n      --max-archive-depth int         allow scanning into nested archives up to this depth (default \"0\", no archive traversal is done)\n      --max-target-megabytes int      files larger than this will be skipped\n      --no-banner                     suppress banner\n      --no-color                      turn off color for verbose output\n      --redact uint[=100]             redact secrets from logs and stdout. To redact only parts of the secret just apply a percent value from 0..100. For example --redact=20 (default 100%)\n  -f, --report-format string          output format (json, csv, junit, sarif, template)\n  -r, --report-path string            report file\n      --report-template string        template file used to generate the report (implies --report-format=template)\n  -v, --verbose                       show verbose output from scan\n      --version                       version for gitleaks\n\nUse \"gitleaks [command] --help\" for more information about a command.\n</code></pre><p>‚ö†Ô∏è v8.19.0 introduced a change that deprecated  and . Those commands are still available but are hidden in the  menu. Take a look at this <a href=\"https://gist.github.com/zricethezav/b325bb93ebf41b9c0b0507acf12810d2\">gist</a> for easy command translations. If you find v8.19.0 broke an existing command (/), please open an issue.</p><p>There are three scanning modes: , , and .</p><p>The  command lets you scan local git repos. Under the hood, gitleaks uses the  command to scan patches. You can configure the behavior of  with the  option. For example, if you wanted to run gitleaks on a range of commits you could use the following command: <code>gitleaks git -v --log-opts=\"--all commitA..commitB\" path_to_repo</code>. See the <a href=\"https://git-scm.com/docs/git-log\">git log</a> documentation for more information. If there is no target specified as a positional argument, then gitleaks will attempt to scan the current working directory as a git repo.</p><p>The  (aliases include , ) command lets you scan directories and files. Example: <code>gitleaks dir -v path_to_directory_or_file</code>. If there is no target specified as a positional argument, then gitleaks will scan the current working directory.</p><p>You can also stream data to gitleaks with the  command. Example: <code>cat some_file | gitleaks -v stdin</code></p><p>When scanning large repositories or repositories with a long history, it can be convenient to use a baseline. When using a baseline, gitleaks will ignore any old findings that are present in the baseline. A baseline can be any gitleaks report. To create a gitleaks report, run gitleaks with the  parameter.</p><pre><code>gitleaks git --report-path gitleaks-report.json # This will save the report in a file called gitleaks-report.json\n</code></pre><p>Once as baseline is created it can be applied when running the detect command again:</p><pre><code>gitleaks git --baseline-path gitleaks-report.json --report-path findings.json\n</code></pre><p>After running the detect command with the --baseline-path parameter, report output (findings.json) will only contain new issues.</p><p>You can run Gitleaks as a pre-commit hook by copying the example  script into your  directory.</p><p>The order of precedence is:</p><ol><li> option: <pre><code>gitleaks git --config /home/dev/customgitleaks.toml .\n</code></pre></li><li>Environment variable  with the file path: <pre><code>export GITLEAKS_CONFIG=\"/home/dev/customgitleaks.toml\"\ngitleaks git .\n</code></pre></li><li>Environment variable  with the file content: <pre><code>export GITLEAKS_CONFIG_TOML=`cat customgitleaks.toml`\ngitleaks git .\n</code></pre></li><li>A  file within the target path: </li></ol><p>If none of the four options are used, then gitleaks will use the default config.</p><p>Gitleaks offers a configuration format you can follow to write your own secret detection rules:</p><pre><code># Title for the gitleaks configuration file.\ntitle = \"Custom Gitleaks configuration\"\n\n# You have basically two options for your custom configuration:\n#\n# 1. define your own configuration, default rules do not apply\n#\n#    use e.g., the default configuration as starting point:\n#    https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml\n#\n# 2. extend a configuration, the rules are overwritten or extended\n#\n#    When you extend a configuration the extended rules take precedence over the\n#    default rules. I.e., if there are duplicate rules in both the extended\n#    configuration and the default configuration the extended rules or\n#    attributes of them will override the default rules.\n#    Another thing to know with extending configurations is you can chain\n#    together multiple configuration files to a depth of 2. Allowlist arrays are\n#    appended and can contain duplicates.\n\n# useDefault and path can NOT be used at the same time. Choose one.\n[extend]\n# useDefault will extend the default gitleaks config built in to the binary\n# the latest version is located at:\n# https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml\nuseDefault = true\n# or you can provide a path to a configuration to extend from.\n# The path is relative to where gitleaks was invoked,\n# not the location of the base config.\n# path = \"common_config.toml\"\n# If there are any rules you don't want to inherit, they can be specified here.\ndisabledRules = [ \"generic-api-key\"]\n\n# An array of tables that contain information that define instructions\n# on how to detect secrets\n[[rules]]\n# Unique identifier for this rule\nid = \"awesome-rule-1\"\n\n# Short human-readable description of the rule.\ndescription = \"awesome rule 1\"\n\n# Golang regular expression used to detect secrets. Note Golang's regex engine\n# does not support lookaheads.\nregex = '''one-go-style-regex-for-this-rule'''\n\n# Int used to extract secret from regex match and used as the group that will have\n# its entropy checked if `entropy` is set.\nsecretGroup = 3\n\n# Float representing the minimum shannon entropy a regex group must have to be considered a secret.\nentropy = 3.5\n\n# Golang regular expression used to match paths. This can be used as a standalone rule or it can be used\n# in conjunction with a valid `regex` entry.\npath = '''a-file-path-regex'''\n\n# Keywords are used for pre-regex check filtering. Rules that contain\n# keywords will perform a quick string compare check to make sure the\n# keyword(s) are in the content being scanned. Ideally these values should\n# either be part of the identiifer or unique strings specific to the rule's regex\n# (introduced in v8.6.0)\nkeywords = [\n  \"auth\",\n  \"password\",\n  \"token\",\n]\n\n# Array of strings used for metadata and reporting purposes.\ntags = [\"tag\",\"another tag\"]\n\n    # ‚ö†Ô∏è In v8.21.0 `[rules.allowlist]` was replaced with `[[rules.allowlists]]`.\n    # This change was backwards-compatible: instances of `[rules.allowlist]` still  work.\n    #\n    # You can define multiple allowlists for a rule to reduce false positives.\n    # A finding will be ignored if _ANY_ `[[rules.allowlists]]` matches.\n    [[rules.allowlists]]\n    description = \"ignore commit A\"\n    # When multiple criteria are defined the default condition is \"OR\".\n    # e.g., this can match on |commits| OR |paths| OR |stopwords|.\n    condition = \"OR\"\n    commits = [ \"commit-A\", \"commit-B\"]\n    paths = [\n      '''go\\.mod''',\n      '''go\\.sum'''\n    ]\n    # note: stopwords targets the extracted secret, not the entire regex match\n    # like 'regexes' does. (stopwords introduced in 8.8.0)\n    stopwords = [\n      '''client''',\n      '''endpoint''',\n    ]\n\n    [[rules.allowlists]]\n    # The \"AND\" condition can be used to make sure all criteria match.\n    # e.g., this matches if |regexes| AND |paths| are satisfied.\n    condition = \"AND\"\n    # note: |regexes| defaults to check the _Secret_ in the finding.\n    # Acceptable values for |regexTarget| are \"secret\" (default), \"match\", and \"line\".\n    regexTarget = \"match\"\n    regexes = [ '''(?i)parseur[il]''' ]\n    paths = [ '''package-lock\\.json''' ]\n\n# You can extend a particular rule from the default config. e.g., gitlab-pat\n# if you have defined a custom token prefix on your GitLab instance\n[[rules]]\nid = \"gitlab-pat\"\n# all the other attributes from the default rule are inherited\n\n    [[rules.allowlists]]\n    regexTarget = \"line\"\n    regexes = [ '''MY-glpat-''' ]\n\n\n# ‚ö†Ô∏è In v8.25.0 `[allowlist]` was replaced with `[[allowlists]]`.\n#\n# Global allowlists have a higher order of precedence than rule-specific allowlists.\n# If a commit listed in the `commits` field below is encountered then that commit will be skipped and no\n# secrets will be detected for said commit. The same logic applies for regexes and paths.\n[[allowlists]]\ndescription = \"global allow list\"\ncommits = [ \"commit-A\", \"commit-B\", \"commit-C\"]\npaths = [\n  '''gitleaks\\.toml''',\n  '''(.*?)(jpg|gif|doc)'''\n]\n# note: (global) regexTarget defaults to check the _Secret_ in the finding.\n# Acceptable values for regexTarget are \"match\" and \"line\"\nregexTarget = \"match\"\nregexes = [\n  '''219-09-9999''',\n  '''078-05-1120''',\n  '''(9[0-9]{2}|666)-\\d{2}-\\d{4}''',\n]\n# note: stopwords targets the extracted secret, not the entire regex match\n# like 'regexes' does. (stopwords introduced in 8.8.0)\nstopwords = [\n  '''client''',\n  '''endpoint''',\n]\n\n# ‚ö†Ô∏è In v8.25.0, `[[allowlists]]` have a new field called |targetRules|.\n#\n# Common allowlists can be defined once and assigned to multiple rules using |targetRules|.\n# This will only run on the specified rules, not globally.\n[[allowlists]]\ntargetRules = [\"awesome-rule-1\", \"awesome-rule-2\"]\ndescription = \"Our test assets trigger false-positives in a couple rules.\"\npaths = ['''tests/expected/._\\.json$''']\n</code></pre><p>If you are knowingly committing a test secret that gitleaks will catch you can add a  comment to that line which will instruct gitleaks to ignore that secret. Ex:</p><pre><code>class CustomClass:\n    discord_client_secret = '8dyfuiRyq=vVc3RRr_edRk-fK__JItpZ'  #gitleaks:allow\n\n</code></pre><p>You can ignore specific findings by creating a  file at the root of your repo. In release v8.10.0 Gitleaks added a  value to the Gitleaks report. Each leak, or finding, has a Fingerprint that uniquely identifies a secret. Add this fingerprint to the  file to ignore that specific secret. See Gitleaks' <a href=\"https://github.com/gitleaks/gitleaks/raw/master/.gitleaksignore\">.gitleaksignore</a> for an example. Note: this feature is experimental and is subject to change in the future.</p><p>Sometimes secrets are encoded in a way that can make them difficult to find with just regex. Now you can tell gitleaks to automatically find and decode encoded text. The flag  enables this feature (the default value \"0\" means the feature is disabled by default).</p><p>Recursive decoding is supported since decoded text can also contain encoded text. The flag  sets the recursion limit. Recursion stops when there are no new segments of encoded text to decode, so setting a really high max depth doesn't mean it will make that many passes. It will only make as many as it needs to decode the text. Overall, decoding only minimally increases scan times.</p><p>The findings for encoded text differ from normal findings in the following ways:</p><ul><li>The location points the bounds of the encoded text \n  <ul><li>If the rule matches outside the encoded text, the bounds are adjusted to include that as well</li></ul></li><li>The match and secret contain the decoded value</li><li>Two tags are added  and </li></ul><p>Currently supported encodings:</p><ul><li> - Any printable ASCII percent encoded values</li><li> - Any printable ASCII hex encoded values &gt;= 32 characters</li><li> - Any printable ASCII base64 encoded values &gt;= 16 characters</li></ul><p>Sometimes secrets are packaged within archive files like zip files or tarballs, making them difficult to discover. Now you can tell gitleaks to automatically extract and scan the contents of archives. The flag  enables this feature for both  and  scan types. The default value of \"0\" means this feature is disabled by default.</p><p>Recursive scanning is supported since archives can also contain other archives. The  flag sets the recursion limit. Recursion stops when there are no new archives to extract, so setting a very high max depth just sets the potential to go that deep. It will only go as deep as it needs to.</p><p>The findings for secrets located within an archive will include the path to the file inside the archive. Inner paths are separated with .</p><p>Example finding (shortened for brevity):</p><pre><code>Finding:     DB_PASSWORD=8ae31cacf141669ddfb5da\n...\nFile:        testdata/archives/nested.tar.gz!archives/files.tar!files/.env.prod\nLine:        4\nCommit:      6e6ee6596d337bb656496425fb98644eb62b4a82\n...\nFingerprint: 6e6ee6596d337bb656496425fb98644eb62b4a82:testdata/archives/nested.tar.gz!archives/files.tar!files/.env.prod:generic-api-key:4\nLink:        https://github.com/leaktk/gitleaks/blob/6e6ee6596d337bb656496425fb98644eb62b4a82/testdata/archives/nested.tar.gz\n</code></pre><p>This means a secret was detected on line 4 of  which is in  which is in <code>testdata/archives/nested.tar.gz</code>.</p><p>Currently supported formats:</p><p>For example, the following template provides a custom JSON output:</p><pre><code># jsonextra.tmpl\n[{{ $lastFinding := (sub (len . ) 1) }}\n{{- range $i, $finding := . }}{{with $finding}}\n    {\n        \"Description\": {{ quote .Description }},\n        \"StartLine\": {{ .StartLine }},\n        \"EndLine\": {{ .EndLine }},\n        \"StartColumn\": {{ .StartColumn }},\n        \"EndColumn\": {{ .EndColumn }},\n        \"Line\": {{ quote .Line }},\n        \"Match\": {{ quote .Match }},\n        \"Secret\": {{ quote .Secret }},\n        \"File\": \"{{ .File }}\",\n        \"SymlinkFile\": {{ quote .SymlinkFile }},\n        \"Commit\": {{ quote .Commit }},\n        \"Entropy\": {{ .Entropy }},\n        \"Author\": {{ quote .Author }},\n        \"Email\": {{ quote .Email }},\n        \"Date\": {{ quote .Date }},\n        \"Message\": {{ quote .Message }},\n        \"Tags\": [{{ $lastTag := (sub (len .Tags ) 1) }}{{ range $j, $tag := .Tags }}{{ quote . }}{{ if ne $j $lastTag }},{{ end }}{{ end }}],\n        \"RuleID\": {{ quote .RuleID }},\n        \"Fingerprint\": {{ quote .Fingerprint }}\n    }{{ if ne $i $lastFinding }},{{ end }}\n{{- end}}{{ end }}\n]\n</code></pre><pre><code>$ gitleaks dir ~/leaky-repo/ --report-path \"report.json\" --report-format template --report-template testdata/report/jsonextra.tmpl\n</code></pre><a href=\"https://coderabbit.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=gitleaks\"></a><a href=\"https://coderabbit.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=gitleaks\"><img alt=\"CodeRabbit.ai Sponsorship\" src=\"https://github.com/gitleaks/gitleaks/assets/15034943/76c30a85-887b-47ca-9956-17a8e55c6c41\" width=\"200\"></a><p>You can always set the exit code when leaks are encountered with the --exit-code flag. Default exit codes below:</p><pre><code>0 - no leaks present\n1 - leaks or error encountered\n126 - unknown flag\n</code></pre>","contentLength":18153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"eriklindernoren/ML-From-Scratch","url":"https://github.com/eriklindernoren/ML-From-Scratch","date":1750905384,"author":"","guid":171155,"unread":true,"content":"<p>Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.</p><p>Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.</p><p>The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible but rather to present the inner workings of them in a transparent and accessible way.</p><pre><code>$ git clone https://github.com/eriklindernoren/ML-From-Scratch\n$ cd ML-From-Scratch\n$ python setup.py install\n</code></pre><pre><code>$ python mlfromscratch/examples/polynomial_regression.py\n</code></pre><p align=\"center\"> Figure: Training progress of a regularized polynomial regression model fitting  temperature data measured in Link√∂ping, Sweden 2016. </p><pre><code>$ python mlfromscratch/examples/convolutional_neural_network.py\n\n+---------+\n| ConvNet |\n+---------+\nInput Shape: (1, 8, 8)\n+----------------------+------------+--------------+\n| Layer Type           | Parameters | Output Shape |\n+----------------------+------------+--------------+\n| Conv2D               | 160        | (16, 8, 8)   |\n| Activation (ReLU)    | 0          | (16, 8, 8)   |\n| Dropout              | 0          | (16, 8, 8)   |\n| BatchNormalization   | 2048       | (16, 8, 8)   |\n| Conv2D               | 4640       | (32, 8, 8)   |\n| Activation (ReLU)    | 0          | (32, 8, 8)   |\n| Dropout              | 0          | (32, 8, 8)   |\n| BatchNormalization   | 4096       | (32, 8, 8)   |\n| Flatten              | 0          | (2048,)      |\n| Dense                | 524544     | (256,)       |\n| Activation (ReLU)    | 0          | (256,)       |\n| Dropout              | 0          | (256,)       |\n| BatchNormalization   | 512        | (256,)       |\n| Dense                | 2570       | (10,)        |\n| Activation (Softmax) | 0          | (10,)        |\n+----------------------+------------+--------------+\nTotal Parameters: 538570\n\nTraining: 100% [------------------------------------------------------------------------] Time: 0:01:55\nAccuracy: 0.987465181058\n</code></pre><p align=\"center\"> Figure: Classification of the digit dataset using CNN. </p><pre><code>$ python mlfromscratch/examples/dbscan.py\n</code></pre><p align=\"center\"> Figure: Clustering of the moons dataset using DBSCAN. </p><h3>Generating Handwritten Digits</h3><pre><code>$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py\n\n+-----------+\n| Generator |\n+-----------+\nInput Shape: (100,)\n+------------------------+------------+--------------+\n| Layer Type             | Parameters | Output Shape |\n+------------------------+------------+--------------+\n| Dense                  | 25856      | (256,)       |\n| Activation (LeakyReLU) | 0          | (256,)       |\n| BatchNormalization     | 512        | (256,)       |\n| Dense                  | 131584     | (512,)       |\n| Activation (LeakyReLU) | 0          | (512,)       |\n| BatchNormalization     | 1024       | (512,)       |\n| Dense                  | 525312     | (1024,)      |\n| Activation (LeakyReLU) | 0          | (1024,)      |\n| BatchNormalization     | 2048       | (1024,)      |\n| Dense                  | 803600     | (784,)       |\n| Activation (TanH)      | 0          | (784,)       |\n+------------------------+------------+--------------+\nTotal Parameters: 1489936\n\n+---------------+\n| Discriminator |\n+---------------+\nInput Shape: (784,)\n+------------------------+------------+--------------+\n| Layer Type             | Parameters | Output Shape |\n+------------------------+------------+--------------+\n| Dense                  | 401920     | (512,)       |\n| Activation (LeakyReLU) | 0          | (512,)       |\n| Dropout                | 0          | (512,)       |\n| Dense                  | 131328     | (256,)       |\n| Activation (LeakyReLU) | 0          | (256,)       |\n| Dropout                | 0          | (256,)       |\n| Dense                  | 514        | (2,)         |\n| Activation (Softmax)   | 0          | (2,)         |\n+------------------------+------------+--------------+\nTotal Parameters: 533762\n</code></pre><p align=\"center\"> Figure: Training progress of a Generative Adversarial Network generating  handwritten digits. </p><h3>Deep Reinforcement Learning</h3><pre><code>$ python mlfromscratch/examples/deep_q_network.py\n\n+----------------+\n| Deep Q-Network |\n+----------------+\nInput Shape: (4,)\n+-------------------+------------+--------------+\n| Layer Type        | Parameters | Output Shape |\n+-------------------+------------+--------------+\n| Dense             | 320        | (64,)        |\n| Activation (ReLU) | 0          | (64,)        |\n| Dense             | 130        | (2,)         |\n+-------------------+------------+--------------+\nTotal Parameters: 450\n</code></pre><p align=\"center\"> Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym. </p><h3>Image Reconstruction With RBM</h3><pre><code>$ python mlfromscratch/examples/restricted_boltzmann_machine.py\n</code></pre><p align=\"center\"> Figure: Shows how the network gets better during training at reconstructing  the digit 2 in the MNIST dataset. </p><h3>Evolutionary Evolved Neural Network</h3><pre><code>$ python mlfromscratch/examples/neuroevolution.py\n\n+---------------+\n| Model Summary |\n+---------------+\nInput Shape: (64,)\n+----------------------+------------+--------------+\n| Layer Type           | Parameters | Output Shape |\n+----------------------+------------+--------------+\n| Dense                | 1040       | (16,)        |\n| Activation (ReLU)    | 0          | (16,)        |\n| Dense                | 170        | (10,)        |\n| Activation (Softmax) | 0          | (10,)        |\n+----------------------+------------+--------------+\nTotal Parameters: 1210\n\nPopulation Size: 100\nGenerations: 3000\nMutation Rate: 0.01\n\n[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]\n[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]\n...\n[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]\nTest set accuracy: 96.7%\n</code></pre><p align=\"center\"> Figure: Classification of the digit dataset by a neural network which has been evolutionary evolved. </p><pre><code>$ python mlfromscratch/examples/genetic_algorithm.py\n\n+--------+\n|   GA   |\n+--------+\nDescription: Implementation of a Genetic Algorithm which aims to produce\nthe user specified target string. This implementation calculates each\ncandidate's fitness based on the alphabetical distance between the candidate\nand the target. A candidate is selected as a parent with probabilities proportional\nto the candidate's fitness. Reproduction is implemented as a single-point\ncrossover between pairs of parents. Mutation is done by randomly assigning\nnew characters with uniform probability.\n\nParameters\n----------\nTarget String: 'Genetic Algorithm'\nPopulation Size: 100\nMutation Rate: 0.05\n\n[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]\n[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]\n[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]\n[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]\n[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]\n...\n[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]\n[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]\n[294 Answer: 'Genetic Algorithm']\n</code></pre><pre><code>$ python mlfromscratch/examples/apriori.py\n+-------------+\n|   Apriori   |\n+-------------+\nMinimum Support: 0.25\nMinimum Confidence: 0.8\nTransactions:\n    [1, 2, 3, 4]\n    [1, 2, 4]\n    [1, 2]\n    [2, 3, 4]\n    [2, 3]\n    [3, 4]\n    [2, 4]\nFrequent Itemsets:\n    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]\nRules:\n    1 -&gt; 2 (support: 0.43, confidence: 1.0)\n    4 -&gt; 2 (support: 0.57, confidence: 0.8)\n    [1, 4] -&gt; 2 (support: 0.29, confidence: 1.0)\n</code></pre><p>If there's some implementation you would like to see here or if you're just feeling social, feel free to <a href=\"mailto:eriklindernoren@gmail.com\">email</a> me or connect with me on <a href=\"https://www.linkedin.com/in/eriklindernoren/\">LinkedIn</a>.</p>","contentLength":7654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["trending"]}