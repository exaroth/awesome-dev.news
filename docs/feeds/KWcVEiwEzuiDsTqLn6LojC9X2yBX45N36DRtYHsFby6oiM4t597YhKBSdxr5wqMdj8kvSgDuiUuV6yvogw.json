{"id":"KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw","title":"GitHub All Languages Daily Trending","displayTitle":"Github Trending","url":"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml","feedLink":"http://mshibanami.github.io/GitHubTrendingRSS","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":47,"items":[{"title":"fmtlib/fmt","url":"https://github.com/fmtlib/fmt","date":1761964521,"author":"","guid":323846,"unread":true,"content":"<p>A modern formatting library</p><img src=\"https://user-images.githubusercontent.com/576385/156254208-f5b743a9-88cf-439d-b0c0-923d53e8d551.png\" alt=\"{fmt}\" width=\"25%\"><p> is an open-source formatting library providing a fast and safe alternative to C stdio and C++ iostreams.</p><p>If you like this project, please consider donating to one of the funds that help victims of the war in Ukraine: <a href=\"https://www.stopputin.net/\">https://www.stopputin.net/</a>.</p><ul><li>Simple <a href=\"https://fmt.dev/latest/api/\">format API</a> with positional arguments for localization</li><li>Fast IEEE 754 floating-point formatter with correct rounding, shortness and round-trip guarantees using the <a href=\"https://github.com/jk-jeon/dragonbox\">Dragonbox</a> algorithm</li><li>Small code size both in terms of source code with the minimum configuration consisting of just three files, ,  and , and compiled code; see <a href=\"https://raw.githubusercontent.com/fmtlib/fmt/master/#compile-time-and-code-bloat\">Compile time and code bloat</a></li><li>Safety: the library is fully type-safe, errors in format strings can be reported at compile time, automatic memory management prevents buffer overflow errors</li><li>Ease of use: small self-contained code base, no external dependencies, permissive MIT <a href=\"https://github.com/fmtlib/fmt/raw/master/LICENSE\">license</a></li><li><a href=\"https://fmt.dev/latest/#portability\">Portability</a> with consistent output across platforms and support for older compilers</li><li>Clean warning-free codebase even on high warning levels such as </li><li>Locale independence by default</li><li>Optional header-only configuration enabled with the  macro</li></ul><pre><code>#include &lt;fmt/base.h&gt;\n\nint main() {\n  fmt::print(\"Hello, world!\\n\");\n}\n</code></pre><pre><code>std::string s = fmt::format(\"The answer is {}.\", 42);\n// s == \"The answer is 42.\"\n</code></pre><p><strong>Format a string using positional arguments</strong> (<a href=\"https://godbolt.org/z/Yn7Txe\">run</a>)</p><pre><code>std::string s = fmt::format(\"I'd rather be {1} than {0}.\", \"right\", \"happy\");\n// s == \"I'd rather be happy than right.\"\n</code></pre><p> (<a href=\"https://godbolt.org/z/c31ExdY3W\">run</a>)</p><pre><code>#include &lt;fmt/chrono.h&gt;\n\nint main() {\n  auto now = std::chrono::system_clock::now();\n  fmt::print(\"Date and time: {}\\n\", now);\n  fmt::print(\"Time: {:%H:%M}\\n\", now);\n}\n</code></pre><pre><code>Date and time: 2023-12-26 19:10:31.557195597\nTime: 19:10\n</code></pre><pre><code>#include &lt;vector&gt;\n#include &lt;fmt/ranges.h&gt;\n\nint main() {\n  std::vector&lt;int&gt; v = {1, 2, 3};\n  fmt::print(\"{}\\n\", v);\n}\n</code></pre><p><strong>Check a format string at compile time</strong></p><pre><code>std::string s = fmt::format(\"{:d}\", \"I am not a number\");\n</code></pre><p>This gives a compile-time error in C++20 because  is an invalid format specifier for a string.</p><p><strong>Write a file from a single thread</strong></p><pre><code>#include &lt;fmt/os.h&gt;\n\nint main() {\n  auto out = fmt::output_file(\"guide.txt\");\n  out.print(\"Don't {}\", \"Panic\");\n}\n</code></pre><p><strong>Print with colors and text styles</strong></p><pre><code>#include &lt;fmt/color.h&gt;\n\nint main() {\n  fmt::print(fg(fmt::color::crimson) | fmt::emphasis::bold,\n             \"Hello, {}!\\n\", \"world\");\n  fmt::print(fg(fmt::color::floral_white) | bg(fmt::color::slate_gray) |\n             fmt::emphasis::underline, \"Olá, {}!\\n\", \"Mundo\");\n  fmt::print(fg(fmt::color::steel_blue) | fmt::emphasis::italic,\n             \"你好{}！\\n\", \"世界\");\n}\n</code></pre><p>Output on a modern terminal with Unicode support:</p><table><tbody><tr></tr><tr></tr></tbody></table><p>{fmt} is the fastest of the benchmarked methods, ~20% faster than .</p><p>The above results were generated by building  on macOS 12.6.1 with <code>clang++ -O3 -DNDEBUG -DSPEED_TEST -DHAVE_FORMAT</code>, and taking the best of three runs. In the test, the format string <code>\"%0.10f:%04d:%+g:%s:%p:%c:%%\\n\"</code> or equivalent is filled 2,000,000 times with output sent to ; for further details refer to the <a href=\"https://github.com/fmtlib/format-benchmark/raw/master/src/tinyformat-test.cc\">source</a>.</p><h2>Compile time and code bloat</h2><p>The script <a href=\"https://github.com/fmtlib/format-benchmark/raw/master/bloat-test.py\">bloat-test.py</a> from <a href=\"https://github.com/fmtlib/format-benchmark\">format-benchmark</a> tests compile time and code bloat for nontrivial projects. It generates 100 translation units and uses  or its alternative five times in each to simulate a medium-sized project. The resulting executable size and compile time (Apple clang version 15.0.0 (clang-1500.1.0.2.5), macOS Sonoma, best of three) is shown in the following tables.</p><table><thead><tr></tr></thead><tbody></tbody></table><p>{fmt} is fast to compile and is comparable to  in terms of per-call binary size (within a rounding error on this system).</p><table><thead><tr></tr></thead><tbody></tbody></table><p>, , and  are all linked as shared libraries to compare formatting function overhead only. Boost Format is a header-only library so it doesn't provide any linkage options.</p><p>Please refer to <a href=\"https://fmt.dev/latest/get-started/#building-from-source\">Building the library</a> for instructions on how to build the library and run the unit tests.</p><p>Benchmarks reside in a separate repository, <a href=\"https://github.com/fmtlib/format-benchmark\">format-benchmarks</a>, so to run the benchmarks you first need to clone this repository and generate Makefiles with CMake:</p><pre><code>$ git clone --recursive https://github.com/fmtlib/format-benchmark.git\n$ cd format-benchmark\n$ cmake .\n</code></pre><p>Then you can run the speed test:</p><p><a href=\"https://clang.llvm.org/extra/clang-tidy/\">clang-tidy</a> v18 provides the <a href=\"https://clang.llvm.org/extra/clang-tidy/checks/modernize/use-std-print.html\">modernize-use-std-print</a> check that is capable of converting occurrences of  and  to  if configured to do so. (By default it converts to .)</p><ul><li><a href=\"https://play0ad.com/\">0 A.D.</a>: a free, open-source, cross-platform real-time strategy game</li><li><a href=\"https://github.com/ampl/mp\">AMPL/MP</a>: an open-source library for mathematical programming</li><li><a href=\"https://github.com/aseprite/aseprite\">Aseprite</a>: animated sprite editor &amp; pixel art tool</li><li><a href=\"https://www.aviobook.aero/en\">AvioBook</a>: a comprehensive aircraft operations suite</li><li><a href=\"https://celestia.space/\">Celestia</a>: real-time 3D visualization of space</li><li><a href=\"https://ceph.com/\">Ceph</a>: a scalable distributed storage system</li><li><a href=\"https://github.com/ClickHouse/ClickHouse\">ClickHouse</a>: an analytical database management system</li><li><a href=\"https://github.com/contour-terminal/contour/\">Contour</a>: a modern terminal emulator</li><li><a href=\"https://cuauv.org/\">CUAUV</a>: Cornell University's autonomous underwater vehicle</li><li><a href=\"https://drake.mit.edu/\">Drake</a>: a planning, control, and analysis toolbox for nonlinear dynamical systems (MIT)</li><li><a href=\"https://github.com/envoyproxy/envoy\">Envoy</a>: C++ L7 proxy and communication bus (Lyft)</li><li><a href=\"https://fivem.net/\">FiveM</a>: a modification framework for GTA V</li><li><a href=\"https://github.com/MengRao/fmtlog\">fmtlog</a>: a performant fmtlib-style logging library with latency in nanoseconds</li><li><a href=\"https://github.com/facebook/folly\">Folly</a>: Facebook open-source library</li><li><a href=\"https://gemrb.org/\">GemRB</a>: a portable open-source implementation of Bioware's Infinity Engine</li><li><a href=\"https://github.com/kbengine/kbengine\">KBEngine</a>: an open-source MMOG server engine</li><li><a href=\"https://kodi.tv/\">Kodi</a> (formerly xbmc): home theater software</li><li><a href=\"https://kth.cash/\">Knuth</a>: high-performance Bitcoin full-node</li><li><a href=\"https://mariadb.org/\">MariaDB</a>: relational database management system</li><li><a href=\"https://mongodb.com/\">MongoDB</a>: distributed document database</li><li><a href=\"https://openspaceproject.com/\">OpenSpace</a>: an open-source astrovisualization framework</li><li><a href=\"https://github.com/pytorch/pytorch\">PyTorch</a>: an open-source machine learning library</li><li><a href=\"https://www.quasardb.net/\">quasardb</a>: a distributed, high-performance, associative database</li><li><a href=\"https://github.com/odygrd/quill\">Quill</a>: asynchronous low-latency logging library</li><li><a href=\"https://github.com/ravijanjam/qkw\">QKW</a>: generalizing aliasing to simplify navigation, and execute complex multi-line terminal command sequences</li><li><a href=\"https://vectorized.io/redpanda\">redpanda</a>: a 10x faster Kafka® replacement for mission-critical systems written in C++</li><li><a href=\"http://rpclib.net/\">rpclib</a>: a modern C++ msgpack-RPC server and client library</li><li><a href=\"https://www.scylladb.com/\">Scylla</a>: a Cassandra-compatible NoSQL data store that can handle 1 million transactions per second on a single server</li><li><a href=\"http://www.seastar-project.org/\">Seastar</a>: an advanced, open-source C++ framework for high-performance server applications on modern hardware</li><li><a href=\"https://github.com/gabime/spdlog\">spdlog</a>: super fast C++ logging library</li><li><a href=\"https://userver.tech/\">🐙 userver framework</a>: open-source asynchronous framework with a rich set of abstractions and database drivers</li></ul><p>If you are aware of other projects using this library, please let me know by <a href=\"mailto:victor.zverovich@gmail.com\">email</a> or by submitting an <a href=\"https://github.com/fmtlib/fmt/issues\">issue</a>.</p><p>So why yet another formatting library?</p><p>There are plenty of methods for doing this task, from standard ones like the printf family of function and iostreams to Boost Format and FastFormat libraries. The reason for creating a new library is that every existing solution that I found either had serious issues or didn't provide all the features I needed.</p><p>The good thing about  is that it is pretty fast and readily available being a part of the C standard library. The main drawback is that it doesn't support user-defined types.  also has safety issues although they are somewhat mitigated with <a href=\"https://gcc.gnu.org/onlinedocs/gcc/Function-Attributes.html\">__attribute__ ((format (printf, ...))</a> in GCC. There is a POSIX extension that adds positional arguments required for <a href=\"https://en.wikipedia.org/wiki/Internationalization_and_localization\">i18n</a> to  but it is not a part of C99 and may not be available on some platforms.</p><p>The main issue with iostreams is best illustrated with an example:</p><pre><code>std::cout &lt;&lt; std::setprecision(2) &lt;&lt; std::fixed &lt;&lt; 1.23456 &lt;&lt; \"\\n\";\n</code></pre><p>which is a lot of typing compared to printf:</p><pre><code>printf(\"%.2f\\n\", 1.23456);\n</code></pre><p>Matthew Wilson, the author of FastFormat, called this \"chevron hell\". iostreams don't support positional arguments by design.</p><p>The good part is that iostreams support user-defined types and are safe although error handling is awkward.</p><p>This is a very powerful library that supports both -like format strings and positional arguments. Its main drawback is performance. According to various benchmarks, it is much slower than other methods considered here. Boost Format also has excessive build times and severe code bloat issues (see <a href=\"https://raw.githubusercontent.com/fmtlib/fmt/master/#benchmarks\">Benchmarks</a>).</p><p>This is an interesting library that is fast, safe and has positional arguments. However, it has significant limitations, citing its author:</p><blockquote><p>Three features that have no hope of being accommodated within the current design are:</p><ul><li>Leading zeros (or any other non-space padding)</li><li>Octal/hexadecimal encoding</li><li>Runtime width/alignment specification</li></ul></blockquote><p>It is also quite big and has a heavy dependency, on STLSoft, which might be too restrictive for use in some projects.</p><p>This is not a formatting library but I decided to include it here for completeness. As iostreams, it suffers from the problem of mixing verbatim text with arguments. The library is pretty fast, but slower on integer formatting than  with format string compilation on Karma's own benchmark, see <a href=\"http://www.zverovich.net/2020/06/13/fast-int-to-string-revisited.html\">Converting a hundred million integers to strings per second</a>.</p><p>{fmt} is distributed under the MIT <a href=\"https://github.com/fmtlib/fmt/raw/master/LICENSE\">license</a>.</p><p>The {fmt} library is maintained by Victor Zverovich (<a href=\"https://github.com/vitaut\">vitaut</a>) with contributions from many other people. See <a href=\"https://github.com/fmtlib/fmt/graphs/contributors\">Contributors</a> and <a href=\"https://github.com/fmtlib/fmt/releases\">Releases</a> for some of the names. Let us know if your contribution is not listed or mentioned incorrectly and we'll make it right.</p><p>This project is maintained by a team of volunteers on a reasonable-effort basis. As such, please give us at least  days to work on a fix before public exposure.</p>","contentLength":9000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"hiyouga/LLaMA-Factory","url":"https://github.com/hiyouga/LLaMA-Factory","date":1761964521,"author":"","guid":323847,"unread":true,"content":"<p>Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)</p><p><strong>Fine-tuning a large language model can be easy as...</strong></p><blockquote><p>[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.</p></blockquote><ul><li>: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.</li><li>: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.</li><li>: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.</li><li>: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.</li><li>: LlamaBoard, TensorBoard, Wandb, MLflow, <a href=\"https://github.com/SwanHubX/SwanLab\">SwanLab</a>, etc.</li></ul><h3>Day-N Support for Fine-Tuning Cutting-Edge Models</h3><table><tbody><tr><td>Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6</td></tr><tr><td>Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4</td></tr></tbody></table><p>[25/08/06] We supported fine-tuning the  models. See <a href=\"https://github.com/hiyouga/LLaMA-Factory/pull/8826\">PR #8826</a> to get started.</p><blockquote><p>[!TIP] If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.</p></blockquote><blockquote><p>[!NOTE] For the \"base\" models, the  argument can be chosen from , ,  etc. But make sure to use the  for the \"instruct/chat\" models.</p><p>Remember to use the  template in training and inference.</p><p>*: You should install the  from main branch and use  to skip version check.</p><p>**: You need to install a specific version of  to use the corresponding model.</p></blockquote><p>Please refer to <a href=\"https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py\">constants.py</a> for a full list of models we supported.</p><h2>Supported Training Approaches</h2><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><blockquote><p>[!TIP] The implementation details of PPO can be found in <a href=\"https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html\">this blog</a>.</p></blockquote><p>Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.</p><pre><code>pip install --upgrade huggingface_hub\nhuggingface-cli login\n</code></pre><table><thead><tr></tr></thead><tbody></tbody></table><table><tbody></tbody></table><table><tbody><tr></tr><tr></tr><tr><td>Freeze/LoRA/GaLore/APOLLO/BAdam/OFT</td></tr><tr></tr><tr></tr><tr></tr></tbody></table><blockquote><p>[!IMPORTANT] Installation is mandatory.</p></blockquote><pre><code>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n</code></pre><p>Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev</p><h4>Install from Docker Image</h4><pre><code>docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n</code></pre><p>This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.</p><p>Please refer to <a href=\"https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md\">data/README.md</a> for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.</p><blockquote><p>[!NOTE] Please update  to use your custom dataset.</p></blockquote><p>Use the following 3 commands to run LoRA ,  and  of the Llama3-8B-Instruct model, respectively.</p><pre><code>llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n</code></pre><blockquote><p>[!TIP] Use  to show help information.</p><p>Read <a href=\"https://github.com/hiyouga/LLaMA-Factory/issues/4614\">FAQs</a> first if you encounter any problems.</p></blockquote><h3>Fine-Tuning with LLaMA Board GUI (powered by <a href=\"https://github.com/gradio-app/gradio\">Gradio</a>)</h3><pre><code>cd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n</code></pre><pre><code>cd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n</code></pre><pre><code>cd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n</code></pre><h3>Deploy with OpenAI-style API and vLLM</h3><pre><code>API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n</code></pre><h3>Download from ModelScope Hub</h3><p>If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.</p><pre><code>export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n</code></pre><p>Train the model by specifying a model ID of the ModelScope Hub as the . You can find a full list of model IDs at <a href=\"https://modelscope.cn/models\">ModelScope Hub</a>, e.g., <code>LLM-Research/Meta-Llama-3-8B-Instruct</code>.</p><h3>Download from Modelers Hub</h3><p>You can also use Modelers Hub to download models and datasets.</p><pre><code>export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n</code></pre><p>Train the model by specifying a model ID of the Modelers Hub as the . You can find a full list of model IDs at <a href=\"https://modelers.cn/models\">Modelers Hub</a>, e.g., .</p><p>To use <a href=\"https://wandb.ai\">Weights &amp; Biases</a> for logging experimental results, you need to add the following arguments to yaml files.</p><pre><code>report_to: wandb\nrun_name: test_run # optional\n</code></pre><p>Set  to <a href=\"https://wandb.ai/authorize\">your key</a> when launching training tasks to log in with your W&amp;B account.</p><p>To use <a href=\"https://github.com/SwanHubX/SwanLab\">SwanLab</a> for logging experimental results, you need to add the following arguments to yaml files.</p><pre><code>use_swanlab: true\nswanlab_run_name: test_run # optional\n</code></pre><p>When launching training tasks, you can log in to SwanLab in three ways:</p><ol><li>Add <code>swanlab_api_key=&lt;your_api_key&gt;</code> to the yaml file, and set it to your <a href=\"https://swanlab.cn/settings\">API key</a>.</li><li>Set the environment variable  to your <a href=\"https://swanlab.cn/settings\">API key</a>.</li><li>Use the  command to complete the login.</li></ol><h2>Projects using LLaMA Factory</h2><p>If you have a project that should be incorporated, please contact via email or create a pull request.</p><p>If this work is helpful, please kindly cite as:</p><pre><code>@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n</code></pre>","contentLength":5456,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wei-Shaw/claude-relay-service","url":"https://github.com/Wei-Shaw/claude-relay-service","date":1761964521,"author":"","guid":323848,"unread":true,"content":"<p>CRS-自建Claude Code镜像，一站式开源中转服务，让 Claude、OpenAI、Gemini、Droid 订阅统一接入，支持拼车共享，更高效分摊成本，原生工具无缝使用。</p><div align=\"center\"><table><tbody><tr><td align=\"left\">项目直营，提供稳定的 Claude Code / Codex CLI 拼车服务</td></tr><tr><td align=\"left\">社区认证，提供 Claude Code / Codex CLI 拼车</td></tr></tbody></table></div><p>🚨 : 使用本项目可能违反Anthropic的服务条款。请在使用前仔细阅读Anthropic的用户协议，使用本项目的一切风险由用户自行承担。</p><p>📖 : 本项目仅供技术学习和研究使用，作者不对因使用本项目导致的账户封禁、服务中断或其他损失承担任何责任。</p><ul><li>🌍 : 所在地区无法直接访问Claude Code服务？</li><li>🔒 : 担心第三方镜像服务会记录或泄露你的对话内容？</li><li>👥 : 想和朋友一起分摊Claude Code Max订阅费用？</li><li>⚡ : 第三方镜像站经常故障不稳定，影响效率 ？</li></ul><p>✅ : 三五好友一起分摊Claude Code Max订阅 ✅ : 不想让第三方镜像看到你的对话内容 ✅ : 有基本的技术基础，愿意自己搭建和维护 ✅ : 需要长期稳定的Claude访问，不想受制于镜像站 ✅ : 无法直接访问Claude官方服务</p><ul><li>🕵️ : 你的对话内容都被人家看得一清二楚，商业机密什么的就别想了</li></ul><ul><li>🔐 : 所有接口请求都只经过你自己的服务器，直连Anthropic API</li><li>⚡ : 就你们几个人用，Max 200刀套餐基本上可以爽用Opus</li><li>💰 : 用了多少token一目了然，按官方价格换算了具体费用</li></ul><ul><li>✅ : 可以添加多个Claude账户自动轮换</li><li>✅ : 给每个人分配独立的Key</li></ul><ul></ul><ul><li>: 能访问到Anthropic API（建议使用US地区的机器）</li><li>: 2核4G的基本够了，网络尽量选回国线路快一点的（为了提高速度，建议不要开代理或者设置服务器的IP直连）</li><li>: 阿里云、腾讯云的海外主机经测试会被Cloudflare拦截，无法直接访问claude api</li></ul><ul></ul><ul></ul><p>推荐使用管理脚本进行一键部署，简单快捷，自动处理所有依赖和配置。</p><pre><code>curl -fsSL https://pincc.ai/manage.sh -o manage.sh &amp;&amp; chmod +x manage.sh &amp;&amp; ./manage.sh install\n</code></pre><ul><li>✅ : 自动检测系统环境，安装 Node.js 18+、Redis 等依赖</li><li>✅ : 友好的配置向导，设置端口、Redis 连接等</li><li>✅ : 安装完成后自动启动服务并显示访问地址</li></ul><pre><code>crs install   # 安装服务\ncrs start     # 启动服务\ncrs stop      # 停止服务\ncrs restart   # 重启服务\ncrs status    # 查看状态\ncrs update    # 更新服务\ncrs uninstall # 卸载服务\n</code></pre><pre><code>$ crs install\n\n# 会依次询问：\n安装目录 (默认: ~/claude-relay-service):\n服务端口 (默认: 3000): 8080\nRedis 地址 (默认: localhost):\nRedis 端口 (默认: 6379):\nRedis 密码 (默认: 无密码):\n\n# 安装完成后自动启动并显示：\n服务已成功安装并启动！\n\n访问地址：\n  本地 Web: http://localhost:8080/web\n  公网 Web: http://YOUR_IP:8080/web\n\n管理员账号信息已保存到: data/init.json\n</code></pre><ul><li>支持系统: Ubuntu/Debian、CentOS/RedHat、Arch Linux、macOS</li></ul><pre><code># 安装Node.js\ncurl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n# 安装Redis\nsudo apt update\nsudo apt install redis-server\nsudo systemctl start redis-server\n</code></pre><pre><code># 安装Node.js\ncurl -fsSL https://rpm.nodesource.com/setup_18.x | sudo bash -\nsudo yum install -y nodejs\n\n# 安装Redis\nsudo yum install redis\nsudo systemctl start redis\n</code></pre><pre><code># 下载项目\ngit clone https://github.com/Wei-Shaw//claude-relay-service.git\ncd claude-relay-service\n\n# 安装依赖\nnpm install\n\n# 复制配置文件（重要！）\ncp config/config.example.js config/config.js\ncp .env.example .env\n</code></pre><pre><code># 这两个密钥随便生成，但要记住\nJWT_SECRET=你的超级秘密密钥\nENCRYPTION_KEY=32位的加密密钥随便写\n\n# Redis配置\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\n\n</code></pre><pre><code>module.exports = {\n  server: {\n    port: 3000, // 服务端口，可以改\n    host: '0.0.0.0' // 不用改\n  },\n  redis: {\n    host: '127.0.0.1', // Redis地址\n    port: 6379 // Redis端口\n  }\n  // 其他配置保持默认就行\n}\n</code></pre><pre><code># 安装前端依赖\nnpm run install:web\n\n# 构建前端（生成 dist 目录）\nnpm run build:web\n</code></pre><pre><code># 初始化\nnpm run setup # 会随机生成后台账号密码信息，存储在 data/init.json\n# 或者通过环境变量预设管理员凭据：\n# export ADMIN_USERNAME=cr_admin_custom\n# export ADMIN_PASSWORD=your-secure-password\n\n# 启动服务\nnpm run service:start:daemon   # 后台运行\n\n# 查看状态\nnpm run service:status\n</code></pre><h4>第一步：下载构建docker-compose.yml文件的脚本并执行</h4><pre><code>curl -fsSL https://pincc.ai/crs-compose.sh -o crs-compose.sh &amp;&amp; chmod +x crs-compose.sh &amp;&amp; ./crs-compose.sh\n</code></pre><ul></ul><ul><li>: JWT密钥，至少32个字符</li><li>: 加密密钥，必须是32个字符</li></ul><ul><li>: 管理员用户名（不设置则自动生成）</li><li>: 管理员密码（不设置则自动生成）</li></ul><ol><li><pre><code>docker logs claude-relay-service\n</code></pre></li><li><pre><code># 在 .env 文件中设置\nADMIN_USERNAME=cr_admin_custom\nADMIN_PASSWORD=your-secure-password\n</code></pre></li></ol><p>浏览器访问：</p><ul><li>环境变量预设：通过 ADMIN_USERNAME 和 ADMIN_PASSWORD 设置</li><li>Docker 部署：查看容器日志 <code>docker logs claude-relay-service</code></li></ul><ol><li>如果你担心多个账号共用1个IP怕被封禁，可以选择设置静态代理IP（可选）</li></ol><ol><li>设置使用限制（可选）： \n  <ul><li>: 限制每个时间窗口的请求次数和Token使用量</li><li>: 限制只允许特定客户端使用（如ClaudeCode、Gemini-CLI等）</li></ul></li></ol><h3>4. 开始使用 Claude Code 和 Gemini CLI</h3><pre><code>export ANTHROPIC_BASE_URL=\"http://127.0.0.1:3000/api/\" # 根据实际填写你服务器的ip地址或者域名\nexport ANTHROPIC_AUTH_TOKEN=\"后台创建的API密钥\"\n</code></pre><p>如果使用 VSCode 的 Claude 插件，需要在  文件中配置：</p><pre><code>{\n    \"primaryApiKey\": \"crs\"\n}\n</code></pre><p>如果该文件不存在，请手动创建。Windows 用户路径为 <code>C:\\Users\\你的用户名\\.claude\\config.json</code>。</p><pre><code>GEMINI_MODEL=\"gemini-2.5-pro\"\nGOOGLE_GEMINI_BASE_URL=\"http://127.0.0.1:3000/gemini\" # 根据实际填写你服务器的ip地址或者域名\nGEMINI_API_KEY=\"后台创建的API密钥\"  # 使用相同的API密钥即可\n</code></pre><pre><code>gemini  # 或其他 Gemini CLI 命令\n</code></pre><p>在  文件添加以下配置：</p><pre><code>model_provider = \"crs\"\nmodel = \"gpt-5-codex\"\nmodel_reasoning_effort = \"high\"\ndisable_response_storage = true\npreferred_auth_method = \"apikey\"\n\n[model_providers.crs]\nname = \"crs\"\nbase_url = \"http://127.0.0.1:3000/openai\"  # 根据实际填写你服务器的ip地址或者域名\nwire_api = \"responses\"\nrequires_openai_auth = true\nenv_key = \"CRS_OAI_KEY\"\n</code></pre><p>在  文件中配置API密钥为 null：</p><pre><code>{\n    \"OPENAI_API_KEY\": null  \n}\n</code></pre><pre><code>export CRS_OAI_KEY=\"后台创建的API密钥\"\n</code></pre><blockquote><p>⚠️ 在通过 Nginx 反向代理 CRS 服务并使用 Codex CLI 时，需要在 http 块中添加 underscores_in_headers on;。因为 Nginx 默认会移除带下划线的请求头（如 session_id），一旦该头被丢弃，多账号环境下的粘性会话功能将失效。</p></blockquote><p>Droid CLI 读取 。可以在该文件中添加自定义模型以指向本服务的新端点：</p><pre><code>{\n  \"custom_models\": [\n    {\n      \"model_display_name\": \"Sonnet 4.5 [crs]\",\n      \"model\": \"claude-sonnet-4-5-20250929\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/claude\",\n      \"api_key\": \"后台创建的API密钥\",\n      \"provider\": \"anthropic\",\n      \"max_tokens\": 8192\n    },\n    {\n      \"model_display_name\": \"GPT5-Codex [crs]\",\n      \"model\": \"gpt-5-codex\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/openai\",\n      \"api_key\": \"后台创建的API密钥\",\n      \"provider\": \"openai\",\n      \"max_tokens\": 16384\n    }\n  ]\n}\n</code></pre><blockquote><p>💡 将示例中的  替换为你的服务域名或公网地址，并写入后台生成的 API 密钥（cr_ 开头）。</p></blockquote><p>本服务支持多种API端点格式，方便接入不同的第三方工具（如Cherry Studio等）。</p><p>Cherry Studio支持多种AI服务的接入，下面是不同账号类型的详细配置：</p><pre><code># API地址\nhttp://你的服务器:3000/claude\n\n# 模型ID示例\nclaude-sonnet-4-5-20250929 # Claude Sonnet 4.5\nclaude-opus-4-20250514     # Claude Opus 4\n</code></pre><ul><li>API地址填入：</li><li>API Key填入：后台创建的API密钥（cr_开头）</li></ul><pre><code># API地址\nhttp://你的服务器:3000/gemini\n\n# 模型ID示例\ngemini-2.5-pro             # Gemini 2.5 Pro\n</code></pre><ul><li>API地址填入：</li><li>API Key填入：后台创建的API密钥（cr_开头）</li></ul><pre><code># API地址\nhttp://你的服务器:3000/openai\n\n# 模型ID（固定）\ngpt-5                      # Codex使用固定模型ID\n</code></pre><ul><li>API地址填入：</li><li>API Key填入：后台创建的API密钥（cr_开头）</li><li>：Codex只支持Openai-Response标准</li></ul><ul><li>✅ ：（不加结尾 ，让 Cherry Studio 自动加上 v1）</li><li>✅ ：<code>http://你的服务器:3000/claude/v1/</code>（手动指定 v1 并加结尾 ）</li><li>💡 ：这两种格式在 Cherry Studio 中是完全等效的</li><li>❌ ：<code>http://你的服务器:3000/claude/</code>（单独的  结尾会被 Cherry Studio 忽略 v1 版本）</li></ul><ul><li>所有账号类型都使用相同的API密钥（在后台统一创建）</li><li> - 使用Droid类型Claude账号池（只建议api调用或Droid Cli中使用）</li><li> - 使用Codex账号（只支持Openai-Response格式）</li><li> - 使用Droid类型OpenAI兼容账号池（只建议api调用或Droid Cli中使用）</li><li>支持所有标准API端点（messages、models等）</li></ul><ul><li>确保在后台已添加对应类型的账号（Claude/Gemini/Codex）</li><li>API密钥可以通用，系统会根据路由自动选择账号类型</li></ul><pre><code># 查看服务状态\nnpm run service:status\n\n# 查看日志\nnpm run service:logs\n\n# 重启服务\nnpm run service:restart:daemon\n\n# 停止服务\nnpm run service:stop\n</code></pre><ul><li>:  - 查看使用统计</li><li>:  - 确认服务正常</li></ul><pre><code># 1. 进入项目目录\ncd claude-relay-service\n\n# 2. 拉取最新代码\ngit pull origin main\n\n# 如果遇到 package-lock.json 冲突，使用远程版本\ngit checkout --theirs package-lock.json\ngit add package-lock.json\n\n# 3. 安装新的依赖（如果有）\nnpm install\n\n# 4. 安装并构建前端\nnpm run install:web\nnpm run build:web\n\n# 5. 重启服务\nnpm run service:restart:daemon\n\n# 6. 检查服务状态\nnpm run service:status\n</code></pre><ul><li>升级前建议备份重要配置文件（.env, config/config.js）</li></ul><p>客户端限制功能允许你控制每个API Key可以被哪些客户端使用，通过User-Agent识别客户端，提高API的安全性。</p><ol><li><ul></ul></li><li><ul><li>: 官方Claude CLI（匹配 <code>claude-cli/x.x.x (external, cli)</code> 格式）</li><li>: Gemini命令行工具（匹配 <code>GeminiCLI/vx.x.x (platform; arch)</code> 格式）</li></ul></li><li><ul><li>通过日志可以查看实际的User-Agent格式，方便配置自定义客户端</li></ul></li></ol><pre><code>🔓 Authenticated request from key: 测试Key (key-id) in 5ms\n   User-Agent: \"claude-cli/1.0.58 (external, cli)\"\n</code></pre><pre><code>🔍 Checking client restriction for key: key-id (测试Key)\n   User-Agent: \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n   Allowed clients: claude_code, gemini_cli\n🚫 Client restriction failed for key: key-id (测试Key) from 127.0.0.1, User-Agent: Mozilla/5.0...\n</code></pre><pre><code># 检查Redis是否启动\nredis-cli ping\n\n# 应该返回 PONG\n</code></pre><ul></ul><ul></ul><p>在生产环境中，建议通过反向代理进行连接，以便使用自动 HTTPS、安全头部和性能优化。下面提供两种常用方案：  和 <strong>Nginx Proxy Manager (NPM)</strong>。</p><p>Caddy 是一款自动管理 HTTPS 证书的 Web 服务器，配置简单、性能优秀，很适合不需要 Docker 环境的部署方案。</p><pre><code># Ubuntu/Debian\nsudo apt install -y debian-keyring debian-archive-keyring apt-transport-https\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | sudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update\nsudo apt install caddy\n\n# CentOS/RHEL/Fedora\nsudo yum install yum-plugin-copr\nsudo yum copr enable @caddy/caddy\nsudo yum install caddy\n</code></pre><pre><code>your-domain.com {\n    # 反向代理到本地服务\n    reverse_proxy 127.0.0.1:3000 {\n        # 支持流式响应或 SSE\n        flush_interval -1\n\n        # 传递真实 IP\n        header_up X-Real-IP {remote_host}\n        header_up X-Forwarded-For {remote_host}\n        header_up X-Forwarded-Proto {scheme}\n\n        # 长读/写超时配置\n        transport http {\n            read_timeout 300s\n            write_timeout 300s\n            dial_timeout 30s\n        }\n    }\n\n    # 安全头部\n    header {\n        Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\n        X-Frame-Options \"DENY\"\n        X-Content-Type-Options \"nosniff\"\n        -Server\n    }\n}\n</code></pre><pre><code>sudo caddy validate --config /etc/caddy/Caddyfile\nsudo systemctl start caddy\nsudo systemctl enable caddy\nsudo systemctl status caddy\n</code></pre><p>Caddy 会自动管理 HTTPS，因此可以将服务限制在本地进行监听：</p><pre><code>// config/config.js\nmodule.exports = {\n  server: {\n    port: 3000,\n    host: '127.0.0.1' // 只监听本地\n  }\n}\n</code></pre><ul></ul><h2>Nginx Proxy Manager (NPM) 方案</h2><p>Nginx Proxy Manager 通过图形化界面管理反向代理和 HTTPS 证书，並以 Docker 容器部署。</p><table><tbody><tr></tr><tr><td>192.168.0.1 (docker 机器 IP)</td></tr><tr></tr></tbody></table><blockquote><ul><li>请确保 Claude Relay Service <strong>监听 host 为  、容器 IP 或本机 IP</strong>，以便 NPM 实现内网连接。</li><li><strong>Websockets Support 和 Cache Assets 必须关闭</strong>，否则会导致 SSE / 流式响应失败。</li></ul></blockquote><ul><li>: Request a new SSL Certificate (Let's Encrypt) 或已有证书</li></ul><p>Custom Nginx Configuration 中添加以下内容：</p><pre><code># 传递真实用户 IP\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n\n# 支持 WebSocket / SSE 等流式通信\nproxy_http_version 1.1;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection \"upgrade\";\nproxy_buffering off;\n\n# 长连接 / 超时设置（适合 AI 聊天流式传输）\nproxy_read_timeout 300s;\nproxy_send_timeout 300s;\nproxy_connect_timeout 30s;\n\n# ---- 安全性设置 ----\n# 严格 HTTPS 策略 (HSTS)\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n\n# 阻挡点击劫持与内容嗅探\nadd_header X-Frame-Options \"DENY\" always;\nadd_header X-Content-Type-Options \"nosniff\" always;\n\n# Referrer / Permissions 限制策略\nadd_header Referrer-Policy \"no-referrer-when-downgrade\" always;\nadd_header Permissions-Policy \"camera=(), microphone=(), geolocation=()\" always;\n\n# 隐藏服务器信息（等效于 Caddy 的 `-Server`）\nproxy_hide_header Server;\n\n# ---- 性能微调 ----\n# 关闭代理端缓存，确保即时响应（SSE / Streaming）\nproxy_cache_bypass $http_upgrade;\nproxy_no_cache $http_upgrade;\nproxy_request_buffering off;\n</code></pre><ul><li>保存后等待 NPM 自动申请 Let's Encrypt 证书（如果有）。</li><li>Dashboard 中查看 Proxy Host 状态，确保显示为 \"Online\"。</li><li>访问 <code>https://relay.example.com</code>，如果显示绿色锁图标即表示 HTTPS 正常。</li></ul><ul></ul><ul><li>: 可以给不同的人分配不同的apikey，可以根据不同的apikey来分析用量</li></ul><ul><li>: 强烈建议使用Caddy反向代理（自动HTTPS），确保数据传输安全</li><li>: 只开放必要的端口（80, 443），隐藏直接服务端口</li></ul><ol></ol><ul></ul><div align=\"center\"><p><strong>⭐ 觉得有用的话给个Star呗，这是对作者最大的鼓励！</strong></p></div>","contentLength":14751,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"666ghj/BettaFish","url":"https://github.com/666ghj/BettaFish","date":1761964521,"author":"","guid":323849,"unread":true,"content":"<p>微舆：人人可用的多Agent舆情分析助手，打破信息茧房，还原舆情原貌，预测未来走向，辅助决策！从0实现，不依赖任何框架。</p><p>“” 是一个从0实现的创新型 多智能体 舆情分析系统，帮助大家破除信息茧房，还原舆情原貌，预测未来走向，辅助决策。用户只需像聊天一样提出分析需求，智能体开始全自动分析 国内外30+主流社媒 与 数百万条大众评论。</p><blockquote><p>“微舆”谐音“微鱼”，BettaFish是一种体型很小但非常好斗、漂亮的鱼，它象征着“小而强大，不畏挑战”</p></blockquote><p>不仅仅体现在报告质量上，相比同类产品，我们拥有🚀六大优势：</p><ol><li><p>：AI爬虫集群7x24小时不间断作业，全面覆盖微博、小红书、抖音、快手等10+国内外关键社媒。不仅实时捕获热点内容，更能下钻至海量用户评论，让您听到最真实、最广泛的大众声音。</p></li><li><p>：我们不仅依赖设计的5类专业Agent，更融合了微调模型、统计模型等中间件。通过多模型协同工作，确保了分析结果的深度、准度与多维视角。</p></li><li><p>：突破图文限制，能深度解析抖音、快手等短视频内容，并精准提取现代搜索引擎中的天气、日历、股票等结构化多模态信息卡片，让您全面掌握舆情动态。</p></li><li><p>：为不同Agent赋予独特的工具集与思维模式，引入辩论主持人模型，通过“论坛”机制进行链式思维碰撞与辩论。这不仅避免了单一模型的思维局限与交流导致的同质化，更催生出更高质量的集体智能与决策支持。</p></li><li><p>：平台不仅分析公开舆情，还提供高安全性的接口，支持您将内部业务数据库与舆情数据无缝集成。打通数据壁垒，为垂直业务提供“外部趋势+内部洞察”的强大分析能力。</p></li><li><p>：基于纯Python模块化设计，实现轻量化、一键式部署。代码结构清晰，开发者可轻松集成自定义模型与业务逻辑，实现平台的快速扩展与深度定制。</p></li></ol><p>。“微舆”的目标，是成为驱动一切业务场景的简洁通用的数据分析引擎。</p><div align=\"center\"><img src=\"https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/system_schematic.png\" alt=\"banner\" width=\"800\"><p>告别传统的数据看板，在“微舆”，一切由一个简单的问题开始，您只需像对话一样，提出您的分析需求</p></div><p> 私有数据库挖掘：私有舆情数据库深度分析AI代理</p><p> 多模态内容分析：具备强大多模态能力的AI代理</p><p> 精准信息搜索：具备国内外网页搜索能力的AI代理</p><p> 智能报告生成：内置模板的多轮报告生成AI代理</p><table><tbody><tr></tr><tr><td>Query Agent、Media Agent、Insight Agent</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>ForumEngine监控Agent发言并生成主持人总结</td></tr><tr></tr><tr><td>Report Agent收集所有分析结果和论坛内容</td></tr><tr></tr></tbody></table><pre><code>Weibo_PublicOpinion_AnalysisSystem/\n├── QueryEngine/                   # 国内外新闻广度搜索Agent\n│   ├── agent.py                   # Agent主逻辑\n│   ├── llms/                      # LLM接口封装\n│   ├── nodes/                     # 处理节点\n│   ├── tools/                     # 搜索工具\n│   ├── utils/                     # 工具函数\n│   └── ...                        # 其他模块\n├── MediaEngine/                   # 强大的多模态理解Agent\n│   ├── agent.py                   # Agent主逻辑\n│   ├── nodes/                     # 处理节点\n│   ├── llms/                      # LLM接口\n│   ├── tools/                     # 搜索工具\n│   ├── utils/                     # 工具函数\n│   └── ...                        # 其他模块\n├── InsightEngine/                 # 私有数据库挖掘Agent\n│   ├── agent.py                   # Agent主逻辑\n│   ├── llms/                      # LLM接口封装\n│   │   └── base.py                # 统一的 OpenAI 兼容客户端\n│   ├── nodes/                     # 处理节点\n│   │   ├── base_node.py           # 基础节点类\n│   │   ├── formatting_node.py     # 格式化节点\n│   │   ├── report_structure_node.py # 报告结构节点\n│   │   ├── search_node.py         # 搜索节点\n│   │   └── summary_node.py        # 总结节点\n│   ├── tools/                     # 数据库查询和分析工具\n│   │   ├── keyword_optimizer.py   # Qwen关键词优化中间件\n│   │   ├── search.py              # 数据库操作工具集\n│   │   └── sentiment_analyzer.py  # 情感分析集成工具\n│   ├── state/                     # 状态管理\n│   │   ├── __init__.py\n│   │   └── state.py               # Agent状态定义\n│   ├── prompts/                   # 提示词模板\n│   │   ├── __init__.py\n│   │   └── prompts.py             # 各类提示词\n│   └── utils/                     # 工具函数\n│       ├── __init__.py\n│       ├── config.py              # 配置管理\n│       └── text_processing.py     # 文本处理工具\n├── ReportEngine/                  # 多轮报告生成Agent\n│   ├── agent.py                   # Agent主逻辑\n│   ├── llms/                      # LLM接口\n│   ├── nodes/                     # 报告生成节点\n│   │   ├── template_selection.py  # 模板选择节点\n│   │   └── html_generation.py     # HTML生成节点\n│   ├── report_template/           # 报告模板库\n│   │   ├── 社会公共热点事件分析.md\n│   │   ├── 商业品牌舆情监测.md\n│   │   └── ...                    # 更多模板\n│   └── flask_interface.py         # Flask API接口\n├── ForumEngine/                   # 论坛引擎简易实现\n│   ├── monitor.py                 # 日志监控和论坛管理\n│   └── llm_host.py                # 论坛主持人LLM模块\n├── MindSpider/                    # 微博爬虫系统\n│   ├── main.py                    # 爬虫主程序\n│   ├── config.py                  # 爬虫配置文件\n│   ├── BroadTopicExtraction/      # 话题提取模块\n│   │   ├── database_manager.py    # 数据库管理器\n│   │   ├── get_today_news.py      # 今日新闻获取\n│   │   ├── main.py                # 话题提取主程序\n│   │   └── topic_extractor.py     # 话题提取器\n│   ├── DeepSentimentCrawling/     # 深度舆情爬取\n│   │   ├── keyword_manager.py     # 关键词管理器\n│   │   ├── main.py                # 深度爬取主程序\n│   │   ├── MediaCrawler/          # 媒体爬虫核心\n│   │   └── platform_crawler.py    # 平台爬虫管理\n│   └── schema/                    # 数据库结构\n│       ├── db_manager.py          # 数据库管理器\n│       ├── init_database.py       # 数据库初始化\n│       └── mindspider_tables.sql  # 数据库表结构\n├── SentimentAnalysisModel/        # 情感分析模型集合\n│   ├── WeiboSentiment_Finetuned/  # 微调BERT/GPT-2模型\n│   ├── WeiboMultilingualSentiment/# 多语言情感分析（推荐）\n│   ├── WeiboSentiment_SmallQwen/  # 小参数Qwen3微调\n│   └── WeiboSentiment_MachineLearning/ # 传统机器学习方法\n├── SingleEngineApp/               # 单独Agent的Streamlit应用\n│   ├── query_engine_streamlit_app.py\n│   ├── media_engine_streamlit_app.py\n│   └── insight_engine_streamlit_app.py\n├── templates/                     # Flask模板\n│   └── index.html                 # 主界面前端\n├── static/                        # 静态资源\n├── logs/                          # 运行日志目录\n├── final_reports/                 # 最终生成的HTML报告文件\n├── utils/                         # 通用工具函数\n│   ├── forum_reader.py            # Agent间论坛通信\n│   └── retry_helper.py            # 网络请求重试机制工具\n├── app.py                         # Flask主应用入口\n├── config.py                      # 全局配置文件\n└── requirements.txt               # Python依赖包清单\n</code></pre><ul><li>: Windows、Linux、MacOS</li><li>: Anaconda或Miniconda</li></ul><pre><code># 创建conda环境\nconda create -n your_conda_name python=3.11\nconda activate your_conda_name\n</code></pre><pre><code># 基础依赖安装\npip install -r requirements.txt\n# 如果不想使用本地情感分析模型（算力需求很小，默认安装cpu版本），可以将该文件中的“机器学习”部分注释掉再执行指令\n</code></pre><pre><code># 安装浏览器驱动（用于爬虫功能）\nplaywright install chromium\n</code></pre><p>编辑  文件，填入您的API密钥（您也可以选择自己的模型、搜索代理，详情见config文件内）：</p><pre><code># MySQL数据库配置\nDB_HOST = \"localhost\"\nDB_PORT = 3306\nDB_USER = \"your_username\"\nDB_PASSWORD = \"your_password\"\nDB_NAME = \"your_db_name\"\nDB_CHARSET = \"utf8mb4\"\n\n# LLM配置\n# 您可以更改每个部分LLM使用的API，只要兼容OpenAI请求格式都可以\n\n# Insight Agent\nINSIGHT_ENGINE_API_KEY = \"your_api_key\"\nINSIGHT_ENGINE_BASE_URL = \"https://api.moonshot.cn/v1\"\nINSIGHT_ENGINE_MODEL_NAME = \"kimi-k2-0711-preview\"\n# Media Agent\n...\n</code></pre><blockquote><p>MindSpider爬虫系统跟舆情系统是各自独立的，所以需要再去配置一下</p></blockquote><pre><code># 本地MySQL数据库初始化\ncd MindSpider\npython schema/init_database.py\n</code></pre><p>我们提供便捷的云数据库服务，包含日均10万+真实舆情数据，目前！</p><ul></ul><blockquote><p>为进行数据合规性审查与服务升级，云数据库自2025年10月1日起暂停接收新的使用申请</p></blockquote><pre><code># 在项目根目录下，激活conda环境\nconda activate your_conda_name\n\n# 启动主应用即可\npython app.py\n</code></pre><blockquote><p>注1：一次运行终止后，streamlit app可能结束异常仍然占用端口，此时搜索占用端口的进程kill掉即可</p></blockquote><blockquote><p>注3：如果服务器远程部署出现页面显示问题，见<a href=\"https://github.com/666ghj/BettaFish/pull/45\">PR#45</a></p></blockquote><pre><code># 启动QueryEngine\nstreamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503\n\n# 启动MediaEngine  \nstreamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502\n\n# 启动InsightEngine\nstreamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501\n</code></pre><div align=\"center\"><img src=\"https://github.com/666ghj/MindSpider/img/example.png\" alt=\"banner\" width=\"600\"></div><pre><code># 进入爬虫目录\ncd MindSpider\n\n# 项目初始化\npython main.py --setup\n\n# 运行完整爬虫流程\npython main.py --complete --date 2024-01-20\n\n# 仅运行话题提取\npython main.py --broad-topic --date 2024-01-20\n\n# 仅运行深度爬取\npython main.py --deep-sentiment --platforms xhs dy wb\n</code></pre><p>每个Agent都有专门的配置文件，可根据需求调整，下面是部分示例：</p><pre><code># QueryEngine/utils/config.py\nclass Config:\n    max_reflections = 2           # 反思轮次\n    max_search_results = 15       # 最大搜索结果数\n    max_content_length = 8000     # 最大内容长度\n    \n# MediaEngine/utils/config.py  \nclass Config:\n    comprehensive_search_limit = 10  # 综合搜索限制\n    web_search_limit = 15           # 网页搜索限制\n    \n# InsightEngine/utils/config.py\nclass Config:\n    default_search_topic_globally_limit = 200    # 全局搜索限制\n    default_get_comments_limit = 500             # 评论获取限制\n    max_search_results_for_llm = 50              # 传给LLM的最大结果数\n</code></pre><pre><code># InsightEngine/tools/sentiment_analyzer.py\nSENTIMENT_CONFIG = {\n    'model_type': 'multilingual',     # 可选: 'bert', 'multilingual', 'qwen'等\n    'confidence_threshold': 0.8,      # 置信度阈值\n    'batch_size': 32,                 # 批处理大小\n    'max_sequence_length': 512,       # 最大序列长度\n}\n</code></pre><p>支持任意openAI调用格式的LLM提供商，只需要在/config.py中填写对应的KEY、BASE_URL、MODEL_NAME即可。</p><blockquote><p>什么是openAI调用格式？下面提供一个简单的例子：</p><pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key=\"your_api_key\", \n               base_url=\"https://api.siliconflow.cn/v1\")\n\nresponse = client.chat.completions.create(\n   model=\"Qwen/Qwen2.5-72B-Instruct\",\n   messages=[\n       {'role': 'user', \n        'content': \"推理模型会给市场带来哪些新的机会\"}\n   ],\n)\n\ncomplete_response = response.choices[0].message.content\nprint(complete_response)\n</code></pre></blockquote><pre><code>cd SentimentAnalysisModel/WeiboMultilingualSentiment\npython predict.py --text \"This product is amazing!\" --lang \"en\"\n</code></pre><pre><code>cd SentimentAnalysisModel/WeiboSentiment_SmallQwen\npython predict_universal.py --text \"这次活动办得很成功\"\n</code></pre><pre><code># 使用BERT中文模型\ncd SentimentAnalysisModel/WeiboSentiment_Finetuned/BertChinese-Lora\npython predict.py --text \"这个产品真的很不错\"\n</code></pre><pre><code>cd SentimentAnalysisModel/WeiboSentiment_Finetuned/GPT2-Lora\npython predict.py --text \"今天心情不太好\"\n</code></pre><pre><code>cd SentimentAnalysisModel/WeiboSentiment_MachineLearning\npython predict.py --model_type \"svm\" --text \"服务态度需要改进\"\n</code></pre><pre><code># config.py 中添加您的业务数据库配置\nBUSINESS_DB_HOST = \"your_business_db_host\"\nBUSINESS_DB_PORT = 3306\nBUSINESS_DB_USER = \"your_business_user\"\nBUSINESS_DB_PASSWORD = \"your_business_password\"\nBUSINESS_DB_NAME = \"your_business_database\"\n</code></pre><pre><code># InsightEngine/tools/custom_db_tool.py\nclass CustomBusinessDBTool:\n    \"\"\"自定义业务数据库查询工具\"\"\"\n    \n    def __init__(self):\n        self.connection_config = {\n            'host': config.BUSINESS_DB_HOST,\n            'port': config.BUSINESS_DB_PORT,\n            'user': config.BUSINESS_DB_USER,\n            'password': config.BUSINESS_DB_PASSWORD,\n            'database': config.BUSINESS_DB_NAME,\n        }\n    \n    def search_business_data(self, query: str, table: str):\n        \"\"\"查询业务数据\"\"\"\n        # 实现您的业务逻辑\n        pass\n    \n    def get_customer_feedback(self, product_id: str):\n        \"\"\"获取客户反馈数据\"\"\"\n        # 实现客户反馈查询逻辑\n        pass\n</code></pre><pre><code># InsightEngine/agent.py 中集成自定义工具\nfrom .tools.custom_db_tool import CustomBusinessDBTool\n\nclass DeepSearchAgent:\n    def __init__(self, config=None):\n        # ... 其他初始化代码\n        self.custom_db_tool = CustomBusinessDBTool()\n    \n    def execute_custom_search(self, query: str):\n        \"\"\"执行自定义业务数据搜索\"\"\"\n        return self.custom_db_tool.search_business_data(query, \"your_table\")\n</code></pre><p>系统支持上传自定义模板文件（.md或.txt格式），可在生成报告时选择使用。</p><p>在 <code>ReportEngine/report_template/</code> 目录下创建新的模板，我们的Agent会自行选用最合适的模板。</p><ol><li>：<code>git checkout -b feature/AmazingFeature</code></li><li>：<code>git commit -m 'Add some AmazingFeature'</code></li><li>：<code>git push origin feature/AmazingFeature</code></li></ol><ul></ul><p>现在系统只完成了\"三板斧\"中的前两步，即：输入要求-&gt;详细分析，还缺少一步预测，直接将他继续交给LLM是不具有说服力的。</p><p>目前我们经过很长一段时间的爬取收集，拥有了大量全网话题热度随时间、爆点等的变化趋势热度数据，已经具备了可以开发预测模型的条件。我们团队将运用时序模型、图神经网络、多模态融合等预测模型技术储备于此，实现真正基于数据驱动的舆情预测功能。</p><ol><li><ul><li>本项目中的所有代码、工具和功能均仅供学习、学术研究和教育目的使用</li><li>严禁将本项目用于任何违法、违规或侵犯他人权益的行为</li></ul></li><li><ul><li>使用者必须遵守目标网站的robots.txt协议和使用条款</li><li>使用者必须遵守相关法律法规，不得进行恶意爬取或数据滥用</li></ul></li><li><ul></ul></li><li><ul><li>作者不对使用本项目造成的任何直接或间接损失承担责任</li></ul></li><li><ul><li>因违反法律法规使用本项目而产生的任何后果由使用者自行承担</li></ul></li></ol><p><strong>请在使用本项目前仔细阅读并理解上述免责声明。使用本项目即表示您已同意并接受上述所有条款。</strong></p><a href=\"https://www.star-history.com/#666ghj/BettaFish&amp;type=date&amp;legend=top-left\"></a>","contentLength":15701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ventoy/Ventoy","url":"https://github.com/ventoy/Ventoy","date":1761964521,"author":"","guid":323850,"unread":true,"content":"<p>A new bootable USB solution.</p><h4 align=\"left\"> Ventoy is an open source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files.  With ventoy, you don't need to format the disk over and over, just copy the image files to the USB drive and boot them. You can copy many image files at a time and ventoy will give you a boot menu to select them. <p> You can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disk and boot them.</p> x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI and MIPS64EL UEFI are supported in the same way.<p> Both MBR and GPT partition style are supported in the same way.</p> Most type of OS supported(Windows/WinPE/Linux/Unix/ChromeOS/Vmware/Xen...) <p> 1200+ ISO files are tested (</p><a href=\"https://www.ventoy.net/en/isolist.html\">List</a>). 90%+ distros in <a href=\"https://distrowatch.com/\">distrowatch.com</a> supported (<a href=\"https://www.ventoy.net/en/distrowatch.html\">Details</a>). <a href=\"https://www.ventoy.net\">https://www.ventoy.net</a></h4><p> Windows 7, Windows 8, Windows 8.1, Windows 10, Windows 11, Windows Server 2012, Windows Server 2012 R2, Windows Server 2016, Windows Server 2019, Windows Server 2022, Windows Server 2025, WinPE</p><p> Debian, Ubuntu, CentOS(6/7/8/9), RHEL(6/7/8/9), Deepin, Fedora, Rocky Linux, AlmaLinux, EuroLinux(6/7/8/9), openEuler, OpenAnolis, SLES, openSUSE, MX Linux, Manjaro, Linux Mint, Endless OS, Elementary OS, Solus, Linx, Zorin, antiX, PClinuxOS, Arch, ArcoLinux, ArchLabs, BlackArch, Obarun, Artix Linux, Puppy Linux, Tails, Slax, Kali, Mageia, Slackware, Q4OS, Archman, Gentoo, Pentoo, NixOS, Kylin, openKylin, Ubuntu Kylin, KylinSec, Lubuntu, Xubuntu, Kubuntu, Ubuntu MATE, Ubuntu Budgie, Ubuntu Studio, Bluestar, OpenMandriva, ExTiX, Netrunner, ALT Linux, Nitrux, Peppermint, KDE neon, Linux Lite, Parrot OS, Qubes, Pop OS, ROSA, Void Linux, Star Linux, EndeavourOS, MakuluLinux, Voyager, Feren, ArchBang, LXLE, Knoppix, Calculate Linux, Clear Linux, Pure OS, Oracle Linux, Trident, Septor, Porteus, Devuan, GoboLinux, 4MLinux, Simplicity Linux, Zeroshell, Android-x86, netboot.xyz, Slitaz, SuperGrub2Disk, Proxmox VE, Kaspersky Rescue, SystemRescueCD, MemTest86, MemTest86+, MiniTool Partition Wizard, Parted Magic, veket, Sabayon, Scientific, alpine, ClearOS, CloneZilla, Berry Linux, Trisquel, Ataraxia Linux, Minimal Linux Live, BackBox Linux, Emmabuntüs, ESET SysRescue Live,Nova Linux, AV Linux, RoboLinux, NuTyX, IPFire, SELKS, ZStack, Enso Linux, Security Onion, Network Security Toolkit, Absolute Linux, TinyCore, Springdale Linux, Frost Linux, Shark Linux, LinuxFX, Snail Linux, Astra Linux, Namib Linux, Resilient Linux, Virage Linux, Blackweb Security OS, R-DriveImage, O-O.DiskImage, Macrium, ToOpPy LINUX, GNU Guix, YunoHost, foxclone, siduction, Adelie Linux, Elive, Pardus, CDlinux, AcademiX, Austrumi, Zenwalk, Anarchy, DuZeru, BigLinux, OpenMediaVault, Ubuntu DP, Exe GNU/Linux, 3CX Phone System, KANOTIX, Grml, Karoshi, PrimTux, ArchStrike, CAELinux, Cucumber, Fatdog, ForLEx, Hanthana, Kwort, MiniNo, Redcore, Runtu, Asianux, Clu Linux Live, Uruk, OB2D, BlueOnyx, Finnix, HamoniKR, Parabola, LinHES, LinuxConsole, BEE free, Untangle, Pearl, Thinstation, TurnKey, tuxtrans, Neptune, HefftorLinux, GeckoLinux, Mabox Linux, Zentyal, Maui, Reborn OS, SereneLinux , SkyWave Linux, Kaisen Linux, Regata OS, TROM-Jaro, DRBL Linux, Chalet OS, Chapeau, Desa OS, BlankOn, OpenMamba, Frugalware, Kibojoe Linux, Revenge OS, Tsurugi Linux, Drauger OS, Hash Linux, gNewSense, Ikki Boot, SteamOS, Hyperbola, VyOS, EasyNAS, SuperGamer, Live Raizo, Swift Linux, RebeccaBlackOS, Daphile, CRUX, Univention, Ufficio Zero, Rescuezilla, Phoenix OS, Garuda Linux, Mll, NethServer, OSGeoLive, Easy OS, Volumio, FreedomBox, paldo, UBOS, Recalbox, batocera, Lakka, LibreELEC, Pardus Topluluk, Pinguy, KolibriOS, Elastix, Arya, Omoikane, Omarine, Endian Firewall, Hamara, Rocks Cluster, MorpheusArch, Redo, Slackel, SME Server, APODIO, Smoothwall, Dragora, Linspire, Secure-K OS, Peach OSI, Photon, Plamo, SuperX, Bicom, Ploplinux, HP SPP, LliureX, Freespire, DietPi, BOSS, Webconverger, Lunar, TENS, Source Mage, RancherOS, T2, Vine, Pisi, blackPanther, mAid, Acronis, Active.Boot, AOMEI, Boot.Repair, CAINE, DaRT, EasyUEFI, R-Drive, PrimeOS, Avira Rescue System, bitdefender, Checkra1n Linux, Lenovo Diagnostics, Clover, Bliss-OS, Lenovo BIOS Update, Arcabit Rescue Disk, MiyoLinux, TeLOS, Kerio Control, RED OS, OpenWrt, MocaccinoOS, EasyStartup, Pyabr, Refracta, Eset SysRescue, Linpack Xtreme, Archcraft, NHVBOOT, pearOS, SeaTools, Easy Recovery Essentional, iKuai, StorageCraft SCRE, ZFSBootMenu, TROMjaro, BunsenLabs, Todo en Uno, ChallengerOS, Nobara, Holo, CachyOS, Peux OS, Vanilla OS, ShredOS, paladin, Palen1x, dban, ReviOS, HelenOS, XeroLinux, Tiny 11, chimera linux, CuteFish, DragonOs, Rhino Linux, vanilladpup, crystal, IGELOS, MiniOS, gnoppix, PikaOS, UwUntu, Noble, PocketHandyBox, DiskGenius, ......</p><p> DragonFly, FreeBSD, pfSense, OPNsense, GhostBSD, FreeNAS, TrueNAS, XigmaNAS, FuryBSD, HardenedBSD, MidnightBSD, ClonOS, EmergencyBootKit, helloSystem</p><p> FydeOS, CloudReady, ChromeOS Flex</p><p> VMware ESXi, Citrix XenServer, Xen XCP-ng</p><p>With Ventoy, you can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disk and boot them. <a href=\"https://www.ventoy.net/en/doc_browser.html\">Notes</a></p><ul><li>Fast (limited only by the speed of copying iso file)</li><li>Can be installed in USB/Local Disk/SSD/NVMe/SD Card</li><li>Directly boot from ISO/WIM/IMG/VHD(x)/EFI files, no extraction needed</li><li>Support to browse and boot ISO/WIM/IMG/VHD(x)/EFI files in local disk</li><li>No need to be continuous in disk for ISO/WIM/IMG/VHD(x)/EFI files</li><li>MBR and GPT partition style supported (1.0.15+)</li><li>x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI, MIPS64EL UEFI supported</li><li>IA32/x86_64 UEFI Secure Boot supported (1.0.07+)</li><li>Linux Persistence supported (1.0.11+)</li><li>Windows auto installation supported (1.0.09+)</li><li>Linux auto installation supported (1.0.09+)</li><li>Variables Expansion supported for Windows/Linux auto installation script</li><li>FAT32/exFAT/NTFS/UDF/XFS/Ext2(3)(4) supported for main partition</li><li>ISO files larger than 4GB supported</li><li>Menu alias, Menu tip message supported</li><li>Password protect supported</li><li>Native boot menu style for Legacy &amp; UEFI</li><li>Most types of OS supported, 1200+ iso files tested</li><li>Linux vDisk boot supported</li><li>Not only boot but also complete installation process</li><li>Menu dynamically switchable between List/TreeView mode</li><li>\"Ventoy Compatible\" concept</li><li>Plugin Framework and GUI plugin configurator</li><li>Injection files to runtime environment</li><li>Boot configuration file dynamically replacement</li><li>Highly customizable theme and menu</li><li>USB drive write-protected support</li><li>USB normal use unaffected</li><li>Data nondestructive during version upgrade</li><li>No need to update Ventoy when a new distro is released</li></ul><p>It would be much appreciated if you want to make a small donation to support my work! Alipay, WeChat Pay, PayPal and Bitcoin are available for donation. You can choose any of them.</p><p> Bitcoin Address <code>19mZDWzZgzkHCi9YX9H3fYCUuCHq3W6wfT</code></p>","contentLength":6629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"janhq/jan","url":"https://github.com/janhq/jan","date":1761878103,"author":"","guid":322479,"unread":true,"content":"<p>Jan is an open source alternative to ChatGPT that runs 100% offline on your computer.</p><img width=\"2048\" height=\"280\" alt=\"github jan banner\" src=\"https://github.com/user-attachments/assets/f3f87889-c133-433b-b250-236218150d3f\"><p>Jan is bringing the best of open-source AI in an easy-to-use product. Download and run LLMs with  and .</p><p>The easiest way to get started is by downloading one of the following versions for your respective operating system:</p><ul><li>: Download and run LLMs (Llama, Gemma, Qwen, GPT-oss etc.) from HuggingFace</li><li>: Connect to GPT models via OpenAI, Claude models via Anthropic, Mistral, Groq, and others</li><li>: Create specialized AI assistants for your tasks</li><li>: Local server at  for other applications</li><li>: MCP integration for agentic capabilities</li><li>: Everything runs locally when you want it to</li></ul><p>For those who enjoy the scenic route:</p><ul></ul><pre><code>git clone https://github.com/janhq/jan\ncd jan\nmake dev\n</code></pre><p>This handles everything: installs dependencies, builds core components, and launches the app.</p><ul><li> - Full development setup and launch</li><li> - Production build</li><li> - Run tests and linting</li><li> - Delete everything and start fresh</li></ul><pre><code>yarn install\nyarn build:tauri:plugin:api\nyarn build:core\nyarn build:extensions\nyarn dev\n</code></pre><p><strong>Minimum specs for a decent experience:</strong></p><ul><li>: 13.6+ (8GB RAM for 3B models, 16GB for 7B, 32GB for 13B)</li><li>: 10+ with GPU support for NVIDIA/AMD/Intel Arc</li><li>: Most distributions work, GPU acceleration available</li></ul><p>Apache 2.0 - Because sharing is caring.</p><p>Built on the shoulders of giants:</p>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"helm/helm","url":"https://github.com/helm/helm","date":1761878103,"author":"","guid":322480,"unread":true,"content":"<p>The Kubernetes Package Manager</p><p>Helm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.</p><ul><li>Share your own applications as Helm Charts</li><li>Create reproducible builds of your Kubernetes applications</li><li>Intelligently manage your Kubernetes manifest files</li><li>Manage releases of Helm packages</li></ul><p>Helm is a tool that streamlines installing and managing Kubernetes applications. Think of it like apt/yum/homebrew for Kubernetes.</p><ul><li>Helm renders your templates and communicates with the Kubernetes API</li><li>Helm runs on your laptop, CI/CD, or wherever you want it to run.</li><li>Charts are Helm packages that contain at least two things: \n  <ul><li>A description of the package ()</li><li>One or more templates, which contain Kubernetes manifest files</li></ul></li><li>Charts can be stored on disk, or fetched from remote chart repositories (like Debian or RedHat packages)</li></ul><h2>Helm Development and Stable Versions</h2><p>Helm v4 is currently under development on the  branch. This is unstable and the APIs within the Go SDK and at the command line are changing. Helm v3 (current stable) is maintained on the  branch. APIs there follow semantic versioning.</p><p>Unpack the  binary and add it to your PATH and you are good to go!</p><p>If you want to use a package manager:</p><ul><li><a href=\"https://chocolatey.org/\">Chocolatey</a> users can use <code>choco install kubernetes-helm</code>.</li><li><a href=\"https://learn.microsoft.com/en-us/windows/package-manager/\">Winget</a> users can use .</li><li><a href=\"https://scoop.sh/\">Scoop</a> users can use .</li><li><a href=\"https://snapcraft.io/\">Snapcraft</a> users can use <code>snap install helm --classic</code>.</li><li><a href=\"https://flox.dev\">Flox</a> users can use <code>flox install kubernetes-helm</code>.</li></ul><p>The development of Helm v4 is currently happening on the  branch while the development of Helm v3, the stable branch, is happening on the  branch. Changes should be made to the  branch prior to being added to the  branch so that all changes are carried along to Helm v4.</p><h2>Community, discussion, contribution, and support</h2><p>You can reach the Helm community and developers via the following channels:</p><p>If you're interested in contributing, please refer to the <a href=\"https://raw.githubusercontent.com/helm/helm/main/CONTRIBUTING.md\">Contributing Guide</a><strong>before submitting a pull request</strong>.</p>","contentLength":1902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"projectdiscovery/nuclei-templates","url":"https://github.com/projectdiscovery/nuclei-templates","date":1761878103,"author":"","guid":322481,"unread":true,"content":"<p>Community curated list of templates for the nuclei engine to find security vulnerabilities.</p><h4 align=\"center\">Community curated list of templates for the nuclei engine to find security vulnerabilities in applications.</h4><p>Templates are the core of the <a href=\"https://github.com/projectdiscovery/nuclei\">nuclei scanner</a> which powers the actual scanning engine. This repository stores and houses various templates for the scanner provided by our team, as well as contributed by the community. We hope that you also contribute by sending templates via  or <a href=\"https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&amp;labels=&amp;template=submit-template.md&amp;title=%5Bnuclei-template%5D+\">Github issues</a> to grow the list.</p><h2>Nuclei Templates overview</h2><p>An overview of the nuclei template project, including statistics on unique tags, author, directory, severity, and type of templates. The table below contains the top ten statistics for each matrix; an expanded version of this is <a href=\"https://raw.githubusercontent.com/projectdiscovery/nuclei-templates/main/TEMPLATES-STATS.md\">available here</a>, and also available in <a href=\"https://raw.githubusercontent.com/projectdiscovery/nuclei-templates/main/TEMPLATES-STATS.json\">JSON</a> format for integration.</p><table><tbody><tr><td><h2>Nuclei Templates Top 10 statistics</h2><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p><strong>848 directories, 11344 files</strong>.</p></td></tr></tbody></table><p>Please navigate to <a href=\"https://nuclei.projectdiscovery.io\">https://nuclei.projectdiscovery.io</a> for detailed documentation to  new or your own  templates. We have also added a set of templates to help you understand how things work.</p><p>Have questions / doubts / ideas to discuss? Feel free to open a discussion on <a href=\"https://github.com/projectdiscovery/nuclei-templates/discussions\">Github discussions</a> board.</p><p>You are welcome to join the active <a href=\"https://discord.gg/projectdiscovery\">Discord Community</a> to discuss directly with project maintainers and share things with others around security and automation. Additionally, you may follow us on <a href=\"https://twitter.com/pdnuclei\">Twitter</a> to be updated on all the things about Nuclei.</p><p>Thanks again for your contribution and keeping this community vibrant. </p>","contentLength":1502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nvm-sh/nvm","url":"https://github.com/nvm-sh/nvm","date":1761878103,"author":"","guid":322482,"unread":true,"content":"<p>Node Version Manager - POSIX-compliant bash script to manage multiple active node.js versions</p><a href=\"https://github.com/nvm-sh/logos\"></a><p> allows you to quickly install and use different versions of node via the command line.</p><pre><code>$ nvm use 16\nNow using node v16.9.1 (npm v7.21.1)\n$ node -v\nv16.9.1\n$ nvm use 14\nNow using node v14.18.0 (npm v6.14.15)\n$ node -v\nv14.18.0\n$ nvm install 12\nNow using node v12.22.6 (npm v6.14.5)\n$ node -v\nv12.22.6\n</code></pre><p>nvm is a version manager for <a href=\"https://nodejs.org/en/\">node.js</a>, designed to be installed per-user, and invoked per-shell.  works on any POSIX-compliant shell (sh, dash, ksh, zsh, bash), in particular on these platforms: unix, macOS, and <a href=\"https://github.com/nvm-sh/nvm#important-notes\">windows WSL</a>.</p><p>To  or  nvm, you should run the <a href=\"https://github.com/nvm-sh/nvm/raw/v0.40.3/install.sh\">install script</a>. To do that, you may either download and run the script manually, or use the following cURL or Wget command:</p><pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><pre><code>wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><p>Running either of the above commands downloads a script and runs it. The script clones the nvm repository to , and attempts to add the source lines from the snippet below to the correct profile file (, , , or ). If you find the install script is updating the wrong profile file, set the  env var to the profile file’s path, and then rerun the installation script.</p><pre><code>export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n</code></pre><ul><li><p>If the environment variable  is present, it will place the  files there.</p></li><li><p>You can add  to the end of the above script to postpone using  until you manually <a href=\"https://raw.githubusercontent.com/nvm-sh/nvm/master/#usage\"></a> it:</p></li></ul><pre><code>export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" --no-use # This loads nvm, without auto-using the default version\n</code></pre><ul><li><p>You can customize the install source, directory, profile, and version using the , , , and  variables. Eg: <code>curl ... | NVM_DIR=\"path/to/nvm\"</code>. Ensure that the  does not contain a trailing slash.</p></li><li><p>The installer can use , , or  to download , whichever is available.</p></li><li><p>You can instruct the installer to not edit your shell config (for example if you already get completions via a <a href=\"https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/nvm\">zsh nvm plugin</a>) by setting  before running the  script. Here's an example one-line command to do that: <code>PROFILE=/dev/null bash -c 'curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash'</code></p></li></ul><p>When invoking bash as a non-interactive shell, like in a Docker container, none of the regular profile files are sourced. In order to use , , and  like normal, you can instead specify the special  variable, which bash sources when invoked non-interactively.</p><pre><code># Use bash for the shell\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n\n# Create a script file sourced by both interactive and non-interactive bash shells\nENV BASH_ENV /home/user/.bash_env\nRUN touch \"${BASH_ENV}\"\nRUN echo '. \"${BASH_ENV}\"' &gt;&gt; ~/.bashrc\n\n# Download and install nvm\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | PROFILE=\"${BASH_ENV}\" bash\nRUN echo node &gt; .nvmrc\nRUN nvm install\n</code></pre><h5>Installing in Docker for CICD-Jobs</h5><pre><code>FROM ubuntu:latest\nARG NODE_VERSION=20\n\n# install curl\nRUN apt update &amp;&amp; apt install curl -y\n\n# install nvm\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n\n# set env\nENV NVM_DIR=/root/.nvm\n\n# install node\nRUN bash -c \"source $NVM_DIR/nvm.sh &amp;&amp; nvm install $NODE_VERSION\"\n\n# set ENTRYPOINT for reloading nvm-environment\nENTRYPOINT [\"bash\", \"-c\", \"source $NVM_DIR/nvm.sh &amp;&amp; exec \\\"$@\\\"\", \"--\"]\n\n# set cmd to bash\nCMD [\"/bin/bash\"]\n\n</code></pre><p>This example defaults to installation of nodejs version 20.x.y. Optionally you can easily override the version with docker build args like:</p><pre><code>docker build -t nvmimage --build-arg NODE_VERSION=19 .\n</code></pre><p>After creation of the image you can start container interactively and run commands, for example:</p><pre><code>docker run --rm -it nvmimage\n\nroot@0a6b5a237c14:/# nvm -v\n0.40.3\n\nroot@0a6b5a237c14:/# node -v\nv19.9.0\n\nroot@0a6b5a237c14:/# npm -v\n9.6.3\n</code></pre><pre><code>user@host:/tmp/test $ docker run --rm -it nvmimage node -v\nv19.9.0\nuser@host:/tmp/test $ docker run --rm -it nvmimage npm -v\n9.6.3\n</code></pre><p>On Linux, after running the install script, if you get  or see no feedback from your terminal after you type , simply close your current terminal, open a new terminal, and try verifying again. Alternatively, you can run the following commands for the different shells on the command line:</p><p>These should pick up the  command.</p><p>Since OS X 10.9,  has been preset by Xcode command line tools, which means we can't properly detect if Git is installed or not. You need to manually install the Xcode command line tools before running the install script, otherwise, it'll fail. (see <a href=\"https://github.com/nvm-sh/nvm/issues/1782\">#1782</a>)</p><p>If you get  after running the install script, one of the following might be the reason:</p><ul><li><p>Since macOS 10.15, the default shell is  and nvm will look for  to update, none is installed by default. Create one with  and run the install script again.</p></li><li><p>If you use bash, the previous default shell, your system may not have  or  files where the command is set up. Create one of them with  or  and run the install script again. Then, run  or  to pick up the  command.</p></li><li><p>You have previously used , but you have  installed. You need to manually add <a href=\"https://raw.githubusercontent.com/nvm-sh/nvm/master/#manual-install\">these lines</a> to  and run .</p></li><li><p>You might need to restart your terminal instance or run . Restarting your terminal/opening a new tab/window, or running the source command will load the command and the new configuration.</p></li><li><p>If the above didn't help, you might need to restart your terminal instance. Try opening a new tab/window in your terminal and retry.</p></li></ul><p>If the above doesn't fix the problem, you may try the following:</p><ul><li><p>If you use bash, it may be that your  (or ) does not source your  properly. You could fix this by adding <code>source ~/&lt;your_profile_file&gt;</code> to it or following the next step below.</p></li><li><p>For more information about this issue and possible workarounds, please <a href=\"https://github.com/nvm-sh/nvm/issues/576\">refer here</a></p></li></ul><p> For Macs with the Apple Silicon chip, node started offering  arch Darwin packages since v16.0.0 and experimental  support when compiling from source since v14.17.0. If you are facing issues installing node using , you may want to update to one of those versions or later.</p><pre><code>- name: Install nvm\n  ansible.builtin.shell: &gt;\n    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n  args:\n    creates: \"{{ ansible_env.HOME }}/.nvm/nvm.sh\"\n</code></pre><p>To verify that nvm has been installed, do:</p><p>which should output  if the installation was successful. Please note that  will not work, since  is a sourced shell function, not an executable binary.</p><p> On Linux, after running the install script, if you get  or see no feedback from your terminal after you type , simply close your current terminal, open a new terminal, and try verifying again.</p><p>If you're running a system without prepackaged binary available, which means you're going to install node or io.js from its source code, you need to make sure your system has a C++ compiler. For OS X, Xcode will work, for Debian/Ubuntu based GNU/Linux, the  and  packages work.</p><p> also supports Windows in some cases. It should work through WSL (Windows Subsystem for Linux) depending on the version of WSL. It should also work with <a href=\"https://gitforwindows.org/\">GitBash</a> (MSYS) or <a href=\"https://cygwin.com\">Cygwin</a>. Otherwise, for Windows, a few alternatives exist, which are neither supported nor developed by us:</p><p> does not support <a href=\"https://fishshell.com\">Fish</a> either (see <a href=\"https://github.com/nvm-sh/nvm/issues/303\">#303</a>). Alternatives exist, which are neither supported nor developed by us:</p><ul><li><a href=\"https://github.com/edc/bass\">bass</a> allows you to use utilities written for Bash in fish shell</li><li><a href=\"https://github.com/brigand/fast-nvm-fish\">fast-nvm-fish</a> only works with version numbers (not aliases) but doesn't significantly slow your shell startup</li><li><a href=\"https://github.com/jorgebucaran/nvm.fish\">nvm.fish</a> - The Node.js version manager you'll adore, crafted just for Fish</li><li><a href=\"https://github.com/FabioAntunes/fish-nvm\">fish-nvm</a> - Wrapper around nvm for fish, delays sourcing nvm until it's actually used.</li></ul><p> We still have some problems with FreeBSD, because there is no official pre-built binary for FreeBSD, and building from source may need <a href=\"https://www.freshports.org/www/node/files/patch-deps_v8_src_base_platform_platform-posix.cc\">patches</a>; see the issue ticket:</p><p> On OS X, if you do not have Xcode installed and you do not wish to download the ~4.3GB file, you can install the . You can check out this blog post on how to just that:</p><p> On OS X, if you have/had a \"system\" node installed and want to install modules globally, keep in mind that:</p><ul><li>When using  you do not need  to globally install a module with , so instead of doing <code>sudo npm install -g grunt</code>, do instead </li><li>If you have an  file, make sure it does not contain any  settings (which is not compatible with )</li><li>You can (but should not?) keep your previous \"system\" node install, but  will only be available to your user account (the one used to install nvm). This might cause version mismatches, as other users will be using <code>/usr/local/lib/node_modules/*</code> VS your user account using <code>~/.nvm/versions/node/vX.X.X/lib/node_modules/*</code></li></ul><p>Homebrew installation is not supported. If you have issues with homebrew-installed , please  it, and install it using the instructions below, before filing an issue.</p><p> If you're using  you can easily install  as a zsh plugin. Install <a href=\"https://github.com/lukechilds/zsh-nvm\"></a> and run  to upgrade (<a href=\"https://github.com/lukechilds/zsh-nvm#auto-use\">you can set</a> to have it automatically detect and use  files).</p><p> Git versions before v1.7 may face a problem of cloning  source from GitHub via https protocol, and there is also different behavior of git before v1.6, and git prior to <a href=\"https://github.com/git/git/commit/5a7d5b683f869d3e3884a89775241afa515da9e7\">v1.17.10</a> can not clone tags, so the minimum required git version is v1.7.10. If you are interested in the problem we mentioned here, please refer to GitHub's <a href=\"https://help.github.com/articles/https-cloning-errors/\">HTTPS cloning errors</a> article.</p><p>If you have  installed (requires git v1.7.10+):</p><ol><li>clone this repo in the root of your user profile \n  <ul><li> from anywhere then <code>git clone https://github.com/nvm-sh/nvm.git .nvm</code></li></ul></li><li> and check out the latest version with </li><li>activate  by sourcing it from your shell: </li></ol><p>Now add these lines to your , , or  file to have it automatically sourced upon login: (you may have to add to more than one of the above files)</p><pre><code>export NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm\n[ -s \"$NVM_DIR/bash_completion\" ] &amp;&amp; \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_completion\n</code></pre><p>For a fully manual install, execute the following lines to first clone the  repository into , and then load :</p><pre><code>export NVM_DIR=\"$HOME/.nvm\" &amp;&amp; (\n  git clone https://github.com/nvm-sh/nvm.git \"$NVM_DIR\"\n  cd \"$NVM_DIR\"\n  git checkout `git describe --abbrev=0 --tags --match \"v[0-9]*\" $(git rev-list --tags --max-count=1)`\n) &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"\n</code></pre><p>Now add these lines to your , , or  file to have it automatically sourced upon login: (you may have to add to more than one of the above files)</p><pre><code>export NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n[ -s \"$NVM_DIR/bash_completion\" ] &amp;&amp; \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_completion\n</code></pre><p>For manual upgrade with  (requires git v1.7.10+):</p><ol><li>pull down the latest changes</li><li>check out the latest version</li></ol><pre><code>(\n  cd \"$NVM_DIR\"\n  git fetch --tags origin\n  git checkout `git describe --abbrev=0 --tags --match \"v[0-9]*\" $(git rev-list --tags --max-count=1)`\n) &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"\n</code></pre><p>To download, compile, and install the latest release of node, do this:</p><pre><code>nvm install node # \"node\" is an alias for the latest version\n</code></pre><p>To install a specific version of node:</p><pre><code>nvm install 14.7.0 # or 16.3.0, 12.22.1, etc\n</code></pre><pre><code>nvm alias my_alias v14.4.0\n</code></pre><p>Make sure that your alias does not contain any spaces or slashes.</p><p>The first version installed becomes the default. New shells will start with the default version of node (e.g., ).</p><p>You can list available versions using :</p><p>And then in any new shell just use the installed version:</p><p>Or, you can run any arbitrary command in a subshell with the desired version of node:</p><pre><code>nvm exec 4.2 node --version\n</code></pre><p>You can also get the path to the executable to where it was installed:</p><p>In place of a version pointer like \"14.7\" or \"16.3\" or \"12.22.1\", you can use the following special default aliases with , , , , , etc:</p><ul><li>: this installs the latest version of <a href=\"https://nodejs.org/en/\"></a></li><li>: this installs the latest version of <a href=\"https://iojs.org/en/\"></a></li><li>: this alias is deprecated, and only truly applies to  and earlier. Currently, this is an alias for .</li><li>: this alias points to  - the last \"unstable\" node release, since post-1.0, all node versions are stable. (in SemVer, versions communicate breakage, not stability).</li></ul><p>Node has a <a href=\"https://github.com/nodejs/Release#release-schedule\">schedule</a> for long-term support (LTS) You can reference LTS versions in aliases and  files with the notation  for the latest LTS, and  for LTS releases from the \"argon\" line, for example. In addition, the following commands support LTS arguments:</p><ul><li> /  /  / </li><li> / <code>nvm uninstall --lts=argon</code> /  / </li><li> /  /  / </li><li> /  /  / </li><li> /  /  / </li><li> / <code>nvm ls-remote --lts=argon</code> / </li><li> / <code>nvm version-remote --lts=argon</code> / <code>nvm version-remote 'lts/*'</code> / <code>nvm version-remote lts/argon</code></li></ul><p>Any time your local copy of  connects to <a href=\"https://nodejs.org\">https://nodejs.org</a>, it will re-create the appropriate local aliases for all available LTS lines. These aliases (stored under ), are managed by , and you should not modify, remove, or create these files - expect your changes to be undone, and expect meddling with these files to cause bugs that will likely not be supported.</p><p>To get the latest LTS version of node and migrate your existing installed packages, use</p><pre><code>nvm install --reinstall-packages-from=current 'lts/*'\n</code></pre><h3>Migrating Global Packages While Installing</h3><p>If you want to install a new version of Node.js and migrate npm packages from a previous version:</p><pre><code>nvm install --reinstall-packages-from=node node\n</code></pre><p>This will first use \"nvm version node\" to identify the current version you're migrating packages from. Then it resolves the new version to install from the remote server and installs it. Lastly, it runs \"nvm reinstall-packages\" to reinstall the npm packages from your prior version of Node to the new one.</p><p>You can also install and migrate npm packages from specific versions of Node like this:</p><pre><code>nvm install --reinstall-packages-from=5 6\nnvm install --reinstall-packages-from=iojs v4.2\n</code></pre><p>Note that reinstalling packages <em>explicitly does not update the npm version</em> — this is to ensure that npm isn't accidentally upgraded to a broken version for the new node version.</p><p>To update npm at the same time add the  flag, like this:</p><pre><code>nvm install --reinstall-packages-from=default --latest-npm 'lts/*'\n</code></pre><p>or, you can at any time run the following command to get the latest supported npm version on the current node version:</p><p>If you've already gotten an error to the effect of \"npm does not support Node.js\", you'll need to (1) revert to a previous node version ( &amp; <code>nvm use &lt;your latest _working_ version from the ls&gt;</code>), (2) delete the newly created node version (<code>nvm uninstall &lt;your _broken_ version of node from the ls&gt;</code>), then (3) rerun your  with the  flag.</p><h3>Default Global Packages From File While Installing</h3><p>If you have a list of default packages you want installed every time you install a new version, we support that too -- just add the package names, one per line, to the file <code>$NVM_DIR/default-packages</code>. You can add anything npm would accept as a package argument on the command line.</p><pre><code># $NVM_DIR/default-packages\n\nrimraf\nobject-inspect@1.0.2\nstevemao/left-pad\n</code></pre><p>If you want to install <a href=\"https://github.com/iojs/io.js/\">io.js</a>:</p><p>If you want to install a new version of io.js and migrate npm packages from a previous version:</p><pre><code>nvm install --reinstall-packages-from=iojs iojs\n</code></pre><p>The same guidelines mentioned for migrating npm packages in node are applicable to io.js.</p><p>If you want to use the system-installed version of node, you can use the special default alias \"system\":</p><pre><code>nvm use system\nnvm run system --version\n</code></pre><p>If you want to see what versions are installed:</p><p>If you want to see what versions are available to install:</p><p>You can set five colors that will be used to display version and alias information. These colors replace the default colors. Initial colors are: g b y r e</p><pre><code>r/R = red / bold red\n\ng/G = green / bold green\n\nb/B = blue / bold blue\n\nc/C = cyan / bold cyan\n\nm/M = magenta / bold magenta\n\ny/Y = yellow / bold yellow\n\nk/K = black / bold black\n\ne/W = light grey / white\n</code></pre><p>If you want the custom colors to persist after terminating the shell, export the  variable in your shell profile. For example, if you want to use cyan, magenta, green, bold red and bold yellow, add the following line:</p><pre><code>export NVM_COLORS='cmgRY'\n</code></pre><h4>Suppressing colorized output</h4><p><code>nvm help (or -h or --help)</code>, ,  and  usually produce colorized output. You can disable colors with the  option (or by setting the environment variable ):</p><pre><code>nvm ls --no-colors\nnvm help --no-colors\nTERM=dumb nvm ls\n</code></pre><p>To restore your PATH, you can deactivate it:</p><p>To set a default Node version to be used in any new shell, use the alias 'default':</p><pre><code>nvm alias default node # this refers to the latest installed version of node\nnvm alias default 18 # this refers to the latest installed v18.x version of node\nnvm alias default 18.12  # this refers to the latest installed v18.12.x version of node\n</code></pre><h3>Use a mirror of node binaries</h3><p>To use a mirror of the node binaries, set :</p><pre><code>export NVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist\nnvm install node\n\nNVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist nvm install 4.2\n</code></pre><p>To use a mirror of the io.js binaries, set :</p><pre><code>export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist\nnvm install iojs-v1.0.3\n\nNVM_IOJS_ORG_MIRROR=https://iojs.org/dist nvm install iojs-v1.0.3\n</code></pre><p> will not, by default, create a \"current\" symlink. Set  to \"true\" to enable this behavior, which is sometimes useful for IDEs. Note that using  in multiple shell tabs with this environment variable enabled can cause race conditions.</p><h4>Pass Authorization header to mirror</h4><p>To pass an Authorization header through to the mirror url, set </p><pre><code>NVM_AUTH_HEADER=\"Bearer secret-token\" nvm install node\n</code></pre><p>You can create a  file containing a node version number (or any other string that  understands; see  for details) in the project root directory (or any parent directory). Afterwards, , , , , and  will use the version specified in the  file if no version is supplied on the command line.</p><p>For example, to make nvm default to the latest 5.9 release, the latest LTS version, or the latest node version for the current directory:</p><pre><code>$ echo \"5.9\" &gt; .nvmrc\n\n$ echo \"lts/*\" &gt; .nvmrc # to default to the latest LTS version\n\n$ echo \"node\" &gt; .nvmrc # to default to the latest version\n</code></pre><p>[NB these examples assume a POSIX-compliant shell version of . If you use a Windows  development environment, eg the  file is used to configure a remote Linux deployment, then keep in mind the s will be copied leading to an invalid file. Remove them.]</p><p>Then when you run nvm use:</p><pre><code>$ nvm use\nFound '/path/to/project/.nvmrc' with version &lt;5.9&gt;\nNow using node v5.9.1 (npm v3.7.3)\n</code></pre><p>Running nvm install will also switch over to the correct version, but if the correct node version isn't already installed, it will install it for you.</p><pre><code>$ nvm install\nFound '/path/to/project/.nvmrc' with version &lt;5.9&gt;\nDownloading and installing node v5.9.1...\nDownloading https://nodejs.org/dist/v5.9.1/node-v5.9.1-linux-x64.tar.xz...\n#################################################################################### 100.0%\nComputing checksum with sha256sum\nChecksums matched!\nNow using node v5.9.1 (npm v3.7.3)\n</code></pre><p> et. al. will traverse directory structure upwards from the current directory looking for the  file. In other words, running  et. al. in any subdirectory of a directory with an  will result in that  being utilized.</p><p>The contents of a  file  contain precisely one  (as described by ) followed by a newline.  files may also have comments. The comment delimiter is , and it and any text after it, as well as blank lines, and leading and trailing white space, will be ignored when parsing.</p><p>Key/value pairs using  are also allowed and ignored, but are reserved for future use, and may cause validation errors in the future.</p><p>Run <a href=\"https://npmjs.com/nvmrc\"></a> to validate an  file. If that tool’s results do not agree with nvm, one or the other has a bug - please file an issue.</p><p>You can use <a href=\"https://github.com/iamogbz/nvshim\"></a> to shim the , , and  bins to automatically use the  config in the current directory.  is  supported by the  maintainers. Please <a href=\"https://github.com/iamogbz/nvshim/issues/new\">report issues to the  team</a>.</p><p>If you prefer a lighter-weight solution, the recipes below have been contributed by  users. They are  supported by the  maintainers. We are, however, accepting pull requests for more examples.</p><h4>Calling  automatically in a directory with a  file</h4><p>In your profile (, , , or ), add the following to  whenever you enter a new directory:</p><p>Put the following at the end of your :</p><pre><code>cdnvm() {\n    command cd \"$@\" || return $?\n    nvm_path=\"$(nvm_find_up .nvmrc | command tr -d '\\n')\"\n\n    # If there are no .nvmrc file, use the default nvm version\n    if [[ ! $nvm_path = *[^[:space:]]* ]]; then\n\n        declare default_version\n        default_version=\"$(nvm version default)\"\n\n        # If there is no default version, set it to `node`\n        # This will use the latest version on your machine\n        if [ $default_version = 'N/A' ]; then\n            nvm alias default node\n            default_version=$(nvm version default)\n        fi\n\n        # If the current version is not the default version, set it to use the default version\n        if [ \"$(nvm current)\" != \"${default_version}\" ]; then\n            nvm use default\n        fi\n    elif [[ -s \"${nvm_path}/.nvmrc\" &amp;&amp; -r \"${nvm_path}/.nvmrc\" ]]; then\n        declare nvm_version\n        nvm_version=$(&lt;\"${nvm_path}\"/.nvmrc)\n\n        declare locally_resolved_nvm_version\n        # `nvm ls` will check all locally-available versions\n        # If there are multiple matching versions, take the latest one\n        # Remove the `-&gt;` and `*` characters and spaces\n        # `locally_resolved_nvm_version` will be `N/A` if no local versions are found\n        locally_resolved_nvm_version=$(nvm ls --no-colors \"${nvm_version}\" | command tail -1 | command tr -d '\\-&gt;*' | command tr -d '[:space:]')\n\n        # If it is not already installed, install it\n        # `nvm install` will implicitly use the newly-installed version\n        if [ \"${locally_resolved_nvm_version}\" = 'N/A' ]; then\n            nvm install \"${nvm_version}\";\n        elif [ \"$(nvm current)\" != \"${locally_resolved_nvm_version}\" ]; then\n            nvm use \"${nvm_version}\";\n        fi\n    fi\n}\n\nalias cd='cdnvm'\ncdnvm \"$PWD\" || exit\n</code></pre><p>This alias would search 'up' from your current directory in order to detect a  file. If it finds it, it will switch to that version; if not, it will use the default version.</p><p>This shell function will install (if needed) and  the specified Node version when an  is found, and  otherwise.</p><p>Put this into your  to call  automatically whenever you enter a directory that contains an  file with a string telling nvm which node to :</p><pre><code># place this after nvm initialization!\nautoload -U add-zsh-hook\n\nload-nvmrc() {\n  local nvmrc_path\n  nvmrc_path=\"$(nvm_find_nvmrc)\"\n\n  if [ -n \"$nvmrc_path\" ]; then\n    local nvmrc_node_version\n    nvmrc_node_version=$(nvm version \"$(cat \"${nvmrc_path}\")\")\n\n    if [ \"$nvmrc_node_version\" = \"N/A\" ]; then\n      nvm install\n    elif [ \"$nvmrc_node_version\" != \"$(nvm version)\" ]; then\n      nvm use\n    fi\n  elif [ -n \"$(PWD=$OLDPWD nvm_find_nvmrc)\" ] &amp;&amp; [ \"$(nvm version)\" != \"$(nvm version default)\" ]; then\n    echo \"Reverting to nvm default version\"\n    nvm use default\n  fi\n}\n\nadd-zsh-hook chpwd load-nvmrc\nload-nvmrc\n</code></pre><p>After saving the file, run  to reload the configuration with the latest changes made.</p><p>This requires that you have <a href=\"https://github.com/edc/bass\">bass</a> installed.</p><pre><code># ~/.config/fish/functions/nvm.fish\nfunction nvm\n  bass source ~/.nvm/nvm.sh --no-use ';' nvm $argv\nend\n\n# ~/.config/fish/functions/nvm_find_nvmrc.fish\nfunction nvm_find_nvmrc\n  bass source ~/.nvm/nvm.sh --no-use ';' nvm_find_nvmrc\nend\n\n# ~/.config/fish/functions/load_nvm.fish\nfunction load_nvm --on-variable=\"PWD\"\n  set -l default_node_version (nvm version default)\n  set -l node_version (nvm version)\n  set -l nvmrc_path (nvm_find_nvmrc)\n  if test -n \"$nvmrc_path\"\n    set -l nvmrc_node_version (nvm version (cat $nvmrc_path))\n    if test \"$nvmrc_node_version\" = \"N/A\"\n      nvm install (cat $nvmrc_path)\n    else if test \"$nvmrc_node_version\" != \"$node_version\"\n      nvm use $nvmrc_node_version\n    end\n  else if test \"$node_version\" != \"$default_node_version\"\n    echo \"Reverting to default Node version\"\n    nvm use default\n  end\nend\n\n# ~/.config/fish/config.fish\n# You must call it on initialization or listening to directory switching won't work\nload_nvm &gt; /dev/stderr\n</code></pre><p>Tests are written in <a href=\"https://git.sdf.org/tlevine/urchin\">Urchin</a>. Install Urchin (and other dependencies) like so:</p><p>There are slow tests and fast tests. The slow tests do things like install node and check that the right versions are used. The fast tests fake this to test things like aliases and uninstalling. From the root of the nvm git repository, run the fast tests like this:</p><p>Run the slow tests like this:</p><p>Run all of the tests like this:</p><p>Nota bene: Avoid running nvm while the tests are running.</p><p>nvm exposes the following environment variables:</p><ul><li> - nvm's installation directory.</li><li> - where node, npm, and global packages for the active version of node are installed.</li><li> - node's include file directory (useful for building C/C++ addons for node).</li><li> - used to maintain compatibility with zsh.</li><li> - version from .nvmrc file if being used.</li></ul><p>Additionally, nvm modifies , and, if present,  and  when changing versions.</p><p>To activate, you need to source :</p><pre><code>[[ -r $NVM_DIR/bash_completion ]] &amp;&amp; \\. $NVM_DIR/bash_completion\n</code></pre><p>Put the above sourcing line just below the sourcing line for nvm in your profile (, ).</p><pre><code>alias               deactivate          install             list-remote         reinstall-packages  uninstall           version\ncache               exec                install-latest-npm  ls                  run                 unload              version-remote\ncurrent             help                list                ls-remote           unalias             use                 which\n</code></pre><pre><code>default      iojs         lts/*        lts/argon    lts/boron    lts/carbon   lts/dubnium  lts/erbium   node         stable       unstable\n</code></pre><pre><code>v10.22.0       v12.18.3      v14.8.0\n</code></pre><pre><code>my_alias        default        v10.22.0       v12.18.3      v14.8.0\n</code></pre><pre><code>my_alias        default        v10.22.0       v12.18.3      v14.8.0\n</code></pre><p> will encounter some issues if you have some non-default settings set. (see <a href=\"https://github.com/nvm-sh/nvm/issues/606\">#606</a>) The following are known to cause issues:</p><pre><code>$NPM_CONFIG_PREFIX\n$PREFIX\n</code></pre><h2>Installing nvm on Alpine Linux</h2><p>In order to provide the best performance (and other optimizations), nvm will download and install pre-compiled binaries for Node (and npm) when you run . The Node project compiles, tests and hosts/provides these pre-compiled binaries which are built for mainstream/traditional Linux distributions (such as Debian, Ubuntu, CentOS, RedHat et al).</p><p>Alpine Linux, unlike mainstream/traditional Linux distributions, is based on <a href=\"https://www.busybox.net/\">BusyBox</a>, a very compact (~5MB) Linux distribution. BusyBox (and thus Alpine Linux) uses a different C/C++ stack to most mainstream/traditional Linux distributions - <a href=\"https://www.musl-libc.org/\">musl</a>. This makes binary programs built for such mainstream/traditional incompatible with Alpine Linux, thus we cannot simply  on Alpine Linux and expect the downloaded binary to run correctly - you'll likely see \"...does not exist\" errors if you try that.</p><p>There is a  flag for  which requests nvm download Node source and compile it locally.</p><p>If installing nvm on Alpine Linux  still what you want or need to do, you should be able to achieve this by running the following from you Alpine Linux shell, depending on which version you are using:</p><pre><code>apk add -U curl bash ca-certificates openssl ncurses coreutils python3 make gcc g++ libgcc linux-headers grep util-linux binutils findutils\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><pre><code>apk add -U curl bash ca-certificates openssl ncurses coreutils python2 make gcc g++ libgcc linux-headers grep util-linux binutils findutils\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><p><em>Note: Alpine 3.5 can only install NodeJS versions up to v6.9.5, Alpine 3.6 can only install versions up to v6.10.3, Alpine 3.7 installs versions up to v8.9.3, Alpine 3.8 installs versions up to v8.14.0, Alpine 3.9 installs versions up to v10.19.0, Alpine 3.10 installs versions up to v10.24.1, Alpine 3.11 installs versions up to v12.22.6, Alpine 3.12 installs versions up to v12.22.12, Alpine 3.13 &amp; 3.14 install versions up to v14.20.0, Alpine 3.15 &amp; 3.16 install versions up to v16.16.0 (<strong>These are all versions on the main branch</strong>). Alpine 3.5 - 3.12 required the package  to build NodeJS, as they are older versions to build. Alpine 3.13+ requires  to successfully build newer NodeJS versions, but you can use  with Alpine 3.13+ if you need to build versions of node supported in Alpine 3.5 - 3.15, you just need to specify what version of NodeJS you need to install in the package install script.</em></p><p>The Node project has some desire but no concrete plans (due to the overheads of building, testing and support) to offer Alpine-compatible binaries.</p><p>To remove  manually, execute the following:</p><p>First, use  to remove the nvm command from your terminal session and delete the installation directory:</p><pre><code>$ nvm_dir=\"${NVM_DIR:-~/.nvm}\"\n$ nvm unload\n$ rm -rf \"$nvm_dir\"\n</code></pre><p>Edit  (or other shell resource config) and remove the lines below:</p><pre><code>export NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n[[ -r $NVM_DIR/bash_completion ]] &amp;&amp; \\. $NVM_DIR/bash_completion\n</code></pre><h2>Docker For Development Environment</h2><p>To make the development and testing work easier, we have a Dockerfile for development usage, which is based on Ubuntu 18.04 base image, prepared with essential and useful tools for  development, to build the docker image of the environment, run the docker command at the root of  repository:</p><pre><code>$ docker build -t nvm-dev .\n</code></pre><p>This will package your current nvm repository with our pre-defined development environment into a docker image named , once it's built with success, validate your image via :</p><pre><code>$ docker images\n\nREPOSITORY         TAG                 IMAGE ID            CREATED             SIZE\nnvm-dev            latest              9ca4c57a97d8        7 days ago          650 MB\n</code></pre><p>If you got no error message, now you can easily involve in:</p><pre><code>$ docker run -h nvm-dev -it nvm-dev\n\nnvm@nvm-dev:~/.nvm$\n</code></pre><p>Please note that it'll take about 8 minutes to build the image and the image size would be about 650MB, so it's not suitable for production usage.</p><p>For more information and documentation about docker, please refer to its official website:</p><ul><li><p>If you try to install a node version and the installation fails, be sure to run  to delete cached node downloads, or you might get an error like the following:</p><p>curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.</p></li><li><p>Where's my ? Check out <a href=\"https://github.com/nvm-sh/nvm/issues/43\">#43</a></p></li><li><p>After the v0.8.6 release of node, nvm tries to install from binary packages. But in some systems, the official binary packages don't work due to incompatibility of shared libs. In such cases, use  option to force install from source:</p></li></ul><ul><li>If setting the  alias does not establish the node version in new shells (i.e.  yields ), ensure that the system's node  is set before the  source line in your shell profile (see <a href=\"https://github.com/nvm-sh/nvm/issues/658\">#658</a>)</li></ul><p><strong>nvm node version not found in vim shell</strong></p><p>If you set node version to a version other than your system node version  and open vim and run  you should see  if you see your system version . You need to run:</p><pre><code>sudo chmod ugo-x /usr/libexec/path_helper\n</code></pre><p><strong>nvm is not compatible with the npm config \"prefix\" option</strong></p><p>Some solutions for this issue can be found <a href=\"https://github.com/nvm-sh/nvm/issues/1245\">here</a></p><p>There is one more edge case causing this issue, and that's a <strong>mismatch between the  path and the user's home directory's actual name</strong>.</p><p>You have to make sure that the user directory name in  and the user directory name you'd see from running <strong>are capitalized the same way</strong> (<a href=\"https://github.com/nvm-sh/nvm/issues/2261\">See this issue</a>).</p><p>To change the user directory and/or account name follow the instructions <a href=\"https://support.apple.com/en-us/HT201548\">here</a></p><p><strong>Homebrew makes zsh directories unsecure</strong></p><pre><code>zsh compinit: insecure directories, run compaudit for list.\nIgnore insecure directories and continue [y] or abort compinit [n]? y\n</code></pre><p>Homebrew causes insecure directories like <code>/usr/local/share/zsh/site-functions</code> and . This is  an  problem - it is a homebrew problem. Refer <a href=\"https://github.com/zsh-users/zsh-completions/issues/680\">here</a> for some solutions related to the issue.</p><p><strong>Macs with Apple Silicon chips</strong></p><p>Experimental support for the Apple Silicon chip architecture was added in node.js v15.3 and full support was added in v16.0. Because of this, if you try to install older versions of node as usual, you will probably experience either compilation errors when installing node or out-of-memory errors while running your code.</p><p>So, if you want to run a version prior to v16.0 on an Apple Silicon Mac, it may be best to compile node targeting the  Intel architecture so that Rosetta 2 can translate the  processor instructions to ARM-based Apple Silicon instructions. Here's what you will need to do:</p><ul><li><p>Install Rosetta, if you haven't already done so</p><pre><code>$ softwareupdate --install-rosetta\n</code></pre><p>You might wonder, \"how will my Apple Silicon Mac know to use Rosetta for a version of node compiled for an Intel chip?\". If an executable contains only Intel instructions, macOS will automatically use Rosetta to translate the instructions.</p></li><li><p>Open a shell that's running using Rosetta</p><p>Note: This same thing can also be accomplished by finding the Terminal or iTerm App in Finder, right clicking, selecting \"Get Info\", and then checking the box labeled \"Open using Rosetta\".</p><p>Note: This terminal session is now running in . If  is not the shell you typically use,  may not be 'd automatically like it probably is for your usual shell through your dotfiles. If that's the case, make sure to source .</p><pre><code>$ source \"${NVM_DIR}/nvm.sh\"\n</code></pre></li><li><p>Install whatever older version of node you are interested in. Let's use 12.22.1 as an example. This will fetch the node source code and compile it, which will take several minutes.</p><pre><code>$ nvm install v12.22.1 --shared-zlib\n</code></pre><p>Note: You're probably curious why  is included. There's a bug in recent versions of Apple's system  compiler. If one of these broken versions is installed on your system, the above step will likely still succeed even if you didn't include the  flag. However, later, when you attempt to  something using your old version of node.js, you will see  errors. If you want to avoid the possible hassle of dealing with this, include that flag. For more details, see <a href=\"https://github.com/nodejs/node/issues/39313\">this issue</a> and <a href=\"https://github.com/nodejs/node/issues/39313#issuecomment-90.40.376\">this comment</a></p></li><li><p>Exit back to your native shell.</p><p>Note: If you selected the box labeled \"Open using Rosetta\" rather than running the CLI command in the second step, you will see  here. Unless you have another reason to have that box selected, you can deselect it now.</p></li><li><p>Check to make sure the architecture is correct.  is the abbreviation for , which is what you want to see.</p><pre><code>$ node -p process.arch\nx64\n</code></pre></li></ul><p>Now you should be able to use node as usual.</p><p>If you've encountered this error on WSL-2:</p><pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload  Total   Spent    Left  Speed\n0     0    0     0    0     0      0      0 --:--:--  0:00:09 --:--:--     0curl: (6) Could not resolve host: raw.githubusercontent.com\n</code></pre><p>It may be due to your antivirus, VPN, or other reasons.</p><p>Where you can  while you can't </p><p>This could simply be solved by running this in your root directory:</p><pre><code>sudo rm /etc/resolv.conf\nsudo bash -c 'echo \"nameserver 8.8.8.8\" &gt; /etc/resolv.conf'\nsudo bash -c 'echo \"[network]\" &gt; /etc/wsl.conf'\nsudo bash -c 'echo \"generateResolvConf = false\" &gt;&gt; /etc/wsl.conf'\nsudo chattr +i /etc/resolv.conf\n</code></pre><p>This deletes your  file that is automatically generated when you run WSL, creates a new file and puts , then creates a  file and adds  and <code>generateResolveConf = false</code> to prevent auto-generation of that file.</p><p>You can check the contents of the file by running:</p><p>Currently, the sole maintainer is <a href=\"https://github.com/ljharb\">@ljharb</a> - more maintainers are quite welcome, and we hope to add folks to the team over time. <a href=\"https://raw.githubusercontent.com/nvm-sh/nvm/master/GOVERNANCE.md\">Governance</a> will be re-evaluated as the project evolves.</p><p>Only the latest version (v0.40.3 at this time) is supported.</p><p>If you are unable to update to the latest version of , our <a href=\"https://openjsf.org/ecosystem-sustainability-program\">partners</a> provide commercial security fixes for all unsupported versions:</p>","contentLength":35843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"yhirose/cpp-httplib","url":"https://github.com/yhirose/cpp-httplib","date":1761878103,"author":"","guid":322483,"unread":true,"content":"<p>A C++ header-only HTTP/HTTPS server and client library</p><p>A C++11 single-file header-only cross platform HTTP/HTTPS library.</p><p>It's extremely easy to set up. Just include the  file in your code!</p><blockquote><p>[!IMPORTANT] This library uses 'blocking' socket I/O. If you are looking for a library with 'non-blocking' socket I/O, this is not the one that you want.</p></blockquote><pre><code>#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\n// HTTP\nhttplib::Server svr;\n\n// HTTPS\nhttplib::SSLServer svr;\n\nsvr.Get(\"/hi\", [](const httplib::Request &amp;, httplib::Response &amp;res) {\n  res.set_content(\"Hello World!\", \"text/plain\");\n});\n\nsvr.listen(\"0.0.0.0\", 8080);\n</code></pre><pre><code>#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\n// HTTP\nhttplib::Client cli(\"http://yhirose.github.io\");\n\n// HTTPS\nhttplib::Client cli(\"https://yhirose.github.io\");\n\nauto res = cli.Get(\"/hi\");\nres-&gt;status;\nres-&gt;body;\n</code></pre><p>SSL support is available with <code>CPPHTTPLIB_OPENSSL_SUPPORT</code>.  and  should be linked.</p><blockquote><p>[!NOTE] cpp-httplib currently supports only version 3.0 or later. Please see <a href=\"https://www.openssl.org/policies/releasestrat.html\">this page</a> to get more information.</p></blockquote><blockquote><p>[!TIP] For macOS: cpp-httplib now can use system certs with <code>CPPHTTPLIB_USE_CERTS_FROM_MACOSX_KEYCHAIN</code>.  and  should be linked with .</p></blockquote><pre><code>#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\n// Server\nhttplib::SSLServer svr(\"./cert.pem\", \"./key.pem\");\n\n// Client\nhttplib::Client cli(\"https://localhost:1234\"); // scheme + host\nhttplib::SSLClient cli(\"localhost:1234\"); // host\nhttplib::SSLClient cli(\"localhost\", 1234); // host, port\n\n// Use your CA bundle\ncli.set_ca_cert_path(\"./ca-bundle.crt\");\n\n// Disable cert verification\ncli.enable_server_certificate_verification(false);\n\n// Disable host verification\ncli.enable_server_hostname_verification(false);\n</code></pre><blockquote><p>[!NOTE] When using SSL, it seems impossible to avoid SIGPIPE in all cases, since on some operating systems, SIGPIPE can only be suppressed on a per-message basis, but there is no way to make the OpenSSL library do so for its internal communications. If your program needs to avoid being terminated on SIGPIPE, the only fully general way might be to set up a signal handler for SIGPIPE to handle or ignore it yourself.</p></blockquote><p>When SSL operations fail, cpp-httplib provides detailed error information through two separate error fields:</p><pre><code>#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\nhttplib::Client cli(\"https://example.com\");\n\nauto res = cli.Get(\"/\");\nif (!res) {\n  // Check the error type\n  const auto err = res.error();\n\n  switch (err) {\n    case httplib::Error::SSLConnection:\n      std::cout &lt;&lt; \"SSL connection failed, SSL error: \"\n                &lt;&lt; res.ssl_error() &lt;&lt; std::endl;\n      break;\n\n    case httplib::Error::SSLLoadingCerts:\n      std::cout &lt;&lt; \"SSL cert loading failed, OpenSSL error: \"\n                &lt;&lt; std::hex &lt;&lt; res.ssl_openssl_error() &lt;&lt; std::endl;\n      break;\n\n    case httplib::Error::SSLServerVerification:\n      std::cout &lt;&lt; \"SSL verification failed, X509 error: \"\n                &lt;&lt; res.ssl_openssl_error() &lt;&lt; std::endl;\n      break;\n\n    case httplib::Error::SSLServerHostnameVerification:\n      std::cout &lt;&lt; \"SSL hostname verification failed, X509 error: \"\n                &lt;&lt; res.ssl_openssl_error() &lt;&lt; std::endl;\n      break;\n\n    default:\n      std::cout &lt;&lt; \"HTTP error: \" &lt;&lt; httplib::to_string(err) &lt;&lt; std::endl;\n  }\n}\n</code></pre><pre><code>#include &lt;httplib.h&gt;\n\nint main(void)\n{\n  using namespace httplib;\n\n  Server svr;\n\n  svr.Get(\"/hi\", [](const Request&amp; req, Response&amp; res) {\n    res.set_content(\"Hello World!\", \"text/plain\");\n  });\n\n  // Match the request path against a regular expression\n  // and extract its captures\n  svr.Get(R\"(/numbers/(\\d+))\", [&amp;](const Request&amp; req, Response&amp; res) {\n    auto numbers = req.matches[1];\n    res.set_content(numbers, \"text/plain\");\n  });\n\n  // Capture the second segment of the request path as \"id\" path param\n  svr.Get(\"/users/:id\", [&amp;](const Request&amp; req, Response&amp; res) {\n    auto user_id = req.path_params.at(\"id\");\n    res.set_content(user_id, \"text/plain\");\n  });\n\n  // Extract values from HTTP headers and URL query params\n  svr.Get(\"/body-header-param\", [](const Request&amp; req, Response&amp; res) {\n    if (req.has_header(\"Content-Length\")) {\n      auto val = req.get_header_value(\"Content-Length\");\n    }\n    if (req.has_param(\"key\")) {\n      auto val = req.get_param_value(\"key\");\n    }\n    res.set_content(req.body, \"text/plain\");\n  });\n\n  // If the handler takes time to finish, you can also poll the connection state\n  svr.Get(\"/task\", [&amp;](const Request&amp; req, Response&amp; res) {\n    const char * result = nullptr;\n    process.run(); // for example, starting an external process\n    while (result == nullptr) {\n      sleep(1);\n      if (req.is_connection_closed()) {\n        process.kill(); // kill the process\n        return;\n      }\n      result = process.stdout(); // != nullptr if the process finishes\n    }\n    res.set_content(result, \"text/plain\");\n  });\n\n  svr.Get(\"/stop\", [&amp;](const Request&amp; req, Response&amp; res) {\n    svr.stop();\n  });\n\n  svr.listen(\"localhost\", 1234);\n}\n</code></pre><p>, ,  and  methods are also supported.</p><h3>Bind a socket to multiple interfaces and any available port</h3><pre><code>int port = svr.bind_to_any_port(\"0.0.0.0\");\nsvr.listen_after_bind();\n</code></pre><pre><code>// Mount / to ./www directory\nauto ret = svr.set_mount_point(\"/\", \"./www\");\nif (!ret) {\n  // The specified base directory doesn't exist...\n}\n\n// Mount /public to ./www directory\nret = svr.set_mount_point(\"/public\", \"./www\");\n\n// Mount /public to ./www1 and ./www2 directories\nret = svr.set_mount_point(\"/public\", \"./www1\"); // 1st order to search\nret = svr.set_mount_point(\"/public\", \"./www2\"); // 2nd order to search\n\n// Remove mount /\nret = svr.remove_mount_point(\"/\");\n\n// Remove mount /public\nret = svr.remove_mount_point(\"/public\");\n</code></pre><pre><code>// User defined file extension and MIME type mappings\nsvr.set_file_extension_and_mimetype_mapping(\"cc\", \"text/x-c\");\nsvr.set_file_extension_and_mimetype_mapping(\"cpp\", \"text/x-c\");\nsvr.set_file_extension_and_mimetype_mapping(\"hh\", \"text/x-h\");\n</code></pre><p>The following are built-in mappings:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td align=\"left\">application/x-7z-compressed</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><blockquote><p>[!WARNING] These static file server methods are not thread-safe.</p></blockquote><pre><code>// The handler is called right before the response is sent to a client\nsvr.set_file_request_handler([](const Request &amp;req, Response &amp;res) {\n  ...\n});\n</code></pre><p>cpp-httplib provides separate logging capabilities for access logs and error logs, similar to web servers like Nginx and Apache.</p><p>Access loggers capture successful HTTP requests and responses:</p><pre><code>svr.set_logger([](const httplib::Request&amp; req, const httplib::Response&amp; res) {\n  std::cout &lt;&lt; req.method &lt;&lt; \" \" &lt;&lt; req.path &lt;&lt; \" -&gt; \" &lt;&lt; res.status &lt;&lt; std::endl;\n});\n</code></pre><p>You can also set a pre-compression logger to capture request/response data before compression is applied:</p><pre><code>svr.set_pre_compression_logger([](const httplib::Request&amp; req, const httplib::Response&amp; res) {\n  // Log before compression - res.body contains uncompressed content\n  // Content-Encoding header is not yet set\n  your_pre_compression_logger(req, res);\n});\n</code></pre><p>The pre-compression logger is only called when compression would be applied. For responses without compression, only the access logger is called.</p><p>Error loggers capture failed requests and connection issues. Unlike access loggers, error loggers only receive the Error and Request information, as errors typically occur before a meaningful Response can be generated.</p><pre><code>svr.set_error_logger([](const httplib::Error&amp; err, const httplib::Request* req) {\n  std::cerr &lt;&lt; httplib::to_string(err) &lt;&lt; \" while processing request\";\n  if (req) {\n    std::cerr &lt;&lt; \", client: \" &lt;&lt; req-&gt;get_header_value(\"X-Forwarded-For\")\n              &lt;&lt; \", request: '\" &lt;&lt; req-&gt;method &lt;&lt; \" \" &lt;&lt; req-&gt;path &lt;&lt; \" \" &lt;&lt; req-&gt;version &lt;&lt; \"'\"\n              &lt;&lt; \", host: \" &lt;&lt; req-&gt;get_header_value(\"Host\");\n  }\n  std::cerr &lt;&lt; std::endl;\n});\n</code></pre><pre><code>svr.set_error_handler([](const auto&amp; req, auto&amp; res) {\n  auto fmt = \"&lt;p&gt;Error Status: &lt;span style='color:red;'&gt;%d&lt;/span&gt;&lt;/p&gt;\";\n  char buf[BUFSIZ];\n  snprintf(buf, sizeof(buf), fmt, res.status);\n  res.set_content(buf, \"text/html\");\n});\n</code></pre><p>The exception handler gets called if a user routing handler throws an error.</p><pre><code>svr.set_exception_handler([](const auto&amp; req, auto&amp; res, std::exception_ptr ep) {\n  auto fmt = \"&lt;h1&gt;Error 500&lt;/h1&gt;&lt;p&gt;%s&lt;/p&gt;\";\n  char buf[BUFSIZ];\n  try {\n    std::rethrow_exception(ep);\n  } catch (std::exception &amp;e) {\n    snprintf(buf, sizeof(buf), fmt, e.what());\n  } catch (...) { // See the following NOTE\n    snprintf(buf, sizeof(buf), fmt, \"Unknown Exception\");\n  }\n  res.set_content(buf, \"text/html\");\n  res.status = StatusCode::InternalServerError_500;\n});\n</code></pre><blockquote><p>[!CAUTION] if you don't provide the  block for a rethrown exception pointer, an uncaught exception will end up causing the server crash. Be careful!</p></blockquote><pre><code>svr.set_pre_routing_handler([](const auto&amp; req, auto&amp; res) {\n  if (req.path == \"/hello\") {\n    res.set_content(\"world\", \"text/html\");\n    return Server::HandlerResponse::Handled;\n  }\n  return Server::HandlerResponse::Unhandled;\n});\n</code></pre><pre><code>svr.set_post_routing_handler([](const auto&amp; req, auto&amp; res) {\n  res.set_header(\"ADDITIONAL_HEADER\", \"value\");\n});\n</code></pre><pre><code>svr.set_pre_request_handler([](const auto&amp; req, auto&amp; res) {\n  if (req.matched_route == \"/user/:user\") {\n    auto user = req.path_params.at(\"user\");\n    if (user != \"john\") {\n      res.status = StatusCode::Forbidden_403;\n      res.set_content(\"error\", \"text/html\");\n      return Server::HandlerResponse::Handled;\n    }\n  }\n  return Server::HandlerResponse::Unhandled;\n});\n</code></pre><h4>URL-encoded form data ('application/x-www-form-urlencoded')</h4><pre><code>svr.Post(\"/form\", [&amp;](const auto&amp; req, auto&amp; res) {\n  // URL query parameters and form-encoded data are accessible via req.params\n  std::string username = req.get_param_value(\"username\");\n  std::string password = req.get_param_value(\"password\");\n\n  // Handle multiple values with same name\n  auto interests = req.get_param_values(\"interests\");\n\n  // Check existence\n  if (req.has_param(\"newsletter\")) {\n    // Handle newsletter subscription\n  }\n});\n</code></pre><h4>'multipart/form-data' POST data</h4><pre><code>svr.Post(\"/multipart\", [&amp;](const Request&amp; req, Response&amp; res) {\n  // Access text fields (from form inputs without files)\n  std::string username = req.form.get_field(\"username\");\n  std::string bio = req.form.get_field(\"bio\");\n\n  // Access uploaded files\n  if (req.form.has_file(\"avatar\")) {\n    const auto&amp; file = req.form.get_file(\"avatar\");\n    std::cout &lt;&lt; \"Uploaded file: \" &lt;&lt; file.filename\n              &lt;&lt; \" (\" &lt;&lt; file.content_type &lt;&lt; \") - \"\n              &lt;&lt; file.content.size() &lt;&lt; \" bytes\" &lt;&lt; std::endl;\n\n    // Access additional headers if needed\n    for (const auto&amp; header : file.headers) {\n      std::cout &lt;&lt; \"Header: \" &lt;&lt; header.first &lt;&lt; \" = \" &lt;&lt; header.second &lt;&lt; std::endl;\n    }\n\n    // Save to disk\n    std::ofstream ofs(file.filename, std::ios::binary);\n    ofs &lt;&lt; file.content;\n  }\n\n  // Handle multiple values with same name\n  auto tags = req.form.get_fields(\"tags\");  // e.g., multiple checkboxes\n  for (const auto&amp; tag : tags) {\n    std::cout &lt;&lt; \"Tag: \" &lt;&lt; tag &lt;&lt; std::endl;\n  }\n\n  auto documents = req.form.get_files(\"documents\");  // multiple file upload\n  for (const auto&amp; doc : documents) {\n    std::cout &lt;&lt; \"Document: \" &lt;&lt; doc.filename\n              &lt;&lt; \" (\" &lt;&lt; doc.content.size() &lt;&lt; \" bytes)\" &lt;&lt; std::endl;\n  }\n\n  // Check existence before accessing\n  if (req.form.has_field(\"newsletter\")) {\n    std::cout &lt;&lt; \"Newsletter subscription: \" &lt;&lt; req.form.get_field(\"newsletter\") &lt;&lt; std::endl;\n  }\n\n  // Get counts for validation\n  if (req.form.get_field_count(\"tags\") &gt; 5) {\n    res.status = StatusCode::BadRequest_400;\n    res.set_content(\"Too many tags\", \"text/plain\");\n    return;\n  }\n\n  // Summary\n  std::cout &lt;&lt; \"Received \" &lt;&lt; req.form.fields.size() &lt;&lt; \" text fields and \"\n            &lt;&lt; req.form.files.size() &lt;&lt; \" files\" &lt;&lt; std::endl;\n\n  res.set_content(\"Upload successful\", \"text/plain\");\n});\n</code></pre><h3>Receive content with a content receiver</h3><pre><code>svr.Post(\"/content_receiver\",\n  [&amp;](const Request &amp;req, Response &amp;res, const ContentReader &amp;content_reader) {\n    if (req.is_multipart_form_data()) {\n      // NOTE: `content_reader` is blocking until every form data field is read\n      // This approach allows streaming processing of large files\n      std::vector&lt;FormData&gt; items;\n      content_reader(\n        [&amp;](const FormData &amp;item) {\n          items.push_back(item);\n          return true;\n        },\n        [&amp;](const char *data, size_t data_length) {\n          items.back().content.append(data, data_length);\n          return true;\n        });\n\n      // Process the received items\n      for (const auto&amp; item : items) {\n        if (item.filename.empty()) {\n          // Text field\n          std::cout &lt;&lt; \"Field: \" &lt;&lt; item.name &lt;&lt; \" = \" &lt;&lt; item.content &lt;&lt; std::endl;\n        } else {\n          // File\n          std::cout &lt;&lt; \"File: \" &lt;&lt; item.name &lt;&lt; \" (\" &lt;&lt; item.filename &lt;&lt; \") - \"\n                    &lt;&lt; item.content.size() &lt;&lt; \" bytes\" &lt;&lt; std::endl;\n        }\n      }\n    } else {\n      std::string body;\n      content_reader([&amp;](const char *data, size_t data_length) {\n        body.append(data, data_length);\n        return true;\n      });\n    }\n  });\n</code></pre><h3>Send content with the content provider</h3><pre><code>const size_t DATA_CHUNK_SIZE = 4;\n\nsvr.Get(\"/stream\", [&amp;](const Request &amp;req, Response &amp;res) {\n  auto data = new std::string(\"abcdefg\");\n\n  res.set_content_provider(\n    data-&gt;size(), // Content length\n    \"text/plain\", // Content type\n    [&amp;, data](size_t offset, size_t length, DataSink &amp;sink) {\n      const auto &amp;d = *data;\n      sink.write(&amp;d[offset], std::min(length, DATA_CHUNK_SIZE));\n      return true; // return 'false' if you want to cancel the process.\n    },\n    [data](bool success) { delete data; });\n});\n</code></pre><pre><code>svr.Get(\"/stream\", [&amp;](const Request &amp;req, Response &amp;res) {\n  res.set_content_provider(\n    \"text/plain\", // Content type\n    [&amp;](size_t offset, DataSink &amp;sink) {\n      if (/* there is still data */) {\n        std::vector&lt;char&gt; data;\n        // prepare data...\n        sink.write(data.data(), data.size());\n      } else {\n        sink.done(); // No more data\n      }\n      return true; // return 'false' if you want to cancel the process.\n    });\n});\n</code></pre><h3>Chunked transfer encoding</h3><pre><code>svr.Get(\"/chunked\", [&amp;](const Request&amp; req, Response&amp; res) {\n  res.set_chunked_content_provider(\n    \"text/plain\",\n    [](size_t offset, DataSink &amp;sink) {\n      sink.write(\"123\", 3);\n      sink.write(\"345\", 3);\n      sink.write(\"789\", 3);\n      sink.done(); // No more data\n      return true; // return 'false' if you want to cancel the process.\n    }\n  );\n});\n</code></pre><pre><code>svr.Get(\"/chunked\", [&amp;](const Request&amp; req, Response&amp; res) {\n  res.set_header(\"Trailer\", \"Dummy1, Dummy2\");\n  res.set_chunked_content_provider(\n    \"text/plain\",\n    [](size_t offset, DataSink &amp;sink) {\n      sink.write(\"123\", 3);\n      sink.write(\"345\", 3);\n      sink.write(\"789\", 3);\n      sink.done_with_trailer({\n        {\"Dummy1\", \"DummyVal1\"},\n        {\"Dummy2\", \"DummyVal2\"}\n      });\n      return true;\n    }\n  );\n});\n</code></pre><pre><code>svr.Get(\"/content\", [&amp;](const Request &amp;req, Response &amp;res) {\n  res.set_file_content(\"./path/to/content.html\");\n});\n\nsvr.Get(\"/content\", [&amp;](const Request &amp;req, Response &amp;res) {\n  res.set_file_content(\"./path/to/content\", \"text/html\");\n});\n</code></pre><h3>'Expect: 100-continue' handler</h3><p>By default, the server sends a  response for an  header.</p><pre><code>// Send a '417 Expectation Failed' response.\nsvr.set_expect_100_continue_handler([](const Request &amp;req, Response &amp;res) {\n  return StatusCode::ExpectationFailed_417;\n});\n</code></pre><pre><code>// Send a final status without reading the message body.\nsvr.set_expect_100_continue_handler([](const Request &amp;req, Response &amp;res) {\n  return res.status = StatusCode::Unauthorized_401;\n});\n</code></pre><pre><code>svr.set_keep_alive_max_count(2); // Default is 100\nsvr.set_keep_alive_timeout(10);  // Default is 5\n</code></pre><pre><code>svr.set_read_timeout(5, 0); // 5 seconds\nsvr.set_write_timeout(5, 0); // 5 seconds\nsvr.set_idle_interval(0, 100000); // 100 milliseconds\n</code></pre><h3>Set maximum payload length for reading a request body</h3><pre><code>svr.set_payload_max_length(1024 * 1024 * 512); // 512MB\n</code></pre><blockquote><p>[!NOTE] When the request body content type is 'www-form-urlencoded', the actual payload length shouldn't exceed <code>CPPHTTPLIB_FORM_URL_ENCODED_PAYLOAD_MAX_LENGTH</code>.</p></blockquote><h3>Default thread pool support</h3><p> is used as the  task queue, with a default thread count of 8 or <code>std::hardware_concurrency() - 1</code>, whichever is greater. You can change it with <code>CPPHTTPLIB_THREAD_POOL_COUNT</code>.</p><p>If you want to set the thread count at runtime, there is no convenient way... But here is how.</p><pre><code>svr.new_task_queue = [] { return new ThreadPool(12); };\n</code></pre><p>You can also provide an optional parameter to limit the maximum number of pending requests, i.e. requests ed by the listener but still waiting to be serviced by worker threads.</p><pre><code>svr.new_task_queue = [] { return new ThreadPool(/*num_threads=*/12, /*max_queued_requests=*/18); };\n</code></pre><p>Default limit is 0 (unlimited). Once the limit is reached, the listener will shutdown the client connection.</p><h3>Override the default thread pool with yours</h3><p>You can supply your own thread pool implementation according to your need.</p><pre><code>class YourThreadPoolTaskQueue : public TaskQueue {\npublic:\n  YourThreadPoolTaskQueue(size_t n) {\n    pool_.start_with_thread_count(n);\n  }\n\n  virtual bool enqueue(std::function&lt;void()&gt; fn) override {\n    /* Return true if the task was actually enqueued, or false\n     * if the caller must drop the corresponding connection. */\n    return pool_.enqueue(fn);\n  }\n\n  virtual void shutdown() override {\n    pool_.shutdown_gracefully();\n  }\n\nprivate:\n  YourThreadPool pool_;\n};\n\nsvr.new_task_queue = [] {\n  return new YourThreadPoolTaskQueue(12);\n};\n</code></pre><pre><code>#include &lt;httplib.h&gt;\n#include &lt;iostream&gt;\n\nint main(void)\n{\n  httplib::Client cli(\"localhost\", 1234);\n\n  if (auto res = cli.Get(\"/hi\")) {\n    if (res-&gt;status == StatusCode::OK_200) {\n      std::cout &lt;&lt; res-&gt;body &lt;&lt; std::endl;\n    }\n  } else {\n    auto err = res.error();\n    std::cout &lt;&lt; \"HTTP error: \" &lt;&lt; httplib::to_string(err) &lt;&lt; std::endl;\n  }\n}\n</code></pre><blockquote><p>[!TIP] Constructor with scheme-host-port string is now supported!</p></blockquote><pre><code>httplib::Client cli(\"localhost\");\nhttplib::Client cli(\"localhost:8080\");\nhttplib::Client cli(\"http://localhost\");\nhttplib::Client cli(\"http://localhost:8080\");\nhttplib::Client cli(\"https://localhost\");\nhttplib::SSLClient cli(\"localhost\");\n</code></pre><p>Here is the list of errors from .</p><pre><code>enum Error {\n  Success = 0,\n  Unknown,\n  Connection,\n  BindIPAddress,\n  Read,\n  Write,\n  ExceedRedirectCount,\n  Canceled,\n  SSLConnection,\n  SSLLoadingCerts,\n  SSLServerVerification,\n  SSLServerHostnameVerification,\n  UnsupportedMultipartBoundaryChars,\n  Compression,\n  ConnectionTimeout,\n  ProxyConnection,\n};\n</code></pre><pre><code>cli.set_logger([](const httplib::Request&amp; req, const httplib::Response&amp; res) {\n  auto duration = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(\n    std::chrono::steady_clock::now() - start_time).count();\n  std::cout &lt;&lt; \"✓ \" &lt;&lt; req.method &lt;&lt; \" \" &lt;&lt; req.path\n            &lt;&lt; \" -&gt; \" &lt;&lt; res.status &lt;&lt; \" (\" &lt;&lt; res.body.size() &lt;&lt; \" bytes, \"\n            &lt;&lt; duration &lt;&lt; \"ms)\" &lt;&lt; std::endl;\n});\n</code></pre><pre><code>cli.set_error_logger([](const httplib::Error&amp; err, const httplib::Request* req) {\n  std::cerr &lt;&lt; \"✗ \";\n  if (req) {\n    std::cerr &lt;&lt; req-&gt;method &lt;&lt; \" \" &lt;&lt; req-&gt;path &lt;&lt; \" \";\n  }\n  std::cerr &lt;&lt; \"failed: \" &lt;&lt; httplib::to_string(err);\n\n  // Add specific guidance based on error type\n  switch (err) {\n    case httplib::Error::Connection:\n      std::cerr &lt;&lt; \" (verify server is running and reachable)\";\n      break;\n    case httplib::Error::SSLConnection:\n      std::cerr &lt;&lt; \" (check SSL certificate and TLS configuration)\";\n      break;\n    case httplib::Error::ConnectionTimeout:\n      std::cerr &lt;&lt; \" (increase timeout or check network latency)\";\n      break;\n    case httplib::Error::Read:\n      std::cerr &lt;&lt; \" (server may have closed connection prematurely)\";\n      break;\n    default:\n      break;\n  }\n  std::cerr &lt;&lt; std::endl;\n});\n</code></pre><pre><code>httplib::Headers headers = {\n  { \"Hello\", \"World!\" }\n};\nauto res = cli.Get(\"/hi\", headers);\n</code></pre><pre><code>auto res = cli.Get(\"/hi\", {{\"Hello\", \"World!\"}});\n</code></pre><pre><code>cli.set_default_headers({\n  { \"Hello\", \"World!\" }\n});\nauto res = cli.Get(\"/hi\");\n</code></pre><pre><code>res = cli.Post(\"/post\", \"text\", \"text/plain\");\nres = cli.Post(\"/person\", \"name=john1&amp;note=coder\", \"application/x-www-form-urlencoded\");\n</code></pre><pre><code>httplib::Params params;\nparams.emplace(\"name\", \"john\");\nparams.emplace(\"note\", \"coder\");\n\nauto res = cli.Post(\"/post\", params);\n</code></pre><pre><code>httplib::Params params{\n  { \"name\", \"john\" },\n  { \"note\", \"coder\" }\n};\n\nauto res = cli.Post(\"/post\", params);\n</code></pre><h3>POST with Multipart Form Data</h3><pre><code>httplib::UploadFormDataItems items = {\n  { \"text1\", \"text default\", \"\", \"\" },\n  { \"text2\", \"aωb\", \"\", \"\" },\n  { \"file1\", \"h\\ne\\n\\nl\\nl\\no\\n\", \"hello.txt\", \"text/plain\" },\n  { \"file2\", \"{\\n  \\\"world\\\", true\\n}\\n\", \"world.json\", \"application/json\" },\n  { \"file3\", \"\", \"\", \"application/octet-stream\" },\n};\n\nauto res = cli.Post(\"/multipart\", items);\n</code></pre><pre><code>res = cli.Put(\"/resource/foo\", \"text\", \"text/plain\");\n</code></pre><pre><code>res = cli.Delete(\"/resource/foo\");\n</code></pre><pre><code>res = cli.Options(\"*\");\nres = cli.Options(\"/resource/foo\");\n</code></pre><pre><code>cli.set_connection_timeout(0, 300000); // 300 milliseconds\ncli.set_read_timeout(5, 0); // 5 seconds\ncli.set_write_timeout(5, 0); // 5 seconds\n\n// This method works the same as curl's `--max-time` option\ncli.set_max_timeout(5000); // 5 seconds\n</code></pre><h3>Receive content with a content receiver</h3><pre><code>std::string body;\n\nauto res = cli.Get(\"/large-data\",\n  [&amp;](const char *data, size_t data_length) {\n    body.append(data, data_length);\n    return true;\n  });\n</code></pre><pre><code>std::string body;\n\nauto res = cli.Get(\n  \"/stream\", Headers(),\n  [&amp;](const Response &amp;response) {\n    EXPECT_EQ(StatusCode::OK_200, response.status);\n    return true; // return 'false' if you want to cancel the request.\n  },\n  [&amp;](const char *data, size_t data_length) {\n    body.append(data, data_length);\n    return true; // return 'false' if you want to cancel the request.\n  });\n</code></pre><h3>Send content with a content provider</h3><pre><code>std::string body = ...;\n\nauto res = cli.Post(\n  \"/stream\", body.size(),\n  [](size_t offset, size_t length, DataSink &amp;sink) {\n    sink.write(body.data() + offset, length);\n    return true; // return 'false' if you want to cancel the request.\n  },\n  \"text/plain\");\n</code></pre><h3>Chunked transfer encoding</h3><pre><code>auto res = cli.Post(\n  \"/stream\",\n  [](size_t offset, DataSink &amp;sink) {\n    sink.os &lt;&lt; \"chunked data 1\";\n    sink.os &lt;&lt; \"chunked data 2\";\n    sink.os &lt;&lt; \"chunked data 3\";\n    sink.done();\n    return true; // return 'false' if you want to cancel the request.\n  },\n  \"text/plain\");\n</code></pre><pre><code>httplib::Client cli(url, port);\n\n// prints: 0 / 000 bytes =&gt; 50% complete\nauto res = cli.Get(\"/\", [](size_t len, size_t total) {\n  printf(\"%lld / %lld bytes =&gt; %d%% complete\\n\",\n    len, total,\n    (int)(len*100/total));\n  return true; // return 'false' if you want to cancel the request.\n}\n);\n</code></pre><pre><code>// Basic Authentication\ncli.set_basic_auth(\"user\", \"pass\");\n\n// Digest Authentication\ncli.set_digest_auth(\"user\", \"pass\");\n\n// Bearer Token Authentication\ncli.set_bearer_token_auth(\"token\");\n</code></pre><blockquote><p>[!NOTE] OpenSSL is required for Digest Authentication.</p></blockquote><pre><code>cli.set_proxy(\"host\", port);\n\n// Basic Authentication\ncli.set_proxy_basic_auth(\"user\", \"pass\");\n\n// Digest Authentication\ncli.set_proxy_digest_auth(\"user\", \"pass\");\n\n// Bearer Token Authentication\ncli.set_proxy_bearer_token_auth(\"pass\");\n</code></pre><blockquote><p>[!NOTE] OpenSSL is required for Digest Authentication.</p></blockquote><pre><code>httplib::Client cli(\"httpbin.org\");\n\nauto res = cli.Get(\"/range/32\", {\n  httplib::make_range_header({{1, 10}}) // 'Range: bytes=1-10'\n});\n// res-&gt;status should be 206.\n// res-&gt;body should be \"bcdefghijk\".\n</code></pre><pre><code>httplib::make_range_header({{1, 10}, {20, -1}})      // 'Range: bytes=1-10, 20-'\nhttplib::make_range_header({{100, 199}, {500, 599}}) // 'Range: bytes=100-199, 500-599'\nhttplib::make_range_header({{0, 0}, {-1, 1}})        // 'Range: bytes=0-0, -1'\n</code></pre><pre><code>httplib::Client cli(\"localhost\", 1234);\n\ncli.Get(\"/hello\");         // with \"Connection: close\"\n\ncli.set_keep_alive(true);\ncli.Get(\"/world\");\n\ncli.set_keep_alive(false);\ncli.Get(\"/last-request\");  // with \"Connection: close\"\n</code></pre><pre><code>httplib::Client cli(\"yahoo.com\");\n\nauto res = cli.Get(\"/\");\nres-&gt;status; // 301\n\ncli.set_follow_location(true);\nres = cli.Get(\"/\");\nres-&gt;status; // 200\n</code></pre><h3>Use a specific network interface</h3><blockquote><p>[!NOTE] This feature is not available on Windows, yet.</p></blockquote><pre><code>cli.set_interface(\"eth0\"); // Interface name, IP address or host name\n</code></pre><p>The client automatically encodes special characters in URL paths by default:</p><pre><code>httplib::Client cli(\"https://example.com\");\n\n// Automatic path encoding (default behavior)\ncli.set_path_encode(true);\nauto res = cli.Get(\"/path with spaces/file.txt\"); // Automatically encodes spaces\n\n// Disable automatic path encoding\ncli.set_path_encode(false);\nauto res = cli.Get(\"/already%20encoded/path\"); // Use pre-encoded paths\n</code></pre><ul><li> - Controls automatic encoding of special characters in URL paths \n  <ul><li> (default): Automatically encodes spaces, plus signs, newlines, and other special characters</li><li>: Sends paths as-is without encoding (useful for pre-encoded URLs)</li></ul></li></ul><h3>Performance Note for Local Connections</h3><blockquote><p>[!WARNING] On Windows systems with improperly configured IPv6 settings, using \"localhost\" as the hostname may cause significant connection delays (up to 2 seconds per request) due to DNS resolution issues. This affects both client and server operations. For better performance when connecting to local services, use \"127.0.0.1\" instead of \"localhost\".</p></blockquote><pre><code>// May be slower on Windows due to DNS resolution delays\nhttplib::Client cli(\"localhost\", 8080);\nhttplib::Server svr;\nsvr.listen(\"localhost\", 8080);\n\n// Faster alternative for local connections\nhttplib::Client cli(\"127.0.0.1\", 8080);\nhttplib::Server svr;\nsvr.listen(\"127.0.0.1\", 8080);\n</code></pre><p>The server can apply compression to the following MIME type contents:</p><ul><li>all text types except text/event-stream</li></ul><p>'gzip' compression is available with .  should be linked.</p><h3>Default  value</h3><p>The default  value contains all possible compression types. So, the following two examples are same.</p><pre><code>res = cli.Get(\"/resource/foo\");\nres = cli.Get(\"/resource/foo\", {{\"Accept-Encoding\", \"br, gzip, deflate, zstd\"}});\n</code></pre><p>If we don't want a response without compression, we have to set  to an empty string. This behavior is similar to curl.</p><pre><code>res = cli.Get(\"/resource/foo\", {{\"Accept-Encoding\", \"\"}});\n</code></pre><h3>Compress request body on client</h3><pre><code>cli.set_compress(true);\nres = cli.Post(\"/resource/foo\", \"...\", \"text/plain\");\n</code></pre><h3>Compress response body on client</h3><pre><code>cli.set_decompress(false);\nres = cli.Get(\"/resource/foo\");\nres-&gt;body; // Compressed data\n\n</code></pre><h2>Unix Domain Socket Support</h2><p>Unix Domain Socket support is available on Linux and macOS.</p><pre><code>// Server\nhttplib::Server svr;\nsvr.set_address_family(AF_UNIX).listen(\"./my-socket.sock\", 80);\n\n// Client\nhttplib::Client cli(\"./my-socket.sock\");\ncli.set_address_family(AF_UNIX);\n</code></pre><p>\"my-socket.sock\" can be a relative path or an absolute path. Your application must have the appropriate permissions for the path. You can also use an abstract socket address on Linux. To use an abstract socket address, prepend a null byte ('\\x00') to the path.</p><p>This library automatically sets the Host header to \"localhost\" for Unix socket connections, similar to curl's behavior:</p><h2>URI Encoding/Decoding Utilities</h2><p>cpp-httplib provides utility functions for URI encoding and decoding:</p><pre><code>#include &lt;httplib.h&gt;\n\nstd::string url = \"https://example.com/search?q=hello world\";\nstd::string encoded = httplib::encode_uri(url);\nstd::string decoded = httplib::decode_uri(encoded);\n\nstd::string param = \"hello world\";\nstd::string encoded_component = httplib::encode_uri_component(param);\nstd::string decoded_component = httplib::decode_uri_component(encoded_component);\n</code></pre><ul><li><code>encode_uri(const std::string &amp;value)</code> - Encodes a full URI, preserving reserved characters like , , , </li><li><code>decode_uri(const std::string &amp;value)</code> - Decodes a URI-encoded string</li><li><code>encode_uri_component(const std::string &amp;value)</code> - Encodes a URI component (query parameter, path segment), encoding all reserved characters</li><li><code>decode_uri_component(const std::string &amp;value)</code> - Decodes a URI component</li></ul><p>Use  for full URLs and  for individual query parameters or path segments.</p><h2>Split httplib.h into .h and .cc</h2><pre><code>$ ./split.py -h\nusage: split.py [-h] [-e EXTENSION] [-o OUT]\n\nThis script splits httplib.h into .h and .cc parts.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTENSION, --extension EXTENSION\n                        extension of the implementation file (default: cc)\n  -o OUT, --out OUT     where to write the files (default: out)\n\n$ ./split.py\nWrote out/httplib.h and out/httplib.cc\n</code></pre><h2>Dockerfile for Static HTTP Server</h2><p>Dockerfile for static HTTP server is available. Port number of this HTTP server is 80, and it serves static files from  directory in the container.</p><pre><code>&gt; docker build -t cpp-httplib-server .\n...\n\n&gt; docker run --rm -it -p 8080:80 -v ./docker/html:/html cpp-httplib-server\nServing HTTP on 0.0.0.0 port 80 ...\n192.168.65.1 - - [31/Aug/2024:21:33:56 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"curl/8.7.1\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"Mozilla/5.0 ...\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET /favicon.ico HTTP/1.1\" 404 152 \"-\" \"Mozilla/5.0 ...\"\n</code></pre><pre><code>&gt; docker run --rm -it -p 8080:80 -v ./docker/html:/html yhirose4dockerhub/cpp-httplib-server\nServing HTTP on 0.0.0.0 port 80 ...\n192.168.65.1 - - [31/Aug/2024:21:33:56 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"curl/8.7.1\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"Mozilla/5.0 ...\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET /favicon.ico HTTP/1.1\" 404 152 \"-\" \"Mozilla/5.0 ...\"\n</code></pre><h3>Regular Expression Stack Overflow</h3><blockquote><p>[!CAUTION] When using complex regex patterns in route handlers, be aware that certain patterns may cause stack overflow during pattern matching. This is a known issue with  implementations and affects the  method.</p><pre><code>// This pattern can cause stack overflow with large input\nsvr.Get(\".*\", handler);\n</code></pre><p>Consider using simpler patterns or path parameters to avoid this issue:</p><pre><code>// Safer alternatives\nsvr.Get(\"/users/:id\", handler);           // Path parameters\nsvr.Get(R\"(/api/v\\d+/.*)\", handler);     // More specific patterns\n</code></pre></blockquote><p>g++ 4.8 and below cannot build this library since  in the versions are <a href=\"https://stackoverflow.com/questions/12530406/is-gcc-4-8-or-earlier-buggy-about-regular-expressions\">broken</a>.</p><p>Include  before  or include  by defining  beforehand.</p><pre><code>#include &lt;httplib.h&gt;\n#include &lt;Windows.h&gt;\n</code></pre><pre><code>#define WIN32_LEAN_AND_MEAN\n#include &lt;Windows.h&gt;\n#include &lt;httplib.h&gt;\n</code></pre><blockquote><p>[!NOTE] cpp-httplib officially supports only the latest Visual Studio. It might work with former versions of Visual Studio, but I can no longer verify it. Pull requests are always welcome for the older versions of Visual Studio unless they break the C++11 conformance.</p></blockquote><blockquote><p>[!NOTE] Windows 8 or lower, Visual Studio 2015 or lower, and Cygwin and MSYS2 including MinGW are neither supported nor tested.</p></blockquote><p>MIT license (© 2025 Yuji Hirose)</p><p><a href=\"https://github.com/yhirose/cpp-httplib/graphs/contributors\">These folks</a> made great contributions to polish this library to totally another level from a simple toy!</p>","contentLength":30425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tencent/WeKnora","url":"https://github.com/Tencent/WeKnora","date":1761878103,"author":"","guid":322484,"unread":true,"content":"<p>LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.</p><p><a href=\"https://weknora.weixin.qq.com\"></a> is an LLM-powered framework designed for deep document understanding and semantic retrieval, especially for handling complex, heterogeneous documents.</p><p>It adopts a modular architecture that combines multimodal preprocessing, semantic vector indexing, intelligent retrieval, and large language model inference. At its core, WeKnora follows the <strong>RAG (Retrieval-Augmented Generation)</strong> paradigm, enabling high-quality, context-aware answers by combining relevant document chunks with model reasoning.</p><p> Starting from v0.1.3, WeKnora includes login authentication functionality to enhance system security. For production deployments, we strongly recommend:</p><ul><li>Deploy WeKnora services in internal/private network environments rather than public internet</li><li>Avoid exposing the service directly to public networks to prevent potential information leakage</li><li>Configure proper firewall rules and access controls for your deployment environment</li><li>Regularly update to the latest version for security patches and improvements</li></ul><p>WeKnora employs a modern modular design to build a complete document understanding and retrieval pipeline. The system primarily includes document parsing, vector processing, retrieval engine, and large model inference as core modules, with each component being flexibly configurable and extendable.</p><ul><li>: Structured content extraction from PDFs, Word documents, images and more into unified semantic views</li><li>: Leverages LLMs to understand document context and user intent for accurate Q&amp;A and multi-turn conversations</li><li>: All components from parsing and embedding to retrieval and generation are decoupled for easy customization</li><li>: Hybrid retrieval strategies combining keywords, vectors, and knowledge graphs</li><li>: Intuitive web interface and standardized APIs for zero technical barriers</li><li>: Support for local deployment and private cloud, ensuring complete data sovereignty</li></ul><table><thead><tr></tr></thead><tbody><tr><td><strong>Enterprise Knowledge Management</strong></td><td>Internal document retrieval, policy Q&amp;A, operation manual search</td><td>Improve knowledge discovery efficiency, reduce training costs</td></tr><tr><td><strong>Academic Research Analysis</strong></td><td>Paper retrieval, research report analysis, scholarly material organization</td><td>Accelerate literature review, assist research decisions</td></tr><tr><td><strong>Product Technical Support</strong></td><td>Product manual Q&amp;A, technical documentation search, troubleshooting</td><td>Enhance customer service quality, reduce support burden</td></tr><tr><td><strong>Legal &amp; Compliance Review</strong></td><td>Contract clause retrieval, regulatory policy search, case analysis</td><td>Improve compliance efficiency, reduce legal risks</td></tr><tr><td><strong>Medical Knowledge Assistance</strong></td><td>Medical literature retrieval, treatment guideline search, case analysis</td><td>Support clinical decisions, improve diagnosis quality</td></tr></tbody></table><table><tbody><tr><td>✅ PDF / Word / Txt / Markdown / Images (with OCR / Caption)</td><td>Support for structured and unstructured documents with text extraction from images</td></tr><tr><td>✅ Local models, BGE / GTE APIs, etc.</td><td>Customizable embedding models, compatible with local deployment and cloud vector generation APIs</td></tr><tr><td>✅ PostgreSQL (pgvector), Elasticsearch</td><td>Support for mainstream vector index backends, flexible switching for different retrieval scenarios</td></tr><tr><td>✅ BM25 / Dense Retrieval / GraphRAG</td><td>Support for sparse/dense recall and knowledge graph-enhanced retrieval with customizable retrieve-rerank-generate pipelines</td></tr><tr><td>✅ Support for Qwen, DeepSeek, etc., with thinking/non-thinking mode switching</td><td>Compatible with local models (e.g., via Ollama) or external API services with flexible inference configuration</td></tr><tr><td>✅ Context-aware, multi-turn dialogue, prompt templates</td><td>Support for complex semantic modeling, instruction control and chain-of-thought Q&amp;A with configurable prompts and context windows</td></tr><tr><td>✅ Retrieval+generation process visualization and metric evaluation</td><td>End-to-end testing tools for evaluating recall hit rates, answer coverage, BLEU/ROUGE and other metrics</td></tr><tr><td>✅ Support for local deployment / Docker images</td><td>Meets private, offline deployment and flexible operation requirements</td></tr><tr><td>Interactive interface and standard API endpoints, suitable for both developers and business users</td></tr></tbody></table><p>Make sure the following tools are installed on your system:</p><pre><code># Clone the main repository\ngit clone https://github.com/Tencent/WeKnora.git\ncd WeKnora\n</code></pre><h4>② Configure environment variables</h4><pre><code># Copy example env file\ncp .env.example .env\n\n# Edit .env and set required values\n# All variables are documented in the .env.example comments\n</code></pre><pre><code># Start all services (Ollama + backend containers)\n./scripts/start_all.sh\n# Or\nmake start-all\n</code></pre><h4>③ Start the services (backup)</h4><pre><code># Start ollama services (Optional)\nollama serve &gt; /dev/null 2&gt;&amp;1 &amp;\n\n# Start the service\ndocker compose up -d\n</code></pre><pre><code>./scripts/start_all.sh --stop\n# Or\nmake stop-all\n</code></pre><p>Once started, services will be available at:</p><ul><li>Backend API: </li><li>Jaeger Tracing: </li></ul><h3>🔌 Using WeChat Dialog Open Platform</h3><ul><li>: Simply upload knowledge to quickly deploy intelligent Q&amp;A services within the WeChat ecosystem, achieving an \"ask and answer\" experience</li><li><strong>Efficient Question Management</strong>: Support for categorized management of high-frequency questions, with rich data tools to ensure accurate, reliable, and easily maintainable answers</li><li><strong>WeChat Ecosystem Integration</strong>: Through the WeChat Dialog Open Platform, WeKnora's intelligent Q&amp;A capabilities can be seamlessly integrated into WeChat Official Accounts, Mini Programs, and other WeChat scenarios, enhancing user interaction experiences</li></ul><h3>🔗 Access WeKnora via MCP Server</h3><pre><code>git clone https://github.com/Tencent/WeKnora\n</code></pre><p>Configure the MCP client to connect to the server:</p><pre><code>{\n  \"mcpServers\": {\n    \"weknora\": {\n      \"args\": [\n        \"path/to/WeKnora/mcp-server/run_server.py\"\n      ],\n      \"command\": \"python\",\n      \"env\":{\n        \"WEKNORA_API_KEY\":\"Enter your WeKnora instance, open developer tools, check the request header x-api-key starting with sk\",\n        \"WEKNORA_BASE_URL\":\"http(s)://your-weknora-address/api/v1\"\n      }\n    }\n  }\n}\n</code></pre><p>Run directly using stdio command:</p><pre><code>pip install weknora-mcp-server\npython -m weknora-mcp-server\n</code></pre><h2>🔧 Initialization Configuration Guide</h2><p>To help users quickly configure various models and reduce trial-and-error costs, we've improved the original configuration file initialization method by adding a Web UI interface for model configuration. Before using, please ensure the code is updated to the latest version. The specific steps are as follows: If this is your first time using this project, you can skip steps ①② and go directly to steps ③④.</p><pre><code>./scripts/start_all.sh --stop\n</code></pre><h3>② Clear existing data tables (recommended when no important data exists)</h3><h3>③ Compile and start services</h3><p>On first access, it will automatically redirect to the initialization configuration page. After configuration is complete, it will automatically redirect to the knowledge base page. Please follow the page instructions to complete model configuration.</p><p><strong>Knowledge Base Management:</strong> Support for dragging and dropping various documents, automatically identifying document structures and extracting core knowledge to establish indexes. The system clearly displays processing progress and document status, achieving efficient knowledge base management.</p><p>WeKnora supports transforming documents into knowledge graphs, displaying the relationships between different sections of the documents. Once the knowledge graph feature is enabled, the system analyzes and constructs an internal semantic association network that not only helps users understand document content but also provides structured support for indexing and retrieval, enhancing the relevance and breadth of search results.</p><h3>MCP Server Integration Effects</h3><img width=\"950\" height=\"2063\" alt=\"MCP Server Integration Demo\" src=\"https://github.com/user-attachments/assets/09111ec8-0489-415c-969d-aa3835778e14\"><p>Detailed API documentation is available at: <a href=\"https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/API.md\">API Docs</a></p><pre><code>WeKnora/\n├── cmd/         # Main entry point\n├── internal/    # Core business logic\n├── config/      # Configuration files\n├── migrations/  # DB migration scripts\n├── scripts/     # Shell scripts\n├── services/    # Microservice logic\n├── frontend/    # Frontend app\n└── docs/        # Project documentation\n</code></pre><pre><code># Wipe all data from DB (use with caution)\nmake clean-db\n</code></pre><p>We welcome community contributions! For suggestions, bugs, or feature requests, please submit an <a href=\"https://github.com/Tencent/WeKnora/issues\">Issue</a> or directly create a Pull Request.</p><ul><li>🐛 : Discover and fix system defects</li><li>✨ : Propose and implement new capabilities</li><li>📚 : Improve project documentation</li><li>🧪 : Write unit and integration tests</li><li>🎨 : Improve user interface and experience</li></ul><ol><li> to your GitHub account</li><li><code>git checkout -b feature/amazing-feature</code></li><li><code>git commit -m 'Add amazing feature'</code></li><li><code>git push origin feature/amazing-feature</code></li><li> with detailed description of changes</li></ol><pre><code>feat: Add document batch upload functionality\nfix: Resolve vector retrieval precision issue\ndocs: Update API documentation\ntest: Add retrieval engine test cases\nrefactor: Restructure document parsing module\n</code></pre><p>This project is licensed under the <a href=\"https://raw.githubusercontent.com/Tencent/WeKnora/main/LICENSE\">MIT License</a>. You are free to use, modify, and distribute the code with proper attribution.</p>","contentLength":8829,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"storybookjs/storybook","url":"https://github.com/storybookjs/storybook","date":1761878103,"author":"","guid":322485,"unread":true,"content":"<p>Storybook is the industry standard workshop for building, documenting, and testing UI components in isolation</p><p align=\"center\">Build bulletproof UI components faster</p><p align=\"center\"> Storybook is a frontend workshop for building UI components and pages in isolation. Thousands of teams use it for UI development, testing, and documentation. Find out more at <a href=\"https://storybook.js.org/?ref=readme\">storybook.js.org</a>! </p><p>Use <a href=\"https://storybook.new\">storybook.new</a> to quickly create an example project in Stackblitz.</p><p>Storybook comes with a lot of <a href=\"https://storybook.js.org/docs/configure/user-interface/storybook-addons?ref=readme\">addons</a> for component design, documentation, testing, interactivity, and so on. Storybook's API makes it possible to configure and extend in various ways. It has even been extended to support React Native, Android, iOS, and Flutter development for mobile.</p><table><tbody><tr><td>Test components for user accessibility in Storybook</td></tr><tr><td>Log actions as users interact with components in the Storybook UI</td></tr><tr><td>Let users choose backgrounds in the Storybook UI</td></tr><tr><td>Dynamically add/remove CSS resources to the component iframe</td></tr><tr><td>View images, videos, and weblinks alongside your story</td></tr><tr><td>Add high quality documentation to your components</td></tr><tr><td>Interactively fire events to components that respond to EventEmitter</td></tr><tr><td>Query a GraphQL server within Storybook stories</td></tr><tr><td>View the results of components' unit tests in Storybook</td></tr><tr><td>Create links between stories</td></tr><tr><td>Visually inspect the layout and box model within the Storybook UI</td></tr><tr><td>Visually debug the CSS layout and alignment within the Storybook UI</td></tr><tr><td>Change display sizes and layouts for responsive components using Storybook</td></tr></tbody></table><p>To continue improving your experience, we have to eventually deprecate or remove certain addons in favor of new and better tools.</p><p>If you're using info/notes, we highly recommend you migrate to <a href=\"https://raw.githubusercontent.com/storybookjs/storybook/next/code/addons/docs/\">docs</a> instead, and <a href=\"https://raw.githubusercontent.com/storybookjs/storybook/next/code/addons/docs/docs/recipes.md#migrating-from-notesinfo-addons\">here is a guide</a> to help you.</p><h2>Badges &amp; Presentation materials</h2><p>We have a badge! Link it to your live Storybook example.</p><pre><code>[![Storybook](https://cdn.jsdelivr.net/gh/storybookjs/brand@main/badge/badge-storybook.svg)](link to site)\n</code></pre><p>If you're looking for material to use in your Storybook presentation, such as logos, video material, and the colors we use, you can find it all on our <a href=\"https://github.com/storybookjs/brand\">brand repo</a>.</p><p>Contributions to Storybook are always welcome!</p><ul><li>📥 Pull requests and 🌟 Stars are always welcome.</li></ul><p>Looking for a first issue to tackle?</p><p>Storybook is organized as a monorepo. Useful scripts include:</p><blockquote><p>Runs a sandbox template storybook with test stories</p></blockquote><blockquote><p>As above, but gives you options to customize the sandbox (e.g. selecting other frameworks)</p></blockquote><blockquote><p>boolean check if code conforms to linting rules - uses remark &amp; eslint</p></blockquote><ul><li> - will check js</li><li> - will check markdown + code samples</li><li> - will automatically fix js</li></ul><blockquote><p>boolean check if unit tests all pass - uses jest</p></blockquote><ul><li><code>yarn run test --core --watch</code> - will run core tests in watch-mode</li></ul><p>Become a sponsor to have your logo and website URL on our README on Github. [<a href=\"https://opencollective.com/storybook#sponsor\">Become a sponsor</a>]</p><p>By making a recurring donation, you can support us and our work. [<a href=\"https://opencollective.com/storybook#backer\">Become a backer</a>]</p>","contentLength":2795,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Project-MONAI/MONAI","url":"https://github.com/Project-MONAI/MONAI","date":1761878103,"author":"","guid":322486,"unread":true,"content":"<p>AI Toolkit for Healthcare Imaging</p><ul><li>Developing a community of academic, industrial and clinical researchers collaborating on a common foundation;</li><li>Creating state-of-the-art, end-to-end training workflows for healthcare imaging;</li><li>Providing researchers with the optimized and standardized way to create and evaluate deep learning models.</li></ul><ul><li>flexible pre-processing for multi-dimensional medical imaging data;</li><li>compositional &amp; portable APIs for ease of integration in existing workflows;</li><li>domain-specific implementations for networks, losses, evaluation metrics and more;</li><li>customizable design for varying user expertise;</li><li>multi-GPU multi-node data parallelism support.</li></ul><ul><li>Major releases of MONAI will have dependency versions stated for them. The current state of the  branch in this repository is the unreleased development version of MONAI which typically will support current versions of dependencies and include updates and bug fixes to do so.</li><li>PyTorch support covers <a href=\"https://github.com/pytorch/pytorch/releases\">the current version</a> plus three previous minor versions. If compatibility issues with a PyTorch version and other dependencies arise, support for a version may be delayed until a major release.</li><li>Our support policy for other dependencies adheres for the most part to <a href=\"https://scientific-python.org/specs/spec-0000\">SPEC0</a>, where dependency versions are supported where possible for up to two years. Discovered vulnerabilities or defects may require certain versions to be explicitly not supported.</li><li>See the  files for dependency version information.</li></ul>","contentLength":1438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mem0ai/mem0","url":"https://github.com/mem0ai/mem0","date":1761878103,"author":"","guid":322487,"unread":true,"content":"<p>Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.</p><p align=\"center\"><strong>⚡ +26% Accuracy vs. OpenAI Memory • 🚀 91% Faster • 💰 90% Fewer Tokens</strong></p><blockquote><p><strong>🎉 mem0ai v1.0.0 is now available!</strong> This major release includes API modernization, improved vector store support, and enhanced GCP integration. <a href=\"https://raw.githubusercontent.com/mem0ai/mem0/main/MIGRATION_GUIDE_v1.0.md\">See migration guide →</a></p></blockquote><ul><li> over OpenAI Memory on the LOCOMO benchmark</li><li> than full-context, ensuring low-latency at scale</li><li> than full-context, cutting costs without compromise</li></ul><p><a href=\"https://mem0.ai\">Mem0</a> (\"mem-zero\") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time—ideal for customer support chatbots, AI assistants, and autonomous systems.</p><ul><li>: Seamlessly retains User, Session, and Agent state with adaptive personalization</li><li>: Intuitive API, cross-platform SDKs, and a fully managed service option</li></ul><ul><li>: Consistent, context-rich conversations</li><li>: Recall past tickets and user history for tailored help</li><li>: Track patient preferences and history for personalized care</li><li>: Adaptive workflows and environments based on user behavior</li></ul><p>Choose between our hosted platform or self-hosted package:</p><p>Get up and running in minutes with automatic updates, analytics, and enterprise security.</p><ol><li>Embed the memory layer via SDK or API keys</li></ol><h3>Self-Hosted (Open Source)</h3><p>Mem0 requires an LLM to function, with `gpt-4.1-nano-2025-04-14 from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our <a href=\"https://docs.mem0.ai/components/llms/overview\">Supported LLMs documentation</a>.</p><p>First step is to instantiate the memory:</p><pre><code>from openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -&gt; str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n\n    # Generate Assistant response\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4.1-nano-2025-04-14\", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre><ul><li>: Personalized chat powered by Mem0 (<a href=\"https://mem0.dev/demo\">Live Demo</a>)</li><li>: Store memories across ChatGPT, Perplexity, and Claude (<a href=\"https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb\">Chrome Extension</a>)</li><li>: Build a customer bot with Langgraph + Mem0 (<a href=\"https://docs.mem0.ai/integrations/langgraph\">Guide</a>)</li><li>: Tailor CrewAI outputs with Mem0 (<a href=\"https://docs.mem0.ai/integrations/crewai\">Example</a>)</li></ul><h2>📚 Documentation &amp; Support</h2><p>We now have a paper you can cite:</p><pre><code>@article{mem0,\n  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},\n  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},\n  journal={arXiv preprint arXiv:2504.19413},\n  year={2025}\n}\n</code></pre><p>Apache 2.0 — see the <a href=\"https://github.com/mem0ai/mem0/raw/main/LICENSE\">LICENSE</a> file for details.</p>","contentLength":3492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"open-telemetry/opentelemetry-collector","url":"https://github.com/open-telemetry/opentelemetry-collector","date":1761791732,"author":"","guid":321070,"unread":true,"content":"<p>The OpenTelemetry Collector offers a vendor-agnostic implementation on how to receive, process and export telemetry data. In addition, it removes the need to run, operate and maintain multiple agents/collectors in order to support open-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) to multiple open-source or commercial back-ends.</p><ul><li>Usable: Reasonable default configuration, supports popular protocols, runs and collects out of the box.</li><li>Performant: Highly stable and performant under varying loads and configurations.</li><li>Observable: An exemplar of an observable service.</li><li>Extensible: Customizable without touching the core code.</li><li>Unified: Single codebase, deployable as an agent or collector with support for traces, metrics and logs.</li></ul><p>The OpenTelemetry Collector SIG is present at the <a href=\"https://cloud-native.slack.com/archives/C01N6P7KR6W\">#otel-collector</a> channel on the CNCF Slack and <a href=\"https://github.com/open-telemetry/community#implementation-sigs\">meets once a week</a> via video calls. Everyone is invited to join those calls, which typically serves the following purposes:</p><ul><li>meet the humans behind the project</li><li>get an opinion about specific proposals</li><li>look for a sponsor for a proposed component after trying already via GitHub and Slack</li><li>get attention to a specific pull-request that got stuck and is difficult to discuss asynchronously</li></ul><p>We rotate our video calls between three time slots, in order to allow everyone to join at least once every three meetings. The rotation order is as follows:</p><p>Contributors to the project are also welcome to have ad-hoc meetings for synchronous discussions about specific points. Post a note in #otel-collector-dev on Slack inviting others, specifying the topic to be discussed. Unless there are strong reasons to keep the meeting private, please make it an open invitation for other contributors to join. Try also to identify who would be the other contributors interested on that topic and in which timezones they are.</p><p>Remember that our source of truth is GitHub: every decision made via Slack or video calls has to be recorded in the relevant GitHub issue. Ideally, the agenda items from the meeting notes would include a link to the issue or pull request where a discussion is happening already. We acknowledge that not everyone can join Slack or the synchronous calls and don't want them to feel excluded.</p><p>When used as a library, the OpenTelemetry Collector attempts to track the currently supported versions of Go, as <a href=\"https://go.dev/doc/devel/release#policy\">defined by the Go team</a>. Removing support for an unsupported Go version is not considered a breaking change.</p><p>Support for Go versions on the OpenTelemetry Collector is updated as follows:</p><ol><li>The first release after the release of a new Go minor version  will add build and tests steps for the new Go minor version.</li><li>The first release after the release of a new Go minor version  will remove support for Go version .</li></ol><p>Official OpenTelemetry Collector distro binaries will be built with a release in the latest Go minor version series.</p><h2>Verifying the images signatures</h2><blockquote><p>[!NOTE] To verify a signed artifact or blob, first <a href=\"https://docs.sigstore.dev/cosign/system_config/installation/\">install Cosign</a>, then follow the instructions below.</p></blockquote><p>We are signing the images <code>otel/opentelemetry-collector</code> and <code>otel/opentelemetry-collector-contrib</code> using <a href=\"https://github.com/sigstore/cosign\">sigstore cosign</a> tool and to verify the signatures you can run the following command:</p><pre><code>$ cosign verify \\\n  --certificate-identity=https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/&lt;RELEASE_TAG&gt; \\\n  --certificate-oidc-issuer=https://token.actions.githubusercontent.com \\\n  &lt;OTEL_COLLECTOR_IMAGE&gt;\n</code></pre><ul><li>: is the release that you want to validate</li><li>: is the image that you want to check</li></ul><pre><code>$ cosign verify --certificate-identity=https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0 --certificate-oidc-issuer=https://token.actions.githubusercontent.com ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.98.0\n\nVerification for ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.98.0 --\nThe following checks were performed on each of these signatures:\n  - The cosign claims were validated\n  - Existence of the claims in the transparency log was verified offline\n  - The code-signing certificate was verified using trusted certificate authority certificates\n\n[{\"critical\":{\"identity\":{\"docker-reference\":\"ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib\"},\"image\":{\"docker-manifest-digest\":\"sha256:5cea85bcbc734a3c0a641368e5a4ea9d31b472997e9f2feca57eeb4a147fcf1a\"},\"type\":\"cosign container image signature\"},\"optional\":{\"1.3.6.1.4.1.57264.1.1\":\"https://token.actions.githubusercontent.com\",\"1.3.6.1.4.1.57264.1.2\":\"push\",\"1.3.6.1.4.1.57264.1.3\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"1.3.6.1.4.1.57264.1.4\":\"Release Contrib\",\"1.3.6.1.4.1.57264.1.5\":\"open-telemetry/opentelemetry-collector-releases\",\"1.3.6.1.4.1.57264.1.6\":\"refs/tags/v0.98.0\",\"Bundle\":{\"SignedEntryTimestamp\":\"MEUCIQDdlmNeKXQrHnonwWiHLhLLwFDVDNoOBCn2sv85J9P8mgIgDQFssWJImo1hn38VlojvSCL7Qq5FMmtnGu0oLsNdOm8=\",\"Payload\":{\"body\":\"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\",\"integratedTime\":1712809120,\"logIndex\":84797936,\"logID\":\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\"}},\"Issuer\":\"https://token.actions.githubusercontent.com\",\"Subject\":\"https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0\",\"githubWorkflowName\":\"Release Contrib\",\"githubWorkflowRef\":\"refs/tags/v0.98.0\",\"githubWorkflowRepository\":\"open-telemetry/opentelemetry-collector-releases\",\"githubWorkflowSha\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"githubWorkflowTrigger\":\"push\"}},{\"critical\":{\"identity\":{\"docker-reference\":\"ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib\"},\"image\":{\"docker-manifest-digest\":\"sha256:5cea85bcbc734a3c0a641368e5a4ea9d31b472997e9f2feca57eeb4a147fcf1a\"},\"type\":\"cosign container image signature\"},\"optional\":{\"1.3.6.1.4.1.57264.1.1\":\"https://token.actions.githubusercontent.com\",\"1.3.6.1.4.1.57264.1.2\":\"push\",\"1.3.6.1.4.1.57264.1.3\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"1.3.6.1.4.1.57264.1.4\":\"Release Contrib\",\"1.3.6.1.4.1.57264.1.5\":\"open-telemetry/opentelemetry-collector-releases\",\"1.3.6.1.4.1.57264.1.6\":\"refs/tags/v0.98.0\",\"Bundle\":{\"SignedEntryTimestamp\":\"MEUCIQD1ehDnPO6fzoPIpeQ3KFuYHHBiX7RcEbpo9B2r7JAlzwIgZ1bsuQz7gAXbNU1IEdsTQgfAnRk3xVXO16GnKXM2sAQ=\",\"Payload\":{\"body\":\"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\",\"integratedTime\":1712809122,\"logIndex\":84797940,\"logID\":\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\"}},\"Issuer\":\"https://token.actions.githubusercontent.com\",\"Subject\":\"https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0\",\"githubWorkflowName\":\"Release Contrib\",\"githubWorkflowRef\":\"refs/tags/v0.98.0\",\"githubWorkflowRepository\":\"open-telemetry/opentelemetry-collector-releases\",\"githubWorkflowSha\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"githubWorkflowTrigger\":\"push\"}}]\n</code></pre><blockquote><p>[!NOTE] We started signing the images with release </p></blockquote><p>Here is a list of community roles with current and previous members:</p><p>In addition to what is described at the organization-level, the SIG Collector requires all core approvers to take part in rotating the role of the <a href=\"https://raw.githubusercontent.com/open-telemetry/opentelemetry-collector/main/docs/release.md#release-manager\">release manager</a>.</p><h3>Thanks to all of our contributors!</h3><a href=\"https://github.com/open-telemetry/opentelemetry-collector/graphs/contributors\"><img alt=\"Repo contributors\" src=\"https://contrib.rocks/image?repo=open-telemetry/opentelemetry-collector\"></a>","contentLength":17593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"dotnet/eShop","url":"https://github.com/dotnet/eShop","date":1761791732,"author":"","guid":321071,"unread":true,"content":"<p>A reference .NET application implementing an eCommerce site</p><p>A reference .NET application implementing an e-commerce website using a services-based architecture using <a href=\"https://learn.microsoft.com/dotnet/aspire/\">.NET Aspire</a>.</p><p>This version of eShop is based on .NET 9.</p><h4>Windows with Visual Studio</h4><ul><li>Install <a href=\"https://visualstudio.microsoft.com/vs/\">Visual Studio 2022 version 17.10 or newer</a>. \n  <ul><li>Select the following workloads: \n    <ul><li><code>ASP.NET and web development</code> workload.</li><li> component in .</li><li>Optional: <code>.NET Multi-platform App UI development</code> to run client apps</li></ul></li></ul></li></ul><ul><li>Run the following commands in a Powershell &amp; Terminal running as  to automatically configure your environment with the required tools to build and run this application. (Note: A restart is required and included in the script below.)</li></ul><pre><code>install-Module -Name Microsoft.WinGet.Configuration -AllowPrerelease -AcceptLicense -Force\n$env:Path = [System.Environment]::GetEnvironmentVariable(\"Path\",\"Machine\") + \";\" + [System.Environment]::GetEnvironmentVariable(\"Path\",\"User\")\nget-WinGetConfiguration -file .\\.configurations\\vside.dsc.yaml | Invoke-WinGetConfiguration -AcceptConfigurationAgreements\n</code></pre><ul><li>From Dev Home go to <code>Machine Configuration -&gt; Clone repositories</code>. Enter the URL for this repository. In the confirmation screen look for the section <code>Configuration File Detected</code> and click .</li></ul><h4>Mac, Linux, &amp; Windows without Visual Studio</h4><ul><li>Run the following commands in a Powershell &amp; Terminal running as  to automatically configuration your environment with the required tools to build and run this application. (Note: A restart is required after running the script below.)</li></ul><h5>Install Visual Studio Code and related extensions</h5><pre><code>install-Module -Name Microsoft.WinGet.Configuration -AllowPrerelease -AcceptLicense  -Force\n$env:Path = [System.Environment]::GetEnvironmentVariable(\"Path\",\"Machine\") + \";\" + [System.Environment]::GetEnvironmentVariable(\"Path\",\"User\")\nget-WinGetConfiguration -file .\\.configurations\\vscode.dsc.yaml | Invoke-WinGetConfiguration -AcceptConfigurationAgreements\n</code></pre><blockquote><p>Note: These commands may require </p></blockquote><blockquote><p>Note: When running on Mac with Apple Silicon (M series processor), Rosetta 2 for grpc-tools.</p></blockquote><blockquote><p>[!WARNING] Remember to ensure that Docker is started</p></blockquote><ul><li>(Windows only) Run the application from Visual Studio:</li></ul><ul><li>Open the  file in Visual Studio</li><li>Ensure that  is your startup project</li><li>Hit Ctrl-F5 to launch Aspire</li></ul><ul><li>Or run the application from your terminal:</li></ul><pre><code>dotnet run --project src/eShop.AppHost/eShop.AppHost.csproj\n</code></pre><p>then look for lines like this in the console output in order to find the URL to open the Aspire dashboard:</p><pre><code>Login to the dashboard at: http://localhost:19888/login?t=uniquelogincodeforyou\n</code></pre><p>When using Azure OpenAI, inside <em>eShop.AppHost/appsettings.json</em>, add the following section:</p><pre><code>  \"ConnectionStrings\": {\n    \"OpenAi\": \"Endpoint=xxx;Key=xxx;\"\n  }\n</code></pre><p>Replace the values with your own. Then, in the eShop.AppHost , set this value to </p><p>You can use the <a href=\"https://aka.ms/azd\">Azure Developer CLI</a> to run this project on Azure with only a few commands. Follow the next instructions:</p><ul><li>Log in  (if you haven't done it before) to your Azure account:</li></ul><ul><li>Initialize  from the root of the repo.</li></ul><ul><li><ul><li>Select <code>Use code in the current directory</code>. Azd will automatically detect the .NET Aspire project.</li><li>Confirm  and continue.</li><li>Select which services to expose to the Internet (exposing  is enough to test the sample).</li><li>Finalize the initialization by giving a name to your environment.</li></ul></li><li><p>Create Azure resources and deploy the sample by running:</p></li></ul><ul><li>The operation takes a few minutes the first time it is ever run for an environment.</li><li>At the end of the process,  will display the  for the webapp. Follow that link to test the sample.</li><li>You can run  after saving changes to the sample to re-deploy and update the sample.</li></ul><p>For a version of this app configured for deployment on Azure, please view <a href=\"https://github.com/Azure-Samples/eShopOnAzure\">the eShop on Azure</a> repo.</p>","contentLength":3677,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"protocolbuffers/protobuf","url":"https://github.com/protocolbuffers/protobuf","date":1761791732,"author":"","guid":321072,"unread":true,"content":"<p>Protocol Buffers - Google's data interchange format</p><p>Copyright 2023 Google LLC</p><p>Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can learn more about it in <a href=\"https://protobuf.dev\">protobuf's documentation</a>.</p><p>This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.</p><h2>Working With Protobuf Source Code</h2><p>If you choose to work from the head revision of the main branch your build will occasionally be broken by source-incompatible changes and insufficiently-tested (and therefore broken) behavior.</p><p>If you are using C++ or otherwise need to build protobuf from source as a part of your project, you should pin to a release commit on a release branch.</p><p>This is because even release branches can experience some instability in between release commits.</p><p>Protobuf supports <a href=\"https://bazel.build/external/module\">Bzlmod</a> with Bazel 7 +. Users should specify a dependency on protobuf in their MODULE.bazel file as follows.</p><pre><code>bazel_dep(name = \"protobuf\", version = &lt;VERSION&gt;)\n</code></pre><p>Users can optionally override the repo name, such as for compatibility with WORKSPACE.</p><pre><code>bazel_dep(name = \"protobuf\", version = &lt;VERSION&gt;, repo_name = \"com_google_protobuf\")\n</code></pre><p>Users can also add the following to their legacy <a href=\"https://bazel.build/external/overview#workspace-system\">WORKSPACE</a> file.</p><p>Note that with the release of 30.x there are a few more load statements to properly set up rules_java and rules_python.</p><pre><code>http_archive(\n    name = \"com_google_protobuf\",\n    strip_prefix = \"protobuf-VERSION\",\n    sha256 = ...,\n    url = ...,\n)\n\nload(\"@com_google_protobuf//:protobuf_deps.bzl\", \"protobuf_deps\")\n\nprotobuf_deps()\n\nload(\"@rules_java//java:rules_java_deps.bzl\", \"rules_java_dependencies\")\n\nrules_java_dependencies()\n\nload(\"@rules_java//java:repositories.bzl\", \"rules_java_toolchains\")\n\nrules_java_toolchains()\n\nload(\"@rules_python//python:repositories.bzl\", \"py_repositories\")\n\npy_repositories()\n</code></pre><h2>Protobuf Compiler Installation</h2><p>The protobuf compiler is written in C++. If you are using C++, please follow the <a href=\"https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md\">C++ Installation Instructions</a> to install protoc along with the C++ runtime.</p><p>For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our <a href=\"https://github.com/protocolbuffers/protobuf/releases\">GitHub release page</a>.</p><p>In the downloads section of each release, you can find pre-built binaries in zip packages: <code>protoc-$VERSION-$PLATFORM.zip</code>. It contains the protoc binary as well as a set of standard  files distributed along with protobuf.</p><p>If you are looking for an old version that is not available in the release page, check out the <a href=\"https://repo1.maven.org/maven2/com/google/protobuf/protoc/\">Maven repository</a>.</p><p>These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it's recommended to build your own protoc binary from source.</p><h2>Protobuf Runtime Installation</h2><p>Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:</p><p>If you want to learn from code examples, take a look at the examples in the <a href=\"https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples\">examples</a> directory.</p><p>To be alerted to upcoming changes in Protocol Buffers and connect with protobuf developers and users, <a href=\"https://groups.google.com/g/protobuf\">join the Google Group</a>.</p>","contentLength":3333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/Web-Dev-For-Beginners","url":"https://github.com/microsoft/Web-Dev-For-Beginners","date":1761791732,"author":"","guid":321073,"unread":true,"content":"<p>24 Lessons, 12 Weeks, Get Started as a Web Developer</p><p>Learn the fundamentals of web development with our 12-week comprehensive course by Microsoft Cloud Advocates. Each of the 24 lessons dives into JavaScript, CSS, and HTML through hands-on projects like terrariums, browser extensions, and space games. Engage with quizzes, discussions, and practical assignments. Enhance your skills and optimize your knowledge retention with our effective project-based pedagogy. Start your coding journey today!</p><p>Join the Azure AI Foundry Discord Community</p><p>Follow these steps to get started using these resources:</p><h4>Supported via GitHub Action (Automated &amp; Always Up-to-Date)</h4><p><strong>If you wish to have additional translations languages supported are listed <a href=\"https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md\">here</a></strong></p><p>Visit <a href=\"https://docs.microsoft.com/learn/student-hub/?WT.mc_id=academic-77807-sagibbon\"></a> where you will find beginner resources, Student packs and even ways to get a free certificate voucher. This is the page you want to bookmark and check from time to time as we switch out content monthly.</p><h3>📣 Announcement - New GitHub Copilot Agent mode challenges to complete!</h3><p>New Challenge added, look for \"GitHub Copilot Agent Challenge 🚀\" in most chapters. That's a new challenge for you to complete using GitHub Copilot and Agent mode. If you haven't used Agent mode before it's capable of not just generating text but can also create and edit files, run commands and more.</p><h3>📣 Announcement - <em>New Project to build using Generative AI</em></h3><p>New AI Assistant project just added, check it out <a href=\"https://raw.githubusercontent.com/microsoft/Web-Dev-For-Beginners/main/09-chat-project/README.md\">project</a></p><h3>📣 Announcement -  on Generative AI for JavaScript was just released</h3><p>Don't miss our new Generative AI curriculum!</p><ul><li>Lessons covering everything from basics to RAG.</li><li>Interact with historical characters using GenAI and our companion app.</li><li>Fun and engaging narrative, you'll be time traveling!</li></ul><p>Each lesson includes an assignment to complete, a knowledge check and a challenge to guide you on learning topics like:</p><ul><li>Prompting and prompt engineering</li><li>Text and image app generation</li></ul><p>, for each lesson, start with a pre-lecture quiz and follow through with reading the lecture material, completing the various activities and check your understanding with the post-lecture quiz.</p><p>To enhance your learning experience, connect with your peers to work on the projects together! Discussions are encouraged in our <a href=\"https://github.com/microsoft/Web-Dev-For-Beginners/discussions\">discussion forum</a> where our team of moderators will be available to answer your questions.</p><p>To further your education, we highly recommend exploring <a href=\"https://learn.microsoft.com/users/wirelesslife/collections/p1ddcy5jwy0jkm?WT.mc_id=academic-77807-sagibbon\">Microsoft Learn</a> for additional study materials.</p><h3>📋 Setting up your environment</h3><p>This curriculum has a development environment ready to go! As you get started you can choose to run the curriculum in a <a href=\"https://github.com/features/codespaces/\">Codespace</a> (<em>a browser-based, no installs needed environment</em>), or locally on your computer using a text editor such as <a href=\"https://code.visualstudio.com/?WT.mc_id=academic-77807-sagibbon\">Visual Studio Code</a>.</p><p>For you to easily save your work, it is recommended that you create your own copy of this repository. You can do this by clicking the  button at the top of the page. This will create a new repository in your GitHub account with a copy of the curriculum.</p><ol><li>: Click on the \"Fork\" button at the top-right corner of this page.</li><li>: <code>git clone https://github.com/microsoft/Web-Dev-For-Beginners.git</code></li></ol><h4>Running the curriculum in a Codespace</h4><p>In your copy of this repository that you created, click the  button and select . This will create a new Codespace for you to work in.</p><h4>Running the curriculum locally on your computer</h4><ol><li><p>Clone your repository to your computer. You can do this by clicking the  button and copying the URL:</p><p>Then, open <a href=\"https://code.visualstudio.com/docs/terminal/basics/?WT.mc_id=academic-77807-sagibbon\">Terminal</a> within <a href=\"https://code.visualstudio.com/?WT.mc_id=academic-77807-sagibbon\">Visual Studio Code</a> and run the following command, replacing  with the URL you just copied:</p><pre><code>git clone &lt;your-repository-url&gt;\n</code></pre></li><li><p>Open the folder in Visual Studio Code. You can do this by clicking  &gt;  and selecting the folder you just cloned.</p></li></ol><blockquote><p>Recommended Visual Studio Code extensions:</p><ul><li><a href=\"https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer&amp;WT.mc_id=academic-77807-sagibbon\">Live Server</a> - to preview HTML pages within Visual Studio Code</li><li><a href=\"https://marketplace.visualstudio.com/items?itemName=GitHub.copilot&amp;WT.mc_id=academic-77807-sagibbon\">Copilot</a> - to help you write code faster</li></ul></blockquote><ul><li>optional supplemental video</li><li>for project-based lessons, step-by-step guides on how to build the project</li></ul><blockquote><p>: All quizzes are contained in the Quiz-app folder, 48 total quizzes of three questions each. They are available <a href=\"https://ff-quizzes.netlify.app/web/\">here</a> the quiz app can be run locally or deployed to Azure; follow the instruction in the  folder.</p></blockquote><table><thead><tr></tr></thead></table><p>Our curriculum is designed with two key pedagogical principles in mind:</p><ul></ul><p>The program teaches the fundamentals of JavaScript, HTML, and CSS, as well as the latest tools and techniques used by today's web developers. Students will have the opportunity to develop hands-on experience by building a typing game, virtual terrarium, eco-friendly browser extension, space-invader-style game, and a banking app for businesses. By the end of the series, students will have gained a solid understanding of web development.</p><blockquote><p>🎓 You can take the first few lessons in this curriculum as a <a href=\"https://docs.microsoft.com/learn/paths/web-development-101/?WT.mc_id=academic-77807-sagibbon\">Learn Path</a> on Microsoft Learn!</p></blockquote><p>By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. We also wrote several starter lessons in JavaScript basics to introduce concepts, paired with a video from the \"<a href=\"https://channel9.msdn.com/Series/Beginners-Series-to-JavaScript/?WT.mc_id=academic-77807-sagibbon\">Beginners Series to: JavaScript</a>\" collection of video tutorials, some of whose authors contributed to this curriculum.</p><p>In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle.</p><p>While we have purposefully avoided introducing JavaScript frameworks to concentrate on the basic skills needed as a web developer before adopting a framework, a good next step to completing this curriculum would be learning about Node.js via another collection of videos: \"<a href=\"https://channel9.msdn.com/Series/Beginners-Series-to-Nodejs/?WT.mc_id=academic-77807-sagibbon\">Beginner Series to: Node.js</a>\".</p><p>You can run this documentation offline by using <a href=\"https://docsify.js.org/#/\">Docsify</a>. Fork this repo, <a href=\"https://docsify.js.org/#/quickstart\">install Docsify</a> on your local machine, and then in the root folder of this repo, type . The website will be served on port 3000 on your localhost: .</p><p>A PDF of all of the lessons can be found <a href=\"https://microsoft.github.io/Web-Dev-For-Beginners/pdf/readme.pdf\">here</a>.</p><p>Our team produces other courses! Check out:</p><h3>Azure / Edge / MCP / Agents</h3><p>If you get stuck or have any questions about building AI apps, join:</p><p>If you have product feedback or errors while building visit:</p><p>This repository is licensed under the MIT license. See the <a href=\"https://raw.githubusercontent.com/microsoft/Web-Dev-For-Beginners/main/LICENSE\">LICENSE</a> file for more information.</p>","contentLength":6298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"smartcontractkit/chainlink","url":"https://github.com/smartcontractkit/chainlink","date":1761791732,"author":"","guid":321074,"unread":true,"content":"<p>node of the decentralized oracle network, bridging on and off-chain computation</p><p><a href=\"https://chain.link/\">Chainlink</a> expands the capabilities of smart contracts by enabling access to real-world data and off-chain computation while maintaining the security and reliability guarantees inherent to blockchain technology.</p><p>Chainlink has an active and ever growing community. <a href=\"https://discordapp.com/invite/aSK4zew\">Discord</a> is the primary communication channel used for day to day communication, answering development questions, and aggregating Chainlink related content. Take a look at the <a href=\"https://raw.githubusercontent.com/smartcontractkit/chainlink/develop/docs/COMMUNITY.md\">community docs</a> for more information regarding Chainlink social accounts, news, and networking.</p><ol><li>Install <a href=\"https://nodejs.org/en/download/package-manager/\">NodeJS v20</a> &amp; <a href=\"https://pnpm.io/installation#using-npm\">pnpm v10 via npm</a>. \n  <ul><li>It might be easier long term to use <a href=\"https://nodejs.org/en/download/package-manager/#nvm\">nvm</a> to switch between node versions for different projects. For example, assuming $NODE_VERSION was set to a valid version of NodeJS, you could run: <code>nvm install $NODE_VERSION &amp;&amp; nvm use $NODE_VERSION</code></li></ul></li><li>Install <a href=\"https://wiki.postgresql.org/wiki/Detailed_installation_guides\">Postgres (&gt;= 12.x)</a>. It is recommended to run the latest major version of postgres. \n  <ul><li>Note if you are running the official Chainlink docker image, the highest supported Postgres version is 16.x due to the bundled client.</li><li>You should <a href=\"https://www.postgresql.org/docs/current/ssl-tcp.html\">configure Postgres</a> to use SSL connection (or for testing you can set  in your Postgres query string).</li></ul></li><li>Download Chainlink: <code>git clone https://github.com/smartcontractkit/chainlink &amp;&amp; cd chainlink</code></li><li>Build and install Chainlink: </li><li>Run the node: </li></ol><p>To build an unofficial testing-only image from a feature branch or PR. You can do one of the following:</p><ol><li>Add the  label to your PR and then either retry the  workflow, or push a new commit.</li></ol><p>Plugins are defined in yaml files within the  directory. Each plugin file is a yaml file and has a  prefix name. Plugins are installed with <a href=\"https://github.com/smartcontractkit/chainlink-common/tree/main/pkg/loop/cmd/loopinstall\">loopinstall</a>.</p><p>To install the plugins, run:</p><p>Some plugins (such as those in <code>plugins/plugins.private.yaml</code>) reference private GitHub repositories. To build these plugins, you must have a GITHUB_TOKEN environment variable set, or preferably use the <a href=\"https://cli.github.com/manual/gh\">gh</a> GitHub CLI tool to use the <a href=\"https://cli.github.com/manual/gh_auth_setup-git\">GitHub CLI credential helper</a> like:</p><pre><code># Sets up a credential helper.\ngh auth setup-git\n</code></pre><p>Then you can build the plugins with:</p><pre><code>make install-plugins-private\n</code></pre><p>To build the experimental \"plugins\" Chainlink docker image, you can run this from the root of the repository:</p><pre><code># The GITHUB_TOKEN is required to access private repos which are used by some plugins.\nexport GITHUB_TOKEN=$(gh auth token) # requires the `gh` cli tool.\nmake docker-plugins\n</code></pre><h3>Ethereum Execution Client Requirements</h3><p>In order to run the Chainlink node you must have access to a running Ethereum node with an open websocket connection. Any Ethereum based network will work once you've <a href=\"https://github.com/smartcontractkit/chainlink#configure\">configured</a> the chain ID. Ethereum node versions currently tested and supported:</p><p>[Supported but broken] These clients are supported by Chainlink, but have bugs that prevent Chainlink from working reliably on these execution clients.</p><p>We cannot recommend specific version numbers for ethereum nodes since the software is being continually updated, but you should usually try to run the latest version available.</p><h2>Running a local Chainlink node</h2><p>: By default, chainlink will run in TLS mode. For local development you can disable this by using a  using  and setting the TOML fields:</p><pre><code>[WebServer]\nSecureCookies = false\nTLS.HTTPSPort = 0\n\n[Insecure]\nDevWebServer = true\n</code></pre><p>Alternatively, you can generate self signed certificates using <code>tools/bin/self-signed-certs</code> or <a href=\"https://github.com/smartcontractkit/chainlink/wiki/Creating-Self-Signed-Certificates\">manually</a>.</p><p>To start your Chainlink node, simply run:</p><p>Chainlink provides a remote CLI client as well as a UI. Once your node has started, you can open a new terminal window to use the CLI. You will need to log in to authorize the client first:</p><p>(You can also set <code>ADMIN_CREDENTIALS_FILE=/path/to/credentials/file</code> in future if you like, to avoid having to login again).</p><p>Now you can view your current jobs with:</p><p>To find out more about the Chainlink CLI, you can always run .</p><p>Check out the <a href=\"https://docs.chain.link/\">doc</a> pages on <a href=\"https://docs.chain.link/docs/jobs/\">Jobs</a> to learn more about how to create Jobs.</p><p>Node configuration is managed by a combination of environment variables and direct setting via API/UI/CLI.</p><p>External adapters are what make Chainlink easily extensible, providing simple integration of custom computations and specialized APIs. A Chainlink node communicates with external adapters via a simple REST API.</p><h2>Verify Official Chainlink Releases</h2><p>It is encourage for any node operator building from the official Chainlink docker image to verify the tagged release version was did indeed built from this workflow.</p><pre><code># tag is the tagged release version - ie. 2.16.0\ncosign verify index.docker.io/smartcontract/chainlink:${tag} \\\n      --certificate-oidc-issuer https://token.actions.githubusercontent.com \\\n      --certificate-identity \"https://github.com/smartcontractkit/chainlink/.github/workflows/build-publish.yml@refs/tags/v${tag}\"\n</code></pre><p>Using the  command will install the correct version.</p><ol start=\"4\"><li>Generate and compile static assets:</li></ol><ol start=\"5\"><li>Prepare your development environment:</li></ol><p>The tests require a postgres database. In turn, the environment variable  must be set to value that can connect to  database, and the user must be able to create and drop the given  database.</p><p>Note: Other environment variables should not be set for all tests to pass</p><p>There helper script for initial setup to create an appropriate test user. It requires postgres to be running on localhost at port 5432. You will be prompted for the  user password</p><p>This script will save the  in </p><p>Changes to database require migrations to be run. Similarly, 'ing the repo may require migrations to run. After the one-time setup above:</p><pre><code>source .dbenv\nmake testdb\n</code></pre><p>If you encounter the error <code>database accessed by other users (SQLSTATE 55006) exit status 1</code> and you want force the database creation then use</p><pre><code>source .dbenv\nmake testdb-force\n</code></pre><ul><li>The  flag can be used to limit CPU usage, for running tests in the background () - the default is </li><li>The  flag can be used to limit the number of  tested concurrently, if they are interferring with one another ()</li><li>The  flag skips tests which depend on the database, for quickly spot checking simpler tests in around one minute</li></ul><p>As of Go 1.1, the runtime includes a data race detector, enabled with the  flag. This is used in CI via the <code>tools/bin/go_core_race_tests</code> script. If the action detects a race, the artifact on the summary page will include  files with detailed stack traces.</p><blockquote><p><em><strong>It will not issue false positives, so take its warnings seriously.</strong></em></p></blockquote><p>For local, targeted race detection, you can run:</p><pre><code>GORACE=\"log_path=$PWD/race\" go test -race ./core/path/to/pkg -count 10\nGORACE=\"log_path=$PWD/race\" go test -race ./core/path/to/pkg -count 100 -run TestFooBar/sub_test\n</code></pre><p>As of Go 1.18, fuzz tests  are included as part of the normal test suite, so existing cases are executed with .</p><p>Additionally, you can run active fuzzing to search for new cases:</p><pre><code>go test ./pkg/path -run=XXX -fuzz=FuzzTestName\n</code></pre><p>This repository contains three Go modules:</p><pre><code>flowchart RL\n    github.com/smartcontractkit/chainlink/v2\n    github.com/smartcontractkit/chainlink/integration-tests --&gt; github.com/smartcontractkit/chainlink/v2\n    github.com/smartcontractkit/chainlink/core/scripts --&gt; github.com/smartcontractkit/chainlink/v2\n\n</code></pre><p>The  and  modules import the root module using a relative replace in their  files, so dependency changes in the root  often require changes in those modules as well. After making a change,  can be run on all three modules using:</p><p>Go generate is used to generate mocks in this project. Mocks are generated with <a href=\"https://github.com/vektra/mockery\">mockery</a> and live in core/internal/mocks.</p><p>Nix defines a declarative, reproducible development environment. Flakes version use deterministic, frozen () dependencies to gain more consistency/reproducibility on the built artifacts.</p><ol start=\"2\"><li>Run . You will be put in shell containing all the dependencies.</li></ol><ul><li>Optionally, <code>nix develop --command $SHELL</code> will make use of your current shell instead of the default (bash).</li><li>You can use  to enable it automatically when -ing into the folder; for that, enable <a href=\"https://github.com/nix-community/nix-direnv\">nix-direnv</a> and  on it.</li></ul><ol start=\"3\"><li>Create a local postgres database:</li></ol><pre><code>mkdir -p $PGDATA &amp;&amp; cd $PGDATA/\ninitdb\npg_ctl -l postgres.log -o \"--unix_socket_directories='$PWD'\" start\ncreatedb chainlink_test -h localhost\ncreateuser --superuser --password chainlink -h localhost\n# then type a test password, e.g.: chainlink, and set it in shell.nix CL_DATABASE_URL\n</code></pre><ol start=\"4\"><li>When re-entering project, you can restart postgres: <code>cd $PGDATA; pg_ctl -l postgres.log -o \"--unix_socket_directories='$PWD'\" start</code> Now you can run tests or compile code as usual.</li><li>When you're done, stop it: <code>cd $PGDATA; pg_ctl -o \"--unix_socket_directories='$PWD'\" stop</code></li></ol><p>We use <a href=\"https://github.com/changesets/changesets\">changesets</a> to manage versioning for libs and the services.</p><p>Every PR that modifies any configuration or code, should most likely accompanied by a changeset file.</p><ol><li>Install  if it is not already installed - <a href=\"https://pnpm.io/installation\">docs</a>.</li></ol><p>Either after or before you create a commit, run the  command to create an accompanying changeset entry which will reflect on the CHANGELOG for the next release.</p><p>Contributions are welcome to Chainlink's source code.</p>","contentLength":8855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"allenai/olmocr","url":"https://github.com/allenai/olmocr","date":1761791732,"author":"","guid":321075,"unread":true,"content":"<p>Toolkit for linearizing PDFs for LLM datasets/training</p><p>A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.</p><ul><li>Convert PDF, PNG, and JPEG based documents into clean Markdown</li><li>Support for equations, tables, handwriting, and complex formatting</li><li>Automatically removes headers and footers</li><li>Convert into text with a natural reading order, even in the presence of figures, multi-column layouts, and insets</li><li>Efficient, less than $200 USD per million pages converted</li><li>(Based on a 7B parameter VLM, so it requires a GPU)</li></ul><ul><li>October 21, 2025 - v0.4.0 - <a href=\"https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8\">New model release</a>, boosts olmOCR-bench score by ~4 points using synthetic data and introduces RL training.</li><li>August 13, 2025 - v0.3.0 - <a href=\"https://huggingface.co/allenai/olmOCR-7B-0825-FP8\">New model release</a>, fixes auto-rotation detection, and hallucinations on blank documents.</li><li>July 24, 2025 - v0.2.1 - <a href=\"https://huggingface.co/allenai/olmOCR-7B-0725-FP8\">New model release</a>, scores 3 points higher on <a href=\"https://github.com/allenai/olmocr/tree/main/olmocr/bench\">olmOCR-Bench</a>, also runs significantly faster because it's default FP8, and needs much fewer retries per document.</li><li>July 23, 2025 - v0.2.0 - New cleaned up <a href=\"https://github.com/allenai/olmocr/tree/main/olmocr/train\">trainer code</a>, makes it much simpler to train olmOCR models yourself.</li><li>June 17, 2025 - v0.1.75 - Switch from sglang to vllm based inference pipeline, updated docker image to CUDA 12.8.</li><li>May 23, 2025 - v0.1.70 - Official docker support and images are now available! <a href=\"https://raw.githubusercontent.com/allenai/olmocr/main/#using-docker\">See Docker usage</a></li><li>May 19, 2025 - v0.1.68 - <a href=\"https://github.com/allenai/olmocr/tree/main/olmocr/bench\">olmOCR-Bench</a> launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.</li><li>Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.</li><li>Feb 25, 2025 - v0.1.58 - Initial public launch and demo.</li></ul><p><a href=\"https://github.com/allenai/olmocr/tree/main/olmocr/bench\"></a>: We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><ul><li>Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 15 GB of GPU RAM</li></ul><p>You will need to install poppler-utils and additional fonts for rendering PDF images.</p><p>Install dependencies (Ubuntu/Debian)</p><pre><code>sudo apt-get update\nsudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools\n</code></pre><p>Set up a conda environment and install olmocr. The requirements for running olmOCR are difficult to install in an existing python environment, so please do make a clean python environment to install into.</p><pre><code>conda create -n olmocr python=3.11\nconda activate olmocr\n\n# For CPU-only operations, ex running the benchmark\npip install olmocr[bench]\n\n# For actually converting the files with your own GPU\npip install olmocr[gpu]  --extra-index-url https://download.pytorch.org/whl/cu128\n\n# Recommended: Install flash infer for faster inference on GPU\npip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl\n</code></pre><p>For quick testing, try the <a href=\"https://olmocr.allen.ai/\">web demo</a>. To run locally, a GPU is required, as inference is powered by <a href=\"https://github.com/sgl-project/sglang\">sglang</a> under the hood.</p><pre><code># Download a sample PDF\ncurl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf\n\n# Convert it to markdown\npython -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf\n</code></pre><pre><code>python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png\n</code></pre><pre><code>python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf\n</code></pre><p>With the addition of the  flag, results will be stored as markdown files inside of <code>./localworkspace/markdown/</code>.</p><p>The  workspace folder will then have both <a href=\"https://github.com/allenai/dolma\">Dolma</a> and markdown files (if using ).</p><pre><code>cat localworkspace/markdown/olmocr-sample.md \n</code></pre><pre><code>olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models\n...\n</code></pre><h3>Using an Inference Provider or External Server</h3><p>If you have a vLLM server already running elsewhere (or any inference platform implementing the OpenAI API), you can point olmOCR to use it instead of spawning a local instance:</p><pre><code># Use external vLLM server instead of local one\npython -m olmocr.pipeline ./localworkspace --server http://remote-server:8000/v1 --markdown --pdfs tests/gnarly_pdfs/*.pdf\n</code></pre><p>The served model name should be . An example vLLM launch command would be:</p><pre><code>vllm serve allenai/olmOCR-2-7B-1025-FP8 --served-model-name olmocr --max-model-len 16384\n</code></pre><h4>Verified External Providers</h4><p>We have tested  on these external model providers and confirmed that they work</p><table><thead><tr></tr></thead><tbody><tr><td><code>python -m olmocr.pipeline ./localworkspace1 --server https://ai2endpoints.cirrascale.ai/api --api_key sk-XXXXXXX --model olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf</code></td></tr><tr><td><code>python -m olmocr.pipeline ./localworkspace1 --server https://api.deepinfra.com/v1/openai --api_key DfXXXXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf</code></td></tr><tr><td><code>python -m olmocr.pipeline ./localworkspace1 --server https://api.parasail.io/v1 --api_key psk-XXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf</code></td></tr></tbody></table><ul><li>: Defines the OpenAI-compatible endpoint: ex <code>https://api.deepinfra.com/v1/openai</code></li><li>: Your API key, bassed in via Authorization Bearer HTTP header</li><li>: You may want a smaller number of pages per group as many external provides have lower concurrent request limits</li><li>: The model identifier, ex. , different providers have different names, and if you run locally, you can use </li><li>Other arguments work the same as with local inference</li></ul><h3>Multi-node / Cluster Usage</h3><p>If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.</p><p>For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.</p><pre><code>python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf\n</code></pre><p>Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.</p><pre><code>python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace\n</code></pre><p>If you are at Ai2 and want to linearize millions of PDFs efficiently using <a href=\"https://www.beaker.org\">beaker</a>, just add the  flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.</p><pre><code>python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4\n</code></pre><pre><code>docker pull alleninstituteforai/olmocr:latest\n</code></pre><p>To run the container interactively:</p><pre><code>docker run -it --gpus all --name olmocr_container alleninstituteforai/olmocr:latest /bin/bash\n</code></pre><p>If you want to access your local files inside the container, use volume mounting:</p><pre><code>docker run -it --gpus all \\\n  -v /path/to/your/local/files:/local_files \\\n  --name olmocr_container \\\n  alleninstituteforai/olmocr:latest /bin/bash\n</code></pre><p>All dependencies are already installed. Once you’re inside the container, you can run olmOCR commands. For example:</p><pre><code>curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf\n\npython -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf\n</code></pre><blockquote><p>You can also visit our Docker repository on <a href=\"https://hub.docker.com/r/alleninstituteforai/olmocr\">Docker Hub</a>.</p></blockquote><h3>Full documentation for the pipeline</h3><pre><code>python -m olmocr.pipeline --help\nusage: pipeline.py [-h] [--pdfs [PDFS ...]] [--model MODEL] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP] [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS]\n                   [--apply_filter] [--stats] [--markdown] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM] [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--guided_decoding] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--max_model_len MAX_MODEL_LEN]\n                   [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--data-parallel-size DATA_PARALLEL_SIZE] [--port PORT] [--server SERVER] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER] [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]\n                   workspace\n\nManager for running millions of PDFs through a batch inference pipeline\n\npositional arguments:\n  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/\n\noptions:\n  -h, --help            show this help message and exit\n  --pdfs [PDFS ...]     Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths\n  --model MODEL         Path where the model is located, allenai/olmOCR-7B-0725-FP8 is the default, can be local, s3, or hugging face.\n  --workspace_profile WORKSPACE_PROFILE\n                        S3 configuration profile for accessing the workspace\n  --pdf_profile PDF_PROFILE\n                        S3 configuration profile for accessing the raw pdf documents\n  --pages_per_group PAGES_PER_GROUP\n                        Aiming for this many pdf pages per work item group\n  --max_page_retries MAX_PAGE_RETRIES\n                        Max number of times we will retry rendering a page\n  --max_page_error_rate MAX_PAGE_ERROR_RATE\n                        Rate of allowable failed pages in a document, 1/250 by default\n  --workers WORKERS     Number of workers to run at a time\n  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam\n  --stats               Instead of running any job, reports some statistics about the current workspace\n  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs\n  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM\n                        Dimension on longest side to use for rendering the pdf pages\n  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN\n                        Maximum amount of anchor text to use (characters), not used for new models\n  --guided_decoding     Enable guided decoding for model YAML type outputs\n\nVLLM arguments:\n  --gpu-memory-utilization GPU_MEMORY_UTILIZATION\n                        Fraction of VRAM vLLM may pre-allocate for KV-cache (passed through to vllm serve).\n  --max_model_len MAX_MODEL_LEN\n                        Upper bound (tokens) vLLM will allocate KV-cache for, lower if VLLM won't start\n  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE\n                        Tensor parallel size for vLLM\n  --data-parallel-size DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE\n                        Data parallel size for vLLM\n  --port PORT           Port to use for the VLLM server\n  --server SERVER       URL of external vLLM (or other compatible provider)\n                        server (e.g., http://hostname:port). If provided,\n                        skips spawning local vLLM instance\n\nbeaker/cluster execution:\n  --beaker              Submit this job to beaker instead of running locally\n  --beaker_workspace BEAKER_WORKSPACE\n                        Beaker workspace to submit to\n  --beaker_cluster BEAKER_CLUSTER\n                        Beaker clusters you want to run on\n  --beaker_gpus BEAKER_GPUS\n                        Number of gpu replicas to run\n  --beaker_priority BEAKER_PRIORITY\n                        Beaker priority level for the job\n</code></pre><p>There are some nice reusable pieces of the code that may be useful for your own projects:</p><p>For olmOCR v1 and OlmOCR-bench:</p><pre><code>@misc{olmocrbench,\n      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},\n      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},\n      year={2025},\n      eprint={2502.18443},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.18443},\n}\n</code></pre><p>For olmOCR v2 Unit Testing Rewards with RL:</p><pre><code>@misc{olmocr2,\n      title={olmOCR 2: Unit Test Rewards for Document OCR}, \n      author={Jake Poznanski and Luca Soldaini and Kyle Lo},\n      year={2025},\n      eprint={2510.19817},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2510.19817}, \n}\n</code></pre>","contentLength":12014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beingpax/VoiceInk","url":"https://github.com/Beingpax/VoiceInk","date":1761791732,"author":"","guid":321076,"unread":true,"content":"<p>Voice-to-text app for macOS to transcribe what you say to text almost instantly</p><p>VoiceInk is a native macOS application that transcribes what you say to text almost instantly. You can find all the information and download the app from <a href=\"https://tryvoiceink.com\">here</a>.</p><p>After dedicating the past 5 months to developing this app, I've decided to open source it for the greater good.</p><p>My goal is to make it <strong>the most efficient and privacy-focused voice-to-text solution for macOS</strong> that is a joy to use. While the source code is now open for experienced developers to build and contribute, purchasing a license helps support continued development and gives you access to automatic updates, priority support, and upcoming features.</p><ul><li>🎙️ : Local AI models that transcribe your voice to text with 99% accuracy, almost instantly</li><li>🔒 : 100% offline processing ensures your data never leaves your device</li><li>⚡ : Intelligent app detection automatically applies your perfect pre-configured settings based on the app/ URL you're on</li><li>🧠 : Smart AI that understands your screen content and adapts to the context</li><li>🎯 : Configurable keyboard shortcuts for quick recording and push-to-talk functionality</li><li>📝 : Train the AI to understand your unique terminology with custom words, industry terms, and smart text replacements</li><li>🔄 : Instantly switch between AI-powered modes optimized for different writing styles and contexts</li><li>🤖 : Built-in voice assistant mode for a quick chatGPT like conversational assistant</li></ul><p>Get the latest version with a free trial from <a href=\"https://tryvoiceink.com\">tryvoiceink.com</a>. Your purchase helps me work on VoiceInk full-time and continuously improve it with new features and updates.</p><p>Alternatively, you can install VoiceInk via :</p><pre><code>brew install --cask voiceink\n</code></pre><p>As an open-source project, you can build VoiceInk yourself by following the instructions in <a href=\"https://raw.githubusercontent.com/Beingpax/VoiceInk/main/BUILDING.md\">BUILDING.md</a>. However, the compiled version includes additional benefits like automatic updates, priority support via Discord and email, and helps fund ongoing development.</p><p>We welcome contributions! However, please note that all contributions should align with the project's goals and vision. Before starting work on any feature or fix:</p><p>This project is licensed under the GNU General Public License v3.0 - see the <a href=\"https://raw.githubusercontent.com/Beingpax/VoiceInk/main/LICENSE\">LICENSE</a> file for details.</p><p>If you encounter any issues or have questions, please:</p><ol><li>Check the existing issues in the GitHub repository</li><li>Create a new issue if your problem isn't already reported</li><li>Provide as much detail as possible about your environment and the problem</li></ol><ul><li><a href=\"https://github.com/ggerganov/whisper.cpp\">whisper.cpp</a> - High-performance inference of OpenAI's Whisper model</li><li><a href=\"https://github.com/FluidInference/FluidAudio\">FluidAudio</a> - Used for Parakeet model implementation</li></ul>","contentLength":2579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"patchy631/ai-engineering-hub","url":"https://github.com/patchy631/ai-engineering-hub","date":1761705381,"author":"","guid":319593,"unread":true,"content":"<p>In-depth tutorials on LLMs, RAGs and real-world AI agent applications.</p><p>Welcome to the  - your comprehensive resource for learning and building with AI!</p><p>AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:</p><ul><li><strong>93+ Production-Ready Projects</strong> across all skill levels</li><li>In-depth tutorials on <strong>LLMs, RAG, Agents, and more</strong></li><li>Real-world  applications</li><li>Examples to implement, adapt, and scale in your projects</li></ul><p>Whether you're a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.</p><p>New to AI Engineering? Start here:</p><h2>📬 Stay Updated with Our Newsletter!</h2><p><strong>Get a FREE Data Science eBook</strong> 📖 with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. <a href=\"https://join.dailydoseofds.com\">Subscribe now!</a></p><p>Perfect for getting started with AI engineering. These projects focus on single components and straightforward implementations.</p><ul><li><a href=\"https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/llama-ocr\"></a> - 100% local OCR app with Llama 3.2 and Streamlit</li><li><a href=\"https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/gemma3-ocr\"></a> - Local OCR with structured text extraction using Gemma-3</li></ul><p>Multi-component systems, agentic workflows, and advanced features for experienced practitioners.</p><h4>MCP (Model Context Protocol)</h4><h4>Model Comparison &amp; Evaluation</h4><p>Complex systems, fine-tuning, production deployments, and cutting-edge implementations.</p><h4>Fine-tuning &amp; Model Development</h4><h4>Advanced MCP &amp; Infrastructure</h4><h2>📢 Contribute to the AI Engineering Hub!</h2><p>We welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Here's how to get involved:</p><ol><li>Create a new branch for your contribution</li><li>Submit a  and describe the improvements</li></ol><p>This repository is licensed under the MIT License - see the <a href=\"https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/LICENSE\">LICENSE</a> file for details.</p><p>For discussions, suggestions, and more, feel free to <a href=\"https://github.com/patchy631/ai-engineering/issues\">create an issue</a> or reach out directly!</p>","contentLength":1920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"harvard-edge/cs249r_book","url":"https://github.com/harvard-edge/cs249r_book","date":1761705381,"author":"","guid":319594,"unread":true,"content":"<p>Introduction to Machine Learning Systems</p><p><em>Principles and Practices of Engineering Artificially Intelligent Systems</em></p><p>The  for learning how to engineer AI systems. It began in Harvard’s CS249r course by <a href=\"https://vijay.seas.harvard.edu\">Prof. Vijay Janapa Reddi</a>. Today, it supports classrooms, study groups, and independent learners around the world.</p><blockquote><p> Accessible AI systems education for anyone, anywhere. One chapter at a time.</p></blockquote><p><em>Many students learn how to train ML models but not how to build and engineer the systems that make those models useful in the real world. As AI becomes more capable, the real bottleneck will not just be algorithms, but engineers who can design efficient, scalable, and sustainable systems that put those algorithms to work responsibly.</em></p><p>This book is part of a broader personal mission to <strong>educate one million learners worldwide</strong> in the foundations of AI systems engineering. The long term impact of AI will be shaped by a generation of engineers and builders who know how to turn ideas into working systems.</p><h3>What Makes This Book Different</h3><p>This project is a living textbook. I keep it updated as the field grows, with community input along the way.</p><p>AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This book is built around those stable foundations.</p><p>Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those “AI bricks” are the solid systems principles that make AI work.</p><p>Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner.</p><p>Thank you for being a part of the story 🙏</p><p>This textbook gives you a <strong>systems level understanding of machine learning</strong>, bridging the gap between algorithms and the real world infrastructure that makes them work. You will learn how to <strong>design, build, and reason about</strong> the components that make modern AI possible.</p><table><tbody><tr><td>How to design and structure end-to-end ML systems that are scalable, modular, and maintainable</td></tr><tr><td>How to build reliable pipelines for collection, labeling, and processing</td></tr><tr><td>How to turn trained models into robust, production-ready services</td></tr><tr><td>How to operate, monitor, and sustain AI systems over time</td></tr><tr><td>How to deploy ML on mobile, embedded, and resource-constrained devices</td></tr><tr><td><strong>Responsible and Sustainable AI</strong></td><td>How to design systems with privacy, security, and environmental impact in mind</td></tr></tbody></table><div align=\"center\"><p>Star the repository. It signals interest and helps us secure resources for open education.</p><p>Your support helps provide TinyML kits, workshops, and infrastructure for learners worldwide.</p></div><h2>🌐 Community and Resources</h2><h2>🎯 For Different Audiences</h2><pre><code># Read online\nopen https://mlsysbook.ai\n\n# Download PDF\ncurl -O https://mlsysbook.ai/pdf\n\n# Download EPUB\ncurl -O https://mlsysbook.ai/epub\n</code></pre><pre><code>git clone https://github.com/harvard-edge/cs249r_book.git\ncd cs249r_book\n\n# Quick setup\n./binder setup\n./binder doctor\n\n# Fast iteration\n./binder preview intro\n./binder build intro\n./binder html intro\n./binder pdf intro\n./binder epub intro\n\n# Build the whole book\n./binder build\n./binder html\n./binder pdf\n./binder epub\n\n# Utilities\n./binder help\n./binder list\n./binder status\n</code></pre><pre><code>@inproceedings{reddi2024mlsysbook,\n  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},\n  author       = {Reddi, Vijay Janapa},\n  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},\n  pages        = {41--42},\n  year         = {2024},\n  organization = {IEEE},\n  url          = {https://mlsysbook.org}\n}\n</code></pre><p>This work is licensed under <strong>Creative Commons Attribution–NonCommercial–ShareAlike 4.0 International</strong> (CC BY-NC-SA 4.0). You may share and adapt the material for non-commercial purposes with appropriate credit.</p><p>Thanks goes to these wonderful people who have contributed to making this resource better for everyone:</p><div align=\"center\"><p><strong>Made with ❤️ for AI learners worldwide</strong></p><p>Our goal is to educate 1 million AI systems engineers for the future at the edge of AI.</p></div>","contentLength":4093,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"iam-veeramalla/aws-devops-zero-to-hero","url":"https://github.com/iam-veeramalla/aws-devops-zero-to-hero","date":1761705381,"author":"","guid":319595,"unread":true,"content":"<p>AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples.</p><p>AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples. Each day's class will provide real-time knowledge on AWS services, allowing you to apply what you've learned and gain practical skills in working with AWS in a DevOps context.</p><h2>Day 1: Introduction to AWS</h2><p>You will learn what is private and public cloud. Why companies are moving to public cloud, what are the advantages of moving to cloud.</p><p>Also, you will be introduced to the basics of AWS, including the core services and their significance in DevOps practices. Finally learn how to set up an AWS account and navigate the AWS Management Console.</p><h2>Day 2: IAM (Identity and Access Management)</h2><p>You will explore IAM, which is used for managing access to AWS resources. You'll learn how to create IAM users, groups, and roles, and how to apply permissions and security best practices to ensure proper access control.</p><p>You'll dive into EC2, which provides virtual servers in the cloud. You'll learn how to launch EC2 instances, connect to them using SSH, and understand key concepts such as instance types, security groups, and key pairs.</p><p>: Deploy a simple web application(such as jenkins) on the ec2 instance and access the application from outside AWS.</p><h2>Day 4: AWS Networking (VPC)</h2><p>You'll explore AWS networking concepts, with a specific focus on VPC (Virtual Private Cloud). You'll learn how to create and configure VPCs, subnets, and route tables, enabling you to design and manage the network infrastructure for your applications.</p><p>This day emphasizes security best practices in AWS. You'll learn how to implement security measures such as security groups, network ACLs (Access Control Lists), and IAM policies to ensure the confidentiality, integrity, and availability of your AWS resources.</p><p> Configure and manage a domain name using Route 53. You'll register a domain, set up DNS records, and explore advanced features such as health checks, routing policies, and DNS-based failover.</p><h2>Day 7: Secure VPC Setup with EC2 Instances</h2><ul><li><p>Design and configure a VPC: Create a VPC with custom IP ranges. Set up public and private subnets. Configure route tables and associate subnets.</p></li><li><p>Implement network security: Set up network access control lists (ACLs) to control inbound and outbound traffic. Configure security groups for EC2 instances to allow specific ports and protocols.</p></li><li><p>Provision EC2 instances: Launch EC2 instances in both the public and private subnets. Configure security groups for the instances to allow necessary traffic. Create and assign IAM roles to the instances with appropriate permissions.</p></li><li><p>Networking and routing: Set up an internet gateway to allow internet access for instances in the public subnet. Configure NAT gateway or NAT instance to enable outbound internet access for instances in the private subnet. Create appropriate route tables and associate them with the subnets.</p></li><li><p>SSH key pair and access control: Generate an SSH key pair and securely store the private key. Configure the instances to allow SSH access only with the generated key pair. Implement IAM policies and roles to control access and permissions to AWS resources.</p></li><li><p>Test and validate the setup: SSH into the EC2 instances using the private key and verify connectivity. Test network connectivity between instances in different subnets. Validate security group rules and network ACL settings.</p></li></ul><p>By implementing this project, you'll gain hands-on experience in setting up a secure VPC with EC2 instances, implementing networking and routing, configuring security groups and IAM roles, and ensuring proper access control. This project will provide a practical understanding of how these AWS services work together to create a secure and scalable infrastructure for your applications.</p><h2>Day 8: AWS Interview Questions on EC2, IAM and VPC</h2><p>This day focuses on Amazon S3, a scalable object storage service. You'll learn how to create S3 buckets, upload and download objects, and organize data using S3 features like versioning, lifecycle policies, and access control.</p><h2>Day 11: AWS CloudFormation</h2><p>This day introduces Infrastructure as Code (IaC) using AWS CloudFormation. You'll learn how to create CloudFormation templates to automate the provisioning of resources, manage stacks, and ensure consistent infrastructure across deployments.</p><p> You'll work on creating a CloudFormation template that provisions a fully configured application stack, including EC2 instances, networking components, and security groups.</p><p>This day focuses on AWS CodeCommit, a managed source control service. You'll learn how to set up a Git repository in CodeCommit, collaborate with team members, and manage version control of your codebase.</p><p> You'll configure a CodeCommit repository for a team project, including setting up access control and collaboration workflows.</p><p>You'll dive into AWS CodePipeline, a fully managed continuous delivery service. You'll learn how to build end-to-end CI/CD pipelines by configuring source, build, and deployment stages, automating the entire software release process.</p><p> You'll create a CI/CD pipeline using CodePipeline for an application deployment, including source code integration, build, and automatic deployment to a target environment.</p><p>This day focuses on AWS CodeBuild, a fully managed build service. You'll learn how to configure build projects in CodeBuild, define build specifications, and perform build and testing processes.</p><p> You'll configure and run CodeBuild for a project, including defining build specifications and integrating with other AWS services.</p><p>You'll explore AWS CodeDeploy, a service for automating application deployments to various compute environments. You'll learn how to create deployment groups, configure deployment strategies, and perform automatic rollbacks if necessary.</p><p> You'll implement a Blue/Green deployment strategy for a sample application using CodeDeploy, ensuring zero-downtime deployments and easy rollback options.</p><p>This day focuses on monitoring AWS resources using AWS CloudWatch. You'll learn how to create alarms, set up notifications, and collect metrics to gain insights into the health and performance of your applications and infrastructure.</p><p> You'll set up CloudWatch alarms for critical metrics of an application, define appropriate threshold conditions, and configure notification actions.</p><p>This day introduces serverless computing with AWS Lambda. You'll learn how to create and deploy serverless functions, trigger them based on events, and leverage Lambda to build scalable and event-driven architectures.</p><h2>Day 18: AWS CloudWatch Events and EventBridge</h2><p>This day focuses on AWS CloudWatch Events and EventBridge, services for event-driven architectures. You'll learn how to create event rules, configure event targets, and build serverless event-driven workflows.</p><p> You'll build a serverless event-driven workflow using CloudWatch Events and EventBridge, demonstrating the integration and automation of different AWS services based on events.</p><p>If you've never heard of CDN or CloudFront before, don't worry, we will start from scratch and gradually build up your understanding. By the end, you'll be well-versed in these technologies.</p><p> You'll configure a s3 bucket to host a static website and learn how to serve the requests to this website through CDN that is AWS Cloud Front.</p><h2>Day 20: AWS ECR (Elastic Container Registry)</h2><p>You'll explore AWS ECR, a fully managed container registry for storing and managing container images. You'll learn how to push and pull Docker images to and from ECR, enabling seamless integration with ECS and other container services.</p><p> You'll build a CI/CD pipeline that automatically builds, pushes, and deploys Docker images to ECR, ensuring streamlined container image management.</p><h2>Day 21: AWS ECS (Elastic Container Service)</h2><p>This day focuses on AWS ECS, a fully managed container orchestration service. You'll learn how to run and manage containers using ECS, including creating task definitions, managing services, and scaling with auto-scaling capabilities.</p><p> You'll deploy a multi-container application using ECS, configure auto-scaling policies, and ensure high availability and efficient resource utilization.</p><h2>Day 22: AWS EKS (Elastic Kubernetes Service)</h2><p>This day introduces AWS EKS, a fully managed Kubernetes service. You'll learn how to deploy and manage Kubernetes clusters using EKS, including launching worker nodes, configuring networking, and deploying applications using Kubernetes manifests.</p><p> You'll deploy a sample application on EKS using Kubernetes manifests, demonstrating the capabilities of running containerized applications on a managed Kubernetes service.</p><h2>Day 23: AWS Systems Manager</h2><p>This day focuses on AWS Secrets Manager, a service for storing and managing secrets such as database credentials, API keys, and other sensitive information. You'll learn how to store, retrieve, and rotate secrets securely in your applications.</p><p> You'll configure Secrets Manager to store and manage secrets, integrate secret retrieval in an application, and implement secret rotation policies.</p><h2>Day 24: Create Infrastructure using Terraform</h2><p>This day focusses on creating infrastructure using Terraform with real time example.</p><p> You'll create a VPC and deploy 2 applications in different availability zones. We will also create a load balancer to balance the load between the instances automatically.</p><h2>Day 25: AWS CloudTrail and Config</h2><p>You'll explore AWS CloudTrail and AWS Config, which provide auditing and compliance capabilities. You'll learn how to track API calls using CloudTrail and ensure compliance with AWS Config rules.</p><p> You'll configure CloudTrail to log API activities and set up AWS Config rules to enforce compliance policies for your AWS resources.</p><h2>Day 26: AWS Elastic Load Balancer</h2><p>You'll explore AWS Elastic Load Balancer, a service for distributing incoming application traffic across multiple targets. You'll learn how to configure and manage load balancers to ensure high availability, fault tolerance, and scalability.</p><p> You'll configure an Elastic Load Balancer for an application, define target groups, and observe the load balancing behavior across instances.</p><h2>Day 27: 500 AWS interview questions and answers topic wise for interviews.</h2><p>This day focuses on learning how to migrate applications to AWS cloud. What are the most popular strategies and tools used to achieve the cloud migration.</p><h2>Day 28: AWS Cloud Migration Strategies and Tools</h2><p>This day focuses on learning how to migrate applications to AWS cloud. What are the most popular strategies and tools used to achieve the cloud migration.</p><h2>Day 29: AWS Best Practices and Job Preparation</h2><p>On the final day, you'll review best practices for AWS services, including security, cost optimization and performance.</p><h2>Day 30: AWS Project with RDS</h2>","contentLength":10917,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"juanfont/headscale","url":"https://github.com/juanfont/headscale","date":1761705381,"author":"","guid":319596,"unread":true,"content":"<p>An open source, self-hosted implementation of the Tailscale control server</p><p>An open source, self-hosted implementation of the Tailscale control server.</p><p> Always select the same GitHub tag as the released version you use to ensure you have the correct example configuration. The  branch might contain unreleased changes. The documentation is available for stable and development versions:</p><p>Everything in Tailscale is Open Source, except the GUI clients for proprietary OS (Windows and macOS/iOS), and the control server.</p><p>The control server works as an exchange point of Wireguard public keys for the nodes in the Tailscale network. It assigns the IP addresses of the clients, creates the boundaries between each user, enables sharing machines between users, and exposes the advertised routes of your nodes.</p><p>Headscale aims to implement a self-hosted, open source alternative to the <a href=\"https://tailscale.com/\">Tailscale</a> control server. Headscale's goal is to provide self-hosters and hobbyists with an open-source server they can use for their projects and labs. It implements a narrow scope, a  Tailscale network (tailnet), suitable for a personal use, or a small open-source organisation.</p><p>If you like  and find it useful, there is a sponsorship and donation buttons available in the repo.</p><p><strong>Please note that we do not support nor encourage the use of reverse proxies and container to run Headscale.</strong></p><p>This project is not associated with Tailscale Inc.</p><p>However, one of the active maintainers for Headscale <a href=\"https://tailscale.com/blog/opensource\">is employed by Tailscale</a> and he is allowed to spend work hours contributing to the project. Contributions from this maintainer are reviewed by other maintainers.</p><p>The maintainers work together on setting the direction for the project. The underlying principle is to serve the community of self-hosters, enthusiasts and hobbyists - while having a sustainable project.</p><p>To contribute to headscale you would need the latest version of <a href=\"https://golang.org\">Go</a> and <a href=\"https://buf.build\">Buf</a> (Protobuf generator).</p><p>We recommend using <a href=\"https://nixos.org/\">Nix</a> to setup a development environment. This can be done with , which will install the tools and give you a shell. This guarantees that you will have the same dev env as  maintainers.</p><p>To ensure we have some consistency with a growing number of contributions, this project has adopted linting and style/formatting rules:</p><p>The  code is linted with <a href=\"https://golangci-lint.run\"></a> and formatted with <a href=\"https://github.com/segmentio/golines\"></a> (width 88) and <a href=\"https://github.com/mvdan/gofumpt\"></a>. Please configure your editor to run the tools while developing and make sure to run  and  before committing any code.</p><p>The  (Markdown, YAML, etc) is formatted with <a href=\"https://prettier.io\"></a>.</p><p>Check out the  and  to see the specific configuration.</p><h3>Install development tools</h3><p>Some parts of the project require the generation of Go code from Protobuf (if changes are made in ) and it must be (re-)generated with:</p><p>: Please check in changes from  in a separate commit to make it easier to review.</p><p>We recommend using Nix for dependency management to ensure you have all required tools. If you prefer to manage dependencies yourself, you can use Make directly:</p><pre><code>nix develop\nmake test\nmake build\n</code></pre><p><strong>With your own dependencies:</strong></p><p>The Makefile will warn you if any required tools are missing and suggest running . Run  to see all available targets.</p><a href=\"https://github.com/juanfont/headscale/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=juanfont/headscale\"></a>","contentLength":3117,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"longbridge/gpui-component","url":"https://github.com/longbridge/gpui-component","date":1761705381,"author":"","guid":319597,"unread":true,"content":"<p>Rust GUI components for building fantastic cross-platform desktop application by using GPUI.</p><p>UI components for building fantastic desktop applications using <a href=\"https://gpui.rs\">GPUI</a>.</p><ul><li>: 60+ cross-platform desktop UI components.</li><li>: Inspired by macOS and Windows controls, combined with shadcn/ui design for a modern experience.</li><li>: Stateless  components, simple and user-friendly.</li><li>: Built-in  and , supporting multi-theme and variable-based configurations.</li><li>: Supports sizes like , , , and .</li><li>: Dock layout for panel arrangements, resizing, and freeform (Tiles) layouts.</li><li>: Virtualized Table and List components for smooth large-data rendering.</li><li>: Native support for Markdown and simple HTML.</li><li>: Built-in charts for visualizing your data.</li><li>: High performance code editor (support up to 200K lines) with LSP (diagnostics, completion, hover, etc).</li><li>: Syntax highlighting for editor and markdown components using Tree Sitter.</li></ul><p>Here is the first application: <a href=\"https://longbridge.com/desktop\">Longbridge Pro</a>, built using GPUI Component.</p><img width=\"1763\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e1ecb9c3-2dd3-431e-bd97-5a819c30e551\"><p>We built multi-theme support in the application. This feature is not included in GPUI Component itself, but is based on the  feature, so it's easy to implement.</p><p>GPUI and GPUI Component are still in development, so you need to add dependencies by git.</p><pre><code>gpui = \"0.2.2\"\ngpui-component = \"0.3.0\"\n</code></pre><pre><code>use gpui::*;\nuse gpui_component::{button::*, *};\n\npub struct HelloWorld;\nimpl Render for HelloWorld {\n    fn render(&amp;mut self, _: &amp;mut Window, _: &amp;mut Context&lt;Self&gt;) -&gt; impl IntoElement {\n        div()\n            .v_flex()\n            .gap_2()\n            .size_full()\n            .items_center()\n            .justify_center()\n            .child(\"Hello, World!\")\n            .child(\n                Button::new(\"ok\")\n                    .primary()\n                    .label(\"Let's Go!\")\n                    .on_click(|_, _, _| println!(\"Clicked!\")),\n            )\n    }\n}\n\nfn main() {\n    let app = Application::new();\n\n    app.run(move |cx| {\n        // This must be called before using any GPUI Component features.\n        gpui_component::init(cx);\n\n        cx.spawn(async move |cx| {\n            cx.open_window(WindowOptions::default(), |window, cx| {\n                let view = cx.new(|_| HelloWorld);\n                // This first level on the window, should be a Root.\n                cx.new(|cx| Root::new(view.into(), window, cx))\n            })?;\n\n            Ok::&lt;_, anyhow::Error&gt;(())\n        })\n        .detach();\n    });\n}\n</code></pre><blockquote><p>Still early and experimental; there are a lot of limitations.</p></blockquote><p>GPUI Component has a  element based on <a href=\"https://github.com/tauri-apps/wry\">Wry</a>. This is an optional feature, which you can enable with a feature flag.</p><pre><code>gpui-component = { version = \"0.3.0\", features = [\"webview\"] }\nwry = { version = \"0.53.3, package = \"lb-wry\" }\n</code></pre><p>More usage examples can be found in the <a href=\"https://github.com/longbridge/gpui-component/tree/main/crates/story\">story</a> directory.</p><p>GPUI Component has an  element, but it does not include SVG files by default.</p><p>The example uses <a href=\"https://lucide.dev\">Lucide</a> icons, but you can use any icons you like. Just name the SVG files as defined in <a href=\"https://github.com/longbridge/gpui-component/raw/main/crates/ui/src/icon.rs#L86\">IconName</a>. You can add any icons you need to your project.</p><p>We have a gallery of applications built with GPUI Component.</p><p>More examples can be found in the  directory. You can run them with <code>cargo run --example &lt;example_name&gt;</code>.</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Yes(Virtual Rows, Columns)</td><td>Yes(Virtual Rows, Columns)</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><blockquote><p>Please submit an issue or PR if any mistakes or outdated are found.</p></blockquote><p>[^1]: Release builds by use simple hello world example.</p>","contentLength":3297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"spipm/Depixelization_poc","url":"https://github.com/spipm/Depixelization_poc","date":1761705381,"author":"","guid":319598,"unread":true,"content":"<p>Depix is a PoC for a technique to recover plaintext from pixelized screenshots.</p><p>Depix is a PoC for a technique to recover plaintext from pixelized screenshots.</p><p>This implementation works on pixelized images that were created with a linear box filter. In <a href=\"https://www.spipm.nl/2030.html\">this article</a> I cover background information on pixelization and similar research.</p><ul></ul><pre><code>python3 depix.py \\\n    -p /path/to/your/input/image.png \\\n    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png \\\n    -o /path/to/your/output.png\n</code></pre><ul><li>Depixelize example image created with Notepad and pixelized with Greenshot. Greenshot averages by averaging the gamma-encoded 0-255 values, which is Depix's default mode.</li></ul><pre><code>python3 depix.py \\\n    -p images/testimages/testimage3_pixels.png \\\n    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png\n</code></pre><ul><li>Depixelize example image created with Sublime and pixelized with Gimp, where averaging is done in linear sRGB. The backgroundcolor option filters out the background color of the editor.</li></ul><pre><code>python3 depix.py \\\n    -p images/testimages/sublime_screenshot_pixels_gimp.png \\\n    -s images/searchimages/debruin_sublime_Linux_small.png \\\n    --backgroundcolor 40,41,35 \\\n    --averagetype linear\n</code></pre><ul><li>(Optional) You can view if the box detector thingie finds your pixels with . Consider a smaller batch of pixels if this looks all mangled. Example of good looking boxes:</li></ul><pre><code>python3 tool_show_boxes.py \\ \n    -p images/testimages/testimage3_pixels.png \\\n    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png\n</code></pre><ul><li>(Optional) You can create pixelized image by using .</li></ul><pre><code>python3 tool_gen_pixelated.py -i /path/to/image.png -o pixed_output.png\n</code></pre><ul><li>For a detailed explanation, please try to run  and .</li></ul><ul><li>Cut out the pixelated blocks from the screenshot as a single rectangle.</li><li>Paste a <a href=\"https://en.wikipedia.org/wiki/De_Bruijn_sequence\">De Bruijn sequence</a> with expected characters in an editor with the same font settings as your input image (Same text size, similar font, same colors).</li><li>Make a screenshot of the sequence.</li><li>Move that screenshot into a folder like .</li><li>Run Depix with the  flag set to the location of this screenshot.</li></ul><ul><li>Cut out the pixelized blocks exactly. See the  for examples.</li><li>It tries to detect blocks but it doesn't do an amazing job. Play with the  script and different cutouts if your blocks aren't properly detected.</li></ul><p>The algorithm uses the fact that the linear box filter processes every block separately. For every block it pixelizes all blocks in the search image to check for direct matches.</p><p>For some pixelized images Depix manages to find single-match results. It assumes these are correct. The matches of surrounding multi-match blocks are then compared to be geometrically at the same distance as in the pixelized image. Matches are also treated as correct. This process is repeated a couple of times.</p><p>After correct blocks have no more geometrical matches, it will output all correct blocks directly. For multi-match blocks, it outputs the average of all matches.</p><ul><li>The algorithm matches by integer block-boundaries. As a result, it has the underlying assumption that for all characters rendered (both in the de Brujin sequence and the pixelated image), the text positioning is done at pixel level. However, some modern text rasterizers position text <a href=\"http://agg.sourceforge.net/antigrain.com/research/font_rasterization/\">at sub-pixel accuracies</a>.</li><li>You need to know the font specifications and in some cases the screen settings with which the screenshot was taken. However, if there is enough plaintext in the original image you might be able to use the original as a search image.</li><li>This approach doesn't work if additional image compression is performed, because it messes up the colors of a block.</li></ul><ul><li>Implement more filter functions</li></ul><p>Create more averaging filters that work like some popular editors do.</p><ul><li>Create a new tool that utilizes HMMs</li></ul><p>Still, anyone who is passionate about this type of depixelization is encouraged to implement their own HMM-based version and share it.</p><p>After creating this program, someone pointed me to a <a href=\"https://www.researchgate.net/publication/305423573_On_the_Ineffectiveness_of_Mosaicing_and_Blurring_as_Tools_for_Document_Redaction\">research document</a> from 2016 where a group of researchers managed to create a similar tool. Their tool has better precision and works across many different fonts. While their original source code is not public, an open-source implementation exists at <a href=\"https://github.com/JonasSchatz/DepixHMM\">DepixHMM</a>.</p><p>Edit 16 Apr '25: Jeff Geerling created a <a href=\"https://www.jeffgeerling.com/blog/2025/its-easier-ever-de-censor-videos\">challenge</a> for depixelating pixelated folder content in a moving image. Three people were able to do it. <a href=\"https://github.com/KoKuToru/de-pixelate_gaV-O6NPWrI\">Here</a> is a repo from KoKuToru showing how to do this with TensorFlow! Amazing!</p>","contentLength":4378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"bol-van/zapret","url":"https://github.com/bol-van/zapret","date":1761618826,"author":"","guid":317092,"unread":true,"content":"<p>DPI bypass multi platform</p><p>zapret является свободным и open source. Всякий, кто понуждает вас скачивать zapret только с его ресурса, требует удалить ссылки, видео, файлы, обосновывая эти требования авторскими правами, сам нарушает <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/LICENSE.txt\">лицензию</a>. Однако, это не исключает <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/#%D0%BF%D0%BE%D0%B4%D0%B4%D0%B5%D1%80%D0%B6%D0%B0%D1%82%D1%8C-%D1%80%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%87%D0%B8%D0%BA%D0%B0\">добровольные пожертвования</a>.</p><p>Автономное средство противодействия DPI, которое не требует подключения каких-либо сторонних серверов. Может помочь обойти блокировки или замедление сайтов HTTP(S), сигнатурный анализ TCP и UDP протоколов, например, с целью блокировки VPN.</p><p>Проект нацелен прежде всего на маломощные embedded устройства - роутеры, работающие под OpenWrt. Поддерживаются традиционные Linux-системы, FreeBSD, OpenBSD, частично macOS. В некоторых случаях возможна самостоятельная прикрутка решения к различным прошивкам.</p><p>Большая часть функционала работает на Windows.</p><p>В самом простейшем случае вы имеете дело с пассивным DPI. Пассивный DPI может читать трафик из потока, может инжектить свои пакеты, но не может блокировать проходящие пакеты. Если запрос \"плохой\", пассивный DPI инжектит пакет RST, опционально дополняя его пакетом HTTP redirect. Если фейк пакет инжектится только для клиента, в этом случае можно обойтись командами iptables для дропа RST и/или редиректа на заглушку по определённым условиям, которые нужно подбирать для каждого провайдера индивидуально. Так мы обходим последствия срабатывания триггера запрета. Если пассивный DPI направляет пакет RST в том числе и серверу, то вы ничего с этим не сможете сделать. Ваша задача — не допустить срабатывания триггера запрета. Одними iptables уже не обойтись. Этот проект нацелен именно на предотвращение срабатывания запрета, а не ликвидацию его последствий.</p><p>Активный DPI ставится в разрез провода и может дропать пакеты по любым критериям, в том числе распознавать TCP-потоки и блокировать любые пакеты, принадлежащие потоку.</p><p>Как не допустить срабатывания триггера запрета? Послать то, на что DPI не рассчитывает и что ломает ему алгоритм распознавания запросов и их блокировки.</p><p>Некоторые DPI не могут распознать HTTP-запрос, если он разделен на TCP-сегменты. Например, запрос вида <code>GET / HTTP/1.1\\r\\nHost: kinozal.tv......</code> мы посылаем двумя частями: сначала идет , затем <code>/ HTTP/1.1\\r\\nHost: kinozal.tv.....</code>. Другие DPI спотыкаются, когда заголовок  пишется в другом регистре: например, . Кое-где работает добавление дополнительного пробела после метода:  →  или добавление точки в конце имени хоста: </p><p>Существует и более продвинутая магия, направленная на преодоление DPI на пакетном уровне.</p><h2>Что сейчас происходит в России</h2><p>Раньше, до внедрения повсеместных систем ТСПУ, использовался зоопарк различных DPI у провайдеров. Какие-то были активными, какие-то пассивными. Сейчас время простых iptables окончательно ушло. Везде активный DPI ТСПУ, но кое-где могут оставаться невыключенными дополнительные старые DPI из зоопарка. В этом случае приходится обходить сразу несколько DPI. Все больше становится внереестровых блокировок, о которых вы узнаете только по факту недоступности чего-либо, в списках этого нет. Применяются блокировки некоторых диапазонов ip адресов (автономный обход невозможен) и протоколов (VPN). На некоторых диапазонах IP используется более строгий фильтр, распознающий попытки обмана через сегментацию. Должно быть это связано с некоторыми сервисами, которые пытаются таким образом обмануть DPI.</p><h2>Как это реализовать на практике в системе linux</h2><p>Если кратко, то варианты можно классифицировать по следующей схеме :</p><ol><li>Пассивный DPI, не отправляющий RST серверу. Помогут индивидуально настраиваемые под провайдера команды iptables. На rutracker в разделе \"обход блокировок - другие способы\" по этому вопросу существует отдельная тема. В данном проекте не рассматривается. Если вы не допустите срабатывание триггера запрета, то и не придется бороться с его последствиями.</li><li>Модификация TCP соединения на уровне потока. Реализуется через proxy или transparent proxy.</li><li>Модификация TCP соединения на уровне пакетов. Реализуется через обработчик очереди NFQUEUE и raw сокеты.</li></ol><p>Для вариантов 2 и 3 реализованы программы tpws и nfqws соответственно. Чтобы они работали, необходимо их запустить с нужными параметрами и перенаправить на них определенный трафик средствами iptables или nftables.</p><h2>Когда это работать не будет</h2><ul><li>Если подменяется DNS. С этой проблемой легко справиться.</li><li>Если блокировка осуществляется по IP.</li><li>Если соединение проходит через фильтр, способный реконструировать TCP соединение, и который следует всем стандартам. Например, нас заворачивают на squid. Соединение идет через полноценный стек tcpip операционной системы. Проект нацелен на обман DPI, который всилу ограниченности ресурсов и большого трафика вынужден интерпретировать его лишь ограниченно. Обмануть полноценный стек ОС и полноценные серверные приложения не получится.</li></ul><p>Эта программа - модификатор пакетов и обработчик очереди NFQUEUE. Для BSD систем существует адаптированный вариант - dvtws, собираемый из тех же исходников (см. <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/bsd.md\">документация BSD</a>).</p><pre><code>@&lt;config_file&gt;|$&lt;config_file&gt;                             ; читать конфигурацию из файла. опция должна быть первой. остальные опции игнорируются.\n\n--debug=0|1                                               ; 1=выводить отладочные сообщения\n--dry-run                                                 ; проверить опции командной строки и выйти. код 0 - успешная проверка.\n--version                                                 ; вывести версию и выйти\n--comment                                                 ; любой текст (игнорируется)\n--daemon                                                  ; демонизировать прогу\n--pidfile=&lt;file&gt;                                          ; сохранить PID в файл\n--user=&lt;username&gt;                                         ; менять uid процесса\n--uid=uid[:gid]                                           ; менять uid процесса\n--qnum=N                                                  ; номер очереди N\n--bind-fix4                                               ; пытаться решить проблему неверного выбора исходящего интерфейса для сгенерированных ipv4 пакетов\n--bind-fix6                                               ; пытаться решить проблему неверного выбора исходящего интерфейса для сгенерированных ipv6 пакетов\n--ctrack-timeouts=S:E:F[:U]                               ; таймауты внутреннего conntrack в состояниях SYN, ESTABLISHED, FIN, таймаут udp. по умолчанию 60:300:60:60\n--ctrack-disable=[0|1]                                    ; 1 или остутствие аргумента отключает conntrack\n--ipcache-lifetime=&lt;int&gt;                                  ; время жизни записей кэша IP в секундах. 0 - без ограничений.\n--ipcache-hostname=[0|1]                                  ; 1 или отсутствие аргумента включают кэширование имен хостов для применения в стратегиях нулевой фазы\n--wsize=&lt;winsize&gt;[:&lt;scale_factor&gt;]                        ; менять tcp window size на указанный размер в SYN,ACK. если не задан scale_factor, то он не меняется (устарело !)\n--wssize=&lt;winsize&gt;[:&lt;scale_factor&gt;]                       ; менять tcp window size на указанный размер в исходящих пакетах. scale_factor по умолчанию 0. (см. conntrack !)\n--wssize-cutoff=[n|d|s]N                                  ; изменять server window size в исходящих пакетах (n), пакетах данных (d), относительных sequence (s) по номеру меньше N\n--wssize-forced-cutoff=0|1                                ; 1(default)=автоматически отключать wssize в случае обнаружения известного протокола\n--synack-split=[syn|synack|acksyn]                        ; выполнить tcp split handshake. вместо SYN,ACK отсылать только SYN, SYN+ACK или ACK+SYN\n--orig-ttl=&lt;int&gt;                                          ; модифицировать TTL оригинального пакета\n--orig-ttl6=&lt;int&gt;                                         ; модифицировать ipv6 hop limit оригинальных пакетов.  если не указано, используется значение --orig-ttl\n--orig-autottl=[&lt;delta&gt;[:&lt;min&gt;[-&lt;max&gt;]]|-]                ; режим auto ttl для ipv4 и ipv6. по умолчанию: +5:3-64. \"0:0-0\" или \"-\" отключает функцию\n--orig-autottl6=[&lt;delta&gt;[:&lt;min&gt;[-&lt;max&gt;]]|-]               ; переопределение предыдущего параметра для ipv6\n--orig-tcp-flags-set=&lt;int|0xHEX|flaglist&gt;                 ; устанавливать указанные tcp флаги (flags |= value). число , либо список через запятую : FIN,SYN,RST,PSH,ACK,URG,ECE,CWR,AE,R1,R2,R3\n--orig-tcp-flags-unset=&lt;int|0xHEX|flaglist&gt;               ; удалять указанные tcp флаги (flags &amp;= ~value)\n--orig-mod-start=[n|d|s]N                                 ; применять orig-mod только в исходящих пакетах (n), пакетах данных (d), относительных sequence (s) по номеру больше или равно N\n--orig-mod-cutoff=[n|d|s]N                                ; применять orig-mod только в исходящих пакетах (n), пакетах данных (d), относительных sequence (s) по номеру меньше N\n--dup=&lt;int&gt;                                               ; высылать N дубликатов до оригинала\n--dup-replace=[0|1]                                       ; 1 или отсутствие аргумента блокирует отправку оригинала. отправляются только дубликаты.\n--dup-ttl=&lt;int&gt;                                           ; модифицировать TTL дубликатов\n--dup-ttl6=&lt;int&gt;                                          ; модифицировать ipv6 hop limit дубликатов. если не указано, используется значение --dup-ttl\n--dup-autottl=[&lt;delta&gt;[:&lt;min&gt;[-&lt;max&gt;]]|-]                 ; режим auto ttl для ipv4 и ipv6. по умолчанию: +1:3-64. \"0:0-0\" или \"-\" отключает функцию\n--dup-autottl6=[&lt;delta&gt;[:&lt;min&gt;[-&lt;max&gt;]]|-]                ; переопределение предыдущего параметра для ipv6\n--dup-tcp-flags-set=&lt;int|0xHEX|flaglist&gt;                  ; устанавливать указанные tcp флаги (flags |= value). число , либо список через запятую : FIN,SYN,RST,PSH,ACK,URG,ECE,CWR,AE,R1,R2,R3\n--dup-tcp-flags-unset=&lt;int|0xHEX|flaglist&gt;                ; удалять указанные tcp флаги (flags &amp;= ~value)\n--dup-fooling=&lt;fooling&gt;                                   ; дополнительные методики как сделать, чтобы дубликат не дошел до сервера. none md5sig badseq badsum datanoack ts hopbyhop hopbyhop2\n--dup-ts-increment=&lt;int|0xHEX&gt;                            ; инкремент TSval для ts. по умолчанию -600000\n--dup-badseq-increment=&lt;int|0xHEX&gt;                        ; инкремент sequence number для badseq. по умолчанию -10000\n--dup-badack-increment=&lt;int|0xHEX&gt;                        ; инкремент ack sequence number для badseq. по умолчанию -66000\n--dup-ip-id=same|zero|seq|rnd                             ; режим назначения ip_id для пакетов dup\n--dup-start=[n|d|s]N                                      ; применять dup только в исходящих пакетах (n), пакетах данных (d), относительных sequence (s) по номеру больше или равно N\n--dup-cutoff=[n|d|s]N                                     ; применять dup только в исходящих пакетах (n), пакетах данных (d), относительных sequence (s) по номеру меньше N\n--hostcase                                                ; менять регистр заголовка \"Host:\" по умолчанию на \"host:\".\n--hostnospace                                             ; убрать пробел после \"Host:\" и переместить его в конец значения \"User-Agent:\" для сохранения длины пакета\n--methodeol                                               ; добавить перевод строки в unix стиле ('\\n') перед методом и убрать пробел из Host: : \"GET / ... Host: domain.com\" =&gt; \"\\nGET  / ... Host:domain.com\"\n--hostspell=HoST                                          ; точное написание заголовка Host (можно \"HOST\" или \"HoSt\"). автоматом включает --hostcase\n--domcase                                                 ; домен после Host: сделать таким : TeSt.cOm\n--ip-id=seq|seqgroup|rnd|zero                             ; режим назначения ip_id для генерированных пакетов\n--dpi-desync=[&lt;mode0&gt;,]&lt;mode&gt;[,&lt;mode2]                    ; атака по десинхронизации DPI. mode : synack syndata fake fakeknown rst rstack hopbyhop destopt ipfrag1 multisplit multidisorder fakedsplit hostfakesplit fakeddisorder ipfrag2 udplen tamper\n--dpi-desync-fwmark=&lt;int|0xHEX&gt;                           ; бит fwmark для пометки десинхронизирующих пакетов, чтобы они повторно не падали в очередь. default = 0x40000000\n--dpi-desync-ttl=&lt;int&gt;                                    ; установить ttl для десинхронизирующих пакетов\n--dpi-desync-ttl6=&lt;int&gt;                                   ; установить ipv6 hop limit для десинхронизирующих пакетов. если не указано, используется значение --dpi-desync-ttl\n--dpi-desync-autottl=[&lt;delta&gt;[:&lt;min&gt;[-&lt;max&gt;]]|-]          ; режим auto ttl для ipv4 и ipv6. по умолчанию: 1:3-20. \"0:0-0\" или \"-\" отключает функцию\n--dpi-desync-autottl6=[&lt;delta&gt;[:&lt;min&gt;[-&lt;max&gt;]]|-]         ; переопределение предыдущего параметра для ipv6\n--dpi-desync-tcp-flags-set=&lt;int|0xHEX|flaglist&gt;           ; устанавливать указанные tcp флаги (flags |= value). число , либо список через запятую : FIN,SYN,RST,PSH,ACK,URG,ECE,CWR,AE,R1,R2,R3\n--dpi-desync-tcp-flags-unset=&lt;int|0xHEX|flaglist&gt;         ; удалять указанные tcp флаги (flags &amp;= ~value)\n--dpi-desync-fooling=&lt;fooling&gt;                            ; дополнительные методики как сделать, чтобы фейковый пакет не дошел до сервера. none md5sig badseq badsum datanoack ts hopbyhop hopbyhop2\n--dpi-desync-repeats=&lt;N&gt;                                  ; посылать каждый генерируемый в nfqws пакет N раз (не влияет на остальные пакеты)\n--dpi-desync-skip-nosni=0|1                               ; 1(default)=не применять dpi desync для запросов без hostname в SNI, в частности для ESNI\n--dpi-desync-split-pos=N|-N|marker+N|marker-N             ; список через запятую маркеров для tcp сегментации в режимах split и disorder\n--dpi-desync-split-seqovl=N|-N|marker+N|marker-N          ; единичный маркер, определяющий величину перекрытия sequence в режимах split и disorder. для split поддерживается только положительное число.\n--dpi-desync-split-seqovl-pattern=[+ofs]@&lt;filename&gt;|0xHEX ; чем заполнять фейковую часть overlap\n--dpi-desync-fakedsplit-pattern=[+ofs]@&lt;filename&gt;|0xHEX   ; чем заполнять фейки в fakedsplit/fakeddisorder\n--dpi-desync-fakedsplit-mod=mod[,mod]                     ; может быть none, altorder=0|1|2|3 + 0|8|16\n--dpi-desync-hostfakesplit-midhost=marker+N|marker-N      ; маркер дополнительного разреза сегмента с оригинальным хостом. должен попадать в пределы хоста.\n--dpi-desync-hostfakesplit-mod=mod[,mod]                  ; может быть none, host=&lt;hostname&gt;, altorder=0|1\n--dpi-desync-ts-increment=&lt;int|0xHEX&gt;                     ; инкремент TSval для ts. по умолчанию -600000\n--dpi-desync-badseq-increment=&lt;int|0xHEX&gt;                 ; инкремент sequence number для badseq. по умолчанию -10000\n--dpi-desync-badack-increment=&lt;int|0xHEX&gt;                 ; инкремент ack sequence number для badseq. по умолчанию -66000\n--dpi-desync-any-protocol=0|1                             ; 0(default)=работать только по http request и tls clienthello  1=по всем непустым пакетам данных\n--dpi-desync-fake-tcp-mod=mod[,mod]                       ; список через запятую режимов runtime модификации tcp фейков (любых) : none, seq\n--dpi-desync-fake-http=[+ofs]@&lt;filename&gt;|0xHEX\t          ; файл, содержащий фейковый http запрос для dpi-desync=fake, на замену стандартному www.iana.org\n--dpi-desync-fake-tls=[+ofs]@&lt;filename&gt;|0xHEX|![+offset]  ; файл, содержащий фейковый tls clienthello для dpi-desync=fake, на замену стандартному. '!' = стандартный фейк\n--dpi-desync-fake-tls-mod=mod[,mod]                       ; список через запятую режимов runtime модификации фейков : none,rnd,rndsni,sni=&lt;sni&gt;,dupsid,padencap\n--dpi-desync-fake-unknown=[+ofs]@&lt;filename&gt;|0xHEX         ; файл, содержащий фейковый пейлоад неизвестного протокола для dpi-desync=fake, на замену стандартным нулям 256 байт\n--dpi-desync-fake-syndata=[+ofs]@&lt;filename&gt;|0xHEX         ; файл, содержащий фейковый пейлоад пакета SYN для режима десинхронизации syndata\n--dpi-desync-fake-quic=[+ofs]@&lt;filename&gt;|0xHEX            ; файл, содержащий фейковый QUIC Initial\n--dpi-desync-fake-wireguard=[+ofs]@&lt;filename&gt;|0xHEX       ; файл, содержащий фейковый wireguard handshake initiation\n--dpi-desync-fake-dht=[+ofs]@&lt;filename&gt;|0xHEX             ; файл, содержащий фейковый пейлоад DHT протокола для dpi-desync=fake, на замену стандартным нулям 64 байт\n--dpi-desync-fake-discord=[+ofs]@&lt;filename&gt;|0xHEX         ; файл, содержащий фейковый пейлоад Discord протокола нахождения IP адреса для голосовых чатов для dpi-desync=fake, на замену стандартным нулям 64 байт\n--dpi-desync-fake-stun=[+ofs]@&lt;filename&gt;|0xHEX            ; файл, содержащий фейковый пейлоад STUN протокола для dpi-desync=fake, на замену стандартным нулям 64 байт\n--dpi-desync-fake-unknown-udp=[+ofs]@&lt;filename&gt;|0xHEX     ; файл, содержащий фейковый пейлоад неизвестного udp протокола для dpi-desync=fake, на замену стандартным нулям 64 байт\n--dpi-desync-udplen-increment=&lt;int&gt;                       ; на сколько увеличивать длину udp пейлоада в режиме udplen\n--dpi-desync-udplen-pattern=[+ofs]@&lt;filename&gt;|0xHEX       ; чем добивать udp пакет в режиме udplen. по умолчанию - нули\n--dpi-desync-start=[n|d|s]N                               ; применять dpi desync только в исходящих пакетах (n), пакетах данных (d), относительных sequence (s) по номеру больше или равно N\n--dpi-desync-cutoff=[n|d|s]N                              ; применять dpi desync только в исходящих пакетах (n), пакетах данных (d), относительных sequence (s) по номеру меньше N\n--hostlist=&lt;filename&gt;                                     ; действовать только над доменами, входящими в список из filename. поддомены автоматически учитываются, если хост не начинается с '^'.\n                                                          ; в файле должен быть хост на каждой строке.\n                                                          ; список читается при старте и хранится в памяти в виде иерархической структуры для быстрого поиска.\n                                                          ; при изменении времени модификации файла он перечитывается автоматически по необходимости\n                                                          ; список может быть запакован в gzip. формат автоматически распознается и разжимается\n                                                          ; списков может быть множество. пустой общий лист = его отсутствие\n                                                          ; хосты извлекаются из Host: хедера обычных http запросов и из SNI в TLS ClientHello.\n--hostlist-domains=&lt;domain_list&gt;                          ; фиксированный список доменов через зяпятую. можно использовать # в начале для комментирования отдельных доменов.\n--hostlist-exclude=&lt;filename&gt;                             ; не применять дурение к доменам из листа. может быть множество листов. схема аналогична include листам.\n--hostlist-exclude-domains=&lt;domain_list&gt;                  ; фиксированный список доменов через зяпятую. можно использовать # в начале для комментирования отдельных доменов.\n--hostlist-auto=&lt;filename&gt;                                ; обнаруживать автоматически блокировки и заполнять автоматический hostlist (требует перенаправления входящего трафика)\n--hostlist-auto-fail-threshold=&lt;int&gt;                      ; сколько раз нужно обнаружить ситуацию, похожую на блокировку, чтобы добавить хост в лист (по умолчанию: 3)\n--hostlist-auto-fail-time=&lt;int&gt;                           ; все эти ситуации должны быть в пределах указанного количества секунд (по умолчанию: 60)\n--hostlist-auto-retrans-threshold=&lt;int&gt;                   ; сколько ретрансмиссий запроса считать блокировкой (по умолчанию: 3)\n--hostlist-auto-debug=&lt;logfile&gt;                           ; лог положительных решений по autohostlist. позволяет разобраться почему там появляются хосты.\n--new                                                     ; начало новой стратегии (новый профиль)\n--skip                                                    ; не использовать этот профиль . полезно для временной деактивации профиля без удаления параметров.\n--filter-l3=ipv4|ipv6                                     ; фильтр версии ip для текущей стратегии\n--filter-tcp=[~]port1[-port2]|*                           ; фильтр портов tcp для текущей стратегии. ~ означает инверсию. установка фильтра tcp и неустановка фильтра udp запрещает udp. поддерживается список через запятую.\n--filter-udp=[~]port1[-port2]|*                           ; фильтр портов udp для текущей стратегии. ~ означает инверсию. установка фильтра udp и неустановка фильтра tcp запрещает tcp. поддерживается список через запятую.\n--filter-l7=&lt;proto&gt;                                       ; фильтр протокола L6-L7. поддерживается несколько значений через запятую. proto : http tls quic wireguard dht discord stun unknown\n--filter-ssid=ssid1[,ssid2,ssid3,...]                     ; фильтр по имени wifi сети (только для linux)\n--ipset=&lt;filename&gt;                                        ; включающий ip list. на каждой строчке ip или cidr ipv4 или ipv6. поддерживается множество листов и gzip. перечитка автоматическая.\n--ipset-ip=&lt;ip_list&gt;                                      ; фиксированный список подсетей через запятую. можно использовать # в начале для комментирования отдельных подсетей.\n--ipset-exclude=&lt;filename&gt;                                ; исключающий ip list. на каждой строчке ip или cidr ipv4 или ipv6. поддерживается множество листов и gzip. перечитка автоматическая.\n--ipset-exclude-ip=&lt;ip_list&gt;                              ; фиксированный список подсетей через запятую. можно использовать # в начале для комментирования отдельных подсетей.\n</code></pre><p> позволяет выводить подробный лог действий на консоль, в syslog или в файл. Может быть важен порядок следования опций.  лучше всего указывать в самом начале. Опции анализируются последовательно. Если ошибка будет при проверке опции, а до анализа  еще дело не дошло, то сообщения не будут выведены в файл или syslog. При логировании в файл процесс не держит файл открытым. Ради каждой записи файл открывается и потом закрывается. Так что файл можно удалить в любой момент, и он будет создан заново при первом же сообщении в лог. Но имейте в виду, что если вы запускаете процесс под root, то будет сменен UID на не-root. В начале на лог файл меняется owner, иначе запись будет невозможна. Если вы потом удалите файл, и у процесса не будет прав на создание файла в его директории, лог больше не будет вестись. Вместо удаления лучше использовать truncate. В шелле это можно сделать через команду \": &gt;filename\"</p><p>Многие параметры, загружающие двоичные данные из файлов, поддерживают загрузку из hex-строки или из файла. hex строка начинается с \"0x\". Имя файла можно писать как есть или использовать префикс \"@\". Если перед префиксом \"@\" указано \"+&lt;число&gt;\", то это означает смещение полезных данных внутри файла. Файл может загружаться целиком с нулевой позиции, к нему могут применяться модификации, требующие полного файла (TLS), но передача пойдет с позиции offset. offset должен быть меньше длины файла. Если к блоку данных применяется мод, который уменьшает размер данных, и offset окажется не меньше новой длины данных, будет ошибка.</p><h3>АТАКА ДЕСИНХРОНИЗАЦИИ DPI</h3><p>Суть ее в следующем. Берется оригинальный запрос, модифицируется, добавляется поддельная информация (фейки) таким образом, чтобы ОС сервера передала серверному процессу оригинальный запрос в неизменном виде, а DPI увидел другое. То, что он блокировать не станет. Сервер видит одно, DPI - другое. DPI не понимает, что передается запрещенный запрос и не блокирует его.</p><p>Есть арсенал возможностей, чтобы достичь такого результата. Это может быть передача фейк пакетов, чтобы они дошли до DPI, но не дошли до сервера. Может использоваться фрагментация на уровне TCP (сегментация) или на уровне IP. Есть атаки, основанные на игре с tcp sequence numbers или с перепутыванием порядка следования tcp сегментов. Методы могут сочетаться в различных вариантах.</p><p>Фейки - это отдельные сгенерированные nfqws пакеты, несущие ложную информацию для DPI. Они либо не должны дойти до сервера, либо могут дойти, но должны быть им отброшены. Иначе получается слом tcp соединения или нарушение целостности передаваемого потока, что гарантированно приводит к поломке ресурса. Есть ряд методов для решения этой задачи.</p><ul><li> добавляет TCP опцию . Работает не на всех серверах. Пакеты с md5 обычно отбрасывают только linux. Требуется значительное увеличение длины tcp пакета, чтобы вместить tcp option. При обработке многосегментных запросов (TLS Kyber) первый пакет идет полный под MTU. При fakedsplit/fakeddisorder на небольших позициях отдельные tcp сегменты достаточно велики, чтобы внедрение md5 tcp option вызвало переполнение MTU и ошибку отправки \"message too long\".  не умеет перераспределять данные между tcp сегментами, поэтому надо или отказываться от kyber, или увеличивать сплит-позицию, или отказываться от fakedsplit/fakeddisorder.</li><li> портит контрольную сумму TCP. Не сработает, если ваше устройство за NAT, который не пропускает пакеты с инвалидной суммой. Наиболее распространенная настройка NAT роутера в Linux их не пропускает. На Linux построено большинство домашних роутеров. Непропускание обеспечивается так : настройка ядра sysctl по умолчанию <code>net.netfilter.nf_conntrack_checksum=1</code> заставляет conntrack проверять tcp и udp чексуммы входящих пакетов и выставлять state INVALID для пакетов с инвалидной суммой. Обычно в правилах iptables вставляется правило для дропа пакетов с состоянием INVALID в цепочке FORWARD. Совместное сочетание этих факторов приводит к непрохождению badsum через такой роутер. В OpenWrt из коробки <code>net.netfilter.nf_conntrack_checksum=0</code>, в других роутерах часто нет, и не всегда это можно изменить. Чтобы nfqws мог работать через роутер, нужно на нем выставить указанное значение sysctl в 0. nfqws на самом роутере будет работать и без этой настройки, потому что чексумма локально созданных пакетов не проверяется никогда. Если роутер за другим NAT, например провайдерским, и он не пропускает invalid packets вы ничего не сможете с этим сделать. Но обычно провайдеры все же пропускают badsum. На некоторых адаптерах/свитчах/драйверах принудительно включен rx-checksum offload, badsum пакеты отсекаются еще до получения в ОС. В этом случае если что-то и можно сделать, то только модифицировать драйвер, что представляется задачей крайне нетривиальной. Установлено, что так себя ведут некоторые роутеры на базе mediatek. badsum пакеты уходят с клиентской ОС, но роутером не видятся в br-lan через tcpdump. При этом если nfqws выполняется на самом роутере, обход может работать. badsum нормально уходят с внешнего интерфейса.</li><li> увеличивает TCP sequence number на определенное значение, выводя его тем самым из TCP window. Такие пакеты будут наверняка отброшены принимающим узлом, но так же и DPI, если он ориентируется на sequence numbers. По умолчанию смещение seq выбирается -10000. Практика показала, что некоторые DPI не пропускают seq вне определенного окна. Однако, такое небольшое смещение может вызвать проблемы при существенной потоковой передаче и потере пакетов. Если вы используете <code>--dpi-desync-any-protocol</code>, может понадобиться установить badseq increment 0x80000000. Это обеспечит надежную гарантию, что поддельный пакет не вклинится в tcp window на сервере. Так же было замечено, что badseq ломает логику некоторых DPI при анализе http, вызывая зависание соединения. Причем на тех же DPI TLS с badseq работает нормально.</li><li> казалось бы - лучший вариант, но он требует индивидуальной настройки под каждого провайдера. Если DPI находится дальше локальных сайтов провайдера, то вы можете отрезать себе доступ к ним. Ситуация усугубляется наличием ТСПУ на магистралах, что вынуждает делать TTL достаточно высоким, увеличивая риск пробоя фейка до сервера. Необходим ip exclude list, заполняемый вручную. Вместе с ttl можно применять md5sig. Это ничего не испортит, зато дает неплохой шанс работы сайтов, до которых \"плохой\" пакет дойдет по TTL. Если не удается найти автоматическое решение, воспользуйтесь файлом <code>zapret-hosts-user-exclude.txt</code>. Некоторые стоковые прошивки роутеров фиксируют исходящий TTL, без отключения этой опции через них работать не будет. КАКИМ СТОИТ ВЫБИРАТЬ TTL : найдите минимальное значение, при котором обход еще работает. Это и будет номер хопа вашего DPI.</li><li> относится только к ipv6. Добавляется ipv6 extenstion header . В варианте  добавляются 2 хедера, что является нарушением стандарта и гарантированно отбрасывается стеком протоколов во всех ОС. Один хедер hop-by-hop принимается всеми ОС, однако на некоторых каналах/провайдерах такие пакеты могут фильтроваться и не доходить. Расчет идет на то, что DPI проанализирует пакет с hop-by-hop, но он либо не дойдет до адресата в силу фильтров провайдера, либо будет отброшен сервером, потому что хедера два.</li><li> высылает фейки со снятым tcp флагом ACK. Сервера такое не принимают, а DPI может принять. Эта техника может ломать NAT и не всегда работает с iptables, если используется masquerade, даже с локальной системы (почти всегда на роутерах ipv4). На системах c iptables без masquerade и на nftables работает без ограничений. Экспериментально выяснено, что многие провайдерские NAT не отбрасывают эти пакеты, потому работает даже с внутренним провайдерским IP. Но linux NAT оно не пройдет, так что за домашним роутером эта техника скорее всего не сработает, но может сработать с него. Может сработать и через роутер, если подключение по проводу, и на роутере включено аппаратное ускорение.</li><li>Манипуляция tcp флагами с помощью <code>--dpi-desync-tcp-flags-set</code> и <code>--dpi-desync-tcp-flags-unset</code>. Можно сделать инвалидное сочетание флагов, которое сервер не примет, а DPI - примет. Например, установить SYN в фейках. Но это может работать не на всех серверах.  может быть заменен <code>--dpi-desync-tcp-flags-unset=ACK</code>. Пакеты с инвалидными флагами могут отбрасываться, проходя через NAT.</li><li> прибавляет к значению TSval таймштампа tcp значение ts increment (по умолчанию -600000). Сервера отбрасывают пакеты с TSval в определенных пределах. По практическим тестам инкремент должен быть где-то от -100 до -0x80000000. timestamps генерирует клиентская ОС. В linux таймштампы включены по умолчанию, в windows выключены по умолчанию. Можно включить через команду <code>netsh interface tcp set global timestamps=enabled</code>. ts fooling требует, чтобы таймштампы были включены, иначе работать не будет. Включать надо на каждом клиентском устройстве. TSecr оставляется без изменений. Так же требуется, чтобы сервер понимал timestamps, но это в большинстве случаев так.</li><li>. Суть режима в автоматическом определении TTL, чтобы пакет почти наверняка прошел DPI и немного не дошел до сервера (). Или наоборот - TTL едва хватило, чтобы он все-таки дошел до сервера (см , ). Берутся базовые значения TTL 64,128,255, смотрится входящий пакет (да, требуется направить первый входящий пакет на nfqws !). Вычисляется длина пути, прибавляется . delta может быть положительной или отрицательной. Чтобы задать положительную дельту, нужно указать унарный знак  перед числом. В случае его отсутствия или при наличии унарного знака  дельта считается отрицательной. Если TTL вне диапазона min,max, то берутся значения min,max, чтобы вписаться в диапазон. Если при этом дельта отрицательная и полученный TTL больше длины пути или дельта положительная и полученный TTL меньше длины пути, то автоматизм не сработал и берутся фиксированные значения : , , . Техника позволяет решить вопрос, когда вся сеть перегорожена шлагбаумами (DPI, ТСПУ) везде где только можно, включая магистралов. Но потенциально может давать сбои. Например, при асимметрии входящего и исходящего канала до конкретного сервера. Некоторые сервера выдают нестандартный TTL (google), потому на них получается полная ерунда. Если не учитывать подобные исключения, то на каких-то провайдерах эта техника будет работать неплохо, на других доставит больше проблем, чем пользы. Где-то может потребоваться тюнинг параметров. Лучше использовать с дополнительным ограничителем.</li></ul><p>Режимы дурения могут сочетаться в любых комбинациях.  берет множество значений через запятую.</p><p>Возможно задание множества фейков через повторение парамеров , кроме <code>--dpi-desync-fake-syndata</code>. Фейки будут отосланы в указанном порядке.  повторяет каждый отосланный фейк. Итоговый порядок будет такой : <code>fake1 fake1 fake1 fake2 fake2 fake2 fake3 fake3 fake3 .....</code></p><p>Любые tcp фейки отправляются с исходным sequence по умолчанию, даже если их несколько. Если задать <code>--dpi-desync-fake-tcp-mod=seq</code>, то несколько фейков будут отправлены с увеличением sequence number таким образом, как будто они являются tcp сегментами одного фейка.</p><p>В nfqws зашит базовый вариант фейка для TLS. Его можно переопределить опцией . Переопределение фейков дает возможность использовать любые данные в качестве фейка для TLS. Можно использовать фейковый Client Hello с любым фингерпринтом и с любым SNI.</p><p>Некоторые модификации можно делать в процессе выполнения с помощью <code>--dpi-desync-fake-tls-mod</code>. Часть из них работает при обработке каждого TLS Client Hello и может подстраиваться под отправляемые данные. Модификации требуют наличия полного валидного TLS Client Hello в качестве фейка, они не работают с произвольными данными.</p><ul><li>. Не применять никакие модификации.</li><li>. Рандомизировать поля  и . Выполняется на каждый запрос.</li><li>. Копировать  из передаваемого TLS Client Hello. Имеет приоритет над . Выполняется на каждый запрос.</li><li>. Рандомизировать SNI. Если SNI &gt;=7 символов, применяется случайный домен 2 уровня с известным TLD, иначе заполняется случайными символами без точки. Выполняется один раз при старте.</li><li>. Заменить sni на указанное значение. Макс длина SNI - 63 байта. Общая длина TLS фейка и длины в структуре TLS Client Hello меняются. Выполняется один раз при старте. Если сочетается с , выполняется до него.</li><li>. Расширяется padding extension на размер передаваемого TLS Client Hello (включая многопакетный вариант с kyber). Если padding отсутствует, он добавляется в конец. Если присутствует - требуется, чтобы padding шел последним extension. Правятся все длины, чтобы создать видимость включения передаваемого TLS Client Hello в padding extension. Размер фейка не изменяется. Расчет идет на DPI, который не анализирует sequence numbers должным образом. Выполняется на каждый запрос.</li></ul><p>По умолчанию если не задан собственный фейк для TLS используются модификации . Если фейк задан, используется . Это соответствует поведению программы более старых версий с добавлением функции .</p><p>Если задан режим модификации и имеется множество TLS фейков, к каждому из них применяется последний режим модификации. Если режим модификации задан после фейка, то он замещает предыдущий режим. Таким образом можно использовать разные режимы модификации для разных фейков. При невозможности модифицировать фейк на этапе запуска программа завершается с ошибкой.</p><p>Если сначала идет TLS фейк, для него задан режим однократной модификации, затем идет не TLS фейк, то будет ошибка. Нужно использовать `--dpi-desync-fake-tls-mod=none'.</p><p>Пример : `--dpi-desync-fake-tls=iana_org.bin --dpi-desync-fake-tls-mod=rndsni --dpi-desync-fake-tls=0xaabbccdd --dpi-desync-fake-tls-mod=none'</p><ul><li>. нарезаем запрос на указанных в  позициях.</li><li>. нарезаем запрос на указанных в  позициях и отправляем в обратном порядке.</li><li>. различные варианты замешивания фейков и оригиналов в прямом порядке</li><li>. различные варианты замешивания фейков и оригиналов в обратном порядке</li><li> (altorder=0). фейкование части запроса с хостом : оригинал до хоста, фейк хоста, оригинал хоста (+ опционально нарезка маркером midhost), фейк хоста, оригинал после хоста</li><li> (altorder=1). фейкование части запроса с хостом : оригинал до хоста, фейк хоста, оригинал после хоста, оригинал хоста (+опционально нарезка маркером midhost)</li><li>. аналогично , только в обратном порядке : фейк 2-й части, 2 часть, фейк 2-й части, фейк 1-й части, 1 часть, фейк 1 части.</li></ul><p>Для  и  предусмотрены вариации порядка следования сегментов. Параметр <code>--dpi-desync-fakedsplit-mod=altorder=N</code> задает число, влияющее на наличие отдельных фейков :</p><p>Режимы altorder для  для части многопакетного запроса, где есть сплит-позиция :</p><ul><li>. фейк 1-й части, 1 часть, фейк 1-й части, фейк 2-й части, 2 часть, фейк 2-й части</li><li>. 1 часть, фейк 1-й части, фейк 2-й части, 2 часть, фейк 2-й части</li><li>. 1 часть, фейк 2-й части, 2 часть, фейк 2-й части</li><li>. 1 часть, фейк 2-й части, 2 часть</li></ul><p>Режимы altorder для  для части многопакетного запроса, где есть сплит-позиция :</p><ul><li>. фейк 2-й части, 2 часть, фейк 2-й части, фейк 1-й части, 1 часть, фейк 1-й части</li><li>. 2 часть, фейк 2-й части, фейк 1-й части, 1 часть, фейк 1-й части</li><li>. 2 часть, фейк 1-й части, 1 часть, фейк 1-й части</li><li>. 2 часть, фейк 1-й части, 1 часть</li></ul><p>Режимы altorder для  и  для части многопакетного запроса, где нет сплит-позиции :</p><ul><li>. фейк, оригинал, фейк</li><li>. оригинал, фейк</li></ul><p>Итоговое число  вычисляется как сумма чисел из этих двух групп. По умолчанию .</p><p>Содержимое фейков в / определяется параметром <code>--dpi-desync-fakedsplit-pattern</code> (по умолчанию 0x00). Данные фейков берутся из паттерна со смещением, соответствующим смещению отсылаемых частей, учитывая смещения пакетов в многопакетных запросах. Размеры фейков соответствуют длинам отсылаемых частей. Цель этих режимов - максимально усложнить выявление оригинальных данных среди фейков.</p><p>Использование  или  на TLS kyber с md5sig fooling может привести к ошибкам \"message too long\", если позиция сплита мала, поскольку будет превышение MTU из-за md5 tcp option.</p><p>Режим 'hostfakesplit' имеет задачу минимального вмешательства фейком - как раз по той части запроса, на основании которой DPI принимает решение о блокировке. Конкретно - имени хоста. По умолчанию фейк хоста генерируется каждый раз случайно из набора . При длине более 7 символов за 3 символа до конца ставится точка, имитируя TLD, а последние 3 символа заполняются одним из нескольких известных TLD.</p><p>Можно переопределить шаблон генерации с помощью <code>--dpi-desync-hostfakesplit-mod=host=&lt;hostname&gt;</code>. В последнем случае справа всегда будет указанный hostname. Слева он будет дополнен до размера оригинального хоста как поддомен со случайными символами. Пример : \"www.networksolutions.com\" -&gt; \"h8xmdba4tv7a8.google.com\". Если размер оригинального хоста меньше шаблона, шаблон будет порезан : \"habr.com\" -&gt; \"ogle.com\". Если размер оригинального хоста больше шаблона на 1, получится инвалидный пустой поддомен : \"www.xxx.com\" =&gt; \".google.com\". Поэтому стоит использовать максимально короткие хосты из разрешенных : \"ya.ru\", \"vk.com\".</p><p><code>--dpi-desync-hostfakesplit-mod=altorder=1</code> позволяет сменить порядок следования частей на альтернативный вариант.  шлет фрагменты в таком порядке, чтобы при последовательной сборке сегментов на DPI он получил полностью собранный оригинал запроса с подмененным хостом. Реальный хост идет отдельным сегментом уже после. То есть в этом варианте применяется разновидность disorder. Сервер принимает фрагменты с нарушенным порядком sequence.</p><p>Опционально можно разрезать оригинальный хост. Например, <code>--dpi-desync-hostfakesplit-midhost=midsld</code>. Позиция нарезки должна попадать внутрь хоста. Многопакетные запросы поддерживаются только, если исходная нарезка пакетов не включает позиции имени хоста. В последнем случае дурение отменяется.</p><p>Вариант  имеет несколько альтернативных порядков нарезки - от 0 до 3. Режим задается в параметре <code>--dpi-desync-fakedsplit-mod=altorder=N</code>. Каждый следующий altorder убирает часть фейков.</p><p>Для определения позиций нарезки используются маркеры.</p><ul><li><strong>Абсолютный положительный маркер</strong> - числовое смещение внутри пакета или группы пакетов от начала.</li><li><strong>Абсолютный отрицательный маркер</strong> - числовое смещение внутри пакета или группы пакетов от следующего за концом байта. -1 указывает на последний байт.</li><li> - положительное или отрицательное смещение относительно логической позиции внутри пакета или группы пакетов.</li></ul><ul><li> - начало метода HTTP ('GET', 'POST', 'HEAD', ...). Метод обычно всегда находится на позиции 0, но может сместиться из-за . Тогда позиция может стать 1 или 2.</li><li> - начало имени хоста в известном протоколе (http, TLS)</li><li> - байт, следующий за последним байтом имени хоста</li><li> - начало домена 2 уровня в имени хоста</li><li> - байт, следующий за последним байтом домена 2 уровня в имени хоста</li><li> - середина домена 2 уровня в имени хоста</li><li> - начало поля данных SNI extension в TLS. Любой extension состоит из 2-байтовых полей type и length, за ними идет поле данных.</li></ul><p>Пример списка маркеров : <code>100,midsld,sniext+1,endhost-2,-10</code>.</p><p>При разбиении пакета первым делом происходит ресолвинг маркеров - нахождение всех указанных относительных позиций и применение смещений. Если относительная позиция отсутствует в текущем протоколе, такие позиции не применяются и отбрасываются. Дальше происходит нормализация позиций относительно смещения текущего пакета в группе пакетов (многопакетные запросы TLS с kyber, например). Выкидываются все позиции, выходящие за пределы текущего пакета. Оставшиеся сортируются в порядке возрастания и удаляются дубли. В вариантах  и  если не осталось ни одной позиции, разбиение не происходит.</p><p>Варианты  и  применяют только одну позицию сплита. Ее поиск среди списка  осуществляется особым образом. Сначала сверяются все относительные маркеры. Если среди них найден подходящий, применяется он. В противном случае сверяются все абсолютные маркеры. Если и среди них ничего не найдено, применяется позиция 1.</p><p>Например, можно написать <code>--dpi-desync-split-pos=method+2,midsld,5</code>. Если протокол http, разбиение будет на позиции . Если протокол TLS - на позиции . Если протокол неизвестен и включено <code>--dpi-desync-any-protocol</code>, разбиение будет на позиции 5. Чтобы все было однозначнее, можно использовать разные профили для разных протоколов и указывать только одну позицию, которая точно есть в этом протоколе.</p><h3>ПЕРЕКРЫТИЕ SEQUENCE NUMBERS</h3><p> добавляет в начало одного из TCP сегментов  байт со смещенным в минус sequence number на величину . Для  - в начало первого сегмента, для  - в начало предпоследнего отсылаемого сегмента (второго в оригинальном порядке следования).</p><p>В случае  расчет идет на то, что предыдущий отсыл, если он был, уже попал в сокет серверного приложения, поэтому новая пришедшая часть лишь частично находится в пределах текущего окна (in-window). Спереди фейковая часть отбрасывается, а оставшаяся часть содержит оригинал и начинается с начала window, поэтому попадает в сокет. Серверное приложение получает все, что реально отсылает клиент, отбрасывая фейковую out-of-window часть. Но DPI не может этого понять, поэтому у него происходит sequence десинхронизация. Обязательно, чтобы первый сегмент вместе с  не превысили длину MTU. Эта ситуация распознается автоматически в Linux, и  отменяется. В остальных системах ситуация не распознается, и это приведет к поломке соединения. Поэтому выбирайте первую позицию сплита и  таким образом, чтобы MTU не был превышен в любом случае. Иначе дурение может не работать или работать хаотично.</p><p>Для  overlap идет на предпоследнюю отсылаемую часть пакета. Для простоты будем считать, что разбиение идет на 2 части, шлются они в порядке \"2 1\" при оригинальном порядке \"1 2\". Обязательно, чтобы  был меньше позиции первого сплита, иначе все отосланное будет передано в сокет сразу же, включая фейк, ломая протокол прикладного уровня. Такая ситуация легко обнаруживается программой, и  отменяется. Увеличение размера пакета невозможно в принципе. При соблюдении условия 2-я часть пакета является полностью in-window, поэтому серверная ОС принимает ее целиком, включая фейк. Но поскольку начальная часть данных из 1 пакета еще не принята, то фейк и реальные данные остаются в памяти ядра, не отправляясь в серверное приложение. Как только приходит 1-я часть пакета, она переписывает фейковую часть в памяти ядра. Ядро получает данные из 1 и 2 части, поэтому далее идет отправка в сокет приложения. Таково поведение всех unix ОС, кроме solaris - оставлять последние принятые данные. Windows оставляет старые данные, поэтому disorder с seqovl будет приводить к зависаниям соединения при работе с Windows серверами. Solaris практически мертв, windows серверов очень немного. Можно использовать листы при необходимости. Метод позволяет обойтись без fooling и TTL. Фейки перемешаны с реальным данными.  по-прежнему добавляют дополнительные отдельные фейки.</p><p> в варианте  может быть только абсолютным положительным значением, поскольку применяется только в первому пакету. В варианте  допустимо применение всех вариантов маркеров. Они автоматически нормализуются к текущему пакету в серии. Можно сплитать на  и делать seqovl на .</p><p>Некоторые DPI секут поле ipv4 заголовка ip_id. Защита заключается в распознавании нехарактерного для разных ОС порядка назначения ip_id, но характерного для некоторого anti-DPI софта. Обычно ОС инкрементируют ip_id для каждого следующего пакета. Например, на ТСПУ повторение ненулевых ip_id фейка и не фейка вызывает триггер блока на диапазонах IP .</p><p>Если отсылаются фейки или дополнительные tcp сегменты, то в любом случае последовательность будет нарушена, поскольку ОС ничего не будет знать о всунутых фейках и не увеличит свой счетчик ip_id на количество фейков или дополнительных tcp сегментов. Чтобы сохранить последовательность, потребовалось бы перехватывать все соединение до конца, что очень затратно по ресурсам. Поэтому после отработки серии генерированных пакетов ip_id возвращается к тому значению, о котором знает ОС.</p><p>Параметр  относится к профилю и задает режим назначения ip_id при отсылке генерированных в nfqws пакетов.</p><ul><li> (по умолчанию) : взять последний ip_id реального пакета. последующие генерированыне пакеты получают увеличенные на 1 ip_id, кроме случая . для  в пределах сегментов, где есть сплит-позиции, значение ip_id увеличивается на количество частей, затем уменьшается на 1 с каждой отосланной частью.</li><li> : то же, что и , но фейки того же размера, что и оригинальные сегменты, маскирующиеся под оригинал получают те же ip_id.</li><li> : всем генерированным пакетам назначать случайный ip_id</li><li> : всем генерированным пакетам назначать ip_id=0 . в этом случае Linux и BSD отошлют 0, Windows назначит последовательные ip_id всем пакетам (тем самым автоматически решается проблема сбоя счетчика пакетов).</li></ul><p>В заголовках ipv6 поле ip_id отсутствует, параметр игнорируется для ipv6.</p><h3>СПЕЦИФИЧЕСКИЕ РЕЖИМЫ IPV6</h3><p>Режимы десинхронизации ,  и  (не путать с fooling !) относятся только к ipv6 и заключается в добавлении хедера ,  или  во все пакеты, попадающие под десинхронизацию. Здесь надо обязательно понимать, что добавление хедера увеличивает размер пакета, потому не может быть применено к пакетам максимального размера. Это имеет место при передаче больших сообщений. В случае невозможности отослать пакет дурение будет отменено, пакет будет выслан в оригинале. Расчет идет на то, что DPI увидит 0 в поле next header основного заголовка  и не будет скакать по extension хедерам в поисках транспортного хедера. Таким образом не поймет, что это tcp или udp, и пропустит пакет без анализа. Возможно, какие-то DPI на это купятся. Может сочетаться с любыми режимами 2-й фазы, кроме варианта . Например,  означает разбить tcp пакет на несколько сегментов, в каждый из них добавить hop-by-hop. При  последовательность хедеров будет : ,,. Режим  может срабатывать не всегда без специальной подготовки. См. раздел .</p><p>Параметры  и  позволяют изменить TTL оригинальных пакетов. Если дальнейшие манипуляции связаны с оригиналом, например, идет TCP сегментация, то исходными данными являются измененные оригинальные пакеты. То есть в данном примере TCP сегменты пойдут с измененным TTL.</p><p>Вариант  и  работает аналогично , но по оригинальным пакетам. Дельту стоит указывать положительную с унарным знаком , иначе оригинал не дойдет до сервера, и вы вообще ничего не получите. Пример : .</p><p> и  задают ограничитель по началу и концу модификации оригинала. Схема аналогична  и .</p><p>Функция может быть полезна, когда DPI охотится за фейками и блокирует соединение при наличии подозрительных признаков, в частности, измененный TTL у фейка относительно оригинала.</p><p>Дубликаты - это копии оригинальных пакетов, высылаемые перед ними. Включаются параметром , где N - количество дублей, не включающее оригинал.  отключает отсылку оригинала.</p><p>Отсылка дублей имеет место только в тех случаях, когда высылается и оригинал без реконструкции. Например, если случилась TCP сегментация, то оригинал фактически дропается и заменяется искусственно сконструированными сегментами. Дубли высланы не будут. Это же касается изменения состава хедеров ipv6, режима tamper для DHT и других.</p><p>Возможно применение всех вариантов дурения, как и для desync : . , . Нужно ли, чтобы эти пакеты доходили до сервера и в каком виде, решаете вы согласно задуманной стратегии.</p><p>Вариант  и  работает аналогично , но по дублям. Дельту можно указывать положительную с унарным знаком , а можно и отрицательную. Зависит от вашей задумки. Пример : .</p><p> и  задают ограничитель по началу и концу применения стратегии дубликатов. Схема аналогична  и .</p><p>Функция может помочь, когда DPI сечет разницу в характеристиках фейков и оригинала. Дубликатами можно попытаться заставить DPI принять , что весь сеанс идет аномальным. Например, у нас имеется TCP сеанс с MD5 сразу с первого SYN пакета. Значит последующие MD5 будут восприниматься нормально.</p><h3>КОМБИНИРОВАНИЕ МЕТОДОВ ДЕСИНХРОНИЗАЦИИ</h3><p>В параметре dpi-desync можно указать до 3 режимов через запятую.</p><ul><li>0 фаза - предполагает работу на этапе установления соединения : , , , . На эту фазу не действуют фильтры по <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/#%D0%BC%D0%BD%D0%BE%D0%B6%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D1%82%D1%80%D0%B0%D1%82%D0%B5%D0%B3%D0%B8%D0%B8\">hostlist</a>, кроме случая, описанного <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/#%D0%BA%D1%8D%D1%88-ip\">далее</a>.</li><li>1 фаза - отсылка чего-либо до оригинального пакета данных : , , .</li><li>2 фаза - отсылка в модифицированном виде оригинального пакета данных (например,  или ).</li></ul><p>Режимы требуют указания в порядке возрастания номеров фаз.</p><p>ipcache представляет собой структуру в памяти процесса, позволяющую по ключу IP адреса и имени интерфейса запоминать некоторую информацию, которую впоследствии можно извлечь и использовать как недостающие данные. На текущий момент это применяются в следующих ситуациях :</p><ol><li><p>IP,interface =&gt; hop count . Кэшируется количество хопов до сервера для последующего применения в autottl прямо с первого пакета, когда еще ответа не было. Пока записи в кэше нет, autottl не будет применен сразу. При повторном запросе до истечения времени жизни записи autottl будет применение сразу.</p></li><li><p>IP =&gt; hostname . Кэшируется имя хоста, вне привязки к интерфейсу, для последующего применения в стратегиях нулевой фазы. Режим отключен по умолчанию и включается через параметры . Данная техника является экспериментальной. Ее проблема в том, что как такового нет однозначного соответствия между доменом и IP. Множество доменов могут ссылаться на тот же IP адрес. При коллизии происходит замещение имени хоста на последний вариант. Домен может скакать по разным IP на CDN. Сейчас один адрес, через час - другой. Эта проблема решается через время жизни записей кэша : . По умолчанию 2 часа. Однако, может случиться и так, что в вашем случае применение техники несет больше пользы, чем проблем. Будьте готовы к непонятному на первый взгляд поведению, которое может быть исследовано только через  лог.</p></li></ol><p>При подаче сигнала SIGUSR2 процесс выводит содержимое ipcache на консоль.</p><h3>РЕАКЦИЯ DPI НА ОТВЕТ СЕРВЕРА</h3><p>Есть DPI, которые анализируют ответы от сервера, в частности сертификат из ServerHello, где прописаны домены. Подтверждением доставки ClientHello является ACK пакет от сервера с номером ACK sequence, соответствующим длине ClientHello+1. В варианте disorder обычно приходит сперва частичное подтверждение (SACK), потом полный ACK. Если вместо ACK или SACK идет RST пакет с минимальной задержкой, то DPI вас отсекает еще на этапе вашего запроса. Если RST идет после полного ACK спустя задержку, равную примерно пингу до сервера, тогда вероятно DPI реагирует на ответ сервера. DPI может отстать от потока, если ClientHello его удовлетворил и не проверять ServerHello. Тогда вам повезло. Вариант fake может сработать. Если же он не отстает и упорно проверяет ServerHello, то можно попробовать заставить сервер высылать ServerHello частями через параметр  (см. conntrack). Если и это не помогает, то сделать с этим что-либо вряд ли возможно без помощи со стороны сервера. Лучшее решение - включить на сервере поддержку TLS 1.3. В нем сертификат сервера передается в зашифрованном виде. Это рекомендация ко всем админам блокируемых сайтов. Включайте TLS 1.3. Так вы дадите больше возможностей преодолеть DPI.</p><p>В документации по geneva это называется \"TCB turnaround\". Попытка ввести DPI в заблуждение относительно ролей клиента и сервера.</p><p>Поскольку режим нарушает работу NAT, техника может сработать только если между атакующим устройством и DPI нет NAT. Атака не сработает через NAT роутер, но может сработать с него. Для реализации атаки на проходящий трафик требуются nftables и схема <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/#nftables-%D0%B4%D0%BB%D1%8F-nfqws\">POSTNAT</a>.</p><p>Тут все просто. Добавляются данные в пакет SYN. Все ОС их игнорируют, если не используется TCP fast open (TFO), а DPI может воспринять, не разобравшись есть там TFO или нет. Оригинальные соединения с TFO не трогаются, поскольку это их точно сломает. Без уточняющего параметра добавляются 16 нулевых байтов.</p><p>Изнутри VM от virtualbox и vmware в режиме NAT не работают многие техники пакетной магии nfqws. Принудительно заменяется ttl, не проходят фейк пакеты. Необходимо настроить сеть в режиме bridge.</p><p>nfqws оснащен ограниченной реализацией слежения за состоянием tcp соединений (conntrack). Он включается для реализации некоторых методов противодействия DPI. conntrack способен следить за фазой соединения : SYN,ESTABLISHED,FIN, количеством пакетов в каждую сторону, sequence numbers. conntrack способен \"кормиться\" пакетами в обе или только в одну сторону. Соединение попадает в таблицу при обнаружении пакетов с выставленными флагами SYN или SYN,ACK. Поэтому если необходим conntrack, в правилах перенаправления iptables соединение должно идти на nfqws с самого первого пакета, хотя затем может обрываться по фильтру connbytes. Для UDP инициатором попадания в таблицу является первый UDP пакет. Он же и определяет направление потока. Считается, что первый UDP пакет исходит от клиента к серверу. Далее все пакеты с совпадающими <code>src_ip,src_port,dst_ip,dst_port</code> считаются принадлежащими этому потоку до истечения времени неактивности. conntrack - простенький, он не писался с учетом всевозможных атак на соединение, он не проверяет пакеты на валидность sequence numbers или чексумму. Его задача - лишь обслуживание нужд nfqws, он обычно кормится только исходящим трафиком, потому нечувствителен к подменам со стороны внешней сети. Соединение удаляется из таблицы, как только отпадает нужда в слежении за ним или по таймауту неактивности. Существуют отдельные таймауты на каждую фазу соединения. Они могут быть изменены параметром .</p><p> позволяет изменить с клиента размер tcp window для сервера, чтобы он послал следующие ответы разбитыми на части. Чтобы это подействовало на все серверные ОС, необходимо менять window size в каждом исходящем с клиента пакете до отсылки сообщения, ответ на которое должен быть разбит (например, TLS ClientHello). Именно поэтому и необходим conntrack, чтобы знать когда надо остановиться. Если не остановиться и все время устанавливать низкий wssize, скорость упадет катастрофически. В linux это может быть купировано через connbytes, но в BSD системах такой возможности нет. В случае http(s) останавливаемся сразу после отсылки первого http запроса или TLS ClientHello. Если вы имеете дело с не http(s), то вам потребуется параметр . Он устанавливает предел, с которого действие wssize прекращается. Префикс d перед номером означает учитывать только пакеты с data payload, префикс s - relative sequence number, проще говоря количество переданных клиентом байтов + 1. Если проскочит пакет с http request или TLS ClientHello, действие wssize прекращается сразу же, не дожидаясь wssize-cutoff, если не указан параметр . Если ваш протокол склонен к долгому бездействию, следует увеличить таймаут фазы ESTABLISHED через параметр . Таймаут по умолчанию низкий - всего 5 минут. Не забывайте, что nfqws кормится приходящими на него пакетами. Если вы ограничили поступление пакетов через connbytes, то в таблице могут остаться повисшие соединения в фазе ESTABLISHED, которые отвалятся только по таймауту. Для диагностики состояния conntrack пошлите сигнал SIGUSR1 процессу nfqws : . Текущая таблица будет выведена nfqws в stdout.</p><p>Обычно в SYN пакете клиент отсылает кроме window size еще и TCP extension .  представляет из себя степень двойки, на которую умножается window size : 0=&gt;1, 1=&gt;2, 2=&gt;4, ..., 8=&gt;256, ... В параметре wssize scaling factor указывается через двоеточие. Scaling factor может только снижаться, увеличение заблокировано, чтобы не допустить превышение размера окна со стороны сервера. Для принуждения сервера к фрагментации ServerHello, чтобы избежать просекание имени сервера из сертификата сервера на DPI, лучше всего использовать . Основное правило - делать  как можно больше, чтобы после восстановления window size итоговый размер окна стал максимально возможным. Если вы сделаете 64:0, будет очень медленно. С другой стороны нельзя допустить, чтобы ответ сервера стал достаточно большим, чтобы DPI нашел там искомое.</p><p> не работает в профилях с хостлистами, поскольку он действует с самого начала соединения, когда еще нельзя принять решение о попадании в лист. Однако, профиль с auto hostlist может содержать --wssize.  может замедлять скорость и/или увеличивать время ответа сайтов, поэтому если есть другие работающие способы обхода DPI, лучше применять их.</p><p> позволяет задать предел, при достижении которого прекращается применение dpi-desync. Доступны префиксы n,d,s по аналогии с . Полезно совместно с <code>--dpi-desync-any-protocol=1</code>. На склонных к бездействию соединениях следует изменить таймауты conntrack. Если соединение выпало из conntrack и задана опция ,  применяться не будет.</p><p>nfqws поддерживает реассемблинг некоторых видов запросов. На текущий момент это TLS и QUIC ClientHello. Они бывают длинными, если в chrome включить пост-квантовую криптографию tls-kyber, и занимают, как правило, 2 или 3 пакета. kyber включен по умолчанию, начиная с chromium 124. chrome рандомизирует фингерпринт TLS. SNI может оказаться как в начале, так и в конце, то есть попасть в любой пакет. stateful DPI обычно реассемблирует запрос целиком, и только потом принимает решение о блокировке. В случае получения TLS или QUIC пакета с частичным ClientHello начинается процесс сборки, а пакеты задерживаются и не отсылаются до ее окончания. По окончании сборки пакеты проходит через десинхронизацию на основании полностью собранного ClientHello. При любой ошибке в процессе сборки задержанные пакеты немедленно отсылаются в сеть, а десинхронизация отменяется.</p><p>Есть специальная поддержка всех вариантов tcp сплита для многосегментного TLS. Если указать позицию сплита больше длины первого пакета, то разбивка происходит не обязательно первого пакета, а того, на который пришлась итоговая позиция. Если, допустим, клиент послал TLS ClientHello длиной 2000, SNI начинается с 1700, и заданы опции , то перед первым пакетом идет fake, затем первый пакет в оригинале, а последний пакет разбивается на 2 сегмента. В итоге имеем фейк в начале и 3 реальных сегмента.</p><p>Атаки на udp более ограничены в возможностях. udp нельзя фрагментировать иначе, чем на уровне ip.</p><p>Для UDP действуют только режимы десинхронизации , , , , , , , . Режимами первой фазы являются , , , , . Второй фазы - , , . Как обычно, возможно сочетание режимов первой и второй фазы, но не двух режимов одной фазы.</p><p> увеличивает размер udp пакета на указанное в <code>--dpi-desync-udplen-increment</code> количество байтов. Паддинг заполняется нулями по умолчанию, но можно задать свой паттерн. Предназначено для обмана DPI, ориентирующегося на размеры пакетов. Может сработать, если пользовательский протокол не привязан жестко к размеру udp пейлоада. Режим tamper означает модификацию пакетов известных протоколов особенным для протокола образом. На текущий момент работает только с DHT. Поддерживается определение пакетов QUIC Initial с расшифровкой содержимого и имени хоста, то есть параметр  будет работать. Определяются пакеты wireguard handshake initiation, DHT (начинается с 'd1', кончается 'e'), STUN и <a href=\"https://discord.com/developers/docs/topics/voice-connections#ip-discovery\">Discord Voice IP Discovery</a>. Для десинхронизации других протоколов обязательно указывать <code>--dpi-desync-any-protocol</code>. Реализован conntrack для udp. Можно пользоваться --dpi-desync-cutoff. Таймаут conntrack для udp можно изменить 4-м параметром в . Атака fake полезна только для stateful DPI, она бесполезна для анализа на уровне отдельных пакетов. По умолчанию fake наполнение - 64 нуля. Можно указать файл в <code>--dpi-desync-fake-unknown-udp</code>.</p><p>Современная сеть практически не пропускает фрагментированные tcp на уровне ip. На udp с этим дело получше, поскольку некоторые udp протоколы могут опираться на этот механизм (IKE старых версий). Однако, кое-где бывает, что режут и фрагментированный udp. Роутеры на базе linux могут самопроизвольно собирать или перефрагментировать пакеты. Позиция фрагментации задается отдельно для tcp и udp. По умолчанию 24 и 8 соответственно, должна быть кратна 8. Смещение считается с транспортного заголовка.</p><p>Существует ряд моментов вокруг работы с фрагментами на Linux, без понимания которых может ничего не получиться.</p><p>ipv4 : Linux дает отсылать ipv4 фрагменты, но стандартные настройки iptables в цепочке OUTPUT могут вызывать ошибки отправки.</p><p>ipv6 : Нет способа для приложения гарантированно отослать фрагменты без дефрагментации в conntrack. На разных системах получается по-разному. Где-то нормально уходят, где-то пакеты дефрагментируются. Для ядер &lt;4.16 похоже, что нет иного способа решить эту проблему, кроме как выгрузить модуль , который подтягивает зависимость . Он то как раз и выполняет дефрагментацию. Для ядер 4.16+ ситуация чуть лучше. Из дефрагментации исключаются пакеты в состоянии NOTRACK. Чтобы не загромождать описание, смотрите пример решения этой проблемы в .</p><p>Иногда требуется подгружать модуль  с параметром . В OpenWrt параметры модулей указываются через пробел после их названий в файлах . В традиционных системах посмотрите используется ли  или . Если legacy, то нужно создать файл <code>/etc/modprobe.d/ip6table_raw.conf</code> с содержимым :</p><pre><code>options ip6table_raw raw_before_defrag=1\n</code></pre><p>В некоторых традиционных дистрибутивах можно изменить текущий ip6tables через : update-alternatives --config ip6tables Если вы хотите оставаться на iptables-nft, вам придется пересобрать патченную версию. Патч совсем небольшой. В  найдите фрагмент:</p><pre><code>        {\n            .name\t= \"PREROUTING\",\n            .type\t= \"filter\",\n            .prio\t= -300,\t/* NF_IP_PRI_RAW */\n            .hook\t= NF_INET_PRE_ROUTING,\n        },\n        {\n            .name\t= \"OUTPUT\",\n            .type\t= \"filter\",\n            .prio\t= -300,\t/* NF_IP_PRI_RAW */\n            .hook\t= NF_INET_LOCAL_OUT,\n        },\n</code></pre><p>и замените везде -300 на -450.</p><p>Это нужно сделать вручную, никакой автоматики в  нет.</p><p>Либо можно раз и навсегда избавиться от этой проблемы, используя . Там можно создать  с любым приоритетом. Используйте приоритет -401 и ниже.</p><p>При использовании iptables и NAT, похоже, что нет способа прицепить обработчик очереди после NAT. Пакет попадает в nfqws с source адресом внутренней сети, затем фрагментируется и уже не обрабатывается NAT. Так и уходит во внешнюю сеть с src ip 192.168.x.x. Следовательно, метод не срабатывает. Видимо единственный рабочий метод - отказаться от iptables и использовать nftables. Хук должен быть с приоритетом 101 или выше.</p><p> способен по-разному реагировать на различные запросы и применять разные стратегии дурения. Это реализовано посредством поддержки множества профилей дурения. Профили разделяются в командной строке параметром . Первый профиль создается автоматически. Для него не нужно . Каждый профиль имеет фильтр. По умолчанию он пуст, то есть профиль удовлетворяет любым условиям. Фильтр может содержать жесткие параметры: версия ip протокола, ipset и порты tcp/udp. Они всегда однозначно идентифицируются даже на нулевой фазе десинхронизации, когда еще хост и L7 неизвестны. В качестве мягкого фильтра могут выступать хост-листы и протокол прикладного уровня (l7). L7 протокол становится известен обычно после первого пакета с данными. При поступлении запроса идет проверка профилей в порядке от первого до последнего до достижения первого совпадения с фильтром. Жесткие параметры фильтра сверяются первыми. При несовпадении идет сразу же переход к следующему профилю. Если какой-то профиль удовлетворяет жесткому фильтру и L7 фильтру и содержит авто-хостлист, он выбирается сразу. Если профиль удовлетворяет жесткому фильтру и L7 фильтру, для него задан хостлист, и у нас еще нет имени хоста, идет переход к следующему профилю. В противном случае идет проверка по хостлистам этого профиля. Если имя хоста удовлетворяет листам, выбирается этот профиль. Иначе идет переход к следующему. Может так случиться, что до получения имени хоста или узнавания L7 протокола соединение идет по одному профилю, а при выяснении этих параметров профиль меняется на лету. Это может произойти даже дважды - при выяснении L7 и имени хоста. Чаще всего это выяснение совмещается в одно действие, поскольку по одному пакету, как правило, узнается и L7, и хост. Поэтому если у вас есть параметры дурения нулевой фазы, тщательно продумывайте что может произойти при переключении стратегии. Смотрите debug log, чтобы лучше понять что делает nfqws. Нумерация профилей идет с 1 до N. Последним в цепочке создается пустой профиль с номером 0. Он используется, когда никакие условия фильтров не совпали.</p><blockquote><p>[!IMPORTANT] Множественные стратегии создавались только для случаев, когда невозможно обьединить имеющиеся стратегии для разных ресурсов. Копирование стратегий из blockcheck для разных сайтов во множество профилей без понимания как они работают приведет к нагромождению параметров, которые все равно не покроют все возможные заблокированные ресурсы. Вы только увязните в этой каше.</p></blockquote><blockquote><p>[!IMPORTANT] user-mode реализация ipset создавалась не как удобная замена *nix версии, реализованной в ядре. Вариант в ядре работает гораздо эффективнее. Это создавалось для систем без поддержки ipset в ядре. Конкретно - Windows и ядра Linux, собранные без nftables и ipset модулей ядра. Например, в android нет ipset.</p></blockquote><p>Имя wifi сети никак не связано с сетевым интерфейсом адаптера wifi. Интерфейс один, подключиться можно к любой сети. Для разных сетей разные стратегии. Стратегия от сети A не работает или ломает сеть B. Что делать ?</p><p>Можно вручную запускать и снимать инстансы nfqws. Но можно поступить иначе. В windows версии winws есть глобальный фильтр . Он включает или отключает инстанс winws в зависимости от подключенности любого адаптера к конкретной wifi сети. При этом не учитывается маршрутизация. Такой подход возможен потому, что к windivert можно прицепить несколько инстансов winws на пересекающихся фильтрах. При смене wifi сети одни будут включаться, другие выключаться.</p><p>Для linux применяется иное решение. Фильтр  относится к конкретному профилю. Невозможно повесить несколько инстансов nfqws на одну и ту же очередь или направить один и тот же трафик на несколько очередей. Подключение и отключение от очереди разных инстансов сопряжено со сложностями синхронизации между ними. Поэтому обрабатывать трафик должен один инстанс, и он должен уметь работать с разными wifi сетями. Это и реализовано в параметре . Он берет список имен wifi сетей (SSID) через запятую аналогично  для winws. При выборе профиля имеет значение куда идет конкретный обрабатываемый пакет. На какой интерфейс. Или с какого интерфейса пакет пришел, если он считается входящим. Поэтому даже если у вас часть трафика идет на одну сеть, часть на другую, а часть вообще не идет по wifi, то все это можно настроить.</p><p>Информация о подключенных сетях берется способом, используемым командой  (nl80211). К сожалению, на ядрах с 5.19 до самых последних (6.14 не работает) этот способ сломан. В этом случае используется способ iwgetid (wireless extensions). wireless extensions считаются deprecated и на новых ядрах реализованы как прослойка совместимости. Некоторые ядра могут быть собраны без wireless extensions. Перед использованием  удостоверьтесь, что любая из этих команд возвращает SSID.</p><p>Сканируются все wifi интерфейсы, составляется список interface-&gt;SSID. Он обновляется по мере поступления пакетов, но не чаще 1 раза в секунду.</p><blockquote><p>[!CAUTION] Начиная с ядер Linux 6.17 присутствует параметр конфигурации ядра CONFIG_NETFILTER_XTABLES_LEGACY, который по умолчанию в дистрибутиве может быть \"not set\". Отсутствие этой настройки выключает iptables-legacy. Это часть процесса депрекации iptables. Тем не менее iptables-nft будут работать, поскольку используют backend nftables.</p></blockquote><p>iptables для задействования атаки на первые пакеты данных в tcp соединении :</p><pre><code>iptables -t mangle -I POSTROUTING -o &lt;внешний_интерфейс&gt; -p tcp -m multiport --dports 80,443 -m connbytes --connbytes-dir=original --connbytes-mode=packets --connbytes 1:6 -m mark ! --mark 0x40000000/0x40000000 -j NFQUEUE --queue-num 200 --queue-bypass\n</code></pre><p>Этот вариант применяем, когда DPI не следит за всеми запросами http внутри keep-alive сессии. Если следит, направляем только первый пакет от https и все пакеты от http :</p><pre><code>iptables -t mangle -I POSTROUTING -o &lt;внешний_интерфейс&gt; -p tcp --dport 443 -m connbytes --connbytes-dir=original --connbytes-mode=packets --connbytes 1:6 -m mark ! --mark 0x40000000/0x40000000 -j NFQUEUE --queue-num 200 --queue-bypass\niptables -t mangle -I POSTROUTING -o &lt;внешний_интерфейс&gt; -p tcp --dport 80 -m mark ! --mark 0x40000000/0x40000000 -j NFQUEUE --queue-num 200 --queue-bypass\n</code></pre><p>mark нужен, чтобы сгенерированный поддельный пакет не попал опять к нам на обработку. nfqws выставляет fwmark при его отсылке. Хотя nfqws способен самостоятельно различать помеченные пакеты, фильтр в iptables по mark нужен при использовании connbytes, чтобы не допустить изменения порядка следования пакетов. Процессинг очереди - процесс отложенный. Если ядро имеет пакеты на отсылку вне очереди - оно их отправляет незамедлительно. Изменение правильного порядка следования пакетов при десинхронизации ломает всю идею. Так же были замечены дедлоки при достаточно большой отсылке пакетов из nfqws и отсутствии mark фильтра. Процесс может зависнуть. Поэтому наличие фильтра по mark в ip/nf tables можно считать обязательным.</p><ul><li>1 - для работы методов десинхронизации 0-й фазы и корректной работы conntrack</li><li>2 - иногда данные идут в 3-м пакете 3-way handshake</li><li>3 - стандартная ситуация приема одного пакета запроса</li><li>4-6 - на случай ретрансмиссии или запроса длиной в несколько пакетов (TLSClientHello с kyber, например)</li></ul><p>Для режима autottl необходимо перенаправление входящего  пакета или первого пакета соединения (что обычно есть то же самое). Для режима autohostlist необходимы входящие RST и http redirect. Можно построить фильтр на tcp flags для выделения  и модуле u32 для поиска характерных паттернов http redirect, но проще использовать connbytes для выделения нескольких начальных входящих пакетов.</p><p><code>iptables -t mangle -I PREROUTING -i &lt;внешний интерфейс&gt; -p tcp -m multiport --sports 80,443 -m connbytes --connbytes-dir=reply --connbytes-mode=packets --connbytes 1:3 -m mark ! --mark 0x40000000/0x40000000 -j NFQUEUE --queue-num 200 --queue-bypass</code></p><pre><code>iptables -t mangle -I POSTROUTING -o &lt;внешний_интерфейс&gt; -p udp --dport 443 -m connbytes --connbytes-dir=original --connbytes-mode=packets --connbytes 1:6 -m mark ! --mark 0x40000000/0x40000000 -j NFQUEUE --queue-num 200 --queue-bypass\n</code></pre><p>6 пакетов берется, чтобы покрыть случаи возможных ретрансмиссий quic initial в случае плохой связи или если сервер плохо себя чувствует, а приложение настаивает именно на quic, не переходя на tcp. А так же для работы autohostlist по quic. Однако, autohostlist для quic не рекомендуется.</p><p>Можно начать с базовой конфигурации.</p><pre><code>IFACE_WAN=wan\n\nnft create table inet ztest\n\nnft add chain inet ztest post \"{type filter hook postrouting priority mangle;}\"\nnft add rule inet ztest post oifname $IFACE_WAN meta mark and 0x40000000 == 0 tcp dport \"{80,443}\" ct original packets 1-6 queue num 200 bypass\nnft add rule inet ztest post oifname $IFACE_WAN meta mark and 0x40000000 == 0 udp dport 443 ct original packets 1-6 queue num 200 bypass\n\n# auto hostlist with avoiding wrong ACK numbers in RST,ACK packets sent by russian DPI\nsysctl net.netfilter.nf_conntrack_tcp_be_liberal=1 \nnft add chain inet ztest pre \"{type filter hook prerouting priority filter;}\"\nnft add rule inet ztest pre iifname $IFACE_WAN tcp sport \"{80,443}\" ct reply packets 1-3 queue num 200 bypass\n</code></pre><p>Для задействования IP фрагментации и  на проходящие пакеты требуется особая конфигурация цепочек, перенаправляющая пакеты после NAT. В скриптах zapret эта схема называется , и она возможна только на nftables. Сгенерированные nfqws пакеты требуется на раннем этапе помечать как , чтобы они не были испорчены NAT.</p><pre><code>IFACE_WAN=wan\n\nnft create table inet ztest\n\nnft add chain inet ztest postnat \"{type filter hook postrouting priority srcnat+1;}\"\nnft add rule inet ztest postnat oifname $IFACE_WAN meta mark and 0x40000000 == 0 tcp dport \"{80,443}\" ct original packets 1-6 queue num 200 bypass\nnft add rule inet ztest postnat oifname $IFACE_WAN meta mark and 0x40000000 == 0 udp dport 443 ct original packets 1-6 queue num 200 bypass\n\nnft add chain inet ztest predefrag \"{type filter hook output priority -401;}\"\nnft add rule inet ztest predefrag \"mark &amp; 0x40000000 != 0x00000000 notrack\"\n</code></pre><p>Удаление тестовой таблицы :</p><pre><code>nft delete table inet ztest\n</code></pre><p>Если ваше устройство поддерживает аппаратное ускорение (flow offloading, hardware nat, hardware acceleration), то iptables могут не работать. При включенном offloading пакет не проходит по обычному пути netfilter. Необходимо или его отключить, или выборочно им управлять.</p><p>В новых ядрах присутствует software flow offloading (SFO). Пакеты, проходящие через SFO, так же проходят мимо большей части механизмов iptables. При включенном SFO работает DNAT/REDIRECT (tpws). Эти соединения исключаются из offloading. Однако, остальные соединения идут через SFO, потому NFQUEUE будет срабатывать только до помещения соединения в flowtable. Практически это означает, что почти весь функционал nfqws работать не будет. Offload включается через специальный target в iptables  или через flowtable в nftables.</p><p>Не обязательно пропускать весь трафик через offload. tpws и так обходит offload \"by design\", а для отработки nfqws достаточно первых нескольких пакетов в tcp соединении или udp сеансе. Пока сеанс не направлен на offload, он процессится обычным образом через полноценный netfilter. Как только срабатывает правило offload по любому входящему или исходящему пакету, весь сеанс окончательно уходит из netfilter в offload. Поэтому скрипты zapret берут правила для NFQUEUE, что они создали, и из них создают exemption правила, которые не дают раньше времени попасть сеансу в offload, а потом его \"отпускают\". При этом входящим пакетам не дают начать offload, триггером выступают только исходящие пакеты. Эта схема обеспечивает практически нулевой негативный эффект на скорость, одновременно покрывая нужды nfqws и упрощая правила таблиц.</p><p>OpenWrt не предусматривает выборочного управления offload, поэтому скрипты zapret поддерживают свою систему выборочного управления.</p><p>iptables target  - это проприетарное изобретение OpenWrt. Управление offload в nftables реализовано в базовом ядре linux без патчей. nftables - единственный способ включения offload на классическом Linux.</p><p>На устройствах mediatek замечены 2 проблемы.</p><p>Драйвер mediatek ethernet отбрасывает tcp и udp пакеты с неверной чексуммой на аппаратном уровне, это не отключается. Как следствие не будет работать fooling badsum через роутер, но будет с него.</p><p>Другая проблема mediatek, затрагивающая как ethernet, так и wireless, проявляется на udp, когда включен offload rx-gro-list. Пока отсутствует nfqueue, все хорошо. Как только nfqueue появляется, часть пакетов выпадает. Особенно заметно это проявляется на дурении QUIC с kyber.</p><p>Этот код нужно вызывать после вставания интерфейса LAN, когда все bridge members уже занесены в bridge. Можно использовать хук в . Должен быть установлен .</p><p>Проблемы mediatek были подтверждены на MT7621 (TP-Link Archer C6U v1) и MT7981 (Xiaomi AX3000T). Другие чипсеты могут быть так же подвержены проблеме, а могут и не быть. Более широкой статистики нет.</p><h3>ДУРЕНИЕ СО СТОРОНЫ СЕРВЕРА</h3><p>Это тоже возможно. nfqws рассчитан на атаку со стороны клиента, поэтому он распознает прямой и обратный трафик на основании роли в установлении tcp соединения. Если проходит SYN, то source IP - это клиент. Если проходит SYN,ACK , то source IP - это сервер. Для UDP клиентом считается source IP первого прошедшего пакета по двум связкам ip-port. На сервере трафиком клиента будет считаться принятый трафик, а трафиком сервера - исходящий.</p><p> работает в любом случае, он может использоваться как на клиенте, так и на сервере. Остальные техники работают только если nfqws считает трафик трафиком клиента. Поэтому для их применения по исходящему с сервера трафику conntrack нужно выключить параметром . Если пакет не найден в conntrack, по нему идет работа как по пакету клиента.</p><p>Большинство протоколов опознаваться не будет, потому что система их опознавания рассчитана на содержание пакетов от клиента. Чтобы задействовать техники типа  или  нужно использовать <code>--dpi-desync-any-protocol</code> с ограничителем connbytes или с ограничителем на основании содержания пакета или его заголовков. start/cutoff недоступны, поскольку завязаны на conntrack.</p><p>Техника  позволяет разбить tcp сегмент SYN,ACK на отдельные части с SYN и с ACK. В ответ на это клиент шлет SYN,ACK , что обычно характеризует сервер. У некоторых DPI от этого может ломаться алгоритм, и они перестают блокировать запрещенный контент. Здесь <a href=\"https://nmap.org/misc/split-handshake.pdf\">подробное описание</a> что есть split handshake.</p><p>Перенаправление трафика обычно идет по номеру source портов и направлению original. original - это исходящий с системы трафик, reply - входящий.</p><p>tpws - это transparent proxy.</p><pre><code>@&lt;config_file&gt;|$&lt;config_file&gt;                     ; читать конфигурацию из файла. опция должна быть первой. остальные опции игнорируются.\n\n--debug=0|1|2|syslog|@&lt;filename&gt;                  ; 0,1,2 = логирование на косоль : 0=тихо, 1(default)=подробно, 2=отладка.\n--debug-level=0|1|2                               ; указать уровень логирования для syslog и @&lt;filename&gt;\n--dry-run                                         ; проверить опции командной строки и выйти. код 0 - успешная проверка.\n--version                                         ; вывести версию и выйти\n\n--daemon                                          ; демонизировать прогу\n--pidfile=&lt;file&gt;                                  ; сохранить PID в файл\n--user=&lt;username&gt;                                 ; менять uid процесса\n--uid=uid[:gid]                                   ; менять uid процесса\n--bind-addr                                       ; на каком адресе слушать. может быть ipv4 или ipv6 адрес\n                                                  ; если указан ipv6 link local, то требуется указать с какого он интерфейса : fe80::1%br-lan\n--bind-linklocal=no|unwanted|prefer|force         ; no : биндаться только на global ipv6\n                                                  ; unwanted (default) : предпочтительно global, если нет - LL\n                                                  ; prefer : предпочтительно LL, если нет - global\n                                                  ; force : биндаться только на LL\n--bind-iface4=&lt;iface&gt;                             ; слушать на первом ipv4 интерфейса iface\n--bind-iface6=&lt;iface&gt;                             ; слушать на первом ipv6 интерфейса iface\n--bind-wait-ifup=&lt;sec&gt;                            ; ждать до N секунд появления и поднятия интерфейса\n--bind-wait-ip=&lt;sec&gt;                              ; ждать до N секунд получения IP адреса (если задан --bind-wait-ifup - время идет после поднятия интерфейса)\n--bind-wait-ip-linklocal=&lt;sec&gt;\n                                                  ; имеет смысл только при задании --bind-wait-ip\n                                                  ; --bind-linklocal=unwanted\t: согласиться на LL после N секунд\n                                                  ; --bind-linklocal=prefer\t: согласиться на global address после N секунд\n--bind-wait-only                                  ; подождать все бинды и выйти. результат 0 в случае успеха, иначе не 0.\n--connect-bind-addr                               ; с какого адреса подключаться во внешнюю сеть. может быть ipv4 или ipv6 адрес\n                                                  ; если указан ipv6 link local, то требуется указать с какого он интерфейса : fe80::1%br-lan\n                                                  ; опция может повторяться для v4 и v6 адресов\n                                                  ; опция не отменяет правил маршрутизации ! выбор интерфейса определяется лишь правилами маршрутизации, кроме случая v6 link local.\n--socks                                           ; вместо прозрачного прокси реализовать socks4/5 proxy\n--no-resolve                                      ; запретить ресолвинг имен через socks5\n--resolve-threads                                 ; количество потоков ресолвера\n--port=&lt;port&gt;                                     ; на каком порту слушать\n--maxconn=&lt;max_connections&gt;                       ; максимальное количество соединений от клиентов к прокси\n--maxfiles=&lt;max_open_files&gt;                       ; макс количество файловых дескрипторов (setrlimit). мин требование (X*connections+16), где X=6 в tcp proxy mode, X=4 в режиме тамперинга.\n                                                  ; стоит сделать запас с коэффициентом как минимум 1.5. по умолчанию maxfiles (X*connections)*1.5+16\n--max-orphan-time=&lt;sec&gt;                           ; если вы запускаете через tpws торрент-клиент с множеством раздач, он пытается установить очень много исходящих соединений,\n                                                  ; большая часть из которых отваливается по таймауту (юзера сидят за NAT, firewall, ...)\n                                                  ; установление соединения в linux может длиться очень долго. локальный конец отвалился, перед этим послав блок данных,\n                                                  ; tpws ждет подключения удаленного конца, чтобы отослать ему этот блок, и зависает надолго.\n                                                  ; настройка позволяет сбрасывать такие подключения через N секунд, теряя блок данных. по умолчанию 5 сек. 0 означает отключить функцию\n                                                  ; эта функция не действует на успешно подключенные ранее соединения\n\n--local-rcvbuf=&lt;bytes&gt;                            ; SO_RCVBUF для соединений client-proxy\n--local-sndbuf=&lt;bytes&gt;                            ; SO_SNDBUF для соединений client-proxy\n--remote-rcvbuf=&lt;bytes&gt;                           ; SO_RCVBUF для соединений proxy-target\n--remote-sndbuf=&lt;bytes&gt;                           ; SO_SNDBUF для соединений proxy-target\n--nosplice                                        ; не использовать splice на linux системах\n--skip-nodelay                                    ; не устанавливать в исходящих соединения TCP_NODELAY. несовместимо со split.\n--local-tcp-user-timeout=&lt;seconds&gt;                ; таймаут соединений client-proxy (по умолчанию : 10 сек, 0 = оставить системное значение)\n--remote-tcp-user-timeout=&lt;seconds&gt;               ; таймаут соединений proxy-target (по умолчанию : 20 сек, 0 = оставить системное значение)\n--fix-seg=&lt;int&gt;                                   ; исправлять неудачи tcp сегментации ценой задержек для всех клиентов и замедления. ждать до N мс. по умолчанию 30 мс.\n--ipcache-lifetime=&lt;int&gt;                          ; время жизни записей кэша IP в секундах. 0 - без ограничений.\n--ipcache-hostname=[0|1]                          ; 1 или отсутствие аргумента включают кэширование имен хостов для применения в стратегиях нулевой фазы\n\n--split-pos=N|-N|marker+N|marker-N                ; список через запятую маркеров для tcp сегментации\n--split-any-protocol                              ; применять сегментацию к любым пакетам. по умолчанию - только к известным протоколам (http, TLS)\n--disorder[=http|tls]                             ; путем манипуляций с сокетом вынуждает отправлять первым второй сегмент разделенного запроса\n--oob[=http|tls]                                  ; отправить байт out-of-band data (OOB) в конце первой части сплита\n--oob-data=&lt;char&gt;|0xHEX                           ; переопределить байт OOB. по умолчанию 0x00.\n--hostcase                                        ; менять регистр заголовка \"Host:\". по умолчанию на \"host:\".\n--hostspell=HoST                                  ; точное написание заголовка Host (можно \"HOST\" или \"HoSt\"). автоматом включает --hostcase\n--hostdot                                         ; добавление точки после имени хоста : \"Host: kinozal.tv.\"\n--hosttab                                         ; добавление табуляции после имени хоста : \"Host: kinozal.tv\\t\"\n--hostnospace                                     ; убрать пробел после \"Host:\"\n--hostpad=&lt;bytes&gt;                                 ; добавить паддинг-хедеров общей длиной &lt;bytes&gt; перед Host:\n--domcase                                         ; домен после Host: сделать таким : TeSt.cOm\n--methodspace                                     ; добавить пробел после метода : \"GET /\" =&gt; \"GET  /\"\n--methodeol                                       ; добавить перевод строки перед методом  : \"GET /\" =&gt; \"\\r\\nGET  /\"\n--unixeol                                         ; конвертировать 0D0A в 0A и использовать везде 0A\n--tlsrec=N|-N|marker+N|marker-N                   ; разбивка TLS ClientHello на 2 TLS records на указанной позиции. Минимальное смещение - 6.\n--mss=&lt;int&gt;                                       ; установить MSS для клиента. может заставить сервер разбивать ответы, но существенно снижает скорость\n--tamper-start=[n]&lt;pos&gt;                           ; начинать дурение только с указанной байтовой позиции или номера блока исходяшего потока (считается позиция начала принятого блока)\n--tamper-cutoff=[n]&lt;pos&gt;                          ; закончить дурение на указанной байтовой позиции или номере блока исходящего потока (считается позиция начала принятого блока)\n--hostlist=&lt;filename&gt;                             ; действовать только над доменами, входящими в список из filename. поддомены автоматически учитываются, если хост не начинается с '^'.\n                                                  ; в файле должен быть хост на каждой строке.\n                                                  ; список читается при старте и хранится в памяти в виде иерархической структуры для быстрого поиска.\n                                                  ; при изменении времени модификации файла он перечитывается автоматически по необходимости\n                                                  ; список может быть запакован в gzip. формат автоматически распознается и разжимается\n                                                  ; списков может быть множество. пустой общий лист = его отсутствие\n                                                  ; хосты извлекаются из Host: хедера обычных http запросов и из SNI в TLS ClientHello.\n--hostlist-domains=&lt;domain_list&gt;                  ; фиксированный список доменов через зяпятую. можно использовать # в начале для комментирования отдельных доменов.\n--hostlist-exclude=&lt;filename&gt;                     ; не применять дурение к доменам из листа. может быть множество листов. схема аналогична include листам.\n--hostlist-exclude-domains=&lt;domain_list&gt;          ; фиксированный список доменов через зяпятую. можно использовать # в начале для комментирования отдельных доменов.\n--hostlist-auto=&lt;filename&gt;                        ; обнаруживать автоматически блокировки и заполнять автоматический hostlist (требует перенаправления входящего трафика)\n--hostlist-auto-fail-threshold=&lt;int&gt;              ; сколько раз нужно обнаружить ситуацию, похожую на блокировку, чтобы добавить хост в лист (по умолчанию: 3)\n--hostlist-auto-fail-time=&lt;int&gt;                   ; все эти ситуации должны быть в пределах указанного количества секунд (по умолчанию: 60)\n--hostlist-auto-debug=&lt;logfile&gt;                   ; лог положительных решений по autohostlist. позволяет разобраться почему там появляются хосты.\n--new                                             ; начало новой стратегии (новый профиль)\n--skip                                            ; не использовать этот профиль . полезно для временной деактивации профиля без удаления параметров.\n--filter-l3=ipv4|ipv6                             ; фильтр версии ip для текущей стратегии\n--filter-tcp=[~]port1[-port2]|*                   ; фильтр портов tcp для текущей стратегии. ~ означает инверсию. поддерживается список через запятую.\n--filter-l7=[http|tls|quic|wireguard|dht|unknown] ; фильтр протокола L6-L7. поддерживается несколько значений через запятую.\n--ipset=&lt;filename&gt;                                ; включающий ip list. на каждой строчке ip или cidr ipv4 или ipv6. поддерживается множество листов и gzip. перечитка автоматическая.\n--ipset-ip=&lt;ip_list&gt;                              ; фиксированный список подсетей через запятую. можно использовать # в начале для комментирования отдельных подсетей.\n--ipset-exclude=&lt;filename&gt;                        ; исключающий ip list. на каждой строчке ip или cidr ipv4 или ipv6. поддерживается множество листов и gzip. перечитка автоматическая.\n--ipset-exclude-ip=&lt;ip_list&gt;                      ; фиксированный список подсетей через запятую. можно использовать # в начале для комментирования отдельных подсетей.\n</code></pre><p>tpws, как и nfqws, поддерживает множественную сегментацию запросов. Сплит позиции задаются в . Указываются маркеры через запятую. Описание маркеров см в разделе <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/#tcp-%D1%81%D0%B5%D0%B3%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%86%D0%B8%D1%8F\">nfqws</a>.</p><p>На прикладном уровне в общем случае нет гарантированного средства заставить ядро выплюнуть блок данных, порезанным в определенном месте. ОС держит буфер отсылки (SNDBUF) у каждого сокета. Если у сокета включена опция TCP_NODELAY и буфер пуст, то каждый send приводит к отсылке отдельного ip пакета или группы пакетов, если блок не вмещается в один ip пакет. Однако, если в момент send уже имеется неотосланный буфер, то ОС присоединит данные к нему, никакой отсылки отдельным пакетом не будет. Но в этом случае и так нет никакой гарантии, что какой-то блок сообщения пойдет в начале пакета, на что собственно и заточены DPI. Разбиение будет производиться согласно MSS, который зависит от MTU исходящего интерфейса. Таким образом DPI, смотрящие в начало поля данных TCP пакета, будут поломаны в любом случае. Протокол http относится к запрос-ответным протоколам. Новое сообщение посылается только тогда, когда сервер получил запрос и полностью вернул ответ. Значит запрос фактически был не только отослан, но и принят другой стороной, а следовательно буфер отсылки пуст, и следующие 2 send приведут к отсылке сегментов данных разными ip пакетами.</p><p>Таким образом tpws обеспечивает сплит только за счет раздельных вызовов send, и это обычно работает надежно, если разбивать не на слишком много частей и не на слишком мелкие подряд следующие части. В последнем случае Linux все же может обьединить некоторые части, что приведет к несоответствию реальной сегментации указанным сплит позициям. Другие ОС в этом вопросе ведут себя более предсказуемо. Спонтанного обьединения замечено не было. Поэтому не стоит злоупотреблять сплитами и в особенности мелкими соседними пакетами.</p><p>Как показывается практика, проблемы могут начаться , если количество сплитов более одного. На каких-то системах наблюдался стабильный результат до 8 сплитов, на других проблемы уже начинались после 2 сплитов. Один сплит работает стабильно, если не является частью массивной потоковой передачи. При неудаче сегментации будет выводиться сообщение <code>WARNING ! segmentation failed</code>. Если вы его видите, это повод снизить количество сплит позиций. Если это не вариант, для ядер Linux &gt;=4.6 есть параметр . Он позволяет подождать завершение отсылки перед отправкой следующей части. Но этот вариант ломает модель асинхронной обработки событий. Пока идет ожидание, все остальные соединения не обрабатываются и кратковременно подвисают. На практике это может быть совсем небольшое ожидание - менее 10 мс. Выполняется оно только , если происходит split, и в ожидании есть реальная необходимость. В высоконагруженных системах данный вариант не рекомендуется. Но для домашнего использования может подойти, и вы эти задержки даже не заметите.</p><p>Если вы пытаетесь сплитнуть массивную передачу с , когда информация поступает быстрее отсылки, то без  ошибки сегментации будут сыпаться сплошным потоком. Работа по массивному потоку без ограничителей  и  обычно лишена смысла.</p><p>tpws работает на уровне сокетов, поэтому длинный запрос, не вмещающийся в 1 пакет (TLS с kyber), он получает целым блоком. На каждую сплит часть он делает отдельный вызов . Но ОС не сможет отослать данные в одном пакете, если размер превысит MTU. В случае слишком большого сегмента ОС дополнительно его порежет на более мелкие. Результат должен быть аналогичен nfqws.</p><p> заставляет слать каждый 2-й пакет с TTL=1, начиная с первого. К серверу приходят все четные пакеты сразу. На остальные ОС делает ретрансмиссию, и они приходят потом. Это само по себе создает дополнительную задержку (200 мс в linux для первой ретрансмиссии). Иным способом сделать disorder в сокет варианте не представляется возможным. Итоговый порядок для 6 сегментов получается .</p><p> высылает 1 байт out-of-band data после первого сплит сегмента.  в каждом сегменте сплита показал себя ненадежным. Сервер получает oob в сокет.</p><p>Сочетание  и  возможно только в Linux. Остальные ОС не умеют с таким справляться. Флаг URG теряется при ретрансмиссиях. Сервер получает oob в сокет. Сочетание этих параметров в ос, кроме Linux, вызывает ошибку на этапе запуска.</p><p> позволяют внутри одного tcp сегмента разрезать TLS ClientHello на 2 TLS records. Можно использовать стандартный механизм маркеров для задания относительных позиций.</p><p> ломает значительное количество сайтов. Криптобиблиотеки (openssl, ...) на оконечных http серверах без проблем принимают разделенные tls сегменты, но мидлбоксы - не всегда. К мидлбоксам можно отнести CDN или системы ddos-защиты. Поэтому применение  без ограничителей вряд ли целесообразно. В РФ  обычно не работает с TLS 1.2, потому что цензор парсит сертификат сервера из ServerHello. Работает только с TLS 1.3, поскольку там эта информация шифруется. Впрочем, сейчас сайтов, не поддерживающих TLS 1.3, осталось немного.</p><p> устанавливает опцию сокета TCP_MAXSEG. Клиент выдает это значение в tcp опциях SYN пакета. Сервер в ответ в SYN,ACK выдает свой MSS. На практике сервера обычно снижают размеры отсылаемых ими пакетов, но они все равно не вписываются в низкий MSS, указанный клиентом. Обычно чем больше указал клиент, тем больше шлет сервер. На TLS 1.2 если сервер разбил заброс так, чтобы домен из сертификата не попал в первый пакет, это может обмануть DPI, секущий ответ сервера. Схема может значительно снизить скорость и сработать не на всех сайтах.</p><p>С фильтром по hostlist совместимо только в <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/#%D0%BC%D0%BD%D0%BE%D0%B6%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5-%D1%81%D1%82%D1%80%D0%B0%D1%82%D0%B5%D0%B3%D0%B8%D0%B8-1\">некоторых случаях</a>, когда возможно узнать имя хоста на момент применения дурения.</p><p>Применяя данную опцию к сайтам TLS1.3, если броузер тоже поддерживает TLS1.3, то вы делаете только хуже. Но нет способа автоматически узнать когда надо применять, когда нет, поскольку MSS идет только в 3-way handshake еще до обмена данными, а версию TLS можно узнать только по ответу сервера, который может привести к реакции DPI. Использовать только когда нет ничего лучше или для отдельных ресурсов. Для http использовать смысла нет, поэтому заводите отдельный desync profile с фильтром по порту 443. Работает только на Linux, не работает на BSD и MacOS.</p><p>Параметр  добавляет паддинг-хедеров перед  на указанное количество байтов. Если размер  слишком большой, то идет разбивка на разные хедеры по 2K. Общий буфер приема http запроса - 64K, больший паддинг не поддерживается, да и http сервера такое уже не принимают. Полезно против DPI, выполняющих реассемблинг TCP с ограниченным буфером. Если техника работает, то после некоторого количества bytes http запрос начнет проходить до сайта. Если при этом критический размер padding около MTU, значит скорее всего DPI не выполняет реассемблинг пакетов, и лучше будет использовать обычные опции TCP сегментации. Если все же реассемблинг выполняется, то критический размер будет около размера буфера DPI. Он может быть 4K или 8K, возможны и другие значения.</p><p>Работают аналогично , кроме некоторых моментов. Нет параметра , поскольку  udp не поддерживает. Методы нулевой фазы () могут работать по хостлисту только в двух случаях: если используется режим socks и удаленный ресолвинг хостов через прокси, либо используется система <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/#%D0%BA%D1%8D%D1%88-ip\">кэша IP</a> для запоминания соответствия IP-&gt;hostname. Работоспособность вашей настройки в одном и том же режиме может зависеть от того, применяет ли клиент удаленный ресолвинг. Это может быть неочевидно. В одной программе работает, в другой - нет.</p><p>Если вы используете профиль с хостлистом , и вам нужен mss всегда, укажите mss в профиле с хостлистом, создайте еще один профиль без хостлиста, если его еще нет, и в нем еще раз укажите mss. Тогда при любом раскладе будет выполняться mss.</p><p>Если вам нужен mss по хостлисту, указывайте  только в профиле с хостлистом и убедитесь в наличии любого из необходимых условий работы в таком режиме.</p><p>Используйте  и  для проверки вашей стратегии. Смотрите вывод , чтобы убедиться в правильности настроек.</p><p> позволяет выводить подробный лог действий на консоль, в syslog или в файл. Может быть важен порядок следования опций.  лучше всего указывать в самом начале. Опции анализируются последовательно. Если ошибка будет при проверке опции, а до анализа  еще дело не дошло, то сообщения не будут выведены в файл или syslog.  позволяют сразу в одном параметре включить логирование на консоль и указать уровень. Сохранено для совместимости с более старыми версиями. Для выбора уровня в режиме syslog или file используйте отдельный параметр . Если в этих режимах  не указывать уровень через , то автоматически назначается уровень 1. При логировании в файл процесс не держит файл открытым. Ради каждой записи файл открывается и потом закрывается. Так что файл можно удалить в любой момент, и он будет создан заново при первом же сообщении в лог. Но имейте в виду, что если вы запускаете процесс под root, то будет сменен UID на не-root. В начале на лог файл меняется owner, иначе запись будет невозможна. Если вы потом удалите файл, и у процесса не будет прав на создание файла в его директории, лог больше не будет вестись. Вместо удаления лучше использовать truncate. В шелле это можно сделать через команду \": &gt;filename\"</p><p>tpws может биндаться на множество интерфейсов и IP адресов (до 32 шт). Порт всегда только один. Параметры  и  создают новый бинд. Остальные параметры  относятся к последнему бинду. Для бинда на все ipv4 укажите , на все ipv6 - .  - биндаемся на все ipv4 и ipv6. Выбор режима использования link local ipv6 адресов () :</p><pre><code>--bind-iface6 --bind-linklocal=no : сначала приватный адрес fc00::/7, затем глобальный адрес\n--bind-iface6 --bind-linklocal=unwanted : сначала приватный адрес fc00::/7, затем глобальный адрес, затем link local.\n--bind-iface6 --bind-linklocal=prefer : сначала link local, затем приватный адрес fc00::/7, затем глобальный адрес.\n--bind-iface6 --bind-linklocal=force : только link local\n</code></pre><p>Если не указано ни одного бинда, то создается бинд по умолчанию на все адреса всех интерфейсов. Для бинда на конкретный link-local address делаем так : <code>--bind-iface6=fe80::aaaa:bbbb:cccc:dddd%iface-name</code> Параметры  могут помочь в ситуациях, когда нужно взять IP с интерфейса, но его еще нет, он не поднят или не сконфигурирован. В разных системах события ifup ловятся по-разному и не гарантируют, что интерфейс уже получил IP адрес определенного типа. В общем случае не существует единого механизма повеситься на событие типа \"на интерфейсе X появился link local address\". Для бинда на известный ip, когда еще интерфейс не сконфигурирован, нужно делать так: <code>--bind-addr=192.168.5.3 --bind-wait-ip=20</code> В режиме transparent бинд возможен на любой несуществующий адрес, в режиме socks - только на существующий.</p><p>Параметры rcvbuf и sndbuf позволяют установить setsockopt SO_RCVBUF SO_SNDBUF для локального и удаленного соединения.</p><p> может быть полезен, когда tpws используется без дурения, чтобы привести MTU к MTU системы, на которой работает tpws. Это может быть полезно для скрытия факта использования VPN. Пониженный MTU - 1 из способов обнаружения подозрительного подключения. С tcp proxy ваши соединения неотличимы от тех, что сделал бы сам шлюз.</p><p> и <code>--remote-tcp-user-timeout</code> устанавливают значение таймаута в секундах для соединений клиент-прокси и прокси-сервер. Этот таймаут соответствует опции сокета linux TCP_USER_TIMEOUT. Под таймаутом подразумевается время, в течение которого буферизированные данные не переданы или на переданные данные не получено подтверждение (ACK) от другой стороны. Этот таймаут никак не касается времени отсутствия какой-либо передачи через сокет лишь потому, что данных для передачи нет. Полезно для сокращения время закрытия подвисших соединений. Поддерживается только на Linux и MacOS.</p><p>Режим  не требует повышенных привилегий (кроме бинда на привилегированные порты 1..1023). Поддерживаются версии socks 4 и 5 без авторизации. Версия протокола распознается автоматически. Подключения к IP того же устройства, на котором работает tpws, включая localhost, запрещены. socks5 позволяет удаленно ресолвить хосты (curl : --socks5-hostname firefox : socks_remote_dns=true). tpws поддерживает эту возможность асинхронно, не блокируя процессинг других соединений, используя многопоточный пул ресолверов. Количество потоков определяется автоматически в зависимости от , но можно задать и вручную через параметр . Запрос к socks выставляется на паузу, пока домен не будет преобразован в ip адрес в одном из потоков ресолвера. Ожидание может быть более длинным, если все потоки заняты. Если задан параметр , то подключения по именам хостов запрещаются, а пул ресолверов не создается. Тем самым экономятся ресурсы.</p><p>Для перенаправления tcp соединения на transparent proxy используются команды следующего вида :</p><pre><code>iptables -t nat -I OUTPUT -o &lt;внешний_интерфейс&gt; -p tcp --dport 80 -m owner ! --uid-owner tpws -j DNAT --to 127.0.0.127:988\niptables -t nat -I PREROUTING -i &lt;внутренний_интерфейс&gt; -p tcp --dport 80 -j DNAT --to 127.0.0.127:988\n</code></pre><p>Первая команда для соединений с самой системы, вторая - для проходящих через роутер соединений.</p><p>DNAT на localhost работает в цепочке OUTPUT, но не работает в цепочке PREROUTING без включения параметра route_localnet :</p><p><code>sysctl -w net.ipv4.conf.&lt;внутренний_интерфейс&gt;.route_localnet=1</code></p><p>Можно использовать <code>-j REDIRECT --to-port 988</code> вместо DNAT, однако в этом случае процесс transparent proxy должен слушать на ip адресе входящего интерфейса или на всех адресах. Слушать на всех - не есть хорошо с точки зрения безопасности. Слушать на одном (локальном) можно, но в случае автоматизированного скрипта придется его узнавать, потом динамически вписывать в команду. В любом случае требуются дополнительные усилия. Использование route_localnet тоже имеет потенциальные проблемы с безопасностью. Вы делаете доступным все, что висит на  для локальной подсети &lt; внутренний_интерфейс&gt;. Службы обычно привязываются к , поэтому можно средствами iptables запретить входящие на  не с интерфейса lo, либо повесить tpws на любой другой IP из , например на , и разрешить входящие не с lo только на этот IP.</p><pre><code>iptables -A INPUT ! -i lo -d 127.0.0.127 -j ACCEPT\niptables -A INPUT ! -i lo -d 127.0.0.0/8 -j DROP\n</code></pre><p>Фильтр по owner необходим для исключения рекурсивного перенаправления соединений от самого tpws. tpws запускается под пользователем , для него задается исключающее правило.</p><p>ip6tables работают почти точно так же, как и ipv4, но есть ряд важных нюансов. В DNAT следует брать адрес --to в квадратные скобки. Например :</p><p><code>ip6tables -t nat -I OUTPUT -o &lt;внешний_интерфейс&gt; -p tcp --dport 80 -m owner ! --uid-owner tpws -j DNAT --to [::1]:988</code></p><p>Параметра route_localnet не существует для ipv6. DNAT на localhost (::1) возможен только в цепочке OUTPUT. В цепочке PREROUTING DNAT возможен на любой global address или на link local address того же интерфейса, откуда пришел пакет. NFQUEUE работает без изменений.</p><pre><code>IFACE_WAN=wan\nIFACE_LAN=br-lan\n\nsysctl -w net.ipv4.conf.$IFACE_LAN.route_localnet=1\n\nnft create table inet ztest\n\nnft create chain inet ztest localnet_protect\nnft add rule inet ztest localnet_protect ip daddr 127.0.0.127 return\nnft add rule inet ztest localnet_protect ip daddr 127.0.0.0/8 drop\nnft create chain inet ztest input \"{type filter hook input priority filter - 1;}\"\nnft add rule inet ztest input iif != \"lo\" jump localnet_protect\n\nnft create chain inet ztest dnat_output \"{type nat hook output priority dstnat;}\"\nnft add rule inet ztest dnat_output meta skuid != tpws oifname $IFACE_WAN tcp dport { 80, 443 } dnat ip to 127.0.0.127:988\nnft create chain inet ztest dnat_pre \"{type nat hook prerouting priority dstnat;}\"\nnft add rule inet ztest dnat_pre meta iifname $IFACE_LAN tcp dport { 80, 443 } dnat ip to 127.0.0.127:988\n</code></pre><pre><code>nft delete table inet ztest\n</code></pre><p>Утилита ip2net предназначена для преобразования ipv4 или ipv6 списка ip в список подсетей с целью сокращения размера списка. Входные данные берутся из stdin, выходные выдаются в .</p><pre><code>-4                             ; лист - ipv4 (по умолчанию)\n-6                             ; лист - ipv6\n--prefix-length=min[-max]      ; диапазон рассматриваемых длин префиксов. например : 22-30 (ipv4), 56-64 (ipv6)\n--v4-threshold=mul/div         ; ipv4 : включать подсети, в которых заполнено по крайней мере mul/div адресов. например : 3/4\n--v6-threshold=N               ; ipv6 : минимальное количество ip для создания подсети\n</code></pre><p>В списке могут присутствовать записи вида ip/prefix и ip1-ip2. Такие записи выкидываются в stdout без изменений. Они принимаются командой ipset. ipset умеет для листов hash:net из ip1-ip2 делать оптимальное покрытие ip/prefix. ipfw из FreeBSD понимает ip/prefix, но не понимает ip1-ip2. ip2net фильтрует входные данные, выкидывая неправильные IP адреса.</p><p>Выбирается подсеть, в которой присутствует указанный минимум адресов. Для ipv4 минимум задается как процент от размера подсети (mul/div. например, 3/4), для ipv6 минимум задается напрямую.</p><p>Размер подсети выбирается следующим алгоритмом: Сначала в указанном диапазоне длин префиксов ищутся подсети, в которых количество адресов - максимально. Если таких сетей найдено несколько, берется наименьшая сеть (префикс больше). Например, заданы параметры v6_threshold=2 prefix_length=32-64, имеются следующие ipv6 :</p><pre><code>1234:5678:aaaa::5\n1234:5678:aaaa::6\n1234:5678:aaac::5\nРезультат будет :\n1234:5678:aaa8::/45\n</code></pre><p>Эти адреса так же входят в подсеть /32. Однако, нет смысла проходиться ковровой бомбардировкой, когда те же самые адреса вполне влезают в /45 и их ровно столько же. Если изменить v6_threshold=4, то результат будет:</p><pre><code>1234:5678:aaaa::5\n1234:5678:aaaa::6\n1234:5678:aaac::5\n</code></pre><p>То есть ip не объединятся в подсеть, потому что их слишком мало. Если изменить , результат будет:</p><pre><code>1234:5678:aaaa::/64\n1234:5678:aaac::5\n</code></pre><p>Требуемое процессорное время для вычислений сильно зависит от ширины диапазона длин префиксов, размера искомых подсетей и длины листа. Если ip2net думает слишком долго, не используйте слишком большие подсети и уменьшите диапазон длин префиксов. Учтите, что арифметика mul/div - целочисленная. При превышении разрядной сетки 32 bit результат непредсказуем. Не надо делать такое: 5000000/10000000. 1/2 - гораздо лучше.</p><p>Программа предназначена для многопоточного ресолвинга больших листов через системный DNS. Она берет из stdin список доменов и выводит в stdout результат ресолвинга. Ошибки выводятся в stderr.</p><pre><code>--threads=&lt;threads_number&gt;\t; количество потоков. по умолчанию 1.\n--family=&lt;4|6|46&gt;              ; выбор семейства IP адресов : ipv4, ipv6, ipv4+ipv6\n--verbose                      ; дебаг-лог на консоль\n--stats=N                      ; выводить статистику каждые N доменов\n--log-resolved=&lt;file&gt;          ; сохранять успешно отресолвленные домены в файл\n--log-failed=&lt;file&gt;            ; сохранять неудачно отресолвленные домены в файл\n--dns-make-query=&lt;domain&gt;      ; вывести в stdout бинарный DNS запрос по домену. если --family=6, запрос будет AAAA, иначе A.\n--dns-parse-query              ; распарсить бинарный DNS ответ и выдать все ivp4 и ipv6 адреса из него в stdout\n</code></pre><p>Параметры  и  позволяют провести ресолвинг одного домена через произвольный канал. Например, следующим образом можно выполнить DoH запрос, используя лишь mdig и curl :</p><pre><code>mdig --family=6 --dns-make-query=rutracker.org | curl --data-binary @- -H \"Content-Type: application/dns-message\" https://cloudflare-dns.com/dns-query | mdig --dns-parse-query\n</code></pre><h2>Способы получения списка заблокированных IP</h2><p>!!! nftables не могут работать с ipset-ами. Собственный аналогичный механизм требует огромного количество RAM !!! для загрузки больших листов. Например, для загона 100K записей в nfset не хватает даже 256 Mb. !!! Если вам нужны большие листы на домашних роутерах, откатывайтесь на iptables+ipset.</p><ol><li>Внесите заблокированные домены в <code>ipset/zapret-hosts-user.txt</code> и запустите  На выходе получите  с IP адресами.</li></ol><p>Cкрипты с названием get_reestr_* оперируют дампом реестра заблокированных сайтов :</p><ol start=\"2\"><li><p><code>ipset/get_reestr_resolve.sh</code> получает список доменов от rublacklist и дальше их ресолвит в ip адреса в файл ipset/zapret-ip.txt.gz. В этом списке есть готовые IP адреса, но судя во всему они там в точности в том виде, что вносит в реестр РосКомПозор. Адреса могут меняться, позор не успевает их обновлять, а провайдеры редко банят по IP : вместо этого они банят http запросы с \"нехорошим\" заголовком \"Host:\" вне зависимости от IP адреса. Поэтому скрипт ресолвит все сам, хотя это и занимает много времени. Используется мультипоточный ресолвер mdig (собственная разработка).</p></li><li><p><code>ipset/get_reestr_preresolved.sh</code>. то же самое, что и 2), только берется уже заресолвленый список со стороннего ресурса.</p></li><li><p><code>ipset/get_reestr_preresolved_smart.sh</code>. то же самое, что и 3), с добавлением всего диапазона некоторых автономных систем (прыгающие IP адреса из cloudflare, facebook, ...) и некоторых поддоменов блокируемых сайтов</p></li></ol><p>Cкрипты с названием  оперируют списками адресов и масок подсетей с сайтов antifilter.network и antifilter.download :</p><p>Все варианты рассмотренных скриптов автоматически создают и заполняют ipset. Варианты 2-10 дополнительно вызывают вариант 1.</p><ol start=\"11\"><li>. этот скрипт вызывает то, что прописано в переменной GETLIST из файла config Если переменная не определена, то ресолвятся лишь листы для ipset nozapret/nozapret6.</li></ol><p>Листы РКН все время изменяются. Возникают новые тенденции. Требования к RAM могут меняться. Поэтому необходима нечастая, но все же регулярная ревизия что же вообще у вас происходит на роутере. Или вы можете узнать о проблеме лишь когда у вас начнет постоянно пропадать wifi, и вам придется его перезагружать каждые 2 часа (метод кувалды).</p><p>Самые щадящие варианты по RAM - <code>get_antifilter_allyouneed.sh</code>, , .</p><p>Листы  и  сохраняются в сжатом виде в файлы .gz. Это позволяет снизить их размер во много раз и сэкономить место на роутере. Отключить сжатие листов можно параметром конфига GZIP_LISTS=0.</p><p>На роутерах не рекомендуется вызывать эти скрипты чаще раза за 2 суток, поскольку сохранение идет либо во внутреннюю флэш память роутера, либо в случае extroot - на флэшку. В обоих случаях слишком частая запись может убить флэшку, но если это произойдет с внутренней флэш памятью, то вы просто убьете роутер.</p><p>Принудительное обновление  выполняет скрипт . Если передан параметр , скрипт не обновляет , а только создает его при его отсутствии и заполняет. Это полезно, когда могут случиться несколько последовательных вызовов скрипта. Нет смысла несколько раз перезаполнять , это длительная операция на больших листах. Листы можно обновлять раз в несколько суток, и только тогда вызывать  без параметра . Во всех остальных случаях стоит применять .</p><p>Список РКН уже достиг внушительных размеров в сотни тысяч IP адресов. Поэтому для оптимизации  применяется утилита . Она берет список отдельных IP адресов и пытается интеллектуально создать из него подсети для сокращения количества адресов.  отсекает неправильные записи в листах, гарантируя отсутствие ошибок при их загрузке.  написан на языке C, поскольку операция ресурсоемкая. Иные способы роутер может не потянуть.</p><p>Можно внести список доменов в <code>ipset/zapret-hosts-user-ipban.txt</code>. Их ip адреса будут помещены в отдельный ipset . Он может использоваться для принудительного завертывания всех соединений на прозрачный proxy  или на VPN.</p><p> : если включен ipv6, то дополнительно создаются листы с таким же именем, но с \"6\" на конце перед расширением.  =&gt;  Создаются ipset-ы zapret6 и ipban6. Листы с antifilter не содержат список ipv6 адресов.</p><p>. Все скрипты ресолвят файл <code>zapret-hosts-user-exclude.txt</code>, создавая  и . Они загоняются в ipset-ы nozapret и nozapret6. Все правила, создаваемые init скриптами, создаются с учетом этих ipset. Помещенные в них IP не участвуют в процессе. <code>zapret-hosts-user-exclude.txt</code> может содержать домены, ipv4 и ipv6 адреса или подсети.</p><p>. Скрипты ipset/*.sh работают так же на FreeBSD. Вместо ipset они создают lookup таблицы ipfw с аналогичными именами. ipfw таблицы в отличие от ipset могут содержать как ipv4, так и ipv6 адреса и подсети в одной таблице, поэтому разделения нет.</p><p>Параметр конфига LISTS_RELOAD задает произвольную команду для перезагрузки листов. Это особенно полезно на BSD системах с PF. LISTS_RELOAD=- отключает перезагрузку листов.</p><h2>Фильтрация по именам доменов</h2><p>Альтернативой ipset является использование tpws или nfqws со списком доменов. Оба демона принимают неограниченное количество листов include () и exclude (). Прежде всего проверяются exclude листы. При вхождении в них происходит отказ от дурения. Далее при наличии include листов проверяется домен на вхождение в них. При невхождении в список отказ от дурения. Если все include листы пустые, это приравнивается к отсутствию include листов. Ограничение перестает работать. В иных случаях происходит дурение. Нет ни одного списка - дурение всегда. Есть только exclude список - дурение всех, кроме. Есть только include список - дурение только их. Есть оба - дурение только include, кроме exclude.</p><p>В системе запуска это обыграно следующим образом. Присутствуют 2 include списка : <code>ipset/zapret-hosts-user.txt.gz</code> или <code>ipset/zapret-hosts-user.txt</code>, <code>ipset/zapret-hosts.txt.gz</code> или  и 1 exclude список <code>ipset/zapret-hosts-user-exclude.txt.gz</code> или <code>ipset/zapret-hosts-user-exclude.txt</code></p><p>При режимах фильтрации  или  система запуска передает  или  все листы, файлы которых присутствуют. Передача происходит через замену маркеров  и  на реальные параметры , , . Если вдруг листы include присутствуют, но все они пустые, то работа аналогична отсутствию include листа. Файл есть, но несмотря на это дурится все, кроме exclude. Если вам нужен именно такой режим - не обязательно удалять . Достаточно сделать его пустым.</p><p>Поддомены учитываются автоматически. Например, строчка \"ru\" вносит в список \"*.ru\". Строчка \"*.ru\" в списке не сработает. Можно использовать символ  в начале хоста, чтобы отказаться от автоматического учета поддоменов.</p><p>Список доменов РКН может быть получен скриптами</p><pre><code>ipset/get_reestr_hostlist.sh\nipset/get_antizapret_domains.sh\nipset/get_reestr_resolvable_domains.sh\nipset/get_refilter_domains.sh\n</code></pre><p>Он кладется в <code>ipset/zapret-hosts.txt.gz</code>.</p><p>При изменении времени модификации или размера файлов списки перечитываются автоматически. После неатомарных операций изменения можно послать tpws/nfqws сигнал HUP для принудительной перечитки всех листов.</p><p>При фильтрации по именам доменов демон должен запускаться без фильтрации по ipset. tpws и nfqws решают нужно ли применять дурение в зависимости от хоста, полученного из протокола прикладного уровня (http, tls, quic). При использовании больших списков, в том числе списка РКН, оцените объем RAM на роутере ! Если после запуска демона RAM под завязку или случаются oom, значит нужно отказаться от таких больших списков.</p><h2>Режим фильтрации autohostlist</h2><p>Этот режим позволяет проанализировать как запросы со стороны клиента, так и ответы от сервера. Если хост еще не находится ни в каких листах и обнаруживается ситуация, похожая на блокировку, происходит автоматическое добавление хоста в список  как в памяти, так и в файле.  или  сами ведут этот файл. Чтобы какой-то хост не смог попась в  используйте . Если он все-же туда попал - удалите запись из файла вручную. Процессы автоматически перечитают файл. / сами назначают владельцем файла юзера, под которым они работают после сброса привилегий, чтобы иметь возможность обновлять лист.</p><p>В случае  данный режим требует перенаправления в том числе и входящего трафика. Крайне рекомендовано использовать ограничитель , чтобы  не обрабатывал гигабайты. По этой же причине не рекомендуется использование режима на BSD системах. Там нет фильтра .</p><p>На linux системах при использовании nfqws и фильтра connbytes может понадобиться : <code>sysctl net.netfilter.nf_conntrack_tcp_be_liberal=1</code> Было замечено, что некоторые DPI в России возвращают RST с неверным ACK. Это принимается tcp/ip стеком linux, но через раз приобретает статус INVALID в conntrack. Поэтому правила с  срабатывают через раз, не пересылая RST пакет .</p><p>Как вообще могут вести себя DPI, получив \"плохой запрос\" и приняв решение о блокировке:</p><ol><li>Зависание: просто отмораживается, блокируя прохождение пакетов по TCP каналу.</li><li>RST: отправляет RST клиенту и/или серверу</li><li>Редирект: (только для http) отправляет редирект на сайт-заглушку</li><li>Подмена сертификата: (только для https) полный перехват TLS сеанса с попыткой всунуть что-то свое клиенту. Применяется нечасто, поскольку броузеры на такое ругаются.</li></ol><p> и  могут сечь варианты 1-3, 4 они не распознают. В силу специфики работы с отдельными пакетами или с TCP каналом tpws и nfqws распознают эти ситуации по-разному. Что считается ситуацией, похожей на блокировку :</p><ol><li> Несколько ретрансмиссий первого запроса в TCP сеансе, в котором имеется host.</li><li> RST, пришедший в ответ на первый запрос с хостом.</li><li> HTTP редирект, пришедший в ответ на первый запрос с хостом, на глобальный адрес с доменом 2 уровня, не совпадающим с доменом 2 уровня оригинального запроса.</li><li> закрытие соединения клиентом после отправки первого запроса с хостом, если не было на него ответа со стороны сервера. Это обычно случается по таймауту, когда нет ответа (случай \"зависание\").</li></ol><p>Чтобы снизить вероятность ложных срабатываний, имеется счетчик ситуаций, похожих на блокировку. Если за определенное время произойдет более определенного их количества, хост считается заблокированным и заносится в . По нему сразу же начинает работать стратегия по обходу блокировки. Если в процессе счета вебсайт отвечает без признаков блокировки, счетчик сбрасывается. Вероятно, это был временный сбой сайта.</p><p>На практике работа с данным режимом выглядит так. Первый раз пользователь заходит на сайт и получает заглушку, сброс соединения или броузер подвисает, вываливаясь по таймауту с сообщением о невозможности загрузить страницу. Надо долбить F5, принуждая броузер повторять попытки. После некоторой попытки сайт начинает работать, и дальше он будет работать всегда.</p><p>С этим режимом можно использовать техники обхода, ломающие значительное количество сайтов. Если сайт не ведет себя как заблокированный, значит обход применен не будет. В противном случае терять все равно нечего. Однако, могут быть временные сбои сервера, приводящие к ситуации, аналогичной блокировке. Могут происходить ложные срабатывания. Если такое произошло, стратегия может начать ломать незаблокированный сайт. Эту ситуацию, увы, придется вам контролировать вручную. Заносите такие домены в <code>ipset/zapret-hosts-user-exclude.txt</code>, чтобы избежать повторения. Чтобы впоследствии разобраться почему домен был занесен в лист, можно включить . Он полезен тем, что работает без постоянного просмотра вывода  в режиме debug. В лог заносятся только основные события, ведущие к занесению хоста в лист. По логу можно понять как избежать ложных срабатываний и подходит ли вообще вам этот режим.</p><p>Можно использовать один  с множеством процессов. Все процессы проверяют время модификации файла. Если файл был изменен в другом процессе, происходит его перечитывание. Все процессы должны работать под одним uid, чтобы были права доступа на файл.</p><p>Скрипты  ведут  в <code>ipset/zapret-hosts-auto.txt</code>.  при апгрейде  сохраняет этот файл. Режим  включает в себя режим . Можно вести <code>ipset/zapret-hosts-user.txt</code>, <code>ipset/zapret-hosts-user-exclude.txt</code>.</p><p>Перед настройкой нужно провести исследование какую бяку устроил вам ваш провайдер.</p><p>Нужно выяснить не подменяет ли он DNS и какой метод обхода DPI работает. В этом вам поможет скрипт .</p><p>Если DNS подменяется, но провайдер не перехватывает обращения к сторонним DNS, поменяйте DNS на публичный. Например: 8.8.8.8, 8.8.4.4, 1.1.1.1, 1.0.0.1, 9.9.9.9 Если DNS подменяется и провайдер перехватывает обращения к сторонним DNS, настройте . Еще один эффективный вариант - использовать ресолвер от yandex 77.88.8.88 на нестандартном порту 1253. Многие провайдеры не анализируют обращения к DNS на нестандартных портах.  если видит подмену DNS автоматически переключается на DoH сервера.</p><p>Следует прогнать  по нескольким заблокированным сайтам и выявить общий характер блокировок. Разные сайты могут быть заблокированы по-разному, нужно искать такую технику, которая работает на большинстве. Чтобы записать вывод  в файл, выполните: <code>./blockcheck.sh | tee /tmp/blockcheck.txt</code>.</p><p>Проанализируйте какие методы дурения DPI работают, в соответствии с ними настройте .</p><p>Имейте в виду, что у провайдеров может быть несколько DPI или запросы могут идти через разные каналы по методу балансировки нагрузки. Балансировка может означать, что на разных ветках разные DPI или они находятся на разных хопах. Такая ситуация может выражаться в нестабильности работы обхода. Дернули несколько раз curl. То работает, то connection reset или редирект.  выдает странноватые результаты. То split работает на 2-м. хопе, то на 4-м. Достоверность результата вызывает сомнения. В этом случае задайте несколько повторов одного и того же теста. Тест будет считаться успешным только, если все попытки пройдут успешно.</p><p>При использовании  следует протестировать как можно больше разных доменов. Эта техника может на одних провайдерах работать стабильно, на других потребуется выяснить при каких параметрах она стабильна, на третьих полный хаос, и проще отказаться.</p><p> имеет 3 уровня сканирования.</p><ul><li> - максимально быстро найти хоть что-то работающее.</li><li> дает возможность провести исследование как и на что реагирует DPI в плане методов обхода.</li><li> дает максимум проверок даже в случаях, когда ресурс работает без обхода или с более простыми стратегиями.</li></ul><p>Есть ряд других параметров, которые не будут спрашиваться в диалоге, но которые можно переопределить через переменные.</p><pre><code>CURL - замена программы curl\nCURL_MAX_TIME - время таймаута curl в секундах\nCURL_MAX_TIME_QUIC - время таймаута curl для quic. если не задано, используется значение CURL_MAX_TIME\nCURL_MAX_TIME_DOH - время таймаута curl для DoH серверов\nCURL_CMD=1 - показывать команды curl\nCURL_OPT - дополнительные параметры curl. `-k` - игнор сертификатов. `-v` - подробный вывод протокола\nDOMAINS - список тестируемых доменов через пробел\nIPVS=4|6|46 - тестируемые версии ip протокола\nENABLE_HTTP=0|1 - включить тест plain http\nENABLE_HTTPS_TLS12=0|1 - включить тест https TLS 1.2\nENABLE_HTTPS_TLS13=0|1 - включить тест https TLS 1.3\nENABLE_HTTP3=0|1 - включить тест QUIC\nREPEATS - количество попыток тестирования\nPARALLEL=0|1 - включить параллельные попытки. может обидеть сайт из-за долбежки и привести к неверному результату\nSCANLEVEL=quick|standard|force - уровень сканирования\nBATCH=1 - пакетный режим без вопросов и ожидания ввода в консоли\nHTTP_PORT, HTTPS_PORT, QUIC_PORT - номера портов для соответствующих протоколов\nSKIP_DNSCHECK=1 - отказ от проверки DNS\nSKIP_IPBLOCK=1 - отказ от тестов блокировки по порту или IP\nSKIP_TPWS=1 - отказ от тестов tpws\nSKIP_PKTWS=1 - отказ от тестов nfqws/dvtws/winws\nPKTWS_EXTRA, TPWS_EXTRA - дополнительные параметры nfqws/dvtws/winws и tpws, указываемые после основной стратегии\nPKTWS_EXTRA_1 .. PKTWS_EXTRA_9, TPWS_EXTRA_1 .. TPWS_EXTRA_9 - отдельно дополнительные параметры, содержащие пробелы\nPKTWS_EXTRA_PRE - дополнительные параметры для nfqws/dvtws/winws, указываемые перед основной стратегией\nPKTWS_EXTRA_PRE_1 .. PKTWS_EXTRA_PRE_9 - отдельно дополнительные параметры, содержащие пробелы\nSECURE_DNS=0|1 - принудительно выключить или включить DoH\nDOH_SERVERS - список URL DoH через пробел для автоматического выбора работающего сервера\nDOH_SERVER - конкретный DoH URL, отказ от поиска\nUNBLOCKED_DOM - незаблокированный домен, который используется для тестов IP block\nMIN_TTL,MAX_TTL - пределы тестов с TTL. MAX_TTL=0 отключает тесты.\nMIN_AUTOTTL_DELTA,MAX_AUTOTTL_DELTA - пределы тестов с autottl по дельте. MAX_AUTOTTL_DELTA=0 отключает тесты.\nSIMULATE=1 - включить режим симуляции для отладки логики скрипта. отключаются реальные запросы через curl, заменяются рандомным результатом.\nSIM_SUCCESS_RATE=&lt;percent&gt; - вероятность успеха симуляции в процентах\n</code></pre><p>Пример запуска с переменными:<code>SECURE_DNS=1 SKIP_TPWS=1 CURL_MAX_TIME=1 CURL=/tmp/curl ./blockcheck.sh</code></p><p> Если в системе присутствует совместимый  (ncat от nmap или openbsd ncat. в OpenWrt по умолчанию нет), то выполняется сканирование портов http или https всех IP адресов домена. Если ни один IP не отвечает, то результат очевиден. Можно останавливать сканирование. Автоматически оно не остановится, потому что netcat-ы недостаточно подробно информируют о причинах ошибки. Если доступна только часть IP, то можно ожидать хаотичных сбоев, т.к. подключение идет к случайному адресу из списка.</p><p><strong>ПРОВЕРКА НА ЧАСТИЧНЫЙ IP block</strong> Под частичным блоком подразумевается ситуация, когда коннект на порты есть, но по определенному транспортному или прикладному протоколу всегда идет реакция DPI вне зависимости от запрашиваемого домена. Эта проверка так же не выдаст автоматического вердикта/решения, потому что может быть очень много вариаций. Вместо этого анализ происходящего возложен на самого пользователя или тех, кто будет читать лог. Суть этой проверки в попытке дернуть неблокированный IP с блокированным доменом и наоборот, анализируя при этом реакцию DPI. Реакция DPI обычно проявляется в виде таймаута (зависание запроса), connection reset или http redirect на заглушку. Любой другой вариант скорее всего говорит об отсутствии реакции DPI. В частности, любые http коды, кроме редиректа, ведущего именно на заглушку, а не куда-то еще. На TLS - ошибки handshake без задержек. Ошибка сертификата может говорить как о реакции DPI с MiTM атакой (подмена сертификата), так и о том, что принимающий сервер неблокированного домена все равно принимает ваш TLS  с чужим доменом, пытаясь при этом выдать сертификат без запрошенного домена. Требуется дополнительный анализ. Если на заблокированный домен есть реакция на всех IP адресах, значит есть блокировка по домену. Если на неблокированный домен есть реакция на IP адресах блокированного домена, значит имеет место блок по IP. Соответственно, если есть и то, и другое, значит есть и блок по IP, и блок по домену. Неблокированный домен первым делом проверяется на доступность на оригинальном адресе. При недоступности тест отменяется, поскольку он будет неинформативен.</p><p>Если выяснено, что есть частичный блок по IP на DPI, то скорее всего все остальные тесты будут провалены вне зависимости от стратегий обхода. Но бывают и некоторые исключения. Например, пробитие через . Или сделать так, чтобы он не мог распознать протокол прикладного уровня. Дальнейшие тесты могут быть не лишены смысла.</p><p><strong>ПРИМЕРЫ БЛОКИРОВКИ ТОЛЬКО ПО ДОМЕНУ БЕЗ БЛОКА ПО IP</strong></p><pre><code>&gt; testing iana.org on it's original\n!!!!! AVAILABLE !!!!!\n&gt; testing rutracker.org on 192.0.43.8 (iana.org)\ncurl: (28) Operation timed out after 1002 milliseconds with 0 bytes received\n&gt; testing iana.org on 172.67.182.196 (rutracker.org)\nHTTP/1.1 409 Conflict\n&gt; testing iana.org on 104.21.32.39 (rutracker.org)\nHTTP/1.1 409 Conflict\n\n&gt; testing iana.org on it's original ip\n!!!!! AVAILABLE !!!!!\n&gt; testing rutracker.org on 192.0.43.8 (iana.org)\ncurl: (28) Connection timed out after 1001 milliseconds\n&gt; testing iana.org on 172.67.182.196 (rutracker.org)\ncurl: (35) OpenSSL/3.2.1: error:0A000410:SSL routines::ssl/tls alert handshake failure\n&gt; testing iana.org on 104.21.32.39 (rutracker.org)\ncurl: (35) OpenSSL/3.2.1: error:0A000410:SSL routines::ssl/tls alert handshake failure\n\n&gt; testing iana.org on it's original ip\n!!!!! AVAILABLE !!!!!\n&gt; testing rutracker.org on 192.0.43.8 (iana.org)\nHTTP/1.1 307 Temporary Redirect\nLocation: https://www.gblnet.net/blocked.php\n&gt; testing iana.org on 172.67.182.196 (rutracker.org)\nHTTP/1.1 409 Conflict\n&gt; testing iana.org on 104.21.32.39 (rutracker.org)\nHTTP/1.1 409 Conflict\n\n&gt; testing iana.org on it's original ip\n!!!!! AVAILABLE !!!!!\n&gt; testing rutracker.org on 192.0.43.8 (iana.org)\ncurl: (35) Recv failure: Connection reset by peer\n&gt; testing iana.org on 172.67.182.196 (rutracker.org)\ncurl: (35) OpenSSL/3.2.1: error:0A000410:SSL routines::ssl/tls alert handshake failure\n&gt; testing iana.org on 104.21.32.39 (rutracker.org)\ncurl: (35) OpenSSL/3.2.1: error:0A000410:SSL routines::ssl/tls alert handshake failure\n</code></pre><p><strong>ПРИМЕР ПОЛНОГО IP БЛОКА ИЛИ БЛОКА TCP ПОРТА ПРИ ОТСУТСТВИИ БЛОКА ПО ДОМЕНУ</strong></p><pre><code>* port block tests ipv4 startmail.com:80\n  ncat -z -w 1 145.131.90.136 80\n  145.131.90.136 does not connect. netcat code 1\n  ncat -z -w 1 145.131.90.152 80\n  145.131.90.152 does not connect. netcat code 1\n\n* curl_test_http ipv4 startmail.com\n- checking without DPI bypass\n  curl: (28) Connection timed out after 2002 milliseconds\n  UNAVAILABLE code=28\n\n- IP block tests (requires manual interpretation)\n\n&gt; testing iana.org on it's original ip\n!!!!! AVAILABLE !!!!!\n&gt; testing startmail.com on 192.0.43.8 (iana.org)\nHTTP/1.1 302 Found\nLocation: https://www.iana.org/\n&gt; testing iana.org on 145.131.90.136 (startmail.com)\ncurl: (28) Connection timed out after 2002 milliseconds\n&gt; testing iana.org on 145.131.90.152 (startmail.com)\ncurl: (28) Connection timed out after 2002 milliseconds\n</code></pre><p>Файл  используется различными компонентами системы и содержит основные настройки. Его нужно просмотреть и при необходимости отредактировать.</p><p>На linux системах можно выбрать использовать  или . По умолчанию на традиционных linux выбирается , если установлен nft. На OpenWrt по умолчанию выбирается  на новых версиях с firewall4.</p><p>На  можно отключить стандартную схему перехвата трафика после NAT и перейти на перехват до NAT. Это сделает невозможным применение некоторых методов дурения на проходящем трафике как в случае с . nfqws начнет получать адреса пакетов из локальной сети и отображать их в логах.</p><p>Существует 3 стандартных опции запуска, настраиваемых раздельно и независимо: , , . Их можно использовать как по отдельности, так и вместе. Например, вам надо сделать комбинацию из методов, доступных только в  и только в . Их можно задействовать вместе.  будет прозрачно локализовывать трафик на системе и применять свое дурение,  будет дурить трафик, исходящий с самой системы после обработки на . А можно на эту же систему повесить без параметров socks proxy, чтобы получать доступ к обходу блокировок через прокси. Таким образом, все 3 режима вполне могут задействоваться вместе. Так же безусловно и независимо, в добавок к стандартным опциям, применяются все custom скрипты в <code>init.d/{sysv,openwrt,macos}/custom.d</code>.</p><p>Однако, при комбинировании tpws и nfqws с пересечением по L3/L4 протоколам не все так просто , как может показаться на первый взгляд. Первым всегда работает tpws, за ним - nfqws. На nfqws попадает уже \"задуренный\" трафик от tpws. Получается, что дурилка дурит дурилку, и дурилка не срабатывает, потому что ее задурили. Вот такой веселый момент. nfqws перестает распознавать протоколы и применять методы. Некоторые методы дурения от tpws nfqws в состоянии распознать и отработать корректно, но большинство - нет. Решение - использование <code>--dpi-desync-any-protocol</code> в nfqws и работа как с неизвестным протоколом. Комбинирование tpws и nfqws является продвинутым вариантом, требующим глубокого понимания происходящего. Очень желательно проанализировать действия nfqws по  логу. Все ли так, как вы задумали.</p><p>Одновременное использование tpws и nfqws без пересечения по L3/L4 (то есть nfqws - udp, tpws - tcp или nfqws - port 443, tpws - port 80 или nfqws - ipv4, tpws - ipv6) проблем не представляет.</p><p> требует настройки параметров , но не требует перехвата трафика. Остальные опции требуют раздельно настройки перехвата трафика и опции самих демонов. Каждая опция предполагает запуск одного инстанса соответствующего демона. Все различия методов дурения для , ,  и т.д. должны быть отражены через схему мультистратегий. В этом смысле настройка похожа на вариант  на Windows, а перенос конфигов не должен представлять больших сложностей.</p><p>Основное правило настройки перехвата - перехватывайте только необходимый минимум. Любой перехват лишнего - это бессмысленная нагрузка на вашу систему. Опции демонов  использовать нужно с умом. Не стоит перехватывать весь трафик, чтобы потом по параметру --ipset выделить лишь горстку IP. Это будет работать, но очень неэффективно с точки зрения нагрузки на систему. Используйте -ы режима ядра. При необходимости пишите и задействуйте . Но если у вас и так идет работа по всем IP, и нужно написать небольшую специализацию по IP, то --ipset вполне уместен.</p><p>Настройки демонов можно для удобства писать на нескольких строках, используя двойные или одинарные кавычки. Чтобы задействовать стандартные обновляемые хост-листы из каталога , используйте маркер \n </p><p><em><strong>Изменение бита mark для предотвращения зацикливания</strong></em></p><p><em><strong>Изменение бита mark для пометки пакетов, проходящих по POSTNAT схеме (только nftables)</strong></em><code>DESYNC_MARK_POSTNAT=0x20000000</code></p><p><em><strong>Если раскоментировано, пометка пакетов, которые должны быть обработаны zapret.</strong></em></p><p>Бит должен быть установлен вашими собственными правилами.</p><ul><li>Для iptables - в цепочках mangle PREROUTING и mangle OUTPUT перед правилами zapret (iptables -I  применения правил zapret).</li><li>Для nftables - в хуках output и prerouting с приоритетом -102 или ниже.</li></ul><p>Критерии пометки любые. Например, IP адрес или интерфейс источника. Это ответ на вопрос \"как мне сделать, чтобы телик не ходил через zapret или чтобы через него ходил только мой комп\".</p><p><em><strong>Включение стандартной опции tpws в режиме socks</strong></em></p><p><em><strong>На каком порту будет слушать tpws socks. прослушивается только localhost и LAN</strong></em></p><p><em><strong>Параметры tpws для режима socks</strong></em></p><pre><code>TPWS_SOCKS_OPT=\"\n--filter-tcp=80 --methodeol &lt;HOSTLIST&gt; --new\n--filter-tcp=443 --split-pos=1,midsld --disorder &lt;HOSTLIST&gt;\"\n</code></pre><p><em><strong>Включение стандартной опции tpws в прозрачном режиме</strong></em></p><p><em><strong>Какие tcp порты следует перенаправлять на tpws</strong></em></p><p><em><strong>Параметры tpws для прозрачного режима</strong></em></p><pre><code>TPWS_OPT=\"\n--filter-tcp=80 --methodeol &lt;HOSTLIST&gt; --new\n--filter-tcp=443 --split-pos=1,midsld --disorder &lt;HOSTLIST&gt;\"\n</code></pre><p><em><strong>Включение стандартной опции nfqws</strong></em></p><p><em><strong>Какие tcp и udp порты следует перенаправлять на nfqws с использованием connbytes ограничителя</strong></em></p><p>connbytes позволяет из каждого соединения перенаправить только заданное количество начальных пакетов по каждому направлению - на вход и на выход. Это более эффективная kernel-mode замена параметра nfqws .</p><pre><code>NFQWS_PORTS_TCP=80,443\nNFQWS_PORTS_UDP=443\n</code></pre><p><em><strong>Сколько начальных входящих и исходящих пакетов нужно перенаправлять на nfqws по каждому направлению</strong></em></p><pre><code>NFQWS_TCP_PKT_OUT=$((6+$AUTOHOSTLIST_RETRANS_THRESHOLD))\nNFQWS_TCP_PKT_IN=3\nNFQWS_UDP_PKT_OUT=$((6+$AUTOHOSTLIST_RETRANS_THRESHOLD))\nNFQWS_UDP_PKT_IN=0\n</code></pre><p><em><strong>Задать порты для перенаправления на nfqws без connbytes ограничителя</strong></em> Есть трафик, исходящий сеанс для которого необходимо перенаправлять весь без ограничителей. Типичное применение - поддержка http keepalives на stateless DPI. Это существенно нагружает процессор. Использовать только если понимаете зачем. Чаще всего это не нужно. Входящий трафик ограничивается по connbytes через параметры PKT_IN. Если указываете здесь какие-то порты, желательно их убрать из версии с connbytes ограничителем</p><pre><code>NFQWS_PORTS_TCP_KEEPALIVE=80\nNFQWS_PORTS_UDP_KEEPALIVE=\n</code></pre><pre><code>NFQWS_OPT=\"\n--filter-tcp=80 --dpi-desync=fake,multisplit --dpi-desync-split-pos=method+2 --dpi-desync-fooling=md5sig &lt;HOSTLIST&gt; --new\n--filter-tcp=443 --dpi-desync=fake,multidisorder --dpi-desync-split-pos=1,midsld --dpi-desync-fooling=badseq,md5sig &lt;HOSTLIST&gt; --new\n--filter-udp=443 --dpi-desync=fake --dpi-desync-repeats=6 &lt;HOSTLIST_NOAUTO&gt;\n</code></pre><pre><code>none - применять дурение ко всем хостам\nipset - ограничить дурение ipset-ом zapret/zapret6\nhostlist - ограничить дурение списком хостов из файла\nautohostlist - режим hostlist + распознавание блокировок и ведение автоматического листа\n</code></pre><p><em><strong>Настройка системы управления выборочным traffic offload (только если поддерживается)</strong></em></p><pre><code>donttouch: выборочное управление отключено, используется системная настройка, простой инсталлятор выключает системную настройку, если она не совместима с выбранным режимом\nnone: выборочное управление отключено, простой инсталлятор выключает системную настройку\nsoftware: выборочное управление включено в режиме software, простой инсталлятор выключает системную настройку\nhardware: выборочное управление включено в режиме hardware, простой инсталлятор выключает системную настройку\n</code></pre><p>Параметр  указывает инсталлятору  какой скрипт дергать для обновления списка заблокированных ip или хостов. Он же вызывается через  из запланированных заданий (crontab или systemd timer). Поместите сюда название скрипта, который будете использовать для обновления листов. Если не нужно, то параметр следует закомментировать.</p><p>Можно индивидуально отключить ipv4 или ipv6. Если параметр закомментирован или не равен \"1\", использование протокола разрешено.</p><pre><code>DISABLE_IPV4=1\nDISABLE_IPV6=1\n</code></pre><p>Количество потоков для многопоточного DNS ресолвера mdig (1..100). Чем их больше, тем быстрее, но не обидится ли на долбежку ваш DNS сервер?</p><p>Место для хранения временных файлов. При скачивании огромных реестров в  места может не хватить. Если файловая система на нормальном носителе (не встроенная память роутера), то можно указать место на флэшке или диске. </p><p><em><strong>Опции для создания ipset-ов и nfset-ов</strong></em></p><pre><code>SET_MAXELEM=262144\nIPSET_OPT=\"hashsize 262144 maxelem 2097152\"\n</code></pre><p>Хук, позволяющий внести ip адреса динамически. $1 = имя таблицы Адреса выводятся в stdout. В случае nfset автоматически решается проблема возможного пересечения интервалов.<code>IPSET_HOOK=\"/etc/zapret.ipset.hook\"</code></p><p><em><strong>ПРО РУГАНЬ в dmesg по поводу нехватки памяти.</strong></em></p><p>Может так случиться, что памяти в системе достаточно, но при попытке заполнить огромный  ядро начинает громко ругаться,  заполняется не полностью. Вероятная причина в том, что превышается , заданный при создании  (create_ipset.sh). Происходит переаллокация списка, не находится непрерывных фрагментов памяти нужной длины. Это лечится увеличением . Но чем больше , тем больше занимает  в памяти. Задавать слишком большой  для недостаточно больших списков нецелесообразно.</p><p><em><strong>Опции для вызова ip2net. Отдельно для листов ipv4 и ipv6.</strong></em></p><pre><code>IP2NET_OPT4=\"--prefix-length=22-30 --v4-threshold=3/4\"\nIP2NET_OPT6=\"--prefix-length=56-64 --v6-threshold=5\"\n</code></pre><p><em><strong>Настройка режима autohostlist.</strong></em></p><p>При увеличении AUTOHOSTLIST_RETRANS_THRESHOLD и использовании nfqws следует пересмотреть значения параметров NFQWS_TCP_PKT_OUT и NFQWS_UDP_PKT_OUT. Все ретрансмиссии должны быть получены nfqws, иначе триггер \"зависание запроса\" не сработает.</p><pre><code>AUTOHOSTLIST_RETRANS_THRESHOLD=3\nAUTOHOSTLIST_FAIL_THRESHOLD=3\nAUTOHOSTLIST_FAIL_TIME=60\nAUTOHOSTLIST_DEBUG=0\n</code></pre><p><em><strong>Включить или выключить сжатие больших листов в скриптах ipset/*.sh.</strong></em></p><p><em><strong>Команда для перезагрузки ip таблиц фаервола.</strong></em></p><p>Если не указано или пустое, выбирается автоматически ipset или ipfw при их наличии. На BSD системах с PF нет автоматической загрузки. Там нужно указать команду явно:  На более новых pfctl (есть в новых FreeBSD, нет в OpenBSD 6.8) можно дать команду загрузки только таблиц: <code>pfctl -Tl -f /etc/pf.conf</code> \"-\" означает отключение загрузки листов даже при наличии поддерживаемого backend.</p><pre><code>LISTS_RELOAD=\"pfctl -f /etc/pf.conf\"\nLISTS_RELOAD=-\n</code></pre><p>В OpenWrt существует сеть по умолчанию 'lan'. Только трафик с этой сети будет перенаправлен на tpws. Но возможно задать другие сети или список сетей:<code>OPENWRT_LAN=\"lan lan2 lan3\"</code></p><p>В OpenWrt в качестве wan берутся интерфейсы, имеющие default route. Отдельно для ipv4 и ipv6. Это можно переопределить:</p><pre><code>OPENWRT_WAN4=\"wan4 vpn\"\nOPENWRT_WAN6=\"wan6 vpn6\"\n</code></pre><p>Параметр  разрешает init скрипту самостоятельно применять правила iptables. При иных значениях или если параметр закомментирован, правила применены не будут.<p> Это полезно, если у вас есть система управления фаерволом, в настройки которой и следует прикрутить правила.</p> На OpenWrt неприменимо при использовании firewall3+iptables.</p><p><code>FILTER_TTL_EXPIRED_ICMP=1</code> включает механизмы блокировки пакетов icmp time exceeded, высылаемые роутерами по пути следования пакета в ответ на исчерпание TTL/HL. В linux соединение обрывается системой, если в ответ на первый пакет (для tcp - SYN) пришел такой icmp. Аналогичная схема имеется и в datagram сокетах. Блокировка icmp идет исключительно за счет средств iptables/nftables. Чтобы не трогать весь трафик, в режиме PRENAT используется connmark для пометки сеансов, над которыми поработал nfqws. В режиме POSTNAT так сделать нельзя, поэтому помечаются все сеансы, заворачиваемые на nfqws. Настройку лучше отключить, если вы не ожидаете проблем от icmp, тк в этом случае будет меньше ненужных вмешательств в трафик.</p><p><em><strong>Следующие настройки не актуальны для openwrt:</strong></em></p><p>Если ваша система работает как роутер, то нужно вписать названия внутренних и внешних интерфейсов:</p><pre><code>IFACE_LAN=eth0\nIFACE_WAN=eth1\nIFACE_WAN6=\"henet ipsec0\"\n</code></pre><p>Несколько интерфейсов могут быть вписаны через пробел. Если IFACE_WAN6 не задан, то берется значение IFACE_WAN.</p><blockquote><p>[!IMPORTANT] Настройка маршрутизации, маскарада и т.д. не входит в задачу zapret. Включаются только режимы, обеспечивающие перехват транзитного трафика. Возможно определить несколько интерфейсов следующим образом:</p></blockquote><p><code>IFACE_LAN=\"eth0 eth1 eth2\"</code></p><h2>Прикручивание к системе управления фаерволом или своей системе запуска</h2><p>Если вы используете какую-то систему управления фаерволом, то она может вступать в конфликт с имеющимся скриптом запуска. При повторном применении правил она могла бы поломать настройки iptables от zapret. В этом случае правила для iptables должны быть прикручены к вашему фаерволу отдельно от запуска tpws или nfqws.</p><p><em>Следующие вызовы позволяют применить или убрать правила iptables отдельно:</em></p><pre><code>/opt/zapret/init.d/sysv/zapret start_fw\n/opt/zapret/init.d/sysv/zapret stop_fw\n/opt/zapret/init.d/sysv/zapret restart_fw\n</code></pre><p><em>А так можно запустить или остановить демоны отдельно от фаервола:</em></p><pre><code>/opt/zapret/init.d/sysv/zapret start_daemons\n/opt/zapret/init.d/sysv/zapret stop_daemons\n/opt/zapret/init.d/sysv/zapret restart_daemons\n</code></pre><p> сводят практически на нет конфликты между разными системами управления, поскольку позволяют использовать независимые таблицы и хуки. Используется отдельная nf-таблица \"zapret\". Если ваша система ее не будет трогать, скорее всего все будет нормально.</p><p><em>Для  предусмотрено несколько дополнительных вызовов:</em></p><p>Посмотреть set-ы интерфейсов, относящихся к lan, wan и wan6. По ним идет завертывание трафика. А так же таблицу flow table с именами интерфейсов ingress hook.<code>/opt/zapret/init.d/sysv/zapret list_ifsets</code></p><p>Обновить set-ы интерфейсов, относящихся к lan, wan и wan6. Для традиционных linux список интерфейсов берется из переменных конфига IFACE_LAN, IFACE_WAN. Для OpenWrt определяется автоматически. Множество lanif может быть расширено параметром OPENWRT_LAN. Все интерфейсы lan и wan так же добавляются в ingress hook от flow table.<code>/opt/zapret/init.d/sysv/zapret reload_ifsets</code></p><p>Просмотр таблицы без содержимого set-ов. Вызывает <code>nft -t list table inet zapret</code><code>/opt/zapret/init.d/sysv/zapret list_table</code></p><p><em>Так же возможно прицепиться своим скриптом к любой стадии применения и снятия фаервола со стороны zapret скриптов:</em></p><pre><code>INIT_FW_PRE_UP_HOOK=\"/etc/firewall.zapret.hook.pre_up\"\nINIT_FW_POST_UP_HOOK=\"/etc/firewall.zapret.hook.post_up\"\nINIT_FW_PRE_DOWN_HOOK=\"/etc/firewall.zapret.hook.pre_down\"\nINIT_FW_POST_DOWN_HOOK=\"/etc/firewall.zapret.hook.post_down\"\n</code></pre><p>Эти настройки доступны в config. Может быть полезно, если вам нужно использовать nftables set-ы, например /. nfset-ы принадлежат только одной таблице, следовательно вам придется писать правила для таблицы zapret, а значит нужно синхронизироваться с применением/снятием правил со стороны zapret скриптов.</p><p>custom скрипты - это маленькие shell программы, управляющие нестандартными режимами применения zapret или частными случаями, которые не могут быть интегрированы в основную часть без загромождения и замусоривания кода. Для применения custom следует помещать файлы в следующие директории в зависимости от вашей системы:</p><pre><code>/opt/zapret/init.d/sysv/custom.d\n/opt/zapret/init.d/openwrt/custom.d\n/opt/zapret/init.d/macos/custom.d\n</code></pre><p>Директория будет просканирована в алфавитном порядке, и каждый скрипт будет применен.</p><p>В  имеется , в  - . Это готовые скрипты, которые можно копировать в . Их можно взять за основу для написания собственных.</p><p><em><strong>Для linux пишется код в функции</strong></em></p><pre><code>zapret_custom_daemons\nzapret_custom_firewall\nzapret_custom_firewall_nft\nzapret_custom_firewall_nft_flush\n</code></pre><pre><code>zapret_custom_daemons\nzapret_custom_firewall_v4\nzapret_custom_firewall_v6\n</code></pre><p>zapret_custom_daemons поднимает демоны / в нужном вам количестве и с нужными вам параметрами. В первом параметре передается код операции: 1 = запуск, 0 = останов. Схема запуска демонов в OpenWrt отличается - используется procd. Поэтому логика останова отсутствует за ненадобностью, останов никогда не вызывается.</p><p>zapret_custom_firewall поднимает и убирает правила . В первом параметре передается код операции: 1 = запуск, 0 = останов.</p><p>zapret_custom_firewall_nft поднимает правила nftables. Логика останова отсутствует за ненадобностью. Стандартные цепочки zapret удаляются автоматически. Однако, sets и правила из ваших собственных цепочек не удаляются. Их нужно подчистить в zapret_custom_firewall_nft_flush. Если set-ов и собственных цепочек у вас нет, функцию можно не определять или оставить пустой.</p><p>Если вам не нужны iptables или nftables - можете не писать соответствующую функцию.</p><p>В linux можно использовать локальные переменные  и . добавляет код к правилам ip/nf tables до кода, генерируемого функциями-хелперами. добавляет код после.</p><p>В linux функции-хелперы добавляют правило в начало цепочек, то есть перед уже имеющимися. Поэтому специализации должны идти после более общих вариантов. Для macos правило обратное. Там правила добавляются в конец. По этой же причине фаервол в Linux сначала применяется в стандартном режиме, потом custom, а в MacOS сначала custom, потом стандартный режим.</p><p>В macos firewall-функции ничего сами никуда не заносят. Их задача - лишь выдать текст в stdout, содержащий правила для pf-якоря. Остальное сделает обертка.</p><p>Особо обратите внимание на номер демона в функциях  , , , ,  , номера портов  и очередей . Они должны быть уникальными во всех скриптах. При накладке будет ошибка. Поэтому используйте функции динамического получения этих значений из пула.</p><p> скрипты могут использовать переменные из . Можно помещать в  свои переменные и задействовать их в скриптах. Можно использовать функции-хелперы. Они являются частью общего пространства функций shell. Полезные функции можно взять из примеров скриптов. Так же смотрите . Используя хелпер функции, вы избавитесь от необходимости учитывать все возможные случаи типа наличия/отсутствия ipv6, является ли система роутером, имена интерфейсов, ...Хелперы это учитывают. Вам нужно сосредоточиться лишь на фильтрах  и параметрах демонов.</p><p> автоматизирует ручные варианты процедур установки. Он поддерживает OpenWrt, linux системы на базе systemd или openrc и MacOS.</p><p>Для более гибкой настройки перед запуском инсталлятора следует выполнить раздел \"Выбор параметров\".</p><p>Если система запуска поддерживается, но используется не поддерживаемый инсталлятором менеджер пакетов или названия пакетов не соответствуют прописанным в инсталлятор, пакеты нужно установить вручную. Всегда требуется curl.  - только для режима , для  - не нужен.</p><p>Для совсем обрезанных дистрибутивов (alpine) требуется отдельно установить  и , либо .</p><p>В комплекте идут статические бинарники для большинства архитектур. Какой-то из них подойдет с вероятностью 99%. Но если у вас экзотическая система, инсталлятор попробует собрать бинарники сам через make. Для этого нужны gcc, make и необходимые  пакеты. Можно форсировать режим компиляции следующим вызовом:</p><p>Под OpenWrt все уже сразу готово для использования системы в качестве роутера. Имена интерфейсов WAN и LAN известны из настроек системы. Под другими системами роутер вы настраиваете самостоятельно. Инсталлятор в это не вмешивается. Инсталлятор в зависимости от выбранного режима может спросить LAN и WAN интерфейсы. Нужно понимать, что заворот проходящего трафика на  в прозрачном режиме происходит до выполнения маршрутизации, следовательно возможна фильтрация по LAN и невозможна по WAN. Решение о завороте на  локального исходящего трафика принимается после выполнения маршрутизации, следовательно ситуация обратная: LAN не имеет смысла, фильтрация по WAN возможна. Заворот на  происходит всегда после маршрутизации, поэтому к нему применима только фильтрация по WAN. Возможность прохождения трафика в том или ином направлении настраивается вами в процессе конфигурации роутера.</p><p>Деинсталляция выполняется через . После выполнения деинсталляции можно удалить каталог .</p><p>Если вам нравится systemd и хочется максимально под него заточиться, можно отказаться от скриптов запуска zapret и поднимать инстансы  и  как отдельные юниты systemd. При этом вам придется вручную написать правила iptables/nftables и каким-то образом их поднимать. Например, написать дополнительный systemd unit для этого. Так же требуется собрать бинарники особым образом через .</p><p>В комплекте zapret есть шаблоны <code>init.d/systemd/{nfqws@.service,tpws@.service}</code>. Краткий перечень команд для их использования приведен в комментариях в этих файлах.</p><h2>Простая установка на openwrt</h2><p>Работает только если у вас на роутере достаточно места.</p><p>Копируем zapret на роутер в .</p><p>Запускаем установщик:<code>sh /tmp/zapret/install_easy.sh</code></p><p>Он скопирует в  только необходимый минимум файлов.</p><p>После успешной установки можно удалить zapret из tmp для освобождения RAM:</p><p>Для более гибкой настройки перед запуском инсталлятора следует выполнить раздел \"Выбор параметров\".</p><p>Система простой инсталяции заточена на любое умышленное или неумышленное изменение прав доступа на файлы. Устойчива к репаку под windows. После копирования в  права будут принудительно восстановлены.</p><h2>Установка на openwrt в режиме острой нехватки места на диске</h2><p>Требуется около 120-200 кб на диске. Придется отказаться от всего, кроме .</p><p><strong>Инструкция для openwrt 22 и выше с nftables</strong></p><p>Никаких зависимостей устанавливать не нужно.</p><ol><li>Скопируйте все из <code>init.d/openwrt-minimal/tpws/*</code> в корень openwrt.</li><li>Скопируйте бинарник  подходящей архитектуры в .</li><li>Установите права на файлы: <code>chmod 755 /etc/init.d/tpws /usr/bin/tpws</code></li><li>Отредактируйте </li></ol><ul><li>Если не нужен ipv6, отредактируйте <code>/etc/nftables.d/90-tpws.nft</code> и закомментируйте строки с редиректом ipv6.</li></ul><ol start=\"5\"></ol><ol><li><code>rm -f /etc/nftables.d/90-tpws.nft /etc/firewall.user /etc/init.d/tpws /usr/bin/tpws</code></li></ol><p><strong>Инструкция для openwrt 21 и ниже с iptables</strong></p><ol><li><code>opkg install iptables-mod-extra</code></li></ol><ul><li>только для IPV6: <code>opkg install ip6tables-mod-nat</code></li></ul><p>Убедитесь, что в  нет ничего значимого. Если есть - не следуйте слепо инструкции. Объедините код или создайте свой  в .</p><ol><li>Скопируйте все из <code>init.d/openwrt-minimal/tpws/*</code> в корень openwrt.</li><li>Скопируйте бинарник  подходящей архитектуры в .</li><li>Установите права на файлы: <code>chmod 755 /etc/init.d/tpws /usr/bin/tpws</code></li><li>Отредактируйте </li></ol><ul><li>Если не нужен ipv6, отредактируйте /etc/firewall.user и установите там DISABLE_IPV6=1.</li></ul><ol start=\"5\"></ol><ol><li><code>rm -f /etc/nftables.d/90-tpws.nft /etc/firewall.user /etc/init.d/tpws</code></li></ol><p>Без рута забудьте про nfqws и tpws в режиме transparent proxy. tpws будет работать только в режиме .</p><p>Ядра Android имеют поддержку NFQUEUE. nfqws работает.</p><p>В стоковых ядрах нет поддержки ipset. В общем случае сложность задачи по поднятию ipset варьируется от \"не просто\" до \"почти невозможно\". Если только вы не найдете готовое собранное ядро под ваш девайс.</p><p>tpws будет работать в любом случае, он не требует чего-либо особенного.</p><p>Хотя linux варианты под Android работают, рекомендуется использовать специально собранные под bionic бинарники. У них не будет проблем с DNS, с локальным временем и именами юзеров и групп. Рекомендуется использовать gid 3003 (AID_INET). Иначе можете получить permission denied на создание сокета. Например:  В iptables укажите:  вместо . Напишите шелл скрипт с iptables и tpws, запускайте его средствами вашего рут менеджера. Скрипты автозапуска лежат тут:<p> magisk : /data/adb/service.d</p> supersu: /system/su.d</p><p> может иметь такой глюк. При запуске с uid по умолчанию (0x7FFFFFFF) при условии работы на сотовом интерфейсе и отключенном кабеле внешнего питания система может частично виснуть. Перестает работать тач и кнопки, но анимация на экране может продолжаться. Если экран был погашен, то включить его кнопкой power невозможно. Изменение UID на низкий (--uid 1 подойдет) позволяет решить эту проблему. Глюк был замечен на android 8.1 на девайсе, основанном на платформе mediatek.</p><p>Ответ на вопрос куда поместить tpws на android без рута, чтобы потом его запускать из приложений. Файл заливаем через adb shell в /data/local/tmp/, лучше всего в субфолдер.</p><pre><code>mkdir /data/local/tmp/zapret\nadb push tpws /data/local/tmp/zapret\nchmod 755 /data/local/tmp/zapret /data/local/tmp/zapret/tpws\nchcon u:object_r:system_file:s0 /data/local/tmp/zapret/tpws\n</code></pre><p>Как найти стратегию обхода сотового оператора: проще всего раздать инет на комп. Для этого подойдет любая поддерживаемая ОС. Подключите android через USB кабель к компу и включите режим модема. Прогоните стандартную процедуру blockcheck. При переносе правил на телефон уменьшить TTL на 1, если правила с TTL присутствуют в стратегии. Если проверялось на windows, убрать параметры .</p><p>Работа blockcheck в android shell не поддерживается, но имея рута можно развернуть rootfs какого-нибудь дистрибутива linux. Это лучше всего делать с компа через adb shell. Если компа нет, то развертка chroot - единственный вариант, хотя и неудобный. Подойдет что-то легковесное, например, alpine или даже OpenWrt. Если это не эмулятор android, то универсальная архитектура - arm (любой вариант). Если вы точно знаете, что ОС у вас 64-разрядная, то лучше вместо arm - arm64. Выяснить архитектуру можно командой .</p><pre><code>mount --bind /dev /data/linux/dev\nmount --bind /proc /data/linux/proc\nmount --bind /sys /data/linux/sys\nchroot /data/linux\n</code></pre><p>Первым делом вам нужно будет один раз настроить DNS. Сам он не заведется.</p><p><code>echo nameserver 1.1.1.1 &gt;/etc/resolv.conf</code></p><p>Далее нужно средствами пакетного менеджера установить iptables-legacy. Обязательно  iptables-nft, который, как правило, присутствует по умолчанию. В ядре android нет nftables. Линк должен указывать на legacy вариант. Если нет, значит устанавливайте нужные пакеты вашего дистрибутива, и убеждайтесь в правильности ссылок. Так можно проверить, что ваш  увидел то, что туда насовал android.  выдаст ошибку. Далее качаем zapret в . Обычные действия с , , .</p><p>Учтите, что стратегии обхода сотового оператора и домашнего wifi вероятно будут разные. Выделить сотового оператора легко через параметр iptables . Имя может быть, например, . Его легко увидеть через . Wifi сеть - обычно .</p><p>Переключать blockcheck между оператором и wifi можно вместе со всем инетом - включив или выключив wifi. Если найдете стратегию для wifi и впишите ее в автостарт, то при подключении к другому wifi она может не сработать или вовсе что-то поломать, потому подумайте стоит ли. Может быть лучше сделать скрипты типа \"запустить обход домашнего wifi\", \"снять обход домашнего wifi\", и пользоваться ими по необходимости из терминала. Но домашний wifi лучше все-же обходить на роутере.</p><h2>Мобильные модемы и роутеры huawei</h2><p>Устройства типа E3372, E8372, E5770 разделяют общую идеологию построения системы. Имеются 2 вычислительных ядра. Одно ядро выполняет vxworks, другое - linux. На 4pda имеются модифицированные прошивки с telnet и adb. Их и нужно использовать.</p><p>Дальнейшие утверждения проверены на E8372. На других может быть аналогично или похоже. Присутствуют дополнительные аппаратные блоки для offload-а сетевых функций. Не весь трафик идет через linux. Исходящий трафик с самого модема проходит цепочку OUTPUT нормально, на FORWARD =&gt;wan часть пакетов выпадает из tcpdump.</p><p>tpws работает обычным образом.</p><p> поломан, можно собрать фиксящий модуль <a href=\"https://github.com/im-0/unfuck-nfqueue-on-e3372h\">https://github.com/im-0/unfuck-nfqueue-on-e3372h</a>, используя исходники с huawei open source. Исходники содержат тулчейн и полусобирающееся, неактуальное ядро. Конфиг можно взять с рабочего модема из . С помощью этих исходников умельцы могут собрать модуль . После его применения NFQUEUE и nfqws для arm работают нормально.</p><p>Чтобы избежать проблемы с offload-ом при использовании nfqws, следует комбинировать tpws в режиме tcp proxy и nfqws. Правила NFQUEUE пишутся для цепочки OUTPUT. connbytes придется опускать, поскольку модуля в ядре нет. Но это не смертельно.</p><p>Скрипт автозапуска - . Создайте свой скрипт настройки zapret, запускайте из конца autorun.sh через \"&amp;\". Скрипт должен в начале делать sleep 5, чтобы дождаться поднятия сети и iptables от huawei.</p><blockquote><p>[!WARNING] На этом модеме происходят хаотические сбросы соединений tcp по непонятным причинам. Выглядит это так, если запускать curl с самого модема:</p></blockquote><pre><code>curl www.ru\ncurl: (7) Failed to connect to www.ru port 80: Host is unreachable\n</code></pre><p>Возникает ошибка сокета EHOSTUNREACH (errno -113). То же самое видно в tpws. В броузере не подгружаются части веб страниц, картинки, стили. В tcpdump на внешнем интерфейсе eth_x виден только единственный и безответный SYN пакет, без сообщений ICMP. ОС каким-то образом узнает о невозможности установить TCP соединение и выдает ошибку. Если выполнять подключение с клиента, то SYN пропадают, соединение не устанавливается. ОС клиента проводит ретрансмиссию, и с какого-то раза подключение удается. Поэтому без tcp проксирования в этой ситуации сайты тупят, но загружаются, а с проксированием подключение выполняется, но вскоре сбрасывается без каких-либо данных, и броузеры не пытаются установить его заново. Поэтому качество броузинга с tpws может быть хуже, но дело не в tpws. Частота сбросов заметно возрастает, если запущен торент клиент, имеется много tcp соединений. Однако, причина не в переполнении таблицы conntrack. Увеличение лимитов и очистка conntrack не помогают. Предположительно эта особенность связана с обработкой пакетов сброса соединения в hardware offload. Точного ответа на вопрос у меня нет. Если вы знаете - поделитесь, пожалуйста. Чтобы не ухудшать качество броузинга, можно фильтровать заворот на tpws по ip фильтру. Поддержка ipset отсутствует. Значит, все, что можно сделать - создать индивидуальные правила на небольшое количество хостов.</p><p>Некоторые наброски скриптов присутствуют в <a href=\"https://raw.githubusercontent.com/bol-van/zapret/files/huawei/\">files/huawei</a>.  Смотрите, изучайте, приспосабливайте. Здесь можно скачать готовые полезные статические бинарники для arm, включая curl : <a href=\"https://github.com/bol-van/bins\">https://github.com/bol-van/bins</a></p><p>Для статических бинарников не имеет значения на чем они запущены: PC, android, приставка, роутер, любой другой девайс. Подойдет любая прошивка, дистрибутив linux. Статические бинарники запустятся на всем. Им нужно только ядро с необходимыми опциями сборки или модулями. Но кроме бинарников в проекте используются еще и скрипты, в которых задействуются некоторые стандартные программы.</p><p>Основные причины почему нельзя просто так взять и установить эту систему на что угодно:</p><ul><li>отсутствие доступа к девайсу через shell</li><li>отсутствие раздела r/w для записи и энергонезависимого хранения файлов</li><li>отсутствие возможности поставить что-то в автозапуск</li><li>неотключаемый flow offload или другая проприетарщина в netfilter</li><li>недостаток модулей ядра или опций его сборки</li><li>недостаток модулей iptables (/usr/lib/iptables/lib*.so)</li><li>недостаток стандартных программ (типа ipset, curl) или их кастрированность (облегченная замена)</li><li>кастрированный или нестандартный шелл sh</li></ul><p>Если в вашей прошивке есть все необходимое, то вы можете адаптировать zapret под ваш девайс в той или иной степени. Может быть у вас не получится поднять все части системы, однако вы можете хотя бы попытаться поднять tpws и завернуть на него через -j REDIRECT весь трафик на порт 80. Если вам есть куда записать tpws, есть возможность выполнять команды при старте, то как минимум это вы сделать сможете. Скорее всего поддержка REDIRECT в ядре есть. Она точно есть на любом роутере, на других устройствах под вопросом. NFQUEUE, ipset на большинстве прошивок отсутствуют из-за ненужности.</p><p>Пересобрать ядро или модули для него будет скорее всего достаточно трудно. Для этого вам необходимо будет по крайней мере получить исходники вашей прошивки. User mode компоненты могут быть привнесены относительно безболезненно, если есть место куда их записать. Специально для девайсов, имеющих область r/w, существует проект entware. Некоторые прошивки даже имеют возможность его облегченной установки через веб интерфейс. entware содержит репозиторий user-mode компонент, которые устанавливаются в /opt. С их помощью можно компенсировать недостаток ПО основной прошивки, за исключением ядра.</p><p>Можно попытаться использовать sysv init script таким образом, как это описано в разделе \"Прикручивание к системе управления фаерволом или своей системе запуска\". В случае ругани на отсутствие каких-то базовых программ, их следует восполнить посредством entware. Перед запуском скрипта путь к дополнительным программам должен быть помещен в PATH.</p><p><em>Подробное описание настроек для других прошивок выходит за рамки данного проекта.</em></p><p>OpenWrt является одной из немногих относительно полноценных linux систем для embedded devices. Она характеризуется следующими вещами, которые и послужили основой выбора именно этой прошивки:</p><ul><li>полный root доступ к девайсу через shell. на заводских прошивках чаще всего отсутствует, на многих альтернативных есть</li><li>корень r/w. это практически уникальная особенность OpenWrt. заводские и большинство альтернативных прошивок построены на базе squashfs root (r/o), а конфигурация хранится в специально отформатированной области встроенной памяти, называемой nvram. не имеющие r/w корня системы сильно кастрированы. они не имеют возможности доустановки ПО из репозитория без специальных вывертов и заточены в основном на чуть более продвинутого, чем обычно, пользователя и управление имеющимся функционалом через веб интерфейс, но функционал фиксированно ограничен. альтернативные прошивки, как правило, могут монтировать r/w раздел в какую-то область файловой системы, заводские обычно могут монтировать лишь флэшки, подключенные к USB, и не факт, что есть поддержка unix файловых системы. может быть поддержка только fat и ntfs.</li><li>возможность выноса корневой файловой системы на внешний носитель (extroot) или создания на нем оверлея (overlay)</li><li>наличие менеджера пакетов opkg и репозитория софта</li><li>flow offload предсказуемо, стандартно и выборочно управляем, а так же отключаем</li><li>в репозитории есть все модули ядра, их можно доустановить через opkg. ядро пересобирать не нужно.</li><li>в репозитории есть все модули iptables, их можно доустановить через opkg</li><li>в репозитории есть огромное количество стандартных программ и дополнительного софта</li><li>наличие SDK, позволяющего собрать недостающее</li></ul><h2>Обход блокировки через сторонний хост</h2><p>Если не работает автономный обход, приходится перенаправлять трафик через сторонний хост. Предлагается использовать прозрачный редирект через socks5 посредством , либо . Настройка варианта с redsocks на OpenWrt описана в <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/redsocks.txt\">redsocks.txt</a>. Настройка варианта с  - в <a href=\"https://raw.githubusercontent.com/bol-van/zapret/master/wireguard_iproute_openwrt.txt\">wireguard_iproute_openwrt.txt</a>.</p><h2>Почему стоит вложиться в покупку VPS</h2><p>VPS — это виртуальный сервер. Существует огромное множество датацентров, предлагающих данную услугу. На VPS могут выполняться какие угодно задачи. От простого веб-сайта до навороченной системы собственной разработки. Можно использовать VPS и для поднятия собственного VPN или прокси. Сама широта возможных способов применения и распространенность услуги сводят к минимуму возможности регуляторов по бану сервисов такого типа. Да, если введут белые списки, то решение загнется, но это будет уже другая реальность, в которой придется изобретать иные решения. Пока этого не сделали, никто не будет банить хостинги просто потому, что они предоставляют хостинг услуги. Вы, как индивидуум, скорее всего, никому не нужны. Подумайте чем вы отличаетесь от известного VPN провайдера. VPN-провайдер предоставляет  и  услугу по обходу блокировок для масс. Этот факт делает его первоочередной целью блокировки. РКН направит уведомление, после отказа сотрудничать заблокирует VPN. Предоплаченная сумма пропадет. У регуляторов нет и никогда не будет ресурсов для тотальной проверки каждого сервера в сети. Возможен китайский расклад, при котором DPI выявляет VPN-протоколы и динамически банит IP серверов, предоставляющих нелицензированный VPN. Но имея знания, голову, вы всегда можете обфусцировать VPN трафик или применить другие типы VPN, более устойчивые к анализу на DPI, или просто менее широкоизвестные, а следовательно с меньшей вероятностью обнаруживаемые регулятором. У вас есть свобода делать на вашем VPS все что вы захотите, адаптируясь к новым условиям. Да, это потребует знаний. Вам выбирать учиться и держать ситуацию под контролем, когда вам ничего запретить не могут, или покориться системе.</p><p>VPS можно приобрести в множестве мест. Существуют специализированные на поиске предложений VPS порталы. Например, <a href=\"https://vps.today\">вот этот</a>. Для персонального VPN сервера обычно достаточно самой минимальной конфигурации, но с безлимитным трафиком или с большим лимитом по трафику (терабайты). Важен и тип VPS. OpenVZ подойдёт для OpenVPN, но вы не поднимете на нем WireGuard, IPsec, то есть все, что требует kernel mode. Для kernel mode требуется тип виртуализации, предполагающий запуск полноценного экземпляра ОС linux вместе с ядром. Подойдут KVM, Xen, Hyper-V, VMware.</p><p>По цене можно найти предложения, которые будут дешевле готовой VPN услуги, но при этом вы сам хозяин в своей лавке и не рискуете попасть под бан регулятора, разве что «заодно» — под ковровую бомбардировку с баном миллионов IP. Кроме того, если вам совсем все кажется сложным, прочитанное вызывает ступор и вы точно знаете, что ничего из описанного сделать не сможете, то вы сможете хотя бы использовать динамическое перенаправление портов SSH для получения шифрованного SOCKS-прокси и прописать его в браузер. Знания linux не нужны совсем. Это вариант наименее напряжный для чайников, хотя и не самый удобный в использовании.</p><p>USDT <code>0x3d52Ce15B7Be734c53fc9526ECbAB8267b63d66E</code></p><p>BTC <code>bc1qhqew3mrvp47uk2vevt5sctp7p2x9m7m5kkchve</code></p><p>ETH <code>0x3d52Ce15B7Be734c53fc9526ECbAB8267b63d66E</code></p>","contentLength":267877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"block/goose","url":"https://github.com/block/goose","date":1761618826,"author":"","guid":317093,"unread":true,"content":"<p>an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM</p><p> is a participating project in Hacktoberfest 2025! We’re so excited for your contributions, and have created a wide variety of issues so that anyone can contribute. Whether you're a seasoned developer or a first-time open source contributor, there's something for everyone.</p><p>Have questions? Connecting with us in our <a href=\"https://discord.gg/goose-oss\">Discord community</a> in the  project channel.</p><p>goose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - .</p><p>Whether you're prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.</p><p>Designed for maximum flexibility, goose works with any LLM and supports multi-model configuration to optimize performance and cost, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation.</p><blockquote><p>Why did the developer choose goose as their AI agent?</p><p>Because it always helps them \"migrate\" their code to production! 🚀</p></blockquote>","contentLength":1376,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"codecrafters-io/build-your-own-x","url":"https://github.com/codecrafters-io/build-your-own-x","date":1761618826,"author":"","guid":317094,"unread":true,"content":"<p><em>What I cannot create, I do not understand — Richard Feynman.</em></p>","contentLength":62,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"qeeqbox/social-analyzer","url":"https://github.com/qeeqbox/social-analyzer","date":1761618826,"author":"","guid":317095,"unread":true,"content":"<p>API, CLI, and Web App for analyzing and finding a person's profile in 1000 social media \\ websites</p><p>Social Analyzer - API, CLI, and Web App for analyzing &amp; finding a person's profile across +1000 social media \\ websites. It includes different analysis and detection modules, and you can choose which modules to use during the investigation process.</p><p>The detection modules utilize a rating mechanism based on different detection techniques, which produces a rate value that starts from 0 to 100 (No-Maybe-Yes). This module is intended to have fewer false positives.</p><p>The analysis and public extracted information from this OSINT tool could help investigate profiles related to suspicious or malicious activities such as cyberbullying, cyber grooming, cyberstalking, and spreading misinformation.</p><p><code>This project is currently used by some law enforcement agencies in countries where resources are limited - The detection database is different than the one shared here..</code></p><p>Websites and applications that enable users to create and share content or to participate in social networking - Oxford Dictionary</p><img src=\"https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/structure.png\"><img src=\"https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/intro_fast.gif\"><img src=\"https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/cli.gif\"><ul><li>String &amp; name analysis (Permutations and Combinations)</li><li>Find a profile using multiple techniques (HTTPS library &amp; Webdriver)</li><li>Multi profile search (Used for correlation - any combination separated with \",\" )</li><li>Multilayers detections (OCR, normal, advanced &amp; special)</li><li>Visualized profile information using Ixora (Metadata &amp; Patterns)</li><li>Metadata &amp; Patterns extraction (Added from Qeeqbox OSINT project)</li><li>Force-directed Graph for Metadata (Needs ExtractPatterns)</li><li>Search by top ranking or by country (Alexa Ranking)</li><li>Search by type (adult, music, etc.. - automated websites stats)</li><li>Profiles stats and static info (Category country)</li><li>Cross Metadata stats (Added from Qeeqbox OSINT project)</li><li>Auto-flirtation to unnecessary output (Enable javascript etc..)</li><li>Search engine lookup (Google API - optional)</li><li>Custom search queries (Google API &amp; DuckDuckGo API - optional)</li><li>Profile screenshot, title, info, and website description</li><li>Find name origins, name similarity &amp; common words by language</li><li>Find possible profile\\person age (Limited analysis)</li><li>Custom user-agent, proxy, timeout &amp; implicit wait</li><li>Python CLI &amp; NodeJS CLI (limited to FindUserProfilesFast option)</li><li>Screenshots of detected profile (The latest version of Chrome must be installed)</li><li>Grid option for faster checking (limited to docker-compose)</li><li>Dump logs to folder or terminal (prettified)</li><li>Adjust finding\\getting profile workers (default 15)</li><li>Re-checking option for failed profiles</li><li>Filter profiles by good, maybe, and bad</li><li>Save the analysis as a JSON file</li><li>Simplified web interface and CLI</li></ul><pre><code>sudo apt-get update\n#Depedning on your Linux distro, you may or may not need these 2 lines\nsudo DEBIAN_FRONTEND=noninteractive apt-get install -y software-properties-common\nsudo add-apt-repository ppa:mozillateam/ppa -y\nsudo apt-get install -y firefox-esr tesseract-ocr git nodejs npm\ngit clone https://github.com/qeeqbox/social-analyzer.git\ncd social-analyzer\nnpm update\nnpm install\nnpm start\n</code></pre><pre><code>sudo apt-get update\n#Depedning on your Linux distro, you may or may not need these 2 lines\nsudo DEBIAN_FRONTEND=noninteractive apt-get install -y software-properties-common\nsudo add-apt-repository ppa:mozillateam/ppa -y\nsudo apt-get install -y firefox-esr tesseract-ocr git nodejs npm\ngit clone https://github.com/qeeqbox/social-analyzer.git\ncd social-analyzer\nnpm install\nnodejs app.js --username \"johndoe\"\n#or\nnodejs app.js --username \"johndoe,janedoe\" --metadata\n#or\nnodejs app.js --username \"johndoe,janedoe\" --metadata --top 100\n#or\nnodejs app.js --username \"johndoe\" --type \"adult\"\n</code></pre><h3>Linux (As python package)</h3><pre><code>sudo apt-get update\nsudo apt-get install python3 python3-pip\npip3 install social-analyzer\npython3 -m social-analyzer --username \"johndoe\"\n#or\npython3 -m social-analyzer --username \"johndoe\" --metadata\n#or\npython3 -m social-analyzer --username \"johndoe\" --metadata --top 100\n#or\npython3 -m social-analyzer --username \"johndoe\" --type \"adult\"\n#or\npython3 -m social-analyzer --username \"johndoe\" --websites \"car\" --logs --screenshots\n</code></pre><pre><code>sudo apt-get update\nsudo apt-get install git python3 python3-pip\ngit clone https://github.com/qeeqbox/social-analyzer\ncd social-analyzer\npip3 install -r requirements.txt\npython3 app.py --username \"janedoe\"\n#or\npython3 app.py --username \"johndoe\" --metadata\n#or\npython3 app.py --username \"johndoe\" --metadata --top 100\n#or\npython3 app.py --username \"johndoe\" --type \"adult\"\n#or\npython3 app.py --username \"johndoe\" --websites \"car\" --logs --screenshots\n</code></pre><h3>Importing as object (python)</h3><pre><code>\n#E.g. #1\nfrom importlib import import_module\nSocialAnalyzer = import_module(\"social-analyzer\").SocialAnalyzer()\nresults = SocialAnalyzer.run_as_object(username=\"johndoe\",silent=True)\nprint(results)\n\n#E.g. #2\nfrom importlib import import_module\nSocialAnalyzer = import_module(\"social-analyzer\").SocialAnalyzer()\nresults = SocialAnalyzer.run_as_object(username=\"johndoe,janedoe\",silent=True,output=\"json\",filter=\"good\",metadata=False,timeout=10, profiles=\"detected\")\nprint(results)\n</code></pre><h3>Linux, Windows, MacOS, Raspberry pi..</h3><ul><li>check this <a href=\"https://github.com/qeeqbox/social-analyzer/wiki/install\">wiki</a> for all possible installation methods</li><li>check this <a href=\"https://github.com/qeeqbox/social-analyzer/wiki/integration\">wiki</a> for integrating social-analyzer with your OSINT tools, feeds, etc...</li></ul><pre><code>Required Arguments:\n  --username   E.g. johndoe, john_doe or johndoe9999\n\nOptional Arguments:\n  --websites    A website or websites separated by space E.g. youtube, tiktokor tumblr\n  --mode        Analysis mode E.g.fast -&gt; FindUserProfilesFast, slow -&gt; FindUserProfilesSlow or special -&gt; FindUserProfilesSpecial\n  --output      Show the output in the following format: json -&gt; json outputfor integration or pretty -&gt; prettify the output\n  --options     Show the following when a profile is found: link, rate, titleor text\n  --method      find -&gt; show detected profiles, get -&gt; show all profiles regardless detected or not, all -&gt; combine find &amp; get\n  --filter      Filter detected profiles by good, maybe or bad, you can do combine them with comma (good,bad) or use all\n  --profiles    Filter profiles by detected, unknown or failed, you can do combine them with comma (detected,failed) or use all\n  --countries   select websites by country or countries separated by space as: us br ru\n  --type        Select websites by type (Adult, Music etc)\n  --top         select top websites as 10, 50 etc...[--websites is not needed]\n  --extract     Extract profiles, urls &amp; patterns if possible\n  --metadata    Extract metadata if possible (pypi QeeqBox OSINT)\n  --trim        Trim long strings\n  --gui         Reserved for a gui (Not implemented)\n  --cli         Reserved for a cli (Not needed)\n\nListing websites &amp; detections:\n  --list        List all available websites\n\nSetting:\n  --headers     Headers as dict\n  --logs_dir    Change logs directory\n  --timeout     Change timeout between each request\n  --silent      Disable output to screen\n</code></pre><ul><li>DuckDuckGo API, Google API, NodeJS, bootstrap, selectize, jQuery, Wikipedia, font-awesome, selenium-webdriver &amp; tesseract.js</li><li>Let me know if I missed a reference or resource!</li></ul>","contentLength":6955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"toeverything/AFFiNE","url":"https://github.com/toeverything/AFFiNE","date":1761618826,"author":"","guid":317096,"unread":true,"content":"<p>There can be more than Notion and Miro. AFFiNE(pronounced [ə‘fain]) is a next-gen knowledge base that brings planning, sorting and creating all together. Privacy first, open-source, customizable and ready to use.</p><div align=\"center\"><em>Docs, canvas and tables are hyper-merged with AFFiNE - just like the word affine (əˈfʌɪn | a-fine).</em></div><h2>Getting started &amp; staying tuned with us.</h2><p>Star us, and you will receive all release notifications from GitHub without any delay!</p><img src=\"https://user-images.githubusercontent.com/79301703/230891830-0110681e-8c7e-483b-b6d9-9e42b291b9ef.gif\"><p><a href=\"https://affine.pro\">AFFiNE</a> is an open-source, all-in-one workspace and an operating system for all the building blocks that assemble your knowledge base and much more -- wiki, knowledge management, presentation and digital assets. It's a better alternative to Notion and Miro.</p><p><strong>A true canvas for blocks in any form. Docs and whiteboard are now fully merged.</strong></p><ul><li>Many editor apps claim to be a canvas for productivity, but AFFiNE is one of the very few which allows you to put any building block on an edgeless canvas -- rich text, sticky notes, any embedded web pages, multi-view databases, linked pages, shapes and even slides. We have it all.</li></ul><p><strong>Multimodal AI partner ready to kick in any work</strong></p><ul><li>Write up professional work report? Turn an outline into expressive and presentable slides? Summary an article into a well-structured mindmap? Sorting your job plan and backlog for tasks? Or... draw and code prototype apps and web pages directly all with one prompt? With you, <a href=\"https://affine.pro/ai\">AFFiNE AI</a> pushes your creativity to the edge of your imagination, just like <a href=\"https://affine.pro/blog/best-canvas-ai\">Canvas AI</a> to generate mind map for brainstorming.</li></ul><p><strong>Local-first &amp; Real-time collaborative</strong></p><ul><li>We love the idea of local-first that you always own your data on your disk, in spite of the cloud. Furthermore, AFFiNE supports real-time sync and collaborations on web and cross-platform clients.</li></ul><p><strong>Self-host &amp; Shape your own AFFiNE</strong></p><ul><li>You have the freedom to manage, self-host, fork and build your own AFFiNE. Plugin community and third-party blocks are coming soon. More tractions on <a href=\"https://blocksuite.io\">Blocksuite</a>. Check there to learn how to <a href=\"https://docs.affine.pro/self-host-affine\">self-host AFFiNE</a>.</li></ul><p>“We shape our tools and thereafter our tools shape us”. A lot of pioneers have inspired us along the way, e.g.:</p><ul><li>Quip &amp; Notion with their great concept of “everything is a block”</li><li>Airtable &amp; Miro with their no-code programmable datasheets</li><li>Miro &amp; Whimiscal with their edgeless visual whiteboard</li><li>Remote &amp; Capacities with their object-based tag system</li></ul><p>There is a large overlap of their atomic “building blocks” between these apps. They are not open source, nor do they have a plugin system like Vscode for contributors to customize. We want to have something that contains all the features we love and also goes one step even further.</p><p>Thanks for checking us out, we appreciate your interest and sincerely hope that AFFiNE resonates with you! 🎵 Checking <a href=\"https://affine.pro/\">https://affine.pro/</a> for more details ions.</p><p>Calling all developers, testers, tech writers and more! Contributions of all types are more than welcome, you can read more in <a href=\"https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/types-of-contributions.md\">docs/types-of-contributions.md</a>. If you are interested in contributing code, read our <a href=\"https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/CONTRIBUTING.md\">docs/CONTRIBUTING.md</a> and feel free to check out our GitHub issues to get stuck in to show us what you’re made of.</p><p><strong>Before you start contributing, please make sure you have read and accepted our <a href=\"https://github.com/toeverything/affine/edit/canary/.github/CLA.md\">Contributor License Agreement</a>. To indicate your agreement, simply edit this file and submit a pull request.</strong></p><p>For ,  and other  you can also <a href=\"https://github.com/toeverything/AFFiNE/issues/new/choose\">create a new issue</a> and choose the most appropriate template for your feedback.</p><p>Looking for  and wondering where to start? Check out the <a href=\"https://community.affine.pro/c/start-here/affine-ambassador\">AFFiNE Ambassador program</a>, we work closely with passionate community members and provide them with a wide range of support and resources.</p><p>If you have questions, you are welcome to contact us. One of the best places to get more info and learn more is in the <a href=\"https://community.affine.pro\">AFFiNE Community</a> where you can engage with other like-minded individuals.</p><p>AFFiNE now provides pre-built <a href=\"https://affine.pro/templates\">templates</a> from our team. Following are the Top 10 most popular templates among AFFiNE users,if you want to contribute, you can contribute your own template so other people can use it too.</p><p>Welcome to the AFFiNE blog section! Here, you’ll find the latest insights, tips, and guides on how to maximize your experience with AFFiNE and AFFiNE AI, the leading Canvas AI tool for flexible note-taking and creative organization.</p><p>We would also like to give thanks to open-source projects that make AFFiNE possible:</p><ul><li><a href=\"https://github.com/toeverything/BlockSuite\">Blocksuite</a> - 💠 BlockSuite is the open-source collaborative editor project behind AFFiNE.</li><li><a href=\"https://github.com/toeverything/OctoBase\">OctoBase</a> - 🐙 OctoBase is the open-source database behind AFFiNE, local-first, yet collaborative. A light-weight, scalable, data engine written in Rust.</li><li><a href=\"https://github.com/yjs/yjs\">yjs</a> - Fundamental support of CRDTs for our implementation on state management and data sync.</li><li><a href=\"https://github.com/electron/electron\">electron</a> - Build cross-platform desktop apps with JavaScript, HTML, and CSS.</li><li><a href=\"https://github.com/facebook/react\">React</a> - The library for web and native user interfaces.</li><li><a href=\"https://github.com/napi-rs/napi-rs\">napi-rs</a> - A framework for building compiled Node.js add-ons in Rust via Node-API.</li><li><a href=\"https://github.com/pmndrs/jotai\">Jotai</a> - Primitive and flexible state management for React.</li><li><a href=\"https://github.com/vitejs/vite\">Vite</a> - Next generation frontend tooling.</li></ul><p>Thanks a lot to the community for providing such powerful and simple libraries, so that we can focus more on the implementation of the product logic, and we hope that in the future our projects will also provide a more easy-to-use knowledge base for everyone.</p><p>We would like to express our gratitude to all the individuals who have already contributed to AFFiNE! If you have any AFFiNE-related project, documentation, tool or template, please feel free to contribute it by submitting a pull request to our curated list on GitHub: <a href=\"https://github.com/toeverything/awesome-affine\">awesome-affine</a>.</p><a href=\"https://github.com/toeverything/affine/graphs/contributors\"><img alt=\"contributors\" src=\"https://opencollective.com/affine/contributors.svg?width=890&amp;button=false\"></a><p>Begin with Docker to deploy your own feature-rich, unrestricted version of AFFiNE. Our team is diligently updating to the latest version. For more information on how to self-host AFFiNE, please refer to our <a href=\"https://docs.affine.pro/self-host-affine\">documentation</a>.</p><p>Some amazing companies, including AFFiNE, are looking for developers! Are you interested in joining AFFiNE or its partners? Check out our <a href=\"https://affine.pro/redirect/discord\">Discord channel</a> for some of the latest jobs available.</p><p>From the GitHub repo main page, click the green \"Code\" button and select \"Create codespace on master\". This will open a new Codespace with the (supposedly auto-forked AFFiNE repo cloned, built, and ready to go.</p><p>See <a href=\"https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/BUILDING.md\">BUILDING.md</a> for instructions on how to build AFFiNE from source code.</p><p>Thanks to <a href=\"https://www.chromatic.com/\">Chromatic</a> for providing the visual testing platform that helps us review UI changes and catch visual regressions.</p><ul><li><p>AFFiNE Community Edition (CE) is the current available version, it's free for self-host under the MIT license.</p></li><li><p>AFFiNE Enterprise Edition (EE) is yet to be published, it will have more advanced features and enterprise-oriented offerings, including but not exclusive to rebranding and SSO, advanced admin and audit, etc., you may refer to <a href=\"https://affine.pro/pricing\">https://affine.pro/pricing</a> for more information</p></li></ul>","contentLength":6716,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"donnemartin/system-design-primer","url":"https://github.com/donnemartin/system-design-primer","date":1761532686,"author":"","guid":315596,"unread":true,"content":"<p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</p><blockquote><p>Learn how to design large-scale systems.</p><p>Prep for the system design interview.</p></blockquote><h3>Learn how to design large-scale systems</h3><p>Learning how to design scalable systems will help you become a better engineer.</p><p>System design is a broad topic. There is a <strong>vast amount of resources scattered throughout the web</strong> on system design principles.</p><p>This repo is an  of resources to help you learn how to build systems at scale.</p><h3>Learn from the open source community</h3><p>This is a continually updated, open source project.</p><h3>Prep for the system design interview</h3><p>In addition to coding interviews, system design is a  of the <strong>technical interview process</strong> at many tech companies.</p><p><strong>Practice common system design interview questions</strong> and  your results with : discussions, code, and diagrams.</p><p>Additional topics for interview prep:</p><p>The provided <a href=\"https://apps.ankiweb.net/\">Anki flashcard decks</a> use spaced repetition to help you retain key system design concepts.</p><p>Great for use while on-the-go.</p><h3>Coding Resource: Interactive Coding Challenges</h3><blockquote><p>Learn from the community.</p></blockquote><p>Feel free to submit pull requests to help:</p><ul></ul><h2>Index of system design topics</h2><blockquote><p>Summaries of various system design topics, including pros and cons. <strong>Everything is a trade-off</strong>.</p><p>Each section contains links to more in-depth resources.</p></blockquote><blockquote><p>Suggested topics to review based on your interview timeline (short, medium, long).</p></blockquote><p><strong>Q: For interviews, do I need to know everything here?</strong></p><p><strong>A: No, you don't need to know everything here to prepare for the interview</strong>.</p><p>What you are asked in an interview depends on variables such as:</p><ul><li>How much experience you have</li><li>What your technical background is</li><li>What positions you are interviewing for</li><li>Which companies you are interviewing with</li></ul><p>More experienced candidates are generally expected to know more about system design. Architects or team leads might be expected to know more than individual contributors. Top tech companies are likely to have one or more design interview rounds.</p><p>Start broad and go deeper in a few areas. It helps to know a little about various key system design topics. Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.</p><ul><li> - Aim for  with system design topics. Practice by solving  interview questions.</li><li> - Aim for  and  with system design topics. Practice by solving  interview questions.</li><li> - Aim for  and  with system design topics. Practice by solving  interview questions.</li></ul><h2>How to approach a system design interview question</h2><blockquote><p>How to tackle a system design interview question.</p></blockquote><p>The system design interview is an . You are expected to lead it.</p><h3>Step 1: Outline use cases, constraints, and assumptions</h3><p>Gather requirements and scope the problem. Ask questions to clarify use cases and constraints. Discuss assumptions.</p><ul><li>How are they going to use it?</li><li>How many users are there?</li><li>What are the inputs and outputs of the system?</li><li>How much data do we expect to handle?</li><li>How many requests per second do we expect?</li><li>What is the expected read to write ratio?</li></ul><h3>Step 2: Create a high level design</h3><p>Outline a high level design with all important components.</p><ul><li>Sketch the main components and connections</li></ul><h3>Step 3: Design core components</h3><ul><li>Generating and storing a hash of the full url \n  <ul></ul></li><li>Translating a hashed url to the full url \n  </li><li>API and object-oriented design</li></ul><p>Identify and address bottlenecks, given the constraints. For example, do you need the following to address scalability issues?</p><ul></ul><h3>Back-of-the-envelope calculations</h3><p>You might be asked to do some estimates by hand. Refer to the <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#appendix\">Appendix</a> for the following resources:</p><h3>Source(s) and further reading</h3><p>Check out the following links to get a better idea of what to expect:</p><h2>System design interview questions with solutions</h2><blockquote><p>Common system design interview questions with sample discussions, code, and diagrams.</p><p>Solutions linked to content in the  folder.</p></blockquote><table><tbody><tr><td>Design the Twitter timeline and search (or Facebook feed and search)</td></tr><tr><td>Design the data structures for a social network</td></tr><tr><td>Design a key-value store for a search engine</td></tr><tr><td>Design Amazon's sales ranking by category feature</td></tr><tr><td>Design a system that scales to millions of users on AWS</td></tr></tbody></table><h3>Design Pastebin.com (or Bit.ly)</h3><h3>Design the Twitter timeline and search (or Facebook feed and search)</h3><h3>Design the data structures for a social network</h3><h3>Design a key-value store for a search engine</h3><h3>Design Amazon's sales ranking by category feature</h3><h3>Design a system that scales to millions of users on AWS</h3><h2>Object-oriented design interview questions with solutions</h2><blockquote><p>Common object-oriented design interview questions with sample discussions, code, and diagrams.</p><p>Solutions linked to content in the  folder.</p></blockquote><blockquote><p><strong>Note: This section is under development</strong></p></blockquote><h2>System design topics: start here</h2><p>First, you'll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.</p><h3>Step 1: Review the scalability video lecture</h3><ul><li>Topics covered: \n  <ul></ul></li></ul><h3>Step 2: Review the scalability article</h3><p>Next, we'll look at high-level trade-offs:</p><ul><li> vs </li></ul><p>Keep in mind that <strong>everything is a trade-off</strong>.</p><p>Then we'll dive into more specific topics such as DNS, CDNs, and load balancers.</p><h2>Performance vs scalability</h2><p>A service is  if it results in increased  in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.</p><p>Another way to look at performance vs scalability:</p><ul><li>If you have a  problem, your system is slow for a single user.</li><li>If you have a  problem, your system is fast for a single user but slow under heavy load.</li></ul><h3>Source(s) and further reading</h3><p> is the time to perform some action or to produce some result.</p><p> is the number of such actions or results per unit of time.</p><p>Generally, you should aim for  with .</p><h3>Source(s) and further reading</h3><h2>Availability vs consistency</h2><p>In a distributed computer system, you can only support two of the following guarantees:</p><ul><li> - Every read receives the most recent write or an error</li><li> - Every request receives a response, without guarantee that it contains the most recent version of the information</li><li> - The system continues to operate despite arbitrary partitioning due to network failures</li></ul><p><em>Networks aren't reliable, so you'll need to support partition tolerance. You'll need to make a software tradeoff between consistency and availability.</em></p><h4>CP - consistency and partition tolerance</h4><p>Waiting for a response from the partitioned node might result in a timeout error. CP is a good choice if your business needs require atomic reads and writes.</p><h4>AP - availability and partition tolerance</h4><p>Responses return the most readily available version of the data available on any node, which might not be the latest. Writes might take some time to propagate when the partition is resolved.</p><p>AP is a good choice if the business needs to allow for <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#eventual-consistency\">eventual consistency</a> or when the system needs to continue working despite external errors.</p><h3>Source(s) and further reading</h3><p>With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data. Recall the definition of consistency from the <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#cap-theorem\">CAP theorem</a> - Every read receives the most recent write or an error.</p><p>After a write, reads may or may not see it. A best effort approach is taken.</p><p>This approach is seen in systems such as memcached. Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games. For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.</p><p>After a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously.</p><p>This approach is seen in systems such as DNS and email. Eventual consistency works well in highly available systems.</p><p>After a write, reads will see it. Data is replicated synchronously.</p><p>This approach is seen in file systems and RDBMSes. Strong consistency works well in systems that need transactions.</p><h3>Source(s) and further reading</h3><p>There are two complementary patterns to support high availability:  and .</p><p>With active-passive fail-over, heartbeats are sent between the active and the passive server on standby. If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.</p><p>The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby. Only the active server handles traffic.</p><p>Active-passive failover can also be referred to as master-slave failover.</p><p>In active-active, both servers are managing traffic, spreading the load between them.</p><p>If the servers are public-facing, the DNS would need to know about the public IPs of both servers. If the servers are internal-facing, application logic would need to know about both servers.</p><p>Active-active failover can also be referred to as master-master failover.</p><h3>Disadvantage(s): failover</h3><ul><li>Fail-over adds more hardware and additional complexity.</li><li>There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive.</li></ul><h4>Master-slave and master-master</h4><p>This topic is further discussed in the <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#database\">Database</a> section:</p><p>Availability is often quantified by uptime (or downtime) as a percentage of time the service is available. Availability is generally measured in number of 9s--a service with 99.99% availability is described as having four 9s.</p><h4>99.9% availability - three 9s</h4><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><h4>99.99% availability - four 9s</h4><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><h4>Availability in parallel vs in sequence</h4><p>If a service consists of multiple components prone to failure, the service's overall availability depends on whether the components are in sequence or in parallel.</p><p>Overall availability decreases when two components with availability &lt; 100% are in sequence:</p><pre><code>Availability (Total) = Availability (Foo) * Availability (Bar)\n</code></pre><p>If both  and  each had 99.9% availability, their total availability in sequence would be 99.8%.</p><p>Overall availability increases when two components with availability &lt; 100% are in parallel:</p><pre><code>Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar))\n</code></pre><p>If both  and  each had 99.9% availability, their total availability in parallel would be 99.9999%.</p><p>A Domain Name System (DNS) translates a domain name such as <a href=\"http://www.example.com\">www.example.com</a> to an IP address.</p><p>DNS is hierarchical, with a few authoritative servers at the top level. Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. DNS results can also be cached by your browser or OS for a certain period of time, determined by the <a href=\"https://en.wikipedia.org/wiki/Time_to_live\">time to live (TTL)</a>.</p><ul><li> - Specifies the DNS servers for your domain/subdomain.</li><li><strong>MX record (mail exchange)</strong> - Specifies the mail servers for accepting messages.</li><li> - Points a name to an IP address.</li><li> - Points a name to another name or  (example.com to <a href=\"http://www.example.com\">www.example.com</a>) or to an  record.</li></ul><p>Services such as <a href=\"https://www.cloudflare.com/dns/\">CloudFlare</a> and <a href=\"https://aws.amazon.com/route53/\">Route 53</a> provide managed DNS services. Some DNS services can route traffic through various methods:</p><ul><li>Accessing a DNS server introduces a slight delay, although mitigated by caching described above.</li><li>DNS services have recently come under <a href=\"http://dyn.com/blog/dyn-analysis-summary-of-friday-october-21-attack/\">DDoS attack</a>, preventing users from accessing websites such as Twitter without knowing Twitter's IP address(es).</li></ul><h3>Source(s) and further reading</h3><p>A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content. The site's DNS resolution will tell clients which server to contact.</p><p>Serving content from CDNs can significantly improve performance in two ways:</p><ul><li>Users receive content from data centers close to them</li><li>Your servers do not have to serve requests that the CDN fulfills</li></ul><p>Push CDNs receive new content whenever changes occur on your server. You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN. You can configure when content expires and when it is updated. Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.</p><p>Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs. Content is placed on the CDNs once, instead of being re-pulled at regular intervals.</p><p>Pull CDNs grab new content from your server when the first user requests the content. You leave the content on your server and rewrite URLs to point to the CDN. This results in a slower request until the content is cached on the CDN.</p><p>A <a href=\"https://en.wikipedia.org/wiki/Time_to_live\">time-to-live (TTL)</a> determines how long content is cached. Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.</p><p>Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.</p><ul><li>CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN.</li><li>Content might be stale if it is updated before the TTL expires it.</li><li>CDNs require changing URLs for static content to point to the CDN.</li></ul><h3>Source(s) and further reading</h3><p>Load balancers distribute incoming client requests to computing resources such as application servers and databases. In each case, the load balancer returns the response from the computing resource to the appropriate client. Load balancers are effective at:</p><ul><li>Preventing requests from going to unhealthy servers</li><li>Preventing overloading resources</li><li>Helping to eliminate a single point of failure</li></ul><p>Load balancers can be implemented with hardware (expensive) or with software such as HAProxy.</p><p>Additional benefits include:</p><ul><li> - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations \n  </li><li> - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions</li></ul><p>Load balancers can route traffic based on various metrics, including:</p><p>Layer 4 load balancers look at info at the <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#communication\">transport layer</a> to decide how to distribute requests. Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet. Layer 4 load balancers forward network packets to and from the upstream server, performing <a href=\"https://www.nginx.com/resources/glossary/layer-4-load-balancing/\">Network Address Translation (NAT)</a>.</p><p>Layer 7 load balancers look at the <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#communication\">application layer</a> to decide how to distribute requests. This can involve contents of the header, message, and cookies. Layer 7 load balancers terminate network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server. For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers.</p><p>At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.</p><p>Load balancers can also help with horizontal scaling, improving performance and availability. Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called . It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.</p><h4>Disadvantage(s): horizontal scaling</h4><ul><li>Scaling horizontally introduces complexity and involves cloning servers \n  <ul><li>Servers should be stateless: they should not contain any user-related data like sessions or profile pictures</li><li>Sessions can be stored in a centralized data store such as a <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#database\">database</a> (SQL, NoSQL) or a persistent <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#cache\">cache</a> (Redis, Memcached)</li></ul></li><li>Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out</li></ul><h3>Disadvantage(s): load balancer</h3><ul><li>The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly.</li><li>Introducing a load balancer to help eliminate a single point of failure results in increased complexity.</li><li>A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity.</li></ul><h3>Source(s) and further reading</h3><h2>Reverse proxy (web server)</h2><p>A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public. Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.</p><p>Additional benefits include:</p><ul><li> - Hide information about backend servers, blacklist IPs, limit number of connections per client</li><li><strong>Increased scalability and flexibility</strong> - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration</li><li> - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations \n  </li><li> - Compress server responses</li><li> - Return the response for cached requests</li><li> - Serve static content directly \n  <ul></ul></li></ul><h3>Load balancer vs reverse proxy</h3><ul><li>Deploying a load balancer is useful when you have multiple servers. Often, load balancers route traffic to a set of servers serving the same function.</li><li>Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section.</li><li>Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing.</li></ul><h3>Disadvantage(s): reverse proxy</h3><ul><li>Introducing a reverse proxy results in increased complexity.</li><li>A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a <a href=\"https://en.wikipedia.org/wiki/Failover\">failover</a>) further increases complexity.</li></ul><h3>Source(s) and further reading</h3><p>Separating out the web layer from the application layer (also known as platform layer) allows you to scale and configure both layers independently. Adding a new API results in adding application servers without necessarily adding additional web servers. The <strong>single responsibility principle</strong> advocates for small and autonomous services that work together. Small teams with small services can plan more aggressively for rapid growth.</p><p>Workers in the application layer also help enable <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#asynchronism\">asynchronism</a>.</p><p>Related to this discussion are <a href=\"https://en.wikipedia.org/wiki/Microservices\">microservices</a>, which can be described as a suite of independently deployable, small, modular services. Each service runs a unique process and communicates through a well-defined, lightweight mechanism to serve a business goal. </p><p>Pinterest, for example, could have the following microservices: user profile, follower, feed, search, photo upload, etc.</p><p>Systems such as <a href=\"https://www.consul.io/docs/index.html\">Consul</a>, <a href=\"https://coreos.com/etcd/docs/latest\">Etcd</a>, and <a href=\"http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper\">Zookeeper</a> can help services find each other by keeping track of registered names, addresses, and ports. <a href=\"https://www.consul.io/intro/getting-started/checks.html\">Health checks</a> help verify service integrity and are often done using an <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#hypertext-transfer-protocol-http\">HTTP</a> endpoint. Both Consul and Etcd have a built in <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#key-value-store\">key-value store</a> that can be useful for storing config values and other shared data.</p><h3>Disadvantage(s): application layer</h3><ul><li>Adding an application layer with loosely coupled services requires a different approach from an architectural, operations, and process viewpoint (vs a monolithic system).</li><li>Microservices can add complexity in terms of deployments and operations.</li></ul><h3>Source(s) and further reading</h3><h3>Relational database management system (RDBMS)</h3><p>A relational database like SQL is a collection of data items organized in tables.</p><p> is a set of properties of relational database <a href=\"https://en.wikipedia.org/wiki/Database_transaction\">transactions</a>.</p><ul><li> - Each transaction is all or nothing</li><li> - Any transaction will bring the database from one valid state to another</li><li> - Executing transactions concurrently has the same results as if the transactions were executed serially</li><li> - Once a transaction has been committed, it will remain so</li></ul><p>There are many techniques to scale a relational database: , <strong>master-master replication</strong>, , , , and .</p><p>The master serves reads and writes, replicating writes to one or more slaves, which serve only reads. Slaves can also replicate to additional slaves in a tree-like fashion. If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.</p><h5>Disadvantage(s): master-slave replication</h5><ul><li>Additional logic is needed to promote a slave to a master.</li></ul><h4>Master-master replication</h4><p>Both masters serve reads and writes and coordinate with each other on writes. If either master goes down, the system can continue to operate with both reads and writes.</p><h5>Disadvantage(s): master-master replication</h5><ul><li>You'll need a load balancer or you'll need to make changes to your application logic to determine where to write.</li><li>Most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization.</li><li>Conflict resolution comes more into play as more write nodes are added and as latency increases.</li></ul><h5>Disadvantage(s): replication</h5><ul><li>There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes.</li><li>Writes are replayed to the read replicas. If there are a lot of writes, the read replicas can get bogged down with replaying writes and can't do as many reads.</li><li>The more read slaves, the more you have to replicate, which leads to greater replication lag.</li><li>On some systems, writing to the master can spawn multiple threads to write in parallel, whereas read replicas only support writing sequentially with a single thread.</li><li>Replication adds more hardware and additional complexity.</li></ul><h5>Source(s) and further reading: replication</h5><p>Federation (or functional partitioning) splits up databases by function. For example, instead of a single, monolithic database, you could have three databases: , , and , resulting in less read and write traffic to each database and therefore less replication lag. Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality. With no single central master serializing writes you can write in parallel, increasing throughput.</p><h5>Disadvantage(s): federation</h5><ul><li>Federation is not effective if your schema requires huge functions or tables.</li><li>You'll need to update your application logic to determine which database to read and write.</li><li>Joining data from two databases is more complex with a <a href=\"http://stackoverflow.com/questions/5145637/querying-data-by-joining-two-tables-in-two-database-on-different-servers\">server link</a>.</li><li>Federation adds more hardware and additional complexity.</li></ul><h5>Source(s) and further reading: federation</h5><p>Sharding distributes data across different databases such that each database can only manage a subset of the data. Taking a users database as an example, as the number of users increases, more shards are added to the cluster.</p><p>Similar to the advantages of <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#federation\">federation</a>, sharding results in less read and write traffic, less replication, and more cache hits. Index size is also reduced, which generally improves performance with faster queries. If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss. Like federation, there is no single central master serializing writes, allowing you to write in parallel with increased throughput.</p><p>Common ways to shard a table of users is either through the user's last name initial or the user's geographic location.</p><h5>Disadvantage(s): sharding</h5><ul><li>You'll need to update your application logic to work with shards, which could result in complex SQL queries.</li><li>Data distribution can become lopsided in a shard. For example, a set of power users on a shard could result in increased load to that shard compared to others. \n  <ul><li>Rebalancing adds additional complexity. A sharding function based on <a href=\"http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html\">consistent hashing</a> can reduce the amount of transferred data.</li></ul></li><li>Joining data from multiple shards is more complex.</li><li>Sharding adds more hardware and additional complexity.</li></ul><h5>Source(s) and further reading: sharding</h5><p>Denormalization attempts to improve read performance at the expense of some write performance. Redundant copies of the data are written in multiple tables to avoid expensive joins. Some RDBMS such as <a href=\"https://en.wikipedia.org/wiki/PostgreSQL\">PostgreSQL</a> and Oracle support <a href=\"https://en.wikipedia.org/wiki/Materialized_view\">materialized views</a> which handle the work of storing redundant information and keeping redundant copies consistent.</p><p>Once data becomes distributed with techniques such as <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#federation\">federation</a> and <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#sharding\">sharding</a>, managing joins across data centers further increases complexity. Denormalization might circumvent the need for such complex joins.</p><p>In most systems, reads can heavily outnumber writes 100:1 or even 1000:1. A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.</p><h5>Disadvantage(s): denormalization</h5><ul><li>Constraints can help redundant copies of information stay in sync, which increases complexity of the database design.</li><li>A denormalized database under heavy write load might perform worse than its normalized counterpart.</li></ul><h6>Source(s) and further reading: denormalization</h6><p>SQL tuning is a broad topic and many <a href=\"https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&amp;field-keywords=sql+tuning\">books</a> have been written as reference.</p><p>It's important to  and  to simulate and uncover bottlenecks.</p><ul><li> - Simulate high-load situations with tools such as <a href=\"http://httpd.apache.org/docs/2.2/programs/ab.html\">ab</a>.</li><li> - Enable tools such as the <a href=\"http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html\">slow query log</a> to help track performance issues.</li></ul><p>Benchmarking and profiling might point you to the following optimizations.</p><ul><li>MySQL dumps to disk in contiguous blocks for fast access.</li><li>Use  instead of  for fixed-length fields. \n  <ul><li> effectively allows for fast, random access, whereas with , you must find the end of a string before moving onto the next one.</li></ul></li><li>Use  for large blocks of text such as blog posts.  also allows for boolean searches. Using a  field results in storing a pointer on disk that is used to locate the text block.</li><li>Use  for larger numbers up to 2^32 or 4 billion.</li><li>Use  for currency to avoid floating point representation errors.</li><li>Avoid storing large , store the location of where to get the object instead.</li><li> is the largest number of characters that can be counted in an 8 bit number, often maximizing the use of a byte in some RDBMS.</li></ul><ul><li>Columns that you are querying (, , , ) could be faster with indices.</li><li>Indices are usually represented as self-balancing <a href=\"https://en.wikipedia.org/wiki/B-tree\">B-tree</a> that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time.</li><li>Placing an index can keep the data in memory, requiring more space.</li><li>Writes could also be slower since the index also needs to be updated.</li><li>When loading large amounts of data, it might be faster to disable indices, load the data, then rebuild the indices.</li></ul><ul><li>Break up a table by putting hot spots in a separate table to help keep it in memory.</li></ul><h5>Source(s) and further reading: SQL tuning</h5><p>NoSQL is a collection of data items represented in a , , , or a . Data is denormalized, and joins are generally done in the application code. Most NoSQL stores lack true ACID transactions and favor <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#eventual-consistency\">eventual consistency</a>.</p><p> is often used to describe the properties of NoSQL databases. In comparison with the <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#cap-theorem\">CAP Theorem</a>, BASE chooses availability over consistency.</p><ul><li> - the system guarantees availability.</li><li> - the state of the system may change over time, even without input.</li><li> - the system will become consistent over a period of time, given that the system doesn't receive input during that period.</li></ul><p>In addition to choosing between <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#sql-or-nosql\">SQL or NoSQL</a>, it is helpful to understand which type of NoSQL database best fits your use case(s). We'll review , , , and  in the next section.</p><p>A key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD. Data stores can maintain keys in <a href=\"https://en.wikipedia.org/wiki/Lexicographical_order\">lexicographic order</a>, allowing efficient retrieval of key ranges. Key-value stores can allow for storing of metadata with a value.</p><p>Key-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer. Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.</p><p>A key-value store is the basis for more complex systems such as a document store, and in some cases, a graph database.</p><h5>Source(s) and further reading: key-value store</h5><blockquote><p>Abstraction: key-value store with documents stored as values</p></blockquote><p>A document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object. Document stores provide APIs or a query language to query based on the internal structure of the document itself. <em>Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.</em></p><p>Based on the underlying implementation, documents are organized by collections, tags, metadata, or directories. Although documents can be organized or grouped together, documents may have fields that are completely different from each other.</p><p>Some document stores like <a href=\"https://www.mongodb.com/mongodb-architecture\">MongoDB</a> and <a href=\"https://blog.couchdb.org/2016/08/01/couchdb-2-0-architecture/\">CouchDB</a> also provide a SQL-like language to perform complex queries. <a href=\"http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf\">DynamoDB</a> supports both key-values and documents.</p><p>Document stores provide high flexibility and are often used for working with occasionally changing data.</p><h5>Source(s) and further reading: document store</h5><blockquote><p>Abstraction: nested map <code>ColumnFamily&lt;RowKey, Columns&lt;ColKey, Value, Timestamp&gt;&gt;</code></p></blockquote><p>A wide column store's basic unit of data is a column (name/value pair). A column can be grouped in column families (analogous to a SQL table). Super column families further group column families. You can access each column independently with a row key, and columns with the same row key form a row. Each value contains a timestamp for versioning and for conflict resolution.</p><p>Google introduced <a href=\"http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf\">Bigtable</a> as the first wide column store, which influenced the open-source <a href=\"https://www.edureka.co/blog/hbase-architecture/\">HBase</a> often-used in the Hadoop ecosystem, and <a href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archIntro.html\">Cassandra</a> from Facebook. Stores such as BigTable, HBase, and Cassandra maintain keys in lexicographic order, allowing efficient retrieval of selective key ranges.</p><p>Wide column stores offer high availability and high scalability. They are often used for very large data sets.</p><h5>Source(s) and further reading: wide column store</h5><p>In a graph database, each node is a record and each arc is a relationship between two nodes. Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.</p><p>Graphs databases offer high performance for data models with complex relationships, such as a social network. They are relatively new and are not yet widely-used; it might be more difficult to find development tools and resources. Many graphs can only be accessed with <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#representational-state-transfer-rest\">REST APIs</a>.</p><h5>Source(s) and further reading: graph</h5><h4>Source(s) and further reading: NoSQL</h4><ul><li>Clear patterns for scaling</li><li>More established: developers, community, code, tools, etc</li><li>Lookups by index are very fast</li></ul><ul><li>Dynamic or flexible schema</li><li>No need for complex joins</li><li>Store many TB (or PB) of data</li><li>Very data intensive workload</li><li>Very high throughput for IOPS</li></ul><p>Sample data well-suited for NoSQL:</p><ul><li>Rapid ingest of clickstream and log data</li><li>Leaderboard or scoring data</li><li>Temporary data, such as a shopping cart</li><li>Frequently accessed ('hot') tables</li></ul><h5>Source(s) and further reading: SQL or NoSQL</h5><p>Caching improves page load times and can reduce the load on your servers and databases. In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.</p><p>Databases often benefit from a uniform distribution of reads and writes across its partitions. Popular items can skew the distribution, causing bottlenecks. Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.</p><p>Caches can be located on the client side (OS or browser), <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#reverse-proxy-web-server\">server side</a>, or in a distinct cache layer.</p><p><a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#content-delivery-network\">CDNs</a> are considered a type of cache.</p><p><a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#reverse-proxy-web-server\">Reverse proxies</a> and caches such as <a href=\"https://www.varnish-cache.org/\">Varnish</a> can serve static and dynamic content directly. Web servers can also cache requests, returning responses without having to contact application servers.</p><p>Your database usually includes some level of caching in a default configuration, optimized for a generic use case. Tweaking these settings for specific usage patterns can further boost performance.</p><p>In-memory caches such as Memcached and Redis are key-value stores between your application and your data storage. Since the data is held in RAM, it is much faster than typical databases where data is stored on disk. RAM is more limited than disk, so <a href=\"https://en.wikipedia.org/wiki/Cache_algorithms\">cache invalidation</a> algorithms such as <a href=\"https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\">least recently used (LRU)</a> can help invalidate 'cold' entries and keep 'hot' data in RAM.</p><p>Redis has the following additional features:</p><ul><li>Built-in data structures such as sorted sets and lists</li></ul><p>There are multiple levels you can cache that fall into two general categories:  and :</p><ul><li>Fully-formed serializable objects</li></ul><p>Generally, you should try to avoid file-based caching, as it makes cloning and auto-scaling more difficult.</p><h3>Caching at the database query level</h3><p>Whenever you query the database, hash the query as a key and store the result to the cache. This approach suffers from expiration issues:</p><ul><li>Hard to delete a cached result with complex queries</li><li>If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell</li></ul><h3>Caching at the object level</h3><p>See your data as an object, similar to what you do with your application code. Have your application assemble the dataset from the database into a class instance or a data structure(s):</p><ul><li>Remove the object from cache if its underlying data has changed</li><li>Allows for asynchronous processing: workers assemble objects by consuming the latest cached object</li></ul><p>Suggestions of what to cache:</p><ul></ul><p>Since you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.</p><p>The application is responsible for reading and writing from storage. The cache does not interact with storage directly. The application does the following:</p><ul><li>Look for entry in cache, resulting in a cache miss</li><li>Load entry from the database</li></ul><pre><code>def get_user(self, user_id):\n    user = cache.get(\"user.{0}\", user_id)\n    if user is None:\n        user = db.query(\"SELECT * FROM users WHERE user_id = {0}\", user_id)\n        if user is not None:\n            key = \"user.{0}\".format(user_id)\n            cache.set(key, json.dumps(user))\n    return user\n</code></pre><p>Subsequent reads of data added to cache are fast. Cache-aside is also referred to as lazy loading. Only requested data is cached, which avoids filling up the cache with data that isn't requested.</p><h5>Disadvantage(s): cache-aside</h5><ul><li>Each cache miss results in three trips, which can cause a noticeable delay.</li><li>Data can become stale if it is updated in the database. This issue is mitigated by setting a time-to-live (TTL) which forces an update of the cache entry, or by using write-through.</li><li>When a node fails, it is replaced by a new, empty node, increasing latency.</li></ul><p>The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:</p><ul><li>Application adds/updates entry in cache</li><li>Cache synchronously writes entry to data store</li></ul><pre><code>set_user(12345, {\"foo\":\"bar\"})\n</code></pre><pre><code>def set_user(user_id, values):\n    user = db.query(\"UPDATE Users WHERE id = {0}\", user_id, values)\n    cache.set(user_id, user)\n</code></pre><p>Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast. Users are generally more tolerant of latency when updating data than reading data. Data in the cache is not stale.</p><h5>Disadvantage(s): write through</h5><ul><li>When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database. Cache-aside in conjunction with write through can mitigate this issue.</li><li>Most data written might never be read, which can be minimized with a TTL.</li></ul><h4>Write-behind (write-back)</h4><p>In write-behind, the application does the following:</p><ul><li>Add/update entry in cache</li><li>Asynchronously write entry to the data store, improving write performance</li></ul><h5>Disadvantage(s): write-behind</h5><ul><li>There could be data loss if the cache goes down prior to its contents hitting the data store.</li><li>It is more complex to implement write-behind than it is to implement cache-aside or write-through.</li></ul><p>You can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration.</p><p>Refresh-ahead can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future.</p><h5>Disadvantage(s): refresh-ahead</h5><ul><li>Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh-ahead.</li></ul><ul><li>Need to maintain consistency between caches and the source of truth such as the database through <a href=\"https://en.wikipedia.org/wiki/Cache_algorithms\">cache invalidation</a>.</li><li>Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.</li><li>Need to make application changes such as adding Redis or memcached.</li></ul><h3>Source(s) and further reading</h3><p>Asynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line. They can also help by doing time-consuming work in advance, such as periodic aggregation of data.</p><p>Message queues receive, hold, and deliver messages. If an operation is too slow to perform inline, you can use a message queue with the following workflow:</p><ul><li>An application publishes a job to the queue, then notifies the user of job status</li><li>A worker picks up the job from the queue, processes it, then signals the job is complete</li></ul><p>The user is not blocked and the job is processed in the background. During this time, the client might optionally do a small amount of processing to make it seem like the task has completed. For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.</p><p> is useful as a simple message broker but messages can be lost.</p><p> is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.</p><p> is hosted but can have high latency and has the possibility of messages being delivered twice.</p><p>Tasks queues receive tasks and their related data, runs them, then delivers their results. They can support scheduling and can be used to run computationally-intensive jobs in the background.</p><p> has support for scheduling and primarily has python support.</p><p>If queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance. <a href=\"http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html\">Back pressure</a> can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue. Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later. Clients can retry the request at a later time, perhaps with <a href=\"https://en.wikipedia.org/wiki/Exponential_backoff\">exponential backoff</a>.</p><h3>Disadvantage(s): asynchronism</h3><ul><li>Use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations, as introducing queues can add delays and complexity.</li></ul><h3>Source(s) and further reading</h3><h3>Hypertext transfer protocol (HTTP)</h3><p>HTTP is a method for encoding and transporting data between a client and a server. It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request. HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression.</p><p>A basic HTTP request consists of a verb (method) and a resource (endpoint). Below are common HTTP verbs:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr><td>Creates a resource or trigger a process that handles data</td><td>Yes if response contains freshness info</td></tr><tr><td>Creates or replace a resource</td></tr><tr><td>Partially updates a resource</td><td>Yes if response contains freshness info</td></tr><tr></tr></tbody></table><p>*Can be called many times without different outcomes.</p><p>HTTP is an application layer protocol relying on lower-level protocols such as  and .</p><h4>Source(s) and further reading: HTTP</h4><h3>Transmission control protocol (TCP)</h3><p>TCP is a connection-oriented protocol over an <a href=\"https://en.wikipedia.org/wiki/Internet_Protocol\">IP network</a>. Connection is established and terminated using a <a href=\"https://en.wikipedia.org/wiki/Handshaking\">handshake</a>. All packets sent are guaranteed to reach the destination in the original order and without corruption through:</p><p>If the sender does not receive a correct response, it will resend the packets. If there are multiple timeouts, the connection is dropped. TCP also implements <a href=\"https://en.wikipedia.org/wiki/Flow_control_(data)\">flow control</a> and <a href=\"https://en.wikipedia.org/wiki/Network_congestion#Congestion_control\">congestion control</a>. These guarantees cause delays and generally result in less efficient transmission than UDP.</p><p>To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage. It can be expensive to have a large number of open connections between web server threads and say, a <a href=\"https://memcached.org/\">memcached</a> server. <a href=\"https://en.wikipedia.org/wiki/Connection_pool\">Connection pooling</a> can help in addition to switching to UDP where applicable.</p><p>TCP is useful for applications that require high reliability but are less time critical. Some examples include web servers, database info, SMTP, FTP, and SSH.</p><ul><li>You need all of the data to arrive intact</li><li>You want to automatically make a best estimate use of the network throughput</li></ul><h3>User datagram protocol (UDP)</h3><p>UDP is connectionless. Datagrams (analogous to packets) are guaranteed only at the datagram level. Datagrams might reach their destination out of order or not at all. UDP does not support congestion control. Without the guarantees that TCP support, UDP is generally more efficient.</p><p>UDP can broadcast, sending datagrams to all devices on the subnet. This is useful with <a href=\"https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol\">DHCP</a> because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address.</p><p>UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.</p><ul><li>You need the lowest latency</li><li>Late data is worse than loss of data</li><li>You want to implement your own error correction</li></ul><h4>Source(s) and further reading: TCP and UDP</h4><h3>Remote procedure call (RPC)</h3><p>In an RPC, a client causes a procedure to execute on a different address space, usually a remote server. The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program. Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls. Popular RPC frameworks include <a href=\"https://developers.google.com/protocol-buffers/\">Protobuf</a>, <a href=\"https://thrift.apache.org/\">Thrift</a>, and <a href=\"https://avro.apache.org/docs/current/\">Avro</a>.</p><p>RPC is a request-response protocol:</p><ul><li> - Calls the client stub procedure. The parameters are pushed onto the stack like a local procedure call.</li><li> - Marshals (packs) procedure id and arguments into a request message.</li><li><strong>Client communication module</strong> - OS sends the message from the client to the server.</li><li><strong>Server communication module</strong> - OS passes the incoming packets to the server stub procedure.</li><li> - Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments.</li><li>The server response repeats the steps above in reverse order.</li></ul><pre><code>GET /someoperation?data=anId\n\nPOST /anotheroperation\n{\n  \"data\":\"anId\";\n  \"anotherdata\": \"another value\"\n}\n</code></pre><p>RPC is focused on exposing behaviors. RPCs are often used for performance reasons with internal communications, as you can hand-craft native calls to better fit your use cases.</p><p>Choose a native library (aka SDK) when:</p><ul><li>You know your target platform.</li><li>You want to control how your \"logic\" is accessed.</li><li>You want to control how error control happens off your library.</li><li>Performance and end user experience is your primary concern.</li></ul><p>HTTP APIs following  tend to be used more often for public APIs.</p><ul><li>RPC clients become tightly coupled to the service implementation.</li><li>A new API must be defined for every new operation or use case.</li><li>It can be difficult to debug RPC.</li><li>You might not be able to leverage existing technologies out of the box. For example, it might require additional effort to ensure <a href=\"https://web.archive.org/web/20170608193645/http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/\">RPC calls are properly cached</a> on caching servers such as <a href=\"http://www.squid-cache.org/\">Squid</a>.</li></ul><h3>Representational state transfer (REST)</h3><p>REST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server. The server provides a representation of resources and actions that can either manipulate or get a new representation of resources. All communication must be stateless and cacheable.</p><p>There are four qualities of a RESTful interface:</p><ul><li><strong>Identify resources (URI in HTTP)</strong> - use the same URI regardless of any operation.</li><li><strong>Change with representations (Verbs in HTTP)</strong> - use verbs, headers, and body.</li><li><strong>Self-descriptive error message (status response in HTTP)</strong> - Use status codes, don't reinvent the wheel.</li><li> - your web service should be fully accessible in a browser.</li></ul><pre><code>GET /someresources/anId\n\nPUT /someresources/anId\n{\"anotherdata\": \"another value\"}\n</code></pre><p>REST is focused on exposing data. It minimizes the coupling between client/server and is often used for public HTTP APIs. REST uses a more generic and uniform method of exposing resources through URIs, <a href=\"https://github.com/for-GET/know-your-http-well/raw/master/headers.md\">representation through headers</a>, and actions through verbs such as GET, POST, PUT, DELETE, and PATCH. Being stateless, REST is great for horizontal scaling and partitioning.</p><ul><li>With REST being focused on exposing data, it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy. For example, returning all updated records from the past hour matching a particular set of events is not easily expressed as a path. With REST, it is likely to be implemented with a combination of URI path, query parameters, and possibly the request body.</li><li>REST typically relies on a few verbs (GET, POST, PUT, DELETE, and PATCH) which sometimes doesn't fit your use case. For example, moving expired documents to the archive folder might not cleanly fit within these verbs.</li><li>Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views, e.g. fetching content of a blog entry and the comments on that entry. For mobile applications operating in variable network conditions, these multiple roundtrips are highly undesirable.</li><li>Over time, more fields might be added to an API response and older clients will receive all new data fields, even those that they do not need, as a result, it bloats the payload size and leads to larger latencies.</li></ul><h3>RPC and REST calls comparison</h3><table><tbody><tr></tr><tr><td> /resign{}</td></tr><tr><td> /readPerson?personid=1234</td></tr><tr><td>Read a person’s items list</td><td> /readUsersItemsList?personid=1234</td></tr><tr><td>Add an item to a person’s items</td><td> /addItemToUsersItemsList{\"itemid\": \"456\"</td><td> /persons/1234/items{}</td></tr><tr><td> /modifyItem{\"key\": \"value\"</td><td> /items/456{}</td></tr><tr><td> /removeItem{}</td></tr></tbody></table><h4>Source(s) and further reading: REST and RPC</h4><p>Security is a broad topic. Unless you have considerable experience, a security background, or are applying for a position that requires knowledge of security, you probably won't need to know more than the basics:</p><ul><li>Encrypt in transit and at rest.</li><li>Sanitize all user inputs or any input parameters exposed to user to prevent <a href=\"https://en.wikipedia.org/wiki/Cross-site_scripting\">XSS</a> and <a href=\"https://en.wikipedia.org/wiki/SQL_injection\">SQL injection</a>.</li><li>Use parameterized queries to prevent SQL injection.</li></ul><h3>Source(s) and further reading</h3><p>You'll sometimes be asked to do 'back-of-the-envelope' estimates. For example, you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take. The  and <strong>Latency numbers every programmer should know</strong> are handy references.</p><pre><code>Power           Exact Value         Approx Value        Bytes\n---------------------------------------------------------------\n7                             128\n8                             256\n10                           1024   1 thousand           1 KB\n16                         65,536                       64 KB\n20                      1,048,576   1 million            1 MB\n30                  1,073,741,824   1 billion            1 GB\n32                  4,294,967,296                        4 GB\n40              1,099,511,627,776   1 trillion           1 TB\n</code></pre><h4>Source(s) and further reading</h4><h3>Latency numbers every programmer should know</h3><pre><code>Latency Comparison Numbers\n--------------------------\nL1 cache reference                           0.5 ns\nBranch mispredict                            5   ns\nL2 cache reference                           7   ns                      14x L1 cache\nMutex lock/unlock                           25   ns\nMain memory reference                      100   ns                      20x L2 cache, 200x L1 cache\nCompress 1K bytes with Zippy            10,000   ns       10 us\nSend 1 KB bytes over 1 Gbps network     10,000   ns       10 us\nRead 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD\nRead 1 MB sequentially from memory     250,000   ns      250 us\nRound trip within same datacenter      500,000   ns      500 us\nRead 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory\nHDD seek                            10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip\nRead 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD\nRead 1 MB sequentially from HDD     30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD\nSend packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms\n\nNotes\n-----\n1 ns = 10^-9 seconds\n1 us = 10^-6 seconds = 1,000 ns\n1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns\n</code></pre><p>Handy metrics based on numbers above:</p><ul><li>Read sequentially from HDD at 30 MB/s</li><li>Read sequentially from 1 Gbps Ethernet at 100 MB/s</li><li>Read sequentially from SSD at 1 GB/s</li><li>Read sequentially from main memory at 4 GB/s</li><li>6-7 world-wide round trips per second</li><li>2,000 round trips per second within a data center</li></ul><h4>Latency numbers visualized</h4><h4>Source(s) and further reading</h4><h3>Additional system design interview questions</h3><blockquote><p>Common system design interview questions, with links to resources on how to solve each.</p></blockquote><blockquote><p>Articles on how real world systems are designed.</p></blockquote><p><strong>Don't focus on nitty gritty details for the following articles, instead:</strong></p><ul><li>Identify shared principles, common technologies, and patterns within these articles</li><li>Study what problems are solved by each component, where it works, where it doesn't</li><li>Review the lessons learned</li></ul><table><tbody><tr><td> - Distributed data processing from Databricks</td></tr><tr><td> - Distributed data processing from Twitter</td></tr><tr><td> - Distributed column-oriented database from Google</td></tr><tr><td> - Distributed column-oriented database from Facebook</td></tr><tr><td> - Document-oriented database from Amazon</td></tr><tr><td> - Distributed memory caching system with persistence and value types</td></tr><tr><td><strong>Hadoop File System (HDFS)</strong> - Open source implementation of GFS</td></tr><tr><td> - Lock service for loosely-coupled distributed systems from Google</td></tr><tr><td> - Centralized infrastructure and services enabling synchronization</td></tr></tbody></table><h3>Company engineering blogs</h3><blockquote><p>Architectures for companies you are interviewing with.</p><p>Questions you encounter might be from the same domain.</p></blockquote><h4>Source(s) and further reading</h4><p>Looking to add a blog? To avoid duplicating work, consider adding your company blog to the following repo:</p><p>Interested in adding a section or helping complete one in-progress? <a href=\"https://raw.githubusercontent.com/donnemartin/system-design-primer/master/#contributing\">Contribute</a>!</p><ul><li>Distributed computing with MapReduce</li></ul><p>Credits and sources are provided throughout this repo.</p><p>Feel free to contact me to discuss any issues, questions, or comments.</p><p><em>I am providing code and resources in this repository to you under an open source license. Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).</em></p><pre><code>Copyright 2017 Donne Martin\n\nCreative Commons Attribution 4.0 International License (CC BY 4.0)\n\nhttp://creativecommons.org/licenses/by/4.0/\n</code></pre>","contentLength":51108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MHSanaei/3x-ui","url":"https://github.com/MHSanaei/3x-ui","date":1761532686,"author":"","guid":315597,"unread":true,"content":"<p>Xray panel supporting multi-protocol multi-user expire day &amp; traffic &amp; IP limit (Vmess, Vless, Trojan, ShadowSocks, Wireguard, Tunnel, Mixed, HTTP)</p><p> — advanced, open-source web-based control panel designed for managing Xray-core server. It offers a user-friendly interface for configuring and monitoring various VPN and proxy protocols.</p><blockquote><p>[!IMPORTANT] This project is only for personal using, please do not use it for illegal purposes, please do not use it in a production environment.</p></blockquote><p>As an enhanced fork of the original X-UI project, 3X-UI provides improved stability, broader protocol support, and additional features.</p><pre><code>bash &lt;(curl -Ls https://raw.githubusercontent.com/mhsanaei/3x-ui/master/install.sh)\n</code></pre><ul><li><a href=\"https://github.com/chocolate4u/Iran-v2ray-rules\">Iran v2ray rules</a> (License: ): <em>Enhanced v2ray/xray and v2ray/xray-clients routing rules with built-in Iranian domains and a focus on security and adblocking.</em></li><li><a href=\"https://github.com/runetfreedom/russia-v2ray-rules-dat\">Russia v2ray rules</a> (License: ): <em>This repository contains automatically updated V2Ray routing rules based on data on blocked domains and addresses in Russia.</em></li></ul><p><strong>If this project is helpful to you, you may wish to give it a</strong></p><a href=\"https://www.buymeacoffee.com/MHSanaei\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/MHSanaei/3x-ui/main/media/default-yellow.png\" alt=\"Buy Me A Coffee\"></a><a href=\"https://nowpayments.io/donation/hsanaei\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://raw.githubusercontent.com/MHSanaei/3x-ui/main/media/donation-button-black.svg?sanitize=true\" alt=\"Crypto donation button by NOWPayments\"></a>","contentLength":1074,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"cloudcommunity/Free-Certifications","url":"https://github.com/cloudcommunity/Free-Certifications","date":1761532686,"author":"","guid":315598,"unread":true,"content":"<p>A curated list of free courses with certifications. Also available at https://free-certifications.com/</p><p>A curated list of <strong>free courses with certifications</strong>. Don't forget to star ⭐ this repository.</p><p>We are excited to announce that this comprehensive collection is now available as a user-friendly website at 🌐  📚!</p><p>📬 <strong>Stay Updated with Our Newsletter!</strong> 📬</p><p>We are launching a <strong>Free Certifications Newsletter</strong> to bring the latest free certification opportunities straight to your inbox! Our newsletter will start distribution sometime between February and March 2025 and will be sent either weekly or biweekly.  and never miss out on new learning opportunities!</p><p>📌 <strong>Love Freebies? Check These Out!</strong> 📌</p><p>If you love freebies, be sure to explore our other curated lists of free resources for various communities:</p><p>Check them out and take advantage of amazing free resources!</p><p>🎉 Brought to you by the <a href=\"https://cloudstudy.net/\">Cloud Study Network</a> - a global tech community that shares knowledge, goodies, and good vibes! 🎉</p><table><thead><tr></tr></thead><tbody><tr><td>Free Certifications paths and badges</td></tr><tr><td>Oracle Cloud Infrastructure, Oracle Cloud Infrastructure, Oracle Cloud Infrastructure</td></tr><tr><td>Machine Learning with Python</td><td>Free lesson and certification</td></tr><tr><td>Free lesson and certification</td></tr><tr><td>Generative AI Fundamentals</td></tr><tr><td>Oracle Cloud Infrastructure 2023 Foundations Associate</td></tr><tr><td>Oracle Cloud Data Management 2023 Foundations Associate</td></tr><tr><td>Free subscription at Pluralsight for Microsoft Azure courses</td></tr><tr><td>Aviatrix, Free ACE - Multicloud Networking associate Course &amp; Certificate code ACEMULTICLOUD</td></tr><tr><td>Build: Azure Developer Challenge</td><td>Learn how to design, build, test, and maintain cloud applications and services on Microsoft Azure.</td></tr><tr><td>Free JumpCloud Core Certification (worth $150).</td></tr><tr><td>Extra 30 days for Google Cloud labs on Qwiklabs.</td></tr><tr><td>Test Automation University</td><td>Free certification courses by the Test Automation University.</td></tr><tr><td>Courses and certifications for free by Juniper Networks (instead of 150 euros).</td></tr><tr><td>Claim 30 days of free Qwiklabs and access to the featured labs.</td></tr><tr><td>Free courses &amp; exams from Huawei Academy for the HCIA, HCIP, and HCIE certifications.</td></tr><tr><td>Free course &amp; certification (HCIA level, $200 value).</td></tr><tr><td>Free courses by JetBrains Academy for learning Java, Kotlin &amp; Python.</td></tr><tr><td>Free access to 11 Elastic Stack courses ($200 value each).</td></tr><tr><td>One free month of access to nanodegree programs by Udacity ($400 value). Credit card / PayPal required. Don’t forget to cancel in time.</td></tr><tr><td>“Architecting on Alibaba Cloud Specialization” at Coursera.</td></tr><tr><td>The Linux Foundation offers 23 free courses with finalizing exams &amp; confirmations.</td></tr><tr><td>Free training through CloudBees University (Jenkins, DevOps).</td></tr><tr><td>6 free training courses and certifications by Sumo Logic.</td></tr><tr><td>6 Free Code Camp learning courses &amp; certifications, incl. RWD, JavaScript, APIs, React…</td></tr><tr><td>~670 free courses at Udemy, incl. certificates.</td></tr><tr><td>Free cPanel Professional Certification (CPP) awarded simply by successfully completing the full series of video lessons. There is no final certification exam required for the CPP status.</td></tr><tr><td>Free Plesk Obsidian Professional Certification</td></tr><tr><td>Free SolusIO Professional Certification</td></tr><tr><td>Google Analytics Academy free courses with certificates</td></tr><tr><td>Silver Peak , offers Free Training &amp; Certification Exam for SD-WAN Profissional</td></tr><tr><td>Free AI courses with proof of completion and badge</td></tr><tr><td>gain or improve digital skills on our eLearning platform</td></tr><tr><td>Free SysTrack certification and badge upon completion of self-paced courses through their online learning platform. They offer three courses: SysTrack Technician, SysTrack Engineer, and SysTrack Dashboard Designer.</td></tr><tr><td>Free Eggplant courses and certifications.</td></tr><tr><td>Free full-time AWS training with certification and launch a career in cloud computing with AWS re/Start for unemployed and underemployed individuals.</td></tr><tr><td>AWS Cloud Quest: Cloud Practitioner</td></tr><tr><td>AWS Solutions Architect - Knowledge Badge Readiness Path</td></tr><tr><td>Postman API Fundamentals Student Expert</td></tr><tr><td>Free API Designer and API Security Architect certifications.</td></tr><tr><td>Event-Native API Management Fundamentals Certification</td></tr><tr><td>Event-Native API Management Professional Certification</td></tr><tr><td>Free Zerto Associate Certification on Zerto University.</td></tr><tr><td>Certified Calico Operator: Level 1.</td></tr><tr><td>Certified Calico Operator: AWS Expert</td></tr><tr><td>Certified Calico Operator: Azure Expert</td></tr><tr><td>Certified Calico Operator: eBPF</td></tr><tr><td>Several free courses with certificates are available for reference in design, computer science, mathematics, etc.</td></tr><tr><td>FREE Chef Principles Certification Exam</td></tr><tr><td>FREE Full Stack Observability Exam</td></tr><tr><td>FREE Programmability Certification Exam</td></tr><tr><td>FREE Gremlin Certified Chaos Engineering practitioner Certification</td></tr><tr><td>Become a Neo4j Certified Professional</td></tr><tr><td>Fundamentals Accreditation</td></tr><tr><td>Fundamentals Accreditation</td></tr><tr><td>Become an ace on Gatling’s Load Testing tool!</td></tr><tr><td>Appium Advanced Certifications</td></tr><tr><td>Selenium Advanced Certifications</td></tr><tr><td>Deep Learning Specialization. Master Deep Learning, and Break into AI</td></tr><tr></tr><tr><td>Free Kubernetes Fundamentals Training with Credly Badge</td></tr><tr><td>Apollo Graph Developer - Associate Certification</td></tr><tr><td>Getting Started with Cilium</td></tr><tr><td>Cilium Ingress Controller</td></tr><tr></tr><tr><td>Isovalent Enterprise for Cilium: Network Policies</td></tr><tr></tr><tr></tr><tr><td>Cilium LoadBalancer IPAM and BGP Service Advertisement</td></tr><tr><td>Cilium LoadBalancer IPAM and L2 Service Announcement</td></tr><tr><td>Isovalent Enterprise for Cilium: Zero Trust Visibility</td></tr><tr><td>Cilium IPv6 Networking and Observability</td></tr><tr></tr><tr><td>Advanced Gateway API Use Cases</td></tr><tr><td>L7 Load-Balancing with Kubernetes Services + Annotations</td></tr><tr><td>Isovalent Enterprise for Cilium: Connectivity Visibility with Hubble</td></tr><tr><td>Golden Signals with Hubble and Grafana</td></tr><tr><td>Mutual Authentication with Cilium</td></tr><tr><td>Cilium Transparent Encryption with IPSec and WireGuard</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Getting Started with Tetragon</td></tr><tr><td>Isovalent Enterprise for Cilium: Security Visibility</td></tr><tr><td>Isovalent Enterprise for Cilium: TLS Visibility</td></tr><tr><td>Getting Started with eBPF</td></tr><tr></tr><tr><td>A repository of over 700 training lessons to help you learn AWS, refine your knowledge of AWS services, and improve your skills so you can put them into practice or apply the knowledge during the many AWS certifications.</td></tr><tr><td>Professional API Management Certification</td></tr><tr><td>Professional API Design Certification</td></tr><tr><td>Associate Developer Certification</td></tr><tr><td>Associate EDI for X12 Certification</td></tr><tr><td>Associate Flow Essentials Certification</td></tr><tr><td>Associate Master Data Hub Certification</td></tr><tr><td>Development and Application Architecture Certification</td></tr><tr><td>Professional Developer Certification</td></tr><tr><td>Professional Flow Developer Certification</td></tr><tr><td>Associate Administrator Certification</td></tr><tr><td>Professional Linux Operational Administrator Certification</td></tr><tr><td>Professional Windows Operational Administrator Certification</td></tr><tr><td>Free Course On Event-Driven Architecture with Apache Kafka and Red Hat OpenShift Application Services Technical Overview</td></tr><tr><td>Free Course On Red Hat OpenStack Technical Overview</td></tr><tr><td>Free Course On Ansible Basics: Automation Technical Overview</td></tr><tr><td>Free Course On Red Hat Agile Integration Technical Overview</td></tr><tr><td>Free Course On Containers, Kubernetes and Red Hat OpenShift Technical Overview</td></tr><tr><td>Free Course On Developing Cloud-Native Applications with Microservices Architectures</td></tr><tr><td>Free Course On Virtualization and Infrastructure Migration Technical Overview</td></tr><tr><td>Free Course On Red Hat Enterprise Linux Technical Overview</td></tr><tr><td>Ansible Automation for SAP</td><td>Free Course On Red Hat Ansible Automation for SAP Technical Overview</td></tr><tr><td>Free Course On Red Hat Satellite Technical Overview</td></tr><tr><td>Free Course On Running Containers with Red Hat Technical Overview</td></tr><tr><td>Site Reliability Engineering</td><td>Free Course On Transitional approach to implementing pragmatic Site Reliability Engineering (SRE) Technical Overview</td></tr><tr><td>Introduction to Continuous Delivery and GitOps using Argo CD</td></tr><tr><td>Understanding Cisco Network Automation Essentials - DEVNAE - 16 continuing education credits free</td></tr><tr></tr><tr><td>Official tutorials provided by Unity, for all skill levels, covering a wide range of features and skills.</td></tr><tr></tr><tr><td>Google Cloud Skills Boost</td></tr><tr><td>Google Cloud Skills Boost</td><td>Introduction to Generative AI</td></tr><tr><td>Google Cloud Skills Boost</td><td>Intro to ML: Language Processing</td></tr><tr><td>Google Cloud Skills Boost</td><td>Integrate with Machine Learning APIs</td></tr><tr><td>Google Cloud Skills Boost</td></tr><tr><td>Free online courses about the basics of AI</td></tr><tr><td>Career Essentials in Generative AI by Microsoft and LinkedIn</td></tr><tr><td>Free online course and certification for Deep Reinforcement Learning</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Cisco Certificate in Ethical Hacking</td></tr><tr><td>Understanding Cisco Cybersecurity Operations Fundamentals - CBROPS - 30 continuing education credits free</td></tr><tr><td>Voucher 100%, Certified in Cybersecurity℠(CC) Exam Voucher: CC1M12312024</td></tr><tr><td>Free certification exam coupon code upon completion of certification prep training during the event.</td></tr><tr><td>Free Network Security training courses &amp; certifications by Fortinet / NSE Institute.</td></tr><tr><td>The Academic Council Of uLektz</td><td>Free Cyber Security training and certification.</td></tr><tr><td>Free Certified Cyber Professional (CCP) Specialism Pilot certification for  (see link for info).</td></tr><tr><td>Free Introduction to Cybersecurity course with Networking Academy badge for completing this course.</td></tr><tr><td>Free ISO/IEC 27001 Information Security Associate™</td></tr><tr><td>Foundations of Purple Teaming</td></tr><tr><td>Foundations of Breach &amp; Attack Simulation</td></tr><tr><td>Foundations of Operationalizing MITRE ATT&amp;CK v13</td></tr><tr><td>Introduction to Cybersecurity</td></tr><tr></tr><tr><td>Google Cloud Skills Boost</td><td>Security &amp; Identity Fundamentals</td></tr><tr><td>Open Source Security Foundation</td><td>Developing Secure Software (LFD121)</td></tr><tr><td>IBM QRadar SIEM Foundation</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Free Exasol training courses and certifications (€150 value each).</td></tr><tr><td>12 free MongoDB courses with proof of completion.</td></tr><tr><td>Free Cert - the core concepts behind distributed databases and give you all the tools you need to get started with CockroachDB</td></tr><tr><td>Free Cert - you will build a full-stack ride-sharing application in Python using the popular SQLAlchemy ORM and CockroachDB</td></tr><tr><td>Learn all about Liquibase fundamentals from free online courses by Liquibase experts and see how to apply them in the real world.</td></tr><tr><td>Redis Certified Developer is our professional certification program for software developers who regularly work with Redis.</td></tr><tr><td>Free Training: Databricks Lakehouse Fundamentals.</td></tr><tr><td>Free SQL Course with Certificate</td></tr><tr><td>Free Exam Certificates and Proof of Completion Course Certificates</td></tr><tr><td>ArangoDB Certified Professional</td></tr><tr><td>CrateDB Academy offers free online courses and certification for our open source multi-model database.</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Free “Scrum Foundations Professional Certificate (SFPC)” certification. Available in en, pt-br &amp; es. Use the code “COVID19Support”</td></tr><tr><td>Free Six Sigma White Belt Training &amp; Certification.</td></tr><tr><td>Free Project Management course and certificate by Oxford Home Study Centre (OHSC).</td></tr><tr><td>Free Project Management Essentials Certified (PMEC) training &amp; certification.</td></tr><tr><td>Free Six Sigma Yellow Belt course &amp; certification.</td></tr><tr><td>Free “Scrum Fundamentals Certified (SFC™)” training course &amp; certification</td></tr><tr><td>Free Certified Associate In Scrum Fundamentals™ (CASF™)</td></tr><tr><td>Free Project Management course</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Fundamentals of Digital Marketing free course &amp; certificate by Google.</td></tr><tr><td>Microsoft Advertising certification and training.</td></tr><tr><td>4 free marketing-related fundamental certifications by SMstudy.</td></tr><tr><td>Free Facebook &amp; Instagram Marketing course and certification by DMAC (Digital Marketing Academy of Canada).</td></tr><tr><td> Free Hootsuite Platform Certification (worth $99) and Social Marketing Certification (worth $199) through Hootsuite's Student Program.</td></tr><tr><td>Free marketing &amp; sales courses with certification.</td></tr><tr><td>Free Online Digital Marketing Courses and Exams.</td></tr><tr><td>Meta Certified Digital Marketing Associate</td></tr><tr><td>Microsoft Advertising Certified Professional.</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Revenera Certification, free of charge to approved members of the legal community.</td></tr><tr><td>Kahoot! Certified for schools, this free program is designed to help teachers become the ultimate Kahoot! superheroes.</td></tr><tr><td>Free Explain Everything course and certification.</td></tr><tr><td>Free SkillFront Entrepreneur Program™: Foundations Of Business And Entrepreneurship™</td></tr><tr><td>Free Remote Work and Virtual Collaboration - RWVCPC</td></tr><tr><td>Business Model Canvas Essentials Professional Certification - BMCEPC</td></tr><tr><td>Business Intelligence Foundation Professional Certification - BIFPC (Spanish Only)</td></tr><tr><td>Free Slack Skill Learning Paths and Badges (issued by accredible.com - these are not certifications but badges for skill specialists)</td></tr><tr><td>Reuters Training Course: Introduction to Digital Journalism</td></tr><tr><td>openSAP is SAP's free learning platform for everyone interested in learning about SAP's latest innovations and how to survive in the digital economy.</td></tr><tr><td>Free online training courses to build your Kami skills and grow as a leader in your professional learning community</td></tr><tr></tr><tr></tr><tr></tr><tr><td>Cloud App Maker, Microsoft</td><td>Microsoft low-code Cloud App Maker Certification: Register and complete the learning path within the start and end date to get a Free voucher to take the Microsoft Associate level certification</td></tr><tr><td>15-minute English quiz to validate reading skills (English grammar and vocabulary) and listening skills, aligned to CEFR levels.</td></tr><tr><td>50-minute test to receive your personalized English certificate to add to your LinkedIn profile or CV, aligned to CEFR levels.</td></tr><tr><td>The Microsoft Licensing Specialist certifications demonstrate competence in a particular area of Microsoft product licensing. You will require an intermediate level of knowledge to achieve these certifications.</td></tr><tr><td>Free Kanban Flow Metrics assessment</td></tr><tr><td>Confluence Fundamentals Badge</td></tr><tr><td>Beginner's Guide to Agile in Jira Badge</td></tr></tbody></table>","contentLength":12743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"chartdb/chartdb","url":"https://github.com/chartdb/chartdb","date":1761532686,"author":"","guid":315599,"unread":true,"content":"<p>Database diagrams editor that allows you to visualize and design your DB with a single query.</p><p align=\"center\"><b>Open-source database diagrams editor</b><b>No installations • No Database password required.</b></p><p>ChartDB is a powerful, web-based database diagramming editor. Instantly visualize your database schema with a single  Customize diagrams, export SQL scripts, and access all features—no account required. Experience seamless database design here.</p><ul><li><p> Run a single query to instantly retrieve your database schema as JSON. This makes it incredibly fast to visualize your database schema, whether for documentation, team discussions, or simply understanding your data better.</p></li><li><p><strong>AI-Powered Export for Easy Migration</strong> Our AI-driven export feature allows you to generate the DDL script in the dialect of your choice. Whether you're migrating from MySQL to PostgreSQL or from SQLite to MariaDB, ChartDB simplifies the process by providing the necessary scripts tailored to your target database.</p></li><li><p> Fine-tune your database schema using our intuitive editor. Easily make adjustments or annotations to better visualize complex structures.</p></li></ul><p>ChartDB is currently in Public Beta. Star and watch this repository to get notified of updates.</p><pre><code>npm install\nnpm run build\n</code></pre><p>Or like this if you want to have AI capabilities:</p><pre><code>npm install\nVITE_OPENAI_API_KEY=&lt;YOUR_OPEN_AI_KEY&gt; npm run build\n</code></pre><pre><code>docker run -e OPENAI_API_KEY=&lt;YOUR_OPEN_AI_KEY&gt; -p 8080:80 ghcr.io/chartdb/chartdb:latest\n</code></pre><pre><code>docker build -t chartdb .\ndocker run -e OPENAI_API_KEY=&lt;YOUR_OPEN_AI_KEY&gt; -p 8080:80 chartdb\n</code></pre><h4>Using Custom Inference Server</h4><pre><code># Build\ndocker build \\\n  --build-arg VITE_OPENAI_API_ENDPOINT=&lt;YOUR_ENDPOINT&gt; \\\n  --build-arg VITE_LLM_MODEL_NAME=&lt;YOUR_MODEL_NAME&gt; \\\n  -t chartdb .\n\n# Run\ndocker run \\\n  -e OPENAI_API_ENDPOINT=&lt;YOUR_ENDPOINT&gt; \\\n  -e LLM_MODEL_NAME=&lt;YOUR_MODEL_NAME&gt; \\\n  -p 8080:80 chartdb\n</code></pre><blockquote><p> ChartDB includes privacy-focused analytics via Fathom Analytics. You can disable this by adding <code>-e DISABLE_ANALYTICS=true</code> to the run command or <code>--build-arg VITE_DISABLE_ANALYTICS=true</code> when building.</p></blockquote><blockquote><p> You must configure either Option 1 (OpenAI API key) OR Option 2 (Custom endpoint and model name) for AI capabilities to work. Do not mix the two options.</p></blockquote><p>Open your browser and navigate to .</p><p>Example configuration for a local vLLM server:</p><pre><code>VITE_OPENAI_API_ENDPOINT=http://localhost:8000/v1\nVITE_LLM_MODEL_NAME=Qwen/Qwen2.5-32B-Instruct-AWQ\n</code></pre><ol><li>Choose the database that you are using.</li><li>Take the magic query and run it in your database.</li><li>Copy and paste the resulting JSON set into ChartDB.</li></ol><ul><li><a href=\"https://discord.gg/QeFwyWSKwC\">Discord</a> (For live discussion with the community and the ChartDB team)</li><li><a href=\"https://github.com/chartdb/chartdb/issues\">GitHub Issues</a> (For any bugs and errors you encounter using ChartDB)</li></ul><p>We welcome community contributions, big or small, and are here to guide you along the way. Message us in the <a href=\"https://discord.gg/QeFwyWSKwC\">ChartDB Community Discord</a>.</p><p>Thank you for helping us make ChartDB better for everyone .</p>","contentLength":2821,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/agent-lightning","url":"https://github.com/microsoft/agent-lightning","date":1761532686,"author":"","guid":315600,"unread":true,"content":"<p>The absolute trainer to light up AI agents.</p><p><strong>The absolute trainer to light up AI agents.</strong></p><ul><li>Turn your agent into an optimizable beast with  (almost)! 💤</li><li>Build with  agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! 🤖</li><li> optimize one or more agents in a multi-agent system. 🎯</li><li>Embraces  like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. 🤗</li></ul><pre><code>pip install agentlightning\n</code></pre><ul><li><a href=\"https://github.com/af-74413592/DeepWerewolf\">DeepWerewolf</a> — A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.</li><li><a href=\"https://agentflow.stanford.edu/\">AgentFlow</a> — A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.</li></ul><p>Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight  helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.</p><p>On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.</p><p>No rewrites, no lock-in, just a clear path from first rollout to steady improvement.</p><p>If you find Agent Lightning useful in your research or projects, please cite our paper:</p><pre><code>@misc{luo2025agentlightningtrainai,\n      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},\n      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},\n      year={2025},\n      eprint={2508.03680},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.03680},\n}\n</code></pre><p>This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <a href=\"https://cla.opensource.microsoft.com\">https://cla.opensource.microsoft.com</a>.</p><p>When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.</p><p>This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow <a href=\"https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general\">Microsoft's Trademark &amp; Brand Guidelines</a>. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.</p><p>This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.</p><p>This project is licensed under the MIT License. See the <a href=\"https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE\">LICENSE</a> file for details.</p>","contentLength":3584,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"yeongpin/cursor-free-vip","url":"https://github.com/yeongpin/cursor-free-vip","date":1761532686,"author":"","guid":315601,"unread":true,"content":"<p>[Support 0.49.x]（Reset Cursor AI MachineID &amp; Bypass Higher Token Limit） Cursor Ai ，自动重置机器ID ， 免费升级使用Pro功能: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.</p><div align=\"center\"><h4>Support Latest 0.49.x Version | 支持最新 0.49.x 版本</h4><p>This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project. This tool will not generate any fake email accounts and OAuth access.</p><p>Supports Windows, macOS and Linux.</p><p>For optimal performance, run with privileges and always stay up to date.</p><p>這是一款用於學習和研究的工具，目前 repo 沒有違反任何法律。請支持原作者。 這款工具不會生成任何假的電子郵件帳戶和 OAuth 訪問。</p><p>支持 Windows、macOS 和 Linux。</p></div><ul><li><p>Support Windows macOS and Linux systems支持 Windows、macOS 和 Linux 系統</p></li><li><p>Reset Cursor's configuration重置 Cursor 的配置</p></li><li><p>Multi-language support (English, 简体中文, 繁體中文, Vietnamese)多語言支持（英文、简体中文、繁體中文、越南語）</p></li></ul><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>If you want to stop the script, please press Ctrl+C要停止腳本，請按 Ctrl+C</p><p>📝 Config | 文件配置 <code>Win / Macos / Linux Path | 路徑 [Documents/.cursor-free-vip/config.ini]</code></p><ul><li><p>Use administrator privileges to run the script 請使用管理員身份運行腳本</p></li><li><p>Confirm that Cursor is closed before running the script 請確保在運行腳本前已經關閉 Cursor</p></li><li><p>This tool is only for learning and research purposes 此工具僅供學習和研究使用</p></li><li><p>Please comply with the relevant software usage terms when using this tool 使用本工具時請遵守相關軟件使用條款</p></li></ul><table><tbody><tr><td align=\"center\">If you encounter permission issues, please ensure:</td><td align=\"center\">This script is run with administrator privileges</td></tr><tr><td align=\"center\">Error 'User is not authorized'</td><td align=\"center\">This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service</td></tr></tbody></table><p>歡迎提交 Issue 和 Pull Request！</p><a href=\"https://github.com/yeongpin/cursor-free-vip/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;preview=true&amp;max=&amp;columns=\"></a><p>本工具僅供學習和研究使用，使用本工具所產生的任何後果由使用者自行承擔。 </p><p>This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne by the user.</p><h2>💰 Buy Me a Coffee | 請我喝杯咖啡</h2>","contentLength":2335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"go-gitea/gitea","url":"https://github.com/go-gitea/gitea","date":1761532686,"author":"","guid":315602,"unread":true,"content":"<p>Git with a cup of tea! Painless self-hosted all-in-one software development service, including Git hosting, code review, team collaboration, package registry and CI/CD</p><p>The goal of this project is to make the easiest, fastest, and most painless way of setting up a self-hosted Git service.</p><p>As Gitea is written in Go, it works across  the platforms and architectures that are supported by Go, including Linux, macOS, and Windows on x86, amd64, ARM and PowerPC architectures. This project has been <a href=\"https://blog.gitea.com/welcome-to-gitea/\">forked</a> from <a href=\"https://gogs.io\">Gogs</a> since November of 2016, but a lot has changed.</p><p>For accessing free Gitea service (with a limited number of repositories), you can visit <a href=\"https://gitea.com/user/login\">gitea.com</a>.</p><p>To quickly deploy your own dedicated Gitea instance on Gitea Cloud, you can start a free trial at <a href=\"https://cloud.gitea.com\">cloud.gitea.com</a>.</p><p>It includes installation, administration, usage, development, contributing guides, and more to help you get started and explore all features effectively.</p><p>From the root of the source tree, run:</p><pre><code>TAGS=\"bindata\" make build\n</code></pre><p>or if SQLite support is required:</p><pre><code>TAGS=\"bindata sqlite sqlite_unlock_notify\" make build\n</code></pre><p>The  target is split into two sub-targets:</p><p>Internet connectivity is required to download the go and npm modules. When building from the official source tarballs which include pre-built frontend files, the  target will not be triggered, making it possible to build without Node.js.</p><p>After building, a binary file named  will be generated in the root of the source tree by default. To run it, use:</p><blockquote><p>[!NOTE] If you're interested in using our APIs, we have experimental support with <a href=\"https://docs.gitea.com/api\">documentation</a>.</p></blockquote><p>Expected workflow is: Fork -&gt; Patch -&gt; Push -&gt; Pull Request</p><blockquote><ol><li>If you have found a vulnerability in the project, please write privately to . Thanks!</li></ol></blockquote><p>Translations are done through <a href=\"https://translate.gitea.com\">Crowdin</a>. If you want to translate to a new language, ask one of the managers in the Crowdin project to add a new language there.</p><p>You can also just create an issue for adding a language or ask on Discord on the #translation channel. If you need context or find some translation issues, you can leave a comment on the string or ask on Discord. For general translation questions there is a section in the docs. Currently a bit empty, but we hope to fill it as questions pop up.</p><h2>Official and Third-Party Projects</h2><p>We maintain a list of Gitea-related projects at <a href=\"https://gitea.com/gitea/awesome-gitea\">gitea/awesome-gitea</a>, where you can discover more third-party projects, including SDKs, plugins, themes, and more.</p><p>Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [<a href=\"https://opencollective.com/gitea#sponsor\">Become a sponsor</a>]</p><p><strong>How do you pronounce Gitea?</strong></p><p>Gitea is pronounced <a href=\"https://youtu.be/EM71-2uDAoY\">/ɡɪ’ti:/</a> as in \"gi-tea\" with a hard g.</p><p><strong>Why is this not hosted on a Gitea instance?</strong></p><p><strong>Where can I find the security patches?</strong></p><p>This project is licensed under the MIT License. See the <a href=\"https://github.com/go-gitea/gitea/raw/main/LICENSE\">LICENSE</a> file for the full license text.</p>","contentLength":2785,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"2dust/v2rayN","url":"https://github.com/2dust/v2rayN","date":1761532686,"author":"","guid":315603,"unread":true,"content":"<p>A GUI client for Windows, Linux and macOS, support Xray and sing-box and others</p>","contentLength":79,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shubhamsaboo/awesome-llm-apps","url":"https://github.com/Shubhamsaboo/awesome-llm-apps","date":1761532686,"author":"","guid":315604,"unread":true,"content":"<p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p><p>A curated collection of <strong>Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.</strong> This repository features LLM apps that use models from <img src=\"https://cdn.simpleicons.org/openai\" alt=\"openai logo\" width=\"25\" height=\"15\"> , <img src=\"https://cdn.simpleicons.org/anthropic\" alt=\"anthropic logo\" width=\"25\" height=\"15\">, <img src=\"https://cdn.simpleicons.org/googlegemini\" alt=\"google logo\" width=\"25\" height=\"18\">, <img src=\"https://cdn.simpleicons.org/x\" alt=\"X logo\" width=\"25\" height=\"15\"> and open-source models like <img src=\"https://cdn.simpleicons.org/alibabacloud\" alt=\"alibaba logo\" width=\"25\" height=\"15\"> or <img src=\"https://cdn.simpleicons.org/meta\" alt=\"meta logo\" width=\"25\" height=\"15\"> that you can run locally on your computer.</p><ul><li>💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.</li><li>🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.</li><li>🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.</li></ul><h3>🎮 Autonomous Game Playing Agents</h3><h3>📀 RAG (Retrieval Augmented Generation)</h3><h3>💾 LLM Apps with Memory Tutorials</h3><h3>🔧 LLM Fine-tuning Tutorials</h3><h3>🧑‍🏫 AI Agent Framework Crash Course</h3><ul><li>Starter agent; model‑agnostic (OpenAI, Claude)</li><li>Structured outputs (Pydantic)</li><li>Tools: built‑in, function, third‑party, MCP tools</li><li>Memory; callbacks; Plugins</li><li>Simple multi‑agent; Multi‑agent patterns</li></ul><ul><li>Starter agent; function calling; structured outputs</li><li>Tools: built‑in, function, third‑party integrations</li><li>Memory; callbacks; evaluation</li><li>Multi‑agent patterns; agent handoffs</li><li>Swarm orchestration; routing logic</li></ul><ol><li><pre><code>git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n</code></pre></li><li><p><strong>Navigate to the desired project directory</strong></p><pre><code>cd awesome-llm-apps/starter_ai_agents/ai_travel_agent\n</code></pre></li><li><p><strong>Install the required dependencies</strong></p><pre><code>pip install -r requirements.txt\n</code></pre></li><li><p><strong>Follow the project-specific instructions</strong> in each project's  file to set up and run the app.</p></li></ol><h3><img src=\"https://cdn.simpleicons.org/github\" alt=\"github logo\" width=\"25\" height=\"20\"> Thank You, Community, for the Support! 🙏</h3><p>🌟 <strong>Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.</strong></p>","contentLength":1845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"public-apis/public-apis","url":"https://github.com/public-apis/public-apis","date":1761446105,"author":"","guid":315154,"unread":true,"content":"<p>A collective list of free APIs</p><p>The Public APIs repository is manually curated by community members like you and folks working at <a href=\"https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo\">APILayer</a>. It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.</p><a href=\"https://apilayer.com\"></a><p>APILayer is the fastest way to integrate APIs into any product. Explore <a href=\"https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo\">APILayer APIs</a> here for your next project.</p><p>Join our <a href=\"https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo\">Discord server</a> to get updates, ask questions, get answers, random community calls, and more.</p><h2>Learn more about Public APIs</h2><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Resource to help get pets adopted</td></tr><tr><td align=\"left\">Collection of axolotl pictures and facts</td></tr><tr><td align=\"left\">Cat as a service (cats pictures and gifs)</td></tr><tr><td align=\"left\">Pictures of cats from Tumblr</td></tr><tr><td align=\"left\">Based on the Stanford Dogs Dataset</td></tr><tr><td align=\"left\">Retrieve recent or notable birding observations within a region</td></tr><tr><td align=\"left\">Information and pictures about individual fish species</td></tr><tr><td align=\"left\">Cat for every HTTP Status</td></tr><tr><td align=\"left\">Dogs for every HTTP response status code</td></tr><tr><td align=\"left\">IUCN Red List of Threatened Species</td></tr><tr><td align=\"left\">Movement and Migration data of animals</td></tr><tr><td align=\"left\">Petfinder is dedicated to helping pets find homes, another resource to get pets adopted</td></tr><tr></tr><tr><td align=\"left\">Random pictures of Shiba Inu, cats or birds</td></tr><tr><td align=\"left\">A public service all about Dogs, free to use when making your fancy new App, Website or Service</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Anime discovery, streaming &amp; syncing with trackers</td></tr><tr></tr><tr><td align=\"left\">Anime discovery &amp; tracking</td></tr><tr><td align=\"left\">Neko images, funny GIFs &amp; more</td></tr><tr><td align=\"left\">Thousands of anime artist database to find good anime art</td></tr><tr><td align=\"left\">Unofficial MyAnimeList API</td></tr><tr></tr><tr><td align=\"left\">Manga Database and Community</td></tr><tr><td align=\"left\">Translate manga pages from one language to another</td></tr><tr><td align=\"left\">Anime and Manga Database and Community</td></tr><tr><td align=\"left\">Neko Images &amp; Anime roleplaying GIFs</td></tr><tr><td align=\"left\">Anime discovery, tracking, forum, rates</td></tr><tr><td align=\"left\">A useful tool to get the exact scene of an anime from a screenshot</td></tr><tr><td align=\"left\">Get waifu pictures from an archive of over 4000 images and multiple tags</td></tr><tr><td align=\"left\">Image sharing platform for anime images</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr><tr><td align=\"left\">Malware execution and analysis</td></tr><tr><td align=\"left\">Provide malware datasets and threat intelligence feeds</td></tr><tr><td align=\"left\">Malware Archive / file sourcing</td></tr><tr></tr><tr><td align=\"left\">Check links to see if they're known phishing attempts</td></tr><tr></tr><tr><td align=\"left\">Simple REST API that can scan submitted documents/files for the presence of threats</td></tr><tr><td align=\"left\">Bulk queries and Download Malware Samples</td></tr><tr><td align=\"left\">VirusTotal File/URL Analysis</td></tr></tbody></table><h3>Authentication &amp; Authorization</h3><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Easy to implement, adaptable authentication and authorization platform</td></tr><tr><td align=\"left\">Implement OTP flow quickly</td></tr><tr><td align=\"left\">Secure and modern passwordless authentication platform</td></tr><tr><td align=\"left\">Simplify login and improve user experience by integrating passwordless authentication in your app</td></tr><tr><td align=\"left\">User infrastructure for modern applications</td></tr><tr><td align=\"left\">APIs for authorization and access control</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Onchain GraphQL APIs &amp; DEX APIs</td></tr><tr><td align=\"left\">Build hybrid smart contracts with Chainlink</td></tr><tr><td align=\"left\">Chainpoint is a global network for anchoring data to the Bitcoin blockchain</td></tr><tr><td align=\"left\">Multi-blockchain data aggregator platform</td></tr><tr><td align=\"left\">Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage</td></tr><tr><td align=\"left\">Blockchain-as-a-service solution that provides high-quality connection via API</td></tr><tr><td align=\"left\">Blockchain-based blogging and social media website</td></tr><tr><td align=\"left\">Indexing protocol for querying networks like Ethereum with GraphQL</td></tr><tr><td align=\"left\">To retrieve Walltime's market info</td></tr><tr><td align=\"left\">Provide simple and reliable API access to Ethereum blockchain</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Do not worry about managing the multiple versions of the Bible</td></tr><tr><td align=\"left\">Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi</td></tr><tr><td align=\"left\">Free Bible API with multiple languages</td></tr><tr><td align=\"left\">Classic Persian poetry works including access to related manuscripts, recitations and music tracks</td></tr><tr><td align=\"left\">Fast and Accurate Gurbani RESTful API</td></tr><tr><td align=\"left\">Web-API for fetching data from Project Gutenberg Books Library</td></tr><tr><td align=\"left\">Enables you to get instant data from our vast poetry collection</td></tr><tr><td align=\"left\">RESTful Quran API with multiple languages</td></tr><tr><td align=\"left\">A RESTful Quran API to retrieve an Ayah, Surah, Juz or the entire Holy Quran</td></tr><tr><td align=\"left\">Free Quran API Service with 90+ different languages and 400+ translations</td></tr><tr><td align=\"left\">Gods and poets, their categories, and the verse meters, with the mandal and sukta number</td></tr><tr><td align=\"left\">Everything you need from the Bible in one discoverable place</td></tr><tr><td align=\"left\">1330 Thirukkural poems and explanation in Tamil and English</td></tr><tr><td align=\"left\">Descriptions of all nouns (names, places, animals, things) from vedic literature</td></tr><tr><td align=\"left\">Get information from the Harry Potter universe</td></tr><tr><td align=\"left\">API for obtaining information about e-books available on the WolneLektury.pl website</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">API to manage your BI dashboards and data sources on Superset</td></tr><tr><td align=\"left\">Search for company logos and embed them in your projects</td></tr><tr><td align=\"left\">Hire freelancers to get work done</td></tr><tr><td align=\"left\">Flexible, RESTful access to the user's inbox</td></tr><tr><td align=\"left\">Collect, configure and analyze your data to reach the right audience</td></tr><tr><td align=\"left\">Post to and update maintenance and incidents on your status page through an HTTP REST API</td></tr><tr><td align=\"left\">Send marketing campaigns and transactional mails</td></tr><tr><td align=\"left\">Marketing email can be sent and mail templates made in MJML or HTML can be sent using API</td></tr><tr><td align=\"left\">Access your queries and dashboards on Redash</td></tr><tr><td align=\"left\">Allows you to programmatically access and Smartsheet data and account information</td></tr><tr><td align=\"left\">Easy way to take payments, manage refunds, and help customers checkout online</td></tr><tr><td align=\"left\">Kanban software, Visualize Work, Increase Organizations Lead Time, Throughput &amp; Productivity</td></tr><tr><td align=\"left\">Email Finder for B2B sales and email marketing and email verifier</td></tr><tr><td align=\"left\">Boards, lists and cards to help you organize and prioritize your projects</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Data on national, regional, and religious holidays via API</td></tr><tr><td align=\"left\">Industry-leading Holiday API. Over 5,000 holidays and thousands of descriptions. Trusted by the World’s leading companies</td></tr><tr><td align=\"left\">Display, create and modify Google calendar events</td></tr><tr><td align=\"left\">Convert between Gregorian and Hebrew, fetch Shabbat and Holiday times, etc</td></tr><tr><td align=\"left\">Historical data regarding holidays</td></tr><tr><td align=\"left\">Protestant liturgical calendar</td></tr><tr><td align=\"left\">Public holidays for more than 90 countries</td></tr><tr><td align=\"left\">Simple REST API for checking working, non-working or short days for Russia, CIS, USA and other</td></tr><tr><td align=\"left\">Bank holidays in England and Wales, Scotland and Northern Ireland</td></tr></tbody></table><h3>Cloud Storage &amp; File Sharing</h3><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Upload and share your files anonymously</td></tr><tr><td align=\"left\">Upload and share your files</td></tr><tr></tr><tr></tr><tr></tr><tr><td align=\"left\">Super simple file sharing, convenient, anonymous and secure</td></tr><tr><td align=\"left\">Filestack File Uploader &amp; File Upload API</td></tr><tr><td align=\"left\">Unlimited size file uploads for free</td></tr><tr><td align=\"left\">Save &amp; Share screen captures instantly</td></tr><tr><td align=\"left\">Simple and quick private image sharing</td></tr><tr></tr><tr><td align=\"left\">Free JSON storage for small projects</td></tr><tr></tr><tr><td align=\"left\">IPFS Pinning Services API</td></tr><tr><td align=\"left\">File Sharing and Storage for groups</td></tr><tr><td align=\"left\">Decentralized Open-Source Cloud Storage</td></tr><tr><td align=\"left\">File Sharing and Storage for Free with 1TB Space</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Resource health helps you diagnose and get support when an Azure issue impacts your resources</td></tr><tr><td align=\"left\">Build tool and processes integrations to create efficient development pipelines</td></tr><tr><td align=\"left\">The fastest continuous integration and continuous delivery platform</td></tr><tr><td align=\"left\">Automate the software development process using continuous integration and continuous delivery</td></tr><tr><td align=\"left\">Codeship is a Continuous Integration Platform in the cloud</td></tr><tr><td align=\"left\">Sync your GitHub projects with Travis CI to test your code in minutes</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">API for querying token and pool stats across various liquidity pools</td></tr><tr><td align=\"left\">API for querying decentralize exchange</td></tr><tr><td align=\"left\">Exchange for Trading Cryptocurrencies based in China</td></tr><tr><td align=\"left\">Get the list of all traded assets in the exchange</td></tr><tr><td align=\"left\">Digital Asset Price Data for the blockchain industry</td></tr><tr><td align=\"left\">Financial and Technical Data related to the Bitcoin Network</td></tr><tr><td align=\"left\">Cryptocurrency Trading Platform</td></tr><tr><td align=\"left\">Real-Time Cryptocurrency derivatives trading platform based in Hong Kong</td></tr><tr><td align=\"left\">Next Generation Crypto Trading Platform</td></tr><tr><td align=\"left\">Bitcoin Payment, Wallet &amp; Transaction Data</td></tr><tr><td align=\"left\">Bitcoin Payment, Wallet &amp; Transaction Data</td></tr><tr><td align=\"left\">Interaction with the Cardano mainnet and several testnets</td></tr><tr><td align=\"left\">Real-time and historic crypto data from more than 200+ exchanges</td></tr><tr><td align=\"left\">Real-time cryptocurrency data, graphs and API that allows buy&amp;sell</td></tr><tr><td align=\"left\">Cryptocurrency data feed and algorithmic trading</td></tr><tr><td align=\"left\">All Currency Exchanges integrate under a single api</td></tr><tr><td align=\"left\">Bitcoin, Bitcoin Cash, Litecoin and Ethereum Prices</td></tr><tr><td align=\"left\">Real time Cryptocurrency prices through a RESTful API</td></tr><tr><td align=\"left\">Cryptocurrency Trading Platform</td></tr><tr><td align=\"left\">CoinDesk's Bitcoin Price Index (BPI) in multiple currencies</td></tr><tr><td align=\"left\">Cryptocurrency Price, Market, and Developer/Social Data</td></tr><tr><td align=\"left\">Interacting with Coinigy Accounts and Exchange Directly</td></tr><tr></tr><tr><td align=\"left\">Cryptocurrencies prices, volume and more</td></tr><tr><td align=\"left\">Cryptocurrencies prices, volume and more</td></tr><tr><td align=\"left\">Cryptocurrencies Payment &amp; Prices</td></tr><tr><td align=\"left\">Cryptocurrency Payment Processor</td></tr><tr><td align=\"left\">Decentralized cryptocurrency exchange</td></tr><tr><td align=\"left\">Ethereum tokens, balances, addresses, history of transactions, contracts, and custom structures</td></tr><tr><td align=\"left\">Cryptocurrencies exchange based in UK</td></tr><tr><td align=\"left\">Complete REST, websocket, and FTX APIs to suit your algorithmic trading needs</td></tr><tr><td align=\"left\">API provides spot, margin and futures trading operations</td></tr><tr><td align=\"left\">Cryptocurrencies Exchange</td></tr><tr><td align=\"left\">Exchange rates between 162 currency &amp; 300 crypto currency update each 5 min, accurate, no limits</td></tr><tr><td align=\"left\">Seychelles based cryptocurrency exchange</td></tr><tr></tr><tr><td align=\"left\">Trade your Bitcoin and other assets with rupiah</td></tr><tr><td align=\"left\">Interaction with the Ethereum mainnet and several testnets</td></tr><tr><td align=\"left\">Cryptocurrencies Exchange</td></tr><tr><td align=\"left\">Cryptocurrency Trading Platform</td></tr><tr><td align=\"left\">Bitcoin API Service focusing on the transaction fee</td></tr><tr><td align=\"left\">Provides API endpoints for thousands of crypto assets</td></tr><tr><td align=\"left\">Automated cryptocurrency exchange service</td></tr><tr><td align=\"left\">Historical and realtime cryptocurrency prices and market data</td></tr><tr><td align=\"left\">NovaDAX API to access all market data, trading management endpoints</td></tr><tr><td align=\"left\">Cryptocurrency exchange based in Seychelles</td></tr><tr><td align=\"left\">US based digital asset exchange</td></tr><tr><td align=\"left\">Provides various endpoints to interact with the Solana Blockchain</td></tr><tr><td align=\"left\">Cryptocurrency Exchange based in South Africa</td></tr><tr><td align=\"left\">Ethereum JSON RPC API and Web3 provider</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Forex currency market data</td></tr><tr><td align=\"left\">Free currency API with over 150 currencies</td></tr><tr><td align=\"left\">Free Currency Exchange Rates API with 150+ Currencies &amp; No Rate Limits</td></tr><tr><td align=\"left\">Provides current and historical currency exchange rates with free plan 1K requests/month</td></tr><tr><td align=\"left\">Exchange rates and currency conversion</td></tr><tr><td align=\"left\">Real-time and historical currency rates JSON API</td></tr><tr><td align=\"left\">Portuguese free currency prices and conversion with no rate limits</td></tr><tr><td align=\"left\">Exchange rates, currency conversion and time series</td></tr><tr><td align=\"left\">Real-time foreign exchange rates for major currency pairs</td></tr><tr><td align=\"left\">Exchange rates, geolocation and VAT number validation</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr><tr><td align=\"left\">Test api server to receive and return value from HTTP method</td></tr><tr><td align=\"left\">Content validator against profanity &amp; obscenity</td></tr><tr><td align=\"left\">Enter address data quickly with real-time address suggestions</td></tr><tr><td align=\"left\">Extract postal addresses from any text including emails</td></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Project to promote open source collaboration during December</td></tr><tr><td align=\"left\">Take programmatic screenshots of web pages from any website</td></tr><tr><td align=\"left\">Estimates the age from a first name</td></tr><tr><td align=\"left\">Multiples services and public APIs</td></tr><tr><td align=\"left\">Extract device details from user-agent string</td></tr><tr><td align=\"left\">Chrome based screenshot API for developers</td></tr><tr><td align=\"left\">Wikipedia for Web APIs, OpenAPI/Swagger specs for public APIs</td></tr><tr><td align=\"left\">The Azure DevOps basic components of a REST API request/response pair</td></tr><tr></tr><tr><td align=\"left\">Build a mock Rest API endpoint in seconds</td></tr><tr><td align=\"left\">La plus grande API de Blagues FR/The biggest FR jokes API</td></tr><tr><td align=\"left\">Schedule screenshots of web pages and sync them to your cloud</td></tr><tr><td align=\"left\">Control IoT Devices from Blynk IoT Cloud</td></tr><tr><td align=\"left\">Find random activities to fight boredom</td></tr><tr><td align=\"left\">Easily make screenshots of web pages in any screen size, as any device</td></tr><tr></tr><tr><td align=\"left\">Structured changelog metadata from open source projects</td></tr><tr><td align=\"left\">Secure random string generator</td></tr><tr><td align=\"left\">Get IP Address, Timestamp, User Agent, Country Code, IATA, HTTP Version, TLS/SSL Version &amp; More</td></tr><tr><td align=\"left\">Online Compiler for Various Languages</td></tr><tr><td align=\"left\">Get around the dreaded CORS error by using this proxy as a middle man</td></tr><tr><td align=\"left\">Free and simple counting service. You can use it to track page hits and specific events</td></tr><tr><td align=\"left\">Service to manage your databricks account,clusters, notebooks, jobs and workspaces</td></tr><tr><td align=\"left\">Domain name search to find all domains containing particular words/phrases/etc</td></tr><tr><td align=\"left\">Provide numerous capabilities for important testing and monitoring methods for websites</td></tr><tr><td align=\"left\">Trigger an email notification with a simple GET request</td></tr><tr><td align=\"left\">Get Published content into your Website, App or other embedded media</td></tr><tr><td align=\"left\">Make use of GitHub repositories, code and user info programmatically</td></tr><tr><td align=\"left\">Automate GitLab interaction programmatically</td></tr><tr></tr><tr><td align=\"left\">API to read, write, and format Google Docs documents</td></tr><tr><td align=\"left\">Google's mobile application development platform that helps build, improve, and grow app</td></tr><tr><td align=\"left\">Metadata for all families served by Google Fonts</td></tr><tr><td align=\"left\">API to read, write, and format Google Keep notes</td></tr><tr><td align=\"left\">API to read, write, and format Google Sheets data</td></tr><tr><td align=\"left\">API to read, write, and format Google Slides presentations</td></tr><tr><td align=\"left\">Online REST API for Testing and Prototyping</td></tr><tr><td align=\"left\">GraphQL and REST API Engine with built in Authorization</td></tr><tr><td align=\"left\">REST API to programmatically create apps, provision add-ons and perform other task on Heroku</td></tr><tr><td align=\"left\">Basic DNS query via HTTP GET request</td></tr><tr><td align=\"left\">Domains Data API for Developers</td></tr><tr><td align=\"left\">Test endpoints for client and server HTTP/2 protocol support</td></tr><tr><td align=\"left\">A Simple HTTP Request &amp; Response Service</td></tr><tr><td align=\"left\">A Simple HTTP Request &amp; Response Service with HTTP/3 Support by Cloudflare</td></tr><tr><td align=\"left\">API for domain search, professional email finder, author finder and email verifier</td></tr><tr></tr><tr><td align=\"left\">Generate charts, QR codes and graph images</td></tr><tr><td align=\"left\">Retrieve structured data from a website or RSS feed</td></tr><tr><td align=\"left\">Geographic location of an IP address or any domain name along with some other useful information</td></tr><tr></tr><tr><td align=\"left\">Another simple IP Address API</td></tr><tr><td align=\"left\">Package info and download stats on jsDelivr CDN</td></tr><tr><td align=\"left\">Convert JSON to JSONP (on-the-fly) for easy cross-domain data requests using client-side JavaScript</td></tr><tr><td align=\"left\">Free JSON storage service. Ideal for small scale Web apps, Websites and Mobile apps</td></tr><tr><td align=\"left\">Creates diagrams from textual descriptions</td></tr><tr><td align=\"left\">Unofficial REST API for choosealicense.com</td></tr><tr></tr><tr></tr><tr><td align=\"left\">Fake Rest API for developers</td></tr><tr><td align=\"left\">Mock user defined test JSON for REST API endpoints</td></tr><tr><td align=\"left\">Get IP address information</td></tr><tr><td align=\"left\">Netlify is a hosting service for the programmable web</td></tr><tr><td align=\"left\">Network calculators, including subnets, DNS, binary, and security tools</td></tr><tr><td align=\"left\">Query information about your favorite Node.js libraries programatically</td></tr><tr><td align=\"left\">Self-serve customer engagement solution for Push Notifications, Email, SMS &amp; In-App</td></tr><tr><td align=\"left\">API for calculating and comparing metrics of different websites using Page Rank algorithm</td></tr><tr><td align=\"left\">The All-in-one API Platform</td></tr><tr><td align=\"left\">Really simple API to retrieve Open Graph data from an URL</td></tr><tr><td align=\"left\">API for Fake Data, image/video conversion, optimization, pdf optimization and thumbnail generation</td></tr><tr><td align=\"left\">Public API for javascript, css and font libraries on PageCDN</td></tr><tr></tr><tr><td align=\"left\">Scraping and crawling anticaptcha service</td></tr><tr><td align=\"left\">Rotating Proxy API that produces a working proxy on every request</td></tr><tr><td align=\"left\">Push notifications for Android &amp; iOS</td></tr><tr><td align=\"left\">Create an easy to read QR code and URL shortener</td></tr><tr><td align=\"left\">Generate and decode / read QR code graphics</td></tr><tr><td align=\"left\">Integrate custom and unique looking QR codes into your system or workflow</td></tr><tr><td align=\"left\">Can be used to get AI Response, jokes, memes, and much more at lightning-fast speed</td></tr><tr><td align=\"left\">Reverse AJAX service to notify clients</td></tr><tr><td align=\"left\">A hosted REST-API ready to respond to your AJAX requests</td></tr><tr><td align=\"left\">A free, RESTful API used to screenshot any desktop, or mobile website</td></tr><tr><td align=\"left\">Scraping API with Chrome fingerprint and residential proxies</td></tr><tr><td align=\"left\">Easily build scalable web scrapers</td></tr><tr><td align=\"left\">Undetectable web scraping API</td></tr><tr><td align=\"left\">Real-time, Scalable Proxy &amp; Web Scraping REST API</td></tr><tr><td align=\"left\">Headless Chrome scraping with a simple API</td></tr><tr><td align=\"left\">Color conversion, complementary, grayscale and contrasted text</td></tr><tr><td align=\"left\">Real-Time &amp; Accurate Google Search Results API</td></tr><tr><td align=\"left\">Easy google sheets integration</td></tr><tr><td align=\"left\">Project Sonar DNS Enumeration API</td></tr><tr><td align=\"left\">SonarQube REST APIs to detect bugs, code smells &amp; security vulnerabilities</td></tr><tr><td align=\"left\">Api and service management platform</td></tr><tr><td align=\"left\">Code compiler supporting 35+ languages mentioned at wandbox.org</td></tr><tr><td align=\"left\">Web Scraping API with built-in proxies and JS rendering</td></tr><tr><td align=\"left\">Web Scraping API that bypasses anti-bot solutions while offering JS rendering, and rotating proxies</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Bilingual Dictionary and Thesaurus Data</td></tr><tr><td align=\"left\">Definitions, phonetics, pronounciations, parts of speech, examples, synonyms</td></tr><tr><td align=\"left\">Word definitions, pronunciations, synonyms, antonyms and others</td></tr><tr><td align=\"left\">Definitions with example sentence and photo if available</td></tr><tr></tr><tr><td align=\"left\">Synonyms, thesaurus and antonyms information for any given word</td></tr><tr></tr><tr><td align=\"left\">Definitions and synonyms for more than 150,000 words</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr><tr><td align=\"left\">Programmatic access to all data in your asana system</td></tr><tr><td align=\"left\">ClickUp is a robust, cloud-based project management tool for boosting productivity</td></tr><tr><td align=\"left\">Clockify's REST-based API can be used to push/pull data to/from it &amp; integrate it with other systems</td></tr><tr><td align=\"left\">Online file converter for audio, video, document, ebook, archive, image, spreadsheet, presentation</td></tr><tr><td align=\"left\">Automatic time tracking for programmers</td></tr><tr><td align=\"left\">Generate PDF documents from templates with a drop-and-drop editor and a simple API</td></tr><tr><td align=\"left\">Automate business workflows</td></tr><tr><td align=\"left\">Convert, merge, split, extract text and add page numbers for PDFs. Free for 250 documents/month</td></tr><tr><td align=\"left\">JIRA is a proprietary issue tracking product that allows bug tracking and agile project management</td></tr><tr><td align=\"left\">An open source platform for developer collaboration</td></tr><tr><td align=\"left\">Programmatically access and update data inside a monday.com account</td></tr><tr></tr><tr><td align=\"left\">DocGen and eSignatures API</td></tr><tr></tr><tr><td align=\"left\">File sharing and productivity</td></tr><tr><td align=\"left\">Data from XML or JSON to PDF, HTML or Image</td></tr><tr><td align=\"left\">Provides screenshot, HTML to PDF and content extraction APIs</td></tr><tr><td align=\"left\">Performs image upscaling by adding detail to images through multiple super-resolution algorithms</td></tr><tr><td align=\"left\">Automated time tracking leaderboards for programmers</td></tr><tr><td align=\"left\">Full stack project management</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Validate email addresses for deliverability and spam</td></tr><tr><td align=\"left\">Validate and detect disposable and temporary email addresses</td></tr><tr><td align=\"left\">GraphQL API for creating and managing ephemeral e-mail inboxes</td></tr><tr></tr><tr><td align=\"left\">API for free email forwarding service</td></tr><tr></tr><tr></tr><tr><td align=\"left\">Validate email address to improve deliverability</td></tr><tr><td align=\"left\">Prevent users to sign up with temporary email addresses</td></tr><tr><td align=\"left\">A service for the safe testing of emails sent from the development and staging environments</td></tr><tr><td align=\"left\">A cloud-based SMTP provider that allows you to send emails without having to maintain email servers</td></tr><tr><td align=\"left\">A service that provides solutions relating to marketing and/or transactional email and/or SMS</td></tr><tr><td align=\"left\">Verifies that a given email is real</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">JSON API for hand curated Chuck Norris jokes</td></tr><tr><td align=\"left\">Get random excuses for various situations</td></tr><tr><td align=\"left\">A simple HTTPS api that can randomly select and return a fact from the FFA database</td></tr><tr><td align=\"left\">Gets an array of popular memes</td></tr><tr><td align=\"left\">REST API for create your own meme</td></tr><tr><td align=\"left\">JSON and Plaintext API for tech-savvy sounding phrases</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Daily Forecast pollen conditions data for a specific location</td></tr><tr><td align=\"left\">API to calculate carbon (C02) emissions estimates for common C02 emitting activities</td></tr><tr><td align=\"left\">Calculate the environmental footprint created by a broad range of emission-generating activities</td></tr><tr><td align=\"left\">API calculates the impact of common carbon-intensive activities in real time</td></tr><tr><td align=\"left\">API calculates and validates the carbon footprint</td></tr><tr><td align=\"left\">Green Power Index for Germany (Grünstromindex/GSI)</td></tr><tr><td align=\"left\">Air quality and weather data</td></tr><tr><td align=\"left\">Predicted and actual air quality components for The Netherlands (RIVM)</td></tr><tr><td align=\"left\">Open data from Great Britain’s Electricity System Operator</td></tr><tr></tr><tr></tr><tr><td align=\"left\">Energy production photovoltaic (PV) energy systems</td></tr><tr><td align=\"left\">Hourly usage energy report for Srp customers</td></tr><tr><td align=\"left\">The Official Carbon Intensity API for Great Britain developed by National Grid</td></tr><tr><td align=\"left\">API to estimate the carbon footprint of loading web pages</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Search events, venues and performers</td></tr><tr><td align=\"left\">Search events, attractions, or venues</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Validate VAT numbers and calculate VAT rates</td></tr><tr><td align=\"left\">Insider trading data, earnings call analysis, financial statements, and more</td></tr><tr><td align=\"left\">Realtime and historical market data on all US equities and ETFs</td></tr><tr><td align=\"left\">Instant IBAN and SWIFT number validation across the globe</td></tr><tr></tr><tr><td align=\"left\">Public access to a database of IIN/BIN information</td></tr><tr><td align=\"left\">A api to generate boletos in Brazil</td></tr><tr><td align=\"left\">All Citigroup account and statement data APIs</td></tr><tr><td align=\"left\">Global macroeconomic data</td></tr><tr><td align=\"left\">U.S. Department of the Treasury Data</td></tr><tr><td align=\"left\">Finage is a stock, currency, cryptocurrency, indices, and ETFs real-time &amp; historical data provider</td></tr><tr><td align=\"left\">Real-Time RESTful APIs and Websocket for Stocks, Currencies, and Crypto</td></tr><tr><td align=\"left\">Economic data from the Federal Reserve Bank of St. Louis</td></tr><tr><td align=\"left\">Front accounting is multilingual and multicurrency software for small businesses</td></tr><tr><td align=\"left\">Stock market data powered by SQL</td></tr><tr><td align=\"left\">Realtime &amp; Historical Stock and Market Data</td></tr><tr><td align=\"left\">Spreadbetting and CFD Market Data</td></tr><tr><td align=\"left\">A wide selection of financial data feeds</td></tr><tr><td align=\"left\">Klarna payment and shopping service</td></tr><tr><td align=\"left\">Mercado Pago API reference - all the information you need to develop your integrations</td></tr><tr><td align=\"left\">Connect with users’ bank accounts and access transaction data in Africa</td></tr><tr><td align=\"left\">The Moov API makes it simple for platforms to send, receive, and store money</td></tr><tr><td align=\"left\">Connect to bank accounts using official bank APIs and get raw transaction data</td></tr><tr><td align=\"left\">Equity, index, futures, options symbology from Bloomberg LP</td></tr><tr><td align=\"left\">Connect with user's bank accounts and access transaction data</td></tr><tr><td align=\"left\">Historical stock market data</td></tr><tr><td align=\"left\">Indian Financial Systems Code (Bank Branch Codes)</td></tr><tr><td align=\"left\">API to access annual reports of public US companies</td></tr><tr><td align=\"left\">Gain access to set of \n    </td></tr><tr><td align=\"left\">Real-Time, Intraday &amp; Historical Market Data, News and Sentiment API</td></tr><tr><td align=\"left\">Realtime and historical stock data and current stock sentiment</td></tr><tr><td align=\"left\">Instant VAT number and tax validation across the globe</td></tr><tr><td align=\"left\">US equity/option market data (delayed, intraday, historical)</td></tr><tr><td align=\"left\">Stock market data (real-time &amp; historical)</td></tr><tr><td align=\"left\">WallstreetBets Stock Comments Sentiment Analysis</td></tr><tr><td align=\"left\">Real time low latency Yahoo Finance API for stock market, crypto currencies, and currency exchange</td></tr><tr></tr><tr><td align=\"left\">Online accounting software, built for your business</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Provides information about Game, DLC, Bundles, Giveaways, Trading</td></tr><tr><td align=\"left\">Diablo III, Hearthstone, StarCraft II and World of Warcraft game data APIs</td></tr><tr><td align=\"left\">Brawl Stars Game Information</td></tr><tr><td align=\"left\">Get information about Bugsnax</td></tr><tr><td align=\"left\">Chess.com read-only REST API</td></tr><tr><td align=\"left\">API for Minecraft skins and faces</td></tr><tr><td align=\"left\">Search for Digimon cards in digimoncard.io</td></tr><tr><td align=\"left\">Information of Disney characters</td></tr><tr><td align=\"left\">Provides information about Player stats , Match stats, Rankings for Dota 2</td></tr><tr><td align=\"left\">Third-Party Developer Documentation</td></tr><tr><td align=\"left\">Show random image of car from Forza</td></tr><tr><td align=\"left\">Easy way to use the Geometry Dash Servers</td></tr><tr><td align=\"left\">Fetch a random geeky/programming related joke for use in all sorts of applications</td></tr><tr><td align=\"left\">GraphQL powered Pokemon API. Supports generations 1 through 8</td></tr><tr><td align=\"left\">GW2Spidy API, Items data on the Guild Wars 2 Trade Market</td></tr><tr><td align=\"left\">Halo 5 and Halo Wars 2 Information</td></tr><tr><td align=\"left\">Hearthstone Cards Information</td></tr><tr></tr><tr></tr><tr><td align=\"left\">Data on all interactive items from The Legend of Zelda: BOTW</td></tr><tr><td align=\"left\">Hytale blog posts and jobs</td></tr><tr></tr><tr><td align=\"left\">Programming, Miscellaneous and Dark Jokes</td></tr><tr><td align=\"left\">Joke of the day and large category of jokes accessible via REST API</td></tr><tr><td align=\"left\">Jeopardy Question Database</td></tr><tr><td align=\"left\">Access to all data of users, games, puzzles and etc on Lichess</td></tr><tr></tr><tr><td align=\"left\">MMO Games Database, News and Giveaways</td></tr><tr></tr><tr></tr><tr><td align=\"left\">E-sports games and results</td></tr><tr><td align=\"left\">Query Minecraft, Steam and XBox Accounts</td></tr><tr></tr><tr><td align=\"left\">Psychonauts World Characters Information and PSI Powers</td></tr><tr></tr><tr><td align=\"left\">Puyo Puyo information from Puyo Nexus Wiki</td></tr><tr><td align=\"left\">Access to various kind of quiz questions</td></tr><tr><td align=\"left\">Provides detailed character and guild rankings for Raiding and Mythic+ content in World of Warcraft</td></tr><tr><td align=\"left\">500,000+ games for 50 platforms including mobiles</td></tr><tr><td align=\"left\">All the Rick and Morty information, including images</td></tr><tr><td align=\"left\">League of Legends Game Information</td></tr><tr><td align=\"left\">Rock, Paper, Scissors with 101 objects</td></tr><tr><td align=\"left\">RuneScape and OSRS RPGs information</td></tr><tr><td align=\"left\">Magic: The Gathering database</td></tr><tr><td align=\"left\">Steam Web API documentation</td></tr><tr><td align=\"left\">Internal Steam Web API documentation</td></tr><tr><td align=\"left\">All SuperHeroes and Villains data from all universes under a single API</td></tr><tr><td align=\"left\">Multi languages Pokémon TCG Information</td></tr><tr><td align=\"left\">Tebex API for information about game purchases</td></tr><tr><td align=\"left\">TETR.IO Tetra Channel API</td></tr><tr><td align=\"left\">The dumbest things Donald Trump has ever said</td></tr><tr><td align=\"left\">An extensive API containing data of most Valorant in-game items, assets and more</td></tr><tr><td align=\"left\">Retrieve xkcd comics as JSON</td></tr><tr><td align=\"left\">Yu-Gi-Oh! TCG Information</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Actinia is an open source REST API for geographical data that uses GRASS GIS</td></tr><tr><td align=\"left\">IP Geolocation API. Collecting data from multiple sources</td></tr><tr><td align=\"left\">Get location information by IP address</td></tr><tr><td align=\"left\">Locate and identify website visitors by IP address</td></tr><tr><td align=\"left\">A (country/region/city) in-cascade location API</td></tr><tr><td align=\"left\">Provides fast and accurate IP geolocation APIs along with security checks and confidence area</td></tr><tr><td align=\"left\">Create/customize digital maps based on Bing Maps data</td></tr><tr><td align=\"left\">Convert British OSGB36 easting and northing (British National Grid) to WGS84 latitude and longitude</td></tr><tr><td align=\"left\">Create maps and markers for anything</td></tr><tr><td align=\"left\">Brazil RESTful API to find information about streets, zip codes, neighborhoods, cities and states</td></tr><tr><td align=\"left\">Open APIs for select European cities</td></tr><tr><td align=\"left\">Get your visitor's country from their IP</td></tr><tr><td align=\"left\">World countries, states, regions, provinces, cities &amp; towns in JSON, SQL, XML, YAML, &amp; CSV format</td></tr><tr><td align=\"left\">API explorer that gives a query URL with a JSON response of locations and cities</td></tr><tr></tr><tr><td align=\"left\">Forward and reverse geocoding, address autocomplete</td></tr><tr><td align=\"left\">Address geocoding / reverse geocoding in bulk</td></tr><tr><td align=\"left\">Provides worldwide forward/reverse geocoding, batch geocoding and geoparsing</td></tr><tr><td align=\"left\">Worldwide geocoding, geoparsing and autocomplete for addresses</td></tr><tr><td align=\"left\">Open geospatial data and API service for Greece</td></tr><tr><td align=\"left\">Geocoding of city name by using latitude and longitude coordinates</td></tr><tr><td align=\"left\">Get global city, region, and country data</td></tr><tr><td align=\"left\">A Country, State, and City GraphQL API</td></tr><tr><td align=\"left\">IP geolocation with ChatOps integration</td></tr><tr><td align=\"left\">Geokeo geocoding service- with 2500 free api requests daily</td></tr><tr><td align=\"left\">Place names and other geographical data</td></tr><tr><td align=\"left\">IP geolocation and currency conversion</td></tr><tr><td align=\"left\">A cloud-based platform for planetary-scale environmental data analysis</td></tr><tr><td align=\"left\">Create/customize digital maps based on Google Maps data</td></tr><tr><td align=\"left\">Country-related data like currencies, languages, flags, regions+subregions and bordering countries</td></tr><tr><td align=\"left\">Get hello translation following user language</td></tr><tr><td align=\"left\">Create/customize digital maps based on HERE Maps data</td></tr><tr><td align=\"left\">Ip to location with country code, currency code &amp; currency name, fast response, unlimited requests</td></tr><tr><td align=\"left\">Aggregate services of IBGE (Brazilian Institute of Geography and Statistics)</td></tr><tr><td align=\"left\">Find location with IP address or domain</td></tr><tr><td align=\"left\">IP geolocation web service to get more than 55 parameters</td></tr><tr><td align=\"left\">Detect proxy and VPN using IP address</td></tr><tr><td align=\"left\">Find IP address location information</td></tr><tr><td align=\"left\">Real-time Geolocation &amp; Reverse IP Lookup REST API</td></tr><tr><td align=\"left\">Unlimited free IP Address API with useful information</td></tr><tr><td align=\"left\">IP Geolocation AP with free plan 30k requests per month</td></tr><tr><td align=\"left\">Free Geolocation tools and APIs for country, region, city and time zone lookup by IP address</td></tr><tr><td align=\"left\">Locate and identify website visitors by IP address</td></tr><tr><td align=\"left\">Kakao Maps provide multiple APIs for Korean maps</td></tr><tr><td align=\"left\">Get the IP geolocation data through the simple REST API. All the responses are JSON encoded</td></tr><tr><td align=\"left\">Provides forward/reverse geocoding and batch geocoding</td></tr><tr><td align=\"left\">Interactive map with detailed places and information portal in Thailand</td></tr><tr><td align=\"left\">Create/customize beautiful digital maps</td></tr><tr><td align=\"left\">To access tools and resources to map the world</td></tr><tr><td align=\"left\">Mexico RESTful zip codes API</td></tr><tr><td align=\"left\">Provides worldwide forward / reverse geocoding</td></tr><tr><td align=\"left\">Singapore Land Authority REST API services for Singapore addresses</td></tr><tr><td align=\"left\">Determine if a lat/lon is on water or land</td></tr><tr><td align=\"left\">Elevation and ocean depth for a latitude and longitude</td></tr><tr><td align=\"left\">Forward and reverse geocoding using open data</td></tr><tr><td align=\"left\">Directions, POIs, isochrones, geocoding (+reverse), elevation, and more</td></tr><tr><td align=\"left\">Navigation, geolocation and geographical data</td></tr><tr><td align=\"left\">A crowdsourced map of public pinball machines</td></tr><tr><td align=\"left\">Forward &amp; Reverse Batch Geocoding REST API</td></tr><tr></tr><tr><td align=\"left\">Provide geolocation data based on postcode for Dutch addresses</td></tr><tr><td align=\"left\">Postcode lookup &amp; Geolocation for the UK</td></tr><tr><td align=\"left\">Access to heat focus data (probable wildfire)</td></tr><tr><td align=\"left\">Get information about countries via a RESTful API</td></tr><tr><td align=\"left\">Rwanda Provences, Districts, Cities, Capital City, Sector, cells, villages and streets</td></tr><tr><td align=\"left\">German city, country, river, database</td></tr><tr><td align=\"left\">Add location based interactions to your mobile app</td></tr><tr><td align=\"left\">Telize offers location information from any IP address</td></tr><tr><td align=\"left\">Maps, Directions, Places and Traffic APIs</td></tr><tr><td align=\"left\">Discover and share maps with friends</td></tr><tr><td align=\"left\">Validate and append data for any US ZipCode</td></tr><tr><td align=\"left\">Utah Web API for geocoding Utah addresses</td></tr><tr><td align=\"left\">Brazil RESTful zip codes API</td></tr><tr><td align=\"left\">Three words as rememberable and unique coordinates worldwide</td></tr><tr><td align=\"left\">US zip code distance, radius and location API</td></tr><tr><td align=\"left\">Get information about place such as country, city, state, etc</td></tr><tr><td align=\"left\">Get the country, state, and city of any US zip-code</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Access to the data from the CMS - medicare.gov</td></tr><tr><td align=\"left\">HTTP API for Latest Covid-19 Data</td></tr><tr><td align=\"left\">Covid 19 spread, infection and recovery</td></tr><tr><td align=\"left\">Covid 19 cases, deaths and recovery per country</td></tr><tr><td align=\"left\">Maps, datasets, applications and more in the context of COVID-19</td></tr><tr><td align=\"left\">Covid 19 statistics state and district wise about cases, vaccinations, recovery within India</td></tr><tr><td align=\"left\">Open-source API for exploring Covid19 cases based on JHU CSSE</td></tr><tr><td align=\"left\">Global and countrywise data of Covid 19 daily Summary, confirmed cases, recovered and deaths</td></tr><tr><td align=\"left\">Indonesian government Covid data per province</td></tr><tr><td align=\"left\">National Nutrient Database for Standard Reference</td></tr><tr><td align=\"left\">Educational content about the US Health Insurance Marketplace</td></tr><tr><td align=\"left\">Humanitarian Data Exchange (HDX) is open platform for sharing data across crises and organisations</td></tr><tr><td align=\"left\">NLP based symptom checker and patient triage API for health diagnosis from text</td></tr><tr><td align=\"left\">SARS-CoV-2 genomic sequences from public sources</td></tr><tr><td align=\"left\">NLP that extracts mentions of clinical concepts from text, gives access to clinical ontology</td></tr><tr></tr><tr><td align=\"left\">National Plan &amp; Provider Enumeration System, info on healthcare providers registered in US</td></tr><tr><td align=\"left\">Worlds largest verified nutrition database</td></tr><tr><td align=\"left\">API for Current cases and more stuff about COVID-19 and Influenza</td></tr><tr><td align=\"left\">Public FDA data about drugs, devices and foods</td></tr><tr><td align=\"left\">Medical platform which allows the development of applications for different healthcare scenarios</td></tr><tr><td align=\"left\">Coronavirus API with free COVID-19 live updates</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr><tr><td align=\"left\">API for Job board aggregator in Europe / Remote</td></tr><tr><td align=\"left\">API for the \"Arbeitsamt\", which is a german Job board aggregator</td></tr><tr></tr><tr></tr><tr><td align=\"left\">Job titles, skills and related jobs data</td></tr><tr></tr><tr><td align=\"left\">Job board and company profiles</td></tr><tr><td align=\"left\">Freelance job board and management system</td></tr><tr></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Image captioning, face recognition, NSFW classification</td></tr><tr><td align=\"left\">Natural Language Processing</td></tr><tr><td align=\"left\">Used for the primary ways for filtering the stopping, stemming words from the text data</td></tr><tr><td align=\"left\">Face detection, face recognition with age estimation/gender estimation, accurate, no quota limits</td></tr><tr><td align=\"left\">Image Recognition Solutions like Tagging, Visual Search, NSFW moderation</td></tr><tr><td align=\"left\">Computer Vision services like Facial detection, Image labeling, NSFW classification</td></tr><tr><td align=\"left\">Face and License Plate Anonymization</td></tr><tr><td align=\"left\">Realtime content moderation API that blocks or blurs unwanted images in real-time</td></tr><tr></tr><tr><td align=\"left\">AI Solutions: Video/Image Classification &amp; Tagging, NSFW, Icon/Image/Audio Search, NLP</td></tr><tr><td align=\"left\">A FREE API for developers to build and monetize personalized ML based chat apps</td></tr><tr><td align=\"left\">NLP API using spaCy and transformers for NER, sentiments, classification, summarization, and more</td></tr><tr><td align=\"left\">Open source computer vision API based on open source models</td></tr><tr><td align=\"left\">NLP API to return probability that if text is toxic, obscene, insulting or threatening</td></tr><tr><td align=\"left\">Face Detection, Face Recognition and Face Grouping</td></tr><tr><td align=\"left\">A time series analysis API</td></tr><tr><td align=\"left\">Forecasting API for timeseries data</td></tr><tr><td align=\"left\">Provides specific answers to questions using data and algorithms</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Api of Music store 7digital</td></tr><tr><td align=\"left\">Api of the streaming music hub Audiomack</td></tr><tr><td align=\"left\">API of Music store Bandcamp</td></tr><tr><td align=\"left\">API to retrieve song information from Gaana</td></tr><tr><td align=\"left\">Crowdsourced lyrics and music knowledge</td></tr><tr><td align=\"left\">API to retrieve song information, album meta data and many more from JioSaavn</td></tr><tr><td align=\"left\">Get music libraries, playlists, charts, and perform out of KKBOX's platform</td></tr><tr><td align=\"left\">Simple API to retrieve the lyrics of a song</td></tr><tr><td align=\"left\">Download curated playlists of streaming tracks (YouTube, SoundCloud, etc...)</td></tr><tr><td align=\"left\">A web-based archive of legal live audio recordings of the improvisational rock band Phish</td></tr><tr><td align=\"left\">Provides guitar, bass and drums tabs and chords</td></tr><tr><td align=\"left\">With SoundCloud API you can build applications that will give more power to control your content</td></tr><tr><td align=\"left\">View Spotify music catalog, manage users' libraries, get recommendations and more</td></tr><tr><td align=\"left\">Similar artist API (also works for movies and TV shows)</td></tr><tr><td align=\"left\">Crowdsourced lyrics and music knowledge</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Search for news and metadata from Associated Press</td></tr><tr><td align=\"left\">Provides access to millions of pages of historic US newspapers from the Library of Congress</td></tr><tr><td align=\"left\">Latest news published in various news sources, blogs and forums</td></tr><tr><td align=\"left\">Search for news from various sources</td></tr><tr><td align=\"left\">Live stock market news with tagged tickers + sentiment and stats JSON API</td></tr><tr><td align=\"left\">Headlines currently published on a range of news sources and blogs</td></tr><tr><td align=\"left\">News data API for live-breaking news and headlines from reputed news sources</td></tr><tr><td align=\"left\">Get or Search Latest Breaking News with ML Powered Summaries 🤖</td></tr><tr><td align=\"left\">Personalized news listening experience from NPR</td></tr><tr><td align=\"left\">Access all the content the Guardian creates, categorised by tags and section</td></tr><tr><td align=\"left\">Aggregated headlines, top story and live news JSON API</td></tr><tr><td align=\"left\">Search through the National Library of Australia collection of 1000s of digitised newspapers</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Unofficial US Federal Government API Development</td></tr><tr><td align=\"left\">An Indian Government platform that provides a lot of APIS for KYC, business, education &amp; employment</td></tr><tr><td align=\"left\">Contribute or search one of the largest black history fact databases on the web</td></tr><tr><td align=\"left\">JSON formatted details about Telegram Bots available in database</td></tr><tr><td align=\"left\">Location Information Prediction</td></tr><tr><td align=\"left\">Federal Statistical Office Germany</td></tr><tr><td align=\"left\">People groups of the world with the fewest followers of Christ</td></tr><tr><td align=\"left\">Create and interact with Datasets, Notebooks, and connect with Kaggle</td></tr><tr><td align=\"left\">Get JSON formatted summary with title, description and preview image for any requested URL</td></tr><tr><td align=\"left\">Get measure resources and influence to rank the relative power of states in Asia</td></tr><tr><td align=\"left\">Extract structured data from any website</td></tr><tr><td align=\"left\">Open data about nobel prizes and events</td></tr><tr><td align=\"left\">Large datasets repository of African open data</td></tr><tr><td align=\"left\">Data on corporate entities and directors in many countries</td></tr><tr><td align=\"left\">Data on international sanctions, crime and politically exposed persons</td></tr><tr><td align=\"left\">News articles and public datasets</td></tr><tr></tr><tr><td align=\"left\">Access to Open Data from Governments, Non-profits and NGOs around the world</td></tr><tr></tr><tr><td align=\"left\">Courses, lecture videos, detailed information for courses etc. for the University of Oslo (Norway)</td></tr><tr><td align=\"left\">More than 1.5 million barcode numbers from all around the world</td></tr><tr><td align=\"left\">The largest set of publicly available real time urban data in the UK</td></tr><tr><td align=\"left\">Collaboratively edited knowledge base operated by the Wikimedia Foundation</td></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">European patent search system api</td></tr><tr><td align=\"left\">API is intended to explore and visualize trends/patterns across the US innovation landscape</td></tr><tr><td align=\"left\">Taiwan patent search system api</td></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Access Forem articles, users and other resources via API</td></tr><tr><td align=\"left\">API to get access to the collection of the most inspiring expressions of mankind</td></tr><tr><td align=\"left\">FavQs allows you to collect, discover and share your favorite quotes</td></tr><tr></tr><tr><td align=\"left\">The largest selection of dad jokes on the internet</td></tr><tr><td align=\"left\">Motivational and Inspirational quotes</td></tr><tr><td align=\"left\">REST API for random Kanye West quotes</td></tr><tr><td align=\"left\">Team radio and interview quotes by Finnish F1 legend Kimi Räikkönen</td></tr><tr><td align=\"left\">Community of readers and writers offering unique perspectives on ideas</td></tr><tr><td align=\"left\">REST API for more than 5000 famous quotes</td></tr><tr><td align=\"left\">Ever-growing list of James Clear quotes from the 3-2-1 Newsletter</td></tr><tr><td align=\"left\">Assess, collect and analyze Personality</td></tr><tr><td align=\"left\">Large collection of Zen quotes for inspiration</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Dynamically generate images and PDFs from templates with a simple API</td></tr><tr><td align=\"left\">Image generation with query string</td></tr><tr><td align=\"left\">Photo optimization and resize</td></tr><tr><td align=\"left\">Image manipulation and processing</td></tr><tr><td align=\"left\">Generate, Edit, Scale and Manage Images and Videos Smarter &amp; Faster</td></tr><tr><td align=\"left\">Generate Hundreds of Personalized Images in Minutes</td></tr><tr></tr><tr><td align=\"left\">Build applications using the world's most powerful imagery</td></tr><tr></tr><tr></tr><tr><td align=\"left\">Integrate Google Photos with your apps or devices</td></tr><tr></tr><tr></tr><tr></tr><tr><td align=\"left\">Free Stock Photos and Videos</td></tr><tr><td align=\"left\">Remove background from images</td></tr><tr><td align=\"left\">Resizable Keanu Reeves placeholder images with grayscale and young Keanu options</td></tr><tr></tr><tr><td align=\"left\">Image management solutions like optimization, manipulation, hosting</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Get access to Codeforces data</td></tr><tr><td align=\"left\">For compiling and running code in several languages</td></tr><tr><td align=\"left\">Online code execution system</td></tr><tr><td align=\"left\">For upcoming and ongoing competitive coding contests</td></tr><tr><td align=\"left\">For programmatically generating documentation for code</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Curated research-sharing platform: physics, mathematics, quantitative finance, and economics</td></tr><tr><td align=\"left\">Access the world's Open Access research papers</td></tr><tr><td align=\"left\">Global Biodiversity Information Facility</td></tr><tr><td align=\"left\">Access millions of museum specimens from organizations around the world</td></tr><tr><td align=\"left\">ISRO Space Crafts Information</td></tr><tr><td align=\"left\">Integrated Taxonomic Information System</td></tr><tr><td align=\"left\">NASA data, including imagery</td></tr><tr><td align=\"left\">NASA Astrophysics Data System</td></tr><tr><td align=\"left\">Symbolic and Arithmetic Math Calculator</td></tr><tr><td align=\"left\">REST API used to access NoctuaSky features</td></tr><tr><td align=\"left\">Number of the day, random number, number facts and anything else you want to do with numbers</td></tr><tr><td align=\"left\">Facts pertaining to the physical science of Oceanography</td></tr><tr><td align=\"left\">Repository and archive for study designs, research materials, data, manuscripts, etc</td></tr><tr><td align=\"left\">Real Time Air Quality Monitoring</td></tr><tr><td align=\"left\">Decodes base64 encoding and parses it to return a solution to the calculation in JSON</td></tr><tr><td align=\"left\">A free, open, dataset about research and scholarly activities</td></tr><tr><td align=\"left\">Company, vehicle, launchpad and launch data</td></tr><tr><td align=\"left\">GraphQL, Company, Ships, launchpad and launch data</td></tr><tr><td align=\"left\">With this API you can add each of the times introduced in the array sended</td></tr><tr></tr><tr><td align=\"left\">Random mathematical expressions</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Provide access to BinaryEdge 40fy scanning platform</td></tr><tr><td align=\"left\">Best open-source password manager</td></tr><tr><td align=\"left\">Botd is a browser library for JavaScript bot detection</td></tr><tr><td align=\"left\">Bugcrowd API for interacting and tracking the reported issues programmatically</td></tr><tr><td align=\"left\">Search engine for Internet connected host and devices</td></tr><tr><td align=\"left\">Encrypting &amp; decrypting text messages</td></tr><tr><td align=\"left\">Chrome extension risk scoring</td></tr><tr><td align=\"left\">Hash decryption MD5, SHA1, SHA3, SHA256, SHA384, SHA512</td></tr><tr><td align=\"left\">Email address threat and risk prediction</td></tr><tr><td align=\"left\">An API for escaping different kind of queries</td></tr><tr><td align=\"left\">Lists of filters for adblockers and firewalls</td></tr><tr><td align=\"left\">Fraud detection API offering highly accurate browser fingerprinting</td></tr><tr><td align=\"left\">Screen order information using AI to detect frauds</td></tr><tr><td align=\"left\">Searchable attack surface database of the entire internet</td></tr><tr><td align=\"left\">Scan files for secrets (API Keys, database credentials)</td></tr><tr><td align=\"left\">Query IPs in the GreyNoise dataset and retrieve a subset of the full IP context data</td></tr><tr><td align=\"left\">The industry’s first hacker API that helps increase productivity towards creative bug bounty hunting</td></tr><tr><td align=\"left\">A REST API to access high level cryptographic functions and methods</td></tr><tr><td align=\"left\">Passwords which have previously been exposed in data breaches</td></tr><tr><td align=\"left\">Managed User Authentication Service</td></tr><tr><td align=\"left\">Generate random passwords of varying complexities</td></tr><tr><td align=\"left\">Generate merchant-specific and one-time use credit card numbers that link back to your bank</td></tr><tr><td align=\"left\">Scan, search and collect threat intelligence data in real-time</td></tr><tr><td align=\"left\">Domain and IP related information such as current and historical WHOIS and DNS records</td></tr><tr><td align=\"left\">Search engine for Internet connected devices</td></tr><tr><td align=\"left\">Access data on all Internet assets and build powerful attack surface management applications</td></tr><tr><td align=\"left\">Risk scoring service from curated threat intelligence data</td></tr><tr><td align=\"left\">Virushee file/data scanning</td></tr><tr><td align=\"left\">VulDB API allows to initiate queries for one or more items along with transactional bots</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Products, Buying Options, Categories, Recommendations, Stores and Commerce</td></tr><tr><td align=\"left\">Retrieve price and inventory of electronic components as well as place orders</td></tr><tr><td align=\"left\">An api to fetch dummy e-commerce products JSON data with placeholder images</td></tr><tr></tr><tr><td align=\"left\">Manage shop and interact with listings</td></tr><tr><td align=\"left\">Product listing management, Order Fulfilment in the Flipkart Marketplace</td></tr><tr><td align=\"left\">Retrieve product ratings and seller performance metrics</td></tr><tr><td align=\"left\">Manage sales, ads, products, services and Shops</td></tr><tr><td align=\"left\">Electronic part data for manufacturing, design, and sourcing</td></tr><tr><td align=\"left\">Integrate with local sites by posting, managing adverts and communicating with OLX users</td></tr><tr><td align=\"left\">Manage orders from Rappi's app</td></tr><tr><td align=\"left\">Shopee's official API for integration of various services from Shopee</td></tr><tr><td align=\"left\">Tokopedia's Official API for integration of various services from Tokopedia</td></tr><tr><td align=\"left\">WooCommerce REST APIS to create, read, update, and delete data on wordpress website in JSON format</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Simple image-based bulletin board dedicated to a variety of topics</td></tr><tr><td>Social media APIs to post, get analytics, and manage multiple users social media accounts</td></tr><tr><td>Daily horoscope info for yesterday, today, and tomorrow</td></tr><tr><td>The Blogger APIs allows client applications to view and update Blogger content</td></tr><tr><td>Make bots for Discord, integrate Discord onto an external platform</td></tr><tr><td>Communicate with Disqus data</td></tr><tr><td>Top meme posts from r/dogecoin which include 'Meme' flair</td></tr><tr><td>Facebook Login, Share on FB, Social Plugins, Analytics and more</td></tr><tr><td>Interact with Foursquare users and places (geolocation-based checkins, photos, tips, events, etc)</td></tr><tr><td>Get Social Media profiles and contact Information</td></tr><tr><td>Social news for CS and entrepreneurship</td></tr><tr><td>A blogging platform built for developers</td></tr><tr><td>Instagram Login, Share on Instagram, Social Plugins and more</td></tr><tr><td>Kakao Login, Share on KakaoTalk, Social Plugins and more</td></tr><tr><td>Retrieve your presence on Discord through an HTTP REST API or WebSocket</td></tr><tr><td>Line Login, Share on Line, Social Plugins and more</td></tr><tr><td>The foundation of all digital integrations with LinkedIn</td></tr><tr><td>Data about Meetups from Meetup.com</td></tr><tr><td>Access the data and intelligence in Microsoft 365, Windows 10, and Enterprise Mobility</td></tr><tr><td>NAVER Login, Share on NAVER, Social Plugins and more</td></tr><tr><td>The world's catalog of ideas</td></tr><tr></tr><tr><td>Revolt open source Discord alternative</td></tr><tr></tr><tr></tr><tr><td>Bot API to interact with TamTam</td></tr><tr><td>Simplified HTTP version of the MTProto API for bots</td></tr><tr><td>Create attractive blogs easily, to share</td></tr><tr><td>Fetches user info and user's video posts on TikTok platform</td></tr><tr><td>A freecycling community with thousands of free items posted every day</td></tr><tr><td>Read and write Tumblr Data</td></tr><tr></tr><tr><td>Read and write Twitter data</td></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Get information about Football Leagues &amp; Cups</td></tr><tr><td align=\"left\">ApiMedic offers a medical symptom checker API primarily for patients</td></tr><tr><td align=\"left\">Balldontlie provides access to stats data from the NBA</td></tr><tr><td align=\"left\">Official Cloudbet API provides real-time sports odds and betting API to place bets programmatically</td></tr><tr><td align=\"left\">Unofficial detailed American college football statistics, records, and results API</td></tr><tr><td align=\"left\">F1 data from the beginning of the world championships in 1950</td></tr><tr></tr><tr><td align=\"left\">A simple Open Source Football API to get squads’ stats, best scorers and more</td></tr><tr><td align=\"left\">Embed codes for goals and highlights from Premier League, Bundesliga, Serie A and many more</td></tr><tr><td align=\"left\">Display football standings e.g epl, la liga, serie a etc. The data is based on espn site</td></tr><tr><td align=\"left\">Football data with matches info, players, teams, and competitions</td></tr><tr><td align=\"left\">All NBA Stats DATA, Games, Livescore, Standings, Statistics</td></tr><tr><td align=\"left\">Current and historical NBA Statistics</td></tr><tr><td align=\"left\">Odds history from multiple UK bookmakers</td></tr><tr><td align=\"left\">Crowd sourced sports league results</td></tr><tr><td align=\"left\">Get sports data from all over the world</td></tr><tr><td align=\"left\">Crowd-source sports places around the world</td></tr><tr><td align=\"left\">Identify sport, brands and gear in an image. Also does image sports captioning</td></tr><tr><td align=\"left\">Football score/schedule, news api, tv channels, stats, history, display standing e.g. epl, la liga</td></tr><tr><td align=\"left\">Fixtures, results and predictions for Australian Football League matches</td></tr><tr><td align=\"left\">Connect with athletes, activities and more</td></tr><tr><td align=\"left\">Query sports data, including teams, players, games, scores and statistics</td></tr><tr><td align=\"left\">Crowd-Sourced Sports Data and Artwork</td></tr><tr><td align=\"left\">Get and set activities, health data and more</td></tr><tr><td align=\"left\">Workout manager data as exercises, muscles or equipment</td></tr></tbody></table><table><thead><tr></tr></thead></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Detect, label, format and enrich the code in your app or in your data pipeline</td></tr><tr><td align=\"left\">Natural Language Processing Tools for Thai Language</td></tr><tr><td align=\"left\">Natural language understanding technology, including sentiment, entity and syntax analysis</td></tr><tr><td align=\"left\">Image to text -text recognition- from image more than 100 language, accurate, unlimited requests</td></tr><tr><td align=\"left\">Translate between 21 of most used languages, accurate, unlimited requests</td></tr><tr><td align=\"left\">Translation tool with 17 available languages</td></tr><tr><td align=\"left\">Text Analytics with sentiment analysis, categorization &amp; named entity extraction</td></tr><tr><td align=\"left\">Multilingual sentiment analysis of texts from different sources</td></tr><tr><td align=\"left\">Text Analytics with focus on detection of abusive content and law enforcement applications</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">API to update, manage and track shipment efficiently</td></tr><tr><td align=\"left\">Integration to provide information and prepare shipments using Correio's services</td></tr><tr><td align=\"left\">API for recording and tracking habits or effort, routines</td></tr><tr><td align=\"left\">API for getting Pincode details in India</td></tr><tr><td align=\"left\">An API to query Brazilian ZIP codes and orders easily, quickly and free</td></tr><tr><td align=\"left\">Provides information about parcels in transport for Sweden and Denmark</td></tr><tr><td align=\"left\">Shipment and Address information</td></tr><tr><td align=\"left\">Automatically place subids in affiliate links to attribute affiliate conversions to click data</td></tr><tr><td align=\"left\">Small application that measures your keyboard/mouse usage</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr><tr><td align=\"left\">URL shortener and link management</td></tr><tr><td align=\"left\">Monitor, compare and optimize your marketing links</td></tr><tr></tr><tr></tr><tr><td align=\"left\">Free URL Shortener offers a powerful API to interact with other sites</td></tr><tr></tr><tr><td align=\"left\">A lightweight URL shortener, focused on ease-of-use for the developer and end-user</td></tr><tr><td align=\"left\">Free Modern URL Shortener</td></tr><tr><td align=\"left\">A simple link obfuscator/shortener</td></tr><tr><td align=\"left\">Custom URL shortener for sharing branded links</td></tr><tr><td align=\"left\">Short URLs support so many domains</td></tr><tr><td align=\"left\">URl Shortener with multiple Domains</td></tr><tr><td align=\"left\">Simple and efficient short link creation</td></tr><tr></tr><tr><td align=\"left\">Simple and efficient short link creation</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Helicopter and passenger drone landing site directory, Helipaddy data and much more</td></tr><tr><td align=\"left\">Telematics data, remotely access vehicle functions, car configurator, locate service dealers</td></tr><tr><td align=\"left\">NHTSA Product Information Catalog and Vehicle Listing</td></tr><tr><td align=\"left\">Lock and unlock vehicles and get data like odometer reading and location. Works on most new cars</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"left\">Weather, especially for Astroweather</td></tr><tr><td align=\"left\">Weather and forecast data from Spain</td></tr><tr></tr><tr><td align=\"left\">Air Quality Index Data for over 1000 cities</td></tr><tr><td align=\"left\">NOAA aviation weather forecasts and observations</td></tr><tr><td align=\"left\">Meteorological data of the Basque Country</td></tr><tr><td align=\"left\">Provides weather forecast data for cities in Brazil</td></tr><tr><td align=\"left\">Provide weather information, earthquake information, and climate data</td></tr><tr><td align=\"left\">Real time weather forecasts and historic data</td></tr><tr><td align=\"left\">Weather and weather webcams</td></tr><tr><td align=\"left\">70+ years of global, hourly historical and forecast weather data from NOAA and ECMWF</td></tr><tr><td align=\"left\">Global weather forecast API for non-commercial use</td></tr><tr><td align=\"left\">Data from Personal Weather Stations called senseBoxes</td></tr><tr><td align=\"left\">Real-time UV Index Forecast</td></tr><tr><td align=\"left\">Location-based weather data</td></tr><tr><td align=\"left\">Radar data collected from different websites across the Internet</td></tr><tr><td align=\"left\">Global marine weather from multiple sources</td></tr><tr><td align=\"left\">Weather API Powered by Proprietary Technology</td></tr><tr><td align=\"left\">A RESTful free API to check the weather</td></tr><tr><td align=\"left\">Weather API with other stuff like Astronomy and Geolocation API</td></tr><tr><td align=\"left\">Assesses weather condition in specific locations</td></tr></tbody></table>","contentLength":42968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ubicloud/ubicloud","url":"https://github.com/ubicloud/ubicloud","date":1761446105,"author":"","guid":315155,"unread":true,"content":"<p>Open source alternative to AWS. Elastic compute, block storage (non replicated), firewall and load balancer, managed Postgres, K8s, AI inference, and IAM services.</p><p>Ubicloud is an open source cloud that can run anywhere. Think of it as an open alternative to cloud providers, like what Linux is to proprietary operating systems.</p><p>Ubicloud provides IaaS cloud features on bare metal providers, such as Hetzner, Leaseweb, and AWS Bare Metal. You can set it up yourself on these providers or you can use our <a href=\"https://console.ubicloud.com\">managed service</a>.</p><p>You can use Ubicloud without installing anything. When you do this, we pass along the underlying provider's benefits to you, such as price or location.</p><p>You can also build your own cloud. To do this, start up Ubicloud's control plane and connect to its cloud console.</p><pre><code>git clone git@github.com:ubicloud/ubicloud.git\n\n# Generate secrets for demo\n./demo/generate_env\n\n# Run containers: db-migrator, app (web &amp; respirate), postgresql\ndocker-compose -f demo/docker-compose.yml up\n\n# Visit localhost:3000\n</code></pre><p>The control plane is responsible for cloudifying bare metal Linux machines. The easiest way to build your own cloud is to lease instances from one of those providers. For example: <a href=\"https://www.hetzner.com/sb\">https://www.hetzner.com/sb</a></p><p>Once you lease instance(s), update the  file with the following environment variables:</p><ul></ul><p>Then, run the following script for each instance to cloudify it. Currently, the script cloudifies bare metal instances leased from Hetzner. After you cloudify your instances, you can provision and manage cloud resources on these machines.</p><pre><code># Enter hostname/IP and provider\ndocker exec -it ubicloud-app ./demo/cloudify_server\n</code></pre><p>Later when you create VMs, Ubicloud will assign them IPv6 addresses. If your ISP doesn't support IPv6, please use a VPN or tunnel broker such as Mullvad or Hurricane Electric's <a href=\"https://tunnelbroker.net/\">https://tunnelbroker.net/</a> to connect. Alternatively, you could lease IPv4 addresses from your provider and add them to your control plane.</p><p>Public cloud providers like AWS, Azure, and Google Cloud have made life easier for start-ups and enterprises. But they are closed source, have you rent computers at a huge premium, and lock you in. Ubicloud offers an open source alternative, reduces your costs, and returns control of your infrastructure back to you. All without sacrificing the cloud's convenience.</p><p>Today, AWS offers about two hundred cloud services. Ultimately, we will implement 10% of the cloud services that make up 80% of that consumption.</p><p>Example workloads and reasons to use Ubicloud today include:</p><ul><li><p>You have an ephemeral workload like a CI/CD pipeline (we're integrating with GitHub Actions), or you'd like to run compute/memory heavy tests. Our managed cloud is ~3x cheaper than AWS, so you save on costs.</p></li><li><p>You want a portable and simple app deployment service like <a href=\"https://github.com/basecamp/kamal\">Kamal</a>. We're moving Ubicloud's control plane from Heroku to Kamal; and we want to provide open and portable services for Kamal's dependencies in the process.</p></li><li><p>You have bare metal machines sitting somewhere. You'd like to build your own cloud for portability, security, or compliance reasons.</p></li></ul><p>You can provide us your feedback, get help, or ask us questions regarding your Ubicloud installations in the <a href=\"https://github.com/ubicloud/ubicloud/discussions\">Community Forum</a>.</p><p>We follow an established architectural pattern in building public cloud services. A control plane manages a data plane, where the data plane leverages open source software. You can find our current cloud components / services below.</p><ul><li><p>: Our control plane communicates with Linux bare metal servers using SSH. We use <a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor\">Cloud Hypervisor</a> as our virtual machine monitor (VMM); and each instance of the VMM is contained within Linux namespaces for further isolation / security.</p></li><li><p>: We use <a href=\"https://en.wikipedia.org/wiki/IPsec\">IPsec</a> tunneling to establish an encrypted and private network environment. We support IPv4 and IPv6 in a dual-stack setup and provide both public and private networking. For security, each customer’s VMs operate in their own networking namespace. For <a href=\"https://www.ubicloud.com/blog/ubicloud-firewalls-how-linux-nftables-enables-flexible-rules\">firewalls</a> and <a href=\"https://www.ubicloud.com/blog/ubicloud-load-balancer-simple-and-cost-free\">load balancers</a>, we use Linux nftables.</p></li><li><p><strong>Block Storage, non replicated</strong>: We use Storage Performance Development Toolkit (<a href=\"https://spdk.io\">SPDK</a>) to provide virtualized block storage to VMs. SPDK enables us to add enterprise features such as snapshot and replication in the future. We follow security best practices and encrypt the data encryption key itself.</p></li><li><p><strong>Attribute-Based Access Control (ABAC)</strong>: With ABAC, you can define attributes, roles, and permissions for users and give them fine-grained access to resources. You can read more about our <a href=\"https://www.ubicloud.com/docs/architecture/attribute-based-access-control-abac#attribute-based-access-control-abac-design\">ABAC design here</a>.</p></li><li><p>: We're planning to work on a managed K8s or metrics/monitoring service next. If you have a workload that would benefit from a specific cloud service, please get in touch with us through our <a href=\"https://github.com/ubicloud/ubicloud/discussions\">Community Forum</a>.</p></li><li><p>Control plane: Manages data plane services and resources. This is a Ruby program that stores its data in Postgres. We use the <a href=\"https://roda.jeremyevans.net/\">Roda</a> framework to serve HTTP requests and <a href=\"http://sequel.jeremyevans.net/\">Sequel</a> to access the database. We manage web authentication with <a href=\"http://rodauth.jeremyevans.net/\">Rodauth</a>. We communicate with data plane servers using SSH, via the library <a href=\"https://github.com/net-ssh/net-ssh\">net-ssh</a>. For our tests, we use <a href=\"https://rspec.info/\">RSpec</a>.</p></li><li><p>Cloud console: Server-side web app served by the Roda framework. For the visual design, we use <a href=\"https://tailwindcss.com\">Tailwind CSS</a> with components from <a href=\"https://tailwindui.com\">Tailwind UI</a>. We also use jQuery for interactivity.</p></li></ul><p>If you’d like to start hacking with Ubicloud, any method of obtaining Ruby and Postgres versions is acceptable. If you have no opinion on this, our development team uses  as <a href=\"https://raw.githubusercontent.com/ubicloud/ubicloud/main/DEVELOPERS.md\">documented here in detail</a>.</p><h3>Do you have any experience with building this sort of thing?</h3><p>Our founding team comes from Azure; and worked at Amazon and Heroku before that. We also have start-up experience. We were co-founders and founding team members at <a href=\"https://github.com/citusdata/citus\">Citus Data</a>, <a href=\"https://news.ycombinator.com/item?id=18990469\">which got acquired by Microsoft</a>.</p><h3>How is this different than OpenStack?</h3><p>We see three differences. First, Ubicloud is available as a managed service (vs boxed software). This way, you can get started in minutes rather than weeks. Since Ubicloud is designed for multi-tenancy, it comes with built-in features such as encryption at rest and in transit, virtual networking, secrets rotation, etc.</p><p>Second, we're initially targeting developers. This -we hope- will give us fast feedback cycles and enable us to have 6 key services in GA form in the next two years. OpenStack is still primarily used for 3 cloud services.</p><p>Last, we're designing for simplicity. With OpenStack, you pick between 10 hypervisors, 10 S3 implementations, and 5 block storage implementations. The software needs to work in a way where all of these implementations are compatible with each other. That leads to consultant-ware. We'll take a more opinionated approach with Ubicloud.</p>","contentLength":6589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"coinbase/x402","url":"https://github.com/coinbase/x402","date":1761446105,"author":"","guid":315156,"unread":true,"content":"<p>A payments protocol for the internet. Built on HTTP.</p><blockquote><p>\"1 line of code to accept digital dollars. No fee, 2 second settlement, $0.001 minimum payment.\"</p></blockquote><pre><code>app.use(\n  // How much you want to charge, and where you want the funds to land\n  paymentMiddleware(\"0xYourAddress\", { \"/your-endpoint\": \"$0.01\" })\n);\n// That's it! See examples/typescript/servers/express.ts for a complete example. Instruction below for running on base-sepolia.\n</code></pre><p>Payments on the internet are fundamentally flawed. Credit Cards are high friction, hard to accept, have minimum payments that are far too high, and don't fit into the programmatic nature of the internet. It's time for an open, internet-native form of payments. A payment rail that doesn't have high minimums + % based fee. Payments that are amazing for humans and AI agents.</p><ul><li> the x402 protocol will never force reliance on a single party</li><li> x402 is meant to seamlessly complement the existing HTTP request made by traditional web services, it should not mandate additional requests outside the scope of a typical client / server flow.</li><li><strong>Chain and token agnostic:</strong> we welcome contributions that add support for new chains, signing standards, or schemes, so long as they meet our acceptance criteria laid out in <a href=\"https://github.com/coinbase/x402/raw/main/CONTRIBUTING.md\">CONTRIBUTING.md</a></li><li> all payment schemes must not allow for the facilitator or resource server to move funds, other than in accordance with client intentions</li><li> x402 needs to be 10x better than existing ways to pay on the internet. This means abstracting as many details of crypto as possible away from the client and resource server, and into the facilitator. This means the client/server should not need to think about gas, rpc, etc.</li></ul><p>The x402 ecosystem is growing! Check out our <a href=\"https://x402.org/ecosystem\">ecosystem page</a> to see projects building with x402, including:</p><ul><li>Ecosystem infrastructure and tooling</li><li>Learning and community resources</li></ul><p>Want to add your project to the ecosystem? See our <a href=\"https://github.com/coinbase/x402/tree/main/typescript/site#adding-your-project-to-the-ecosystem\">demo site README</a> for detailed instructions on how to submit your project.</p><ul><li>: Something on the internet. This could be a webpage, file server, RPC service, API, any resource on the internet that accepts HTTP / HTTPS requests.</li><li>: An entity wanting to pay for a resource.</li><li>: A server that facilitates verification and execution of on-chain payments.</li><li>: An HTTP server that provides an API or other resource for a client.</li></ul><ul><li>Permissionless and secure for clients and servers</li><li>Gasless for client and resource servers</li><li>Minimal integration for the resource server and client (1 line for the server, 1 function for the client)</li><li>Ability to trade off speed of response for guarantee of payment</li><li>Extensible to different payment flows and chains</li></ul><p>The  protocol is a chain agnostic standard for payments on top of HTTP, leverage the existing  HTTP status code to indicate that a payment is required for access to the resource.</p><ol><li>A schema for how servers can respond to clients to facilitate payment for a resource ()</li><li>A standard header  that is set by clients paying for resources</li><li>A standard schema and encoding method for data in the  header</li><li>A recommended flow for how payments should be verified and settled by a resource server</li><li>A REST specification for how a resource server can perform verification and settlement against a remote 3rd party server ()</li><li>A specification for a  header that can be used by resource servers to communicate blockchain transactions details to the client in their HTTP response</li></ol><p>The following outlines the flow of a payment using the  protocol. Note that steps (1) and (2) are optional if the client already knows the payment details accepted for a resource.</p><ol><li><p> makes an HTTP request to a .</p></li><li><p> responds with a  status and a <code>Payment Required Response</code> JSON object in the response body.</p></li><li><p> selects one of the  returned by the server response and creates a  based on the  of the  they have selected.</p></li><li><p> sends the HTTP request with the  header containing the  to the resource server.</p></li><li><p> verifies the  is valid either via local verification or by POSTing the  and  to the  endpoint of a .</p></li><li><p> performs verification of the object based on the  and  of the  and returns a .</p></li><li><p>If the  is valid, the resource server performs the work to fulfill the request. If the  is invalid, the resource server returns a  status and a <code>Payment Required Response</code> JSON object in the response body.</p></li><li><p> either settles the payment by interacting with a blockchain directly, or by POSTing the  and <code>Payment PaymentRequirements</code> to the  endpoint of a .</p></li><li><p> submits the payment to the blockchain based on the  and  of the .</p></li><li><p> waits for the payment to be confirmed on the blockchain.</p></li><li><p> returns a <code>Payment Execution Response</code> to the resource server.</p></li><li><p> returns a  response to the  with the resource they requested as the body of the HTTP response, and a  header containing the  as Base64 encoded JSON if the payment was executed successfully.</p></li></ol><p><strong>Payment Required Response</strong></p><pre><code>{\n  // Version of the x402 payment protocol\n  x402Version: int,\n\n  // List of payment requirements that the resource server accepts. A resource server may accept on multiple chains, or in multiple currencies.\n  accepts: [paymentRequirements]\n\n  // Message from the resource server to the client to communicate errors in processing payment\n  error: string\n}\n</code></pre><pre><code>{\n  // Scheme of the payment protocol to use\n  scheme: string;\n\n  // Network of the blockchain to send payment on\n  network: string;\n\n  // Maximum amount required to pay for the resource in atomic units of the asset\n  maxAmountRequired: uint256 as string;\n\n  // URL of resource to pay for\n  resource: string;\n\n  // Description of the resource\n  description: string;\n\n  // MIME type of the resource response\n  mimeType: string;\n\n  // Output schema of the resource response\n  outputSchema?: object | null;\n\n  // Address to pay value to\n  payTo: string;\n\n  // Maximum time in seconds for the resource server to respond\n  maxTimeoutSeconds: number;\n\n  // Address of the EIP-3009 compliant ERC20 contract\n  asset: string;\n\n  // Extra information about the payment details specific to the scheme\n  // For `exact` scheme on a EVM network, expects extra to contain the records `name` and `version` pertaining to asset\n  extra: object | null;\n}\n</code></pre><p> (included as the  header in base64 encoded json)</p><pre><code>{\n  // Version of the x402 payment protocol\n  x402Version: number;\n\n  // scheme is the scheme value of the accepted `paymentRequirements` the client is using to pay\n  scheme: string;\n\n  // network is the network id of the accepted `paymentRequirements` the client is using to pay\n  network: string;\n\n  // payload is scheme dependent\n  payload: &lt;scheme dependent&gt;;\n}\n</code></pre><h4>Facilitator Types &amp; Interface</h4><p>A  is a 3rd party service that can be used by a  to verify and settle payments, without the  needing to have access to a blockchain node or wallet.</p><p>. Verify a payment with a supported scheme and network:</p><ul><li>Request body JSON: <pre><code>{\n  x402Version: number;\n  paymentHeader: string;\n  paymentRequirements: paymentRequirements;\n}\n</code></pre></li><li>Response: <pre><code>{\n  isValid: boolean;\n  invalidReason: string | null;\n}\n</code></pre></li></ul><p>. Settle a payment with a supported scheme and network:</p><ul><li><pre><code>{\n  x402Version: number;\n  paymentHeader: string;\n  paymentRequirements: paymentRequirements;\n}\n</code></pre></li><li><pre><code>{\n  // Whether the payment was successful\n  success: boolean;\n\n  // Error message from the facilitator server\n  error: string | null;\n\n  // Transaction hash of the settled payment\n  txHash: string | null;\n\n  // Network id of the blockchain the payment was settled on\n  networkId: string | null;\n}\n</code></pre></li></ul><p>. Get supported payment schemes and networks:</p><ul><li>Response: <pre><code>{\n  kinds: [\n    {\n      \"scheme\": string,\n      \"network\": string,\n    }\n  ]\n}\n</code></pre></li></ul><p>A scheme is a logical way of moving money.</p><p>Blockchains allow for a large number of flexible ways to move money. To help facilitate an expanding number of payment use cases, the  protocol is extensible to different ways of settling payments via its  field.</p><p>Each payment scheme may have different operational functionality depending on what actions are necessary to fulfill the payment. For example , the first scheme shipping as part of the protocol, would have different behavior than .  transfers a specific amount (ex: pay $1 to read an article), while a theoretical  would transfer up to an amount, based on the resources consumed during a request (ex: generating tokens from an LLM).</p><p>See  for more details on schemes, and see <code>specs/schemes/exact/scheme_exact_evm.md</code> to see the first proposed scheme for exact payment on EVM chains.</p><p>Because a scheme is a logical way of moving money, the way a scheme is implemented can be different for different blockchains. (ex: the way you need to implement  on Ethereum is very different from the way you need to implement  on Solana).</p><p>Clients and facilitators must explicitly support different  pairs in order to be able to create proper payloads and verify / settle payments.</p><p> Node.js v24 or higher</p><ol><li><p>From  run  and  to ensure all dependent packages and examples are setup.</p></li><li><p>Select a server, i.e. express, and  into that example. Add your server's ethereum address to get paid to into the  file, and then run  in that directory.</p></li><li><p>Select a client, i.e. axios, and  into that example. Add your private key for the account making payments into the  file, and then run  in that directory.</p></li></ol><p>You should see activities in the client terminal, which will display a weather report.</p><ol><li>Navigate to the typescript directory: </li><li>Install dependencies: </li><li>Run the unit tests: </li></ol><p>This will run the unit tests for the x402 packages.</p>","contentLength":9290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TheRobotStudio/SO-ARM100","url":"https://github.com/TheRobotStudio/SO-ARM100","date":1761446105,"author":"","guid":315157,"unread":true,"content":"<div align=\"center\"><h2><p>Build Your Own SO-101 Robot!</p></h2><p>The SO‑101 is the next‑generation version of the SO‑100 robot arm, originally designed by the <a href=\"https://www.therobotstudio.com\">RobotStudio</a> in collaboration with <a href=\"https://huggingface.co/lerobot\">Hugging Face</a>. It has improved wiring, is easier to assemble (no gear removal) and uses updated motors for the leader arm.</p><p>These arms are designed to work seamlessly with the open‑source 🤗 LeRobot library. Join our community on <a href=\"https://discord.gg/ggrqhPTsMe\">Discord</a> to collaborate on both hardware and software, and help make end‑to‑end AI for robotics more accessible.</p></div><ul><li>For the SO‑101 docs, follow this page further.</li></ul><p>After sourcing all parts you can setup your SO-101 with LeRobot <a href=\"https://huggingface.co/docs/lerobot/so101\">tutorial</a>.</p><p>This repository also includes a range of optional hardware designs such as a raised leader base and different camera mounts. Here you can explore the <a href=\"https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#optional-hardware\">full list</a>.</p><p>You can find all optional for SO-100/SO-101 kits here:</p><p>Additionally you can find SO-100 follower arm kit (without leader arm) on <a href=\"https://robots.phospho.ai\">Phospho</a>. It can be especially useful if you own a VR headset.</p><p>The follower and leader arm for this teleoperation setup will almost the same off the shelf parts (except for the motors). If you plan on creating the classic teleoperation set up to be used with the  library please buy from the Parts for Two Arms below.</p><p>We only have links for US, EU, CN, and JP for now. If you find links for other countries, please create an issue or PR so that we add them to the list. Note that prices and items may vary depending on geographic location.</p><blockquote><p>[!IMPORTANT] The STS3215 motors for the follower arm comes in two sizes. The 7.4V has a stall torque of 16.5kg.cm at 6V (and likely slightly less for a 5V power supply). The 12V version has a stall torque of 30kg.cm. While we found the 7.4V to be sufficient, if you would like more powerful motors you can buy the 12V version <a href=\"https://www.alibaba.com/product-detail/6PCS-12V-30KG-STS3215-High-Torque_1601216757543.html\">here</a>. Note if you do this, you will also have to buy a 12V 5A+ power supply instead of a 5V one. The leader arm is always 7.4V for the SO101.</p></blockquote><h4>Parts For Two Arms (Follower and Leader Setup):</h4><p>: You can buy <strong>all six STS3215 servos needed for the SO-101 leader arm</strong> (3 × 1/147 gear (C046), 2 × 1/191 gear (C044), 1 × 1/345 gear (C001)) in a single bundle on <a href=\"https://www.alibaba.com/product-detail/6PCS-7-4V-STS3215-Servos-for_1601428584027.html?spm=a2747.product_manager.0.0.757c2c3clU7uH3\">Alibaba</a>.</p><h4>Parts for One Follower Arm:</h4><p>: You do not need to use this exact screwdriver set, but it is highly recommended to have phillips head screw driver sizes #0 and #1 for easiest screw installation and removal. These are both standard sizes which will likely appear in most small screwdriver sets.</p><p>A variety of 3D printers are acceptable to print the parts necessary of the follower and leader arm. Follow the steps below to ensure a good print.</p><p>The STL files provided are ready to print on many FDM printers. Below are the tested and suggested settings though others may work.</p><h3>Step 2: Set up the Printer</h3><ol><li>Ensure that the printer is calibrated and the bed level is correctly set using the printer specific instructions.</li><li>Clean the print bed, making sure it is free from dust, or grease. If cleaning the bed using water, or other liquid, dry the bed.</li><li>If your printer recommends it, use a standard glue stick and apply a thin, even layer of glue across the print area of the bed. Avoid clumping or uneven application.</li><li>Load the printer filament using printer specific instructions.</li><li>Ensure the printer settings match the ones suggested above (most printers have multiple settings so choose the ones that most closely match).</li><li>Set for supports everywhere but ignore slopes greater than 45 degrees to the horizontal.</li><li>There should be no supports in the screw holes with horizontal axes.</li></ol><h3>Step 3: Check Printer Accuracy</h3><ol><li>In the <a href=\"https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/Gauges\">Gauges</a> folder, there are two types of gauges, one to check the size of print against a standard 4x2 lego block and one against a STS3215 servo. \n  </li><li>Test the gauge 0 against your given object (Lego or Servo). The fit should be similar to this <a href=\"https://youtu.be/dss8E3DG2rA\">tutorial</a>.</li><li>If the fit is appropriate, go onto Step 4, otherwise, change your printer settings and try again or create an issue.</li></ol><p>All the parts for the leader or follower are for easy 3D printing already contained in a , correctly orientated for z upwards to minimize supports.</p><ol><li>For printer bed sizes of 220mmx220mm (such as the Ender), print these files: \n  </li><li>For printer bed sizes of 205mm x 250mm (such as the Prusa/Up): \n  </li></ol><p>This table contains all individual files:</p><ol><li>After the print is done, use a putty knife to scrape the the parts off the print bed.</li><li>Remove any support material from parts.</li></ol><p>Extend your SO‑100/SO‑101 with these add-ons.</p><p>For debugging, any Windows PC can connect over USB to program the servos and to debug or do tests. To do so download <a href=\"https://www.feetechrc.com/software.html\">Feetech Software</a>. For Ubuntu, you can use <a href=\"https://github.com/Kotakku/FT_SCServo_Debug_Qt\">FT_SCServo_Debug_Qt</a>. Note: This step is not necessary as motors can be configured using the LeRobot Library, but this can be helpful for debugging.</p>","contentLength":4715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ashishps1/awesome-system-design-resources","url":"https://github.com/ashishps1/awesome-system-design-resources","date":1761446105,"author":"","guid":315158,"unread":true,"content":"<p>Learn System Design concepts and prepare for interviews using free resources.</p><p>This repository contains free resources to learn System Design concepts and prepare for interviews.</p><h2>🌐 Networking Fundamentals</h2><h2>🔄 Asynchronous Communication</h2><h2>🧩 Distributed System and Microservices</h2><h2>🖇️ Architectural Patterns</h2><h2>⚖️ System Design Tradeoffs</h2><h2>💻 System Design Interview Problems</h2><h2>📜 Must-Read Engineering Articles</h2><h2>🗞️ Must-Read Distributed Systems Papers</h2><p align=\"center\"><i>If you find this resource helpful, please give it a star ⭐️ and share it with others!</i></p>","contentLength":541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"cjpais/Handy","url":"https://github.com/cjpais/Handy","date":1761446105,"author":"","guid":315159,"unread":true,"content":"<p>A free, open source, and extensible speech-to-text application that works completely offline.</p><p><strong>A free, open source, and extensible speech-to-text application that works completely offline.</strong></p><p>Handy is a cross-platform desktop application built with Tauri (Rust + React/TypeScript) that provides simple, privacy-focused speech transcription. Press a shortcut, speak, and have your words appear in any text field—all without sending your voice to the cloud.</p><p>Handy was created to fill the gap for a truly open source, extensible speech-to-text tool. As stated on <a href=\"https://handy.computer\">handy.computer</a>:</p><ul><li>: Accessibility tooling belongs in everyone's hands, not behind a paywall</li><li>: Together we can build further. Extend Handy for yourself and contribute to something bigger</li><li>: Your voice stays on your computer. Get transcriptions without sending audio to the cloud</li><li>: One tool, one job. Transcribe what you say and put it into a text box</li></ul><p>Handy isn't trying to be the best speech-to-text app—it's trying to be the most forkable one.</p><ol><li> a configurable keyboard shortcut to start/stop recording (or use push-to-talk mode)</li><li> your words while the shortcut is active</li><li> and Handy processes your speech using Whisper</li><li> your transcribed text pasted directly into whatever app you're using</li></ol><p>The process is entirely local:</p><ul><li>Silence is filtered using VAD (Voice Activity Detection) with Silero</li><li>Transcription uses your choice of models: \n  <ul><li> (Small/Medium/Turbo/Large) with GPU acceleration when available</li><li> - CPU-optimized model with excellent performance and automatic language detection</li></ul></li><li>Works on Windows, macOS, and Linux</li></ul><ol><li>Install the application following platform-specific instructions</li><li>Launch Handy and grant necessary system permissions (microphone, accessibility)</li><li>Configure your preferred keyboard shortcuts in Settings</li></ol><p>For detailed build instructions including platform-specific requirements, see <a href=\"https://raw.githubusercontent.com/cjpais/Handy/main/BUILD.md\">BUILD.md</a>.</p><p>Handy is built as a Tauri application combining:</p><ul><li>: React + TypeScript with Tailwind CSS for the settings UI</li><li>: Rust for system integration, audio processing, and ML inference</li><li>: \n  <ul><li>: Local speech recognition with Whisper models</li><li>: CPU-optimized speech recognition with Parakeet models</li><li>: Cross-platform audio I/O</li><li>: Voice Activity Detection</li><li>: Global keyboard shortcuts and system events</li></ul></li></ul><p>Handy includes an advanced debug mode for development and troubleshooting. Access it by pressing:</p><ul><li>: </li></ul><h2>Known Issues &amp; Current Limitations</h2><p>This project is actively being developed and has some <a href=\"https://github.com/cjpais/Handy/issues\">known issues</a>. We believe in transparency about the current state:</p><ul><li><strong>macOS (both Intel and Apple Silicon)</strong></li></ul><h3>System Requirements/Recommendations</h3><p>The following are recommendations for running Handy on your own machine. If you don't meet the system requirements, the performance of the application may be degraded. We are working on improving the performance across all kinds of computers and hardware.</p><ul><li>: M series Mac, Intel Mac</li><li>: Intel, AMD, or NVIDIA GPU</li><li>: Intel, AMD, or NVIDIA GPU \n  </li></ul><ul><li> - runs on a wide variety of hardware</li><li>: Intel Skylake (6th gen) or equivalent AMD processors</li><li>: ~5x real-time speed on mid-range hardware (tested on i5)</li><li><strong>Automatic language detection</strong> - no manual language selection required</li></ul><p>The goal is to create both a useful tool and a foundation for others to build upon—a well-patterned, simple codebase that serves the community.</p><div align=\"center\">\n  We're grateful for the support of our sponsors who help make Handy possible: \n <a href=\"https://wordcab.com\"><img src=\"https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/wordcab.png\" alt=\"Wordcab\" width=\"120\" height=\"120\"></a><a href=\"https://github.com/epicenter-so/epicenter\"><img src=\"https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/epicenter.png\" alt=\"Epicenter\" width=\"120\" height=\"120\"></a></div><p>MIT License - see <a href=\"https://raw.githubusercontent.com/cjpais/Handy/main/LICENSE\">LICENSE</a> file for details.</p><ul><li> by OpenAI for the speech recognition model</li><li> for amazing cross-platform whisper inference/acceleration</li><li> for great lightweight VAD</li><li> team for the excellent Rust-based app framework</li><li> helping make Handy better</li></ul><p><em>\"Your search for the right speech-to-text tool can end here—not because Handy is perfect, but because you can make it perfect for you.\"</em></p>","contentLength":3699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["trending"]}