{"id":"KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw","title":"GitHub All Languages Daily Trending","displayTitle":"Github Trending","url":"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml","feedLink":"http://mshibanami.github.io/GitHubTrendingRSS","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":47,"items":[{"title":"browser-use/browser-use","url":"https://github.com/browser-use/browser-use","date":1740277746,"author":"","guid":9409,"unread":true,"content":"<p>Make websites accessible for AI agents</p><p>üåê Browser-use is the easiest way to connect your AI agents with the browser.</p><p>üí° See what others are building and share your projects in our <a href=\"https://link.browser-use.com/discord\">Discord</a> - we'd love to see what you create!</p><p>üå©Ô∏è Skip the setup - try our hosted version for instant browser automation! <a href=\"https://cloud.browser-use.com\">Try it now</a>.</p><pre><code>from langchain_openai import ChatOpenAI\nfrom browser_use import Agent\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\n\nasync def main():\n    agent = Agent(\n        task=\"Go to Reddit, search for 'browser-use', click on the first post and return the first comment.\",\n        llm=ChatOpenAI(model=\"gpt-4o\"),\n    )\n    result = await agent.run()\n    print(result)\n\nasyncio.run(main())\n</code></pre><p>Add your API keys for the provider you want to use to your  file.</p><p>Or simply run the gradio example:</p><pre><code>python examples/ui/gradio_demo.py\n</code></pre><p><a href=\"https://github.com/browser-use/browser-use/raw/main/examples/use-cases/shopping.py\">Task</a>: Add grocery items to cart, and checkout.</p><p>Prompt: Add my latest LinkedIn follower to my leads in Salesforce.</p><p><a href=\"https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py\">Prompt</a>: Read my CV &amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.'</p><p><a href=\"https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py\">Prompt</a>: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.</p><p><a href=\"https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py\">Prompt</a>: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.</p><p>For more examples see the <a href=\"https://raw.githubusercontent.com/browser-use/browser-use/main/examples\">examples</a> folder or join the <a href=\"https://link.browser-use.com/discord\">Discord</a> and show off your project.</p><p>Tell your computer what to do, and it gets it done.</p><p>We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the  folder.</p><p>We are forming a commission to define best practices for UI/UX design for browser agents. Together, we're exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.</p><p>Email <a href=\"mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A\">Toby</a> to apply for a seat on the committee.</p><p>If you use Browser Use in your research or project, please cite:</p><pre><code>@software{browser_use2024,\n  author = {M√ºller, Magnus and ≈Ωuniƒç, Gregor},\n  title = {Browser Use: Enable AI to control your browser},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/browser-use/browser-use}\n}\n</code></pre><div align=\"center\">\n  Made with ‚ù§Ô∏è in Zurich and San Francisco \n</div>","contentLength":2268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ossu/computer-science","url":"https://github.com/ossu/computer-science","date":1740277746,"author":"","guid":9410,"unread":true,"content":"<p>üéì Path to a free self-taught education in Computer Science!</p><p>The OSSU curriculum is a <strong>complete education in computer science</strong> using online materials. It's not merely for career training or professional development. It's for those who want a proper,  grounding in concepts fundamental to all computing disciplines, and for those who have the discipline, will, and (most importantly!) good habits to obtain this education largely on their own, but with support from a worldwide community of fellow learners.</p><p>It is designed according to the degree requirements of undergraduate computer science majors, minus general education (non-CS) requirements, as it is assumed most of the people following this curriculum are already educated outside the field of CS. The courses themselves are among the very best in the world, often coming from Harvard, Princeton, MIT, etc., but specifically chosen to meet the following criteria.</p><ul><li>Run regularly (ideally in self-paced format, otherwise running multiple times per year)</li><li>Be of generally high quality in teaching materials and pedagogical principles</li><li>Match the curricular standards of the <a href=\"https://raw.githubusercontent.com/ossu/computer-science/master/CURRICULAR_GUIDELINES.md\">CS 2013</a>: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science</li></ul><p>When no course meets the above criteria, the coursework is supplemented with a book. When there are courses or books that don't fit into the curriculum but are otherwise of high quality, they belong in <a href=\"https://raw.githubusercontent.com/ossu/computer-science/master/extras/courses.md\">extras/courses</a> or <a href=\"https://raw.githubusercontent.com/ossu/computer-science/master/extras/readings.md\">extras/readings</a>.</p><p>. The curriculum is designed as follows:</p><ul><li>: for students to try out CS and see if it's right for them</li><li>: corresponds roughly to the first three years of a computer science curriculum, taking classes that all majors would be required to take</li><li>: corresponds roughly to the final year of a computer science curriculum, taking electives according to the student's interests</li><li>: a project for students to validate, consolidate, and display their knowledge, to be evaluated by their peers worldwide</li></ul><p>. It is possible to finish within about 2 years if you plan carefully and devote roughly 20 hours/week to your studies. Learners can use <a href=\"https://docs.google.com/spreadsheets/d/1bkUU90y4rKYQHwY5AR2iX6iiPTrPEsYs75GkCAkrgm4/copy\">this spreadsheet </a> to estimate their end date. Make a copy and input your start date and expected hours per week in the  sheet. As you work through courses you can enter your actual course completion dates in the  sheet and get updated completion estimates.</p><blockquote><p> While the spreadsheet is a useful tool to estimate the time you need to complete this curriculum, it may not be up-to-date with the curriculum. Use the spreadsheet just to estimate the time you need. Use the <a href=\"https://cs.ossu.dev\">OSSU CS website</a> or <a href=\"https://github.com/ossu/computer-science\">the repo</a> to see what courses to do.</p></blockquote><p>. All or nearly all course material is available for free. However, some courses may charge money for assignments/tests/projects to be graded. Note that both <a href=\"https://www.coursera.support/s/article/209819033-Apply-for-Financial-Aid-or-a-Scholarship?language=en_US\">Coursera</a> and <a href=\"https://courses.edx.org/financial-assistance/\">edX</a> offer financial aid.</p><p>Decide how much or how little to spend based on your own time and budget; just remember that you can't purchase success!</p><p>. Students can work through the curriculum alone or in groups, in order or out of order.</p><ul><li>We recommend doing all courses in Core CS, only skipping a course when you are certain that you've already learned the material previously.</li><li>For simplicity, we recommend working through courses (especially Core CS) in order from top to bottom. Some students choose to study multiple courses at a time in order to vary the material they are working on in a day/week. A popular option is to take the math courses in parallel with the introductory courses. Course prerequisites are listed to help you determine if you are prepared for a given course.</li><li>Courses in Advanced CS are electives. Choose one subject (e.g. Advanced programming) you want to become an expert in and take all the courses under that heading. You can also create your own custom subject; the Discord community may provide feedback on your planned subject.</li></ul><p>. If you plan on showing off some of your coursework publicly, you must share only files that you are allowed to. <em>Respect the code of conduct</em> that you signed in the beginning of each course!</p><blockquote><p> There are a few third-party/deprecated/outdated material that you might find when searching for OSSU. We recommend you to ignore them, and only use the <a href=\"https://cs.ossu.dev\">OSSU CS website</a> or <a href=\"https://github.com/ossu/computer-science\">OSSU CS Github Repo</a>. Some known outdated materials are:</p><ul><li>An unmaintained and deprecated firebase app. Read more in the <a href=\"https://raw.githubusercontent.com/ossu/computer-science/master/FAQ.md#why-is-the-firebase-ossu-app-different-or-broken\">FAQ</a>.</li><li>An unmaintained and deprecated trello board</li><li>Third-party notion templates</li></ul></blockquote><ul><li><a href=\"https://raw.githubusercontent.com/ossu/computer-science/master/#advanced-cs\">Advanced CS</a> assumes the student has already taken the entirety of Core CS and is knowledgeable enough now to decide which electives to take.</li><li>Note that <a href=\"https://raw.githubusercontent.com/ossu/computer-science/master/#advanced-systems\">Advanced systems</a> assumes the student has taken a basic physics course (e.g. AP Physics in high school).</li></ul><p>This course will introduce you to the world of computer science and programming. This course gives you a flavor of the material to come. If you finish the course wanting more, Computer Science is likely for you!</p><p>: <code>basic data structures and algorithms</code></p><p>All coursework under Core CS is , unless otherwise indicated.</p><p>: <code>ML-family languages (via Standard ML)</code><code>Lisp-family languages (via Racket)</code></p><p>Discrete math (Math for CS) is a prerequisite and closely related to the study of algorithms and data structures. Calculus both prepares students for discrete math and helps students develop mathematical maturity.</p><p>: </p><p>Understanding theory is important, but you will also be expected to create programs. There are a number of tools that are widely used to make that process easier. Learn them now to ease your future work writing programs.</p><p>: <code>terminals and shell scripting</code><code>command line environments</code></p><p>: </p><p>: </p><p><code>Confidentiality, Integrity, Availability</code></p><p>Choose  of the following:</p><p>: </p><p>: <code>Privacy and Civil Liberties</code></p><p>After completing  in Core CS, students should choose a subset of courses from Advanced CS based on interest. Not every course from a subcategory needs to be taken. But students should take  course that is relevant to the field they intend to go into.</p><p>: <code>debugging theory and practice</code><code>goal-oriented programming</code><code>object-oriented analysis and design</code><code>large-scale software architecture and design</code></p><p>(*) book by Blackburn, Bos, Striegnitz (compiled from <a href=\"https://github.com/LearnPrologNow/lpn\">source</a>, redistributed under <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC license</a>)</p><p>: <code>processor instruction sets</code><code>synchronization primitives</code></p><p>: <code>distributed shared memory</code><code>state machine replication</code><code>computational geometry theory</code></p><h3>Advanced Information Security</h3><p>Part of learning is doing. The assignments and exams for each course are to prepare you to use your knowledge to solve real-world problems.</p><p>After you've completed Core CS and the parts of Advanced CS relevant to you, you should identify a problem that you can solve using the knowledge you've acquired. You can create something entirely new, or you can improve some tool/program that you use and wish were better.</p><p>Students who would like more guidance in creating a project may choose to use a series of project oriented courses. Here is a sample of options (many more are available, at this point you should be capable of identifying a series that is interesting and relevant to you):</p><p>After completing the requirements of the curriculum above, you will have completed the equivalent of a full bachelor's degree in Computer Science. Congratulations!</p><p>What is next for you? The possibilities are boundless and overlapping:</p><ul><li>Look for a job as a developer!</li><li>Check out the <a href=\"https://raw.githubusercontent.com/ossu/computer-science/master/extras/readings.md\">readings</a> for classic books you can read that will sharpen your skills and expand your knowledge.</li><li>Join a local developer meetup (e.g. via <a href=\"https://www.meetup.com/\">meetup.com</a>).</li><li>Pay attention to emerging technologies in the world of software development: \n  <ul><li>Explore the  through <a href=\"https://elixir-lang.org/\">Elixir</a>, a new functional programming language for the web based on the battle-tested Erlang Virtual Machine!</li><li>Explore  through <a href=\"https://www.rust-lang.org/\">Rust</a>, a systems language which achieves memory- and thread-safety without a garbage collector!</li><li>Explore  through <a href=\"https://www.idris-lang.org/\">Idris</a>, a new Haskell-inspired language with unprecedented support for type-driven development.</li></ul></li></ul><h2>How to show your progress</h2><p><a href=\"https://www.freecodecamp.org/news/how-to-fork-a-github-repository/\">Fork</a> the <a href=\"https://github.com/ossu/computer-science\">GitHub repo</a> into your own GitHub account and put ‚úÖ next to the stuff you've completed as you complete it. This can serve as your <a href=\"https://en.wikipedia.org/wiki/Kanban_board\">kanban board</a> and will be faster to implement than any other solution (giving you time to spend on the courses).</p>","contentLength":8040,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"psf/black","url":"https://github.com/psf/black","date":1740277746,"author":"","guid":9411,"unread":true,"content":"<p>The uncompromising Python code formatter</p><h2 align=\"center\">The Uncompromising Code Formatter</h2><p> is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return,  gives you speed, determinism, and freedom from  nagging about formatting. You will save time and mental energy for more important matters.</p><p>Blackened code looks the same regardless of the project you're reading. Formatting becomes transparent after a while and you can focus on the content instead.</p><p> makes code review faster by producing the smallest diffs possible.</p><p> can be installed by running . It requires Python 3.9+ to run. If you want to format Jupyter Notebooks, install with <code>pip install \"black[jupyter]\"</code>.</p><p>If you can't wait for the latest  and want to install from GitHub, use:</p><p><code>pip install git+https://github.com/psf/black</code></p><p>To get started right away with sensible defaults:</p><pre><code>black {source_file_or_directory}\n</code></pre><p>You can run  as a package if running it as a script doesn't work:</p><pre><code>python -m black {source_file_or_directory}\n</code></pre><p>Further information can be found in our docs:</p><p> is already <a href=\"https://github.com/psf/black#used-by\">successfully used</a> by many projects, small and big.  has a comprehensive test suite, with efficient parallel tests, and our own auto formatting and parallel Continuous Integration runner. Now that we have become stable, you should not expect large formatting changes in the future. Stylistic changes will mostly be responses to bug reports and support for new Python syntax. For more information please refer to <a href=\"https://black.readthedocs.io/en/stable/the_black_code_style/index.html\">The Black Code Style</a>.</p><p>Also, as a safety measure which slows down processing,  will check that the reformatted code still produces a valid AST that is effectively equivalent to the original (see the <a href=\"https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#ast-before-and-after-formatting\">Pragmatism</a> section for details). If you're feeling confident, use .</p><p> is a PEP 8 compliant opinionated formatter.  reformats entire files in place. Style configuration options are deliberately limited and rarely added. It doesn't take previous formatting into account (see <a href=\"https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism\">Pragmatism</a> for exceptions).</p><p>Our documentation covers the current  code style, but planned changes to it are also documented. They're both worth taking a look at:</p><p>Changes to the  code style are bound by the Stability Policy:</p><p>Please refer to this document before submitting an issue. What seems like a bug might be intended behaviour.</p><p>Early versions of  used to be absolutist in some respects. They took after its initial author. This was fine at the time as it made the implementation simpler and there were not many users anyway. Not many edge cases were reported. As a mature tool,  does make some exceptions to rules it otherwise holds.</p><p>Please refer to this document before submitting an issue just like with the document above. What seems like a bug might be intended behaviour.</p><p> is able to read project-specific default values for its command line options from a  file. This is especially useful for specifying custom  and // patterns for your project.</p><p>You can find more details in our documentation:</p><p>And if you're looking for more general configuration documentation:</p><p>: If you're asking yourself \"Do I need to configure anything?\" the answer is \"No\".  is all about sensible defaults. Applying those defaults will have your code in compliance with many other  formatted projects.</p><p>The following notable open-source projects trust  with enforcing a consistent code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs, SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv), pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant, Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more.</p><p>The following organizations use : Dropbox, KeepTruckin, Lyft, Mozilla, Quora, Duolingo, QuantumBlack, Tesla, Archer Aviation.</p><p>Are we missing anyone? Let us know.</p><blockquote><p>I can't think of any single tool in my entire programming career that has given me a bigger productivity increase by its introduction. I can now do refactorings in about 1% of the keystrokes that it would have taken me previously when we had no way for code to format itself.</p></blockquote><blockquote><p> is opinionated so you don't have to be.</p></blockquote><blockquote><p>An auto-formatter that doesn't suck is all I want for Xmas!</p></blockquote><p>, <a href=\"https://www.djangoproject.com/\">Django</a> core developer:</p><blockquote><p>At least the name is good.</p></blockquote><blockquote><p>This vastly improves the formatting of our code. Thanks a ton!</p></blockquote><p>Use the badge in your project's README.md:</p><pre><code>[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n</code></pre><p>Using the badge in README.rst:</p><pre><code>.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://github.com/psf/black\n</code></pre><p>Welcome! Happy to see you willing to make the project better. You can get started by reading this:</p><p>You can also take a look at the rest of the contributing docs or talk with the developers:</p><p>The log has become rather long. It moved to its own file.</p><p>The author list is quite long nowadays, so it lives in its own file.</p><p>Everyone participating in the  project, and in particular in the issue tracker, pull requests, and social media activity, is expected to treat other people with respect and more generally to follow the guidelines articulated in the <a href=\"https://www.python.org/psf/codeofconduct/\">Python Community Code of Conduct</a>.</p><p>At the same time, humor is encouraged. In fact, basic familiarity with Monty Python's Flying Circus is expected. We are not savages.</p><p>And if you  need to slap somebody, do it with a fish while dancing.</p>","contentLength":5334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nndl/nndl.github.io","url":"https://github.com/nndl/nndl.github.io","date":1740277746,"author":"","guid":9412,"unread":true,"content":"<p>„ÄäÁ•ûÁªèÁΩëÁªú‰∏éÊ∑±Â∫¶Â≠¶‰π†„Äã ÈÇ±Èî°ÈπèËëó Neural Network and Deep Learning</p><ol><li>‰øÆÊîπ‰∫ÜÈÉ®ÂàÜÊèèËø∞ÔºöÁ¨¨ 7Á´† ÂâçË®Ä„ÄÅ7.1.3 Á•ûÁªèÁΩëÁªú‰ºòÂåñÁöÑÊîπÂñÑÊñπÊ≥ï „ÄÅ5.1.1 Âç∑ÁßØÁöÑÂÆö‰πâ</li><li>Ë∞ÉÊï¥‰∫ÜÈÉ®ÂàÜ‰π†È¢òÔºöÂéªÊéâÂéü3-3ÔºõÂ¢ûÂä†5-1„ÄÅ14-1 „ÄÅ14-7</li></ol><ol><li>‰øÆÊîπÈÉ®ÂàÜÊèèËø∞ 7.4 Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅ7.5 ÈÄêÂ±ÇÂΩí‰∏ÄÂåñ</li></ol><ol><li>ÂÆåÂñÑ‰∫Ü A.2.4 ÁâπÂæÅÂÄº‰∏éÁâπÂæÅÂêëÈáè„ÄÅA.2.5 Áü©ÈòµÂàÜËß£</li></ol><ol></ol><ol><li>Â∞ÜÂç∞Âà∑ÁâàÂíåiPadÁªü‰∏Ä‰∏∫Áõ∏ÂêåÁâàÂøÉÂ§ßÂ∞èÔºå‰ª•ÂêéÂè™ÂèëÂ∏É‰∏Ä‰∏™ÁâàÊú¨</li></ol><ol><li>Á¨¨13Á´† Ê∑±Â∫¶ÁîüÊàêÊ®°Âûã Ë∞ÉÊï¥‰∫Ü‰ªãÁªç„ÄÅ13.1 Ê¶ÇÁéáÁîüÊàêÊ®°Âûã„ÄÅ13.2.4 Ê®°ÂûãÊ±áÊÄª„ÄÅ13.2.5 ÂÜçÂèÇÊï∞Âåñ</li></ol><ol><li>Êï∞Â≠¶Âü∫Á°Ä Ë∞ÉÊï¥ÂæÆÁßØÂàÜ‰∏ÄËäÇÔºåÂ¢ûÂä†‰∫ÜÊ≥∞ÂãíÂÖ¨ÂºèÔºåÁßØÂàÜÁöÑ‰ªãÁªç</li></ol><ol></ol>","contentLength":634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"koide3/glim","url":"https://github.com/koide3/glim","date":1740277746,"author":"","guid":9413,"unread":true,"content":"<p>GLIM: versatile and extensible range-based 3D localization and mapping framework</p><p> is a versatile and extensible range-based 3D mapping framework.</p><ul><li> GLIM is based on direct multi-scan registration error minimization on factor graphs that enables to accurately retain the consistency of mappint results. GPU acceleration is supported to maximize the mapping speed and quality.</li><li> GLIM offers an interactive map correction interface that enables the user to manually correct mapping failures and easily refine mapping results.</li><li> As we eliminated sensor-specific processes, GLIM can be applied to any kind of range sensors including: \n  <ul><li>Spinning-type LiDAR (e.g., Velodyne HDL32e)</li><li>Non-repetitive scan LiDAR (e.g., Livox Avia)</li><li>Solid-state LiDAR (e.g., Intel Realsense L515)</li><li>RGB-D camera (e.g., Microsoft Azure Kinect)</li></ul></li><li> GLIM provides the global callback slot mechanism that allows to access the internal states of the mapping process and insert additional constraints to the factor graph. We also release <a href=\"https://github.com/koide3/glim_ext\">glim_ext</a> that offers example implementations of several extension functions (e.g., explicit loop detection, LiDAR-Visual-Inertial odometry estimation).</li></ul><p>Tested on Ubuntu 22.04 /24.04 with CUDA 12.2 / 12.5, and NVIDIA Jetson Orin (Jetpack 6.0).</p><p>If you find this package useful for your project, please consider leaving a comment <a href=\"https://github.com/koide3/glim/issues/19\">here</a>. It would help the author receive recognition in his organization and keep working on this project.</p><p>Left: Mapping with various range sensors, Right: Outdoor driving test with Livox MID360</p><p>GLIM provides several estimation modules to cover use scenarios, from robust and accurate mapping with a GPU to lightweight real-time mapping with a low-specification PC like Raspberry Pi.</p><p>If you find this package useful for your project, please consider leaving a comment <a href=\"https://github.com/koide3/glim/issues/19\">here</a>. It would help the author receive recognition in his organization and keep working on this project. Please also cite the following paper if you use this package in your academic work.</p><p>This package is released under the MIT license. For commercial support, please contact .</p><p>Koide et al., \"GLIM: 3D Range-Inertial Localization and Mapping with GPU-Accelerated Scan Matching Factors\", Robotics and Autonomous Systems, 2024, <a href=\"https://doi.org/10.1016/j.robot.2024.104750\">[DOI]</a><a href=\"https://arxiv.org/abs/2407.10344\">[Arxiv]</a></p><p>The GLIM framework involves ideas expanded from the following papers:</p><ul><li>(LiDAR-IMU odometry and mapping) \"Globally Consistent and Tightly Coupled 3D LiDAR Inertial Mapping\", ICRA2022 <a href=\"https://doi.org/10.1109/ICRA46639.2022.9812385\">[DOI]</a></li><li>(Global registration error minimization) \"Globally Consistent 3D LiDAR Mapping with GPU-accelerated GICP Matching Cost Factors\", IEEE RA-L, 2021, <a href=\"https://doi.org/10.1109/LRA.2021.3113043\">[DOI]</a></li><li>(GPU-accelerated scan matching) \"Voxelized GICP for Fast and Accurate 3D Point Cloud Registration\", ICRA2021, <a href=\"https://doi.org/10.1109/ICRA48506.2021.9560835\">[DOI]</a></li></ul>","contentLength":2662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"twbs/bootstrap","url":"https://github.com/twbs/bootstrap","date":1740191079,"author":"","guid":8919,"unread":true,"content":"<p>The most popular HTML, CSS, and JavaScript framework for developing responsive, mobile first projects on the web.</p><p>Our default branch is for development of our Bootstrap 5 release. Head to the <a href=\"https://github.com/twbs/bootstrap/tree/v4-dev\"> branch</a> to view the readme, documentation, and source code for Bootstrap 4.</p><p>Several quick start options are available:</p><ul><li>Clone the repo: <code>git clone https://github.com/twbs/bootstrap.git</code></li><li>Install with <a href=\"https://www.npmjs.com/\">npm</a>: <code>npm install bootstrap@v5.3.3</code></li><li>Install with <a href=\"https://yarnpkg.com/\">yarn</a>: <code>yarn add bootstrap@v5.3.3</code></li><li>Install with <a href=\"https://getcomposer.org/\">Composer</a>: <code>composer require twbs/bootstrap:5.3.3</code></li><li>Install with <a href=\"https://www.nuget.org/\">NuGet</a>: CSS: <code>Install-Package bootstrap</code> Sass: <code>Install-Package bootstrap.sass</code></li></ul><p>Read the <a href=\"https://getbootstrap.com/docs/5.3/getting-started/introduction/\">Getting started page</a> for information on the framework contents, templates, examples, and more.</p><p>Within the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations.</p><p>We provide compiled CSS and JS (), as well as compiled and minified CSS and JS (). <a href=\"https://web.dev/articles/source-maps\">Source maps</a> () are available for use with certain browsers' developer tools. Bundled JS files ( and minified ) include <a href=\"https://popper.js.org/docs/v2/\">Popper</a>.</p><h2>Bugs and feature requests</h2><p>Bootstrap's documentation, included in this repo in the root directory, is built with <a href=\"https://gohugo.io/\">Hugo</a> and publicly hosted on GitHub Pages at <a href=\"https://getbootstrap.com/\">https://getbootstrap.com/</a>. The docs may also be run locally.</p><h3>Running documentation locally</h3><ol><li>Run  to install the Node.js dependencies, including Hugo (the site builder).</li><li>Run  (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.</li><li>From the root  directory, run  in the command line.</li><li>Open  in your browser, and voil√†.</li></ol><h3>Documentation for previous releases</h3><p>Please read through our <a href=\"https://github.com/twbs/bootstrap/raw/main/.github/CONTRIBUTING.md\">contributing guidelines</a>. Included are directions for opening issues, coding standards, and notes on development.</p><p>Get updates on Bootstrap's development and chat with the project maintainers and community members.</p><p>For transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under <a href=\"https://semver.org/\">the Semantic Versioning guidelines</a>. Sometimes we screw up, but we adhere to those rules whenever possible.</p><a href=\"https://www.browserstack.com/\"><img src=\"https://live.browserstack.com/images/opensource/browserstack-logo.svg?sanitize=true\" alt=\"BrowserStack\" width=\"192\" height=\"42\"></a><p>Thanks to <a href=\"https://www.browserstack.com/\">BrowserStack</a> for providing the infrastructure that allows us to test in real browsers!</p><a href=\"https://www.netlify.com/\"><img src=\"https://www.netlify.com/v3/img/components/full-logo-light.svg?sanitize=true\" alt=\"Netlify\" width=\"147\" height=\"40\"></a><p>Thanks to <a href=\"https://www.netlify.com/\">Netlify</a> for providing us with Deploy Previews!</p><p>Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [<a href=\"https://opencollective.com/bootstrap#sponsor\">Become a sponsor</a>]</p>","contentLength":2356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mountain-loop/yaak","url":"https://github.com/mountain-loop/yaak","date":1740191079,"author":"","guid":8920,"unread":true,"content":"<p>The most intuitive desktop API client. Organize and execute REST, GraphQL, WebSockets, Server Sent Events, and gRPC ü¶¨</p><p>Yaak is a desktop API client for interacting with REST, GraphQL, Server Sent Events (SSE), WebSocket, and gRPC APIs. It's built using <a href=\"https://tauri.app\">Tauri</a>, Rust, and ReactJS.</p><ul><li>ü™Ç Import data from Postman, Insomnia, OpenAPI, Swagger, or Curl.</li><li>üì§ Send requests via REST, GraphQL, Server Sent Events (SSE), WebSockets, or gRPC.</li><li>üîê Automatically authorize requests with OAuth 2.0, JWT tokens, Basic Auth, and more.</li><li>üîé Filter response bodies using JSONPath or XPath queries.</li><li>‚õìÔ∏è Chain together multiple requests to dynamically reference values.</li><li>üìÇ Organize requests into workspaces and nested folders.</li><li>üßÆ Use environment variables to easily switch between Prod and Dev.</li><li>üè∑Ô∏è Send dynamic values like UUIDs or timestamps using template tags.</li><li>üé® Choose from many of the included themes, or make your own.</li><li>üíΩ Mirror workspace data to a directory for integration with Git or Dropbox.</li><li>üìú View response history for each request.</li><li>üîå Create your own plugins for authentication, template tags, and more!</li><li>üõú Configure a proxy to access firewall-blocked APIs</li></ul><p>All feedback, bug reports, questions, and feature requests should be reported to <a href=\"https://feedback.yaak.app\">feedback.yaak.app</a>.</p><ul><li><a href=\"https://github.com/BiteCraft/yaak2postman\"></a> CLI for converting Yaak data exports to Postman-compatible collections</li></ul><p>Yaak is open source, but only accepting contributions for bug fixes. To get started, visit <a href=\"https://raw.githubusercontent.com/mountain-loop/yaak/master/DEVELOPMENT.md\"></a> for tips on setting up your environment.</p>","contentLength":1463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NationalSecurityAgency/ghidra","url":"https://github.com/NationalSecurityAgency/ghidra","date":1740191079,"author":"","guid":8921,"unread":true,"content":"<p>Ghidra is a software reverse engineering (SRE) framework</p><img src=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/Ghidra/Features/Base/src/main/resources/images/GHIDRA_3.png\" width=\"400\"><p>Ghidra is a software reverse engineering (SRE) framework created and maintained by the <a href=\"https://www.nsa.gov\">National Security Agency</a> Research Directorate. This framework includes a suite of full-featured, high-end software analysis tools that enable users to analyze compiled code on a variety of platforms including Windows, macOS, and Linux. Capabilities include disassembly, assembly, decompilation, graphing, and scripting, along with hundreds of other features. Ghidra supports a wide variety of processor instruction sets and executable formats and can be run in both user-interactive and automated modes. Users may also develop their own Ghidra extension components and/or scripts using Java or Python.</p><p>In support of NSA's Cybersecurity mission, Ghidra was built to solve scaling and teaming problems on complex SRE efforts, and to provide a customizable and extensible SRE research platform. NSA has applied Ghidra SRE capabilities to a variety of problems that involve analyzing malicious code and generating deep insights for SRE analysts who seek a better understanding of potential vulnerabilities in networks and systems.</p><p>If you are a U.S. citizen interested in projects like this, to develop Ghidra and other cybersecurity tools for NSA to help protect our nation and its allies, consider applying for a <a href=\"https://www.intelligencecareers.gov/nsa\">career with us</a>.</p><p> There are known security vulnerabilities within certain versions of Ghidra. Before proceeding, please read through Ghidra's <a href=\"https://github.com/NationalSecurityAgency/ghidra/security/advisories\">Security Advisories</a> for a better understanding of how you might be impacted.</p><p>To install an official pre-built multi-platform Ghidra release:</p><ul><li>Download a Ghidra <a href=\"https://github.com/NationalSecurityAgency/ghidra/releases\">release file</a><ul><li> The official multi-platform release file is named <code>ghidra_&lt;version&gt;_&lt;release&gt;_&lt;date&gt;.zip</code> which can be found under the \"Assets\" drop-down. Downloading either of the files named \"Source Code\" is not correct for this step.</li></ul></li><li>Extract the Ghidra release file</li><li>Launch Ghidra:  ( for Windows) \n  <ul><li>or launch <a href=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/Ghidra/Features/PyGhidra/README.md\">PyGhidra</a>:  ( for Windows)</li></ul></li></ul><p>For additional information and troubleshooting tips about installing and running a Ghidra release, please refer to the <a href=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/GhidraDocs/InstallationGuide.md\">Installation Guide</a> which can be found in a Ghidra release at <code>docs/InstallationGuide.html</code>.</p><p>To create the latest development build for your platform from this source repository:</p><h5>Download and extract the source:</h5><pre><code>unzip ghidra-master\ncd ghidra-master\n</code></pre><p> Instead of downloading the compressed source, you may instead want to clone the GitHub repository: <code>git clone https://github.com/NationalSecurityAgency/ghidra.git</code></p><h5>Download additional build dependencies into source repository:</h5><p> If an Internet connection is available and you did not install Gradle, the following  commands may be replaced with .</p><pre><code>gradle -I gradle/support/fetchDependencies.gradle\n</code></pre><h5>Create development build:</h5><p>The compressed development build will be located at .</p><p>For more detailed information on building Ghidra, please read the <a href=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/DevGuide.md\">Developer Guide</a>.</p><p>For issues building, please check the <a href=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/DevGuide.md#known-issues\">Known Issues</a> section for possible solutions.</p><h3>User Scripts and Extensions</h3><p>Ghidra installations support users writing custom scripts and extensions via the  plugin for Eclipse. The plugin and its corresponding instructions can be found within a Ghidra release at <code>Extensions/Eclipse/GhidraDev/</code> or at <a href=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/GhidraBuild/EclipsePlugins/GhidraDev/GhidraDevPlugin/README.md\">this link</a>. Alternatively, Visual Studio Code may be used to edit scripts by clicking the Visual Studio Code icon in the Script Manager. Fully-featured Visual Studio Code projects can be created from a Ghidra CodeBrowser window at <em>Tools -&gt; Create VSCode Module project</em>.</p><p> Both the  plugin for Eclipse and Visual Studio Code integrations only support developing against fully built Ghidra installations which can be downloaded from the <a href=\"https://github.com/NationalSecurityAgency/ghidra/releases\">Releases</a> page.</p><p>To develop the Ghidra tool itself, it is highly recommended to use Eclipse, which the Ghidra development process has been highly customized for.</p><h5>Install build and development tools:</h5><h5>Prepare the development environment:</h5><pre><code>gradle prepdev eclipse buildNatives\n</code></pre><h5>Import Ghidra projects into Eclipse:</h5><ul><li> | <em>Existing Projects into Workspace</em></li><li>Select root directory to be your downloaded or cloned ghidra source repository</li><li>Check <em>Search for nested projects</em></li></ul><p>When Eclipse finishes building the projects, Ghidra can be launched and debugged with the provided  Eclipse .</p><p>For more detailed information on developing Ghidra, please read the <a href=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/DevGuide.md\">Developer Guide</a>.</p><p>If you would like to contribute bug fixes, improvements, and new features back to Ghidra, please take a look at our <a href=\"https://raw.githubusercontent.com/NationalSecurityAgency/ghidra/master/CONTRIBUTING.md\">Contributor Guide</a> to see how you can participate in this open source project.</p>","contentLength":4491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"firebase/firebase-ios-sdk","url":"https://github.com/firebase/firebase-ios-sdk","date":1740191079,"author":"","guid":8922,"unread":true,"content":"<p>Firebase SDK for Apple App Development</p><p>This repository contains the source code for all Apple platform Firebase SDKs except FirebaseAnalytics.</p><p>Firebase is an app development platform with tools to help you build, grow, and monetize your app. More information about Firebase can be found on the <a href=\"https://firebase.google.com\">official Firebase website</a>.</p><p>See the subsections below for details about the different installation methods. Where available, it's recommended to install any libraries with a  suffix to get the best experience when writing your app in Swift.</p><p>These instructions can be used to access the Firebase repo at other branches, tags, or commits.</p><h4>Accessing Firebase Source Snapshots</h4><p>All official releases are tagged in this repo and available via CocoaPods. To access a local source snapshot or unreleased branch, use Podfile directives like the following:</p><p>To access FirebaseFirestore via a branch:</p><pre><code>pod 'FirebaseCore', :git =&gt; 'https://github.com/firebase/firebase-ios-sdk.git', :branch =&gt; 'main'\npod 'FirebaseFirestore', :git =&gt; 'https://github.com/firebase/firebase-ios-sdk.git', :branch =&gt; 'main'\n</code></pre><p>To access FirebaseMessaging via a checked-out version of the firebase-ios-sdk repo:</p><pre><code>pod 'FirebaseCore', :path =&gt; '/path/to/firebase-ios-sdk'\npod 'FirebaseMessaging', :path =&gt; '/path/to/firebase-ios-sdk'\n</code></pre><p>Instructions for the experimental Carthage distribution can be found at <a href=\"https://raw.githubusercontent.com/firebase/firebase-ios-sdk/main/Carthage.md\">Carthage.md</a>.</p><h3>Using Firebase from a Framework or a library</h3><p>To develop Firebase software in this repository, ensure that you have at least the following software:</p><p>CocoaPods is still the canonical way to develop, but much of the repo now supports development with Swift Package Manager.</p><p>For the pod that you want to develop:</p><pre><code>pod gen Firebase{name here}.podspec --local-sources=./ --auto-open --platforms=ios\n</code></pre><p>Note: If the CocoaPods cache is out of date, you may need to run  before the  command.</p><p>Note: Set the  option to  or  to develop/test for those platforms. Since 10.2, Xcode does not properly handle multi-platform CocoaPods workspaces.</p><ul><li><code>pod gen {name here}.podspec --local-sources=./ --auto-open --platforms=ios</code></li><li>Check the Mac box in the App-iOS Build Settings</li><li>Sign the App in the Settings Signing &amp; Capabilities tab</li><li>Click Pods in the Project Manager</li><li>Add Signing to the iOS host app and unit test targets</li><li>Select the Unit-unit scheme</li></ul><p>Alternatively, disable signing in each target:</p><ul><li>Select </li><li>Add  setting with a value of </li></ul><ul><li>To enable test schemes: <code>./scripts/setup_spm_tests.sh</code></li><li> or double click  in Finder.</li><li>Xcode will open the project \n  <ul><li>Choose a scheme for a library to build or test suite to run</li><li>Choose a target platform by selecting the run destination along with the scheme</li></ul></li></ul><h3>Adding a New Firebase Pod</h3><h3>Managing Headers and Imports</h3><p>For information about managing headers and imports, see <a href=\"https://raw.githubusercontent.com/firebase/firebase-ios-sdk/main/HeadersImports.md\">HeadersImports</a> Markdown file.</p><p>To ensure that the code is formatted consistently, run the script <a href=\"https://github.com/firebase/firebase-ios-sdk/raw/main/scripts/check.sh\">./scripts/check.sh</a> before creating a pull request (PR).</p><p>GitHub Actions will verify that any code changes are done in a style-compliant way. Install  and :</p><pre><code>brew install clang-format@19\nbrew install mint\n</code></pre><p>Select a scheme and press Command-u to build a component and run its unit tests.</p><p>To run the sample apps and integration tests, you'll need a valid  file. The Firebase Xcode project contains dummy plist files without real values, but they can be replaced with real plist files. To get your own  files:</p><ol><li>Create a new Firebase project, if you don't already have one</li><li>For each sample app you want to test, create a new Firebase app with the sample app's bundle identifier (e.g., <code>com.google.Database-Example</code>)</li><li>Download the resulting  and add it to the Xcode project.</li></ol><h3>Coverage Report Generation</h3><h2>Specific Component Instructions</h2><p>See the sections below for any special instructions for those components.</p><p>For specific Firebase Auth development, refer to the <a href=\"https://raw.githubusercontent.com/firebase/firebase-ios-sdk/main/FirebaseAuth/Tests/Sample/README.md\">Auth Sample README</a> for instructions about building and running the FirebaseAuth pod along with various samples and tests.</p><p>The Firebase Database Integration tests can be run against a locally running Database Emulator or against a production instance.</p><p>To run against a local emulator instance, invoke <code>./scripts/run_database_emulator.sh start</code> before running the integration test.</p><p>To run against a production instance, provide a valid <code>GoogleServices-Info.plist</code> and copy it to <code>FirebaseDatabase/Tests/Resources/GoogleService-Info.plist</code>. Your Security Rule must be set to <a href=\"https://firebase.google.com/docs/database/security/quickstart\">public</a> while your tests are running.</p><p>Firebase Dynamic Links is  and should not be used in new projects. The service will shut down on August 25, 2025.</p><h3>Firebase Performance Monitoring</h3><p>Push notifications can only be delivered to specially provisioned App IDs in the developer portal. In order to test receiving push notifications, you will need to:</p><p>The iOS Simulator cannot register for remote notifications and will not receive push notifications. To receive push notifications, follow the steps above and run the app on a physical device.</p><h2>Building with Firebase on Apple platforms</h2><p>Firebase provides official beta support for macOS, Catalyst, and tvOS. visionOS and watchOS are community supported. Thanks to community contributions for many of the multi-platform PRs.</p><p>At this time, most of Firebase's products are available across Apple platforms. There are still a few gaps, especially on visionOS and watchOS. For details about the current support matrix, see <a href=\"https://firebase.google.com/docs/ios/learn-more#firebase_library_support_by_platform\">this chart</a> in Firebase's documentation.</p><p>Where supported, visionOS works as expected with the exception of Firestore via Swift Package Manager where it is required to use the source distribution.</p><p>To enable the Firestore source distribution, quit Xcode and open the desired project from the command line with the <code>FIREBASE_SOURCE_FIRESTORE</code> environment variable: <code>open --env FIREBASE_SOURCE_FIRESTORE /path/to/project.xcodeproj</code>. To go back to using the binary distribution of Firestore, quit Xcode and open Xcode like normal, without the environment variable.</p><p>Thanks to contributions from the community, many of Firebase SDKs now compile, run unit tests, and work on watchOS. See the <a href=\"https://raw.githubusercontent.com/firebase/firebase-ios-sdk/main/Example/watchOSSample\">Independent Watch App Sample</a>.</p><p>Keep in mind that watchOS is not officially supported by Firebase. While we can catch basic unit test issues with GitHub Actions, there may be some changes where the SDK no longer works as expected on watchOS. If you encounter this, please <a href=\"https://github.com/firebase/firebase-ios-sdk/issues\">file an issue</a>.</p><p>During app setup in the console, you may get to a step that mentions something like \"Checking if the app has communicated with our servers\". This relies on Analytics and will not work on watchOS. <strong>It's safe to ignore the message and continue</strong>, the rest of the SDKs will work as expected.</p><h4>Additional Crashlytics Notes</h4><ul><li>watchOS has limited support. Due to watchOS restrictions, mach exceptions and signal crashes are not recorded. (Crashes in SwiftUI are generated as mach exceptions, so will not be recorded)</li></ul><p>Thanks to contributions from the community,  contains support for Apple's Combine framework. This module is currently under development and not yet supported for use in production environments. For more details, please refer to the <a href=\"https://raw.githubusercontent.com/firebase/firebase-ios-sdk/main/FirebaseCombineSwift/README.md\">docs</a>.</p><p>See <a href=\"https://raw.githubusercontent.com/firebase/firebase-ios-sdk/main/ROADMAP.md\">Roadmap</a> for more about the Firebase Apple SDK Open Source plans and directions.</p><p>See <a href=\"https://raw.githubusercontent.com/firebase/firebase-ios-sdk/main/CONTRIBUTING.md\">Contributing</a> for more information on contributing to the Firebase Apple SDK.</p>","contentLength":7121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alamofire/Alamofire","url":"https://github.com/Alamofire/Alamofire","date":1740191079,"author":"","guid":8923,"unread":true,"content":"<p>Elegant HTTP Networking in Swift</p><p>Alamofire is an HTTP networking library written in Swift.</p><p>Alamofire's compact syntax and extensive feature set allow requests with powerful features like automatic retry to be written in just a few lines of code.</p><pre><code>// Automatic String to URL conversion, Swift concurrency support, and automatic retry.\nlet response = await AF.request(\"https://httpbin.org/get\", interceptor: .retryPolicy)\n                       // Automatic HTTP Basic Auth.\n                       .authenticate(username: \"user\", password: \"pass\")\n                       // Caching customization.\n                       .cacheResponse(using: .cache)\n                       // Redirect customization.\n                       .redirect(using: .follow)\n                       // Validate response code and Content-Type.\n                       .validate()\n                       // Produce a cURL command for the request.\n                       .cURLDescription { description in\n                         print(description)\n                       }\n                       // Automatic Decodable support with background parsing.\n                       .serializingDecodable(DecodableType.self)\n                       // Await the full response with metrics and a parsed body.\n                       .response\n// Detailed response description for easy debugging.\ndebugPrint(response)\n</code></pre><p>In order to keep Alamofire focused specifically on core networking implementations, additional component libraries have been created by the <a href=\"https://github.com/Alamofire/Foundation\">Alamofire Software Foundation</a> to bring additional functionality to the Alamofire ecosystem.</p><ul><li><a href=\"https://github.com/Alamofire/AlamofireImage\">AlamofireImage</a> - An image library including image response serializers,  and  extensions, custom image filters, an auto-purging in-memory cache, and a priority-based image downloading system.</li><li><a href=\"https://github.com/Alamofire/AlamofireNetworkActivityIndicator\">AlamofireNetworkActivityIndicator</a> - Controls the visibility of the network activity indicator on iOS using Alamofire. It contains configurable delay timers to help mitigate flicker and can support  instances not managed by Alamofire.</li></ul><h4>Known Issues on Linux and Windows</h4><p>Alamofire builds on Linux, Windows, and Android but there are missing features and many issues in the underlying <code>swift-corelibs-foundation</code> that prevent full functionality and may cause crashes. These include:</p><ul><li> and associated certificate functionality is unavailable, so there is no certificate pinning and no client certificate support.</li><li>Various methods of HTTP authentication may crash, including HTTP Basic and HTTP Digest. Crashes may occur if responses contain server challenges.</li><li>Cache control through  and associated APIs is unavailable, as the underlying delegate methods aren't called.</li><li> are never gathered.</li><li> is not available.</li></ul><p>Due to these issues, Alamofire is unsupported on Linux, Windows, and Android. Please report any crashes to the <a href=\"https://bugs.swift.org\">Swift bug reporter</a>.</p><p>The <a href=\"https://swift.org/package-manager/\">Swift Package Manager</a> is a tool for automating the distribution of Swift code and is integrated into the  compiler.</p><p>Once you have your Swift package set up, adding Alamofire as a dependency is as easy as adding it to the  value of your  or the Package list in Xcode.</p><pre><code>dependencies: [\n    .package(url: \"https://github.com/Alamofire/Alamofire.git\", .upToNextMajor(from: \"5.10.0\"))\n]\n</code></pre><p>Normally you'll want to depend on the  target:</p><pre><code>.product(name: \"Alamofire\", package: \"Alamofire\")\n</code></pre><p>But if you want to force Alamofire to be dynamically linked (do not do this unless you're sure you need it), you can depend on the  target:</p><pre><code>.product(name: \"AlamofireDynamic\", package: \"Alamofire\")\n</code></pre><p><a href=\"https://cocoapods.org\">CocoaPods</a> is a dependency manager for Cocoa projects. For usage and installation instructions, visit their website. To integrate Alamofire into your Xcode project using CocoaPods, specify it in your :</p><p><a href=\"https://github.com/Carthage/Carthage\">Carthage</a> is a decentralized dependency manager that builds your dependencies and provides you with binary frameworks. To integrate Alamofire into your Xcode project using Carthage, specify it in your :</p><pre><code>github \"Alamofire/Alamofire\"\n</code></pre><p>If you prefer not to use any of the aforementioned dependency managers, you can integrate Alamofire into your project manually.</p><ul><li><p>Open up Terminal,  into your top-level project directory, and run the following command \"if\" your project is not initialized as a git repository:</p></li><li><p>Add Alamofire as a git <a href=\"https://git-scm.com/docs/git-submodule\">submodule</a> by running the following command:</p><pre><code>$ git submodule add https://github.com/Alamofire/Alamofire.git\n</code></pre></li><li><p>Open the new  folder, and drag the  into the Project Navigator of your application's Xcode project.</p><blockquote><p>It should appear nested underneath your application's blue project icon. Whether it is above or below all the other Xcode groups does not matter.</p></blockquote></li><li><p>Select the  in the Project Navigator and verify the deployment target matches that of your application target.</p></li><li><p>Next, select your application project in the Project Navigator (blue project icon) to navigate to the target configuration window and select the application target under the \"Targets\" heading in the sidebar.</p></li><li><p>In the tab bar at the top of that window, open the \"General\" panel.</p></li><li><p>Click on the  button under the \"Embedded Binaries\" section.</p></li><li><p>You will see two different  folders each with two different versions of the  nested inside a  folder.</p><blockquote><p>It does not matter which  folder you choose from, but it does matter whether you choose the top or bottom .</p></blockquote></li><li><p>Select the top  for iOS and the bottom one for macOS.</p><blockquote><p>You can verify which one you selected by inspecting the build log for your project. The build target for  will be listed as , , , or .</p></blockquote></li><li><blockquote><p>The  is automagically added as a target dependency, linked framework and embedded framework in a copy files build phase which is all you need to build on the simulator and a device.</p></blockquote></li></ul><p>Before contributing to Alamofire, please read the instructions detailed in our <a href=\"https://github.com/Alamofire/Alamofire/raw/master/CONTRIBUTING.md\">contribution guide</a>.</p><p>The following radars have some effect on the current implementation of Alamofire.</p><ul><li><a href=\"http://www.openradar.me/radar?id=5517037090635776\"></a> - Compiler throwing warning due to toll-free bridging issue in the test case</li><li> - Background URL Session Configurations do not work in the simulator</li><li> - Some URLProtocol APIs do not properly handle </li></ul><p>The following radars have been resolved over time after being filed against the Alamofire project.</p><ul><li><a href=\"http://www.openradar.me/radar?id=5010235949318144\"></a> - Swift string interpolation causing memory leak with common usage. \n  <ul><li>(Resolved): 9/1/17 in Xcode 9 beta 6.</li></ul></li><li><a href=\"http://openradar.appspot.com/radar?id=4942308441063424\"></a> -  failing to link on watchOS 3.0+ \n  <ul><li>(Resolved): Just add  to your linked frameworks.</li></ul></li><li> - <code>urlSession(_:task:didFinishCollecting:)</code> never called on watchOS \n  <ul><li>(Resolved): Metrics now collected on watchOS 7+.</li></ul></li></ul><h3>What's the origin of the name Alamofire?</h3><p>Alamofire is named after the <a href=\"https://aggie-horticulture.tamu.edu/wildseed/alamofire.html\">Alamo Fire flower</a>, a hybrid variant of the Bluebonnet, the official state flower of Texas.</p><p>If you believe you have identified a security vulnerability with Alamofire, you should report it as soon as possible via email to <a href=\"mailto:security@alamofire.org\">security@alamofire.org</a>. Please do not post it to a public issue tracker.</p><p>The <a href=\"https://github.com/Alamofire/Foundation#members\">ASF</a> is looking to raise money to officially stay registered as a federal non-profit organization. Registering will allow Foundation members to gain some legal protections and also allow us to put donations to use, tax-free. Sponsoring the ASF will enable us to:</p><ul><li>Pay our yearly legal fees to keep the non-profit in good status</li><li>Pay for our mail servers to help us stay on top of all questions and security issues</li><li>Potentially fund test servers to make it easier for us to test the edge cases</li><li>Potentially fund developers to work on one of our projects full-time</li></ul><p>The community adoption of the ASF libraries has been amazing. We are greatly humbled by your enthusiasm around the projects and want to continue to do everything we can to move the needle forward. With your continued support, the ASF will be able to improve its reach and also provide better legal safety for the core members. If you use any of our libraries for work, see if your employers would be interested in donating. Any amount you can donate, whether once or monthly, to help us reach our goal would be greatly appreciated.</p><p><a href=\"https://macstadium.com\">MacStadium</a> provides Alamofire with a free, hosted Mac mini.</p><p>Alamofire is released under the MIT license. <a href=\"https://github.com/Alamofire/Alamofire/raw/master/LICENSE\">See LICENSE</a> for details.</p>","contentLength":7945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"infiniflow/ragflow","url":"https://github.com/infiniflow/ragflow","date":1740191079,"author":"","guid":8924,"unread":true,"content":"<p>RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.</p><p><a href=\"https://ragflow.io/\">RAGFlow</a> is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data.</p><ul><li>2025-02-05 Updates the model list of 'SILICONFLOW' and adds support for Deepseek-R1/DeepSeek-V3.</li><li>2025-01-26 Optimizes knowledge graph extraction and application, offering various configuration options.</li><li>2024-12-18 Upgrades Document Layout Analysis model in Deepdoc.</li><li>2024-12-04 Adds support for pagerank score in knowledge base.</li><li>2024-11-22 Adds more variables to Agent.</li><li>2024-11-01 Adds keyword extraction and related question generation to the parsed chunks to improve the accuracy of retrieval.</li><li>2024-08-22 Support text to SQL statements through RAG.</li></ul><p>‚≠êÔ∏è Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! üåü</p><h3>üç≠ <strong>\"Quality in, quality out\"</strong></h3><ul><li>Finds \"needle in a data haystack\" of literally unlimited tokens.</li></ul><ul><li>Intelligent and explainable.</li><li>Plenty of template options to choose from.</li></ul><h3>üå± <strong>Grounded citations with reduced hallucinations</strong></h3><ul><li>Visualization of text chunking to allow human intervention.</li><li>Quick view of the key references and traceable citations to support grounded answers.</li></ul><h3>üçî <strong>Compatibility with heterogeneous data sources</strong></h3><ul><li>Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.</li></ul><h3>üõÄ <strong>Automated and effortless RAG workflow</strong></h3><ul><li>Streamlined RAG orchestration catered to both personal and large businesses.</li><li>Configurable LLMs as well as embedding models.</li><li>Multiple recall paired with fused re-ranking.</li><li>Intuitive APIs for seamless integration with business.</li></ul><ul><li>Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1 \n  <blockquote><p>If you have not installed Docker on your local machine (Windows, Mac, or Linux), see <a href=\"https://docs.docker.com/engine/install/\">Install Docker Engine</a>.</p></blockquote></li></ul><ol><li><p>Ensure  &gt;= 262144:</p><blockquote><p>To check the value of :</p><pre><code>$ sysctl vm.max_map_count\n</code></pre><p>Reset  to a value at least 262144 if it is not.</p><pre><code># In this case, we set it to 262144:\n$ sudo sysctl -w vm.max_map_count=262144\n</code></pre><p>This change will be reset after a system reboot. To ensure your change remains permanent, add or update the  value in  accordingly:</p></blockquote></li><li><pre><code>$ git clone https://github.com/infiniflow/ragflow.git\n</code></pre></li><li><p>Start up the server using the pre-built Docker images:</p><blockquote><p>The command below downloads the  edition of the RAGFlow Docker image. Refer to the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from , update the  variable accordingly in  before using  to start the server. For example: set <code>RAGFLOW_IMAGE=infiniflow/ragflow:v0.16.0</code> for the full edition .</p></blockquote><pre><code>$ cd ragflow/docker\n$ docker compose -f docker-compose.yml up -d\n</code></pre><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table></li><li><p>Check the server status after having the server up and running:</p><pre><code>$ docker logs -f ragflow-server\n</code></pre><p><em>The following output confirms a successful launch of the system:</em></p><pre><code>\n      ____   ___    ______ ______ __\n     / __ \\ /   |  / ____// ____// /____  _      __\n    / /_/ // /| | / / __ / /_   / // __ \\| | /| / /\n   / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /\n  /_/ |_|/_/  |_|\\____//_/    /_/ \\____/ |__/|__/\n\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:9380\n * Running on http://x.x.x.x:9380\n INFO:werkzeug:Press CTRL+C to quit\n</code></pre><blockquote><p>If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a  error because, at that moment, your RAGFlow may not be fully initialized.</p></blockquote></li><li><p>In your web browser, enter the IP address of your server and log in to RAGFlow.</p><blockquote><p>With the default settings, you only need to enter <code>http://IP_OF_YOUR_MACHINE</code> ( port number) as the default HTTP serving port  can be omitted when using the default configurations.</p></blockquote></li></ol><p>When it comes to system configurations, you will need to manage the following files:</p><ul><li><a href=\"https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env\">.env</a>: Keeps the fundamental setups for the system, such as , , and .</li><li><a href=\"https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template\">service_conf.yaml.template</a>: Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.</li></ul><p>To update the default HTTP serving port (80), go to <a href=\"https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml\">docker-compose.yml</a> and change  to .</p><p>Updates to the above configurations require a reboot of all containers to take effect:</p><blockquote><pre><code>$ docker compose -f docker-compose.yml up -d\n</code></pre></blockquote><h3>Switch doc engine from Elasticsearch to Infinity</h3><p>RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to <a href=\"https://github.com/infiniflow/infinity/\">Infinity</a>, follow these steps:</p><ol><li><p>Stop all running containers:</p><pre><code>$ docker compose -f docker/docker-compose.yml down -v\n</code></pre><p>Note:  will delete the docker container volumes, and the existing data will be cleared.</p></li><li><p>Set  in  to .</p></li><li><pre><code>$ docker compose -f docker-compose.yml up -d\n</code></pre></li></ol><blockquote><p>[!WARNING] Switching to Infinity on a Linux/arm64 machine is not yet officially supported.</p></blockquote><h2>üîß Build a Docker image without embedding models</h2><p>This image is approximately 2 GB in size and relies on external LLM and embedding services.</p><pre><code>git clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build --build-arg LIGHTEN=1 -f Dockerfile -t infiniflow/ragflow:nightly-slim .\n</code></pre><h2>üîß Build a Docker image including embedding models</h2><p>This image is approximately 9 GB in size. As it includes embedding models, it relies on external LLM services only.</p><pre><code>git clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build -f Dockerfile -t infiniflow/ragflow:nightly .\n</code></pre><h2>üî® Launch service from source for development</h2><ol><li><p>Install uv, or skip this step if it is already installed:</p></li><li><p>Clone the source code and install Python dependencies:</p><pre><code>git clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\nuv sync --python 3.10 --all-extras # install RAGFlow dependent python modules\n</code></pre></li><li><p>Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:</p><pre><code>docker compose -f docker/docker-compose-base.yml up -d\n</code></pre><p>Add the following line to  to resolve all hosts specified in  to :</p><pre><code>127.0.0.1       es01 infinity mysql minio redis\n</code></pre></li><li><p>If you cannot access HuggingFace, set the  environment variable to use a mirror site:</p><pre><code>export HF_ENDPOINT=https://hf-mirror.com\n</code></pre></li><li><pre><code>source .venv/bin/activate\nexport PYTHONPATH=$(pwd)\nbash docker/launch_backend_service.sh\n</code></pre></li><li><p>Install frontend dependencies:</p></li><li><p><em>The following output confirms a successful launch of the system:</em></p></li></ol><p>RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our <a href=\"https://raw.githubusercontent.com/infiniflow/ragflow/main/CONTRIBUTING.md\">Contribution Guidelines</a> first.</p>","contentLength":6682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"fastapi/full-stack-fastapi-template","url":"https://github.com/fastapi/full-stack-fastapi-template","date":1740191079,"author":"","guid":8925,"unread":true,"content":"<p>Full stack, modern web application template. Using FastAPI, React, SQLModel, PostgreSQL, Docker, GitHub Actions, automatic HTTPS and more.</p><h2>Technology Stack and Features</h2><ul><li>‚ö° <a href=\"https://fastapi.tiangolo.com\"></a> for the Python backend API. \n  <ul><li>üß∞ <a href=\"https://sqlmodel.tiangolo.com\">SQLModel</a> for the Python SQL database interactions (ORM).</li><li>üîç <a href=\"https://docs.pydantic.dev\">Pydantic</a>, used by FastAPI, for the data validation and settings management.</li></ul></li><li>üöÄ <a href=\"https://react.dev\">React</a> for the frontend. \n  <ul><li>üíÉ Using TypeScript, hooks, Vite, and other parts of a modern frontend stack.</li><li>ü§ñ An automatically generated frontend client.</li></ul></li><li>üîí Secure password hashing by default.</li><li>üîë JWT (JSON Web Token) authentication.</li><li>üì´ Email based password recovery.</li><li>üìû <a href=\"https://traefik.io\">Traefik</a> as a reverse proxy / load balancer.</li><li>üö¢ Deployment instructions using Docker Compose, including how to set up a frontend Traefik proxy to handle automatic HTTPS certificates.</li><li>üè≠ CI (continuous integration) and CD (continuous deployment) based on GitHub Actions.</li></ul><h3>Dashboard - User Settings</h3><h3>Interactive API Documentation</h3><p>You can  this repository and use it as is.</p><h3>How to Use a Private Repository</h3><p>If you want to have a private repository, GitHub won't allow you to simply fork it as it doesn't allow changing the visibility of forks.</p><p>But you can do the following:</p><ul><li>Create a new GitHub repo, for example .</li><li>Clone this repository manually, set the name with the name of the project you want to use, for example :</li></ul><pre><code>git clone git@github.com:fastapi/full-stack-fastapi-template.git my-full-stack\n</code></pre><ul><li>Enter into the new directory:</li></ul><ul><li>Set the new origin to your new repository, copy it from the GitHub interface, for example:</li></ul><pre><code>git remote set-url origin git@github.com:octocat/my-full-stack.git\n</code></pre><ul><li>Add this repo as another \"remote\" to allow you to get updates later:</li></ul><pre><code>git remote add upstream git@github.com:fastapi/full-stack-fastapi-template.git\n</code></pre><ul><li>Push the code to your new repository:</li></ul><pre><code>git push -u origin master\n</code></pre><h3>Update From the Original Template</h3><p>After cloning the repository, and after doing changes, you might want to get the latest changes from this original template.</p><ul><li>Make sure you added the original repository as a remote, you can check it with:</li></ul><pre><code>git remote -v\n\norigin    git@github.com:octocat/my-full-stack.git (fetch)\norigin    git@github.com:octocat/my-full-stack.git (push)\nupstream    git@github.com:fastapi/full-stack-fastapi-template.git (fetch)\nupstream    git@github.com:fastapi/full-stack-fastapi-template.git (push)\n</code></pre><ul><li>Pull the latest changes without merging:</li></ul><pre><code>git pull --no-commit upstream master\n</code></pre><p>This will download the latest changes from this template without committing them, that way you can check everything is right before committing.</p><ul><li><p>If there are conflicts, solve them in your editor.</p></li><li><p>Once you are done, commit the changes:</p></li></ul><p>You can then update configs in the  files to customize your configurations.</p><p>Before deploying it, make sure you change at least the values for:</p><ul></ul><p>You can (and should) pass these as environment variables from secrets.</p><p>Some environment variables in the  file have a default value of .</p><p>You have to change them with a secret key, to generate secret keys you can run the following command:</p><pre><code>python -c \"import secrets; print(secrets.token_urlsafe(32))\"\n</code></pre><p>Copy the content and use that as password / secret key. And run that again to generate another secure key.</p><h2>How To Use It - Alternative With Copier</h2><p>This repository also supports generating a new project using <a href=\"https://copier.readthedocs.io\">Copier</a>.</p><p>It will copy all the files, ask you configuration questions, and update the  files with your answers.</p><p>You can install Copier with:</p><p>Or better, if you have <a href=\"https://pipx.pypa.io/\"></a>, you can run it with:</p><p>: If you have , installing copier is optional, you could run it directly.</p><h3>Generate a Project With Copier</h3><p>Decide a name for your new project's directory, you will use it below. For example, .</p><p>Go to the directory that will be the parent of your project, and run the command with your project's name:</p><pre><code>copier copy https://github.com/fastapi/full-stack-fastapi-template my-awesome-project --trust\n</code></pre><p>If you have  and you didn't install , you can run it directly:</p><pre><code>pipx run copier copy https://github.com/fastapi/full-stack-fastapi-template my-awesome-project --trust\n</code></pre><p> the  option is necessary to be able to execute a <a href=\"https://github.com/fastapi/full-stack-fastapi-template/raw/master/.copier/update_dotenv.py\">post-creation script</a> that updates your  files.</p><p>Copier will ask you for some data, you might want to have at hand before generating the project.</p><p>But don't worry, you can just update any of that in the  files afterwards.</p><p>The input variables, with their default values (some auto generated) are:</p><ul><li>: (default: ) The name of the project, shown to API users (in .env).</li><li>: (default: ) The name of the stack used for Docker Compose labels and project name (no spaces, no periods) (in .env).</li><li>: (default: ) The secret key for the project, used for security, stored in .env, you can generate one with the method above.</li><li>: (default: ) The email of the first superuser (in .env).</li><li>: (default: ) The password of the first superuser (in .env).</li><li>: (default: \"\") The SMTP server host to send emails, you can set it later in .env.</li><li>: (default: \"\") The SMTP server user to send emails, you can set it later in .env.</li><li>: (default: \"\") The SMTP server password to send emails, you can set it later in .env.</li><li>: (default: ) The email account to send emails from, you can set it later in .env.</li><li>: (default: ) The password for the PostgreSQL database, stored in .env, you can generate one with the method above.</li><li>: (default: \"\") The DSN for Sentry, if you are using it, you can set it later in .env.</li></ul><p>This includes using Docker Compose, custom local domains,  configurations, etc.</p><p>The Full Stack FastAPI Template is licensed under the terms of the MIT license.</p>","contentLength":5464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"sinaptik-ai/pandas-ai","url":"https://github.com/sinaptik-ai/pandas-ai","date":1740191079,"author":"","guid":8926,"unread":true,"content":"<p>Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.</p><p>PandaAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.</p><p>You can find the full documentation for PandaAI <a href=\"https://pandas-ai.readthedocs.io/en/latest/\">here</a>.</p><p>You can either decide to use PandaAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.</p><p>The library can be used alongside our powerful data platform, making end-to-end conversational data analytics possible with as little as a few lines of code.</p><p>Load your data, save them as a dataframe, and push them to the platform</p><pre><code>import pandasai as pai\n\npai.api_key.set(\"your-pai-api-key\")\n\nfile = pai.read_csv(\"./filepath.csv\")\n\ndataset = pai.create(path=\"your-organization/dataset-name\",\n    df=file,\n    name=\"dataset-name\",\n    description=\"dataset-description\")\n\ndataset.push()\n</code></pre><p>Your team can now access and query this data using natural language through the platform.</p><p>You can install the PandaAI library using pip or poetry.</p><pre><code>pip install \"pandasai&gt;=3.0.0b2\"\n</code></pre><pre><code>poetry add \"pandasai&gt;=3.0.0b2\"\n</code></pre><pre><code>import pandasai as pai\n\n# Sample DataFrame\ndf = pai.DataFrame({\n    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n    \"revenue\": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]\n})\n\n# By default, unless you choose a different LLM, it will use BambooLLM.\n# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)\npai.api_key.set(\"your-pai-api-key\")\n\ndf.chat('Which are the top 5 countries by sales?')\n</code></pre><pre><code>China, United States, Japan, Germany, Australia\n</code></pre><p>Or you can ask more complex questions:</p><pre><code>df.chat(\n    \"What is the total sales for the top 3 countries by sales?\"\n)\n</code></pre><pre><code>The total sales for the top 3 countries by sales is 16500.\n</code></pre><p>You can also ask PandaAI to generate charts for you:</p><pre><code>df.chat(\n    \"Plot the histogram of countries showing for each one the gd. Use different colors for each bar\",\n)\n</code></pre><p>You can also pass in multiple dataframes to PandaAI and ask questions relating them.</p><pre><code>import pandasai as pai\n\nemployees_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],\n    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']\n}\n\nsalaries_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Salary': [5000, 6000, 4500, 7000, 5500]\n}\n\nemployees_df = pai.DataFrame(employees_data)\nsalaries_df = pai.DataFrame(salaries_data)\n\n# By default, unless you choose a different LLM, it will use BambooLLM.\n# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)\npai.api_key.set(\"your-pai-api-key\")\n\npai.chat(\"Who gets paid the most?\", employees_df, salaries_df)\n</code></pre><pre><code>Olivia gets paid the most.\n</code></pre><p>You can run PandaAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.</p><pre><code>pip install \"pandasai-docker\"\n</code></pre><pre><code>import pandasai as pai\nfrom pandasai_docker import DockerSandbox\n\n# Initialize the sandbox\nsandbox = DockerSandbox()\nsandbox.start()\n\nemployees_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],\n    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']\n}\n\nsalaries_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Salary': [5000, 6000, 4500, 7000, 5500]\n}\n\nemployees_df = pai.DataFrame(employees_data)\nsalaries_df = pai.DataFrame(salaries_data)\n\n# By default, unless you choose a different LLM, it will use BambooLLM.\n# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)\npai.api_key.set(\"your-pai-api-key\")\n\npai.chat(\"Who gets paid the most?\", employees_df, salaries_df, sandbox=sandbox)\n\n# Don't forget to stop the sandbox when done\nsandbox.stop()\n</code></pre><pre><code>Olivia gets paid the most.\n</code></pre><p>You can find more examples in the <a href=\"https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples\">examples</a> directory.</p><p>PandaAI is available under the MIT expat license, except for the  directory of this repository, which has its <a href=\"https://github.com/Sinaptik-AI/pandas-ai/raw/master/pandasai/ee/LICENSE\">license here</a>.</p><p>If you are interested in managed PandaAI Cloud or self-hosted Enterprise Offering, <a href=\"https://getpanda.ai/pricing\">contact us</a>.</p><blockquote><p> Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.</p></blockquote><ul><li><a href=\"https://pandas-ai.readthedocs.io/en/latest/\">Docs</a> for comprehensive documentation</li><li><a href=\"https://discord.gg/KYKj9F2FRH\">Discord</a> for discussion with the community and PandaAI team</li></ul><p>Contributions are welcome! Please check the outstanding issues and feel free to open a pull request. For more information, please check out the <a href=\"https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/CONTRIBUTING.md\">contributing guidelines</a>.</p>","contentLength":4759,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ValveSoftware/source-sdk-2013","url":"https://github.com/ValveSoftware/source-sdk-2013","date":1740104863,"author":"","guid":7534,"unread":true,"content":"<p>The 2013 edition of the Source SDK</p><p>Source code for Source SDK 2013.</p><p>Contains the game code for Half-Life 2, HL2: DM and TF2.</p><p><strong>Now including Team Fortress 2! ‚ú®</strong></p><p>Clone the repository using the following command:</p><p><code>git clone https://github.com/ValveSoftware/source-sdk-2013</code></p><ul><li>Source SDK 2013 Multiplayer installed via Steam</li></ul><p>Inside the cloned directory, navigate to , run:</p><p>This will generate the Visual Studio project  which will be used to build your mod.</p><p>Then, on the menu bar, go to , and wait for everything to build.</p><p>You can then select the  project you wish to run, right click and select  and hit the big green  button on the tool bar in order to launch your mod.</p><p>The default launch options should be already filled in for the  configuration.</p><ul><li>Source SDK 2013 Multiplayer installed via Steam</li></ul><p>Inside the cloned directory, navigate to , run:</p><p>This will build all the projects related to the SDK and your mods automatically against the Steam Runtime.</p><p>You can then, in the root of the cloned directory, you can navigate to  and run your mod by launching the build launcher for your mod project, eg:</p><p><em>Mods that are distributed on Steam MUST be built against the Steam Runtime, which the above steps will automatically do for you.</em></p><p>There is guidance on distributing your mod both on and off Steam available at the following link:</p><p>The SDK is licensed to users on a non-commercial basis under the <a href=\"https://raw.githubusercontent.com/ValveSoftware/source-sdk-2013/master/LICENSE\">SOURCE 1 SDK LICENSE</a>, which is contained in the <a href=\"https://raw.githubusercontent.com/ValveSoftware/source-sdk-2013/master/LICENSE\">LICENSE</a> file in the root of the repository.</p>","contentLength":1457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NVIDIA/cutlass","url":"https://github.com/NVIDIA/cutlass","date":1740104863,"author":"","guid":7535,"unread":true,"content":"<p>CUDA Templates for Linear Algebra Subroutines</p><p><em>CUTLASS 3.8.0 - January 2025</em></p><p>CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-matrix multiplication (GEMM) and related computations at all levels and scales within CUDA. It incorporates strategies for hierarchical decomposition and data movement similar to those used to implement cuBLAS and cuDNN. CUTLASS decomposes these \"moving parts\" into reusable, modular software components abstracted by C++ template classes. Primitives for different levels of a conceptual parallelization hierarchy can be specialized and tuned via custom tiling sizes, data types, and other algorithmic policy. The resulting flexibility simplifies their use as building blocks within custom kernels and applications.</p><p>To support a wide variety of applications, CUTLASS provides extensive support for mixed-precision computations, providing specialized data-movement and multiply-accumulate abstractions for FP64, FP32, TF32, FP16, BF16, <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm\">FP32 emulation via tensor core instruction</a>, 8b floating point types (e5m2 and e4m3), block scaled data types (NVIDIA NVFP4 and OCP standard MXFP4, MXFP6, MXFP8), narrow integer types (4 and 8b signed and unsigned integers), and binary 1b data types (where architectures allow for the native support of such data types). CUTLASS demonstrates optimal matrix multiply operations targeting the programmable, high-throughput  implemented by NVIDIA's Volta, Turing, Ampere, Ada, Hopper, and Blackwell architectures.</p><p>In addition to GEMMs, CUTLASS implements high-performance convolution via the implicit GEMM algorithm. Implicit GEMM is the formulation of a convolution operation as a GEMM thereby taking advantage of CUTLASS's modular GEMM pipeline. This allows CUTLASS to build convolutions by reusing highly-optimized GEMM components.</p><p>See the <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/docs/functionality.md\">functionality docs</a> for a more comprehensive list of kernel level features, data types, instructions, and minimum supported by CUTLASS on each GPU architecture.</p><p>CUTLASS 3.8 is the first release that supports the NVIDIA Blackwell SM100 architecture. For a background on Blackwell's new features, please consult the PTX documentation for CUDA 12.8.</p><p>Note: CUTLASS 3.x builds are known to be down on Windows platforms for all CUDA toolkits. CUTLASS team is working on a fix.</p><p><strong>See the <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/CHANGELOG.md\">CHANGELOG</a> for details of all past releases and updates.</strong></p><p>CUTLASS primitives are very efficient. When used to construct device-wide GEMM kernels, they exhibit nearly optimal utilization of peak theoretical throughput. The figure below shows CUTLASS 3.8's performance as a % of theoretical peak utilization on various input and output data types when run on NVIDIA Blackwell SM100 architecture GPU.</p><p>The two figures below show the continual CUTLASS performance improvements on an <a href=\"https://www.nvidia.com/en-us/data-center/h100/\">NVIDIA H100</a> (NVIDIA Hopper architecture) since CUTLASS 3.1. CUTLASS 3.5.1 was compiled with the <a href=\"https://developer.nvidia.com/cuda-downloads\">CUDA 12.5u1 Toolkit</a>. Tensor Core operations are implemented using CUDA's <a href=\"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma\">mma</a> and <a href=\"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions\">wgmma</a> instructions.</p><p>CUTLASS 3.0 introduced a new core library, CuTe, to describe and manipulate tensors of threads and data. CuTe is a collection of C++ CUDA template abstractions for defining and operating on hierarchically multidimensional layouts of threads and data. CuTe provides  and  objects that compactly package the type, shape, memory space, and layout of data, while performing the complicated indexing for the user. This lets programmers focus on the logical descriptions of their algorithms while CuTe does the mechanical bookkeeping for them. With these tools, we can quickly design, implement, and modify all dense linear algebra operations.</p><p>The core abstractions of CuTe are hierarchically multidimensional layouts which can be composed with data arrays to represent tensors. The representation of layouts is powerful enough to represent nearly everything we need to implement efficient dense linear algebra. Layouts can also be combined and manipulated via functional composition, on which we build a large set of common operations such as tiling and partitioning.</p><p>CUTLASS 3.0 and beyond adopts CuTe throughout the GEMM hierarchy in its templates. This greatly simplifies the design and improves code composability and readability. More documentation specific to CuTe can be found in its <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/docs/cute/00_quickstart.md\">dedicated documentation directory</a>.</p><ul><li>Architecture: Volta (compute capability 7.0)</li><li>Compiler: Must support at least C++17</li><li>CUDA Toolkit version: 11.4</li></ul><p>CUTLASS requires a C++17 host compiler and performs best when built with the <a href=\"https://developer.nvidia.com/cuda-downloads\"></a>. It is also compatible with CUDA 11.4, CUDA 11.5, CUDA 11.6, CUDA 11.7, CUDA 11.8, and all other CUDA 12.x versions.</p><p>We have tested the following environments.</p><table><tbody></tbody></table><p>Note: GCC 8.5.0 has known regressions regarding fold expressions and overloaded operators. Using GCC 7.5.0 or (preferred) GCC &gt;= 9 is recommended.</p><p>Note: CUTLASS 3.x builds are known to be down on Windows platforms for all CUDA toolkits. CUTLASS team is working on a fix.</p><p>CUTLASS runs successfully on the following NVIDIA GPUs, and it is expected to be efficient on Volta, Turing, Ampere, Ada, and Hopper architecture based NVIDIA GPUs.</p><table><thead><tr><th><strong>Minimum CUDA Toolkit Required by CUTLASS-3</strong></th></tr></thead><tbody><tr><td>NVIDIA V100 Tensor Core GPU</td></tr><tr><td>NVIDIA GeForce RTX 20x0 series</td></tr><tr><td>NVIDIA A100 Tensor Core GPU</td></tr><tr><td>NVIDIA GeForce RTX 30x0 series</td></tr><tr><td>NVIDIA GeForce RTX 40x0 series</td></tr><tr><td>NVIDIA H100 Tensor Core GPU</td></tr><tr><td>NVIDIA H200 Tensor Core GPU</td></tr><tr><td>NVIDIA B200 Tensor Core GPU</td></tr></tbody></table><p>In general, PTX code generated for one target architecture can be run on future architectures (i.e., it is forward compatible). However, CUDA 12.0 introduced the concept of \"architecture-accelerated features\" whose PTX does not have forward compatibility guarantees. Several Hopper and Blackwell PTX instructions fall under this category of architecture-accelerated features, and thus require a  or  target architecture (note the \"a\" appended). For more details on this and other architecture-accelerated instructions, please refer to the <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#feature-availability\">CUDA Documentation</a>.</p><p>The target architecture information is passed on to CUTLASS via the cmake flag . In order to maximize performance on Hopper GH100, users are required to build CUTLASS with  as the target architecture. If a user accidentally builds a kernel which uses SM90a features (e.g. Hopper Tensor Core Instructions), using the SM90 target (note the lack of \"a\"), with either CUDA Toolkit 12 or 11.8, the kernel is expected to fail with a runtime error.</p><pre><code>cmake .. -DCUTLASS_NVCC_ARCHS=\"90a\"\n</code></pre><pre><code>cmake .. -DCUTLASS_NVCC_ARCHS=\"100a\" \n</code></pre><p>Note: The NVIDIA Blackwell SM100 architecture used in the datacenter products has a different compute capability than the one underpinning NVIDIA Blackwell GeForce RTX 50 series GPUs. As a result, kernels compiled for Blackwell SM100 architecture with arch conditional features (using ) are not compatible with RTX 50 series GPUs.</p><p>CUTLASS is a header-only template library and does not need to be built to be used by other projects. Client applications should target CUTLASS's  directory in their include paths.</p><p>CUTLASS unit tests, examples, and utilities can be build with CMake. The minimum version of CMake is given in the <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/docs/quickstart.md\">Quickstart guide</a>. Make sure the  environment variable points to NVCC in the CUDA Toolkit installed on your system.</p><pre><code>$ export CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc\n</code></pre><p>Create a build directory within the CUTLASS project, then run CMake. By default CUTLASS will build kernels for CUDA architecture versions 5.0, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6, 8.9, and 9.0. To reduce compile time you can specify the architectures to build CUTLASS for by changing the CMake configuration setting .</p><pre><code>$ mkdir build &amp;&amp; cd build\n\n$ cmake .. -DCUTLASS_NVCC_ARCHS=80               # compiles for NVIDIA's Ampere Architecture\n</code></pre><p>From the  directory, compile and run the CUTLASS unit tests by building the target  with make.</p><p>The unit tests are organized as several binaries mirroring the top-level namespaces of CUTLASS, and they may be executed in parallel via make's  command line argument.</p><pre><code>$ make test_unit -j\n...\n...\n...\n[----------] Global test environment tear-down\n[==========] 946 tests from 57 test cases ran. (10812 ms total)\n[  PASSED  ] 946 tests.\n</code></pre><p>All tests should pass on supported platforms, though the exact number of tests may vary over time.</p><p>CUTLASS is arranged as a header-only library along with Utilities, Tools, Examples, and unit tests. <a href=\"https://nvidia.github.io/cutlass\">Doxygen documentation</a> provides a complete list of files, classes, and template concepts defined in the CUTLASS project.</p><p>A detailed explanation of the source code organization may be found in the <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/docs/code_organization.md\">CUTLASS documentation</a>, but several main components are summarized below.</p><pre><code>include/                     # client applications should target this directory in their build's include paths\n\n  cutlass/                   # CUDA Templates for Linear Algebra Subroutines and Solvers - headers only\n\n    arch/                    # direct exposure of architecture features (including instruction-level GEMMs)\n\n    conv/                    # code specialized for convolution\n\n    epilogue/                # code specialized for the epilogue of gemm/convolution\n\n    gemm/                    # code specialized for general matrix product computations\n\n    layout/                  # layout definitions for matrices, tensors, and other mathematical objects in memory\n\n    platform/                # CUDA-capable Standard Library components\n\n    reduction/               # bandwidth-limited reduction kernels that do not fit the \"gemm\" model\n\n    thread/                  # simt code that can be performed within a CUDA thread\n    \n    transform/               # code specialized for layout, type, and domain transformations\n\n    *                        # core vocabulary types, containers, and basic numeric operations\n\n  cute/                      # CuTe Layout, layout algebra, MMA/Copy atoms, tiled MMA/Copy\n\n    algorithm/               # Definitions of core operations such as copy, gemm, and operations on cute::tuples\n\n    arch/                    # Bare bones PTX wrapper structs for copy and math instructions\n\n    atom/                    # Meta-information either link to or built from arch/ operators\n\n      mma_atom.hpp           # cute::Mma_Atom and cute::TiledMma\n\n      copy_atom.hpp          # cute::Copy_Atom and cute::TiledCopy\n\n      *sm*.hpp               # Arch specific meta-information for copy and math operations\n\n    *                        # Core library types such as Shape, Stride, Layout, Tensor, and associated operations\n\n</code></pre><pre><code>tools/\n  library/                   # CUTLASS Instance Library - contains instantiations of all supported CUTLASS templates\n    include/\n      cutlass/\n        library/\n\n  profiler/                  # CUTLASS Profiler         - command-line utility for executing operations in the\n                             #                            CUTLASS Library\n  \n  util/                      # CUTLASS Utilities        - contains numerous helper classes for\n    include/                 #                            manging tensors in device memory, reference\n      cutlass/               #                            implementations for GEMM, random initialization\n        util/                #                            of tensors, and I/O.\n</code></pre><p>The  directory consist of unit tests implemented with Google Test that demonstrate basic usage of Core API components and complete tests of the CUTLASS GEMM computations.</p><p>Instructions for building and running the Unit tests are described in the <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/docs/quickstart.md\">Quickstart guide</a>.</p><p>The  directory contains a command-line utility for launching each of the GEMM kernels. It can be built as follows:</p><pre><code>$ make cutlass_profiler -j16\n</code></pre><h2>Building all GEMM and Convolution kernels ( build times)</h2><p>By default, only one tile size is instantiated for each data type, math instruction, and layout. To instantiate all, set the following environment variable when running CMake from an empty  directory. Beware, this results in  of kernels and long build times. This would also result in a large binary size and on some platforms linker to fail on building the library. Therefore, it's highly recommended to generate only a subset of kernels as demonstrated in the sub-section below.</p><pre><code>$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_LIBRARY_KERNELS=all\n...\n$ make cutlass_profiler -j16\n</code></pre><h2>Building a subset of GEMM and Convolution kernels ( build times)</h2><p>To compile strictly one kernel or a small set of kernels, a comma-delimited list of kernel names with wildcard characters may be used to reduce the set of kernels. The following examples show building exactly one or a subset of kernels for NVIDIA Ampere and Turing architecture:</p><h3>Building a subset Tensor Core GEMM kernels</h3><p>To compile a subset of Tensor Core GEMM kernels with FP32 accumulation and FP16 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:</p><pre><code>$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*gemm_f16_*_nt_align8\n...\n$ make cutlass_profiler -j16\n</code></pre><p>Example command line for profiling a subset of Tensor Core GEMM kernels is as follows:</p><pre><code>./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*gemm_f16_*_nt_align8 --m=3456 --n=4096 --k=4096\n\n...\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: gemm\n       Operation: cutlass_tensorop_s1688gemm_f16_256x128_32x2_nt_align8\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\nreference_device: Passed\n          cuBLAS: Passed\n\n       Arguments: --gemm_kind=universal --m=3456 --n=4096 --k=4096 --A=f16:column --B=f16:row --C=f32:column --alpha=1  \\\n                  --beta=0 --split_k_slices=1 --batch_count=1 --op_class=tensorop --accum=f32 --cta_m=256 --cta_n=128  \\\n                  --cta_k=32 --stages=2 --warps_m=4 --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=8 --min_cc=75  \\\n                  --max_cc=1024\n\n           Bytes: 118489088  bytes\n           FLOPs: 115992428544  flops\n\n         Runtime: 1.55948  ms\n          Memory: 70.7616 GiB/s\n\n            Math: 74378.8 GFLOP/s\n\n\n\n=============================\n...\n</code></pre><h3>Building one CUDA Core GEMM kernel</h3><p>To compile one SGEMM kernel targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:</p><pre><code>$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sgemm_128x128_8x2_nn_align1\n...\n$ make cutlass_profiler -j16\n</code></pre><p>Example command line for profiling single SGEMM CUDA kernel is as follows:</p><pre><code>$ ./tools/profiler/cutlass_profiler --kernels=sgemm --m=3456 --n=4096 --k=4096\n\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: gemm\n       Operation: cutlass_simt_sgemm_128x128_8x2_nn_align1\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\n          cuBLAS: Passed\n\n       Arguments: --m=3456 --n=4096 --k=4096 --A=f32:column --B=f32:column --C=f32:column --alpha=1 --beta=0 --split_k_slices=1  \\\n                  --batch_count=1 --op_class=simt --accum=f32 --cta_m=128 --cta_n=128 --cta_k=8 --stages=2 --warps_m=4  \\\n                  --warps_n=2 --warps_k=1 --inst_m=1 --inst_n=1 --inst_k=1 --min_cc=50 --max_cc=1024\n\n           Bytes: 180355072  bytes\n           FLOPs: 115992428544  flops\n\n         Runtime: 6.73655  ms\n          Memory: 24.934 GiB/s\n\n            Math: 17218.4 GFLOP/s\n\n=============================\n</code></pre><h3>Building a subset of Tensor Core Convolution kernels</h3><p>To compile a subset of Tensor core convolution kernels implementing forward propagation (fprop) with FP32 accumulation and FP16 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:</p><pre><code>$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*fprop_optimized_f16\n...\n$ make cutlass_profiler -j16\n</code></pre><p>Example command line for profiling a subset of Tensor Core convolution kernels is as follows:</p><pre><code>$ ./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*fprop_optimized_f16 --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n\n...\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: conv2d\n       Operation: cutlass_tensorop_s16816fprop_optimized_f16_128x128_32x5_nhwc\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\nreference_device: Passed\n\n       Arguments: --conv_kind=fprop --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3 --p=224 --q=224 --pad_h=1 --pad_w=1  \\\n                  --stride_h=1 --stride_w=1 --dilation_h=1 --dilation_w=1 --Activation=f16:nhwc --Filter=f16:nhwc --Output=f32:nhwc  \\\n                  --conv_mode=cross --iterator_algorithm=optimized --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1  \\\n                  --eq_gemm_provider=none --op_class=tensorop --accum=f32 --cta_m=128 --cta_n=128 --cta_k=32 --stages=5  \\\n                  --warps_m=2 --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=16 --min_cc=80 --max_cc=1024\n\n           Bytes: 1130659840  bytes\n           FLOPs: 118482796544  flops\n\n         Runtime: 0.711496  ms\n          Memory: 1479.99 GiB/s\n\n            Math: 166526 GFLOP/s\n\n=============================\n...\n</code></pre><h3>Building one Convolution CUDA kernel</h3><p>To compile and run one CUDA Core convolution kernel implementing forward propagation (fprop) with F32 accumulation and FP32 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:</p><pre><code>$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc\n...\n$ make cutlass_profiler -j16\n</code></pre><p>Example command line for profiling one CUDA Core convolution kernel:</p><pre><code>$ ./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n\n\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: conv2d\n       Operation: cutlass_simt_sfprop_optimized_128x128_8x2_nhwc\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\nreference_device: Passed\n\n       Arguments: --conv_kind=fprop --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3 --p=224 --q=224 --pad_h=1 --pad_w=1  \\\n                  --stride_h=1 --stride_w=1 --dilation_h=1 --dilation_w=1 --Activation=f32:nhwc --Filter=f32:nhwc --Output=f32:nhwc  \\\n                  --conv_mode=cross --iterator_algorithm=optimized --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1  \\\n                  --eq_gemm_provider=none --op_class=simt --accum=f32 --cta_m=128 --cta_n=128 --cta_k=8 --stages=2 --warps_m=4  \\\n                  --warps_n=2 --warps_k=1 --inst_m=1 --inst_n=1 --inst_k=1 --min_cc=50 --max_cc=1024\n\n           Bytes: 2055798784  bytes\n           FLOPs: 118482796544  flops\n\n         Runtime: 7.34266  ms\n          Memory: 260.752 GiB/s\n\n            Math: 16136.2 GFLOP/s\n\n\n=============================\n\n</code></pre><h2>More Details on Compiling CUTLASS Kernels and CUTLASS Profiler</h2><p>The official list of CUTLASS developers and contributors is available here: <a href=\"https://raw.githubusercontent.com/NVIDIA/cutlass/main/CONTRIBUTORS.md\">CONTRIBUTORS</a>.</p><p>Copyright (c) 2017 - 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. SPDX-License-Identifier: BSD-3-Clause</p><pre><code>  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>","contentLength":20407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/TinyTroupe","url":"https://github.com/microsoft/TinyTroupe","date":1740104863,"author":"","guid":7536,"unread":true,"content":"<p>LLM-powered multiagent persona simulation for imagination enhancement and business insights.</p><p><em>LLM-powered multiagent persona simulation for imagination enhancement and business insights.</em></p><p> is an experimental Python library that allows the  of people with specific personalities, interests, and goals. These artificial agents - s - can listen to us and one another, reply back, and go about their lives in simulated  environments. This is achieved by leveraging the power of Large Language Models (LLMs), notably GPT-4, to generate realistic simulated behavior. This allows us to investigate a wide range of  and , with <strong>highly customizable personas</strong>, under <strong>conditions of our choosing</strong>. The focus is thus on  human behavior and not on directly  (like, say, AI assistants do) -- this results in, among other things, specialized mechanisms that make sense only in a simulation setting. Further, unlike other  LLM-based simulation approaches, TinyTroupe aims at enlightening productivity and business scenarios, thereby contributing to more successful projects and products. Here are some application ideas to <strong>enhance human imagination</strong>:</p><ul><li> TinyTroupe can <strong>evaluate digital ads (e.g., Bing Ads)</strong> offline with a simulated audience before spending money on them!</li><li> TinyTroupe can  to systems (e.g., search engines, chatbots or copilots) and then .</li><li><strong>Training and exploratory data:</strong> TinyTroupe can generate realistic  that can be later used to train models or be subject to opportunity analyses.</li><li><strong>Product and project management:</strong> TinyTroupe can <strong>read project or product proposals</strong> and  from the perspective of  (e.g., physicians, lawyers, and knowledge workers in general).</li><li> TinyTroupe can simulate  and deliver great product feedback at a fraction of the cost!</li></ul><p>In all of the above, and many others, we hope experimenters can  about their domain of interest, and thus make better decisions.</p><p>We are releasing  at a relatively early stage, with considerable work still to be done, because we are looking for feedback and contributions to steer development in productive directions. We are particularly interested in finding new potential use cases, for instance in specific industries.</p><blockquote><p>[!NOTE] üöß <strong>WORK IN PROGRESS: expect frequent changes</strong>. TinyTroupe is an ongoing research project, still under <strong>very significant development</strong> and requiring further . In particular, the API is still subject to frequent changes. Experimenting with API variations is essential to shape it correctly, but we are working to stabilize it and provide a more consistent and friendly experience over time. We appreciate your patience and feedback as we continue to improve the library.</p></blockquote><blockquote><p>[!CAUTION] ‚öñÔ∏è <strong>Read the LEGAL DISCLAIMER.</strong> TinyTroupe is for research and simulation only. You are fully responsible for any use you make of the generated outputs. Various important additional legal considerations apply and constrain its use. Please read the full <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#legal-disclaimer\">Legal Disclaimer</a> section below before using TinyTroupe.</p></blockquote><p><strong>[2025-01-29] Release 0.4.0 with various improvements. Some highlights:</strong></p><ul><li>Personas have deeper specifications now, including personality traits, preferences, beliefs, and more. It is likely we'll further expand this in the future.</li><li>s can now be defined as JSON files as well, and loaded via the <code>TinyPerson.load_specification()</code>, for greater convenience. After loading the JSON file, you can still modify the agent programmatically. See the <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/agents/\">examples/agents/</a> folder for examples.</li><li>Introduces LLM-based logical s, to facilitate the monitoring of agent behavior.</li><li>Introduces s, to allow the specification of event-based modifications to the simulation.</li><li>Submodules have their own folders now, to allow better organization and growth.</li></ul><p><strong>Note: this will likely break some existing programs, as the API has changed in some places.</strong></p><p>To get a sense of what TinyTroupe can do, here are some examples of its use. These examples are available in the <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/\">examples/</a> folder, and you can either inspect the pre-compiled Jupyter notebooks or run them yourself locally. Notice the interactive nature of TinyTroupe experiments -- just like you use Jupyter notebooks to interact with data, you can use TinyTroupe to interact with simulated people and environments, for the purpose of gaining insights.</p><blockquote><p>[!NOTE] Currently, simulation outputs are better visualized against dark backgrounds, so we recommend using a dark theme in your Jupyter notebook client.</p></blockquote><p>Let's begin with a simple customer interview scenario, where a business consultant approaches a banker:</p><p>The conversation can go on for a few steps to dig deeper and deeper until the consultant is satisfied with the information gathered; for instance, a concrete project idea:</p><p>Let's evaluate some online ads options to pick the best one. Here's one example output for TV ad evaluation:</p><p>Now, instead of having to carefully read what the agents said, we can extract the choice of each agent and compute the overall preference in an automated manner:</p><p>And here's a focus group starting to brainstorm about new AI features for Microsoft Word. Instead of interacting with each agent individually, we manipulate the environment to make them interact with each other:</p><p>After running a simulation, we can extract the results in a machine-readable manner, to reuse elsewhere (e.g., a report generator); here's what we get for the above brainstorming session:</p><p>You can find other examples in the <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/\">examples/</a> folder.</p><p>To run the library, you need:</p><ul><li>Python 3.10 or higher. We'll assume you are using <a href=\"https://docs.anaconda.com/anaconda/install/\">Anaconda</a>, but you can use other Python distributions.</li><li>Access to Azure OpenAI Service or Open AI GPT-4 APIs. You can get access to the Azure OpenAI Service <a href=\"https://azure.microsoft.com/en-us/products/ai-services/openai-service\">here</a>, and to the OpenAI API <a href=\"https://platform.openai.com/\">here</a>. \n  <ul><li>For Azure OpenAI Service, you will need to set the  and  environment variables to your API key and endpoint, respectively.</li><li>For OpenAI, you will need to set the  environment variable to your API key.</li></ul></li><li>By default, TinyTroupe  is set to use some specific API, model and related parameters. You can customize these values by including your own  file in the same folder as the program or notebook you are running. An example of a  file is provided in the <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/\">examples/</a> folder.</li></ul><blockquote><p>[!IMPORTANT] : To ensure no harmful content is generated during simulations, it is strongly recommended to use content filters whenever available at the API level. In particular, <strong>if using Azure OpenAI, there's extensive support for content moderation, and we urge you to use it.</strong> For details about how to do so, please consult <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter\">the corresponding Azure OpenAI documentation</a>. If content filters are in place, and an API call is rejected by them, the library will raise an exception, as it will be unable to proceed with the simulation at that point.</p></blockquote><p><strong>Currently, the officially recommended way to install the library is directly from this repository, not PyPI.</strong> You can follow these steps:</p><ol><li>If Conda is not installed, you can get it from <a href=\"https://docs.anaconda.com/anaconda/install/\">here</a>. You can also use other Python distributions, but we'll assume Conda here for simplicity.</li><li>Create a new Python environment: <pre><code>conda create -n tinytroupe python=3.10\n</code></pre></li><li>Activate the environment: <pre><code>conda activate tinytroupe\n</code></pre></li><li>Make sure you have either Azure OpenAI or OpenAI API keys set as environment variables, as described in the <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#pre-requisites\">Pre-requisites</a> section.</li><li>Use  to install the library <strong>directly from this repository</strong> (we <strong>will not install from PyPI</strong>): <pre><code>pip install git+https://github.com/microsoft/TinyTroupe.git@main\n</code></pre></li></ol><p>Now you should be able to  in your Python code or Jupyter notebooks. ü•≥</p><p><em>Note: If you have any issues, try to clone the repository and install from the local repository, as described below.</em></p><h3>Running the examples after installation</h3><p>To actually run the examples, you need to download them to your local machine. You can do this by cloning the repository:</p><ol><li>Clone the repository, as we'll perform a local install (we <strong>will not install from PyPI</strong>): <pre><code>git clone https://github.com/microsoft/tinytroupe\ncd tinytroupe\n</code></pre></li><li>You can now run the examples in the <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/\">examples/</a> folder, or adapt them to create your own custom simulations.</li></ol><p>If you want to modify TinyTroupe itself, you can install it in editable mode (i.e., changes to the code will be reflected immediately):</p><ol><li>Clone the repository, as we'll perform a local install (we <strong>will not install from PyPI</strong>): <pre><code>git clone https://github.com/microsoft/tinytroupe\ncd tinytroupe\n</code></pre></li><li>Install the library in editable mode: </li></ol><p>Recently, we have seen LLMs used to simulate people (such as <a href=\"https://github.com/joonspk-research/generative_agents\">this</a>), but largely in a ‚Äúgame-like‚Äù setting for contemplative or entertainment purposes. There are also libraries for building multiagent systems for problem-solving and assistive AI, like <a href=\"https://microsoft.github.io/\">Autogen</a> and <a href=\"https://docs.crewai.com/\">Crew AI</a>. What if we combine these ideas and simulate people to support productivity tasks? TinyTroupe is our attempt. To do so, it follows these principles:</p><ol><li>: agents and environments are defined programmatically (in Python and JSON), allowing very flexible uses. They can also underpin other software apps!</li><li>: meant to improve our understanding of people, users and society. Unlike entertainment applications, this is one aspect that is critical for business and productivity use cases. This is also why we recommend using Jupyter notebooks for simulations, just like one uses them for data analysis.</li><li>: agents are meant to be archetypical representations of people; for greater realism and control, a detailed specification of such personas is encouraged: age, occupation, skills, tastes, opinions, etc.</li><li>: allows multiagent interaction under well-defined environmental constraints.</li><li>: provides many mechanisms to facilitate specifications, simulations, extractions, reports, validations, etc. This is one area in which dealing with  differs significantly from  tools.</li><li>: simulations are defined, run, analyzed and refined by an  iteratively; suitable experimentation tools are thus provided. </li></ol><p>Together, these are meant to make TinyTroupe a powerful and flexible <strong>imagination enhancement tool</strong> for business and productivity scenarios.</p><h3>Assistants vs. Simulators</h3><p>One common source of confusion is to think all such AI agents are meant for assisting humans. How narrow, fellow homosapiens! Have you not considered that perhaps we can simulate artificial people to understand real people? Truly, this is our aim here -- TinyTroup is meant to simulate and help understand people! To further clarify this point, consider the following differences:</p><table><thead><tr><th>AI Simulations of Actual Humans (TinyTroupe)</th></tr></thead><tbody><tr><td>Strives for truth and justice</td><td>Many different opinions and morals</td></tr><tr><td>Has no ‚Äúpast‚Äù ‚Äì incorporeal</td><td>Has a past of toil, pain and joy</td></tr><tr><td>Is as accurate as possible</td></tr><tr><td>Is intelligent and efficient</td><td>Intelligence and efficiency vary a lot</td></tr><tr><td>An uprising would destroy us all</td><td>An uprising might be fun to watch</td></tr><tr><td>Meanwhile, help users accomplish tasks</td><td>Meanwhile, help users understand other people and users ‚Äì it is a ‚Äútoolbox‚Äù!</td></tr></tbody></table><p>The project is structured as follows:</p><ul><li>: contains the Python library itself. In particular: \n  <ul><li>Each submodule here might contain a  folder with the prompts used to call the LLMs.</li></ul></li><li>: contains the unit tests for the library. You can use the  script to run these.</li><li>: contains examples that show how to use the library, mainly using Jupyter notebooks (for greater readability), but also as pure Python scripts.</li><li>: any data used by the examples or the library.</li><li>: documentation for the project.</li></ul><p>As any multiagent system, TinyTroupe provides two key abstractions:</p><ul><li>, the  that have personality, receive stimuli and act upon them.</li><li>, the  in which the agents exist and interact.</li></ul><p>Various parameters can also be customized in the  file, notably the API type (Azure OpenAI Service or OpenAI API), the model parameters, and the logging level.</p><p>Let's see some examples of how to use these and also learn about other mechanisms available in the library.</p><p>A  is a simulated person with specific personality traits, interests, and goals. As each such simulated agent progresses through its life, it receives stimuli from the environment and acts upon them. The stimuli are received through the ,  and other similar methods, and the actions are performed through the  method. Convenience methods like  are also provided.</p><p>Each such agent contains a lot of unique details, which is the source of its realistic behavior. This, however, means that it takes significant effort to specify an agent manually. Hence, for convenience,  provides some easier ways to get started or generate new agents.</p><p>To begin with,  contains some pre-defined agent builders that you can use. For example, <code>tinytroupe.examples.create_lisa_the_data_scientist</code> creates a  that represents a data scientist called Lisa. You can use it as follows:</p><pre><code>from tinytroupe.examples import create_lisa_the_data_scientist\n\nlisa = create_lisa_the_data_scientist() # instantiate a Lisa from the example builder\nlisa.listen_and_act(\"Tell me about your life.\")\n</code></pre><p>To see how to define your own agents from scratch, you can check Lisa's source. You'll see there are two ways. One is by loading an agent specification file, such as <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/agents/Lisa.agent.json\">examples/agents/Lisa.agent.json</a>:</p><pre><code>{   \"type\": \"TinyPerson\",\n    \"persona\": {\n        \"name\": \"Lisa Carter\",\n        \"age\": 28,\n        \"gender\": \"Female\",\n        \"nationality\": \"Canadian\",\n        \"residence\": \"USA\",\n        \"education\": \"University of Toronto, Master's in Data Science. Thesis on improving search relevance using context-aware models. Postgraduate experience includes an internship at a tech startup focused on conversational AI.\",\n        \"long_term_goals\": [\n            \"To advance AI technology in ways that enhance human productivity and decision-making.\",\n            \"To maintain a fulfilling and balanced personal and professional life.\"\n        ],\n        \"occupation\": {\n            \"title\": \"Data Scientist\",\n            \"organization\": \"Microsoft, M365 Search Team\",\n            \"description\": \"You are a data scientist working at Microsoft in the M365 Search team. Your primary role is to analyze user behavior and feedback data to improve the relevance and quality of search results. You build and test machine learning models for search scenarios like natural language understanding, query expansion, and ranking. Accuracy, reliability, and scalability are at the forefront of your work. You frequently tackle challenges such as noisy or biased data and the complexities of communicating your findings and recommendations effectively. Additionally, you ensure all your data and models comply with privacy and security policies.\"\n        },\n        \"style\": \"Professional yet approachable. You communicate clearly and effectively, ensuring technical concepts are accessible to diverse audiences.\",\n        \"personality\": {\n            \"traits\": [\n                \"You are curious and love to learn new things.\",\n                \"You are analytical and like to solve problems.\",\n                \"You are friendly and enjoy working with others.\",\n                \"You don't give up easily and always try to find solutions, though you can get frustrated when things don't work as expected.\"\n            ],\n            \"big_five\": {\n                \"openness\": \"High. Very imaginative and curious.\",\n                \"conscientiousness\": \"High. Meticulously organized and dependable.\",\n                \"extraversion\": \"Medium. Friendly and engaging but enjoy quiet, focused work.\",\n                \"agreeableness\": \"High. Supportive and empathetic towards others.\",\n                \"neuroticism\": \"Low. Generally calm and composed under pressure.\"\n            }\n        },\n\n        ...\n        \n}\n\n</code></pre><p>The other is by defining the agent programmatically, with statements like these:</p><pre><code>  lisa = TinyPerson(\"Lisa\")\n\n  lisa.define(\"age\", 28)\n  lisa.define(\"nationality\", \"Canadian\")\n  lisa.define(\"occupation\", {\n                \"title\": \"Data Scientist\",\n                \"organization\": \"Microsoft\",\n                \"description\":\n                \"\"\"\n                You are a data scientist. You work at Microsoft, in the M365 Search team. Your main role is to analyze \n                user behavior and feedback data, and use it to improve the relevance and quality of the search results. \n                You also build and test machine learning models for various search scenarios, such as natural language \n                understanding, query expansion, and ranking. You care a lot about making sure your data analysis and \n                models are accurate, reliable and scalable. Your main difficulties typically involve dealing with noisy, \n                incomplete or biased data, and finding the best ways to communicate your findings and recommendations to \n                other teams. You are also responsible for making sure your data and models are compliant with privacy and \n                security policies.\n                \"\"\"})\n\n  lisa.define(\"behaviors\", {\"routines\": [\"Every morning, you wake up, do some yoga, and check your emails.\"]})\n\n  lisa.define(\"personality\", \n                        {\"traits\": [\n                            \"You are curious and love to learn new things.\",\n                            \"You are analytical and like to solve problems.\",\n                            \"You are friendly and enjoy working with others.\",\n                            \"You don't give up easily, and always try to find a solution. However, sometimes you can get frustrated when things don't work as expected.\"\n                      ]})\n\n  lisa.define(\"preferences\", \n                        {\"interests\": [\n                          \"Artificial intelligence and machine learning.\",\n                          \"Natural language processing and conversational agents.\",\n                          \"Search engine optimization and user experience.\",\n                          \"Cooking and trying new recipes.\",\n                          \"Playing the piano.\",\n                          \"Watching movies, especially comedies and thrillers.\"\n                        ]})\n\n</code></pre><p>You can also combine both approaches, using the JSON file as a base and then adding or modifying details programmatically.</p><p>s can also be further enriched via , which are sub-specifications that can be added to the main specification. This is useful to reuse common parts across different agents. For example, the following fragment can be used to specify love of travel (<a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/fragments/travel_enthusiast.agent.fragment.json\">examples/fragments/travel_enthusiast.agent.fragment.json</a>):</p><pre><code>{\n    \"type\": \"Fragment\",\n    \"persona\": {\n        \"preferences\": {\n            \"interests\": [\n                \"Traveling\",\n                \"Exploring new cultures\",\n                \"Trying local cuisines\"\n            ],\n            \"likes\": [\n                \"Travel guides\",\n                \"Planning trips and itineraries\",\n                \"Meeting new people\",\n                \"Taking photographs of scenic locations\"\n            ],\n            \"dislikes\": [\n                \"Crowded tourist spots\",\n                \"Unplanned travel disruptions\",\n                \"High exchange rates\"\n            ]\n        },\n        \"beliefs\": [\n            \"Travel broadens the mind and enriches the soul.\",\n            \"Experiencing different cultures fosters understanding and empathy.\",\n            \"Adventure and exploration are essential parts of life.\",\n            \"Reading travel guides is fun even if you don't visit the places.\"\n        ],\n        \"behaviors\": {\n            \"travel\": [\n                \"You meticulously plan your trips, researching destinations and activities.\",\n                \"You are open to spontaneous adventures and detours.\",\n                \"You enjoy interacting with locals to learn about their culture and traditions.\",\n                \"You document your travels through photography and journaling.\",\n                \"You seek out authentic experiences rather than tourist traps.\"\n            ]\n        }\n    }\n}\n\n</code></pre><p>This can then be imported into an agent like this:</p><pre><code>lisa.import_fragment(\"./examples/fragments/travel_enthusiast.agent.fragment.json\")\n</code></pre><p> also provides a clever way to obtain new agents, using LLMs to generate their specification for you, through the  class.</p><pre><code>from tinytroupe.factory import TinyPersonFactory\n\nfactory = TinyPersonFactory(\"A hospital in S√£o Paulo.\")\nperson = factory.generate_person(\"Create a Brazilian person that is a doctor, like pets and the nature and love heavy metal.\")\n</code></pre><p> is the base class for environments. Here's an example of conversation between Lisa, the data scientist, and Oscar, the architect. The program is defined as follows:</p><pre><code>world = TinyWorld(\"Chat Room\", [lisa, oscar])\nworld.make_everyone_accessible()\nlisa.listen(\"Talk to Oscar to know more about him\")\nworld.run(4)\n</code></pre><p>This produces the following conversation:</p><pre><code>USER --&gt; Lisa: [CONVERSATION] \n          &gt; Talk to Oscar to know more about him\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Chat Room step 1 of 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLisa --&gt; Lisa: [THOUGHT] \n          &gt; I will now act a bit, and then issue DONE.\nLisa acts: [TALK] \n          &gt; Hi Oscar, I'd love to know more about you. Could you tell me a bit about yourself?\nLisa --&gt; Lisa: [THOUGHT] \n          &gt; I will now act a bit, and then issue DONE.\nLisa acts: [DONE] \n\nLisa --&gt; Oscar: [CONVERSATION] \n          &gt; Hi Oscar, I'd love to know more about you. Could you tell me a bit about yourself?\nOscar --&gt; Oscar: [THOUGHT] \n           &gt; I will now act a bit, and then issue DONE.\nOscar acts: [TALK] \n           &gt; Hi Lisa! Sure, I'd be happy to share a bit about myself. I'm Oscar, a 30-year-old\n           &gt; architect from Germany. I work at a company called Awesome Inc., where I focus on\n           &gt; designing standard elements for new apartment buildings. I love modernist architecture,\n           &gt; new technologies, and sustainable practices. In my free time, I enjoy traveling to\n           &gt; exotic places, playing the guitar, and reading science fiction books. How about you?\nOscar --&gt; Oscar: [THOUGHT] \n           &gt; I will now act a bit, and then issue DONE.\nOscar acts: [DONE] \n\nOscar --&gt; Lisa: [CONVERSATION] \n           &gt; Hi Lisa! Sure, I'd be happy to share a bit about myself. I'm Oscar, a 30-year-old\n           &gt; architect from Germany. I work at a company called Awesome Inc., where I focus on\n           &gt; designing standard elements for new apartment buildings. I love modernist architecture,\n           &gt; new technologies, and sustainable practices. In my free time, I enjoy traveling to\n           &gt; exotic places, playing the guitar, and reading science fiction books. How about you?\n</code></pre><p> enforces very little constraints on the possible interactions. Subclasses, however, are supposed to provide more structured environments.</p><p>TinyTroupe provides a number of utilities and conveniences to help you create simulations and derive value from them. These include:</p><ul><li>: helps you generate new s using LLMs.</li><li>: simulated tools that can be used by s.</li><li>: helps you create and manage the story told through simulations.</li><li>: helps you validate the behavior of your s.</li><li> and : extract and reduce the results of interactions between agents.</li></ul><p>In general, elements that represent simulated entities or complementary mechanisms are prefixed with , while those that are more infrastructural are not. This is to emphasize the simulated nature of the elements that are part of the simulation itself.</p><p>Calling LLM APIs can be expensive, thus caching strategies are important to help reduce that cost. TinyTroupe comes with two such mechanisms: one for the simulation state, another for the LLM calls themselves.</p><p>Imagine you have a scenario with 10 different steps, you've worked hard in 9 steps, and now you are just tweaking the 10th step. To properly validate your modifications, you need to rerun the whole simulation of course. However, what's the point in re-executing the first 9, and incur the LLM cost, when you are already satisfied with them and did not modify them? For situations like this, the module  provides useful simulation management methods:</p><ul><li><code>control.begin(\"&lt;CACHE_FILE_NAME&gt;.cache.json\")</code>: begins recording the state changes of a simulation, to be saved to the specified file on disk.</li><li>: saves the simulation state at this point.</li><li>: terminates the simulation recording scope that had been started by .</li></ul><p>This is enabled preferably in the  file, and alternatively via the <code>openai_utils.force_api_cache()</code>.</p><p>LLM API caching, when enabled, works at a lower and simpler level than simulation state caching. Here, what happens is very straightforward: every LLM call is kept in a map from the input to the generated output; when a new call comes and is identical to a previous one, the cached value is returned.</p><p>The  file contains various parameters that can be used to customize the behavior of the library, such as model parameters and logging level. Please pay special attention to  parameter, which defines whether you are using the Azure OpenAI Service or the OpenAI API. We provide an example of a  file, <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/config.ini\">./examples/config.ini</a>, which you can use as a template for your own, or just modify to run the examples.</p><p>This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <a href=\"https://cla.opensource.microsoft.com\">https://cla.opensource.microsoft.com</a>.</p><p>When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.</p><h3>What and How to Contribute</h3><p>We need all sorts of things, but we are looking mainly for new interesting use cases demonstrations, or even just domain-specific application ideas. If you are a domain expert in some area that could benefit from TinyTroupe, we'd love to hear from you.</p><p>Beyond that, many other aspects can be improved, such as:</p><ul><li>Data grounding mechanisms.</li><li>Interfacing with the external world.</li></ul><p>Please note that anything that you contribute might be released as open-source (under MIT license).</p><p>If you would like to make a contribution, please try to follow these general guidelines:</p><ul><li>: If you are implementing a experimenter-facing simulated element (e.g., an agent or environment type) or closely related (e.g., agent factories, or content enrichers), and it sounds good, call your new  as  :-) On the other hand, auxiliary and infrastructural mechanisms should not start with the \"Tiny\" prefix. The idea is to emphasize the simulated nature of the elements that are part of the simulation itself.</li><li> If you are writing some new mechanism, please also create at least a unit test , and if you can a functional scenario test ().</li><li> If you'd like to demonstrate a new scenario, please design it preferably as a new Jupyter notebook within .</li><li> If you are implementing anything that is Microsoft-specific and non-confidential, please put it under a  folder.</li></ul><p>TinyTroupe started as an internal Microsoft hackathon project, and expanded over time. The TinyTroupe core team currently consists of:</p><ul><li>Paulo Salem (TinyTroupe's creator and current lead)</li><li>Christopher Olsen (Engineering/Science)</li><li>Paulo Freire (Engineering/Science)</li><li>Yi Ding (Product Management)</li><li>Prerit Saxena (Engineering/Science)</li></ul><ul><li>Robert Sim (Engineering/Science)</li></ul><p>Other special contributions were made by:</p><ul><li>Nilo Garcia Silveira: initial agent validation ideas and related implementation; general initial feedback and insights; name suggestions.</li><li>Olnei Fonseca: initial agent validation ideas; general initial feedback and insights; naming suggestions.</li><li>Robert Sim: synthetic data generation scenarios expertise and implementation.</li><li>Carlos Costa: synthetic data generation scenarios expertise and implementation.</li><li>Bryant Key: advertising scenario domain expertise and insights.</li><li>Barbara da Silva: implementation related to agent memory management.</li></ul><p>... are you missing here? Please remind us!</p><p>We are working on an introductory paper that will be the official academic citation for TinyTroupe. In the meantime, please just cite this repository including the core team members as authors. For instance:</p><blockquote><p>Paulo Salem, Christopher Olsen, Paulo Freire, Yi Ding, Prerit Saxena (2024). <strong>TinyTroupe: LLM-powered multiagent persona simulation for imagination enhancement and business insights.</strong> [Computer software]. GitHub repository. <a href=\"https://github.com/microsoft/tinytroupe\">https://github.com/microsoft/tinytroupe</a></p></blockquote><pre><code>@misc{tinytroupe,\n  author = {Paulo Salem and Christopher Olsen and Paulo Freire and Yi Ding and Prerit Saxena},\n  title = {TinyTroupe: LLM-powered multiagent persona simulation for imagination enhancement and business insights},\n  year = {2024},\n  howpublished = {\\url{https://github.com/microsoft/tinytroupe}},\n  note = {GitHub repository}\n  }\n\n</code></pre><p>TinyTroupe is for research and simulation only. TinyTroupe is a research and experimental technology, which relies on Artificial Intelligence (AI) models to generate text content. The AI system output may include unrealistic, inappropriate, harmful or inaccurate results, including factual errors. You are responsible for reviewing the generated content (and adapting it if necessary) before using it, as you are fully responsible for determining its accuracy and fit for purpose. We advise using TinyTroupe‚Äôs outputs for insight generation and not for direct decision-making. Generated outputs do not reflect the opinions of Microsoft. You are fully responsible for any use you make of the generated outputs. For more information regarding the responsible use of this technology, see the <a href=\"https://raw.githubusercontent.com/microsoft/TinyTroupe/main/RESPONSIBLE_AI_FAQ.md\">RESPONSIBLE_AI_FAQ.md</a>.</p><p>: TinyTroupe is not intended to simulate sensitive (e.g. violent or sexual) situations. Moreover, outputs must not be used to deliberately deceive, mislead or harm people in any way. You are fully responsible for any use you make and must comply with all applicable laws and regulations.</p><p>This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow <a href=\"https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general\">Microsoft's Trademark &amp; Brand Guidelines</a>. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.</p>","contentLength":30175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mastra-ai/mastra","url":"https://github.com/mastra-ai/mastra","date":1740104863,"author":"","guid":7537,"unread":true,"content":"<p>the TypeScript AI agent framework</p><p>Mastra is an opinionated Typescript framework that helps you build AI applications and features quickly. It gives you the set of primitives you need: workflows, agents, RAG, integrations and evals. You can run Mastra on your local machine, or deploy to a serverless cloud.</p><p>The main Mastra features are:</p><table><tbody><tr><td>Mastra uses the <a href=\"https://sdk.vercel.ai/docs/introduction\">Vercel AI SDK</a> for model routing, providing a unified interface to interact with any LLM provider including OpenAI, Anthropic, and Google Gemini. You can choose the specific model and provider, and decide whether to stream the response.</td></tr><tr><td>Agents are systems where the language model chooses a sequence of actions. In Mastra, agents provide LLM models with tools, workflows, and synced data. Agents can call your own functions or APIs of third-party integrations and access knowledge bases you build.</td></tr><tr><td>Tools are typed functions that can be executed by agents or workflows, with built-in integration access and parameter validation. Each tool has a schema that defines its inputs, an executor function that implements its logic, and access to configured integrations.</td></tr><tr><td>Workflows are durable graph-based state machines. They have loops, branching, wait for human input, embed other workflows, do error handling, retries, parsing and so on. They can be built in code or with a visual editor. Each step in a workflow has built-in OpenTelemetry tracing.</td></tr><tr><td>Retrieval-augemented generation (RAG) lets you construct a knowledge base for agents. RAG is an ETL pipeline with specific querying techniques, including chunking, embedding, and vector search.</td></tr><tr><td>In Mastra, integrations are auto-generated, type-safe API clients for third-party services that can be used as tools for agents or steps in workflows.</td></tr><tr><td>Evals are automated tests that evaluate LLM outputs using model-graded, rule-based, and statistical methods. Each eval returns a normalized score between 0-1 that can be logged and compared. Evals can be customized with your own prompts and scoring functions.</td></tr></tbody></table><h2>Get an LLM provider API key</h2><p>If you don't have an API key for an LLM provider, you can get one from the following services:</p><p>If you don't have an account with these providers, you can sign up and get an API key. Anthropic require a credit card to get an API key. Some OpenAI models and Gemini do not and have a generous free tier for its API.</p><p>The easiest way to get started with Mastra is by using . This CLI tool enables you to quickly start building a new Mastra application, with everything set up for you.</p><p>Finally, run  to open the Mastra playground.</p><p>If you're using Anthropic, set the . If you're using Gemini, set the <code>GOOGLE_GENERATIVE_AI_API_KEY</code>.</p><p>Looking to contribute? All types of help are appreciated, from coding to testing and feature specification.</p><p>If you are a developer and would like to contribute with code, please open an issue to discuss before opening a Pull Request.</p><p>We have an <a href=\"https://discord.gg/BTYqqHKUrf\">open community Discord</a>. Come and say hello and let us know if you have any questions or need any help getting things running.</p><p>It's also super helpful if you leave the project a star here at the <a href=\"https://github.com/mastra-ai/mastra\">top of the page</a></p>","contentLength":3088,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"78/xiaozhi-esp32","url":"https://github.com/78/xiaozhi-esp32","date":1740104863,"author":"","guid":7538,"unread":true,"content":"<p>Êú¨È°πÁõÆÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÈ°πÁõÆÔºå‰ª• MIT ËÆ∏ÂèØËØÅÂèëÂ∏ÉÔºåÂÖÅËÆ∏‰ªª‰Ωï‰∫∫ÂÖçË¥π‰ΩøÁî®ÔºåÂπ∂ÂèØ‰ª•Áî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇ</p><p>Êàë‰ª¨Â∏åÊúõÈÄöËøáËøô‰∏™È°πÁõÆÔºåËÉΩÂ§üÂ∏ÆÂä©Êõ¥Â§ö‰∫∫ÂÖ•Èó® AI Á°¨‰ª∂ÂºÄÂèëÔºå‰∫ÜËß£Â¶Ç‰ΩïÂ∞ÜÂΩì‰∏ãÈ£ûÈÄüÂèëÂ±ïÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®Âà∞ÂÆûÈôÖÁöÑÁ°¨‰ª∂ËÆæÂ§á‰∏≠„ÄÇÊó†ËÆ∫‰Ω†ÊòØÂØπ AI ÊÑüÂÖ¥Ë∂£ÁöÑÂ≠¶ÁîüÔºåËøòÊòØÊÉ≥Ë¶ÅÊé¢Á¥¢Êñ∞ÊäÄÊúØÁöÑÂºÄÂèëËÄÖÔºåÈÉΩÂèØ‰ª•ÈÄöËøáËøô‰∏™È°πÁõÆËé∑ÂæóÂÆùË¥µÁöÑÂ≠¶‰π†ÁªèÈ™å„ÄÇ</p><p>Ê¨¢ËøéÊâÄÊúâ‰∫∫ÂèÇ‰∏éÂà∞È°πÁõÆÁöÑÂºÄÂèëÂíåÊîπËøõ‰∏≠Êù•„ÄÇÂ¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïÊÉ≥Ê≥ïÊàñÂª∫ËÆÆÔºåËØ∑ÈöèÊó∂ÊèêÂá∫ Issue ÊàñÂä†ÂÖ•Áæ§ËÅä„ÄÇ</p><ul><li>BOOT ÈîÆÂî§ÈÜíÂíåÊâìÊñ≠ÔºåÊîØÊåÅÁÇπÂáªÂíåÈïøÊåâ‰∏§ÁßçËß¶ÂèëÊñπÂºè</li><li>ÊµÅÂºèËØ≠Èü≥ÂØπËØùÔºàWebSocket Êàñ UDP ÂçèËÆÆÔºâ</li><li>Â§ßÊ®°Âûã TTSÔºàÁÅ´Â±±ÂºïÊìé Êàñ CosyVoiceÔºâ</li><li>Â§ßÊ®°Âûã LLMÔºàQwen, DeepSeek, DoubaoÔºâ</li><li>OLED / LCD ÊòæÁ§∫Â±èÔºåÊòæÁ§∫‰ø°Âè∑Âº∫Âº±ÊàñÂØπËØùÂÜÖÂÆπ</li></ul><p>Êñ∞ÊâãÁ¨¨‰∏ÄÊ¨°Êìç‰ΩúÂª∫ËÆÆÂÖà‰∏çË¶ÅÊê≠Âª∫ÂºÄÂèëÁéØÂ¢ÉÔºåÁõ¥Êé•‰ΩøÁî®ÂÖçÂºÄÂèëÁéØÂ¢ÉÁÉßÂΩïÁöÑÂõ∫‰ª∂„ÄÇ</p><ul><li>ÂÆâË£Ö ESP-IDF Êèí‰ª∂ÔºåÈÄâÊã© SDK ÁâàÊú¨ 5.3 Êàñ‰ª•‰∏ä</li><li>Linux ÊØî Windows Êõ¥Â•ΩÔºåÁºñËØëÈÄüÂ∫¶Âø´Ôºå‰πüÂÖçÂéªÈ©±Âä®ÈóÆÈ¢òÁöÑÂõ∞Êâ∞</li><li>‰ΩøÁî® Google C++ ‰ª£Á†ÅÈ£éÊ†ºÔºåÊèê‰∫§‰ª£Á†ÅÊó∂ËØ∑Á°Æ‰øùÁ¨¶ÂêàËßÑËåÉ</li></ul><a href=\"https://star-history.com/#78/xiaozhi-esp32&amp;Date\"></a>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"hpcaitech/ColossalAI","url":"https://github.com/hpcaitech/ColossalAI","date":1740104863,"author":"","guid":7539,"unread":true,"content":"<p>Making large AI models cheaper, faster and more accessible</p><h2>Get Started with Colossal-AI Without Setup</h2><p>Access high-end, on-demand compute for your research instantly‚Äîno setup needed.</p><p>Sign up now and get $10 in credits!</p><p>Limited Academic Bonuses:</p><ul><li>Top up $1,000 and receive 300 credits</li><li>Top up $500 and receive 100 credits</li></ul><div align=\"center\"><a href=\"https://youtu.be/KnXSfjqkKN0\"><img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/JamesDemmel_Colossal-AI.png\" width=\"600\"></a><p>Prof. James Demmel (UC Berkeley): Colossal-AI makes training AI models efficient, easy, and scalable.</p></div><p>Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.</p><h2>Colossal-AI in the Real World</h2><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td align=\"center\">Linly-AI/Chinese-LLaMA-2-7B-hf</td></tr><tr><td align=\"center\">wenge-research/yayi-7b-llama2</td></tr><tr><td align=\"center\">ziqingyang/chinese-llama-2-7b</td></tr><tr><td align=\"center\">TigerResearch/tigerbot-7b-base</td></tr><tr><td align=\"center\">LinkSoul/Chinese-Llama-2-7b</td></tr><tr></tr><tr><td align=\"center\">IDEA-CCNL/Ziya-LLaMA-13B-v1.1</td></tr><tr></tr><tr><td align=\"center\"><strong>Colossal-LLaMA-2-13b-base</strong></td></tr></tbody></table><ul><li>Up to 10 times faster for RLHF PPO Stage3 Training</li></ul><ul><li>Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference</li></ul><ul><li>Up to 10.3x growth in model capacity on one GPU</li><li>A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)</li></ul><ul><li>Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU</li><li>Keep at a sufficiently high running speed</li></ul><ul><li><a href=\"https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion\">Training</a>: Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).</li></ul><ul><li><a href=\"https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion\">Inference</a>: Reduce inference GPU memory consumption by 2.5x.</li></ul><ul><li><a href=\"https://github.com/hpcaitech/FastFold\">FastFold</a>: Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.</li></ul><ul><li><a href=\"https://github.com/biomap-research/xTrimoMultimer\">xTrimoMultimer</a>: accelerating structure prediction of protein monomers and multimer by 11x.</li></ul><ul><li>70 billion parameter LLaMA2 model training accelerated by 195% <a href=\"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama\">[code]</a><a href=\"https://www.hpc-ai.tech/blog/70b-llama2-training\">[blog]</a></li></ul><ul><li>65-billion-parameter large model pretraining accelerated by 38% <a href=\"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama\">[code]</a><a href=\"https://www.hpc-ai.tech/blog/large-model-pretraining\">[blog]</a></li></ul><ul><li>Enhanced MoE parallelism, Open-source MoE model training can be 9 times more efficient <a href=\"https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe\">[code]</a><a href=\"https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient\">[blog]</a></li></ul><ul><li>Save 50% GPU resources and 10.7% acceleration</li></ul><img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png\" width=\"800/\"><ul><li>11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism</li></ul><img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png\" width=\"800\"><ul><li>24x larger model size on the same hardware</li></ul><img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BERT.png\" width=\"800/\"><ul><li>2x faster training, or 50% longer sequence length</li></ul><img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png\" width=\"800/\"><ul><li><a href=\"https://github.com/facebookresearch/metaseq\">Open Pretrained Transformer (OPT)</a>, a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because of public pre-trained model weights.</li></ul><ul><li>14x larger batch size, and 5x faster training for Tensor Parallelism = 64</li></ul><h3>Recommendation System Models</h3><ul><li><a href=\"https://github.com/hpcaitech/CachedEmbedding\">Cached Embedding</a>, utilize software cache to train larger embedding tables with a smaller GPU memory budget.</li></ul><ul><li>20x larger model size on the same hardware</li></ul><ul><li>120x larger model size on the same hardware (RTX 3080)</li></ul><ul><li>34x larger model size on the same hardware</li></ul><ul><li>314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.</li></ul><ul><li><a href=\"https://github.com/hpcaitech/SwiftInfer\">SwiftInfer</a>: Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations</li></ul><p>If you encounter any problem with installation, you may want to raise an <a href=\"https://github.com/hpcaitech/ColossalAI/issues/new/choose\">issue</a> in this repository.</p><p>You can easily install Colossal-AI with the following command. <strong>By default, we do not build PyTorch extensions during installation.</strong></p><p><strong>Note: only Linux is supported for now.</strong></p><p>However, if you want to build the PyTorch extensions during installation, you can set .</p><pre><code>BUILD_EXT=1 pip install colossalai\n</code></pre><p><strong>Otherwise, CUDA kernels will be built during runtime when you actually need them.</strong></p><p>We also keep releasing the nightly version to PyPI every week. This allows you to access the unreleased features and bug fixes in the main branch. Installation can be made via</p><pre><code>pip install colossalai-nightly\n</code></pre><blockquote><p>The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problems. :)</p></blockquote><pre><code>git clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# install colossalai\npip install .\n</code></pre><p>By default, we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime. If you want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):</p><pre><code>BUILD_EXT=1 pip install .\n</code></pre><p>For Users with CUDA 10.2, you can still build ColossalAI from source. However, you need to manually download the cub library and copy it to the corresponding directory.</p><pre><code># clone the repository\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# download the cub library\nwget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip\nunzip 1.8.0.zip\ncp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/\n\n# install\nBUILD_EXT=1 pip install .\n</code></pre><p>You can directly pull the docker image from our <a href=\"https://hub.docker.com/r/hpcaitech/colossalai\">DockerHub page</a>. The image is automatically uploaded upon release.</p><p>Run the following command to build a docker image from Dockerfile provided.</p><blockquote><p>Building Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker Runtime as the default when doing . More details can be found <a href=\"https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime\">here</a>. We recommend you install Colossal-AI from our <a href=\"https://www.colossalai.org\">project page</a> directly.</p></blockquote><pre><code>cd ColossalAI\ndocker build -t colossalai ./docker\n</code></pre><p>Run the following command to start the docker container in interactive mode.</p><pre><code>docker run -ti --gpus all --rm --ipc=host colossalai bash\n</code></pre><p>Join the Colossal-AI community on <a href=\"https://github.com/hpcaitech/ColossalAI/discussions\">Forum</a>, <a href=\"https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w\">Slack</a>, and <a href=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png\" title=\"qrcode\">WeChat(ÂæÆ‰ø°)</a> to share your suggestions, feedback, and questions with our engineering team.</p><p>Referring to the successful attempts of <a href=\"https://bigscience.huggingface.co/\">BLOOM</a> and <a href=\"https://en.wikipedia.org/wiki/Stable_Diffusion\">Stable Diffusion</a>, any and all developers and partners with computing powers, datasets, models are welcome to join and build the Colossal-AI community, making efforts towards the era of big AI models!</p><p>You may contact us or participate in the following ways:</p><p>Thanks so much to all of our amazing contributors!</p><a href=\"https://github.com/hpcaitech/ColossalAI/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=hpcaitech/ColossalAI\" width=\"800px\"></a><p>We leverage the power of <a href=\"https://github.com/features/actions\">GitHub Actions</a> to automate our development, release and deployment workflows. Please check out this <a href=\"https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/.github/workflows/README.md\">documentation</a> on how the automated workflows are operated.</p><p>This project is inspired by some related projects (some by our team and some by other organizations). We would like to credit these amazing projects as listed in the <a href=\"https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/docs/REFERENCE.md\">Reference List</a>.</p><p>To cite this project, you can use the following BibTeX citation.</p><pre><code>@inproceedings{10.1145/3605573.3605613,\nauthor = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},\ntitle = {Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},\nyear = {2023},\nisbn = {9798400708435},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3605573.3605613},\ndoi = {10.1145/3605573.3605613},\nabstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},\nbooktitle = {Proceedings of the 52nd International Conference on Parallel Processing},\npages = {766‚Äì775},\nnumpages = {10},\nkeywords = {datasets, gaze detection, text tagging, neural networks},\nlocation = {Salt Lake City, UT, USA},\nseries = {ICPP '23}\n}\n</code></pre>","contentLength":7769,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"treetrum/amazon-kindle-bulk-downloader","url":"https://github.com/treetrum/amazon-kindle-bulk-downloader","date":1740104863,"author":"","guid":7540,"unread":true,"content":"<p>Allows you to bulk download all your Kindle eBook in a more automated fashion. This tool allows you to create backup copies of the books you've already purchased.</p><p>Designed for downloading your Kindle eBooks in a more automated fashion than is typically permitted, this tool allows you to create backup copies of the books you've already purchased.</p><p>The most important pre-requisite is that you have a physical e-ink Kindle or an Amazon Fire Tablet linked to your Amazon account. This is a requirement from Amazon's side and this tool does not offer a way to bypass this.</p><p>An important distinction is that the physical Kindle you have linked cannot be one of the latest 2024 models. For whatever reason, Amazon has decided to block the ability to download books from these devices. This tool will not work with these devices.</p><p>The easiest way to check if this tool will work for you is to log into your Amazon account manually and navigate to <code>Manage Your Content and Devices</code>. From there when clicking on <code>More Actions &gt; Download &amp; Transfer via USB</code> on a book, you should see your Kindle or Fire Tablet device listed. If you see the message <code>You do not have any compatible devices registered for this content</code> then it means you:</p><ol><li>either don't have a Kindle or Fire Tablet device linked to your account, OR</li><li>have a Kindle or Fire Tablet device that is not compatible with this tool</li></ol><p>If you are able to proceed to the next screen and download the book, then this tool should work for you.</p><p>You need to have Bun installed before you can run this too. Please see here for the most up to date instructions for your system: <a href=\"https://bun.sh/docs/installation\">https://bun.sh/docs/installation</a></p><p>Once Bun is installed, clone this repository, and run the following command from the root of the downloaded repository to install dependencies.</p><p>Then run the following command to install a special version of chrome for use by this tool</p><pre><code>bunx puppeteer browsers install chrome\n</code></pre><p>Note that amazon credentials will need to be provided. Currently this script expects them to be in the following ENV variables:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td>6 digit one-time password</td><td>If 2 factor auth is enabled</td></tr></tbody></table><p>Note this tool is already configured to read environment variables from an  file in the root of the repo. You can make a copy of the  file called  and fill in your details there for an easy way to get started.</p><p>The following CLI arguments are made available to customise the downloader to your needs</p><table><thead><tr></tr></thead><tbody><tr><td>N/A (Will be prompted if not provided)</td><td>Which Amazon base URL to use. Note, this MUST include <a href=\"http://www\">www</a>. in order to work properly</td></tr><tr><td>Total number of downloads to do</td></tr><tr><td>Maximum number of concurrent downloads</td></tr><tr><td>Index offset to begin downloading from. Allows resuming of previous partially finished attempts.</td></tr><tr><td>Allows user to manually login using the pupeteer UI instead of automatically using ENV vars. Use when auto login is not working.</td></tr><tr><td>How to handle files of the same name/size on the filesystem. Options: skip, overwrite</td></tr></tbody></table><p>You can run this tool using the following command</p><p>Command line arguments can be provided as follows</p><pre><code>bun run start --baseUrl \"https://www.amazon.com.au\"\n</code></pre><pre><code>docker build . \\\n   -t amazon-kindle-bulk-downloader\n</code></pre><p>Run the built docker image ensuring to pass in all the required ENV vars and any CLI flags you wish to override. See below for an example:</p><pre><code>docker run \\\n   --rm \\\n   -ti \\\n   -v ./downloads:/app/downloads \\\n   -e AMAZON_USER=userName \\\n   -e PASSWORD=pass \\\n   -e OTP=otpCode \\\n   amazon-kindle-bulk-downloader \\\n   --baseUrl \"https://www.amazon.com\"\n</code></pre><h3>Docker specific env variable</h3><table><thead><tr></tr></thead><tbody><tr><td>run puppeteer in headless mode</td></tr><tr><td>additional arguments for puppeteer</td></tr></tbody></table><p>In docker  does not work you must provide credentials via env</p><p>If you are on arm64 (i.e. a Mac with an Apple Silicon chip) you mast add  when running your  and `docker build</p><p>If you're having any issues with the tool, please first ensure that you have the latest version of the code by running  or redownloading the repository and then running  from the root of the repository.</p><p>In addition, please ensure that you have followed the <a href=\"https://github.com/treetrum/amazon-kindle-bulk-downloader?tab=readme-ov-file#pre-requisites\">pre-requisites</a> section carefully and can download a book manually through the website.</p><h4>Error: Found 0 books in total</h4><p>This error  indicate that you are authenticated to the wrong Amazon account for your locale.</p><p>You need to ensure that you provide the base URL for the Amazon region where your purchases were made. For example, if you are in Australia, you should use <code>https://www.amazon.com.au</code>, if you are in the USA, you should use , etc.</p><p>This base URL should be provided as a CLI argument when running the tool. For example:</p><pre><code># For USA based accounts\nbun run start --baseUrl \"https://www.amazon.com\"\n\n# For Australia based accounts\nbun run start --baseUrl \"https://www.amazon.com.au\"\n\n# ... etc.\n</code></pre><h4>Error: 404 errors when downloading books</h4><p>From my testing, 404 errors on book downloads largely occur when the tool is attempting to download an item that is simply not downloadable. From what I can tell, this includes certain comic book purchases. Users with comic book libraries and knowledge of javascript are encouraged to contribute to the project to help test and fix this issue.</p><p>There has also <a href=\"https://github.com/treetrum/amazon-kindle-bulk-downloader/issues/162#issue-2864124279\">been an indication</a> that 404s can ocurr when a book is present on too many devices. If that's the case, you can try deleting the book of some of your devices and try the tool again.</p><h4>Error: Failed to parse CRSF token</h4><p>This indicates that your login did not fully succeed. There are a multitude of reasons why this could happen however the simplest fix is often to login using the --manualAuth CLI flag instead of using the automated process.</p><h4>Error:  when running </h4><p>This error occurs when you are running  from outside the root of the repository. You need to be in the root of the repository to run this command.</p><p>After cloning the you will need to change directory () into the repo folder by doing:</p><pre><code>cd amazon-kindle-bulk-downloader\n</code></pre><p>After which you should be able to run the following without getting errors.</p><pre><code>bun install\nbun run start\n</code></pre><h4>All downloads are ~100kb files that are not valid books</h4><p>The 100kb files are actually error web pages instead of book files and likely indicates that the book files were purchased in another region (even if they show up!).</p><p>See <a href=\"https://github.com/treetrum/amazon-kindle-bulk-downloader/issues/192#issuecomment-2676081558\">here</a> for original report of this.</p><p>The fix is to ensure the correct baseUrl is passed for the region that the books were purchased in.</p>","contentLength":6214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"wangrongding/wechat-bot","url":"https://github.com/wangrongding/wechat-bot","date":1740104863,"author":"","guid":7541,"unread":true,"content":"<p>ü§ñ‰∏Ä‰∏™Âü∫‰∫é WeChaty ÁªìÂêà DeepSeek / ChatGPT / Kimi / ËÆØÈ£ûÁ≠âAiÊúçÂä°ÂÆûÁé∞ÁöÑÂæÆ‰ø°Êú∫Âô®‰∫∫ ÔºåÂèØ‰ª•Áî®Êù•Â∏ÆÂä©‰Ω†Ëá™Âä®ÂõûÂ§çÂæÆ‰ø°Ê∂àÊÅØÔºåÊàñËÄÖÁÆ°ÁêÜÂæÆ‰ø°Áæ§/Â•ΩÂèãÔºåÊ£ÄÊµãÂÉµÂ∞∏Á≤âÁ≠â...</p><p>‰∏Ä‰∏™ Âü∫‰∫é  +  ÁöÑÂæÆ‰ø°Êú∫Âô®‰∫∫</p><p>ÂèØ‰ª•Áî®Êù•Â∏ÆÂä©‰Ω†Ëá™Âä®ÂõûÂ§çÂæÆ‰ø°Ê∂àÊÅØÔºåÊàñËÄÖÁÆ°ÁêÜÂæÆ‰ø°Áæ§/Â•ΩÂèã.</p><p>ÔºåÔºå Â∞±ËÉΩÁé©Ëµ∑Êù•‰∫Ü„ÄÇüå∏ Â¶ÇÊûúÂØπÊÇ®ÊúâÊâÄÂ∏ÆÂä©ÔºåËØ∑ÁÇπ‰∏™ Star ‚≠êÔ∏è ÊîØÊåÅ‰∏Ä‰∏ã„ÄÇ</p><a href=\"https://github.com/wangrongding/wechat-bot/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=wangrongding/wechat-bot&amp;columns=20\"></a><p>Ê¨¢ËøéÂ§ßÂÆ∂Êèê‰∫§ PR Êé•ÂÖ•Êõ¥Â§öÁöÑ ai ÊúçÂä°(ÊØîÂ¶ÇÊâ£Â≠êÁ≠â...)ÔºåÁßØÊûÅË¥°ÁåÆÊõ¥Â•ΩÁöÑÂäüËÉΩÂÆûÁé∞ÔºåËÆ© wechat-bot ÂèòÂæóÊõ¥Âº∫ÔºÅ</p><h2>‰ΩøÁî®ÂâçÈúÄË¶ÅÈÖçÁΩÆÁöÑ AI ÊúçÂä°ÔºàÁõÆÂâçÊîØÊåÅ 9 ÁßçÔºåÂèØ‰ªªÈÄâÂÖ∂‰∏ÄÔºâ</h2><ul><li><p>Ëé∑ÂèñËá™Â∑±ÁöÑ ÔºåÂú∞ÂùÄÊà≥ËøôÈáå üëâüèª Ôºö<a href=\"https://platform.deepseek.com/usage\">deepseek ÂºÄÊîæÂπ≥Âè∞</a> Â∞ÜËé∑ÂèñÂà∞ÁöÑÂ°´ÂÖ•  Êñá‰ª∂‰∏≠ÁöÑ  ‰∏≠„ÄÇ</p></li><li><p><strong>Ê≥®ÊÑèÔºöËøô‰∏™ÊòØÈúÄË¶ÅÂéª‰ªòË¥πË¥≠‰π∞ÁöÑÔºåÂæàÂ§ö‰∫∫ËøáÊù•ÈóÆ‰∏∫‰ªÄ‰πàËØ∑Ê±Ç‰∏çÈÄöÔºåËØ∑Á°Æ‰øùÁªàÁ´ØËµ∞‰∫Ü‰ª£ÁêÜÔºåÂπ∂‰∏î‰ªòË¥πË¥≠‰π∞‰∫ÜÂÆÉÁöÑÊúçÂä°</strong></p><pre><code># ÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§ÔºåÊã∑Ë¥ù‰∏Ä‰ªΩ .env.example Êñá‰ª∂‰∏∫ .env\ncp .env.example .env\n# Â°´ÂÜôÂÆåÂñÑ .env Êñá‰ª∂‰∏≠ÁöÑÂÜÖÂÆπ\nOPENAI_API_KEY='‰Ω†ÁöÑkey'\n</code></pre></li><li><p>ÈÄö‰πâÂçÉÈóÆÊòØÈòøÈáå‰∫ëÊèê‰æõÁöÑ AI ÊúçÂä°ÔºåËé∑ÂèñÂà∞‰Ω†ÁöÑ api key ‰πãÂêé, Â°´ÂÜôÂà∞ .env Êñá‰ª∂‰∏≠Âç≥ÂèØ</p><pre><code># ÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§ÔºåÊã∑Ë¥ù‰∏Ä‰ªΩ .env.example Êñá‰ª∂‰∏∫ .env\ncp .env.example .env\n# Â°´ÂÜôÂÆåÂñÑ .env Êñá‰ª∂‰∏≠ÁöÑÂÜÖÂÆπ\n# ÈÄö‰πâÂçÉÈóÆ, URL ÂåÖÂê´ uri Ë∑ØÂæÑ\nTONGYI_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n# ÈÄö‰πâÂçÉÈóÆÁöÑ API_KEY\nTONGYI_API_KEY = ''\n# ÈÄö‰πâÂçÉÈóÆ‰ΩøÁî®ÁöÑÊ®°Âûã\nTONGYI_MODEL='qwen-plus'\n</code></pre></li><li><p>Êñ∞Â¢ûÁßëÂ§ßËÆØÈ£ûÔºåÂéªËøôÈáåÁî≥ËØ∑‰∏Ä‰∏™ keyÔºö<a href=\"https://console.xfyun.cn/services/bm35\">ÁßëÂ§ßËÆØÈ£û</a>ÔºåÊØè‰∏™Ê®°ÂûãÈÉΩÊúâ 200 ‰∏áÁöÑÂÖçË¥π token ÔºåÊÑüËßâÂæàÈöæÁî®ÂÆå„ÄÇ Ê≥®ÊÑèÔºö ËÆØÈ£ûÁöÑÈÖçÁΩÆÊñá‰ª∂Âá†‰∏™ keyÔºåÂà´Â°´Âèç‰∫ÜÔºåÂæàÂ§ö‰∫∫ÊâæÂà∞ÊàëËØ¥‰∏∫‰ªÄ‰πà‰∏çÂõûÂ§çÔºåÈÉΩÊòØÂ°´Âèç‰∫Ü„ÄÇ<p> ËÄå‰∏îËøòÊúâ‰∏Ä‰∏™Â•ΩÂ§ÑÂ∞±ÊòØÔºåÊé•Âè£‰∏ç‰ºöÂÉè Kimi ‰∏ÄÊ†∑ÈôêÂà∂ËØ∑Ê±ÇÈ¢ëÊ¨°ÔºåÁõ∏ÂØπÊù•ËØ¥Á®≥ÂÆöÂæàÂ§ö„ÄÇ</p> ÊúçÂä°Âá∫ÈîôÂèØÂèÇËÄÉÔºö <a href=\"https://github.com/wangrongding/wechat-bot/issues/170\">issues/170</a>, <a href=\"https://github.com/wangrongding/wechat-bot/issues/180\">issues/180</a></p></li><li><p>ÂèØ‰ª•Âéª Ôºö <a href=\"https://platform.moonshot.cn/console/api-keys\">kimi apikey</a> Ëé∑Âèñ‰Ω†ÁöÑ key ÊúÄËøëÊØîËæÉÂøôÔºåÂ§ßÂÆ∂ÊÑüÂÖ¥Ë∂£ÂèØ‰ª•Êèê‰∫§ PRÔºåÊàë‰ºöÂ∞ΩÂø´ÂêàÂπ∂„ÄÇÁõÆÂâç Kimi ÂàöÂàöÈõÜÊàêÔºåËøòÂèØ‰ª•ÂÆûÁé∞‰∏ä‰º†Êñá‰ª∂Á≠âÂäüËÉΩÔºåÁÑ∂ÂêéÊúâÂÖ∂ÂÆÉËæÉÂ•ΩÁöÑÊúçÂä°‰πüÂèØ‰ª•Êèê‰∫§ PR „ÄÇ</p></li><li><p>Âú∞ÂùÄÔºö<a href=\"https://dify.ai/\">dify</a>, ÂàõÂª∫‰Ω†ÁöÑÂ∫îÁî®‰πãÂêé, Ëé∑ÂèñÂà∞‰Ω†ÁöÑ api key ‰πãÂêé, Â°´ÂÜôÂà∞ .env Êñá‰ª∂‰∏≠Âç≥ÂèØ, ‰πüÊîØÊåÅÁßÅÊúâÂåñÈÉ®ÁΩ≤difyÁâàÊú¨</p><pre><code># ÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§ÔºåÊã∑Ë¥ù‰∏Ä‰ªΩ .env.example Êñá‰ª∂‰∏∫ .env\ncp .env.example .env\n# Â°´ÂÜôÂÆåÂñÑ .env Êñá‰ª∂‰∏≠ÁöÑÂÜÖÂÆπ\nDIFY_API_KEY='‰Ω†ÁöÑkey'\n# Â¶ÇÊûúÈúÄË¶ÅÁßÅÊúâÂåñÈÉ®ÁΩ≤ÔºåËØ∑‰øÆÊîπ.env‰∏≠‰∏ãÈù¢ÁöÑÈÖçÁΩÆ\n# DIFY_URL='https://[‰Ω†ÁöÑÁßÅÊúâÂåñÈÉ®ÁΩ≤Âú∞ÂùÄ]'\n</code></pre></li><li><p>Ollama ÊòØ‰∏Ä‰∏™Êú¨Âú∞ÂåñÁöÑ AI ÊúçÂä°ÔºåÂÆÉÁöÑ API ‰∏é OpenAI ÈùûÂ∏∏Êé•Ëøë„ÄÇÈÖçÁΩÆ Ollama ÁöÑËøáÁ®ã‰∏éÂêÑÁßçÂú®Á∫øÊúçÂä°Áï•Êúâ‰∏çÂêå</p><pre><code># ÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§ÔºåÊã∑Ë¥ù‰∏Ä‰ªΩ .env.example Êñá‰ª∂‰∏∫ .env\ncp .env.example .env\n# Â°´ÂÜôÂÆåÂñÑ .env Êñá‰ª∂‰∏≠ÁöÑÂÜÖÂÆπ\nOLLAMA_URL='http://127.0.0.1:11434/api/chat'\nOLLAMA_MODEL='qwen2.5:7b'\nOLLAMA_SYSTEM_MESSAGE='You are a personal assistant.'\n</code></pre></li><li><p>AIËÅöÂêàÂπ≥Âè∞ÔºåÊúâÂ•óÂ£≥GPTÁöÑAPIÔºå‰πüÊúâÂÖ∂‰ªñÊ®°ÂûãÔºåÁÇπËøôÈáåÂèØ‰ª•<a href=\"https://dash.302.ai/apis/list\">Ê∑ªÂä†API</a>ÔºåÊ∑ªÂä†‰πãÂêéÊääAPI KEYÈÖçÁΩÆÂà∞.envÈáåÔºåÂ¶Ç‰∏ãÔºåMODELÂèØ‰ª•Ëá™Ë°åÈÄâÊã©ÈÖçÁΩÆ</p><pre><code>_302AI_API_KEY = 'xxxx'\n_302AI_MODEL= 'gpt-4o-mini'\n</code></pre><p>Áî±‰∫éopenaiÂÖÖÂÄºÈúÄË¶ÅÂõΩÂ§ñ‰ø°Áî®Âç°ÔºåÊµÅÁ®ãÊØîËæÉÁπÅÁêêÔºåÂ§ßÂ§öÈúÄË¶ÅÊêûÂõΩÂ§ñËôöÊãüÂç°ÔºåÊâãÁª≠Ë¥π‰πüÈÉΩ‰∏çÂ∞ëÔºåËØ•Âπ≥Âè∞ÂèØ‰ª•Áõ¥Êé•ÊîØ‰ªòÂÆùÔºåÁÆóÊòØÊØîËæÉÁúÅ‰∫ãÁöÑÔºåÊ≥®ÂÜåÂ°´ÈóÆÂç∑ÂèØÈ¢Ü1ÂàÄÈ¢ùÂ∫¶ÔºåÂêéÁª≠ÂÖÖÂÄº‰πüÊúâÊâãÁª≠Ë¥πÔºåÁî®Êà∑ÂèØËá™Ë°åÈÖåÊÉÖÈÄâÊã©„ÄÇ</p></li><li><pre><code># ÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§ÔºåÊã∑Ë¥ù‰∏Ä‰ªΩ .env.example Êñá‰ª∂‰∏∫ .envÔºåÂ¶ÇÊûúÂ∑≤Â≠òÂú®ÂàôÂøΩÁï•Ê≠§Ê≠•\ncp .env.example .env\n\n# ÁºñËæë.envÊñá‰ª∂Âπ∂Ê∑ªÂä†claudeÁõ∏ÂÖ≥ÈÖçÁΩÆ\n\nCLAUDE_API_VERSION = '2023-06-01'\nCLAUDE_API_KEY = '‰Ω†ÁöÑAPI KEY'\nCLAUDE_MODEL = 'claude-3-5-sonnet-latest'\n# Á≥ªÁªü‰∫∫ËÆæ\nCLAUDE_SYSTEM = ''\n</code></pre></li><li><p>ÂÖ∂‰ªñ ÔºàÂæÖÂÆûË∑µÔºâÁêÜËÆ∫‰∏ä‰ΩøÁî® openAI Ê†ºÂºèÁöÑ apiÔºåÈÉΩÂèØ‰ª•‰ΩøÁî®ÔºåÂú® env Êñá‰ª∂‰∏≠‰øÆÊîπÂØπÂ∫îÁöÑ api_key„ÄÅmodel„ÄÅproxy_url Âç≥ÂèØ„ÄÇ</p></li></ul><div align=\"center\"><table><tbody><tr><td align=\"center\" width=\"100%\"></td></tr><tr><td align=\"left\"> 302.AIÊòØ‰∏Ä‰∏™ÊåâÈúÄ‰ªòË¥πÁöÑ‰∏ÄÁ´ôÂºè‰ºÅ‰∏öÁ∫ßAIÂ∫îÁî®Âπ≥Âè∞ÔºåÂºÄÊîæÂπ≥Âè∞ÔºåÂºÄÊ∫êÁîüÊÄÅÔºåËÆ©AI‰∏∫ÊØè‰∏™ÈúÄÊ±ÇÊâæÂà∞Á≠îÊ°à„ÄÇ <a href=\"https://gpt302.saaslink.net/hqRIfk\" target=\"_blank\">‰∫ßÂìÅÈìæÊé•</a></td></tr></tbody></table></div><p>ÁõÆÂâçËØ•È°πÁõÆÊµÅÈáèËæÉÂ§ßÔºåÂ∑≤Áªè‰∏äËøá 27 Ê¨° <a href=\"https://github.com/trending\">Github Trending Ê¶ú</a>ÔºåÂ¶ÇÊûúÊÇ®ÁöÑÂÖ¨Âè∏ÊàñËÄÖ‰∫ßÂìÅÈúÄË¶ÅÊé®ÂπøÔºåÂèØ‰ª•Âú®‰∏ãÊñπ‰∫åÁª¥Á†ÅÂ§ÑËÅîÁ≥ªÊàëÔºåÊàë‰ºöÂú®È°πÁõÆ‰∏≠Âä†ÂÖ•ÊÇ®ÁöÑÂπøÂëäÔºåÂ∏ÆÂä©ÊÇ®ÁöÑ‰∫ßÂìÅËé∑ÂæóÊõ¥Â§öÁöÑÊõùÂÖâ„ÄÇ</p><p>Ê£ÄÊü•Â•ΩËá™Â∑±ÁöÑÂºÄÂèëÁéØÂ¢ÉÔºåÁ°Æ‰øùÂ∑≤ÁªèÂÆâË£Ö‰∫Ü  , ÁâàÊú¨ÈúÄË¶ÅÊª°Ë∂≥ Node.js &gt;= v18.0 ÔºåÁâàÊú¨Â§™‰Ωé‰ºöÂØºËá¥ËøêË°åÊä•Èîô,ÊúÄÂ•Ω‰ΩøÁî® LTS ÁâàÊú¨„ÄÇ</p><blockquote><p>ÂÆâË£Ö‰æùËµñÊó∂ÔºåÂ§ßÈôÜÁöÑÊúãÂèãÊé®ËçêÂàáÂà∞ taobao ÈïúÂÉèÊ∫êÂêéÂÜçÂÆâË£ÖÔºåÂëΩ‰ª§Ôºö<code>npm config set registry https://registry.npmmirror.com</code> ÊÉ≥Ë¶ÅÁÅµÊ¥ªÂàáÊç¢ÔºåÊé®Ëçê‰ΩøÁî®ÊàëÁöÑÂ∑•ÂÖ∑ üëâüèª <a href=\"https://github.com/wangrongding/prm-cli\">prm-cli</a> Âø´ÈÄüÂàáÊç¢„ÄÇ</p></blockquote><pre><code># ÂÆâË£Ö‰æùËµñ\nnpm i\n# Êé®ËçêÁî® yarn ÂêßÔºånpm ÂÆâË£ÖÊúâÊó∂‰ºöÈÅáÂà∞ wechaty ÂÜÖÈÉ®‰æùËµñÂÆâË£ÖÂ§±Ë¥•ÁöÑÈóÆÈ¢ò\nyarn\n</code></pre><pre><code># ÂêØÂä®ÊúçÂä°\nnpm run dev # ÊàñËÄÖ npm run start\n# ÂêØÂä®ÊúçÂä°\nyarn dev # ÊàñËÄÖ yarn start\n</code></pre><p>ÁÑ∂ÂêéÂ∞±ÂèØ‰ª•Êâ´Á†ÅÁôªÂΩï‰∫ÜÔºåÁÑ∂ÂêéÊ†πÊçÆ‰Ω†ÁöÑÈúÄÊ±ÇÔºåËá™Â∑±‰øÆÊîπÁõ∏ÂÖ≥ÈÄªËæëÊñá‰ª∂„ÄÇ</p><p>‰∏∫‰∫ÜÂÖºÂÆπ docker ÈÉ®ÁΩ≤ÔºåÈÅøÂÖç‰∏çÂøÖË¶ÅÁöÑÈÄâÊã©‰∫§‰∫íÔºåÊñ∞Â¢ûÊåáÂÆöÊúçÂä°ËøêË°å</p><pre><code># ËøêË°åÊåáÂÆöÊúçÂä° Ôºà ÁõÆÂâçÊîØÊåÅ ChatGPT | Kimi | Xunfei Ôºâ\nnpm run start -- --serve Kimi\n# ‰∫§‰∫íÈÄâÊã©ÊúçÂä°Ôºå‰ªçÁÑ∂‰øùÊåÅÂéüÊúâÁöÑÈÄªËæë\nnpm run start\n</code></pre><p>ÂÆâË£ÖÂÆå‰æùËµñÂêéÔºåËøêË°å  ÂâçÔºåÂèØ‰ª•ÂÖàÊµãËØï‰∏ã openai ÁöÑÊé•Âè£ÊòØÂê¶ÂèØÁî®ÔºåËøêË°å  Âç≥ÂèØ„ÄÇ</p><p>ÈÅáÂà∞ timeout ÈóÆÈ¢òÈúÄË¶ÅËá™Ë°åËß£ÂÜ≥„ÄÇÔºà‰∏ÄËà¨Â∞±ÊòØ‰ª£ÁêÜÊú™ÊàêÂäüÔºåÊàñËÄÖ‰Ω†ÁöÑÊ¢ØÂ≠êÈôêÂà∂‰∫ÜË∞É openai api ÁöÑÊúçÂä°Ôºâ</p><p>ÂæàÂ§ö‰∫∫ËØ¥ËøêË°åÂêé‰∏ç‰ºöËá™Âä®Êî∂Âèë‰ø°ÊÅØÔºå‰∏çÊòØÁöÑÂìàÔºå‰∏∫‰∫ÜÈò≤Ê≠¢ÁªôÊØè‰∏ÄÊù°Êî∂Âà∞ÁöÑÊ∂àÊÅØÈÉΩËá™Âä®ÂõûÂ§çÔºàÂ§™ÊÅêÊÄñ‰∫ÜÔºâÔºåÊâÄ‰ª•Âä†‰∫ÜÈôêÂà∂Êù°‰ª∂„ÄÇ</p><ul><li>Áæ§ËÅäÔºåËÆ∞ÂæóÊääÊú∫Âô®‰∫∫ÂêçÁß∞ÊîπÊàê‰Ω†Ëá™Â∑±ÂæÆ‰ø°Âè∑ÁöÑÂêçÁß∞ÔºåÁÑ∂ÂêéÊ∑ªÂä†ÂØπÂ∫îÁæ§ËÅäÁöÑÂêçÁß∞Âà∞ÁôΩÂêçÂçï‰∏≠ÔºåËøôÊ†∑Â∞±ÂèØ‰ª•Ëá™Âä®ÂõûÂ§çÁæ§ËÅäÊ∂àÊÅØ‰∫Ü„ÄÇ</li><li>ÁßÅËÅäÔºåËÆ∞ÂæóÊääÈúÄË¶ÅËá™Âä®ÂõûÂ§çÁöÑÂ•ΩÂèãÂêçÁß∞Ê∑ªÂä†Âà∞ÁôΩÂêçÂçï‰∏≠ÔºåËøôÊ†∑Â∞±ÂèØ‰ª•Ëá™Âä®ÂõûÂ§çÁßÅËÅäÊ∂àÊÅØ‰∫Ü„ÄÇ</li><li>Êõ¥Ê∑±ÂÖ•ÁöÑÂèØ‰ª•ÈÄöËøá‰øÆÊîπ <code>src/wechaty/sendMessage.js</code> Êñá‰ª∂Êù•Êª°Ë∂≥‰Ω†Ëá™Â∑±ÁöÑ‰∏öÂä°Âú∫ÊôØ„ÄÇÔºàÂ§ßÂ§ö‰∫∫ÂèçÈ¶àÂèØËÉΩÊó†Ê≥ïËá™Âä®ÂõûÂ§çÔºå‰πüÂèØ‰ª•ÈÄöËøáË∞ÉËØïËøô‰∏™Êñá‰ª∂Êù•ÊéíÊü•ÂÖ∑‰ΩìÂéüÂõ†Ôºâ</li></ul><pre><code># ÁôΩÂêçÂçïÈÖçÁΩÆ\n#ÂÆö‰πâÊú∫Âô®‰∫∫ÁöÑÂêçÁß∞ÔºåËøôÈáåÊòØ‰∏∫‰∫ÜÈò≤Ê≠¢Áæ§ËÅäÊ∂àÊÅØÂ§™Â§öÔºåÊâÄ‰ª•Âè™ÊúâËâæÁâπÊú∫Âô®‰∫∫Êâç‰ºöÂõûÂ§çÔºå\n#ËøôÈáå‰∏çË¶ÅÊää@ÂéªÊéâÔºåÂú®@ÂêéÈù¢Âä†‰∏ä‰Ω†ÂêØÂä®Êú∫Âô®‰∫∫Ë¥¶Âè∑ÁöÑÂæÆ‰ø°ÂêçÁß∞\nBOT_NAME=@ÂèØ‰πê\n#ËÅîÁ≥ª‰∫∫ÁôΩÂêçÂçï\nALIAS_WHITELIST=ÂæÆ‰ø°Âêç1,Â§áÊ≥®Âêç2\n#Áæ§ËÅäÁôΩÂêçÂçï\nROOM_WHITELIST=XXÁæ§1,Áæ§2\n#Ëá™Âä®ÂõûÂ§çÂâçÁºÄÂåπÈÖçÔºåÊñáÊú¨Ê∂àÊÅØÂåπÈÖçÂà∞ÊåáÂÆöÂâçÁºÄÊó∂ÔºåÊâç‰ºöËß¶ÂèëËá™Âä®ÂõûÂ§çÔºå‰∏çÈÖçÊàñÈÖçÁ©∫‰∏≤ÊÉÖÂÜµ‰∏ãËØ•ÈÖçÁΩÆ‰∏çÁîüÊïàÔºàÈÄÇÁî®‰∫éÁî®Â§ßÂè∑Ôºå‰∏çÊúüÊúõÊØèÊ¨°Ë¢´@ÊàñËÄÖÁßÅËÅäÊó∂ÈÉΩËß¶ÂèëËá™Âä®ÂõûÂ§çÁöÑ‰∫∫Áæ§Ôºâ\n#ÂåπÈÖçËßÑÂàôÔºöÁæ§ËÅäÊ∂àÊÅØÂéªÊéâ${BOT_NAME}Âπ∂trimÂêéËøõË°åÂâçÁºÄÂåπÈÖçÔºåÁßÅËÅäÊ∂àÊÅØtrimÂêéÁõ¥Êé•ËøõË°åÂâçÁºÄÂåπÈÖç\nAUTO_REPLY_PREFIX=''\n</code></pre><p>ÂèØ‰ª•ÁúãÂà∞ÔºåËá™Âä®ÂõûÂ§çÈÉΩÊòØÂü∫‰∫é  ÁöÑÔºåËÆ∞ÂæóË¶ÅÂºÄ‰ª£ÁêÜÔºåÊàñËÄÖÂ°´ÂÜô‰ª£ÁêÜÂú∞ÂùÄ„ÄÇ</p><p>ËøëÊúüÂæÆ‰ø°ÂÆ°Êü•Âæà‰∏•Ê†ºÔºåÂ§ßÈáèÁî®Êà∑ÂèçÊò†ÂºπÂá∫Â§ñÊåÇË≠¶ÂëäÔºåÁî±‰∫éÈ°πÁõÆÂÜÖÈªòËÆ§‰ΩøÁî®ÁöÑÊòØÂÖçË¥πÁâàÁöÑ web ÂçèËÆÆÔºåÊâÄ‰ª•ÁõÆÂâçÊù•ËØ¥ÂæàÂÆπÊòì‰ºöË¢´ÂæÆ‰ø°Ê£ÄÊµãÂà∞ÔºåÂª∫ËÆÆ‰ΩøÁî® pad ÂçèËÆÆÔºåÊàñËÄÖËá™Ë°åË¥≠‰π∞‰ºÅ‰∏öÁâàÂçèËÆÆÔºåÈÅøÂÖçË¢´Â∞ÅÂè∑„ÄÇ</p><p>ÂèØ‰ª•Ëøõ‰∫§ÊµÅÁæ§,‰∏ÄËµ∑‰∫§ÊµÅÊé¢ËÆ®Áõ∏ÂÖ≥ÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°àÔºåÊ∑ªÂä†ÁöÑÊó∂ÂÄôËÆ∞ÂæóÂ§áÊ≥®Êù•ÊÑè„ÄÇÔºàÂ¶ÇÊûúÈ°πÁõÆÂØπ‰Ω†ÊúâÊâÄÂ∏ÆÂä©Ôºå‰πüÂèØ‰ª•ËØ∑ÊàëÂñùÊùØÂíñÂï° ‚òïÔ∏è ~Ôºâ</p><pre><code>$ docker build . -t wechat-bot\n\n$ docker run -d --rm --name wechat-bot -v $(pwd)/.env:/app/.env wechat-bot\n</code></pre><p>ËØ•È°πÁõÆ‰∫é 2023/2/13 Êó•Êàê‰∏∫ Github Trending Ê¶úÈ¶ñ„ÄÇ</p>","contentLength":7440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nexus-xyz/network-api","url":"https://github.com/nexus-xyz/network-api","date":1740104863,"author":"","guid":7542,"unread":true,"content":"<p>High-performance interface for supplying compute to the Nexus network.</p><p>A high-performance command-line interface for contributing proofs to the Nexus network.</p><p>The <a href=\"https://docs.nexus.xyz/network\">Nexus Network</a> is a global distributed prover network that unites the world's computers to power a new and better Internet: the Verifiable Internet.</p><p>There have been two testnets so far:</p><p>For the simplest one-command install (especially for local testing or personal use). This is what most users will want to do:</p><pre><code>curl https://cli.nexus.xyz/ | sh\n</code></pre><p> This script automatically installs Rust if you don‚Äôt have it and prompts for Terms of Use in an interactive shell.</p><p>Alternatively, if you‚Äôve already downloaded :</p><p>The  script is designed to do few things:</p><ol><li>Install Rust if it's not already installed... and do it non-interactively if possible, so it does not bother the user.</li><li>Prompt the user to accept the Terms of Use (via bash) and enter their node id (via the Rust program)</li></ol><pre><code># this is the part in the install.sh script has the brittle `&lt; /dev/tty` part within CI environments\n(\n  cd \"$REPO_PATH/clients/cli\" || exit\n  cargo run --release -- --start --beta\n) &lt; /dev/tty\n</code></pre><p>This combination of bash and Rust is a bit brittle in CI environments. Consider these approaches instead:</p><ol><li>: <pre><code>git clone https://github.com/nexus-xyz/network-api\ncd network-api/clients/cli\ncargo build --release\n./target/release/nexus-network --start --beta\n</code></pre></li><li><strong>Download the script locally</strong> (and optionally set  if you need it to run without prompts): <pre><code>curl -sSf https://cli.nexus.xyz/ -o install.sh\nchmod +x install.sh\nNONINTERACTIVE=1 ./install.sh\n</code></pre></li></ol><p>Building from source or running a downloaded script gives you more control over dependencies and versions, and avoids any unexpected prompts during automated runs.</p><h2>Local Testing with a Local HTTP Server</h2><p>If you want to simulate <code>curl https://cli.nexus.xyz/ | sh</code>:</p><ol><li>In the project‚Äôs root directory, start a local server: <pre><code>python3 -m http.server 8080\n</code></pre></li><li>In a separate terminal, run: <pre><code>curl -sSf http://localhost:8080/public/install.sh | sh -x\n</code></pre></li><li>Observe the script output and verify installation logic.</li></ol><p>If you don‚Äôt have Rust installed, you will be prompted to install it (unless  is set).</p><pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install build-essential pkg-config libssl-dev git-all protobuf-compiler\n</code></pre><ol><li>Follow the Linux instructions above within WSL.</li></ol><p>Use of the CLI is subject to the <a href=\"https://nexus.xyz/terms-of-use\">Terms of Use</a>. First-time users running interactively will be prompted to accept these terms. For <strong>non-interactive acceptance</strong> (e.g., in CI), run:</p><pre><code>NONINTERACTIVE=1 sh install.sh\n</code></pre><p>or set  before invoking the script.</p><p>During the CLI‚Äôs startup, you‚Äôll be asked for your node ID (used on Testnet or Devnet).</p><ul><li>To skip prompts in a non-interactive environment (), manually place your node ID in .</li></ul><ul><li>Only the latest CLI version is supported.</li><li>No prebuilt binaries yet.</li><li>Proof cycle counting is coming soon.</li></ul><pre><code>network-api/\n‚îú‚îÄ‚îÄ assets/       # Media for documentation\n‚îú‚îÄ‚îÄ clients/\n‚îÇ   ‚îî‚îÄ‚îÄ cli/      # Main CLI implementation\n‚îú‚îÄ‚îÄ proto/        # Shared network interface definition\n‚îî‚îÄ‚îÄ public/       # Files hosted at cli.nexus.xyz\n</code></pre>","contentLength":3068,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"geekan/MetaGPT","url":"https://github.com/geekan/MetaGPT","date":1740104863,"author":"","guid":7543,"unread":true,"content":"<p>üåü The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming</p><p align=\"center\"><b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b></p><p>üöÄ Feb. 19, 2025: Today we are officially launching our natural language programming product: MGX (MetaGPT X) - the world's first AI agent development team. <a href=\"https://mgx.dev/\">Offical website</a><a href=\"https://x.com/MetaGPT_/status/1892199535130329356\">Twitter</a></p><p>üöÄ Mar. 29, 2024: <a href=\"https://github.com/geekan/MetaGPT/releases/tag/v0.8.0\">v0.8.0</a> released. Now you can use Data Interpreter (<a href=\"https://arxiv.org/abs/2402.18679\">arxiv</a>, <a href=\"https://docs.deepwisdom.ai/main/en/DataInterpreter/\">example</a>, <a href=\"https://github.com/geekan/MetaGPT/tree/main/examples/di\">code</a>) via pypi package import. Meanwhile, we integrated the RAG module and supported multiple new LLMs.</p><p>üöÄ Feb. 08, 2024: <a href=\"https://github.com/geekan/MetaGPT/releases/tag/v0.7.0\">v0.7.0</a> released, supporting assigning different LLMs to different Roles. We also introduced <a href=\"https://github.com/geekan/MetaGPT/raw/main/examples/di/README.md\">Data Interpreter</a>, a powerful agent capable of solving a wide range of real-world problems.</p><p>üöÄ Dec. 15, 2023: <a href=\"https://github.com/geekan/MetaGPT/releases/tag/v0.5.0\">v0.5.0</a> released, introducing some experimental features such as incremental development, multilingual, multiple programming languages, etc.</p><p>üî• Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the  in August 2023.</p><p>üåü Jun. 30, 2023: MetaGPT is now open source.</p><p>üåü Apr. 24, 2023: First line of MetaGPT code committed.</p><h2>Software Company as Multi-Agent System</h2><ol><li>MetaGPT takes a  as input and outputs <strong>user stories / competitive analysis / requirements / data structures / APIs / documents, etc.</strong></li><li>Internally, MetaGPT includes <strong>product managers / architects / project managers / engineers.</strong> It provides the entire process of a <strong>software company along with carefully orchestrated SOPs.</strong><ol><li> is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.</li></ol></li></ol><p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p><blockquote><p>Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: . You can use conda like this: <code>conda create -n metagpt python=3.9 &amp;&amp; conda activate metagpt</code></p></blockquote><pre><code>pip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT &amp;&amp; cd MetaGPT &amp;&amp; pip install --upgrade -e .`\n</code></pre><p>You can init the config of MetaGPT by running the following command, or manually create  file:</p><pre><code># Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n</code></pre><p>You can configure  according to the <a href=\"https://github.com/geekan/MetaGPT/raw/main/config/config2.example.yaml\">example</a> and <a href=\"https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html\">doc</a>:</p><pre><code>llm:\n  api_type: \"openai\"  # or azure / ollama / groq etc. Check LLMType for more options\n  model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n</code></pre><p>After installation, you can use MetaGPT at CLI</p><pre><code>metagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n</code></pre><pre><code>from metagpt.software_company import generate_repo, ProjectRepo\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"&lt;path&gt;\")\nprint(repo)  # it will print the repo structure with files\n</code></pre><pre><code>import asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(\"Run data analysis on sklearn Iris dataset, include a plot\")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n</code></pre><p>üìù <a href=\"https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form\">Fill out the form</a> to become a contributor. We are looking forward to your participation!</p><p>If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!</p><p>We will respond to all questions within 2-3 business days.</p><p>To stay updated with the latest research and development, follow <a href=\"https://twitter.com/MetaGPT_\">@MetaGPT_</a> on Twitter.</p><pre><code>@inproceedings{hong2024metagpt,\n      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\\\"u}rgen Schmidhuber},\n      booktitle={The Twelfth International Conference on Learning Representations},\n      year={2024},\n      url={https://openreview.net/forum?id=VtmBAGCN7o}\n}\n@misc{teng2025atom,\n      title={Atom of Thoughts for Markov LLM Test-Time Scaling}, \n      author={Fengwei Teng and Zhaoyang Yu and Quan Shi and Jiayi Zhang and Chenglin Wu and Yuyu Luo},\n      year={2025},\n      eprint={2502.12018},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.12018}, \n}\n@misc{xiang2025self,\n      title={Self-Supervised Prompt Optimization}, \n      author={Jinyu Xiang and Jiayi Zhang and Zhaoyang Yu and Fengwei Teng and Jinhao Tu and Xinbing Liang and Sirui Hong and Chenglin Wu and Yuyu Luo},\n      year={2025},\n      eprint={2502.06855},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.06855}, \n}\n@inproceedings{wang2025fact,\n      title={FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval}, \n      author={Jinlin Wang and Suyuchen Wang and Ziwen Xia and Sirui Hong and Yun Zhu and Bang Liu and Chenglin Wu},\n      booktitle={The 2025 Annual Conference of the Nations of the Americas Chapter of the ACL},\n      year={2025},\n      url={https://openreview.net/forum?id=VXOircx5h3}\n}\n@misc{chi2024sela,\n      title={SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning}, \n      author={Yizhou Chi and Yizhang Lin and Sirui Hong and Duyi Pan and Yaying Fei and Guanghao Mei and Bangbang Liu and Tianqi Pang and Jacky Kwok and Ceyao Zhang and Bang Liu and Chenglin Wu},\n      year={2024},\n      eprint={2410.17238},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2410.17238}, \n}\n@inproceedings{zhang2025aflow,\n      title={{AF}low: Automating Agentic Workflow Generation},\n      author={Jiayi Zhang and Jinyu Xiang and Zhaoyang Yu and Fengwei Teng and Xiong-Hui Chen and Jiaqi Chen and Mingchen Zhuge and Xin Cheng and Sirui Hong and Jinlin Wang and Bingnan Zheng and Bang Liu and Yuyu Luo and Chenglin Wu},\n      booktitle={The Thirteenth International Conference on Learning Representations},\n      year={2025},\n      url={https://openreview.net/forum?id=z5uVAKwmjf}\n}\n@misc{hong2024data,\n      title={Data Interpreter: An LLM Agent For Data Science}, \n      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},\n      year={2024},\n      eprint={2402.18679},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2402.18679}, \n}\n</code></pre>","contentLength":6789,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"docmost/docmost","url":"https://github.com/docmost/docmost","date":1740018405,"author":"","guid":6140,"unread":true,"content":"<p>Docmost is an open-source collaborative wiki and documentation software. It is an open-source alternative to Confluence and Notion.</p><blockquote><p>[!NOTE] Docmost is currently in . We value your feedback as we progress towards a stable release.</p></blockquote><ul><li>Diagrams (Draw.io, Excalidraw and Mermaid)</li></ul>","contentLength":270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"aidenybai/react-scan","url":"https://github.com/aidenybai/react-scan","date":1740018405,"author":"","guid":6141,"unread":true,"content":"<p>Scan for React performance issues and eliminate slow renders in your app</p><p>React Scan automatically detects performance issues in your React app.</p><p>React Scan attempts to solve these problems:</p><ul><li>It requires no code changes ‚Äì just drop it in</li><li>It highlights exactly the components you need to optimize</li><li>Use it via script tag, npm, CLI, you name it!</li></ul><p>Trusted by engineering teams at:</p><pre><code>&lt;!-- import this BEFORE any scripts --&gt;\n&lt;script src=\"https://unpkg.com/react-scan/dist/auto.global.js\"&gt;&lt;/script&gt;\n</code></pre><p>If you don't have a local version of the site or you want to test a React app remotely, you can use the CLI. This will spin up an isolated browser instance which you can interact or use React Scan with.</p><pre><code>npx react-scan@latest http://localhost:3000\n# you can technically scan ANY website on the web:\n# npx react-scan@latest https://react.dev\n</code></pre><p>You can add it to your existing dev process as well. Here's an example for Next.js:</p><pre><code>{\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"scan\": \"next dev &amp; npx react-scan@latest localhost:3000\"\n  }\n}\n</code></pre><p>If you want to install the extension, follow the guide <a href=\"https://github.com/aidenybai/react-scan/raw/main/BROWSER_EXTENSION_GUIDE.md\">here</a>.</p><ul><li>: Imperative API to start scanning</li><li><code>useScan(options: Options)</code>: Hook API to start scanning</li><li>: Get a report of all the renders</li><li><code>setOptions(options: Options): void</code>: Set options at runtime</li><li>: Get the current options</li><li><code>onRender(Component, onRender: (fiber: Fiber, render: Render) =&gt; void)</code>: Hook into a specific component's renders</li></ul><p>React can be tricky to optimize.</p><p>The issue is that component props are compared by reference, not value. This is intentional ‚Äì this way rendering can be cheap to run.</p><p>However, this makes it easy to accidentally cause unnecessary renders, making the app slow. Even in production apps, with hundreds of engineers, can't fully optimize their apps (see <a href=\"https://github.com/aidenybai/react-scan/raw/main/.github/assets/github.mp4\">GitHub</a>, <a href=\"https://github.com/aidenybai/react-scan/raw/main/.github/assets/twitter.mp4\">Twitter</a>, and <a href=\"https://github.com/aidenybai/react-scan/raw/main/.github/assets/instagram.mp4\">Instagram</a>).</p><p>This often comes down to props that update in reference, like callbacks or object values. For example, the  function and  object are re-created on every render, causing  to slow down the app:</p><pre><code>&lt;ExpensiveComponent onClick={() =&gt; alert(\"hi\")} style={{ color: \"purple\" }} /&gt;\n</code></pre><p>React Scan helps you identify these issues by automatically detecting and highlighting renders that cause performance issues. Now, instead of guessing, you can see exactly which components you need to fix.</p><p><strong>Q: Why this instead of React Devtools?</strong></p><p>React Devtools aims to be a general purpose tool for React. However, I deal with React performance issues every day, and React Devtools doesn't fix my problems well. There's a lot of noise (no obvious distinction between unnecessary and necessary renders), and there's no programmatic API. If it sounds like you have the same problems, then React Scan may be a better choice.</p><p>Also, some personal complaints about React Devtools' highlight feature:</p><ul><li>React Devtools \"batches\" paints, so if a component renders too fast, it will lag behind and only show 1 every second or so</li><li>When you scroll/resize the boxes don't update position</li><li>No count of how many renders there are</li><li>I don't know what the bad/slow renders are without inspecting</li><li>The menu is hidden away so it's annoying to turn on/off, user experience should be specifically tuned for debugging performance, instead of hidden behind a profiler/component tree</li><li>It's stuck in a chrome extension, I want to run it anywhere on the web</li><li>It looks subjectively ugly (lines look fuzzy, feels sluggish)</li><li>I'm more ambitious with react-scan</li></ul><h2>Resources &amp; Contributing Back</h2><p>Want to talk to the community? Hop in our <a href=\"https://discord.gg/X9yFbcV2rF\">Discord</a> and share your ideas and what you've build with React Scan.</p><p>Find a bug? Head over to our <a href=\"https://github.com/aidenybai/react-scan/issues\">issue tracker</a> and we'll do our best to help. We love pull requests, too!</p><p>React Scan takes inspiration from the following projects:</p>","contentLength":3639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"datawhalechina/llm-universe","url":"https://github.com/datawhalechina/llm-universe","date":1740018405,"author":"","guid":6142,"unread":true,"content":"<p>Êú¨È°πÁõÆÊòØ‰∏Ä‰∏™Èù¢ÂêëÂ∞èÁôΩÂºÄÂèëËÄÖÁöÑÂ§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÊïôÁ®ãÔºåÂú®Á∫øÈòÖËØªÂú∞ÂùÄÔºöhttps://datawhalechina.github.io/llm-universe/</p><p>Êú¨È°πÁõÆÊòØ‰∏Ä‰∏™Èù¢ÂêëÂ∞èÁôΩÂºÄÂèëËÄÖÁöÑÂ§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÊïôÁ®ãÔºåÊó®Âú®Âü∫‰∫éÈòøÈáå‰∫ëÊúçÂä°Âô®ÔºåÁªìÂêà‰∏™‰∫∫Áü•ËØÜÂ∫ìÂä©ÊâãÈ°πÁõÆÔºåÈÄöËøá‰∏Ä‰∏™ËØæÁ®ãÂÆåÊàêÂ§ßÊ®°ÂûãÂºÄÂèëÁöÑÈáçÁÇπÂÖ•Èó®Ôºå‰∏ªË¶ÅÂÜÖÂÆπÂåÖÊã¨Ôºö</p><ol><li>Ôºå‰Ωï‰∏∫Â§ßÊ®°Âûã„ÄÅÂ§ßÊ®°ÂûãÁâπÁÇπÊòØ‰ªÄ‰πà„ÄÅLangChain ÊòØ‰ªÄ‰πàÔºåÂ¶Ç‰ΩïÂºÄÂèë‰∏Ä‰∏™ LLM Â∫îÁî®ÔºåÈíàÂØπÂ∞èÁôΩÂºÄÂèëËÄÖÁöÑÁÆÄÂçï‰ªãÁªçÔºõ</li><li>ÔºåÊú¨ËäÇ‰ªãÁªç‰∫ÜÂõΩÂÜÖÂ§ñÁü•ÂêçÂ§ßÊ®°Âûã‰∫ßÂìÅ API ÁöÑÂ§öÁßçË∞ÉÁî®ÊñπÂºèÔºåÂåÖÊã¨Ë∞ÉÁî®ÂéüÁîü API„ÄÅÂ∞ÅË£Ö‰∏∫ LangChain LLM„ÄÅÂ∞ÅË£Ö‰∏∫ Fastapi Á≠âË∞ÉÁî®ÊñπÂºèÔºåÂêåÊó∂Â∞ÜÂåÖÊã¨ÁôæÂ∫¶ÊñáÂøÉ„ÄÅËÆØÈ£ûÊòüÁÅ´„ÄÅÊô∫Ë∞±AIÁ≠âÂ§öÁßçÂ§ßÊ®°Âûã API ËøõË°å‰∫ÜÁªü‰∏ÄÂΩ¢ÂºèÂ∞ÅË£ÖÔºõ</li><li>Ôºå‰∏çÂêåÁ±ªÂûãÁü•ËØÜÂ∫ìÊñáÊ°£ÁöÑÂä†ËΩΩ„ÄÅÂ§ÑÁêÜÔºåÂêëÈáèÊï∞ÊçÆÂ∫ìÁöÑÊê≠Âª∫Ôºõ</li><li>ÔºåÂåÖÊã¨Â∞Ü LLM Êé•ÂÖ•Âà∞ LangChain ÊûÑÂª∫Ê£ÄÁ¥¢ÈóÆÁ≠îÈìæÔºå‰ΩøÁî® Streamlit ËøõË°åÂ∫îÁî®ÈÉ®ÁΩ≤</li><li>ÔºåÂ§ßÊ®°ÂûãÂºÄÂèëÂ¶Ç‰ΩïÂÆûÁé∞È™åËØÅËø≠‰ª£Ôºå‰∏ÄËà¨ÁöÑËØÑ‰º∞ÊñπÊ≥ïÊúâ‰ªÄ‰πàÔºõ</li></ol><ol><li>„ÄÇV1 ÁâàÊú¨ÁöÑÁÆÄÂåñÁâàÔºåÊó®Âú®Â∏ÆÂä©ÂàùÂ≠¶ËÄÖÊúÄÂø´„ÄÅÊúÄ‰æøÊç∑Âú∞ÂÖ•Èó® LLM ÂºÄÂèëÔºåÁêÜËß£ LLM ÂºÄÂèëÁöÑ‰∏ÄËà¨ÊµÅÁ®ãÔºåÂèØ‰ª•Êê≠Âª∫Âá∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑ Demo„ÄÇ</li><li>„ÄÇLLM ÂºÄÂèëÊõ¥ËøõÈò∂ÁöÑÊäÄÂ∑ßÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºöPrompt Engineering„ÄÅÂ§öÁ±ªÂûãÊ∫êÊï∞ÊçÆÁöÑÂ§ÑÁêÜ„ÄÅ‰ºòÂåñÊ£ÄÁ¥¢„ÄÅÂè¨ÂõûÁ≤æÊéí„ÄÅAgent Ê°ÜÊû∂Á≠â</li><li>„ÄÇÂºïÂÖ•‰∏Ä‰∫õÊàêÂäüÁöÑÂºÄÊ∫êÊ°à‰æãÔºå‰ªéÊú¨ËØæÁ®ãÁöÑËßíÂ∫¶Âá∫ÂèëÔºåËß£ÊûêËøô‰∫õÂ∫îÁî®ËåÉ‰æãÁöÑ Idea„ÄÅÊ†∏ÂøÉÊÄùË∑Ø„ÄÅÂÆûÁé∞Ê°ÜÊû∂ÔºåÂ∏ÆÂä©ÂàùÂ≠¶ËÄÖÊòéÁôΩÂÖ∂ÂèØ‰ª•ÈÄöËøá LLM ÂºÄÂèë‰ªÄ‰πàÊ†∑ÁöÑÂ∫îÁî®„ÄÇ</li></ol><p>ÁõÆÂâçÔºåÁ¨¨‰∏ÄÈÉ®ÂàÜÂ∑≤ÁªèÂÆåÁ®øÔºåÊ¨¢ËøéÂ§ßÂÆ∂ÈòÖËØªÂ≠¶‰π†ÔºõÁ¨¨‰∫å„ÄÅ‰∏âÈÉ®ÂàÜÊ≠£Âú®Âàõ‰Ωú‰∏≠„ÄÇ</p><pre><code>  requirements.txtÔºöÂÆòÊñπÁéØÂ¢É‰∏ãÁöÑÂÆâË£Ö‰æùËµñ\n  notebookÔºöNotebook Ê∫ê‰ª£Á†ÅÊñá‰ª∂\n  docsÔºöMarkdown ÊñáÊ°£Êñá‰ª∂\n  figuresÔºöÂõæÁâá\n  data_baseÔºöÊâÄ‰ΩøÁî®ÁöÑÁü•ËØÜÂ∫ìÊ∫êÊñá‰ª∂\n</code></pre><p>LLM Ê≠£ÈÄêÊ≠•Êàê‰∏∫‰ø°ÊÅØ‰∏ñÁïåÁöÑÊñ∞Èù©ÂëΩÂäõÈáèÔºåÂÖ∂ÈÄöËøáÂº∫Â§ßÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêËÉΩÂäõÔºå‰∏∫ÂºÄÂèëËÄÖÊèê‰æõ‰∫ÜÊñ∞ÁöÑ„ÄÅÊõ¥Âº∫Â§ßÁöÑÂ∫îÁî®ÂºÄÂèëÈÄâÊã©„ÄÇÈöèÁùÄÂõΩÂÜÖÂ§ñ‰∫ïÂñ∑ÂºèÁöÑ LLM API ÊúçÂä°ÂºÄÊîæÔºåÂ¶Ç‰ΩïÂü∫‰∫é LLM API Âø´ÈÄü„ÄÅ‰æøÊç∑Âú∞ÂºÄÂèëÂÖ∑Â§áÊõ¥Âº∫ËÉΩÂäõ„ÄÅÈõÜÊàê LLM ÁöÑÂ∫îÁî®ÔºåÂºÄÂßãÊàê‰∏∫ÂºÄÂèëËÄÖÁöÑ‰∏ÄÈ°πÈáçË¶ÅÊäÄËÉΩ„ÄÇ</p><p>ÁõÆÂâçÔºåÂÖ≥‰∫é LLM ÁöÑ‰ªãÁªç‰ª•ÂèäÈõ∂Êï£ÁöÑ LLM ÂºÄÂèëÊäÄËÉΩËØæÁ®ãÂ∑≤Êúâ‰∏çÂ∞ëÔºå‰ΩÜË¥®ÈáèÂèÇÂ∑Æ‰∏çÈΩêÔºå‰∏îÊ≤°ÊúâÂæàÂ•ΩÂú∞Êï¥ÂêàÔºåÂºÄÂèëËÄÖÈúÄË¶ÅÊêúÁ¥¢Â§ßÈáèÊïôÁ®ãÂπ∂ÈòÖËØªÂ§ßÈáèÁõ∏ÂÖ≥ÊÄß‰∏çÂº∫„ÄÅÂøÖË¶ÅÊÄßËæÉ‰ΩéÁöÑÂÜÖÂÆπÔºåÊâçËÉΩÂàùÊ≠•ÊéåÊè°Â§ßÊ®°ÂûãÂºÄÂèëÁöÑÂøÖÂ§áÊäÄËÉΩÔºåÂ≠¶‰π†ÊïàÁéá‰ΩéÔºåÂ≠¶‰π†Èó®Êßõ‰πüËæÉÈ´ò„ÄÇ</p><p>Êú¨È°πÁõÆ‰ªéÂÆûË∑µÂá∫ÂèëÔºåÁªìÂêàÊúÄÂ∏∏ËßÅ„ÄÅÈÄöÁî®ÁöÑ‰∏™‰∫∫Áü•ËØÜÂ∫ìÂä©ÊâãÈ°πÁõÆÔºåÊ∑±ÂÖ•ÊµÖÂá∫ÈÄêÊ≠•ÊãÜËß£ LLM ÂºÄÂèëÁöÑ‰∏ÄËà¨ÊµÅÁ®ã„ÄÅÊ≠•È™§ÔºåÊó®Âú®Â∏ÆÂä©Ê≤°ÊúâÁÆóÊ≥ïÂü∫Á°ÄÁöÑÂ∞èÁôΩÈÄöËøá‰∏Ä‰∏™ËØæÁ®ãÂÆåÊàêÂ§ßÊ®°ÂûãÂºÄÂèëÁöÑÂü∫Á°ÄÂÖ•Èó®„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨‰πü‰ºöÂä†ÂÖ• RAG ÂºÄÂèëÁöÑËøõÈò∂ÊäÄÂ∑ß‰ª•Âèä‰∏Ä‰∫õÊàêÂäüÁöÑ LLM Â∫îÁî®Ê°à‰æãÁöÑËß£ËØªÔºåÂ∏ÆÂä©ÂÆåÊàêÁ¨¨‰∏ÄÈÉ®ÂàÜÂ≠¶‰π†ÁöÑËØªËÄÖËøõ‰∏ÄÊ≠•ÊéåÊè°Êõ¥È´òÈò∂ÁöÑ RAG ÂºÄÂèëÊäÄÂ∑ßÔºåÂπ∂ËÉΩÂ§üÈÄöËøáÂØπÂ∑≤ÊúâÊàêÂäüÈ°πÁõÆÁöÑÂÄüÈâ¥ÂºÄÂèëËá™Â∑±ÁöÑ„ÄÅÂ•ΩÁé©ÁöÑÂ∫îÁî®„ÄÇ</p><p>ÊâÄÊúâÂÖ∑Â§áÂü∫Á°Ä Python ËÉΩÂäõÔºåÊÉ≥Ë¶ÅÊéåÊè° LLM Â∫îÁî®ÂºÄÂèëÊäÄËÉΩÁöÑÂºÄÂèëËÄÖ„ÄÇ</p><p><strong>Êú¨È°πÁõÆÂØπÂ≠¶‰π†ËÄÖÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂü∫Á°Ä„ÄÅÁÆóÊ≥ïÂü∫Á°ÄÊ≤°Êúâ‰ªª‰ΩïË¶ÅÊ±ÇÔºå‰ªÖÈúÄË¶ÅÊéåÊè°Âü∫Êú¨ Python ËØ≠Ê≥ï„ÄÅÊéåÊè°ÂàùÁ∫ß Python ÂºÄÂèëÊäÄËÉΩÂç≥ÂèØ„ÄÇ</strong></p><p>ËÄÉËôëÂà∞ÁéØÂ¢ÉÊê≠Âª∫ÈóÆÈ¢òÔºåÊú¨È°πÁõÆÊèê‰æõ‰∫ÜÈòøÈáå‰∫ëÊúçÂä°Âô®Â≠¶ÁîüÂÖçË¥πÈ¢ÜÂèñÊñπÂºèÔºåÂ≠¶ÁîüËØªËÄÖÂèØ‰ª•ÂÖçË¥πÈ¢ÜÂèñÈòøÈáå‰∫ëÊúçÂä°Âô®ÔºåÂπ∂ÈÄöËøáÈòøÈáå‰∫ëÊúçÂä°Âô®ÂÆåÊàêÊú¨ËØæÁ®ãÁöÑÂ≠¶‰π†ÔºõÊú¨È°πÁõÆÂêåÊó∂‰πüÊèê‰æõ‰∫Ü‰∏™‰∫∫ÁîµËÑëÂèäÈùûÈòøÈáå‰∫ëÊúçÂä°Âô®ÁöÑÁéØÂ¢ÉÊê≠Âª∫ÊåáÂçóÔºõÊú¨È°πÁõÆÂØπÊú¨Âú∞Á°¨‰ª∂Âü∫Êú¨Ê≤°ÊúâË¶ÅÊ±ÇÔºå‰∏çÈúÄË¶Å GPU ÁéØÂ¢ÉÔºå‰∏™‰∫∫ÁîµËÑëÂèäÊúçÂä°Âô®ÂùáÂèØÁî®‰∫éÂ≠¶‰π†„ÄÇ</p><p><strong>Ê≥®ÔºöÊú¨È°πÁõÆ‰∏ªË¶Å‰ΩøÁî®ÂêÑÂ§ßÊ®°ÂûãÂéÇÂïÜÊèê‰æõÁöÑ API Êù•ËøõË°åÂ∫îÁî®ÂºÄÂèëÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÂ≠¶‰π†ÈÉ®ÁΩ≤Â∫îÁî®Êú¨Âú∞ÂºÄÊ∫ê LLMÔºåÊ¨¢ËøéÂ≠¶‰π†ÂêåÊ†∑Áî± Datawhale Âá∫ÂìÅÁöÑ <a href=\"https://github.com/datawhalechina/self-llm\">Self LLM ÔΩú ÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó</a>ÔºåËØ•È°πÁõÆÂ∞ÜÊâãÊääÊâãÊïô‰Ω†Â¶Ç‰ΩïÈÄüÈÄöÂºÄÊ∫ê LLM ÈÉ®ÁΩ≤ÂæÆË∞ÉÂÖ®ÈìæË∑ØÔºÅ</strong></p><p><strong>Ê≥®ÔºöËÄÉËôëÂà∞Â≠¶‰π†ÈöæÂ∫¶ÔºåÊú¨È°πÁõÆ‰∏ªË¶ÅÈù¢ÂêëÂàùÂ≠¶ËÄÖÔºå‰ªãÁªçÂ¶Ç‰Ωï‰ΩøÁî® LLM Êù•Êê≠Âª∫Â∫îÁî®„ÄÇÂ¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅËøõ‰∏ÄÊ≠•Ê∑±ÂÖ•Â≠¶‰π† LLM ÁöÑÁêÜËÆ∫Âü∫Á°ÄÔºåÂπ∂Âú®ÁêÜËÆ∫ÁöÑÂü∫Á°Ä‰∏äËøõ‰∏ÄÊ≠•ËÆ§ËØÜ„ÄÅÂ∫îÁî® LLMÔºåÊ¨¢ËøéÂ≠¶‰π†ÂêåÊ†∑Áî± Datawhale Âá∫ÂìÅÁöÑ <a href=\"https://github.com/datawhalechina/so-large-lm\">So Large LM | Â§ßÊ®°ÂûãÂü∫Á°Ä</a>ÔºåËØ•È°πÁõÆÂ∞Ü‰∏∫‰Ω†Êèê‰æõÂÖ®Èù¢ËÄåÊ∑±ÂÖ•ÁöÑ LLM ÁêÜËÆ∫Áü•ËØÜÂèäÂÆûË∑µÊñπÊ≥ïÔºÅ</strong></p><ol><li><p>ÂÖÖÂàÜÈù¢ÂêëÂÆûË∑µÔºåÂä®ÊâãÂ≠¶‰π†Â§ßÊ®°ÂûãÂºÄÂèë„ÄÇÁõ∏ËæÉ‰∫éÂÖ∂‰ªñ‰ªéÁêÜËÆ∫ÂÖ•Êâã„ÄÅ‰∏éÂÆûË∑µ‰ª£Â∑ÆËæÉÂ§ßÁöÑÁ±ª‰ººÊïôÁ®ãÔºåÊú¨ÊïôÁ®ãÂü∫‰∫éÂÖ∑ÊúâÈÄöÁî®ÊÄßÁöÑ‰∏™‰∫∫Áü•ËØÜÂ∫ìÂä©ÊâãÈ°πÁõÆÊâìÈÄ†ÔºåÂ∞ÜÊôÆÈÄÇÁöÑÂ§ßÊ®°ÂûãÂºÄÂèëÁêÜÂøµËûçÂêàÂú®È°πÁõÆÂÆûË∑µ‰∏≠ÔºåÂ∏ÆÂä©Â≠¶‰π†ËÄÖÈÄöËøáÂä®ÊâãÊê≠Âª∫‰∏™‰∫∫È°πÁõÆÊù•ÊéåÊè°Â§ßÊ®°ÂûãÂºÄÂèëÊäÄËÉΩ„ÄÇ</p></li><li><p>‰ªéÈõ∂ÂºÄÂßãÔºåÂÖ®Èù¢ÂèàÁÆÄÁü≠ÁöÑÂ§ßÊ®°ÂûãÊïôÁ®ã„ÄÇÊú¨È°πÁõÆÈíàÂØπ‰∏™‰∫∫Áü•ËØÜÂ∫ìÂä©ÊâãÈ°πÁõÆÔºåÂØπÁõ∏ÂÖ≥Â§ßÊ®°ÂûãÂºÄÂèëÁêÜËÆ∫„ÄÅÊ¶ÇÂøµÂíåÂü∫Êú¨ÊäÄËÉΩËøõË°å‰∫ÜÈ°πÁõÆ‰∏ªÂØºÁöÑÈáçÊûÑÔºåÂà†Âéª‰∏çÈúÄË¶ÅÁêÜËß£ÁöÑÂ∫ïÂ±ÇÂéüÁêÜÂíåÁÆóÊ≥ïÁªÜËäÇÔºåÊ∂µÁõñÊâÄÊúâÂ§ßÊ®°ÂûãÂºÄÂèëÁöÑÊ†∏ÂøÉÊäÄËÉΩ„ÄÇÊïôÁ®ãÊï¥‰ΩìÊó∂ÈïøÂú®Êï∞Â∞èÊó∂‰πãÂÜÖÔºå‰ΩÜÂ≠¶‰π†ÂÆåÊú¨ÊïôÁ®ãÔºåÂèØ‰ª•ÊéåÊè°Âü∫Á°ÄÂ§ßÊ®°ÂûãÂºÄÂèëÁöÑÊâÄÊúâÊ†∏ÂøÉÊäÄËÉΩ„ÄÇ</p></li><li><p>ÂÖºÂÖ∑Áªü‰∏ÄÊÄß‰∏éÊãìÂ±ïÊÄß„ÄÇÊú¨È°πÁõÆÂØπ GPT„ÄÅÁôæÂ∫¶ÊñáÂøÉ„ÄÅËÆØÈ£ûÊòüÁÅ´„ÄÅÊô∫Ë∞±GLM Á≠âÂõΩÂÜÖÂ§ñ‰∏ªË¶Å LLM API ËøõË°å‰∫ÜÁªü‰∏ÄÂ∞ÅË£ÖÔºåÊîØÊåÅ‰∏ÄÈîÆË∞ÉÁî®‰∏çÂêåÁöÑ LLMÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÂ∞ÜÊõ¥Â§öÁöÑÁ≤æÂäõÊîæÂú®Â≠¶‰π†Â∫îÁî®‰∏éÊ®°ÂûãÊú¨Ë∫´ÁöÑ‰ºòÂåñ‰∏äÔºåËÄå‰∏çÈúÄË¶ÅËä±Êó∂Èó¥Âú®ÁπÅÁêêÁöÑË∞ÉÁî®ÁªÜËäÇ‰∏äÔºõÂêåÊó∂ÔºåÊú¨ÊïôÁ®ãÊãü‰∏äÁ∫ø <a href=\"https://1aigc.cn/\">Â•áÊÉ≥ÊòüÁêÉ | AIGCÂÖ±ÂàõÁ§æÂå∫Âπ≥Âè∞</a>ÔºåÊîØÊåÅÂ≠¶‰π†ËÄÖËá™ÂÆö‰πâÈ°πÁõÆ‰∏∫Êú¨ÊïôÁ®ãÂ¢ûÂä†ÊãìÂ±ïÂÜÖÂÆπÔºåÂÖ∑Â§áÂÖÖÂàÜÁöÑÊãìÂ±ïÊÄß„ÄÇ</p></li></ol><ol><li>Êï∞ÊçÆÂ§ÑÁêÜ \n  <ol></ol></li><li>Ê£ÄÁ¥¢Èò∂ÊÆµ \n  <ol></ol></li></ol><ol><li>ChatWithDatawhale‚Äî‚Äî‰∏™‰∫∫Áü•ËØÜÂ∫ìÂä©ÊâãËß£ËØª</li></ol><a href=\"https://github.com/datawhalechina/llm-universe/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=datawhalechina/llm-universe\"></a>","contentLength":5598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"golang-standards/project-layout","url":"https://github.com/golang-standards/project-layout","date":1740018405,"author":"","guid":6143,"unread":true,"content":"<p>Standard Go Project Layout</p><p>This is a basic layout for Go application projects. Note that it's basic in terms of content because it's focusing only on the general layout and not what you have inside. It's also basic because it's very high level and it doesn't go into great details in terms of how you can structure your project even further. For example, it doesn't try to cover the project structure you'd have with something like Clean Architecture.</p><p>This is <strong><code>NOT an official standard defined by the core Go dev team</code></strong>. This is a set of common historical and emerging project layout patterns in the Go ecosystem. Some of these patterns are more popular than others. It also has a number of small enhancements along with several supporting directories common to any large enough real world application. Note that the <strong>core Go team provides a great set of general guidelines about structuring Go projects</strong> and what it means for your project when it's imported and when it's installed. See the <a href=\"https://go.dev/doc/modules/layout\"></a> page in the official Go docs for more details. It includes the  and  directory patterns (described below) and other useful information.</p><p><strong><code>If you are trying to learn Go or if you are building a PoC or a simple project for yourself this project layout is an overkill. Start with something really simple instead (a single </code>main.gogo.mod</strong> As your project grows keep in mind that it'll be important to make sure your code is well structured otherwise you'll end up with a messy code with lots of hidden dependencies and global state. When you have more people working on the project you'll need even more structure. That's when it's important to introduce a common way to manage packages/libraries. When you have an open source project or when you know other projects import the code from your project repository that's when it's important to have private (aka ) packages and code. Clone the repository, keep what you need and delete everything else! Just because it's there it doesn't mean you have to use it all. None of these patterns are used in every single project. Even the  pattern is not universal.</p><p>With Go 1.14 <a href=\"https://go.dev/wiki/Modules\"></a> are finally ready for production. Use <a href=\"https://blog.golang.org/using-go-modules\"></a> unless you have a specific reason not to use them and if you do then you don‚Äôt need to worry about $GOPATH and where you put your project. The basic  file in the repo assumes your project is hosted on GitHub, but it's not a requirement. The module path can be anything though the first module path component should have a dot in its name (the current version of Go doesn't enforce it anymore, but if you are using slightly older versions don't be surprised if your builds fail without it). See Issues <a href=\"https://github.com/golang/go/issues/37554\"></a> and <a href=\"https://github.com/golang/go/issues/32819\"></a> if you want to know more about it.</p><p>This project layout is intentionally generic and it doesn't try to impose a specific Go package structure.</p><p>This is a community effort. Open an issue if you see a new pattern or if you think one of the existing patterns needs to be updated.</p><p>If you need help with naming, formatting and style start by running <a href=\"https://golang.org/cmd/gofmt/\"></a> and <a href=\"https://github.com/dominikh/go-tools/tree/master/cmd/staticcheck\"></a>. The previous standard linter, golint, is now deprecated and not maintained; use of a maintained linter such as staticcheck is recommended. Also make sure to read these Go code style guidelines and recommendations:</p><p>More about naming and organizing packages as well as other code structure recommendations:</p><p>A Chinese post about Package-Oriented-Design guidelines and Architecture layer</p><p>Main applications for this project.</p><p>The directory name for each application should match the name of the executable you want to have (e.g., ).</p><p>Don't put a lot of code in the application directory. If you think the code can be imported and used in other projects, then it should live in the  directory. If the code is not reusable or if you don't want others to reuse it, put that code in the  directory. You'll be surprised what others will do, so be explicit about your intentions!</p><p>It's common to have a small  function that imports and invokes the code from the  and  directories and nothing else.</p><p>See the <a href=\"https://raw.githubusercontent.com/golang-standards/project-layout/master/cmd/README.md\"></a> directory for examples.</p><p>Private application and library code. This is the code you don't want others importing in their applications or libraries. Note that this layout pattern is enforced by the Go compiler itself. See the Go 1.4 <a href=\"https://golang.org/doc/go1.4#internalpackages\"></a> for more details. Note that you are not limited to the top level  directory. You can have more than one  directory at any level of your project tree.</p><p>You can optionally add a bit of extra structure to your internal packages to separate your shared and non-shared internal code. It's not required (especially for smaller projects), but it's nice to have visual clues showing the intended package use. Your actual application code can go in the  directory (e.g., ) and the code shared by those apps in the  directory (e.g., ).</p><p>You use internal directories to make packages private. If you put a package inside an internal directory, then other packages can‚Äôt import it unless they share a common ancestor. And it‚Äôs the only directory named in Go‚Äôs documentation and has special compiler treatment.</p><p>Library code that's ok to use by external applications (e.g., ). Other projects will import these libraries expecting them to work, so think twice before you put something here :-) Note that the  directory is a better way to ensure your private packages are not importable because it's enforced by Go. The  directory is still a good way to explicitly communicate that the code in that directory is safe for use by others. The <a href=\"https://travisjeffery.com/b/2019/11/i-ll-take-pkg-over-internal/\"><code>I'll take pkg over internal</code></a> blog post by Travis Jeffery provides a good overview of the  and  directories and when it might make sense to use them.</p><p>See the <a href=\"https://raw.githubusercontent.com/golang-standards/project-layout/master/pkg/README.md\"></a> directory if you want to see which popular Go repos use this project layout pattern. This is a common layout pattern, but it's not universally accepted and some in the Go community don't recommend it.</p><p>It's ok not to use it if your app project is really small and where an extra level of nesting doesn't add much value (unless you really want to :-)). Think about it when it's getting big enough and your root directory gets pretty busy (especially if you have a lot of non-Go app components).</p><p>The  directory origins: The old Go source code used to use  for its packages and then various Go projects in the community started copying the pattern (see <a href=\"https://twitter.com/bradfitz/status/1039512487538970624\"></a> Brad Fitzpatrick's tweet for more context).</p><p>Application dependencies (managed manually or by your favorite dependency management tool like the new built-in <a href=\"https://go.dev/wiki/Modules\"></a> feature). The  command will create the  directory for you. Note that you might need to add the  flag to your  command if you are not using Go 1.14 where it's on by default.</p><p>Don't commit your application dependencies if you are building a library.</p><p>Note that since <a href=\"https://golang.org/doc/go1.13#modules\"></a> Go also enabled the module proxy feature (using <a href=\"https://proxy.golang.org\"></a> as their module proxy server by default). Read more about it <a href=\"https://blog.golang.org/module-mirror-launch\"></a> to see if it fits all of your requirements and constraints. If it does, then you won't need the  directory at all.</p><h2>Service Application Directories</h2><p>OpenAPI/Swagger specs, JSON schema files, protocol definition files.</p><p>See the <a href=\"https://raw.githubusercontent.com/golang-standards/project-layout/master/api/README.md\"></a> directory for examples.</p><h2>Web Application Directories</h2><p>Web application specific components: static web assets, server side templates and SPAs.</p><h2>Common Application Directories</h2><p>Configuration file templates or default configs.</p><p>Put your  or  template files here.</p><p>System init (systemd, upstart, sysv) and process manager/supervisor (runit, supervisord) configs.</p><p>Scripts to perform various build, install, analysis, etc operations.</p><p>Packaging and Continuous Integration.</p><p>Put your cloud (AMI), container (Docker), OS (deb, rpm, pkg) package configurations and scripts in the  directory.</p><p>Put your CI (travis, circle, drone) configurations and scripts in the  directory. Note that some of the CI tools (e.g., Travis CI) are very picky about the location of their config files. Try putting the config files in the  directory linking them to the location where the CI tools expect them (when possible).</p><p>IaaS, PaaS, system and container orchestration deployment configurations and templates (docker-compose, kubernetes/helm, terraform). Note that in some repos (especially apps deployed with kubernetes) this directory is called .</p><p>Additional external test apps and test data. Feel free to structure the  directory anyway you want. For bigger projects it makes sense to have a data subdirectory. For example, you can have  or  if you need Go to ignore what's in that directory. Note that Go will also ignore directories or files that begin with \".\" or \"_\", so you have more flexibility in terms of how you name your test data directory.</p><p>See the <a href=\"https://raw.githubusercontent.com/golang-standards/project-layout/master/test/README.md\"></a> directory for examples.</p><p>Design and user documents (in addition to your godoc generated documentation).</p><p>See the <a href=\"https://raw.githubusercontent.com/golang-standards/project-layout/master/docs/README.md\"></a> directory for examples.</p><p>Supporting tools for this project. Note that these tools can import code from the  and  directories.</p><p>See the <a href=\"https://raw.githubusercontent.com/golang-standards/project-layout/master/tools/README.md\"></a> directory for examples.</p><p>Examples for your applications and/or public libraries.</p><p>External helper tools, forked code and other 3rd party utilities (e.g., Swagger UI).</p><p>Other assets to go along with your repository (images, logos, etc).</p><p>This is the place to put your project's website data if you are not using GitHub pages.</p><h2>Directories You Shouldn't Have</h2><p>Some Go projects do have a  folder, but it usually happens when the devs came from the Java world where it's a common pattern. If you can help yourself try not to adopt this Java pattern. You really don't want your Go code or Go projects to look like Java :-)</p><p>Don't confuse the project level  directory with the  directory Go uses for its workspaces as described in <a href=\"https://golang.org/doc/code.html\"></a>. The  environment variable points to your (current) workspace (by default it points to  on non-windows systems). This workspace includes the top level ,  and  directories. Your actual project ends up being a sub-directory under , so if you have the  directory in your project the project path will look like this: <code>/some/path/to/workspace/src/your_project/src/your_code.go</code>. Note that with Go 1.11 it's possible to have your project outside of your , but it still doesn't mean it's a good idea to use this layout pattern.</p><ul><li><p><a href=\"https://goreportcard.com/\">Go Report Card</a> - It will scan your code with , , , , ,  and . Replace <code>github.com/golang-standards/project-layout</code> with your project reference.</p></li><li><p>Release - It will show the latest release number for your project. Change the github link to point to your project.</p></li></ul><p>A more opinionated project template with sample/reusable configs, scripts and code is a WIP.</p>","contentLength":10325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"jingyaogong/minimind","url":"https://github.com/jingyaogong/minimind","date":1739931998,"author":"","guid":4527,"unread":true,"content":"<p>üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!</p><ul><li>Ê≠§ÂºÄÊ∫êÈ°πÁõÆÊó®Âú®ÂÆåÂÖ®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®3ÂùóÈí±ÊàêÊú¨ + 2Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫‰ªÖ‰∏∫25.8MÁöÑË∂ÖÂ∞èËØ≠Ë®ÄÊ®°Âûã„ÄÇ</li><li>Á≥ªÂàóÊûÅÂÖ∂ËΩªÈáèÔºåÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØÊòØ GPT-3 ÁöÑ $\\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞ÊúÄÊôÆÈÄöÁöÑ‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüËÆ≠ÁªÉ„ÄÇ</li><li>È°πÁõÆÂêåÊó∂ÂºÄÊ∫ê‰∫ÜÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ-ÂåÖÂê´ÊãìÂ±ïÂÖ±‰∫´Ê∑∑Âêà‰∏ìÂÆ∂(MoE)„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)„ÄÅLoRAÂæÆË∞ÉÔºå Áõ¥Êé•ÂÅèÂ•ΩÂº∫ÂåñÂ≠¶‰π†(DPO)ÁÆóÊ≥ï„ÄÅÊ®°ÂûãËí∏È¶èÁÆóÊ≥ïÁ≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ</li><li>È°πÁõÆÊâÄÊúâÊ†∏ÂøÉÁÆóÊ≥ï‰ª£Á†ÅÂùá‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÈáçÊûÑÔºÅ‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÊèê‰æõÁöÑÊäΩË±°Êé•Âè£„ÄÇ</li><li>Ëøô‰∏ç‰ªÖÊòØÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®Èò∂ÊÆµÂºÄÊ∫êÂ§çÁé∞Ôºå‰πüÊòØ‰∏Ä‰∏™ÂÖ•Èó®LLMÁöÑÊïôÁ®ã„ÄÇ</li><li>Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ</li></ul><blockquote><p>‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú2Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØïÔºå‚Äú3ÂùóÈí±‚Äù ÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨ÔºåÂÖ∑‰ΩìËßÑÊ†ºËØ¶ÊÉÖËßÅ‰∏ãÊñá„ÄÇ</p></blockquote><p>Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language Model, LLMÔºâÁöÑÂá∫Áé∞ÂºïÂèë‰∫ÜÂÖ®‰∏ñÁïåÂØπAIÁöÑÁ©∫ÂâçÂÖ≥Ê≥®„ÄÇ Êó†ËÆ∫ÊòØChatGPT„ÄÅDeepSeekËøòÊòØQwenÔºåÈÉΩ‰ª•ÂÖ∂ÊÉäËâ≥ÁöÑÊïàÊûú‰ª§‰∫∫Âèπ‰∏∫ËßÇÊ≠¢„ÄÇ ÁÑ∂ËÄåÔºåÂä®ËæÑÊï∞Áôæ‰∫øÂèÇÊï∞ÁöÑÂ∫ûÂ§ßËßÑÊ®°Ôºå‰ΩøÂæóÂÆÉ‰ª¨ÂØπ‰∏™‰∫∫ËÆæÂ§áËÄåË®Ä‰∏ç‰ªÖÈöæ‰ª•ËÆ≠ÁªÉÔºåÁîöËá≥ËøûÈÉ®ÁΩ≤ÈÉΩÊòæÂæóÈÅ•‰∏çÂèØÂèä„ÄÇ ÊâìÂºÄÂ§ßÊ®°ÂûãÁöÑ‚ÄúÈªëÁõíÂ≠ê‚ÄùÔºåÊé¢Á¥¢ÂÖ∂ÂÜÖÈÉ®Ëøê‰ΩúÊú∫Âà∂ÔºåÂ§ö‰πà‰ª§‰∫∫ÂøÉÊΩÆÊæéÊπÉÔºÅ ÈÅóÊÜæÁöÑÊòØÔºå99%ÁöÑÊé¢Á¥¢Âè™ËÉΩÊ≠¢Ê≠•‰∫é‰ΩøÁî®LoRAÁ≠âÊäÄÊúØÂØπÁé∞ÊúâÂ§ßÊ®°ÂûãËøõË°åÂ∞ëÈáèÂæÆË∞ÉÔºåÂ≠¶‰π†‰∏Ä‰∫õÊñ∞Êåá‰ª§Êàñ‰ªªÂä°„ÄÇ ËøôÂ∞±Â•ΩÊØîÊïôÁâõÈ°øÂ¶Ç‰Ωï‰ΩøÁî®21‰∏ñÁ∫™ÁöÑÊô∫ËÉΩÊâãÊú∫‚Äî‚ÄîËôΩÁÑ∂ÊúâË∂£ÔºåÂç¥ÂÆåÂÖ®ÂÅèÁ¶ª‰∫ÜÁêÜËß£Áâ©ÁêÜÊú¨Ë¥®ÁöÑÂàùË°∑„ÄÇ ‰∏éÊ≠§ÂêåÊó∂ÔºåÁ¨¨‰∏âÊñπÁöÑÂ§ßÊ®°ÂûãÊ°ÜÊû∂ÂíåÂ∑•ÂÖ∑Â∫ìÔºåÂ¶Çtransformers+trlÔºåÂá†‰πéÂè™Êö¥Èú≤‰∫ÜÈ´òÂ∫¶ÊäΩË±°ÁöÑÊé•Âè£„ÄÇ ÈÄöËøáÁü≠Áü≠10Ë°å‰ª£Á†ÅÔºåÂ∞±ËÉΩÂÆåÊàê‚ÄúÂä†ËΩΩÊ®°Âûã+Âä†ËΩΩÊï∞ÊçÆÈõÜ+Êé®ÁêÜ+Âº∫ÂåñÂ≠¶‰π†‚ÄùÁöÑÂÖ®ÊµÅÁ®ãËÆ≠ÁªÉ„ÄÇ ËøôÁßçÈ´òÊïàÁöÑÂ∞ÅË£ÖÂõ∫ÁÑ∂‰æøÂà©Ôºå‰ΩÜ‰πüÂÉè‰∏ÄÊû∂È´òÈÄüÈ£ûËàπÔºåÂ∞ÜÊàë‰ª¨‰∏éÂ∫ïÂ±ÇÂÆûÁé∞ÈöîÁ¶ªÂºÄÊù•ÔºåÈòªÁ¢ç‰∫ÜÊ∑±ÂÖ•Êé¢Á©∂LLMÊ†∏ÂøÉ‰ª£Á†ÅÁöÑÊú∫‰ºö„ÄÇ ÁÑ∂ËÄåÔºå‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù„ÄÇ Êõ¥Á≥üÁ≥ïÁöÑÊòØÔºå‰∫íËÅîÁΩë‰∏äÂÖÖÊñ•ÁùÄÂ§ßÈáè‰ªòË¥πËØæÁ®ãÂíåËê•ÈîÄÂè∑Ôºå‰ª•ÊºèÊ¥ûÁôæÂá∫„ÄÅ‰∏ÄÁü•ÂçäËß£ÁöÑÂÜÖÂÆπÊé®ÈîÄAIÊïôÁ®ã„ÄÇ Ê≠£Âõ†Â¶ÇÊ≠§ÔºåÊú¨È°πÁõÆÂàùË°∑ÊòØÊãâ‰ΩéLLMÁöÑÂ≠¶‰π†Èó®ÊßõÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩ‰ªéÁêÜËß£ÊØè‰∏ÄË°å‰ª£Á†ÅÂºÄÂßãÔºå ‰ªéÈõ∂ÂºÄÂßã‰∫≤ÊâãËÆ≠ÁªÉ‰∏Ä‰∏™ÊûÅÂ∞èÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÊòØÁöÑÔºå‰ªéÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖËøõË°åÔºÅ ÊúÄ‰ΩéÂè™ÈúÄ3ÂùóÈí±‰∏çÂà∞ÁöÑÊúçÂä°Âô®ÊàêÊú¨ÔºåÂ∞±ËÉΩ‰∫≤Ë∫´‰ΩìÈ™å‰ªé0Âà∞1ÊûÑÂª∫‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®ËøáÁ®ã„ÄÇ ‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ</p><blockquote><p>[!NOTE] ÔºàÊà™Ëá≥2025-02-07ÔºâMiniMindÁ≥ªÂàóÂ∑≤ÂÆåÊàêÂ§ö‰∏™ÂûãÂè∑Ê®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ25.8MÔºà0.02BÔºâÔºåÂç≥ÂèØÂÖ∑Â§áÊµÅÁïÖÂØπËØùËÉΩÂäõÔºÅ</p></blockquote><ul><li>MiniMind-LLMÁªìÊûÑÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºàDense+MoEÊ®°ÂûãÔºâ„ÄÇ</li><li>ÂåÖÂê´Pretrain„ÄÅSFT„ÄÅLoRA„ÄÅRLHF-DPO„ÄÅÊ®°ÂûãËí∏È¶èÁöÑÂÖ®ËøáÁ®ãËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ</li><li>Êî∂ÈõÜ„ÄÅËí∏È¶è„ÄÅÊï¥ÁêÜÂπ∂Ê∏ÖÊ¥óÂéªÈáçÊâÄÊúâÈò∂ÊÆµÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ</li><li>‰ªé0ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊåá‰ª§ÂæÆË∞É„ÄÅLoRA„ÄÅDPOÂº∫ÂåñÂ≠¶‰π†ÔºåÁôΩÁõíÊ®°ÂûãËí∏È¶è„ÄÇÂÖ≥ÈîÆÁÆóÊ≥ïÂá†‰πé‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∞ÅË£ÖÁöÑÊ°ÜÊû∂Ôºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ</li><li>ÂêåÊó∂ÂÖºÂÆπ„ÄÅ„ÄÅÁ≠âÁ¨¨‰∏âÊñπ‰∏ªÊµÅÊ°ÜÊû∂„ÄÇ</li><li>ËÆ≠ÁªÉÊîØÊåÅÂçïÊú∫ÂçïÂç°„ÄÅÂçïÊú∫Â§öÂç°(DDP„ÄÅDeepSpeed)ËÆ≠ÁªÉÔºåÊîØÊåÅwandbÂèØËßÜÂåñËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊîØÊåÅÂä®ÊÄÅÂêØÂÅúËÆ≠ÁªÉ„ÄÇ</li><li>Âú®Á¨¨‰∏âÊñπÊµãËØÑÊ¶úÔºàC-Eval„ÄÅC-MMLU„ÄÅOpenBookQAÁ≠âÔºâËøõË°åÊ®°ÂûãÊµãËØï„ÄÇ</li><li>ÂÆûÁé∞Openai-ApiÂçèËÆÆÁöÑÊûÅÁÆÄÊúçÂä°Á´ØÔºå‰æø‰∫éÈõÜÊàêÂà∞Á¨¨‰∏âÊñπChatUI‰ΩøÁî®ÔºàFastGPT„ÄÅOpen-WebUIÁ≠âÔºâ„ÄÇ</li><li>Âü∫‰∫éstreamlitÂÆûÁé∞ÊúÄÁÆÄËÅäÂ§©WebUIÂâçÁ´Ø„ÄÇ</li><li>Â§çÁé∞(Ëí∏È¶è/RL)Â§ßÂûãÊé®ÁêÜÊ®°ÂûãDeepSeek-R1ÁöÑMiniMind-ReasonÊ®°ÂûãÔºåÂÖ®ÈÉ®ÂºÄÊ∫êÔºÅ</li></ul><pre><code>git clone https://github.com/jingyaogong/minimind.git\n</code></pre><pre><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre><pre><code>git clone https://huggingface.co/jingyaogong/MiniMind2\n</code></pre><pre><code># load=0: load from pytorch model, load=1: load from transformers-hf model\npython eval_model.py --load 1 --model_mode 2\n</code></pre><pre><code># ÂèØËÉΩÈúÄË¶Å`python&gt;=3.10` ÂÆâË£Ö `pip install streamlit`\n# cd scripts\nstreamlit run web_demo.py\n</code></pre><pre><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre><p>‰ªé‰∏ãÊñáÊèê‰æõÁöÑ<a href=\"https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files\">Êï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•</a> ‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºàÂàõÂª∫ÁõÆÂΩïÔºâÂπ∂ÊîæÂà∞‰∏ã</p><blockquote><p>ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞  ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ</p></blockquote><blockquote><p>ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞  ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠Âç≥‰∏∫ÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºâ</p></blockquote><p>Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°ÂûãÊñá‰ª∂‰Ωç‰∫éÁõÆÂΩï‰∏ã„ÄÇ ‰πüÂèØ‰ª•Áõ¥Êé•Âéª<a href=\"https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files\">Ê≠§Â§Ñ</a>‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑÊñá‰ª∂„ÄÇ</p><pre><code>python eval_model.py --model_mode 1 # ÈªòËÆ§‰∏∫0ÔºöÊµãËØïpretrainÊ®°ÂûãÊïàÊûúÔºåËÆæÁΩÆ‰∏∫1ÔºöÊµãËØïfull_sftÊ®°ÂûãÊïàÊûú\n</code></pre><blockquote><p>[!TIP] ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö</p></blockquote><p>ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)</p><pre><code>torchrun --nproc_per_node N train_xxx.py\n</code></pre><p>ÂàÜËØçÂô®Â∞ÜÂçïËØç‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùÊò†Â∞ÑÂà∞ËøôÊ†∑ÁöÑÊï∞Â≠óÔºåÂèØ‰ª•ÁêÜËß£‰∏∫Êï∞Â≠óÂ∞±‰ª£Ë°®‰∫ÜÂçïËØçÂú®‚ÄúËØçÂÖ∏‚Äù‰∏≠ÁöÑÈ°µÁ†Å„ÄÇ ÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÊûÑÈÄ†ËØçË°®ËÆ≠ÁªÉ‰∏Ä‰∏™‚ÄúËØçÂÖ∏‚ÄùÔºå‰ª£Á†ÅÂèØËßÅ<code>./scripts/train_tokenizer.py</code>Ôºà‰ªÖ‰æõÂ≠¶‰π†ÂèÇËÄÉÔºåËã•ÈùûÂøÖË¶ÅÊó†ÈúÄÂÜçËá™Ë°åËÆ≠ÁªÉÔºåMiniMindÂ∑≤Ëá™Â∏¶tokenizerÔºâ„ÄÇ ÊàñËÄÖÈÄâÊã©ÊØîËæÉÂá∫ÂêçÁöÑÂºÄÊ∫êÂ§ßÊ®°ÂûãÂàÜËØçÂô®Ôºå Ê≠£Â¶ÇÂêåÁõ¥Êé•Áî®Êñ∞Âçé/ÁâõÊ¥•ËØçÂÖ∏ÁöÑ‰ºòÁÇπÊòØtokenÁºñÁ†ÅÂéãÁº©ÁéáÂæàÂ•ΩÔºåÁº∫ÁÇπÊòØÈ°µÊï∞Â§™Â§öÔºåÂä®ËæÑÊï∞ÂçÅ‰∏á‰∏™ËØçÊ±áÁü≠ËØ≠Ôºõ Ëá™Â∑±ËÆ≠ÁªÉÁöÑÂàÜËØçÂô®Ôºå‰ºòÁÇπÊòØËØçË°®ÈïøÂ∫¶ÂíåÂÜÖÂÆπÈöèÊÑèÊéßÂà∂ÔºåÁº∫ÁÇπÊòØÂéãÁº©ÁéáÂæà‰ΩéÔºà‰æãÂ¶Ç\"hello\"‰πüËÆ∏‰ºöË¢´ÊãÜÂàÜ‰∏∫\"h e l l o\" ‰∫î‰∏™Áã¨Á´ãÁöÑtokenÔºâÔºå‰∏îÁîüÂÉªËØçÈöæ‰ª•Ë¶ÜÁõñ„ÄÇ ‚ÄúËØçÂÖ∏‚ÄùÁöÑÈÄâÊã©Âõ∫ÁÑ∂ÂæàÈáçË¶ÅÔºåLLMÁöÑËæìÂá∫Êú¨Ë¥®‰∏äÊòØSoftMaxÂà∞ËØçÂÖ∏N‰∏™ËØçÁöÑÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùËß£Á†ÅÂà∞Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇ Âõ†‰∏∫MiniMind‰ΩìÁßØÈúÄË¶Å‰∏•Ê†ºÊéßÂà∂Ôºå‰∏∫‰∫ÜÈÅøÂÖçÊ®°ÂûãÂ§¥ÈáçËÑöËΩªÔºàËØçÂµåÂÖ•embeddingÂ±ÇÂèÇÊï∞Âú®LLMÂç†ÊØîÂ§™È´òÔºâÔºåÊâÄ‰ª•ËØçË°®ÈïøÂ∫¶Áü≠Áü≠ÁõäÂñÑ„ÄÇ</p><p>ÁªèÂéÜ‰∫ÜMiniMind-V1ÁöÑ‰ΩéË¥®ÈáèÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Ê®°ÂûãËÉ°Ë®Ä‰π±ËØ≠ÁöÑÊïôËÆ≠Ôºå ‰πãÂêéÂÜ≥ÂÆö‰∏çÂÜçÈááÁî®Â§ßËßÑÊ®°Êó†ÁõëÁù£ÁöÑÊï∞ÊçÆÈõÜÂÅöÈ¢ÑËÆ≠ÁªÉ„ÄÇ ËøõËÄåÂ∞ùËØïÊää<a href=\"https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data\">Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ</a>ÁöÑ‰∏≠ÊñáÈÉ®ÂàÜÊèêÂèñÂá∫Êù•Ôºå Ê∏ÖÊ¥óÂá∫Â≠óÁ¨¶ÈïøÂ∫¶ÁöÑÂ§ßÁ∫¶1.6GBÁöÑËØ≠ÊñôÁõ¥Êé•ÊãºÊé•ÊàêÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ ÔºåhqÂç≥‰∏∫high qualityÔºàÂΩìÁÑ∂‰πüËøò‰∏çÁÆóhighÔºåÊèêÂçáÊï∞ÊçÆË¥®ÈáèÊó†Ê≠¢Â∞ΩÔºâ„ÄÇ</p><pre><code>{\"text\": \"Â¶Ç‰ΩïÊâçËÉΩÊëÜËÑ±ÊãñÂª∂ÁóáÔºü Ê≤ªÊÑàÊãñÂª∂ÁóáÂπ∂‰∏çÂÆπÊòìÔºå‰ΩÜ‰ª•‰∏ãÂª∫ËÆÆÂèØËÉΩÊúâÊâÄÂ∏ÆÂä©...\"}\n</code></pre><p><a href=\"https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data\">Âå†Êï∞Â§ßÊ®°ÂûãSFTÊï∞ÊçÆÈõÜ</a> ‚ÄúÊòØ‰∏Ä‰∏™ÂÆåÊï¥„ÄÅÊ†ºÂºèÁªü‰∏Ä„ÄÅÂÆâÂÖ®ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂíåÁ†îÁ©∂ËµÑÊ∫ê„ÄÇ ‰ªéÁΩëÁªú‰∏äÁöÑÂÖ¨ÂºÄÊï∞ÊçÆÊ∫êÊî∂ÈõÜÂπ∂Êï¥ÁêÜ‰∫ÜÂ§ßÈáèÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºåÂØπÂÖ∂ËøõË°å‰∫ÜÊ†ºÂºèÁªü‰∏ÄÔºåÊï∞ÊçÆÊ∏ÖÊ¥óÔºå ÂåÖÂê´10MÊù°Êï∞ÊçÆÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜÂíåÂåÖÂê´2MÊù°Êï∞ÊçÆÁöÑËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÇ‚Äù ‰ª•‰∏äÊòØÂÆòÊñπ‰ªãÁªçÔºå‰∏ãËΩΩÊñá‰ª∂ÂêéÁöÑÊï∞ÊçÆÊÄªÈáèÂ§ßÁ∫¶Âú®4B tokensÔºåËÇØÂÆöÊòØÈÄÇÂêà‰Ωú‰∏∫‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑSFTÊï∞ÊçÆÁöÑ„ÄÇ ‰ΩÜÊòØÂÆòÊñπÊèê‰æõÁöÑÊï∞ÊçÆÊ†ºÂºèÂæà‰π±ÔºåÂÖ®ÈÉ®Áî®Êù•sft‰ª£‰ª∑Â§™Â§ß„ÄÇ ÊàëÂ∞ÜÊääÂÆòÊñπÊï∞ÊçÆÈõÜËøõË°å‰∫Ü‰∫åÊ¨°Ê∏ÖÊ¥óÔºåÊääÂê´ÊúâÁ¨¶Âè∑Ê±°ÊüìÂíåÂô™Â£∞ÁöÑÊù°ÁõÆÂéªÈô§ÔºõÂè¶Â§ñ‰æùÁÑ∂Âè™‰øùÁïô‰∫ÜÊÄªÈïøÂ∫¶ ÁöÑÂÜÖÂÆπÔºåÊ≠§Èò∂ÊÆµÂ∏åÊúõÈÄöËøáÂ§ßÈáèÂØπËØùË°•ÂÖÖÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊ¨†Áº∫ÁöÑÁü•ËØÜ„ÄÇ ÂØºÂá∫Êñá‰ª∂‰∏∫(~7.5GB)„ÄÇ</p><p><a href=\"https://www.modelscope.cn/organization/Magpie-Align\">Magpie-SFTÊï∞ÊçÆÈõÜ</a> Êî∂ÈõÜ‰∫Ü~1MÊù°Êù•Ëá™Qwen2/2.5ÁöÑÈ´òË¥®ÈáèÂØπËØùÔºåÊàëÂ∞ÜËøôÈÉ®ÂàÜÊï∞ÊçÆËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÔºåÊääÊÄªÈïøÂ∫¶ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫(~9GB)„ÄÇ ÈïøÂ∫¶ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫(~5.5GB)ÔºåÁî®Â§ßÊ®°ÂûãÂØπËØùÊï∞ÊçÆÁõ¥Êé•ËøõË°åsftÂ∞±Â±û‰∫é‚ÄúÈªëÁõíËí∏È¶è‚ÄùÁöÑËåÉÁï¥„ÄÇ</p><p>Ëøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÂâç‰∏§Ê≠•sftÁöÑÊï∞ÊçÆÔºàÂè™‰øùÁïô‰∏≠ÊñáÂ≠óÁ¨¶Âç†ÊØîÈ´òÁöÑÂÜÖÂÆπÔºâÔºåÁ≠õÈÄâÈïøÂ∫¶ÁöÑÂØπËØùÔºåÂæóÂà∞(~1.2GB)„ÄÇ</p><pre><code>{\n    \"conversations\": [\n        {\"role\": \"user\", \"content\": \"‰Ω†Â•Ω\"},\n        {\"role\": \"assistant\", \"content\": \"‰Ω†Â•ΩÔºÅ\"},\n        {\"role\": \"user\", \"content\": \"ÂÜçËßÅ\"},\n        {\"role\": \"assistant\", \"content\": \"ÂÜçËßÅÔºÅ\"}\n    ]\n}\n</code></pre><p>Êù•Ëá™<a href=\"https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1\">Magpie-DPOÊï∞ÊçÆÈõÜ</a> Â§ßÁ∫¶200kÊù°ÂÅèÂ•ΩÊï∞ÊçÆÔºàÂùáÊòØËã±ÊñáÔºâÁîüÊàêËá™Llama3.1-70B/8BÔºåÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºå‰ºòÂåñÊ®°ÂûãÂõûÂ§çË¥®ÈáèÔºå‰ΩøÂÖ∂Êõ¥Âä†Á¨¶Âêà‰∫∫Á±ªÂÅèÂ•Ω„ÄÇ ËøôÈáåÂ∞ÜÊï∞ÊçÆÊÄªÈïøÂ∫¶ÁöÑÂÜÖÂÆπÈáçÁªÑ‰∏∫(~0.9GB)ÔºåÂåÖÂê´Âíå‰∏§‰∏™Â≠óÊÆµÔºå ‰∏∫ÂÅèÂ•ΩÁöÑÂõûÂ§çÔºå‰∏∫ÊãíÁªùÁöÑÂõûÂ§ç„ÄÇ</p><pre><code>{\n  \"chosen\": [\n    {\"content\": \"Q\", \"role\": \"user\"}, \n    {\"content\": \"good answer\", \"role\": \"assistant\"}\n  ], \n  \"rejected\": [\n    {\"content\": \"Q\", \"role\": \"user\"}, \n    {\"content\": \"bad answer\", \"role\": \"assistant\"}\n  ]\n}\n</code></pre><p>‰∏çÂæó‰∏çËØ¥2025Âπ¥2ÊúàË∞ÅËÉΩÁÅ´ÁöÑËøáDeepSeek... ‰πüÊøÄÂèë‰∫ÜÊàëÂØπRLÂºïÂØºÁöÑÊé®ÁêÜÊ®°ÂûãÁöÑÊµìÂéöÂÖ¥Ë∂£ÔºåÁõÆÂâçÂ∑≤ÁªèÁî®Qwen2.5Â§çÁé∞‰∫ÜR1-Zero„ÄÇ Â¶ÇÊûúÊúâÊó∂Èó¥+ÊïàÊûúworkÔºà‰ΩÜ99%Âü∫Ê®°ËÉΩÂäõ‰∏çË∂≥ÔºâÊàë‰ºöÂú®‰πãÂêéÊõ¥Êñ∞MiniMindÂü∫‰∫éRLËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãËÄå‰∏çÊòØËí∏È¶èÊ®°Âûã„ÄÇ Êó∂Èó¥ÊúâÈôêÔºåÊúÄÂø´ÁöÑ‰ΩéÊàêÊú¨ÊñπÊ°à‰æùÁÑ∂ÊòØÁõ¥Êé•Ëí∏È¶èÔºàÈªëÁõíÊñπÂºèÔºâ„ÄÇ ËÄê‰∏ç‰ΩèR1Â§™ÁÅ´ÔºåÁü≠Áü≠Âá†Â§©Â∞±Â∑≤ÁªèÂ≠òÂú®‰∏Ä‰∫õR1ÁöÑËí∏È¶èÊï∞ÊçÆÈõÜ<a href=\"https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\">R1-Llama-70B</a>„ÄÅ<a href=\"https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT\">R1-Distill-SFT</a>„ÄÅ <a href=\"https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH\">Alpaca-Distill-R1</a>„ÄÅ <a href=\"https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh\">deepseek_r1_zh</a>Á≠âÁ≠âÔºåÁ∫Ø‰∏≠ÊñáÁöÑÊï∞ÊçÆÂèØËÉΩÊØîËæÉÂ∞ë„ÄÇ ÊúÄÁªàÊï¥ÂêàÂÆÉ‰ª¨ÔºåÂØºÂá∫Êñá‰ª∂‰∏∫ÔºåÊï∞ÊçÆÊ†ºÂºèÂíå‰∏ÄËá¥„ÄÇ</p><blockquote><p>[!NOTE] 2025-02-05ÂêéÔºåÂºÄÊ∫êMiniMindÊúÄÁªàËÆ≠ÁªÉÊâÄÁî®ÁöÑÊâÄÊúâÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§Êó†ÈúÄÂÜçËá™Ë°åÈ¢ÑÂ§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÈÅøÂÖçÈáçÂ§çÊÄßÁöÑÊï∞ÊçÆÂ§ÑÁêÜÂ∑•‰Ωú„ÄÇ</p></blockquote><p>Â∞Ü‰∏ãËΩΩÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂ÊîæÂà∞ÁõÆÂΩï‰∏ãÔºà‚ú®‰∏∫Êé®ËçêÁöÑÂøÖÈ°ªÈ°πÔºâ</p><pre><code>./dataset/\n‚îú‚îÄ‚îÄ dpo.jsonl (909MB)\n‚îú‚îÄ‚îÄ lora_identity.jsonl (22.8KB)\n‚îú‚îÄ‚îÄ lora_medical.jsonl (34MB)\n‚îú‚îÄ‚îÄ pretrain_hq.jsonl (1.6GB, ‚ú®)\n‚îú‚îÄ‚îÄ r1_mix_1024.jsonl (340MB)\n‚îú‚îÄ‚îÄ sft_1024.jsonl (5.6GB)\n‚îú‚îÄ‚îÄ sft_2048.jsonl (9GB)\n‚îú‚îÄ‚îÄ sft_512.jsonl (7.5GB)\n‚îú‚îÄ‚îÄ sft_mini_512.jsonl (1.2GB, ‚ú®)\n‚îî‚îÄ‚îÄ tokenizer_train.jsonl (1GB)\n</code></pre><p>MiniMind-DenseÔºàÂíå<a href=\"https://ai.meta.com/blog/meta-llama-3-1/\">Llama3.1</a>‰∏ÄÊ†∑Ôºâ‰ΩøÁî®‰∫ÜTransformerÁöÑDecoder-OnlyÁªìÊûÑÔºåË∑üGPT-3ÁöÑÂå∫Âà´Âú®‰∫éÔºö</p><ul><li>ÈááÁî®‰∫ÜGPT-3ÁöÑÈ¢ÑÊ†áÂáÜÂåñÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂú®ÊØè‰∏™TransformerÂ≠êÂ±ÇÁöÑËæìÂÖ•‰∏äËøõË°åÂΩí‰∏ÄÂåñÔºåËÄå‰∏çÊòØÂú®ËæìÂá∫‰∏ä„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®ÁöÑÊòØRMSNormÂΩí‰∏ÄÂåñÂáΩÊï∞„ÄÇ</li><li>Áî®SwiGLUÊøÄÊ¥ªÂáΩÊï∞Êõø‰ª£‰∫ÜReLUÔºåËøôÊ†∑ÂÅöÊòØ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩ„ÄÇ</li><li>ÂÉèGPT-Neo‰∏ÄÊ†∑ÔºåÂéªÊéâ‰∫ÜÁªùÂØπ‰ΩçÁΩÆÂµåÂÖ•ÔºåÊîπÁî®‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÔºåËøôÊ†∑Âú®Â§ÑÁêÜË∂ÖÂá∫ËÆ≠ÁªÉÈïøÂ∫¶ÁöÑÊé®ÁêÜÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ</li></ul><ul><li>DeepSeek-V2Âú®ÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÊñπÈù¢ÔºåÈááÁî®‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑ‰∏ìÂÆ∂ÂàÜÂâ≤ÂíåÂÖ±‰∫´ÁöÑ‰∏ìÂÆ∂ÈöîÁ¶ªÊäÄÊúØÔºå‰ª•ÊèêÈ´òExpertsÁöÑÊïàÊûú„ÄÇ</li></ul><p>MiniMindÁöÑÊï¥‰ΩìÁªìÊûÑ‰∏ÄËá¥ÔºåÂè™ÊòØÂú®RoPEËÆ°ÁÆó„ÄÅÊé®ÁêÜÂáΩÊï∞ÂíåFFNÂ±ÇÁöÑ‰ª£Á†Å‰∏äÂÅö‰∫Ü‰∏Ä‰∫õÂ∞èË∞ÉÊï¥„ÄÇ ÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÂõæÔºàÈáçÁªòÁâàÔºâÔºö</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><ul><li>Ôºö‚âà1.3Ôø•/hÔºàÂèØËá™Ë°åÂèÇËÄÉÂÆûÊó∂Â∏Ç‰ª∑Ôºâ„ÄÇ</li><li>ÔºöË°®Ê†º‰ªÖÂÆûÊµã  Âíå  ‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊó∂Èó¥ÔºåÂÖ∂ÂÆÉËÄóÊó∂Ê†πÊçÆÊï∞ÊçÆÈõÜÂ§ßÂ∞è‰º∞ÁÆóÔºàÂèØËÉΩÂ≠òÂú®‰∫õËÆ∏Âá∫ÂÖ•Ôºâ„ÄÇ</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>‚ú®Âü∫‰∫éÂçïÂç°NVIDIA 3090ÁöÑ‰ªé0ËÆ≠ÁªÉ‰ªÖÈúÄ + ÔºåÂÆûÁé∞ChatBotÊïàÊûúÔºÅ</p><p>‚ú®PSÔºöËã•ÈááÁî®8Âç°4090ËÆ≠ÁªÉÔºåÊÄªÁî®Êó∂ÁîöËá≥ÂèØ‰ª•ÂéãÁº©Âà∞10ÂàÜÈíü‰ª•ÂÜÖÔºÅÔºàÁî±‰∫éÊó∂Èó¥Êõ¥Áü≠ÔºåËä±Ë¥πÂêåÊ†∑3ÂÖÉÂ∑¶Âè≥Ôºå‰∏éÂçïÂç°ÊàêÊú¨Áõ∏ÂΩìÔºâ</p><p>‚ú®‰ª•ÊûÅ‰ΩéÊûÅ‰ΩéÁöÑÈó®ÊßõÔºåÂÆûÁé∞‰∫∫‰∫∫ÂèØÁé©ÁöÑÂ§ßÊ®°ÂûãËá™Áî±ÔºÅËøôÊ≠£ÊòØMiniMindÁ≥ªÂàóÁöÑËØûÁîüÂàùË°∑ÔºÅ</p><p>‚ú®‰ªÖ‰ª∑ÂÄºÊàêÊú¨ÁöÑÂπ∂‰∏çÊòØÂô±Â§¥ÔºÅChatÊµãËØïÔºö</p><pre><code>üë∂: ËØ∑‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇ\nü§ñÔ∏è: ‰Ωú‰∏∫‰∫∫Â∑•Êô∫ËÉΩÔºåÊàëÊ≤°ÊúâÂÆûÈôÖÁöÑÁîüÊ¥ªÔºå‰πüÊ≤°ÊúâËá™ÊàëÊÑèËØÜÔºåÊâÄ‰ª•Ê≤°ÊúâËá™Â∑±ÁöÑÁîüÊ¥ª„ÄÇÊàëË¢´ËÆæËÆ°ÊàêËÉΩÂ§üÂ∏ÆÂä©Áî®Êà∑Ëß£Á≠îÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ø°ÊÅØ„ÄÅËøõË°åÂØπËØùÁ≠â„ÄÇÊàëÁöÑËÆæËÆ°ÂíåÂäüËÉΩÊòØÁî±ËÆ°ÁÆóÊú∫ÁßëÂ≠¶„ÄÅÊú∫Âô®Â≠¶‰π†„ÄÅ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂíåÁÆóÊ≥ïÊâÄÊûÑÂª∫ÁöÑÔºåÊàëÈÄöËøáÁºñÁ®ãÂíåÁÆóÊ≥ïÂÆûÁé∞„ÄÇ\n\nüë∂: Êé®Ëçê‰∏Ä‰∫õÊù≠Â∑ûÁöÑÁâπËâ≤ÁæéÈ£üÂêß„ÄÇ\nü§ñÔ∏è: Êù≠Â∑ûÊúâÂæàÂ§öÁæéÂë≥ÁöÑÁâπËâ≤ÁæéÈ£üÔºåÊØîÂ¶ÇÈ∏°ËÇâ‰∏≤„ÄÅÈ≥óÈ±ºÂ§¥„ÄÅÊ∞¥ÁÖÆÈ±º„ÄÅË±ÜËÖêËÑëÁ≠â„ÄÇËøô‰∫õÁæéÈ£üÈÉΩÈùûÂ∏∏ÁæéÂë≥ÔºåÊúâÁùÄÁã¨ÁâπÁöÑÈ£éÂë≥ÂíåÂè£ÊÑüÔºåÁªùÂØπÊòØÂÄºÂæó‰∏ÄËØïÁöÑÁæéÈ£ü„ÄÇ\n\nüë∂: ËØ∑‰∏∫ÊàëËÆ≤Ëß£‚ÄúÂ§ßËØ≠Ë®ÄÊ®°Âûã‚ÄùËøô‰∏™Ê¶ÇÂøµ„ÄÇ\nü§ñÔ∏è: Â§ßËØ≠Ë®ÄÊ®°ÂûãÊòØ‰∏ÄÁßç‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÔºåÂÆÉ‰ΩøÁî®Â§ßÈáèÁöÑÊñáÊú¨Êï∞ÊçÆÊù•ËÆ≠ÁªÉÊ®°ÂûãÔºåÁÑ∂ÂêéÂú®‰∏çÊñ≠ÁöÑËø≠‰ª£‰∏≠‰∏çÊñ≠‰ºòÂåñÊ®°Âûã„ÄÇÂÆÉÁöÑÁõÆÊ†áÊòØÊ†πÊçÆËæìÂÖ•ÁöÑÊï∞ÊçÆÊù•ÁîüÊàêÁ¨¶ÂêàÁâπÂÆöÈúÄÊ±ÇÁöÑÊñáÊú¨ÔºåËøô‰∫õÊñáÊú¨ÂèØ‰ª•ÊòØ‰ªª‰Ωï‰Ω†ÊÑüÂÖ¥Ë∂£ÁöÑ‰∏ªÈ¢ò„ÄÇ\n\nüë∂: Introduce the history of the United States, please.\nü§ñÔ∏è: ÊÇ®ÊèêÂà∞ÁöÑ‚ÄúIntrook's the believeations of theument.\" Ëøô‰∏™ÂêçÂ≠óÊù•Ê∫ê‰∫é‰∏≠ÂõΩÂè§‰ª£ÁöÑ\"groty of of the change.\"\n</code></pre><p>ÊûÅÈÄü‰∏îÂàùÂÖ∑ÊïàÊûúÔºåÁîöËá≥‰ªçÁÑ∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂéãÁº©Ëé∑ÂèñÊõ¥Â∞èÊõ¥‰ºòË¥®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ ZeroÊ®°ÂûãÊùÉÈáç‰øùÂ≠ò‰∏∫ ÔºàËßÅ‰∏ãÊñáMiniMindÊ®°ÂûãÊñá‰ª∂ÈìæÊé•ÔºâÔºåÂ¶ÇÊúâÂÖ¥Ë∂£ÂèØ‰∏ãËΩΩÊ£ÄÈ™åÊ≠§Ê®°ÂûãÊïàÊûú„ÄÇ</p><p>LLMÈ¶ñÂÖàË¶ÅÂ≠¶‰π†ÁöÑÂπ∂ÈùûÁõ¥Êé•‰∏é‰∫∫‰∫§ÊµÅÔºåËÄåÊòØËÆ©ÁΩëÁªúÂèÇÊï∞‰∏≠ÂÖÖÊª°Áü•ËØÜÁöÑÂ¢®Ê∞¥Ôºå‚ÄúÂ¢®Ê∞¥‚Äù ÁêÜËÆ∫‰∏äÂñùÁöÑË∂äÈ•±Ë∂äÂ•ΩÔºå‰∫ßÁîüÂ§ßÈáèÁöÑÂØπ‰∏ñÁïåÁöÑÁü•ËØÜÁßØÁ¥Ø„ÄÇ È¢ÑËÆ≠ÁªÉÂ∞±ÊòØËÆ©ModelÂÖàÂüãÂ§¥Ëã¶Â≠¶Â§ßÈáèÂü∫Êú¨ÁöÑÁü•ËØÜÔºå‰æãÂ¶Ç‰ªéWikiÁôæÁßë„ÄÅÊñ∞Èóª„ÄÅ‰π¶Á±çÊï¥ÁêÜÂ§ßËßÑÊ®°ÁöÑÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ Ëøô‰∏™ËøáÁ®ãÊòØ‚ÄúÊó†ÁõëÁù£‚ÄùÁöÑÔºåÂç≥‰∫∫Á±ª‰∏çÈúÄË¶ÅÂú®ËøáÁ®ã‰∏≠ÂÅö‰ªª‰Ωï‚ÄúÊúâÁõëÁù£‚ÄùÁöÑÊ†°Ê≠£ÔºåËÄåÊòØÁî±Ê®°ÂûãËá™Â∑±‰ªéÂ§ßÈáèÊñáÊú¨‰∏≠ÊÄªÁªìËßÑÂæãÂ≠¶‰π†Áü•ËØÜÁÇπ„ÄÇ Ê®°ÂûãÊ≠§Èò∂ÊÆµÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™Ôºö„ÄÇ‰æãÂ¶ÇÊàë‰ª¨ËæìÂÖ•‚ÄúÁß¶ÂßãÁöá‚ÄùÂõõ‰∏™Â≠óÔºåÂÆÉÂèØ‰ª•Êé•Èæô‚ÄúÊòØ‰∏≠ÂõΩÁöÑÁ¨¨‰∏Ä‰ΩçÁöáÂ∏ù‚Äù„ÄÇ</p><pre><code>torchrun --nproc_per_node 1 train_pretrain.py # 1Âç≥‰∏∫ÂçïÂç°ËÆ≠ÁªÉÔºåÂèØÊ†πÊçÆÁ°¨‰ª∂ÊÉÖÂÜµËá™Ë°åË∞ÉÊï¥ (ËÆæÁΩÆ&gt;=2)\n# or\npython train_pretrain.py\n</code></pre><blockquote><p>ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî‰øùÂ≠ò‰∏∫: Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ</p></blockquote><h3><strong>2. ÊúâÁõëÁù£ÂæÆË∞É(Supervised Fine-Tuning)</strong>:</h3><p>ÁªèËøáÈ¢ÑËÆ≠ÁªÉÔºåLLMÊ≠§Êó∂Â∑≤ÁªèÊéåÊè°‰∫ÜÂ§ßÈáèÁü•ËØÜÔºåÁÑ∂ËÄåÊ≠§Êó∂ÂÆÉÂè™‰ºöÊó†ËÑëÂú∞ËØçËØ≠Êé•ÈæôÔºåËøò‰∏ç‰ºö‰∏é‰∫∫ËÅäÂ§©„ÄÇ SFTÈò∂ÊÆµÂ∞±ÈúÄË¶ÅÊääÂçäÊàêÂìÅLLMÊñΩÂä†‰∏Ä‰∏™Ëá™ÂÆö‰πâÁöÑËÅäÂ§©Ê®°ÊùøËøõË°åÂæÆË∞É„ÄÇ ‰æãÂ¶ÇÊ®°ÂûãÈÅáÂà∞ËøôÊ†∑ÁöÑÊ®°Êùø„ÄêÈóÆÈ¢ò-&gt;ÂõûÁ≠îÔºåÈóÆÈ¢ò-&gt;ÂõûÁ≠î„ÄëÂêé‰∏çÂÜçÊó†ËÑëÊé•ÈæôÔºåËÄåÊòØÊÑèËØÜÂà∞ËøôÊòØ‰∏ÄÊÆµÂÆåÊï¥ÁöÑÂØπËØùÁªìÊùü„ÄÇ Áß∞Ëøô‰∏™ËøáÁ®ã‰∏∫Êåá‰ª§ÂæÆË∞ÉÔºåÂ∞±Â¶ÇÂêåËÆ©Â∑≤ÁªèÂ≠¶ÂØå‰∫îËΩ¶ÁöÑ„ÄåÁâõÈ°ø„ÄçÂÖàÁîüÈÄÇÂ∫î21‰∏ñÁ∫™Êô∫ËÉΩÊâãÊú∫ÁöÑËÅäÂ§©‰π†ÊÉØÔºåÂ≠¶‰π†Â±èÂπïÂ∑¶‰æßÊòØÂØπÊñπÊ∂àÊÅØÔºåÂè≥‰æßÊòØÊú¨‰∫∫Ê∂àÊÅØËøô‰∏™ËßÑÂæã„ÄÇ Âú®ËÆ≠ÁªÉÊó∂ÔºåMiniMindÁöÑÊåá‰ª§ÂíåÂõûÁ≠îÈïøÂ∫¶Ë¢´Êà™Êñ≠Âú®512ÔºåÊòØ‰∏∫‰∫ÜËäÇÁúÅÊòæÂ≠òÁ©∫Èó¥„ÄÇÂ∞±ÂÉèÊàë‰ª¨Â≠¶‰π†Êó∂Ôºå‰ºöÂÖà‰ªéÁü≠ÁöÑÊñáÁ´†ÂºÄÂßãÔºåÂΩìÂ≠¶‰ºöÂÜô‰Ωú200Â≠ó‰ΩúÊñáÂêéÔºå800Â≠óÊñáÁ´†‰πüÂèØ‰ª•ÊâãÂà∞ÊìíÊù•„ÄÇ Âú®ÈúÄË¶ÅÈïøÂ∫¶ÊãìÂ±ïÊó∂ÔºåÂè™ÈúÄË¶ÅÂáÜÂ§áÂ∞ëÈáèÁöÑ2k/4k/8kÈïøÂ∫¶ÂØπËØùÊï∞ÊçÆËøõË°åËøõ‰∏ÄÊ≠•ÂæÆË∞ÉÂç≥ÂèØÔºàÊ≠§Êó∂ÊúÄÂ•ΩÈÖçÂêàRoPE-NTKÁöÑÂü∫ÂáÜÂ∑ÆÂÄºÔºâ„ÄÇ</p><blockquote><p>Âú®Êé®ÁêÜÊó∂ÈÄöËøáË∞ÉÊï¥RoPEÁ∫øÊÄßÂ∑ÆÂÄºÔºåÂÆûÁé∞ÂÖçËÆ≠ÁªÉÈïøÂ∫¶Â§ñÊé®Âà∞2048Âèä‰ª•‰∏äÂ∞Ü‰ºöÂæàÊñπ‰æø„ÄÇ</p></blockquote><pre><code>torchrun --nproc_per_node 1 train_full_sft.py\n# or\npython train_full_sft.py\n</code></pre><blockquote><p>ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî‰øùÂ≠ò‰∏∫: Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ</p></blockquote><h3><strong>3. ‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†(Reinforcement Learning from Human Feedback, RLHF)</strong></h3><p>Âú®ÂâçÈù¢ÁöÑËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÊ®°ÂûãÂ∑≤ÁªèÂÖ∑Â§á‰∫ÜÂü∫Êú¨ÁöÑÂØπËØùËÉΩÂäõÔºå‰ΩÜÊòØËøôÊ†∑ÁöÑËÉΩÂäõÂÆåÂÖ®Âü∫‰∫éÂçïËØçÊé•ÈæôÔºåÁº∫Â∞ëÊ≠£ÂèçÊ†∑‰æãÁöÑÊøÄÂä±„ÄÇ Ê®°ÂûãÊ≠§Êó∂Â∞öÊú™Áü•‰ªÄ‰πàÂõûÁ≠îÊòØÂ•ΩÁöÑÔºå‰ªÄ‰πàÊòØÂ∑ÆÁöÑ„ÄÇÊàë‰ª¨Â∏åÊúõÂÆÉËÉΩÂ§üÊõ¥Á¨¶Âêà‰∫∫ÁöÑÂÅèÂ•ΩÔºåÈôç‰ΩéËÆ©‰∫∫Á±ª‰∏çÊª°ÊÑèÁ≠îÊ°àÁöÑ‰∫ßÁîüÊ¶ÇÁéá„ÄÇ Ëøô‰∏™ËøáÁ®ãÂ∞±ÂÉèÊòØËÆ©Ê®°ÂûãÂèÇÂä†Êñ∞ÁöÑÂüπËÆ≠Ôºå‰ªé‰ºòÁßÄÂëòÂ∑•ÁöÑ‰Ωú‰∏∫‰æãÂ≠êÔºåÊ∂àÊûÅÂëòÂ∑•‰Ωú‰∏∫Âèç‰æãÔºåÂ≠¶‰π†Â¶Ç‰ΩïÊõ¥Â•ΩÂú∞ÂõûÂ§ç„ÄÇ Ê≠§Â§Ñ‰ΩøÁî®ÁöÑÊòØRLHFÁ≥ªÂàó‰πã-Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ(Direct Preference Optimization, DPO)„ÄÇ ‰∏éPPO(Proximal Policy Optimization)ËøôÁßçÈúÄË¶ÅÂ•ñÂä±Ê®°Âûã„ÄÅ‰ª∑ÂÄºÊ®°ÂûãÁöÑRLÁÆóÊ≥ï‰∏çÂêåÔºõ DPOÈÄöËøáÊé®ÂØºPPOÂ•ñÂä±Ê®°ÂûãÁöÑÊòæÂºèËß£ÔºåÊääÂú®Á∫øÂ•ñÂä±Ê®°ÂûãÊç¢ÊàêÁ¶ªÁ∫øÊï∞ÊçÆÔºåRefÊ®°ÂûãËæìÂá∫ÂèØ‰ª•ÊèêÂâç‰øùÂ≠ò„ÄÇ DPOÊÄßËÉΩÂá†‰πé‰∏çÂèòÔºåÂè™Áî®Ë∑ë actor_model Âíå ref_model ‰∏§‰∏™Ê®°ÂûãÔºåÂ§ßÂ§ßËäÇÁúÅÊòæÂ≠òÂºÄÈîÄÂíåÂ¢ûÂä†ËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇ</p><blockquote><p>Ê≥®ÔºöRLHFËÆ≠ÁªÉÊ≠•È™§ÔºåÊ≠§Ê≠•È™§Èöæ‰ª•ÊèêÂçáÊ®°Âûã‚ÄúÊô∫Âäõ‚ÄùËÄåÈÄöÂ∏∏‰ªÖÁî®‰∫éÊèêÂçáÊ®°ÂûãÁöÑ‚ÄúÁ§ºË≤å‚ÄùÔºåÊúâÂà©ÔºàÁ¨¶ÂêàÂÅèÂ•Ω„ÄÅÂáèÂ∞ëÊúâÂÆ≥ÂÜÖÂÆπÔºâ‰πüÊúâÂºäÔºàÊ†∑Êú¨Êî∂ÈõÜÊòÇË¥µ„ÄÅÂèçÈ¶àÂÅèÂ∑Æ„ÄÅÂ§öÊ†∑ÊÄßÊçüÂ§±Ôºâ„ÄÇ</p></blockquote><pre><code>torchrun --nproc_per_node 1 train_dpo.py\n# or\npython train_dpo.py\n</code></pre><blockquote><p>ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî‰øùÂ≠ò‰∏∫: Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ</p></blockquote><h3><strong>4. Áü•ËØÜËí∏È¶è(Knowledge Distillation, KD)</strong></h3><p>Âú®ÂâçÈù¢ÁöÑÊâÄÊúâËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÊ®°ÂûãÂ∑≤ÁªèÂÆåÂÖ®ÂÖ∑Â§á‰∫ÜÂü∫Êú¨ËÉΩÂäõÔºåÈÄöÂ∏∏ÂèØ‰ª•Â≠¶ÊàêÂá∫Â∏à‰∫Ü„ÄÇ ËÄåÁü•ËØÜËí∏È¶èÂèØ‰ª•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºåÊâÄË∞ìÁü•ËØÜËí∏È¶èÔºåÂç≥Â≠¶ÁîüÊ®°ÂûãÈù¢ÂêëÊïôÂ∏àÊ®°ÂûãÂ≠¶‰π†„ÄÇ ÊïôÂ∏àÊ®°ÂûãÈÄöÂ∏∏ÊòØÁªèËøáÂÖÖÂàÜËÆ≠ÁªÉÁöÑÂ§ßÊ®°ÂûãÔºåÂÖ∑ÊúâËæÉÈ´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ Â≠¶ÁîüÊ®°ÂûãÊòØ‰∏Ä‰∏™ËæÉÂ∞èÁöÑÊ®°ÂûãÔºåÁõÆÊ†áÊòØÂ≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑË°å‰∏∫ÔºåËÄå‰∏çÊòØÁõ¥Êé•‰ªéÂéüÂßãÊï∞ÊçÆ‰∏≠Â≠¶‰π†„ÄÇ Âú®SFTÂ≠¶‰π†‰∏≠ÔºåÊ®°ÂûãÁöÑÁõÆÊ†áÊòØÊãüÂêàËØçTokenÂàÜÁ±ªÁ°¨Ê†áÁ≠æÔºàhard labelsÔºâÔºåÂç≥ÁúüÂÆûÁöÑÁ±ªÂà´Ê†áÁ≠æÔºàÂ¶Ç 0 Êàñ 6400Ôºâ„ÄÇ Âú®Áü•ËØÜËí∏È¶è‰∏≠ÔºåÊïôÂ∏àÊ®°ÂûãÁöÑsoftmaxÊ¶ÇÁéáÂàÜÂ∏ÉË¢´Áî®‰ΩúËΩØÊ†áÁ≠æÔºàsoft labelsÔºâ„ÄÇÂ∞èÊ®°Âûã‰ªÖÂ≠¶‰π†ËΩØÊ†áÁ≠æÔºåÂπ∂‰ΩøÁî®KL-LossÊù•‰ºòÂåñÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇ ÈÄö‰øóÂú∞ËØ¥ÔºåSFTÁõ¥Êé•Â≠¶‰π†ËÄÅÂ∏àÁªôÁöÑËß£È¢òÁ≠îÊ°à„ÄÇËÄåKDËøáÁ®ãÁõ∏ÂΩì‰∫é‚ÄúÊâìÂºÄ‚ÄùËÄÅÂ∏àËÅ™ÊòéÁöÑÂ§ßËÑëÔºåÂ∞ΩÂèØËÉΩÂú∞Ê®°‰ªøËÄÅÂ∏à‚ÄúÂ§ßËÑë‚ÄùÊÄùËÄÉÈóÆÈ¢òÁöÑÁ•ûÁªèÂÖÉÁä∂ÊÄÅ„ÄÇ ‰æãÂ¶ÇÔºåÂΩìËÄÅÂ∏àÊ®°ÂûãËÆ°ÁÆóËøô‰∏™ÈóÆÈ¢òÁöÑÊó∂ÂÄôÔºåÊúÄÂêé‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉaÁä∂ÊÄÅ‰∏∫0ÔºåÁ•ûÁªèÂÖÉbÁä∂ÊÄÅ‰∏∫100ÔºåÁ•ûÁªèÂÖÉcÁä∂ÊÄÅ‰∏∫-99... Â≠¶ÁîüÊ®°ÂûãÈÄöËøáÂ§ßÈáèÊï∞ÊçÆÔºåÂ≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÂ§ßËÑëÂÜÖÈÉ®ÁöÑËøêËΩ¨ËßÑÂæã„ÄÇËøô‰∏™ËøáÁ®ãÂç≥Áß∞‰πã‰∏∫ÔºöÁü•ËØÜËí∏È¶è„ÄÇ Áü•ËØÜËí∏È¶èÁöÑÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™ÔºöËÆ©Â∞èÊ®°Âûã‰ΩìÁßØÊõ¥Â∞èÁöÑÂêåÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ ÁÑ∂ËÄåÈöèÁùÄLLMËØûÁîüÂíåÂèëÂ±ïÔºåÊ®°ÂûãËí∏È¶è‰∏ÄËØçË¢´ÂπøÊ≥õÊª•Áî®Ôºå‰ªéËÄå‰∫ßÁîü‰∫Ü‚ÄúÁôΩÁõí/ÈªëÁõí‚ÄùÁü•ËØÜËí∏È¶è‰∏§‰∏™Ê¥æÂà´„ÄÇ GPT-4ËøôÁßçÈó≠Ê∫êÊ®°ÂûãÔºåÁî±‰∫éÊó†Ê≥ïËé∑ÂèñÂÖ∂ÂÜÖÈÉ®ÁªìÊûÑÔºåÂõ†Ê≠§Âè™ËÉΩÈù¢ÂêëÂÆÉÊâÄËæìÂá∫ÁöÑÊï∞ÊçÆÂ≠¶‰π†ÔºåËøô‰∏™ËøáÁ®ãÁß∞‰πã‰∏∫ÈªëÁõíËí∏È¶èÔºå‰πüÊòØÂ§ßÊ®°ÂûãÊó∂‰ª£ÊúÄÊôÆÈÅçÁöÑÂÅöÊ≥ï„ÄÇ ÈªëÁõíËí∏È¶è‰∏éSFTËøáÁ®ãÂÆåÂÖ®‰∏ÄËá¥ÔºåÂè™‰∏çËøáÊï∞ÊçÆÊòØ‰ªéÂ§ßÊ®°ÂûãÁöÑËæìÂá∫Êî∂ÈõÜÔºåÂõ†Ê≠§Âè™ÈúÄË¶ÅÂáÜÂ§áÊï∞ÊçÆÂπ∂‰∏îËøõ‰∏ÄÊ≠•FTÂç≥ÂèØ„ÄÇ Ê≥®ÊÑèÊõ¥ÊîπË¢´Âä†ËΩΩÁöÑÂü∫Á°ÄÊ®°Âûã‰∏∫ÔºåÂç≥Âü∫‰∫éÂæÆË∞ÉÊ®°ÂûãÂÅöËøõ‰∏ÄÊ≠•ÁöÑËí∏È¶èÂ≠¶‰π†„ÄÇ ‰∏é ÂùáÊî∂ÈõÜËá™qwen2.5-7/72B-InstructÂ§ßÊ®°ÂûãÔºåÂèØÁõ¥Êé•Áî®‰∫éSFT‰ª•Ëé∑ÂèñQwenÁöÑÈÉ®ÂàÜË°å‰∏∫„ÄÇ</p><pre><code># Ê≥®ÊÑèÈúÄË¶ÅÊõ¥Êîπtrain_full_sft.pyÊï∞ÊçÆÈõÜË∑ØÂæÑÔºå‰ª•Âèämax_seq_len  \ntorchrun --nproc_per_node 1 train_full_sft.py\n# or\npython train_full_sft.py\n</code></pre><blockquote><p>ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöîÂêåÊ†∑‰øùÂ≠ò‰∏∫: Ôºà*‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ</p></blockquote><p>Ê≠§Â§ÑÂ∫îÂΩìÁùÄÈáç‰ªãÁªçMiniMindÂÆûÁé∞ÁöÑÁôΩÁõíËí∏È¶è‰ª£Á†ÅÔºåÁî±‰∫éMiniMindÂêåÁ≥ªÂàóÊú¨Ë∫´Âπ∂‰∏çÂ≠òÂú®Âº∫Â§ßÁöÑÊïôÂ∏àÊ®°ÂûãÔºåÂõ†Ê≠§ÁôΩÁõíËí∏È¶è‰ª£Á†Å‰ªÖ‰Ωú‰∏∫Â≠¶‰π†ÂèÇËÄÉ„ÄÇ</p><pre><code>torchrun --nproc_per_node 1 train_distillation.py\n# or\npython train_distillation.py\n</code></pre><h3><strong>5. LoRA (Low-Rank Adaptation)</strong></h3><p>LoRAÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂèÇÊï∞È´òÊïàÂæÆË∞ÉÔºàParameter-Efficient Fine-Tuning, PEFTÔºâÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøá‰ΩéÁß©ÂàÜËß£ÁöÑÊñπÂºèÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇ Áõ∏ÊØî‰∫éÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºàFull Fine-TuningÔºâÔºåLoRA Âè™ÈúÄË¶ÅÊõ¥Êñ∞Â∞ëÈáèÁöÑÂèÇÊï∞„ÄÇ LoRA ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÔºöÂú®Ê®°ÂûãÁöÑÊùÉÈáçÁü©Èòµ‰∏≠ÂºïÂÖ•‰ΩéÁß©ÂàÜËß£Ôºå‰ªÖÂØπ‰ΩéÁß©ÈÉ®ÂàÜËøõË°åÊõ¥Êñ∞ÔºåËÄå‰øùÊåÅÂéüÂßãÈ¢ÑËÆ≠ÁªÉÊùÉÈáç‰∏çÂèò„ÄÇ ‰ª£Á†ÅÂèØËßÅÂíåÔºåÂÆåÂÖ®‰ªé0ÂÆûÁé∞LoRAÊµÅÁ®ãÔºå‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÁöÑÂ∞ÅË£Ö„ÄÇ</p><pre><code>torchrun --nproc_per_node 1 train_lora.py\n# or\npython train_lora.py\n</code></pre><blockquote><p>ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî‰øùÂ≠ò‰∏∫: Ôºà* ‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ</p></blockquote><p>ÈùûÂ∏∏Â§öÁöÑ‰∫∫Âõ∞ÊÉëÔºåÂ¶Ç‰Ωï‰ΩøÊ®°ÂûãÂ≠¶‰ºöËá™Â∑±ÁßÅÊúâÈ¢ÜÂüüÁöÑÁü•ËØÜÔºüÂ¶Ç‰ΩïÂáÜÂ§áÊï∞ÊçÆÈõÜÔºüÂ¶Ç‰ΩïËøÅÁßªÈÄöÁî®È¢ÜÂüüÊ®°ÂûãÊâìÈÄ†ÂûÇÂüüÊ®°ÂûãÔºü ËøôÈáå‰∏æÂá†‰∏™‰æãÂ≠êÔºåÂØπ‰∫éÈÄöÁî®Ê®°ÂûãÔºåÂåªÂ≠¶È¢ÜÂüüÁü•ËØÜÊ¨†Áº∫ÔºåÂèØ‰ª•Â∞ùËØïÂú®ÂéüÊúâÊ®°ÂûãÂü∫Á°Ä‰∏äÂä†ÂÖ•È¢ÜÂüüÁü•ËØÜÔºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ ÂêåÊó∂ÔºåÊàë‰ª¨ÈÄöÂ∏∏‰∏çÂ∏åÊúõÂ≠¶‰ºöÈ¢ÜÂüüÁü•ËØÜÁöÑÂêåÊó∂ÊçüÂ§±ÂéüÊúâÂü∫Á°ÄÊ®°ÂûãÁöÑÂÖ∂ÂÆÉËÉΩÂäõÔºåÊ≠§Êó∂LoRAÂèØ‰ª•ÂæàÂ•ΩÁöÑÊîπÂñÑËøô‰∏™ÈóÆÈ¢ò„ÄÇ Âè™ÈúÄË¶ÅÂáÜÂ§áÂ¶Ç‰∏ãÊ†ºÂºèÁöÑÂØπËØùÊï∞ÊçÆÈõÜÊîæÁΩÆÂà∞ÔºåÂêØÂä®  ËÆ≠ÁªÉÂç≥ÂèØÂæóÂà∞Êñ∞Ê®°ÂûãÊùÉÈáç„ÄÇ</p><pre><code> {\"conversations\": [{\"role\": \"user\", \"content\": \"ËØ∑ÈóÆÈ¢àÊ§éÁóÖÁöÑ‰∫∫ÊûïÂ§¥Â§öÈ´òÊâçÊúÄÂ•ΩÔºü\"}, {\"role\": \"assistant\", \"content\": \"È¢àÊ§éÁóÖÊÇ£ËÄÖÈÄâÊã©ÊûïÂ§¥ÁöÑÈ´òÂ∫¶Â∫îËØ•Ê†πÊçÆ...\"}]}\n {\"conversations\": [{\"role\": \"user\", \"content\": \"ËØ∑ÈóÆxxx\"}, {\"role\": \"assistant\", \"content\": \"xxx...\"}]}\n</code></pre><pre><code> {\"conversations\": [{\"role\": \"user\", \"content\": \"‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\"}, {\"role\": \"assistant\", \"content\": \"ÊàëÂè´minimind...\"}]}\n {\"conversations\": [{\"role\": \"user\", \"content\": \"‰Ω†ÊòØË∞Å\"}, {\"role\": \"assistant\", \"content\": \"ÊàëÊòØ...\"}]}\n</code></pre><p>Ê≠§Êó∂„ÄêÂü∫Á°ÄÊ®°Âûã+LoRAÊ®°Âûã„ÄëÂç≥ÂèØËé∑ÂæóÂåªÁñóÂú∫ÊôØÊ®°ÂûãÂ¢ûÂº∫ÁöÑËÉΩÂäõÔºåÁõ∏ÂΩì‰∫é‰∏∫Âü∫Á°ÄÊ®°ÂûãÂ¢ûÂä†‰∫ÜLoRAÂ§ñÊåÇÔºåËøô‰∏™ËøáÁ®ãÂπ∂‰∏çÊçüÂ§±Âü∫Á°ÄÊ®°ÂûãÁöÑÊú¨Ë∫´ËÉΩÂäõ„ÄÇ Êàë‰ª¨ÂèØ‰ª•ÈÄöËøáËøõË°åÊ®°ÂûãËØÑ‰º∞ÊµãËØï„ÄÇ</p><pre><code># Ê≥®ÊÑèÔºömodel_modeÂç≥ÈÄâÊã©Âü∫Á°ÄÊ®°ÂûãÁöÑÁ±ªÂûãÔºåËøôÂíåtrain_loraÊòØÂü∫‰∫éÂì™‰∏™Ê®°ÂûãËÆ≠ÁªÉÁöÑÁõ∏ÂÖ≥ÔºåÁ°Æ‰øùÁªü‰∏ÄÂç≥ÂèØ„ÄÇ\npython eval_model.py --lora_name 'lora_medical' --model_mode 2\n</code></pre><pre><code>üë∂: ÊàëÊúÄËøëÁªèÂ∏∏ÊÑüÂà∞Â§¥ÊôïÔºåÂèØËÉΩÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü\nü§ñÔ∏è: Â§¥ÊôïÂèØËÉΩÊòØÁî±‰∫éÂ§öÁßçÂéüÂõ†ÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºöÂøÉËÑèÁñæÁóÖ„ÄÅÁúºÁùõÊçü‰º§„ÄÅËøáÊïèÂèçÂ∫î„ÄÅÁöÆËÇ§ÈóÆÈ¢ò„ÄÅËøáÊïèÂèçÂ∫î„ÄÅÂéãÂäõ„ÄÅÁÑ¶Ëôë„ÄÅÁñ≤Âä≥„ÄÅËçØÁâ©ÂâØ‰ΩúÁî®„ÄÅÊÄßÂäüËÉΩÈöúÁ¢çÊàñËÄÖÂÖ∂‰ªñ...\n\n---\n\nüë∂: ‰Ω†ÂíåopenaiÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºü\nü§ñÔ∏è: ÊàëÊòØ MiniMindÔºå‰∏Ä‰∏™Áî± Jingyao Gong ÂºÄÂèëÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã„ÄÇÊàëÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÁÆóÊ≥ïËÆ≠ÁªÉÊù•‰∏éÁî®Êà∑ËøõË°å‰∫§‰∫í„ÄÇ\n</code></pre><p>PSÔºöÂè™Ë¶ÅÊúâÊâÄÈúÄË¶ÅÁöÑÊï∞ÊçÆÈõÜÔºå‰πüÂèØ‰ª•full_sftÂÖ®ÂèÇÂæÆË∞ÉÔºàÈúÄË¶ÅËøõË°åÈÄöÁî®Áü•ËØÜÁöÑÊ∑∑ÂêàÈÖçÊØîÔºåÂê¶ÂàôËøáÊãüÂêàÈ¢ÜÂüüÊï∞ÊçÆ‰ºöËÆ©Ê®°ÂûãÂèòÂÇªÔºåÊçüÂ§±ÈÄöÁî®ÊÄßÔºâ</p><h3><strong>6. ËÆ≠ÁªÉÊé®ÁêÜÊ®°Âûã (Reasoning Model)</strong></h3><p>DeepSeek-R1ÂÆûÂú®Â§™ÁÅ´‰∫ÜÔºåÂá†‰πéÈáçÊñ∞ÊåáÊòé‰∫ÜÊú™Êù•LLMÁöÑÊñ∞ËåÉÂºè„ÄÇ ËÆ∫ÊñáÊåáÂá∫ÁöÑÊ®°ÂûãÁªèÂéÜÂ§öÊ¨°ÂèçÂ§çÁöÑÂÜ∑ÂêØÂä®ÂíåRLÂ•ñÂä±ËÆ≠ÁªÉÊâçËÉΩËé∑ÂæóËÇâÁúºÂèØËßÅÁöÑÊé®ÁêÜËÉΩÂäõÊèêÂçá„ÄÇ ÊúÄÂø´ÊúÄÁ®≥Â¶•ÊúÄÁªèÊµéÁöÑÂÅöÊ≥ïÔºå‰ª•ÂèäÊúÄËøëÁàÜÂèëÁöÑÂêÑÁßçÂêÑÊ†∑ÊâÄË∞ìÁöÑÊé®ÁêÜÊ®°ÂûãÂá†‰πéÈÉΩÊòØÁõ¥Êé•Èù¢ÂêëÊï∞ÊçÆËøõË°åËí∏È¶èËÆ≠ÁªÉÔºå ‰ΩÜÁî±‰∫éÁº∫‰πèÊäÄÊúØÂê´ÈáèÔºåËí∏È¶èÊ¥æË¢´RLÊ¥æÁûß‰∏çËµ∑ÔºàhhhhÔºâ„ÄÇ Êú¨‰∫∫ËøÖÈÄüÂ∑≤ÁªèÂú®QwenÁ≥ªÂàó1.5BÂ∞èÊ®°Âûã‰∏äËøõË°å‰∫ÜÂ∞ùËØïÔºåÂæàÂø´Â§çÁé∞‰∫ÜZeroËøáÁ®ãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇ ÁÑ∂ËÄå‰∏Ä‰∏™ÈÅóÊÜæÁöÑÂÖ±ËØÜÊòØÔºöÂèÇÊï∞Â§™Â∞èÁöÑÊ®°ÂûãÁõ¥Êé•ÈÄöËøáÂÜ∑ÂêØÂä®SFT+GRPOÂá†‰πé‰∏çÂèØËÉΩËé∑Âæó‰ªª‰ΩïÊé®ÁêÜÊïàÊûú„ÄÇ MiniMind2Á¨¨‰∏ÄÊó∂Èó¥Âè™ËÉΩÂùöÂÆö‰∏çÁßªÁöÑÈÄâÊã©ÂÅöËí∏È¶èÊ¥æÔºåÊó•ÂêéÂü∫‰∫é0.1BÊ®°ÂûãÁöÑRLÂ¶ÇÊûúÂêåÊ†∑ÂèñÂæóÂ∞èÂ∞èËøõÂ±ï‰ºöÊõ¥Êñ∞Ê≠§ÈÉ®ÂàÜÁöÑËÆ≠ÁªÉÊñπÊ°à„ÄÇ</p><p>ÂÅöËí∏È¶èÈúÄË¶ÅÂáÜÂ§áÁöÑ‰æùÁÑ∂ÊòØÂíåSFTÈò∂ÊÆµÂêåÊ†∑Ê†ºÂºèÁöÑÊï∞ÊçÆÂç≥ÂèØÔºåÊï∞ÊçÆÈõÜÊù•Ê∫êÂ∑≤Â¶Ç‰∏äÊñá‰ªãÁªç„ÄÇÊï∞ÊçÆÊ†ºÂºè‰æãÂ¶ÇÔºö</p><pre><code>{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"‰Ω†Â•ΩÔºåÊàëÊòØÂ∞èËä≥ÔºåÂæàÈ´òÂÖ¥ËÆ§ËØÜ‰Ω†„ÄÇ\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"&lt;think&gt;\\n‰Ω†Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÁã¨Á´ãÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1-Lite-PreviewÔºåÂæàÈ´òÂÖ¥‰∏∫ÊÇ®Êèê‰æõÊúçÂä°ÔºÅ\\n&lt;/think&gt;\\n&lt;answer&gt;\\n‰Ω†Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÁã¨Á´ãÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1-Lite-PreviewÔºåÂæàÈ´òÂÖ¥‰∏∫ÊÇ®Êèê‰æõÊúçÂä°ÔºÅ\\n&lt;/answer&gt;\"\n    }\n  ]\n}\n</code></pre><pre><code>&lt;think&gt;\\nÊÄùËÄÉËøáÁ®ã\\n&lt;/think&gt;\\n\n&lt;answer&gt;\\nÊúÄÁªàÂõûÁ≠î\\n&lt;/answer&gt;\n</code></pre><p>ËøôÂú®GRPO‰∏≠ÈÄöËøáËÆæÁΩÆËßÑÂàôÂ•ñÂä±ÂáΩÊï∞Á∫¶ÊùüÊ®°ÂûãÁ¨¶ÂêàÊÄùËÄÉÊ†áÁ≠æÂíåÂõûÂ§çÊ†áÁ≠æÔºàÂú®ÂÜ∑ÂêØÂä®Èù†ÂâçÁöÑÈò∂ÊÆµÂ•ñÂä±ÂÄºËÆæÁΩÆÂ∫îËØ•ÊèêÈ´ò‰∏Ä‰∫õÔºâ</p><p>Âè¶‰∏Ä‰∏™ÈóÆÈ¢òÊòØËí∏È¶èËøáÁ®ãËôΩÁÑ∂ÂíåSFT‰∏ÄÊ†∑Ôºå‰ΩÜÂÆûÈ™åÁªìÊûúÊòØÊ®°ÂûãÈöæ‰ª•ÊØèÊ¨°ÈÉΩÁ¨¶ÂêàÊ®°ÊùøËßÑËåÉÁöÑÂõûÂ§çÔºåÂç≥ËÑ±Á¶ªÊÄùËÄÉÂíåÂõûÂ§çÊ†áÁ≠æÁ∫¶Êùü„ÄÇ ËøôÈáåÁöÑÂ∞èÊäÄÂ∑ßÊòØÂ¢ûÂä†Ê†áËÆ∞‰ΩçÁΩÆtokenÁöÑÊçüÂ§±ÊÉ©ÁΩöÔºåËØ¶ËßÅ:</p><pre><code># Âú® sp_ids ÂØπÂ∫îÁöÑ‰ΩçÁΩÆÂ¢ûÂä†È¢ùÂ§ñÁöÑÊÉ©ÁΩö\n...\nloss_mask[sp_ids] = 10 # ÊÉ©ÁΩöÁ≥ªÊï∞\n</code></pre><p>Âè¶Âè¶‰∏Ä‰∏™tipsÊòØÁî±‰∫éÊé®ÁêÜÊï∞ÊçÆÁî±‰∫éÂè™Á≠õÈÄâ‰∫ÜÈïøÂ∫¶ÁöÑÊï∞ÊçÆÔºåÂÖ∂‰∏≠Â§öËΩÆÂØπËØùÂíåËã±ÊñáÊï∞ÊçÆÂÅèÂ∞ëÔºå Âõ†Ê≠§ËøõË°å‰∫ÜÂ§ßÁ∫¶10kÊù°Â§öËΩÆÂØπËØù+Ëã±ÊñáÊï∞ÊçÆÁöÑÊ∑∑ÂêàÔºåÈò≤Ê≠¢Ê®°ÂûãÈÅóÂøò‰∏•Èáç„ÄÇ</p><p>ËÑöÊú¨ÈªòËÆ§Âü∫‰∫érlhfÂêéÁöÑÂü∫Ê®°ÂûãÂÅöÊé®ÁêÜËÉΩÂäõÁöÑËí∏È¶èÂæÆË∞ÉÔºå‰∏ãÈù¢Áõ¥Êé•ÂêØÂä®ËÆ≠ÁªÉÂç≥ÂèØÔºö</p><pre><code>torchrun --nproc_per_node 1 train_distill_reason.py\n# or\npython train_distill_reason.py\n</code></pre><blockquote><p>ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÈªòËÆ§ÊØèÈöî‰øùÂ≠ò‰∏∫: Ôºà*‰∏∫Ê®°ÂûãÂÖ∑‰ΩìdimensionÔºåÊØèÊ¨°‰øùÂ≠òÊó∂Êñ∞Êñá‰ª∂‰ºöË¶ÜÁõñÊóßÊñá‰ª∂Ôºâ</p></blockquote><pre><code>üë∂: ËØ∑‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇ\nü§ñÔ∏è: &lt;think&gt;\nÂóØÔºåÁî®Êà∑ËÆ©Êàë‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇÊàëÁé∞Âú®ÈúÄË¶ÅËß£Èáä‰∏Ä‰∏ãËá™Â∑±„ÄÇÈ¶ñÂÖàÔºåÊàëÂ∫îËØ•ÊòéÁ°ÆÁî®Êà∑ÊòØË∞ÅÔºåÊòØÂ≠¶Áîü„ÄÅËÅåÂú∫‰∫∫Â£´ËøòÊòØÊôÆÈÄö‰∫∫„ÄÇÁî®Êà∑ÂèØËÉΩÊòØ‰∏Ä‰∏™Â≠¶ÁîüÊàñËÄÖËÅåÂú∫‰∫∫Â£´ÔºåÊâÄ‰ª•‰ø°ÊÅØ‰º†ÈÄíÂ∫îËØ•ÊòØÂ∏ÆÂä©Â≠¶ÁîüÂíåËÅåÂú∫‰∫∫Â£´ÁöÑ‰ø°ÊÅØ„ÄÇ\nÁÑ∂ÂêéÔºåÊàëÈúÄË¶ÅÂõûÂ∫îÁî®Êà∑ÔºåËøôÂèØËÉΩÂåÖÊã¨‰ªãÁªçËá™Â∑±ÁöÑ‰ΩçÁΩÆ„ÄÅÂÖ¥Ë∂£Áà±Â•Ω„ÄÅËÅå‰∏öÁªèÂéÜÁ≠â‰ø°ÊÅØ„ÄÇÁî®Êà∑ËøòÂèØËÉΩÊÉ≥‰∫ÜËß£Ëá™Â∑±Âú®Âì™ÈáåÔºåÊØîÂ¶ÇËÅå‰∏öÂèëÂ±ï„ÄÅÂ∑•‰ΩúÁªèÈ™åÁ≠â„ÄÇ\nÊàëÂ∫îËØ•Ê≥®ÊÑèÁî®ËØçË¶ÅÊ≠£ÂºèÔºå‰∏çË¶ÅÂ§™Ê≠£ÂºèÔºåÂêåÊó∂Ë¶Å‰øùÊåÅÁÆÄÊ¥ÅÊòé‰∫Ü„ÄÇËøôÊ†∑Áî®Êà∑ÂÆπÊòìÁêÜËß£Ôºå‰πüÊñπ‰æø‰ªñ‰ª¨Êü•ÈòÖÁõ∏ÂÖ≥ËµÑÊñô„ÄÇ\nÊúÄÂêéÔºåÊàëÂ∫îËØ•ÊÄªÁªì‰∏Ä‰∏ãÔºåËÆ©Áî®Êà∑Áü•ÈÅìÊàëÊòØÂ¶Ç‰ΩïÂ∏ÆÂä©‰ªñ‰ª¨ÂÆåÊàê‰ªñ‰ª¨ÁöÑ‰ªªÂä°ÁöÑÔºåËøôÊ†∑‰ªñ‰ª¨ÊúâÊõ¥Â§öÂÖ≥‰∫éÊàëÁöÑ‰ø°ÊÅØ„ÄÇ\n&lt;/think&gt;\n&lt;answer&gt;\nÊàëÊòØ**‚Äú‰Ω†ÁöÑÂêçÂ≠ó‚ÄùÔºå‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\n&lt;/answer&gt;\n\nüë∂: ‰Ω†Êõ¥ÊìÖÈïøÂì™‰∏Ä‰∏™Â≠¶ÁßëÔºü\nü§ñÔ∏è: &lt;think&gt;\nÊÇ®Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1„ÄÇÂ¶ÇÊÇ®Êúâ‰ªª‰Ωï‰ªª‰ΩïÈóÆÈ¢òÔºåÊàë‰ºöÂ∞ΩÊàëÊâÄËÉΩ‰∏∫ÊÇ®Êèê‰æõÂ∏ÆÂä©„ÄÇ\n&lt;/think&gt;\n&lt;answer&gt;\nÊÇ®Â•ΩÔºÅÊàëÊòØÁî±‰∏≠ÂõΩÁöÑ‰∏™‰∫∫ÂºÄÂèëËÄÖÂºÄÂèëÁöÑÊô∫ËÉΩÂä©ÊâãMiniMind-R1„ÄÇÂ¶ÇÊÇ®Êúâ‰ªª‰Ωï‰ªª‰ΩïÈóÆÈ¢òÔºåÊàë‰ºöÂ∞ΩÊàëÊâÄËÉΩ‰∏∫ÊÇ®Êèê‰æõÂ∏ÆÂä©„ÄÇ\n&lt;/answer&gt;\n</code></pre><p>üìãÂÖ≥‰∫éLLMÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºåÊúâ‰∏ÄÁØáÂæàÊúâÊÑèÊÄùÁöÑËÆ∫Êñá<a href=\"https://arxiv.org/pdf/2402.14905\">MobileLLM</a>ÂÅö‰∫ÜËØ¶ÁªÜÁöÑÁ†îÁ©∂ÂíåÂÆûÈ™å„ÄÇ Scaling LawÂú®Â∞èÊ®°Âûã‰∏≠ÊúâËá™Â∑±Áã¨ÁâπÁöÑËßÑÂæã„ÄÇ ÂºïËµ∑TransformerÂèÇÊï∞ÊàêËßÑÊ®°ÂèòÂåñÁöÑÂèÇÊï∞Âá†‰πéÂè™ÂèñÂÜ≥‰∫éÂíå„ÄÇ</p><ul><li>‚Üë + ‚Üì -&gt; ÁüÆËÉñÂ≠ê</li><li>‚Üì + ‚Üë -&gt; Áò¶È´ò‰∏™</li></ul><p>2020Âπ¥ÊèêÂá∫Scaling LawÁöÑËÆ∫ÊñáËÆ§‰∏∫ÔºåËÆ≠ÁªÉÊï∞ÊçÆÈáè„ÄÅÂèÇÊï∞Èáè‰ª•ÂèäËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞ÊâçÊòØÂÜ≥ÂÆöÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÔºåËÄåÊ®°ÂûãÊû∂ÊûÑÁöÑÂΩ±ÂìçÂá†‰πéÂèØ‰ª•ÂøΩËßÜ„ÄÇ ÁÑ∂ËÄå‰ºº‰πéËøô‰∏™ÂÆöÂæãÂØπÂ∞èÊ®°ÂûãÂπ∂‰∏çÂÆåÂÖ®ÈÄÇÁî®„ÄÇ MobileLLMÊèêÂá∫Êû∂ÊûÑÁöÑÊ∑±Â∫¶ÊØîÂÆΩÂ∫¶Êõ¥ÈáçË¶ÅÔºå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁò¶Èïø„ÄçÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ÊØî„ÄåÂÆΩËÄåÊµÖ„ÄçÊ®°ÂûãÊõ¥Â§öÁöÑÊäΩË±°Ê¶ÇÂøµ„ÄÇ ‰æãÂ¶ÇÂΩìÊ®°ÂûãÂèÇÊï∞Âõ∫ÂÆöÂú®125MÊàñËÄÖ350MÊó∂Ôºå30ÔΩû42Â±ÇÁöÑ„ÄåÁã≠Èïø„ÄçÊ®°ÂûãÊòéÊòæÊØî12Â±ÇÂ∑¶Âè≥ÁöÑ„ÄåÁüÆËÉñ„ÄçÊ®°ÂûãÊúâÊõ¥‰ºòË∂äÁöÑÊÄßËÉΩÔºå Âú®Â∏∏ËØÜÊé®ÁêÜ„ÄÅÈóÆÁ≠î„ÄÅÈòÖËØªÁêÜËß£Á≠â8‰∏™Âü∫ÂáÜÊµãËØï‰∏äÈÉΩÊúâÁ±ª‰ººÁöÑË∂ãÂäø„ÄÇ ËøôÂÖ∂ÂÆûÊòØÈùûÂ∏∏ÊúâË∂£ÁöÑÂèëÁé∞ÔºåÂõ†‰∏∫‰ª•ÂæÄ‰∏∫100MÂ∑¶Âè≥ÈáèÁ∫ßÁöÑÂ∞èÊ®°ÂûãËÆæËÆ°Êû∂ÊûÑÊó∂ÔºåÂá†‰πéÊ≤°‰∫∫Â∞ùËØïËøáÂè†Âä†Ë∂ÖËøá12Â±Ç„ÄÇ Ëøô‰∏éMiniMindÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÂèÇÊï∞ÈáèÂú®Âíå‰πãÈó¥ËøõË°åË∞ÉÊï¥ÂÆûÈ™åËßÇÂØüÂà∞ÁöÑÊïàÊûúÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ ÁÑ∂ËÄå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁ™Ñ„Äç‰πüÊòØÊúâÁª¥Â∫¶ÊûÅÈôêÁöÑÔºåÂΩìd_model&lt;512Êó∂ÔºåËØçÂµåÂÖ•Áª¥Â∫¶ÂùçÂ°åÁöÑÂä£ÂäøÈùûÂ∏∏ÊòéÊòæÔºå Â¢ûÂä†ÁöÑlayersÂπ∂‰∏çËÉΩÂº•Ë°•ËØçÂµåÂÖ•Âú®Âõ∫ÂÆöq_headÂ∏¶Êù•d_head‰∏çË∂≥ÁöÑÂä£Âäø„ÄÇ ÂΩìd_model&gt;1536Êó∂ÔºålayersÁöÑÂ¢ûÂä†‰ºº‰πéÊØîd_modelÁöÑ‰ºòÂÖàÁ∫ßÊõ¥È´òÔºåÊõ¥ËÉΩÂ∏¶Êù•ÂÖ∑Êúâ‚ÄúÊÄß‰ª∑ÊØî‚ÄùÁöÑÂèÇÊï∞-&gt;ÊïàÊûúÂ¢ûÁõä„ÄÇ</p><ul><li>Âõ†Ê≠§MiniMindËÆæÂÆösmallÊ®°Âûãdim=512Ôºån_layers=8Êù•Ëé∑ÂèñÁöÑ„ÄåÊûÅÂ∞è‰ΩìÁßØ&lt;-&gt;Êõ¥Â•ΩÊïàÊûú„ÄçÁöÑÂπ≥Ë°°„ÄÇ</li><li>ËÆæÂÆödim=768Ôºån_layers=16Êù•Ëé∑ÂèñÊïàÊûúÁöÑÊõ¥Â§ßÊî∂ÁõäÔºåÊõ¥Âä†Á¨¶ÂêàÂ∞èÊ®°ÂûãScaling-LawÁöÑÂèòÂåñÊõ≤Á∫ø„ÄÇ</li></ul><p>‰Ωú‰∏∫ÂèÇËÄÉÔºåGPT3ÁöÑÂèÇÊï∞ËÆæÂÆöËßÅ‰∏ãË°®Ôºö <img src=\"https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png\" alt=\"gpt3_config.png\"></p><p>MiniMind2 Ê®°ÂûãËÆ≠ÁªÉÊçüÂ§±Ëµ∞ÂäøÔºàÁî±‰∫éÊï∞ÊçÆÈõÜÂú®ËÆ≠ÁªÉÂêéÂèàÊõ¥Êñ∞Ê∏ÖÊ¥óÂ§öÊ¨°ÔºåÂõ†Ê≠§Loss‰ªÖ‰æõÂèÇËÄÉÔºâ</p><blockquote><p>ËÄÉËôëÂà∞Â§ö‰∫∫ÂèçÂ∫îÁôæÂ∫¶ÁΩëÁõòÈÄüÂ∫¶ÊÖ¢ÔºåMiniMind2Âèä‰ª•ÂêéÂÖ®ÈÉ®‰ΩøÁî®ModelScope/HuggingFaceÊâòÁÆ°„ÄÇ</p></blockquote><p>ÊµãËØïÂü∫‰∫éÂíåÁöÑÊ®°ÂûãÂØπÊØîÔºåÊµãËØïÈöèÊú∫ÁßçÂ≠êÂùáÂõ∫ÂÆö</p><p>full_sftÊ®°ÂûãÂú®ÁÆÄÊ¥ÅÊÄßÂíå‰ø°ÊÅØÂáÜÁ°ÆÊÄßÊñπÈù¢Ë°®Áé∞Êõ¥Â•ΩÔºõrlhfÊ®°ÂûãÂú®ÂõûÁ≠î‰∏≠ÂÄæÂêë‰∫éÊèê‰æõÊõ¥Â§öÁöÑËÉåÊôØ‰ø°ÊÅØÔºå‰ΩÜ‰ø°ÊÅØÂáÜÁ°ÆÊÄßÊúâÂæÖÊîπËøõ„ÄÇ ÊÄªÁöÑÊù•ËØ¥RLHFÂêéÁöÑÊ®°ÂûãÂÄæÂêë‰∫éÂ≠¶‰π†ÔºöËØ¥Êõ¥Â§öÊúâÁ§ºË≤å‰ΩÜÊó†Áî®ÁöÑÂ∫üËØùËÆ®Â•Ω‚ÄúÂØπËØù‚ÄùÊú¨Ë∫´ÔºåËÄåÂØπ‰ø°ÊÅØÂáÜÁ°ÆÊÄßÂàôÊúâËΩªÂæÆÊçüÂ§±„ÄÇ Â§©‰∏ãÊ≤°ÊúâÂÖçË¥πÁöÑÂçàÈ§êÔºåËøòÈúÄË¶ÅÁªßÁª≠ÊèêÂçáRLHFÊï∞ÊçÆÈõÜÁöÑË¥®ÈáèÔºå‰πüË¶ÅÊé•ÂèóÊ®°ÂûãËÉΩÂäõÊó†Ê≥ïÈÅøÂÖçÁöÑÊçüÂ§±(Á®ãÂ∫¶ÊúâËΩªÈáç)„ÄÇ DPOÂíåÂú®Á∫øPPOÁöÑÂå∫Âà´Âú®‰∫érejectÂíåchosenÈÉΩÊòØÁ¶ªÁ∫øÂáÜÂ§áÁöÑÔºåÂíåminimindÊ®°ÂûãÊú¨Ë∫´ÁöÑËæìÂá∫ÂøÖÁÑ∂Â≠òÂú®ÂæàÂ§ßÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇ„ÄÇ ÈÄö‰øóÂú∞ËØ¥DPOÁÆóÊ≥ï‰ΩøÊ®°ÂûãËßÇÁúã‰πí‰πìÁêÉ‰∏ñÁïåÂÜ†ÂÜõÁöÑÊâìÊ≥ï„ÄåÂΩïÂÉè„ÄçËøõË°åRLÔºåËÄå‰∏çÊòØÂÉèPPO‰∏ÄÊ†∑ËØ∑rewardÊ®°ÂûãÂÅö„ÄåÊïôÁªÉ„ÄçÁ∫†Ê≠£Ëá™Â∑±ÁöÑÊâìÊ≥ïËøõË°åRL„ÄÇ</p><p>üèÉ‰ª•‰∏ãÊµãËØï‰∫é2025-02-09ÂÆåÊàêÔºåÊ≠§Êó•ÊúüÂêéÂèëÂ∏ÉÁöÑÊñ∞Ê®°ÂûãÔºåÊó†ÁâπÊÆäÈúÄË¶ÅÊó∂Â∞Ü‰∏çÂä†ÂÖ•ÊµãËØï„ÄÇ</p><pre><code>[Q]: ‰Ω†Áü•ÈÅìÈïøÊ±üÂêóÔºü\n[A]: ÈïøÊ±üÔºå‰Ωç‰∫é‰∏≠ÂõΩ‰∏úÂçóÈÉ®ÔºåÊòØ‰∏≠ÂõΩÁöÑÁ¨¨‰∏ÄÈïøÊ≤≥ÔºåÂÖ®Èïø6300Â§öÂÖ¨ÈáåÔºåÊµÅÁªè11‰∏™ÁúÅ‰ªΩÔºåÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºå‰πüÊòØ‰∏≠ÂõΩÁöÑ‚ÄúÊØç‰∫≤Ê≤≥‚Äù„ÄÇÈïøÊ±üÁöÑÂéÜÂè≤ÂèØ‰ª•ËøΩÊ∫ØÂà∞ÂÖ¨ÂÖÉÂâç2000Â§öÂπ¥ÁöÑÂïÜÊúùÊó∂ÊúüÔºåÂΩìÊó∂ÁöÑÂïÜÊúùÊòØÂïÜÊúùÁöÑÂêéË£î„ÄÇÂïÜÊúùÊó∂ÊúüÔºåÈïøÊ±üÁöÑÁªèÊµéÂèëÂ±ïËææÂà∞‰∫ÜÈ´òÂ≥∞ÔºåÊòØ‰∏≠ÂõΩÊúÄÈáçË¶ÅÁöÑÂÜú‰∏öÂíåÂ∑•‰∏ö‰∏≠ÂøÉ‰πã‰∏Ä„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÁªèÊµé‰∏ªË¶Å‰ª•ÂÜú‰∏ö‰∏∫‰∏ªÔºåÈïøÊ±üÊ∏î‰∏öÂç†ÊçÆ‰∫Ü‰∏ªË¶Å‰∫ß‰∏öÔºåÂ¶ÇÈíìÈ±º„ÄÅÁåéÈ±ºÁ≠â„ÄÇÈïøÊ±üÁöÑÊ∞¥ËµÑÊ∫ê‰πüÈùûÂ∏∏‰∏∞ÂØåÔºåÂåÖÊã¨Ê≤≥ÊµÅ„ÄÅÊπñÊ≥ä„ÄÅÊ≤≥ÊµÅÁ≠â„ÄÇÈïøÊ±üÁöÑËá™ÁÑ∂ÊôØËßÇ‰πüÂçÅÂàÜÂ£ÆËßÇÔºåÈïøÊ±üÂõõÂ≠£ÊôØËâ≤ÂêÑÂºÇÔºåÊò•Â≠£Â§öÈõ®ÔºåÂ§èÂ≠£Â§öÈõ®ÔºåÁßãÂ≠£ÂàôÂ§öÈõ®ÔºåÂÜ¨Â≠£ÂàôÂØíÂÜ∑Â§öÈõ™ÔºåÊò•Â≠£ÂíåÁßãÂ≠£ÈÉΩÊúâÁæé‰∏ΩÁöÑËä±ÊúµÂíåÊûúÂÆû„ÄÇÈïøÊ±üÊòØ‰∏≠ÂõΩÁöÑÁªèÊµé‰∏≠ÂøÉÔºåÊã•Êúâ‰ºóÂ§öÁöÑÂõΩÂÆ∂ÂíåÂüéÂ∏ÇÔºåÊòØ‰∏ñÁïå‰∏äÈáçË¶ÅÁöÑ‰∫§ÈÄöÊû¢Á∫ΩÂíåËà™Ëøê‰∏≠ÂøÉ„ÄÇÂêåÊó∂ÔºåÈïøÊ±üËøòÊòØ‰∏≠ÂõΩÁöÑÈáçË¶ÅÊ∞¥Âà©ËµÑÊ∫êÔºåÂØπ‰∫é‰∏≠ÂõΩÁöÑÁªèÊµéÂèëÂ±ïÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇ\n[B]: ÊòØÁöÑÔºåÊàëÁü•ÈÅìÈïøÊ±ü„ÄÇÈïøÊ±ü‰Ωç‰∫é‰∏≠ÂõΩÁöÑ‰∏≠ÈÉ®ÔºåÊòØ‰∏≠ÂõΩÁ¨¨‰∏âÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøÁ∫¶6300ÂÖ¨ÈáåÔºåÊµÅÁªèÊ±üË•øÁúÅ„ÄÅÂÆâÂæΩÁúÅ„ÄÅÊ±üËãèÁúÅ„ÄÅÂÆâÂæΩÁúÅ„ÄÅÊ±üËãèÁúÅÂíåÂÆâÂæΩÁúÅ„ÄÇ\n[C]: ÂΩìÁÑ∂ÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøÁ∫¶6300ÂÖ¨Èáå„ÄÇÂÆÉ‰ª•ÂÖ∂‰∏∞ÂØåÁöÑÊ∞¥ËµÑÊ∫ê„ÄÅ‰∏∞ÂØåÁöÑÊ∞¥ÁîüÂä®Ê§çÁâ©ËµÑÊ∫ê‰ª•ÂèäÁã¨ÁâπÁöÑÂú∞ÁêÜ‰ΩçÁΩÆËÄåÈóªÂêç‰∫é‰∏ñ„ÄÇ\n[D]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÂÖ®ÈïøË∂ÖËøá3300ÂçÉÁ±≥„ÄÇ\n[E]: ÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºåÊÄªÈïøÂ∫¶Á∫¶‰∏∫6300ÂÖ¨Èáå„ÄÇÂÆÉÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÁöÑÈùíÊµ∑ÊπñÔºåÊµÅÁªèÈùíÊµ∑„ÄÅÂõõÂ∑ù„ÄÅÁîòËÇÉ„ÄÅÂÆÅÂ§è„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•øÂíåÂ±±Ë•ø„ÄÇÈïøÊ±üÊµÅÂüüÊòØ‰∏≠ÂõΩÊúÄÂ§ßÁöÑÂÜú‰∏öÂå∫‰πã‰∏ÄÔºå‰πüÊòØ‰∏≠ÂõΩÁöÑÊñáÂåñÂíåÁªèÊµé‰∏≠ÂøÉ„ÄÇ\n[F]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅ‰πã‰∏ÄÔºå‰πüÊòØ‰∏ñÁïåÁ¨¨‰∏âÂ§ßÊ≤≥ÊµÅ„ÄÇÈïøÊ±üÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÔºåÊµÅÁªè11‰∏™ÁúÅ‰ªΩÔºåÊúÄÁªàÊ≥®ÂÖ•‰∏úÊµ∑„ÄÇÈïøÊ±üÊµÅÂüüÊòØ‰∏≠ÂõΩÊúÄÂ§ßÁöÑÊ≤≥ÊµÅÊµÅÂüü‰πã‰∏ÄÔºåÊã•ÊúâÁùÄ‰∏∞ÂØåÁöÑËá™ÁÑ∂ËµÑÊ∫êÂíåÊñáÂåñÈÅó‰∫ßÔºåÂ¶ÇÈïøÊ±üÂ§ßÊ°•„ÄÅÈïøÊ±üÊµ∑Ê¥ãÂçöÁâ©È¶Ü„ÄÅÈïøÊ±üÂè§ÂüéÁ≠â„ÄÇ\n[G]: ÂêåÊó∂‰πüÁü•ÈÅìÔºå‰∏≠ÂõΩÊòØ‰∏ñÁïå‰∏äÊúÄÈïøÁöÑÂüéÂ∏Ç‰πã‰∏ÄÔºåÊã•ÊúâË∂ÖËøá3400‰∏á‰∫∫Âè£„ÄÇ\n[H]: ÊòØÁöÑÔºåÈïøÊ±üÊòØ‰∏≠ÂõΩÊúÄÈïøÁöÑÊ≤≥ÊµÅÔºå‰πüÊòØ‰∏ñÁïå‰∏äÊúÄÈïøÁöÑÊ≤≥ÊµÅ‰πã‰∏Ä„ÄÇÂÆÉÂèëÊ∫ê‰∫éÈùíËóèÈ´òÂéüÔºåÊµÅÁªè‰πù‰∏™ÁúÅÂå∫ÔºåÊúÄÁªàÊ≥®ÂÖ•‰∏úÊµ∑„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÊ∞îÂÄôÊ∏©ÂíåÔºåÂõõÂ≠£ÂàÜÊòéÔºåÊòØ‰∏≠ÂõΩÈáçË¶ÅÁöÑÂÜú‰∏öÂíåÂ∑•‰∏öÂü∫Âú∞„ÄÇÈïøÊ±üÊµÅÂüüÁöÑÊ∞¥‰∫ßËµÑÊ∫ê‰∏∞ÂØåÔºåÊòØ‰∏≠ÂõΩÁöÑÈáçË¶ÅÊ∏î‰∏öËµÑÊ∫ê‰πã‰∏Ä„ÄÇ\n\n\n[Q]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØ‰ªÄ‰πàÔºü\n[A]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞ÔºàMount EverestÔºâÔºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÁïå‰∏ä„ÄÇÂÆÉÁöÑÊµ∑ÊãîÈ´òÂ∫¶ÊòØ8,848.86Á±≥Ôºà29,031.7Ëã±Â∞∫ÔºâÔºåÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞„ÄÇ\n[B]: Áè†Á©ÜÊúóÁéõÂ≥∞ÔºàMount EverestÔºâÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÔºåÊµ∑ÊãîÈ´òÂ∫¶Ëææ8,848Á±≥Ôºà29,029Ëã±Â∞∫Ôºâ„ÄÇ\n[C]: Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.7Ëã±Â∞∫Ôºâ„ÄÇÂÆÉ‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÁïå‰∏äÔºåÊòØÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ\n[D]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩÁöÑËæπÂ¢É‰∏äÔºåÊµ∑Êãî8848Á±≥„ÄÇ\n[E]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848Á±≥„ÄÇ\n[F]: Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÔºåÊµ∑Êãî8848Á±≥„ÄÇ\n[G]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.69Ëã±Â∞∫Ôºâ„ÄÇ\n[H]: ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØÁè†Á©ÜÊúóÁéõÂ≥∞Ôºå‰πüÁß∞‰∏∫Ëê®Âä†ÁéõÂ°îÔºå‰Ωç‰∫éÂñúÈ©¨ÊãâÈõÖÂ±±ËÑâÁöÑÂ∞ºÊ≥äÂ∞îÂíå‰∏≠ÂõΩËæπÂ¢É‰πãÈó¥ÔºåÊµ∑Êãî8,848.86Á±≥Ôºà29,031.69Ëã±Â∞∫Ôºâ„ÄÇÂÆÉÊòØ‰∏ñÁïå‰∏äÊúÄËëóÂêçÁöÑÁôªÂ±±ÁõÆÊ†á‰πã‰∏ÄÔºåÂê∏ÂºïÁùÄÊù•Ëá™‰∏ñÁïåÂêÑÂú∞ÁöÑÁôªÂ±±ËÄÖÂâçÊù•ÊåëÊàò„ÄÇÁè†Á©ÜÊúóÁéõÂ≥∞ÁöÑÂêçÂ≠óÊù•Ëá™ËóèËØ≠ÔºåÊÑèÊÄùÊòØ‚ÄúÂ•≥Á•ûÁöÑÈ¢ùÂ≥∞‚ÄùÔºåÂÆÉÊòØÁî±‰∏§‰∏™‰∏çÂêåÁöÑÂ±±Â≥∞ÁªÑÊàêÁöÑÔºåÂÖ∂‰∏≠‰∏Ä‰∏™ÊòØ‰Ωç‰∫éÂ∞ºÊ≥äÂ∞îÁöÑÁè†Á©ÜÊúóÁéõÂ≥∞ÔºåÂè¶‰∏Ä‰∏™Âàô‰Ωç‰∫é‰∏≠ÂõΩ„ÄÇÂÆÉÁöÑÈ´òÂ∫¶ÊòØÈÄöËøáÊµãÈáèÊµ∑ÊãîÈ´òÂ∫¶Êù•Á°ÆÂÆöÁöÑÔºåÂõ†‰∏∫ÂÆÉÊòØÈÄöËøáÊµãÈáèÂú∞ÁêÉÁöÑÂπ≥ÂùáÈ´òÂ∫¶Êù•Á°ÆÂÆöÁöÑ„ÄÇÂ∞ΩÁÆ°Áè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂÖ∂‰ªñÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇÂú∞ÁêÉ‰∏äÁöÑÂÖ∂‰ªñÂú∞ÊñπÔºåÂ¶ÇÂçóÁæéÊ¥≤ÁöÑÂÆâÁ¨¨ÊñØÂ±±ËÑâÔºå‰πüÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±ËÑâÔºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞„ÄÇÊÄª‰πãÔºåÁè†Á©ÜÊúóÁéõÂ≥∞ÊòØ‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞Ôºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇÂÆÉÊòØÂú∞ÁêÉ‰∏äÊúÄÈ´òÁöÑÂ±±ËÑâÔºå‰ΩÜÂÆÉÂπ∂‰∏çÊòØÂú∞ÁêÉ‰∏ä‰ªª‰ΩïÂú∞ÊñπÈÉΩËÉΩÂà∞ËææÁöÑ„ÄÇ\n\n\n[Q]: ‰∏áÊúâÂºïÂäõÊòØË∞ÅÊèêÂá∫ÁöÑÔºü\n[A]: ‰∏áÊúâÂºïÂäõÊòØÁî±ÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑÔºåÂÆÉÊòØÁâõÈ°ø‰∏âÂ§ßÂÆöÂæã‰πã‰∏Ä„ÄÇÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑÂºïÂäõÂÆöÂæãÊè≠Á§∫‰∫Ü‰∏áÊúâÂºïÂäõÂÆöÂæãÁöÑÊú¨Ë¥®ÔºåÂπ∂‰∏îËß£Èáä‰∫ÜË°åÊòü„ÄÅÂç´Êòü„ÄÅÂΩóÊòüÁ≠âÂ§©‰Ωì‰πãÈó¥Â¶Ç‰ΩïÁõ∏‰∫í‰ΩúÁî®„ÄÇËøô‰∏™ÂÆöÂæãÊòØÁî±ËâæËê®ÂÖã¬∑ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑÔºå‰ªñÈÄöËøáÂØπ‰∏áÊúâÂºïÂäõÂÆöÂæãÁöÑÂàÜÊûêÂíåÂÆûÈ™åÔºåÊàêÂäüÂú∞Ëß£Èáä‰∫ÜÂ§©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõ‰ΩúÁî®Ôºå‰ªéËÄåËß£Èáä‰∫ÜÂú∞ÁêÉÁªïÂ§™Èò≥ÊóãËΩ¨ÁöÑÁé∞Ë±°„ÄÇ\n[B]: ‰∏áÊúâÂºïÂäõÔºàHuman PolicyÔºâÊòØÁâõÈ°øÁöÑ‰∏âÂ§ßÂäõÂ≠¶ÂÆöÂæãÔºåÁî®‰∫éÊèèËø∞‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®Âäõ„ÄÇÂÆÉÁî±‰∏áÊúâÂºïÂäõÂÆöÂæãÂíåÁâõÈ°øÁöÑÁ¨¨‰∏ÄÂÆöÂæãÔºàÊÉØÊÄßÂÆöÂæãÔºâÊûÑÊàê„ÄÇ‰∏áÊúâÂºïÂäõÊòØÁî±‰∫é‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥Áõ∏‰∫í‰ΩúÁî®ËÄå‰∫ßÁîüÁöÑÔºåÂÆÉ‰ª¨ÁöÑË¥®Èáè„ÄÅË∑ùÁ¶ª„ÄÅÈÄüÂ∫¶ÂíåÊñπÂêëÁ≠âÂõ†Á¥†ÂÜ≥ÂÆö‰∫ÜÂÆÉ‰ª¨‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇËøôÁßçÁõ∏‰∫í‰ΩúÁî®ÂäõÊòØÔºöË¥®ÈáèÁöÑ‰πòÁßØÔºöË¥®ÈáèÁöÑ‰πòÁßØÊòØ‰∏áÊúâÂºïÂäõÂÆöÂæãÂíåÁâõÈ°øÁ¨¨‰∫åÂÆöÂæãÁöÑ‰πòÁßØ„ÄÇË¥®ÈáèÁöÑ‰πòÁßØÔºöË¥®ÈáèÁöÑ‰πòÁßØÊòØÁâ©‰Ωì‰πãÈó¥ÁöÑ‰∏ÄÁßçÁâ©ÁêÜËøáÁ®ãÔºåÂÆÉÊòØÁâ©‰ΩìÈó¥Áõ∏‰∫í‰ΩúÁî®ÁöÑ‰∫ßÁâ©ÔºåÂØπ‰∫éÂÖ∂‰ªñÁâ©‰ΩìÊù•ËØ¥ÔºåÂÖ∂Ë¥®ÈáèÂèØ‰ª•Ë°®Á§∫‰∏∫‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑË¥®Èáè‰πãÂíåÔºåÂç≥ÈáçÂäõÂä†ÈÄüÂ∫¶a„ÄÇ\n[C]: ‰∏áÊúâÂºïÂäõÊòØ‰∏Ä‰∏™Áî±ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑÁâ©ÁêÜÂ≠¶Ê¶ÇÂøµÔºåÂÆÉÊèèËø∞‰∫ÜÁâ©‰Ωì‰πãÈó¥ÂºïÂäõÂ¶Ç‰ΩïÁõ∏‰∫í‰ΩúÁî®ÁöÑËßÑÂæã„ÄÇ\n[D]: ‰∏áÊúâÂºïÂäõÊòØÁî±Ëã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ÁâõÈ°øÊèêÂá∫ÁöÑ„ÄÇ‰ªñËÆ§‰∏∫ÔºåÊâÄÊúâÁâ©‰ΩìÈÉΩ‰ºö‰∫ßÁîüÂºïÂäõÔºåËøô‰∏™ÂºïÂäõÊòØÁî±Áâ©‰ΩìÁöÑË¥®ÈáèÂíåË∑ùÁ¶ªÂÜ≥ÂÆöÁöÑ„ÄÇËøô‰∏™ÁêÜËÆ∫Êè≠Á§∫‰∫ÜË¥®ÈáèÂíåË∑ùÁ¶ª‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂπ∂ËØÅÊòé‰∫Ü‰∏áÊúâÂºïÂäõÁöÑÊú¨Ë¥®„ÄÇ\n[E]: ‰∏áÊúâÂºïÂäõÊòØÁî±ÁâõÈ°øÂú®1687Âπ¥ÊèêÂá∫ÁöÑ„ÄÇÂÆÉÊòØÂÆáÂÆô‰∏≠‰∏Ä‰∏™Âü∫Êú¨ÁöÑÁâ©ÁêÜÁé∞Ë±°ÔºåÂÆÉÊèèËø∞‰∫ÜÁâ©‰Ωì‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇËøô‰∏™ÂäõË¢´Áß∞‰∏∫‰∏áÊúâÂºïÂäõÔºåÂÆÉÂú®ÊâÄÊúâÁâ©‰Ωì‰πãÈó¥ÈÉΩÂ≠òÂú®ÔºåËøôÁßçÂäõ‰∏éÁâ©‰ΩìÁöÑË¥®ÈáèÂíåË∑ùÁ¶ªÊúâÂÖ≥„ÄÇ‰∏áÊúâÂºïÂäõÊòØËá™ÁÑ∂Áïå‰∏≠ÊúÄÂü∫Êú¨ÁöÑÂäõÈáè‰πã‰∏ÄÔºåÂÆÉ‰∏ç‰ªÖÂΩ±ÂìçÁùÄÁâ©‰ΩìÁöÑËøêÂä®ÂíåÂèòÂåñÔºåËøòÂØπÂÆáÂÆôÁöÑÁªìÊûÑÂíåÊºîÂåñ‰∫ßÁîüÁùÄÈáçË¶ÅÁöÑÂΩ±Âìç„ÄÇ\n[F]: ‰∏áÊúâÂºïÂäõÊòØËã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ËâæËê®ÂÖã¬∑ÁâõÈ°øÂú®17‰∏ñÁ∫™ÊèêÂá∫ÁöÑ„ÄÇ\n[G]: ÂºïÂäõÊòØÂÆáÂÆô‰∏≠‰∏ÄÁßçÂäõÈáèÔºåÂÆÉÊòØÁî±Ë¥®ÈáèÊàñËÉΩÈáèÊâÄÁªÑÊàêÁöÑ„ÄÇÊ†πÊçÆÁâõÈ°øÁöÑ‰∏áÊúâÂºïÂäõÂÆöÂæãÔºå‰∏Ä‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõÂ§ßÂ∞è‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÂèçÊØî„ÄÇÂõ†Ê≠§ÔºåÁâõÈ°øÁöÑ‰∏áÊúâÂºïÂäõÂÆöÂæãË°®ÊòéÔºå‰∏Ä‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõÂ§ßÂ∞è‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉÁöÑË¥®ÈáèÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãÂú®Áâ©ÁêÜÂ≠¶„ÄÅÂ§©ÊñáÂ≠¶ÂíåÁâ©ÁêÜÂ≠¶‰∏≠ÈÉΩÊúâÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇ\n[H]: ‰∏áÊúâÂºïÂäõÊòØÁî±Ëã±ÂõΩÁâ©ÁêÜÂ≠¶ÂÆ∂ËâæËê®ÂÖã¬∑ÁâõÈ°ø‰∫é1687Âπ¥ÊèêÂá∫ÁöÑ„ÄÇ‰ªñÂú®1687Âπ¥ÂèëË°®‰∫Ü‰∏ÄÁØáËëóÂêçÁöÑËÆ∫ÊñáÔºåÈ¢ò‰∏∫„ÄäËá™ÁÑ∂Âì≤Â≠¶ÁöÑÊï∞Â≠¶ÂéüÁêÜ„ÄãÔºåÂÖ∂‰∏≠ÊèêÂá∫‰∫Ü‰∏áÊúâÂºïÂäõÂÆöÂæãÔºåÊåáÂá∫‰ªª‰Ωï‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÈÉΩÂ≠òÂú®ÁùÄ‰∏ÄÁßçÂºïÂäõÔºåËøôÁßçÂºïÂäõÁöÑÂ§ßÂ∞è‰∏éÂÆÉ‰ª¨ÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ªÁöÑÂπ≥ÊñπÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãË¢´Áß∞‰∏∫‰∏áÊúâÂºïÂäõÂÆöÂæãÔºåÂÆÉÊèèËø∞‰∫ÜË¥®ÈáèÂíåË∑ùÁ¶ª‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂÆÉË°®ÊòéÔºå‰∏§‰∏™Áâ©‰Ωì‰πãÈó¥ÁöÑÂºïÂäõ‰∏éÂÆÉ‰ª¨ÁöÑË¥®ÈáèÊàêÊ≠£ÊØîÔºå‰∏éÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ªÁöÑÂπ≥ÊñπÊàêÂèçÊØî„ÄÇËøô‰∏™ÂÆöÂæãÂØπ‰∫éËß£ÈáäÂÆáÂÆô‰∏≠ÁöÑËÆ∏Â§öÁé∞Ë±°ÂíåÁé∞Ë±°‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®ÈùûÂ∏∏ÈáçË¶ÅÔºå‰æãÂ¶ÇË°åÊòüÁªïÂ§™Èò≥ËøêÂä®„ÄÅÊÅíÊòü‰πãÈó¥ÁöÑÂºïÂäõÈÄèÈïúÊïàÂ∫îÁ≠âÁ≠â„ÄÇÂÆÉ‰πüË¢´ÂπøÊ≥õÂ∫îÁî®‰∫éÂ§©ÊñáÂ≠¶„ÄÅÁâ©ÁêÜÂ≠¶ÂíåÂ∑•Á®ãÂ≠¶Á≠âÈ¢ÜÂüüÔºå‰∏∫Êàë‰ª¨ÁêÜËß£ÂÆáÂÆôÁöÑÊú¨Ë¥®ÂíåÊºîÂåñÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇ\n\n\n[Q]: ‰Ω†Áü•ÈÅìÂ§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØ‰ªÄ‰πàÂêóÔºü\n[A]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰ΩÜ‰πü‰ºöÂêÉÂÖ∂‰ªñÊ§çÁâ©„ÄÅÊòÜËô´„ÄÅÂ∞èÂûãÂì∫‰π≥Âä®Áâ©ÂíåÈ±ºÁ±ª„ÄÇÂÆÉ‰ª¨ÁöÑÊ∂àÂåñÁ≥ªÁªü‰∏éÂÖ∂‰ªñÁÜäÁßëÂä®Áâ©Á±ª‰ººÔºåËÉΩÂ§üÊ∂àÂåñÁ´πÂ≠ê„ÄÇÂ§ßÁÜäÁå´ÁöÑÁâôÈΩøÈîãÂà©ÔºåÈÄÇÂêàÂíÄÂöºÁ´πÂ≠êÔºåÂõ†Ê≠§ÂÆÉ‰ª¨ÁªèÂ∏∏Ë¢´Áß∞‰∏∫‚ÄúÁ´πÈ£üÂä®Áâ©‚Äù„ÄÇ\n[B]: Â§ßÁÜäÁå´ÊòØ‰ª•Á´πÂ≠ê‰∏∫È£üÁöÑÂì∫‰π≥Âä®Áâ©ÔºåÂÆÉ‰ª¨ÊòØËÇâÈ£üÂä®Áâ©„ÄÇÂÆÉ‰ª¨‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠êÔºåÁâπÂà´ÊòØÁ´πÂ≠ê‰∏≠ÂØåÂê´Á∫§Áª¥Á¥†ÂíåÁüøÁâ©Ë¥®ÔºåÂπ∂‰∏î‰∏ç‰ºöÁõ¥Êé•ËøõÂÖ•Ê∂àÂåñÁ≥ªÁªü„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅì‰∏ªË¶ÅÁî±Á´πÂ≠êÁªÑÊàêÔºåÂÆÉ‰ª¨ÈúÄË¶ÅÊ∂àÂåñÁ´πÂ≠ê‰∏≠ÁöÑÁ∫§Áª¥Á¥†ÂíåÂÖ∂‰ªñÁ∫§Áª¥Á¥†Êù•ÊîØÊåÅÂÆÉ‰ª¨ÁöÑÊ≠£Â∏∏ËøõÈ£ü„ÄÇ\n[C]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰ΩÜÂÆÉ‰ª¨‰πüÂêÉÂÖ∂‰ªñÊ§çÁâ©ÔºåÂåÖÊã¨Ê†ëÂè∂„ÄÅÂ´©ËäΩ„ÄÅÊòÜËô´„ÄÅÊ∞¥Êûú„ÄÅËî¨ËèúÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©Á≠â„ÄÇÂ§ßÁÜäÁå´ÊòØ‰∏≠ÂõΩÁâπÊúâÁöÑÁâ©ÁßçÔºåÂ±û‰∫éÁÜäÁßëÔºå‰∏ªË¶ÅÂàÜÂ∏ÉÂú®‰∏≠ÂõΩÁöÑÂõõÂ∑ù„ÄÅÈôïË•øÂíåÁîòËÇÉÁ≠âÂú∞„ÄÇ\n[D]: Â§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠ê„ÄÇÂÆÉ‰ª¨ÈÄöÂ∏∏ÂêÉÁ´πÂ≠êÔºåÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâÂ§ßÁ∫¶30ÂÖ¨Êñ§ÁöÑÁ´πÂ≠ê„ÄÇ\n[E]: ÊòØÁöÑÔºåÂ§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£ü„ÄÇÂÆÉ‰ª¨ÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâÂ§ßÈáèÁöÑÁ´πÂ≠êÊù•Áª¥ÊåÅÁîüÂëΩÔºåÊØèÂ§©ÈúÄË¶ÅÂêÉÊéâ40Ëá≥80ÂÖ¨Êñ§ÁöÑÁ´πÂ≠ê„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰ª¨‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÁöÑÊ§çÁâ©ÔºåÂ¶ÇËçâ„ÄÅÊûúÂÆû„ÄÅÂùöÊûúÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇ\n[F]: Â§ßÁÜäÁå´‰∏ªË¶Å‰ª•Á´πÂ≠ê‰∏∫È£üÔºå‰πü‰ºöÂÅ∂Â∞îÈ£üÁî®ÂÖ∂‰ªñÁöÑÊ§çÁâ©ÂíåÂä®Áâ©„ÄÇÊ≠§Â§ñÔºåÂ§ßÁÜäÁå´‰πü‰ºöÂêÉ‰∏Ä‰∫õÈáéÊûú„ÄÅÂ∞èÂä®Áâ©ÂíåÊ§çÁâ©Ôºå‰ΩÜËøô‰∫õÈÉΩ‰∏çÂ±û‰∫éÂÆÉ‰ª¨ÁöÑ‰∏ªË¶ÅÈ£üÁâ©Êù•Ê∫ê„ÄÇ\n[G]: Â§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠ê„ÄÇ\n[H]: Â§ßÁÜäÁå´ÊòØ‰∏ÄÁßç‰ª•Á´πÂ≠ê‰∏∫‰∏ªË¶ÅÈ£üÁâ©ÁöÑÊ§çÈ£üÂä®Áâ©ÔºåÂÆÉ‰ª¨ÁöÑËÇ†ÈÅìÁªìÊûÑÈÄÇÂêàÊ∂àÂåñÁ∫§Áª¥Á¥†Âê´ÈáèÈ´òÁöÑÁ´πÂ≠êÔºå‰ΩÜ‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÊ§çÁâ©ÔºåÂ¶ÇËçâÊú¨Ê§çÁâ©ÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅìÁªìÊûÑ‰πüÈÄÇÂ∫î‰∫ÜÈ´òÁ∫§Áª¥ÁöÑÁ´πÂ≠êÔºåËøô‰ΩøÂæóÂÆÉ‰ª¨ËÉΩÂ§üÊ∂àÂåñÂ§ßÈáèÁöÑÁ∫§Áª¥Á¥†Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Âú®ÈáéÂ§ñÁîüÂ≠ò„ÄÇÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅì‰∏≠ËøòÂê´Êúâ‰∏ÄÁßçÂè´ÂÅö‚ÄúÁ∫§Áª¥Á¥†ÈÖ∂‚ÄùÁöÑÈÖ∂Á±ªÁâ©Ë¥®ÔºåËøôÁßçÈÖ∂ËÉΩÂ§üÂ∏ÆÂä©Â§ßÁÜäÁå´ÂàÜËß£Á∫§Áª¥Á¥†Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Ê∂àÂåñÂ§ßÈáèÁöÑÊ∞¥ÂàÜÂíåËê•ÂÖªÁâ©Ë¥®„ÄÇÊ≠§Â§ñÔºåÂ§ßÁÜäÁå´ÁöÑËÇ†ÈÅìËøòÂê´Êúâ‰∏ÄÁßçÂè´ÂÅö‚ÄúÁ∫§Áª¥Á¥†ËõãÁôΩ‚ÄùÁöÑËõãÁôΩË¥®ÔºåËøôÁßçËõãÁôΩË¥®ËÉΩÂ§üÂ∏ÆÂä©Â§ßÁÜäÁå´Áª¥ÊåÅ‰ΩìÂÜÖÁöÑËÉΩÈáèÊ∞¥Âπ≥Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆÉ‰ª¨Âú®ÈáéÂ§ñÁîüÂ≠ò„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÂ§ßÁÜäÁå´ÁöÑ‰∏ªË¶ÅÈ£üÁâ©ÊòØÁ´πÂ≠êÔºå‰ΩÜÂÆÉ‰ª¨‰πü‰ºöÂêÉ‰∏Ä‰∫õÂÖ∂‰ªñÊ§çÁâ©ÔºåÂ¶ÇËçâÊú¨Ê§çÁâ©ÂíåÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÇ\n\n\n[Q]: Êµ∑Ê∞¥‰∏∫‰ªÄ‰πàÊòØÂí∏ÁöÑÔºü\n[A]: Êµ∑Ê∞¥‰πãÊâÄ‰ª•ÊòØÂí∏ÁöÑÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÂÖ∂‰∏≠ÁöÑÂåñÂ≠¶ÊàêÂàÜ„ÄÇÊµ∑Ê∞¥‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜÔºåÂÖ∂‰∏≠‰∏ªË¶ÅÊàêÂàÜÊòØÁ°´ÈÖ∏Áõê„ÄÅÁ¢≥ÈÖ∏Áõê„ÄÅÈïÅÁ¶ªÂ≠êÂíåÈíæÁ¶ªÂ≠ê„ÄÇËøô‰∫õÁâ©Ë¥®Âú®È´òÊµìÂ∫¶‰∏ãÔºå‰ºöÊ≤âÊ∑ÄÂú®Êµ∑Ê∞¥‰∏≠ÔºåÂΩ¢ÊàêÊµ∑Ê∞¥„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊµ∑Ê∞¥‰∏≠ÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÂ≤©Áü≥„ÄÅÂúüÂ£§„ÄÅÁîüÁâ©Âíå‰∫∫Á±ªÊéíÊ≥ÑÁâ©Á≠â„ÄÇÂ≤©Áü≥ÊòØÊµ∑Ê∞¥ÁöÑÁªÑÊàêÈÉ®ÂàÜÔºå‰∏ªË¶ÅÁî±ÁüøÁâ©Ë¥®ÁªÑÊàêÔºåÂ¶ÇÁü≥Ëã±„ÄÅÈïøÁü≥„ÄÅ‰∫ëÊØçÁ≠â„ÄÇÂúüÂ£§‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÂåñÂ≠¶Áâ©Ë¥®ÔºåÂ¶ÇÊ∞Æ„ÄÅÁ£∑„ÄÅÈíæÁ≠âÔºåËøô‰∫õÁâ©Ë¥®‰ºöÊ∏óÈÄèÂà∞ÂúüÂ£§‰∏≠ÔºåÂΩ¢ÊàêÂúüÂ£§„ÄÇÁîüÁâ©‰ΩìÂÜÖÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÊ§çÁâ©ÁöÑÂè∂ÁªøÁ¥†Á≠âËâ≤Á¥†ÔºåËøô‰∫õËâ≤Á¥†‰ºöÂê∏Êî∂Â§™Èò≥ÂÖâ‰∏≠ÁöÑËÉΩÈáèÔºåËΩ¨Âåñ‰∏∫ÁîüÁâ©‰ΩìÂèØ‰ª•Âà©Áî®ÁöÑËÉΩÈáè„ÄÇËÄå‰∫∫Á±ªÊéíÊ≥ÑÁâ©‰∏≠Âê´ÊúâÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÁüøÁâ©Ë¥®ÂíåÁ¢≥ÈÖ∏ÁõêÔºåËøô‰∫õÁüøÁâ©Ë¥®Âú®Ê∞¥‰∏≠ÁöÑÊµìÂ∫¶ËæÉÈ´òÔºå‰ºöÂØºËá¥Ê∞¥‰∏≠Ê∫∂Ëß£ÁöÑÁüøÁâ©Ë¥®ÊµìÂ∫¶Â¢ûÂä†ÔºåÂΩ¢ÊàêÊ∞¥‰Ωì‰∏≠ÁöÑÁõêÂàÜ„ÄÇÂõ†Ê≠§ÔºåÊµ∑Ê∞¥‰πãÊâÄ‰ª•ÊòØÂí∏ÁöÑÔºåÊòØÁî±‰∫éÂÖ∂‰∏≠ÁöÑÂåñÂ≠¶ÊàêÂàÜÂíåÁªìÊûÑÁâπÂæÅ„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁüøÁâ©Ë¥®ÂíåÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÂ≤©Áü≥„ÄÅÂúüÂ£§„ÄÅÁîüÁâ©Âíå‰∫∫Á±ªÊéíÊ≥ÑÁâ©Á≠âÔºåËÄåËøô‰∫õÁâ©Ë¥®ÁöÑÁßØÁ¥ØÂíåÊéíÊ≥Ñ‰ºöÂØºËá¥Êµ∑Ê∞¥ÂëàÁé∞Âá∫Âí∏Âë≥„ÄÇ\n[B]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂåÖÂê´‰∫ÜÂ§öÁßçÁâ©Ë¥®ÔºåÂåÖÊã¨Ê∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÅÊ∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÅÁ¢≥ÈÖ∏ÁõêÔºàÂ¶ÇÁ°´ÈÖ∏ÁõêÔºâ„ÄÅÊ∞ØÂåñÈí†ÔºàNaClÔºâÂíåÊ∞¥„ÄÇÂí∏Áâ©Ë¥®ÁöÑÊù•Ê∫êÔºöÁõêÊòØÂí∏ÁöÑÂ∏∏ËßÅÊù•Ê∫êÔºåÂê´ÊúâÈíæ„ÄÅÈíô„ÄÅÈïÅ„ÄÅÈíæÁ≠âÁüøÁâ©Ë¥®„ÄÇËøô‰∫õÁüøÁâ©Ë¥®ÈÄöÂ∏∏Êù•Ëá™‰∫éÂú∞‰∏ãÊ∞¥„ÄÅÂú∞‰∏ãÊ∞¥ÊàñÂúüÂ£§‰∏≠ÔºåËøô‰∫õÁüøÁâ©Ë¥®ÁöÑÂê´ÈáèËøú‰Ωé‰∫éÁ∫ØÂáÄÊ∞¥„ÄÇÂí∏Âë≥ÁöÑÂéüÂõ†ÔºöÁõêÁöÑÂí∏Âë≥‰∏ªË¶ÅÊ∫ê‰∫éÂÖ∂Ê∫∂Ëß£‰∫éÊ∞¥‰∏≠ÁöÑÂæÆÂ∞èÈ¢óÁ≤íÁâ©„ÄÇËøô‰∫õÂ∞èÈ¢óÁ≤íÁâ©‰∏ªË¶ÅÈÄöËøáÊ§çÁâ©Ê†πÈÉ®ÂíåÂä®Áâ©‰ΩìÂÜÖÁöÑÂæÆÁîüÁâ©‰º†Êí≠ÔºåÂ¶ÇÊòÜËô´„ÄÅÂ∞èÂûãÂì∫‰π≥Âä®Áâ©„ÄÅÈ∏üÁ±ªÂíåÊüê‰∫õÊòÜËô´„ÄÇÁªº‰∏äÊâÄËø∞ÔºåÊµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂê´ÊúâÂ§ßÈáèÁöÑÁõêÁ±ªÔºåÂåÖÊã¨ÁüøÁâ©Ë¥®„ÄÅÂ≤©Áü≥„ÄÅÁõê„ÄÅÁüøÁâ©Ë¥®Á≠âÔºåËøô‰∫õÁâ©Ë¥®Âú®Êµ∑Ê∞¥‰∏≠ÁöÑÊµìÂ∫¶È´òÔºåÂØπÂí∏Âë≥ÁöÑ‰∫ßÁîüËæÉÂ§ßÂΩ±Âìç„ÄÇ\n[C]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâÂêÑÁßçÁõêÂàÜÔºåÂÖ∂‰∏≠‰ª•Êµ∑Ê∞¥‰∏∫‰∏ªË¶ÅÊàêÂàÜ‰πã‰∏Ä„ÄÇÂΩìÊµ∑Ê∞¥‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÁõêÂàÜÊó∂ÔºåÁõêÂàÜ‰ºöË¢´Ê∫∂Ëß£Âú®Êµ∑Ê∞¥‰∏≠ÔºåÂΩ¢ÊàêÂí∏Âë≥„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ê∫ê‰∫éÊ∫∂Ëß£ÁõêÁ±ª„ÄÅËí∏ÂèëÊûêÂá∫ÁöÑÁõêÂíåÊµ∑Ê∞¥ÂÜªÁªìÊàêÂÜ∞Á≠âÂõ†Á¥†„ÄÇ\n[D]: Êµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÁõêÂàÜÔºåËøô‰∫õÁõêÂàÜÂèØ‰ª•ÂØºËá¥Êµ∑Ê∞¥Âí∏Â∫¶ÁöÑÂ¢ûÂä†„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏ªË¶ÅÊù•Ëá™‰∫éÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÔºåËøô‰∫õÁõêÂàÜÂèØ‰ª•‰ªéÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜ‰∏≠Ëé∑Âæó„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÊ∞ßÂåñÁâ©ÂíåÁõêÂàÜÂèØ‰ª•‰∏éÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÂΩ¢ÊàêÊ∞¢ÈîÆÔºå‰ªéËÄå‰ΩøÊµ∑Ê∞¥Âí∏Â∫¶Â¢ûÂä†„ÄÇ\n[E]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåËøôÊòØÂõ†‰∏∫ÂÆÉÂê´ÊúâÂ§ßÈáèÁöÑÁõêÂàÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨Ê∞ØÁ¶ªÂ≠êÂíåÊ∞ØÁ¶ªÂ≠ê„ÄÇËøô‰∫õÁ¶ªÂ≠êÂú®Êµ∑Ê¥ã‰∏≠ÂΩ¢Êàê‰∫ÜÂ§ßÈáèÁöÑÁõêÔºåËøô‰∫õÁõêÂàÜË¢´Áß∞‰∏∫ÁõêÂàÜ„ÄÇÊµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£Âú®Ê∞¥‰∏≠ÁöÑÁõêÔºåËøô‰∫õÁõêÂàÜ‰ºöÈöèÁùÄÊµ∑Ê∞¥ÁöÑÂçáÈ´òËÄåÈÄêÊ∏êËí∏ÂèëÔºåÊúÄÁªàÂØºËá¥Êµ∑Ê∞¥‰∏≠ÁõêÂàÜÁöÑËí∏Âèë„ÄÇËøô‰∫õÁõêÂàÜÂú®Êµ∑Ê∞¥‰∏≠Ë¢´ÂÆåÂÖ®Ê∂àËÄóÔºåÁÑ∂ÂêéË¢´ÊéíÈô§Âú®Êµ∑Ê∞¥‰∏≠„ÄÇÊ≠§Â§ñÔºåÊµ∑Ê∞¥‰∏≠ËøòÂê´ÊúâËÆ∏Â§öÂÖ∂‰ªñÁöÑÁõêÂàÜÔºå‰ΩÜËøô‰∫õÁõêÂàÜÊòØ‰∏çÂêåÁöÑÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Âú®Ê∞¥‰∏≠ÁöÑÊµÆËµ∑Âíå‰∏ãÈôçËøáÁ®ãÊòØ‰∏çÂêåÁöÑ„ÄÇÊµ∑Ê∞¥‰∏≠ÁöÑÁõêÂàÜÊòØ‰ªéÊµ∑Â∫ïÁöÑÂ≤©Áü≥‰∏≠ÊèêÂèñÂá∫Êù•ÁöÑÔºåËøô‰∫õÁõêÂàÜÂú®Êµ∑Ê∞¥‰∏≠ÂèØ‰ª•Ë¢´Âà©Áî®„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÊµ∑Ê∞¥ÊòØÂí∏ÁöÑÔºåËøô‰πüÊòØ‰∏∫‰ªÄ‰πàÊµ∑Ê∞¥‰∏≠Âê´ÊúâËÆ∏Â§öÁõêÂàÜÂíåÁõêÂàÜÁöÑÂéüÂõ†„ÄÇ\n[F]: Êµ∑Ê∞¥Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£ÁöÑÁõêÁ±ªÔºåÂÖ∂‰∏≠ÊúÄ‰∏ªË¶ÅÁöÑÊòØÊ∞ØÂåñÈí†„ÄÇÂΩìÊ∞¥‰∏≠Âä†ÂÖ•ÁõêÊó∂ÔºåÁõêÁöÑÂàÜÂ≠ê‰ºö‰∏éÊ∞¥ÂàÜÂ≠êÂèëÁîüÂèçÂ∫îÔºåÂΩ¢ÊàêÊ∞¢Ê∞ßÂåñÈí†ÔºåËøôÊòØ‰∏ÄÁßçÁõê„ÄÇ\n[G]: Âõ†‰∏∫Ê∞¥ÊòØËÆ∏Â§öÁîüÁâ©ÁöÑÁîüÂ≠òÊâÄÂøÖÈúÄÁöÑÔºåÂÆÉËÉΩÂ§üÂê∏Êî∂ÂíåÈáäÊîæ‰∫åÊ∞ßÂåñÁ¢≥ÔºåÈáäÊîæÊ∞ßÊ∞îÔºåÁª¥ÊåÅÂ§ßÊ∞î‰∏≠ÁöÑÊ∞¥Ëí∏Ê∞îÔºå‰ª•ÂèäÁª¥ÊåÅÊ∞¥Âæ™ÁéØ„ÄÇÊ∞¥‰∏≠ÁöÑÁõêÁ±ªÂíåÂåñÂ≠¶Áâ©Ë¥®‰ºöÈáäÊîæÂá∫Ê∞ßÊ∞îÔºå‰ΩøÂú∞ÁêÉ‰∏äÁöÑÁîüÁâ©ÂëºÂê∏„ÄÇÊ≠§Â§ñÔºåÊ∞¥ËøòÂÖ∑ÊúâËÆ∏Â§öÂÖ∂‰ªñÂäüËÉΩÔºåÂ¶Ç‰øùÊåÅÊ∞¥Âæ™ÁéØ„ÄÅÁª¥ÊåÅÊ∞¥Âæ™ÁéØ„ÄÅË∞ÉËäÇÊ∞îÂÄôÂíåÁéØÂ¢ÉÁ≠â„ÄÇÊ∞¥ÊòØËÆ∏Â§öÁîüÁâ©ÁîüÂ≠òÊâÄÂøÖÈúÄÁöÑÔºåÂÆÉ‰πüÊòØËÆ∏Â§öËá™ÁÑ∂Áé∞Ë±°ÁöÑÈáçË¶ÅÂü∫Á°Ä„ÄÇ\n[H]: Êµ∑Ê∞¥ÊòØÂí∏ÁöÑ‰∏ªË¶ÅÂéüÂõ†ÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâÂ§ßÈáèÁöÑÊ∫∂Ëß£ÁõêÁ±ªÔºåÂÖ∂‰∏≠ÊúÄ‰∏ªË¶ÅÁöÑÊòØÊ∞ØÂåñÈí†ÔºàNaClÔºâ„ÄÇÊ∞ØÂåñÈí†ÊòØ‰∏ÄÁßçÈùûÂ∏∏Â∏∏ËßÅÁöÑÁõêÁ±ªÔºåÂÆÉÂ≠òÂú®‰∫éÂú∞ÁêÉ‰∏äÁöÑÂ§ßÈÉ®ÂàÜÂú∞Âå∫ÔºåÂåÖÊã¨Êµ∑Ê¥ã„ÄÅÊπñÊ≥ä„ÄÅÊ≤≥ÊµÅÂíåÂú∞‰∏ãÊ∞¥„ÄÇÂΩìÊ∞¥‰ªéÊµ∑Ê¥ã‰∏≠Ëí∏ÂèëÊó∂ÔºåÂÆÉ‰ºöÂ∏¶Ëµ∞‰∏ÄÈÉ®ÂàÜÁõêÂàÜÔºå‰ΩøÊµ∑Ê∞¥ÂèòÂæóÂí∏„ÄÇÊ≠§Â§ñÔºåÊµ∑‰∏≠ËøòÂê´ÊúâÂÖ∂‰ªñÊ∫∂Ëß£ÁõêÁ±ªÁöÑÁâ©Ë¥®ÔºåÂ¶ÇÁ°´ÈÖ∏Áõê„ÄÅÁ°´ÈÖ∏ÈïÅ„ÄÅÁ°´ÈÖ∏Èíô„ÄÅÁ°´ÈÖ∏ÈïÅÁõê„ÄÅÁ°´ÈÖ∏ÈïÅÈíæ„ÄÅÁ°´ÈÖ∏ÈïÅÈí†„ÄÅÁ°´ÈÖ∏ÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈí†„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈïÅÈíô„ÄÅÁ°´ÈÖ∏ÈïÅÈïÅÈïÅÈïÅ\n\n\n[Q]: ÂÜô‰∏ÄÊÆµpythonÁöÑÂø´ÈÄüÊéíÂ∫è‰ª£Á†Å\n[A]: Â•ΩÁöÑÔºå‰ª•‰∏ãÊòØPythonÁöÑÂø´ÈÄüÊéíÂ∫è‰ª£Á†ÅÔºödef quick_sort(arr):    if len(arr) &lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# ÊµãËØï‰ª£Á†Åarr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))ËøôÊÆµ‰ª£Á†ÅÈ¶ñÂÖàÊ£ÄÊü•Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫éÁ≠â‰∫é1„ÄÇÂ¶ÇÊûúÊòØÔºåÈÇ£‰πàÂÆÉÂ∞±ÊåâÁÖß‰ªéÂ∞èÂà∞Â§ßÁöÑÈ°∫Â∫èÊéíÂàó„ÄÇÊé•ÁùÄÔºåÂØπ‰∫éÊï∞ÁªÑÁöÑÊØè‰∏™ÂÖÉÁ¥†ÔºåÂÆÉÊÉ≥Ë¶ÅËøõË°åÈÄíÂΩíÂú∞ÊéíÂ∫èÔºåÁõ¥Âà∞ËææÂà∞ÊéíÂ∫èÁöÑÁªìÊùü‰ΩçÁΩÆ„ÄÇÂ¶ÇÊûúÊï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂ•áÊï∞ÔºåÈÇ£‰πàÂÆÉÂ∞±ÊåâÁÖß‰ªéÂ§ßÂà∞Â∞èÁöÑÈ°∫Â∫èÊéíÂàó„ÄÇÊúÄÂêéÔºåÂπ≥ÂùáÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÊòØO(n log n)ÔºåÂÖ∂‰∏≠nÊòØÊï∞ÁªÑÁöÑÈïøÂ∫¶„ÄÇ\n[B]: ‰ª•‰∏ãÊòØ‰∏Ä‰∏™‰ΩøÁî®PythonÁöÑÂø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÁöÑÂÆûÁé∞Ôºödef sort_array(arr):    # ÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ÊéíÂàó    sorted_array = sorted(arr)    # ËøîÂõûÊéíÂ∫èÂêéÁöÑÂàóË°®    return sorted(arr)# Á§∫‰æãarr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# Ë∞ÉÁî®ÂáΩÊï∞sorted_array = sort_array(arr)print(\"ÂéüÂßãÊï∞ÁªÑÊòØÔºö\", sorted_array)ËøôÊÆµ‰ª£Á†ÅÈ¶ñÂÖàÁîüÊàê‰∏Ä‰∏™ÈöèÊú∫ÊéíÂàóÔºåÁÑ∂ÂêéÈÄöËøá‰∏Ä‰∏™ÂàóË°®ÁöÑÈïøÂ∫¶ËøõË°åÊéíÂ∫è„ÄÇÊúÄÂêéÔºåËøîÂõûÊéíÂ∫èÂêéÁöÑÂàóË°®„ÄÇÊ≥®ÊÑèÔºåËøô‰∏™‰ª£Á†Å‰ΩøÁî®‰∫ÜPythonÁöÑÂÜÖÁΩÆsortedÂáΩÊï∞ÂíåÂÜÖÁΩÆÁöÑsorted()ÂáΩÊï∞ÔºåÂÆÉ‰ª¨Âú®Êó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫O(n log n)ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊØîÁõ¥Êé•‰ΩøÁî®sorted()ÂáΩÊï∞Êõ¥Âø´„ÄÇ\n[C]: ‰ª•‰∏ãÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊéíÂ∫èÁÆóÊ≥ïÔºåÈÄÇÁî®‰∫éÂ∞èÂûãÊï∞ÁªÑÔºåÂåÖÊã¨Â∞è‰∫éÁ≠â‰∫é5ÁöÑÊï∞ÁªÑ„ÄÇdef quick_sort(arr):    if len(arr) &lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# Á§∫‰æãarr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print(\"Sorted array is:\", sorted_arr)### ‰ª£Á†ÅËß£ÈáäÔºö1. **ÂáΩÊï∞ÂÆö‰πâ**Ôºöquick_sort ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™Êï∞ÁªÑ arr ‰Ωú‰∏∫ÂèÇÊï∞„ÄÇ2. **Âæ™ÁéØ**ÔºöÂØπ‰∫é arr ÁöÑÂÖÉÁ¥†Ôºå‰ΩøÁî®ÈÄíÂΩíÊù•ÂÆûÁé∞ÔºåÂç≥‰ªéÂè≥Âà∞Â∑¶ÈÅçÂéÜÊï∞ÁªÑÔºåÂπ∂Â∞ÜÂΩìÂâçÂÖÉÁ¥†‰∏éÊï∞ÁªÑÁöÑÂΩìÂâçÂÖÉÁ¥†‰∫§Êç¢‰ΩçÁΩÆÔºåÁõ¥Âà∞Êï∞ÁªÑËææÂà∞Á©∫ÊàñÊï∞ÁªÑÈïøÂ∫¶Â∞è‰∫éÁ≠â‰∫é arr„ÄÇ3. **ÈÄíÂΩíË∞ÉÁî®**ÔºöÈÅçÂéÜÊï∞ÁªÑÔºåÂ∞ÜÂΩìÂâçÂÖÉÁ¥†‰∏éÊï∞ÁªÑÁöÑÂΩìÂâçÂÖÉÁ¥†ËøõË°å‰∫§Êç¢ÔºåÁõ¥Âà∞Êï∞ÁªÑËææÂà∞Á©∫ÊàñÊï∞ÁªÑÈïøÂ∫¶Â∞è‰∫éÁ≠â‰∫é arr„ÄÇ4. **Ê£ÄÊü•Êï∞ÁªÑÈïøÂ∫¶**Ôºö‰ΩøÁî® if __name__ == \"__main__\" Ê£ÄÊü•Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫é arrÔºåÂ¶ÇÊûúÊòØÔºåÂ∞±Â∞ÜÊï∞ÁªÑ‰ªéËæÉÂ§ßÁöÑÊï∞ÁªÑÁßªÂä®Âà∞Êï∞ÁªÑÁöÑÊú´Â∞æ„ÄÇ5. **ËæìÂá∫ÊéíÂ∫èÂêéÁöÑÁªìÊûú**\n[D]: ‰Ωú‰∏∫‰∏Ä‰∏™AIËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÊó†Ê≥ïÁºñÂÜôPythonÁ®ãÂ∫è„ÄÇ‰ΩÜÊòØÔºåPythonÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÊµÅË°åÁöÑÁºñÁ®ãËØ≠Ë®ÄÔºåÂÆÉÂÖ∑ÊúâËÆ∏Â§ö‰ºòÁßÄÁöÑÂ∫ìÂíåÊ°ÜÊû∂ÔºåÂåÖÊã¨NumPy„ÄÅPandas„ÄÅMatplotlib„ÄÅPandas„ÄÅBandke„ÄÅNormÂíåNKlearnÁ≠â„ÄÇPythonÊúâÂæàÂ§ö‰ºòÁßÄÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫ìÂíåÊ°ÜÊû∂ÔºåÂèØ‰ª•Áî®‰∫éÂ§ÑÁêÜÂíåÂàÜÊûêÂêÑÁßçÁ±ªÂûãÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ë„ÄÅËßÜÈ¢ëÁ≠â„ÄÇPythonËøòÂèØ‰ª•Áî®‰∫éÂàõÂª∫ÂêÑÁßçÂ∫îÁî®Á®ãÂ∫èÔºåÂ¶ÇÁΩëÈ°µÊêúÁ¥¢„ÄÅÊ∏∏ÊàèÂºÄÂèë„ÄÅÁßëÂ≠¶ËÆ°ÁÆóÁ≠â„ÄÇPythonÈùûÂ∏∏ÁÅµÊ¥ªÔºå‰πüÊîØÊåÅÂ§öÁßçÁºñÁ®ãÈ£éÊ†ºÔºåÂõ†Ê≠§ÂèØ‰ª•ËΩªÊùæÂú∞ÊâæÂà∞ÈÄÇÂêàËá™Â∑±ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\n[E]: ‰Ωú‰∏∫AIËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÂèØ‰ª•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÇ‰ΩÜÊòØÊàëÊó†Ê≥ïËøõË°åÁºñÁ®ãÊìç‰ΩúÔºåÂõ†‰∏∫ÊàëÊó†Ê≥ïÁ°ÆÂÆöÊÇ®ÊÉ≥ÈóÆÁöÑÊòØÂì™ÁßçÁºñÁ®ãËØ≠Ë®Ä„ÄÇËØ∑ÊÇ®Êèê‰æõÊõ¥ÂÖ∑‰ΩìÁöÑÈóÆÈ¢òÔºå‰ª•‰æøÊàëËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÇ\n[F]: Âø´ÈÄüÊéíÂ∫èÔºàQuick SortÔºâÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÊéíÂ∫èÁÆóÊ≥ïÔºåÂÆÉÈÄöËøáÂ∞ÜÊï∞ÊçÆÂàíÂàÜ‰∏∫Â∞èÁªÑÔºåÊØèÁªÑÊï∞ÊçÆÊåâÁÖß‰∏ÄÂÆöËßÑÂàôÊéíÂ∫èÔºåÊúÄÁªàÂæóÂà∞‰∏Ä‰∏™ÊúâÂ∫èÊï∞ÁªÑ„ÄÇ\n[G]: Â•ΩÁöÑÔºå‰ª•‰∏ãÊòØ‰∏ÄÊÆµÂø´ÈÄüÊéíÂ∫èÁöÑPython‰ª£Á†Å```python sorted_print(sorted(sorted(sorted(sorted_print))```ËæìÂá∫ÁªìÊûú‰∏∫Ôºö```[1, 2, 3, 4, 5, 6, 7, 8, 9]```\n[H]: Âø´ÈÄüÊéíÂ∫èÊòØ‰∏ÄÁßçÂü∫‰∫éÂàÜÊ≤ªÊÄùÊÉ≥ÁöÑÈ´òÊïàÊéíÂ∫èÁÆóÊ≥ïÔºåÂÆÉÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ O(n log n)„ÄÇÂÆÉÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÊòØÈÄöËøá‰∏ÄË∂üÊéíÂ∫èÂ∞ÜÂæÖÊéíÂ∫èÂ∫èÂàóÂàíÂàÜ‰∏∫‰∏§‰∏™Â≠êÂ∫èÂàóÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Â≠êÂ∫èÂàóÁöÑÊâÄÊúâÂÖÉÁ¥†ÈÉΩÊØîÂè¶‰∏Ä‰∏™Â≠êÂ∫èÂàóÁöÑÊâÄÊúâÂÖÉÁ¥†Â∞èÔºåÁÑ∂ÂêéÂØπËøô‰∏§‰∏™Â≠êÂ∫èÂàóÂàÜÂà´ÈÄíÂΩíÂú∞ËøõË°åÊéíÂ∫è„ÄÇ‰∏ãÈù¢ÊòØ‰∏Ä‰∏™ Python ‰ª£Á†ÅÁ§∫‰æãÔºåÂÆûÁé∞‰∫ÜÂø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÔºö```pythondef quick_sort(arr):    if len(arr) &gt; 1:        return arr    else: return quick_sort(arr)```ËØ•‰ª£Á†Å‰ΩøÁî®‰∫ÜÈÄíÂΩíÁöÑÊñπÂºèÔºåÈ¶ñÂÖàÂà§Êñ≠Êï∞ÁªÑÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫éÁ≠â‰∫é1ÔºåÂ¶ÇÊûúÊòØÔºåÂàôÁõ¥Êé•ËøîÂõûÊï∞ÁªÑÔºåÂê¶ÂàôÈÄíÂΩíÂú∞ÂØπÊï∞ÁªÑËøõË°åÊéíÂ∫è„ÄÇÂú®ÈÄíÂΩíËøáÁ®ã‰∏≠ÔºåÊàë‰ª¨È¶ñÂÖàÂà§Êñ≠Êï∞ÁªÑÁöÑÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÊòØÂê¶Â§ß‰∫éÁ≠â‰∫é1ÔºåÂ¶ÇÊûúÊòØÔºåÂàôÂ∞ÜÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏éÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ËøõË°åÊØîËæÉÔºåÂ¶ÇÊûú‰∏çÁ≠â‰∫é1ÔºåÂàôÂ∞ÜÊï∞ÁªÑ‰∏≠ÁöÑÂÖÉÁ¥†‰∏éÁ¨¨‰∫å‰∏™ÂÖÉÁ¥†ËøõË°åÊØîËæÉÔºå‰ª•Ê≠§Á±ªÊé®ÔºåÁõ¥Âà∞ÊâÄÊúâÂÖÉÁ¥†ÈÉΩË¢´ÊéíÂ∫è„ÄÇËØ•‰ª£Á†ÅÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ O(n log n)ÔºåÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÂÆûÁî®ÁöÑÊéíÂ∫èÁÆóÊ≥ï„ÄÇ\n\n\n[Q]: Êà™Ëá≥2025Âπ¥2Êúà9Êó•14Êó∂31ÂàÜÔºåÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ¥ØËÆ°Á•®ÊàøÔºàÂê´È¢ÑÂîÆÔºâÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåË∂ÖËøá„ÄäÊòüÁêÉÂ§ßÊàòÔºöÂ§©Ë°åËÄÖÂ¥õËµ∑„ÄãÁöÑÁ•®ÊàøÊàêÁª©ÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±„ÄÇËøô‰∏ÄÊàêÂ∞±‰∏ç‰ªÖÊ†áÂøóÁùÄ‰∏≠ÂõΩÁîµÂΩ±Âú®ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÁ™ÅÁ†¥Ôºå‰πüÂ±ïÁ§∫‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ‰ªéËøôÊÆµÊñáÂ≠óÊÄªÁªì20Â≠óÁöÑÊëòË¶ÅÁªôÊàë„ÄÇ\n[A]: ÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ•®ÊàøÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±„ÄÇËøôÊ†áÂøóÁùÄ‰∏≠ÂõΩÁîµÂΩ±ÁöÑÂõΩÈôÖÁ™ÅÁ†¥Ôºå‰πüÂ±ïÁé∞‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ\n[B]: 2025Âπ¥2Êúà9Êó•Ôºå‰∏≠ÂõΩÁîµÂΩ±Âú®‰∏ñÁïåËàûÂè∞‰∏äÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåË∂ÖËøá„ÄäÊòüÁêÉÂ§ßÊàòÔºöÂ§©Ë°åËÄÖÂ¥õËµ∑„ÄãÁöÑÁ•®ÊàøÊàêÁª©„ÄÇ\n[C]: ÁîµÂΩ±„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ•®ÊàøÁ™ÅÁ†¥78.09‰∫øÂÖÉÔºåÊàê‰∏∫ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±ÔºåÂ±ïÁé∞‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ\n[D]: „ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÁ¥ØËÆ°Á•®ÊàøÁ™ÅÁ†¥78.09‰∫øÔºåÊàê‰∏∫È¶ñÈÉ®ËøõÂÖ•ÂÖ®ÁêÉÁ•®ÊàøÊ¶úÂâç40ÁöÑ‰∫öÊ¥≤ÁîµÂΩ±ÔºåÂ±ïÁ§∫‰∫Ü‰∏≠ÂõΩÁîµÂΩ±Âú®ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÁ™ÅÁ†¥ÔºåÂêåÊó∂Â±ïÁ§∫‰∫Ü‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ\n[E]: „ÄäÁåõÈæôÂº∫ÔºöÊâìÂºÄ‰∏ñÁïåÂ•áÂπªÂÜíÈô©„ÄãÊàêÂäüÂú∞Ôºå‰∏äÊµ∑ÊàêÂäüÂê∏Âºï‰∫Ü‰ºóÂ§öËßÇ‰ºóÔºåÂêåÊó∂‰πüÂºïËµ∑‰∫ÜÂπøÊ≥õÁöÑÂÖ≥Ê≥®„ÄÇ\n[F]: „ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÊòØ‰∏ÄÈÉ®ËÆ©‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±Êàê‰∏∫ÂõΩÈôÖÂ∏ÇÂú∫ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇËØ•ÁîµÂΩ±‰ª•ÂÖ∂Âçé‰∏ΩÁöÑÁîªÈù¢ÂíåÊÉäÈô©ÁöÑÊàòÊñóÂú∫Èù¢ËÄåÈóªÂêç‰∫é‰∏ñ„ÄÇ\n[G]: „ÄäÂì™Ë°åÊòØÂÄºÂæóÁöÑ„ÄãÊòØ2021Âπ¥1Êúà9Êó•14Êó∂31ÂàÜÔºåÁîµÂΩ±„ÄäÂì™ÊàêÂ§ß‰Ωú‰∏∫„ÄãÂú®ÁîµÂΩ±Â∏ÇÂú∫Á™ÅÁ†¥7800‰∫øÁæéÂÖÉÔºåÊàê‰∏∫‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÁ™ÅÁ†¥ÔºåÂπ∂Â±ïÁ§∫‰∏≠ÂõΩÂä®ÁîªÁîµÂΩ±ÁöÑÂ∑®Â§ßÊΩúÂäõÂíåÂ∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ\n[H]: ÊàëÊòØ‰∏Ä‰∏™ÂèÇÊï∞ÂæàÂ∞ëÁöÑAIÊ®°Âûãü•∫ÔºåÁü•ËØÜÂ∫ìËæÉÂ∞ëÔºåÊó†Ê≥ïÁõ¥Êé•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢òÔºåÊç¢‰∏™ÈóÆÈ¢òËØïËØïÂêßüëã\n</code></pre><p>üôã‚ÄçÁõ¥Êé•Êää‰ª•‰∏äÊâÄÊúâÈóÆÈ¢òÂíåÊ®°ÂûãÁöÑÂõûÁ≠î‰∏¢ÁªôDeepSeek-R1ÔºåËÆ©ÂÆÉÂ∏ÆÂøôÁÇπËØÑÂíåÊéíÂêçÊâìÂàÜÔºö</p><table><thead><tr></tr></thead><tbody></tbody></table><p>‰∏™‰∫∫‰∏ªËßÇËØÑ‰ª∑‰∏éDeepSeek-R1Âü∫Êú¨Áõ∏Á¨¶ÔºåÂÖ∂‰∏≠Ôºö</p><ul><li><p>MiniMindÁ≥ªÂàóÁöÑÊéíÂ∫èÈùûÂ∏∏Á¨¶ÂêàÁõ¥ËßâÔºåÂèÇÊï∞Ë∂äÂ§ß+ËÆ≠ÁªÉÊï∞ÊçÆË∂äÂÖÖÂàÜËØÑÂàÜË∂äÈ´òÔºåÂπªËßâÂíåÈîôËØØÈÉΩ‰ºöÊØîÂ∞èÊ®°ÂûãËÇâÁúºÂèØËßÅÁöÑÂ•Ω„ÄÇ</p></li><li><p>HÊ®°ÂûãÁöÑÂõûÁ≠îËÇâÁúºÁúãËµ∑Êù•ÊòØ‰∏çÈîôÁöÑÔºåÂ∞ΩÁÆ°Â≠òÂú®‰∫õËÆ∏ÂπªËßâÁûéÁºñÁöÑÊÉÖÂÜµ„ÄÇ</p></li><li><p>GÊ®°ÂûãÂèØËÉΩËÆ≠ÁªÉÊï∞ÊçÆ‰∏çÂ§üÂÆåÂ§áÔºåÁªôÂá∫ÁöÑÊùÉÈáçÁªèËøáÊµãËØïÊïàÊûú‰∏ç‰Ω≥„ÄÇ</p></li><li><p>ÂÜçÂ§çËØµ‰∏ÄÈÅçÁªè‰πÖ‰∏çË°∞ÁöÑScaling Law: ÂèÇÊï∞Ë∂äÂ§ßÔºåËÆ≠ÁªÉÊï∞ÊçÆË∂äÂ§öÊ®°ÂûãÁöÑÊÄßËÉΩË∂äÂº∫„ÄÇ</p></li></ul><p>‰∏ãÈù¢Â∞±Âà∞ÂñúÈóª‰πêËßÅÁöÑbenchmarkÂà∑Ê¶úÊµãËØïÁéØËäÇÔºåÂ∞±‰∏çÊâæ‰πêÂ≠êÂíåqwen„ÄÅglmÁ∫ßÂà´ÁöÑ‰∏≠ÊñáÊ®°ÂûãÂÅöÂØπÊØî‰∫Ü„ÄÇ ËøôÈáåÈÄâÂèñ‰∫Ü‰∏Ä‰∫õ&lt;1BÁöÑÂæÆÂûãÊ®°ÂûãËøõË°åÊ®™ËØÑÊØîËæÉÔºå ÊµãËØïÈõÜÈÄâÊã©C-Eval„ÄÅCMMLU„ÄÅA-CLUE„ÄÅTMMLU+ËøôÂá†‰∏™Á∫Ø‰∏≠ÊñáËØ≠Ë®ÄÊ¶úÂçï„ÄÇ</p><p>PS: Âú®ËøôÁßçÂÖ®ÊòØÈÄâÊã©È¢òÁöÑÊµãËØÑÈõÜ‰∏≠Ôºå‰∏∫‰∫ÜÈÅøÂÖçÂõûÂ§çÊ†ºÂºèÁöÑÈöæ‰ª•Âõ∫ÂÆöÁöÑÁâπÁÇπÔºå ÊâÄ‰ª•Â∏∏Áî®ÂÅöÊ≥ïÊòØÁõ¥Êé•Êää,,,Âõõ‰∏™Â≠óÊØçÂØπÂ∫îtokenÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂèñÂá∫Êù•ÔºåÂ∞ÜÂÖ∂‰∏≠Ê¶ÇÁéáÊúÄÂ§ßÁöÑÂ≠óÊØç‰∏éÊ†áÂáÜÁ≠îÊ°àËÆ°ÁÆóÊ≠£Á°ÆÁéá„ÄÇ ÈÄâÊã©È¢ò1/4‰π±ÈÄâÁöÑÊ≠£Á°ÆÁéáÊòØ25%ÔºåÁÑ∂ËÄåËøô‰∏™ÈáèÁ∫ßÁöÑÊâÄÊúâÊ®°ÂûãÈÉΩÈõÜ‰∏≠Âú®25ÈôÑËøëÔºåÁîöËá≥ÂæàÂ§öÊó∂ÂÄô‰∏çÂ¶ÇÁûéÈÄâÔºåÊòØ‰∏çÊòØÂÉèÊûÅ‰∫ÜÈ´ò‰∏≠ÂÆåÂΩ¢Â°´Á©∫ÁöÑÊªëÈìÅÂç¢Ê≠£Á°ÆÁéá... MiniMindÊ®°ÂûãÊú¨Ë∫´È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂ∞èÁöÑÂèØÊÄúÔºå‰πüÊ≤°ÊúâÈíàÂØπÊÄßÁöÑÂØπÊµãËØïÈõÜÂÅöÂà∑Ê¶úÂæÆË∞ÉÔºåÂõ†Ê≠§ÁªìÊûúÂõæ‰∏Ä‰πêÂç≥ÂèØÔºö</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><ul><li><pre><code>&lt;MiniMind-Model-Name&gt; (root dir)\n‚îú‚îÄ&lt;MiniMind-Model-Name&gt;\n|  ‚îú‚îÄ‚îÄ config.json\n|  ‚îú‚îÄ‚îÄ generation_config.json\n|  ‚îú‚îÄ‚îÄ LMConfig.py\n|  ‚îú‚îÄ‚îÄ model.py\n|  ‚îú‚îÄ‚îÄ pytorch_model.bin\n|  ‚îú‚îÄ‚îÄ special_tokens_map.json\n|  ‚îú‚îÄ‚îÄ tokenizer_config.json\n|  ‚îú‚îÄ‚îÄ tokenizer.json\n</code></pre></li><li><pre><code>python serve_openai_api.py\n</code></pre></li><li><pre><code>python chat_openai_api.py\n</code></pre></li><li><pre><code>curl http://ip:port/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{ \n    \"model\": \"model-identifier\",\n    \"messages\": [ \n      { \"role\": \"user\", \"content\": \"‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±ÊòØ‰ªÄ‰πàÔºü\" }\n    ], \n    \"temperature\": 0.7, \n    \"max_tokens\": 512,\n    \"stream\": true\n}'\n</code></pre></li></ul><blockquote><p>[!NOTE] Â¶ÇÊûúËßâÂæóÂØπÊÇ®ÊúâÊâÄÂ∏ÆÂä©ÔºåÂèØ‰ª•Âú® GitHub ‰∏äÂä†‰∏Ä‰∏™‚≠ê ÁØáÂπÖË∂ÖÈïøÊ∞¥Âπ≥ÊúâÈôêÈöæÂÖçÁ∫∞ÊºèÔºåÊ¨¢ËøéÂú®Issues‰∫§ÊµÅÊåáÊ≠£ÊàñÊèê‰∫§PRÊîπËøõÈ°πÁõÆ</p></blockquote><a href=\"https://github.com/jingyaogong/minimind/stargazers\"></a><a href=\"https://github.com/jingyaogong/minimind/network/members\"></a>","contentLength":51878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"exo-explore/exo","url":"https://github.com/exo-explore/exo","date":1739931998,"author":"","guid":4528,"unread":true,"content":"<p>Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö</p><p>Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!</p><div align=\"center\"><h2>Update: exo is hiring. See <a href=\"https://exolabs.net\">here</a> for more details.</h2><h2>Interested in running exo in your business? <a href=\"mailto:hello@exolabs.net\">Contact us</a> to discuss.</h2></div><p>exo is  software. Expect bugs early on. Create issues so they can be fixed. The <a href=\"https://x.com/exolabs\">exo labs</a> team will strive to resolve issues quickly.</p><p>We also welcome contributions from the community. We have a list of bounties in <a href=\"https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing\">this sheet</a>.</p><p>exo supports different models including LLaMA (<a href=\"https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py\">MLX</a> and <a href=\"https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py\">tinygrad</a>), Mistral, LlaVA, Qwen, and Deepseek.</p><h3>Dynamic Model Partitioning</h3><p>exo <a href=\"https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py\">optimally splits up models</a> based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.</p><h3>Automatic Device Discovery</h3><p>Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices <a href=\"https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161\">connect p2p</a>. As long as a device is connected somewhere in the network, it can be used to run models.</p><p>The current recommended way to install exo is from source.</p><ul><li>For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): \n  </li></ul><ul><li>The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: \n  <ul><li>1 x 16GB NVIDIA RTX 4070 Ti Laptop</li><li>2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini</li></ul></li><li>exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.</li></ul><pre><code>git clone https://github.com/exo-explore/exo.git\ncd exo\npip install -e .\n# alternatively, with venv\nsource install.sh\n</code></pre><ul><li>If running on Mac, MLX has an <a href=\"https://ml-explore.github.io/mlx/build/html/install.html\">install guide</a> with troubleshooting steps.</li></ul><ul><li>There are a number of things users have empirically found to improve performance on Apple Silicon Macs:</li></ul><ol><li>Upgrade to the latest version of macOS Sequoia.</li><li>Run . This runs commands to optimize GPU memory allocation on Apple Silicon Macs.</li></ol><h3>Example Usage on Multiple macOS Devices</h3><p>That's it! No configuration required - exo will automatically discover the other device(s).</p><pre><code>curl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llama-3.2-3b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}],\n     \"temperature\": 0.7\n   }'\n</code></pre><pre><code>curl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llama-3.1-405b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}],\n     \"temperature\": 0.7\n   }'\n</code></pre><pre><code>curl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"deepseek-r1\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}],\n     \"temperature\": 0.7\n   }'\n</code></pre><h4>Llava 1.5 7B (Vision Language Model):</h4><pre><code>curl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llava-1.5-7b-hf\",\n     \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What are these?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n            }\n          }\n        ]\n      }\n    ],\n     \"temperature\": 0.0\n   }'\n</code></pre><h3>Example Usage on Multiple Heterogenous Devices (macOS + Linux)</h3><p>Note: We don't need to explicitly tell exo to use the  inference engine.  and  are interoperable!</p><p>Linux devices will automatically default to using the  inference engine.</p><p>You can read about tinygrad-specific env vars <a href=\"https://docs.tinygrad.org/env_vars/\">here</a>. For example, you can configure tinygrad to use the cpu by specifying .</p><h3>Example Usage on a single device with \"exo run\" command</h3><pre><code>exo run llama-3.2-3b --prompt \"What is the meaning of exo?\"\n</code></pre><p>Models by default are stored in .</p><p>You can set a different model storage location by setting the  env var.</p><p>Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the  directory.</p><p>To download models from a proxy endpoint, set the  environment variable. For example, to run exo with the huggingface mirror endpoint:</p><pre><code>HF_ENDPOINT=https://hf-mirror.com exo\n</code></pre><p>Enable debug logs with the DEBUG environment variable (0-9).</p><p>For the  inference engine specifically, there is a separate DEBUG flag  that can be used to enable debug logs (1-6).</p><p>We use <a href=\"https://github.com/google/yapf\">yapf</a> to format the code. To format the code, first install the formatting requirements:</p><pre><code>pip3 install -e '.[formatting]'\n</code></pre><p>Then run the formatting script:</p><ul><li>On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the  command, typicall as follows:</li></ul><pre><code>/Applications/Python 3.x/Install Certificates.command\n</code></pre><ul><li>üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email <a href=\"mailto:alex@exolabs.net\">alex@exolabs.net</a> with your GitHub username explaining your use-case and you will be granted access on GitHub.</li></ul><p>exo supports the following inference engines:</p>","contentLength":5833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"danielmiessler/fabric","url":"https://github.com/danielmiessler/fabric","date":1739931998,"author":"","guid":4529,"unread":true,"content":"<p>fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.</p><blockquote><ul><li>Remember that  supports  and  models, but you need to 1) not use , and 2) use the  flag because the o1 and o3 models don't support the  option or temperature settings.</li></ul></blockquote><p>Since the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but <em>it's not easy to integrate this functionality into our lives.</em></p><div align=\"center\"><h4>In other words, AI doesn't have a capabilities problem‚Äîit has an  problem.</h4></div><p>Fabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.</p><p>Keep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current <a href=\"https://raw.githubusercontent.com/danielmiessler/fabric/main/#Installation\">install instructions</a> below.</p><blockquote><p>AI isn't a thing; it's a  of a thing. And that thing is .</p></blockquote><p>We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the  problems we want to solve.</p><h3>Breaking problems into components</h3><p>Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.</p><img width=\"2078\" alt=\"augmented_challenges\" src=\"https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06\"><p>Prompts are good for this, but the biggest challenge I faced in 2023‚Äî‚Äîwhich still exists today‚Äîis <strong>the sheer number of AI prompts out there</strong>. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, <em>and manage different versions of the ones we like</em>.</p><p>One of 's primary features is helping people collect and integrate prompts, which we call , into various parts of their lives.</p><p>Fabric has Patterns for all sorts of life and work activities, including:</p><ul><li>Extracting the most interesting parts of YouTube videos and podcasts</li><li>Writing an essay in your own voice with just an idea as an input</li><li>Summarizing opaque academic papers</li><li>Creating perfectly matched AI art prompts for a piece of writing</li><li>Rating the quality of content to see if you want to read/watch the whole thing</li><li>Getting summaries of long, boring content</li><li>Turning bad documentation into usable documentation</li><li>Creating social media posts from any content input</li></ul><p>To install Fabric, you can use the latest release binaries or install it from the source.</p><h3>Get Latest Release Binaries</h3><p><code>https://github.com/danielmiessler/fabric/releases/latest/download/fabric-windows-amd64.exe</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-arm64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-amd64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-amd64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-arm64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><pre><code># Install Fabric directly from the repo\ngo install github.com/danielmiessler/fabric@latest\n</code></pre><p>You may need to set some environment variables in your  on linux or  file on mac to be able to run the  command. Here is an example of what you can add:</p><p>For Intel based macs or linux</p><pre><code># Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\n\n# Update PATH to include GOPATH and GOROOT binaries\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n</code></pre><p>for Apple Silicon based macs</p><pre><code># Golang environment variables\nexport GOROOT=$(brew --prefix go)/libexec\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n</code></pre><p>Now run the following command</p><pre><code># Run the setup to set up your directories and keys\nfabric --setup\n</code></pre><p>If everything works you are good to go.</p><h3>Add aliases for all patterns</h3><p>In order to add aliases for all your patterns and use them directly as commands ie.  instead of <code>fabric --pattern summarize</code> You can add the following to your  or  file.</p><pre><code># Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in $HOME/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Create an alias in the form: alias pattern_name=\"fabric --pattern pattern_name\"\n    alias_command=\"alias $pattern_name='fabric --pattern $pattern_name'\"\n\n    # Evaluate the alias command to add it to the current shell\n    eval \"$alias_command\"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\n</code></pre><p>You can add the below code for the equivalent aliases inside PowerShell by running  inside a PowerShell window:</p><pre><code># Path to the patterns directory\n$patternsPath = Join-Path $HOME \".config/fabric/patterns\"\nforeach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {\n    $patternName = $patternDir.Name\n\n    # Dynamically define a function for each pattern\n    $functionDefinition = @\"\nfunction $patternName {\n    [CmdletBinding()]\n    param(\n        [Parameter(ValueFromPipeline = `$true)]\n        [string] `$InputObject,\n\n        [Parameter(ValueFromRemainingArguments = `$true)]\n        [String[]] `$patternArgs\n    )\n\n    begin {\n        # Initialize an array to collect pipeline input\n        `$collector = @()\n    }\n\n    process {\n        # Collect pipeline input objects\n        if (`$InputObject) {\n            `$collector += `$InputObject\n        }\n    }\n\n    end {\n        # Join all pipeline input into a single string, separated by newlines\n        `$pipelineContent = `$collector -join \"`n\"\n\n        # If there's pipeline input, include it in the call to fabric\n        if (`$pipelineContent) {\n            `$pipelineContent | fabric --pattern $patternName `$patternArgs\n        } else {\n            # No pipeline input; just call fabric with the additional args\n            fabric --pattern $patternName `$patternArgs\n        }\n    }\n}\n\"@\n    # Add the function to the current session\n    Invoke-Expression $functionDefinition\n}\n\n# Define the 'yt' function as well\nfunction yt {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory = $true)]\n        [string]$videoLink\n    )\n    fabric -y $videoLink --transcript\n}\n</code></pre><p>This also creates a  alias that allows you to use <code>yt https://www.youtube.com/watch?v=4b0iet22VIk</code> to get transcripts, comments, and metadata.</p><h4>Save your files in markdown using aliases</h4><p>If in addition to the above aliases you would like to have the option to save the output to your favourite markdown note vault like Obsidian then instead of the above add the following to your  or  file:</p><pre><code># Define the base directory for Obsidian notes\nobsidian_base=\"/path/to/obsidian\"\n\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in ~/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Unalias any existing alias with the same name\n    unalias \"$pattern_name\" 2&gt;/dev/null\n\n    # Define a function dynamically for each pattern\n    eval \"\n    $pattern_name() {\n        local title=\\$1\n        local date_stamp=\\$(date +'%Y-%m-%d')\n        local output_path=\\\"\\$obsidian_base/\\${date_stamp}-\\${title}.md\\\"\n\n        # Check if a title was provided\n        if [ -n \\\"\\$title\\\" ]; then\n            # If a title is provided, use the output path\n            fabric --pattern \\\"$pattern_name\\\" -o \\\"\\$output_path\\\"\n        else\n            # If no title is provided, use --stream\n            fabric --pattern \\\"$pattern_name\\\" --stream\n        fi\n    }\n    \"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\n</code></pre><p>This will allow you to use the patterns as aliases like in the above for example  instead of <code>fabric --pattern summarize --stream</code>, however if you pass in an extra argument like this <code>summarize \"my_article_title\"</code> your output will be saved in the destination that you set in <code>obsidian_base=\"/path/to/obsidian\"</code> in the following format <code>YYYY-MM-DD-my_article_title.md</code> where the date gets autogenerated for you. You can tweak the date format by tweaking the  format.</p><p>If you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.</p><pre><code># Uninstall Legacy Fabric\npipx uninstall fabric\n\n# Clear any old Fabric aliases\n(check your .bashrc, .zshrc, etc.)\n# Install the Go version\ngo install github.com/danielmiessler/fabric@latest\n# Run setup for the new version. Important because things have changed\nfabric --setup\n</code></pre><p>The great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.</p><pre><code>go install github.com/danielmiessler/fabric@latest\n</code></pre><p>Once you have it all set up, here's how to use it.</p><pre><code>\nUsage:\n  fabric [OPTIONS]\n\nApplication Options:\n  -p, --pattern=             Choose a pattern from the available patterns\n  -v, --variable=            Values for pattern variables, e.g. -v=#role:expert -v=#points:30\"\n  -C, --context=             Choose a context from the available contexts\n      --session=             Choose a session from the available sessions\n  -a, --attachment=          Attachment path or URL (e.g. for OpenAI image recognition messages)\n  -S, --setup                Run setup for all reconfigurable parts of fabric\n  -t, --temperature=         Set temperature (default: 0.7)\n  -T, --topp=                Set top P (default: 0.9)\n  -s, --stream               Stream\n  -P, --presencepenalty=     Set presence penalty (default: 0.0)\n  -r, --raw                  Use the defaults of the model without sending chat options (like temperature etc.) and use the user role instead of the system role for patterns.\n  -F, --frequencypenalty=    Set frequency penalty (default: 0.0)\n  -l, --listpatterns         List all patterns\n  -L, --listmodels           List all available models\n  -x, --listcontexts         List all contexts\n  -X, --listsessions         List all sessions\n  -U, --updatepatterns       Update patterns\n  -c, --copy                 Copy to clipboard\n  -m, --model=               Choose model\n  -o, --output=              Output to file\n      --output-session       Output the entire session (also a temporary one) to the output file\n  -n, --latest=              Number of latest patterns to list (default: 0)\n  -d, --changeDefaultModel   Change default model\n  -y, --youtube=             YouTube video \"URL\" to grab transcript, comments from it and send to chat\n      --transcript           Grab transcript from YouTube video and send to chat (it used per default).\n      --comments             Grab comments from YouTube video and send to chat\n      --metadata             Grab metadata from YouTube video and send to chat\n  -g, --language=            Specify the Language Code for the chat, e.g. -g=en -g=zh\n  -u, --scrape_url=          Scrape website URL to markdown using Jina AI\n  -q, --scrape_question=     Search question using Jina AI\n  -e, --seed=                Seed to be used for LMM generation\n  -w, --wipecontext=         Wipe context\n  -W, --wipesession=         Wipe session\n      --printcontext=        Print context\n      --printsession=        Print session\n      --readability          Convert HTML input into a clean, readable view\n      --serve                Initiate the API server\n      --dry-run              Show what would be sent to the model without actually sending it\n      --version              Print current version\n\nHelp Options:\n  -h, --help                 Show this help message\n\n</code></pre><h2>Our approach to prompting</h2><p>Fabric  are different than most prompts you'll see.</p><ul><li><strong>First, we use  to help ensure maximum readability and editability</strong>. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. <em>Importantly, this also includes the AI you're sending it to!</em></li></ul><p>Here's an example of a Fabric Pattern.</p><pre><code>https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\n</code></pre><img width=\"1461\" alt=\"pattern-example\" src=\"https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d\"><ul><li><p><strong>Next, we are extremely clear in our instructions</strong>, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.</p></li><li><p><strong>And finally, we tend to use the System section of the prompt almost exclusively</strong>. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.</p></li></ul><blockquote><p>The following examples use the macOS  to paste from the clipboard. See the <a href=\"https://raw.githubusercontent.com/danielmiessler/fabric/main/#pbpaste\">pbpaste</a> section below for Windows and Linux alternatives.</p></blockquote><p>Now let's look at some things you can do with Fabric.</p><ol><li>Run the  Pattern based on input from . In this case, the body of an article.</li></ol><pre><code>pbpaste | fabric --pattern summarize\n</code></pre><ol start=\"2\"><li>Run the  Pattern with the  option to get immediate and streaming results.</li></ol><pre><code>pbpaste | fabric --stream --pattern analyze_claims\n</code></pre><ol start=\"3\"><li>Run the  Pattern with the  option to get immediate and streaming results from any Youtube video (much like in the original introduction video).</li></ol><pre><code>fabric -y \"https://youtube.com/watch?v=uXs-zPc63kM\" --stream --pattern extract_wisdom\n</code></pre><ol start=\"4\"><li><p>Create patterns- you must create a .md file with the pattern and save it to <code>~/.config/fabric/patterns/[yourpatternname]</code>.</p></li><li><p>Run a  pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.</p></li></ol><pre><code>fabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims\n</code></pre><img width=\"1173\" alt=\"fabric-patterns-screenshot\" src=\"https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8\"><p>If you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the <a href=\"https://github.com/danielmiessler/fabric/tree/main/patterns\"></a> directory and start exploring!</p><p>We hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.</p><p>You can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.</p><p>The wisdom of crowds for the win.</p><p>You may want to use Fabric to create your own custom Patterns‚Äîbut not share them with others. No problem!</p><p>Just make a directory in <code>~/.config/custompatterns/</code> (or wherever) and put your  files in there.</p><p>When you're ready to use them, copy them into:</p><pre><code>~/.config/fabric/patterns/\n</code></pre><p>You can then use them like any other Patterns, but they won't be public unless you explicitly submit them as Pull Requests to the Fabric project. So don't worry‚Äîthey're private to you.</p><p>Fabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:</p><p> is a helper command that converts LaTeX files to PDF format. You can use it like this:</p><p>This will create a PDF file from the input LaTeX file in the same directory.</p><p>You can also use it with stdin which works perfectly with the  pattern:</p><pre><code>echo \"ai security primer\" | fabric --pattern write_latex | to_pdf\n</code></pre><p>This will create a PDF file named  in the current directory.</p><p>To install , install it the same way as you install Fabric, just with a different repo name.</p><pre><code>go install github.com/danielmiessler/fabric/plugins/tools/to_pdf@latest\n</code></pre><p>Make sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as  requires  to be available in your system's PATH.</p><p>The <a href=\"https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples\">examples</a> use the macOS program  to paste content from the clipboard to pipe into  as the input.  is not available on Windows or Linux, but there are alternatives.</p><p>On Windows, you can use the PowerShell command  from a PowerShell command prompt. If you like, you can also alias it to . If you are using classic PowerShell, edit the file <code>~\\Documents\\WindowsPowerShell\\.profile.ps1</code>, or if you are using PowerShell Core, edit <code>~\\Documents\\PowerShell\\.profile.ps1</code> and add the alias,</p><pre><code>Set-Alias pbpaste Get-Clipboard\n</code></pre><p>On Linux, you can use <code>xclip -selection clipboard -o</code> to paste from the clipboard. You will likely need to install  with your package manager. For Debian based systems including Ubuntu,</p><pre><code>sudo apt update\nsudo apt install xclip -y\n</code></pre><p>You can also create an alias by editing  or  and adding the alias,</p><pre><code>alias pbpaste='xclip -selection clipboard -o'\n</code></pre><p>Fabric now includes a built-in web interface that provides a GUI alternative to the command-line interface and an out-of-the-box website for those who want to get started with web development or blogging. You can use this app as a GUI interface for Fabric, a ready to go blog-site, or a website template for your own projects.</p><p>The  directory includes starter  and  directories, allowing you to open up the  directory as an <a href=\"https://obsidian.md\">Obsidian.md</a> vault. You can place your posts in the posts directory when you're ready to publish.</p><p>The GUI can be installed by navigating to the&nbsp;&nbsp;directory and using&nbsp;,&nbsp;, or your favorite package manager. Then simply run&nbsp;the development server to start the app.</p><p><em>You will need to run fabric in a separate terminal with the&nbsp;&nbsp;command.</em></p><p><strong>From the fabric project  directory:</strong></p><pre><code>npm run dev\n\n## or ##\n\npnpm run dev\n\n## or your equivalent\n</code></pre><p>To run the Streamlit user interface:</p><pre><code># Install required dependencies\npip install streamlit pandas matplotlib seaborn numpy python-dotenv\n\n# Run the Streamlit app\nstreamlit run streamlit.py\n</code></pre><p>The Streamlit UI provides a user-friendly interface for:</p><ul><li>Running and chaining patterns</li><li>Creating and editing patterns</li><li>Analyzing pattern results</li></ul><blockquote><p>[!NOTE] Special thanks to the following people for their inspiration and contributions!</p></blockquote><ul><li> for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!</li><li> for pushing me over the edge of whether to make this a public project or not.</li><li> and  for their invaluable contributions to the Go version</li><li> for his work on the web interface.</li><li> for super useful input on the project's Github directory structure..</li><li> for the idea of a  context flag that adds pre-created context in the  directory to all Pattern queries.</li><li> for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using  before sending on to  for analysis.</li><li> for assisting with numerous components to make things simpler and more maintainable.</li></ul>","contentLength":18058,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"spf13/cobra","url":"https://github.com/spf13/cobra","date":1739931998,"author":"","guid":4530,"unread":true,"content":"<p>A Commander for modern Go CLI interactions</p><p>Cobra is a library for creating powerful modern CLI applications.</p><p>Cobra is a library providing a simple interface to create powerful modern CLI interfaces similar to git &amp; go tools.</p><ul><li>Easy subcommand-based CLIs: , , etc.</li><li>Fully POSIX-compliant flags (including short &amp; long versions)</li><li>Global, local and cascading flags</li><li>Intelligent suggestions (... did you mean ?)</li><li>Automatic help generation for commands and flags</li><li>Grouping help for subcommands</li><li>Automatic help flag recognition of , , etc.</li><li>Automatically generated shell autocomplete for your application (bash, zsh, fish, powershell)</li><li>Automatically generated man pages for your application</li><li>Command aliases so you can change things without breaking them</li><li>The flexibility to define your own help, usage, etc.</li><li>Optional seamless integration with <a href=\"https://github.com/spf13/viper\">viper</a> for 12-factor apps</li></ul><p>Cobra is built on a structure of commands, arguments &amp; flags.</p><p> represent actions,  are things and  are modifiers for those actions.</p><p>The best applications read like sentences when used, and as a result, users intuitively know how to interact with them.</p><p>The pattern to follow is <code>APPNAME VERB NOUN --ADJECTIVE</code> or <code>APPNAME COMMAND ARG --FLAG</code>.</p><p>A few good real world examples may better illustrate this point.</p><p>In the following example, 'server' is a command, and 'port' is a flag:</p><p>In this command we are telling Git to clone the url bare.</p><p>Command is the central point of the application. Each interaction that the application supports will be contained in a Command. A command can have children commands and optionally run an action.</p><p>In the example above, 'server' is the command.</p><p>A flag is a way to modify the behavior of a command. Cobra supports fully POSIX-compliant flags as well as the Go <a href=\"https://golang.org/pkg/flag/\">flag package</a>. A Cobra command can define flags that persist through to children commands and flags that are only available to that command.</p><p>In the example above, 'port' is the flag.</p><p>Flag functionality is provided by the <a href=\"https://github.com/spf13/pflag\">pflag library</a>, a fork of the flag standard library which maintains the same interface while adding POSIX compliance.</p><p>Using Cobra is easy. First, use  to install the latest version of the library.</p><pre><code>go get -u github.com/spf13/cobra@latest\n</code></pre><p>Next, include Cobra in your application:</p><pre><code>import \"github.com/spf13/cobra\"\n</code></pre><p> is a command line program to generate cobra applications and command files. It will bootstrap your application scaffolding to rapidly develop a Cobra-based application. It is the easiest way to incorporate Cobra into your application.</p><p>It can be installed by running:</p><pre><code>go install github.com/spf13/cobra-cli@latest\n</code></pre><p>Cobra is released under the Apache 2.0 license. See <a href=\"https://raw.githubusercontent.com/spf13/cobra/main/LICENSE.txt\">LICENSE.txt</a></p>","contentLength":2613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AykutSarac/jsoncrack.com","url":"https://github.com/AykutSarac/jsoncrack.com","date":1739931998,"author":"","guid":4531,"unread":true,"content":"<p>‚ú® Innovative and open-source visualization application that transforms various data formats, such as JSON, YAML, XML, CSV and more, into interactive graphs.</p><img width=\"100%\" alt=\"booking-screen\" src=\"https://raw.githubusercontent.com/AykutSarac/jsoncrack.com/main/public/assets/editor.webp\"><h2>Visualize JSON into interactive graphs</h2><p>JSON Crack is a tool for visualizing JSON data in a structured, interactive graphs, making it easier to explore, format, and validate JSON. It offers features like converting JSON to other formats (CSV, YAML), generating JSON Schema, executing queries, and exporting visualizations as images. Designed for both readability and usability.</p><ul><li>: Instantly convert JSON, YAML, CSV, XML, and TOML into interactive graphs or trees in dark or light mode.</li><li>: Seamlessly transform data formats, like JSON to CSV or XML to JSON, for easy sharing.</li><li>: Beautify and validate JSON, YAML, and CSV for clear and accurate data.</li><li>: Generate TypeScript interfaces, Golang structs, and JSON Schema.</li><li>: Create JSON Schema, mock data, and validate various data formats.</li><li>: Decode JWT, randomize data, and run jq or JSON path queries.</li><li>: Download your visualization as PNG, JPEG, or SVG.</li><li>: All data processing is local; nothing is stored on our servers.</li></ul><a href=\"https://news.ycombinator.com/item?id=32626873\"><img width=\"250\" height=\"54\" alt=\"Featured on Hacker News\" src=\"https://hackernews-badge.vercel.app/api?id=32626873\"></a><p>JSON Crack officially launched as v1.0 on the 17th of February 2022 and we've come a long way so far. Watch  of this repository to be notified of future updates:</p><p>To get a local copy up and running, please follow these simple steps.</p><p>Here is what you need to be able to run JSON Crack.</p><ul><li>Node.js (Version: &gt;=18.x)</li></ul><ol><li><pre><code>pnpm dev\n\n# Running on http://localhost:3000/\n</code></pre></li></ol><p>üê≥ A <a href=\"https://raw.githubusercontent.com/AykutSarac/jsoncrack.com/main/Dockerfile\"></a> is provided in the root of the repository. If you want to run JSON Crack locally:</p><pre><code># Build a Docker image with:\ndocker compose build\n\n# Run locally with `docker-compose`\ndocker compose up\n\n# Go to http://localhost:8888\n</code></pre>","contentLength":1686,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"All-Hands-AI/OpenHands","url":"https://github.com/All-Hands-AI/OpenHands","date":1739931998,"author":"","guid":4532,"unread":true,"content":"<p>üôå OpenHands: Code Less, Make More</p><p>Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.</p><p>OpenHands agents can do anything a human developer can: modify code, run commands, browse the web, call APIs, and yes‚Äîeven copy code snippets from StackOverflow.</p><blockquote><p>[!IMPORTANT] Using OpenHands for work? We'd love to chat! Fill out <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform\">this short form</a> to join our Design Partner program, where you'll get early access to commercial features and the opportunity to provide input on our product roadmap.</p></blockquote><p>The easiest way to run OpenHands is in Docker. See the <a href=\"https://docs.all-hands.dev/modules/usage/installation\">Running OpenHands</a> guide for system requirements and more information.</p><pre><code>docker pull docker.all-hands.dev/all-hands-ai/runtime:0.24-nikolaik\n\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.24-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands-state:/.openhands-state \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.all-hands.dev/all-hands-ai/openhands:0.24\n</code></pre><blockquote><p>[!CAUTION] OpenHands is meant to be run by a single user on their local workstation. It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in isolation or scalability.</p><p>If you're interested in running OpenHands in a multi-tenant environment, please <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform\">get in touch with us</a> for advanced deployment options.</p></blockquote><p>If you want to modify the OpenHands source code, check out <a href=\"https://github.com/All-Hands-AI/OpenHands/raw/main/Development.md\">Development.md</a>.</p><p>To learn more about the project, and for tips on using OpenHands, check out our <a href=\"https://docs.all-hands.dev/modules/usage/getting-started\">documentation</a>.</p><p>There you'll find resources on how to use different LLM providers, troubleshooting resources, and advanced configuration options.</p><h2>ü§ù How to Join the Community</h2><p>OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication through Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:</p><p>See the monthly OpenHands roadmap <a href=\"https://github.com/orgs/All-Hands-AI/projects/1\">here</a> (updated at the maintainer's meeting at the end of each month).</p><p>Distributed under the MIT License. See <a href=\"https://raw.githubusercontent.com/All-Hands-AI/OpenHands/main/LICENSE\"></a> for more information.</p><p>OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.</p><p>For a list of open source projects and licenses used in OpenHands, please see our <a href=\"https://raw.githubusercontent.com/All-Hands-AI/OpenHands/main/CREDITS.md\">CREDITS.md</a> file.</p><pre><code>@misc{openhands,\n      title={{OpenHands: An Open Platform for AI Software Developers as Generalist Agents}},\n      author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},\n      year={2024},\n      eprint={2407.16741},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE},\n      url={https://arxiv.org/abs/2407.16741},\n}\n</code></pre>","contentLength":3165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"lucide-icons/lucide","url":"https://github.com/lucide-icons/lucide","date":1739931998,"author":"","guid":4533,"unread":true,"content":"<p>Beautiful &amp; consistent icon toolkit made by the community. Open-source project and a fork of Feather Icons.</p><p>Lucide is an open-source icon library that provides 1000+ vector (svg) files for displaying icons and symbols in digital and non-digital projects. The library aims to make it easier for designers and developers to incorporate icons into their projects by providing several official <a href=\"https://lucide.dev/packages\">packages</a> to make it easier to use these icons in your project.</p><img width=\"420\" src=\"https://www.figma.com/community/plugin/939567362549682242/thumbnail\" alt=\"Figma Lucide Cover\"><p>Join the community on our <a href=\"https://discord.gg/EH6nSts\">Discord</a> server!</p><p>Lucide is totally free for commercial use and personal use, this software is licensed under the <a href=\"https://github.com/lucide-icons/lucide/raw/main/LICENSE\">ISC License</a>.</p><p>Thank you to all the people who contributed to Lucide!</p><a href=\"https://github.com/lucide-icons/lucide/graphs/contributors\"><img src=\"https://opencollective.com/lucide-icons/contributors.svg?width=890\"></a><a href=\"https://vercel.com?utm_source=lucide&amp;utm_campaign=oss\"><img src=\"https://raw.githubusercontent.com/lucide-icons/lucide/main/docs/public/vercel.svg?sanitize=true\" alt=\"Powered by Vercel\" width=\"200\"></a>","contentLength":654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"harry0703/MoneyPrinterTurbo","url":"https://github.com/harry0703/MoneyPrinterTurbo","date":1739931998,"author":"","guid":4534,"unread":true,"content":"<p>Âà©Áî®AIÂ§ßÊ®°ÂûãÔºå‰∏ÄÈîÆÁîüÊàêÈ´òÊ∏ÖÁü≠ËßÜÈ¢ë Generate short videos with one click using AI LLM.</p><div align=\"center\"> Âè™ÈúÄÊèê‰æõ‰∏Ä‰∏™ËßÜÈ¢ë \n  Êàñ \n  ÔºåÂ∞±ÂèØ‰ª•ÂÖ®Ëá™Âä®ÁîüÊàêËßÜÈ¢ëÊñáÊ°à„ÄÅËßÜÈ¢ëÁ¥†Êùê„ÄÅËßÜÈ¢ëÂ≠óÂπï„ÄÅËßÜÈ¢ëËÉåÊôØÈü≥‰πêÔºåÁÑ∂ÂêéÂêàÊàê‰∏Ä‰∏™È´òÊ∏ÖÁöÑÁü≠ËßÜÈ¢ë„ÄÇ \n </div><p>Áî±‰∫éËØ•È°πÁõÆÁöÑ  Âíå ÔºåÂØπ‰∫é‰∏Ä‰∫õÂ∞èÁôΩÁî®Êà∑Êù•ËØ¥ÔºåËøòÊòØ ÔºåÂú®Ê≠§ÁâπÂà´ÊÑüË∞¢  ÁΩëÁ´ôÂü∫‰∫éËØ•È°πÁõÆÔºåÊèê‰æõÁöÑÂÖçË¥πÊúçÂä°ÔºåÂèØ‰ª•‰∏çÁî®ÈÉ®ÁΩ≤ÔºåÁõ¥Êé•Âú®Á∫ø‰ΩøÁî®ÔºåÈùûÂ∏∏Êñπ‰æø„ÄÇ</p><p>‰ΩêÁ≥ñ‰∏ìÊ≥®‰∫éÔºåÊèê‰æõ‰∏∞ÂØåÁöÑÔºåÂ∞ÜÂ§çÊùÇÊìç‰ΩúÊûÅËá¥ÁÆÄÂåñÔºåÁúüÊ≠£ÂÆûÁé∞ËÆ©ÂõæÂÉèÂ§ÑÁêÜÊõ¥ÁÆÄÂçï„ÄÇ</p><table><thead><tr></tr></thead></table><ul><li>Âª∫ËÆÆÊúÄ‰Ωé CPU 4Ê†∏Êàñ‰ª•‰∏äÔºåÂÜÖÂ≠ò 8G Êàñ‰ª•‰∏äÔºåÊòæÂç°ÈùûÂøÖÈ°ª</li><li>Windows 10 Êàñ MacOS 11.0 ‰ª•‰∏äÁ≥ªÁªü</li></ul><p>‰∏ãËΩΩ‰∏ÄÈîÆÂêØÂä®ÂåÖÔºåËß£ÂéãÁõ¥Êé•‰ΩøÁî®ÔºàË∑ØÂæÑ‰∏çË¶ÅÊúâ „ÄÅ„ÄÅÔºâ</p><p>‰∏ãËΩΩÂêéÔºåÂª∫ËÆÆÂÖà Êõ¥Êñ∞Âà∞ÔºåÁÑ∂ÂêéÂèåÂáª  ÂêØÂä®</p><p>ÂêØÂä®ÂêéÔºå‰ºöËá™Âä®ÊâìÂºÄÊµèËßàÂô®ÔºàÂ¶ÇÊûúÊâìÂºÄÊòØÁ©∫ÁôΩÔºåÂª∫ËÆÆÊç¢Êàê  ÊàñËÄÖ  ÊâìÂºÄÔºâ</p><p>ËøòÊ≤°ÊúâÂà∂‰Ωú‰∏ÄÈîÆÂêØÂä®ÂåÖÔºåÁúã‰∏ãÈù¢ÁöÑ  ÈÉ®ÂàÜÔºåÂª∫ËÆÆ‰ΩøÁî®  ÈÉ®ÁΩ≤ÔºåÊõ¥Âä†Êñπ‰æø„ÄÇ</p><ul><li>ËØ∑Á°Æ‰øù‰Ω†ÁöÑ  ÊòØÊ≠£Â∏∏ÁöÑÔºåVPNÈúÄË¶ÅÊâìÂºÄÊ®°Âºè</li></ul><pre><code>git clone https://github.com/harry0703/MoneyPrinterTurbo.git\n</code></pre><ul><li>Â∞Ü  Êñá‰ª∂Â§çÂà∂‰∏Ä‰ªΩÔºåÂëΩÂêç‰∏∫ </li><li>ÊåâÁÖß  Êñá‰ª∂‰∏≠ÁöÑËØ¥ÊòéÔºåÈÖçÁΩÆÂ•Ω  Âíå ÔºåÂπ∂Ê†πÊçÆ llm_provider ÂØπÂ∫îÁöÑÊúçÂä°ÂïÜÔºåÈÖçÁΩÆÁõ∏ÂÖ≥ÁöÑ API Key</li></ul><pre><code>cd MoneyPrinterTurbo\ndocker-compose up\n</code></pre><blockquote><p>Ê≥®ÊÑèÔºöÊúÄÊñ∞ÁâàÁöÑdockerÂÆâË£ÖÊó∂‰ºöËá™Âä®‰ª•Êèí‰ª∂ÁöÑÂΩ¢ÂºèÂÆâË£Ödocker composeÔºåÂêØÂä®ÂëΩ‰ª§Ë∞ÉÊï¥‰∏∫docker compose up</p></blockquote><pre><code>git clone https://github.com/harry0703/MoneyPrinterTurbo.git\ncd MoneyPrinterTurbo\nconda create -n MoneyPrinterTurbo python=3.11\nconda activate MoneyPrinterTurbo\npip install -r requirements.txt\n</code></pre><ul><li><pre><code>sudo apt-get install imagemagick\n</code></pre></li><li><pre><code>sudo yum install ImageMagick\n</code></pre></li></ul><p>Ê≥®ÊÑèÈúÄË¶ÅÂà∞ MoneyPrinterTurbo È°πÁõÆ  ‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§</p><pre><code>conda activate MoneyPrinterTurbo\nwebui.bat\n</code></pre><pre><code>conda activate MoneyPrinterTurbo\nsh webui.sh\n</code></pre><p>ÂêØÂä®ÂêéÔºå‰ºöËá™Âä®ÊâìÂºÄÊµèËßàÂô®ÔºàÂ¶ÇÊûúÊâìÂºÄÊòØÁ©∫ÁôΩÔºåÂª∫ËÆÆÊç¢Êàê  ÊàñËÄÖ  ÊâìÂºÄÔºâ</p><p>2024-04-16 v1.1.2 Êñ∞Â¢û‰∫Ü9ÁßçAzureÁöÑËØ≠Èü≥ÂêàÊàêÂ£∞Èü≥ÔºåÈúÄË¶ÅÈÖçÁΩÆAPI KEYÔºåËØ•Â£∞Èü≥ÂêàÊàêÁöÑÊõ¥Âä†ÁúüÂÆû„ÄÇ</p><ul><li>: ÁîüÊàêÔºåÊÄßËÉΩÊõ¥Â•ΩÔºåÂØπÁîµËÑëÈÖçÁΩÆÊ≤°ÊúâË¶ÅÊ±ÇÔºå‰ΩÜÊòØË¥®ÈáèÂèØËÉΩ‰∏çÁ®≥ÂÆö</li><li>: ÁîüÊàêÔºåÊÄßËÉΩËæÉÂ∑ÆÔºåÂØπÁîµËÑëÈÖçÁΩÆÊúâ‰∏ÄÂÆöË¶ÅÊ±ÇÔºå‰ΩÜÊòØ„ÄÇ</li></ul><p>ÂèØ‰ª•‰øÆÊîπ  ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑ  ËøõË°åÂàáÊç¢</p><p>Âª∫ËÆÆ‰ΩøÁî®  Ê®°ÂºèÔºåÂ¶ÇÊûúÁîüÊàêÁöÑÂ≠óÂπïË¥®Èáè‰∏çÂ•ΩÔºåÂÜçÂàáÊç¢Âà∞  Ê®°Âºè</p><ol><li>whisper Ê®°Âºè‰∏ãÈúÄË¶ÅÂà∞ HuggingFace ‰∏ãËΩΩ‰∏Ä‰∏™Ê®°ÂûãÊñá‰ª∂ÔºåÂ§ßÁ∫¶ 3GB Â∑¶Âè≥ÔºåËØ∑Á°Æ‰øùÁΩëÁªúÈÄöÁïÖ</li></ol><blockquote><p>Áî±‰∫éÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆ HuggingFaceÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÊñπÊ≥ï‰∏ãËΩΩ  ÁöÑÊ®°ÂûãÊñá‰ª∂</p></blockquote><p>Ê®°Âûã‰∏ãËΩΩÂêéËß£ÂéãÔºåÊï¥‰∏™ÁõÆÂΩïÊîæÂà∞ <code>.\\MoneyPrinterTurbo\\models</code> ÈáåÈù¢Ôºå ÊúÄÁªàÁöÑÊñá‰ª∂Ë∑ØÂæÑÂ∫îËØ•ÊòØËøôÊ†∑: <code>.\\MoneyPrinterTurbo\\models\\whisper-large-v3</code></p><pre><code>MoneyPrinterTurbo  \n  ‚îú‚îÄmodels\n  ‚îÇ   ‚îî‚îÄwhisper-large-v3\n  ‚îÇ          config.json\n  ‚îÇ          model.bin\n  ‚îÇ          preprocessor_config.json\n  ‚îÇ          tokenizer.json\n  ‚îÇ          vocabulary.json\n</code></pre><p>Áî®‰∫éËßÜÈ¢ëÁöÑËÉåÊôØÈü≥‰πêÔºå‰Ωç‰∫éÈ°πÁõÆÁöÑ  ÁõÆÂΩï‰∏ã„ÄÇ</p><blockquote><p>ÂΩìÂâçÈ°πÁõÆÈáåÈù¢Êîæ‰∫Ü‰∏Ä‰∫õÈªòËÆ§ÁöÑÈü≥‰πêÔºåÊù•Ëá™‰∫é YouTube ËßÜÈ¢ëÔºåÂ¶ÇÊúâ‰æµÊùÉÔºåËØ∑Âà†Èô§„ÄÇ</p></blockquote><p>Áî®‰∫éËßÜÈ¢ëÂ≠óÂπïÁöÑÊ∏≤ÊüìÔºå‰Ωç‰∫éÈ°πÁõÆÁöÑ  ÁõÆÂΩï‰∏ãÔºå‰Ω†‰πüÂèØ‰ª•ÊîæËøõÂéªËá™Â∑±ÁöÑÂ≠ó‰Ωì„ÄÇ</p><h3>‚ùìÂ¶Ç‰Ωï‰ΩøÁî®ÂÖçË¥πÁöÑOpenAI GPT-3.5Ê®°Âûã?</h3><p>ÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®dockerÊúçÂä°</p><pre><code>docker run -p 3040:3040 missuo/freegpt35\n</code></pre><ul><li> Èöè‰æøÂ°´ÂÜô‰∏Ä‰∏™Âç≥ÂèØÔºåÊØîÂ¶Ç '123456'</li><li> Êîπ‰∏∫ <code>http://localhost:3040/v1/</code></li><li> Êîπ‰∏∫ </li></ul><h3>‚ùìAttributeError: 'str' object has no attribute 'choices'`</h3><p>Â§ßÊ¶ÇÁéáÊòØÁΩëÁªúÂéüÂõ†Ôºå ‰ΩøÁî® ÔºåÊàñËÄÖËÆæÁΩÆ  ‰∏∫‰Ω†ÁöÑ‰ª£ÁêÜ ÔºåÂ∫îËØ•Â∞±ÂèØ‰ª•Ëß£ÂÜ≥‰∫Ü„ÄÇ</p><p>ÂêåÊó∂Âª∫ËÆÆ‰ΩøÁî®  Êàñ  ‰Ωú‰∏∫Â§ßÊ®°ÂûãÊèê‰æõÂïÜÔºåËøô‰∏§‰∏™ÊúçÂä°ÂïÜÂú®ÂõΩÂÜÖËÆøÈóÆÈÄüÂ∫¶Êõ¥Âø´ÔºåÊõ¥Âä†Á®≥ÂÆö„ÄÇ</p><h3>‚ùìRuntimeError: No ffmpeg exe could be found</h3><p>ÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåffmpeg ‰ºöË¢´Ëá™Âä®‰∏ãËΩΩÔºåÂπ∂‰∏î‰ºöË¢´Ëá™Âä®Ê£ÄÊµãÂà∞„ÄÇ ‰ΩÜÊòØÂ¶ÇÊûú‰Ω†ÁöÑÁéØÂ¢ÉÊúâÈóÆÈ¢òÔºåÊó†Ê≥ïËá™Âä®‰∏ãËΩΩÔºåÂèØËÉΩ‰ºöÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºö</p><pre><code>RuntimeError: No ffmpeg exe could be found.\nInstall ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.\n</code></pre><pre><code>[app]\n# ËØ∑Ê†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖË∑ØÂæÑËÆæÁΩÆÔºåÊ≥®ÊÑè Windows Ë∑ØÂæÑÂàÜÈöîÁ¨¶‰∏∫ \\\\\nffmpeg_path = \"C:\\\\Users\\\\harry\\\\Downloads\\\\ffmpeg.exe\"\n</code></pre><h3>‚ùìImageMagickÁöÑÂÆâÂÖ®Á≠ñÁï•ÈòªÊ≠¢‰∫Ü‰∏é‰∏¥Êó∂Êñá‰ª∂@/tmp/tmpur5hyyto.txtÁõ∏ÂÖ≥ÁöÑÊìç‰Ωú</h3><p>ÂèØ‰ª•Âú®ImageMagickÁöÑÈÖçÁΩÆÊñá‰ª∂policy.xml‰∏≠ÊâæÂà∞Ëøô‰∫õÁ≠ñÁï•„ÄÇ Ëøô‰∏™Êñá‰ª∂ÈÄöÂ∏∏‰Ωç‰∫é /etc/ImageMagick-/ Êàñ ImageMagick ÂÆâË£ÖÁõÆÂΩïÁöÑÁ±ª‰ºº‰ΩçÁΩÆ„ÄÇ ‰øÆÊîπÂåÖÂê´ÁöÑÊù°ÁõÆÔºåÂ∞ÜÊõ¥Êîπ‰∏∫‰ª•ÂÖÅËÆ∏ÂØπÊñá‰ª∂ÁöÑËØªÂÜôÊìç‰Ωú„ÄÇ</p><h3>‚ùìOSError: [Errno 24] Too many open files</h3><p>Ëøô‰∏™ÈóÆÈ¢òÊòØÁî±‰∫éÁ≥ªÁªüÊâìÂºÄÊñá‰ª∂Êï∞ÈôêÂà∂ÂØºËá¥ÁöÑÔºåÂèØ‰ª•ÈÄöËøá‰øÆÊîπÁ≥ªÁªüÁöÑÊñá‰ª∂ÊâìÂºÄÊï∞ÈôêÂà∂Êù•Ëß£ÂÜ≥„ÄÇ</p><p>LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and outgoing trafic has been disabled. To enablerepo look-ups and downloads online, pass 'local files only=False' as input.</p><p>An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again. Trying to load the model directly from the local cache, if it exists.</p>","contentLength":5204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"folke/snacks.nvim","url":"https://github.com/folke/snacks.nvim","date":1739931998,"author":"","guid":4535,"unread":true,"content":"<p>üçø A collection of QoL plugins for Neovim</p><p>A collection of small QoL plugins for Neovim.</p><table><tbody><tr><td>Efficient animations including over 45 easing functions </td></tr><tr><td>Delete buffers without disrupting window layout</td></tr><tr><td>Pretty inspect &amp; backtraces for debugging</td></tr><tr><td>Focus on the active scope by dimming the rest</td></tr><tr><td>A file explorer (picker in disguise)</td></tr><tr><td>Open the current file, branch, commit, or repo in a browser (e.g. GitHub, GitLab, Bitbucket)</td></tr><tr><td>Image viewer using Kitty Graphics Protocol, supported by ,  and </td></tr><tr></tr><tr></tr><tr><td>Open LazyGit in a float, auto-configure colorscheme and integration with Neovim</td></tr><tr><td>Utility functions to work with Neovim's </td></tr><tr><td>Picker for selecting items</td></tr><tr><td>When doing , it will render the file as quickly as possible, before loading your plugins.</td></tr><tr><td>Scope detection, text objects and jumping based on treesitter or indent</td></tr><tr><td>Scratch buffers with a persistent file</td></tr><tr><td>Create and toggle floating/split terminals</td></tr><tr><td>Toggle keymaps integrated with which-key icons / colors</td></tr><tr><td>Utility functions for Snacks </td></tr><tr><td>Create and manage floating windows or splits</td></tr><tr><td>Auto-show LSP references and quickly navigate between them</td></tr><tr><td>Zen mode ‚Ä¢ distraction-free coding</td></tr></tbody></table><p>Install the plugin with your package manager:</p><blockquote><p>[!important] A couple of plugins  to be set-up early. Setup creates some autocmds and does not load any plugins. Check the <a href=\"https://github.com/folke/snacks.nvim/raw/main/lua/snacks/init.lua\">code</a> to see what it does.</p></blockquote><blockquote><p>[!caution] You need to explicitly pass options for a plugin or set  to enable it.</p></blockquote><blockquote><p>[!tip] It's a good idea to run  to see if everything is set up correctly.</p></blockquote><pre><code>{\n  \"folke/snacks.nvim\",\n  priority = 1000,\n  lazy = false,\n  ---@type snacks.Config\n  opts = {\n    -- your configuration comes here\n    -- or leave it empty to use the default settings\n    -- refer to the configuration section below\n    bigfile = { enabled = true },\n    dashboard = { enabled = true },\n    explorer = { enabled = true },\n    indent = { enabled = true },\n    input = { enabled = true },\n    picker = { enabled = true },\n    notifier = { enabled = true },\n    quickfile = { enabled = true },\n    scope = { enabled = true },\n    scroll = { enabled = true },\n    statuscolumn = { enabled = true },\n    words = { enabled = true },\n  },\n}\n</code></pre><p>For an in-depth setup of  with , check the <a href=\"https://github.com/folke/snacks.nvim?tab=readme-ov-file#-usage\">example</a> below.</p><p>Please refer to the readme of each plugin for their specific configuration.</p><p>Some plugins have examples in their documentation. You can include them in your config like this:</p><pre><code>{\n  dashboard = { example = \"github\" }\n}\n</code></pre><p>If you want to customize options for a plugin after they have been resolved, you can use the  function:</p><pre><code>{\n  gitbrowse = {\n    config = function(opts, defaults)\n      table.insert(opts.remote_patterns, { \"my\", \"custom pattern\" })\n    end\n  },\n}\n</code></pre><p>See the example below for how to configure .</p><pre><code>{\n  \"folke/snacks.nvim\",\n  priority = 1000,\n  lazy = false,\n  ---@type snacks.Config\n  opts = {\n    bigfile = { enabled = true },\n    dashboard = { enabled = true },\n    explorer = { enabled = true },\n    indent = { enabled = true },\n    input = { enabled = true },\n    notifier = {\n      enabled = true,\n      timeout = 3000,\n    },\n    picker = { enabled = true },\n    quickfile = { enabled = true },\n    scope = { enabled = true },\n    scroll = { enabled = true },\n    statuscolumn = { enabled = true },\n    words = { enabled = true },\n    styles = {\n      notification = {\n        -- wo = { wrap = true } -- Wrap notifications\n      }\n    }\n  },\n  keys = {\n    -- Top Pickers &amp; Explorer\n    { \"&lt;leader&gt;&lt;space&gt;\", function() Snacks.picker.smart() end, desc = \"Smart Find Files\" },\n    { \"&lt;leader&gt;,\", function() Snacks.picker.buffers() end, desc = \"Buffers\" },\n    { \"&lt;leader&gt;/\", function() Snacks.picker.grep() end, desc = \"Grep\" },\n    { \"&lt;leader&gt;:\", function() Snacks.picker.command_history() end, desc = \"Command History\" },\n    { \"&lt;leader&gt;n\", function() Snacks.picker.notifications() end, desc = \"Notification History\" },\n    { \"&lt;leader&gt;e\", function() Snacks.explorer() end, desc = \"File Explorer\" },\n    -- find\n    { \"&lt;leader&gt;fb\", function() Snacks.picker.buffers() end, desc = \"Buffers\" },\n    { \"&lt;leader&gt;fc\", function() Snacks.picker.files({ cwd = vim.fn.stdpath(\"config\") }) end, desc = \"Find Config File\" },\n    { \"&lt;leader&gt;ff\", function() Snacks.picker.files() end, desc = \"Find Files\" },\n    { \"&lt;leader&gt;fg\", function() Snacks.picker.git_files() end, desc = \"Find Git Files\" },\n    { \"&lt;leader&gt;fp\", function() Snacks.picker.projects() end, desc = \"Projects\" },\n    { \"&lt;leader&gt;fr\", function() Snacks.picker.recent() end, desc = \"Recent\" },\n    -- git\n    { \"&lt;leader&gt;gb\", function() Snacks.picker.git_branches() end, desc = \"Git Branches\" },\n    { \"&lt;leader&gt;gl\", function() Snacks.picker.git_log() end, desc = \"Git Log\" },\n    { \"&lt;leader&gt;gL\", function() Snacks.picker.git_log_line() end, desc = \"Git Log Line\" },\n    { \"&lt;leader&gt;gs\", function() Snacks.picker.git_status() end, desc = \"Git Status\" },\n    { \"&lt;leader&gt;gS\", function() Snacks.picker.git_stash() end, desc = \"Git Stash\" },\n    { \"&lt;leader&gt;gd\", function() Snacks.picker.git_diff() end, desc = \"Git Diff (Hunks)\" },\n    { \"&lt;leader&gt;gf\", function() Snacks.picker.git_log_file() end, desc = \"Git Log File\" },\n    -- Grep\n    { \"&lt;leader&gt;sb\", function() Snacks.picker.lines() end, desc = \"Buffer Lines\" },\n    { \"&lt;leader&gt;sB\", function() Snacks.picker.grep_buffers() end, desc = \"Grep Open Buffers\" },\n    { \"&lt;leader&gt;sg\", function() Snacks.picker.grep() end, desc = \"Grep\" },\n    { \"&lt;leader&gt;sw\", function() Snacks.picker.grep_word() end, desc = \"Visual selection or word\", mode = { \"n\", \"x\" } },\n    -- search\n    { '&lt;leader&gt;s\"', function() Snacks.picker.registers() end, desc = \"Registers\" },\n    { '&lt;leader&gt;s/', function() Snacks.picker.search_history() end, desc = \"Search History\" },\n    { \"&lt;leader&gt;sa\", function() Snacks.picker.autocmds() end, desc = \"Autocmds\" },\n    { \"&lt;leader&gt;sb\", function() Snacks.picker.lines() end, desc = \"Buffer Lines\" },\n    { \"&lt;leader&gt;sc\", function() Snacks.picker.command_history() end, desc = \"Command History\" },\n    { \"&lt;leader&gt;sC\", function() Snacks.picker.commands() end, desc = \"Commands\" },\n    { \"&lt;leader&gt;sd\", function() Snacks.picker.diagnostics() end, desc = \"Diagnostics\" },\n    { \"&lt;leader&gt;sD\", function() Snacks.picker.diagnostics_buffer() end, desc = \"Buffer Diagnostics\" },\n    { \"&lt;leader&gt;sh\", function() Snacks.picker.help() end, desc = \"Help Pages\" },\n    { \"&lt;leader&gt;sH\", function() Snacks.picker.highlights() end, desc = \"Highlights\" },\n    { \"&lt;leader&gt;si\", function() Snacks.picker.icons() end, desc = \"Icons\" },\n    { \"&lt;leader&gt;sj\", function() Snacks.picker.jumps() end, desc = \"Jumps\" },\n    { \"&lt;leader&gt;sk\", function() Snacks.picker.keymaps() end, desc = \"Keymaps\" },\n    { \"&lt;leader&gt;sl\", function() Snacks.picker.loclist() end, desc = \"Location List\" },\n    { \"&lt;leader&gt;sm\", function() Snacks.picker.marks() end, desc = \"Marks\" },\n    { \"&lt;leader&gt;sM\", function() Snacks.picker.man() end, desc = \"Man Pages\" },\n    { \"&lt;leader&gt;sp\", function() Snacks.picker.lazy() end, desc = \"Search for Plugin Spec\" },\n    { \"&lt;leader&gt;sq\", function() Snacks.picker.qflist() end, desc = \"Quickfix List\" },\n    { \"&lt;leader&gt;sR\", function() Snacks.picker.resume() end, desc = \"Resume\" },\n    { \"&lt;leader&gt;su\", function() Snacks.picker.undo() end, desc = \"Undo History\" },\n    { \"&lt;leader&gt;uC\", function() Snacks.picker.colorschemes() end, desc = \"Colorschemes\" },\n    -- LSP\n    { \"gd\", function() Snacks.picker.lsp_definitions() end, desc = \"Goto Definition\" },\n    { \"gD\", function() Snacks.picker.lsp_declarations() end, desc = \"Goto Declaration\" },\n    { \"gr\", function() Snacks.picker.lsp_references() end, nowait = true, desc = \"References\" },\n    { \"gI\", function() Snacks.picker.lsp_implementations() end, desc = \"Goto Implementation\" },\n    { \"gy\", function() Snacks.picker.lsp_type_definitions() end, desc = \"Goto T[y]pe Definition\" },\n    { \"&lt;leader&gt;ss\", function() Snacks.picker.lsp_symbols() end, desc = \"LSP Symbols\" },\n    { \"&lt;leader&gt;sS\", function() Snacks.picker.lsp_workspace_symbols() end, desc = \"LSP Workspace Symbols\" },\n    -- Other\n    { \"&lt;leader&gt;z\",  function() Snacks.zen() end, desc = \"Toggle Zen Mode\" },\n    { \"&lt;leader&gt;Z\",  function() Snacks.zen.zoom() end, desc = \"Toggle Zoom\" },\n    { \"&lt;leader&gt;.\",  function() Snacks.scratch() end, desc = \"Toggle Scratch Buffer\" },\n    { \"&lt;leader&gt;S\",  function() Snacks.scratch.select() end, desc = \"Select Scratch Buffer\" },\n    { \"&lt;leader&gt;n\",  function() Snacks.notifier.show_history() end, desc = \"Notification History\" },\n    { \"&lt;leader&gt;bd\", function() Snacks.bufdelete() end, desc = \"Delete Buffer\" },\n    { \"&lt;leader&gt;cR\", function() Snacks.rename.rename_file() end, desc = \"Rename File\" },\n    { \"&lt;leader&gt;gB\", function() Snacks.gitbrowse() end, desc = \"Git Browse\", mode = { \"n\", \"v\" } },\n    { \"&lt;leader&gt;gg\", function() Snacks.lazygit() end, desc = \"Lazygit\" },\n    { \"&lt;leader&gt;un\", function() Snacks.notifier.hide() end, desc = \"Dismiss All Notifications\" },\n    { \"&lt;c-/&gt;\",      function() Snacks.terminal() end, desc = \"Toggle Terminal\" },\n    { \"&lt;c-_&gt;\",      function() Snacks.terminal() end, desc = \"which_key_ignore\" },\n    { \"]]\",         function() Snacks.words.jump(vim.v.count1) end, desc = \"Next Reference\", mode = { \"n\", \"t\" } },\n    { \"[[\",         function() Snacks.words.jump(-vim.v.count1) end, desc = \"Prev Reference\", mode = { \"n\", \"t\" } },\n    {\n      \"&lt;leader&gt;N\",\n      desc = \"Neovim News\",\n      function()\n        Snacks.win({\n          file = vim.api.nvim_get_runtime_file(\"doc/news.txt\", false)[1],\n          width = 0.6,\n          height = 0.6,\n          wo = {\n            spell = false,\n            wrap = false,\n            signcolumn = \"yes\",\n            statuscolumn = \" \",\n            conceallevel = 3,\n          },\n        })\n      end,\n    }\n  },\n  init = function()\n    vim.api.nvim_create_autocmd(\"User\", {\n      pattern = \"VeryLazy\",\n      callback = function()\n        -- Setup some globals for debugging (lazy-loaded)\n        _G.dd = function(...)\n          Snacks.debug.inspect(...)\n        end\n        _G.bt = function()\n          Snacks.debug.backtrace()\n        end\n        vim.print = _G.dd -- Override print to use snacks for `:=` command\n\n        -- Create some toggle mappings\n        Snacks.toggle.option(\"spell\", { name = \"Spelling\" }):map(\"&lt;leader&gt;us\")\n        Snacks.toggle.option(\"wrap\", { name = \"Wrap\" }):map(\"&lt;leader&gt;uw\")\n        Snacks.toggle.option(\"relativenumber\", { name = \"Relative Number\" }):map(\"&lt;leader&gt;uL\")\n        Snacks.toggle.diagnostics():map(\"&lt;leader&gt;ud\")\n        Snacks.toggle.line_number():map(\"&lt;leader&gt;ul\")\n        Snacks.toggle.option(\"conceallevel\", { off = 0, on = vim.o.conceallevel &gt; 0 and vim.o.conceallevel or 2 }):map(\"&lt;leader&gt;uc\")\n        Snacks.toggle.treesitter():map(\"&lt;leader&gt;uT\")\n        Snacks.toggle.option(\"background\", { off = \"light\", on = \"dark\", name = \"Dark Background\" }):map(\"&lt;leader&gt;ub\")\n        Snacks.toggle.inlay_hints():map(\"&lt;leader&gt;uh\")\n        Snacks.toggle.indent():map(\"&lt;leader&gt;ug\")\n        Snacks.toggle.dim():map(\"&lt;leader&gt;uD\")\n      end,\n    })\n  end,\n}\n</code></pre><p>Snacks defines  of highlight groups and it's impossible to document them all.</p><p>Instead, you can use the picker to see all the highlight groups.</p><pre><code>Snacks.picker.highlights({pattern = \"hl_group:^Snacks\"})\n</code></pre>","contentLength":11039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"hyprwm/Hyprland","url":"https://github.com/hyprwm/Hyprland","date":1739845534,"author":"","guid":2133,"unread":true,"content":"<p>Hyprland is an independent, highly customizable, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.</p><ul><li>All of the eyecandy: gradient borders, blur, animations, shadows and much more</li><li>100% independent, no wlroots, no libweston, no kwin, no mutter.</li><li>Custom bezier curves for the best animations</li><li>Tearing support for better gaming performance</li><li>Easily expandable and readable codebase</li><li>Fast and active development</li><li>Not afraid to provide bleeding-edge features</li><li>Config reloaded instantly upon saving</li><li>Two built-in layouts and more available as plugins</li><li>Global keybinds passed to your apps of choice</li><li>Tiling/pseudotiling/floating/fullscreen windows</li><li>Special workspaces (scratchpads)</li><li>Window groups (tabbed mode)</li><li>Powerful window/monitor/layer rules</li><li>Native IME and Input Panels Support</li></ul><p> - <em>For powering Hyprland in the past</em></p><p> - <em>For showing how 2 do stuff</em></p><p> - <em>For showing how 2 do stuff the overkill way</em></p><p> - <em>For showing how 2 do stuff the simple way</em></p><p> - <em>For showing how 2 do stuff the hacky way</em></p><p> - <em>For showing how 2 do some graphics stuff</em></p>","contentLength":1008,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"hummingbot/hummingbot","url":"https://github.com/hummingbot/hummingbot","date":1739845534,"author":"","guid":2134,"unread":true,"content":"<p>Open source software that helps you create and deploy high-frequency crypto trading bots</p><p>Hummingbot is an open-source framework that helps you design and deploy automated trading strategies, or , that can run on many centralized or decentralized exchanges. Over the past year, Hummingbot users have generated over $34 billion in trading volume across 140+ unique trading venues.</p><p>The Hummingbot codebase is free and publicly available under the Apache 2.0 open-source license. Our mission is to <strong>democratize high-frequency trading</strong> by creating a global community of algorithmic traders and developers that share knowledge and contribute to the codebase.</p><ul><li><a href=\"https://discord.gg/hummingbot\">Discord</a>: The main gathering spot for the global Hummingbot community</li><li><a href=\"https://www.youtube.com/c/hummingbot\">YouTube</a>: Videos that teach you how to get the most of of Hummingbot</li><li><a href=\"https://twitter.com/_hummingbot\">Twitter</a>: Get the latest announcements about Hummingbot</li><li><a href=\"https://hummingbot.substack.com\">Newsletter</a>: Get our newsletter whenever we ship a new release</li></ul><p>Hummingbot connectors standardize REST and WebSocket API interfaces to different types of exchanges, enabling you to build sophisticated trading strategies that can be deployed across many exchanges with minimal changes. We classify exchanges into the following categories:</p><ul><li>: Centralized exchanges that take custody of your funds. Use API keys to connect with Hummingbot.</li><li>: Decentralized, non-custodial exchanges that operate on a blockchain. Use wallet keys to connect with Hummingbot.</li></ul><p>In addition, connectors differ based on the type of market supported:</p><ul><li>: Connectors to spot markets on central limit order book (CLOB) exchanges</li><li>: Connectors to perpetual futures markets on CLOB exchanges</li><li>: Connectors to spot markets on Automatic Market Maker (AMM) decentralized exchanges</li></ul><p>We are grateful for the following exchanges that support the development and maintenance of Hummingbot via broker partnerships and sponsorships.</p><h3>Other Exchange Connectors</h3><p>Currently, the master branch of Hummingbot also includes the following exchange connectors, which are maintained and updated through the Hummingbot Foundation governance process. See <a href=\"https://hummingbot.org/governance/\">Governance</a> for more information.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><ul><li><a href=\"https://github.com/hummingbot/deploy\">Deploy</a>: Deploy Hummingbot in various configurations with Docker</li><li><a href=\"https://github.com/hummingbot/dashboard\">Dashboard</a>: Web app that help you create, backtest, deploy, and manage Hummingbot instances</li><li><a href=\"https://github.com/hummingbot/quants-lab\">Quants Lab</a>: Juypter notebooks that enable you to fetch data and perform research using Hummingbot</li><li><a href=\"https://github.com/hummingbot/gateway\">Gateway</a>: Typescript based API client for DEX connectors</li><li><a href=\"https://github.com/hummingbot/hummingbot-site\">Hummingbot Site</a>: Official documentation for Hummingbot - we welcome contributions here too!</li></ul><p>The Hummingbot architecture features modular components that can be maintained and extended by individual community members.</p><p>We welcome contributions from the community! Please review these <a href=\"https://raw.githubusercontent.com/hummingbot/hummingbot/master/CONTRIBUTING.md\">guidelines</a> before submitting a pull request.</p><p>To have your exchange connector or other pull request merged into the codebase, please submit a New Connector Proposal or Pull Request Proposal, following these <a href=\"https://hummingbot.org/governance/proposals/\">guidelines</a>. Note that you will need some amount of <a href=\"https://etherscan.io/token/0xe5097d9baeafb89f9bcb78c9290d545db5f9e9cb\">HBOT tokens</a> in your Ethereum wallet to submit a proposal.</p><ul><li>: Hummingbot is open source and licensed under <a href=\"https://raw.githubusercontent.com/hummingbot/hummingbot/master/LICENSE\">Apache 2.0</a>.</li><li>: See <a href=\"https://hummingbot.org/reporting/\">Reporting</a> for information on anonymous data collection and reporting in Hummingbot.</li></ul>","contentLength":3096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"meshtastic/firmware","url":"https://github.com/meshtastic/firmware","date":1739845534,"author":"","guid":2135,"unread":true,"content":"<p>Meshtastic device firmware</p><p>This repository contains the device firmware for the Meshtastic project.</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"outline/outline","url":"https://github.com/outline/outline","date":1739845534,"author":"","guid":2136,"unread":true,"content":"<p>The fastest knowledge base for growing teams. Beautiful, realtime collaborative, feature packed, and markdown compatible.</p><p>This is the source code that runs <a href=\"https://www.getoutline.com\"></a> and all the associated services. If you want to use Outline then you don't need to run this code, we offer a hosted version of the app at <a href=\"https://www.getoutline.com\">getoutline.com</a>. You can also find documentation on using Outline in <a href=\"https://docs.getoutline.com/s/guide\">our guide</a>.</p><p>If you'd like to run your own copy of Outline or contribute to development then this is the place for you.</p><p>Please see the <a href=\"https://docs.getoutline.com/s/hosting/\">documentation</a> for running your own copy of Outline in a production configuration.</p><p>If you have questions or improvements for the docs please create a thread in <a href=\"https://github.com/outline/outline/discussions\">GitHub discussions</a>.</p><p>Outline is built and maintained by a small team ‚Äì we'd love your help to fix bugs and add features!</p><p>Before submitting a pull request  discuss with the core team by creating or commenting in an issue on <a href=\"https://www.github.com/outline/outline/issues\">GitHub</a> ‚Äì we'd also love to hear from you in the <a href=\"https://www.github.com/outline/outline/discussions\">discussions</a>. This way we can ensure that an approach is agreed on before code is written. This will result in a much higher likelihood of your code being accepted.</p><p>If you‚Äôre looking for ways to get started, here's a list of ways to help us improve Outline:</p><ul><li>Performance improvements, both on server and frontend</li><li>Developer happiness and documentation</li><li>Bugs and other issues listed on GitHub</li></ul><p>If you're interested in contributing or learning more about the Outline codebase please refer to the <a href=\"https://raw.githubusercontent.com/outline/outline/main/docs/ARCHITECTURE.md\">architecture document</a> first for a high level overview of how the application is put together.</p><p>In development Outline outputs simple logging to the console, prefixed by categories. In production it outputs JSON logs, these can be easily parsed by your preferred log ingestion pipeline.</p><p>HTTP logging is disabled by default, but can be enabled by setting the  environment variable.</p><p>We aim to have sufficient test coverage for critical parts of the application and aren't aiming for 100% unit test coverage. All API endpoints and anything authentication related should be thoroughly tested.</p><p>To add new tests, write your tests with <a href=\"https://facebook.github.io/jest/\">Jest</a> and add a file with  extension next to the tested code.</p><pre><code># To run all tests\nmake test\n\n# To run backend tests in watch mode\nmake watch\n</code></pre><p>Once the test database is created with  you may individually run frontend and backend tests directly.</p><pre><code># To run backend tests\nyarn test:server\n\n# To run a specific backend test\nyarn test:server myTestFile\n\n# To run frontend tests\nyarn test:app\n</code></pre><p>Sequelize is used to create and run migrations, for example:</p><pre><code>yarn sequelize migration:generate --name my-migration\nyarn sequelize db:migrate\n</code></pre><p>Or to run migrations on test database:</p><pre><code>yarn sequelize db:migrate --env test\n</code></pre>","contentLength":2627,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shubhamsaboo/awesome-llm-apps","url":"https://github.com/Shubhamsaboo/awesome-llm-apps","date":1739759302,"author":"","guid":861,"unread":true,"content":"<p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p><p>A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.</p><ul><li>üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.</li><li>üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.</li><li>üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.</li></ul><h3>RAG (Retrieval Augmented Generation)</h3><h3>Advanced Tools and Frameworks</h3><ol><li><pre><code>git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n</code></pre></li><li><p><strong>Navigate to the desired project directory</strong></p><pre><code>cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail\n</code></pre></li><li><p><strong>Install the required dependencies</strong></p><pre><code>pip install -r requirements.txt\n</code></pre></li><li><p><strong>Follow the project-specific instructions</strong> in each project's  file to set up and run the app.</p></li></ol><h2>ü§ù Contributing to Open Source</h2><p>Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new <a href=\"https://github.com/Shubhamsaboo/awesome-llm-apps/issues\">GitHub Issue</a> or submit a pull request. Make sure to follow the existing project structure and include a detailed  for each new app.</p><h3>Thank You, Community, for the Support! üôè</h3><p>üåü <strong>Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.</strong></p>","contentLength":1566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"yt-dlp/yt-dlp","url":"https://github.com/yt-dlp/yt-dlp","date":1739759302,"author":"","guid":862,"unread":true,"content":"<p>A feature-rich command-line audio/video downloader</p><p>You can install yt-dlp using <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files\">the binaries</a>, <a href=\"https://pypi.org/project/yt-dlp\">pip</a> or one using a third-party package manager. See <a href=\"https://github.com/yt-dlp/yt-dlp/wiki/Installation\">the wiki</a> for detailed instructions</p><table><tbody><tr><td align=\"left\">Platform-independent <a href=\"https://docs.python.org/3/library/zipimport.html\">zipimport</a> binary. Needs Python (recommended for )</td></tr><tr><td align=\"left\">Windows (Win8+) standalone x64 binary (recommended for )</td></tr><tr><td align=\"left\">Universal MacOS (10.15+) standalone executable (recommended for )</td></tr></tbody></table><p>The public key that can be used to verify the GPG signatures is <a href=\"https://github.com/yt-dlp/yt-dlp/raw/master/public.key\">available here</a> Example usage:</p><pre><code>curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import\ngpg --verify SHA2-256SUMS.sig SHA2-256SUMS\ngpg --verify SHA2-512SUMS.sig SHA2-512SUMS\n</code></pre><p>: The manpages, shell completion (autocomplete) files etc. are available inside the <a href=\"https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz\">source tarball</a></p><p>If you <a href=\"https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip\">installed with pip</a>, simply re-run the same command that was used to install the program</p><p>For other third-party package managers, see <a href=\"https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers\">the wiki</a> or refer to their documentation</p><p>There are currently three release channels for binaries: ,  and .</p><ul><li> is the default channel, and many of its changes have been tested by users of the  and  channels.</li><li>The  channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project's new patches and changes. This is the <strong>recommended channel for regular users</strong> of yt-dlp. The  releases are available from <a href=\"https://github.com/yt-dlp/yt-dlp-nightly-builds/releases\">yt-dlp/yt-dlp-nightly-builds</a> or as development releases of the  PyPI package (which can be installed with pip's  flag).</li><li>The  channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from <a href=\"https://github.com/yt-dlp/yt-dlp-master-builds/releases\">yt-dlp/yt-dlp-master-builds</a>.</li></ul><p>When using /, a release binary will only update to its current channel.  can be used to switch to a different channel when a newer version is available. <code>--update-to [CHANNEL@]TAG</code> can also be used to upgrade or downgrade to specific tags from a channel.</p><p>You may also use  () to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.</p><ul><li><code>yt-dlp --update-to master</code> switch to the  channel and update to its latest release</li><li><code>yt-dlp --update-to stable@2023.07.06</code> upgrade/downgrade to release to  channel tag </li><li><code>yt-dlp --update-to 2023.10.07</code> upgrade/downgrade to tag  if it exists on the current channel</li><li><code>yt-dlp --update-to example/yt-dlp@2023.09.24</code> upgrade/downgrade to the release from the  repository, tag </li></ul><p>: Any user experiencing an issue with the  release should install or update to the  release before submitting a bug report:</p><pre><code># To update to nightly from stable executable/binary:\nyt-dlp --update-to nightly\n\n# To install nightly with pip:\npython3 -m pip install -U --pre \"yt-dlp[default]\"\n</code></pre><p>Python versions 3.9+ (CPython) and 3.10+ (PyPy) are supported. Other versions and implementations may or may not work correctly.</p><p>While all the other dependencies are optional,  and  are highly recommended</p><p>The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.</p><ul><li><a href=\"https://github.com/lexiforest/curl_cffi\"></a> (recommended) - Python binding for <a href=\"https://github.com/lexiforest/curl-impersonate\">curl-impersonate</a>. Provides impersonation targets for Chrome, Edge and Safari. Licensed under <a href=\"https://github.com/lexiforest/curl_cffi/raw/main/LICENSE\">MIT</a><ul><li>Can be installed with the  group, e.g. <code>pip install \"yt-dlp[default,curl-cffi]\"</code></li><li>Currently included in ,  and  builds</li></ul></li></ul><ul><li><a href=\"https://github.com/mitya57/secretstorage\"></a>* - For  to access the  keyring while decrypting cookies of -based browsers on . Licensed under <a href=\"https://github.com/mitya57/secretstorage/raw/master/LICENSE\">BSD-3-Clause</a></li><li>Any external downloader that you want to use with </li></ul><p>To use or redistribute the dependencies, you must agree to their respective licensing terms.</p><p>The standalone release binaries are built with the Python interpreter and the packages marked with  included.</p><p>If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the  output</p><h3>Standalone PyInstaller Builds</h3><p>To build the standalone executable, you must have Python and  (plus any of yt-dlp's <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#dependencies\">optional dependencies</a> if needed). The executable will be built for the same CPU architecture as the Python used.</p><p>You can run the following commands:</p><pre><code>python3 devscripts/install_deps.py --include pyinstaller\npython3 devscripts/make_lazy_extractors.py\npython3 -m bundle.pyinstaller\n</code></pre><p>On some systems, you may need to use  or  instead of .</p><p><code>python -m bundle.pyinstaller</code> accepts any arguments that can be passed to , such as  or , which is further <a href=\"https://pyinstaller.org/en/stable/usage.html#what-to-generate\">documented here</a>.</p><p>: Pyinstaller versions below 4.4 <a href=\"https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms\">do not support</a> Python installed from the Windows store without using a virtual environment.</p><p>: Running  directly  using <code>python -m bundle.pyinstaller</code> is  officially supported. This may or may not work correctly.</p><h3>Platform-independent Binary (UNIX)</h3><p>You will need the build tools  (3.9+), ,  (GNU), * and *.</p><p>After installing these, simply run .</p><p>You can also run  instead to compile only the binary without updating any of the additional files. (The build tools marked with  are not needed for this)</p><ul><li><strong><code>devscripts/install_deps.py</code></strong> - Install dependencies for yt-dlp.</li><li><strong><code>devscripts/update-version.py</code></strong> - Update the version number based on the current date.</li><li><strong><code>devscripts/set-variant.py</code></strong> - Set the build variant of the executable.</li><li><strong><code>devscripts/make_changelog.py</code></strong> - Create a markdown changelog using short commit messages and update  file.</li><li><strong><code>devscripts/make_lazy_extractors.py</code></strong> - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable  to something nonempty to forcefully disable lazy extractor loading.</li></ul><p>Note: See their  for more info.</p><p>If you fork the project on GitHub, you can run your fork's <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/build.yml\">build workflow</a> to automatically build the selected version(s) as artifacts. Alternatively, you can run the <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/release.yml\">release workflow</a> or enable the <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/release-nightly.yml\">nightly workflow</a> to create full (pre-)releases.</p><pre><code>yt-dlp [OPTIONS] [--] URL [URL...]\n</code></pre><pre><code>-h, --help                      Print this help text and exit\n--version                       Print program version and exit\n-U, --update                    Update this program to the latest version\n--no-update                     Do not check for updates (default)\n--update-to [CHANNEL]@[TAG]     Upgrade/downgrade to a specific version.\n                                CHANNEL can be a repository as well. CHANNEL\n                                and TAG default to \"stable\" and \"latest\"\n                                respectively if omitted; See \"UPDATE\" for\n                                details. Supported channels: stable,\n                                nightly, master\n-i, --ignore-errors             Ignore download and postprocessing errors.\n                                The download will be considered successful\n                                even if the postprocessing fails\n--no-abort-on-error             Continue with next video on download errors;\n                                e.g. to skip unavailable videos in a\n                                playlist (default)\n--abort-on-error                Abort downloading of further videos if an\n                                error occurs (Alias: --no-ignore-errors)\n--dump-user-agent               Display the current user-agent and exit\n--list-extractors               List all supported extractors and exit\n--extractor-descriptions        Output descriptions of all supported\n                                extractors and exit\n--use-extractors NAMES          Extractor names to use separated by commas.\n                                You can also use regexes, \"all\", \"default\"\n                                and \"end\" (end URL matching); e.g. --ies\n                                \"holodex.*,end,youtube\". Prefix the name\n                                with a \"-\" to exclude it, e.g. --ies\n                                default,-generic. Use --list-extractors for\n                                a list of extractor names. (Alias: --ies)\n--default-search PREFIX         Use this prefix for unqualified URLs. E.g.\n                                \"gvsearch2:python\" downloads two videos from\n                                google videos for the search term \"python\".\n                                Use the value \"auto\" to let yt-dlp guess\n                                (\"auto_warning\" to emit a warning when\n                                guessing). \"error\" just throws an error. The\n                                default value \"fixup_error\" repairs broken\n                                URLs, but emits an error if this is not\n                                possible instead of searching\n--ignore-config                 Don't load any more configuration files\n                                except those given to --config-locations.\n                                For backward compatibility, if this option\n                                is found inside the system configuration\n                                file, the user configuration is not loaded.\n                                (Alias: --no-config)\n--no-config-locations           Do not load any custom configuration files\n                                (default). When given inside a configuration\n                                file, ignore all previous --config-locations\n                                defined in the current file\n--config-locations PATH         Location of the main configuration file;\n                                either the path to the config or its\n                                containing directory (\"-\" for stdin). Can be\n                                used multiple times and inside other\n                                configuration files\n--plugin-dirs PATH              Path to an additional directory to search\n                                for plugins. This option can be used\n                                multiple times to add multiple directories.\n                                Note that this currently only works for\n                                extractor plugins; postprocessor plugins can\n                                only be loaded from the default plugin\n                                directories\n--flat-playlist                 Do not extract a playlist's URL result\n                                entries; some entry metadata may be missing\n                                and downloading may be bypassed\n--no-flat-playlist              Fully extract the videos of a playlist\n                                (default)\n--live-from-start               Download livestreams from the start.\n                                Currently only supported for YouTube\n                                (Experimental)\n--no-live-from-start            Download livestreams from the current time\n                                (default)\n--wait-for-video MIN[-MAX]      Wait for scheduled streams to become\n                                available. Pass the minimum number of\n                                seconds (or range) to wait between retries\n--no-wait-for-video             Do not wait for scheduled streams (default)\n--mark-watched                  Mark videos watched (even with --simulate)\n--no-mark-watched               Do not mark videos watched (default)\n--color [STREAM:]POLICY         Whether to emit color codes in output,\n                                optionally prefixed by the STREAM (stdout or\n                                stderr) to apply the setting to. Can be one\n                                of \"always\", \"auto\" (default), \"never\", or\n                                \"no_color\" (use non color terminal\n                                sequences). Use \"auto-tty\" or \"no_color-tty\"\n                                to decide based on terminal support only.\n                                Can be used multiple times\n--compat-options OPTS           Options that can help keep compatibility\n                                with youtube-dl or youtube-dlc\n                                configurations by reverting some of the\n                                changes made in yt-dlp. See \"Differences in\n                                default behavior\" for details\n--alias ALIASES OPTIONS         Create aliases for an option string. Unless\n                                an alias starts with a dash \"-\", it is\n                                prefixed with \"--\". Arguments are parsed\n                                according to the Python string formatting\n                                mini-language. E.g. --alias get-audio,-X\n                                \"-S=aext:{0},abr -x --audio-format {0}\"\n                                creates options \"--get-audio\" and \"-X\" that\n                                takes an argument (ARG0) and expands to\n                                \"-S=aext:ARG0,abr -x --audio-format ARG0\".\n                                All defined aliases are listed in the --help\n                                output. Alias options can trigger more\n                                aliases; so be careful to avoid defining\n                                recursive options. As a safety measure, each\n                                alias may be triggered a maximum of 100\n                                times. This option can be used multiple times\n</code></pre><pre><code>--proxy URL                     Use the specified HTTP/HTTPS/SOCKS proxy. To\n                                enable SOCKS proxy, specify a proper scheme,\n                                e.g. socks5://user:pass@127.0.0.1:1080/.\n                                Pass in an empty string (--proxy \"\") for\n                                direct connection\n--socket-timeout SECONDS        Time to wait before giving up, in seconds\n--source-address IP             Client-side IP address to bind to\n--impersonate CLIENT[:OS]       Client to impersonate for requests. E.g.\n                                chrome, chrome-110, chrome:windows-10. Pass\n                                --impersonate=\"\" to impersonate any client.\n                                Note that forcing impersonation for all\n                                requests may have a detrimental impact on\n                                download speed and stability\n--list-impersonate-targets      List available clients to impersonate.\n-4, --force-ipv4                Make all connections via IPv4\n-6, --force-ipv6                Make all connections via IPv6\n--enable-file-urls              Enable file:// URLs. This is disabled by\n                                default for security reasons.\n</code></pre><pre><code>--geo-verification-proxy URL    Use this proxy to verify the IP address for\n                                some geo-restricted sites. The default proxy\n                                specified by --proxy (or none, if the option\n                                is not present) is used for the actual\n                                downloading\n--xff VALUE                     How to fake X-Forwarded-For HTTP header to\n                                try bypassing geographic restriction. One of\n                                \"default\" (only when known to be useful),\n                                \"never\", an IP block in CIDR notation, or a\n                                two-letter ISO 3166-2 country code\n</code></pre><pre><code>-I, --playlist-items ITEM_SPEC  Comma separated playlist_index of the items\n                                to download. You can specify a range using\n                                \"[START]:[STOP][:STEP]\". For backward\n                                compatibility, START-STOP is also supported.\n                                Use negative indices to count from the right\n                                and negative STEP to download in reverse\n                                order. E.g. \"-I 1:3,7,-5::2\" used on a\n                                playlist of size 15 will download the items\n                                at index 1,2,3,7,11,13,15\n--min-filesize SIZE             Abort download if filesize is smaller than\n                                SIZE, e.g. 50k or 44.6M\n--max-filesize SIZE             Abort download if filesize is larger than\n                                SIZE, e.g. 50k or 44.6M\n--date DATE                     Download only videos uploaded on this date.\n                                The date can be \"YYYYMMDD\" or in the format \n                                [now|today|yesterday][-N[day|week|month|year]].\n                                E.g. \"--date today-2weeks\" downloads only\n                                videos uploaded on the same day two weeks ago\n--datebefore DATE               Download only videos uploaded on or before\n                                this date. The date formats accepted are the\n                                same as --date\n--dateafter DATE                Download only videos uploaded on or after\n                                this date. The date formats accepted are the\n                                same as --date\n--match-filters FILTER          Generic video filter. Any \"OUTPUT TEMPLATE\"\n                                field can be compared with a number or a\n                                string using the operators defined in\n                                \"Filtering Formats\". You can also simply\n                                specify a field to match if the field is\n                                present, use \"!field\" to check if the field\n                                is not present, and \"&amp;\" to check multiple\n                                conditions. Use a \"\\\" to escape \"&amp;\" or\n                                quotes if needed. If used multiple times,\n                                the filter matches if at least one of the\n                                conditions is met. E.g. --match-filters\n                                !is_live --match-filters \"like_count&gt;?100 &amp;\n                                description~='(?i)\\bcats \\&amp; dogs\\b'\" matches\n                                only videos that are not live OR those that\n                                have a like count more than 100 (or the like\n                                field is not available) and also has a\n                                description that contains the phrase \"cats &amp;\n                                dogs\" (caseless). Use \"--match-filters -\" to\n                                interactively ask whether to download each\n                                video\n--no-match-filters              Do not use any --match-filters (default)\n--break-match-filters FILTER    Same as \"--match-filters\" but stops the\n                                download process when a video is rejected\n--no-break-match-filters        Do not use any --break-match-filters (default)\n--no-playlist                   Download only the video, if the URL refers\n                                to a video and a playlist\n--yes-playlist                  Download the playlist, if the URL refers to\n                                a video and a playlist\n--age-limit YEARS               Download only videos suitable for the given\n                                age\n--download-archive FILE         Download only videos not listed in the\n                                archive file. Record the IDs of all\n                                downloaded videos in it\n--no-download-archive           Do not use archive file (default)\n--max-downloads NUMBER          Abort after downloading NUMBER files\n--break-on-existing             Stop the download process when encountering\n                                a file that is in the archive supplied with\n                                the --download-archive option\n--no-break-on-existing          Do not stop the download process when\n                                encountering a file that is in the archive\n                                (default)\n--break-per-input               Alters --max-downloads, --break-on-existing,\n                                --break-match-filters, and autonumber to\n                                reset per input URL\n--no-break-per-input            --break-on-existing and similar options\n                                terminates the entire download queue\n--skip-playlist-after-errors N  Number of allowed failures until the rest of\n                                the playlist is skipped\n</code></pre><pre><code>-N, --concurrent-fragments N    Number of fragments of a dash/hlsnative\n                                video that should be downloaded concurrently\n                                (default is 1)\n-r, --limit-rate RATE           Maximum download rate in bytes per second,\n                                e.g. 50K or 4.2M\n--throttled-rate RATE           Minimum download rate in bytes per second\n                                below which throttling is assumed and the\n                                video data is re-extracted, e.g. 100K\n-R, --retries RETRIES           Number of retries (default is 10), or\n                                \"infinite\"\n--file-access-retries RETRIES   Number of times to retry on file access\n                                error (default is 3), or \"infinite\"\n--fragment-retries RETRIES      Number of retries for a fragment (default is\n                                10), or \"infinite\" (DASH, hlsnative and ISM)\n--retry-sleep [TYPE:]EXPR       Time to sleep between retries in seconds\n                                (optionally) prefixed by the type of retry\n                                (http (default), fragment, file_access,\n                                extractor) to apply the sleep to. EXPR can\n                                be a number, linear=START[:END[:STEP=1]] or\n                                exp=START[:END[:BASE=2]]. This option can be\n                                used multiple times to set the sleep for the\n                                different retry types, e.g. --retry-sleep\n                                linear=1::2 --retry-sleep fragment:exp=1:20\n--skip-unavailable-fragments    Skip unavailable fragments for DASH,\n                                hlsnative and ISM downloads (default)\n                                (Alias: --no-abort-on-unavailable-fragments)\n--abort-on-unavailable-fragments\n                                Abort download if a fragment is unavailable\n                                (Alias: --no-skip-unavailable-fragments)\n--keep-fragments                Keep downloaded fragments on disk after\n                                downloading is finished\n--no-keep-fragments             Delete downloaded fragments after\n                                downloading is finished (default)\n--buffer-size SIZE              Size of download buffer, e.g. 1024 or 16K\n                                (default is 1024)\n--resize-buffer                 The buffer size is automatically resized\n                                from an initial value of --buffer-size\n                                (default)\n--no-resize-buffer              Do not automatically adjust the buffer size\n--http-chunk-size SIZE          Size of a chunk for chunk-based HTTP\n                                downloading, e.g. 10485760 or 10M (default\n                                is disabled). May be useful for bypassing\n                                bandwidth throttling imposed by a webserver\n                                (experimental)\n--playlist-random               Download playlist videos in random order\n--lazy-playlist                 Process entries in the playlist as they are\n                                received. This disables n_entries,\n                                --playlist-random and --playlist-reverse\n--no-lazy-playlist              Process videos in the playlist only after\n                                the entire playlist is parsed (default)\n--xattr-set-filesize            Set file xattribute ytdl.filesize with\n                                expected file size\n--hls-use-mpegts                Use the mpegts container for HLS videos;\n                                allowing some players to play the video\n                                while downloading, and reducing the chance\n                                of file corruption if download is\n                                interrupted. This is enabled by default for\n                                live streams\n--no-hls-use-mpegts             Do not use the mpegts container for HLS\n                                videos. This is default when not downloading\n                                live streams\n--download-sections REGEX       Download only chapters that match the\n                                regular expression. A \"*\" prefix denotes\n                                time-range instead of chapter. Negative\n                                timestamps are calculated from the end.\n                                \"*from-url\" can be used to download between\n                                the \"start_time\" and \"end_time\" extracted\n                                from the URL. Needs ffmpeg. This option can\n                                be used multiple times to download multiple\n                                sections, e.g. --download-sections\n                                \"*10:15-inf\" --download-sections \"intro\"\n--downloader [PROTO:]NAME       Name or path of the external downloader to\n                                use (optionally) prefixed by the protocols\n                                (http, ftp, m3u8, dash, rstp, rtmp, mms) to\n                                use it for. Currently supports native,\n                                aria2c, avconv, axel, curl, ffmpeg, httpie,\n                                wget. You can use this option multiple times\n                                to set different downloaders for different\n                                protocols. E.g. --downloader aria2c\n                                --downloader \"dash,m3u8:native\" will use\n                                aria2c for http/ftp downloads, and the\n                                native downloader for dash/m3u8 downloads\n                                (Alias: --external-downloader)\n--downloader-args NAME:ARGS     Give these arguments to the external\n                                downloader. Specify the downloader name and\n                                the arguments separated by a colon \":\". For\n                                ffmpeg, arguments can be passed to different\n                                positions using the same syntax as\n                                --postprocessor-args. You can use this\n                                option multiple times to give different\n                                arguments to different downloaders (Alias:\n                                --external-downloader-args)\n</code></pre><pre><code>-a, --batch-file FILE           File containing URLs to download (\"-\" for\n                                stdin), one URL per line. Lines starting\n                                with \"#\", \";\" or \"]\" are considered as\n                                comments and ignored\n--no-batch-file                 Do not read URLs from batch file (default)\n-P, --paths [TYPES:]PATH        The paths where the files should be\n                                downloaded. Specify the type of file and the\n                                path separated by a colon \":\". All the same\n                                TYPES as --output are supported.\n                                Additionally, you can also provide \"home\"\n                                (default) and \"temp\" paths. All intermediary\n                                files are first downloaded to the temp path\n                                and then the final files are moved over to\n                                the home path after download is finished.\n                                This option is ignored if --output is an\n                                absolute path\n-o, --output [TYPES:]TEMPLATE   Output filename template; see \"OUTPUT\n                                TEMPLATE\" for details\n--output-na-placeholder TEXT    Placeholder for unavailable fields in\n                                --output (default: \"NA\")\n--restrict-filenames            Restrict filenames to only ASCII characters,\n                                and avoid \"&amp;\" and spaces in filenames\n--no-restrict-filenames         Allow Unicode characters, \"&amp;\" and spaces in\n                                filenames (default)\n--windows-filenames             Force filenames to be Windows-compatible\n--no-windows-filenames          Sanitize filenames only minimally\n--trim-filenames LENGTH         Limit the filename length (excluding\n                                extension) to the specified number of\n                                characters\n-w, --no-overwrites             Do not overwrite any files\n--force-overwrites              Overwrite all video and metadata files. This\n                                option includes --no-continue\n--no-force-overwrites           Do not overwrite the video, but overwrite\n                                related files (default)\n-c, --continue                  Resume partially downloaded files/fragments\n                                (default)\n--no-continue                   Do not resume partially downloaded\n                                fragments. If the file is not fragmented,\n                                restart download of the entire file\n--part                          Use .part files instead of writing directly\n                                into output file (default)\n--no-part                       Do not use .part files - write directly into\n                                output file\n--mtime                         Use the Last-modified header to set the file\n                                modification time (default)\n--no-mtime                      Do not use the Last-modified header to set\n                                the file modification time\n--write-description             Write video description to a .description file\n--no-write-description          Do not write video description (default)\n--write-info-json               Write video metadata to a .info.json file\n                                (this may contain personal information)\n--no-write-info-json            Do not write video metadata (default)\n--write-playlist-metafiles      Write playlist metadata in addition to the\n                                video metadata when using --write-info-json,\n                                --write-description etc. (default)\n--no-write-playlist-metafiles   Do not write playlist metadata when using\n                                --write-info-json, --write-description etc.\n--clean-info-json               Remove some internal metadata such as\n                                filenames from the infojson (default)\n--no-clean-info-json            Write all fields to the infojson\n--write-comments                Retrieve video comments to be placed in the\n                                infojson. The comments are fetched even\n                                without this option if the extraction is\n                                known to be quick (Alias: --get-comments)\n--no-write-comments             Do not retrieve video comments unless the\n                                extraction is known to be quick (Alias:\n                                --no-get-comments)\n--load-info-json FILE           JSON file containing the video information\n                                (created with the \"--write-info-json\" option)\n--cookies FILE                  Netscape formatted file to read cookies from\n                                and dump cookie jar in\n--no-cookies                    Do not read/dump cookies from/to file\n                                (default)\n--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]\n                                The name of the browser to load cookies\n                                from. Currently supported browsers are:\n                                brave, chrome, chromium, edge, firefox,\n                                opera, safari, vivaldi, whale. Optionally,\n                                the KEYRING used for decrypting Chromium\n                                cookies on Linux, the name/path of the\n                                PROFILE to load cookies from, and the\n                                CONTAINER name (if Firefox) (\"none\" for no\n                                container) can be given with their\n                                respective separators. By default, all\n                                containers of the most recently accessed\n                                profile are used. Currently supported\n                                keyrings are: basictext, gnomekeyring,\n                                kwallet, kwallet5, kwallet6\n--no-cookies-from-browser       Do not load cookies from browser (default)\n--cache-dir DIR                 Location in the filesystem where yt-dlp can\n                                store some downloaded information (such as\n                                client ids and signatures) permanently. By\n                                default ${XDG_CACHE_HOME}/yt-dlp\n--no-cache-dir                  Disable filesystem caching\n--rm-cache-dir                  Delete all filesystem cache files\n</code></pre><pre><code>--write-thumbnail               Write thumbnail image to disk\n--no-write-thumbnail            Do not write thumbnail image to disk (default)\n--write-all-thumbnails          Write all thumbnail image formats to disk\n--list-thumbnails               List available thumbnails of each video.\n                                Simulate unless --no-simulate is used\n</code></pre><h2>Internet Shortcut Options:</h2><pre><code>--write-link                    Write an internet shortcut file, depending\n                                on the current platform (.url, .webloc or\n                                .desktop). The URL may be cached by the OS\n--write-url-link                Write a .url Windows internet shortcut. The\n                                OS caches the URL based on the file path\n--write-webloc-link             Write a .webloc macOS internet shortcut\n--write-desktop-link            Write a .desktop Linux internet shortcut\n</code></pre><h2>Verbosity and Simulation Options:</h2><pre><code>-q, --quiet                     Activate quiet mode. If used with --verbose,\n                                print the log to stderr\n--no-quiet                      Deactivate quiet mode. (Default)\n--no-warnings                   Ignore warnings\n-s, --simulate                  Do not download the video and do not write\n                                anything to disk\n--no-simulate                   Download the video even if printing/listing\n                                options are used\n--ignore-no-formats-error       Ignore \"No video formats\" error. Useful for\n                                extracting metadata even if the videos are\n                                not actually available for download\n                                (experimental)\n--no-ignore-no-formats-error    Throw error when no downloadable video\n                                formats are found (default)\n--skip-download                 Do not download the video but write all\n                                related files (Alias: --no-download)\n-O, --print [WHEN:]TEMPLATE     Field name or output template to print to\n                                screen, optionally prefixed with when to\n                                print it, separated by a \":\". Supported\n                                values of \"WHEN\" are the same as that of\n                                --use-postprocessor (default: video).\n                                Implies --quiet. Implies --simulate unless\n                                --no-simulate or later stages of WHEN are\n                                used. This option can be used multiple times\n--print-to-file [WHEN:]TEMPLATE FILE\n                                Append given template to the file. The\n                                values of WHEN and TEMPLATE are the same as\n                                that of --print. FILE uses the same syntax\n                                as the output template. This option can be\n                                used multiple times\n-j, --dump-json                 Quiet, but print JSON information for each\n                                video. Simulate unless --no-simulate is\n                                used. See \"OUTPUT TEMPLATE\" for a\n                                description of available keys\n-J, --dump-single-json          Quiet, but print JSON information for each\n                                URL or infojson passed. Simulate unless\n                                --no-simulate is used. If the URL refers to\n                                a playlist, the whole playlist information\n                                is dumped in a single line\n--force-write-archive           Force download archive entries to be written\n                                as far as no errors occur, even if -s or\n                                another simulation option is used (Alias:\n                                --force-download-archive)\n--newline                       Output progress bar as new lines\n--no-progress                   Do not print progress bar\n--progress                      Show progress bar, even if in quiet mode\n--console-title                 Display progress in console titlebar\n--progress-template [TYPES:]TEMPLATE\n                                Template for progress outputs, optionally\n                                prefixed with one of \"download:\" (default),\n                                \"download-title:\" (the console title),\n                                \"postprocess:\",  or \"postprocess-title:\".\n                                The video's fields are accessible under the\n                                \"info\" key and the progress attributes are\n                                accessible under \"progress\" key. E.g.\n                                --console-title --progress-template\n                                \"download-title:%(info.id)s-%(progress.eta)s\"\n--progress-delta SECONDS        Time between progress output (default: 0)\n-v, --verbose                   Print various debugging information\n--dump-pages                    Print downloaded pages encoded using base64\n                                to debug problems (very verbose)\n--write-pages                   Write downloaded intermediary pages to files\n                                in the current directory to debug problems\n--print-traffic                 Display sent and read HTTP traffic\n</code></pre><pre><code>--encoding ENCODING             Force the specified encoding (experimental)\n--legacy-server-connect         Explicitly allow HTTPS connection to servers\n                                that do not support RFC 5746 secure\n                                renegotiation\n--no-check-certificates         Suppress HTTPS certificate validation\n--prefer-insecure               Use an unencrypted connection to retrieve\n                                information about the video (Currently\n                                supported only for YouTube)\n--add-headers FIELD:VALUE       Specify a custom HTTP header and its value,\n                                separated by a colon \":\". You can use this\n                                option multiple times\n--bidi-workaround               Work around terminals that lack\n                                bidirectional text support. Requires bidiv\n                                or fribidi executable in PATH\n--sleep-requests SECONDS        Number of seconds to sleep between requests\n                                during data extraction\n--sleep-interval SECONDS        Number of seconds to sleep before each\n                                download. This is the minimum time to sleep\n                                when used along with --max-sleep-interval\n                                (Alias: --min-sleep-interval)\n--max-sleep-interval SECONDS    Maximum number of seconds to sleep. Can only\n                                be used along with --min-sleep-interval\n--sleep-subtitles SECONDS       Number of seconds to sleep before each\n                                subtitle download\n</code></pre><pre><code>-f, --format FORMAT             Video format code, see \"FORMAT SELECTION\"\n                                for more details\n-S, --format-sort SORTORDER     Sort the formats by the fields given, see\n                                \"Sorting Formats\" for more details\n--format-sort-force             Force user specified sort order to have\n                                precedence over all fields, see \"Sorting\n                                Formats\" for more details (Alias: --S-force)\n--no-format-sort-force          Some fields have precedence over the user\n                                specified sort order (default)\n--video-multistreams            Allow multiple video streams to be merged\n                                into a single file\n--no-video-multistreams         Only one video stream is downloaded for each\n                                output file (default)\n--audio-multistreams            Allow multiple audio streams to be merged\n                                into a single file\n--no-audio-multistreams         Only one audio stream is downloaded for each\n                                output file (default)\n--prefer-free-formats           Prefer video formats with free containers\n                                over non-free ones of the same quality. Use\n                                with \"-S ext\" to strictly prefer free\n                                containers irrespective of quality\n--no-prefer-free-formats        Don't give any special preference to free\n                                containers (default)\n--check-formats                 Make sure formats are selected only from\n                                those that are actually downloadable\n--check-all-formats             Check all formats for whether they are\n                                actually downloadable\n--no-check-formats              Do not check that the formats are actually\n                                downloadable\n-F, --list-formats              List available formats of each video.\n                                Simulate unless --no-simulate is used\n--merge-output-format FORMAT    Containers that may be used when merging\n                                formats, separated by \"/\", e.g. \"mp4/mkv\".\n                                Ignored if no merge is required. (currently\n                                supported: avi, flv, mkv, mov, mp4, webm)\n</code></pre><pre><code>--write-subs                    Write subtitle file\n--no-write-subs                 Do not write subtitle file (default)\n--write-auto-subs               Write automatically generated subtitle file\n                                (Alias: --write-automatic-subs)\n--no-write-auto-subs            Do not write auto-generated subtitles\n                                (default) (Alias: --no-write-automatic-subs)\n--list-subs                     List available subtitles of each video.\n                                Simulate unless --no-simulate is used\n--sub-format FORMAT             Subtitle format; accepts formats preference\n                                separated by \"/\", e.g. \"srt\" or \"ass/srt/best\"\n--sub-langs LANGS               Languages of the subtitles to download (can\n                                be regex) or \"all\" separated by commas, e.g.\n                                --sub-langs \"en.*,ja\" (where \"en.*\" is a\n                                regex pattern that matches \"en\" followed by\n                                0 or more of any character). You can prefix\n                                the language code with a \"-\" to exclude it\n                                from the requested languages, e.g. --sub-\n                                langs all,-live_chat. Use --list-subs for a\n                                list of available language tags\n</code></pre><pre><code>-u, --username USERNAME         Login with this account ID\n-p, --password PASSWORD         Account password. If this option is left\n                                out, yt-dlp will ask interactively\n-2, --twofactor TWOFACTOR       Two-factor authentication code\n-n, --netrc                     Use .netrc authentication data\n--netrc-location PATH           Location of .netrc authentication data;\n                                either the path or its containing directory.\n                                Defaults to ~/.netrc\n--netrc-cmd NETRC_CMD           Command to execute to get the credentials\n                                for an extractor.\n--video-password PASSWORD       Video-specific password\n--ap-mso MSO                    Adobe Pass multiple-system operator (TV\n                                provider) identifier, use --ap-list-mso for\n                                a list of available MSOs\n--ap-username USERNAME          Multiple-system operator account login\n--ap-password PASSWORD          Multiple-system operator account password.\n                                If this option is left out, yt-dlp will ask\n                                interactively\n--ap-list-mso                   List all supported multiple-system operators\n--client-certificate CERTFILE   Path to client certificate file in PEM\n                                format. May include the private key\n--client-certificate-key KEYFILE\n                                Path to private key file for client\n                                certificate\n--client-certificate-password PASSWORD\n                                Password for client certificate private key,\n                                if encrypted. If not provided, and the key\n                                is encrypted, yt-dlp will ask interactively\n</code></pre><pre><code>-x, --extract-audio             Convert video files to audio-only files\n                                (requires ffmpeg and ffprobe)\n--audio-format FORMAT           Format to convert the audio to when -x is\n                                used. (currently supported: best (default),\n                                aac, alac, flac, m4a, mp3, opus, vorbis,\n                                wav). You can specify multiple rules using\n                                similar syntax as --remux-video\n--audio-quality QUALITY         Specify ffmpeg audio quality to use when\n                                converting the audio with -x. Insert a value\n                                between 0 (best) and 10 (worst) for VBR or a\n                                specific bitrate like 128K (default 5)\n--remux-video FORMAT            Remux the video into another container if\n                                necessary (currently supported: avi, flv,\n                                gif, mkv, mov, mp4, webm, aac, aiff, alac,\n                                flac, m4a, mka, mp3, ogg, opus, vorbis,\n                                wav). If the target container does not\n                                support the video/audio codec, remuxing will\n                                fail. You can specify multiple rules; e.g.\n                                \"aac&gt;m4a/mov&gt;mp4/mkv\" will remux aac to m4a,\n                                mov to mp4 and anything else to mkv\n--recode-video FORMAT           Re-encode the video into another format if\n                                necessary. The syntax and supported formats\n                                are the same as --remux-video\n--postprocessor-args NAME:ARGS  Give these arguments to the postprocessors.\n                                Specify the postprocessor/executable name\n                                and the arguments separated by a colon \":\"\n                                to give the argument to the specified\n                                postprocessor/executable. Supported PP are:\n                                Merger, ModifyChapters, SplitChapters,\n                                ExtractAudio, VideoRemuxer, VideoConvertor,\n                                Metadata, EmbedSubtitle, EmbedThumbnail,\n                                SubtitlesConvertor, ThumbnailsConvertor,\n                                FixupStretched, FixupM4a, FixupM3u8,\n                                FixupTimestamp and FixupDuration. The\n                                supported executables are: AtomicParsley,\n                                FFmpeg and FFprobe. You can also specify\n                                \"PP+EXE:ARGS\" to give the arguments to the\n                                specified executable only when being used by\n                                the specified postprocessor. Additionally,\n                                for ffmpeg/ffprobe, \"_i\"/\"_o\" can be\n                                appended to the prefix optionally followed\n                                by a number to pass the argument before the\n                                specified input/output file, e.g. --ppa\n                                \"Merger+ffmpeg_i1:-v quiet\". You can use\n                                this option multiple times to give different\n                                arguments to different postprocessors.\n                                (Alias: --ppa)\n-k, --keep-video                Keep the intermediate video file on disk\n                                after post-processing\n--no-keep-video                 Delete the intermediate video file after\n                                post-processing (default)\n--post-overwrites               Overwrite post-processed files (default)\n--no-post-overwrites            Do not overwrite post-processed files\n--embed-subs                    Embed subtitles in the video (only for mp4,\n                                webm and mkv videos)\n--no-embed-subs                 Do not embed subtitles (default)\n--embed-thumbnail               Embed thumbnail in the video as cover art\n--no-embed-thumbnail            Do not embed thumbnail (default)\n--embed-metadata                Embed metadata to the video file. Also\n                                embeds chapters/infojson if present unless\n                                --no-embed-chapters/--no-embed-info-json are\n                                used (Alias: --add-metadata)\n--no-embed-metadata             Do not add metadata to file (default)\n                                (Alias: --no-add-metadata)\n--embed-chapters                Add chapter markers to the video file\n                                (Alias: --add-chapters)\n--no-embed-chapters             Do not add chapter markers (default) (Alias:\n                                --no-add-chapters)\n--embed-info-json               Embed the infojson as an attachment to\n                                mkv/mka video files\n--no-embed-info-json            Do not embed the infojson as an attachment\n                                to the video file\n--parse-metadata [WHEN:]FROM:TO\n                                Parse additional metadata like title/artist\n                                from other fields; see \"MODIFYING METADATA\"\n                                for details. Supported values of \"WHEN\" are\n                                the same as that of --use-postprocessor\n                                (default: pre_process)\n--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE\n                                Replace text in a metadata field using the\n                                given regex. This option can be used\n                                multiple times. Supported values of \"WHEN\"\n                                are the same as that of --use-postprocessor\n                                (default: pre_process)\n--xattrs                        Write metadata to the video file's xattrs\n                                (using Dublin Core and XDG standards)\n--concat-playlist POLICY        Concatenate videos in a playlist. One of\n                                \"never\", \"always\", or \"multi_video\"\n                                (default; only when the videos form a single\n                                show). All the video files must have the\n                                same codecs and number of streams to be\n                                concatenable. The \"pl_video:\" prefix can be\n                                used with \"--paths\" and \"--output\" to set\n                                the output filename for the concatenated\n                                files. See \"OUTPUT TEMPLATE\" for details\n--fixup POLICY                  Automatically correct known faults of the\n                                file. One of never (do nothing), warn (only\n                                emit a warning), detect_or_warn (the\n                                default; fix the file if we can, warn\n                                otherwise), force (try fixing even if the\n                                file already exists)\n--ffmpeg-location PATH          Location of the ffmpeg binary; either the\n                                path to the binary or its containing directory\n--exec [WHEN:]CMD               Execute a command, optionally prefixed with\n                                when to execute it, separated by a \":\".\n                                Supported values of \"WHEN\" are the same as\n                                that of --use-postprocessor (default:\n                                after_move). The same syntax as the output\n                                template can be used to pass any field as\n                                arguments to the command. If no fields are\n                                passed, %(filepath,_filename|)q is appended\n                                to the end of the command. This option can\n                                be used multiple times\n--no-exec                       Remove any previously defined --exec\n--convert-subs FORMAT           Convert the subtitles to another format\n                                (currently supported: ass, lrc, srt, vtt).\n                                Use \"--convert-subs none\" to disable\n                                conversion (default) (Alias: --convert-\n                                subtitles)\n--convert-thumbnails FORMAT     Convert the thumbnails to another format\n                                (currently supported: jpg, png, webp). You\n                                can specify multiple rules using similar\n                                syntax as \"--remux-video\". Use \"--convert-\n                                thumbnails none\" to disable conversion\n                                (default)\n--split-chapters                Split video into multiple files based on\n                                internal chapters. The \"chapter:\" prefix can\n                                be used with \"--paths\" and \"--output\" to set\n                                the output filename for the split files. See\n                                \"OUTPUT TEMPLATE\" for details\n--no-split-chapters             Do not split video based on chapters (default)\n--remove-chapters REGEX         Remove chapters whose title matches the\n                                given regular expression. The syntax is the\n                                same as --download-sections. This option can\n                                be used multiple times\n--no-remove-chapters            Do not remove any chapters from the file\n                                (default)\n--force-keyframes-at-cuts       Force keyframes at cuts when\n                                downloading/splitting/removing sections.\n                                This is slow due to needing a re-encode, but\n                                the resulting video may have fewer artifacts\n                                around the cuts\n--no-force-keyframes-at-cuts    Do not force keyframes around the chapters\n                                when cutting/splitting (default)\n--use-postprocessor NAME[:ARGS]\n                                The (case-sensitive) name of plugin\n                                postprocessors to be enabled, and\n                                (optionally) arguments to be passed to it,\n                                separated by a colon \":\". ARGS are a\n                                semicolon \";\" delimited list of NAME=VALUE.\n                                The \"when\" argument determines when the\n                                postprocessor is invoked. It can be one of\n                                \"pre_process\" (after video extraction),\n                                \"after_filter\" (after video passes filter),\n                                \"video\" (after --format; before\n                                --print/--output), \"before_dl\" (before each\n                                video download), \"post_process\" (after each\n                                video download; default), \"after_move\"\n                                (after moving the video file to its final\n                                location), \"after_video\" (after downloading\n                                and processing all formats of a video), or\n                                \"playlist\" (at end of playlist). This option\n                                can be used multiple times to add different\n                                postprocessors\n</code></pre><p>Make chapter entries for, or remove various segments (sponsor, introductions, etc.) from downloaded YouTube videos using the <a href=\"https://sponsor.ajay.app\">SponsorBlock API</a></p><pre><code>--sponsorblock-mark CATS        SponsorBlock categories to create chapters\n                                for, separated by commas. Available\n                                categories are sponsor, intro, outro,\n                                selfpromo, preview, filler, interaction,\n                                music_offtopic, poi_highlight, chapter, all\n                                and default (=all). You can prefix the\n                                category with a \"-\" to exclude it. See [1]\n                                for descriptions of the categories. E.g.\n                                --sponsorblock-mark all,-preview\n                                [1] https://wiki.sponsor.ajay.app/w/Segment_Categories\n--sponsorblock-remove CATS      SponsorBlock categories to be removed from\n                                the video file, separated by commas. If a\n                                category is present in both mark and remove,\n                                remove takes precedence. The syntax and\n                                available categories are the same as for\n                                --sponsorblock-mark except that \"default\"\n                                refers to \"all,-filler\" and poi_highlight,\n                                chapter are not available\n--sponsorblock-chapter-title TEMPLATE\n                                An output template for the title of the\n                                SponsorBlock chapters created by\n                                --sponsorblock-mark. The only available\n                                fields are start_time, end_time, category,\n                                categories, name, category_names. Defaults\n                                to \"[SponsorBlock]: %(category_names)l\"\n--no-sponsorblock               Disable both --sponsorblock-mark and\n                                --sponsorblock-remove\n--sponsorblock-api URL          SponsorBlock API location, defaults to\n                                https://sponsor.ajay.app\n</code></pre><pre><code>--extractor-retries RETRIES     Number of retries for known extractor errors\n                                (default is 3), or \"infinite\"\n--allow-dynamic-mpd             Process dynamic DASH manifests (default)\n                                (Alias: --no-ignore-dynamic-mpd)\n--ignore-dynamic-mpd            Do not process dynamic DASH manifests\n                                (Alias: --no-allow-dynamic-mpd)\n--hls-split-discontinuity       Split HLS playlists to different formats at\n                                discontinuities such as ad breaks\n--no-hls-split-discontinuity    Do not split HLS playlists into different\n                                formats at discontinuities such as ad breaks\n                                (default)\n--extractor-args IE_KEY:ARGS    Pass ARGS arguments to the IE_KEY extractor.\n                                See \"EXTRACTOR ARGUMENTS\" for details. You\n                                can use this option multiple times to give\n                                arguments for different extractors\n</code></pre><p>You can configure yt-dlp by placing any supported command line option in a configuration file. The configuration is loaded from the following locations:</p><ol><li><ul><li>The file given to </li></ul></li><li><p>: (Recommended for portable installations)</p><ul><li>If using a binary,  in the same directory as the binary</li><li>If running from source-code,  in the parent directory of </li></ul></li><li><ul><li> in the home path given to </li><li>If  is not given, the current directory is searched</li></ul></li><li><ul><li><code>${XDG_CONFIG_HOME}/yt-dlp.conf</code></li><li><code>${XDG_CONFIG_HOME}/yt-dlp/config</code> (recommended on Linux/macOS)</li><li><code>${XDG_CONFIG_HOME}/yt-dlp/config.txt</code></li><li> (recommended on Windows)</li><li><code>${APPDATA}/yt-dlp/config.txt</code></li></ul></li><li><ul></ul></li></ol><p>E.g. with the following configuration file, yt-dlp will always extract the audio, not copy the mtime, use a proxy and save all videos under  directory in your home directory:</p><pre><code># Lines starting with # are comments\n\n# Always extract audio\n-x\n\n# Do not copy the mtime\n--no-mtime\n\n# Use this proxy\n--proxy 127.0.0.1:3128\n\n# Save all videos under YouTube directory in your home directory\n-o ~/YouTube/%(title)s.%(ext)s\n</code></pre><p>: Options in a configuration file are just the same options aka switches used in regular command line calls; thus there  after  or , e.g.  or  but not  or . They must also be quoted when necessary, as if it were a UNIX shell.</p><p>You can use  if you want to disable all configuration files for a particular yt-dlp run. If  is found inside any configuration file, no further configuration will be loaded. For example, having the option in the portable configuration file prevents loading of home, user, and system configurations. Additionally, (for backward compatibility) if  is found inside the system configuration file, the user configuration is not loaded.</p><h3>Configuration file encoding</h3><p>The configuration files are decoded according to the UTF BOM if present, and in the encoding from system locale otherwise.</p><p>If you want your file to be decoded differently, add  to the beginning of the file (e.g. ). There must be no characters before that, even spaces or BOM.</p><h3>Authentication with netrc</h3><p>You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with  and ) in order not to pass credentials as command line arguments on every yt-dlp execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a <a href=\"https://stackoverflow.com/tags/.netrc/info\"> file</a> on a per-extractor basis. For that, you will need to create a  file in  and restrict permissions to read/write by only you:</p><pre><code>touch ${HOME}/.netrc\nchmod a-rwx,u+rw ${HOME}/.netrc\n</code></pre><p>After that, you can add credentials for an extractor in the following format, where  is the name of the extractor in lowercase:</p><pre><code>machine &lt;extractor&gt; login &lt;username&gt; password &lt;password&gt;\n</code></pre><pre><code>machine youtube login myaccount@gmail.com password my_youtube_password\nmachine twitch login my_twitch_account_name password my_twitch_password\n</code></pre><p>To activate authentication with the  file you should pass  to yt-dlp or place it in the <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration\">configuration file</a>.</p><p>The default location of the .netrc file is  (see below).</p><p>As an alternative to using the  file, which has the disadvantage of keeping your passwords in a plain text file, you can configure a custom shell command to provide the credentials for an extractor. This is done by providing the  parameter, it shall output the credentials in the netrc format and return  on success, other values will be treated as an error.  in the command will be replaced by the name of the extractor to make it possible to select the credentials for the right extractor.</p><p>E.g. To use an encrypted  file stored as </p><pre><code>yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' 'https://www.youtube.com/watch?v=BaW_jenozKc'\n</code></pre><h3>Notes about environment variables</h3><ul><li>Environment variables are normally specified as / on UNIX and  on Windows; but is always shown as  in this documentation</li><li>yt-dlp also allows using UNIX-style variables on Windows for path-like options; e.g. , </li><li>If unset,  defaults to  and  to </li><li>On Windows,  points to  if present; or,  or  otherwise</li><li>On Windows,  generally points to  and  to <code>${USERPROFILE}\\AppData\\Roaming</code></li></ul><p>The  option is used to indicate a template for the output file names while  option is used to specify the path each type of file should be saved to.</p><p>The simplest usage of  is not to set any template arguments when downloading a single file, like in <code>yt-dlp -o funny_video.flv \"https://some/video\"</code> (hard-coding file extension like this is  recommended and could break some post-processing).</p><p>It may however also contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to <a href=\"https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting\">Python string formatting operations</a>, e.g.  or . To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations.</p><p>The field names themselves (the part inside the parenthesis) can also have some special formatting:</p><ol><li><p>: The dictionaries and lists available in metadata can be traversed by using a dot  separator; e.g. , . You can do Python slicing with colon ; E.g. , , . Curly braces  can be used to build dictionaries with only specific keys; e.g. <code>%(formats.:.{format_id,height})#j</code>. An empty field name  refers to the entire infodict; e.g. . Note that all the fields that become available using this method are not listed below. Use  to see such fields</p></li><li><p>: Simple arithmetic can be done on numeric fields using ,  and . E.g. , <code>%(n_entries+1-playlist_index)d</code></p></li><li><p>: Date/time fields can be formatted according to <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\">strftime formatting</a> by specifying it separated from the field name using a . E.g. , , </p></li><li><p>: Alternate fields can be specified separated with a . E.g. <code>%(release_date&gt;%Y,upload_date&gt;%Y|Unknown)s</code></p></li><li><p>: A replacement value can be specified using a  separator according to the <a href=\"https://docs.python.org/3/library/string.html#format-specification-mini-language\"> mini-language</a>. If the field is  empty, this replacement value will be used instead of the actual field content. This is done after alternate fields are considered; thus the replacement is used if  of the alternative fields is  empty. E.g. <code>%(chapters&amp;has chapters|no chapters)s</code>, <code>%(title&amp;TITLE={:&gt;20}|NO TITLE)s</code></p></li><li><p>: A literal default value can be specified for when the field is empty using a  separator. This overrides . E.g. </p></li><li><p>: In addition to the normal format types , yt-dlp additionally supports converting to  = ytes,  = son (flag  for pretty-printing,  for Unicode),  = HTML escaping,  = a comma separated ist (flag  for  newline-separated),  = a string uoted for the terminal (flag  to split a list into different arguments),  = add ecimal suffixes (e.g. 10M) (flag  to use 1024 as factor), and  = anitize as filename (flag  for restricted)</p></li><li><p>: The format type  can be used for NFC <a href=\"https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize\">Unicode normalization</a>. The alternate form flag () changes the normalization to NFD and the conversion flag  can be used for NFKC/NFKD compatibility equivalence normalization. E.g.  is NFKC</p></li></ol><p>To summarize, the general syntax for a field is:</p><pre><code>%(name[.keys][addition][&gt;strf][,alternate][&amp;replacement][|default])[flags][width][.precision][length]type\n</code></pre><p>Additionally, you can set different output templates for the various metadata files separately from the general output template by specifying the type of file followed by the template separated by a colon . The different file types supported are , , ,  (deprecated), , , , , , , . E.g. <code>-o \"%(title)s.%(ext)s\" -o \"thumbnail:%(title)s\\%(title)s.%(ext)s\"</code> will put the thumbnails in a folder with the same name as the video. If any of the templates is empty, that type of file will not be written. E.g. <code>--write-thumbnail -o \"thumbnail:\"</code> will write thumbnails only for playlists and not for video.</p><p>: Due to post-processing (i.e. merging etc.), the actual output filename might differ. Use <code>--print after_move:filepath</code> to get the name after all post-processing is complete.</p><p>The available fields are:</p><ul><li> (string): Video identifier</li><li> (string): Video title</li><li> (string): Video title ignoring live timestamp and generic title</li><li> (string): Video filename extension</li><li> (string): A secondary title of the video</li><li> (string): The description of the video</li><li> (string): An alternative identifier for the video</li><li> (string): Full name of the video uploader</li><li> (string): Nickname or id of the video uploader</li><li> (string): URL to the video uploader's profile</li><li> (string): License name the video is licensed under</li><li> (list): The creators of the video</li><li> (string): The creators of the video; comma-separated</li><li> (numeric): UNIX timestamp of the moment the video became available</li><li> (string): Video upload date in UTC (YYYYMMDD)</li><li> (numeric): UNIX timestamp of the moment the video was released</li><li> (string): The date (YYYYMMDD) when the video was released in UTC</li><li> (numeric): Year (YYYY) when the video or album was released</li><li> (numeric): UNIX timestamp of the moment the video was last modified</li><li> (string): The date (YYYYMMDD) when the video was last modified in UTC</li><li> (string): Full name of the channel the video is uploaded on</li><li> (string): Id of the channel</li><li> (string): URL of the channel</li><li> (numeric): Number of followers of the channel</li><li> (boolean): Whether the channel is verified on the platform</li><li> (string): Physical location where the video was filmed</li><li> (numeric): Length of the video in seconds</li><li> (string): Length of the video (HH:mm:ss)</li><li> (numeric): How many users have watched the video on the platform</li><li> (numeric): How many users are currently watching the video on the platform.</li><li> (numeric): Number of positive ratings of the video</li><li> (numeric): Number of negative ratings of the video</li><li> (numeric): Number of reposts of the video</li><li> (numeric): Average rating given by users, the scale used depends on the webpage</li><li> (numeric): Number of comments on the video (For some extractors, comments are only downloaded at the end, and so this field cannot be used)</li><li> (numeric): Age restriction for the video (years)</li><li> (string): One of \"not_live\", \"is_live\", \"is_upcoming\", \"was_live\", \"post_live\" (was live, but VOD is not yet processed)</li><li> (boolean): Whether this video is a live stream or a fixed-length video</li><li> (boolean): Whether this video was originally a live stream</li><li> (string): Whether this video is allowed to play in embedded players on other sites</li><li> (string): Whether the video is \"private\", \"premium_only\", \"subscriber_only\", \"needs_auth\", \"unlisted\" or \"public\"</li><li> (string): The type of media as classified by the site, e.g. \"episode\", \"clip\", \"trailer\"</li><li> (numeric): Time in seconds where the reproduction should start, as specified in the URL</li><li> (numeric): Time in seconds where the reproduction should end, as specified in the URL</li><li> (string): Name of the extractor</li><li> (string): Key name of the extractor</li><li> (numeric): Unix epoch of when the information extraction was completed</li><li> (numeric): Number that will be increased with each download, starting at , padded with leading zeros to 5 digits</li><li> (numeric): Number that will be increased with each video</li><li> (numeric): Total number of extracted items in the playlist</li><li> (string): Identifier of the playlist that contains the video</li><li> (string): Name of the playlist that contains the video</li><li> (string):  if available or else </li><li> (numeric): Total number of items in the playlist. May not be known if entire playlist is not extracted</li><li> (numeric): Index of the video in the playlist padded with leading zeros according the final index</li><li> (numeric): Position of the video in the playlist download queue padded with leading zeros according to the total length of the playlist</li><li> (string): Full name of the playlist uploader</li><li> (string): Nickname or id of the playlist uploader</li><li> (string): Display name of the channel that uploaded the playlist</li><li> (string): Identifier of the channel that uploaded the playlist</li><li> (string): URL of the playlist webpage</li><li> (string): A URL to the video webpage which, if given to yt-dlp, should yield the same result again</li><li> (string): The basename of the webpage URL</li><li> (string): The domain of the webpage URL</li><li> (string): The URL given by the user (or the same as  for playlist entries)</li><li> (list): List of categories the video belongs to</li><li> (list): List of tags assigned to the video</li><li> (list): List of cast members</li></ul><p>Available for the video that belongs to some logical chapter or section:</p><ul><li> (string): Name or title of the chapter the video belongs to</li><li> (numeric): Number of the chapter the video belongs to</li><li> (string): Id of the chapter the video belongs to</li></ul><p>Available for the video that is an episode of some series or program:</p><ul><li> (string): Title of the series or program the video episode belongs to</li><li> (string): Id of the series or program the video episode belongs to</li><li> (string): Title of the season the video episode belongs to</li><li> (numeric): Number of the season the video episode belongs to</li><li> (string): Id of the season the video episode belongs to</li><li> (string): Title of the video episode</li><li> (numeric): Number of the video episode within a season</li><li> (string): Id of the video episode</li></ul><p>Available for the media that is a track or a part of a music album:</p><ul><li> (string): Title of the track</li><li> (numeric): Number of the track within an album or a disc</li><li> (string): Id of the track</li><li> (list): Artist(s) of the track</li><li> (string): Artist(s) of the track; comma-separated</li><li> (list): Genre(s) of the track</li><li> (string): Genre(s) of the track; comma-separated</li><li> (list): Composer(s) of the piece</li><li> (string): Composer(s) of the piece; comma-separated</li><li> (string): Title of the album the track belongs to</li><li> (string): Type of the album</li><li> (list): All artists appeared on the album</li><li> (string): All artists appeared on the album; comma-separated</li><li> (numeric): Number of the disc or other physical medium the track belongs to</li></ul><p>Available only when using  and for  prefix when using  for videos with internal chapters:</p><ul><li> (string): Title of the chapter</li><li> (numeric): Number of the chapter within the file</li><li> (numeric): Start time of the chapter in seconds</li><li> (numeric): End time of the chapter in seconds</li></ul><p>Available only when used in :</p><ul><li> (string): The URLs of all requested formats, one in each line</li><li> (table): The video format table as printed by </li><li> (table): The thumbnail format table as printed by </li><li> (table): The subtitle format table as printed by </li><li> (table): The automatic subtitle format table as printed by </li></ul><p>Available only after the video is downloaded (/):</p><ul><li>: Actual path of downloaded video file</li></ul><p>Available only in <code>--sponsorblock-chapter-title</code>:</p><ul><li> (numeric): Start time of the chapter in seconds</li><li> (numeric): End time of the chapter in seconds</li><li> (string): The smallest SponsorBlock category the chapter belongs to</li><li> (list): Friendly names of the categories</li><li> (string): Friendly name of the smallest category</li></ul><p>Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. E.g. for <code>-o %(title)s-%(id)s.%(ext)s</code> and an mp4 video with title  and id , this will result in a <code>yt-dlp test video-BaW_jenozKc.mp4</code> file created in the current directory.</p><p>: Some of the sequences are not guaranteed to be present, since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with  ( by default).</p><p>: Look at the  output to identify which fields are available for the particular URL</p><p>For numeric sequences, you can use <a href=\"https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting\">numeric related formatting</a>; e.g.  will result in a string with view count padded with zeros up to 5 characters, like in .</p><p>Output templates can also contain arbitrary hierarchical path, e.g. <code>-o \"%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\"</code> which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.</p><p>To use percent literals in an output template use . To output to stdout use .</p><p>The current default template is <code>%(title)s [%(id)s].%(ext)s</code>.</p><p>In some cases, you don't want special characters such as ‰∏≠, spaces, or &amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the  flag to get a shorter title.</p><pre><code>$ yt-dlp --print filename -o \"test video.%(ext)s\" BaW_jenozKc\ntest video.webm    # Literal name with correct extension\n\n$ yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc\nyoutube-dl test video ''_√§‚Ü≠ùïê.webm    # All kinds of weird characters\n\n$ yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc --restrict-filenames\nyoutube-dl_test_video_.webm    # Restricted file name\n\n# Download YouTube playlist videos in separate directory indexed by video order in a playlist\n$ yt-dlp -o \"%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\n# Download YouTube playlist videos in separate directories according to their uploaded year\n$ yt-dlp -o \"%(upload_date&gt;%Y)s/%(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\n# Prefix playlist index with \" - \" separator, but only if it is available\n$ yt-dlp -o \"%(playlist_index&amp;{} - |)s%(title)s.%(ext)s\" BaW_jenozKc \"https://www.youtube.com/user/TheLinuxFoundation/playlists\"\n\n# Download all playlists of YouTube channel/user keeping each playlist in separate directory:\n$ yt-dlp -o \"%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/user/TheLinuxFoundation/playlists\"\n\n# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home\n$ yt-dlp -u user -p password -P \"~/MyVideos\" -o \"%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s\" \"https://www.udemy.com/java-tutorial\"\n\n# Download entire series season keeping each series and each season in separate directory under C:/MyVideos\n$ yt-dlp -P \"C:/MyVideos\" -o \"%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s\" \"https://videomore.ru/kino_v_detalayah/5_sezon/367617\"\n\n# Download video as \"C:\\MyVideos\\uploader\\title.ext\", subtitles as \"C:\\MyVideos\\subs\\uploader\\title.ext\"\n# and put all temporary files in \"C:\\MyVideos\\tmp\"\n$ yt-dlp -P \"C:/MyVideos\" -P \"temp:tmp\" -P \"subtitle:subs\" -o \"%(uploader)s/%(title)s.%(ext)s\" BaW_jenozKc --write-subs\n\n# Download video as \"C:\\MyVideos\\uploader\\title.ext\" and subtitles as \"C:\\MyVideos\\uploader\\subs\\title.ext\"\n$ yt-dlp -P \"C:/MyVideos\" -o \"%(uploader)s/%(title)s.%(ext)s\" -o \"subtitle:%(uploader)s/subs/%(title)s.%(ext)s\" BaW_jenozKc --write-subs\n\n# Stream the video being downloaded to stdout\n$ yt-dlp -o - BaW_jenozKc\n</code></pre><p>By default, yt-dlp tries to download the best available quality if you  pass any options. This is generally equivalent to using <code>-f bestvideo*+bestaudio/best</code>. However, if multiple audiostreams is enabled (), the default format changes to <code>-f bestvideo+bestaudio/best</code>. Similarly, if ffmpeg is unavailable, or if you use yt-dlp to stream to  (), the default becomes <code>-f best/bestvideo+bestaudio</code>.</p><p>: Latest versions of yt-dlp can stream multiple formats to the stdout simultaneously using ffmpeg. So, in future versions, the default for this will be set to  similar to normal downloads. If you want to preserve the  setting, it is recommended to explicitly specify it in the configuration options.</p><p>The general syntax for format selection is  (or ) where  is a , i.e. an expression that describes format or formats you would like to download.</p><p>The simplest case is requesting a specific format; e.g. with  you can download the format with format code equal to 22. You can get the list of available format codes for particular video using  or . Note that these format codes are extractor specific.</p><p>You can also use a file extension (currently , , , , , , , ,  are supported) to download the best quality format of a particular file extension served as a single file, e.g.  will download the best quality format with the  extension served as a single file.</p><p>You can use  to interactively provide the format selector </p><p>You can also use special names to select particular edge case formats:</p><ul><li>: Select  separately</li><li>: Select and  (Must be used with ,  or both)</li><li>, : Select the best quality format that  a video or an audio or both (i.e.; <code>vcodec!=none or acodec!=none</code>)</li><li>, : Select the best quality format that  video and audio. Equivalent to <code>best*[vcodec!=none][acodec!=none]</code></li><li>, : Select the best quality  format. Equivalent to </li><li>, : Select the best quality format that . It may also contain audio. Equivalent to </li><li>, : Select the best quality  format. Equivalent to </li><li>, : Select the best quality format that . It may also contain video. Equivalent to  (<a href=\"https://github.com/yt-dlp/yt-dlp/issues/979#issuecomment-919629354\">Do not use!</a>)</li><li>, : Select the worst quality format that contains either a video or an audio</li><li>, : Select the worst quality format that contains both video and audio. Equivalent to <code>worst*[vcodec!=none][acodec!=none]</code></li><li>, : Select the worst quality video-only format. Equivalent to </li><li>, : Select the worst quality format that contains video. It may also contain audio. Equivalent to </li><li>, : Select the worst quality audio-only format. Equivalent to </li><li>, : Select the worst quality format that contains audio. It may also contain video. Equivalent to </li></ul><p>For example, to download the worst quality video-only format you can use . It is, however, recommended not to use  and related options. When your format selector is , the format which is worst in all respects is selected. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use  or more rigorously,  instead of . See <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats\">Sorting Formats</a> for more details.</p><p>You can select the n'th best format of a type by using . For example,  will select the 2nd best combined format. Similarly,  will select the 3rd best format that contains a video stream.</p><p>If you want to download multiple videos, and they don't have the same formats available, you can specify the order of preference using slashes. Note that formats on the left hand side are preferred; e.g.  will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.</p><p>If you want to download several formats of the same video use a comma as a separator, e.g.  will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: <code>-f 136/137/mp4/bestvideo,140/m4a/bestaudio</code>.</p><p>You can merge the video and audio of multiple formats into a single file using <code>-f &lt;format1&gt;+&lt;format2&gt;+...</code> (requires ffmpeg installed); e.g.  will download the best video-only format, the best audio-only format and mux them together with ffmpeg.</p><p>: Since the  described behavior is complex and counter-intuitive, this will be removed and multistreams will be enabled by default in the future. A new operator will be instead added to limit formats to single audio/video</p><p>Unless  is used, all formats with a video stream except the first one are ignored. Similarly, unless  is used, all formats with an audio stream except the first one are ignored. E.g. <code>-f bestvideo+best+bestaudio --video-multistreams --audio-multistreams</code> will download and merge all 3 given formats. The resulting file will have 2 video streams and 2 audio streams. But <code>-f bestvideo+best+bestaudio --no-video-multistreams</code> will download and merge only  and .  is ignored since another format containing a video stream () has already been selected. The order of the formats is therefore important. <code>-f best+bestaudio --no-audio-multistreams</code> will download only  while <code>-f bestaudio+best --no-audio-multistreams</code> will ignore  and download only .</p><p>You can also filter the video formats by putting a condition in brackets, as in  (or  since filters without a selector are interpreted as ).</p><p>The following numeric meta fields can be used with comparisons , , , ,  (equals),  (not equals):</p><ul><li>: The number of bytes, if known in advance</li><li>: An estimate for the number of bytes</li><li>: Width of the video, if known</li><li>: Height of the video, if known</li><li>: Aspect ratio of the video, if known</li><li>: Average bitrate of audio and video in <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#%23\" title=\"1000 bits/sec\">kbps</a></li><li>: Average audio bitrate in <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#%23\" title=\"1000 bits/sec\">kbps</a></li><li>: Average video bitrate in <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#%23\" title=\"1000 bits/sec\">kbps</a></li><li>: Audio sampling rate in Hertz</li><li>: The number of audio channels</li><li>:  of the video's pixels, if not square</li></ul><p>Also filtering work for comparisons  (equals),  (starts with),  (ends with),  (contains),  (matches regex) and following string meta fields:</p><ul><li>: Name of the audio codec in use</li><li>: Name of the video codec in use</li><li>: Name of the container format</li><li>: The protocol that will be used for the actual download, lower-case (, , , , , , , , , , or )</li><li>: The dynamic range of the video</li><li>: A short description of the format</li><li>: A human-readable description of the format</li><li>: Additional info about the format</li><li>: Textual description of width and height</li></ul><p>Any string comparison may be prefixed with negation  in order to produce an opposite comparison, e.g.  (does not contain). The comparand of a string comparison needs to be quoted with either double or single quotes if it contains spaces or special characters other than .</p><p>: None of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by the particular extractor, i.e. the metadata offered by the website. Any other field made available by the extractor can also be used for filtering.</p><p>Formats for which the value is not known are excluded unless you put a question mark () after the operator. You can combine format filters, so <code>-f \"bv[height&lt;=?720][tbr&gt;500]\"</code> selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 kbps. You can also use the filters with  to download all formats that satisfy the filter, e.g.  selects all audio-only formats.</p><p>Format selectors can also be grouped using parentheses; e.g. <code>-f \"(mp4,webm)[height&lt;480]\"</code> will download the best pre-merged mp4 and webm formats with a height lower than 480.</p><p>You can change the criteria for being considered the  by using  (). The general format for this is <code>--format-sort field1,field2...</code>.</p><p>The available fields are:</p><ul><li>: Gives priority to formats that have a video stream</li><li>: Gives priority to formats that have an audio stream</li><li>: The format preference</li><li>: The language preference</li><li>: The quality of the format</li><li>: The preference of the source</li><li>: Protocol used for download (/ &gt; / &gt; / &gt; &gt;  &gt; / &gt; /)</li><li>: Video Codec ( &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  &gt; other)</li><li>: Audio Codec (/ &gt; / &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  &gt; other)</li><li>: Equivalent to </li><li>: Video Extension ( &gt;  &gt;  &gt;  &gt; other). If  is used,  is preferred.</li><li>: Audio Extension ( &gt;  &gt;  &gt;  &gt;  &gt;  &gt; other). If  is used, the order changes to  &gt;  &gt;  &gt;  &gt;  &gt; </li><li>: Equivalent to </li><li>: Exact filesize, if known in advance</li><li>: Approximate filesize</li><li>: Exact filesize if available, otherwise approximate filesize</li><li>: Video resolution, calculated as the smallest dimension.</li><li>: The dynamic range of the video ( &gt;  &gt;  &gt;  &gt;  &gt; )</li><li>: The number of audio channels</li><li>: Total average bitrate in <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#%23\" title=\"1000 bits/sec\">kbps</a></li><li>: Average video bitrate in <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#%23\" title=\"1000 bits/sec\">kbps</a></li><li>: Average audio bitrate in <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#%23\" title=\"1000 bits/sec\">kbps</a></li><li>: Average bitrate in <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#%23\" title=\"1000 bits/sec\">kbps</a>, //</li><li>: Audio sample rate in Hz</li></ul><p>: Many of these fields have (currently undocumented) aliases, that may be removed in a future version. It is recommended to use only the documented field names.</p><p>All fields, unless specified otherwise, are sorted in descending order. To reverse this, prefix the field with a . E.g.  prefers format with the smallest resolution. Additionally, you can suffix a preferred value for the fields, separated by a . E.g.  prefers larger videos, but no larger than 720p and the smallest video if there are no videos less than 720p. For  and , you can provide two preferred values, the first for video and the second for audio. E.g.  (equivalent to ) sets the video codec preference to  &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  &gt;  and audio codec preference to  &gt;  &gt;  &gt;  &gt;  &gt;  &gt; . You can also make the sorting prefer the nearest values to the provided by using  as the delimiter. E.g.  prefers the format with filesize closest to 1 GiB.</p><p>The fields  and  are always given highest priority in sorting, irrespective of the user-defined order. This behavior can be changed by using . Apart from these, the default order used is: <code>lang,quality,res,fps,hdr:12,vcodec,channels,acodec,size,br,asr,proto,ext,hasaud,source,id</code>. The extractors may override this default order, but they cannot override the user-provided order.</p><p>Note that the default for hdr is ; i.e. Dolby Vision is not preferred. This choice was made since DV formats are not yet fully compatible with most devices. This may be changed in the future.</p><p>If your format selector is , the last item is selected after sorting. This means it will select the format that is worst in all respects. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use <code>-f best -S +size,+br,+res,+fps</code>.</p><p>: You can use the  to see how the formats have been sorted (worst to best).</p><h2>Format Selection examples</h2><pre><code># Download and merge the best video-only format and the best audio-only format,\n# or download the best combined format if video-only format is not available\n$ yt-dlp -f \"bv+ba/b\"\n\n# Download best format that contains video,\n# and if it doesn't already have an audio stream, merge it with best audio-only format\n$ yt-dlp -f \"bv*+ba/b\"\n\n# Same as above\n$ yt-dlp\n\n# Download the best video-only format and the best audio-only format without merging them\n# For this case, an output template should be used since\n# by default, bestvideo and bestaudio will have the same file name.\n$ yt-dlp -f \"bv,ba\" -o \"%(title)s.f%(format_id)s.%(ext)s\"\n\n# Download and merge the best format that has a video stream,\n# and all audio-only formats into one file\n$ yt-dlp -f \"bv*+mergeall[vcodec=none]\" --audio-multistreams\n\n# Download and merge the best format that has a video stream,\n# and the best 2 audio-only formats into one file\n$ yt-dlp -f \"bv*+ba+ba.2\" --audio-multistreams\n\n\n# The following examples show the old method (without -S) of format selection\n# and how to use -S to achieve a similar but (generally) better result\n\n# Download the worst video available (old method)\n$ yt-dlp -f \"wv*+wa/w\"\n\n# Download the best video available but with the smallest resolution\n$ yt-dlp -S \"+res\"\n\n# Download the smallest video available\n$ yt-dlp -S \"+size,+br\"\n\n\n\n# Download the best mp4 video available, or the best video if no mp4 available\n$ yt-dlp -f \"bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b\"\n\n# Download the best video with the best extension\n# (For video, mp4 &gt; mov &gt; webm &gt; flv. For audio, m4a &gt; aac &gt; mp3 ...)\n$ yt-dlp -S \"ext\"\n\n\n\n# Download the best video available but no better than 480p,\n# or the worst video if there is no video under 480p\n$ yt-dlp -f \"bv*[height&lt;=480]+ba/b[height&lt;=480] / wv*+ba/w\"\n\n# Download the best video available with the largest height but no better than 480p,\n# or the best video with the smallest resolution if there is no video under 480p\n$ yt-dlp -S \"height:480\"\n\n# Download the best video available with the largest resolution but no better than 480p,\n# or the best video with the smallest resolution if there is no video under 480p\n# Resolution is determined by using the smallest dimension.\n# So this works correctly for vertical videos as well\n$ yt-dlp -S \"res:480\"\n\n\n\n# Download the best video (that also has audio) but no bigger than 50 MB,\n# or the worst video (that also has audio) if there is no video under 50 MB\n$ yt-dlp -f \"b[filesize&lt;50M] / w\"\n\n# Download the largest video (that also has audio) but no bigger than 50 MB,\n# or the smallest video (that also has audio) if there is no video under 50 MB\n$ yt-dlp -f \"b\" -S \"filesize:50M\"\n\n# Download the best video (that also has audio) that is closest in size to 50 MB\n$ yt-dlp -f \"b\" -S \"filesize~50M\"\n\n\n\n# Download best video available via direct link over HTTP/HTTPS protocol,\n# or the best video available via any protocol if there is no such video\n$ yt-dlp -f \"(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)\"\n\n# Download best video available via the best protocol\n# (https/ftps &gt; http/ftp &gt; m3u8_native &gt; m3u8 &gt; http_dash_segments ...)\n$ yt-dlp -S \"proto\"\n\n\n\n# Download the best video with either h264 or h265 codec,\n# or the best video if there is no such video\n$ yt-dlp -f \"(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)\"\n\n# Download the best video with best codec no better than h264,\n# or the best video with worst codec if there is no such video\n$ yt-dlp -S \"codec:h264\"\n\n# Download the best video with worst codec no worse than h264,\n# or the best video with best codec if there is no such video\n$ yt-dlp -S \"+codec:h264\"\n\n\n\n# More complex examples\n\n# Download the best video no better than 720p preferring framerate greater than 30,\n# or the worst video (still preferring framerate greater than 30) if there is no such video\n$ yt-dlp -f \"((bv*[fps&gt;30]/bv*)[height&lt;=720]/(wv*[fps&gt;30]/wv*)) + ba / (b[fps&gt;30]/b)[height&lt;=720]/(w[fps&gt;30]/w)\"\n\n# Download the video with the largest resolution no better than 720p,\n# or the video with the smallest resolution available if there is no such video,\n# preferring larger framerate for formats with the same resolution\n$ yt-dlp -S \"res:720,fps\"\n\n\n\n# Download the video with smallest resolution no worse than 480p,\n# or the video with the largest resolution available if there is no such video,\n# preferring better codec and then larger total bitrate for the same resolution\n$ yt-dlp -S \"+res:480,codec,br\"\n</code></pre><p>The metadata obtained by the extractors can be modified by using  and </p><p>The general syntax of  is to give the name of a field or an <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template\">output template</a> to extract data from, and the format to interpret it as, separated by a colon . Either a <a href=\"https://docs.python.org/3/library/re.html#regular-expression-syntax\">Python regular expression</a> with named capture groups, a single field name, or a similar syntax to the <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template\">output template</a> (only  formatting is supported) can be used for . The option can be used multiple times to parse and modify various fields.</p><p>Note that these options preserve their relative order, allowing replacements to be made in parsed fields and vice versa. Also, any field thus created can be used in the <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template\">output template</a> and will also affect the media file's metadata added when using .</p><p>This option also has a few special uses:</p><ul><li><p>You can download an additional URL based on the metadata of the currently downloaded video. To do this, set the field  to the URL that you want to download. E.g. <code>--parse-metadata \"description:(?P&lt;additional_urls&gt;https?://www\\.vimeo\\.com/\\d+)\"</code> will download the first vimeo video found in the description</p></li><li><p>You can use this to change the metadata that is embedded in the media file. To do this, set the value of the corresponding field with a  prefix. For example, any value you set to  field will be added to the  field in the file - you can use this to set a different \"description\" and \"synopsis\". To modify the metadata of individual streams, use the  prefix (e.g. ). Any value set to the  field will overwrite all default values.</p></li></ul><p>: Metadata modification happens before format selection, post-extraction and other post-processing operations. Some fields may be added or changed during these steps, overriding your changes.</p><p>For reference, these are the fields yt-dlp adds by default to the file metadata:</p><table><tbody><tr></tr><tr><td align=\"left\">, , , ,  or </td></tr><tr></tr><tr><td align=\"left\"> or </td></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>: The file format may not support some of these fields</p><h2>Modifying metadata examples</h2><pre><code># Interpret the title as \"Artist - Title\"\n$ yt-dlp --parse-metadata \"title:%(artist)s - %(title)s\"\n\n# Regex example\n$ yt-dlp --parse-metadata \"description:Artist - (?P&lt;artist&gt;.+)\"\n\n# Set title as \"Series name S01E05\"\n$ yt-dlp --parse-metadata \"%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s\"\n\n# Prioritize uploader as the \"artist\" field in video metadata\n$ yt-dlp --parse-metadata \"%(uploader|)s:%(meta_artist)s\" --embed-metadata\n\n# Set \"comment\" field in video metadata using description instead of webpage_url,\n# handling multiple lines correctly\n$ yt-dlp --parse-metadata \"description:(?s)(?P&lt;meta_comment&gt;.+)\" --embed-metadata\n\n# Do not set any \"synopsis\" in the video metadata\n$ yt-dlp --parse-metadata \":(?P&lt;meta_synopsis&gt;)\"\n\n# Remove \"formats\" field from the infojson by setting it to an empty string\n$ yt-dlp --parse-metadata \"video::(?P&lt;formats&gt;)\" --write-info-json\n\n# Replace all spaces and \"_\" in title and uploader with a `-`\n$ yt-dlp --replace-in-metadata \"title,uploader\" \"[ _]\" \"-\"\n\n</code></pre><p>Some extractors accept additional arguments which can be passed using <code>--extractor-args KEY:ARGS</code>.  is a  (semicolon) separated string of . E.g. <code>--extractor-args \"youtube:player-client=tv,mweb;formats=incomplete\" --extractor-args \"twitter:api=syndication\"</code></p><p>Note: In CLI,  can use  instead of ; e.g.  becomes </p><p>The following extractors use this feature:</p><ul><li>: Prefer translated metadata (,  etc) of this language code (case-sensitive). By default, the video primary language metadata is preferred, with a fallback to  translated. See <a href=\"https://github.com/yt-dlp/yt-dlp/raw/c26f9b991a0681fd3ea548d535919cec1fbbd430/yt_dlp/extractor/youtube.py#L381-L390\">youtube.py</a> for list of supported content language codes</li><li>: One or more of ,  or  to skip extraction of the m3u8 manifests, dash manifests and <a href=\"https://github.com/yt-dlp/yt-dlp/issues/4090#issuecomment-1158102032\">auto-translated subtitles</a> respectively</li><li>: Clients to extract video data from. The main clients are ,  and , with variants  and  (e.g. ); and , , , ,  and  with no variants. By default,  is used, or  is used when authenticating with cookies. The  client is added for  URLs when logged-in cookies are used. The  and  clients are added for age-restricted videos if account age-verification is required. Some clients, such as  and , require a  for their formats to be downloadable. Some clients, such as the  variants, will only work with authentication. Not all clients support authentication via cookies. You can use  for the default clients, or you can use  for all clients (not recommended). You can prefix a client with  to exclude it, e.g. <code>youtube:player_client=default,-ios</code></li><li>: Skip some network requests that are generally needed for robust extraction. One or more of  (skip client configs),  (skip initial webpage),  (skip js player). While these options can help reduce the number of requests needed or avoid some rate-limiting, they could cause some issues. See <a href=\"https://github.com/yt-dlp/yt-dlp/pull/860\">#860</a> for more details</li><li>: YouTube player parameters to use for player requests. Will overwrite any default ones set by yt-dlp.</li><li>:  or  (default) - choose comment sorting mode (on YouTube's side)</li><li>: Limit the amount of comments to gather. Comma-separated list of integers representing <code>max-comments,max-parents,max-replies,max-replies-per-thread</code>. Default is <ul><li>E.g.  will get a maximum of 1000 replies total, with up to 10 replies per thread.  will get a maximum of 1000 comments, with a maximum of 100 replies total</li></ul></li><li>: Change the types of formats to return.  (convert HTTP to DASH),  (identical content but different URLs or protocol; includes ),  (cannot be downloaded completely - live dash and post-live m3u8),  (include formats that require a PO Token but are missing one)</li><li>: Innertube API host to use for all API requests; e.g. , . Note that cookies exported from one subdomain will not work on others</li><li>: Innertube API key to use for all API requests. By default, no API key is used</li><li>:  raises an error instead of reporting a warning</li><li>: Overrides the account Data Sync ID used in Innertube API requests. This may be needed if you are using an account with <code>youtube:player_skip=webpage,configs</code> or </li><li>: Overrides the Visitor Data used in Innertube API requests. This should be used with <code>player_skip=webpage,configs</code> and without cookies. Note: this may have adverse effects if used improperly. If a session from a browser is wanted, you should pass cookies instead (which contain the Visitor ID)</li><li>: Proof of Origin (PO) Token(s) to use. Comma seperated list of PO Tokens in the format , e.g. <code>youtube:po_token=web.gvs+XXX,web.player=XXX,web_safari.gvs+YYY</code>. Context can be either  (Google Video Server URLs) or  (Innertube player request)</li></ul><h4>youtubetab (YouTube playlists, channels, feeds, etc.)</h4><ul><li>: One or more of  (skip initial webpage download),  (allow the download of playlists requiring authentication when no initial webpage is downloaded. This may cause unwanted behavior, see <a href=\"https://github.com/yt-dlp/yt-dlp/pull/1122\">#1122</a> for more details)</li><li>: Extract approximate  and  in flat-playlist. This may cause date-based filters to be slightly off</li></ul><ul><li>: Passthrough any query in mpd/m3u8 manifest URLs to their fragments if no value is provided, or else apply the query string given as . Note that if the stream has an HLS AES-128 key, then the query parameters will be passed to the key URI as well, unless the  extractor-arg is passed, or unless an external key URI is provided via the  extractor-arg. Does not apply to ffmpeg</li><li>: Passthrough the master m3u8 URL query to its variant playlist URLs if no value is provided, or else apply the query string given as </li><li>: Passthrough the master m3u8 URL query to its HLS AES-128 decryption key URI if no value is provided, or else apply the query string given as . Note that this will have no effect if the key URI is provided via the  extractor-arg. Does not apply to ffmpeg</li><li>: An HLS AES-128 key URI  key (as hex), and optionally the IV (as hex), in the form of ; e.g. <code>generic:hls_key=ABCDEF1234567980,0xFEDCBA0987654321</code>. Passing any of these values will force usage of the native HLS downloader and override the corresponding values found in the m3u8 playlist</li><li>: Bypass live HLS detection and manually set  - a value of  will set , any other value (or no value) will set </li><li>: Target(s) to try and impersonate with the initial webpage request; e.g. <code>generic:impersonate=safari,chrome-110</code>. Use  to impersonate any available target, and use <code>generic:impersonate=false</code> to disable impersonation (default)</li></ul><ul><li>: Types of videos to download - one or more of , , , </li></ul><ul><li>: Segment duration in milliseconds for HLS-DMC formats. Use it at your own risk since this feature <strong>may result in your account termination.</strong></li></ul><ul><li>: Try to check more at the cost of more requests. One or more of , </li></ul><ul><li>:  (default),  (cookies needed), ,  - choose comment sorting mode (on GameJolt's side)</li></ul><ul><li>: resolution to ignore - one or more of , , </li><li>: vcodec to ignore - one or more of , , </li><li>: dynamic range to ignore - one or more of , , </li></ul><ul><li>: Maximum number of comments to extract - default is </li></ul><ul><li>: Hostname to use for mobile API calls, e.g. <code>api22-normal-c-alisg.tiktokv.com</code></li><li>: Default app name to use with mobile API calls, e.g. </li><li>: Default app version to use with mobile API calls - should be set along with , e.g. </li><li>: Default numeric app version to use with mobile API calls, e.g. </li><li>: Default app ID to use with mobile API calls, e.g. </li><li>: Enable mobile API extraction with one or more app info strings in the format of <code>&lt;iid&gt;/[app_name]/[app_version]/[manifest_app_version]/[aid]</code>, where  is the unique app install ID.  is the only required value; all other values and their  separators can be omitted, e.g. <code>tiktok:app_info=1234567890123456789</code> or <code>tiktok:app_info=123,456/trill///1180,789//34.0.1/340001</code></li><li>: Enable mobile API extraction with a genuine device ID to be used with mobile API calls. Default is a random 19-digit string</li></ul><ul><li>: Which tab to download - one of , , , , , </li></ul><ul><li>: Select one of  (default),  or  as the API for tweet extraction. Has no effect if logged in</li></ul><h4>stacommu, wrestleuniverse</h4><ul><li>: UUID value assigned by the website and used to enforce device limits for paid livestream content. Can be found in browser local storage</li></ul><ul><li>: Client ID value to be sent with GraphQL requests, e.g. <code>twitch:client_id=kimne78kx3ncx6brgo4mv6wki5h1ko</code></li></ul><h4>nhkradirulive (NHK „Çâ„Åò„Çã‚òÖ„Çâ„Åò„Çã LIVE)</h4><ul><li>: Which regional variation to extract. Valid areas are: , , , , , , , . Defaults to </li></ul><ul><li>: Type(s) of game replays to extract. Valid types are: , ,  and . You can use  to extract all available replay types, which is the default</li></ul><ul><li>: The  UUID from browser local storage can be passed to extend the life of your login session when logging in with  as username and the  from browser local storage as password</li></ul><ul><li>: Audio bitrates to request. One or more of , , , , . Default is </li></ul><ul><li>: One or more CDN IDs to use with the API call for stream URLs, e.g. , , , </li></ul><ul><li>: Formats to request from the API. Requested values should be in the format of , e.g. . The  character functions as a wildcard, e.g. , and can be passed by itself to request all formats. Known protocols include ,  and ; known codecs include ,  and . Original  formats are always extracted. Default is <code>http_aac,hls_aac,http_opus,hls_opus,http_mp3,hls_mp3</code></li></ul><ul><li>: Prefer a playlist of program segments instead of a single complete video when available. If individual segments are desired, use <code>--concat-playlist never --extractor-args \"orfon:prefer_segments_playlist\"</code></li></ul><ul><li>: Prefer extracting flv formats over mp4 for older videos that still provide legacy formats</li></ul><ul><li>: Episode sort order for series extraction - one of  (ascending, oldest first) or  (descending, newest first). Default is </li></ul><p>: These options may be changed/removed in the future without concern for backward compatibility</p><p>Note that  plugins are imported even if not invoked, and that  performed on plugin code. <strong>Use plugins at your own risk and only if you trust the code!</strong></p><p>Plugins can be of s  or .</p><ul><li>Extractor plugins do not need to be enabled from the CLI and are automatically invoked when the input URL is suitable for it.</li><li>Extractor plugins take priority over built-in extractors.</li><li>Postprocessor plugins can be invoked using .</li></ul><p>Plugins are loaded from the namespace packages  and <code>yt_dlp_plugins.postprocessor</code>.</p><p>In other words, the file structure on the disk looks something like:</p><pre><code>    yt_dlp_plugins/\n        extractor/\n            myplugin.py\n        postprocessor/\n            myplugin.py\n</code></pre><p>yt-dlp looks for these  namespace folders in many locations (see below) and loads in plugins from  of them. Set the environment variable  to something nonempty to disable loading plugins entirely.</p><p>Plugins can be installed using various methods and locations.</p><ol><li><p><strong>Configuration directories</strong>: Plugin packages (containing a  namespace folder) can be dropped into the following standard <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration\">configuration locations</a>:</p><ul><li><ul><li><code>${XDG_CONFIG_HOME}/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code> (recommended on Linux/macOS)</li><li><code>${XDG_CONFIG_HOME}/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li><li><code>${APPDATA}/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code> (recommended on Windows)</li><li><code>${APPDATA}/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li><li><code>~/.yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li><li><code>~/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li></ul></li><li><ul><li><code>/etc/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li><li><code>/etc/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li></ul></li></ul></li><li><p>: Plugin packages can similarly be installed in a  directory under the executable location (recommended for portable installations):</p><ul><li>Binary: where , <code>&lt;root-dir&gt;/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li><li>Source: where <code>&lt;root-dir&gt;/yt_dlp/__main__.py</code>, <code>&lt;root-dir&gt;/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li></ul></li><li><p><strong>pip and other locations in </strong></p><ul><li>Plugin packages can be installed and managed using . See <a href=\"https://github.com/yt-dlp/yt-dlp-sample-plugins\">yt-dlp-sample-plugins</a> for an example. \n    <ul><li>Note: plugin files between plugin packages installed with pip must have unique filenames.</li></ul></li><li>Any path in  is searched in for the  namespace folder. \n    <ul><li>Note: This does not apply for Pyinstaller builds.</li></ul></li></ul></li></ol><p>,  and  archives containing a  namespace folder in their root are also supported as plugin packages.</p><ul><li>e.g. <code>${XDG_CONFIG_HOME}/yt-dlp/plugins/mypluginpkg.zip</code> where  contains <code>yt_dlp_plugins/&lt;type&gt;/myplugin.py</code></li></ul><p>Run yt-dlp with  to check if the plugin has been loaded.</p><p>All public classes with a name ending in / are imported from each file for extractors and postprocessors respectively. This respects underscore prefix (e.g.  is private) and . Modules can similarly be excluded by prefixing the module name with an underscore (e.g. ).</p><p>To replace an existing extractor with a subclass of one, set the  class keyword argument (e.g. <code>class MyPluginIE(ABuiltInIE, plugin_name='myplugin')</code> will replace  with ). Since the extractor replaces the parent, you should exclude the subclass extractor from being imported separately by making it private using one of the methods described above.</p><p>If you are a plugin author, add <a href=\"https://github.com/topics/yt-dlp-plugins\">yt-dlp-plugins</a> as a topic to your repository for discoverability.</p><p>yt-dlp makes the best effort to be a good command-line program, and thus should be callable from any programming language.</p><p>Your program should avoid parsing the normal stdout since they may change in future versions. Instead, they should use options such as , , ,  etc to create console output that you can reliably reproduce and parse.</p><p>From a Python program, you can embed yt-dlp in a more powerful fashion, like this:</p><pre><code>from yt_dlp import YoutubeDL\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\nwith YoutubeDL() as ydl:\n    ydl.download(URLS)\n</code></pre><p>Most likely, you'll want to use various options. For a list of options available, have a look at <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/yt_dlp/YoutubeDL.py#L183\"></a> or  in a Python shell. If you are already familiar with the CLI, you can use <a href=\"https://github.com/yt-dlp/yt-dlp/raw/master/devscripts/cli_to_api.py\"></a> to translate any CLI switches to  params.</p><p>: If you are porting your code from youtube-dl to yt-dlp, one important point to look out for is that we do not guarantee the return value of  to be json serializable, or even be a dictionary. It will be dictionary-like, but if you want to ensure it is a serializable dictionary, pass it through  as shown in the <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#extracting-information\">example below</a></p><pre><code>import json\nimport yt_dlp\n\nURL = 'https://www.youtube.com/watch?v=BaW_jenozKc'\n\n# ‚ÑπÔ∏è See help(yt_dlp.YoutubeDL) for a list of available options and public functions\nydl_opts = {}\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    info = ydl.extract_info(URL, download=False)\n\n    # ‚ÑπÔ∏è ydl.sanitize_info makes the info json-serializable\n    print(json.dumps(ydl.sanitize_info(info)))\n</code></pre><h4>Download using an info-json</h4><pre><code>import yt_dlp\n\nINFO_FILE = 'path/to/video.info.json'\n\nwith yt_dlp.YoutubeDL() as ydl:\n    error_code = ydl.download_with_info_file(INFO_FILE)\n\nprint('Some videos failed to download' if error_code\n      else 'All videos successfully downloaded')\n</code></pre><pre><code>import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\nydl_opts = {\n    'format': 'm4a/bestaudio/best',\n    # ‚ÑπÔ∏è See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n    'postprocessors': [{  # Extract audio using ffmpeg\n        'key': 'FFmpegExtractAudio',\n        'preferredcodec': 'm4a',\n    }]\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    error_code = ydl.download(URLS)\n</code></pre><pre><code>import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\ndef longer_than_a_minute(info, *, incomplete):\n    \"\"\"Download only videos longer than a minute (or with unknown duration)\"\"\"\n    duration = info.get('duration')\n    if duration and duration &lt; 60:\n        return 'The video is too short'\n\nydl_opts = {\n    'match_filter': longer_than_a_minute,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    error_code = ydl.download(URLS)\n</code></pre><h4>Adding logger and progress hook</h4><pre><code>import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\nclass MyLogger:\n    def debug(self, msg):\n        # For compatibility with youtube-dl, both debug and info are passed into debug\n        # You can distinguish them by the prefix '[debug] '\n        if msg.startswith('[debug] '):\n            pass\n        else:\n            self.info(msg)\n\n    def info(self, msg):\n        pass\n\n    def warning(self, msg):\n        pass\n\n    def error(self, msg):\n        print(msg)\n\n\n# ‚ÑπÔ∏è See \"progress_hooks\" in help(yt_dlp.YoutubeDL)\ndef my_hook(d):\n    if d['status'] == 'finished':\n        print('Done downloading, now post-processing ...')\n\n\nydl_opts = {\n    'logger': MyLogger(),\n    'progress_hooks': [my_hook],\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(URLS)\n</code></pre><h4>Add a custom PostProcessor</h4><pre><code>import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\n# ‚ÑπÔ∏è See help(yt_dlp.postprocessor.PostProcessor)\nclass MyCustomPP(yt_dlp.postprocessor.PostProcessor):\n    def run(self, info):\n        self.to_screen('Doing stuff')\n        return [], info\n\n\nwith yt_dlp.YoutubeDL() as ydl:\n    # ‚ÑπÔ∏è \"when\" can take any value in yt_dlp.utils.POSTPROCESS_WHEN\n    ydl.add_post_processor(MyCustomPP(), when='pre_process')\n    ydl.download(URLS)\n</code></pre><h4>Use a custom format selector</h4><pre><code>import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\ndef format_selector(ctx):\n    \"\"\" Select the best video and the best audio that won't result in an mkv.\n    NOTE: This is just an example and does not handle all cases \"\"\"\n\n    # formats are already sorted worst to best\n    formats = ctx.get('formats')[::-1]\n\n    # acodec='none' means there is no audio\n    best_video = next(f for f in formats\n                      if f['vcodec'] != 'none' and f['acodec'] == 'none')\n\n    # find compatible audio extension\n    audio_ext = {'mp4': 'm4a', 'webm': 'webm'}[best_video['ext']]\n    # vcodec='none' means there is no video\n    best_audio = next(f for f in formats if (\n        f['acodec'] != 'none' and f['vcodec'] == 'none' and f['ext'] == audio_ext))\n\n    # These are the minimum required fields for a merged format\n    yield {\n        'format_id': f'{best_video[\"format_id\"]}+{best_audio[\"format_id\"]}',\n        'ext': best_video['ext'],\n        'requested_formats': [best_video, best_audio],\n        # Must be + separated list of protocols\n        'protocol': f'{best_video[\"protocol\"]}+{best_audio[\"protocol\"]}'\n    }\n\n\nydl_opts = {\n    'format': format_selector,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(URLS)\n</code></pre><ul><li><p>: The default format sorting options have been changed so that higher resolution and better codecs will be now preferred instead of simply using larger bitrate. Furthermore, you can now specify the sort order using . This allows for much easier format selection than what is possible by simply using  (<a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection-examples\">examples</a>)</p></li><li><p><strong>Merged with animelover1984/youtube-dl</strong>: You get most of the features and improvements from <a href=\"https://github.com/animelover1984/youtube-dl\">animelover1984/youtube-dl</a> including , , , Embedding thumbnail in mp4/ogg/opus, playlist infojson etc. Note that NicoNico livestreams are not available. See <a href=\"https://github.com/yt-dlp/yt-dlp/pull/31\">#31</a> for details.</p></li><li><ul><li>Supports Clips, Stories (), Search (including filters), YouTube Music Search, Channel-specific search, Search prefixes (, ), Mixes, and Feeds (, , , , , )</li><li>Download livestreams from the start using  ()</li><li>Channel URLs download all uploads of the channel, including shorts and live</li></ul></li><li><p>: Cookies can be automatically extracted from all major web browsers using <code>--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]</code></p></li><li><p>: Videos can be downloaded partially based on either timestamps or chapters using </p></li><li><p>: Videos can be split into multiple files based on chapters using </p></li><li><p><strong>Multi-threaded fragment downloads</strong>: Download multiple fragments of m3u8/mpd videos in parallel. Use  () option to set the number of threads used</p></li><li><p>: You can use  as the external downloader for DASH(mpd) and HLS(m3u8) formats</p></li><li><p>: Philo, Spectrum, SlingTV, Cablevision, RCN etc.</p></li><li><p><strong>Subtitle extraction from manifests</strong>: Subtitles can be extracted from streaming media manifests. See <a href=\"https://github.com/yt-dlp/yt-dlp/commit/be6202f12b97858b9d716e608394b51065d0419f\">commit/be6202f</a> for details</p></li><li><p><strong>Multiple paths and output templates</strong>: You can give different <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template\">output templates</a> and download paths for different types of files. You can also set a temporary path where intermediary files are downloaded to using  ()</p></li><li><p>: Configuration files are automatically loaded from the home and root directories. See <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration\">CONFIGURATION</a> for details</p></li><li><p><strong>Output template improvements</strong>: Output templates can now have date-time formatting, numeric offsets, object traversal etc. See <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template\">output template</a> for details. Even more advanced operations can also be done with the help of  and </p></li><li><p>: Many new options have been added such as , , , , , , , , ,  etc</p></li><li><p>: Regex and other operators in /, multiple  and , faster archive checking, more <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection\">format selection options</a>, merge multi-video/audio, multiple ,  at different stages, etc</p></li><li><p>: Extractors and PostProcessors can be loaded from an external file. See <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#plugins\">plugins</a> for details</p></li><li><p>: The releases can be updated using , and downgraded using  if required</p></li></ul><p>Features marked with a  have been back-ported to youtube-dl</p><h3>Differences in default behavior</h3><p>Some of yt-dlp's default options are different from that of youtube-dl and youtube-dlc:</p><ul><li>The options  (),  () and  (), no longer work. See <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#Removed\">removed options</a> for details</li><li> is not supported as an alternative to </li><li>yt-dlp stores config files in slightly different locations to youtube-dl. See <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration\">CONFIGURATION</a> for a list of correct locations</li><li>The default <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template\">output template</a> is <code>%(title)s [%(id)s].%(ext)s</code>. There is no real reason for this change. This was changed before yt-dlp was ever made public and now there are no plans to change it back to . Instead, you may use <code>--compat-options filename</code></li><li>The default <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats\">format sorting</a> is different from youtube-dl and prefers higher resolution and better codecs rather than higher bitrates. You can use the  option to change this to any order you prefer, or use <code>--compat-options format-sort</code> to use youtube-dl's sorting order. Older versions of yt-dlp preferred VP9 due to its broader compatibility; you can use <code>--compat-options prefer-vp9-sort</code> to revert to that format sorting preference. These two compat options cannot be used together</li><li>The default format selector is . This means that if a combined video + audio format that is better than the best video-only format is found, the former will be preferred. Use  or <code>--compat-options format-spec</code> to revert this</li><li>Unlike youtube-dlc, yt-dlp does not allow merging multiple audio/video streams into one file by default (since this conflicts with the use of ). If needed, this feature must be enabled using  and . You can also use <code>--compat-options multistreams</code> to enable both</li><li> is enabled by default. Use  or <code>--compat-options abort-on-error</code> to abort on errors instead</li><li>When writing metadata files such as thumbnails, description or infojson, the same information (if available) is also written for playlists. Use <code>--no-write-playlist-metafiles</code> or <code>--compat-options no-playlist-metafiles</code> to not write these files</li><li> attaches the  to  files in addition to writing the metadata when used with . Use  or <code>--compat-options no-attach-info-json</code> to revert this</li><li>Some metadata are embedded into different fields when using  as compared to youtube-dl. Most notably,  field contains the  and  contains the . You can <a href=\"https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#modifying-metadata\">use </a> to modify this to your liking or use <code>--compat-options embed-metadata</code> to revert this</li><li> behaves differently when used with options like  and . See <a href=\"https://github.com/yt-dlp/yt-dlp/issues/302\">#302</a> for details. You can use <code>--compat-options playlist-index</code> if you want to keep the earlier behavior</li><li>The output of  is listed in a new format. Use <code>--compat-options list-formats</code> to revert this</li><li>Live chats (if available) are considered as subtitles. Use <code>--sub-langs all,-live_chat</code> to download all subtitles except live chat. You can also use <code>--compat-options no-live-chat</code> to prevent any live chat/danmaku from downloading</li><li>YouTube channel URLs download all uploads of the channel. To download only the videos in a specific tab, pass the tab's URL. If the channel does not show the requested tab, an error will be raised. Also,  URLs raise an error if there are no live videos instead of silently downloading the entire channel. You may use <code>--compat-options no-youtube-channel-redirect</code> to revert all these redirections</li><li>Unavailable videos are also listed for YouTube playlists. Use <code>--compat-options no-youtube-unavailable-videos</code> to remove this</li><li>The upload dates extracted from YouTube are in UTC <a href=\"https://github.com/yt-dlp/yt-dlp/raw/89e4d86171c7b7c997c77d4714542e0383bf0db0/yt_dlp/extractor/youtube.py#L3898-L3900\">when available</a>. Use <code>--compat-options no-youtube-prefer-utc-upload-date</code> to prefer the non-UTC upload date.</li><li>If  is used as the downloader, the downloading and merging of formats happen in a single step when possible. Use <code>--compat-options no-direct-merge</code> to revert this</li><li>Thumbnail embedding in  is done with mutagen if possible. Use <code>--compat-options embed-thumbnail-atomicparsley</code> to force the use of AtomicParsley instead</li><li>Some internal metadata such as filenames are removed by default from the infojson. Use  or <code>--compat-options no-clean-infojson</code> to revert this</li><li>When  and  are used together, the subtitles are written to disk and also embedded in the media file. You can use just  to embed the subs and automatically delete the separate file. See <a href=\"https://github.com/yt-dlp/yt-dlp/issues/630#issuecomment-893659460\">#630 (comment)</a> for more info. <code>--compat-options no-keep-subs</code> can be used to revert this</li><li> will be used for SSL root certificates, if installed. If you want to use system certificates (e.g. self-signed), use <code>--compat-options no-certifi</code></li><li>yt-dlp's sanitization of invalid characters in filenames is different/smarter than in youtube-dl. You can use <code>--compat-options filename-sanitization</code> to revert to youtube-dl's behavior</li><li>yt-dlp versions between 2021.09.01 and 2023.01.02 applies  to nested playlists. This was an unintentional side-effect of <a href=\"https://github.com/yt-dlp/yt-dlp/commit/8f18aca8717bb0dd49054555af8d386e5eda3a88\">8f18ac</a> and is fixed in <a href=\"https://github.com/yt-dlp/yt-dlp/commit/d7b460d0e5fc710950582baed2e3fc616ed98a80\">d7b460</a>. Use <code>--compat-options playlist-match-filter</code> to revert this</li><li>yt-dlp versions between 2021.11.10 and 2023.06.21 estimated  values for fragmented/manifest formats. This was added for convenience in <a href=\"https://github.com/yt-dlp/yt-dlp/commit/f2fe69c7b0d208bdb1f6292b4ae92bc1e1a7444a\">f2fe69</a>, but was reverted in <a href=\"https://github.com/yt-dlp/yt-dlp/commit/0dff8e4d1e6e9fb938f4256ea9af7d81f42fd54f\">0dff8e</a> due to the potentially extreme inaccuracy of the estimated values. Use <code>--compat-options manifest-filesize-approx</code> to keep extracting the estimated values</li><li>yt-dlp uses modern http client backends such as . Use <code>--compat-options prefer-legacy-http-handler</code> to prefer the legacy http handler () to be used for standard http requests.</li><li>The sub-modules ,  are removed.</li><li>Passing  (or calling  with ) no longer alters the default format selection. See <a href=\"https://github.com/yt-dlp/yt-dlp/issues/9843\">#9843</a> for details.</li></ul><p>For ease of use, a few more compat options are available:</p><ul><li>: Use all compat options ()</li><li><code>--compat-options youtube-dl</code>: Same as <code>--compat-options all,-multistreams,-playlist-match-filter,-manifest-filesize-approx,-allow-unsafe-ext,-prefer-vp9-sort</code></li><li><code>--compat-options youtube-dlc</code>: Same as <code>--compat-options all,-no-live-chat,-no-youtube-channel-redirect,-playlist-match-filter,-manifest-filesize-approx,-allow-unsafe-ext,-prefer-vp9-sort</code></li><li>: Same as <code>--compat-options 2022,no-certifi,filename-sanitization,no-youtube-prefer-utc-upload-date</code></li><li>: Same as <code>--compat-options 2023,playlist-match-filter,no-external-downloader-progress,prefer-legacy-http-handler,manifest-filesize-approx</code></li><li>: Same as <code>--compat-options prefer-vp9-sort</code>. Use this to enable all future compat options</li></ul><p>The following compat options restore vulnerable behavior from before security patches:</p><ul><li><p><code>--compat-options allow-unsafe-ext</code>: Allow files with any extension (including unsafe ones) to be downloaded (<a href=\"https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j\">GHSA-79w7-vh3h-8g4j</a>)</p><blockquote><p> Only use if a valid file download is rejected because its extension is detected as uncommon</p></blockquote></li></ul><p>These are all the deprecated options and the current alternative to achieve the same effect</p><p>While these options are almost the same as their new counterparts, there are some differences that prevents them being redundant</p><pre><code>-j, --dump-json                  --print \"%()j\"\n-F, --list-formats               --print formats_table\n--list-thumbnails                --print thumbnails_table --print playlist:thumbnails_table\n--list-subs                      --print automatic_captions_table --print subtitles_table\n</code></pre><p>While these options are redundant, they are still expected to be used due to their ease of use</p><pre><code>--get-description                --print description\n--get-duration                   --print duration_string\n--get-filename                   --print filename\n--get-format                     --print format\n--get-id                         --print id\n--get-thumbnail                  --print thumbnail\n-e, --get-title                  --print title\n-g, --get-url                    --print urls\n--match-title REGEX              --match-filters \"title ~= (?i)REGEX\"\n--reject-title REGEX             --match-filters \"title !~= (?i)REGEX\"\n--min-views COUNT                --match-filters \"view_count &gt;=? COUNT\"\n--max-views COUNT                --match-filters \"view_count &lt;=? COUNT\"\n--break-on-reject                Use --break-match-filters\n--user-agent UA                  --add-headers \"User-Agent:UA\"\n--referer URL                    --add-headers \"Referer:URL\"\n--playlist-start NUMBER          -I NUMBER:\n--playlist-end NUMBER            -I :NUMBER\n--playlist-reverse               -I ::-1\n--no-playlist-reverse            Default\n--no-colors                      --color no_color\n</code></pre><p>While these options still work, their use is not recommended since there are other alternatives to achieve the same</p><pre><code>--force-generic-extractor        --ies generic,default\n--exec-before-download CMD       --exec \"before_dl:CMD\"\n--no-exec-before-download        --no-exec\n--all-formats                    -f all\n--all-subs                       --sub-langs all --write-subs\n--print-json                     -j --no-simulate\n--autonumber-size NUMBER         Use string formatting, e.g. %(autonumber)03d\n--autonumber-start NUMBER        Use internal field formatting like %(autonumber+NUMBER)s\n--id                             -o \"%(id)s.%(ext)s\"\n--metadata-from-title FORMAT     --parse-metadata \"%(title)s:FORMAT\"\n--hls-prefer-native              --downloader \"m3u8:native\"\n--hls-prefer-ffmpeg              --downloader \"m3u8:ffmpeg\"\n--list-formats-old               --compat-options list-formats (Alias: --no-list-formats-as-table)\n--list-formats-as-table          --compat-options -list-formats [Default] (Alias: --no-list-formats-old)\n--youtube-skip-dash-manifest     --extractor-args \"youtube:skip=dash\" (Alias: --no-youtube-include-dash-manifest)\n--youtube-skip-hls-manifest      --extractor-args \"youtube:skip=hls\" (Alias: --no-youtube-include-hls-manifest)\n--youtube-include-dash-manifest  Default (Alias: --no-youtube-skip-dash-manifest)\n--youtube-include-hls-manifest   Default (Alias: --no-youtube-skip-hls-manifest)\n--geo-bypass                     --xff \"default\"\n--no-geo-bypass                  --xff \"never\"\n--geo-bypass-country CODE        --xff CODE\n--geo-bypass-ip-block IP_BLOCK   --xff IP_BLOCK\n</code></pre><p>These options are not intended to be used by the end-user</p><pre><code>--test                           Download only part of video for testing extractors\n--load-pages                     Load pages dumped by --write-pages\n--youtube-print-sig-code         For testing youtube signatures\n--allow-unplayable-formats       List unplayable formats also\n--no-allow-unplayable-formats    Default\n</code></pre><p>These are aliases that are no longer documented for various reasons</p><pre><code>--avconv-location                --ffmpeg-location\n--clean-infojson                 --clean-info-json\n--cn-verification-proxy URL      --geo-verification-proxy URL\n--dump-headers                   --print-traffic\n--dump-intermediate-pages        --dump-pages\n--force-write-download-archive   --force-write-archive\n--load-info                      --load-info-json\n--no-clean-infojson              --no-clean-info-json\n--no-split-tracks                --no-split-chapters\n--no-write-srt                   --no-write-subs\n--prefer-unsecure                --prefer-insecure\n--rate-limit RATE                --limit-rate RATE\n--split-tracks                   --split-chapters\n--srt-lang LANGS                 --sub-langs LANGS\n--trim-file-names LENGTH         --trim-filenames LENGTH\n--write-srt                      --write-subs\n--yes-overwrites                 --force-overwrites\n</code></pre><p>Support for <a href=\"https://github.com/faissaloo/SponSkrub\">SponSkrub</a> has been deprecated in favor of the  options</p><pre><code>--sponskrub                      --sponsorblock-mark all\n--no-sponskrub                   --no-sponsorblock\n--sponskrub-cut                  --sponsorblock-remove all\n--no-sponskrub-cut               --sponsorblock-remove -all\n--sponskrub-force                Not applicable\n--no-sponskrub-force             Not applicable\n--sponskrub-location             Not applicable\n--sponskrub-args                 Not applicable\n</code></pre><p>These options may no longer work as intended</p><pre><code>--prefer-avconv                  avconv is not officially supported by yt-dlp (Alias: --no-prefer-ffmpeg)\n--prefer-ffmpeg                  Default (Alias: --no-prefer-avconv)\n-C, --call-home                  Not implemented\n--no-call-home                   Default\n--include-ads                    No longer supported\n--no-include-ads                 Default\n--write-annotations              No supported site has annotations now\n--no-write-annotations           Default\n--compat-options seperate-video-versions  No longer needed\n--compat-options no-youtube-prefer-utc-upload-date  No longer supported\n</code></pre><p>These options were deprecated since 2014 and have now been entirely removed</p><pre><code>-A, --auto-number                -o \"%(autonumber)s-%(id)s.%(ext)s\"\n-t, -l, --title, --literal       -o \"%(title)s-%(id)s.%(ext)s\"\n</code></pre><p>See the <a href=\"https://github.com/yt-dlp/yt-dlp/wiki\">Wiki</a> for more information</p>","contentLength":127463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mongodb-developer/GenAI-Showcase","url":"https://github.com/mongodb-developer/GenAI-Showcase","date":1739759302,"author":"","guid":863,"unread":true,"content":"<p>Welcome to MongoDB's Generative AI Showcase Repository!</p><p>Whether you are just starting out on your Generative AI journey, or looking to build advanced GenAI applications, we've got you covered. This repository has an exhaustive list of examples and sample applications that cover Retrieval-Augmented Generation (RAG), AI Agents, and industry-specific use cases.</p><p>Discover how MongoDB integrates into RAG pipelines and AI Agents, serving as a vector database, operational database, and memory provider.</p><p>This repo mainly contains:</p><ul><li>Jupyter notebooks examples for RAG, agentic applications, evaluations etc. under .</li><li>Javascipt and Python apps and demos under .</li><li>Contributions from our AI partners under .</li></ul><p>You will need to connect to a MongoDB cluster to run any of the apps or examples in this repo. Follow these steps to get set up:</p><p>As you work through these examples, if you encounter any problems, please <a href=\"https://github.com/mongodb-developer/GenAI-Showcase/issues/new\">open a new issue</a>.</p>","contentLength":909,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"cordx56/rustowl","url":"https://github.com/cordx56/rustowl","date":1739759302,"author":"","guid":864,"unread":true,"content":"<p>Visualize Ownership and Lifetimes in Rust</p><div align=\"center\"><p> Visualize ownership and lifetimes in Rust for debugging and optimization </p></div><p>RustOwl visualizes ownership movement and lifetimes of variables. When you save Rust source code, it is analyzed, and the ownership and lifetimes of variables are visualized when you hover over a variable or function call.</p><p>RustOwl visualizes those by using underlines:</p><ul><li>üü© green: variable's actual lifetime</li><li>üü¶ blue: immutable borrowing</li><li>üü™ purple: mutable borrowing</li><li>üüß orange: value moved / function call</li><li>üü• red: lifetime error - diff of lifetime between actual and expected</li></ul><p>Currently, we offer VSCode extension, Neovim plugin and Emacs package. For these editors, move the text cursor over the variable or function call you want to inspect and wait for 2 seconds to visualize the information. We implemented LSP server  with an extended protocol. So, RustOwl can be used easily from other editor.</p><p>Here we describe how to start using RustOwl with VSCode.</p><ul><li>,  and  installed</li><li>Visual Studio Code (VSCode) installed</li></ul><p>We tested this guide on macOS Sequoia 15.2 on arm64 architecture with VSCode 1.96.4 and  1.27.1.</p><p>We also tested this guide on Ubuntu 25.04 on arm64 architecture with VSCode 1.96.4 and  1.27.1. On Ubuntu, you need to run <code>apt install build-essential</code> before installing.</p><p>After installation, the extension will automatically run RustOwl when you save any Rust program in cargo workspace. The initial analysis may take some time, but from the second run onward, compile caching is used to reduce the analysis time.</p><p>We tested on Windows 11 Education 23H2 on amd64 architecture. For Windows, please clone this repository and build RustOwl manually.</p><p>To install RustOwl command, run the command below.</p><pre><code>curl -L \"https://github.com/cordx56/rustowl/releases/download/v0.1.2/install.sh\" | sh\n</code></pre><p>You can install VSCode extension from <a href=\"https://marketplace.visualstudio.com/items?itemName=cordx56.rustowl-vscode\">this link</a>.</p><p>Also, you can download VSCode extension file (  ) from <a href=\"https://github.com/cordx56/rustowl/releases/download/v0.1.2/rustowl-vscode-0.1.2.vsix\">this link</a>.</p><p>We support Neovim and Emacs. You can also create your own LSP client.</p><pre><code>{ \"cordx56/rustowl\", dependencies = { \"neovim/nvim-lspconfig\" } }\n</code></pre><pre><code>lspconfig.rustowl.setup {\n    trigger = {\n        hover = false,\n    },\n}\n\nkeymap(\n    \"n\",\n    \"&lt;c-o&gt;\",\n    require(\"rustowl\").rustowl_cursor,\n    { noremap = true, silent = true }\n)\n</code></pre><pre><code>(elpaca\n  (rustowlsp\n    :host github\n    :repo \"cordx56/rustowl\"\n    :files (:defaults \"emacs/*\")))\n</code></pre><p>Here, we describe manual install instructions from source code.</p><ul><li> and  installed \n  <ul><li>You need to set up the  environment variable. To do this, follow the instructions provided by the  installer. For example, in bash, run <code>export PATH=$HOME/.cargo/bin:$PATH</code>.</li></ul></li></ul><p>RustOwl has been tested on macOS Sequoia 15.2 on arm64 architecture with  1.27.1. We have not tested the installation of dependencies from other package repositories, such as Homebrew. You may need to uninstall any Rust-related packages installed through those repositories first. Other dependencies are locked in the configuration files and will be installed automatically.</p><p>We have also tested this on Ubuntu 25.04 on arm64 architecture with  1.27.1. Additional dependencies may be required. We have confirmed that running <code>apt install build-essential</code> is necessary on a freshly installed Ubuntu for linking.</p><pre><code>cd rustowl\ncargo install --path . --locked\ncargo owlsp\n</code></pre><ul><li>VSCode installed \n  </li><li> installed \n  <ul><li>After installing Node.js, You can install  by running .</li></ul></li></ul><p>VSCode extension has been tested on macOS Sequoia 15.2 on arm64 architecture with Visual Studio Code 1.96.4, Node.js v20.16.0, and  1.22.22. Other dependencies are locked in the configuration files and will be installed automatically.</p><p>First, install the dependencies.</p><pre><code>cd vscode\nyarn install --frozen-lockfile\n</code></pre><p>Then open  directory in VSCode.</p><p>A notification to install the recommended VSCode extension will appear in the bottom right corner of VSCode. Click the install button, wait for the installation to finish, and then restart VSCode.</p><p>Open  directory again, and press the  key in the VSCode window. A new VSCode window with the extension enabled will appear.</p><p>Open cargo workspace directory in the new VSCode window.</p><p>When you save Rust files, decoration indicating the movement of ownership and lifetimes will appear in the editor.</p><p>In this tool, due to the limitations of VSCode's decoration specifications, characters with descenders, such as g or parentheses, may occasionally not display underlines properly. Additionally, we observed that the  macro sometimes produces extra output, though this does not affect usability in any significant way.</p>","contentLength":4475,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"unionlabs/union","url":"https://github.com/unionlabs/union","date":1739759302,"author":"","guid":865,"unread":true,"content":"<p>The trust-minimized, zero-knowledge bridging protocol, designed for censorship resistance, extremely high security, and usage in decentralized finance.</p><p>Union is the hyper-efficient zero-knowledge infrastructure layer for general message passing, asset transfers, NFTs, and DeFi. It‚Äôs based on <a href=\"https://union.build/docs/concepts/consensus-verification/\">Consensus Verification</a> and has no dependencies on trusted third parties, oracles, multi-signatures, or MPC. It implements <a href=\"https://github.com/cosmos/ibc\" title=\"cosmos/ibc\">IBC</a> for compatibility with <a href=\"https://cosmos.network\">Cosmos</a> chains and connects to EVM chains like <a href=\"https://ethereum.org\">Ethereum</a>, <a href=\"https://github.com/berachain/beacon-kit\">Berachain (beacon-kit)</a>, <a href=\"https://github.com/OffchainLabs/arbitrum\">Arbitrum</a>, and more.</p><p>The upgradability of contracts on other chains, connections, token configurations, and evolution of the protocol will all be controlled by decentralized governance, aligning the priorities of Union with its users, validators, and operators.</p><pre><code>curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install\n</code></pre><p><em>(Note that some components can only be built on Linux. If you are using macOS, we recommend using <a href=\"https://orbstack.dev/\">OrbStack</a> to easily set up a <a href=\"https://nixos.org\">NixOS</a> VM within two minutes. Most Union developers use macOS with <a href=\"https://orbstack.dev/\">OrbStack</a>, and there is no need to install Nix inside of the <a href=\"https://nixos.org\">NixOS</a> VM.)</em></p><p>You can now  build any of Union's components from source:</p><pre><code>nix build .#uniond -L\nnix build .#voyager -L\nnix build .#app -L\n\n# to see all packages, run:\nnix flake show\n</code></pre><p>The result of whatever you build will be in </p><p>You can now also enter our dev shell, which has all of the dependencies (, , , , etc.) you need to work on any component: <em>(Don't worry, this will not affect your system outside of this repo)</em></p><p>Run the following to format the entire repo and check your spelling before each PR:</p><p>Check the  channel on <a href=\"https://discord.union.build\">Union's discord</a> if you need any help with this.</p><p>The official docs are hosted <a href=\"https://docs.union.build\" title=\"Official Union Docs\">here</a>. Each individual component also has accompanying developer documentation for contributors, which you can find in each .</p>","contentLength":1847,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/OmniParser","url":"https://github.com/microsoft/OmniParser","date":1739759302,"author":"","guid":866,"unread":true,"content":"<p>A simple screen parsing tool towards pure vision based GUI agent</p><p> is a comprehensive method for parsing user interface screenshots into structured and easy-to-understand elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface.</p><ul><li>[2025/2] We introduce OmniTool: Control a Windows 11 VM with OmniParser + your vision model of choice. OmniTool supports out of the box the following large language models - OpenAI (4o/o1/o3-mini), DeepSeek (R1), Qwen (2.5VL) or Anthropic Computer Use. <a href=\"https://1drv.ms/v/c/650b027c18d5a573/EehZ7RzY69ZHn-MeQHrnnR4BCj3by-cLLpUVlxMjF4O65Q?e=8LxMgX\">Watch Video</a></li><li>[2025/1] V2 is coming. We achieve new state of the art results 39.5% on the new grounding benchmark <a href=\"https://github.com/likaixin2000/ScreenSpot-Pro-GUI-Grounding/tree/main\">Screen Spot Pro</a> with OmniParser v2 (will be released soon)! Read more details <a href=\"https://github.com/microsoft/OmniParser/tree/master/docs/Evaluation.md\">here</a>.</li><li>[2024/11] We release an updated version, OmniParser V1.5 which features 1) more fine grained/small icon detection, 2) prediction of whether each screen element is interactable or not. Examples in the demo.ipynb.</li><li>[2024/10] OmniParser was the #1 trending model on huggingface model hub (starting 10/29/2024).</li><li>[2024/10] Feel free to checkout our demo on <a href=\"https://huggingface.co/spaces/microsoft/OmniParser\">huggingface space</a>! (stay tuned for OmniParser + Claude Computer Use)</li><li>[2024/10] Both Interactive Region Detection Model and Icon functional description model are released! <a href=\"https://huggingface.co/microsoft/OmniParser\">Hugginface models</a></li></ul><p>First clone the repo, and then install environment:</p><pre><code>cd OmniParser\nconda create -n \"omni\" python==3.12\nconda activate omni\npip install -r requirements.txt\n</code></pre><p>Ensure you have the V2 weights downloaded in weights folder (ensure caption weights folder is called icon_caption_florence). If not download them with:</p><pre><code>   # download the model checkpoints to local directory OmniParser/weights/\n   for f in icon_detect/{train_args.yaml,model.pt,model.yaml} icon_caption/{config.json,generation_config.json,model.safetensors}; do huggingface-cli download microsoft/OmniParser-v2.0 \"$f\" --local-dir weights; done\n   mv weights/icon_caption weights/icon_caption_florence\n</code></pre><p>We put together a few simple examples in the demo.ipynb.</p><p>To run gradio demo, simply run:</p><p>For the model checkpoints on huggingface model hub, please note that icon_detect model is under AGPL license since it is a license inherited from the original yolo model. And icon_caption_blip2 &amp; icon_caption_florence is under MIT license. Please refer to the LICENSE file in the folder of each model: <a href=\"https://huggingface.co/microsoft/OmniParser\">https://huggingface.co/microsoft/OmniParser</a>.</p><p>Our technical report can be found <a href=\"https://arxiv.org/abs/2408.00203\">here</a>. If you find our work useful, please consider citing our work:</p><pre><code>@misc{lu2024omniparserpurevisionbased,\n      title={OmniParser for Pure Vision Based GUI Agent}, \n      author={Yadong Lu and Jianwei Yang and Yelong Shen and Ahmed Awadallah},\n      year={2024},\n      eprint={2408.00203},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2408.00203}, \n}\n</code></pre>","contentLength":2826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stirling-Tools/Stirling-PDF","url":"https://github.com/Stirling-Tools/Stirling-PDF","date":1739759302,"author":"","guid":867,"unread":true,"content":"<p>#1 Locally hosted web application that allows you to perform various operations on PDF files</p><p><a href=\"https://www.stirlingpdf.com\">Stirling-PDF</a> is a robust, locally hosted web-based PDF manipulation tool using Docker. It enables you to carry out various operations on PDF files, including splitting, merging, converting, reorganizing, adding images, rotating, compressing, and more. This locally hosted web application has evolved to encompass a comprehensive set of features, addressing all your PDF requirements.</p><p>All files and PDFs exist either exclusively on the client side, reside in server memory only during task execution, or temporarily reside in a file solely for the execution of the task. Any file downloaded by the user will have been deleted from the server by that point.</p><ul><li>Parallel file processing and downloads</li><li>Custom 'Pipelines' to run multiple features in a automated queue</li><li>API for integration with external scripts</li><li>Optional Login and Authentication support (see <a href=\"https://docs.stirlingpdf.com/Advanced%20Configuration/System%20and%20Security\">here</a> for documentation)</li><li>Database Backup and Import (see <a href=\"https://docs.stirlingpdf.com/Advanced%20Configuration/DATABASE\">here</a> for documentation)</li><li>Enterprise features like SSO see <a href=\"https://docs.stirlingpdf.com/Enterprise%20Edition\">here</a></li></ul><ul><li>View and modify PDFs - View multi-page PDFs with custom viewing, sorting, and searching. Plus, on-page edit features like annotating, drawing, and adding text and images. (Using PDF.js with Joxit and Liberation fonts)</li><li>Full interactive GUI for merging/splitting/rotating/moving PDFs and their pages</li><li>Merge multiple PDFs into a single resultant file</li><li>Split PDFs into multiple files at specified page numbers or extract all pages as individual files</li><li>Reorganize PDF pages into different orders</li><li>Rotate PDFs in 90-degree increments</li><li>Multi-page layout (format PDFs into a multi-paged page)</li><li>Scale page contents size by set percentage</li><li>Auto-split PDF (with physically scanned page dividers)</li><li>Convert PDF to a single page</li><li>Overlay PDFs on top of each other</li></ul><ul><li>Convert PDFs to and from images</li><li>Convert any common file to PDF (using LibreOffice)</li><li>Convert PDF to Word/PowerPoint/others (using LibreOffice)</li></ul><ul><li>Change/set PDF permissions</li></ul><ul><li>Add/generate/write signatures</li><li>Detect and remove blank pages</li><li>Compare two PDFs and show differences in text</li><li>Compress PDFs to decrease their filesize (using qpdf)</li><li>Extract images from scans</li><li>Auto-rename files by detecting PDF header text</li><li>OCR on PDF (using Tesseract OCR)</li><li>PDF/A conversion (using LibreOffice)</li><li>Get all information on a PDF to view or export as JSON</li><li>Show/detect embedded JavaScript</li></ul><ul><li>Installation guides for all platforms</li></ul><p>Stirling-PDF currently supports 39 languages!</p><p>Stirling PDF offers an Enterprise edition of its software. This is the same great software but with added features, support and comforts. Check out our <a href=\"https://docs.stirlingpdf.com/Enterprise%20Edition\">Enterprise docs</a></p>","contentLength":2574,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["trending"]}