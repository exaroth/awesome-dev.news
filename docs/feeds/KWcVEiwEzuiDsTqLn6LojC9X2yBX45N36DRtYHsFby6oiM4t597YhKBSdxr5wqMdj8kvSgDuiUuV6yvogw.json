{"id":"KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw","title":"GitHub All Languages Daily Trending","displayTitle":"Github Trending","url":"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml","feedLink":"http://mshibanami.github.io/GitHubTrendingRSS","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":37,"items":[{"title":"n8n-io/self-hosted-ai-starter-kit","url":"https://github.com/n8n-io/self-hosted-ai-starter-kit","date":1750473272,"author":"","guid":163964,"unread":true,"content":"<p>The Self-hosted AI Starter Kit is an open-source template that quickly sets up a local AI environment. Curated by n8n, it provides essential tools for creating secure, self-hosted AI workflows.</p><p><strong>Self-hosted AI Starter Kit</strong> is an open-source Docker Compose template designed to swiftly initialize a comprehensive local AI and low-code development environment.</p><p>Curated by <a href=\"https://github.com/n8n-io\">https://github.com/n8n-io</a>, it combines the self-hosted n8n platform with a curated list of compatible AI products and components to quickly get started with building self-hosted AI workflows.</p><p>‚úÖ <a href=\"https://n8n.io/\"></a> - Low-code platform with over 400 integrations and advanced AI components</p><p>‚úÖ <a href=\"https://ollama.com/\"></a> - Cross-platform LLM platform to install and run the latest local LLMs</p><p>‚úÖ <a href=\"https://qdrant.tech/\"></a> - Open-source, high performance vector store with an comprehensive API</p><p>‚úÖ <a href=\"https://www.postgresql.org/\"></a> - Workhorse of the Data Engineering world, handles large amounts of data safely.</p><p>‚≠êÔ∏è  for scheduling appointments</p><p>‚≠êÔ∏è  securely without data leaks</p><p>‚≠êÔ∏è  for enhanced company communications and IT operations</p><p>‚≠êÔ∏è <strong>Private Financial Document Analysis</strong> at minimal cost</p><pre><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\n</code></pre><h3>Running n8n using Docker Compose</h3><pre><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\ndocker compose --profile gpu-nvidia up\n</code></pre><h3>For AMD GPU users on Linux</h3><pre><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\ndocker compose --profile gpu-amd up\n</code></pre><h4>For Mac / Apple Silicon users</h4><p>If you‚Äôre using a Mac with an M1 or newer processor, you can't expose your GPU to the Docker instance, unfortunately. There are two options in this case:</p><ol><li>Run the starter kit fully on CPU, like in the section \"For everyone else\" below</li><li>Run Ollama on your Mac for faster inference, and connect to that from the n8n instance</li></ol><p>If you want to run Ollama on your mac, check the <a href=\"https://ollama.com/\">Ollama homepage</a> for installation instructions, and run the starter kit as follows:</p><pre><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\ndocker compose up\n</code></pre><h5>For Mac users running OLLAMA locally</h5><p>If you're running OLLAMA locally on your Mac (not in Docker), you need to modify the OLLAMA_HOST environment variable in the n8n service configuration. Update the x-n8n section in your Docker Compose file as follows:</p><pre><code>x-n8n: &amp;service-n8n\n  # ... other configurations ...\n  environment:\n    # ... other environment variables ...\n    - OLLAMA_HOST=host.docker.internal:11434\n</code></pre><pre><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\ndocker compose --profile cpu up\n</code></pre><p>The core of the Self-hosted AI Starter Kit is a Docker Compose file, pre-configured with network and storage settings, minimizing the need for additional installations. After completing the installation steps above, simply follow the steps below to get started.</p><ol><li>Click the  button at the bottom of the canvas, to start running the workflow.</li><li>If this is the first time you‚Äôre running the workflow, you may need to wait until Ollama finishes downloading Llama3.2. You can inspect the docker console logs to check on the progress.</li></ol><p>With your n8n instance, you‚Äôll have access to over 400 integrations and a suite of basic and advanced AI nodes such as <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/\">AI Agent</a>, <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/\">Text classifier</a>, and <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/\">Information Extractor</a> nodes. To keep everything local, just remember to use the Ollama node for your language model and Qdrant as your vector store.</p><blockquote><p>[!NOTE] This starter kit is designed to help you get started with self-hosted AI workflows. While it‚Äôs not fully optimized for production environments, it combines robust components that work well together for proof-of-concept projects. You can customize it to meet your specific needs</p></blockquote><pre><code>docker compose --profile gpu-nvidia pull\ndocker compose create &amp;&amp; docker compose --profile gpu-nvidia up\n</code></pre><ul><li><h3>For Mac / Apple Silicon users</h3></li></ul><pre><code>docker compose pull\ndocker compose create &amp;&amp; docker compose up\n</code></pre><pre><code>docker compose --profile cpu pull\ndocker compose create &amp;&amp; docker compose --profile cpu up\n</code></pre><p>n8n is full of useful content for getting started quickly with its AI concepts and nodes. If you run into an issue, go to <a href=\"https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/#support\">support</a>.</p><p>For more AI workflow ideas, visit the <a href=\"https://n8n.io/workflows/categories/ai/\"><strong>official n8n AI template gallery</strong></a>. From each workflow, select the  button to automatically import the workflow into your local n8n instance.</p><p>The self-hosted AI starter kit will create a shared folder (by default, located in the same directory) which is mounted to the n8n container and allows n8n to access files on disk. This folder within the n8n container is located at  -- this is the path you‚Äôll need to use in nodes that interact with the local filesystem.</p><p><strong>Nodes that interact with the local filesystem</strong></p><p>This project is licensed under the Apache License 2.0 - see the <a href=\"https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/LICENSE\">LICENSE</a> file for details.</p><p>Join the conversation in the <a href=\"https://community.n8n.io/\">n8n Forum</a>, where you can:</p><ul><li>: Show off what you‚Äôve built with n8n and inspire others in the community.</li><li>: Whether you‚Äôre just getting started or you‚Äôre a seasoned pro, the community and our team are ready to support with any challenges.</li><li>: Have an idea for a feature or improvement? Let us know! We‚Äôre always eager to hear what you‚Äôd like to see next.</li></ul>","contentLength":5161,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mail-0/Zero","url":"https://github.com/Mail-0/Zero","date":1750473272,"author":"","guid":163965,"unread":true,"content":"<p>Experience email the way you want with Mail0 ‚Äì the first open source email app that puts your privacy and safety first. Join the discord: https://discord.gg/mail0</p><p>An Open-Source Gmail Alternative for the Future of Email</p><p>Zero is an open-source AI email solution that gives users the power to  their own email app while also integrating external services like Gmail and other email providers. Our goal is to modernize and improve emails through AI agents to truly modernize emails.</p><p>Most email services today are either , , or . 0.email is different:</p><ul><li>‚úÖ  ‚Äì No hidden agendas, fully transparent.</li><li>ü¶æ  - Enhance your emails with Agents &amp; LLMs.</li><li>üîí  ‚Äì Your emails, your data. Zero does not track, collect, or sell your data in any way. Please note: while we integrate with external services, the data passed through them is not under our control and falls under their respective privacy policies and terms of service.</li><li>‚öôÔ∏è  ‚Äì Run your own email app with ease.</li><li>üì¨  ‚Äì Connect multiple email providers like Gmail, Outlook, and more.</li><li>üé® <strong>Customizable UI &amp; Features</strong> ‚Äì Tailor your email experience the way you want it.</li><li>üöÄ  ‚Äì Built with extensibility and integrations in mind.</li></ul><p>Zero is built with modern and reliable technologies:</p><ul><li>: Next.js, React, TypeScript, TailwindCSS, Shadcn UI</li><li>: Node.js, Drizzle ORM</li><li>: Better Auth, Google OAuth</li></ul><p>Before running the application, you'll need to set up services and configure environment variables. For more details on environment variables, see the <a href=\"https://raw.githubusercontent.com/Mail-0/Zero/staging/#environment-variables\">Environment Variables</a> section.</p><p>You can set up Zero in two ways:</p><ol><li><ul><li><p>Open the  file and change the BETTER_AUTH_SECRET to a random string. (Use  to generate a 32 character string)</p><pre><code>BETTER_AUTH_SECRET=your_secret_key\n</code></pre></li></ul></li><li><p> (Required for Gmail integration)</p><ul><li><ul><li>Use the links above and click 'Enable' or</li><li>Go to 'APIs and Services' &gt; 'Enable APIs and Services' &gt; Search for 'Google People API' and click 'Enable'</li><li>Go to 'APIs and Services' &gt; 'Enable APIs and Services' &gt; Search for 'Gmail API' and click 'Enable'</li></ul></li><li><p>Enable the Google OAuth2 API</p></li><li><p>Create OAuth 2.0 credentials (Web application type)</p></li><li><p>Add authorized redirect URIs:</p><ul><li>Development: \n      <ul><li><code>http://localhost:8787/api/auth/callback/google</code></li></ul></li><li>Production: \n      <ul><li><code>https://your-production-url/api/auth/callback/google</code></li></ul></li></ul></li><li><pre><code>GOOGLE_CLIENT_ID=your_client_id\nGOOGLE_CLIENT_SECRET=your_client_secret\n</code></pre></li><li><p>Add yourself as a test user:</p><ul><li>Under 'Test users' click 'Add Users'</li><li>Add your email and click 'Save'</li></ul></li></ul></li></ol><blockquote><p>[!WARNING] The authorized redirect URIs in Google Cloud Console must match  what you configure in the , including the protocol (http/https), domain, and path - these are provided above.</p></blockquote><ol start=\"3\"><li><p> (Required for some encryption)</p><p>-Go to <a href=\"https://useautumn.com/\">Autumn</a> -For Local Use, click <a href=\"https://app.useautumn.com/sandbox/onboarding\">onboarding</a> button and generate an Autumn Secret Key -For production, select the production mode from upper left corner and generate an fill the other fields. After that, generate an Autumn Secret Key</p><pre><code>AUTUMN_SECRET_KEY=your_autumn_secret\n</code></pre></li></ol><p>Run  to setup your environment variables. It will copy the  file to  and fill in the variables for you. For local development a connection string example is provided in the  file located in the same folder as the database.</p><p>Zero uses PostgreSQL for storing data. Here's how to set it up:</p><ol><li><p>Run this command to start a local PostgreSQL instance:</p><p>This creates a database with:</p><ul></ul></li><li><p><strong>Set Up Database Connection</strong></p><p>Make sure your database connection string is in  file. And you have ran  to sync the latest env.</p><p>For local development use:</p><pre><code>DATABASE_URL=\"postgresql://postgres:postgres@localhost:5432/zerodotemail\"\n</code></pre></li><li><ul><li><p> (after schema changes):</p></li><li><blockquote><p>If you run  in your terminal, the studio command should be automatically running with the app.</p></blockquote></li></ul></li></ol><p>If you'd like to help with translating Zero to other languages, check out our <a href=\"https://raw.githubusercontent.com/Mail-0/Zero/staging/.github/TRANSLATION.md\">translation guide</a>.</p><h2>This project wouldn't be possible without these awesome companies</h2>","contentLength":3734,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"grafana/loki","url":"https://github.com/grafana/loki","date":1750473272,"author":"","guid":163966,"unread":true,"content":"<p>Like Prometheus, but for logs.</p><p>Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by <a href=\"https://prometheus.io/\">Prometheus</a>. It is designed to be very cost effective and easy to operate. It does not index the contents of the logs, but rather a set of labels for each log stream.</p><p>Compared to other log aggregation systems, Loki:</p><ul><li>does not do full text indexing on logs. By storing compressed, unstructured logs and only indexing metadata, Loki is simpler to operate and cheaper to run.</li><li>indexes and groups log streams using the same labels you‚Äôre already using with Prometheus, enabling you to seamlessly switch between metrics and logs using the same labels that you‚Äôre already using with Prometheus.</li><li>is an especially good fit for storing <a href=\"https://kubernetes.io/\">Kubernetes</a> Pod logs. Metadata such as Pod labels is automatically scraped and indexed.</li><li>has native support in Grafana (needs Grafana v6.0).</li></ul><p>A Loki-based logging stack consists of 3 components:</p><ul><li><a href=\"https://github.com/grafana/alloy\">Alloy</a> is agent, responsible for gathering logs and sending them to Loki.</li><li><a href=\"https://github.com/grafana/loki\">Loki</a> is the main service, responsible for storing logs and processing queries.</li><li><a href=\"https://github.com/grafana/grafana\">Grafana</a> for querying and displaying the logs.</li></ul><p><strong>Note that Alloy replaced Promtail in the stack, because Promtail is considered to be feature complete, and future development for logs collection will be in <a href=\"https://github.com/grafana/alloy\">Grafana Alloy</a>.</strong></p><p>Loki is like Prometheus, but for logs: we prefer a multidimensional label-based approach to indexing, and want a single-binary, easy to operate system with no dependencies. Loki differs from Prometheus by focusing on logs instead of metrics, and delivering logs via push, instead of pull.</p><p>If you have any questions or feedback regarding Loki:</p><p>Your feedback is always welcome.</p><p>Loki can be run in a single host, no-dependencies mode using the following commands.</p><p>You need an up-to-date version of <a href=\"https://go.dev/\">Go</a>, we recommend using the version found in our <a href=\"https://github.com/grafana/loki/raw/main/Makefile\">Makefile</a></p><pre><code># Checkout source code\n$ git clone https://github.com/grafana/loki\n$ cd loki\n\n# Build binary\n$ go build ./cmd/loki\n\n# Run executable\n$ ./loki -config.file=./cmd/loki/loki-local-config.yaml\n</code></pre><p>Alternatively, on Unix systems you can use  to build the binary, which adds additional arguments to the  command.</p><pre><code># Build binary\n$ make loki\n\n# Run executable\n$ ./cmd/loki/loki -config.file=./cmd/loki/loki-local-config.yaml\n</code></pre><p>To build Promtail on non-Linux platforms, use the following command:</p><pre><code>$ go build ./clients/cmd/promtail\n</code></pre><p>On Linux, Promtail requires the systemd headers to be installed if Journal support is enabled. To enable Journal support the go build tag flag  should be passed</p><p>With Journal support on Ubuntu, run with the following commands:</p><pre><code>$ sudo apt install -y libsystemd-dev\n$ go build --tags=promtail_journal_enabled ./clients/cmd/promtail\n</code></pre><p>With Journal support on CentOS, run with the following commands:</p><pre><code>$ sudo yum install -y systemd-devel\n$ go build --tags=promtail_journal_enabled ./clients/cmd/promtail\n</code></pre><p>Otherwise, to build Promtail without Journal support, run  with CGO disabled:</p><pre><code>$ CGO_ENABLED=0 go build ./clients/cmd/promtail\n</code></pre><p>Please see <a href=\"https://raw.githubusercontent.com/grafana/loki/main/ADOPTERS.md\">ADOPTERS.md</a> for some of the organizations using Loki today. If you would like to add your organization to the list, please open a PR to add it to the list.</p>","contentLength":3153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"gabime/spdlog","url":"https://github.com/gabime/spdlog","date":1750473272,"author":"","guid":163967,"unread":true,"content":"<p>Fast C++ logging library.</p><p>Copy the include <a href=\"https://raw.githubusercontent.com/gabime/spdlog/v1.x/include/spdlog\">folder</a> to your build tree and use a C++11 compiler.</p><h4>Compiled version (recommended - much faster compile times)</h4><pre><code>$ git clone https://github.com/gabime/spdlog.git\n$ cd spdlog &amp;&amp; mkdir build &amp;&amp; cd build\n$ cmake .. &amp;&amp; cmake --build .\n</code></pre><ul><li>Linux, FreeBSD, OpenBSD, Solaris, AIX</li><li>Windows (msvc 2013+, cygwin)</li></ul><ul><li>Debian: <code>sudo apt install libspdlog-dev</code></li><li>Homebrew: </li><li>MacPorts: </li><li>FreeBSD: </li><li>Fedora: </li><li>Gentoo: </li><li>Arch Linux: </li><li>openSUSE: <code>sudo zypper in spdlog-devel</code></li><li>ALT Linux: <code>apt-get install libspdlog-devel</code></li><li>vcpkg: </li><li>conan: <code>conan install --requires=spdlog/[*]</code></li><li>conda: <code>conda install -c conda-forge spdlog</code></li><li>build2: </li></ul><ul><li>Feature-rich formatting, using the excellent <a href=\"https://github.com/fmtlib/fmt\">fmt</a> library.</li><li>Asynchronous mode (optional)</li><li>Multi/Single threaded loggers.</li><li>Various log targets: \n  <ul><li>Console logging (colors supported).</li><li>Windows debugger ().</li></ul></li><li>Log filtering - log levels can be modified at runtime as well as compile time.</li><li>Support for loading log levels from argv or environment var.</li><li><a href=\"https://raw.githubusercontent.com/gabime/spdlog/v1.x/#backtrace-support\">Backtrace</a> support - store debug messages in a ring buffer and display them later on demand.</li></ul><pre><code>#include \"spdlog/spdlog.h\"\n\nint main() \n{\n    spdlog::info(\"Welcome to spdlog!\");\n    spdlog::error(\"Some error message with arg: {}\", 1);\n    \n    spdlog::warn(\"Easy padding in numbers like {:08d}\", 12);\n    spdlog::critical(\"Support for int: {0:d};  hex: {0:x};  oct: {0:o}; bin: {0:b}\", 42);\n    spdlog::info(\"Support for floats {:03.2f}\", 1.23456);\n    spdlog::info(\"Positional args are {1} {0}..\", \"too\", \"supported\");\n    spdlog::info(\"{:&lt;30}\", \"left aligned\");\n    \n    spdlog::set_level(spdlog::level::debug); // Set global log level to debug\n    spdlog::debug(\"This message should be displayed..\");    \n    \n    // change log pattern\n    spdlog::set_pattern(\"[%H:%M:%S %z] [%n] [%^---%L---%$] [thread %t] %v\");\n    \n    // Compile time log levels\n    // Note that this does not change the current log level, it will only\n    // remove (depending on SPDLOG_ACTIVE_LEVEL) the call on the release code.\n    SPDLOG_TRACE(\"Some trace message with param {}\", 42);\n    SPDLOG_DEBUG(\"Some debug message\");\n}\n\n</code></pre><h4>Create stdout/stderr logger object</h4><pre><code>#include \"spdlog/spdlog.h\"\n#include \"spdlog/sinks/stdout_color_sinks.h\"\nvoid stdout_example()\n{\n    // create a color multi-threaded logger\n    auto console = spdlog::stdout_color_mt(\"console\");    \n    auto err_logger = spdlog::stderr_color_mt(\"stderr\");    \n    spdlog::get(\"console\")-&gt;info(\"loggers can be retrieved from a global registry using the spdlog::get(logger_name)\");\n}\n</code></pre><pre><code>#include \"spdlog/sinks/basic_file_sink.h\"\nvoid basic_logfile_example()\n{\n    try \n    {\n        auto logger = spdlog::basic_logger_mt(\"basic_logger\", \"logs/basic-log.txt\");\n    }\n    catch (const spdlog::spdlog_ex &amp;ex)\n    {\n        std::cout &lt;&lt; \"Log init failed: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n    }\n}\n</code></pre><pre><code>#include \"spdlog/sinks/rotating_file_sink.h\"\nvoid rotating_example()\n{\n    // Create a file rotating logger with 5 MB size max and 3 rotated files\n    auto max_size = 1048576 * 5;\n    auto max_files = 3;\n    auto logger = spdlog::rotating_logger_mt(\"some_logger_name\", \"logs/rotating.txt\", max_size, max_files);\n}\n</code></pre><pre><code>\n#include \"spdlog/sinks/daily_file_sink.h\"\nvoid daily_example()\n{\n    // Create a daily logger - a new file is created every day at 2:30 am\n    auto logger = spdlog::daily_logger_mt(\"daily_logger\", \"logs/daily.txt\", 2, 30);\n}\n\n</code></pre><pre><code>// Debug messages can be stored in a ring buffer instead of being logged immediately.\n// This is useful to display debug logs only when needed (e.g. when an error happens).\n// When needed, call dump_backtrace() to dump them to your log.\n\nspdlog::enable_backtrace(32); // Store the latest 32 messages in a buffer. \n// or my_logger-&gt;enable_backtrace(32)..\nfor(int i = 0; i &lt; 100; i++)\n{\n  spdlog::debug(\"Backtrace message {}\", i); // not logged yet..\n}\n// e.g. if some error happened:\nspdlog::dump_backtrace(); // log them now! show the last 32 messages\n// or my_logger-&gt;dump_backtrace(32)..\n</code></pre><pre><code>// periodically flush all *registered* loggers every 3 seconds:\n// warning: only use if all your loggers are thread-safe (\"_mt\" loggers)\nspdlog::flush_every(std::chrono::seconds(3));\n\n</code></pre><pre><code>// Stopwatch support for spdlog\n#include \"spdlog/stopwatch.h\"\nvoid stopwatch_example()\n{\n    spdlog::stopwatch sw;    \n    spdlog::debug(\"Elapsed {}\", sw);\n    spdlog::debug(\"Elapsed {:.3}\", sw);       \n}\n\n</code></pre><pre><code>// many types of std::container&lt;char&gt; types can be used.\n// ranges are supported too.\n// format flags:\n// {:X} - print in uppercase.\n// {:s} - don't separate each byte with space.\n// {:p} - don't print the position on each line start.\n// {:n} - don't split the output into lines.\n// {:a} - show ASCII if :n is not set.\n\n#include \"spdlog/fmt/bin_to_hex.h\"\n\nvoid binary_example()\n{\n    auto console = spdlog::get(\"console\");\n    std::array&lt;char, 80&gt; buf;\n    console-&gt;info(\"Binary example: {}\", spdlog::to_hex(buf));\n    console-&gt;info(\"Another binary example:{:n}\", spdlog::to_hex(std::begin(buf), std::begin(buf) + 10));\n    // more examples:\n    // logger-&gt;info(\"uppercase: {:X}\", spdlog::to_hex(buf));\n    // logger-&gt;info(\"uppercase, no delimiters: {:Xs}\", spdlog::to_hex(buf));\n    // logger-&gt;info(\"uppercase, no delimiters, no position info: {:Xsp}\", spdlog::to_hex(buf));\n}\n\n</code></pre><h4>Logger with multi sinks - each with a different format and log level</h4><pre><code>\n// create a logger with 2 targets, with different log levels and formats.\n// The console will show only warnings or errors, while the file will log all.\nvoid multi_sink_example()\n{\n    auto console_sink = std::make_shared&lt;spdlog::sinks::stdout_color_sink_mt&gt;();\n    console_sink-&gt;set_level(spdlog::level::warn);\n    console_sink-&gt;set_pattern(\"[multi_sink_example] [%^%l%$] %v\");\n\n    auto file_sink = std::make_shared&lt;spdlog::sinks::basic_file_sink_mt&gt;(\"logs/multisink.txt\", true);\n    file_sink-&gt;set_level(spdlog::level::trace);\n\n    spdlog::logger logger(\"multi_sink\", {console_sink, file_sink});\n    logger.set_level(spdlog::level::debug);\n    logger.warn(\"this should appear in both console and file\");\n    logger.info(\"this message should not appear in the console, only in the file\");\n}\n</code></pre><h4>User-defined callbacks about log events</h4><pre><code>\n// create a logger with a lambda function callback, the callback will be called\n// each time something is logged to the logger\nvoid callback_example()\n{\n    auto callback_sink = std::make_shared&lt;spdlog::sinks::callback_sink_mt&gt;([](const spdlog::details::log_msg &amp;msg) {\n         // for example you can be notified by sending an email to yourself\n    });\n    callback_sink-&gt;set_level(spdlog::level::err);\n\n    auto console_sink = std::make_shared&lt;spdlog::sinks::stdout_color_sink_mt&gt;();\n    spdlog::logger logger(\"custom_callback_logger\", {console_sink, callback_sink});\n\n    logger.info(\"some info log\");\n    logger.error(\"critical issue\"); // will notify you\n}\n</code></pre><pre><code>#include \"spdlog/async.h\"\n#include \"spdlog/sinks/basic_file_sink.h\"\nvoid async_example()\n{\n    // default thread pool settings can be modified *before* creating the async logger:\n    // spdlog::init_thread_pool(8192, 1); // queue with 8k items and 1 backing thread.\n    auto async_file = spdlog::basic_logger_mt&lt;spdlog::async_factory&gt;(\"async_file_logger\", \"logs/async_log.txt\");\n    // alternatively:\n    // auto async_file = spdlog::create_async&lt;spdlog::sinks::basic_file_sink_mt&gt;(\"async_file_logger\", \"logs/async_log.txt\");   \n}\n\n</code></pre><h4>Asynchronous logger with multi sinks</h4><pre><code>#include \"spdlog/async.h\"\n#include \"spdlog/sinks/stdout_color_sinks.h\"\n#include \"spdlog/sinks/rotating_file_sink.h\"\n\nvoid multi_sink_example2()\n{\n    spdlog::init_thread_pool(8192, 1);\n    auto stdout_sink = std::make_shared&lt;spdlog::sinks::stdout_color_sink_mt &gt;();\n    auto rotating_sink = std::make_shared&lt;spdlog::sinks::rotating_file_sink_mt&gt;(\"mylog.txt\", 1024*1024*10, 3);\n    std::vector&lt;spdlog::sink_ptr&gt; sinks {stdout_sink, rotating_sink};\n    auto logger = std::make_shared&lt;spdlog::async_logger&gt;(\"loggername\", sinks.begin(), sinks.end(), spdlog::thread_pool(), spdlog::async_overflow_policy::block);\n    spdlog::register_logger(logger);\n}\n</code></pre><pre><code>template&lt;&gt;\nstruct fmt::formatter&lt;my_type&gt; : fmt::formatter&lt;std::string&gt;\n{\n    auto format(my_type my, format_context &amp;ctx) const -&gt; decltype(ctx.out())\n    {\n        return fmt::format_to(ctx.out(), \"[my_type i={}]\", my.i);\n    }\n};\n\nvoid user_defined_example()\n{\n    spdlog::info(\"user defined type: {}\", my_type(14));\n}\n\n</code></pre><h4>User-defined flags in the log pattern</h4><pre><code>// Log patterns can contain custom flags.\n// the following example will add new flag '%*' - which will be bound to a &lt;my_formatter_flag&gt; instance.\n#include \"spdlog/pattern_formatter.h\"\nclass my_formatter_flag : public spdlog::custom_flag_formatter\n{\npublic:\n    void format(const spdlog::details::log_msg &amp;, const std::tm &amp;, spdlog::memory_buf_t &amp;dest) override\n    {\n        std::string some_txt = \"custom-flag\";\n        dest.append(some_txt.data(), some_txt.data() + some_txt.size());\n    }\n\n    std::unique_ptr&lt;custom_flag_formatter&gt; clone() const override\n    {\n        return spdlog::details::make_unique&lt;my_formatter_flag&gt;();\n    }\n};\n\nvoid custom_flags_example()\n{    \n    auto formatter = std::make_unique&lt;spdlog::pattern_formatter&gt;();\n    formatter-&gt;add_flag&lt;my_formatter_flag&gt;('*').set_pattern(\"[%n] [%*] [%^%l%$] %v\");\n    spdlog::set_formatter(std::move(formatter));\n}\n\n</code></pre><pre><code>void err_handler_example()\n{\n    // can be set globally or per logger(logger-&gt;set_error_handler(..))\n    spdlog::set_error_handler([](const std::string &amp;msg) { spdlog::get(\"console\")-&gt;error(\"*** LOGGER ERROR ***: {}\", msg); });\n    spdlog::get(\"console\")-&gt;info(\"some invalid message to trigger an error {}{}{}{}\", 3);\n}\n\n</code></pre><pre><code>#include \"spdlog/sinks/syslog_sink.h\"\nvoid syslog_example()\n{\n    std::string ident = \"spdlog-example\";\n    auto syslog_logger = spdlog::syslog_logger_mt(\"syslog\", ident, LOG_PID);\n    syslog_logger-&gt;warn(\"This is warning that will end up in syslog.\");\n}\n</code></pre><pre><code>#include \"spdlog/sinks/android_sink.h\"\nvoid android_example()\n{\n    std::string tag = \"spdlog-android\";\n    auto android_logger = spdlog::android_logger_mt(\"android\", tag);\n    android_logger-&gt;critical(\"Use \\\"adb shell logcat\\\" to view this message.\");\n}\n</code></pre><h4>Load log levels from the env variable or argv</h4><pre><code>#include \"spdlog/cfg/env.h\"\nint main (int argc, char *argv[])\n{\n    spdlog::cfg::load_env_levels();\n    // or specify the env variable name:\n    // MYAPP_LEVEL=info,mylogger=trace &amp;&amp; ./example\n    // spdlog::cfg::load_env_levels(\"MYAPP_LEVEL\");\n    // or from the command line:\n    // ./example SPDLOG_LEVEL=info,mylogger=trace\n    // #include \"spdlog/cfg/argv.h\" // for loading levels from argv\n    // spdlog::cfg::load_argv_levels(argc, argv);\n}\n</code></pre><pre><code>$ export SPDLOG_LEVEL=info,mylogger=trace\n$ ./example\n</code></pre><h4>Log file open/close event handlers</h4><pre><code>// You can get callbacks from spdlog before/after a log file has been opened or closed. \n// This is useful for cleanup procedures or for adding something to the start/end of the log file.\nvoid file_events_example()\n{\n    // pass the spdlog::file_event_handlers to file sinks for open/close log file notifications\n    spdlog::file_event_handlers handlers;\n    handlers.before_open = [](spdlog::filename_t filename) { spdlog::info(\"Before opening {}\", filename); };\n    handlers.after_open = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(\"After opening\\n\", fstream); };\n    handlers.before_close = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(\"Before closing\\n\", fstream); };\n    handlers.after_close = [](spdlog::filename_t filename) { spdlog::info(\"After closing {}\", filename); };\n    auto my_logger = spdlog::basic_logger_st(\"some_logger\", \"logs/events-sample.txt\", true, handlers);        \n}\n</code></pre><h4>Replace the Default Logger</h4><pre><code>void replace_default_logger_example()\n{\n    auto new_logger = spdlog::basic_logger_mt(\"new_default_logger\", \"logs/new-default-log.txt\", true);\n    spdlog::set_default_logger(new_logger);\n    spdlog::info(\"new logger log message\");\n}\n</code></pre><h4>Log to Qt with nice colors</h4><pre><code>#include \"spdlog/spdlog.h\"\n#include \"spdlog/sinks/qt_sinks.h\"\nMainWindow::MainWindow(QWidget *parent) : QMainWindow(parent)\n{\n    setMinimumSize(640, 480);\n    auto log_widget = new QTextEdit(this);\n    setCentralWidget(log_widget);\n    int max_lines = 500; // keep the text widget to max 500 lines. remove old lines if needed.\n    auto logger = spdlog::qt_color_logger_mt(\"qt_logger\", log_widget, max_lines);\n    logger-&gt;info(\"Some info message\");\n}\n</code></pre><h4>Mapped Diagnostic Context</h4><pre><code>// Mapped Diagnostic Context (MDC) is a map that stores key-value pairs (string values) in thread local storage.\n// Each thread maintains its own MDC, which loggers use to append diagnostic information to log outputs.\n// Note: it is not supported in asynchronous mode due to its reliance on thread-local storage.\n#include \"spdlog/mdc.h\"\nvoid mdc_example()\n{\n    spdlog::mdc::put(\"key1\", \"value1\");\n    spdlog::mdc::put(\"key2\", \"value2\");\n    // if not using the default format, use the %&amp; formatter to print mdc data\n    // spdlog::set_pattern(\"[%H:%M:%S %z] [%^%L%$] [%&amp;] %v\");\n}\n</code></pre><p>Below are some <a href=\"https://raw.githubusercontent.com/gabime/spdlog/v1.x/bench/bench.cpp\">benchmarks</a> done in Ubuntu 64 bit, Intel i7-4770 CPU @ 3.40GHz</p><pre><code>[info] **************************************************************\n[info] Single thread, 1,000,000 iterations\n[info] **************************************************************\n[info] basic_st         Elapsed: 0.17 secs        5,777,626/sec\n[info] rotating_st      Elapsed: 0.18 secs        5,475,894/sec\n[info] daily_st         Elapsed: 0.20 secs        5,062,659/sec\n[info] empty_logger     Elapsed: 0.07 secs       14,127,300/sec\n[info] **************************************************************\n[info] C-string (400 bytes). Single thread, 1,000,000 iterations\n[info] **************************************************************\n[info] basic_st         Elapsed: 0.41 secs        2,412,483/sec\n[info] rotating_st      Elapsed: 0.72 secs        1,389,196/sec\n[info] daily_st         Elapsed: 0.42 secs        2,393,298/sec\n[info] null_st          Elapsed: 0.04 secs       27,446,957/sec\n[info] **************************************************************\n[info] 10 threads, competing over the same logger object, 1,000,000 iterations\n[info] **************************************************************\n[info] basic_mt         Elapsed: 0.60 secs        1,659,613/sec\n[info] rotating_mt      Elapsed: 0.62 secs        1,612,493/sec\n[info] daily_mt         Elapsed: 0.61 secs        1,638,305/sec\n[info] null_mt          Elapsed: 0.16 secs        6,272,758/sec\n</code></pre><pre><code>[info] -------------------------------------------------\n[info] Messages     : 1,000,000\n[info] Threads      : 10\n[info] Queue        : 8,192 slots\n[info] Queue memory : 8,192 x 272 = 2,176 KB \n[info] -------------------------------------------------\n[info] \n[info] *********************************\n[info] Queue Overflow Policy: block\n[info] *********************************\n[info] Elapsed: 1.70784 secs     585,535/sec\n[info] Elapsed: 1.69805 secs     588,910/sec\n[info] Elapsed: 1.7026 secs      587,337/sec\n[info] \n[info] *********************************\n[info] Queue Overflow Policy: overrun\n[info] *********************************\n[info] Elapsed: 0.372816 secs    2,682,285/sec\n[info] Elapsed: 0.379758 secs    2,633,255/sec\n[info] Elapsed: 0.373532 secs    2,677,147/sec\n\n</code></pre><p>Documentation can be found in the <a href=\"https://github.com/gabime/spdlog/wiki\">wiki</a> pages.</p><a href=\"https://jb.gg/OpenSource\"><img src=\"https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg?sanitize=true\" alt=\"JetBrains logo\" width=\"200\"></a>","contentLength":15134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rasbt/LLMs-from-scratch","url":"https://github.com/rasbt/LLMs-from-scratch","date":1750473272,"author":"","guid":163968,"unread":true,"content":"<p>Implement a ChatGPT-like LLM in PyTorch from scratch, step by step</p><p>In <a href=\"http://mng.bz/orYv\"><em>Build a Large Language Model (From Scratch)</em></a>, you'll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I'll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.</p><p>The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.</p><p>To download a copy of this repository, click on the <a href=\"https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip\">Download ZIP</a> button or execute the following command in your terminal:</p><pre><code>git clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git\n</code></pre><p>Please note that this  file is a Markdown () file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven't installed a Markdown editor yet, <a href=\"https://www.marktext.cc\">MarkText</a> is a good free option.</p><blockquote><p> If you're seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the <a href=\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/README.md\">README.md</a> file located in the <a href=\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup\">setup</a> directory.</p></blockquote><p>The mental model below summarizes the contents covered in this book.</p><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg\" width=\"650px\"><p>The code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the <a href=\"https://github.com/rasbt/LLMs-from-scratch/raw/main/setup/README.md\">setup</a> doc for additional recommendations.)</p><p>Several folders contain optional materials as a bonus for interested readers:</p><h2>Questions, Feedback, and Contributing to This Repository</h2><p>I welcome all sorts of feedback, best shared via the <a href=\"https://livebook.manning.com/forum?product=raschka&amp;page=1\">Manning Forum</a> or <a href=\"https://github.com/rasbt/LLMs-from-scratch/discussions\">GitHub Discussions</a>. Likewise, if you have any questions or just want to bounce ideas off others, please don't hesitate to post these in the forum as well.</p><p>Please note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.</p><p>If you find this book or code useful for your research, please consider citing it.</p><blockquote><p>Raschka, Sebastian. <em>Build A Large Language Model (From Scratch)</em>. Manning, 2024. ISBN: 978-1633437166.</p></blockquote><pre><code>@book{build-llms-from-scratch-book,\n  author       = {Sebastian Raschka},\n  title        = {Build A Large Language Model (From Scratch)},\n  publisher    = {Manning},\n  year         = {2024},\n  isbn         = {978-1633437166},\n  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},\n  github       = {https://github.com/rasbt/LLMs-from-scratch}\n}\n</code></pre>","contentLength":3033,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ManimCommunity/manim","url":"https://github.com/ManimCommunity/manim","date":1750473272,"author":"","guid":163969,"unread":true,"content":"<p>A community-maintained Python framework for creating mathematical animations.</p><p>Manim is an animation engine for explanatory math videos. It's used to create precise animations programmatically, as demonstrated in the videos of <a href=\"https://www.3blue1brown.com/\">3Blue1Brown</a>.</p><blockquote><p>[!NOTE] The community edition of Manim (ManimCE) is a version maintained and developed by the community. It was forked from 3b1b/manim, a tool originally created and open-sourced by Grant Sanderson, also creator of the 3Blue1Brown educational math videos. While Grant Sanderson continues to maintain his own repository, we recommend this version for its continued development, improved features, enhanced documentation, and more active community-driven maintenance. If you would like to study how Grant makes his videos, head over to his repository (<a href=\"https://github.com/3b1b/manim\">3b1b/manim</a>).</p></blockquote><blockquote><p>[!CAUTION] These instructions are for the community version . Trying to use these instructions to install <a href=\"https://github.com/3b1b/manim\">3b1b/manim</a> or instructions there to install this version will cause problems. Read <a href=\"https://docs.manim.community/en/stable/faq/installation.html#why-are-there-different-versions-of-manim\">this</a> and decide which version you wish to install, then only follow the instructions for your desired version.</p></blockquote><p>Manim requires a few dependencies that must be installed prior to using it. If you want to try it out first before installing it locally, you can do so <a href=\"https://try.manim.community/\">in our online Jupyter environment</a>.</p><p>For local installation, please visit the <a href=\"https://docs.manim.community/en/stable/installation.html\">Documentation</a> and follow the appropriate instructions for your operating system.</p><p>Manim is an extremely versatile package. The following is an example  you can construct:</p><pre><code>from manim import *\n\n\nclass SquareToCircle(Scene):\n    def construct(self):\n        circle = Circle()\n        square = Square()\n        square.flip(RIGHT)\n        square.rotate(-3 * TAU / 8)\n        circle.set_fill(PINK, opacity=0.5)\n\n        self.play(Create(square))\n        self.play(Transform(square, circle))\n        self.play(FadeOut(square))\n</code></pre><p>In order to view the output of this scene, save the code in a file called . Then, run the following in a terminal window:</p><pre><code>manim -p -ql example.py SquareToCircle\n</code></pre><p>You should see your native video player program pop up and play a simple scene in which a square is transformed into a circle. You may find some more simple examples within this <a href=\"https://raw.githubusercontent.com/ManimCommunity/manim/main/example_scenes\">GitHub repository</a>. You can also visit the <a href=\"https://docs.manim.community/en/stable/examples.html\">official gallery</a> for more advanced examples.</p><p>The general usage of Manim is as follows:</p><p>The  flag in the command above is for previewing, meaning the video file will automatically open when it is done rendering. The  flag is for a faster rendering at a lower quality.</p><p>Some other useful flags include:</p><ul><li> to skip to the end and just show the final frame.</li><li> to skip ahead to the 'th animation of a scene.</li><li> show the file in the file browser.</li></ul><p>For a thorough list of command line arguments, visit the <a href=\"https://docs.manim.community/en/stable/guides/configuration.html\">documentation</a>.</p><p>The community also maintains a docker image (), which can be found <a href=\"https://hub.docker.com/r/manimcommunity/manim\">on DockerHub</a>. Instructions on how to install and use it can be found in our <a href=\"https://docs.manim.community/en/stable/installation/docker.html\">documentation</a>.</p><p>If you need help installing or using Manim, feel free to reach out to our <a href=\"https://www.manim.community/discord/\">Discord Server</a> or <a href=\"https://www.reddit.com/r/manim\">Reddit Community</a>. If you would like to submit a bug report or feature request, please open an issue.</p><p>Contributions to Manim are always welcome. In particular, there is a dire need for tests and documentation. For contribution guidelines, please see the <a href=\"https://docs.manim.community/en/stable/contributing.html\">documentation</a>.</p><p>However, please note that Manim is currently undergoing a major refactor. In general, contributions implementing new features will not be accepted in this period. The contribution guide may become outdated quickly; we highly recommend joining our <a href=\"https://www.manim.community/discord/\">Discord server</a> to discuss any potential contributions and keep up to date with the latest developments.</p><p>Most developers on the project use  for management. You'll want to have uv installed and available in your environment. Learn more about  at its <a href=\"https://docs.astral.sh/uv/\">documentation</a> and find out how to install manim with uv at the <a href=\"https://docs.manim.community/en/latest/contributing/development.html\">manim dev-installation guide</a> in the manim documentation.</p><p>We acknowledge the importance of good software to support research, and we note that research becomes more valuable when it is communicated effectively. To demonstrate the value of Manim, we ask that you cite Manim in your work. Currently, the best way to cite Manim is to go to our <a href=\"https://github.com/ManimCommunity/manim\">repository page</a> (if you aren't already) and click the \"cite this repository\" button on the right sidebar. This will generate a citation in your preferred format, and will also integrate well with citation managers.</p><p>Our full code of conduct, and how we enforce it, can be read on <a href=\"https://docs.manim.community/en/stable/conduct.html\">our website</a>.</p><p>The software is double-licensed under the MIT license, with copyright by 3blue1brown LLC (see LICENSE), and copyright by Manim Community Developers (see LICENSE.community).</p>","contentLength":4595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"n8n-io/n8n","url":"https://github.com/n8n-io/n8n","date":1750473272,"author":"","guid":163970,"unread":true,"content":"<p>Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.</p><p>n8n is a workflow automation platform that gives technical teams the flexibility of code with the speed of no-code. With 400+ integrations, native AI capabilities, and a fair-code license, n8n lets you build powerful automations while maintaining full control over your data and deployments.</p><ul><li>: Write JavaScript/Python, add npm packages, or use the visual interface</li><li>: Build AI agent workflows based on LangChain with your own data and models</li><li>: Self-host with our fair-code license or use our <a href=\"https://app.n8n.cloud/login\">cloud offering</a></li><li>: Advanced permissions, SSO, and air-gapped deployments</li><li>: 400+ integrations and 900+ ready-to-use <a href=\"https://n8n.io/workflows\">templates</a></li></ul><pre><code>docker volume create n8n_data\ndocker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n\n</code></pre><p>Need help? Our community forum is the place to get support and connect with other users: <a href=\"https://community.n8n.io\">community.n8n.io</a></p><ul><li>: Always visible source code</li><li>: Deploy anywhere</li><li>: Add your own nodes and functionality</li></ul><p>Additional information about the license model can be found in the <a href=\"https://docs.n8n.io/reference/license/\">docs</a>.</p><p>Want to shape the future of automation? Check out our <a href=\"https://n8n.io/careers\">job posts</a> and join our team!</p><p> It means \"nodemation\" and is pronounced as n-eight-n.</p><p> \"I get that question quite often (more often than I expected) so I decided it is probably best to answer it here. While looking for a good name for the project with a free domain I realized very quickly that all the good ones I could think of were already taken. So, in the end, I chose nodemation. 'node-' in the sense that it uses a Node-View and that it uses Node.js and '-mation' for 'automation' which is what the project is supposed to help with. However, I did not like how long the name was and I could not imagine writing something that long every time in the CLI. That is when I then ended up on 'n8n'.\" - <strong>Jan Oberhauser, Founder and CEO, n8n.io</strong></p>","contentLength":1934,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MystenLabs/sui","url":"https://github.com/MystenLabs/sui","date":1750386936,"author":"","guid":163242,"unread":true,"content":"<p>Sui, a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the Move programming language</p><p><a href=\"https://sui.io\">Sui</a> is a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the <a href=\"https://github.com/MystenLabs/awesome-move\">Move programming language</a>.</p><p>Sui offers the following benefits and capabilities:</p><ul><li>Unmatched scalability, instant settlement</li><li>A safe smart contract language accessible to mainstream developers</li><li>Ability to define rich and composable on-chain assets</li><li>Better user experience for web3 apps</li></ul><p>Sui is the only blockchain today that can scale with the growth of web3 while achieving industry-leading performance, cost, programmability, and usability. As Sui approaches Mainnet launch, it will demonstrate capacity beyond the transaction processing capabilities of established systems ‚Äì traditional and blockchain alike. Sui is the first internet-scale programmable blockchain platform, a foundational layer for web3.</p><pre><code>flowchart LR\n    CC(CLI Client) --&gt; ClientService\n    RC(Rest Client) --&gt; ClientService\n    RPCC(RPC Client) --&gt; ClientService\n    ClientService --&gt; AuthorityAggregator\n    AuthorityAggregator --&gt; AC1[AuthorityClient] &amp; AC2[AuthorityClient]\n    subgraph Authority1\n      AS[AuthorityState]\n    end\n    subgraph Authority2\n      AS2[AuthorityState]\n    end\n    AC1 &lt;==&gt;|Network TCP| Authority1\n    AC2 &lt;==&gt;|Network TCP| Authority2\n</code></pre><p>Sui is a smart contract platform maintained by a permissionless set of authorities that play a role similar to validators or miners in other blockchain systems.</p><p>Sui offers scalability and unprecedented low-latency for common use cases. Sui makes the vast majority of transactions processable in parallel, which makes better use of processing resources, and offers the option to increase throughput with more resources. Sui forgoes consensus to instead use simpler and lower-latency primitives for common use cases, such as payment transactions and asset transfers. This is unprecedented in the blockchain world and enables a number of new latency-sensitive distributed applications, ranging from gaming to retail payment at physical points of sale.</p><p>Sui is written in <a href=\"https://www.rust-lang.org\">Rust</a> and supports smart contracts written in the <a href=\"https://github.com/move-language/move\">Move programming language</a> to define assets that may have an owner. Move programs define operations on these assets including custom rules for their creation, the transfer of these assets to new owners, and operations that mutate assets.</p><p>Sui has a native token called SUI, with a fixed supply. The SUI token is used to pay for gas, and is also used as <a href=\"https://learn.bybit.com/blockchain/delegated-proof-of-stake-dpos/\">delegated stake on authorities</a> within an epoch. The voting power of authorities within this epoch is a function of this delegated stake. Authorities are periodically reconfigured according to the stake delegated to them. In any epoch, the set of authorities is <a href=\"https://pmg.csail.mit.edu/papers/osdi99.pdf\">Byzantine fault tolerant</a>. At the end of the epoch, fees collected through all transactions processed are distributed to authorities according to their contribution to the operation of the system. Authorities can in turn share some of the fees as rewards to users that delegated stakes to them.</p><p>Sui is supported by several cutting-edge <a href=\"https://github.com/MystenLabs/sui/raw/main/docs/content/concepts/research-papers.mdx\">peer-reviewed studies</a> and extensive years of open-source development.</p><p>Use the following links to learn more about Sui and the Sui ecosystem:</p><p>See the <a href=\"https://raw.githubusercontent.com/MystenLabs/sui/main/LICENSE\">LICENSE</a> file for more details.</p>","contentLength":3363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"espressif/esp-idf","url":"https://github.com/espressif/esp-idf","date":1750386936,"author":"","guid":163243,"unread":true,"content":"<p>Espressif IoT Development Framework. Official development framework for Espressif SoCs.</p><p>ESP-IDF is the development framework for Espressif SoCs supported on Windows, Linux and macOS.</p><p>The following table shows ESP-IDF support of Espressif SoCs where <img src=\"https://img.shields.io/badge/-preview-orange\" alt=\"alt text\" title=\"preview\"> and <img src=\"https://img.shields.io/badge/-supported-green\" alt=\"alt text\" title=\"supported\"> denote preview status and support, respectively. The preview support is usually limited in time and intended for beta versions of chips. Please use an ESP-IDF release where the desired SoC is already supported.</p><p>Espressif SoCs released before 2016 (ESP8266 and ESP8285) are supported by <a href=\"https://github.com/espressif/ESP8266_RTOS_SDK\">RTOS SDK</a> instead.</p><p> Each SoC series and each ESP-IDF release has its own documentation. Please see Section <a href=\"https://docs.espressif.com/projects/esp-idf/en/latest/esp32/versions.html\">Versions</a> on how to find documentation and how to checkout specific release of ESP-IDF.</p><p>ESP-IDF uses relative locations as its submodules URLs (<a href=\"https://raw.githubusercontent.com/espressif/esp-idf/master/.gitmodules\">.gitmodules</a>). So they link to GitHub. If ESP-IDF is forked to a Git repository which is not on GitHub, you will need to run the script <a href=\"https://raw.githubusercontent.com/espressif/esp-idf/master/tools/set-submodules-to-github.sh\">tools/set-submodules-to-github.sh</a> after git clone.</p><p>The script sets absolute URLs for all submodules, allowing <code>git submodule update --init --recursive</code> to complete. If cloning ESP-IDF from GitHub, this step is not needed.</p><p>As well as the <a href=\"https://github.com/espressif/esp-idf-template\">esp-idf-template</a> project mentioned in Getting Started, ESP-IDF comes with some example projects in the <a href=\"https://raw.githubusercontent.com/espressif/esp-idf/master/examples\">examples</a> directory.</p><p>Once you've found the project you want to work with, change to its directory and you can configure and build it.</p><p>To start your own project based on an example, copy the example project directory outside of the ESP-IDF directory.</p><p>See the Getting Started guide links above for a detailed setup guide. This is a quick reference for common commands when working with ESP-IDF projects:</p><p>(See the Getting Started guide listed above for a full list of required steps with more details.)</p><ul><li>Install host build dependencies mentioned in the Getting Started guide.</li><li>Run the install script to set up the build environment. The options include  or  for Windows, and  or  for Unix shells.</li><li>Run the export script on Windows () or source it on Unix () in every shell environment before using ESP-IDF.</li></ul><ul><li><code>idf.py set-target &lt;chip_name&gt;</code> sets the target of the project to . Run  without any arguments to see a list of supported targets.</li><li> opens a text-based configuration menu where you can configure the project.</li></ul><p>... will compile app, bootloader and generate a partition table based on the config.</p><p>When the build finishes, it will print a command line to use esptool.py to flash the chip. However you can also do this automatically by running:</p><p>Replace PORT with the name of your serial port (like  on Windows,  on Linux, or  on MacOS. If the  option is left out,  will try to flash the first available serial port.</p><p>This will flash the entire project (app, bootloader and partition table) to a new chip. The settings for serial port flashing can be configured with .</p><p>You don't need to run  before running ,  will automatically rebuild anything which needs it.</p><p>Exit the monitor by typing Ctrl-].</p><p>To build, flash and monitor output in one pass, you can run:</p><h2>Compiling &amp; Flashing Only the App</h2><p>After the initial flash, you may just want to build and flash just your app, not the bootloader and partition table:</p><ul><li> - build just the app.</li><li> - flash just the app.</li></ul><p> will automatically rebuild the app if any source files have changed.</p><p>(In normal development there's no downside to reflashing the bootloader and partition table each time, if they haven't changed.)</p><p>The  target does not erase the entire flash contents. However it is sometimes useful to set the device back to a totally erased state, particularly when making partition table changes or OTA app updates. To erase the entire flash, run .</p><p>This can be combined with other targets, ie <code>idf.py -p PORT erase-flash flash</code> will erase everything and then re-flash the new app, bootloader and partition table.</p>","contentLength":3773,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"vllm-project/vllm","url":"https://github.com/vllm-project/vllm","date":1750386936,"author":"","guid":163244,"unread":true,"content":"<p>A high-throughput and memory-efficient inference and serving engine for LLMs</p><h3 align=\"center\"> Easy, fast, and cheap LLM serving for everyone </h3><ul><li>[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement <a href=\"https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/\">here</a>.</li><li>[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post <a href=\"https://blog.vllm.ai/2025/01/27/v1-alpha-release.html\">here</a>.</li></ul><p>vLLM is a fast and easy-to-use library for LLM inference and serving.</p><p>Originally developed in the <a href=\"https://sky.cs.berkeley.edu\">Sky Computing Lab</a> at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.</p><ul><li>State-of-the-art serving throughput</li><li>Efficient management of attention key and value memory with <a href=\"https://blog.vllm.ai/2023/06/20/vllm.html\"></a></li><li>Continuous batching of incoming requests</li><li>Fast model execution with CUDA/HIP graph</li><li>Optimized CUDA kernels, including integration with FlashAttention and FlashInfer</li></ul><p>vLLM is flexible and easy to use with:</p><ul><li>Seamless integration with popular Hugging Face models</li><li>High-throughput serving with various decoding algorithms, including , , and more</li><li>Tensor parallelism and pipeline parallelism support for distributed inference</li><li>OpenAI-compatible API server</li><li>Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron</li></ul><p>vLLM seamlessly supports most popular open-source models on HuggingFace, including:</p><ul><li>Transformer-like LLMs (e.g., Llama)</li><li>Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)</li><li>Embedding Models (e.g., E5-Mistral)</li><li>Multi-modal LLMs (e.g., LLaVA)</li></ul><p>Find the full list of supported models <a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\">here</a>.</p><p>We welcome and value any contributions and collaborations. Please check out <a href=\"https://docs.vllm.ai/en/latest/contributing/index.html\">Contributing to vLLM</a> for how to get involved.</p><p>vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!</p><ul></ul><ul></ul><p>We also have an official fundraising venue through <a href=\"https://opencollective.com/vllm\">OpenCollective</a>. We plan to use the fund to support the development, maintenance, and adoption of vLLM.</p><p>If you use vLLM for your research, please cite our <a href=\"https://arxiv.org/abs/2309.06180\">paper</a>:</p><pre><code>@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n</code></pre>","contentLength":2498,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ollama/ollama","url":"https://github.com/ollama/ollama","date":1750386936,"author":"","guid":163245,"unread":true,"content":"<p>Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models.</p><p>Get up and running with large language models.</p><pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre><p>Here are some example models that can be downloaded:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td><code>ollama run deepseek-r1:671b</code></td></tr><tr></tr><tr><td><code>ollama run llama4:maverick</code></td></tr><tr></tr><tr></tr><tr></tr><tr><td><code>ollama run llama3.2-vision</code></td></tr><tr><td><code>ollama run llama3.2-vision:90b</code></td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td><code>ollama run llama2-uncensored</code></td></tr><tr></tr><tr></tr></tbody></table><blockquote><p>[!NOTE] You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.</p></blockquote><p>Ollama supports importing GGUF models in the Modelfile:</p><ol><li><p>Create a file named , with a  instruction with the local filepath to the model you want to import.</p><pre><code>FROM ./vicuna-33b.Q4_0.gguf\n</code></pre></li><li><p>Create the model in Ollama</p><pre><code>ollama create example -f Modelfile\n</code></pre></li></ol><p>See the <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/import.md\">guide</a> on importing models for more information.</p><p>Models from the Ollama library can be customized with a prompt. For example, to customize the  model:</p><pre><code>FROM llama3.2\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n\n# set the system message\nSYSTEM \"\"\"\nYou are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n\"\"\"\n</code></pre><p>Next, create and run the model:</p><pre><code>ollama create mario -f ./Modelfile\nollama run mario\n&gt;&gt;&gt; hi\nHello! It's your friend Mario.\n</code></pre><p>For more information on working with a Modelfile, see the <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/modelfile.md\">Modelfile</a> documentation.</p><p> is used to create a model from a Modelfile.</p><pre><code>ollama create mymodel -f ./Modelfile\n</code></pre><blockquote><p>This command can also be used to update a local model. Only the diff will be pulled.</p></blockquote><pre><code>ollama cp llama3.2 my-model\n</code></pre><p>For multiline input, you can wrap text with :</p><pre><code>&gt;&gt;&gt; \"\"\"Hello,\n... world!\n... \"\"\"\nI'm a basic program that prints the famous \"Hello, world!\" message to the console.\n</code></pre><pre><code>ollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n</code></pre><blockquote><p>: The image features a yellow smiley face, which is likely the central focus of the picture.</p></blockquote><h3>Pass the prompt as an argument</h3><pre><code>ollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n</code></pre><blockquote><p>: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.</p></blockquote><h3>List models on your computer</h3><h3>List which models are currently loaded</h3><h3>Stop a model which is currently running</h3><p> is used when you want to start ollama without running the desktop application.</p><p>Finally, in a separate shell, run a model:</p><p>Ollama has a REST API for running and managing models.</p><pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n</code></pre><pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n</code></pre><ul><li><a href=\"https://github.com/aws-samples/swift-chat\">SwiftChat</a> (Cross-platform AI chat app supporting Apple Vision Pro via \"Designed for iPad\")</li></ul><ul><li><a href=\"https://github.com/timescale/pgai\">pgai</a> - PostgreSQL as a vector database (Create and search embeddings from Ollama models using pgvector) \n  </li><li><a href=\"https://github.com/mindsdb/mindsdb/raw/staging/mindsdb/integrations/handlers/ollama_handler/README.md\">MindsDB</a> (Connects Ollama models with nearly 200 data platforms and apps)</li><li><a href=\"https://github.com/dbkangaroo/kangaroo\">Kangaroo</a> (AI-powered SQL client and admin tool for popular databases)</li></ul><ul><li><a href=\"https://github.com/aws-samples/swift-chat\">SwiftChat</a> (Lightning-fast Cross-platform AI chat app with native UI for Android, iOS, and iPad)</li><li><a href=\"https://github.com/JHubi1/ollama-app\">Ollama App</a> (Modern and easy-to-use multi-platform client for Ollama)</li><li><a href=\"https://github.com/1runeberg/confichat\">ConfiChat</a> (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)</li><li><a href=\"https://github.com/sunshine0523/OllamaServer\">Ollama Android Chat</a> (No need for Termux, start the Ollama service with one click on an Android device)</li><li><a href=\"https://github.com/ibrahimcetin/reins\">Reins</a> (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)</li></ul><ul><li><a href=\"https://www.comet.com/docs/opik/cookbook/ollama\">Opik</a> is an open-source platform to debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards. Opik supports native intergration to Ollama.</li><li><a href=\"https://lunary.ai/docs/integrations/ollama\">Lunary</a> is the leading open-source LLM observability platform. It provides a variety of enterprise-grade features such as real-time analytics, prompt templates management, PII masking, and comprehensive agent tracing.</li><li><a href=\"https://github.com/openlit/openlit\">OpenLIT</a> is an OpenTelemetry-native tool for monitoring Ollama Applications &amp; GPUs using traces and metrics.</li><li><a href=\"https://docs.honeyhive.ai/integrations/ollama\">HoneyHive</a> is an AI observability and evaluation platform for AI agents. Use HoneyHive to evaluate agent performance, interrogate failures, and monitor quality in production.</li><li><a href=\"https://langfuse.com/docs/integrations/ollama\">Langfuse</a> is an open source LLM observability platform that enables teams to collaboratively monitor, evaluate and debug AI applications.</li><li><a href=\"https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing\">MLflow Tracing</a> is an open source LLM observability tool with a convenient API to log and visualize traces, making it easy to debug and evaluate GenAI applications.</li></ul>","contentLength":4677,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"firebase/genkit","url":"https://github.com/firebase/genkit","date":1750386936,"author":"","guid":163246,"unread":true,"content":"<p>An open source framework for building AI-powered apps with familiar code-centric patterns. Genkit makes it easy to develop, integrate, and test AI features with observability and evaluations. Genkit works with various models and platforms.</p><p>Genkit is an open-source framework for building full-stack AI-powered applications, built and used in production by Google's Firebase. It provides SDKs for multiple programming languages with varying levels of stability:</p><ul><li><strong>JavaScript/TypeScript (Stable)</strong>: Production-ready with full feature support</li><li>: Feature-complete but may have breaking changes</li><li>: Early development with core functionality</li></ul><p>It offers a unified interface for integrating AI models from providers like <a href=\"https://genkit.dev/docs/plugins/google-genai\">Google</a>, <a href=\"https://thefireco.github.io/genkit-plugins/docs/plugins/genkitx-openai\">OpenAI</a>, <a href=\"https://thefireco.github.io/genkit-plugins/docs/plugins/genkitx-anthropic\">Anthropic</a>, <a href=\"https://genkit.dev/docs/plugins/ollama/\">Ollama</a>, and more. Rapidly build and deploy production-ready chatbots, automations, and recommendation systems using streamlined APIs for multimodal content, structured outputs, tool calling, and agentic workflows.</p><p>Get started with just a few lines of code:</p><pre><code>import { genkit } from 'genkit';\nimport { googleAI } from '@genkit-ai/googleai';\n\nconst ai = genkit({ plugins: [googleAI()] });\n\nconst { text } = await ai.generate({\n    model: googleAI.model('gemini-2.0-flash'),\n    prompt: 'Why is Firebase awesome?'\n});\n</code></pre><h2>Explore &amp; build with Genkit</h2><p>Play with AI sample apps, with visualizations of the Genkit code that powers them, at no cost to you.</p><table><tbody><tr><td>Use a unified interface to integrate with hundreds of models from providers like <a href=\"https://genkit.dev/docs/plugins/google-genai\">Google</a>, <a href=\"https://thefireco.github.io/genkit-plugins/docs/plugins/genkitx-openai\"> OpenAI</a>, <a href=\"https://thefireco.github.io/genkit-plugins/docs/plugins/genkitx-anthropic\"> Anthropic</a>, <a href=\"https://genkit.dev/docs/plugins/ollama\">Ollama</a>, and more. Explore, compare, and use the best models for your needs.</td></tr><tr><td>Integrate seamlessly with frameworks and platforms including Next.js, React, Angular, iOS, Android, using purpose-built <a href=\"https://genkit.dev/docs/firebase\">client SDKs</a> and helpers.</td></tr><tr><td>Build with the language that best fits your project. Genkit provides SDKs for JavaScript/TypeScript (Stable), Go (Beta), and Python (Alpha) with consistent APIs and capabilities across all supported languages.</td></tr><tr><td>Accelerate AI development with a purpose-built, local <a href=\"https://genkit.dev/docs/devtools\">CLI and Developer UI</a>. Test prompts and flows against individual inputs or datasets, compare outputs from different models, debug with detailed execution traces, and use immediate visual feedback to iterate rapidly on prompts.</td></tr><tr><td>Ship AI features with confidence using comprehensive production monitoring. Track model performance, and request volumes, latency, and error rates in a <a href=\"https://genkit.dev/docs/observability/getting-started\"> purpose-built dashboard</a>. Identify issues quickly with detailed observability metrics, and ensure your AI features meet quality and performance targets in real-world usage.</td></tr></tbody></table><p>Genkit simplifies AI integration with an open-source SDK and unified APIs that work across various model providers and programming languages. It abstracts away complexity so you can focus on delivering great user experiences.</p><p>Some key features offered by Genkit include:</p><p>Genkit is designed for server-side deployment in multiple language environments, and also provides seamless client-side integration through dedicated helpers and <a href=\"https://genkit.dev/docs/firebase\">client SDKs</a>.</p><table><tbody><tr><td>Choose your language and model provider</td><td>Select the Genkit SDK for your preferred language (JavaScript/TypeScript (Stable), Go (Beta), or Python (Alpha)). Choose a model provider like <a href=\"https://genkit.dev/docs/plugins/google-genai\">Google Gemini</a> or Anthropic, and get an API key. Some providers, like <a href=\"https://genkit.dev/docs/plugins/vertex-ai\">Vertex AI</a>, may rely on a different means of authentication.</td></tr><tr><td>Install the SDK and initialize</td><td>Install the Genkit SDK, model-provider package of your choice, and the Genkit CLI. Import the Genkit and provider packages and initialize Genkit with the provider API key.</td></tr><tr><td>Write and test AI features</td><td>Use the Genkit SDK to build AI features for your use case, from basic text generation to complex multi-step workflows and agents. Use the CLI and Developer UI to help you rapidly test and iterate.</td></tr><tr><td>Deploy your AI features to Firebase, Google Cloud Run, or any environment that supports your chosen programming language. Integrate them into your app, and monitor them in production in the Firebase console.</td></tr></tbody></table><p>Genkit provides a CLI and a local UI to streamline your AI development workflow.</p><p>The Genkit CLI includes commands for running and evaluating your Genkit functions (flows) and collecting telemetry and logs.</p><ul><li><code>npm install -g genkit-cli</code></li><li><strong>Run a command, wrapped with telemetry, a interactive developer UI, etc:</strong><code>genkit start -- &lt;command to run your code&gt;</code></li></ul><p>The Genkit developer UI is a local interface for testing, debugging, and iterating on your AI application.</p><ul><li> Execute and experiment with Genkit flows, prompts, queries, and more in dedicated playgrounds.</li><li> Analyze detailed traces of past executions, including step-by-step breakdowns of complex flows.</li><li> Review the results of evaluations run against your flows, including performance metrics and links to relevant traces.</li></ul><img src=\"https://raw.githubusercontent.com/firebase/genkit/main/docs/resources/readme-ui-traces-screenshot.png\" width=\"700\" alt=\"Screenshot of Genkit Developer UI showing traces\"><h2>Try Genkit in Firebase Studio</h2><p>Want to skip the local setup? Click below to try out Genkit using <a href=\"https://firebase.studio\">Firebase Studio</a>, Google's AI-assisted workspace for full-stack app development in the cloud.</p><a href=\"https://studio.firebase.google.com/new/genkit\"><img height=\"32\" alt=\"Open in Firebase Studio\" src=\"https://cdn.firebasestudio.dev/btn/open_bright_32.svg?sanitize=true\"></a><p>Contributions to Genkit are welcome and highly appreciated! See our <a href=\"https://raw.githubusercontent.com/firebase/genkit/main/CONTRIBUTING.md\">Contribution Guide</a> to get started.</p>","contentLength":4936,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kortix-ai/suna","url":"https://github.com/kortix-ai/suna","date":1750386936,"author":"","guid":163247,"unread":true,"content":"<li><p> (<a href=\"https://www.suna.so/share/5ee791ac-e19c-4986-a61c-6d0659d0e5bc\">Watch</a>) - <em>\"Analyze the market for my next company in the healthcare industry, located in the UK. Give me the major players, their market size, strengths, and weaknesses, and add their website URLs. Once done, generate a PDF report.\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/804d20a3-cf1c-4adb-83bb-0e77cc6adeac\">Watch</a>) - <em>\"Give me the list of the most important VC Funds in the United States based on Assets Under Management. Give me website URLs, and if possible an email to reach them out.\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/3ae581b0-2db8-4c63-b324-3b8d29762e74\">Watch</a>) - <em>\"Go on LinkedIn, and find me 10 profiles available - they are not working right now - for a junior software engineer position, who are located in Munich, Germany. They should have at least one bachelor's degree in Computer Science or anything related to it, and 1-year of experience in any field/role.\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/725e64a0-f1e2-4bb6-8a1f-703c2833fd72\">Watch</a>) - <em>\"Generate me a route plan for my company. We should go to California. We'll be in 8 people. Compose the trip from the departure (Paris, France) to the activities we can do considering that the trip will be 7 days long - departure on the 21st of Apr 2025. Check the weather forecast and temperature for the upcoming days, and based on that, you can plan our activities (outdoor vs indoor).\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/128f23a4-51cd-42a6-97a0-0b458b32010e\">Watch</a>) - <em>\"My company asked me to set up an Excel spreadsheet with all the information about Italian lottery games (Lotto, 10eLotto, and Million Day). Based on that, generate and send me a spreadsheet with all the basic information (public ones).\"</em></p></li><li><p><strong>Automate Event Speaker Prospecting</strong> (<a href=\"https://www.suna.so/share/7a7592ea-ed44-4c69-bcb5-5f9bb88c188c\">Watch</a>) - <em>\"Find 20 AI ethics speakers from Europe who've spoken at conferences in the past year. Scrapes conference sites, cross-references LinkedIn and YouTube, and outputs contact info + talk summaries.\"</em></p></li><li><p><strong>Summarize and Cross-Reference Scientific Papers</strong> (<a href=\"https://www.suna.so/share/c2081b3c-786e-4e7c-9bf4-46e9b23bb662\">Watch</a>) - <em>\"Research and compare scientific papers talking about Alcohol effects on our bodies during the last 5 years. Generate a report about the most important scientific papers talking about the topic I wrote before.\"</em></p></li><li><p><strong>Research + First Contact Draft</strong> (<a href=\"https://www.suna.so/share/6b6296a6-8683-49e5-9ad0-a32952d12c44\">Watch</a>) - <em>\"Research my potential customers (B2B) on LinkedIn. They should be in the clean tech industry. Find their websites and their email addresses. After that, based on the company profile, generate a personalized first contact email where I present my company which is offering consulting services to cleantech companies to maximize their profits and reduce their costs.\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/43491cb0-cd6c-45f0-880c-66ddc8c4b842\">Watch</a>) - <em>\"Based on my website suna.so, generate an SEO report analysis, find top-ranking pages by keyword clusters, and identify topics I'm missing.\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/37b31907-8349-4f63-b0e5-27ca597ed02a\">Watch</a>) - <em>\"Generate a personal trip to London, with departure from Bangkok on the 1st of May. The trip will last 10 days. Find an accommodation in the center of London, with a rating on Google reviews of at least 4.5. Find me interesting outdoor activities to do during the journey. Generate a detailed itinerary plan.\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/8b2a897e-985a-4d5e-867b-15239274f764\">Watch</a>) - <em>\"Go on Crunchbase, Dealroom, and TechCrunch, filter by Series A funding rounds in the SaaS Finance Space, and build a report with company data, founders, and contact info for outbound sales.\"</em></p></li><li><p> (<a href=\"https://www.suna.so/share/7d7a5d93-a20d-48b0-82cc-e9a876e9fd04\">Watch</a>) - <em>\"I need to find the best beauty centers in Rome, but I want to find them by using open forums that speak about this topic. Go on Google, and scrape the forums by looking for beauty center discussions located in Rome. Then generate a list of 5 beauty centers with the best comments about them.\"</em></p></li>","contentLength":3267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"cloudflare/ai","url":"https://github.com/cloudflare/ai","date":1750386936,"author":"","guid":163248,"unread":true,"content":"<p>This repository contains various packages and demo apps related consuming Cloudflare's AI offerings on the client-side. It is a monorepo powered by <a href=\"https://nx.dev/\">Nx</a> and <a href=\"https://github.com/changesets/changesets\">Changesets</a>.</p><ol><li><pre><code>git clone git@github.com:cloudflare/ai.git\n</code></pre></li><li><p>From the root directory, run:</p></li><li><p>To start a development server for a specific app (for instance, ):</p><p><em>Ideally all commands should be executed from the repository root with the  prefix. This will ensure that the dependency graph is managed correctly, e.g. if one package relies on the output of an other.</em></p></li></ol><ul><li><p>To execute your continuous integration tests for a specific project (e.g., ):</p><pre><code>pnpm nx test:ci workers-ai-provider\n</code></pre></li><li><p>To lint a specific project:</p></li><li><p>To run a more comprehensive sweep of tasks (lint, tests, type checks, build) against one or more projects:</p><pre><code>pnpm nx run-many -t lint test:ci type-check build -p \"my-project other-project\"\n</code></pre></li></ul><ul><li>: Compiles a project or a set of projects.</li><li>: Runs project tests in watch mode.</li><li>: Runs tests in CI mode (no watch).</li><li>: Runs smoke tests.</li><li>: Performs TypeScript type checks.</li></ul><p>In order to scaffold a new demo app, you can use the  script. This script will create a new demo app in the  directory.</p><pre><code>pnpm create-demo &lt;demo-name&gt;\n</code></pre><p>After creating the app,  will be run to install the dependencies, and <code>pnpm nx cf-typegen &lt;demo-name&gt;</code> will be run to generate the types for the demo app. Then it's simply a case of starting the app with:</p><p>We appreciate contributions and encourage pull requests. Please follow these guidelines:</p><ol><li>Project Setup: After forking or cloning, install dependencies with .</li><li>Branching: Create a new branch for your feature or fix.</li></ol><ul><li>Add or update relevant tests.</li><li>On pushing your changes, automated tasks will be run (courtesy of a Husky pre-push hook).</li></ul><ol start=\"4\"><li>Changesets: If your changes affect a published package, run  to create a changeset. Provide a concise summary of your changes in the changeset prompt.</li><li>Pull Request: Submit a pull request to the  branch. The team will review it and merge if everything is in order.</li></ol><p>This repository uses <a href=\"https://github.com/changesets/changesets\">Changesets</a> to manage versioning and publication:</p><ol><li><p>: Whenever a change is made that warrants a new release (e.g., bug fixes, new features), run:</p><p>Provide a clear description of the changes.</p></li><li><p>: Once the changeset is merged into , our GitHub Actions workflows will:</p></li></ol><ul><li>Detect the changed packages, and create a Version Packages PR.</li><li>Increment versions automatically (via Changesets).</li><li>Publish any package that has a version number to npm. (Demos and other internal items do not require versioning.)</li></ul><ol start=\"3\"><li>: The release workflow (<code>.github/workflows/release.yml</code>) will run on every push to . It ensures each published package is tagged and released on npm. Any package with a version field in its  will be included in this process.</li></ol><p>For any queries or guidance, kindly open an issue or submit a pull request. We hope this structure and process help you to contribute effectively.</p>","contentLength":2807,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"dail8859/NotepadNext","url":"https://github.com/dail8859/NotepadNext","date":1750386936,"author":"","guid":163249,"unread":true,"content":"<p>A cross-platform, reimplementation of Notepad++</p><p>A cross-platform, reimplementation of Notepad++.</p><p>Though the application overall is stable and usable, it should not be considered safe for critically important work.</p><p>There are numerous bugs and half working implementations. Pull requests are greatly appreciated.</p><p>Packages are available for Windows, Linux, and MacOS.</p><p>Below are the supported distribution mechanisms. There may be other ways to download/install the application, but this project will likely not be able to offer any support for those since they are made availble by other individuals.</p><p>Windows packages are available as an installer or a stand-alone zip file on the <a href=\"https://github.com/dail8859/NotepadNext/releases\">release</a> page. The installer provides additional components such as an auto-updater and Windows context menu integration. You can easily install it with Winget:</p><pre><code>winget install dail8859.NotepadNext\n</code></pre><p>Linux packages can be obtained by downloading the stand-alone AppImage on the <a href=\"https://github.com/dail8859/NotepadNext/releases\">release</a> page or by installing the <a href=\"https://flathub.org/apps/details/com.github.dail8859.NotepadNext\">flatpak</a> by executing:</p><pre><code>flatpak install flathub com.github.dail8859.NotepadNext\n</code></pre><p>MacOS disk images can be downloaded from the <a href=\"https://github.com/dail8859/NotepadNext/releases\">release</a> page.</p><p>It can also be installed using brew:</p><pre><code>brew tap dail8859/notepadnext\nbrew install --no-quarantine notepadnext\n</code></pre><p>By default, MacOS enables font smoothing which causes text to appear quite differently from the Windows version. This can be disabled system-wide using the following command:</p><pre><code>defaults -currentHost write -g AppleFontSmoothing -int 0\n</code></pre><p>A restart is required for this to take effect.</p><p>Current development is done using QtCreator with the Microsft Visual C++ (msvc) compiler. Qt 6.5 is the currently supported Qt version. Older versions of Qt are likely to work but are not tested. Any fixes for older versions will be accepted as long as they do not introduce complex fixes. This application is also known to build successfully on various Linux distributions and macOS. Other platforms/compilers should be usable with minor modifications.</p><p>If you are familiar with building C++ Qt desktop applications with Qt Creator, then this should be as simple as opening  and build/run the project.</p><p>If you are new to building C++ Qt desktop applications, there is a more detailed guide <a href=\"https://raw.githubusercontent.com/dail8859/NotepadNext/master/doc/Building.md\">here</a>.</p>","contentLength":2190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"php/frankenphp","url":"https://github.com/php/frankenphp","date":1750300590,"author":"","guid":161723,"unread":true,"content":"<p>üßü The modern PHP app server</p><p>FrankenPHP is a modern application server for PHP built on top of the <a href=\"https://caddyserver.com/\">Caddy</a> web server.</p><p>FrankenPHP works with any PHP app and makes your Laravel and Symfony projects faster than ever thanks to their official integrations with the worker mode.</p><p>FrankenPHP can also be used as a standalone Go library to embed PHP in any app using .</p><p>We provide static FrankenPHP binaries for Linux and macOS containing <a href=\"https://www.php.net/releases/8.4/en.php\">PHP 8.4</a> and most popular PHP extensions.</p><p>On Windows, use <a href=\"https://learn.microsoft.com/windows/wsl/\">WSL</a> to run FrankenPHP.</p><p><a href=\"https://github.com/dunglas/frankenphp/releases\">Download FrankenPHP</a> or copy this line into your terminal to automatically install the version appropriate for your platform:</p><pre><code>curl https://frankenphp.dev/install.sh | sh\nmv frankenphp /usr/local/bin/\n</code></pre><p>To serve the content of the current directory, run:</p><p>You can also run command-line scripts with:</p><pre><code>frankenphp php-cli /path/to/your/script.php\n</code></pre><pre><code>docker run -v .:/app/public \\\n    -p 80:80 -p 443:443 -p 443:443/udp \\\n    dunglas/frankenphp\n</code></pre><p>Go to , and enjoy!</p><blockquote><p>Do not attempt to use . Use  and accept the self-signed certificate. Use the <a href=\"https://raw.githubusercontent.com/php/frankenphp/main/docs/config.md#environment-variables\"> environment variable</a> to change the domain to use.</p></blockquote><p>FrankenPHP is also available as a <a href=\"https://brew.sh\">Homebrew</a> package for macOS and Linux.</p><pre><code>brew install dunglas/frankenphp/frankenphp\n</code></pre><p>To serve the content of the current directory, run:</p>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"moby/buildkit","url":"https://github.com/moby/buildkit","date":1750300590,"author":"","guid":161724,"unread":true,"content":"<p>concurrent, cache-efficient, and Dockerfile-agnostic builder toolkit</p><p>BuildKit is a toolkit for converting source code to build artifacts in an efficient, expressive and repeatable manner.</p><ul><li>Automatic garbage collection</li><li>Extendable frontend formats</li><li>Concurrent dependency resolution</li><li>Efficient instruction caching</li><li>Build cache import/export</li><li>Nested build job invocations</li><li>Execution without root privileges</li></ul><blockquote><p>[!NOTE] If you are visiting this repo for the usage of BuildKit-only Dockerfile features like <code>RUN --mount=type=(bind|cache|tmpfs|secret|ssh)</code>, please refer to the <a href=\"https://docs.docker.com/engine/reference/builder/\">Dockerfile reference</a>.</p></blockquote><blockquote><p>[!NOTE] <a href=\"https://docs.docker.com/build/architecture/\">uses Buildx and BuildKit by default</a> since Docker Engine 23.0. You don't need to read this document unless you want to use the full-featured standalone version of BuildKit.</p></blockquote><p>BuildKit is used by the following projects:</p><p>BuildKit is composed of the  daemon and the  client. While the  client is available for Linux, macOS, and Windows, the  daemon is only available for Linux and *Windows currently.</p><p>The latest binaries of BuildKit are available <a href=\"https://github.com/moby/buildkit/releases\">here</a> for Linux, macOS, and Windows.</p><p>The  daemon requires the following components to be installed:</p><p><strong>Starting the  daemon:</strong> You need to run  as the root user on the host.</p><p>The buildkitd daemon supports two worker backends: OCI (runc) and containerd.</p><p>By default, the OCI (runc) worker is used. You can set <code>--oci-worker=false --containerd-worker=true</code> to use the containerd worker.</p><p>We are open to adding more backends.</p><p>To start the buildkitd daemon using systemd socket activation, you can install the buildkit systemd unit files. See <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/#systemd-socket-activation\">Systemd socket activation</a></p><p>The Homebrew formula does not contain the daemon ().</p><p>For example, <a href=\"https://lima-vm.io\">Lima</a> can be used for launching the daemon inside a Linux VM.</p><pre><code>brew install lima\nlimactl start template://buildkit\nexport BUILDKIT_HOST=\"unix://$HOME/.lima/buildkit/sock/buildkitd.sock\"\n</code></pre><p>BuildKit builds are based on a binary intermediate format called LLB that is used for defining the dependency graph for processes running part of your build. tl;dr: LLB is to Dockerfile what LLVM IR is to C.</p><ul><li>Marshaled as Protobuf messages</li><li>Vendor-neutral (i.e. non-Dockerfile languages can be easily implemented)</li></ul><p>Currently, the following high-level languages have been implemented for LLB:</p><p>Frontends are components that run inside BuildKit and convert any build definition to LLB. There is a special frontend called gateway () that allows using any image as a frontend.</p><p>During development, Dockerfile frontend () is also part of the BuildKit repo. In the future, this will be moved out, and Dockerfiles can be built using an external image.</p><h4>Building a Dockerfile with </h4><pre><code>buildctl build \\\n    --frontend=dockerfile.v0 \\\n    --local context=. \\\n    --local dockerfile=.\n# or\nbuildctl build \\\n    --frontend=dockerfile.v0 \\\n    --local context=. \\\n    --local dockerfile=. \\\n    --opt target=foo \\\n    --opt build-arg:foo=bar\n</code></pre><p> exposes local source files from client to the builder.  and  are the names Dockerfile frontend looks for build context and Dockerfile location.</p><p>If the Dockerfile has a different filename it can be specified with <code>--opt filename=./Dockerfile-alternative</code>.</p><h4>Building a Dockerfile using external frontend</h4><p>External versions of the Dockerfile frontend are pushed to <a href=\"https://hub.docker.com/r/docker/dockerfile-upstream\">https://hub.docker.com/r/docker/dockerfile-upstream</a> and <a href=\"https://hub.docker.com/r/docker/dockerfile\">https://hub.docker.com/r/docker/dockerfile</a> and can be used with the gateway frontend. The source for the external frontend is currently located in <code>./frontend/dockerfile/cmd/dockerfile-frontend</code> but will move out of this repository in the future (<a href=\"https://github.com/moby/buildkit/issues/163\">#163</a>). For automatic build from master branch of this repository <code>docker/dockerfile-upstream:master</code> or <code>docker/dockerfile-upstream:master-labs</code> image can be used.</p><pre><code>buildctl build \\\n    --frontend gateway.v0 \\\n    --opt source=docker/dockerfile \\\n    --local context=. \\\n    --local dockerfile=.\nbuildctl build \\\n    --frontend gateway.v0 \\\n    --opt source=docker/dockerfile \\\n    --opt context=https://github.com/moby/moby.git \\\n    --opt build-arg:APT_MIRROR=cdn-fastly.deb.debian.org\n</code></pre><p>By default, the build result and intermediate cache will only remain internally in BuildKit. An output needs to be specified to retrieve the result.</p><pre><code>buildctl build ... --output type=image,name=docker.io/username/image,push=true\n</code></pre><p>To export the image to multiple registries:</p><pre><code>buildctl build ... --output type=image,\\\"name=docker.io/username/image,docker.io/username2/image2\\\",push=true\n</code></pre><p>To export the cache embed with the image and pushing them to registry together, type  is required to import the cache, you should specify <code>--export-cache type=inline</code> and <code>--import-cache type=registry,ref=...</code>. To export the cache to a local directly, you should specify <code>--export-cache type=local</code>. Details in <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/#export-cache\">Export cache</a>.</p><pre><code>buildctl build ...\\\n  --output type=image,name=docker.io/username/image,push=true \\\n  --export-cache type=inline \\\n  --import-cache type=registry,ref=docker.io/username/image\n</code></pre><p>Keys supported by image output:</p><ul><li>: specify image name(s)</li><li>: push after creating the image</li><li>: push unnamed image</li><li>: push to insecure HTTP registry</li><li>: use OCI mediatypes in configuration JSON instead of Docker's</li><li>: use OCI artifact format for attestations</li><li>: unpack image after creation (for use with containerd)</li><li><code>dangling-name-prefix=&lt;value&gt;</code>: name image with , used for anonymous images</li><li>: add additional canonical name </li><li><code>compression=&lt;uncompressed|gzip|estargz|zstd&gt;</code>: choose compression type for layers newly created and cached, gzip is default value. estargz should be used with .</li><li><code>compression-level=&lt;value&gt;</code>: compression level for gzip, estargz (0-9) and zstd (0-22)</li><li>: rewrite the file timestamps to the  value. See <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/docs/build-repro.md\"></a> for how to specify the  value.</li><li>: forcefully apply  option to all layers (including already existing layers)</li><li>: store the result images to the worker's (e.g. containerd) image store as well as ensures that the image has all blobs in the content store (default ). Ignored if the worker doesn't have image store (e.g. OCI worker).</li><li>: attach an annotation with the respective  and  to the built image \n  <ul><li>Using the extended syntaxes, <code>annotation-&lt;type&gt;.&lt;key&gt;=&lt;value&gt;</code>, <code>annotation[&lt;platform&gt;].&lt;key&gt;=&lt;value&gt;</code> and both combined with <code>annotation-&lt;type&gt;[&lt;platform&gt;].&lt;key&gt;=&lt;value&gt;</code>, allows configuring exactly where to attach the annotation.</li><li> specifies what object to attach to, and can be any of  (the default), ,  and </li><li> specifies which objects to attach to (by default, all), and is the same key passed into the  opt, see <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/docs/multi-platform.md\"></a>.</li></ul></li></ul><p>If credentials are required,  will attempt to read Docker configuration file <code>$DOCKER_CONFIG/config.json</code>.  defaults to .</p><p>The local client will copy the files directly to the client. This is useful if BuildKit is being used for building something else than container images.</p><pre><code>buildctl build ... --output type=local,dest=path/to/output-dir\n</code></pre><p>To export specific files use multi-stage builds with a scratch stage and copy the needed files into that stage with .</p><pre><code>...\nFROM scratch as testresult\n\nCOPY --from=builder /usr/src/app/testresult.xml .\n...\n</code></pre><pre><code>buildctl build ... --opt target=testresult --output type=local,dest=path/to/output-dir\n</code></pre><p>With a <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/docs/multi-platform.md\">multi-platform build</a>, a subfolder matching each target platform will be created in the destination directory:</p><pre><code>FROM busybox AS build\nARG TARGETOS\nARG TARGETARCH\nRUN mkdir /out &amp;&amp; echo foo &gt; /out/hello-$TARGETOS-$TARGETARCH\n\nFROM scratch\nCOPY --from=build /out /\n</code></pre><pre><code>$ buildctl build \\\n  --frontend dockerfile.v0 \\\n  --opt platform=linux/amd64,linux/arm64 \\\n  --output type=local,dest=./bin/release\n\n$ tree ./bin\n./bin/\n‚îî‚îÄ‚îÄ release\n    ‚îú‚îÄ‚îÄ linux_amd64\n    ‚îÇ   ‚îî‚îÄ‚îÄ hello-linux-amd64\n    ‚îî‚îÄ‚îÄ linux_arm64\n        ‚îî‚îÄ‚îÄ hello-linux-arm64\n</code></pre><p>You can set  to merge files from all platforms together into same directory:</p><pre><code>$ buildctl build \\\n  --frontend dockerfile.v0 \\\n  --opt platform=linux/amd64,linux/arm64 \\\n  --output type=local,dest=./bin/release,platform-split=false\n\n$ tree ./bin\n./bin/\n‚îî‚îÄ‚îÄ release\n    ‚îú‚îÄ‚îÄ hello-linux-amd64\n    ‚îî‚îÄ‚îÄ hello-linux-arm64\n</code></pre><p>Tar exporter is similar to local exporter but transfers the files through a tarball.</p><pre><code>buildctl build ... --output type=tar,dest=out.tar\nbuildctl build ... --output type=tar &gt; out.tar\n</code></pre><pre><code># exported tarball is also compatible with OCI spec\nbuildctl build ... --output type=docker,name=myimage | docker load\n</code></pre><pre><code>buildctl build ... --output type=oci,dest=path/to/output.tar\nbuildctl build ... --output type=oci &gt; output.tar\n</code></pre><p>The containerd worker needs to be used</p><pre><code>buildctl build ... --output type=image,name=docker.io/username/image\nctr --namespace=buildkit images ls\n</code></pre><p>To show local build cache ():</p><p>To prune local build cache:</p><p>BuildKit supports the following cache exporters:</p><ul><li>: embed the cache into the image, and push them to the registry together</li><li>: push the image and the cache separately</li><li>: export to a local directory</li><li>: export to GitHub Actions cache</li></ul><p>In most case you want to use the  cache exporter. However, note that the  cache exporter only supports  cache mode. To enable  cache mode, push the image and the cache separately by using  cache exporter.</p><p> and  exporters both store the cache in the registry. For importing the cache,  is sufficient for both, as specifying the cache format is not necessary.</p><h4>Inline (push image and cache together)</h4><pre><code>buildctl build ... \\\n  --output type=image,name=docker.io/username/image,push=true \\\n  --export-cache type=inline \\\n  --import-cache type=registry,ref=docker.io/username/image\n</code></pre><p>Inline cache embeds cache metadata into the image config. The layers in the image will be left untouched compared to the image with no cache information.</p><p> Docker-integrated BuildKit (<code>DOCKER_BUILDKIT=1 docker build</code>) and requires <code>--build-arg BUILDKIT_INLINE_CACHE=1</code> to be specified to enable the  cache exporter. However, the standalone  does NOT require <code>--opt build-arg:BUILDKIT_INLINE_CACHE=1</code> and the build-arg is simply ignored.</p><h4>Registry (push image and cache separately)</h4><pre><code>buildctl build ... \\\n  --output type=image,name=localhost:5000/myrepo:image,push=true \\\n  --export-cache type=registry,ref=localhost:5000/myrepo:buildcache \\\n  --import-cache type=registry,ref=localhost:5000/myrepo:buildcache\n</code></pre><ul><li>: specify cache layers to export (default: ) \n  <ul><li>: only export layers for the resulting image</li><li>: export all the layers of all intermediate steps</li></ul></li><li>: specify repository reference to store cache, e.g. </li><li><code>image-manifest=&lt;true|false&gt;</code>: whether to export cache manifest as an OCI-compatible image manifest rather than a manifest list/index (default:  since BuildKit , must be used with )</li><li><code>oci-mediatypes=&lt;true|false&gt;</code>: whether to use OCI mediatypes in exported manifests (default: , since BuildKit )</li><li><code>compression=&lt;uncompressed|gzip|estargz|zstd&gt;</code>: choose compression type for layers newly created and cached, gzip is default value. estargz and zstd should be used with </li><li><code>compression-level=&lt;value&gt;</code>: choose compression level for gzip, estargz (0-9) and zstd (0-22)</li><li>: forcibly apply  option to all layers</li><li><code>ignore-error=&lt;false|true&gt;</code>: specify if error is ignored in case cache export fails (default: )</li></ul><ul><li>: specify repository reference to retrieve cache from, e.g. </li></ul><pre><code>buildctl build ... --export-cache type=local,dest=path/to/output-dir\nbuildctl build ... --import-cache type=local,src=path/to/input-dir\n</code></pre><p>The directory layout conforms to OCI Image Spec v1.0.</p><ul><li>: specify cache layers to export (default: ) \n  <ul><li>: only export layers for the resulting image</li><li>: export all the layers of all intermediate steps</li></ul></li><li>: destination directory for cache exporter</li><li>: specify custom tag of image to write to local index (default: )</li><li><code>image-manifest=&lt;true|false&gt;</code>: whether to export cache manifest as an OCI-compatible image manifest rather than a manifest list/index (default:  since BuildKit , must be used with )</li><li><code>oci-mediatypes=&lt;true|false&gt;</code>: whether to use OCI mediatypes in exported manifests (default , since BuildKit )</li><li><code>compression=&lt;uncompressed|gzip|estargz|zstd&gt;</code>: choose compression type for layers newly created and cached, gzip is default value. estargz and zstd should be used with .</li><li><code>compression-level=&lt;value&gt;</code>: compression level for gzip, estargz (0-9) and zstd (0-22)</li><li>: forcibly apply  option to all layers</li><li><code>ignore-error=&lt;false|true&gt;</code>: specify if error is ignored in case cache export fails (default: )</li></ul><ul><li>: source directory for cache importer</li><li>: specify custom tag of image to read from local index (default: )</li><li><code>digest=sha256:&lt;sha256digest&gt;</code>: specify explicit digest of the manifest list to import</li></ul><h4>GitHub Actions cache (experimental)</h4><pre><code>buildctl build ... \\\n  --output type=image,name=docker.io/username/image,push=true \\\n  --export-cache type=gha \\\n  --import-cache type=gha\n</code></pre><p>GitHub Actions cache saves both cache metadata and layers to GitHub's Cache service. This cache currently has a <a href=\"https://docs.github.com/en/actions/advanced-guides/caching-dependencies-to-speed-up-workflows#usage-limits-and-eviction-policy\">size limit of 10GB</a> that is shared across different caches in the repo. If you exceed this limit, GitHub will save your cache but will begin evicting caches until the total size is less than 10 GB. Recycling caches too often can result in slower runtimes overall.</p><ul><li>: Cache server URL (default  or fallback to )</li><li>: Cache v2 server URL if <code>$ACTIONS_CACHE_SERVICE_V2</code> set on the runner (default )</li><li>: Access token (default )</li></ul><ul><li>: specify cache layers to export (default: ) \n  <ul><li>: only export layers for the resulting image</li><li>: export all the layers of all intermediate steps</li></ul></li><li>: which scope cache object belongs to (default )</li><li><code>ignore-error=&lt;false|true&gt;</code>: specify if error is ignored in case cache export fails (default: )</li><li>: sets the timeout duration for cache export (default: )</li></ul><ul><li>: which scope cache object belongs to (default )</li><li>: sets the timeout duration for cache import (default: )</li></ul><pre><code>buildctl build ... \\\n  --output type=image,name=docker.io/username/image,push=true \\\n  --export-cache type=s3,region=eu-west-1,bucket=my_bucket,name=my_image \\\n  --import-cache type=s3,region=eu-west-1,bucket=my_bucket,name=my_image\n</code></pre><p>The following attributes are required:</p><ul><li>: AWS S3 bucket (default: )</li><li>: AWS region (default: )</li></ul><ul><li>blobs: <code>s3://&lt;bucket&gt;/&lt;prefix&gt;&lt;blobs_prefix&gt;/&lt;sha256&gt;</code>, default: <code>s3://&lt;bucket&gt;/blobs/&lt;sha256&gt;</code></li><li>manifests: <code>s3://&lt;bucket&gt;/&lt;prefix&gt;&lt;manifests_prefix&gt;/&lt;name&gt;</code>, default: <code>s3://&lt;bucket&gt;/manifests/&lt;name&gt;</code></li></ul><ul><li>: global prefix to store / read blobs on s3 (default: )</li><li>: global prefix to store / read manifests on s3 (default: )</li><li>: specify a specific S3 endpoint (default: empty)</li><li>: if set to , put the bucket name in the URL instead of in the hostname (default: )</li></ul><p>Beware, these configurations must be available at buildkit daemon level, not at client level.</p><ul><li>The following attributes can be used to forward static credentials from a buildkit client to the daemon (buildx for example). \n  <ul><li>: Access Key ID</li><li>: Secret Access Key</li><li>: Session Token</li></ul></li></ul><ul><li>: specify cache layers to export (default: ) \n  <ul><li>: only export layers for the resulting image</li><li>: export all the layers of all intermediate steps</li></ul></li><li>: set global prefix to store / read files on s3 (default: empty)</li><li>: specify name of the manifest to use (default ) \n  <ul><li>Multiple manifest names can be specified at the same time, separated by . The standard use case is to use the git sha1 as name, and the branch name as duplicate, and load both with 2  commands.</li></ul></li><li><code>ignore-error=&lt;false|true&gt;</code>: specify if error is ignored in case cache export fails (default: )</li><li>: Instead of being uploaded again when not changed, blobs files will be \"touched\" on s3 every , default is 24h. Due to this, an expiration policy can be set on the S3 bucket to cleanup useless files automatically. Manifests files are systematically rewritten, there is no need to touch them.</li><li>: This parameter changes the number of layers uploaded to s3 in parallel. Each individual layer is uploaded with 5 threads, using the Upload manager provided by the AWS SDK.</li></ul><ul><li>: set global prefix to store / read files on s3 (default: empty)</li><li>: set global prefix to store / read blobs on s3 (default: )</li><li><code>manifests_prefix=&lt;prefix&gt;</code>: set global prefix to store / read manifests on s3 (default: )</li><li>: name of the manifest to use (default )</li></ul><h4>Azure Blob Storage cache (experimental)</h4><pre><code>buildctl build ... \\\n  --output type=image,name=docker.io/username/image,push=true \\\n  --export-cache type=azblob,account_url=https://myaccount.blob.core.windows.net,name=my_image \\\n  --import-cache type=azblob,account_url=https://myaccount.blob.core.windows.net,name=my_image\n</code></pre><p>The following attributes are required:</p><ul><li>: The Azure Blob Storage account URL (default: <code>$BUILDKIT_AZURE_STORAGE_ACCOUNT_URL</code>)</li></ul><ul><li>blobs: <code>&lt;account_url&gt;/&lt;container&gt;/&lt;prefix&gt;&lt;blobs_prefix&gt;/&lt;sha256&gt;</code>, default: <code>&lt;account_url&gt;/&lt;container&gt;/blobs/&lt;sha256&gt;</code></li><li>manifests: <code>&lt;account_url&gt;/&lt;container&gt;/&lt;prefix&gt;&lt;manifests_prefix&gt;/&lt;name&gt;</code>, default: <code>&lt;account_url&gt;/&lt;container&gt;/manifests/&lt;name&gt;</code></li></ul><p>Azure Blob Storage configuration:</p><ul><li>: The Azure Blob Storage container name (default:  or <code>$BUILDKIT_AZURE_STORAGE_CONTAINER</code> if set)</li><li>: Global prefix to store / read blobs on the Azure Blob Storage container () (default: )</li><li>: Global prefix to store / read blobs on the Azure Blob Storage container () (default: )</li></ul><p>Azure Blob Storage authentication:</p><p>There are 2 options supported for Azure Blob Storage authentication:</p><ul><li>Any system using environment variables supported by the <a href=\"https://docs.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication\">Azure SDK for Go</a>. The configuration must be available for the buildkit daemon, not for the client.</li><li>Secret Access Key, using the  attribute to specify the primary or secondary account key for your Azure Blob Storage account. <a href=\"https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage\">Azure Blob Storage account keys</a></li></ul><blockquote><p>[!NOTE] Account name can also be specified with  attribute (or <code>$BUILDKIT_AZURE_STORAGE_ACCOUNT_NAME</code>) if it is not part of the account URL host.</p></blockquote><ul><li>: specify cache layers to export (default: ) \n  <ul><li>: only export layers for the resulting image</li><li>: export all the layers of all intermediate steps</li></ul></li><li>: set global prefix to store / read files on the Azure Blob Storage container () (default: empty)</li><li>: specify name of the manifest to use (default: ) \n  <ul><li>Multiple manifest names can be specified at the same time, separated by . The standard use case is to use the git sha1 as name, and the branch name as duplicate, and load both with 2  commands.</li></ul></li><li><code>ignore-error=&lt;false|true&gt;</code>: specify if error is ignored in case cache export fails (default: )</li></ul><ul><li>: set global prefix to store / read files on the Azure Blob Storage container () (default: empty)</li><li>: set global prefix to store / read blobs on the Azure Blob Storage container () (default: )</li><li><code>manifests_prefix=&lt;prefix&gt;</code>: set global prefix to store / read manifests on the Azure Blob Storage container () (default: )</li><li>: name of the manifest to use (default: )</li></ul><p>If you have multiple BuildKit daemon instances, but you don't want to use registry for sharing cache across the cluster, consider client-side load balancing using consistent hashing.</p><p>To output build metadata such as the image digest, pass the  flag. The metadata will be written as a JSON object to the specified file. The directory of the specified file must already exist and be writable.</p><pre><code>buildctl build ... --metadata-file metadata.json\n</code></pre><pre><code>{\n  \"containerimage.config.digest\": \"sha256:2937f66a9722f7f4a2df583de2f8cb97fc9196059a410e7f00072fc918930e66\",\n  \"containerimage.descriptor\": {\n    \"annotations\": {\n      \"config.digest\": \"sha256:2937f66a9722f7f4a2df583de2f8cb97fc9196059a410e7f00072fc918930e66\",\n      \"org.opencontainers.image.created\": \"2022-02-08T21:28:03Z\"\n    },\n    \"digest\": \"sha256:19ffeab6f8bc9293ac2c3fdf94ebe28396254c993aea0b5a542cfb02e0883fa3\",\n    \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n    \"size\": 506\n  },\n  \"containerimage.digest\": \"sha256:19ffeab6f8bc9293ac2c3fdf94ebe28396254c993aea0b5a542cfb02e0883fa3\"\n}\n</code></pre><h2>Systemd socket activation</h2><p>On Systemd based systems, you can communicate with the daemon via <a href=\"http://0pointer.de/blog/projects/socket-activation.html\">Systemd socket activation</a>, use . You can find examples of using Systemd socket activation with BuildKit and Systemd in <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/examples/systemd\"></a>.</p><h2>Expose BuildKit as a TCP service</h2><p>The  daemon can listen the gRPC API on a TCP socket.</p><p>It is highly recommended to create TLS certificates for both the daemon and the client (mTLS). Enabling TCP without mTLS is dangerous because the executor containers (aka Dockerfile  containers) can call BuildKit API as well.</p><pre><code>buildkitd \\\n  --addr tcp://0.0.0.0:1234 \\\n  --tlscacert /path/to/ca.pem \\\n  --tlscert /path/to/cert.pem \\\n  --tlskey /path/to/key.pem\n</code></pre><pre><code>buildctl \\\n  --addr tcp://example.com:1234 \\\n  --tlscacert /path/to/ca.pem \\\n  --tlscert /path/to/clientcert.pem \\\n  --tlskey /path/to/clientkey.pem \\\n  build ...\n</code></pre><p> can be called against randomly load balanced  daemons.</p><p>BuildKit can also be used by running the  daemon inside a Docker container and accessing it remotely.</p><ul><li>: built from the latest regular <a href=\"https://github.com/moby/buildkit/releases\">release</a></li><li>: same as  but runs as an unprivileged user, see <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/docs/rootless.md\"></a></li><li>: built from the master branch</li><li><code>moby/buildkit:master-rootless</code>: same as master but runs as an unprivileged user, see <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/docs/rootless.md\"></a></li></ul><p>To run daemon in a container:</p><pre><code>docker run -d --name buildkitd --privileged moby/buildkit:latest\nexport BUILDKIT_HOST=docker-container://buildkitd\nbuildctl build --help\n</code></pre><p>To connect to a BuildKit daemon running in a Podman container, use  instead of  .</p><pre><code>podman run -d --name buildkitd --privileged moby/buildkit:latest\nbuildctl --addr=podman-container://buildkitd build --frontend dockerfile.v0 --local context=. --local dockerfile=. --output type=oci | podman load foo\n</code></pre><p>To connect to a BuildKit daemon running in a Nerdctl container, use  instead of .</p><pre><code>nerdctl run -d --name buildkitd --privileged moby/buildkit:latest\nbuildctl --addr=nerdctl-container://buildkitd build --frontend dockerfile.v0 --local context=. --local dockerfile=. --output type=oci | nerdctl load\n</code></pre><p>To run the client and an ephemeral daemon in a single container (\"daemonless mode\"):</p><pre><code>docker run \\\n    -it \\\n    --rm \\\n    --privileged \\\n    -v /path/to/dir:/tmp/work \\\n    --entrypoint buildctl-daemonless.sh \\\n    moby/buildkit:master \\\n        build \\\n        --frontend dockerfile.v0 \\\n        --local context=/tmp/work \\\n        --local dockerfile=/tmp/work\n</code></pre><pre><code>docker run \\\n    -it \\\n    --rm \\\n    --security-opt seccomp=unconfined \\\n    --security-opt apparmor=unconfined \\\n    -e BUILDKITD_FLAGS=--oci-worker-no-process-sandbox \\\n    -v /path/to/dir:/tmp/work \\\n    --entrypoint buildctl-daemonless.sh \\\n    moby/buildkit:master-rootless \\\n        build \\\n        --frontend \\\n        dockerfile.v0 \\\n        --local context=/tmp/work \\\n        --local dockerfile=/tmp/work\n</code></pre><p>BuildKit supports <a href=\"https://opentelemetry.io/\">OpenTelemetry</a> for buildkitd gRPC API and buildctl commands. To capture the trace to <a href=\"https://github.com/jaegertracing/jaeger\">Jaeger</a>, set  environment variable to the collection address.</p><pre><code>docker run -d -p6831:6831/udp -p16686:16686 jaegertracing/all-in-one:latest\nexport JAEGER_TRACE=0.0.0.0:6831\n# restart buildkitd and buildctl so they know JAEGER_TRACE\n# any buildctl command should be traced to http://127.0.0.1:16686/\n</code></pre><blockquote><p>On Windows, if you are running Jaeger outside of a container, <a href=\"https://www.jaegertracing.io/docs/1.57/getting-started/#all-in-one\"></a>, set the environment variable <code>setx -m JAEGER_TRACE \"0.0.0.0:6831\"</code>, restart  in a new terminal and the traces will be collected automatically.</p></blockquote><h2>Running BuildKit without root privileges</h2><h2>Building multi-platform images</h2><p> has support for modifying the colors that are used to output information to the terminal. You can set the environment variable  to something like <code>run=green:warning=yellow:error=red:cancel=255,165,0</code> to set the colors that you would like to use. Setting  to anything will disable any colorized output as recommended by <a href=\"https://no-color.org/\">no-color.org</a>.</p><p>Parsing errors will be reported but ignored. This will result in default color values being used where needed.</p><h4>Number of log lines (for active steps in tty mode)</h4><p>You can change how many log lines are visible for active steps in tty mode by setting  to a number (default: 6).</p><p>Want to contribute to BuildKit? Awesome! You can find information about contributing to this project in the <a href=\"https://raw.githubusercontent.com/moby/buildkit/master/.github/CONTRIBUTING.md\">CONTRIBUTING.md</a></p>","contentLength":23220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"alibaba/lowcode-engine","url":"https://github.com/alibaba/lowcode-engine","date":1750300590,"author":"","guid":161725,"unread":true,"content":"<p>An enterprise-class low-code technology stack with scale-out design / ‰∏ÄÂ•óÈù¢ÂêëÊâ©Â±ïËÆæËÆ°ÁöÑ‰ºÅ‰∏öÁ∫ß‰Ωé‰ª£Á†ÅÊäÄÊúØ‰ΩìÁ≥ª</p><ul><li>üåà An extension-oriented kernel engine extracted from an enterprise-level low-code platform, pursuing the design concept of the smallest kernel and the strongest ecology</li><li>üì¶ Out-of-the-box high-quality ecological elements, including material systems, setters, plugins, etc.</li><li>‚öôÔ∏è A complete tool chain, supporting the full-link R&amp;D cycle of ecological elements such as material systems, setters, and plug-ins</li><li>üîå Powerful expansion capability, has supported nearly 100 various vertical low-code platforms</li><li>üõ° Developed with TypeScript, providing complete type definition files</li></ul><h2>üéØ Compatible Environments</h2><ul><li>Modern browsers (Chrome &gt;= 80, Edge &gt;= 80, last 2 safari versions, last 2 firefox versions)</li></ul><p>The engine fully implements the \"LowCodeEngine Basic Construction Protocol Specification\" and \"LowCodeEngine Material Protocol Specification\". The protocol stack is a key part of whether materials in the low-code field can be circulated.</p><pre><code>npm install @alilc/lowcode-engine --save-dev\n</code></pre><blockquote><p><strong>TIPS: Only cdn import is supported, npm package is used to provide code hinting capabilities such as typings</strong></p></blockquote><pre><code>import { init, skeleton } from '@alilc/lowcode-engine';\n\nskeleton.add({\n  area: 'topArea',\n  type: 'Widget',\n  name: 'logo',\n  content: YourFantasticLogo,\n  contentProps: {\n    logo:\n      'https://img.alicdn.com/tfs/TB1_SocGkT2gK0jSZFkXXcIQFXa-66-66.png',\n    href: '/',\n  },\n  props: {\n    align: 'left',\n    width: 100,\n  },\n});\n\ninit(document.getElementById('lce'));\n</code></pre><h3>Engineering configuration:</h3><pre><code>{\n  \"externals\": {\n    \"@alilc/lowcode-engine\": \"var window.AliLowCodeEngine\",\n    \"@alilc/lowcode-engine-ext\": \"var window.AliLowCodeEngineExt\"\n  }\n}\n</code></pre><pre><code>https://alifd.alicdn.com/npm/@alilc/lowcode-engine@1.0.18/dist/js/engine-core.js\n\nhttps://alifd.alicdn.com/npm/@alilc/lowcode-react-simulator-renderer@1.0.18/dist/js/react-simulator-renderer.js\n</code></pre><pre><code>https://uipaas-assets.com/prod/npm/@alilc/lowcode-engine/1.0.18/dist/js/engine-core.js\n\nhttps://uipaas-assets.com/prod/npm/@alilc/lowcode-react-simulator-renderer/1.0.18/dist/js/react-simulator-renderer.js\n</code></pre><pre><code>https://unpkg.com/@alilc/lowcode-engine@1.0.18/dist/js/engine-core.js\n\nhttps://unpkg.com/@alilc/lowcode-react-simulator-renderer@1.0.18/dist/js/react-simulator-renderer.js\n</code></pre><pre><code>https://cdn.jsdelivr.net/npm/@alilc/lowcode-engine@1.0.18/dist/js/engine-core.js\n\nhttps://cdn.jsdelivr.net/npm/@alilc/lowcode-react-simulator-renderer@1.0.18/dist/js/react-simulator-renderer.js\n</code></pre><h4>Method 5: Use your own cdn</h4><p>Pass the files under packages/engine/dist and packages/react-simulator-renderer/dist in the source code to your cdn provider</p><p>This <a href=\"https://github.com/lowcode-workspace/awesome-lowcode-engine\">awesome-lowcode-engine</a> page links to a repository which records all of the tools\\materials\\solutions that use or built for the lowcode-engine, PR is welcomed.</p><pre><code>$ git clone git@github.com:alibaba/lowcode-engine.git\n$ cd lowcode-engine\n$ npm install\n$ npm run setup\n$ npm start\n</code></pre><blockquote><p>üì¢ npm access speed is slow, Alibaba employees can use tnpm, other students recommend using cnpm or specifying a mirror registry.</p><p>üì¢ Windows environment must use <a href=\"https://docs.microsoft.com/en-us/windows/wsl/install\">WSL</a>, other terminals are not guaranteed to work normally</p></blockquote><p>After lowcode-engine is started, several umd files are provided, which can be debugged in combination with the <a href=\"https://github.com/alibaba/lowcode-demo\">lowcode-demo</a> project. Refer to the file proxy rules <a href=\"https://lowcode-engine.cn/site/docs/participate/prepare\">here</a>.</p><ul><li>set the target branch to  other than </li></ul><p>Special thanks to everyone who contributed to this project.</p>","contentLength":3442,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YimMenu/YimMenuV2","url":"https://github.com/YimMenu/YimMenuV2","date":1750300590,"author":"","guid":161726,"unread":true,"content":"<p>Experimental menu for GTA 5: Enhanced</p><p>Experimental menu for GTA 5: Enhanced</p><ol><li>Download the latest version of FSL from <a href=\"https://www.unknowncheats.me/forum/grand-theft-auto-v/616977-fsl-local-gtao-saves.html\">here</a> and place version.dll in your GTA V directory. Using FSL is now optional but highly recommended for account safety</li><li>Download an injector, such as <a href=\"https://www.unknowncheats.me/forum/general-programming-and-reversing/124013-xenos-injector-v2-3-2-a.html\">Xenos</a></li><li>Open Rockstar Launcher, select Grand Theft Auto V Enhanced, go to settings, and disable BattlEye. If you are using Steam or Epic Games, you may have to pass the -nobattleye command line parameter as well</li><li>Launch GTA V, then use your injector to inject YimMenuV2.dll at the main menu</li></ol><p>Press the  key or  to open the menu</p><h3>I keep getting desynced from public sessions every five minutes</h3><p>We currently do not have a BattlEye bypass, and legitimate hosts will eventually remove you due to a heartbeat failure. There is currently no way to stop this other than using an actual (private) bypass</p><h3>I removed FSL and all my progress disappeared!</h3><p>FSL reroutes account save data to disk, so any progress made with FSL will only show up if you have FSL enabled. If you don't want this, you can also use YimMenuV2 without FSL, but this is not recommended</p><h3>I removed FSL and the game doesn't start up anymore</h3><p>This is a known issue; delete \"Documents/GTAV Enhanced/Profiles\" to fix</p>","contentLength":1220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nvm-sh/nvm","url":"https://github.com/nvm-sh/nvm","date":1750300590,"author":"","guid":161727,"unread":true,"content":"<p>Node Version Manager - POSIX-compliant bash script to manage multiple active node.js versions</p><a href=\"https://github.com/nvm-sh/logos\"></a><p> allows you to quickly install and use different versions of node via the command line.</p><pre><code>$ nvm use 16\nNow using node v16.9.1 (npm v7.21.1)\n$ node -v\nv16.9.1\n$ nvm use 14\nNow using node v14.18.0 (npm v6.14.15)\n$ node -v\nv14.18.0\n$ nvm install 12\nNow using node v12.22.6 (npm v6.14.5)\n$ node -v\nv12.22.6\n</code></pre><p>nvm is a version manager for <a href=\"https://nodejs.org/en/\">node.js</a>, designed to be installed per-user, and invoked per-shell.  works on any POSIX-compliant shell (sh, dash, ksh, zsh, bash), in particular on these platforms: unix, macOS, and <a href=\"https://github.com/nvm-sh/nvm#important-notes\">windows WSL</a>.</p><p>To  or  nvm, you should run the <a href=\"https://github.com/nvm-sh/nvm/raw/v0.40.3/install.sh\">install script</a>. To do that, you may either download and run the script manually, or use the following cURL or Wget command:</p><pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><pre><code>wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><p>Running either of the above commands downloads a script and runs it. The script clones the nvm repository to , and attempts to add the source lines from the snippet below to the correct profile file (, , , or ). If you find the install script is updating the wrong profile file, set the  env var to the profile file‚Äôs path, and then rerun the installation script.</p><pre><code>export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n</code></pre><ul><li><p>If the environment variable  is present, it will place the  files there.</p></li><li><p>You can add  to the end of the above script to postpone using  until you manually <a href=\"https://raw.githubusercontent.com/nvm-sh/nvm/master/#usage\"></a> it:</p></li></ul><pre><code>export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" --no-use # This loads nvm, without auto-using the default version\n</code></pre><ul><li><p>You can customize the install source, directory, profile, and version using the , , , and  variables. Eg: <code>curl ... | NVM_DIR=\"path/to/nvm\"</code>. Ensure that the  does not contain a trailing slash.</p></li><li><p>The installer can use , , or  to download , whichever is available.</p></li><li><p>You can instruct the installer to not edit your shell config (for example if you already get completions via a <a href=\"https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/nvm\">zsh nvm plugin</a>) by setting  before running the  script. Here's an example one-line command to do that: <code>PROFILE=/dev/null bash -c 'curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash'</code></p></li></ul><p>When invoking bash as a non-interactive shell, like in a Docker container, none of the regular profile files are sourced. In order to use , , and  like normal, you can instead specify the special  variable, which bash sources when invoked non-interactively.</p><pre><code># Use bash for the shell\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n\n# Create a script file sourced by both interactive and non-interactive bash shells\nENV BASH_ENV /home/user/.bash_env\nRUN touch \"${BASH_ENV}\"\nRUN echo '. \"${BASH_ENV}\"' &gt;&gt; ~/.bashrc\n\n# Download and install nvm\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.2/install.sh | PROFILE=\"${BASH_ENV}\" bash\nRUN echo node &gt; .nvmrc\nRUN nvm install\n</code></pre><h5>Installing in Docker for CICD-Jobs</h5><pre><code>FROM ubuntu:latest\nARG NODE_VERSION=20\n\n# install curl\nRUN apt update &amp;&amp; apt install curl -y\n\n# install nvm\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n\n# set env\nENV NVM_DIR=/root/.nvm\n\n# install node\nRUN bash -c \"source $NVM_DIR/nvm.sh &amp;&amp; nvm install $NODE_VERSION\"\n\n# set ENTRYPOINT for reloading nvm-environment\nENTRYPOINT [\"bash\", \"-c\", \"source $NVM_DIR/nvm.sh &amp;&amp; exec \\\"$@\\\"\", \"--\"]\n\n# set cmd to bash\nCMD [\"/bin/bash\"]\n\n</code></pre><p>This example defaults to installation of nodejs version 20.x.y. Optionally you can easily override the version with docker build args like:</p><pre><code>docker build -t nvmimage --build-arg NODE_VERSION=19 .\n</code></pre><p>After creation of the image you can start container interactively and run commands, for example:</p><pre><code>docker run --rm -it nvmimage\n\nroot@0a6b5a237c14:/# nvm -v\n0.40.3\n\nroot@0a6b5a237c14:/# node -v\nv19.9.0\n\nroot@0a6b5a237c14:/# npm -v\n9.6.3\n</code></pre><pre><code>user@host:/tmp/test $ docker run --rm -it nvmimage node -v\nv19.9.0\nuser@host:/tmp/test $ docker run --rm -it nvmimage npm -v\n9.6.3\n</code></pre><p>On Linux, after running the install script, if you get  or see no feedback from your terminal after you type , simply close your current terminal, open a new terminal, and try verifying again. Alternatively, you can run the following commands for the different shells on the command line:</p><p>These should pick up the  command.</p><p>Since OS X 10.9,  has been preset by Xcode command line tools, which means we can't properly detect if Git is installed or not. You need to manually install the Xcode command line tools before running the install script, otherwise, it'll fail. (see <a href=\"https://github.com/nvm-sh/nvm/issues/1782\">#1782</a>)</p><p>If you get  after running the install script, one of the following might be the reason:</p><ul><li><p>Since macOS 10.15, the default shell is  and nvm will look for  to update, none is installed by default. Create one with  and run the install script again.</p></li><li><p>If you use bash, the previous default shell, your system may not have  or  files where the command is set up. Create one of them with  or  and run the install script again. Then, run  or  to pick up the  command.</p></li><li><p>You have previously used , but you have  installed. You need to manually add <a href=\"https://raw.githubusercontent.com/nvm-sh/nvm/master/#manual-install\">these lines</a> to  and run .</p></li><li><p>You might need to restart your terminal instance or run . Restarting your terminal/opening a new tab/window, or running the source command will load the command and the new configuration.</p></li><li><p>If the above didn't help, you might need to restart your terminal instance. Try opening a new tab/window in your terminal and retry.</p></li></ul><p>If the above doesn't fix the problem, you may try the following:</p><ul><li><p>If you use bash, it may be that your  (or ) does not source your  properly. You could fix this by adding <code>source ~/&lt;your_profile_file&gt;</code> to it or following the next step below.</p></li><li><p>For more information about this issue and possible workarounds, please <a href=\"https://github.com/nvm-sh/nvm/issues/576\">refer here</a></p></li></ul><p> For Macs with the Apple Silicon chip, node started offering  arch Darwin packages since v16.0.0 and experimental  support when compiling from source since v14.17.0. If you are facing issues installing node using , you may want to update to one of those versions or later.</p><pre><code>- name: Install nvm\n  ansible.builtin.shell: &gt;\n    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n  args:\n    creates: \"{{ ansible_env.HOME }}/.nvm/nvm.sh\"\n</code></pre><p>To verify that nvm has been installed, do:</p><p>which should output  if the installation was successful. Please note that  will not work, since  is a sourced shell function, not an executable binary.</p><p> On Linux, after running the install script, if you get  or see no feedback from your terminal after you type , simply close your current terminal, open a new terminal, and try verifying again.</p><p>If you're running a system without prepackaged binary available, which means you're going to install node or io.js from its source code, you need to make sure your system has a C++ compiler. For OS X, Xcode will work, for Debian/Ubuntu based GNU/Linux, the  and  packages work.</p><p> also supports Windows in some cases. It should work through WSL (Windows Subsystem for Linux) depending on the version of WSL. It should also work with <a href=\"https://gitforwindows.org/\">GitBash</a> (MSYS) or <a href=\"https://cygwin.com\">Cygwin</a>. Otherwise, for Windows, a few alternatives exist, which are neither supported nor developed by us:</p><p> does not support <a href=\"https://fishshell.com\">Fish</a> either (see <a href=\"https://github.com/nvm-sh/nvm/issues/303\">#303</a>). Alternatives exist, which are neither supported nor developed by us:</p><ul><li><a href=\"https://github.com/edc/bass\">bass</a> allows you to use utilities written for Bash in fish shell</li><li><a href=\"https://github.com/brigand/fast-nvm-fish\">fast-nvm-fish</a> only works with version numbers (not aliases) but doesn't significantly slow your shell startup</li><li><a href=\"https://github.com/jorgebucaran/nvm.fish\">nvm.fish</a> - The Node.js version manager you'll adore, crafted just for Fish</li><li><a href=\"https://github.com/FabioAntunes/fish-nvm\">fish-nvm</a> - Wrapper around nvm for fish, delays sourcing nvm until it's actually used.</li></ul><p> We still have some problems with FreeBSD, because there is no official pre-built binary for FreeBSD, and building from source may need <a href=\"https://www.freshports.org/www/node/files/patch-deps_v8_src_base_platform_platform-posix.cc\">patches</a>; see the issue ticket:</p><p> On OS X, if you do not have Xcode installed and you do not wish to download the ~4.3GB file, you can install the . You can check out this blog post on how to just that:</p><p> On OS X, if you have/had a \"system\" node installed and want to install modules globally, keep in mind that:</p><ul><li>When using  you do not need  to globally install a module with , so instead of doing <code>sudo npm install -g grunt</code>, do instead </li><li>If you have an  file, make sure it does not contain any  settings (which is not compatible with )</li><li>You can (but should not?) keep your previous \"system\" node install, but  will only be available to your user account (the one used to install nvm). This might cause version mismatches, as other users will be using <code>/usr/local/lib/node_modules/*</code> VS your user account using <code>~/.nvm/versions/node/vX.X.X/lib/node_modules/*</code></li></ul><p>Homebrew installation is not supported. If you have issues with homebrew-installed , please  it, and install it using the instructions below, before filing an issue.</p><p> If you're using  you can easily install  as a zsh plugin. Install <a href=\"https://github.com/lukechilds/zsh-nvm\"></a> and run  to upgrade (<a href=\"https://github.com/lukechilds/zsh-nvm#auto-use\">you can set</a> to have it automatically detect and use  files).</p><p> Git versions before v1.7 may face a problem of cloning  source from GitHub via https protocol, and there is also different behavior of git before v1.6, and git prior to <a href=\"https://github.com/git/git/commit/5a7d5b683f869d3e3884a89775241afa515da9e7\">v1.17.10</a> can not clone tags, so the minimum required git version is v1.7.10. If you are interested in the problem we mentioned here, please refer to GitHub's <a href=\"https://help.github.com/articles/https-cloning-errors/\">HTTPS cloning errors</a> article.</p><p>If you have  installed (requires git v1.7.10+):</p><ol><li>clone this repo in the root of your user profile \n  <ul><li> from anywhere then <code>git clone https://github.com/nvm-sh/nvm.git .nvm</code></li></ul></li><li> and check out the latest version with </li><li>activate  by sourcing it from your shell: </li></ol><p>Now add these lines to your , , or  file to have it automatically sourced upon login: (you may have to add to more than one of the above files)</p><pre><code>export NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm\n[ -s \"$NVM_DIR/bash_completion\" ] &amp;&amp; \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_completion\n</code></pre><p>For a fully manual install, execute the following lines to first clone the  repository into , and then load :</p><pre><code>export NVM_DIR=\"$HOME/.nvm\" &amp;&amp; (\n  git clone https://github.com/nvm-sh/nvm.git \"$NVM_DIR\"\n  cd \"$NVM_DIR\"\n  git checkout `git describe --abbrev=0 --tags --match \"v[0-9]*\" $(git rev-list --tags --max-count=1)`\n) &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"\n</code></pre><p>Now add these lines to your , , or  file to have it automatically sourced upon login: (you may have to add to more than one of the above files)</p><pre><code>export NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n[ -s \"$NVM_DIR/bash_completion\" ] &amp;&amp; \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_completion\n</code></pre><p>For manual upgrade with  (requires git v1.7.10+):</p><ol><li>pull down the latest changes</li><li>check out the latest version</li></ol><pre><code>(\n  cd \"$NVM_DIR\"\n  git fetch --tags origin\n  git checkout `git describe --abbrev=0 --tags --match \"v[0-9]*\" $(git rev-list --tags --max-count=1)`\n) &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"\n</code></pre><p>To download, compile, and install the latest release of node, do this:</p><pre><code>nvm install node # \"node\" is an alias for the latest version\n</code></pre><p>To install a specific version of node:</p><pre><code>nvm install 14.7.0 # or 16.3.0, 12.22.1, etc\n</code></pre><pre><code>nvm alias my_alias v14.4.0\n</code></pre><p>Make sure that your alias does not contain any spaces or slashes.</p><p>The first version installed becomes the default. New shells will start with the default version of node (e.g., ).</p><p>You can list available versions using :</p><p>And then in any new shell just use the installed version:</p><p>Or, you can run any arbitrary command in a subshell with the desired version of node:</p><pre><code>nvm exec 4.2 node --version\n</code></pre><p>You can also get the path to the executable to where it was installed:</p><p>In place of a version pointer like \"14.7\" or \"16.3\" or \"12.22.1\", you can use the following special default aliases with , , , , , etc:</p><ul><li>: this installs the latest version of <a href=\"https://nodejs.org/en/\"></a></li><li>: this installs the latest version of <a href=\"https://iojs.org/en/\"></a></li><li>: this alias is deprecated, and only truly applies to  and earlier. Currently, this is an alias for .</li><li>: this alias points to  - the last \"unstable\" node release, since post-1.0, all node versions are stable. (in SemVer, versions communicate breakage, not stability).</li></ul><p>Node has a <a href=\"https://github.com/nodejs/Release#release-schedule\">schedule</a> for long-term support (LTS) You can reference LTS versions in aliases and  files with the notation  for the latest LTS, and  for LTS releases from the \"argon\" line, for example. In addition, the following commands support LTS arguments:</p><ul><li> /  /  / </li><li> / <code>nvm uninstall --lts=argon</code> /  / </li><li> /  /  / </li><li> /  /  / </li><li> /  /  / </li><li> / <code>nvm ls-remote --lts=argon</code> / </li><li> / <code>nvm version-remote --lts=argon</code> / <code>nvm version-remote 'lts/*'</code> / <code>nvm version-remote lts/argon</code></li></ul><p>Any time your local copy of  connects to <a href=\"https://nodejs.org\">https://nodejs.org</a>, it will re-create the appropriate local aliases for all available LTS lines. These aliases (stored under ), are managed by , and you should not modify, remove, or create these files - expect your changes to be undone, and expect meddling with these files to cause bugs that will likely not be supported.</p><p>To get the latest LTS version of node and migrate your existing installed packages, use</p><pre><code>nvm install --reinstall-packages-from=current 'lts/*'\n</code></pre><h3>Migrating Global Packages While Installing</h3><p>If you want to install a new version of Node.js and migrate npm packages from a previous version:</p><pre><code>nvm install --reinstall-packages-from=node node\n</code></pre><p>This will first use \"nvm version node\" to identify the current version you're migrating packages from. Then it resolves the new version to install from the remote server and installs it. Lastly, it runs \"nvm reinstall-packages\" to reinstall the npm packages from your prior version of Node to the new one.</p><p>You can also install and migrate npm packages from specific versions of Node like this:</p><pre><code>nvm install --reinstall-packages-from=5 6\nnvm install --reinstall-packages-from=iojs v4.2\n</code></pre><p>Note that reinstalling packages <em>explicitly does not update the npm version</em> ‚Äî this is to ensure that npm isn't accidentally upgraded to a broken version for the new node version.</p><p>To update npm at the same time add the  flag, like this:</p><pre><code>nvm install --reinstall-packages-from=default --latest-npm 'lts/*'\n</code></pre><p>or, you can at any time run the following command to get the latest supported npm version on the current node version:</p><p>If you've already gotten an error to the effect of \"npm does not support Node.js\", you'll need to (1) revert to a previous node version ( &amp; <code>nvm use &lt;your latest _working_ version from the ls&gt;</code>), (2) delete the newly created node version (<code>nvm uninstall &lt;your _broken_ version of node from the ls&gt;</code>), then (3) rerun your  with the  flag.</p><h3>Default Global Packages From File While Installing</h3><p>If you have a list of default packages you want installed every time you install a new version, we support that too -- just add the package names, one per line, to the file <code>$NVM_DIR/default-packages</code>. You can add anything npm would accept as a package argument on the command line.</p><pre><code># $NVM_DIR/default-packages\n\nrimraf\nobject-inspect@1.0.2\nstevemao/left-pad\n</code></pre><p>If you want to install <a href=\"https://github.com/iojs/io.js/\">io.js</a>:</p><p>If you want to install a new version of io.js and migrate npm packages from a previous version:</p><pre><code>nvm install --reinstall-packages-from=iojs iojs\n</code></pre><p>The same guidelines mentioned for migrating npm packages in node are applicable to io.js.</p><p>If you want to use the system-installed version of node, you can use the special default alias \"system\":</p><pre><code>nvm use system\nnvm run system --version\n</code></pre><p>If you want to see what versions are installed:</p><p>If you want to see what versions are available to install:</p><p>You can set five colors that will be used to display version and alias information. These colors replace the default colors. Initial colors are: g b y r e</p><pre><code>r/R = red / bold red\n\ng/G = green / bold green\n\nb/B = blue / bold blue\n\nc/C = cyan / bold cyan\n\nm/M = magenta / bold magenta\n\ny/Y = yellow / bold yellow\n\nk/K = black / bold black\n\ne/W = light grey / white\n</code></pre><p>If you want the custom colors to persist after terminating the shell, export the  variable in your shell profile. For example, if you want to use cyan, magenta, green, bold red and bold yellow, add the following line:</p><pre><code>export NVM_COLORS='cmgRY'\n</code></pre><h4>Suppressing colorized output</h4><p><code>nvm help (or -h or --help)</code>, ,  and  usually produce colorized output. You can disable colors with the  option (or by setting the environment variable ):</p><pre><code>nvm ls --no-colors\nnvm help --no-colors\nTERM=dumb nvm ls\n</code></pre><p>To restore your PATH, you can deactivate it:</p><p>To set a default Node version to be used in any new shell, use the alias 'default':</p><pre><code>nvm alias default node # this refers to the latest installed version of node\nnvm alias default 18 # this refers to the latest installed v18.x version of node\nnvm alias default 18.12  # this refers to the latest installed v18.12.x version of node\n</code></pre><h3>Use a mirror of node binaries</h3><p>To use a mirror of the node binaries, set :</p><pre><code>export NVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist\nnvm install node\n\nNVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist nvm install 4.2\n</code></pre><p>To use a mirror of the io.js binaries, set :</p><pre><code>export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist\nnvm install iojs-v1.0.3\n\nNVM_IOJS_ORG_MIRROR=https://iojs.org/dist nvm install iojs-v1.0.3\n</code></pre><p> will not, by default, create a \"current\" symlink. Set  to \"true\" to enable this behavior, which is sometimes useful for IDEs. Note that using  in multiple shell tabs with this environment variable enabled can cause race conditions.</p><h4>Pass Authorization header to mirror</h4><p>To pass an Authorization header through to the mirror url, set </p><pre><code>NVM_AUTH_HEADER=\"Bearer secret-token\" nvm install node\n</code></pre><p>You can create a  file containing a node version number (or any other string that  understands; see  for details) in the project root directory (or any parent directory). Afterwards, , , , , and  will use the version specified in the  file if no version is supplied on the command line.</p><p>For example, to make nvm default to the latest 5.9 release, the latest LTS version, or the latest node version for the current directory:</p><pre><code>$ echo \"5.9\" &gt; .nvmrc\n\n$ echo \"lts/*\" &gt; .nvmrc # to default to the latest LTS version\n\n$ echo \"node\" &gt; .nvmrc # to default to the latest version\n</code></pre><p>[NB these examples assume a POSIX-compliant shell version of . If you use a Windows  development environment, eg the  file is used to configure a remote Linux deployment, then keep in mind the s will be copied leading to an invalid file. Remove them.]</p><p>Then when you run nvm use:</p><pre><code>$ nvm use\nFound '/path/to/project/.nvmrc' with version &lt;5.9&gt;\nNow using node v5.9.1 (npm v3.7.3)\n</code></pre><p>Running nvm install will also switch over to the correct version, but if the correct node version isn't already installed, it will install it for you.</p><pre><code>$ nvm install\nFound '/path/to/project/.nvmrc' with version &lt;5.9&gt;\nDownloading and installing node v5.9.1...\nDownloading https://nodejs.org/dist/v5.9.1/node-v5.9.1-linux-x64.tar.xz...\n#################################################################################### 100.0%\nComputing checksum with sha256sum\nChecksums matched!\nNow using node v5.9.1 (npm v3.7.3)\n</code></pre><p> et. al. will traverse directory structure upwards from the current directory looking for the  file. In other words, running  et. al. in any subdirectory of a directory with an  will result in that  being utilized.</p><p>The contents of a  file  contain precisely one  (as described by ) followed by a newline.  files may also have comments. The comment delimiter is , and it and any text after it, as well as blank lines, and leading and trailing white space, will be ignored when parsing.</p><p>Key/value pairs using  are also allowed and ignored, but are reserved for future use, and may cause validation errors in the future.</p><p>Run <a href=\"https://npmjs.com/nvmrc\"></a> to validate an  file. If that tool‚Äôs results do not agree with nvm, one or the other has a bug - please file an issue.</p><p>You can use <a href=\"https://github.com/iamogbz/nvshim\"></a> to shim the , , and  bins to automatically use the  config in the current directory.  is  supported by the  maintainers. Please <a href=\"https://github.com/iamogbz/nvshim/issues/new\">report issues to the  team</a>.</p><p>If you prefer a lighter-weight solution, the recipes below have been contributed by  users. They are  supported by the  maintainers. We are, however, accepting pull requests for more examples.</p><h4>Calling  automatically in a directory with a  file</h4><p>In your profile (, , , or ), add the following to  whenever you enter a new directory:</p><p>Put the following at the end of your :</p><pre><code>cdnvm() {\n    command cd \"$@\" || return $?\n    nvm_path=\"$(nvm_find_up .nvmrc | command tr -d '\\n')\"\n\n    # If there are no .nvmrc file, use the default nvm version\n    if [[ ! $nvm_path = *[^[:space:]]* ]]; then\n\n        declare default_version\n        default_version=\"$(nvm version default)\"\n\n        # If there is no default version, set it to `node`\n        # This will use the latest version on your machine\n        if [ $default_version = 'N/A' ]; then\n            nvm alias default node\n            default_version=$(nvm version default)\n        fi\n\n        # If the current version is not the default version, set it to use the default version\n        if [ \"$(nvm current)\" != \"${default_version}\" ]; then\n            nvm use default\n        fi\n    elif [[ -s \"${nvm_path}/.nvmrc\" &amp;&amp; -r \"${nvm_path}/.nvmrc\" ]]; then\n        declare nvm_version\n        nvm_version=$(&lt;\"${nvm_path}\"/.nvmrc)\n\n        declare locally_resolved_nvm_version\n        # `nvm ls` will check all locally-available versions\n        # If there are multiple matching versions, take the latest one\n        # Remove the `-&gt;` and `*` characters and spaces\n        # `locally_resolved_nvm_version` will be `N/A` if no local versions are found\n        locally_resolved_nvm_version=$(nvm ls --no-colors \"${nvm_version}\" | command tail -1 | command tr -d '\\-&gt;*' | command tr -d '[:space:]')\n\n        # If it is not already installed, install it\n        # `nvm install` will implicitly use the newly-installed version\n        if [ \"${locally_resolved_nvm_version}\" = 'N/A' ]; then\n            nvm install \"${nvm_version}\";\n        elif [ \"$(nvm current)\" != \"${locally_resolved_nvm_version}\" ]; then\n            nvm use \"${nvm_version}\";\n        fi\n    fi\n}\n\nalias cd='cdnvm'\ncdnvm \"$PWD\" || exit\n</code></pre><p>This alias would search 'up' from your current directory in order to detect a  file. If it finds it, it will switch to that version; if not, it will use the default version.</p><p>This shell function will install (if needed) and  the specified Node version when an  is found, and  otherwise.</p><p>Put this into your  to call  automatically whenever you enter a directory that contains an  file with a string telling nvm which node to :</p><pre><code># place this after nvm initialization!\nautoload -U add-zsh-hook\n\nload-nvmrc() {\n  local nvmrc_path\n  nvmrc_path=\"$(nvm_find_nvmrc)\"\n\n  if [ -n \"$nvmrc_path\" ]; then\n    local nvmrc_node_version\n    nvmrc_node_version=$(nvm version \"$(cat \"${nvmrc_path}\")\")\n\n    if [ \"$nvmrc_node_version\" = \"N/A\" ]; then\n      nvm install\n    elif [ \"$nvmrc_node_version\" != \"$(nvm version)\" ]; then\n      nvm use\n    fi\n  elif [ -n \"$(PWD=$OLDPWD nvm_find_nvmrc)\" ] &amp;&amp; [ \"$(nvm version)\" != \"$(nvm version default)\" ]; then\n    echo \"Reverting to nvm default version\"\n    nvm use default\n  fi\n}\n\nadd-zsh-hook chpwd load-nvmrc\nload-nvmrc\n</code></pre><p>After saving the file, run  to reload the configuration with the latest changes made.</p><p>This requires that you have <a href=\"https://github.com/edc/bass\">bass</a> installed.</p><pre><code># ~/.config/fish/functions/nvm.fish\nfunction nvm\n  bass source ~/.nvm/nvm.sh --no-use ';' nvm $argv\nend\n\n# ~/.config/fish/functions/nvm_find_nvmrc.fish\nfunction nvm_find_nvmrc\n  bass source ~/.nvm/nvm.sh --no-use ';' nvm_find_nvmrc\nend\n\n# ~/.config/fish/functions/load_nvm.fish\nfunction load_nvm --on-variable=\"PWD\"\n  set -l default_node_version (nvm version default)\n  set -l node_version (nvm version)\n  set -l nvmrc_path (nvm_find_nvmrc)\n  if test -n \"$nvmrc_path\"\n    set -l nvmrc_node_version (nvm version (cat $nvmrc_path))\n    if test \"$nvmrc_node_version\" = \"N/A\"\n      nvm install (cat $nvmrc_path)\n    else if test \"$nvmrc_node_version\" != \"$node_version\"\n      nvm use $nvmrc_node_version\n    end\n  else if test \"$node_version\" != \"$default_node_version\"\n    echo \"Reverting to default Node version\"\n    nvm use default\n  end\nend\n\n# ~/.config/fish/config.fish\n# You must call it on initialization or listening to directory switching won't work\nload_nvm &gt; /dev/stderr\n</code></pre><p>Tests are written in <a href=\"https://git.sdf.org/tlevine/urchin\">Urchin</a>. Install Urchin (and other dependencies) like so:</p><p>There are slow tests and fast tests. The slow tests do things like install node and check that the right versions are used. The fast tests fake this to test things like aliases and uninstalling. From the root of the nvm git repository, run the fast tests like this:</p><p>Run the slow tests like this:</p><p>Run all of the tests like this:</p><p>Nota bene: Avoid running nvm while the tests are running.</p><p>nvm exposes the following environment variables:</p><ul><li> - nvm's installation directory.</li><li> - where node, npm, and global packages for the active version of node are installed.</li><li> - node's include file directory (useful for building C/C++ addons for node).</li><li> - used to maintain compatibility with zsh.</li><li> - version from .nvmrc file if being used.</li></ul><p>Additionally, nvm modifies , and, if present,  and  when changing versions.</p><p>To activate, you need to source :</p><pre><code>[[ -r $NVM_DIR/bash_completion ]] &amp;&amp; \\. $NVM_DIR/bash_completion\n</code></pre><p>Put the above sourcing line just below the sourcing line for nvm in your profile (, ).</p><pre><code>alias               deactivate          install             list-remote         reinstall-packages  uninstall           version\ncache               exec                install-latest-npm  ls                  run                 unload              version-remote\ncurrent             help                list                ls-remote           unalias             use                 which\n</code></pre><pre><code>default      iojs         lts/*        lts/argon    lts/boron    lts/carbon   lts/dubnium  lts/erbium   node         stable       unstable\n</code></pre><pre><code>v10.22.0       v12.18.3      v14.8.0\n</code></pre><pre><code>my_alias        default        v10.22.0       v12.18.3      v14.8.0\n</code></pre><pre><code>my_alias        default        v10.22.0       v12.18.3      v14.8.0\n</code></pre><p> will encounter some issues if you have some non-default settings set. (see <a href=\"https://github.com/nvm-sh/nvm/issues/606\">#606</a>) The following are known to cause issues:</p><pre><code>$NPM_CONFIG_PREFIX\n$PREFIX\n</code></pre><h2>Installing nvm on Alpine Linux</h2><p>In order to provide the best performance (and other optimizations), nvm will download and install pre-compiled binaries for Node (and npm) when you run . The Node project compiles, tests and hosts/provides these pre-compiled binaries which are built for mainstream/traditional Linux distributions (such as Debian, Ubuntu, CentOS, RedHat et al).</p><p>Alpine Linux, unlike mainstream/traditional Linux distributions, is based on <a href=\"https://www.busybox.net/\">BusyBox</a>, a very compact (~5MB) Linux distribution. BusyBox (and thus Alpine Linux) uses a different C/C++ stack to most mainstream/traditional Linux distributions - <a href=\"https://www.musl-libc.org/\">musl</a>. This makes binary programs built for such mainstream/traditional incompatible with Alpine Linux, thus we cannot simply  on Alpine Linux and expect the downloaded binary to run correctly - you'll likely see \"...does not exist\" errors if you try that.</p><p>There is a  flag for  which requests nvm download Node source and compile it locally.</p><p>If installing nvm on Alpine Linux  still what you want or need to do, you should be able to achieve this by running the following from you Alpine Linux shell, depending on which version you are using:</p><pre><code>apk add -U curl bash ca-certificates openssl ncurses coreutils python3 make gcc g++ libgcc linux-headers grep util-linux binutils findutils\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><pre><code>apk add -U curl bash ca-certificates openssl ncurses coreutils python2 make gcc g++ libgcc linux-headers grep util-linux binutils findutils\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n</code></pre><p><em>Note: Alpine 3.5 can only install NodeJS versions up to v6.9.5, Alpine 3.6 can only install versions up to v6.10.3, Alpine 3.7 installs versions up to v8.9.3, Alpine 3.8 installs versions up to v8.14.0, Alpine 3.9 installs versions up to v10.19.0, Alpine 3.10 installs versions up to v10.24.1, Alpine 3.11 installs versions up to v12.22.6, Alpine 3.12 installs versions up to v12.22.12, Alpine 3.13 &amp; 3.14 install versions up to v14.20.0, Alpine 3.15 &amp; 3.16 install versions up to v16.16.0 (<strong>These are all versions on the main branch</strong>). Alpine 3.5 - 3.12 required the package  to build NodeJS, as they are older versions to build. Alpine 3.13+ requires  to successfully build newer NodeJS versions, but you can use  with Alpine 3.13+ if you need to build versions of node supported in Alpine 3.5 - 3.15, you just need to specify what version of NodeJS you need to install in the package install script.</em></p><p>The Node project has some desire but no concrete plans (due to the overheads of building, testing and support) to offer Alpine-compatible binaries.</p><p>To remove  manually, execute the following:</p><p>First, use  to remove the nvm command from your terminal session and delete the installation directory:</p><pre><code>$ nvm_dir=\"${NVM_DIR:-~/.nvm}\"\n$ nvm unload\n$ rm -rf \"$nvm_dir\"\n</code></pre><p>Edit  (or other shell resource config) and remove the lines below:</p><pre><code>export NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n[[ -r $NVM_DIR/bash_completion ]] &amp;&amp; \\. $NVM_DIR/bash_completion\n</code></pre><h2>Docker For Development Environment</h2><p>To make the development and testing work easier, we have a Dockerfile for development usage, which is based on Ubuntu 18.04 base image, prepared with essential and useful tools for  development, to build the docker image of the environment, run the docker command at the root of  repository:</p><pre><code>$ docker build -t nvm-dev .\n</code></pre><p>This will package your current nvm repository with our pre-defined development environment into a docker image named , once it's built with success, validate your image via :</p><pre><code>$ docker images\n\nREPOSITORY         TAG                 IMAGE ID            CREATED             SIZE\nnvm-dev            latest              9ca4c57a97d8        7 days ago          650 MB\n</code></pre><p>If you got no error message, now you can easily involve in:</p><pre><code>$ docker run -h nvm-dev -it nvm-dev\n\nnvm@nvm-dev:~/.nvm$\n</code></pre><p>Please note that it'll take about 8 minutes to build the image and the image size would be about 650MB, so it's not suitable for production usage.</p><p>For more information and documentation about docker, please refer to its official website:</p><ul><li><p>If you try to install a node version and the installation fails, be sure to run  to delete cached node downloads, or you might get an error like the following:</p><p>curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.</p></li><li><p>Where's my ? Check out <a href=\"https://github.com/nvm-sh/nvm/issues/43\">#43</a></p></li><li><p>After the v0.8.6 release of node, nvm tries to install from binary packages. But in some systems, the official binary packages don't work due to incompatibility of shared libs. In such cases, use  option to force install from source:</p></li></ul><ul><li>If setting the  alias does not establish the node version in new shells (i.e.  yields ), ensure that the system's node  is set before the  source line in your shell profile (see <a href=\"https://github.com/nvm-sh/nvm/issues/658\">#658</a>)</li></ul><p><strong>nvm node version not found in vim shell</strong></p><p>If you set node version to a version other than your system node version  and open vim and run  you should see  if you see your system version . You need to run:</p><pre><code>sudo chmod ugo-x /usr/libexec/path_helper\n</code></pre><p><strong>nvm is not compatible with the npm config \"prefix\" option</strong></p><p>Some solutions for this issue can be found <a href=\"https://github.com/nvm-sh/nvm/issues/1245\">here</a></p><p>There is one more edge case causing this issue, and that's a <strong>mismatch between the  path and the user's home directory's actual name</strong>.</p><p>You have to make sure that the user directory name in  and the user directory name you'd see from running <strong>are capitalized the same way</strong> (<a href=\"https://github.com/nvm-sh/nvm/issues/2261\">See this issue</a>).</p><p>To change the user directory and/or account name follow the instructions <a href=\"https://support.apple.com/en-us/HT201548\">here</a></p><p><strong>Homebrew makes zsh directories unsecure</strong></p><pre><code>zsh compinit: insecure directories, run compaudit for list.\nIgnore insecure directories and continue [y] or abort compinit [n]? y\n</code></pre><p>Homebrew causes insecure directories like <code>/usr/local/share/zsh/site-functions</code> and . This is  an  problem - it is a homebrew problem. Refer <a href=\"https://github.com/zsh-users/zsh-completions/issues/680\">here</a> for some solutions related to the issue.</p><p><strong>Macs with Apple Silicon chips</strong></p><p>Experimental support for the Apple Silicon chip architecture was added in node.js v15.3 and full support was added in v16.0. Because of this, if you try to install older versions of node as usual, you will probably experience either compilation errors when installing node or out-of-memory errors while running your code.</p><p>So, if you want to run a version prior to v16.0 on an Apple Silicon Mac, it may be best to compile node targeting the  Intel architecture so that Rosetta 2 can translate the  processor instructions to ARM-based Apple Silicon instructions. Here's what you will need to do:</p><ul><li><p>Install Rosetta, if you haven't already done so</p><pre><code>$ softwareupdate --install-rosetta\n</code></pre><p>You might wonder, \"how will my Apple Silicon Mac know to use Rosetta for a version of node compiled for an Intel chip?\". If an executable contains only Intel instructions, macOS will automatically use Rosetta to translate the instructions.</p></li><li><p>Open a shell that's running using Rosetta</p><p>Note: This same thing can also be accomplished by finding the Terminal or iTerm App in Finder, right clicking, selecting \"Get Info\", and then checking the box labeled \"Open using Rosetta\".</p><p>Note: This terminal session is now running in . If  is not the shell you typically use,  may not be 'd automatically like it probably is for your usual shell through your dotfiles. If that's the case, make sure to source .</p><pre><code>$ source \"${NVM_DIR}/nvm.sh\"\n</code></pre></li><li><p>Install whatever older version of node you are interested in. Let's use 12.22.1 as an example. This will fetch the node source code and compile it, which will take several minutes.</p><pre><code>$ nvm install v12.22.1 --shared-zlib\n</code></pre><p>Note: You're probably curious why  is included. There's a bug in recent versions of Apple's system  compiler. If one of these broken versions is installed on your system, the above step will likely still succeed even if you didn't include the  flag. However, later, when you attempt to  something using your old version of node.js, you will see  errors. If you want to avoid the possible hassle of dealing with this, include that flag. For more details, see <a href=\"https://github.com/nodejs/node/issues/39313\">this issue</a> and <a href=\"https://github.com/nodejs/node/issues/39313#issuecomment-90.40.376\">this comment</a></p></li><li><p>Exit back to your native shell.</p><p>Note: If you selected the box labeled \"Open using Rosetta\" rather than running the CLI command in the second step, you will see  here. Unless you have another reason to have that box selected, you can deselect it now.</p></li><li><p>Check to make sure the architecture is correct.  is the abbreviation for , which is what you want to see.</p><pre><code>$ node -p process.arch\nx64\n</code></pre></li></ul><p>Now you should be able to use node as usual.</p><p>If you've encountered this error on WSL-2:</p><pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload  Total   Spent    Left  Speed\n0     0    0     0    0     0      0      0 --:--:--  0:00:09 --:--:--     0curl: (6) Could not resolve host: raw.githubusercontent.com\n</code></pre><p>It may be due to your antivirus, VPN, or other reasons.</p><p>Where you can  while you can't </p><p>This could simply be solved by running this in your root directory:</p><pre><code>sudo rm /etc/resolv.conf\nsudo bash -c 'echo \"nameserver 8.8.8.8\" &gt; /etc/resolv.conf'\nsudo bash -c 'echo \"[network]\" &gt; /etc/wsl.conf'\nsudo bash -c 'echo \"generateResolvConf = false\" &gt;&gt; /etc/wsl.conf'\nsudo chattr +i /etc/resolv.conf\n</code></pre><p>This deletes your  file that is automatically generated when you run WSL, creates a new file and puts , then creates a  file and adds  and <code>generateResolveConf = false</code> to prevent auto-generation of that file.</p><p>You can check the contents of the file by running:</p><p>Currently, the sole maintainer is <a href=\"https://github.com/ljharb\">@ljharb</a> - more maintainers are quite welcome, and we hope to add folks to the team over time. <a href=\"https://raw.githubusercontent.com/nvm-sh/nvm/master/GOVERNANCE.md\">Governance</a> will be re-evaluated as the project evolves.</p><p>Only the latest version (v0.40.3 at this time) is supported.</p><p>If you are unable to update to the latest version of , our <a href=\"https://openjsf.org/ecosystem-sustainability-program\">partners</a> provide commercial security fixes for all unsupported versions:</p>","contentLength":35843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DataExpert-io/data-engineer-handbook","url":"https://github.com/DataExpert-io/data-engineer-handbook","date":1750300590,"author":"","guid":161728,"unread":true,"content":"<p>This is a repo with links to everything you'd ever want to learn about data engineering</p><p>This repo has all the resources you need to become an amazing data engineer!</p><p>For more applied learning:</p><ul><li>Check out the <a href=\"https://raw.githubusercontent.com/DataExpert-io/data-engineer-handbook/main/projects.md\">projects</a> section for more hands-on examples!</li><li>Check out the <a href=\"https://raw.githubusercontent.com/DataExpert-io/data-engineer-handbook/main/interviews.md\">interviews</a> section for more advice on how to pass data engineering interviews!</li><li>Check out the <a href=\"https://raw.githubusercontent.com/DataExpert-io/data-engineer-handbook/main/books.md\">books</a> section for a list of high quality data engineering books</li><li>Check out the <a href=\"https://raw.githubusercontent.com/DataExpert-io/data-engineer-handbook/main/communities.md\">communities</a> section for a list of high quality data engineering communities to join</li></ul><p>Top 3 must read books are:</p><p>Top must-join communities for DE:</p><p>Top must-join communities for ML:</p><h3>Data Engineering blogs of companies:</h3><h3>Data Engineering Whitepapers:</h3><p>Here's the mostly comprehensive list of data engineering creators: <strong>(You have to have at least 5k followers somewhere to be added!)</strong></p><p>Top must follow newsletters for data engineering:</p>","contentLength":849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nocodb/nocodb","url":"https://github.com/nocodb/nocodb","date":1750300590,"author":"","guid":161729,"unread":true,"content":"<p>üî• üî• üî• Open Source Airtable Alternative</p><p align=\"center\"> NocoDB is the fastest and easiest way to build databases online. </p><img src=\"https://static.scarf.sh/a.png?x-pxid=c12a77cc-855e-4602-8a0f-614b2d0da56a\"><a href=\"https://discord.gg/5RgZmkW\" target=\"_blank\"><img src=\"https://discordapp.com/api/guilds/661905455894888490/widget.png?style=banner3\" alt=\"\"></a><pre><code>docker run -d \\\n  --name noco \\\n  -v \"$(pwd)\"/nocodb:/usr/app/data/ \\\n  -p 8080:8080 \\\n  nocodb/nocodb:latest\n</code></pre><pre><code>docker run -d \\\n  --name noco \\\n  -v \"$(pwd)\"/nocodb:/usr/app/data/ \\\n  -p 8080:8080 \\\n  -e NC_DB=\"pg://host.docker.internal:5432?u=root&amp;p=password&amp;d=d1\" \\\n  -e NC_AUTH_JWT_SECRET=\"569a1821-0a93-45e8-87ab-eb857f20a010\" \\\n  nocodb/nocodb:latest\n</code></pre><pre><code>nix run github:nocodb/nocodb\n</code></pre><p>To use NocoDB as a NixOS module, a flake.nix would be as follows:</p><pre><code>{\n  description = \"Bane's NixOS configuration\";\n\n  inputs = {\n    nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";\n    nocodb.url = \"github:nocodb/nocodb\";\n  };\n\n  outputs = inputs@{ nixpkgs, nocodb, ... }: {\n    nixosConfigurations = {\n      hostname = nixpkgs.lib.nixosSystem {\n        system = \"x86_64-linux\";\n        modules = [\n          ./configuration.nix\n          nocodb.nixosModules.nocodb\n\n          {\n            services.nocodb.enable = true;\n          }\n        ];\n      };\n    };\n  };\n}\n</code></pre><p>Auto-upstall is a single command that sets up NocoDB on a server for production usage. Behind the scenes it auto-generates docker-compose for you.</p><pre><code>bash &lt;(curl -sSL http://install.nocodb.com/noco.sh) &lt;(mktemp)\n</code></pre><p>Auto-upstall does the following: üïä</p><ul><li>üê≥ Automatically installs all pre-requisites like docker, docker-compose</li><li>üöÄ Automatically installs NocoDB with PostgreSQL, Redis, Minio, Traefik gateway using Docker Compose. üêò üóÑÔ∏è üåê</li><li>üîÑ Automatically upgrades NocoDB to the latest version when you run the command again.</li><li>üîí Automatically setups SSL and also renews it. Needs a domain or subdomain as input while installation.</li></ul><blockquote><p>Binaries are only for quick testing locally.</p></blockquote><table><thead><tr></tr></thead><tbody><tr><td><code>curl http://get.nocodb.com/macos-arm64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb</code></td></tr><tr><td><code>curl http://get.nocodb.com/macos-x64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb</code></td></tr><tr><td><code>curl http://get.nocodb.com/linux-arm64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb</code></td></tr><tr><td><code>curl http://get.nocodb.com/linux-x64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb</code></td></tr><tr><td><code>iwr http://get.nocodb.com/win-arm64.exe -OutFile Noco-win-arm64.exe &amp;&amp; .\\Noco-win-arm64.exe</code></td></tr><tr><td><code>iwr http://get.nocodb.com/win-x64.exe -OutFile Noco-win-x64.exe &amp;&amp; .\\Noco-win-x64.exe</code></td></tr></tbody></table><p>For more installation methods, please refer to <a href=\"https://docs.nocodb.com/category/installation\">our docs</a></p><h3>Rich Spreadsheet Interface</h3><ul><li>‚ö° &nbsp;Basic Operations: Create, Read, Update and Delete Tables, Columns, and Rows</li><li>‚ö° &nbsp;Fields Operations: Sort, Filter, Group, Hide / Unhide Columns</li><li>‚ö° &nbsp;Multiple Views Types: Grid (By default), Gallery, Form, Kanban and Calendar View</li><li>‚ö° &nbsp;View Permissions Types: Collaborative Views, &amp; Locked Views</li><li>‚ö° &nbsp;Share Bases / Views: either Public or Private (with Password Protected)</li><li>‚ö° &nbsp;Variant Cell Types: ID, Links, Lookup, Rollup, SingleLineText, Attachment, Currency, Formula, User, etc</li><li>‚ö° &nbsp;Access Control with Roles: Fine-grained Access Control at different levels</li></ul><h3>App Store for Workflow Automations</h3><p>We provide different integrations in three main categories. See <a href=\"https://docs.nocodb.com/account-settings/oss-specific-details/#app-store\" target=\"_blank\">App Store</a> for details.</p><ul><li>‚ö° &nbsp;Chat: Slack, Discord, Mattermost, and etc</li><li>‚ö° &nbsp;Email: AWS SES, SMTP, MailerSend, and etc</li><li>‚ö° &nbsp;Storage: AWS S3, Google Cloud Storage, Minio, and etc</li></ul><p>We provide the following ways to let users programmatically invoke actions. You can use a token (either JWT or Social Auth) to sign your requests for authorization to NocoDB.</p><ul></ul><p>Most internet businesses equip themselves with either spreadsheet or a database to solve their business needs. Spreadsheets are used by Billion+ humans collaboratively every single day. However, we are way off working at similar speeds on databases which are way more powerful tools when it comes to computing. Attempts to solve this with SaaS offerings have meant horrible access controls, vendor lock-in, data lock-in, abrupt price changes &amp; most importantly a glass ceiling on what's possible in the future.</p><p>Our mission is to provide the most powerful no-code interface for databases that is open source to every single internet business in the world. This would not only democratise access to a powerful computing tool but also bring forth a billion+ people who will have radical tinkering-and-building abilities on the internet.</p><p> This project is licensed under <a href=\"https://raw.githubusercontent.com/nocodb/nocodb/develop/LICENSE\">AGPLv3</a>. </p><p>Thank you for your contributions! We appreciate all the contributions from the community.</p><a href=\"https://github.com/nocodb/nocodb/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=nocodb/nocodb\"></a>","contentLength":4299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"menloresearch/jan","url":"https://github.com/menloresearch/jan","date":1750214124,"author":"","guid":160068,"unread":true,"content":"<p>Jan is an open source alternative to ChatGPT that runs 100% offline on your computer</p><p>Jan is a ChatGPT-alternative that runs 100% offline on your device. Our goal is to make it easy for a layperson to download and run LLMs and use AI with  and .</p><p><strong>‚ö†Ô∏è Jan is in active development.</strong></p><p>Because clicking a button is still the easiest way to get started:</p><ul><li>: Download and run LLMs (Llama, Gemma, Qwen, etc.) from HuggingFace</li><li>: Connect to OpenAI, Anthropic, Mistral, Groq, and others</li><li>: Create specialized AI assistants for your tasks</li><li>: Local server at  for other applications</li><li>: MCP integration for enhanced capabilities</li><li>: Everything runs locally when you want it to</li></ul><p>For those who enjoy the scenic route:</p><ul></ul><pre><code>git clone https://github.com/menloresearch/jan\ncd jan\nmake dev\n</code></pre><p>This handles everything: installs dependencies, builds core components, and launches the app.</p><p>If you prefer the verbose approach:</p><pre><code># Setup and development\nyarn install\nyarn build:core\nyarn build:extensions\nyarn dev\n\n# Production build\nyarn build\n\n# Clean slate (when things inevitably break)\nmake clean\n</code></pre><ul><li> - Full development setup and launch (recommended)</li><li> - Tauri development (deprecated, use )</li><li> - Production build</li><li> - Install dependencies and build core/extensions</li><li> - Run tests and linting</li><li> - Check your code doesn't offend the linters</li><li> - Nuclear option: delete everything and start fresh</li></ul><p><strong>Minimum specs for a decent experience:</strong></p><ul><li>: 13.6+ (8GB RAM for 3B models, 16GB for 7B, 32GB for 13B)</li><li>: 10+ with GPU support for NVIDIA/AMD/Intel Arc</li><li>: Most distributions work, GPU acceleration available</li></ul><p>When things go sideways (they will):</p><p>We keep logs for 24 hours, so don't procrastinate on reporting issues.</p><p>: We're not trying to scam you.</p><ul><li>We won't ask for personal information</li><li>Jan is completely free (no premium version exists)</li><li>We don't have a cryptocurrency or ICO</li><li>We're bootstrapped and not seeking your investment (yet)</li></ul><p>Apache 2.0 - Because sharing is caring.</p><p>Built on the shoulders of giants:</p>","contentLength":1910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"continuedev/continue","url":"https://github.com/continuedev/continue","date":1750214124,"author":"","guid":160069,"unread":true,"content":"<p>‚è© Create, share, and use custom AI code assistants with our open-source IDE extensions and hub of models, rules, prompts, docs, and other building blocks</p><div align=\"center\"><a target=\"_blank\" href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true\"></a><a target=\"_blank\" href=\"https://docs.continue.dev\"><img src=\"https://img.shields.io/badge/continue_docs-%23BE1B55\"></a><a target=\"_blank\" href=\"https://changelog.continue.dev\"><img src=\"https://img.shields.io/badge/changelog-%96EFF3\"></a><a target=\"_blank\" href=\"https://discord.gg/vapESyrFmJ\"><img src=\"https://img.shields.io/badge/discord-join-continue.svg?labelColor=191937&amp;color=6F6FF7&amp;logo=discord\"></a><p><a href=\"https://continue.dev/docs/agent/how-to-use-it\">Agent</a> enables you to make more substantial changes to your codebase</p><p><a href=\"https://continue.dev/docs/chat/how-to-use-it\">Chat</a> makes it easy to ask for help from an LLM without needing to leave the IDE</p><p><a href=\"https://continue.dev/docs/edit/how-to-use-it\">Edit</a> is a convenient way to modify code without leaving your current file</p></div><p>Learn about how to install and use Continue in the docs <a href=\"https://continue.dev/docs/getting-started/install\">here</a></p>","contentLength":434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"automatisch/automatisch","url":"https://github.com/automatisch/automatisch","date":1750214124,"author":"","guid":160070,"unread":true,"content":"<p>The open source Zapier alternative. Build workflow automation without spending time and money.</p><p>üßê Automatisch is a business automation tool that lets you connect different services like Twitter, Slack, and more to automate your business processes.</p><p>üí∏ Automating your workflows doesn't have to be a difficult or expensive process. You also don't need any programming knowledge to use Automatisch.</p><p>There are other existing solutions in the market, like Zapier and Integromat, so you might be wondering why you should use Automatisch.</p><p>‚úÖ One of the main benefits of using Automatisch is that it allows you to store your data on your own servers, which is essential for businesses that handle sensitive user information and cannot risk sharing it with external cloud services. This is especially relevant for industries such as healthcare and finance, as well as for European companies that must adhere to the General Data Protection Regulation (GDPR).</p><p>ü§ì Your contributions are vital to the development of Automatisch. As an open-source software, anyone can have an impact on how it is being developed.</p><p>üíô No vendor lock-in. If you ever decide that Automatisch is no longer helpful for your business, you can switch to any other provider, which will be easier than switching from the one cloud provider to another since you have all data and flexibility.</p><pre><code># Clone the repository\ngit clone https://github.com/automatisch/automatisch.git\n\n# Go to the repository folder\ncd automatisch\n\n# Start\ndocker compose up\n</code></pre><p>You can use  email address and  password to login to Automatisch. Please do not forget to change your email and password from the settings page.</p><p>For other installation types, you can check the <a href=\"https://automatisch.io/docs/guide/installation\">installation</a> guide.</p><p>If you have any questions or problems, please visit our GitHub issues page, and we'll try to help you as soon as possible.</p><p>Automatisch Community Edition (Automatisch CE) is an open-source software with the <a href=\"https://raw.githubusercontent.com/automatisch/automatisch/main/LICENSE.agpl\">AGPL-3.0 license</a>.</p><p>Automatisch Enterprise Edition (Automatisch EE) is a commercial offering with the <a href=\"https://raw.githubusercontent.com/automatisch/automatisch/main/LICENSE.enterprise\">Enterprise license</a>.</p><p>The Automatisch repository contains both AGPL-licensed and Enterprise-licensed files. We maintain a single repository to make development easier.</p><p>See the <a href=\"https://raw.githubusercontent.com/automatisch/automatisch/main/LICENSE\">LICENSE</a> file for more information.</p>","contentLength":2228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"infiniflow/ragflow","url":"https://github.com/infiniflow/ragflow","date":1750214124,"author":"","guid":160071,"unread":true,"content":"<p>RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.</p><p><a href=\"https://ragflow.io/\">RAGFlow</a> is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data.</p><ul><li>2025-05-23 Adds a Python/JavaScript code executor component to Agent.</li><li>2025-05-05 Supports cross-language query.</li><li>2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.</li><li>2025-02-28 Combined with Internet search (Tavily), supports reasoning like Deep Research for any LLMs.</li><li>2024-12-18 Upgrades Document Layout Analysis model in DeepDoc.</li><li>2024-08-22 Support text to SQL statements through RAG.</li></ul><p>‚≠êÔ∏è Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! üåü</p><h3>üç≠ <strong>\"Quality in, quality out\"</strong></h3><ul><li>Finds \"needle in a data haystack\" of literally unlimited tokens.</li></ul><ul><li>Intelligent and explainable.</li><li>Plenty of template options to choose from.</li></ul><h3>üå± <strong>Grounded citations with reduced hallucinations</strong></h3><ul><li>Visualization of text chunking to allow human intervention.</li><li>Quick view of the key references and traceable citations to support grounded answers.</li></ul><h3>üçî <strong>Compatibility with heterogeneous data sources</strong></h3><ul><li>Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.</li></ul><h3>üõÄ <strong>Automated and effortless RAG workflow</strong></h3><ul><li>Streamlined RAG orchestration catered to both personal and large businesses.</li><li>Configurable LLMs as well as embedding models.</li><li>Multiple recall paired with fused re-ranking.</li><li>Intuitive APIs for seamless integration with business.</li></ul><ul><li>Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1</li><li><a href=\"https://gvisor.dev/docs/user_guide/install/\">gVisor</a>: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.</li></ul><blockquote><p>[!TIP] If you have not installed Docker on your local machine (Windows, Mac, or Linux), see <a href=\"https://docs.docker.com/engine/install/\">Install Docker Engine</a>.</p></blockquote><ol><li><p>Ensure  &gt;= 262144:</p><blockquote><p>To check the value of :</p><pre><code>$ sysctl vm.max_map_count\n</code></pre><p>Reset  to a value at least 262144 if it is not.</p><pre><code># In this case, we set it to 262144:\n$ sudo sysctl -w vm.max_map_count=262144\n</code></pre><p>This change will be reset after a system reboot. To ensure your change remains permanent, add or update the  value in  accordingly:</p></blockquote></li><li><pre><code>$ git clone https://github.com/infiniflow/ragflow.git\n</code></pre></li><li><p>Start up the server using the pre-built Docker images:</p></li></ol><blockquote><p>[!CAUTION] All Docker images are built for x86 platforms. We don't currently offer Docker images for ARM64. If you are on an ARM64 platform, follow <a href=\"https://ragflow.io/docs/dev/build_docker_image\">this guide</a> to build a Docker image compatible with your system.</p></blockquote><blockquote><p>The command below downloads the  edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from , update the  variable accordingly in  before using  to start the server. For example: set <code>RAGFLOW_IMAGE=infiniflow/ragflow:v0.19.0</code> for the full edition .</p></blockquote><pre><code>$ cd ragflow/docker\n# Use CPU for embedding and DeepDoc tasks:\n$ docker compose -f docker-compose.yml up -d\n\n# To use GPU to accelerate embedding and DeepDoc tasks:\n# docker compose -f docker-compose-gpu.yml up -d\n</code></pre><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><ol start=\"4\"><li><p>Check the server status after having the server up and running:</p><pre><code>$ docker logs -f ragflow-server\n</code></pre><p><em>The following output confirms a successful launch of the system:</em></p><pre><code>\n      ____   ___    ______ ______ __\n     / __ \\ /   |  / ____// ____// /____  _      __\n    / /_/ // /| | / / __ / /_   / // __ \\| | /| / /\n   / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /\n  /_/ |_|/_/  |_|\\____//_/    /_/ \\____/ |__/|__/\n\n * Running on all addresses (0.0.0.0)\n</code></pre><blockquote><p>If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a  error because, at that moment, your RAGFlow may not be fully initialized.</p></blockquote></li><li><p>In your web browser, enter the IP address of your server and log in to RAGFlow.</p><blockquote><p>With the default settings, you only need to enter <code>http://IP_OF_YOUR_MACHINE</code> ( port number) as the default HTTP serving port  can be omitted when using the default configurations.</p></blockquote></li></ol><p>When it comes to system configurations, you will need to manage the following files:</p><ul><li><a href=\"https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env\">.env</a>: Keeps the fundamental setups for the system, such as , , and .</li><li><a href=\"https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template\">service_conf.yaml.template</a>: Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.</li></ul><p>To update the default HTTP serving port (80), go to <a href=\"https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml\">docker-compose.yml</a> and change  to .</p><p>Updates to the above configurations require a reboot of all containers to take effect:</p><blockquote><pre><code>$ docker compose -f docker-compose.yml up -d\n</code></pre></blockquote><h3>Switch doc engine from Elasticsearch to Infinity</h3><p>RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to <a href=\"https://github.com/infiniflow/infinity/\">Infinity</a>, follow these steps:</p><ol><li><p>Stop all running containers:</p><pre><code>$ docker compose -f docker/docker-compose.yml down -v\n</code></pre></li></ol><blockquote><p>[!WARNING]  will delete the docker container volumes, and the existing data will be cleared.</p></blockquote><ol start=\"2\"><li><p>Set  in  to .</p></li><li><pre><code>$ docker compose -f docker-compose.yml up -d\n</code></pre></li></ol><blockquote><p>[!WARNING] Switching to Infinity on a Linux/arm64 machine is not yet officially supported.</p></blockquote><h2>üîß Build a Docker image without embedding models</h2><p>This image is approximately 2 GB in size and relies on external LLM and embedding services.</p><pre><code>git clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build --platform linux/amd64 --build-arg LIGHTEN=1 -f Dockerfile -t infiniflow/ragflow:nightly-slim .\n</code></pre><h2>üîß Build a Docker image including embedding models</h2><p>This image is approximately 9 GB in size. As it includes embedding models, it relies on external LLM services only.</p><pre><code>git clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .\n</code></pre><h2>üî® Launch service from source for development</h2><ol><li><p>Install uv, or skip this step if it is already installed:</p><pre><code>pipx install uv pre-commit\n</code></pre></li><li><p>Clone the source code and install Python dependencies:</p><pre><code>git clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\nuv sync --python 3.10 --all-extras # install RAGFlow dependent python modules\nuv run download_deps.py\npre-commit install\n</code></pre></li><li><p>Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:</p><pre><code>docker compose -f docker/docker-compose-base.yml up -d\n</code></pre><p>Add the following line to  to resolve all hosts specified in  to :</p><pre><code>127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager\n</code></pre></li><li><p>If you cannot access HuggingFace, set the  environment variable to use a mirror site:</p><pre><code>export HF_ENDPOINT=https://hf-mirror.com\n</code></pre></li><li><p>If your operating system does not have jemalloc, please install it as follows:</p><pre><code># ubuntu\nsudo apt-get install libjemalloc-dev\n# centos\nsudo yum install jemalloc\n</code></pre></li><li><pre><code>source .venv/bin/activate\nexport PYTHONPATH=$(pwd)\nbash docker/launch_backend_service.sh\n</code></pre></li><li><p>Install frontend dependencies:</p></li><li><p><em>The following output confirms a successful launch of the system:</em></p></li><li><p>Stop RAGFlow front-end and back-end service after development is complete:</p><pre><code>pkill -f \"ragflow_server.py|task_executor.py\"\n</code></pre></li></ol><p>RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our <a href=\"https://ragflow.io/docs/dev/contributing\">Contribution Guidelines</a> first.</p>","contentLength":7330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"deepseek-ai/DeepEP","url":"https://github.com/deepseek-ai/DeepEP","date":1750214124,"author":"","guid":160072,"unread":true,"content":"<p>DeepEP: an efficient expert-parallel communication library</p><p>DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.</p><p>To align with the group-limited gating algorithm proposed in the <a href=\"https://github.com/deepseek-ai/DeepSeek-V3\">DeepSeek-V3</a> paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (Streaming Multiprocessors) number control.</p><p>For latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.</p><p>Notice: the implementation in this library may have some slight differences from the <a href=\"https://github.com/deepseek-ai/DeepSeek-V3\">DeepSeek-V3</a> paper.</p><h3>Normal kernels with NVLink and RDMA forwarding</h3><p>We test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>: with optimizations from Tencent Network Platform Department, performance was enhanced by up to 30%, see <a href=\"https://github.com/deepseek-ai/DeepEP/pull/130\">#130</a> for more details. Thanks for the contribution!</p><h3>Low-latency kernels with pure RDMA</h3><p>We test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>: low-latency kernels now leverage NVLink as much as possible, see <a href=\"https://github.com/deepseek-ai/DeepEP/pull/173\">#173</a> for more details. Thanks for the contribution!</p><ul><li>Ampere (SM80), Hopper (SM90) GPUs, or other architectures with SM90 PTX ISA support</li><li>CUDA version \n  <ul><li>CUDA 11.0 and above for SM80 GPUs</li><li>CUDA 12.3 and above for SM90 GPUs</li></ul></li><li>NVLink for intranode communication</li><li>RDMA network for internode communication</li></ul><h3>Download and install NVSHMEM dependency</h3><pre><code># Build and make symbolic links for SO files\nNVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build\n# You may modify the specific SO names according to your own platform\nln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so\n\n# Run test cases\n# NOTES: you may modify the `init_dist` function in `tests/utils.py`\n# according to your own cluster settings, and launch into multiple nodes \npython tests/test_intranode.py\npython tests/test_internode.py\npython tests/test_low_latency.py\n</code></pre><pre><code>NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install\n</code></pre><h4>Installation environment variables</h4><ul><li>: the path to the NVSHMEM directory, disable all internode and low-latency features if not specified</li><li>: 0 or 1, whether to disable SM90 features, it is required for SM90 devices or CUDA 11</li><li>: the list of target architectures, e.g. <code>TORCH_CUDA_ARCH_LIST=\"9.0\"</code></li><li><code>DISABLE_AGGRESSIVE_PTX_INSTRS</code>: 0 or 1, whether to disable aggressive load/store instructions, see <a href=\"https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/#undefined-behavior-ptx-usage\">Undefine behavior PTX usage</a> for more details</li></ul><p>Then, import  in your Python project, and enjoy!</p><p>DeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (RoCE) as well.</p><p>Traffic isolation is supported by InfiniBand through Virtual Lanes (VL).</p><p>To prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:</p><ul><li>workloads using normal kernels</li><li>workloads using low-latency kernels</li></ul><p>For DeepEP, you can control the virtual lane assignment by setting the  environment variable.</p><p>Adaptive routing is an advanced routing feature provided by InfiniBand switches that can evenly distribute traffic across multiple paths. Enabling adaptive routing can completely eliminate network congestion caused by routing conflicts, but it also introduces additional latency. We recommend the following configuration for optimal performance:</p><ul><li>enable adaptive routing in environments with heavy network loads</li><li>use static routing in environments with light network loads</li></ul><p>Congestion control is disabled as we have not observed significant congestion in our production environment.</p><h3>Example use in model training or inference prefilling</h3><p>The normal kernels can be used in model training or the inference prefilling phase (without the backward part) as the below example code shows.</p><pre><code>import torch\nimport torch.distributed as dist\nfrom typing import List, Tuple, Optional, Union\n\nfrom deep_ep import Buffer, EventOverlap\n\n# Communication buffer (will allocate at runtime)\n_buffer: Optional[Buffer] = None\n\n# Set the number of SMs to use\n# NOTES: this is a static variable\nBuffer.set_num_sms(24)\n\n\n# You may call this function at the framework initialization\ndef get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -&gt; Buffer:\n    global _buffer\n    \n    # NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests\n    num_nvl_bytes, num_rdma_bytes = 0, 0\n    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):\n        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)\n        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)\n\n    # Allocate a buffer if not existed or not enough buffer size\n    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes &lt; num_nvl_bytes or _buffer.num_rdma_bytes &lt; num_rdma_bytes:\n        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)\n    return _buffer\n\n\ndef get_hidden_bytes(x: torch.Tensor) -&gt; int:\n    t = x[0] if isinstance(x, tuple) else x\n    return t.size(1) * max(t.element_size(), 2)\n\n\ndef dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n                     topk_idx: torch.Tensor, topk_weights: torch.Tensor,\n                     num_experts: int, previous_event: Optional[EventOverlap] = None) -&gt; \\\n        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:\n    # NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency \n    # of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please\n    # refer to the docs of `Buffer.dispatch`\n    global _buffer\n\n    # Calculate layout before actual dispatch\n    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \\\n        _buffer.get_dispatch_layout(topk_idx, num_experts,\n                                    previous_event=previous_event, async_finish=True,\n                                    allocate_on_comm_stream=previous_event is not None)\n    # Do MoE dispatch\n    # NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph\n    # Unless you specify `num_worst_tokens`, but this flag is for intranode only\n    # For more advanced usages, please refer to the docs of the `dispatch` function\n    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \\\n        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,\n                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,\n                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert,\n                         previous_event=previous_event, async_finish=True,\n                         allocate_on_comm_stream=True)\n    # For event management, please refer to the docs of the `EventOverlap` class\n    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event\n\n\ndef dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -&gt; \\\n        Tuple[torch.Tensor, torch.Tensor, EventOverlap]:\n    global _buffer\n\n    # The backward process of MoE dispatch is actually a combine\n    # For more advanced usages, please refer to the docs of the `combine` function\n    combined_grad_x, combined_grad_recv_topk_weights, event = \\\n        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)\n\n    # For event management, please refer to the docs of the `EventOverlap` class\n    return combined_grad_x, combined_grad_recv_topk_weights, event\n\n\ndef combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -&gt; \\\n        Tuple[torch.Tensor, EventOverlap]:\n    global _buffer\n\n    # Do MoE combine\n    # For more advanced usages, please refer to the docs of the `combine` function\n    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,\n                                           allocate_on_comm_stream=previous_event is not None)\n\n    # For event management, please refer to the docs of the `EventOverlap` class\n    return combined_x, event\n\n\ndef combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n                     handle: Tuple, previous_event: Optional[EventOverlap] = None) -&gt; \\\n        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:\n    global _buffer\n\n    # The backward process of MoE combine is actually a dispatch\n    # For more advanced usages, please refer to the docs of the `dispatch` function\n    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,\n                                                 previous_event=previous_event,\n                                                 allocate_on_comm_stream=previous_event is not None)\n\n    # For event management, please refer to the docs of the `EventOverlap` class\n    return grad_x, event\n</code></pre><p>Moreover, inside the dispatch function, we may not know how many tokens to receive for the current rank. So an implicit CPU wait for GPU received count signal will be involved, as the following figure shows.</p><h3>Example use in inference decoding</h3><p>The low latency kernels can be used in the inference decoding phase as the below example code shows.</p><pre><code>import torch\nimport torch.distributed as dist\nfrom typing import Tuple, Optional\n\nfrom deep_ep import Buffer\n\n# Communication buffer (will allocate at runtime)\n# NOTES: there is no SM control API for the low-latency kernels\n_buffer: Optional[Buffer] = None\n\n\n# You may call this function at the framework initialization\ndef get_buffer(group: dist.ProcessGroup, num_max_dispatch_tokens_per_rank: int, hidden: int, num_experts: int) -&gt; Buffer:\n    # NOTES: the low-latency mode will consume much more space than the normal mode\n    # So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256\n    global _buffer\n    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts)\n\n    # Allocate a buffer if not existed or not enough buffer size\n    if _buffer is None or _buffer.group != group or not _buffer.low_latency_mode or _buffer.num_rdma_bytes &lt; num_rdma_bytes:\n        # NOTES: for the best performance, the QP number **must** be equal to the number of the local experts\n        assert num_experts % group.size() == 0\n        _buffer = Buffer(group, 0, num_rdma_bytes, low_latency_mode=True, num_qps_per_rank=num_experts // group.size())\n    return _buffer\n\n\ndef low_latency_dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, num_max_dispatch_tokens_per_rank: int, num_experts: int):\n    global _buffer\n\n    # Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)\n    recv_hidden_states, recv_expert_count, handle, event, hook = \\\n        _buffer.low_latency_dispatch(hidden_states, topk_idx, num_max_dispatch_tokens_per_rank, num_experts,\n                                     async_finish=False, return_recv_hook=True)\n\n    # NOTES: the actual tensor will not be received only if you call `hook()`,\n    # it is useful for double-batch overlapping, but **without any SM occupation**\n    # If you don't want to overlap, please set `return_recv_hook=False`\n    # Later, you can use our GEMM library to do the computation with this specific format\n    return recv_hidden_states, recv_expert_count, handle, event, hook\n\n\ndef low_latency_combine(hidden_states: torch.Tensor,\n                        topk_idx: torch.Tensor, topk_weights: torch.Tensor, handle: Tuple):\n    global _buffer\n\n    # Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)\n    combined_hidden_states, event_overlap, hook = \\\n        _buffer.low_latency_combine(hidden_states, topk_idx, topk_weights, handle,\n                                    async_finish=False, return_recv_hook=True)\n\n    # NOTES: the same behavior as described in the dispatch kernel\n    return combined_hidden_states, event_overlap, hook\n</code></pre><p>For two-micro-batch overlapping, you can refer to the following figure. With our receiving hook interface, the RDMA network traffic is happening in the background, without costing any GPU SMs from the computation part. But notice, the overlapped parts can be adjusted, i.e., the 4 parts of attention/dispatch/MoE/combine may not have the exact same execution time. You may adjust the stage settings according to your workload.</p><h4>Easier potential overall design</h4><p>The current DeepEP implementation uses queues for communication buffers which save memory but introduce complexity and potential deadlocks. If you're implementing your own version based on DeepEP, consider using fixed-size buffers allocated to maximum capacity for simplicity and better performance. For a detailed discussion of this alternative approach, see <a href=\"https://github.com/deepseek-ai/DeepEP/issues/39\">https://github.com/deepseek-ai/DeepEP/issues/39</a>.</p><h4>Undefined-behavior PTX usage</h4><ul><li>For extreme performance, we discover and use an undefined-behavior PTX usage: using read-only PTX <code>ld.global.nc.L1::no_allocate.L2::256B</code> to . The PTX modifier  indicates that a non-coherent cache is used. But the correctness is tested to be guaranteed with  on Hopper architectures, and performance will be much better. The reason we guess may be: the non-coherent cache is unified with L1, and the L1 modifier is not just a hint but a strong option, so that the correctness can be guaranteed by no dirty data in L1.</li><li>Initially, because NVCC could not automatically unroll volatile read PTX, we tried using  (i.e., ). Even compared to manually unrolled volatile reads, it was significantly faster (likely due to additional compiler optimizations). However, the results could be incorrect or dirty. After consulting the PTX documentation, we discovered that L1 and non-coherent cache are unified on Hopper architectures. We speculated that  might resolve the issue, leading to this discovery.</li><li>If you find kernels not working on some other platforms, you may add <code>DISABLE_AGGRESSIVE_PTX_INSTRS=1</code> to  and disable this, or file an issue.</li></ul><h4>Auto-tuning on your cluster</h4><p>For better performance on your cluster, we recommend to run all the tests and use the best auto-tuned configuration. The default configurations are optimized on the DeepSeek's internal cluster.</p><p>This code repository is released under <a href=\"https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/LICENSE\">the MIT License</a>, except for codes that reference NVSHMEM (including <code>csrc/kernels/ibgda_device.cuh</code> and <code>third-party/nvshmem.patch</code>), which are subject to <a href=\"https://docs.nvidia.com/nvshmem/api/sla.html\">NVSHMEM SLA</a>.</p><p>If you use this codebase or otherwise find our work valuable, please cite:</p><pre><code>@misc{deepep2025,\n      title={DeepEP: an efficient expert-parallel communication library},\n      author={Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao},\n      year={2025},\n      publisher = {GitHub},\n      howpublished = {\\url{https://github.com/deepseek-ai/DeepEP}},\n}\n</code></pre>","contentLength":16147,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"immich-app/immich","url":"https://github.com/immich-app/immich","date":1750127771,"author":"","guid":157721,"unread":true,"content":"<p>High performance self-hosted photo and video management solution.</p><h3 align=\"center\">High performance self-hosted photo and video management solution</h3><a href=\"https://immich.app\"><img src=\"https://raw.githubusercontent.com/immich-app/immich/main/design/immich-screenshots.png\" title=\"Main Screenshot\"></a><ul><li>‚ö†Ô∏è The project is under  development.</li><li>‚ö†Ô∏è Expect bugs and breaking changes.</li><li>‚ö†Ô∏è <strong>Do not use the app as the only way to store your photos and videos.</strong></li><li>‚ö†Ô∏è Always follow <a href=\"https://www.backblaze.com/blog/the-3-2-1-backup-strategy/\">3-2-1</a> backup plan for your precious photos and videos!</li></ul><blockquote><p>[!NOTE] You can find the main documentation, including installation guides, at <a href=\"https://immich.app/\">https://immich.app/</a>.</p></blockquote><p>Access the demo <a href=\"https://demo.immich.app\">here</a>. For the mobile app, you can use  for the .</p><table><tbody><tr><td align=\"left\">Upload and view videos and photos</td></tr><tr><td align=\"left\">Auto backup when the app is opened</td></tr><tr><td align=\"left\">Prevent duplication of assets</td></tr><tr><td align=\"left\">Selective album(s) for backup</td></tr><tr><td align=\"left\">Download photos and videos to local device</td></tr><tr></tr><tr><td align=\"left\">Scrubbable/draggable scrollbar</td></tr><tr></tr><tr><td align=\"left\">Metadata view (EXIF, map)</td></tr><tr><td align=\"left\">Search by metadata, objects, faces, and CLIP</td></tr><tr><td align=\"left\">Administrative functions (user management)</td></tr><tr><td align=\"left\">LivePhoto/MotionPhoto backup and playback</td></tr><tr><td align=\"left\">Support 360 degree image display</td></tr><tr><td align=\"left\">User-defined storage structure</td></tr><tr></tr><tr><td align=\"left\">Facial recognition and clustering</td></tr><tr></tr></tbody></table><p>Read more about translations <a href=\"https://immich.app/docs/developer/translations\">here</a>.</p><a href=\"https://hosted.weblate.org/engage/immich/\"><img src=\"https://hosted.weblate.org/widget/immich/immich/multi-auto.svg?sanitize=true\" alt=\"Translation status\"></a><a href=\"https://star-history.com/#immich-app/immich&amp;Date\"></a><a href=\"https://github.com/alextran1502/immich/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=immich-app/immich\" width=\"100%\"></a>","contentLength":1000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"panaversity/learn-agentic-ai","url":"https://github.com/panaversity/learn-agentic-ai","date":1750127771,"author":"","guid":157722,"unread":true,"content":"<p>Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern and Agent-Native Cloud Technologies: OpenAI Agents SDK, Memory, MCP, A2A, Knowledge Graphs, Dapr, Rancher Desktop, and Kubernetes.</p><p>We have Two Hunches, the future of Pakistan depends on it, let's make sure that we are not wrong:</p><p>It is very important for Pakistan that we bet on the right horses for the upcoming age of Agentic AI. We will be training millions of Agentic AI Developers all over Pakistan and online around the world and building startups, we cant afford to be wrong.</p><p>Hunch #1: Dapr We feel Dapr, Dapr Actors, Dapr Workflows, and Dapr Agents will be the core technology in building the next generation multi ai agentic systems, is my hunch correct?</p><p>Hunch #2: OpenAI Agents SDK We also have a hunch that OpenAI Agents SDK will be the go to framework for beginners to start learning Agentic AI?</p><p>Let us see what the best AI has to say about our hunches:</p><h2>This Panaversity Initiative Tackles the Critical Challenge:</h2><p><strong>‚ÄúHow do we design AI Agents that can handle 10 million concurrent AI Agents without failing?‚Äù</strong></p><p>Note: The challenge is intensified as we must guide our students to solve this issue with minimal financial resources available during training.</p><p>Kubernetes with Dapr can theoretically handle 10 million concurrent agents in an agentic AI system without failing, but achieving this requires extensive optimization, significant infrastructure, and careful engineering. While direct evidence at this scale is limited, logical extrapolation from existing benchmarks, Kubernetes‚Äô scalability, and Dapr‚Äôs actor model supports feasibility, especially with rigorous tuning and resource allocation.</p><p><strong>Condensed Argument with Proof and Logic</strong>:</p><ol><li><ul><li>: Kubernetes supports up to 5,000 nodes and 150,000 pods per cluster (Kubernetes docs), with real-world examples like PayPal scaling to 4,000 nodes and 200,000 pods (InfoQ, 2023) and KubeEdge managing 100,000 edge nodes and 1 million pods (KubeEdge case studies). OpenAI‚Äôs 2,500-node cluster for AI workloads (OpenAI blog, 2022) shows Kubernetes can handle compute-intensive tasks.</li><li>: For 10 million users, a cluster of 5,000‚Äì10,000 nodes (e.g., AWS g5 instances with GPUs) can distribute workloads. Each node can run hundreds of pods, and Kubernetes‚Äô horizontal pod autoscaling (HPA) dynamically adjusts to demand. Bottlenecks (e.g., API server, networking) can be mitigated by tuning etcd, using high-performance CNIs like Cilium, and optimizing DNS.</li></ul></li><li><p><strong>Dapr‚Äôs Efficiency for Agentic AI</strong>:</p><ul><li>: Dapr‚Äôs actor model supports thousands of virtual actors per CPU core with double-digit millisecond latency (Dapr docs, 2024). Case studies show Dapr handling millions of events, e.g., Tempestive‚Äôs IoT platform processing billions of messages (Dapr blog, 2023) and DeFacto‚Äôs system managing 3,700 events/second (320 million daily) on Kubernetes with Kafka (Microsoft case study, 2022).</li><li>: Agentic AI relies on stateful, low-latency agents. Dapr Agents, built on the actor model, can represent 10 million users as actors, distributed across a Kubernetes cluster. Dapr‚Äôs state management (e.g., Redis) and pub/sub messaging (e.g., Kafka) ensure efficient coordination and resilience, with automatic retries preventing failures. Sharding state stores and message brokers scales to millions of operations/second.</li></ul></li><li><ul><li>: LLM inference frameworks like vLLM and TGI serve thousands of requests/second per GPU (vLLM benchmarks, 2024). Kubernetes orchestrates GPU workloads effectively, as seen Kubernetes manages GPU workloads, as seen in NVIDIA‚Äôs AI platform scaling to thousands of GPUs (NVIDIA case study, 2023).</li><li>: Assuming each user generates 1 request/second requiring 0.01 GPU, 10 million users need ~100,000 GPUs. Batching, caching, and model parallelism reduce this to a feasible ~10,000‚Äì20,000 GPUs, achievable in hyperscale clouds (e.g., AWS). Kubernetes‚Äô resource scheduling ensures optimal GPU utilization.</li></ul></li><li><ul><li>: EMQX on Kubernetes handled 1 million concurrent connections with tuning (EMQX blog, 2024). C10M benchmarks (2013) achieved 10 million connections using optimized stacks. Dapr‚Äôs state stores (e.g., Redis) support millions of operations/second (Redis benchmarks, 2024).</li><li>: 10 million connections require ~100‚Äì1,000 Gbps bandwidth, supported by modern clouds. High-throughput databases (e.g., CockroachDB) and caching (e.g., Redis Cluster) handle 10 TB of state data for 10 million users (1 KB/user). Kernel bypass (e.g., DPDK) and eBPF-based CNIs (e.g., Cilium) minimize networking latency.</li></ul></li><li><p><strong>Resilience and Monitoring</strong>:</p><ul><li>: Dapr‚Äôs resiliency policies (retries, circuit breakers) and Kubernetes‚Äô self-healing (pod restarts) ensure reliability (Dapr docs, 2024). Dapr‚Äôs OpenTelemetry integration scales monitoring for millions of agents (Prometheus case studies, 2023).</li><li>: Real-time metrics (e.g., latency, error rates) and distributed tracing prevent cascading failures. Kubernetes‚Äô liveness probes and Dapr‚Äôs workflow engine recover from crashes, ensuring 99.999% uptime.</li></ul></li></ol><p><strong>Feasibility with Constraints</strong>:</p><ul><li>: No direct benchmark exists for 10 million concurrent users with Dapr/Kubernetes in an agentic AI context. Infrastructure costs (e.g., $10M‚Äì$100M for 10,000 nodes) are prohibitive for low-budget scenarios.</li><li>: Use open-source tools (e.g., Minikube, kind) for local testing and cloud credits (e.g., AWS Educate) for students. Simulate 10 million users with tools like Locust on smaller clusters (e.g., 100 nodes), extrapolating results. Optimize Dapr‚Äôs actor placement and Kubernetes‚Äô resource quotas to maximize efficiency on limited hardware. Leverage free-tier databases (e.g., MongoDB Atlas) and message brokers (e.g., RabbitMQ).</li></ul><p>: Kubernetes with Dapr can handle 10 million concurrent users in an agentic AI system, supported by their proven scalability, real-world case studies, and logical extrapolation. For students with minimal budgets, small-scale simulations, open-source tools, and cloud credits make the problem tractable, though production-scale deployment requires hyperscale resources and expertise.</p><p><strong>Agentic AI Top Trend of 2025</strong></p><h2>The Dapr Agentic Cloud Ascent (DACA) Design Pattern Addresses 10 Million AI Agents Challenge</h2><p>Let's understand and learn about \"Dapr Agentic Cloud Ascent (DACA)\", our winning design pattern for developing and deploying planet scale multi-agent systems.</p><h3>Executive Summary: Dapr Agentic Cloud Ascent (DACA)</h3><p>The Dapr Agentic Cloud Ascent (DACA) guide introduces a strategic design pattern for building and deploying sophisticated, scalable, and resilient agentic AI systems. Addressing the complexities of modern AI development, DACA integrates the OpenAI Agents SDK for core agent logic with the Model Context Protocol (MCP) for standardized tool use and the Agent2Agent (A2A) protocol for seamless inter-agent communication, all underpinned by the distributed capabilities of Dapr. <strong>Grounded in AI-first and cloud-first principles</strong>, DACA promotes the use of stateless, containerized applications deployed on platforms like Azure Container Apps (Serverless Containers) or Kubernetes, enabling efficient scaling from local development to planetary-scale production, potentially leveraging free-tier cloud services and self-hosted LLMs for cost optimization. The pattern emphasizes modularity, context-awareness, and standardized communication, envisioning an  where diverse AI agents collaborate intelligently. Ultimately, DACA offers a robust, flexible, and cost-effective framework for developers and architects aiming to create complex, cloud-native agentic AI applications that are built for scalability and resilience from the ground up.</p><ul><li><strong>Agentic AI Developer and AgentOps Professionals</strong></li></ul><h3>Why OpenAI Agents SDK should be the main framework for agentic development for most use cases?</h3><p><strong>Table 1: Comparison of Abstraction Levels in AI Agent Frameworks</strong></p><table><thead><tr></tr></thead><tbody><tr><td>Python-first, core primitives (Agents, Handoffs, Guardrails), direct control</td></tr><tr><td>Role-based agents, crews, tasks, focus on collaboration</td></tr><tr><td>Conversational agents, flexible conversation patterns, human-in-the-loop support</td></tr><tr><td>Multi-agent hierarchies, Google Cloud integration (Gemini, Vertex AI), rich tool ecosystem, bidirectional streaming</td></tr><tr><td>Graph-based workflows, nodes, edges, explicit state management</td></tr><tr><td>Stateful virtual actors, event-driven multi-agent workflows, Kubernetes integration, 50+ data connectors, built-in resiliency</td></tr></tbody></table><p>The table clearly identifies why OpenAI Agents SDK should be the main framework for agentic development for most use cases:</p><ul><li>It excels in  and , making it the best choice for rapid development and broad accessibility.</li><li>It offers  with , providing the flexibility needed for agentic development without the complexity of frameworks like LangGraph.</li><li>It outperforms most alternatives (CrewAI, AutoGen, Google ADK, Dapr Agents) in balancing usability and power, and while LangGraph offers more control, its complexity makes it less practical for general use.</li></ul><p>If your priority is ease of use, flexibility, and quick iteration in agentic development, OpenAI Agents SDK is the clear winner based on the table. However, if your project requires enterprise-scale features (e.g., Dapr Agents) or maximum control for complex workflows (e.g., LangGraph), you might consider those alternatives despite their added complexity.</p><h2>Core DACA Agentic AI Courses:</h2><h3>AI-201: Fundamentals of Agentic AI and DACA AI-First Development (14 weeks)</h3><ul><li>‚Å†Agentic &amp; DACA Theory - 1 week</li><li>UV &amp; ‚Å†OpenAI Agents SDK - 5 weeks</li><li>‚Å†Agentic Design Patterns - 2 weeks</li><li>‚Å†Memory [LangMem &amp; mem0] 1 week</li><li>Postgres/Redis (Managed Cloud) - 1 week</li><li>FastAPI (Basic) - 2 weeks</li><li>‚Å†Containerization (Rancher Desktop) - 1 week</li><li>Hugging Face Docker Spaces - 1 week</li></ul><p>Note: These videos are for additional learning, and do not cover all the material taught in the onsite classes.</p><h3>AI-202: DACA Cloud-First Agentic AI Development (14 weeks)</h3><ul><li>Rancher Desktop with Local Kubernetes - 4 weeks</li><li>Advanced FastAPI with Kubernetes - 2 weeks</li><li>Dapr [workflows, state, pubsub, secrets] - 3 Week</li><li>CockRoachdb &amp; RabbitMQ Managed Services - 2 weeks</li><li>‚Å†Model Context Protocol - 2 weeks</li><li>‚Å†Serverless Containers Deployment (ACA) - 2 weeks</li></ul><p>Prerequisite: Successful completion of AI-201</p><h3>AI-301 DACA Planet-Scale Distributed AI Agents (14 Weeks)</h3><ul><li>‚Å†Certified Kubernetes Application Developer (CKAD) - 4 weeks</li><li>‚Å†Dapr Agents/Google ADK - 2 weeks</li><li>‚Å†Self-LLMs Hosting - 1 week</li><li>Finetuning LLMs - 3 weeks</li></ul><p>Prerequisite: Successful completion of AI-201 &amp; AI-202</p><p>Quizzes + Hackathons (Everything is Onsite)</p><ol><li>Advanced Modern Python (including asyncio) [Q1]</li><li>OpenAI Agents SDK (48 MCQ in 2 hour) [01_ai_agents_first]</li><li>Protocols &amp; Design Patterns (A2A and MCP) [05_ai_protocols]</li><li>Hackathon1 - 8 Hours (Using Above Quiz Stack)</li><li>Containerization + FastAPI [05_daca_agent_native_dev = 01 + 02 ]</li><li>Kubernetes (Rancher Desktop) [Stimulations] [05_daca_agent_native_dev = 02 ]</li><li>Dapr-1 - State, PubSub, Bindings, Invocation [05_daca_agent_native_dev = 03 ]</li><li>Dapr-2 - Workflows, Virtual Actors [04_agent_native = 04, 05, 06]</li><li>Hackathon2 - 8 Hours (Agent Native Startup)</li><li>CKAD + DAPR + ArgoCD (Simulations) [06_daca_deployment_guide + 07_ckad]</li></ol><h3>Fundamentals of Agentic AI Quiz</h3><p>Difficulty Level: Intermediate or Advanced (NOT beginner-level)</p><p>This is a well-constructed, comprehensive quiz that accurately tests deep knowledge of the OpenAI Agents SDK. However, it's significantly more challenging than typical beginner-level assessments.</p><p><strong>Difficulty Level for Beginners</strong></p><p>The quiz is challenging for beginners due to the following factors:</p><ul><li><p>: Questions require understanding the OpenAI Agents SDK‚Äôs architecture (e.g., Agents, Tools, Handoffs, Runner), Pydantic models, async programming, and prompt engineering. These are advanced topics for someone new to AI or Python.</p></li><li><p>: Topics like dynamic instructions, context management, error handling, and Chain-of-Thought prompting require familiarity with both theoretical and practical aspects of agentic AI.</p></li><li><p>: Many questions involve analyzing code snippets, understanding execution paths, and predicting outcomes, which demand strong Python and debugging skills. Domain Knowledge: Questions on Markdown are simpler, but the majority focus on niche SDK features, making the quiz specialized.</p></li><li><p>: Beginners (e.g., those with basic Python knowledge and minimal AI experience) would struggle with SDK-specific concepts like Runner.run_sync, tool_choice, and Pydantic validation, as well as async programming and multi-agent workflows.</p></li><li><p>: Advanced (not beginner-friendly). Beginners would need foundational knowledge in Python, async programming, and LLMs, plus specific training on the OpenAI Agents SDK to perform well.</p></li></ul><p>To excel in this quiz, focus on understanding the core components and philosophy of the OpenAI Agents SDK, such as its \"Python-first\" design for orchestration, the roles of Agents and Tools, and how primitives like \"Handoffs\" facilitate multi-agent collaboration. Pay close attention to how the SDK manages the agent loop, handles tool calls and Pydantic models for typed inputs/outputs, and uses context objects. Review concepts like dynamic instructions, agent cloning, error handling during tool execution, and the nuances of Runner.run_sync() versus streaming. Additionally, refresh your knowledge of prompt engineering techniques, including crafting clear instructions, guiding the agent's reasoning (e.g., Chain-of-Thought), and managing sensitive data through persona and careful prompting. Finally, ensure you're comfortable with basic Markdown syntax for links and images.</p><p><strong>Preparation Guide for Beginner Students</strong></p><p>This OpenAI Agents SDK quiz is designed for intermediate to advanced learners and requires substantial preparation to succeed. Before attempting this assessment, ensure you have a solid foundation in Python programming, including object-oriented concepts, async/await patterns, decorators, and error handling. You'll need to thoroughly study Pydantic models for data validation, understanding field definitions, default values, and validation behavior. Dedicate significant time to the OpenAI Agents SDK documentation (<a href=\"https://openai.github.io/openai-agents-python/\">https://openai.github.io/openai-agents-python/</a>), focusing on core concepts like Agents, Tools, Handoffs, context management, and the agent execution loop. Practice writing and analyzing code that uses the @function_tool decorator, Runner.run_sync(), agent cloning, and multi-agent orchestration patterns. Review prompt engineering techniques from the OpenAI cookbook, particularly Chain-of-Thought prompting, system message design, and handling sensitive data. Finally, familiarize yourself with basic Markdown syntax for links and images. Plan to spend at least 2-3 weeks studying these materials, complete hands-on coding exercises with the SDK. Consider this quiz a capstone assessment that requires comprehensive understanding rather than a beginner-level introduction to the concepts.</p><p><strong>You Can Generate Mock Quizzes for Practice using LLMs from this Prompt:</strong></p><p>Create a comprehensive quiz covering OpenAI Agents SDK. It should include as many MCQ Quiz Questions as required to test the material, the questions should be difficult and at the graduate level and should test both concepts and include code were required. From the following following documentation:</p>","contentLength":15113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/fluentui-system-icons","url":"https://github.com/microsoft/fluentui-system-icons","date":1750127771,"author":"","guid":157723,"unread":true,"content":"<p>Fluent System Icons are a collection of familiar, friendly and modern icons from Microsoft.</p><p>Fluent UI System Icons are a collection of familiar, friendly and modern icons from Microsoft.</p><p>Within the metadata.json file for an icon, a property named  is used to indicate the direction of the icon. This property can have one of the following values:</p><ul><li>, meaning that the icon is unique and has a specific RTL and LTR version</li><li>, meaning that the icon can be mirrored for RTL or LTR languages</li></ul><p>The property  is also used to indicate the default direction that should be used for the icon.</p><p>The library is published via Maven Central, please ensure that the  repository has been added to the root  file:</p><pre><code>repositories {\n    ...\n    mavenCentral()\n}\n</code></pre><p>Include the following dependency in your project's :</p><pre><code>implementation 'com.microsoft.design:fluent-system-icons:1.1.304@aar'\n</code></pre><pre><code>use_frameworks!\n\npod \"FluentIcons\", \"1.1.304\"\n</code></pre><pre><code>git \"git@github.com:microsoft/fluentui-system-icons.git\" \"1.1.304\"\n</code></pre><p>In the  of your flutter project, add the following dependency:</p><pre><code>dependencies:\n  ...\n  fluentui_system_icons: ^1.1.304\n</code></pre><p>The importer generates the Android and iOS libraries from the icons in the  directory.</p><p>Install npm dependencies:</p><pre><code>npm install\nnpm run clean\n</code></pre><p>List all the available commands:</p><p>Our <a href=\"https://github.com/microsoft/fluentui-system-icons/actions\">build pipeline</a> runs  and  to create the libraries. The build definitions are located in .</p><p>You can build and run the demo apps following the steps below.</p><ol><li>Follow the  section above and run the command </li><li>Open the <a href=\"https://raw.githubusercontent.com/microsoft/fluentui-system-icons/main/android\">android</a> directory in Android Studio</li><li>Select the  in the build configuration dropdown</li></ol><p>Prerequisite: Make sure you have flutter configured in Android Studio</p><ol><li>Open the <a href=\"https://raw.githubusercontent.com/microsoft/fluentui-system-icons/main/flutter\">flutter</a> directory in Android Studio</li><li>Select the  in the directory and open it in Android Studio</li></ol><p>Please feel free to <a href=\"https://github.com/microsoft/fluentui-system-icons/issues/new\">open a GitHub issue</a> and assign to the following points of contact with questions or requests.</p>","contentLength":1826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"anthropics/anthropic-cookbook","url":"https://github.com/anthropics/anthropic-cookbook","date":1750127771,"author":"","guid":157724,"unread":true,"content":"<p>A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.</p><p>The Anthropic Cookbook provides code and guides designed to help developers build with Claude, offering copy-able code snippets that you can easily integrate into your own projects.</p><p>To make the most of the examples in this cookbook, you'll need an Anthropic API key (sign up for free <a href=\"https://www.anthropic.com\">here</a>).</p><p>While the code examples are primarily written in Python, the concepts can be adapted to any programming language that supports interaction with the Anthropic API.</p><p>Looking for more resources to enhance your experience with Claude and AI assistants? Check out these helpful links:</p><p>The Anthropic Cookbook thrives on the contributions of the developer community. We value your input, whether it's submitting an idea, fixing a typo, adding a new guide, or improving an existing one. By contributing, you help make this resource even more valuable for everyone.</p><p>To avoid duplication of efforts, please review the existing issues and pull requests before contributing.</p><p>If you have ideas for new examples or guides, share them on the <a href=\"https://github.com/anthropics/anthropic-cookbook/issues\">issues page</a>.</p><ul><li><a href=\"https://github.com/aws-samples/anthropic-on-aws\">Anthropic on AWS</a>: Explore examples and solutions for using Claude on AWS infrastructure.</li><li><a href=\"https://github.com/aws-samples/\">AWS Samples</a>: A collection of code samples from AWS which can be adapted for use with Claude. Note that some samples may require modification to work optimally with Claude.</li></ul>","contentLength":1371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shubhamsaboo/awesome-llm-apps","url":"https://github.com/Shubhamsaboo/awesome-llm-apps","date":1750127771,"author":"","guid":157725,"unread":true,"content":"<p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p><p>A curated collection of <strong>Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.</strong> This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.</p><ul><li>üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.</li><li>üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.</li><li>üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.</li></ul><h3>üéÆ Autonomous Game Playing Agents</h3><h3>üìÄ RAG (Retrieval Augmented Generation)</h3><h3>üíæ LLM Apps with Memory Tutorials</h3><h3>üîß LLM Fine-tuning Tutorials</h3><ol><li><pre><code>git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n</code></pre></li><li><p><strong>Navigate to the desired project directory</strong></p><pre><code>cd awesome-llm-apps/starter_ai_agents/ai_travel_agent\n</code></pre></li><li><p><strong>Install the required dependencies</strong></p><pre><code>pip install -r requirements.txt\n</code></pre></li><li><p><strong>Follow the project-specific instructions</strong> in each project's  file to set up and run the app.</p></li></ol><h2>ü§ù Contributing to Open Source</h2><p>Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new <a href=\"https://github.com/Shubhamsaboo/awesome-llm-apps/issues\">GitHub Issue</a> or submit a pull request. Make sure to follow the existing project structure and include a detailed  for each new app.</p><h3>Thank You, Community, for the Support! üôè</h3><p>üåü <strong>Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.</strong></p>","contentLength":1699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"gtsteffaniak/filebrowser","url":"https://github.com/gtsteffaniak/filebrowser","date":1750127771,"author":"","guid":157726,"unread":true,"content":"<blockquote><p>[!WARNING] There is no stable version --  planned for 2025. (<a href=\"https://github.com/gtsteffaniak/filebrowser/discussions/628\">Read more</a>)</p></blockquote><p>FileBrowser Quantum provides an easy way to access and manage your files from the web. It has has a web page interface that allows you to create secure shared links, users with their own specific permissions and settings, and offers a great viewing experience for many file types.</p><p>This version is called \"Quantum\" because it packs tons of advanced features into a tiny easy to run file. Unlike the majority of alternative options, FileBrowser Quantum is simple to install and easy to configure.</p><p>The goal for this repo is to become the best open-source self-hosted file browsing application that exists -- . This repo will always be free and open-source.</p><p>FileBrowser Quantum is a massive fork of the file browser open-source project with the following changes:</p><ol><li>‚úÖ Multiple sources support</li><li>‚úÖ Login support for OIDC, password + 2FA, and proxy.</li><li>‚úÖ Simplified configuration via  config file.</li><li>‚úÖ Ultra-efficient <a href=\"https://github.com/gtsteffaniak/filebrowser/wiki/Indexing\">indexing</a> and real-time updates \n  <ul><li>Real-time search results as you type.</li><li>Real-time monitoring and updates in the UI.</li><li>Search supports file and folder sizes, along with various filters.</li></ul></li><li>‚úÖ Better listing browsing \n  <ul><li>More file type previews, such as  and  file previews</li><li>Instantly switches view modes and sort order without reloading data.</li><li>Folder sizes are displayed.</li><li>Navigating remembers the last scroll position.</li></ul></li><li>‚úÖ Developer API support \n  <ul><li>Ability to create long-lived API Tokens.</li><li>A helpful Swagger page is available at  endpoint for API enabled users.</li></ul></li></ol><p>Notable features that this fork  have (removed):</p><ul><li> jobs are not supported yet.</li><li> rules are not supported yet.</li><li>‚ùå shell commands are completely removed and will not be returned.</li></ul><blockquote><p>[!WARNING] Every file and directory in the source gets indexed (by default). This enables powerful features such as instant search, but large source filesystems can increase your system requirements. <a href=\"https://github.com/gtsteffaniak/filebrowser/wiki/Indexing\">See indexing wiki</a> for more info.</p></blockquote><p>FileBrowser Quantum differs significantly from the original version. Many of these changes required a significant overhaul. Creating a fork was a necessary process to make the program better. There have been many growing pains, but a stable release is planned and coming soon.</p><p>The UI has a simple three-component navigation system:</p><ol><li>(Left) Multi-action button with slide-out panel.</li><li>(Middle) The powerful search bar.</li><li>(Right) The view change toggle.</li></ol><p>All other functions are moved either into the action menu or pop-up menus. If the action does not depend on context, it will exist in the slide-out action panel. If the action is available based on context, it will show up as a pop-up menu.</p><h2>Install and Configuration</h2><h2>Migration from the original filebrowser</h2>","contentLength":2669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"zen-browser/desktop","url":"https://github.com/zen-browser/desktop","date":1750127771,"author":"","guid":157727,"unread":true,"content":"<p>Welcome to a calmer internet</p><img src=\"https://raw.githubusercontent.com/zen-browser/desktop/dev/docs/assets/zen-dark.svg?sanitize=true\" width=\"100px\" align=\"left\"><p>Zen is a firefox-based browser with the aim of pushing your productivity to a new level!</p><ul><li><a href=\"https://zen-browser.app/download\"></a> - Is currently built using Firefox version ! üöÄ</li><li><a href=\"https://zen-browser.app/download?twilight\"></a> - Is currently built using Firefox version !</li></ul><p>Zen is an open-source project, and we welcome contributions from the community! Please take a look at the <a href=\"https://raw.githubusercontent.com/zen-browser/desktop/dev/docs/contribute.md\">contribution guidelines</a> before getting started!</p>","contentLength":364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"linshenkx/prompt-optimizer","url":"https://github.com/linshenkx/prompt-optimizer","date":1750127771,"author":"","guid":157728,"unread":true,"content":"<p>Prompt OptimizerÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑAIÊèêÁ§∫ËØç‰ºòÂåñÂ∑•ÂÖ∑ÔºåÂ∏ÆÂä©‰Ω†ÁºñÂÜôÊõ¥Â•ΩÁöÑAIÊèêÁ§∫ËØçÔºåÊèêÂçáAIËæìÂá∫Ë¥®Èáè„ÄÇÊîØÊåÅWebÂ∫îÁî®ÂíåChromeÊèí‰ª∂‰∏§Áßç‰ΩøÁî®ÊñπÂºè„ÄÇ</p><ul><li>üéØ Ôºö‰∏ÄÈîÆ‰ºòÂåñÊèêÁ§∫ËØçÔºåÊîØÊåÅÂ§öËΩÆËø≠‰ª£ÊîπËøõÔºåÊèêÂçáAIÂõûÂ§çÂáÜÁ°ÆÂ∫¶</li><li>üîÑ ÔºöÊîØÊåÅÂéüÂßãÊèêÁ§∫ËØçÂíå‰ºòÂåñÂêéÊèêÁ§∫ËØçÁöÑÂÆûÊó∂ÂØπÊØîÔºåÁõ¥ËßÇÂ±ïÁ§∫‰ºòÂåñÊïàÊûú</li><li>ü§ñ ÔºöÊîØÊåÅOpenAI„ÄÅGemini„ÄÅDeepSeek„ÄÅÊô∫Ë∞±AI„ÄÅSiliconFlowÁ≠â‰∏ªÊµÅAIÊ®°Âûã</li><li>‚öôÔ∏è ÔºöÊîØÊåÅ‰∏∫ÊØè‰∏™Ê®°ÂûãÂçïÁã¨ÈÖçÁΩÆtemperature„ÄÅmax_tokensÁ≠âLLMÂèÇÊï∞</li><li>üîí ÔºöÁ∫ØÂÆ¢Êà∑Á´ØÂ§ÑÁêÜÔºåÊï∞ÊçÆÁõ¥Êé•‰∏éAIÊúçÂä°ÂïÜ‰∫§‰∫íÔºå‰∏çÁªèËøá‰∏≠Èó¥ÊúçÂä°Âô®</li><li>üíæ ÔºöÊú¨Âú∞Âä†ÂØÜÂ≠òÂÇ®ÂéÜÂè≤ËÆ∞ÂΩïÂíåAPIÂØÜÈí•ÔºåÊîØÊåÅÊï∞ÊçÆÂØºÂÖ•ÂØºÂá∫</li><li>üì± ÔºöÂêåÊó∂Êèê‰æõWebÂ∫îÁî®ÂíåChromeÊèí‰ª∂‰∏§Áßç‰ΩøÁî®ÊñπÂºè</li><li>üé® ÔºöÁÆÄÊ¥ÅÁõ¥ËßÇÁöÑÁïåÈù¢ËÆæËÆ°ÔºåÂìçÂ∫îÂºèÂ∏ÉÂ±ÄÂíåÊµÅÁïÖ‰∫§‰∫íÂä®Êïà</li><li>üåê ÔºöVercelÈÉ®ÁΩ≤Êó∂ÊîØÊåÅ‰ΩøÁî®Edge Runtime‰ª£ÁêÜËß£ÂÜ≥Ë∑®ÂüüÈóÆÈ¢ò</li></ul><p>È°πÁõÆÊòØÁ∫ØÂâçÁ´ØÈ°πÁõÆÔºåÊâÄÊúâÊï∞ÊçÆÂè™Â≠òÂÇ®Âú®ÊµèËßàÂô®Êú¨Âú∞Ôºå‰∏ç‰ºö‰∏ä‰º†Ëá≥‰ªª‰ΩïÊúçÂä°Âô®ÔºåÂõ†Ê≠§Áõ¥Êé•‰ΩøÁî®Âú®Á∫øÁâàÊú¨‰πüÊòØÂÆâÂÖ®ÂèØÈù†ÁöÑ</p><p>ÊñπÂºè1Ôºö‰∏ÄÈîÆÈÉ®ÁΩ≤Âà∞Ëá™Â∑±ÁöÑVercelÔºö <a href=\"https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flinshenkx%2Fprompt-optimizer\"><img src=\"https://vercel.com/button\" alt=\"ÈÉ®ÁΩ≤Âà∞ Vercel\"></a></p><p>ÊñπÂºè2: ForkÈ°πÁõÆÂêéÂú®Vercel‰∏≠ÂØºÂÖ•ÔºàÊé®ËçêÔºâÔºö</p><ul><li>ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºö \n  <ul><li>ÔºöËÆæÁΩÆËÆøÈóÆÂØÜÁ†ÅÔºåÂêØÁî®ËÆøÈóÆÈôêÂà∂</li><li>Á≠âÔºöÈÖçÁΩÆÂêÑAIÊúçÂä°ÂïÜÁöÑAPIÂØÜÈí•</li></ul></li></ul><ol></ol><pre><code># ËøêË°åÂÆπÂô®ÔºàÈªòËÆ§ÈÖçÁΩÆÔºâ\ndocker run -d -p 80:80 --restart unless-stopped --name prompt-optimizer linshen/prompt-optimizer\n\n# ËøêË°åÂÆπÂô®ÔºàÈÖçÁΩÆAPIÂØÜÈí•ÂíåËÆøÈóÆÂØÜÁ†ÅÔºâ\ndocker run -d -p 80:80 \\\n  -e VITE_OPENAI_API_KEY=your_key \\\n  -e ACCESS_USERNAME=your_username \\  # ÂèØÈÄâÔºåÈªòËÆ§‰∏∫\"admin\"\n  -e ACCESS_PASSWORD=your_password \\  # ËÆæÁΩÆËÆøÈóÆÂØÜÁ†Å\n  --restart unless-stopped \\\n  --name prompt-optimizer \\\n  linshen/prompt-optimizer\n  \n</code></pre><pre><code># 1. ÂÖãÈöÜ‰ªìÂ∫ì\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd prompt-optimizer\n\n# 2. ÂèØÈÄâÔºöÂàõÂª∫.envÊñá‰ª∂ÈÖçÁΩÆAPIÂØÜÈí•ÂíåËÆøÈóÆËÆ§ËØÅ\ncat &gt; .env &lt;&lt; EOF\n# APIÂØÜÈí•ÈÖçÁΩÆ\nVITE_OPENAI_API_KEY=your_openai_api_key\nVITE_GEMINI_API_KEY=your_gemini_api_key\nVITE_DEEPSEEK_API_KEY=your_deepseek_api_key\nVITE_ZHIPU_API_KEY=your_zhipu_api_key\nVITE_SILICONFLOW_API_KEY=your_siliconflow_api_key\n\n# BasicËÆ§ËØÅÈÖçÁΩÆÔºàÂØÜÁ†Å‰øùÊä§Ôºâ\nACCESS_USERNAME=your_username  # ÂèØÈÄâÔºåÈªòËÆ§‰∏∫\"admin\"\nACCESS_PASSWORD=your_password  # ËÆæÁΩÆËÆøÈóÆÂØÜÁ†Å\nEOF\n\n# 3. ÂêØÂä®ÊúçÂä°\ndocker compose up -d\n\n# 4. Êü•ÁúãÊó•Âøó\ndocker compose logs -f\n</code></pre><p>‰Ω†ËøòÂèØ‰ª•Áõ¥Êé•ÁºñËæëdocker-compose.ymlÊñá‰ª∂ÔºåËá™ÂÆö‰πâÈÖçÁΩÆÔºö</p><pre><code>services:\n  prompt-optimizer:\n    image: linshen/prompt-optimizer:latest\n    container_name: prompt-optimizer\n    restart: unless-stopped\n    ports:\n      - \"8081:80\"  # ‰øÆÊîπÁ´ØÂè£Êò†Â∞Ñ\n    environment:\n      - VITE_OPENAI_API_KEY=your_key_here  # Áõ¥Êé•Âú®ÈÖçÁΩÆ‰∏≠ËÆæÁΩÆÂØÜÈí•\n</code></pre><ol><li>ÁÇπÂáªÈúÄË¶ÅÈÖçÁΩÆÁöÑÊ®°ÂûãÔºàÂ¶ÇOpenAI„ÄÅGemini„ÄÅDeepSeekÁ≠âÔºâ</li></ol><ul><li>OpenAI (gpt-3.5-turbo, gpt-4, gpt-4o)</li><li>Gemini (gemini-1.5-pro, gemini-2.0-flash)</li><li>DeepSeek (deepseek-chat, deepseek-coder)</li><li>ZhipuÊô∫Ë∞± (glm-4-flash, glm-4, glm-3-turbo)</li><li>SiliconFlow (Pro/deepseek-ai/DeepSeek-V3)</li></ul><p>Èô§‰∫ÜAPIÂØÜÈí•ÔºåÊÇ®ËøòÂèØ‰ª•Âú®Ê®°ÂûãÈÖçÁΩÆÁïåÈù¢‰∏∫ÊØè‰∏™Ê®°ÂûãÂçïÁã¨ËÆæÁΩÆÈ´òÁ∫ßLLMÂèÇÊï∞„ÄÇËøô‰∫õÂèÇÊï∞ÈÄöËøá‰∏Ä‰∏™Âêç‰∏∫  ÁöÑÂ≠óÊÆµËøõË°åÈÖçÁΩÆÔºåÂÆÉÂÖÅËÆ∏ÊÇ®‰ª•ÈîÆÂÄºÂØπÁöÑÂΩ¢ÂºèÊåáÂÆöLLM SDKÊîØÊåÅÁöÑ‰ªª‰ΩïÂèÇÊï∞Ôºå‰ªéËÄåÊõ¥Á≤æÁªÜÂú∞ÊéßÂà∂Ê®°ÂûãË°å‰∏∫„ÄÇ</p><ul><li>: <code>{\"temperature\": 0.7, \"max_tokens\": 4096, \"timeout\": 60000}</code></li><li>: <code>{\"temperature\": 0.8, \"maxOutputTokens\": 2048, \"topP\": 0.95}</code></li><li>: <code>{\"temperature\": 0.5, \"top_p\": 0.9, \"frequency_penalty\": 0.1}</code></li></ul><pre><code>-e VITE_OPENAI_API_KEY=your_key\n-e VITE_GEMINI_API_KEY=your_key\n-e VITE_DEEPSEEK_API_KEY=your_key\n-e VITE_ZHIPU_API_KEY=your_key\n-e VITE_SILICONFLOW_API_KEY=your_key\n-e VITE_CUSTOM_API_KEY=your_custom_api_key\n-e VITE_CUSTOM_API_BASE_URL=your_custom_api_base_url\n-e VITE_CUSTOM_API_MODEL=your_custom_model_name\n</code></pre><pre><code># 1. ÂÖãÈöÜÈ°πÁõÆ\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd prompt-optimizer\n\n# 2. ÂÆâË£Ö‰æùËµñ\npnpm install\n\n# 3. ÂêØÂä®ÂºÄÂèëÊúçÂä°\npnpm dev               # ‰∏ªÂºÄÂèëÂëΩ‰ª§ÔºöÊûÑÂª∫core/uiÂπ∂ËøêË°åwebÂ∫îÁî®\npnpm dev:web          # ‰ªÖËøêË°åwebÂ∫îÁî®\npnpm dev:fresh        # ÂÆåÊï¥ÈáçÁΩÆÂπ∂ÈáçÊñ∞ÂêØÂä®ÂºÄÂèëÁéØÂ¢É\n</code></pre><a href=\"https://star-history.com/#linshenkx/prompt-optimizer&amp;Date\"></a><h4>Q1: ‰∏∫‰ªÄ‰πàÈÖçÁΩÆÂ•ΩAPIÂØÜÈí•Âêé‰ªçÁÑ∂Êó†Ê≥ïËøûÊé•Âà∞Ê®°ÂûãÊúçÂä°Ôºü</h4><p>: Â§ßÂ§öÊï∞ËøûÊé•Â§±Ë¥•ÊòØÁî±ÔºàCORSÔºâÂØºËá¥ÁöÑ„ÄÇÁî±‰∫éÊú¨È°πÁõÆÊòØÁ∫ØÂâçÁ´ØÂ∫îÁî®ÔºåÊµèËßàÂô®Âá∫‰∫éÂÆâÂÖ®ËÄÉËôë‰ºöÈòªÊ≠¢Áõ¥Êé•ËÆøÈóÆ‰∏çÂêåÊ∫êÁöÑAPIÊúçÂä°„ÄÇÊ®°ÂûãÊúçÂä°Â¶ÇÊú™Ê≠£Á°ÆÈÖçÁΩÆCORSÁ≠ñÁï•Ôºå‰ºöÊãíÁªùÊù•Ëá™ÊµèËßàÂô®ÁöÑÁõ¥Êé•ËØ∑Ê±Ç„ÄÇ</p><p>: OllamaÂÆåÂÖ®ÊîØÊåÅOpenAIÊ†áÂáÜÊé•Âè£ÔºåÂè™ÈúÄÈÖçÁΩÆÊ≠£Á°ÆÁöÑË∑®ÂüüÁ≠ñÁï•Ôºö</p><ol><li>ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè  ÂÖÅËÆ∏‰ªªÊÑèÊù•Ê∫êÁöÑËØ∑Ê±Ç</li><li>Â¶Ç‰ªçÊúâÈóÆÈ¢òÔºåËÆæÁΩÆ <code>OLLAMA_HOST=0.0.0.0:11434</code> ÁõëÂê¨‰ªªÊÑèIPÂú∞ÂùÄ</li></ol><h4>Q3: Â¶Ç‰ΩïËß£ÂÜ≥ÂïÜ‰∏öAPIÔºàÂ¶ÇNvidiaÁöÑDS API„ÄÅÂ≠óËäÇË∑≥Âä®ÁöÑÁÅ´Â±±APIÔºâÁöÑË∑®ÂüüÈóÆÈ¢òÔºü</h4><p>: Ëøô‰∫õÂπ≥Âè∞ÈÄöÂ∏∏Êúâ‰∏•Ê†ºÁöÑË∑®ÂüüÈôêÂà∂ÔºåÊé®Ëçê‰ª•‰∏ãËß£ÂÜ≥ÊñπÊ°àÔºö</p><ol><li><ul></ul></li></ol><p>: ‰ΩøÁî®Vercel‰ª£ÁêÜÂèØËÉΩ‰ºöËß¶ÂèëÊüê‰∫õÊ®°ÂûãÊúçÂä°Êèê‰æõÂïÜÁöÑÈ£éÊéßÊú∫Âà∂„ÄÇÈÉ®ÂàÜÂéÇÂïÜÂèØËÉΩ‰ºöÂ∞ÜÊù•Ëá™VercelÁöÑËØ∑Ê±ÇÂà§ÂÆö‰∏∫‰ª£ÁêÜË°å‰∏∫Ôºå‰ªéËÄåÈôêÂà∂ÊàñÊãíÁªùÊúçÂä°„ÄÇÂ¶ÇÈÅáÊ≠§ÈóÆÈ¢òÔºåÂª∫ËÆÆ‰ΩøÁî®Ëá™ÈÉ®ÁΩ≤ÁöÑ‰∏≠ËΩ¨ÊúçÂä°„ÄÇ</p><ol><li>ÂàõÂª∫ÁâπÊÄßÂàÜÊîØ (<code>git checkout -b feature/AmazingFeature</code>)</li><li>Êèê‰∫§Êõ¥Êîπ ()</li><li>Êé®ÈÄÅÂà∞ÂàÜÊîØ (<code>git push origin feature/AmazingFeature</code>)</li></ol><ol><li>ÊåâÁÖßÂÆ°Êü•Êä•ÂëäÊ†ºÂºèÊ£ÄÊü•: \n  <ul></ul></li></ol><a href=\"https://github.com/linshenkx/prompt-optimizer/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=linshenkx/prompt-optimizer\" alt=\"Ë¥°ÁåÆËÄÖ\"></a><p>Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ËÄÉËôëÁªôÂÆÉ‰∏Ä‰∏™ Star ‚≠êÔ∏è</p><ul></ul>","contentLength":5193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"anthropics/prompt-eng-interactive-tutorial","url":"https://github.com/anthropics/prompt-eng-interactive-tutorial","date":1750127771,"author":"","guid":157729,"unread":true,"content":"<p>Anthropic's Interactive Prompt Engineering Tutorial</p><h2>Course introduction and goals</h2><p>This course is intended to provide you with a comprehensive step-by-step understanding of how to engineer optimal prompts within Claude.</p><p><strong>After completing this course, you will be able to</strong>:</p><ul><li>Master the basic structure of a good prompt</li><li>Recognize common failure modes and learn the '80/20' techniques to address them</li><li>Understand Claude's strengths and weaknesses</li><li>Build strong prompts from scratch for common use cases</li></ul><h2>Course structure and content</h2><p>This course is structured to allow you many chances to practice writing and troubleshooting prompts yourself. The course is broken up into <strong>9 chapters with accompanying exercises</strong>, as well as an appendix of even more advanced methods. It is intended for you to <strong>work through the course in chapter order</strong>.</p><p><strong>Each lesson has an \"Example Playground\" area</strong> at the bottom where you are free to experiment with the examples in the lesson and see for yourself how changing prompts can change Claude's responses. There is also an <a href=\"https://docs.google.com/spreadsheets/d/1jIxjzUWG-6xBVIa2ay6yDpLyeuOh_hR_ZB75a47KX_E/edit?usp=sharing\">answer key</a>.</p><p>Note: This tutorial uses our smallest, fastest, and cheapest model, Claude 3 Haiku. Anthropic has <a href=\"https://docs.anthropic.com/claude/docs/models-overview\">two other models</a>, Claude 3 Sonnet and Claude 3 Opus, which are more intelligent than Haiku, with Opus being the most intelligent.</p><p>When you are ready to begin, go to <code>01_Basic Prompt Structure</code> to proceed.</p><p>Each chapter consists of a lesson and a set of exercises.</p><ul><li><p> Basic Prompt Structure</p></li><li><p> Being Clear and Direct</p></li><li><p> Assigning Roles</p></li></ul><ul><li><p> Separating Data from Instructions</p></li><li><p> Formatting Output &amp; Speaking for Claude</p></li><li><p> Precognition (Thinking Step by Step)</p></li></ul><ul><li><p> Avoiding Hallucinations</p></li><li><p> Building Complex Prompts (Industry Use Cases)</p><ul><li>Complex Prompts from Scratch - Chatbot</li><li>Complex Prompts for Legal Services</li><li> Complex Prompts for Financial Services</li><li> Complex Prompts for Coding</li><li>Congratulations &amp; Next Steps</li></ul></li><li><p> Beyond Standard Prompting</p><ul></ul></li></ul>","contentLength":1830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"huggingface/lerobot","url":"https://github.com/huggingface/lerobot","date":1750127771,"author":"","guid":157730,"unread":true,"content":"<p>ü§ó LeRobot: Making AI for Robotics more accessible with end-to-end learning</p><div align=\"center\"><p><strong>Meet the updated SO100, the SO-101 ‚Äì Just ‚Ç¨114 per arm!</strong></p><p>Train it in minutes with a few simple moves on your laptop.</p><p>Then sit back and watch your creation act autonomously! ü§Ø</p><p>Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!</p><img src=\"https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp?raw=true\" alt=\"LeKiwi mobile robot\" title=\"LeKiwi mobile robot\" width=\"50%\"></div><h3 align=\"center\"><p>LeRobot: State-of-the-art AI for real-world robotics</p></h3><p>ü§ó LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.</p><p>ü§ó LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.</p><p>ü§ó LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.</p><h4>Examples of pretrained models on simulation environments</h4><ul><li>The LeRobot team ü§ó for building SmolVLA <a href=\"https://arxiv.org/abs/2506.01844\">Paper</a>, <a href=\"https://huggingface.co/blog/smolvla\">Blog</a>.</li><li>Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from <a href=\"https://tonyzhaozh.github.io/aloha\">ALOHA</a> and <a href=\"https://mobile-aloha.github.io\">Mobile ALOHA</a>.</li><li>Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from <a href=\"https://diffusion-policy.cs.columbia.edu\">Diffusion Policy</a> and <a href=\"https://umi-gripper.github.io\">UMI Gripper</a>.</li><li>Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from <a href=\"https://github.com/nicklashansen/tdmpc\">TDMPC</a> and <a href=\"https://www.yunhaifeng.com/FOWM\">FOWM</a>.</li><li>Thanks to Antonio Loquercio and Ashish Kumar for their early support.</li></ul><p>Download our source code:</p><pre><code>git clone https://github.com/huggingface/lerobot.git\ncd lerobot\n</code></pre><p>Create a virtual environment with Python 3.10 and activate it, e.g. with <a href=\"https://docs.anaconda.com/free/miniconda/index.html\"></a>:</p><pre><code>conda create -y -n lerobot python=3.10\nconda activate lerobot\n</code></pre><p>When using , install  in your environment:</p><pre><code>conda install ffmpeg -c conda-forge\n</code></pre><blockquote><p> This usually installs  for your platform compiled with the  encoder. If  is not supported (check supported encoders with ), you can:</p><ul><li> Explicitly install  using:</li></ul><pre><code>conda install ffmpeg=7.1.1 -c conda-forge\n</code></pre></blockquote><blockquote><p> If you encounter build errors, you may need to install additional dependencies (, , and ). On Linux, run: <code>sudo apt-get install cmake build-essential python3-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev pkg-config</code>. For other systems, see: <a href=\"https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg\">Compiling PyAV</a></p></blockquote><p>For simulations, ü§ó LeRobot comes with gymnasium environments that can be installed as extras:</p><p>For instance, to install ü§ó LeRobot with aloha and pusht, use:</p><pre><code>pip install -e \".[aloha, pusht]\"\n</code></pre><p>(note: you will also need to enable WandB in the configuration. See below.)</p><pre><code>.\n‚îú‚îÄ‚îÄ examples             # contains demonstration examples, start here to learn about LeRobot\n|   ‚îî‚îÄ‚îÄ advanced         # contains even more examples for those who have mastered the basics\n‚îú‚îÄ‚îÄ lerobot\n|   ‚îú‚îÄ‚îÄ configs          # contains config classes with all options that you can override in the command line\n|   ‚îú‚îÄ‚îÄ common           # contains classes and utilities\n|   |   ‚îú‚îÄ‚îÄ datasets       # various datasets of human demonstrations: aloha, pusht, xarm\n|   |   ‚îú‚îÄ‚îÄ envs           # various sim environments: aloha, pusht, xarm\n|   |   ‚îú‚îÄ‚îÄ policies       # various policies: act, diffusion, tdmpc\n|   |   ‚îú‚îÄ‚îÄ robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots\n|   |   ‚îî‚îÄ‚îÄ utils          # various utilities\n|   ‚îî‚îÄ‚îÄ scripts          # contains functions to execute via command line\n|       ‚îú‚îÄ‚îÄ eval.py                 # load policy and evaluate it on an environment\n|       ‚îú‚îÄ‚îÄ train.py                # train a policy via imitation learning and/or reinforcement learning\n|       ‚îú‚îÄ‚îÄ control_robot.py        # teleoperate a real robot, record data, run a policy\n|       ‚îú‚îÄ‚îÄ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub\n|       ‚îî‚îÄ‚îÄ visualize_dataset.py    # load a dataset and render its demonstrations\n‚îú‚îÄ‚îÄ outputs               # contains results of scripts execution: logs, videos, model checkpoints\n‚îî‚îÄ‚îÄ tests                 # contains pytest utilities for continuous integration\n</code></pre><p>Check out <a href=\"https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py\">example 1</a> that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.</p><p>You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:</p><pre><code>python lerobot/scripts/visualize_dataset.py \\\n    --repo-id lerobot/pusht \\\n    --episode-index 0\n</code></pre><p>or from a dataset in a local folder with the  option and the  (in the following case the dataset will be searched for in <code>./my_local_data_dir/lerobot/pusht</code>)</p><pre><code>python lerobot/scripts/visualize_dataset.py \\\n    --repo-id lerobot/pusht \\\n    --root ./my_local_data_dir \\\n    --local-files-only 1 \\\n    --episode-index 0\n</code></pre><p>It will open  and display the camera streams, robot states and actions, like this:</p><p>Our script can also visualize datasets stored on a distant server. See <code>python lerobot/scripts/visualize_dataset.py --help</code> for more instructions.</p><p>A dataset in  format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. <code>dataset = LeRobotDataset(\"lerobot/aloha_static_coffee\")</code> and can be indexed into like any Hugging Face and PyTorch dataset. For instance  will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.</p><p>A specificity of  is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting  to a list of relative times with respect to the indexed frame. For example, with <code>delta_timestamps = {\"observation.image\": [-1, -0.5, -0.2, 0]}</code> one can retrieve, for a given index, 4 frames: 3 \"previous\" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example <a href=\"https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py\">1_load_lerobot_dataset.py</a> for more details on .</p><p>Under the hood, the  format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.</p><p>Here are the important details and internal structure organization of a typical  instantiated with <code>dataset = LeRobotDataset(\"lerobot/aloha_static_coffee\")</code>. The exact features will change from dataset to dataset but not the main aspects:</p><pre><code>dataset attributes:\n  ‚îú hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:\n  ‚îÇ  ‚îú observation.images.cam_high (VideoFrame):\n  ‚îÇ  ‚îÇ   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}\n  ‚îÇ  ‚îú observation.state (list of float32): position of an arm joints (for instance)\n  ‚îÇ  ... (more observations)\n  ‚îÇ  ‚îú action (list of float32): goal position of an arm joints (for instance)\n  ‚îÇ  ‚îú episode_index (int64): index of the episode for this sample\n  ‚îÇ  ‚îú frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode\n  ‚îÇ  ‚îú timestamp (float32): timestamp in the episode\n  ‚îÇ  ‚îú next.done (bool): indicates the end of an episode ; True for the last frame in each episode\n  ‚îÇ  ‚îî index (int64): general index in the whole dataset\n  ‚îú episode_data_index: contains 2 tensors with the start and end indices of each episode\n  ‚îÇ  ‚îú from (1D int64 tensor): first frame index for each episode ‚Äî shape (num episodes,) starts with 0\n  ‚îÇ  ‚îî to: (1D int64 tensor): last frame index for each episode ‚Äî shape (num episodes,)\n  ‚îú stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance\n  ‚îÇ  ‚îú observation.images.cam_high: {'max': tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}\n  ‚îÇ  ...\n  ‚îú info: a dictionary of metadata on the dataset\n  ‚îÇ  ‚îú codebase_version (str): this is to keep track of the codebase version the dataset was created with\n  ‚îÇ  ‚îú fps (float): frame per second the dataset is recorded/synchronized to\n  ‚îÇ  ‚îú video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files\n  ‚îÇ  ‚îî encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos\n  ‚îú videos_dir (Path): where the mp4 videos or png images are stored/accessed\n  ‚îî camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[\"observation.images.cam_high\", ...]`)\n</code></pre><p>A  is serialised using several widespread file formats for each of its parts, namely:</p><ul><li>hf_dataset stored using Hugging Face datasets library serialization to parquet</li><li>videos are stored in mp4 format to save space</li><li>metadata are stored in plain json/jsonl files</li></ul><p>Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the  argument if it's not in the default <code>~/.cache/huggingface/lerobot</code> location.</p><h3>Evaluate a pretrained policy</h3><p>Check out <a href=\"https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py\">example 2</a> that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.</p><p>We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on <a href=\"https://huggingface.co/lerobot/diffusion_pusht\">lerobot/diffusion_pusht</a>:</p><pre><code>python lerobot/scripts/eval.py \\\n    --policy.path=lerobot/diffusion_pusht \\\n    --env.type=pusht \\\n    --eval.batch_size=10 \\\n    --eval.n_episodes=10 \\\n    --policy.use_amp=false \\\n    --policy.device=cuda\n</code></pre><p>Note: After training your own policy, you can re-evaluate the checkpoints with:</p><pre><code>python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model\n</code></pre><p>See <code>python lerobot/scripts/eval.py --help</code> for more instructions.</p><p>Check out <a href=\"https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py\">example 3</a> that illustrates how to train a model using our core library in python, and <a href=\"https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md\">example 4</a> that shows how to use our training script from command line.</p><p>To use wandb for logging training and evaluation curves, make sure you've run  as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding .</p><p>A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check <a href=\"https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md#typical-logs-and-metrics\">here</a> for the explanation of some commonly used metrics in logs.</p><p>Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use  to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See <code>python lerobot/scripts/eval.py --help</code> for more instructions.</p><h4>Reproduce state-of-the-art (SOTA)</h4><p>We provide some pretrained policies on our <a href=\"https://huggingface.co/lerobot\">hub page</a> that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running:</p><pre><code>python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht\n</code></pre><p>reproduces SOTA results for Diffusion Policy on the PushT task.</p><p>Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like  (e.g. <a href=\"https://huggingface.co/lerobot/diffusion_pusht\">lerobot/diffusion_pusht</a>).</p><p>You first need to find the checkpoint folder located inside your experiment directory (e.g. <code>outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500</code>). Within that there is a  directory which should contain:</p><ul><li>: A serialized version of the policy configuration (following the policy's dataclass config).</li><li>: A consolidated configuration containing all parameters used for training. The policy configuration should match  exactly. This is useful for anyone who wants to evaluate your policy or for reproducibility.</li></ul><p>To upload these to the hub, run the following:</p><pre><code>huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model\n</code></pre><p>See <a href=\"https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py\">eval.py</a> for an example of how other people may use your policy.</p><h3>Improve your code with profiling</h3><p>An example of a code snippet to profile the evaluation of a policy:</p><pre><code>from torch.profiler import profile, record_function, ProfilerActivity\n\ndef trace_handler(prof):\n    prof.export_chrome_trace(f\"tmp/trace_schedule_{prof.step_num}.json\")\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    schedule=torch.profiler.schedule(\n        wait=2,\n        warmup=2,\n        active=3,\n    ),\n    on_trace_ready=trace_handler\n) as prof:\n    with record_function(\"eval_policy\"):\n        for i in range(num_episodes):\n            prof.step()\n            # insert code to profile, potentially whole body of eval_policy function\n</code></pre><p>If you want, you can cite this work with:</p><pre><code>@misc{cadene2024lerobot,\n    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascale, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},\n    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},\n    howpublished = \"\\url{https://github.com/huggingface/lerobot}\",\n    year = {2024}\n}\n</code></pre><p>Additionally, if you are using any of the particular policy architecture, pretrained models, or datasets, it is recommended to cite the original authors of the work as they appear below:</p><pre><code>@article{shukor2025smolvla,\n  title={SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics},\n  author={Shukor, Mustafa and Aubakirova, Dana and Capuano, Francesco and Kooijmans, Pepijn and Palma, Steven and Zouitine, Adil and Aractingi, Michel and Pascal, Caroline and Russi, Martino and Marafioti, Andres and Alibert, Simon and Cord, Matthieu and Wolf, Thomas and Cadene, Remi},\n  journal={arXiv preprint arXiv:2506.01844},\n  year={2025}\n}\n</code></pre><pre><code>@article{chi2024diffusionpolicy,\n\tauthor = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},\n\ttitle ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},\n\tjournal = {The International Journal of Robotics Research},\n\tyear = {2024},\n}\n</code></pre><pre><code>@article{zhao2023learning,\n  title={Learning fine-grained bimanual manipulation with low-cost hardware},\n  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},\n  journal={arXiv preprint arXiv:2304.13705},\n  year={2023}\n}\n</code></pre><pre><code>@inproceedings{Hansen2022tdmpc,\n\ttitle={Temporal Difference Learning for Model Predictive Control},\n\tauthor={Nicklas Hansen and Xiaolong Wang and Hao Su},\n\tbooktitle={ICML},\n\tyear={2022}\n}\n</code></pre><pre><code>@article{lee2024behavior,\n  title={Behavior generation with latent actions},\n  author={Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},\n  journal={arXiv preprint arXiv:2403.03181},\n  year={2024}\n}\n</code></pre><pre><code>@Article{luo2024hilserl,\ntitle={Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning},\nauthor={Jianlan Luo and Charles Xu and Jeffrey Wu and Sergey Levine},\nyear={2024},\neprint={2410.21845},\narchivePrefix={arXiv},\nprimaryClass={cs.RO}\n}\n</code></pre>","contentLength":15839,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["trending"]}