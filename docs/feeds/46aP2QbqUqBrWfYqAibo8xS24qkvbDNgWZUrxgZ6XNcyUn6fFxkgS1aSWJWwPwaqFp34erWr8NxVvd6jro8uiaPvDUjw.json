{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"How to set necessary permissions to use oidc from github actions to aws eks?","url":"https://www.reddit.com/r/kubernetes/comments/1ikmlns/how_to_set_necessary_permissions_to_use_oidc_from/","date":1739019132,"author":"/u/HumanResult3379","guid":421,"unread":true,"content":"<p>I want to run ,  and  in github actions workflow to operate kubernetes cluster in AWS EKS.</p><p>If use AWS' OIDC, create a role for github actions, how many permissions are necessary to set?</p><p>Also, is it okay just create an OIDC role in AWS? Is it necessary to create a service account in kubernetes to allow the operation from GitHub Actions?</p><p>Is there a good example about this case?</p>","contentLength":375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In a production environment how do you organise nodes?","url":"https://www.reddit.com/r/kubernetes/comments/1ikkbwh/in_a_production_environment_how_do_you_organise/","date":1739010081,"author":"/u/ReverendRou","guid":424,"unread":true,"content":"<p>With all my learning not a great deal has been discussed with how you would actually allocate your nodes. I understand that concepts of taints/tolerations affinoties and so on. But in a real production environment what would a typical setup look like with nodes and applications. </p><p>For example, if you have a Postgres Database, I imagine you would want a large node for the primary which is dedicated to this database And perhaps another node dedicated to a hot standby. </p><p>What is the general guidance then with mixing different applications onto a single node. Is it just a case wanting to put applications onto their own nodes to enforce isolation and separation in the event of failure. </p><p>For the most part, in my homelab, my only experience with kubernetes, it's just been a case of everything being on two nodes. And letting the scheduler place things </p>","contentLength":851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Calico SNAT - how to specify the source interface","url":"https://www.reddit.com/r/kubernetes/comments/1ik79kd/calico_snat_how_to_specify_the_source_interface/","date":1738966321,"author":"/u/Wrong_username_404","guid":423,"unread":true,"content":"<p>Hey all! I'm struggling to get SNAT setup correctly in a test cluster. I have 3 worker nodes running Alma9 with 2 interfaces each:</p><ul><li>10.1.1.X bond0 - 1G management network</li><li>10.1.2.X bond1 - 10G data network</li></ul><p>I was able to get the pod-to-pod traffic working correctly by setting the node-ip in the kubelet startup on each host :</p><p><code>echo 'KUBELET_EXTRA_ARGS=\"--node-ip=10.1.2.225\"' &gt; /etc/sysconfig/kubelet</code></p><p>and patching calico's nodeAddressAutodectionV4:</p><p><code>kubectl patch installation.operator.tigera.io default --type merge --patch '{\"spec\":{\"calicoNetwork\":{\"nodeAddressAutodetectionV4\":{\"cidr\": \"10.1.2.0/24\"}}}}'</code></p><p>kubectl shows each node with the IP from the 10G interface:</p><pre><code>kube44.ord Ready 19d v1.32.0 10.1.2.224 kube45.ord Ready 115m v1.32.0 10.1.2.225 kube46.ord Ready 15d v1.32.1 10.1.2.226 </code></pre><p>And ip routes are being set correctly on the host:</p><pre><code>10.45.115.0/26 via 10.1.2.226 dev tunl0 proto bird onlink 10.45.117.64/26 via 10.1.2.225 dev tunl0 proto bird onlink 10.45.145.64/26 via 10.1.2.224 dev tunl0 proto bird onlink </code></pre><p>But when I try to ping a resource outside of the cluster, it's grabbing the address on 1G connection:</p><pre><code>[kube45.grr ~]# tcpdump -i bond0 -n | grep 154.33 14:17:22.059449 IP 10.1.1.225 &gt; 172.16.154.33: ICMP echo request, id 29199, seq 1, length 64 </code></pre><p>Anyone know what I'm missing? </p><p>I saw the option for natOutgoingAddress but that doesn't seem to be node-specific. </p>","contentLength":1361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kubelet did not evict pods under node memory pressure condition","url":"https://www.reddit.com/r/kubernetes/comments/1ik69cs/kubelet_did_not_evict_pods_under_node_memory/","date":1738963775,"author":"/u/0x4ddd","guid":422,"unread":true,"content":"<p>Under specific conditions one of our pods was undergoing memory leak so it was consuming more and more memory and finally node reached 100% memory usage according to our node level metrics.</p><p>Due to memory pressure kubelet in the end stopped responding (most likely) as node was reported as NotReady. 100% memory usage rendered node unstable and it wasn't able to come up online back again in reasonable time (we waited 10 minutes) so we needed to manually restart virtual machine.</p><p>We could and probably should have set container level memory limit so kubelet restarts it sooner but why it didn't kill pod when hard eviction threshold was reached? Do you have any ideas? Maybe default value of 100Mb is too low and kubelet simply stopped responding before being able to evict pod?</p>","contentLength":776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artem Lajko gives a presentation on Considerations when building an IDP and how you can use Kubernetes + GitOps + vCluster (there's a demo too)","url":"https://youtu.be/7p1GdyS7kmA","date":1738945707,"author":"/u/mpetersen_loft-sh","guid":425,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1ijz05g/artem_lajko_gives_a_presentation_on/"},{"title":"Kubernetes Cluster per Developer","url":"https://www.reddit.com/r/kubernetes/comments/1ijx3n6/kubernetes_cluster_per_developer/","date":1738940885,"author":"/u/Born-Organization836","guid":426,"unread":true,"content":"<p>I'm working in a team which consists of about 15 developers. Currently we're using only one shared Kubernetes cluster (via Openshift) aside from prod which we call preprod. Obviously this comes with plenty of hardships - our preprod environment is consistently broken and everytime we want to test some code we need to configure plenty of deployments to match prod's deployments, make the changes we need to test our code and pray no one else is going to override our configuration.</p><p>I've been hearing that the standard today is to create an isolated dev environment for each developer in the team, which, as far as I understand, would require a different Kubernetes cluster/namespace per developer.</p><p>We don't have enough resources in our cluster to create a namespace per developer, plus we don't have enough resources in our personal computers to run a Kubernetes cluster locally. We do however have enough resources to run a copy of the prod cluster in a VM. So the natural solution, as I see it, would be to run a Kubernetes cluster (pereferably with Openshift) on a different VM for every developer, or alternatively one Kubernetes cluster with a namespace per developer.</p><p>What tools do you recommend to run a Kubernetes cluster in a VM with good DX when working locally? Also how would you suggest to mimic prod's cluster configuration as good as possible (networking configuration, etc)? I've heard plenty about TIlt and wondered if it'd be applicable here.</p><p>If you have an alternative suggestion or something you do differently in your company, please share!</p>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","k8s"]}