{"id":"Lx2uDWR7f1yr334","title":"Programming","displayTitle":"Programming","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":265,"items":[{"title":"Don't \"optimize\" conditional moves in shaders with mix()+step()","url":"https://iquilezles.org/articles/gpuconditionals/","date":1739631147,"author":"/u/namanyayg","guid":638,"unread":true,"content":"\nIn this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with conditionals when selecting between two results in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation, and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up a little:\nSo, say I have this code, which I actually published the other day:<div> snap45(  v )\n{\n     s = (v);\n     x = (v.x);\n     x&gt;?(s.x,):\n           x&gt;?s*():\n                      (,s.y);\n}</div>\nThe exact details of what it does don't matter for this discussion. All we care about is the two ternary operations deciding what's the final value this function should return. Indeed, depending on the value of the variable , the function will return one of three results, which are simple to compute. I could also have implemented this function with regular  statements, and all that I'm going to say in this article stays true.<p>\nNow, here's the problem - when seeing code like this, somebody somewhere will step up and invariably propose the following \"optimization\", which replaces what they believe (erroneously) are \"conditional branches\" in the code, by arithmetic operations. They will suggest something like this:</p><div> snap45(  v )\n{\n     s = (v);\n     x = (v.x);\n\n     w0 = (,x);\n     w1 = (,x)*(-w0);\n     w2 = -w0-w1;\n\n     res0 = (s.x,);\n     res1 = (s.x,s.y)*();\n     res2 = (,s.y);\n\n     w0*res0 + w1*res1 + w2*res2;\n}</div>\nThere are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. \"select\"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. I'll repeat it again - there is no branching, the instruction pointer isn't manipulated, there's no prediction involved, no pipe to flush, no instruction cache to invalidation, no nothing.<p>\nFor the record, of course GPUs can do real branching, and those are fine and fast and totally worth it when big chunks of code and computation are to be skipped given a condition. As with all things computing, always check the generated machine code to know what is happening exactly and when. But one thing you can safely assume without having to check any generated code - when moving simple values or computations like in my original example, you are guaranteed to not branch. This has been true for decades at this point, with GPUs. And while I'm not an expert in CPUs, I am pretty sure this is true for them as well.</p><p>\nThe second wrong thing with the supposedly optimized version is that it actually runs much slower than the original version. You can measure it in a variety of hardware. I can only assume that's because the </p> function is probably implemented with some sort of conditional move or subtract + bit propagation + AND.<div> step(  x,  y )\n{\n     x &lt; y ?  : ;\n}</div>\nEither way, using the step() \"optimization\" are either using the ternary operation anyways, which produces the  or  which they will use to mask in and out the different potential outputs with a series of arithmetic multiplications and additions. Which is wasteful, the values could have been conditionally moved directly, which is what the original shader code did.<p>\nBut don't take my word for it, let's look at the generated machine code for the original code I published:</p><div><div>\nGLSL<div> x&gt;?(s.x,):\n       x&gt;?s*():\n                  (,s.y);</div></div><div>\nAMD Compiler<div>     s0,      v3, , v1\n     v4, , v0\n     s1,   vcc, (v2), s0\n v3, 0, v3, vcc\n v0, v0, v4, vcc\n vcc, (v2), s1\n v1, v1, v3, vcc\n v0, 0, v0, vcc</div></div><div>\nMicrosoft Compiler<div>   r0.xy, l(, ), v0.xy\n   r0.zw, v0.xy, l(, )\n r0.xy, -r0.xyxx, r0.zwzz\n r0.xy, r0.xyxx\n  r1.xyzw, r0.xyxy, l4()\n   r2.xy, l(,), v0.xx  r0.z, l()\n r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy\n o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw</div></div></div>\nHere we can confirm that the GPU is not branching, as I explained. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).<p>\nThe Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison (</p> - \"lt\"=less than) and the masking or conditional move ( - mov=move, c=conditionally).<p>\nThere are no jump/branch instructions in these listings.</p><p>\nSomething not related to the discussion but interesting, is that some of the </p> GLSL calls I had in my shader before the ternary operator we are discussing, didn't become GPU instructions but rather instruction modifiers, which is the reason you see them in the listing. This means you can think of abs() calls as being free.\nSo, if you ever see somebody proposing this<div> a = ( b, c, ( y, x ) );</div>\nas an optimization to\nthen please correct them for me.","contentLength":5296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq2z4j/dont_optimize_conditional_moves_in_shaders_with/"},{"title":"What is Event Sourcing?","url":"https://newsletter.scalablethread.com/p/what-is-event-sourcing","date":1739628306,"author":"/u/scalablethread","guid":639,"unread":true,"content":"<p>Traditional data storage typically focuses on the current state of an entity. For example, in an e-commerce system, you might store the current state of a customer's order: items, quantities, shipping address, etc. Event sourcing takes a different approach. Instead of storing the current state directly, it stores the events that led to that state. Each event represents a fact that happened in the past. Think of it as a detailed log of transactions on your bank statement. These events are immutable and stored in an append-only event store. The core idea is that an application's state can be derived by replaying events in the order they occurred, just like you can get your current bank balance by replaying all the transactions from the beginning. This makes Event Sourcing particularly useful for applications that require a high degree of audibility and traceability.</p><p>Every change to the application state is captured as an event object in an Event Sourcing system. These events are then stored in an event store, a database optimized for handling event data. Here's a step-by-step breakdown of how Event Sourcing works:</p><ol></ol><p>Reconstructing the state from events involves reading all the events related to an entity from the event store and applying them in sequence to reconstruct the current state. It's like simulating all the changes that have occurred to construct the current state. For example, consider an e-commerce application where an order goes through various states like \"Created,\" \"Paid,\" and \"Shipped.\" To determine the current state of an order, you would:</p><ol><li><p>Retrieve all events related to the order from the event store.</p></li><li><p>Initialize an empty order object.</p></li><li><p>Apply each event to the order object in the order in which they were stored.</p></li></ol><p>By the end of this process, the order object will reflect the current state of the order.</p><p>As the number of events grows, replaying the entire event stream to reconstruct the state can become slow and inefficient. This is where snapshots come in. A snapshot is a saved state of an entity at a specific point in time. Instead of replaying all events from the beginning, the application can load the latest snapshot and then replay only the events that occurred after the snapshot was taken. </p><ul></ul><ul></ul><p><em>If you enjoyed this article, please hit the ‚ù§Ô∏è like button.</em></p><p><em>If you think someone else will benefit from this, then please üîÅ share this post.</em></p>","contentLength":2380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq20v8/what_is_event_sourcing/"},{"title":"Chinese Vice Minister says China and the US must work together to control rogue AI: \"If not... I am afraid that the probability of the machine winning will be high.\"","url":"https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says","date":1739622429,"author":"/u/MetaKnowing","guid":632,"unread":true,"content":"<div datatype=\"p\" data-qa=\"Component-Component\">A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in <a target=\"_self\" href=\"https://www.scmp.com/topics/artificial-intelligence?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> (AI).</div><p datatype=\"p\" data-qa=\"Component-Component\">But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.</p><p datatype=\"p\" data-qa=\"Component-Component\">‚ÄúRealistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,‚Äù Fu said.</p><p datatype=\"p\" data-qa=\"Component-Component\">‚ÄúAs long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.‚Äù</p><div datatype=\"p\" data-qa=\"Component-Component\">The panel discussion is part of a two-day global <a target=\"_self\" href=\"https://www.scmp.com/news/world/europe/article/3297992/trumps-ai-ambition-and-chinas-deepseek-overshadow-major-ai-summit-paris?module=Europe&amp;pgtype=section?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> that started in Paris on Monday.</div><p datatype=\"p\" data-qa=\"Component-Component\">Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden‚Äôs administration and the United Nations.</p>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq0b4t/chinese_vice_minister_says_china_and_the_us_must/"},{"title":"[P] Daily ArXiv filtering powered by LLM judge","url":"https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/","date":1739618056,"author":"/u/MadEyeXZ","guid":634,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go Nullable with Generics v2.0.0 - now supports omitzero","url":"https://github.com/LukaGiorgadze/gonull","date":1739617221,"author":"/u/Money-Relative-1184","guid":613,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ipz22f/go_nullable_with_generics_v200_now_supports/"},{"title":"what do you use golang for?","url":"https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/","date":1739615068,"author":"/u/Notalabel_4566","guid":617,"unread":true,"content":"<p>Is there any other major use than web development?</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Kreuzberg ‚Äì Modern async Python library for document text extraction","url":"https://github.com/Goldziher/kreuzberg","date":1739614043,"author":"nhirschfeld","guid":325,"unread":true,"content":"<p>I'm excited to showcase Kreuzberg!</p><p>Kreuzberg is a modern Python library built from the ground up with async/await, type hints, and optimized I/O handling.</p><p>It provides a unified interface for extracting text from documents (PDFs, images, office files) without external API dependencies.</p><p>Key technical features:\n- Built with modern Python best practices (async/await, type hints, functional-first)\n- Optimized async I/O with anyio for multi-loop compatibility\n- Smart worker process pool for CPU-bound tasks (OCR, doc conversion)\n- Efficient batch processing with concurrent extractions\n- Clean error handling with context-rich exceptions</p><p>I built this after struggling with existing solutions that were either synchronous-only, required complex deployments, or had poor async support. The goal was to create something that works well in modern async Python applications, can be easily dockerized or used in serverless contexts, and relies only on permissive OSS.</p><p>Key advantages over alternatives:\n- True async support with optimized I/O\n- Minimal dependencies (much smaller than alternatives)\n- Perfect for serverless and async web apps\n- Local processing without API calls\n- Built for modern Python codebases with rigorous typing and testing</p><p>The library is MIT licensed and open to contributions.</p>","contentLength":1289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43057375"},{"title":"Lessons from David Lynch: A Software Developer's Perspective","url":"https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/","date":1739612430,"author":"/u/aijan1","guid":642,"unread":true,"content":"<p>David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He‚Äôs perhaps best known for the groundbreaking TV series <a href=\"https://en.wikipedia.org/wiki/Twin_Peaks\">Twin Peaks</a>, which inspired countless shows, including The X-Files, The Sopranos, and Lost.</p><p>Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down ‚Äì even those who truly deserved it.</p><p>Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that <a href=\"https://en.wikipedia.org/wiki/Mulholland_Drive_(film)\">Mulholland Drive</a> remained compulsively watchable while refusing to yield to interpretation.</p><p>While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I‚Äôd like to share my perspective on his life lessons from a software developer‚Äôs viewpoint.</p><blockquote><p>Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you‚Äôve got to go deeper.</p></blockquote><p>We‚Äôve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one ‚Äìbecause they‚Äôre so rare‚Äì write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it‚Äôs a film, a painting, or software.</p><blockquote><p>The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.</p></blockquote><p>Software development is part art, part engineering. We don‚Äôt build the same software over and over again ‚Äì virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it‚Äôs very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.</p><p>It‚Äôs a good habit to listen to what users have to say, but they often can only describe their problems ‚Äì they rarely come up with good ideas to solve them. And that‚Äôs OK. It‚Äôs our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.</p><blockquote><p>My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.</p></blockquote><p>Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  ‚Äì that magical state of mind where we lose track of time and produce code effortlessly. That‚Äôs why many developers hate meetings ‚Äì they are toxic to our productivity.</p><blockquote><p>I believe you need technical knowledge. And also, it‚Äôs really, really great to learn by doing. So, you should make a film.</p></blockquote><p>Software development is one of those rare fields where a college degree isn‚Äôt required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.</p><p>The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It‚Äôs crucial to never stop learning, experimenting, and iterating on our craft.</p><blockquote><p>Happy accidents are real gifts, and they can open the door to a future that didn‚Äôt even exist.</p></blockquote><p>Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.</p><p>Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.</p><blockquote><p>I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.</p></blockquote><p>Be kind to your teammates, don‚Äôt embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety ‚Äìthat is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by <a href=\"https://rework.withgoogle.com/en/guides/understanding-team-effectiveness\">Google‚Äôs research</a> on the subject.</p><p>It‚Äôs OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.</p><blockquote><p>Most of Hollywood is about making money - and I love money, but I don‚Äôt make the films thinking about money.</p></blockquote><p>Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.</p><p>What makes these projects remarkable is that they didn‚Äôt emerge from corporate boardrooms ‚Äì they were built by communities of passionate developers, collaborating across the world.</p><p>Money is just a means to an end. Unfortunately, many get this confused.</p><p>David, thank you for making the world a better place!</p>","contentLength":5845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipy01t/lessons_from_david_lynch_a_software_developers/"},{"title":"[D] What's the most promising successor to the Transformer?","url":"https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/","date":1739600221,"author":"/u/jsonathan","guid":636,"unread":true,"content":"<p>All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also <a href=\"https://arxiv.org/pdf/2405.04517\">xLSTM</a> and <a href=\"https://arxiv.org/pdf/2405.13956\">Aaren</a>.</p><p>What do y'all think is the most promising alternative architecture to the transformer?</p>","contentLength":283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kafka Delay Queue: When Messages Need a Nap Before They Work","url":"https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need","date":1739596108,"author":"/u/Sushant098123","guid":641,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipu9n3/kafka_delay_queue_when_messages_need_a_nap_before/"},{"title":"Webassembly and go 2025","url":"https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/","date":1739595637,"author":"/u/KosekiBoto","guid":616,"unread":true,"content":"<div><p>so I found <a href=\"https://www.youtube.com/watch?v=HShIpUgCPp4\">this video </a>and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you</p></div>   submitted by   <a href=\"https://www.reddit.com/user/KosekiBoto\"> /u/KosekiBoto </a>","contentLength":402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vq: A Vector Quantization Library for Rust ü¶Ä","url":"https://www.reddit.com/r/rust/comments/1ipu2jg/vq_a_vector_quantization_library_for_rust/","date":1739595392,"author":"/u/West-Bottle9609","guid":619,"unread":true,"content":"<p>I've created a Rust library called Vq that implements several <a href=\"https://en.wikipedia.org/wiki/Vector_quantization\">vector quantization</a> algorithms. At the moment, these algorithms include binary, scalar, product, optimized product, tree-structured, and residual quantization. I think the library can be useful for tasks like data compression, similarity search, creating RAG pipelines, and speeding up machine learning computations.</p><p>This is my second Rust project, as I'm currently learning Rust. I'd like to get some feedback from the community and hear about any use cases you might have for the library, so I'm making this announcement.</p><p>The library is available on crates.io: <a href=\"https://crates.io/crates/vq\">vq</a>, and the source code is on GitHub: <a href=\"https://github.com/habedi/vq\">vq</a>.</p>","contentLength":664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modern Java Deep Dive","url":"https://www.youtube.com/watch?v=z4qsidg261E","date":1739591964,"author":"/u/BlueGoliath","guid":637,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipt4pe/modern_java_deep_dive/"},{"title":"Bringing Nest.js to Rust: Meet Toni.rs, the Framework You‚Äôve Been Waiting For! üöÄ","url":"https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/","date":1739587338,"author":"/u/Mysterious-Rust","guid":622,"unread":true,"content":"<p>As a Rust developer coming from TypeScript, I‚Äôve been missing a Nest.js-like framework ‚Äî its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn‚Äôt have a direct counterpart (yet!), I decided to build one myself! üõ†Ô∏è</p><p>Introducing‚Ä¶ <a href=\"https://crates.io/crates/toni\">Toni.rs</a> ‚Äî a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it‚Äôs live in beta! üéâ</p><p>Here‚Äôs what makes this project interesting:</p><p>Scalable maintainability üß©:</p><p>A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code ‚Äî each module lives in its own context, clean and focused.</p><p>Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?</p><p>Automatic Dependency Injection ü§ñ:</p><p>Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.</p><p>Leave your thoughts below ‚Äî suggestions, questions, or even just enthusiasm! üöÄ </p>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I Became A Machine Learning Engineer (No CS Degree, No Bootcamp)","url":"https://towardsdatascience.com/how-i-became-a-machine-learning-engineer-no-cs-degree-no-bootcamp/","date":1739586781,"author":"Egor Howell","guid":11,"unread":true,"content":"<p>Machine learning and <a href=\"https://towardsdatascience.com/tag/ai/\" title=\"AI\">AI</a> are among the most popular topics nowadays, especially within the tech space. I am fortunate enough to work and develop with these technologies every day as a machine learning engineer!</p><p>In this article, I will walk you through my journey to becoming a machine learning engineer, shedding some light and advice on how you can become one yourself!</p><p>In one of my previous articles, I extensively wrote about my journey from school to securing my first <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a> job. I recommend you <a href=\"https://medium.com/towards-data-science/how-i-became-a-data-scientist-no-cs-degree-no-bootcamp-82c321904986\">check out that article</a>, but I will summarise the key timeline here.</p><p>Pretty much everyone in my family studied some sort of STEM subject. My great-grandad was an engineer, both my grandparents studied physics, and my mum is a maths teacher.</p><p><em>So, my path was always paved for me.</em></p><p>I chose to study physics at university after watching The Big Bang Theory at age 12; it‚Äôs fair to say everyone was very proud!</p><p>At school, I wasn‚Äôt dumb by any means. I was actually relatively bright, but I didn‚Äôt fully apply myself. I got decent grades, but definitely not what I was fully capable of.</p><p>I was very arrogant and thought I would do well with zero work.</p><p>I applied to top universities like Oxford and Imperial College, but given my work ethic, I was delusional thinking I had a chance. On results day, I ended up in clearing as I missed my offers. This was probably one of the saddest days of my life.</p><p>Clearing in the UK is where universities offer places to students on certain courses where they have space. It‚Äôs mainly for students who don‚Äôt have a university offer.</p><p>I was lucky enough to be offered a chance to study physics at the University of Surrey, and I went on to earn a first-class master‚Äôs degree in physics!</p><p>There is genuinely no substitute for hard work. It is a cringy cliche, but it is true!</p><p>My original plan was to do a PhD and be a full-time researcher or professor, but during my degree, I did a research year, and I just felt a career in research was not for me. Everything moved so slowly, and it didn‚Äôt seem there was much opportunity in the space.</p><p>During this time, DeepMind released their<a href=\"https://www.youtube.com/watch?v=WXuK6gekU1Y&amp;t=1539s\"></a>documentary on YouTube, which popped up on my home feed.</p><p>From the video, I started to understand how AI worked and learn about neural networks, reinforcement learning, and deep learning. To be honest, to this day I am still not an expert in these areas.</p><p>Naturally, I dug deeper and found that a data scientist uses AI and machine learning algorithms to solve problems. I immediately wanted in and started applying for data science graduate roles.</p><p>I spent countless hours coding, taking courses, and working on projects. I applied to and eventually landed my first data science graduate scheme in September 2021.</p><p><em>You can hear more about my journey from a <a href=\"https://tobeadatascientist.substack.com/p/overcoming-rejection-lessons-from-egor-howell\">podcast</a>.</em></p><p>I started my career in an insurance company, where I built various supervised learning models, mainly using gradient boosted tree packages like CatBoost, XGBoost, and<a href=\"https://medium.com/towards-data-science/breaking-down-generalized-linear-models-d9212526e51d?sk=fda0298cebcb8e9e0c20cb6af8ed4f06\"> generalised linear models (GLMs)</a>.</p><p>I built models to predict:</p><ul><li>‚Ää‚Äî‚ÄäDid someone fraudulently make a claim to profit.</li><li>‚Äî‚ÄäWhat‚Äôs the premium we should give someone.</li><li>‚Äî‚ÄäHow many claims will someone have.</li><li>‚Ää‚Äî‚ÄäWhat‚Äôs the average claim value someone will have.</li></ul><p>I made around six models spanning the regression and classification space. I learned so much here, especially in statistics, as I worked very closely with Actuaries, so my maths knowledge was excellent.</p><p>However, due to the company‚Äôs structure and setup, it was difficult for my models to advance past the PoC stage, so I felt I lacked the ‚Äútech‚Äù side of my toolkit and understanding of how companies use machine learning in production.</p><p>After a year, my previous employer reached out to me asking if I wanted to apply to a junior data scientist role that specialises in<a href=\"https://medium.com/@egorhowell/list/time-series-00bbfb9f5359\"> time series forecasting</a> and<a href=\"https://medium.com/@egorhowell/list/optimisation-algorithms-069bf9c6c8d5\"> optimisation</a> problems. I really liked the company, and after a few interviews, I was offered the job!</p><p>I worked at this company for about 2.5 years, where I became an expert in forecasting and combinatorial optimisation problems.</p><p>I developed many algorithms and deployed my models to production through AWS using software engineering best practices, such as unit testing, lower environment, shadow system, CI/CD pipelines, and much more.</p><p><em>Fair to say I learned a lot.&nbsp;</em></p><p>I worked very closely with software engineers, so I picked up a lot of engineering knowledge and continued self-studying machine learning and statistics on the side.</p><p>Over time, I realised the actual value of data science is using it to make live decisions. There is a good quote by<a href=\"https://www.linkedin.com/posts/pau-labarta-bajo-4432074b_machinelearning-mlops-realworldml-activity-7195694289178214400-gZyw\"> Pau Labarta Bajo</a></p><p>ML models inside Jupyter notebooks have a business value of $0</p><p>There is no point in building a really complex and sophisticated model if it will not produce results. Seeking out that extra 0.1% accuracy by staking multiple models is often not worth it.</p><p>You are better off building something simple that you can deploy, and that will bring real financial benefit to the company.</p><p>With this in mind, I started thinking about the future of data science. In my head, there are two avenues:</p><ul><li> -&gt; You work primarily to gain insight into what the business should be doing and what it should be looking into to boost its performance.</li><li> -&gt; You ship solutions (models, decision algorithms, etc.) that bring business value.</li></ul><p>I feel the data scientist who analyses and builds PoC models will become extinct in the next few years because, as we said above, they don‚Äôt provide tangible value to a business.</p><p>That‚Äôs not to say they are entirely useless; you have to think of it from the business perspective of their return on investment. Ideally, the value you bring in should be more than your salary.</p><p>You want to say that you did ‚ÄúX that produced Y‚Äù, which the above two avenues allow you to do.</p><p>The engineering side was the most interesting and enjoyable for me. I genuinely enjoy coding and building stuff that benefits people, and that they can use, so naturally, that‚Äôs where I gravitated towards.</p><p>To move to the ML engineering side, I asked my line manager if I could deploy the algorithms and ML models I was building myself. I would get help from software engineers, but I would write all the production code, do my own system design, and set up the deployment process independently.</p><p><em>And that‚Äôs exactly what I did.</em></p><p>Coincidentally, my current employer contacted me around this time and asked if I wanted to apply for a machine learning engineer role that specialises in general ML and optimisation at their company!</p><p>Call it luck, but clearly, the universe was telling me something. After several interview rounds, I was offered the role, and I am now a fully fledged machine learning engineer!</p><p>Fortunately, a role kind of ‚Äúfell to me,‚Äù but I created my own luck through up-skilling and documenting my learning. That is why I always tell people to show their work‚Ää‚Äî‚Ääyou don‚Äôt know what may come from it.</p><p>I want to share the main bits of advice that helped me transition from a machine learning engineer to a data scientist.</p><ul><li>‚Ää‚Äî‚ÄäA machine learning engineer is  an entry-level position in my opinion. You need to be well-versed in data science, machine learning, software engineering, etc. You don‚Äôt need to be an expert in all of them, but have good fundamentals across the board. That‚Äôs why I recommend having a couple of years of experience as either a software engineer or data scientist and self-study other areas.</li><li>‚Ää‚Äî‚ÄäIf you are from data science, you must learn to write good, well-tested production code. You must know things like typing, linting, unit tests, formatting, mocking and CI/CD. It‚Äôs not too difficult, but it just requires some practice. I recommend asking your current company to work with software engineers to gain this knowledge, it worked for me!</li><li>‚Ää‚Äî‚ÄäMost companies nowadays deploy many of their architecture and systems on the cloud, and machine learning models are no exception. So, it‚Äôs best to get practice with these tools and understand how they enable models to go live. I learned most of this on the job, to be honest, but there are courses you can take.</li><li>‚Ää‚Äî‚ÄäI am sure most of you know this already, but every tech professional should be proficient in the command line. You will use it extensively when deploying and writing production code. I have a basic guide you can checkout<a href=\"https://medium.com/towards-data-science/an-introduction-to-the-shell-676ee5b899df?sk=0c6e101165b4314b98ab39d11525366c\"> here</a>.</li><li><strong>Data Structures &amp; Algorithms‚Ää</strong>‚Äî‚ÄäUnderstanding the fundamental algorithms in computer science are very useful for MLE roles. Mainly because you will likely be asked about it in interviews. It‚Äôs not too hard to learn compared to machine learning; it just takes time. Any course will do the trick.</li><li>‚Ää‚Äî‚ÄäAgain, most tech professionals should know Git, but as an MLE, it is essential. How to squash commits, do code reviews, and write outstanding pull requests are musts.</li><li>‚Ää‚Äî‚ÄäMany MLE roles I saw required you to have some specialisation in a particular area. I specialise in time series forecasting, optimisation, and general ML based on my previous experience. This helps you stand out in the market, and most companies are looking for specialists nowadays.</li></ul><p>The main theme here is that I basically up-skilled my software engineering abilities. This makes sense as I already had all the math, stats, and machine learning knowledge from being a data scientist.</p><p>If I were a software engineer, the transition would likely be the reverse. This is why securing a machine learning engineer role can be quite challenging, as it requires proficiency across a wide range of skills.</p><h3><strong>Summary &amp; Further Thoughts</strong></h3><p>I have a free newsletter, <a href=\"https://dishingthedata.substack.com/\"></a>, where I share weekly tips and advice as a practising data scientist. Plus, when you subscribe, you will get my and<strong> short PDF version of my AI roadmap</strong>!</p>","contentLength":9683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Impressions at Netflix","url":"https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb?source=rss----2615bd06b42e---4","date":1739582000,"author":"Netflix Technology Blog","guid":39,"unread":true,"content":"<h4>Part 1: Creating the Source of Truth for Impressions</h4><p>Imagine scrolling through Netflix, where each movie poster or promotional banner competes for your attention. Every image you hover over isn‚Äôt just a visual placeholder; it‚Äôs a critical data point that fuels our sophisticated personalization engine. At Netflix, we call these images ‚Äòimpressions,‚Äô and they play a pivotal role in transforming your interaction from simple browsing into an immersive binge-watching experience, all tailored to your unique&nbsp;tastes.</p><p>Capturing these moments and turning them into a personalized journey is no simple feat. It requires a state-of-the-art system that can track and process these impressions while maintaining a detailed history of each profile‚Äôs exposure. This nuanced integration of data and technology empowers us to offer bespoke content recommendations.</p><p>In this multi-part blog series, we take you behind the scenes of our system that processes billions of impressions daily. We will explore the challenges we encounter and unveil how we are building a resilient solution that transforms these client-side impressions into a personalized content discovery experience for every Netflix&nbsp;viewer.</p><h3>Why do we need impression history?</h3><p>To tailor recommendations more effectively, it‚Äôs crucial to track what content a user has already encountered. Having impression history helps us achieve this by allowing us to identify content that has been displayed on the homepage but not engaged with, helping us deliver fresh, engaging recommendations.</p><p>By maintaining a history of impressions, we can implement frequency capping to prevent over-exposure to the same content. This ensures users aren‚Äôt repeatedly shown identical options, keeping the viewing experience vibrant and reducing the risk of frustration or disengagement.</p><h4>Highlighting New&nbsp;Releases</h4><p>For new content, impression history helps us monitor initial user interactions and adjust our merchandising efforts accordingly. We can experiment with different content placements or promotional strategies to boost visibility and engagement.</p><p>Additionally, impression history offers insightful information for addressing a number of platform-related analytics queries. Analyzing impression history, for example, might help determine how well a specific row on the home page is functioning or assess the effectiveness of a merchandising strategy.</p><p>The first pivotal step in managing impressions begins with the creation of a Source-of-Truth (SOT) dataset. This foundational dataset is essential, as it supports various downstream workflows and enables a multitude of use&nbsp;cases.</p><h4>Collecting Raw Impression Events</h4><p>As Netflix members explore our platform, their interactions with the user interface spark a vast array of raw events. These events are promptly relayed from the client side to our servers, entering a centralized event processing queue. This queue ensures we are consistently capturing raw events from our global user&nbsp;base.</p><p>After raw events are collected into a centralized queue, a custom event extractor processes this data to identify and extract all impression events. These extracted events are then routed to an Apache Kafka topic for immediate processing needs and simultaneously stored in an Apache Iceberg table for long-term retention and historical analysis. This dual-path approach leverages Kafka‚Äôs capability for low-latency streaming and Iceberg‚Äôs efficient management of large-scale, immutable datasets, ensuring both real-time responsiveness and comprehensive historical data availability.</p><h4>Filtering &amp; Enriching Raw Impressions</h4><p>Once the raw impression events are queued, a stateless Apache Flink job takes charge, meticulously processing this data. It filters out any invalid entries and enriches the valid ones with additional metadata, such as show or movie title details, and the specific page and row location where each impression was presented to users. This refined output is then structured using an Avro schema, establishing a definitive source of truth for Netflix‚Äôs impression data. The enriched data is seamlessly accessible for both real-time applications via Kafka and historical analysis through storage in an Apache Iceberg table. This dual availability ensures immediate processing capabilities alongside comprehensive long-term data retention.</p><h4>Ensuring High Quality Impressions</h4><p>Maintaining the highest quality of impressions is a top priority. We accomplish this by gathering detailed column-level metrics that offer insights into the state and quality of each impression. These metrics include everything from validating identifiers to checking that essential columns are properly filled. The data collected feeds into a comprehensive quality dashboard and supports a tiered threshold-based alerting system. These alerts promptly notify us of any potential issues, enabling us to swiftly address regressions. Additionally, while enriching the data, we ensure that all columns are in agreement with each other, offering in-place corrections wherever possible to deliver accurate&nbsp;data.</p><p>We handle a staggering volume of 1 to 1.5 million impression events globally every second, with each event approximately 1.2KB in size. To efficiently process this massive influx in real-time, we employ Apache Flink for its low-latency stream processing capabilities, which seamlessly integrates both batch and stream processing to facilitate efficient backfilling of historical data and ensure consistency across real-time and historical analyses. Our Flink configuration includes 8 task managers per region, each equipped with 8 CPU cores and 32GB of memory, operating at a parallelism of 48, allowing us to handle the necessary scale and speed for seamless performance delivery. The Flink job‚Äôs sink is equipped with a data mesh connector, as detailed in our <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh platform</a> which has two outputs: Kafka and Iceberg. This setup allows for efficient streaming of real-time data through Kafka and the preservation of historical data in Iceberg, providing a comprehensive and flexible data processing and storage solution.</p><p>We utilize the ‚Äòisland model‚Äô for deploying our Flink jobs, where all dependencies for a given application reside within a single region. This approach ensures high availability by isolating regions, so if one becomes degraded, others remain unaffected, allowing traffic to be shifted between regions to maintain service continuity. Thus, all data in one region is processed by the Flink job deployed within that&nbsp;region.</p><h4>Addressing the Challenge of Unschematized Events</h4><p>Allowing raw events to land on our centralized processing queue unschematized offers significant flexibility, but it also introduces challenges. Without a defined schema, it can be difficult to determine whether missing data was intentional or due to a logging error. We are investigating solutions to introduce schema management that maintains flexibility while providing clarity.</p><h4>Automating Performance Tuning with Autoscalers</h4><p>Tuning the performance of our Apache Flink jobs is currently a manual process. The next step is to integrate with autoscalers, which can dynamically adjust resources based on workload demands. This integration will not only optimize performance but also ensure more efficient resource utilization.</p><h4>Improving Data Quality&nbsp;Alerts</h4><p>Right now, there‚Äôs a lot of business rules dictating when a data quality alert needs to be fired. This leads to a lot of false positives that require manual judgement. A lot of times it is difficult to track changes leading to regression due to inadequate data lineage information. We are investing in building a comprehensive data quality platform that more intelligently identifies anomalies in our impression stream, keeps track of data lineage and data governance, and also, generates alerts notifying producers of any regressions. This approach will enhance efficiency, reduce manual oversight, and ensure a higher standard of data integrity.</p><p>Creating a reliable source of truth for impressions is a complex but essential task that enhances personalization and discovery experience. Stay tuned for the next part of this series, where we‚Äôll delve into how we use this SOT dataset to create a microservice that provides impression histories. We invite you to share your thoughts in the comments and continue with us on this journey of discovering impressions.</p><p>We are genuinely grateful to our amazing colleagues whose contributions were essential to the success of Impressions: Julian Jaffe, Bryan Keller, Yun Wang, Brandon Bremen, Kyle Alford, Ron Brown and Shriya&nbsp;Arora.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e2b67c88c9fb\" width=\"1\" height=\"1\" alt=\"\">","contentLength":8607,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tabiew 0.8.4 Released","url":"https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/","date":1739578902,"author":"/u/shshemi","guid":623,"unread":true,"content":"<p>Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...</p><ul><li>üìä Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and Sqlite</li><li>üóÇÔ∏è Multi-table functionality</li></ul><ul><li>UI is updated to be more modern and responsive</li><li>Horizontally scrollable tables</li><li>Visible data frame can be referenced with name \"_\"</li><li>Compatibility with older versions of glibc</li><li>Two new themes (Tokyo Night and Catppuccin)</li></ul>","contentLength":450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditional types in TypeScript","url":"https://2ality.com/2025/02/conditional-types-typescript.html","date":1739577600,"author":"Dr. Axel Rauschmayer","guid":353,"unread":true,"content":"<p>In TypeScript, conditional types let us make decisions (think if-then-else expressions) ‚Äì which is especially useful in generic types. They are also an essential tool for working with union types because they let use ‚Äúloop‚Äù over them. Read on if you want to know how all of that works.</p>","contentLength":291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: VimLM ‚Äì A Local, Offline Coding Assistant for Vim","url":"https://github.com/JosefAlbers/VimLM","date":1739576081,"author":"JosefAlbers","guid":324,"unread":true,"content":"<p>VimLM is a local, offline coding assistant for Vim. It‚Äôs like Copilot but runs entirely on your machine‚Äîno APIs, no tracking, no cloud.</p><p>- Deep Context: Understands your codebase (current file, selections, references).  \n- Conversational: Iterate with follow-ups like \"Add error handling\".  \n- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  \n- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.</p><p>Perfect for privacy-conscious devs or air-gapped environments.</p><p>Try it:  \n```\npip install vimlm\nvimlm\n```</p>","contentLength":558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43054244"},{"title":"GOGC & GOMEMLIMIT ?","url":"https://www.reddit.com/r/golang/comments/1ipnxxk/gogc_gomemlimit/","date":1739575182,"author":"/u/mistyrouge","guid":614,"unread":true,"content":"<div><p>If the GC cost is fixed with regards to the amount of memory being freed up. Why would I not want to put  and  to say 70% of the memory I have available? Specially in an application that is known to be cpu bound.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mistyrouge\"> /u/mistyrouge </a>","contentLength":245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dell Nears $5 Billion AI Server Deal for Elon Musk‚Äôs xAI","url":"https://www.bnnbloomberg.ca/business/technology/2025/02/14/dell-nears-us5-billion-ai-server-deal-for-elon-musks-xai/","date":1739571937,"author":"/u/F0urLeafCl0ver","guid":631,"unread":true,"content":"<p>(Bloomberg) -- Dell Technologies Inc. is in advanced stages of securing a deal worth more than $5 billion to provide Elon Musk‚Äôs xAI with servers optimized for artificial intelligence work.</p><p>The company will sell servers containing Nvidia Corp. GB200 semiconductors to Musk‚Äôs AI startup for delivery this year, according to people familiar with the matter, who asked to not to be named because the work is private. Some details are being finalized and still may change, some of the people added.&nbsp;</p><p>Demand for computing to run AI workloads has led to a boom for makers of high-powered servers like Dell, Super Micro Computer Inc. and Hewlett Packard Enterprise Co. Musk‚Äôs companies, including carmaker Tesla Inc. and xAI, have emerged as major customers for the hardware.&nbsp;</p><p>Dell and Nvidia declined to comment. xAI didn‚Äôt respond to a request for comment.</p><p>Dell shares jumped as much as 6% to $116.88 Friday on the news before paring some gains. The stock had slipped 4.3% this year through Thursday‚Äôs close.</p><p>A supercomputer project being built by xAI in Memphis has used a mix of Dell and Super Micro servers. In December, Dell said it had deployed tens of thousands of graphics processing units, or GPUs, there and was working to win an ‚Äúunfair share‚Äù of the remaining build-out. GPUs are the key chips to power AI workloads and Nvidia is the top maker of those processing units.</p><p>Analysts expect Dell will have shipped more than $10 billion of AI servers in the fiscal year ending last month and project that value will jump to $14 billion in the fiscal year ending in January 2026. Dell is scheduled to report fiscal fourth-quarter earnings on Feb. 27, with the AI server business a major focus for investors.</p><p>The deal with xAI ‚Äúwould firmly establish the company as a leading AI-server provider and boost sales, though the impact on profitability is less clear,‚Äù wrote Woo Jin Ho, an analyst at Bloomberg Intelligence.</p><p>AI startup xAI‚Äôs main product, a chatbot called Grok, has primarily been available to paying users of X, the social network formerly known as Twitter. Firms that Musk runs are known to share employees, technology and computing power.</p><p>--With assistance from Ian King and Kurt Wagner.</p><p>(Updates with comments from analyst in the eighth paragraph.)</p>","contentLength":2274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ipmsdv/dell_nears_5_billion_ai_server_deal_for_elon/"},{"title":"Django Weblog: DjangoCongress JP 2025 Announcement and Live Streaming!","url":"https://www.djangoproject.com/weblog/2025/feb/14/djangocongress-jp-2025-announcement-and-livestream/","date":1739571130,"author":"","guid":285,"unread":true,"content":"<p>It will be streamed on the following YouTube Live channels:</p><p>This year there will be talks not only about Django, but also about FastAPI and other asynchronous web topics. There will also be talks on Django core development, Django Software Foundation (DSF) governance, and other topics from around the world. Simultaneous translation will be provided in both English and Japanese.</p><ul><li>The Async Django ORM: Where Is it?</li><li>Speed at Scale for Django Web Applications</li><li>Implementing Agentic AI Solutions in Django from scratch</li><li>Diving into DSF governance: past, present and future</li></ul><ul><li>Getting Knowledge from Django Hits: Using Grafana and Prometheus</li><li>Culture Eats Strategy for Breakfast: Why Psychological Safety Matters in Open Source</li><li>¬µDjango. The next step in the evolution of asynchronous microservices technology.</li></ul><p>A public viewing of the event will also be held in Tokyo. A reception will also be held, so please check the following connpass page if you plan to attend.</p>","contentLength":948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eli Bendersky: Decorator JITs - Python as a DSL","url":"https://eli.thegreenplace.net/2025/decorator-jits-python-as-a-dsl/","date":1739569771,"author":"","guid":284,"unread":true,"content":"<p>Spend enough time looking at Python programs and packages for machine learning,\nand you'll notice that the \"JIT decorator\" pattern is pretty popular. For\nexample, this JAX snippet:</p><div><pre></pre></div><div><pre></pre></div><p>In both cases, the function decorated with  doesn't get executed by the\nPython interpreter in the normal sense. Instead, the code inside is more like\na DSL (Domain Specific Language) processed by a special purpose compiler built\ninto the library (JAX or Triton). Another way to think about it is that Python\nis used as a  to describe computations.</p><p>In this post I will describe some implementation strategies used by libraries to\nmake this possible.</p><div><h2>Preface - where we're going</h2><p>The goal is to explain how different kinds of  decorators work by using\na simplified, educational example that implements several approaches from\nscratch. All the approaches featured in this post will be using this flow:</p> Expr IR --&gt; LLVM IR --&gt; Execution\" /&gt; Expr IR --&gt; LLVM IR --&gt; Execution\" class=\"align-center\" src=\"https://eli.thegreenplace.net/images/2025/decjit-python.png\" /&gt;\n<p>These are the steps that happen when a Python function wrapped with\nour educational  decorator is called:</p><ol><li>The function is translated to an \"expression IR\" - .</li><li>This expression IR is converted to LLVM IR.</li><li>Finally, the LLVM IR is JIT-executed.</li></ol><p>First, let's look at the  IR. Here we'll make a big simplification -\nonly supporting functions that define a single expression, e.g.:</p><div><pre></pre></div><p>Naturally, this can be easily generalized - after all, LLVM IR can be used to\nexpress fully general computations.</p><p>Here are the  data structures:</p><div><pre></pre></div><p>To convert an  into LLVM IR and JIT-execute it, we'll use this function:</p><div><pre></pre></div><p>It uses the  class to actually generate LLVM IR from .\nThis process is straightforward and covered extensively in the resources I\nlinked to earlier; take a look at <a href=\"https://github.com/eliben/code-for-blog/blob/main/2025/decjit/exprcode.py\">the full code here</a>.</p><p>My goal with this architecture is to make things simple, but .\nOn one hand - there are several simplifications: only single expressions are\nsupported, very limited set of operators, etc. It's very easy to extend this!\nOn the other hand, we could have just trivially evaluated the \nwithout resorting to LLVM IR; I do want to show a more complete compilation\npipeline, though, to demonstrate that an arbitrary amount of complexity can\nbe hidden behind these simple interfaces.</p><p>With these building blocks in hand, we can review the strategies used by\n decorators to convert Python functions into s.</p></div><div><p>Python comes with powerful code reflection and introspection capabilities out\nof the box. Here's the  decorator:</p><div><pre></pre></div><p>This is a standard Python decorator. It takes a function and returns another\nfunction that will be used in its place ( ensures that\nfunction attributes like the name and docstring of the wrapper match the\nwrapped function).</p><div><pre></pre></div><p>After  is applied to , what  holds is the\nwrapper. When  is called, the wrapper is invoked with\n.</p><p>The wrapper obtains the AST of the wrapped function, and then uses\n to convert this AST into an :</p><div><pre></pre></div><p>When  finishes visiting the AST it's given, its\n field will contain the  representing the function's\nreturn value. The wrapper then invokes  with this .</p><p>Note how our decorator interjects into the regular Python execution process.\nWhen  is called, instead of the standard Python compilation and\nexecution process (code is compiled into bytecode, which is then executed\nby the VM), we translate its code to our own representation and emit LLVM from\nit, and then JIT execute the LLVM IR. While it seems kinda pointless in this\nartificial example, in reality this means we can execute the function's code\nin any way we like.</p><div><h3>AST JIT case study: Triton</h3><p>This approach is almost exactly how the Triton language works. The body of a\nfunction decorated with  gets parsed to a Python AST, which then\n- through a series of internal IRs - ends up in LLVM IR; this in turn is lowered\nto <a href=\"https://docs.nvidia.com/cuda/parallel-thread-execution/\">PTX</a> by the\n<a href=\"https://llvm.org/docs/NVPTXUsage.html\">NVPTX LLVM backend</a>.\nThen, the code runs on a GPU using a standard CUDA pipeline.</p><p>Naturally, the subset of Python that can be compiled down to a GPU is limited;\nbut it's sufficient to run performant kernels, in a language that's much\nfriendlier than CUDA and - more importantly - lives in the same file with the\n\"host\" part written in regular Python. For example, if you want testing and\ndebugging, you can run Triton in \"interpreter mode\" which will just run the\nsame kernels locally on a CPU.</p><p>Note that Triton lets us import names from the  package\nand use them inside kernels; these serve as the  for the language\n- special calls the compiler handles directly.</p></div></div><div><p>Python is a fairly complicated language with  of features. Therefore,\nif our JIT has to support some large portion of Python semantics, it may make\nsense to leverage more of Python's own compiler. Concretely, we can have it\ncompile the wrapped function all the way <a href=\"https://github.com/python/cpython/blob/main/InternalDocs/interpreter.md\">to bytecode</a>,\nand start our translation from there.</p><p>Here's the  decorator that does just this :</p><div><pre></pre></div><p>The Python VM is a stack machine; so we emulate a stack to convert the\nfunction's bytecode to  IR (a bit like an <a href=\"https://en.wikipedia.org/wiki/Reverse_Polish_notation\">RPN evaluator</a>).\nAs before, we then use our  utility function to lower\n to LLVM IR and JIT execute it.</p><p>Using this JIT is as simple as the previous one - just swap \nfor :</p><div><pre></pre></div><div><h3>Bytecode JIT case study: Numba</h3><p><a href=\"https://numba.pydata.org/\">Numba</a> is a compiler for Python itself. The idea\nis that you can speed up specific functions in your code by slapping a\n decorator on them. What happens next is similar in spirit to\nour simple , but of course much more complicated because it\nsupports a very large portion of Python semantics.</p><p>Numba uses the Python compiler to emit bytecode, just as we did; it then\nconverts it into its own IR, and then to LLVM using .</p><p>By starting with the bytecode, Numba makes its life easier (no need to rewrite\nthe entire Python compiler). On the other hand, it also makes some analyses\n, because by the time we're in bytecode, a lot of semantic information\nexisting in higher-level representations is lost. For example, Numba has to\nsweat a bit to recover control flow information from the bytecode (by\nrunning it through a special interpreter first).</p></div></div><div><p>The two approaches we've seen so far are similar in many ways - both rely on\nPython's introspection capabilities to compile the source code of the JIT-ed\nfunction to some extent (one to AST, the other all the way to bytecode), and\nthen work on this lowered representation.</p><p>The tracing strategy is very different. It doesn't analyze the source code of\nthe wrapped function at all - instead, it  its execution by means of\nspecially-boxed arguments, leveraging overloaded operators and functions, and\nthen works on the generated trace.</p><p>The code implementing this for our smile demo is surprisingly compact:</p><div><pre></pre></div><p>Each runtime argument of the wrapped function is assigned a , and\nthat is placed in a , a placeholder class which lets us\ndo operator overloading:</p><div><pre></pre></div><p>The remaining key function is :</p><div><pre></pre></div><p>To understand how this works, consider this trivial example:</p><div><pre></pre></div><p>After the decorated function is defined,  holds the wrapper function\ndefined inside . When  is called, the wrapper runs:</p><ol><li>For each argument of  itself (that is  and ), it creates\na new  holding a . This denotes a named variable in\nthe  IR.</li><li>It then calls the wrapped function, passing it the boxes as runtime\nparameters.</li><li>When (the wrapped)  runs, it invokes . This is caught by the overloaded\n operator of , and it creates a new  with\nthe s representing  and  as children. This\n is then returned .</li><li>The wrapper unboxes the returned  and passes it to\n to emit LLVM IR from it and JIT execute it with the\nactual runtime arguments of the call: .</li></ol><p>This might be a little mind-bending at first, because there are two different\nexecutions that happen:</p><ul><li>The first is calling the wrapped  function itself, letting the Python\ninterpreter run it as usual, but with special arguments that build up the IR\ninstead of doing any computations. This is the .</li><li>The second is lowering this IR our tracing step built into LLVM IR and then\nJIT executing it with the actual runtime argument values ; this is\nthe .</li></ul><p>This tracing approach has some interesting characteristics. Since we don't\nhave to analyze the source of the wrapped functions but only trace through\nthe execution, we can \"magically\" support a much richer set of programs, e.g.:</p><div><pre></pre></div><p>This  with our basic . Since Python variables are\nplaceholders (references) for values, our tracing step is oblivious to them - it\nfollows the flow of values. Another example:</p><div><pre></pre></div><p>This also just works! The created  will be a long chain of \nadditions of 's runtime values through the loop, added to the \nfor .</p><p>This last example also leads us to a limitation of the tracing approach; the\nloop cannot be  - it cannot depend on the function's arguments,\nbecause the tracing step has no concept of runtime values and wouldn't know\nhow many iterations to run through; or at least, it doesn't know this unless\nwe want to perform the tracing run for every runtime execution .</p><div><h3>Tracing JIT case study: JAX</h3><p>The <a href=\"https://jax.readthedocs.io/en/latest/\">JAX ML framework</a> uses a tracing\napproach very similar to the one described here. The first code sample in this\npost shows the JAX notation. JAX cleverly wraps Numpy with its own version which\nis traced (similar to our , but JAX calls these boxes \"tracers\"),\nletting you write regular-feeling Numpy code that can be JIT optimized and\nexecuted on accelerators like GPUs and TPUs via <a href=\"https://github.com/openxla\">XLA</a>. JAX's tracer builds up an underlying IR (called\n<a href=\"https://jax.readthedocs.io/en/latest/jaxpr.html\">jaxpr</a>) which can then be\nemitted to XLA ops and passed to XLA for further lowering and execution.</p><p>For a fairly deep overview of how JAX works, I recommend reading the\n<a href=\"https://jax.readthedocs.io/en/latest/autodidax.html\">autodidax doc</a>.</p><p>As mentioned earlier, JAX has <a href=\"https://jax.readthedocs.io/en/latest/jit-compilation.html\">some limitations</a>\nwith things like data-dependent control flow in native Python. This won't work,\nbecause there's control flow\nthat depends on a runtime value ():</p><div><pre></pre></div><p>When  is executed, JAX will throw an exception, saying something\nlike:</p><blockquote>\nThis concrete value was not available in Python because it depends on the\nvalue of the argument count.</blockquote><p>As a remedy, JAX has its\nown built-in intrinsics from the <a href=\"https://jax.readthedocs.io/en/latest/jax.lax.html\">jax.lax package</a>.\nHere's the example rewritten in a way that actually works:</p><div><pre></pre></div><p> (and many other built-ins in the  package) is something JAX\ncan trace through, generating a corresponding XLA operation (XLA has support for\n<a href=\"https://openxla.org/xla/operation_semantics\">While loops</a>, to which this\n can be lowered).</p><p>The tracing approach has clear benefits for JAX as well; because it only cares\nabout the flow of values, it can handle arbitrarily complicated Python code,\nas long as the flow of values can be traced. Just like the local variables and\ndata-independent loops shown earlier, but also things like closures. This makes\nmeta-programming and templating easy .</p></div></div><div><p>The full code for this post is available <a href=\"https://github.com/eliben/code-for-blog/tree/main/2025/decjit\">on GitHub</a>.</p></div>","contentLength":10514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.","url":"https://v.redd.it/sglstazd96je1","date":1739568243,"author":"/u/eternviking","guid":633,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iple9t/an_art_exhibit_in_japan_where_a_chained_robot_dog/"},{"title":"GitHub - Clivern/Peanut: üê∫ Deploy Databases and Services Easily for Development and Testing Pipelines.","url":"https://github.com/Clivern/Peanut","date":1739567268,"author":"/u/Clivern","guid":615,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ipl12p/github_clivernpeanut_deploy_databases_and/"},{"title":"Shutdown Go server","url":"https://www.reddit.com/r/golang/comments/1ipj5zn/shutdown_go_server/","date":1739562402,"author":"/u/Kennedy-Vanilla","guid":618,"unread":true,"content":"<div><p>Hi, recently I saw that many people shutdown their servers like this or similar</p><pre><code>serverCtx, serverStopCtx serverCtx, serverStopCtx := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { &lt;-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { &lt;-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal(\"graceful shutdown timed out.. forcing exit.\") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf(\"error shutting down server: %v\", err) } serverStopCtx() }() log.Printf(\"Server starting on port %s...\\n\", port) err = server.ListenAndServe() if err != nil &amp;&amp; err != http.ErrServerClosed { log.Printf(\"error starting server: %v\", err) os.Exit(1) } &lt;-serverCtx.Done() log.Println(\"Server stopped\") } := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { &lt;-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { &lt;-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal(\"graceful shutdown timed out.. forcing exit.\") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf(\"error shutting down server: %v\", err) } serverStopCtx() }() log.Printf(\"Server starting on port %s...\\n\", port) err = server.ListenAndServe() if err != nil &amp;&amp; err != http.ErrServerClosed { log.Printf(\"error starting server: %v\", err) os.Exit(1) } &lt;-serverCtx.Done() log.Println(\"Server stopped\") </code></pre><p>Is it necessary? Like it's so many code for the simple operation</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Kennedy-Vanilla\"> /u/Kennedy-Vanilla </a>","contentLength":1805,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"the ref keyword","url":"https://www.reddit.com/r/rust/comments/1ipixny/the_ref_keyword/","date":1739561817,"author":"/u/Tickstart","guid":621,"unread":true,"content":"<p>I've made a quick mock situation which is analogous to my situation the other day:</p><pre><code>fn main() { let mut v: Option&lt;Vec&lt;usize&gt;&gt; = None; let mut h = 20; while h.ne(&amp;0) { if (h % 3).ge(&amp;1) { match v { Some(ref mut v) =&gt; (*v).push(h), None =&gt; v = Some(vec![h]) } } h -= 1 } println!(\"{v:?}\") } </code></pre><p>I was a bit confused on how it \"should\" be solved. My issue is the \"ref mut\". It made sense to me that I didn't want to consume the vector v, just add to it if it existed and I tried adding ref (then mut), which worked. When I goodled, it seemed ref was a legacy thing and not needed anymore. My question is, how is the idiomatic way to write this? Perhaps it's possible to do in a much simpler way and I just found a way to complicate it for no reason.</p><p>Also, don't worry I know this is a terrible pattern, it was mostly for tesing something.</p>","contentLength":828,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Macro-Less, Highly Integrated OpenAPI Document Generation in Rust with Ohkami","url":"https://medium.com/@kanarus786/macro-less-highly-integrated-openapi-document-generation-in-rust-with-ohkami-912de388adc1","date":1739557605,"author":"/u/kanarus","guid":620,"unread":true,"content":"<p>In Rust web dev, <a href=\"https://github.com/juhaku/utoipa\" rel=\"noopener ugc nofollow\" target=\"_blank\">utoipa</a> is the most popular crate for generating OpenAPI document from server code. While it‚Äôs a great tool, it can be frustrating due to excessive macro use.</p><p>A new web framework Ohkami offers a ,  way to generate OpenAPI document with its ‚Äúopenapi‚Äù feature.</p><p>Let‚Äôs take following code as an example. It‚Äôs the same sample from the ‚Äúopenapi‚Äù section of the README, but with openapi-related parts removed:</p><pre></pre><p>While this compiles and works as a pseudo user management server, activating ‚Äúopenapi‚Äù feature causes a compile error, telling that User and CreateUser don‚Äôt implement ohkami::openapi::Schema.</p><p>As indicated by this, Ohkami with ‚Äúopenapi‚Äù feature effectively handles type information and intelligently collects its endpoints‚Äô metadata. It allows code like:</p><pre></pre><p>to assemble metadata into an OpenAPI document and output it to a file .</p><p>Then, how we implement Schema? Actually we can easily impl Schema by hand, or just #[derive(Schema)] is available! In this case, derive is enough:</p><pre></pre><p>That‚Äôs it! Just adding these derives allows Ohkami::generate to output following file:</p><pre></pre><p>Additionally, it‚Äôs easy to define the User schema as a component instead of duplicating inline schemas. In derive, just add #[openapi(component)] helper attribute:</p><pre></pre><pre></pre><p>And  #[operation] attribute is available to set summary, description, and override operationId and each response‚Äôs description:</p><pre></pre><pre></pre><p>Let‚Äôs take a look at how this document generation works!</p><p>First, the #[derive(Schema)]s are expanded as following:</p><pre></pre><pre></pre><p>The DSL enables to easily impl manually.</p><p>Schema trait links the struct to an item of type called ‚ÄúSchemaRef‚Äù.</p><h2>2. openapi_* hooks of FromParam, FromRequest, IntoResponse</h2><p>FromParam, FromRequest and IntoResponse are Ohkami‚Äôs core traits appeared in the handler bound:</p><pre></pre><p>When ‚Äúopenapi‚Äù feature is activated, they additionally have following methods:</p><pre></pre><p>Ohkami leverages these methods in IntoHandler to generate consistent openapi::Operation, reflecting the actual handler signature like <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/fang/handler/into_handler.rs#L328-L335\" rel=\"noopener ugc nofollow\" target=\"_blank\">this</a>.</p><p>Moreover, Ohkami properly propagates schema information in common cases like <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/response/into_response.rs#L114-L128\" rel=\"noopener ugc nofollow\" target=\"_blank\">this</a>, allowing users to focus only on the types and schemas of their app.</p><h2>3. routes metadata of Router</h2><p>In Ohkami, what‚Äôs called router::base::Router <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/router/base.rs#L8-L18\" rel=\"noopener ugc nofollow\" target=\"_blank\">has ‚Äúroutes‚Äù property</a> that stores all the routes belonging to an Ohkami instance. This is returned alongside router::final::Router from ‚Äúfinalize‚Äù step, and is used to assemble metadata of all endpoints.</p><p>What Ohkami::generate itself does is just to serialize an item of type openapi::document::Document and write it to a file.</p><p>The openapi::document::Document item is created by<a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/router/final.rs#L54-L59\" rel=\"noopener ugc nofollow\" target=\"_blank\"> ‚Äúgen_openapi_doc‚Äù of router::final::Router</a>, summarized as follows:</p><pre></pre><p>That‚Äôs how Ohkami generates OpenAPI document!</p><p>There is, however, a problem in , Cloudflare Workers: Ohkami is loaded to Miniflare or Cloudflare Workers as WASM, so it can only generate OpenAPI document andcannot write it to the user‚Äôs local file system.</p><p>To work around this, Ohkami provides a CLI tool <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/scripts/workers_openapi.js\" rel=\"noopener ugc nofollow\" target=\"_blank\">scripts/workers_openapi.js</a>. This is, for example, used in package.json of Cloudflare Workers + OpenAPI template:</p><pre></pre><p>generates OpenAPI document!</p>","contentLength":3119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iph9lk/macroless_highly_integrated_openapi_document/"},{"title":"OpenAI: The Age of AI Is Here!","url":"https://www.youtube.com/watch?v=97kQRYwL3P0","date":1739557087,"author":"Two Minute Papers","guid":522,"unread":true,"content":"<article>‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nüìù The paper \"Competitive Programming with Large Reasoning Models\" is available here:\nhttps://arxiv.org/abs/2502.06807\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu</article>","contentLength":1040,"flags":null,"enclosureUrl":"https://www.youtube.com/v/97kQRYwL3P0?version=3","enclosureMime":"","commentsUrl":null},{"title":"[P] GNNs for time series anomaly detection","url":"https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/","date":1739555819,"author":"/u/Important-Gear-325","guid":635,"unread":true,"content":"<p>For the past few months, my partner and I have been working on a project exploring the use of Graph Neural Networks (GNNs) for Time Series Anomaly Detection (TSAD). As we are near the completion of our work, I‚Äôd love to get feedback from this amazing community!</p><p>Any comments, suggestions, or discussions are more than welcome! If you find the repo interesting, dropping a ‚≠ê would mean a lot. : )</p><p>We're also planning to publish a detailed report with our findings and insights in the coming months, so stay tuned!</p><p>The repo is still under development so don't be too harsh :)</p><p>Looking forward to hearing your thoughts!</p>","contentLength":615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Roadmap to Becoming a Data Scientist, Part 4: Advanced Machine Learning","url":"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-4-advanced-machine-learning/","date":1739552400,"author":"Vyacheslav Efimov","guid":10,"unread":true,"content":"<p>Data science is undoubtedly one of the most fascinating fields today.&nbsp;Following significant breakthroughs in machine learning about a decade ago, data science has surged in popularity within the tech community.&nbsp;Each year, we witness increasingly powerful tools that once seemed unimaginable.&nbsp;Innovations such as the&nbsp;,&nbsp;, the&nbsp;<em>Retrieval-Augmented Generation (RAG</em>) framework, and state-of-the-art&nbsp;&nbsp;‚Äî including&nbsp;&nbsp;‚Äî have had a profound impact on our world.</p><p>However, with the abundance of tools and the ongoing hype surrounding AI, it can be overwhelming ‚Äî especially for beginners ‚Äî to determine which skills to prioritize when aiming for a career in data science.&nbsp;Moreover, this field is highly demanding, requiring substantial dedication and perseverance.</p><p>The first three parts of this series outlined the necessary skills to become a data scientist in three key areas: <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-1-maths-2dc9beb69b27/\">math</a>, <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-2-software-engineering-e2fee3fe4d71/\">software engineering</a>, and <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-3-machine-learning-628248c96cb5/\">machine learning</a>.&nbsp;While knowledge of classical <a href=\"https://towardsdatascience.com/tag/machine-learning/\" title=\"Machine Learning\">Machine Learning</a> and neural network algorithms is an excellent starting point for aspiring data specialists, there are still many&nbsp;important topics in machine learning that must be mastered to work on more advanced projects.</p><blockquote><p><em>This article will focus solely on the math skills necessary to start a career in Data Science.&nbsp;Whether pursuing this path is a worthwhile choice based on your background and other factors will be discussed in a separate article.</em></p></blockquote><h2>The importance of learning evolution of methods in machine learning</h2><blockquote><p><em>The section below provides information about the evolution of methods in natural language processing (NLP).</em></p></blockquote><p>In contrast to previous articles in this series, I have decided to change the format in which I present the necessary skills for aspiring data scientists. Instead of directly listing specific competencies to develop and the motivation behind mastering them, I will briefly outline the most important approaches, presenting them in chronological order as they have been developed and used over the past decades in machine learning.</p><p>The reason is that I believe it is crucial to study these algorithms from the very beginning. In machine learning, many new methods are built upon older approaches, which is especially true for <a href=\"https://towardsdatascience.com/tag/nlp/\" title=\"NLP\">NLP</a> and computer vision.</p><p>For example, jumping directly into the implementation details of modern&nbsp;large language models (LLMs)&nbsp;without any preliminary knowledge may make it very difficult for beginners to grasp the motivation and underlying ideas of specific mechanisms.</p><p><em>Given this, in the next two sections, I will highlight in&nbsp;</em><em>&nbsp;the key concepts that should be studied.</em></p><p><strong>Natural language processing (NLP)</strong>&nbsp;is a broad field that focuses on processing textual information. Machine learning algorithms cannot work directly with raw text, which is why text is usually preprocessed and converted into numerical vectors that are then fed into neural networks.</p><p>Before being converted into vectors, words undergo&nbsp;, which includes simple techniques such as&nbsp;,&nbsp;<strong>stemming, lemmatization, normalization</strong>, or removing&nbsp;. After preprocessing, the resulting text is encoded into&nbsp;. Tokens represent the smallest textual elements in a collection of documents. Generally, a token can be a part of a word, a sequence of symbols, or an individual symbol. Ultimately, tokens are converted into numerical vectors.</p><p>The&nbsp;&nbsp;method is the most basic way to encode tokens, focusing on counting the frequency of tokens in each document. However, in practice, this is usually not sufficient, as it is also necessary to account for token importance ‚Äî a concept introduced in the&nbsp;&nbsp;and&nbsp;&nbsp;methods. While TF-IDF improves upon the naive counting approach of bag of words, researchers have developed a completely new approach called embeddings.</p><p>&nbsp;are numerical vectors whose components preserve the semantic meanings of words. Because of this, embeddings play a crucial role in NLP, enabling input data to be trained or used for model inference. Additionally, embeddings can be used to compare text similarity, allowing for the retrieval of the most relevant documents from a collection.</p><blockquote><p><em>Embeddings can also be used to encode other unstructured data, including images, audio, and videos.</em></p></blockquote><p>As a field, NLP has been evolving rapidly over the last 10‚Äì20 years to efficiently solve various text-related problems. Complex tasks like text translation and text generation were initially addressed using&nbsp;<strong>recurrent neural networks (RNNs)</strong>, which introduced the concept of memory, allowing neural networks to capture and retain key contextual information in long documents.</p><p>Although RNN performance gradually improved, it remained suboptimal for certain tasks. Moreover, RNNs are relatively slow, and their sequential prediction process does not allow for parallelization during training and inference, making them less efficient.</p><p>Additionally, the original Transformer architecture can be decomposed into two separate modules:&nbsp;&nbsp;and&nbsp;. Both of these form the foundation of the most state-of-the-art models used today to solve various NLP problems. Understanding their principles is valuable knowledge that will help learners advance further when studying or working with other&nbsp;<strong>large language models (LLMs)</strong>.</p><p>When it comes to LLMs, I strongly recommend studying the evolution of at least the first three GPT models, as they have had a significant impact on the AI world we know today. In particular, I would like to highlight the concepts of&nbsp;&nbsp;and&nbsp;, introduced in&nbsp;GPT-2, which enable LLMs to solve text generation tasks without explicitly receiving any training examples for them.</p><p>Another important technique developed in recent years is&nbsp;<strong>retrieval-augmented generation (RAG)</strong>.&nbsp;<em>The main limitation of LLMs is that they are only aware of the context used during their training.</em>&nbsp;As a result, they lack knowledge of any information beyond their training data.</p><p>The retriever converts the input prompt into an embedding, which is then used to query a vector database. The database returns the most relevant context based on the similarity to the embedding. This retrieved context is then combined with the original prompt and passed to a generative model. The model processes both the initial prompt and the additional context to generate a more informed and contextually accurate response.</p><blockquote><p><em>A good example of this limitation is the first version of the ChatGPT model, which was trained on data up to the year 2022 and had no knowledge of events that occurred from 2023 onward.</em></p></blockquote><p>To address this limitation, OpenAI researchers developed a RAG pipeline, which includes a constantly updated database containing new information from external sources. When ChatGPT is given a task that requires external knowledge, it queries the database to retrieve the most relevant context and integrates it into the final prompt sent to the machine learning model.</p><p>The goal of distillation is to create a smaller model that can imitate a larger one. In practice, this means that if a large model makes a prediction, the smaller model is expected to produce a similar result.</p><p>In the modern era, LLM development has led to models with millions or even billions of parameters. As a consequence, the overall size of these models may exceed the hardware limitations of standard computers or small portable devices, which come with many constraints.</p><p>Quantization is the process of reducing the memory required to store numerical values representing a model‚Äôs weights.</p><p>This is where optimization techniques become particularly useful, allowing LLMs to be compressed without significantly compromising their performance. The most commonly used techniques today include&nbsp;,, and&nbsp;.</p><p>Pruning refers to discarding the least important weights of a model.</p><p>Regardless of the area in which you wish to specialize, knowledge of&nbsp;&nbsp;is a must-have skill! Fine-tuning is a powerful concept that allows you to efficiently adapt a pre-trained model to a new task.</p><p>Fine-tuning is especially useful when working with very large models. For example, imagine you want to use BERT to perform semantic analysis on a specific dataset. While BERT is trained on general data, it might not fully understand the context of your dataset. At the same time, training BERT from scratch for your specific task would require a massive amount of resources.</p><p>Here is where fine-tuning comes in: it involves taking a pre-trained BERT (or another model) and freezing some of its layers (usually those at the beginning). As a result, BERT is retrained, but this time only on the new dataset provided. Since BERT updates only a subset of its weights and the new dataset is likely much smaller than the original one BERT was trained on, fine-tuning becomes a very efficient technique for adapting BERT‚Äôs rich knowledge to a specific domain.</p><blockquote><p><em>Fine-tuning is widely used not only in NLP but also across many other domains.</em></p></blockquote><p>As the name suggests,&nbsp;&nbsp;involves analyzing images and videos using machine learning. The most common tasks include image classification, object detection, image segmentation, and generation.</p><p>Most CV algorithms are based on neural networks, so it is essential to understand how they work in detail. In particular, CV uses a special type of network called&nbsp;<strong>convolutional neural networks (CNNs)</strong>. These are similar to fully connected networks, except that they typically begin with a set of specialized mathematical operations called&nbsp;.</p><blockquote><p><em>In simple terms, convolutions act as filters, enabling the model to extract the most important features from an image, which are then passed to fully connected layers for further analysis.</em></p></blockquote><p>The next step is to study the most popular CNN architectures for classification tasks, such as&nbsp;<strong>AlexNet, VGG, Inception, ImageNet</strong>, and&nbsp;.</p><p>Speaking of the object detection task, the&nbsp;&nbsp;algorithm is a clear winner. It is not necessary to study all of the dozens of versions of YOLO. In reality, going through the original paper of the first YOLO should be sufficient to understand how a relatively difficult problem like object detection is elegantly transformed into both classification and regression problems. This approach in YOLO also provides a nice intuition on how more complex CV tasks can be reformulated in simpler terms.</p><p>While there are many architectures for performing image segmentation, I would strongly recommend learning about&nbsp;, which introduces an encoder-decoder architecture.</p><p>Finally, image generation is probably one of the most challenging tasks in CV. Personally, I consider it an optional topic for learners, as it involves many advanced concepts. Nevertheless, gaining a high-level intuition of how&nbsp;<strong>generative adversial networks (GAN)</strong>&nbsp;function to generate images is a good way to broaden one‚Äôs horizons.</p><blockquote><p><em>In some problems, the training data might not be enough to build a performant model. In such cases, the data augmentation technique is commonly used. It involves the artificial generation of training data from already existing data (images). By feeding the model more diverse data, it becomes capable of learning and recognizing more patterns.</em></p></blockquote><p>It would be very hard to present in detail the <a href=\"https://towardsdatascience.com/tag/roadmaps/\" title=\"Roadmaps\">Roadmaps</a> for all existing machine learning domains in a single article. That is why, in this section, I would like to briefly list and explain some of the other most popular areas in data science worth exploring.</p><p>First of all,&nbsp;<strong>recommender systems (RecSys)</strong>&nbsp;have gained a lot of popularity in recent years. They are increasingly implemented in online shops, social networks, and streaming services. The key idea of most algorithms is to take a large initial matrix of all users and items and decompose it into a product of several matrices in a way that associates every user and every item with a high-dimensional embedding. This approach is very flexible, as it then allows different types of comparison operations on embeddings to find the most relevant items for a given user. Moreover, it is much more rapid to perform analysis on small matrices rather than the original, which usually tends to have huge dimensions.</p><p> often goes hand in hand with RecSys. When a RecSys has identified a set of the most relevant items for the user, ranking algorithms are used to sort them to determine the order in which they will be shown or proposed to the user. A good example of their usage is search engines, which filter query results from top to bottom on a web page.</p><p>Closely related to ranking, there is also a&nbsp;&nbsp;problem that aims to optimally map objects from two sets, A and B, in a way that, on average, every object pair&nbsp;is mapped ‚Äúwell‚Äù according to a matching criterion. A use case example might include distributing a group of students to different university disciplines, where the number of spots in each class is limited.</p><p>&nbsp;is an unsupervised machine learning task whose objective is to split a dataset into several regions (clusters), with each dataset object belonging to one of these clusters. The splitting criteria can vary depending on the task. Clustering is useful because it allows for grouping similar objects together. Moreover, further analysis can be applied to treat objects in each cluster separately.</p><p>The goal of clustering is to group dataset objects (on the left) into several categories (on the right) based on their similarity.</p><p>&nbsp;is another unsupervised problem, where the goal is to compress an input dataset. When the dimensionality of the dataset is large, it takes more time and resources for machine learning algorithms to analyze it. By identifying and removing noisy dataset features or those that do not provide much valuable information, the data analysis process becomes considerably easier.</p><p>&nbsp;is an area that focuses on designing algorithms and data structures (indexes) to optimize searches in a large database of embeddings (vector database). More precisely, given an input embedding and a vector database, the goal is to&nbsp;&nbsp;find the most similar embedding in the database relative to the input embedding.</p><p>The goal of similarity search is to approximately find the most similar embedding in a vector database relative to a query embedding.</p><p>The word ‚Äúapproximately‚Äù means that the search is not guaranteed to be 100% precise. Nevertheless, this is the main idea behind similarity search algorithms ‚Äî sacrificing a bit of accuracy in exchange for significant gains in prediction speed or data compression.</p><p>&nbsp;involves studying the behavior of a target variable over time. This problem can be solved using classical tabular algorithms. However, the presence of time introduces new factors that cannot be captured by standard algorithms. For instance:</p><ul><li>the target variable can have an overall&nbsp;, where in the long term its values increase or decrease&nbsp;<em>(e.g., the average yearly temperature rising due to global warming)</em>.</li><li>the target variable can have a&nbsp;&nbsp;which makes its values change based on the currently given period&nbsp;<em>(e.g. temperature is lower in winter and higher in summer)</em>.</li></ul><p>Most of the time series models take both of these factors into account. In general, time series models are mainly used a lot in financial, stock or demographic analysis.</p><p>Another advanced area I would recommend exploring is&nbsp;, which fundamentally changes the algorithm design compared to classical machine learning.&nbsp;In simple terms, its goal is to train an agent in an environment to make optimal decisions based on a reward system (also known as the&nbsp;<em>‚Äútrial and error approach‚Äù</em>).&nbsp;By taking an action, the agent receives a reward, which helps it understand whether the chosen action had a positive or negative effect.&nbsp;After that, the agent slightly adjusts its strategy, and the entire cycle repeats.</p><p>Reinforcement learning is particularly popular in complex environments where classical algorithms are not capable of solving a problem.&nbsp;Given the complexity of reinforcement learning algorithms and the computational resources they require, this area is not yet fully mature, but it has high potential to gain even more popularity in the future.</p><p>Currently the most popular applications are:</p><ul><li>.&nbsp;Existing approaches can design optimal game strategies and outperform humans.&nbsp;The most well-known examples are chess and Go.</li><li>.&nbsp;Advanced algorithms can be incorporated into robots to help them move, carry objects or complete routine tasks at home.</li><li>.&nbsp;Reinforcement learning methods can be developed to automatically drive cars, control helicopters or drones.</li></ul><p>This article was a logical continuation of the previous part and expanded the skill set needed to become a data scientist. While most of the mentioned topics require time to master, they can add significant value to your portfolio. This is especially true for the NLP and CV domains, which are in high demand today.</p><blockquote><p>After reaching a high level of expertise in data science, it is still crucial to stay motivated and consistently push yourself to learn new topics and explore emerging algorithms.</p></blockquote><p>Data science is a constantly evolving field, and in the coming years, we might witness the development of new state-of-the-art approaches that we could not have imagined in the past.</p><p><em>All images are by the author unless noted otherwise.</em></p>","contentLength":17044,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Siren Call of SQLite on the Server","url":"https://pid1.dev/posts/siren-call-of-sqlite-on-the-server/","date":1739551579,"author":"/u/sausagefeet","guid":640,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipevoh/siren_call_of_sqlite_on_the_server/"},{"title":"Unlocking global AI potential with next-generation subsea infrastructure","url":"https://engineering.fb.com/2025/02/14/connectivity/project-waterworth-ai-subsea-infrastructure/","date":1739550486,"author":"","guid":472,"unread":true,"content":"<p><a href=\"https://globaldigitalinclusion.org/wp-content/uploads/2024/01/GDIP-Good-Practices-for-Subsea-Cables-Policy-Investing-in-Digital-Inclusion.pdf\"></a></p><p><a href=\"https://engineering.fb.com/2021/03/28/connectivity/echo-bifrost/\"></a><a href=\"https://engineering.fb.com/2021/09/28/connectivity/2africa-pearls/\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm very impressed by how Rust supports both beginners and pro's","url":"https://www.reddit.com/r/rust/comments/1ipe6m7/im_very_impressed_by_how_rust_supports_both/","date":1739549777,"author":"/u/ConstructionShot2026","guid":624,"unread":true,"content":"<p>I would go as far saying it supports a syntax abstraction that is simpler than python to read.</p><p>I just find it amazing, with a performance level so close to C++.</p><p>Its your choice how many complex features you want to add for control and optimization, and the compiler is so cool, that it can add them automatically if I don't see it necessary.</p><p>I believe if more knew how simple it could be, more would use it outside systems programming :D</p>","contentLength":434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Publish Interactive Data Visualizations for Free with Python and Marimo","url":"https://towardsdatascience.com/publish-interactive-data-visualizations-for-free-with-python-and-marimo/","date":1739548800,"author":"Sam Minot","guid":9,"unread":true,"content":"<p>Working in <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a>, it can be hard to share insights from complex datasets using only static figures. All the facets that describe the shape and meaning of interesting data are not always captured in a handful of pre-generated figures. While we have powerful technologies available for presenting interactive figures‚Ää‚Äî‚Ääwhere a viewer can rotate, filter, zoom, and generally explore complex data‚Ää ‚Äî‚Ää they always come with tradeoffs.</p><p>Here I present my experience using a recently released Python library‚Ää‚Äî‚Ää<a href=\"https://marimo.io\">marimo</a>‚Ää‚Äî‚Ääwhich opens up exciting new opportunities for publishing interactive visualizations across the entire field of data science.</p><h2>Interactive Data Visualization</h2><p>The tradeoffs to consider when selecting an approach for presenting data visualizations can be broken into three categories:</p><ul><li>‚Ää‚Äî‚Ääwhat visualizations and interactivity am I able to present to the user?</li><li>‚Ää‚Äî‚Ääwhat are the resources needed for displaying this visualization to users (e.g. running servers, hosting websites)?</li><li> ‚Äì how much of a new skillset / codebase do I need to learn upfront?</li></ul><p> is the foundation of portable interactivity. Every user has a web browser installed on their computer and there are many different frameworks available for displaying any degree of interactivity or visualization you might imagine (for example, this <a href=\"https://threejs.org/\">gallery of amazing things people have made with three.js</a>). Since the application is running on the user‚Äôs computer, no costly servers are needed. However, a significant drawback for the data science community is ease of use, as JS does not have many of the high-level (i.e. easy-to-use) libraries that data scientists use for data manipulation, plotting, and interactivity.</p><p> provides a useful point of comparison. Because of its <a href=\"https://flatironschool.com/blog/python-popularity-the-rise-of-a-global-programming-language/\">continually growing popularity</a>, some have called this the <a href=\"https://towardsdatascience.com/we-are-living-in-the-era-of-python-bc032d595f6a\">‚ÄúEra of Python‚Äù</a>. For data scientists in particular, Python stands alongside R as one of the foundational languages for quickly and effectively wielding complex data. While Python may be easier to use than Javascript, there are fewer options for presenting interactive visualizations. Some popular projects providing interactivity and visualization have been <a href=\"https://flask.palletsprojects.com/en/stable/\">Flask</a>, <a href=\"https://dash.plotly.com/\">Dash</a>, and <a href=\"https://streamlit.io/\">Streamlit</a> (also worth mentioning‚Ää‚Äî‚Ää<a href=\"https://docs.bokeh.org/en/latest/docs/gallery.html\">bokeh</a>, <a href=\"https://holoviews.org/\">HoloViews</a>, <a href=\"https://altair-viz.github.io/altair-tutorial/README.html\">altair</a>, and <a href=\"https://plotly.com/python/\">plotly</a>). The biggest tradeoff for using Python has been the cost for publishing ‚Äì delivering the tool to users. In the same way that <a href=\"https://www.shinyapps.io/\">shinyapps</a> require a running computer to serve up the visualization, these Python-based frameworks have exclusively been server-based. This is by no means prohibitive for authors with a budget to spend, but it does limit the number of users who can take advantage of a particular project.</p><p><a href=\"https://pyodide.org/en/stable/\"></a> is an intriguing middle ground‚Ää‚Äî‚ÄäPython code running directly in the web browser using <a href=\"https://webassembly.org/\">WebAssembly</a> (WASM). There are resource limitations (only 1 thread and 2GB memory) that make this impractical for doing the heavy lifting of data science. , this can be more than sufficient for building visualizations and updating based on user input. Because it runs in the browser, no servers are required for hosting. Tools that use Pyodide as a foundation are interesting to explore because they give data scientists an opportunity to write Python code which runs directly on users‚Äô computers without their having to install or run anything outside of the web browser.</p><p>As an aside, <a href=\"https://towardsdatascience.com/python-based-data-viz-with-no-installation-required-aaf2358c881\">I‚Äôve been interested previously in</a> one project that has tried this approach: <a href=\"https://github.com/whitphx/stlite\">stlite</a>, <a href=\"https://edit.share.stlite.net/\">an in-browser implementation of Streamlit</a> that lets you deploy these flexible and powerful apps to a broad range of users. However, a core limitation is that Streamlit itself is distinct from stlite (the port of Streamlit to WASM), which means that not all features are supported and that advancement of the project is dependent on two separate groups working along compatible lines.</p><ul><li>The interface resembles a Jupyter , which will be familiar to users.</li><li>Execution of cells is , so that updating one cell will rerun all cells which depend on its output.</li><li> can be captured with a flexible set of UI components.</li><li>Notebooks can be quickly converted into , hiding the code and showing only the input/output elements.</li><li>Apps can be run locally or converted into using WASM/Pyodide.</li></ul><p>marimo balances the tradeoffs of technology in a way that is well suited to the skill set of the typical data scientists:</p><ul><li>‚Ää‚Äî‚Ääuser input and visual display features are rather extensive, <a href=\"https://docs.marimo.io/guides/working_with_data/plotting/#reactive-plots\">supporting user input</a> via Altair and Plotly plots.</li><li>‚Ää‚Äî‚Äädeploying as static webpages is basically free‚Ää‚Äî‚Ääno servers required</li><li>‚Ää‚Äî‚Ääfor users familiar with Python notebooks, marimo will feel very familiar and be easy to pick up.</li></ul><h2>Publishing Marimo Apps on the Web</h2><p>As a simple example of the type of display that can be useful in data science, consisting of explanatory text interspersed with interactive displays, I have created a barebones <a href=\"https://github.com/FredHutch/marimo-publication\">GitHub repository</a>. Try it out yourself <a href=\"https://fredhutch.github.io/marimo-publication/\">here</a>.</p><p>Using just a little bit of code, users can:</p><ul><li>Generate visualizations with flexible interactivity</li><li>Write narrative text describing their findings</li><li>Publish to the web for free (i.e. using GitHub Pages)</li></ul><h2>Public App / Private Data</h2><p>This new technology offers an exciting new opportunity for collaboration‚Ää‚Äî‚Ääpublish the app publicly to the world, but users can only see specific datasets that they have permission to access.</p><p>Rather than building a dedicated data backend for every app, user data can be stored in a generic backend which can be securely authenticated and accessed using a Python client library‚Ää‚Äî‚Ääall contained within the user‚Äôs web browser. For example, the user is given an OAuth login link that will authenticate them with the backend and allow the app to temporarily access input data.</p><p>As a proof of concept, I built a simple visualization app which connects to <a href=\"https://cirro.bio\">the Cirro data platform</a>, which is used at my institution to manage scientific data. Full disclosure: I was part of the team that built this platform before it spun out as an independent company. In this manner users can:</p><ul><li>Load the public visualization app‚Ää‚Äî‚Äähosted on GitHub Pages</li><li>Connect securely to their private data store</li><li>Load the appropriate dataset for display</li><li>Share a link which will direct authorized collaborators to the same data</li></ul><p>As a data scientist, this approach of publishing free and open-source visualization apps which can be used to interact with private datasets is extremely exciting. Building and publishing a new app can take hours and days instead of weeks and years, letting researchers quickly share their insights with collaborators and then publish them to the wider world.</p>","contentLength":6594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hugo van Kemenade: Improving licence metadata","url":"https://hugovk.dev/blog/2025/improving-licence-metadata/","date":1739545860,"author":"","guid":283,"unread":true,"content":"<p><a href=\"https://peps.python.org/pep-0639/\" target=\"_blank\" rel=\"noreferrer\">PEP 639</a> defines a spec on how to document licences\nused in Python projects.</p><p>Change  as follows.</p><p>I usually use Hatchling as a build backend, and support was added in 1.27:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Replace the freeform  field with a valid SPDX license expression, and add\n which points to the licence files in the repo. There‚Äôs often only one,\nbut if you have more than one, list them all:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Optionally delete the deprecated licence classifier:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Then make sure to use a PyPI uploader that supports this.</p><p>pip can also show you the metadata:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>A lot of work went into this. Thank you to PEP authors\n<a href=\"https://github.com/pombredanne\" target=\"_blank\" rel=\"noreferrer\">Philippe Ombredanne</a> for creating the first draft in\n2019, to <a href=\"https://github.com/cam-gerlach\" target=\"_blank\" rel=\"noreferrer\">C.A.M. Gerlach</a> for the second draft in 2021,\nand especially to <a href=\"https://karolinasurma.eu/\" target=\"_blank\" rel=\"noreferrer\">Karolina Surma</a> for getting the third\ndraft finish line and helping with the implementation.</p><p>And many projects were updated to support this, thanks to the maintainers and\ncontributors of at least:</p>","contentLength":878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 Tips for Building a Data Science Portfolio","url":"https://www.kdnuggets.com/5-tips-building-data-science-portfolio","date":1739545225,"author":"Nate Rosidi","guid":590,"unread":true,"content":"<article>Not every data science portfolio is worth showcasing. Follow these five tips to build a portfolio that impresses employers and gets you a job.</article>","contentLength":142,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Rosidi_5_Tips_for_Building_a_DS_Portfolio_4.png","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Transform your codebase into a single Markdown doc for feeding into AI","url":"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html","date":1739539403,"author":"tesserato","guid":323,"unread":true,"content":"<p>CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.\nThe output for the current repository can be found <a href=\"https://github.com/tesserato/CodeWeaver/blob/main/codebase.md\">here</a>.</p><ul><li><strong>Comprehensive Codebase Documentation:</strong> Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format.</li><li> Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.</li><li>  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types).</li><li> Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.</li><li><strong>Simple Command-Line Interface:</strong>  Offers an intuitive command-line interface with straightforward options for customization.</li></ul><p>If you have Go installed, run <code>go install github.com/tesserato/CodeWeaver@latest</code>to install the latest version of CodeWeaver or <code>go install github.com/tesserato/CodeWeaver@vX.Y.Z</code> to install a specific version.</p><p>Alternatively, download the appropriate pre built executable from the <a href=\"https://github.com/tesserato/CodeWeaver/releases\">releases page</a>.</p><p>If necessary, make the  executable by using the  command:</p><table><thead><tr></tr></thead><tbody><tr><td>The root directory to scan and document.</td></tr><tr><td>The name of the output Markdown file.</td></tr><tr><td><code>-ignore \"&lt;regex patterns&gt;\"</code></td><td>Comma-separated list of regular expression patterns for paths to exclude.</td></tr><tr><td><code>-included-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were included in the documentation.</td></tr><tr><td><code>-excluded-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were excluded from the documentation.</td></tr><tr><td>Display this help message and exit.</td></tr></tbody></table><h2><strong>Generate documentation for the current directory:</strong></h2><p>This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).</p><h2><strong>Specify a different input directory and output file:</strong></h2><pre><code>./codeweaver -dir=my_project -output=project_docs.md\n</code></pre><p>This command will process the  directory and save the documentation to .</p><h2><strong>Ignore specific file types and directories:</strong></h2><pre><code>./codeweaver -ignore=\"\\.log,temp,build\" -output=detailed_docs.md\n</code></pre><p>This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.</p><h2><strong>Save lists of included and excluded paths:</strong></h2><pre><code>./codeweaver -ignore=\"node_modules\" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md\n</code></pre><p>This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .</p><p>Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.</p><p>CodeWeaver is released under the <a href=\"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/LICENSE\">MIT License</a>. See the  file for complete license details.</p>","contentLength":3311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43048027"},{"title":"Show HN: A New Way to Learn Languages","url":"https://www.langturbo.com/","date":1739534938,"author":"sebnun","guid":322,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43047554"},{"title":"Real Python: The Real Python Podcast ‚Äì Episode #239: Behavior-Driven vs Test-Driven Development &amp; Using Regex in Python","url":"https://realpython.com/podcasts/rpp/239/","date":1739534400,"author":"","guid":185,"unread":true,"content":"<p>What is behavior-driven development, and how does it work alongside test-driven development? How do you communicate requirements between teams in an organization? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.</p>","contentLength":279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Daniel Roy Greenfeld: Building a playing card deck","url":"https://daniel.feldroy.com/posts/2025-02-deck-of-cards","date":1739526604,"author":"","guid":282,"unread":true,"content":"<article>Today is Valentine's Day. That makes it the perfect day to write a blog post about showing how to not just build a deck of cards, but show off cards from the heart suite.</article>","contentLength":170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a Data Engineering Center of Excellence","url":"https://towardsdatascience.com/building-a-data-engineering-center-of-excellence/","date":1739500548,"author":"Richie Bachala","guid":8,"unread":true,"content":"<p>As data continues to grow in importance and become more complex, the need for skilled data engineers has never been greater. But what is data engineering, and why is it so important? In this blog post, we will discuss the essential components of a functioning data engineering practice and why data engineering is becoming increasingly critical for businesses today, and how you can build your very own Data Engineering Center of Excellence!</p><p>I‚Äôve had the privilege to build, manage, lead, and foster a sizeable high-performing team of data warehouse &amp; ELT engineers for many years. With the help of my team, I have spent a considerable amount of time every year consciously planning and preparing to manage the growth of our data month-over-month and address the changing reporting and analytics needs for our&nbsp;<em>20000+ global data consumers</em>. We built many data warehouses to store and centralize massive amounts of data generated from many OLTP sources. We‚Äôve implemented Kimball methodology by creating star schemas both within our on-premise data warehouses and in the ones in the cloud.</p><p>The objective is to enable our user-base to perform fast analytics and reporting on the data; so our analysts‚Äô community and business users can make accurate data-driven decisions.</p><p>It took me about three years to transform&nbsp;&nbsp;() of data warehouse and ETL programmers into one cohesive Data Engineering team.</p><p><em>I have compiled some of my learnings building a global data engineering team in this post in hopes that Data professionals and leaders of all levels of technical proficiency can benefit.</em></p><h2>Evolution of the Data Engineer</h2><p>It has never been a better time to be a data engineer. Over the last decade, we have seen a massive awakening of enterprises now recognizing their data as the company‚Äôs heartbeat, making data engineering the job function that ensures accurate, current, and quality data flow to the solutions that depend on it.</p><p>Historically, the role of Data Engineers has evolved from that of&nbsp;<strong><em>data warehouse developers&nbsp;</em></strong>and the&nbsp;&nbsp;(extract, transform and load).</p><p>The data warehouse developers are responsible for designing, building, developing, administering, and maintaining data warehouses to meet an enterprise‚Äôs reporting needs. This is done primarily via extracting data from operational and transactional systems and piping it using extract transform load methodology (ETL/ ELT) to a storage layer like a data warehouse or a data lake. The data warehouse or the data lake is where data analysts, data scientists, and business users consume data. The developers also perform transformations to conform the ingested data to a data model with aggregated data for easy analysis.</p><blockquote><p>A data engineer‚Äôs prime responsibility is to produce and make data securely available for multiple consumers.</p></blockquote><p>Data engineers oversee the ingestion, transformation, modeling, delivery, and movement of data through every part of an organization. Data extraction happens from many different data sources &amp; applications. Data Engineers load the data into data warehouses and data lakes, which are transformed not just for the <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a> &amp; predictive analytics initiatives (as everyone likes to talk about) but primarily for data analysts. Data analysts &amp; data scientists perform operational reporting, exploratory analytics, service-level agreement (SLA) based business intelligence reports and dashboards on the catered data. In this book, we will address all of these job functions.</p><p>The role of a data engineer is to acquire, store, and aggregate data from both cloud and on-premise, new, and existing systems, with data modeling and feasible data architecture. Without the data engineers, analysts and data scientists won‚Äôt have valuable data to work with, and hence, data engineers are the first to be hired at the inception of every new data team. Based on the data and analytics tools available within an enterprise, data engineering teams‚Äô role profiles, constructs, and approaches have several options for what should be included in their responsibilities which we will discuss in this chapter.</p><p>Software is increasingly automating the historically manual and tedious tasks of data engineers. Data processing tools and technologies have evolved massively over several years and will continue to grow. For example, cloud-based data warehouses (Snowflake, for instance) have made data storage and processing affordable and fast. Data pipeline services (like&nbsp;<a href=\"https://www.informatica.com/blogs/welcome-to-informatica-intelligent-cloud-services.html\" rel=\"noreferrer noopener\" target=\"_blank\">Informatica IICS</a>,&nbsp;<a href=\"https://airflow.apache.org/\" rel=\"noreferrer noopener\" target=\"_blank\">Apache Airflow</a>,&nbsp;<a href=\"https://www.matillion.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Matillion</a>,&nbsp;<a href=\"http://fivetran.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Fivetran</a>) have turned data extraction into work that can be completed quickly and efficiently. The data engineering team should be leveraging such technologies as force multipliers, taking a consistent and cohesive approach to integration and management of enterprise data, not just relying on legacy siloed approaches to building custom data pipelines with fragile, non-performant, hard to maintain code. Continuing with the latter approach will stifle the pace of innovation within the said enterprise and force the future focus to be around managing data infrastructure issues rather than how to help generate value for your business.</p><p>The primary role of an enterprise Data Engineering team should be to&nbsp;&nbsp;into a shape that‚Äôs ready for analysis ‚Äî laying the foundation for real-world analytics and data science application.</p><p>The Data Engineering team should serve as the&nbsp;&nbsp;for enterprise-level data with the responsibility to curate the organization‚Äôs data and act as a resource for those who want to make use of it, such as Reporting &amp; Analytics teams, Data Science teams, and other groups that are doing more self-service or business group driven analytics leveraging the enterprise data platform. This team should serve as the&nbsp;&nbsp;of organizational knowledge, managing and refining the catalog so that analysis can be done more effectively. Let‚Äôs look at the essential responsibilities of a well-functioning Data Engineering team.</p><h2>Responsibilities of a Data Engineering Team</h2><p>The Data Engineering team should provide a&nbsp;&nbsp;within the enterprise that cuts across to support both the Reporting/Analytics and Data Science capabilities to provide access to clean, transformed, formatted, scalable, and secure data ready for analysis. The Data Engineering teams‚Äô core responsibilities should include:</p><blockquote><p>¬∑ Build, manage, and optimize the core data platform infrastructure</p><p>¬∑ Build and maintain custom and off-the-shelf data integrations and ingestion pipelines from a variety of structured and unstructured sources</p><p>¬∑ Manage overall data pipeline orchestration</p><p>¬∑ Manage transformation of data either before or after load of raw data through both technical processes and business logic</p><p>¬∑ Support analytics teams with design and performance optimizations of data warehouses</p></blockquote><p><strong><em>Data is an Enterprise Asset.</em></strong></p><p><strong><em>Data as an Asset should be shared and protected.</em></strong></p><p>Data should be valued as an Enterprise asset, leveraged across all Business Units to enhance the company‚Äôs value to its respective customer base by accelerating decision making, and improving competitive advantage with the help of data. Good data stewardship, legal and regulatory requirements dictate that we protect the data owned from unauthorized access and disclosure.</p><p>In other words,&nbsp;<strong><em>managing Security is a crucial responsibility.</em></strong></p><h2>Why Create a Centralized Data Engineering Team?</h2><p>Treating Data Engineering as a standard and core capability that underpins both the Analytics and Data Science capabilities will help an enterprise evolve how to approach Data and Analytics. The enterprise needs to stop vertically treating data based on the technology stack involved as we tend to see often and move to more of a horizontal approach of managing a&nbsp;&nbsp;or&nbsp;&nbsp;that cuts across the organization and can connect to various technologies as needed drive analytic initiatives. This is a new way of thinking and working, but it can drive efficiency as the various data organizations look to scale. Additionally ‚Äî there is value in creating a dedicated structure and career path for Data Engineering resources. Data engineering skill sets are in high demand in the market; therefore, hiring outside the company can be costly. Companies must enable programmers, database administrators, and software developers with a career path to gain the needed experience with the above-defined skillsets by working across technologies. Usually, forming a data engineering center of excellence or a capability center would be the first step for making such progression possible.</p><h2>Challenges for creating a centralized Data Engineering Team</h2><p>The centralization of the Data Engineering team as a service approach is different from how Reporting &amp; Analytics and Data Science teams operate. It does, in principle, mean&nbsp;<strong><em>giving up some level of control of resources</em></strong>&nbsp;and establishing new processes for how these teams will collaborate and work together to deliver initiatives.</p><p>The Data Engineering team will need to demonstrate that it can effectively support the needs of both Reporting &amp; Analytics and Data Science teams, no matter how large these teams are. Data Engineering teams must&nbsp;<strong><em>effectively prioritize workloads&nbsp;</em></strong>while ensuring they can bring the right skillsets and experience to assigned projects.</p><p>Data engineering is essential because it serves as the backbone of data-driven companies. It enables analysts to work with clean and well-organized data, necessary for deriving insights and making sound decisions. To build a functioning data engineering practice, you need the following critical components:</p><p>The Data Engineering team should be a core capability within the enterprise, but it should effectively serve as a support function involved in almost everything data-related. It should interact with the Reporting and Analytics and Data Science teams in a collaborative support role to make the entire team successful.</p><p>The&nbsp;<em>Data Engineering team doesn‚Äôt create direct business value</em>&nbsp;‚Äî but the value should come in making the Reporting and Analytics, and Data Science teams more productive and efficient to ensure delivery of maximum value to business stakeholders through Data &amp; Analytics initiatives. To make that possible, the six key responsibilities within the data engineering capability center would be as follow ‚Äì</p><p>Let‚Äôs review the&nbsp;<strong><em>6 pillars of responsibilities</em></strong>:</p><p><strong>1. Determine Central Data Location for Collation and Wrangling</strong></p><p>Understanding and having a strategy for a&nbsp;(<em>a centralized data repository or data warehouse for the mass consumption of data for analysis</em>). Defining requisite data tables and where they will be joined in the context of data engineering and subsequently converting raw data into digestible and valuable formats.</p><p><strong>2. Data Ingestion and Transformation</strong></p><p>Moving data from one or more sources to a new destination (<em>your data lake or cloud data warehouse)&nbsp;</em>where it can be stored and further analyzed and then converting data from the format of the source system to that of the destination</p><p>Extracting, transforming, and loading data from one or more sources into a destination system to represent the data in a new context or style.</p><p>Data modeling is an essential function of a data engineering team, granted not all data engineers excel with this capability. Formalizing relationships between data objects and business rules into a conceptual representation through understanding information system workflows, modeling required queries, designing tables, determining primary keys, and effectively utilizing data to create informed output.</p><p>I‚Äôve seen engineers in interviews mess up more with this than coding in technical discussions. It‚Äôs essential to understand the differences between Dimensions, Facts, Aggregate tables.</p><p>Ensuring that sensitive data is protected and implementing proper authentication and authorization to reduce the risk of a data breach</p><p><strong>6. Architecture and Administration</strong></p><p>Defining the models, policies, and standards that administer what data is collected, where and how it is stored, and how it such data is integrated into various analytical systems.</p><blockquote><p>The six pillars of responsibilities for data engineering capabilities center on the ability to determine a central data location for collation and wrangling, ingest and transform data, execute ETL/ELT operations, model data, secure access and administer an architecture. While all companies have their own specific needs with regards to these functions, it is important to ensure that your team has the necessary skillset in order to build a foundation for big data success.</p></blockquote><p>Besides the Data Engineering following are the other capability centers that need to be considered within an enterprise:</p><h2>Analytics Capability Center</h2><p>The analytics capability center enables consistent, effective, and efficient BI, analytics, and advanced analytics capabilities across the company. Assist business functions in triaging, prioritizing, and achieving their objectives and goals through reporting, analytics, and dashboard solutions, while providing operational reports and visualizations, self-service analytics, and required tools to automate the generation of such insights.</p><h2>Data Science Capability Center</h2><p>The data science capability center is for exploring cutting-edge technologies and concepts to unlock new insights and opportunities, better inform employees and create a culture of prescriptive information usage using Automated AI and Automated ML solutions such as&nbsp;<a href=\"https://medium.com/u/9aea625dfc27?source=post_page---user_mention--b83d51cedb6a---------------------------------------\" rel=\"noreferrer noopener\" target=\"_blank\">H2O.ai</a>,&nbsp;<a href=\"https://medium.com/u/27e43843bc9f?source=post_page---user_mention--b83d51cedb6a---------------------------------------\" rel=\"noreferrer noopener\" target=\"_blank\">Dataiku</a>,&nbsp;<a href=\"http://www.aible.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Aible</a>, DataRobot,&nbsp;<a href=\"https://medium.com/u/3aaaf223f1e?source=post_page---user_mention--b83d51cedb6a---------------------------------------\" rel=\"noreferrer noopener\" target=\"_blank\">C3.ai</a></p><p>The data governance office empowers users with trusted, understood, and timely data to drive effectiveness while keeping the integrity and sanctity of data in the right hands for mass consumption.</p><blockquote><p><em>As your company grows, you will want to make sure that the data engineering capabilities are in place to support the six pillars of responsibilities. By doing this, you will be able to ensure that all aspects of data management and analysis are covered and that your data is safe and accessible by those who need it. Have you started thinking about how your company will grow? What steps have you taken to put a centralized data engineering team in place?</em></p></blockquote>","contentLength":14079,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It's time to go ESM-only","url":"https://javascriptweekly.com/issues/723","date":1739491200,"author":"","guid":601,"unread":true,"content":"<p>ü§Ø Pfft, vehicle data is a joke without <a href=\"https://javascriptweekly.com/link/165818/rss\">CarsXE</a>. API goes brrrrr. VIN decoding, plate lookup, market value reports. <a href=\"https://javascriptweekly.com/link/165818/rss\">Get&nbsp;Serious!</a></p>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mapped types in TypeScript","url":"https://2ality.com/2025/02/mapped-types-typescript.html","date":1739491200,"author":"Dr. Axel Rauschmayer","guid":352,"unread":true,"content":"<p>A mapped type is a loop over keys that produces an object or tuple type and looks as follows:</p><pre><code>{[]: }\n</code></pre><p>In this blog post, we examine how mapped types work and see examples of using them. Their most importing use cases are transforming objects and mapping tuples.</p>","contentLength":259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: SQL Noir ‚Äì Learn SQL by solving crimes","url":"https://www.sqlnoir.com/","date":1739483356,"author":"chrisBHappy","guid":321,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43041827"},{"title":"Bojan Mihelac: Prefixed Parameters for Django querystring tag","url":"http://code.informatikamihelac.com/en/query-string-with-prefixed-parameters/","date":1739482638,"author":"","guid":281,"unread":true,"content":"<article>An overview of Django 5.1's new querystring tag and how to add support for prefixed parameters.</article>","contentLength":95,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer ‚Äî Part 5: The Training","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-5-the-training/","date":1739480672,"author":"David Martin","guid":7,"unread":true,"content":"<p>In this fifth part of my series, I will outline the steps for creating a Docker container for training your image classification model, evaluating performance, and preparing for deployment.</p><p>AI/ML engineers would prefer to focus on model training and data engineering, but the reality is that we also need to understand the infrastructure and mechanics behind the scenes.</p><p>I hope to share some tips, not only to get your training run running, but how to streamline the process in a cost efficient manner on cloud resources such as Kubernetes.</p><p>I will reference elements from my previous articles for getting the best model performance, so be sure to check out&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>&nbsp;and&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>&nbsp;on the data sets, as well as&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>&nbsp;and&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>&nbsp;on model evaluation.</p><p>Here are the learnings that I will share with you, once we lay the groundwork on the infrastructure:</p><ul><li>Building your Docker container</li><li>Executing your training run</li></ul><p>First, let me provide a brief description of the setup that I created, specifically around Kubernetes. Your setup may be entirely different, and that is just fine. I simply want to set the stage on the infrastructure so that the rest of the discussion makes sense.</p><p>This is a server you deploy that provides a user interface to for your subject matter experts to label and evaluate images for the image classification application. The server can run as a pod on your Kubernetes cluster, but you may find that running a dedicated server with faster disk may be better.</p><p>Image files are stored in a directory structure like the following, which is self-documenting and easily modified.</p><pre><code>Image_Library/\n  - cats/\n    - image1001.png\n  - dogs/\n    - image2001.png</code></pre><p>Ideally, these files would reside on local server storage (instead of cloud or cluster storage) for better performance. The reason for this will become clear as we see what happens as the image library grows.</p><p><a href=\"https://towardsdatascience.com/tag/cloud-storage/\" title=\"Cloud Storage\">Cloud Storage</a> allows for a virtually limitless and convenient way to share files between systems. In this case, the image library on your management system could access the same files as your Kubernetes cluster or Docker engine.</p><p>However, the downside of cloud storage is the latency to open a file. Your image library will have&nbsp;&nbsp;of images, and the latency to read each file will have a significant impact on your training run time. Longer training runs means more cost for using the expensive GPU processors!</p><p>The way that I found to speed things up is to create a&nbsp;&nbsp;file of your image library on your management system and copy them to cloud storage. Even better would be to create multiple tar files&nbsp;, each containing 10,000 to 20,000 images.</p><p>This way you only have network latency on a handful of files (which contain thousands, once extracted) and you start your training run much sooner.</p><h3>Kubernetes or Docker engine</h3><p>A Kubernetes cluster, with proper configuration, will allow you to dynamically scale up/down nodes, so you can perform your model training on GPU hardware as needed. Kubernetes is a rather heavy setup, and there are other container engines that will work.</p><blockquote><p>The technology options change constantly!</p></blockquote><p>The main idea is that you want to spin up the resources you need ‚Äî for only as long as you need them ‚Äî then scale down to reduce your time (and therefore cost) of running expensive GPU resources.</p><p>Once your GPU node is started and your <a href=\"https://towardsdatascience.com/tag/docker/\" title=\"Docker\">Docker</a> container is running, you can extract the&nbsp;&nbsp;files above to&nbsp;&nbsp;storage, such as an&nbsp;, on your node. The node typically has high-speed SSD disk, ideal for this type of workload. There is one caveat ‚Äî the storage capacity on your node must be able to handle your image library.</p><p>Assuming we are good, let‚Äôs talk about building your Docker container so that you can train your model on your image library.</p><h2>Building your Docker container</h2><p>Being able to execute a training run in a consistent manner lends itself perfectly to building a Docker container. You can ‚Äúpin‚Äù the version of libraries so you know exactly how your scripts will run every time. You can version control your containers as well, and revert to a known good image in a pinch. What is really nice about Docker is you can run the container pretty much anywhere.</p><p>The tradeoff when running in a container, especially with an <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> model, is the speed of file storage. You can attach any number of volumes to your container, but they are usually&nbsp;&nbsp;attached, so there is latency on each file read. This may not be a problem if you have a small number of files. But when dealing with hundreds of thousands of files like image data, that latency adds up!</p><p>This is why using the&nbsp;&nbsp;file method outlined above can be beneficial.</p><p>Also, keep in mind that Docker containers could be terminated unexpectedly, so you should make sure to store important information outside the container, on cloud storage or a database. I‚Äôll show you how below.</p><p>Knowing that you will need to run on GPU hardware (here I will assume Nvidia), be sure to select the right base image for your Dockerfile, such as&nbsp;&nbsp;with the ‚Äúdevel&nbsp;flavor that will contain the right drivers.</p><p>Next, you will add the script files to your container, along with a ‚Äúbatch‚Äù script to coordinate the execution. Here is an example Dockerfile, and then I‚Äôll describe what each of the scripts will be doing.</p><pre><code>#####   Dockerfile   #####\nFROM nvidia/cuda:12.8.0-devel-ubuntu24.04\n\n# Install system software\nRUN apt-get -y update &amp;&amp; apg-get -y upgrade\nRUN apt-get install -y python3-pip python3-dev\n\n# Setup python\nWORKDIR /app\nCOPY requirements.txt\nRUN python3 -m pip install --upgrade pip\nRUN python3 -m pip install -r requirements.txt\n\n# Pythong and batch scripts\nCOPY ExtractImageLibrary.py .\nCOPY Training.py .\nCOPY Evaluation.py .\nCOPY ScorePerformance.py .\nCOPY ExportModel.py .\nCOPY BulkIdentification.py .\nCOPY BatchControl.sh .\n\n# Allow for interactive shell\nCMD tail -f /dev/null</code></pre><p>Dockerfiles are declarative, almost like a cookbook for building a small server ‚Äî you know what you‚Äôll get every time. Python libraries benefit, too, from this declarative approach. Here is a sample&nbsp;&nbsp;file that loads the TensorFlow libraries with CUDA support for GPU acceleration.</p><pre><code>#####   requirements.txt   #####\nnumpy==1.26.3\npandas==2.1.4\nscipy==1.11.4\nkeras==2.15.0\ntensorflow[and-cuda]</code></pre><h3>Extract Image Library script</h3><p>In <a href=\"https://towardsdatascience.com/tag/kubernetes/\" title=\"Kubernetes\">Kubernetes</a>, the Docker container can access local, high speed storage on the physical node. This can be achieved via the&nbsp;&nbsp;volume type. As mentioned before, this will only work if the local storage on your node can handle the size of your library.</p><pre><code>#####   sample 25GB emptyDir volume in Kubernetes   #####\ncontainers:\n  - name: training-container\n    volumeMounts:\n      - name: image-library\n        mountPath: /mnt/image-library\nvolumes:\n  - name: image-library\n    emptyDir:\n      sizeLimit: 25Gi</code></pre><p>You would want to have another&nbsp;&nbsp;to your cloud storage where you have the&nbsp;&nbsp;files. What this looks like will depend on your provider, or if you are using a persistent volume claim, so I won‚Äôt go into detail here.</p><p>Now you can extract the&nbsp;&nbsp;files ‚Äî ideally in parallel for an added performance boost ‚Äî to the local mount point.</p><p>As AI/ML engineers, the model training is where we want to spend most of our time.</p><blockquote><p>This is where the magic happens!</p></blockquote><p>With your image library now extracted, we can create our train-validation-test sets, load a pre-trained model or build a new one, fit the model, and save the results.</p><p>One key technique that has served me well is to load the most recently trained model as my base. I discuss this in more detail in&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>&nbsp;under ‚ÄúFine tuning‚Äù, this results in faster training time and significantly improved model performance.</p><p>Be sure to take advantage of the local storage to checkpoint your model during training since the models are quite large and you are paying for the GPU even while it sits idle writing to disk.</p><p>This of course raises a concern about what happens if the Docker container dies part-way though the training. The risk is (hopefully) low from a cloud provider, and you may not want an incomplete training anyway. But if that does happen, you will at least want to understand&nbsp;, and this is where saving the main log file to cloud storage (described below) or to a package like MLflow comes in handy.</p><p>After your training run has completed and you have taken proper precaution on saving your work, it is time to see how well it performed.</p><p>Normally this evaluation script will pick up on the model that just finished. But you may decide to point it at a previous model version through an interactive session. This is why have the script as stand-alone.</p><p>With it being a separate script, that means it will need to read the completed model from disk ‚Äî ideally local disk for speed. I like having two separate scripts (training and evaluation), but you might find it better to combine these to avoid reloading the model.</p><p>Now that the model is loaded, the evaluation script should generate predictions on&nbsp;&nbsp;image in the training, validation, test, and benchmark sets. I save the results as a&nbsp;&nbsp;matrix with the softmax confidence score for each class label. So, if there are 1,000 classes and 100,000 images, that‚Äôs a table with 100 million scores!</p><p>I save these results in&nbsp;&nbsp;files that are then used in the score generation next.</p><p>Taking the matrix of scores produced by the evaluation script above, we can now create various metrics of model performance. Again, this process could be combined with the evaluation script above, but my preference is for independent scripts. For example, I might want to regenerate scores on previous training runs. See what works for you.</p><p>Here are some of the&nbsp;&nbsp;functions that produce useful insights like F1, log loss, AUC-ROC, Matthews correlation coefficient.</p><pre><code>from sklearn.metrics import average_precision_score, classification_report\nfrom sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score</code></pre><p>Aside from these basic statistical analyses for each dataset (train, validation, test, and benchmark), it is also useful to identify:</p><ul><li>Which&nbsp;&nbsp;labels get the most number of errors?</li><li>Which&nbsp;&nbsp;labels get the most number of incorrect guesses?</li><li>How many&nbsp;<strong>ground-truth-to-predicted</strong>&nbsp;label pairs are there? In other words, which classes are easily confused?</li><li>What is the&nbsp;&nbsp;when applying a minimum softmax confidence score threshold?</li><li>What is the&nbsp;&nbsp;above that softmax threshold?</li><li>For the ‚Äúdifficult‚Äù benchmark sets, do you get a sufficiently&nbsp;&nbsp;score?</li><li>For the ‚Äúout-of-scope‚Äù benchmark sets, do you get a sufficiently&nbsp;&nbsp;score?</li></ul><p>As you can see, there are multiple calculations and it‚Äôs not easy to come up with a single evaluation to decide if the trained model is good enough to be moved to production.</p><p>In fact, for an image classification model, it is helpful to manually review the images that the model got wrong, as well as the ones that got a low softmax confidence score. Use the scores from this script to create a list of images to manually review, and then get a&nbsp;&nbsp;for how well the model performs.</p><p>Check out&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>&nbsp;for more in-depth discussion on evaluation and scoring.</p><p>All of the heavy lifting is done by this point. Since your Docker container will be shutdown soon, now is the time to copy the model artifacts to cloud storage and prepare them for being put to use.</p><p>The example Python code snippet below is more geared to Keras and TensorFlow. This will take the trained model and export it as a&nbsp;. Later, I will show how this is used by TensorFlow Serving in the&nbsp;&nbsp;section below.</p><pre><code># Increment current version of model and create new directory\nnext_version_dir, version_number = create_new_version_folder()\n\n# Copy model artifacts to the new directory\ncopy_model_artifacts(next_version_dir)\n\n# Create the directory to save the model export\nsaved_model_dir = os.path.join(next_version_dir, str(version_number))\n\n# Save the model export for use with TensorFlow Serving\ntf.keras.backend.set_learning_phase(0)\nmodel = tf.keras.models.load_model(keras_model_file)\ntf.saved_model.save(model, export_dir=saved_model_dir)</code></pre><p>This script also copies the other training run artifacts such as the model evaluation results, score summaries, and log files generated from model training. Don‚Äôt forget about your label map so you can give human readable names to your classes!</p><h3>Bulk identification script</h3><p>Your training run is complete, your model has been scored, and a new version is exported and ready to be served. Now is the time to use this latest model to assist you on trying to identify unlabeled images.</p><p>As I described in&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, you may have a collection of ‚Äúunknowns‚Äù ‚Äî really good pictures, but no idea what they are. Let your new model provide a best guess on these and record the results to a file or a database. Now you can create filters based on closest match and by high/low scores. This allows your subject matter experts to leverage these filters to find new image classes, add to existing classes, or to remove images that have very low scores and are no good.</p><p>By the way, I put this step inside the GPU container since you may have thousands of ‚Äúunknown‚Äù images to process and the accelerated hardware will make light work of it. However, if you are not in a hurry, you could perform this step on a separate CPU node, and shutdown your GPU node sooner to save cost. This would especially make sense if your ‚Äúunknowns‚Äù folder is on slower cloud storage.</p><p>All of the scripts described above perform a specific task ‚Äî from extracting your image library, executing model training, performing evaluation and scoring, exporting the model artifacts for deployment, and perhaps even bulk identification.</p><blockquote><p>One script to rule them all</p></blockquote><p>To coordinate the entire show, this batch script gives you the entry point for your container and an easy way to trigger everything. Be sure to produce a log file in case you need to analyze any failures along the way. Also, be sure to write the log to your cloud storage in case the container dies unexpectedly.</p><pre><code>#!/bin/bash\n# Main batch control script\n\n# Redirect standard output and standard error to a log file\nexec &gt; /cloud_storage/batch-logfile.txt 2&gt;&amp;1\n\n/app/ExtractImageLibrary.py\n/app/Training.py\n/app/Evaluation.py\n/app/ScorePerformance.py\n/app/ExportModel.py\n/app/BulkIdentification.py</code></pre><h2>Executing your training run</h2><p>So, now it‚Äôs time to put everything in motion‚Ä¶</p><p>Let‚Äôs go through the steps to prepare your image library, fire up your Docker container to train your model, and then examine the results.</p><h3>Image library ‚Äòtar‚Äô files</h3><p>Your image management system should now create a&nbsp;&nbsp;file backup of your data. Since&nbsp;&nbsp;is a single-threaded function, you will get significant speed improvement by creating multiple tar files in parallel, each with a portion of you data.</p><p>Now these files can be copied to your shared cloud storage for the next step.</p><p>All the hard work you put into creating your container (described above) will be put to the test. If you are running Kubernetes, you can create a Job that will execute the&nbsp;&nbsp;script.</p><p>Inside the Kubernetes Job definition, you can pass environment variables to adjust the execution of your script. For example, the batch size and number of epochs are set here and then pulled into your Python scripts, so you can alter the behavior without changing your code.</p><pre><code>#####   sample Job in Kubernetes   #####\ncontainers:\n  - name: training-job\n    env:\n      - name: BATCH_SIZE\n        value: 50\n      - name: NUM_EPOCHS\n        value: 30\n    command: [\"/app/BatchControl.sh\"]</code></pre><p>Once the Job is completed, be sure to verify that the GPU node properly scales back down to zero according to your scaling configuration in Kubernetes ‚Äî you don‚Äôt want to be saddled with a huge bill over a simple configuration error.</p><p>With the training run complete, you should now have model artifacts saved and can examine the performance. Look through the metrics, such as F1 and log loss, and benchmark accuracy for high softmax confidence scores.</p><p>As mentioned earlier, the reports only tell part of the story. It is worth the time and effort to manually review the images that the model got wrong or where it produced a low confidence score.</p><p>Don‚Äôt forget about the bulk identification. Be sure to leverage these to locate new images to fill out your data set, or to find new classes.</p><p>Once you have reviewed your model performance and are satisfied with the results, it is time to modify your TensorFlow Serving container to put the new model into production.</p><p>TensorFlow Serving is available as a Docker container and provides a very quick and convenient way to serve your model. This container can listen and respond to API calls for your model.</p><p>Let‚Äôs say your new model is version 7, and your&nbsp;&nbsp;script (see above) has saved the model in your cloud share as&nbsp;<em>/image_application/models/007</em>. You can start the TensorFlow Serving container with that volume mount. In this example, the&nbsp;&nbsp;points to folder for version 007.</p><pre><code>#####   sample TensorFlow pod in Kubernetes   #####\ncontainers:\n  - name: tensorflow-serving\n    image: bitnami/tensorflow-serving:2.18.0\n    ports:\n      - containerPort: 8501\n    env:\n      - name: TENSORFLOW_SERVING_MODEL_NAME\n        value: \"image_application\"\n    volumeMounts:\n      - name: models-subfolder\n        mountPath: \"/bitnami/model-data\"\n\nvolumes:\n  - name: models-subfolder\n    azureFile:\n      shareName: \"image_application/models/007\"</code></pre><p>A subtle note here ‚Äî the export script should create a sub-folder, named 007 (same as the base folder), with the saved model export. This may seem a little confusing, but TensorFlow Serving will mount this share folder as&nbsp;&nbsp;and detect the numbered sub-folder inside it for the version to serve. This will allow you to query the API for the model version as well as the identification.</p><p>As I mentioned at the start of this article, this setup has worked for my situation. This is certainly not the only way to approach this challenge, and I invite you to customize your own solution.</p><p>I wanted to share my hard-fought learnings as I embraced cloud services in Kubernetes, with the desire to keep costs under control. Of course, doing all this while maintaining a high level of model performance is an added challenge, but one that you can achieve.</p><p>I hope I have provided enough information here to help you with your own endeavors. Happy learnings!</p>","contentLength":18226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer ‚Äî Part 3: The Evaluation","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/","date":1739480406,"author":"David Martin","guid":6,"unread":true,"content":"<p>In this third part of my series, I will explore the evaluation process which is a critical piece that will lead to a cleaner data set and elevate your model performance. We will see the difference between evaluation of a&nbsp;&nbsp;model (one not yet in production), and evaluation of a&nbsp;&nbsp;model (one making real-world predictions).</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, I discussed the process of labelling your image data that you use in your <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> project. I showed how to define ‚Äúgood‚Äù images and create sub-classes. In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, I went over various data sets, beyond the usual train-validation-test sets, such as benchmark sets, plus how to handle synthetic data and duplicate images.</p><h2><strong>Evaluation of the trained model</strong></h2><p>As machine learning engineers we look at accuracy, F1, log loss, and other metrics to decide if a model is ready to move to production. These are all important measures, but from my experience, these scores can be deceiving especially as the number of classes grows.</p><p>Although it can be time consuming, I find it very important to manually review the images that the model gets&nbsp;, as well as the images that the model gives a&nbsp;&nbsp;softmax ‚Äúconfidence‚Äù score to. This means adding a step immediately after your training run completes to calculate scores for&nbsp;&nbsp;images ‚Äî training, validation, test, and the benchmark sets. You only need to bring up for manual review the ones that the model had problems with. This should only be a small percentage of the total number of images. See the Double-check process below</p><p>What you do during the manual evaluation is to put yourself in a ‚Äú‚Äù to ensure that the labelling standards are being followed that you setup in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>. Ask yourself:</p><ul><li>‚ÄúIs this a good image?‚Äù Is the subject front and center, and can you clearly see all the features?</li><li>‚ÄúIs this the correct label?‚Äù Don‚Äôt be surprised if you find wrong labels.</li></ul><p>You can either remove the bad images or fix the labels if they are wrong. Otherwise you can keep them in the data set and force the model to do better next time. Other questions I ask are:</p><ul><li>‚ÄúWhy did the model get this wrong?‚Äù</li><li>‚ÄúWhy did this image get a low score?‚Äù</li><li>‚ÄúWhat is it about the image that caused confusion?‚Äù</li></ul><p>Sometimes the answer has nothing to do with&nbsp;&nbsp;specific image. Frequently, it has to do with the&nbsp;&nbsp;images, either in the ground truth class or in the predicted class. It is worth the effort to Double-check all images in both sets if you see a consistently bad guess. Again, don‚Äôt be surprised if you find poor images or wrong labels.</p><p>When doing the evaluation of the trained model (above), we apply a lot of subjective analysis ‚Äî ‚ÄúWhy did the model get this wrong?‚Äù and ‚ÄúIs this a good image?‚Äù From these, you may only get a&nbsp;.</p><p>Frequently, I will decide to hold off moving a model forward to production based on that gut feel. But how can you justify to your manager that you want to hit the brakes? This is where putting a more&nbsp;&nbsp;analysis comes in by creating a weighted average of the softmax ‚Äúconfidence‚Äù scores.</p><p>In order to apply a weighted evaluation, we need to identify sets of classes that deserve adjustments to the score. Here is where I create a list of ‚Äúcommonly confused‚Äù classes.</p><h2><strong>Commonly confused classes</strong></h2><p>Certain animals at our zoo can easily be mistaken. For example, African elephants and Asian elephants have different ear shapes. If your model gets these two mixed up, that is not as bad as guessing a giraffe! So perhaps you give partial credit here. You and your subject matter experts (SMEs) can come up with a list of these pairs and a weighted adjustment for each.</p><p>This weight can be factored into a modified cross-entropy loss function in the equation below. The back half of this equation will reduce the impact of being wrong for specific pairs of ground truth and prediction by using the ‚Äúweight‚Äù function as a lookup. By default, the weighted adjustment would be 1 for all pairings, and the commonly confused classes would get something like 0.5.</p><p>In other words, it‚Äôs better to be unsure (have a&nbsp;&nbsp;confidence score) when you are wrong, compared to being super confident and wrong.</p><p>Once this weighted log loss is calculated, I can compare to previous training runs to see if the new model is ready for production.</p><h2><strong>Confidence threshold report</strong></h2><p>Another valuable measure that incorporates the confidence threshold (in my example, 95) is to report on accuracy and false positive rates. Recall that when we apply the confidence threshold before presenting results, we help reduce false positives from being shown to the end user.</p><p>In this table, we look at the breakdown of ‚Äútrue positive above 95‚Äù for each data set. We get a sense that when a ‚Äúgood‚Äù picture comes through (like the ones from our train-validation-test set) it is very likely to surpass the threshold, thus the user is ‚Äúhappy‚Äù with the outcome. Conversely, the ‚Äúfalse positive above 95‚Äù is extremely low for good pictures, thus only a small number of our users will be ‚Äúsad‚Äù about the results.</p><p>We expect the train-validation-test set results to be exceptional since our data is curated. So, as long as people take ‚Äúgood‚Äù pictures, the model should do very well. But to get a sense of how it does on extreme situations, let‚Äôs take a look at our benchmarks.</p><p>The ‚Äúdifficult‚Äù benchmark has more modest true positive and false positive rates, which reflects the fact that the images are more challenging. These values are much easier to compare across training runs, so that lets me set a min/max target. So for example, if I target a minimum of 80% for true positive, and maximum of 5% for false positive on this benchmark, then I can feel confident moving this to production.</p><p>The ‚Äúout-of-scope‚Äù benchmark has no true positive rate because&nbsp;&nbsp;of the images belong to any class the model can identify. Remember, we picked things like a bag of popcorn, etc., that are not zoo animals, so there cannot be any true positives. But we do get a false positive rate, which means the model gave a confident score to that bag of popcorn as some animal. And if we set a target maximum of 10% for this benchmark, then we may not want to move it to production.</p><p>Right now, you may be thinking, ‚ÄúWell, what animal did it pick for the bag of popcorn?‚Äù Excellent question! Now you understand the importance of doing a manual review of the images that get bad results.</p><h2><strong>Evaluation of the deployed model</strong></h2><p>The evaluation that I described above applies to a model immediately after&nbsp;. Now, you want to evaluate how your model is doing in the&nbsp;. The process is similar, but requires you to shift to a ‚Äú‚Äù and asking yourself, ‚ÄúDid the model get this correct?‚Äù and ‚ÄúShould it have gotten this correct?‚Äù and ‚ÄúDid we tell the user the right thing?‚Äù</p><p>So, imagine that you are logging in for the morning ‚Äî after sipping on your&nbsp;<a href=\"https://medium.com/@dmartin0409/cold-brew-coffee-0aabd53a1f3e\">cold brew coffee</a>, of course ‚Äî and are presented with 500 images that your zoo guests took yesterday of different animals. Your job is to determine how satisfied the guests were using your model to identify the zoo animals.</p><p>Using the softmax ‚Äúconfidence‚Äù score for each image, we have a threshold before presenting results. Above the threshold, we tell the guest what the model predicted. I‚Äôll call this the ‚Äúhappy path‚Äù. And below the threshold is the ‚Äúsad path‚Äù where we ask them to try again.</p><p>Your review interface will first show you all the ‚Äúhappy path‚Äù images one at a time. This is where you ask yourself, ‚ÄúDid we get this right?‚Äù Hopefully, yes!</p><p>But if not, this is where things get tricky. So now you have to ask, ‚ÄúWhy not?‚Äù Here are some things that it could be:</p><ul><li>‚ÄúBad‚Äù picture ‚Äî Poor lighting, bad angle, zoomed out, etc ‚Äî refer to your labelling standards.</li><li>Out-of-scope ‚Äî It‚Äôs a zoo animal, but unfortunately one that isn‚Äôt found in&nbsp;&nbsp;zoo. Maybe it belongs to another zoo (your guest likes to travel and try out your app). Consider adding these to your data set.</li><li>Out-of-scope ‚Äî It‚Äôs not a zoo animal. It could be an animal in your zoo, but not one typically&nbsp;&nbsp;there, like a neighborhood sparrow or mallard duck. This might be a candidate to add.</li><li>Out-of-scope ‚Äî It‚Äôs something found in the zoo. A zoo usually has interesting trees and shrubs, so people might try to identify those. Another candidate to add.</li><li>Prankster ‚Äî Completely out-of-scope. Because people like to play with technology, there‚Äôs the possibility you have a prankster that took a picture of a bag of popcorn, or a soft drink cup, or even a selfie. These are hard to prevent, but hopefully get a low enough score (below the threshold) so the model did not identify it as a zoo animal. If you see enough pattern in these, consider creating a class with special handling on the front-end.</li></ul><p>After reviewing the ‚Äúhappy path‚Äù images, you move on to the ‚Äúsad path‚Äù images ‚Äî the ones that got a low confidence score and the app gave a ‚Äúsorry, try again‚Äù message. This time you ask yourself, ‚Äú&nbsp;the model have given this image a higher score?‚Äù which would have put it in the ‚Äúhappy path‚Äù. If so, then you want to ensure these images are added to the training set so next time it will do better. But most of time, the low score reflects many of the ‚Äúbad‚Äù or out-of-scope situations mentioned above.</p><p>Perhaps your model performance is suffering and it has nothing to do with your model. Maybe it is the ways you users interacting with the app. Keep an eye out of non-technical problems and share your observations with the rest of your team. For example:</p><ul><li>Are your users using the application in the ways you expected?</li><li>Are they not following the instructions?</li><li>Do the instructions need to be stated more clearly?</li><li>Is there anything you can do to improve the experience?</li></ul><h2><strong>Collect statistics and new images</strong></h2><p>Both of the manual evaluations above open a gold mine of data. So, be sure to collect these statistics and feed them into a dashboard ‚Äî your manager and your future self will thank you!</p><p>Keep track of these stats and generate reports that you and your can reference:</p><ul><li>How often the model is being called?</li><li>What times of the day, what days of the week is it used?</li><li>Are your system resources able to handle the peak load?</li><li>What classes are the most common?</li><li>After evaluation, what is the accuracy for each class?</li><li>What is the breakdown for confidence scores?</li><li>How many scores are above and below the confidence threshold?</li></ul><p>The single best thing you get from a deployed model is the additional real-world images! You can add these now images to improve coverage of your existing zoo animals. But more importantly, they provide you insight on&nbsp;&nbsp;classes to add. For example, let‚Äôs say people enjoy taking a picture of the large walrus statue at the gate. Some of these may make sense to incorporate into your data set to provide a better user experience.</p><p>Creating a new class, like the walrus statue, is not a huge effort, and it avoids the false positive responses. It would be more embarrassing to identify a walrus statue as an elephant! As for the prankster and the bag of popcorn, you can configure your front-end to quietly handle these. You might even get creative and have fun with it like, ‚ÄúThank you for visiting the food court.‚Äù</p><p>It is a good idea to double-check your image set when you suspect there may be problems with your data. I‚Äôm not suggesting a top-to-bottom check, because that would a monumental effort! Rather specific classes that you suspect could contain bad data that is degrading your model performance.</p><p>Immediately after my training run completes, I have a script that will use this new model to generate predictions for my&nbsp;&nbsp;data set. When this is complete, it will take the list of incorrect identifications, as well as the low scoring predictions, and automatically feed that list into the Double-check interface.</p><p>This interface will show, one at a time, the image in question, alongside an example image of the ground truth and an example image of what the model predicted. I can visually compare the three, side-by-side. The first thing I do is ensure the original image is a ‚Äúgood‚Äù picture, following my labelling standards. Then I check if the ground-truth label is indeed correct, or if there is something that made the model think it was the predicted label.</p><ul><li>Remove the original image if the image quality is poor.</li><li>Relabel the image if it belongs in a different class.</li></ul><p>During this manual evaluation, you might notice dozens of the same wrong prediction. Ask yourself why the model made this mistake when the images seem perfectly fine. The answer may be some incorrect labels on images in the ground truth, or even in the predicted class!</p><p>Don‚Äôt hesitate to add those classes and sub-classes back into the Double-check interface and step through them all. You may have 100‚Äì200 pictures to review, but there is a good chance that one or two of the images will stand out as being the culprit.</p><p>With a different mindset for a trained model versus a deployed model, we can now evaluate performances to decide which models are ready for production, and how well a production model is going to serve the public. This relies on a solid Double-check process and a critical eye on your data. And beyond the ‚Äúgut feel‚Äù of your model, we can rely on the benchmark scores to support us.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, we kick off the training run, but there are some subtle techniques to get the most out of the process and even ways to leverage throw-away models to expand your library image data.<a href=\"https://medium.com/tag/machine-learning?source=post_page-----e4a8dbb035e0---------------------------------------\"></a></p>","contentLength":13463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build a dynamic, role-based AI agent using Amazon Bedrock inline agents","url":"https://aws.amazon.com/blogs/machine-learning/build-a-dynamic-role-based-ai-agent-using-amazon-bedrock-inline-agents/","date":1739480188,"author":"Ishan Singh","guid":236,"unread":true,"content":"<p>AI agents continue to gain momentum, as businesses use the power of generative AI to reinvent customer experiences and automate complex workflows. We are seeing <a href=\"https://aws.amazon.com/bedrock/agents/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Agents</a> applied in investment research, insurance claims processing, root cause analysis, advertising campaigns, and much more. Agents use the reasoning capability of foundation models (FMs) to break down user-requested tasks into multiple steps. They use developer-provided instructions to create an orchestration plan and carry out that plan by securely invoking company APIs and accessing knowledge bases using Retrieval Augmented Generation (RAG) to accurately handle the user‚Äôs request.</p><p>Although organizations see the benefit of agents that are defined, configured, and tested as managed resources, we have increasingly seen the need for an additional, more dynamic way to invoke agents. Organizations need solutions that adjust on the fly‚Äîwhether to test new approaches, respond to changing business rules, or customize solutions for different clients. This is where the new <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/agents-create-inline.html\" target=\"_blank\" rel=\"noopener\">inline agents</a> capability in Amazon Bedrock Agents becomes transformative. It allows you to dynamically adjust your agent‚Äôs behavior at runtime by changing its instructions, tools, guardrails, knowledge bases, prompts, and even the FMs it uses‚Äîall without redeploying your application.</p><p>In this post, we explore how to build an application using Amazon Bedrock inline agents, demonstrating how a single AI assistant can adapt its capabilities dynamically based on user roles.</p><h2>Inline agents in Amazon Bedrock Agents</h2><p>This runtime flexibility enabled by inline agents opens powerful new possibilities, such as:</p><ul><li> ‚Äì Inline agents minimize the time-consuming create/update/prepare cycles traditionally required for agent configuration changes. Developers can instantly test different combinations of models, tools, and knowledge bases, dramatically accelerating the development process.</li><li><strong>A/B testing and experimentation</strong> ‚Äì Data science teams can systematically evaluate different model-tool combinations, measure performance metrics, and analyze response patterns in controlled environments. This empirical approach enables quantitative comparison of configurations before production deployment.</li><li><strong>Subscription-based personalization</strong> ‚Äì Software companies can adapt features based on each customer‚Äôs subscription level, providing more advanced tools for premium users.</li><li><strong>Persona-based data source integration</strong> ‚Äì Institutions can adjust content complexity and tone based on the user‚Äôs profile, providing persona-appropriate explanations and resources by changing the knowledge bases associated to the agent on the fly.</li><li> ‚Äì Developers can create applications with hundreds of APIs, and quickly and accurately carry out tasks by dynamically choosing a small subset of APIs for the agent to consider for a given request. This is particularly helpful for large software as a service (SaaS) platforms needing multi-tenant scaling.</li></ul><p>Inline agents expand your options for building and deploying agentic solutions with Amazon Bedrock Agents. For workloads needing managed and versioned agent resources with a pre-determined and tested configuration (specific model, instructions, tools, and so on), developers can continue to use InvokeAgent on resources created with CreateAgent. For workloads that need dynamic runtime behavior changes for each agent invocation, you can use the new <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_InvokeInlineAgent.html\" target=\"_blank\" rel=\"noopener\">InvokeInlineAgent API</a>. With either approach, your agents will be secure and scalable, with configurable guardrails, a flexible set of model inference options, native access to knowledge bases, code interpretation, session memory, and more.</p><p>Our HR assistant example shows how to build a single AI assistant that adapts to different user roles using the new inline agent capabilities in Amazon Bedrock Agents. When users interact with the assistant, the assistant dynamically configures agent capabilities (such as model, instructions, knowledge bases, action groups, and guardrails) based on the user‚Äôs role and their specific selections. This approach creates a flexible system that adjusts its functionality in real time, making it more efficient than creating separate agents for each user role or tool combination. The complete code for this HR assistant example is available on our <a href=\"https://github.com/awslabs/amazon-bedrock-agent-samples/tree/main/examples/agents_ux/inline-agent-hr-assistant\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>.</p><p>This dynamic tool selection enables a personalized experience. When an employee logs in without direct reports, they see a set of tools that they have access to based on their role. They can select from options like requesting vacation time, checking company policies using the knowledge base, using a code interpreter for data analysis, or submitting expense reports. The inline agent assistant is then configured with only these selected tools, allowing it to assist the employee with their chosen tasks. In a real-world example, the user would not need to make the selection, because the application would make that decision and automatically configure the agent invocation at runtime. We make it explicit in this application so that you can demonstrate the impact.</p><p>Similarly, when a manager logs in to the same system, they see an extended set of tools reflecting their additional permissions. In addition to the employee-level tools, managers have access to capabilities like running performance reviews. They can select which tools they want to use for their current session, instantly configuring the inline agent with their choices.</p><p>The inclusion of knowledge bases is also adjusted based on the user‚Äôs role. Employees and managers see different levels of company policy information, with managers getting additional access to confidential data like performance review and compensation details. For this demo, we‚Äôve implemented <a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-knowledge-bases-now-supports-metadata-filtering-to-improve-retrieval-accuracy/\" target=\"_blank\" rel=\"noopener\">metadata filtering</a> to retrieve only the appropriate level of documents based on the user‚Äôs access level, further enhancing efficiency and security.</p><p>Let‚Äôs look at how the interface adapts to different user roles.</p><p>The employee view provides access to essential HR functions like vacation requests, expense submissions, and company policy lookups. Users can select which of these tools they want to use for their current session.</p><p>The manager view extends these options to include supervisory functions like compensation management, demonstrating how the inline agent can be configured with a broader set of tools based on user permissions.</p><p>The manager view extends these capabilities to include supervisory functions like compensation management, demonstrating how the inline agent dynamically adjusts its available tools based on user permissions. Without inline agents, we would need to build and maintain two separate agents.</p><p>As shown in the preceding screenshots, the same HR assistant offers different tool selections based on the user‚Äôs role. An employee sees options like Knowledge Base, Apply Vacation Tool, and Submit Expense, whereas a manager has additional options like Performance Evaluation. Users can select which tools they want to add to the agent for their current interaction.</p><p>This flexibility allows for quick adaptation to user needs and preferences. For instance, if the company introduces a new policy for creating business travel requests, the tool catalog can be quickly updated to include a Create Business Travel Reservation tool. Employees can then choose to add this new tool to their agent configuration when they need to plan a business trip, or the application could automatically do so based on their role.</p><p>With Amazon Bedrock inline agents, you can create a catalog of actions that is dynamically selected by the application or by users of the application. This increases the level of flexibility and adaptability of your solutions, making them a perfect fit for navigating the complex, ever-changing landscape of modern business operations. Users have more control over their AI assistant‚Äôs capabilities, and the system remains efficient by only loading the necessary tools for each interaction.</p><h2>Technical foundation: Dynamic configuration and action selection</h2><p>Inline agents allow dynamic configuration at runtime, enabling a single agent to effectively perform the work of many. By specifying action groups and modifying instructions on the fly, even within the same session, you can create versatile AI applications that adapt to various scenarios without multiple agent deployments.</p><p>The following are key points about inline agents:</p><ul><li> ‚Äì Change the agent‚Äôs configuration, including its FM, at runtime. This enables rapid experimentation and adaptation without redeploying the application, reducing development cycles.</li><li> ‚Äì Apply governance and access control at the tool level. With agents changing dynamically at runtime, tool-level governance helps maintain security and compliance regardless of the agent‚Äôs configuration.</li><li> ‚Äì Provide only necessary tools and instructions at runtime to reduce token usage and improve the agent accuracy. With fewer tools to choose from, it‚Äôs less complicated for the agent to select the right one, reducing hallucinations in the tool selection process. This approach can also lead to lower costs and improved latency compared to static agents because removing unnecessary tools, knowledge bases, and instructions reduces the number of input and output tokens being processed by the agent‚Äôs large language model (LLM).</li><li> ‚Äì Create reusable actions for dynamic selection based on specific needs. This modular approach simplifies maintenance, updates, and scalability of your AI applications.</li></ul><p>The following are examples of reusable actions:</p><ul><li><strong>Enterprise system integration</strong> ‚Äì Connect with systems like Salesforce, GitHub, or databases</li><li> ‚Äì Perform common tasks such as sending emails or managing calendars</li><li> ‚Äì Interact with specialized internal tools and services</li><li> ‚Äì Analyze text, structured data, or other information</li><li> ‚Äì Fetch weather updates, stock prices, or perform web searches</li><li> ‚Äì Use specific machine learning (ML) models for targeted tasks</li></ul><p>When using inline agents, you configure parameters for the following:</p><ul><li>Contextual tool selection based on user intent or conversation flow</li><li>Adaptation to different user roles and permissions</li><li>Switching between communication styles or personas</li><li>Model selection based on task complexity</li></ul><p>The inline agent uses the configuration you provide at runtime, allowing for highly flexible AI assistants that efficiently handle various tasks across different business contexts.</p><h2>Building an HR assistant using inline agents</h2><p>Let‚Äôs look at how we built our HR Assistant using Amazon Bedrock inline agents:</p><ol><li> ‚Äì We developed a demo catalog of HR-related tools, including: \n  <ul type=\"a\"><li> ‚Äì Using <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> for accessing company policies and guidelines based on the role of the application user. In order to filter the knowledge base content based on the user‚Äôs role, you also need to provide a metadata file specifying the type of employee‚Äôs roles that can access each file</li><li>‚Äì For requesting and tracking time off.</li><li>‚Äì For submitting and managing expense reports.</li><li> ‚Äì For performing calculations and data analysis.</li><li>‚Äì for conducting and reviewing employee compensation assessments (manager only access).</li></ul></li><li> ‚Äì We defined multiple conversation tones to suit different interaction styles: \n  <ul><li> ‚Äì For formal, business-like interactions.</li><li> ‚Äì For friendly, everyday support.</li><li> ‚Äì For upbeat, encouraging assistance.</li></ul></li><li> ‚Äì We implemented role-based access control. The application backend checks the user‚Äôs role (employee or manager) and provides access to appropriate tools and information and passes this information to the inline agent. The role information is also used to configure metadata filtering in the knowledge bases to generate relevant responses. The system allows for dynamic tool use at runtime. Users can switch personas or add and remove tools during their session, allowing the agent to adapt to different conversation needs in real time.</li><li><strong>Integrate the agent with other services and tools</strong> ‚Äì We connected the inline agent to: \n  <ul><li>Amazon Bedrock Knowledge Bases for company policies, with metadata filtering for role-based access.</li><li><a href=\"http://aws.amazon.com/lambda\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> functions for executing specific actions (such as submitting vacation requests or expense reports).</li><li>A code interpreter tool for performing calculations and data analysis.</li></ul></li><li> ‚Äì We created a Flask-based UI that performs the following actions: \n  <ul><li>Displays available tools based on the user‚Äôs role.</li><li>Allows users to select different personas.</li><li>Provides a chat window for interacting with the HR assistant.</li></ul></li></ol><p>To understand how this dynamic role-based functionality works under the hood, let‚Äôs examine the following system architecture diagram.</p><p>As shown in preceding architecture diagram, the system works as follows:</p><ol><li>The end-user logs in and is identified as either a manager or an employee.</li><li>The user selects the tools that they have access to and makes a request to the HR assistant.</li><li>The agent breaks down the problems and uses the available tools to solve for the query in steps, which may include: \n  <ol><li>Amazon Bedrock Knowledge Bases (with metadata filtering for role-based access).</li><li>Lambda functions for specific actions.</li><li>Code interpreter tool for calculations.</li><li>Compensation tool (accessible only to managers to submit base pay raise requests).</li></ol></li><li>The application uses the Amazon Bedrock inline agent to dynamically pass in the appropriate tools based on the user‚Äôs role and request.</li><li>The agent uses the selected tools to process the request and provide a response to the user.</li></ol><p>This approach provides a flexible, scalable solution that can quickly adapt to different user roles and changing business needs.</p><p>In this post, we introduced the Amazon Bedrock inline agent functionality and highlighted its application to an HR use case. We dynamically selected tools based on the user‚Äôs roles and permissions, adapted instructions to set a conversation tone, and selected different models at runtime. With inline agents, you can transform how you build and deploy AI assistants. By dynamically adapting tools, instructions, and models at runtime, you can:</p><ul><li>Create personalized experiences for different user roles</li><li>Optimize costs by matching model capabilities to task complexity</li><li>Streamline development and maintenance</li><li>Scale efficiently without managing multiple agent configurations</li></ul><p>For organizations demanding highly dynamic behavior‚Äîwhether you‚Äôre an AI startup, SaaS provider, or enterprise solution team‚Äîinline agents offer a scalable approach to building intelligent assistants that grow with your needs. To get started, explore our <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/blob/main/agents-and-function-calling/bedrock-agents/features-examples/15-invoke-inline-agents/inline-agent-api-usage.ipynb\" target=\"_blank\" rel=\"noopener\">GitHub repo</a> and <a href=\"https://github.com/awslabs/amazon-bedrock-agent-samples/tree/main/examples/agents_ux/inline-agent-hr-assistant\" target=\"_blank\" rel=\"noopener\">HR assistant demo application</a>, which demonstrate key implementation patterns and best practices.</p><p>To learn more about how to be most successful in your agent journey, read our two-part blog series:</p><p>To get started with Amazon Bedrock Agents, check out the following <a href=\"https://github.com/awslabs/amazon-bedrock-agent-samples/\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> with example code.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/blog-image-ishansin-1.jpeg\" alt=\"\" width=\"100\" height=\"133\">&nbsp;is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/mttanke-225x300-1-1.jpg\" alt=\"\" width=\"100\" height=\"133\">&nbsp;is a Senior Generative AI Data Scientist at AWS. With a background in machine learning, she has over 10 years of experience architecting and building AI applications with customers across industries. As a technical lead, she helps customers accelerate their achievement of business value through generative AI solutions on Amazon Bedrock. In her free time, Maira enjoys traveling, playing with her cat, and spending time with her family someplace warm.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/roymark-225x300-1-1.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Mark‚Äôs work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS certifications, including the ML Specialty Certification.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/enitin.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Sr. Enterprise Solutions Architect at AWS, experienced in Software Engineering, Enterprise Architecture, and AI/ML. He is deeply passionate about exploring the possibilities of generative AI. He collaborates with customers to help them build well-architected applications on the AWS platform, and is dedicated to solving technology challenges and assisting with their cloud journey.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/Image-from-iOS-244x300-1.jpg\" alt=\"\" width=\"100\" height=\"123\"> is a Software Development Engineer at Amazon Web Services (AWS). He specializes in backend system design, distributed architectures, and scalable solutions, contributing to the development and launch of high-impact systems at Amazon. Outside of work, he spends his time playing ping pong and hiking through Cascade trails, enjoying the outdoors as much as he enjoys building systems.</p><p> is a Software Development Engineer at Amazon Web Services (AWS), working in Agents for Amazon Bedrock. He focuses on developing scalable systems on the cloud that enable AI applications frameworks and orchestrations. Shubham also has a background in building distributed, scalable, high-volume-high-throughput systems in IoT architectures.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/09/18/vivekbh-100.jpg\" alt=\"\" width=\"100\" height=\"134\"> is a Principal Engineer for Amazon Bedrock.&nbsp;He focuses on building deep learning-based AI and computer vision solutions for AWS customers. Oustide of work, Vivek enjoys trekking and following cricket.</p>","contentLength":17760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer ‚Äî Part 1: The Data","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/","date":1739480153,"author":"David Martin","guid":5,"unread":true,"content":"<p>It is said that in order for a machine learning model to be successful, you need to have good data. While this is true (and pretty much obvious), it is extremely difficult to define, build, and sustain good data. Let me share with you the unique processes that I have learned over several years building an ever-growing image classification system and how you can apply these techniques to your own application.</p><p>With persistence and diligence, you can avoid the classic ‚Äúgarbage in, garbage out‚Äù, maximize your model accuracy, and demonstrate real business value.</p><p>In this series of articles, I will dive into the care and feeding of a multi-class, single-label image classification app and what it takes to reach the highest level of performance. I won‚Äôt get into any coding or specific user interfaces, just the main concepts that you can incorporate to suit your needs with the tools at your disposal.</p><p>Here is a brief description of the articles. You will notice that the model is last on the list since we need to focus on curating the data first and foremost:</p><p>Over the past six years, I have been primarily focused on building and maintaining an image classification application for a manufacturing company. Back when I started, most of the software did not exist or was too expensive, so I created these from scratch. In this time, I have deployed two identifier applications, the largest handles 1,500 classes and achieves 97‚Äì98% accuracy.</p><p>It was about eight years ago that I started online studies for <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a> and machine learning. So, when the exciting opportunity to create an AI application presented itself, I was prepared to build the tools I needed to leverage the latest advancements. I jumped in with both feet!</p><p>I quickly found that building and deploying a model is probably the easiest part of the job. Feeding high quality data into the model is the best way to improve performance, and that requires focus and patience. Attention to detail is what I do best, so this was a perfect fit.</p><h2><strong>It all starts with the data</strong></h2><p>I feel that so much attention is given to the model selection (deciding which neural network is best) and that the data is just an afterthought. I have found the hard way that even one or two pieces of bad data can significantly impact model performance, so that is where we need to focus.</p><p>For example, let‚Äôs say you train the classic cat versus dog image classifier. You have 50 pictures of cats and 50 pictures of dogs, however one of the ‚Äúcats‚Äù is clearly (objectively) a picture of a dog. The computer doesn‚Äôt have the luxury of ignoring the mislabelled image, and instead adjusts the model weights to make it fit. Square peg meets round hole.</p><p>Another example would be a picture of a cat that climbed up into a tree. But when you take a wholistic view of it, you would describe it as a picture of a tree (first) with a cat (second). Again, the computer doesn‚Äôt know to ignore the big tree and focus on the cat ‚Äî it will start to identify trees as cats, even if there is a dog. You can think of these pictures as outliers and should be removed.</p><p>It doesn‚Äôt matter if you have the best neural network in the world, you can count on the model making poor predictions when it is trained on ‚Äúbad‚Äù data. I‚Äôve learned that any time I see the model make mistakes, it‚Äôs time to review the data.</p><h2><strong>Example Application ‚Äî Zoo animals</strong></h2><p>For the rest of this write-up, I will use an example of identifying zoo animals. Let‚Äôs assume your goal is to create a mobile app where guests at the zoo can take pictures of the animals they see and have the app identify them. Specifically, this is a multi-class, single-label application.</p><ul><li>&nbsp;‚Äî There are a lot of different animals at the zoo and many of them look very similar.</li><li>&nbsp;‚Äî Guests using the app don‚Äôt always take good pictures (zoomed out, blurry, too dark), so we don‚Äôt want to provide an answer if the image is poor.</li><li>&nbsp;‚Äî The zoo keeps expanding and adding new species all the time.</li><li>&nbsp;‚Äî Occasionally you might find that people take pictures of the sparrows near the food court grabbing some dropped popcorn.</li><li>&nbsp;‚Äî Just for fun, guests may take a picture of the bag of popcorn just to see what it comes back with.</li></ul><p>These are all real challenges ‚Äî being able to tell the subtle differences between animals, handling out-of-scope cases, and just plain poor images.</p><p>Before we get there, let‚Äôs start from the beginning.</p><p>There are a lot of tools these days to help you with this part of the process, but the challenge remains the same ‚Äî collecting, labelling, and curating the data.</p><p>Having data to collect is challenge #1. Without images, you have nothing to train. You may need to get creative on sourcing the data, or even creating synthetic data. More on that later.</p><p>A quick note about image pre-processing. I convert all my images to the input size of my neural network and save them as PNG. Inside this square PNG, I preserve the aspect ratio of the original picture and fill the background black. I don‚Äôt stretch the image nor crop any features out. This also helps center the subject.</p><p>Challenge #2 is to establish standards for data quality‚Ä¶and ensure that these standards are followed! These standards will guide you toward that ‚Äúgood‚Äù data. And this assumes, of course, correct labels. Having both is much easier said than done!</p><p>I hope to show how ‚Äúgood‚Äù and ‚Äúcorrect‚Äù actually go hand-in-hand, and how important it is to apply these standards to every image.</p><p>First, I want to point out that the image data discussed here is for the training set. What qualifies as a good image for&nbsp;&nbsp;is a bit different than what qualifies as a good image for&nbsp;. More on that in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>So, what is ‚Äúgood‚Äù data when talking about images? ‚ÄúA picture is worth a thousand words‚Äù, and if the&nbsp;&nbsp;you use to describe the picture do not include the subject you are trying to label, then it is not good and you need remove it from your training set.</p><p>For example, let‚Äôs say you are shown a picture of a zebra and (removing bias toward your application) you describe it as an ‚Äúopen field with a zebra in the distance‚Äù. In other words, if ‚Äúopen field‚Äù is the first thing you notice, then you likely do&nbsp;&nbsp;want to use that image. The opposite is also true ‚Äî if the picture is way too close, you would described it as ‚Äúzebra pattern‚Äù.</p><p>What you want is a description like, ‚Äúa zebra, front and center‚Äù. This would have your subject taking up about 80‚Äì90% of the total frame. Sometimes I will take the time to crop the original image so the subject is framed properly.</p><p>Keep in mind the use of image augmentation at the time of training. Having that buffer around the edges will allow ‚Äúzoom in‚Äù augmentation. And ‚Äúzoom out‚Äù augmentation will simulate smaller subjects, so don‚Äôt start out less than 50% of the total frame for your subject since you lose detail.</p><p>Another aspect of a ‚Äúgood‚Äù image relates to the label. If you can only see the back side of your zoo animal, can you really tell, for example, that it is a cheetah versus a leopard? The key identifying features need to be visible. If a human struggles to identify it, you can‚Äôt expect the computer to learn anything.</p><p>What does a ‚Äúbad‚Äù image look like? Here is what I frequently watch out for:</p><ul><li>Wide angle lens stretching</li><li>High contrast or dark shadows</li><li>‚ÄúDoctored‚Äù images, drawn lines and arrows</li><li>‚ÄúUnusual‚Äù angles or situations</li><li>Picture of a mobile device that has a picture of your subject</li></ul><p>If you have a team of subject matter experts (SMEs) on hand to label the images, you are in a good starting position. Animal trainers at the zoo know the various species, and can spot the differences between, for example, a chimpanzee and a bonobo.</p><p>To a <a href=\"https://towardsdatascience.com/tag/machine-learning-engineer/\" title=\"Machine Learning Engineer\">Machine Learning Engineer</a>, it is easy for you to assume all labels from your SMEs are correct and move right on to training the model. However, even experts make mistakes, so if you can get a second opinion on the labels, your error rate should go down.</p><p>In reality, it can be prohibitively expensive to get one, let alone two, subject matter experts to review image labels. The SME usually has years of experience that make them more valuable to the business in other areas of work. My experience is that the machine learning engineer (that‚Äôs you and me) becomes the second opinion, and often the first opinion as well.</p><p>Over time, you can become pretty adept at labelling, but certainly not an SME. If you do have the luxury of access to an expert, explain to them the labelling standards and how these are required for the application to be successful. Emphasize ‚Äúquality over quantity‚Äù.</p><p>It goes without saying that having a&nbsp;&nbsp;label is so important. However, all it takes is one or two mislabelled images to degrade performance. These can easily slip into your data set with careless or hasty labelling. So, take the time to get it right.</p><p>Ultimately, we as the ML engineer are responsible for model performance. So, if we take the approach of only working on model training and deployment, we will find ourselves wondering why performance is falling short.</p><p>A lot of times, you will come across a really good picture of a very interesting subject, but have no idea what it is! It would be a shame to simply dispose of it. What you can do is assign it a generic label, like ‚ÄúUnknown Bird‚Äù or ‚ÄúRandom Plant‚Äù that are&nbsp;&nbsp;included in your training set. Later in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, you‚Äôll see how to come back to these images at a later date when you have a better idea what they are, and you‚Äôll be glad you saved them.</p><p>If you have done any image labelling, then you know how time consuming and difficult it can be. But this is where having a model, even a less-than-perfect model, can help you.</p><p>Typically, you have a large collection of unlabelled image and you need to go through them one at a time to assign labels. Simply having the model offer a best guess and display the top 3 results lets you step through each image in a matter of seconds!</p><p>Even if the top 3 results are wrong, this can help you narrow down your search. Over time, newer models will get better, and the labelling process can even be somewhat fun!</p><p>In <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, I will show how you can bulk identify images and take this to the next level for faster labelling.</p><p>I mentioned the example above of two species that look very similar, the chimpanzee and the bonobo. When you start out building your data set, you may have very sparse coverage of one or both of these species. In machine learning terms, we these ‚Äúclasses‚Äù. One option is to roll with what you have and hope that the model picks up on the differences with only a handful of example images.</p><p>The option that I have used is to merge two or more classes into one, at least temporarily. So, in this case I would create a class called ‚Äúchimp-bonobo‚Äù, which is composed of the limited example pictures of chimpanzee and bonobo species classes. Combined, these may give me enough to train the model on ‚Äúchimp-bonobo‚Äù, with the trade-off that it‚Äôs a more generic identification.</p><p>Sub-classes can even be normal variations. For example,&nbsp;&nbsp;pink flamingos are grey instead of pink. Or, male and female orangutans have distinct facial features. You wan to have a fairly balanced number of images for these normal variations, and keeping sub-classes will allow you to accomplish this.</p><p>Don‚Äôt be concerned that you are merging completely different looking classes ‚Äî the neural network does a nice job of applying the ‚ÄúOR‚Äù operator. This works both ways ‚Äî it can help you identify male or female variations as one species, but it can hurt you when ‚Äúbad‚Äù outlier images sneak in like the example ‚Äúopen field with a zebra in the distance.‚Äù</p><p>Over time, you will (hopefully) be able to collect more images of the sub-classes and then be able to successfully split them apart (if necessary) and train the model to identify them separately. This process has worked very well for me. Just be sure to double-check all the images when you split them to ensure the labels didn‚Äôt get accidentally mixed up ‚Äî it will be time well spent.</p><p>All of this certainly depends on your user requirements, and you can handle this in different ways either by creating a unique class label like ‚Äúchimp-bonobo‚Äù, or at the front-end presentation layer where you notify the user that you have intentionally merged these classes and provide guidance on further refining the results. Even after you decide to split the two classes, you may want to caution the user that the model could be wrong since the two classes are so similar.</p><p>I realize this was a long write-up for something that on the surface seems intuitive, but these are all areas that I have tripped me up in the past because I didn‚Äôt give them enough attention. Once you have a solid understanding of these principles, you can go on to build a successful application.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, we will take the curated data we collected here to create the classic data sets, with a custom benchmark set that will further enhance your data. Then we will see how best to evaluate our trained model using a specific ‚Äútraining mindset‚Äù, and switch to a ‚Äúproduction mindset‚Äù when evaluating a deployed model.</p>","contentLength":13198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer ‚Äî Part 4: The Model","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/","date":1739480022,"author":"David Martin","guid":4,"unread":true,"content":"<p>In this latest part of my series, I will share what I have learned on selecting a model for <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> and how to fine tune that model. I will also show how you can leverage the model to accelerate your labelling process, and finally how to justify your efforts by generating usage and performance statistics.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, I discussed the process of labelling your image data that you use in your image classification project. I showed how define ‚Äúgood‚Äù images and create sub-classes. In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, I went over various data sets, beyond the usual train-validation-test sets, with benchmark sets, plus how to handle synthetic data and duplicate images. In <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>, I explained how to apply different evaluation criteria to a trained model versus a deployed model, and using benchmarks to determine when to deploy a model.</p><p>So far I have focused a lot of time on labelling and curating the set of images, and also evaluating model performance, which is like putting the cart before the horse. I‚Äôm not trying to minimize what it takes to design a massive neural network ‚Äî this is a very important part of the application you are building. In my case, I spent a few weeks experimenting with different available models before settling on one that fit the bill.</p><p>Once you pick a model structure, you usually don‚Äôt make any major changes to it. For me, six years into deployment, I‚Äôm still using the same one. Specifically, I chose Inception V4 because it has a large input image size and an adequate number of layers to pick up on subtle image features. It also performs inference fast enough on CPU, so I don‚Äôt need to run expensive hardware to serve the model.</p><p>Your mileage may vary. But again, the main takeaway is that focusing on your data will pay dividends versus searching for the best model.</p><p>I will share a process that I found to work extremely well. Once I decided on the model to use, I randomly initialized the weights and let the model train for about 120 epoch before improvements plateau at a fairly modest accuracy, like 93%. At this point, I performed the evaluation of the trained model (see <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>) to clean up the data set. I also incorporated new images as part of the data pipeline (see <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>) and prepared the data sets for the next training run.</p><p>Before starting the next training run, I simply take the last trained model, pop the output layer, and add it back in with random weights. Since the number of output classes are constantly increasing in my case, I have to pop that layer anyway to account for the new number of classes. Importantly, I leave the rest of the trained weights as they were and allow them to continue updating for the new classes.</p><p>This allows the model to train much faster before improvements stall. After repeating this process dozens of times, the training reaches plateau after about 20 epochs, and the test accuracy can reach 99%! The model is building upon the low-level features that it established from the previous runs while re-learning the output weights to prevent overfitting.</p><p>It took me a while to trust this process, and for a few years I would train from scratch every time. But after I attempted this and saw the training time (not to mention the cost of cloud GPU) go down while the accuracy continued to go up, I started to embrace the process. More importantly, I continue to see the evaluation metrics of the deployed model return solid performances.</p><p>During training, you can apply transformations on your images (called ‚Äúaugmentation‚Äù) to give you more diversity from you data set. With our zoo animals, it is fairly safe to apply left-right flop, slight rotations clockwise and counterclockwise, and slight resize that will zoom in and out.</p><p>With these transformations in mind, make sure your images are still able to act as good training images. In other words, an image where the subject is already small will be even smaller with a zoom out, so you probably want to discard the original. Also, some of your original pictures may need to be re-oriented by 90 degrees to be upright since a further rotation would make them look unusual.</p><p>As I mentioned in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, you can use the trained model to assist you in labelling images one at a time. But the way to take this even further is to have your newly trained model identify hundreds at a time while building a list of the results that you can then filter.</p><p>Typically, we have large collections of&nbsp;&nbsp;images that have come in either through regular usage of the application or some other means. Recall from <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a> assigning ‚Äúunknown‚Äù labels to interesting pictures but you have no clue what it is. By using the bulk identification method, we can sift through the collections quickly to target the labelling once we know what they are.</p><p>By combining your current image counts with the bulk identification results, you can target classes that need expanded coverage. Here are a few ways you can leverage bulk identification:</p><ul><li><strong>Increase low image counts</strong>&nbsp;‚Äî Some of your classes may have just barely made the cutoff to be included in the training set, which means you need more examples to improve coverage. Filter for images that have low counts.</li><li><strong>Replace staged or synthetic images</strong>&nbsp;‚Äî Some classes may be built entirely using non-real-world images. These pictures may be good enough to get started with, but may cause performance issues down the road because they look different than what typically comes through. Filter for classes that depend on staged images.</li><li>&nbsp;‚Äî A class in your data set may look like another one. For example, let‚Äôs say your model can identify an antelope, and that looks like a gazelle which your model cannot identify yet. Setting a filter for antelope and a lower confidence score may reveal gazelle images that you can label.</li><li>&nbsp;‚Äî You may not have known how to identify the dozens of cute wallaby pictures, so you saved them under ‚ÄúUnknown‚Äù because it was a good image. Now that you know what it is, you can filter for its look-alike kangaroo and quickly add a new class.</li><li><strong>Mass removal of low scores</strong>&nbsp;‚Äî As a way to clean out your large collection of unlabelled images that have nothing worth labelling, set a filter for lowest scores.</li></ul><p>Recall the decision I made to have image cutoffs from <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, which allows us to ensure an adequate number of example images of a class before we train and server a model to the public. The problem is that you may have a number of classes that are&nbsp;&nbsp;below your cutoff (in my case, 40) and don‚Äôt make it into the model.</p><p>The way I approach this is with a ‚Äúthrow-away‚Äù training run that I do not intend to move to production. I will decrease the lower cutoff from 40 to perhaps 35, build my train-validation-test sets, then train and evaluate like I normally do. The most important part of this is the bulk identification at the end!</p><p>There is a chance that somewhere in the large collection of unlabelled images I will find the few that I need. Doing the bulk identification with this throw-away model helps find them.</p><p>One very important aspect of any machine learning application is being able to show usage and performance reports. Your manager will likely want to see how many times the application is being used to justify the expense, and you as the ML engineer will want to see how the latest model is performing compared to the previous one.</p><p>You should build logging into your model serving to record every transaction going through the system. Also, the manual evaluations from <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a> should be recorded so you can report on performance for such things as accuracy over time, by model version, by confidence scores, by class, etc. You will be able to detect trends and make adjustments to improve the overall solution.</p><p>There are a lot of reporting tools, so I won‚Äôt recommend one over the other. Just make sure you are collecting as much information as you can to build these dashboards. This will justify the time, effort, and cost associated with maintaining the application.</p><p>We covered a lot of ground across this four-part series on building an image classification project and deploying it in the real world. It all starts with the data, and by investing the time and effort into maintaining the highest quality image library, you can reach impressive levels of model performance that will gain the trust and confidence of your business partners.</p><p>As a <a href=\"https://towardsdatascience.com/tag/machine-learning-engineer/\" title=\"Machine Learning Engineer\">Machine Learning Engineer</a>, you are primarily responsible for building and deploying your model. But it doesn‚Äôt stop there ‚Äî dive into the data. The more familiar you are with the data, the better you will understand the strengths and weaknesses of your model. Take a close look at the evaluations and use them as an opportunity to adjust the data set.</p><p>I hope these articles have helped you find new ways to improve your own machine learning project. And by the way, don‚Äôt let the machine do all the learning ‚Äî as humans, our job is to continue our own learning, so don‚Äôt ever stop!</p><p>Thank you for taking this deep dive with me into a data-driven approach to model optimization. I look forward to your feedback and how you can apply this to your own application.<a href=\"https://medium.com/tag/machine-learning?source=post_page-----7f530bc91383---------------------------------------\"></a></p>","contentLength":9124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Use language embeddings for zero-shot classification and semantic search with Amazon Bedrock","url":"https://aws.amazon.com/blogs/machine-learning/use-language-embeddings-for-zero-shot-classification-and-semantic-search-with-amazon-bedrock/","date":1739480012,"author":"Tom Rogers","guid":235,"unread":true,"content":"<p>In this post, we discuss what embeddings are, show how to practically use language embeddings, and explore how to use them to add functionality such as zero-shot classification and semantic search. We then use <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> and language embeddings to add these features to a <a href=\"https://en.wikipedia.org/wiki/RSS\" target=\"_blank\" rel=\"noopener\">really simple syndication (RSS)</a> aggregator application.</p><p>Amazon Bedrock is a fully managed service that makes <a href=\"https://aws.amazon.com/what-is/foundation-models/\" target=\"_blank\" rel=\"noopener\">foundation models</a> (FMs) from leading AI startups and Amazon available through an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. Amazon Bedrock offers a serverless experience, so you can get started quickly, privately customize FMs with your own data, and integrate and deploy them into your applications using <a href=\"https://aws.amazon.com/\" target=\"_blank\" rel=\"noopener\">Amazon Web Services</a> (AWS) services without having to manage infrastructure. For this post, we use the <a href=\"https://aws.amazon.com/bedrock/cohere-command-embed/\" target=\"_blank\" rel=\"noopener\">Cohere v3 Embed</a> model on Amazon Bedrock to create our language embeddings.</p><p>To demonstrate some of the possible uses of these language embeddings, we developed an RSS aggregator website. RSS is a web feed that allows publications to publish updates in a standardized, computer-readable way. On our website, users can subscribe to an RSS feed and have an aggregated, categorized list of the new articles. We use embeddings to add the following functionalities:</p><p>This post uses this application as a reference point to discuss the technical implementation of the semantic search and zero-shot classification features.</p><p>This solution uses the following services:</p><p>The following diagram illustrates the solution architecture.</p><p>This section offers a quick primer on what embeddings are and how they can be used.</p><p>Embeddings are numerical representations of concepts or objects, such as language or images. In this post, we discuss language embeddings. By reducing these concepts to numerical representations, we can then use them in a way that a computer can understand and operate on.</p><p>Let‚Äôs take Berlin and Paris as an example. As humans, we understand the conceptual links between these two words. Berlin and Paris are both cities, they‚Äôre capitals of their respective countries, and they‚Äôre both in Europe. We understand their conceptual similarities almost instinctively, because we can create a model of the world in our head. However, computers have no built-in way of representing these concepts.</p><p>To represent these concepts in a way a computer can understand, we convert them into language embeddings. Language embeddings are high dimensional vectors that learn their relationships with each other through the training of a neural network. During training, the neural network is exposed to enormous amounts of text and learns patterns based on how words are colocated and relate to each other in different contexts.</p><p>Embedding vectors allow computers to model the world from language. For instance, if we embed ‚ÄúBerlin‚Äù and ‚ÄúParis,‚Äù we can now perform mathematical operations on these embeddings. We can then observe some fairly interesting relationships. For instance, we could do the following: Paris ‚Äì France + Germany ~= Berlin. This is because the embeddings capture the relationships between the words ‚ÄúParis‚Äù and ‚ÄúFrance‚Äù and between ‚ÄúGermany‚Äù and ‚ÄúBerlin‚Äù‚Äîspecifically, that Paris and Berlin are both capital cities of their respective countries.</p><p>The following graph shows the word vector distance between countries and their respective capitals.</p><p>Subtracting ‚ÄúFrance‚Äù from ‚ÄúParis‚Äù removes the country semantics, leaving a vector representing the concept of a capital city. Adding ‚ÄúGermany‚Äù to this vector, we are left with something closely resembling ‚ÄúBerlin,‚Äù the capital of Germany. The vectors for this relationship are shown in the following graph.</p><p>For our use case, we use the pre-trained Cohere Embeddings model in Amazon Bedrock, which embeds entire texts rather than a single word. The embeddings represent the meaning of the text and can be operated on using mathematical operations. This property can be useful to map relationships such as similarity between texts.</p><p>One way in which we use language embeddings is by using their properties to calculate how similar an article is to one of the topics.</p><p>To do this, we break down a topic into a series of different and related embeddings. For instance, for culture, we have a set of embeddings for sports, TV programs, music, books, and so on. We then embed the incoming title and description of the RSS articles, and calculate the similarity against the topic embeddings. From this, we can assign topic labels to an article.</p><p>The following figure illustrates how this works. The embeddings that Cohere generates are highly dimensional, containing 1,024 values (or dimensions). However, to demonstrate how this system works, we use an algorithm designed to reduce the dimensionality of the embeddings, <a href=\"https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\" target=\"_blank\" rel=\"noopener\">t-distributed Stochastic Neighbor Embedding (t-SNE)</a>, so that we can view them in two dimensions. The following image uses these embeddings to visualize how topics are clustered based on similarity and meaning.</p><p>You can use the embedding of an article and check the similarity of the article against the preceding embeddings. You can then say that if an article is clustered closely to one of these embeddings, it can be classified with the associated topic.</p><p>This is the <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" target=\"_blank\" rel=\"noopener\">k-nearest neighbor (k-NN) algorithm</a>. This algorithm is used to perform classification and regression tasks. In k-NN, you can make assumptions around a data point based on its proximity to other data points. For instance, you can say that an article that has proximity to the music topic shown in the preceding diagram can be tagged with the culture topic.</p><p>The following figure demonstrates this with an <a href=\"https://arstechnica.com/science/2024/07/the-climate-is-changing-so-fast-that-we-havent-seen-how-bad-extreme-weather-could-get/\" target=\"_blank\" rel=\"noopener\">ArsTechnica article</a>. We plot against the embedding of an article‚Äôs title and description: (The climate is changing so fast that we haven‚Äôt seen how bad extreme weather could get: Decades-old statistics no longer represent what is possible in the present day).</p><p>The advantage of this approach is that you can add custom, user-generated topics. You can create a topic by first creating a series of embeddings of conceptually related items. For instance, an AI topic would be similar to the embeddings for AI, Generative AI, LLM, and Anthropic, as shown in the following screenshot.</p><p>In a traditional classification system, we‚Äôd be required to train a classifier‚Äîa supervised learning task where we‚Äôd need to provide a series of examples to establish whether an article belongs to its respective topic. Doing so can be quite an intensive task, requiring labeled data and training the model. For our use case, we can provide examples, create a cluster, and tag articles without having to provide labeled examples or train additional models. This is shown in the following screenshot of results page of our website.</p><p>In our application, we ingest new articles on a schedule. We use EventBridge schedules to periodically call a Lambda function, which checks if there are new articles. If there are, it creates an embedding from them using Amazon Bedrock and Cohere.</p><p>We calculate the article‚Äôs distance to the different topic embeddings, and can then determine whether the article belongs to that category. This is done with Aurora PostgreSQL with pgvector. We store the embeddings of the topics and then calculate their distance using the following SQL query:</p><div><pre><code>const topics = await sqlClient.then(it=&gt; it.query(\n    `SELECT name, embedding_description, similarity\n     FROM (SELECT topic_id as name, embedding_description, (1- ABS( 1 ‚Äì(embed.embedding &lt;-&gt; $1))) AS \"similarity\" FROM topic_embedding_link embed)  topics\n     ORDER BY similarity desc`,\n    [toSql(articleEmbedding)]\n  ))\n</code></pre></div><p>The &lt;-&gt; operator in the preceding code calculates the Euclidean distance between the article and the topic embedding. This number allows us to understand how close an article is to one of the topics. We can then determine the appropriateness of a topic based on this ranking.</p><p>We then tag the article with the topic. We do this so that the subsequent request for a topic is as computationally as light as possible; we do a simple join rather than calculating the Euclidean distance.</p><div><div><pre><code>const formattedTopicInsert = pgformat(\n    `INSERT INTO feed_article_topic_link(topic_id, feed_article_id) VALUES %L ON CONFLICT DO NOTHING`,\n    topicLinks\n  )</code></pre></div></div><p>We also cache a specific topic/feed combination because these are calculated hourly and aren‚Äôt expected to change in the interim.</p><p>As previously discussed, the embeddings produced by Cohere contain a multitude of features; they embed the meanings and semantics of a word of phrase. We‚Äôve also found that we can perform mathematical operations on these embeddings to do things such as calculate the similarity between two phrases or words.</p><p>We can use these embeddings and calculate the similarity between a search term and an embedding of an article with the k-NN algorithm to find articles that have similar semantics and meanings to the search term we‚Äôve provided.</p><p>For example, in one of our RSS feeds, we have a lot of different articles that rate products. In a traditional search system, we‚Äôd rely on keyword matches to provide relevant results. Although it might be simple to find a specific article (for example, by searching ‚Äúbest digital notebooks‚Äù), we would need a different method to capture multiple product list articles.</p><p>In a semantic search system, we first transform the term ‚ÄúProduct list‚Äù in an embedding. We can then use the properties of this embedding to perform a search within our embedding space. Using the k-NN algorithm, we can find articles that are semantically similar. As shown in the following screenshot, despite not containing the text ‚ÄúProduct list‚Äù in either the title or description, we‚Äôve been able to find articles that contain a product list. This is because we were able to capture the semantics of the query and match it to the existing embeddings we have for each article.</p><p>In our application, we store these embeddings using pgvector on Aurora PostgreSQL. pgvector is an open source extension that enables vector similarity search in PostgreSQL. We transform our search term into an embedding using Amazon Bedrock and Cohere v3 Embed.</p><p>After we‚Äôve converted the search term to an embedding, we can compare it with the embeddings on the article that have been saved during the ingestion process. We can then use pgvector to find articles that are clustered together. The SQL code for that is as follows:</p><div><pre><code>SELECT *\nFROM (\n    SELECT feed_articles.id as id, title, feed_articles.feed_id as feed, feedName, slug, description, url, author, image, published_at as published, 1 - ABS(1 - (embedding &lt;-&gt; $2)) AS \"similarity\"\n    FROM feed_articles\n    INNER JOIN (select feed_id, name as feedName from feed_user_subscription fus where fus.user_id=$1) sub on feed_articles.feed_id=sub.feed_id\n    ${feedId != undefined ? `WHERE feed_articles.feed_id = $4` : \"\"}\n)\nWHERE similarity &gt; 0.95\nORDER BY similarity desc\nLIMIT $3;\n</code></pre></div><p>This code calculates the distance between the topics, and the embedding of this article as ‚Äúsimilarity.‚Äù If this distance is close, then we can assume that the topic of the article is related, and we therefore attach the topic to the article.</p><p>To deploy this application in your own account, you need the following prerequisites:</p><ul><li>Model access for Cohere Embed English. On the Amazon Bedrock console, choose  in the navigation pane, then choose . Select the FMs of your choice and request access.</li></ul><p>When the prerequisite steps are complete, you‚Äôre ready to set up the solution:</p><ol start=\"2\"><li>Navigate to the solution directory:</li></ol><ol start=\"3\"><li>In your terminal, export your AWS credentials for a role or user in ACCOUNT_ID. The role needs to have all necessary permissions for AWS CDK deployment: \n  <ul><li><strong>export AWS_REGION=‚Äù&lt;region&gt;‚Äù</strong> ‚Äì The <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\" target=\"_blank\" rel=\"noopener\">AWS Region</a> you want to deploy the application to</li><li><strong>export AWS_ACCESS_KEY_ID=‚Äù&lt;access-key&gt;‚Äù</strong> ‚Äì The access key of your role or user</li><li><strong>export AWS_SECRET_ACCESS_KEY=‚Äù&lt;secret-key&gt;‚Äù</strong> ‚Äì The secret key of your role or user</li></ul></li></ol><ol start=\"4\"><li>If you‚Äôre deploying the AWS CDK for the first time, run the following command:</li></ol><ol start=\"5\"><li>To synthesize the <a href=\"http://aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a> template, run the following command:<code>cdk synth -c vpc_id=&lt;ID Of your VPC&gt;</code></li></ol><ol start=\"6\"><li>To deploy, use the following command:<code>cdk deploy -c vpc_id=&lt;ID Of your VPC&gt;</code></li></ol><p>When deployment is finished, you can check these deployed stacks by visiting the AWS CloudFormation console, as shown in the following screenshot.</p><p>Run the following command in the terminal to delete the CloudFormation stack provisioned using the AWS CDK:</p><p>In this post, we explored what language embeddings are and how they can be used to enhance your application. We‚Äôve learned how, by using the properties of embeddings, we can implement a real-time zero-shot classifier and can add powerful features such as semantic search.</p><p>The code for this application can be found on the accompanying <a href=\"https://github.com/aws-samples/rss-aggregator-using-cohere-embeddings-bedrock\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>. We encourage you to experiment with language embeddings and find out what powerful features they can enable for your applications!</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/ML-17434-About-Author.jpeg\" alt=\"About the Author\" width=\"100\" height=\"133\">is a Solutions Architect based in Amsterdam, the Netherlands. He has a background in software engineering. At AWS, Thomas helps customers build cloud solutions, focusing on modernization, data, and integrations.</p>","contentLength":13247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer ‚Äî Part 2: The Data Sets","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/","date":1739478579,"author":"David Martin","guid":3,"unread":true,"content":"<p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, we discussed the importance of collecting good image data and assigning proper labels for your <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> project to be successful. Also, we talked about classes and sub-classes of your data. These may seem pretty straight forward concepts, but it‚Äôs important to have a solid understanding going forward. So, if you haven‚Äôt, please check it out.</p><p>Now we will discuss how to build the various data sets and the techniques that have worked well for my application. Then in the <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">next part</a>, we will dive into the evaluation of your models, beyond simple accuracy.</p><p>I will again use the example zoo animals image classification app.</p><p>As machine learning engineers, we are all familiar with the train-validation-test sets, but when we include the concept of sub-classes discussed in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, and incorporate to concepts discussed below to set a minimum and maximum image count per class, as well as staged and synthetic data to the mix, the process gets a bit more complicated. I had to create a custom script to handle these options.</p><p>I will walk you through these concepts before we split the data for training:</p><ul><li>&nbsp;‚Äî Too few images and your model performance will suffer. Too many and you spend more time training than it‚Äôs worth.</li><li>&nbsp;‚Äî Your model indicates how confident it is in the predictions. Let‚Äôs use that to decide when to present results to the user.</li><li>&nbsp;‚Äî Real-world data is messy and the benchmark sets should reflect that. These need to stretch the model to the limit and help us decide when it is ready for production.</li><li><strong>Staged and synthetic data</strong>&nbsp;‚Äî Real-world data is king, but sometimes you need to produce the your own or even generate data to get off the ground. Be careful it doesn‚Äôt hurt performance.</li><li>&nbsp;‚Äî Repeat data can skew your results and give you a false sense of performance. Make sure your data is diverse.</li><li>&nbsp;‚Äî Combine sub-classes, apply cutoffs, and create your train-validation-test sets. Now we are ready to get the show started.</li></ul><p>In my experience, using a minimum of 40 images per class provides descent performance. Since I like to use 10% each for the test set and validation set, that means at least 4 images will be used to check the training set, which feels just barely adequate. Using fewer than 40 images per class, I notice my model evaluation tends to suffer.</p><p>On the other end, I set a maximum of about 125 images per class. I have found that the performance gains tend to plateau beyond this, so having more data will slow down the training run with little to show for it. Having more than the maximum is fine, and these ‚Äúoverflow‚Äù can be added to the test set, so they don‚Äôt go to waste.</p><p>There are times when I will drop the minimum cutoff to, say 35, with no intention of moving the trained model to production. Instead, the purpose is to leverage this throw-away model to find more images from my unlabelled set. This is a technique that I will go into more detail in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>You are likely familiar with the softmax score. As a reminder, softmax is the probability assigned to each label. I like to think of it as a confidence score, and we are interested in the class that receives the highest confidence. Softmax is a value between zero and one, but I find it easier to interpret confidence scores between zero and 100, like a percentage.</p><p>In order to decide if the model is confident enough with its prediction, I have chosen a threshold of 95. I use this threshold when determining if I want to present results to the user.</p><p>Scores above the threshold have a better changes of being right, so I can confidently provide the results. Scores below the threshold may not be right ‚Äî in fact it could be ‚Äúout-of-scope‚Äù, meaning it‚Äôs something the model doesn‚Äôt know how to identify. So, instead of taking the risk of presenting incorrect results, I instead prompt the user to try again and offer suggestions on how to take a ‚Äúgood‚Äù picture.</p><p>Admittedly this is somewhat arbitrary cutoff, and you should decide for your use-case what is appropriate. In fact, this score could probably be adjusted for each trained model, but this would make it harder to compare performance across models.</p><p>I will refer to this confidence score frequently in the evaluations section in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>Let me introduce what I call the benchmark sets, which you can think of as extended test sets. These are hand-picked images designed to stretch the limits of your model, and provide a measure for specific classes of your data. Use these benchmarks to justify moving your model to production, and for an objective measure to show to your manager.</p><ul><li>&nbsp;‚Äî These are the ‚Äúextra credit‚Äù images, like the bonus questions a professor would add to the quiz to see which students are paying attention. You need a keen eye to spot the difference between the ground truth and a similar looking class. For example, a cheetah sleeping in the shade that could pass as a leopard if you don‚Äôt look closely.</li><li>&nbsp;‚Äî These are the ‚Äútrick question‚Äù images. Our model is trained on zoo animals, but people are known for not following the rules. For example, a zoo guest takes a picture of their child wearing cheetah face paint.</li><li>&nbsp;‚Äî These are your ‚Äúbread and butter‚Äù classes that need to get near perfect scores and zero errors. This would be a make-or-break benchmark for moving to production.</li><li>&nbsp;‚Äî These are your ‚Äúrare but exceptional‚Äù classes that again need to be correct, but reach a minimum score like the confidence threshold.</li></ul><p>When looking for images to add to the benchmarks, you can likely find them in real-world images from your deployed model. See the evaluation in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>For each benchmark, calculate the min, max, median, and mean scores, and also how many images get scores above and below the confidence threshold. Now you can compare these measures against what is currently in production, and against your minimum requirements, to help decide if the new model is production worthy.</p><p>Perhaps the biggest hurdle to any supervised machine learning application is having data to train the model. Clearly, ‚Äúreal-world‚Äù data that comes from actual users of the application is ideal. However you can‚Äôt really collect these until the model is deployed. Chicken and egg problem.</p><p>One way to get started to is to have volunteers collect ‚Äústaged‚Äù images for you, trying to act like real users. So, let‚Äôs have our zoo staff go around taking pictures of the animals. This is a good start, but there will be a certain level of bias introduced in these images. For example, the staff may take the photos over a few days, so you may not get the year-round weather conditions.</p><p>Another way to get pictures is use computer-generated ‚Äúsynthetic‚Äù images. I would avoid these at all costs, to be honest. Based on my experience, the model struggles with these because they look‚Ä¶different. The lighting is not natural, the subject may superimposed on a background and so the edges look too sharp, etc. Granted, some of the AI generated images look very realistic, but if you look closely you may spot something unusual. The neural network in your model will notice these, so be careful.</p><p>The way that I handle these staged or synthetic images is as a sub-class that gets merged into the training set, but only&nbsp;&nbsp;giving preference to the real-world images. I cap the number of staged images to 60, so if I have 10 real-world, I now only need 50 staged. Eventually, these staged and synthetic images are phased out completely, and I rely entirely on real-world.</p><p>One problem that can creep into your image set are duplicate images. These can be exact copies of pictures, or they can be extremely similar. You may think that this is harmless, but imagine having 100 pictures of an elephant that are exactly the same ‚Äî your model will not know what to do with a different angle of the elephant.</p><p>Now, let‚Äôs say you have only&nbsp;&nbsp;pictures that are nearly the same. Not so bad, right? Well, here is what can happen to them:</p><ul><li>Both pictures go in the training set ‚Äî The model doesn‚Äôt learn anything from the repeated image and it wastes time processing them.</li><li>One goes into the training set, the other goes into the test set ‚Äî Your test score will be higher, but it is not an accurate evaluation.</li><li>Both are in the test set ‚Äî Your test score will be compounded either higher or lower than it should be.</li></ul><p>None of these will help your model.</p><p>There are a few ways to find duplicates. The approach I have taken is to calculate a hamming distance on all the pictures and identify the ones that are very close. I have an interface that displays the duplicates and I decide which one I like best, and remove the other.</p><p>Another way (I haven‚Äôt tried this yet) is to create a vector representation of your images. Store these a vector database, and you can do a similarity search to find nearly identical images.</p><p>Whatever method you use, it is important to clean up the duplicates.</p><p>Now we are ready to build the traditional training, validation, and test sets. This is no longer a straight forward task since I want to:</p><ol><li>Merge sub-classes into a main class.</li><li>Prioritize real-world images over staged or synthetic images.</li><li>Apply a minimum number of images per class.</li><li>Apply a maximum number of images per class, sending the ‚Äúoverflow‚Äù to the test set.</li></ol><p>This process is somewhat complicated and depends on how you manage your image library. First, I would recommend keeping your images in a folder structure that has sub-class folders. You can get image counts by using a script to simply read the folders. Second is to keep a configuration of how the sub-classes are merged. To really set yourself up for success, put these image counts and merge rules in a database for faster lookups.</p><p>My train-validation-test set splits are usually 90‚Äì10‚Äì0. I originally started out using 80‚Äì10‚Äì10, but with diligence on keeping the entire data set clean, I noticed validation and test scores became pretty even. This allowed me to increase the training set size, and use ‚Äúoverflow‚Äù to become the test set, as well as using the benchmark sets.</p><p>In this part, we‚Äôve built our data sets by merging sub-classes and using the image count cutoffs. Plus we handle staged and synthetic data as well as cleaning up duplicate images. We also created benchmark sets and defined confidence thresholds, which help us decide when to move a model to production.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>, we will discuss how we are going to evaluate the different model performances. And then finally we will get to the actual model training and the techniques to enhance accuracy.<a href=\"https://medium.com/tag/machine-learning?source=post_page-----1948daf60483---------------------------------------\"></a></p>","contentLength":10545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coding Interviews were HARD Until I Learned These 20 Tips","url":"https://blog.algomaster.io/p/20-coding-interviews-tips","date":1739467827,"author":"Ashish Pratap Singh","guid":751,"unread":true,"content":"<p>I gave my first  in 2016‚Äîand failed. I failed the next five interviews as well before finally landing my first job at .</p><p>Since then, I‚Äôve interviewed with many companies and faced my fair share of rejections. However, over the years, my failure rate in coding interviews dropped significantly.</p><p>By 2022, with just 1.5 months of focused preparation, I successfully cleared interviews at  and .</p><p>Surprisingly, my success wasn‚Äôt due to a dramatic improvement in problem-solving skills. The real game-changer was my approach‚Äî and  during the interview.</p><p>In this article, I‚Äôll share  that made coding interviews significantly easier for me.</p><p>These tips cover everything you need to know, including:</p><ul><li><p>How to systematically approach coding interview problems</p></li><li><p>Key concepts and patterns you should know</p></li><li><p>The type of problems you should practice</p></li><li><p>How to choose the right algorithm for a given problem</p></li><li><p>Techniques to optimize your solution</p></li><li><p>How to communicate your thought process effectively</p></li></ul><p>By applying these strategies, you‚Äôll be able to tackle coding interviews with confidence and massively increase your chances of success.</p><p>In a coding interview, interviewers want to see how well you , , and  under pressure.</p><p>Here's a breakdown of what they look for:</p><ol><li><p><strong>Understanding the problem</strong>: Do you ask clarifying questions instead of making assumptions to ensure you fully understand the problem?</p></li><li><p>: Can you decompose the problem into smaller, manageable parts?</p></li><li><p>: Can you design an optimal solution in terms of time and space complexity?</p></li><li><p>: Do you handle edge cases like empty inputs, duplicates, large values, or special conditions?</p></li><li><p>: Can you explain why one approach is better than another?</p></li><li><p>: Do you have a strong grasp of data structures and algorithms, and can you choose the right one for the problem?</p></li><li><p>Can you quickly compute the time and space complexity of your solution?</p></li><li><p><strong>Explaining your thought process</strong>: Can you clearly articulate your approach and why it works?</p></li><li><p>: Are you receptive to hints and able to adjust your approach accordingly?</p></li><li><p>: Do you follow good coding practices (meaningful variable names, proper indentation, modular functions etc..)?</p></li><li><p><strong>Improving the initial solution</strong>: Can you optimize and refine your first solution when prompted?</p></li><li><p>Are you able to tackle variations of the original problem?</p></li><li><p>Can you manually walk through your code with sample inputs to verify correctness?</p></li></ol><p>Most coding interviews last </p><p>Depending on the company and interviewer, you may be asked to solve 2-3easy/medium problems or 1 hard problem with follow-ups.</p><p>Lets assume you are given one problem, with a follow up in a 45-minute interview. Here‚Äôs how you can optimally allocate your time:</p><ol><li><p>The interviewer may ask you to introduce yourself. Prepare a concise 1-2 minute introduction that highlights your background, experience, and key strengths. Practice it beforehand so that you can deliver it smoothly.</p></li><li><p><strong>Understand the Problem (5-10 mins):  </strong>Carefully read the problem statement, ask clarifying questions, and walk through sample inputs and expected outputs.</p></li><li><p><strong>Plan the Approach (10-20 mins): </strong>Brainstorm possible solutions, evaluate trade-offs, and discuss time and space complexity.</p></li><li><p><strong>Implement the Code (20-30 mins): </strong>Write a clean, modular and readable code.</p></li><li><p>Dry-run your code with sample inputs, debug any issues, and ensure edge cases are handled.</p></li><li><p><strong>Follow-ups and Wrap Up (35-45 mins): </strong>Answer follow up questions, and ask thoughtful questions to the interviewer about the company, role, or team.</p></li></ol><p>One of the biggest mistakes candidates make in coding interviews is jumping into coding too soon.</p><p>If you don't fully understand the question, you might end up solving the </p><p>Here‚Äôs how to ensure you grasp the problem before coding:</p><h4><strong>Read the Problem Carefully</strong></h4><p>Take a moment to absorb the problem statement. Rephrase it in your own words to confirm your understanding. </p><p>Identify the expected input/output format and any hidden constraints.</p><p>If anything is unclear, ask questions before diving into the solution. Interviewers appreciate when you seek clarity. Never assume details that aren‚Äôt explicitly mentioned in the problem statement.</p><p>Common clarifications include:</p><ul><li><p>Are there duplicate values?</p></li><li><p>Can the input be empty? If so, what should the output be?</p></li><li><p>Should the solution handle negative numbers?</p></li><li><p>Should the output maintain the original order of elements?</p></li><li><p>Is the graph directed or undirected?</p></li><li><p>Does the input contain only lowercase English letters, or can it have uppercase, digits, or special characters?</p></li><li><p>What should happen if multiple solutions exist? Should I return any valid solution, or does the problem have specific requirements?</p></li></ul><h4><strong>Walk Through Input/Output Examples</strong></h4><p>Once you understand the problem statement and constraints, go over a few input and output examples to make sure you get it.</p><p>Draw them out if it helps, especially for visual data structures like trees or graphs.</p><p>Try to take examples that cover different scenarios of the problem. Think about any  that might come up.</p>","contentLength":4908,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/61c3f6c0-4027-4d37-b4a7-a30fc183fa12_1602x1032.png","enclosureMime":"","commentsUrl":null},{"title":"Looking back at our Bug Bounty program in 2024","url":"https://engineering.fb.com/2025/02/13/security/looking-back-at-our-bug-bounty-program-in-2024/","date":1739466046,"author":"","guid":471,"unread":true,"content":"<ul><li aria-level=\"1\"><a href=\"https://about.fb.com/news/2019/01/designing-security-for-billions/\" target=\"_blank\" rel=\"noopener\"></a></li></ul><h2></h2><ul></ul><h2></h2><h2></h2><p><b>Ads audience tools designed to help people choose a target audience for their ads: </b><a href=\"https://bugbounty.meta.com/payout-guidelines/ads-audience/\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://www.facebook.com/business/help/717368264947302\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://bugbounty.meta.com/payout-guidelines/ads-audience/\" target=\"_blank\" rel=\"noopener\"></a></p><p><b>Mixed reality hardware products:</b><a href=\"http://hardwear.io\" target=\"_blank\" rel=\"noopener\"></a></p><h2></h2><p><b>Organizing community events and presenting joint research:</b></p><p><b>Providing resources and timely updates for the research community:</b></p>","contentLength":239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A new tool for visualizing Rust lifetimes","url":"https://www.youtube.com/watch?v=NV6Xo_el_2o","date":1739458834,"author":"Let's Get Rusty","guid":475,"unread":true,"content":"<article>See how RustOwl can help you understand lifetimes in a real Rust codebase. A brand-new tool designed to visualize Rust lifetimes and make learning Rust easier. Check it out and see how it can change the way you write Rust!\n\nFree Rust training: https://letsgetrusty.com/bootcamp\n\nRustOwl: https://github.com/cordx56/rustowl\n\nCorrections:\n- Bacon is a CLI tool, not a library. Check it out here: https://github.com/Canop/bacon</article>","contentLength":424,"flags":null,"enclosureUrl":"https://www.youtube.com/v/NV6Xo_el_2o?version=3","enclosureMime":"","commentsUrl":null},{"title":"Bridging the Gap: Democratizing AI for All","url":"https://www.kdnuggets.com/bridging-gap-democratizing-ai","date":1739458803,"author":"Vidhi Chugh","guid":589,"unread":true,"content":"<article>Let‚Äôs explore how democratizing AI can level the playing field and create opportunities for all, no matter the background or resources.</article>","contentLength":137,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/chugh_Bridging-the-Gap-Democratizing-AI-for-All_1.png","enclosureMime":"","commentsUrl":null},{"title":"Rust vs C++ Performance","url":"https://www.youtube.com/watch?v=WnMin9cf78g","date":1739457041,"author":"Anton Putra","guid":537,"unread":true,"content":"<article>C++ vs Rust Speed.\n\nüî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)\n\nüçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&amp;si=p-UOaVM_6_SFx52H\n\nüëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience &amp; Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)\n‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra\n‚ñ∫  Twitter/X: https://twitter.com/antonvputra\n‚ñ∫  GitHub: https://github.com/antonputra\n‚ñ∫  Email: me@antonputra.com\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/245/lessons/245\n\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/429\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/431\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/433\n\n#rust #golang #devops</article>","contentLength":1133,"flags":null,"enclosureUrl":"https://www.youtube.com/v/WnMin9cf78g?version=3","enclosureMime":"","commentsUrl":null},{"title":"How to Scale Sklearn with Dask","url":"https://www.kdnuggets.com/how-to-scale-sklearn-dask","date":1739451649,"author":"Iv√°n Palomares Carrascosa","guid":588,"unread":true,"content":"<article>Here's how Dask applies the building blocks of sklearn to bring ML modeling workflows to the next level of scalability via high-performance parallel computing</article>","contentLength":158,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/crVYYGyvTE2Jilsmvukhpw.jpeg","enclosureMime":"","commentsUrl":null},{"title":"Peter Bengtsson: get in JavaScript is the same as property in Python","url":"http://www.peterbe.com/plog/get-in-javascript-is-the-same-as-property-in-python","date":1739450516,"author":"","guid":280,"unread":true,"content":"<article>Prefix a function, in an object or class, with `get` and then that acts as a function call without brackets. Just like Python's `property` decorator.</article>","contentLength":149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GenAI Patterns: Reranker","url":"https://martinfowler.com/articles/gen-ai-patterns/#reranker","date":1739441760,"author":"Martin Fowler","guid":300,"unread":true,"content":"<p>LLMs struggle with large amounts of context. <b>Bharani\n      Subramaniam</b> and I explain how to mitigate this common RAG\n      problem with a <a href=\"https://martinfowler.com/articles/gen-ai-patterns/#reranker\">Reranker</a> which takes the document\n      fragments from the retriever, and ranks them according to their usefulness.</p>","contentLength":253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EuroPython: EuroPython February 2025 Newsletter","url":"https://blog.europython.eu/europython-february-2025-newsletter/","date":1739435771,"author":"","guid":279,"unread":true,"content":"<p>Hope you&amp;aposre all having a fantastic February. We sure have been busy and got some exciting updates for you as we gear up for EuroPython 2025, which is taking place once again in the beautiful city of Prague. So let&amp;aposs dive right in!</p><p>EuroPython 2025 is right around the corner and our programme team is hard at work putting together an amazing lineup. But we need your help to shape the conference! We received over 572 fantastic proposals, and now it‚Äôs time for Community Voting! üéâ If you&amp;aposve attended EuroPython before or submitted a proposal this year, you‚Äôre eligible to vote.</p><p>üì¢ More votes = a stronger, more diverse programme! Spread the word and get your EuroPython friends to cast their votes too.</p><p>üèÉThe deadline is , so don‚Äôt miss your chance!</p><p>Want to play a key role in building an incredible conference? Join our review team and help select the best talks for EuroPython 2025! Whether you&amp;aposre a Python expert or an enthusiastic community member, your insights matter.</p><p>We‚Äôd like to also thank the over 100 people who have already signed up to review! For those who haven‚Äôt done so yet, please remember to accept your Pretalx link and get your reviews in by </p><p>You can already start reviewing proposals, and each review takes as little as 5 minutes. We encourage reviewers to go through at least 20-30 proposals, but if you can do more, even better! With almost 600 submissions to pick from, your help ensures we curate a diverse and engaging programme.</p><p>üèÉThe deadline is Monday next week, so don‚Äôt delay!</p><p>EuroPython isn‚Äôt just present at other Python events‚Äîwe actively support them too! As a community sponsor, we love helping local PyCons grow and thrive. We love giving back to the community and strengthening Python events across Europe! üêçüíô</p><p>The EuroPython team had a fantastic time at PyCon + Web in Berlin, meeting fellow Pythonistas, exchanging ideas, and spreading the word about EuroPython 2025. It was great to connect with speakers, organizers, and attendees.&nbsp;</p><p>Ever wondered how long it takes to walk from Berlin to Prague? A huge thank you to our co-organizers, Cheuk, Artur, and Cristi√°n, for answering that in their fantastic lightning talk about EuroPython!</p><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeeVCMBckwBAVXDAhKyHXq8LJVHJysbWS61RNAeilg_qHzHZgzbC1SALu5u3qS-YjyjSQoUE3zIzVCcSHNj26mzPf43RP4PgcV3iUJPzcu_gAEXO25atB6gn7ZQ8O4Pu6vGttw_MQ?key=7MRGElcwE05wjMSe8jkdrI9g\" alt=\"alt\" width=\"512\" height=\"344\"><p>We had some members of the EuroPython team at FOSDEM 2025, connecting with the open-source community and spreading the Python love! üéâ We enjoyed meeting fellow enthusiasts, sharing insights about the EuroPython Society, and giving away the first EuroPython 2025 stickers. If you stopped by‚Äîthank you and we hope to see you in Prague this July.</p><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd6Dp3RdSY2Wm04WwjDJE6NP1fW_NRCUEVZws-cdneLjGf0gvbq7WWphC8AHEwS0qJsAT6ao4Nm36N3e2QhN15S-w1Xmv1AJxEo0480IWXXwLAVhXZ_TcR5qDW31Hf40hYeDKJzyw?key=7MRGElcwE05wjMSe8jkdrI9g\" alt=\"alt\" width=\"517\" height=\"388\"><h2>ü¶í Speaker Mentorship Programme</h2><p>The signups for The Speaker Mentorship Programme closed on 22nd January 2025. We‚Äôre excited to have matched 43 mentees with 24 mentors from our community. We had an increase in the number of mentees who signed up and that‚Äôs amazing! We‚Äôre glad to be contributing to the journey of new speakers in the Python community. A massive thank you to our mentors for supporting the mentees and to our mentees; we‚Äôre proud of you for taking this step in your journey as a speaker.&nbsp;</p><p>26 mentees submitted at least 1 proposal. Out of this number, 13 mentees submitted 1 proposal, 9 mentees submitted 2 proposals, 2 mentees submitted 3 proposals, 1 mentee submitted 4 proposals and lastly, 1 mentee submitted 5 proposals. We wish our mentees the best of luck. We look forward to the acceptance of their proposals.</p><p>In a few weeks, we will host an online panel session with 2‚Äì3 experienced community members who will share their advice with first-time speakers. At the end of the panel, there will be a Q&amp;A session to answer all the participants‚Äô questions.</p><p>You can watch the recording of the previous year‚Äôs workshop here:</p><p>EuroPython is one of the largest Python conferences in Europe, and it wouldn‚Äôt be possible without our sponsors. We are so grateful for the companies who have already expressed interest. If you‚Äôre interested in sponsoring EuroPython 2025 as well, please reach out to us at <a href=\"mailto:sponsoring@europython.eu\">sponsoring@europython.eu</a>.</p><h2>üé§ EuroPython Speakers Share Their Experiences</h2><p>We asked our past speakers to share their experiences speaking at EuroPython. These videos have been published on YouTube as shorts, and we&amp;aposve compiled them into brief clips for you to watch.</p><p>A big thanks goes to Sebastian Witowski, Jan Smitka, Yuliia Barabash, Jodie Burchell, Max Kahan, and Cheuk Ting Ho for sharing their experiences.</p><p>Why You Should Submit a Proposal for EuroPython? Part 2</p><p>Why You Should Submit a Proposal for EuroPython? Part 3</p><h2>üìä EuroPython Society Board Report&nbsp;</h2><p>The EuroPython conference wouldn‚Äôt be what it is without the incredible volunteers who make it all happen. üíû Behind the scenes, there‚Äôs also the EuroPython Society‚Äîa volunteer-led non-profit that manages the fiscal and legal aspects of running the conference, oversees its organization, and works on a few smaller projects like the grants programme. To keep everyone in the loop and promote transparency, the Board is sharing regular updates on what we‚Äôre working on.</p><p>That&amp;aposs all for now! Keep an eye on your inbox and our website for more news and announcements. We&amp;aposre counting down the days until we can come together in Prague to celebrate our shared love for Python. üêç‚ù§Ô∏è</p><p>Cheers,The EuroPython Team</p>","contentLength":5275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Extensible Wasm Applications with Go","url":"https://go.dev/blog/wasmexport","date":1739404800,"author":"Cherry Mui","guid":243,"unread":true,"content":"<p>\n      Cherry Mui\n      13 February 2025\n      </p><p>Go 1.24 enhances its WebAssembly (Wasm) capabilities with the\naddition of the  directive and the ability to build a reactor\nfor WebAssembly System Interface (WASI).\nThese features enable Go developers to export Go functions to Wasm,\nfacilitating better integration with Wasm hosts and expanding the possibilities\nfor Go-based Wasm applications.</p><h2>WebAssembly and the WebAssembly System Interface</h2><p><a href=\"https://webassembly.org/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly (Wasm)</a> is a binary instruction format\nthat was initially created for web browsers, providing the execution of\nhigh-performance, low-level code at speeds approaching native performance.\nSince then, Wasm‚Äôs utility has expanded, and it is now used in various\nenvironments beyond the browser.\nNotably, cloud providers offer services that directly execute Wasm\nexecutables, taking advantage of the\n<a href=\"https://wasi.dev/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly System Interface (WASI)</a> system call API.\nWASI allows these executables to interact with system resources.</p><p>Go first added support for compiling to Wasm in the 1.11 release, through the\n port.\nGo 1.21 added a new port targeting the WASI preview 1 syscall API through the\nnew  port.</p><h2>Exporting Go Functions to Wasm with </h2><p>Go 1.24 introduces a new compiler directive, , which allows\ndevelopers to export Go functions to be called from outside of the\nWasm module, typically from a host application that runs the Wasm runtime.\nThis directive instructs the compiler to make the annotated function available\nas a Wasm <a href=\"https://webassembly.github.io/spec/core/valid/modules.html?highlight=export#exports\" rel=\"noreferrer\" target=\"_blank\">export</a>\nin the resulting Wasm binary.</p><p>To use the  directive, simply add it to a function definition:</p><pre><code>//go:wasmexport add\nfunc add(a, b int32) int32 { return a + b }\n</code></pre><p>With this, the Wasm module will have an exported function named  that\ncan be called from the host.</p><p>This is analogous to the <a href=\"https://go.dev/cmd/cgo#hdr-C_references_to_Go\">cgo  directive</a>,\nwhich makes the function available to be called from C,\nthough  uses a different, simpler mechanism.</p><p>A WASI reactor is a WebAssembly module that operates continuously, and\ncan be called upon multiple times to react on events or requests.\nUnlike a ‚Äúcommand‚Äù module, which terminates after its main function finishes,\na reactor instance remains live after initialization, and its exports remain\naccessible.</p><p>With Go 1.24, one can build a WASI reactor with the  build\nflag.</p><pre><code>$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm\n</code></pre><p>The build flag signals to the linker not to generate the  function\n(the entry point for a command module), and instead generate an\n function, which performs runtime and package initialization,\nalong with any exported functions and their dependencies.\nThe  function must be called before any other exported functions.\nThe  function will not be automatically invoked.</p><p>To use a WASI reactor, the host application first initializes it by calling\n, then simply invoke the exported functions.\nHere is an example using <a href=\"https://wazero.io/\" rel=\"noreferrer\" target=\"_blank\">Wazero</a>, a Go-based Wasm runtime\nimplementation:</p><pre><code>// Create a Wasm runtime, set up WASI.\nr := wazero.NewRuntime(ctx)\ndefer r.Close(ctx)\nwasi_snapshot_preview1.MustInstantiate(ctx, r)\n\n// Configure the module to initialize the reactor.\nconfig := wazero.NewModuleConfig().WithStartFunctions(\"_initialize\")\n\n// Instantiate the module.\nwasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)\n\n// Call the exported function.\nfn := wasmModule.ExportedFunction(\"add\")\nvar a, b int32 = 1, 2\nres, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))\nc := api.DecodeI32(res[0])\nfmt.Printf(\"add(%d, %d) = %d\\n\", a, b, c)\n\n// The instance is still alive. We can call the function again.\nres, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))\nfmt.Printf(\"add(%d, %d) = %d\\n\", b, c, api.DecodeI32(res[0]))\n</code></pre><p>The  directive and the reactor build mode allow applications to\nbe extended by calling into Go-based Wasm code.\nThis is particularly valuable for applications that have adopted Wasm as a\nplugin or extension mechanism with well-defined interfaces.\nBy exporting Go functions, applications can leverage the Go Wasm modules to\nprovide functionality without needing to recompile the entire application.\nFurthermore, building as a reactor ensures that the exported functions can be\ncalled multiple times without requiring reinitialization, making it suitable\nfor long-running applications or services.</p><h2>Supporting rich types between the host and the client</h2><p>Go 1.24 also relaxes the constraints on types that can be used as input and\nresult parameters with  functions.\nFor example, one can pass a bool, a string, a pointer to an , or a\npointer to a struct which embeds  and contains supported\nfield types\n(see the <a href=\"https://go.dev/cmd/compile#hdr-WebAssembly_Directives\">documentation</a> for detail).\nThis allows Go Wasm applications to be written in a more natural and ergonomic\nway, and removes some unnecessary type conversions.</p><p>While Go 1.24 has made significant enhancements to its Wasm capabilities,\nthere are still some notable limitations.</p><p>Wasm is a single-threaded architecture with no parallelism.\nA  function can spawn new goroutines.\nBut if a function creates a background goroutine, it will not continue\nexecuting when the  function returns, until calling back into\nthe Go-based Wasm module.</p><p>While some type restrictions have been relaxed in Go 1.24, there are still\nlimitations on the types that can be used with  and\n functions.\nDue to the unfortunate mismatch between the 64-bit architecture of the client\nand the 32-bit architecture of the host, it is not possible to pass pointers in\nmemory.\nFor example, a  function cannot take a pointer to a struct that\ncontains a pointer-typed field.</p><p>The addition of the ability to build a WASI reactor and export Go functions to\nWasm in Go 1.24 represent a significant step forward for Go‚Äôs WebAssembly\ncapabilities.\nThese features empower developers to create more versatile and powerful Go-based\nWasm applications, opening up new possibilities for Go in the Wasm ecosystem.</p>","contentLength":5777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"2024 State of Rust Survey Results","url":"https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results.html","date":1739404800,"author":"The Rust Survey Team","guid":241,"unread":true,"content":"<p>The Rust Survey Team is excited to share the results of our <a href=\"https://blog.rust-lang.org/2024/12/05/annual-survey-2024-launch.html\">2024 survey on the Rust Programming language</a>, conducted between December 5, 2024 and December 23, 2024.\nAs in previous years, the 2024 State of Rust Survey was focused on gathering insights and feedback from Rust users, and all those who are interested in the future of Rust more generally.</p><p>This ninth edition of the survey surfaced new insights and learning opportunities straight from the global Rust language community, which we will summarize below. In addition to this blog post,  containing charts with aggregated results of all questions in the survey.</p><p><strong>Our sincerest thanks to every community member who took the time to express their opinions and experiences with Rust over the past year. Your participation will help us make Rust better for everyone.</strong></p><p>There's a lot of data to go through, so strap in and enjoy!</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>As shown above, in 2024, we have received fewer survey views than in the previous year. This was likely caused simply by the fact that the survey ran only for two weeks, while in the previous year it ran for almost a month. However, the completion rate has also dropped, which seems to suggest that the survey might be a bit too long. We will take this into consideration for the next edition of the survey.</p><p>The State of Rust survey not only gives us excellent insight into how many Rust users around the world are using and experiencing the language but also gives us insight into the makeup of our global community. This information gives us a sense of where the language is being used and where access gaps might exist for us to address over time. We hope that this data and our related analysis help further important discussions about how we can continue to prioritize global access and inclusivity in the Rust community.</p><p>Same as every year, we asked our respondents in which country they live in. The top 10 countries represented were, in order: United States (22%), Germany (14%), United Kingdom (6%), France (6%), China (5%), Canada (3%), Netherlands (3%), Russia (3%), Australia (2%), and Sweden (2%). We are happy to see that Rust is enjoyed by users from all around the world! You can try to find your country in the chart below:</p><p>We also asked whether respondents consider themselves members of a marginalized community. Out of those who answered, 74.5% selected no, 15.5% selected yes, and 10% preferred not to say.</p><p>We have asked the group that selected ‚Äúyes‚Äù which specific groups they identified as being a member of. The majority of those who consider themselves a member of an underrepresented or marginalized group in technology identify as lesbian, gay, bisexual, or otherwise non-heterosexual. The second most selected option was neurodivergent at 46% followed by trans at 35%.</p><p>Each year, we must acknowledge the diversity, equity, and inclusivity (DEI) related gaps in the Rust community and open source as a whole. We believe that excellent work is underway at the Rust Foundation to advance global access to Rust community gatherings and distribute grants to a diverse pool of maintainers each cycle, which you can learn more about <a href=\"https://rustfoundation.org/community\">here</a>. Even so, global inclusion and access is just one element of DEI, and the survey working group will continue to advocate for progress in this domain.</p><p>The number of respondents that self-identify as a Rust user was quite similar to last year, around 92%. This high number is not surprising, since we primarily target existing Rust developers with this survey.</p><p>Similarly as last year, around 31% of those who did not identify as Rust users cited the perception of difficulty as the primary reason for not using Rust. The most common reason for not using Rust was that the respondents simply haven‚Äôt had the chance to try it yet.</p><p>Of the former Rust users who participated in the 2024 survey, 36% cited factors outside their control as a reason why they no longer use Rust, which is a 10pp decrease from last year. This year, we also asked respondents if they would consider using Rust again if an opportunity comes up, which turns out to be true for a large fraction of the respondents (63%). That is good to hear!</p><blockquote><p>Closed answers marked with N/A were not present in the previous version(s) of the survey.</p></blockquote><p>Those not using Rust anymore told us that it is because they don't really need it (or the goals of their company changed) or because it was not the right tool for the job. A few reported being overwhelmed by the language or its ecosystem in general or that switching to or introducing Rust would have been too expensive in terms of human effort.</p><p>Of those who used Rust in 2024, 53% did so on a daily (or nearly daily) basis ‚Äî an increase of 4pp from the previous year. We can observe an upward trend in the frequency of Rust usage over the past few years, which suggests that Rust is being increasingly used at work. This is also confirmed by other answers mentioned in the Rust at Work section later below.</p><p>Rust expertise is also continually increasing amongst our respondents! 20% of respondents can write (only) simple programs in Rust (a decrease of 3pp from 2023), while 53% consider themselves productive using Rust ‚Äî up from 47% in 2023. While the survey is just one tool to measure the changes in Rust expertise overall, these numbers are heartening as they represent knowledge growth for many Rustaceans returning to the survey year over year.</p><p>Unsurprisingly, the most popular version of Rust is , either the most recent one or whichever comes with the users' Linux distribution. Almost a third of users also use the latest nightly release, due to various reasons (see below). However, it seems that the beta toolchain is not used much, which is a bit unfortunate. We would like to encourage Rust users to use the beta toolchain more (e.g. in CI environments) to help test soon-to-be stabilized versions of Rust.</p><p>People that use the nightly toolchain mostly do it to gain access to specific unstable language features. Several users have also mentioned that rustfmt works better for them on nightly or that they use the nightly compiler because of faster compilation times.</p><p>To use Rust, programmers first have to learn it, so we are always interested in finding out how do they approach that. Based on the survey results, it seems that most users learn from Rust documentation and also from <a href=\"https://doc.rust-lang.org/book/\">The Rust Programming Language</a> book, which has been a favourite learning resource of new Rustaceans for a long time. Many people also seem to learn by reading the source code of Rust crates. The fact that both the documentation and source code of tens of thousands of Rust crates is available on <a href=\"https://docs.rs\">docs.rs</a> and GitHub makes this easier.</p><p>In terms of answers belonging to the \"Other\" category, they can be clustered into three categories: people using LLM (large language model) assistants (Copilot, ChatGPT, Claude, etc.), reading the official Rust forums (Discord, <a href=\"https://users.rust-lang.org/\">URLO</a>) or being mentored while contributing to Rust projects. We would like to extend a big thank you to those making our spaces friendly and welcoming for newcomers, as it is important work and it pays off. Interestingly, a non-trivial number of people \"learned by doing\" and used rustc error messages and clippy as a guide, which is a good indicator of the quality of Rust diagnostics.</p><p>In terms of formal education, it seems that Rust has not yet penetrated university curriculums, as this is typically a very slowly moving area. Only a very small number of respondents (around 3%) have taken a university Rust course or used university learning materials.</p><p>In terms of operating systems used by Rustaceans, Linux was the most popular choice, and it seems that it is getting increasingly popular year after year. It is followed by macOS and Windows, which have a very similar share of usage.</p><blockquote><p>As you can see in the <a href=\"https://blog.rust-lang.org/images/2025-02-13-rust-survey-2024/which-os-do-you-use-wordcloud.png\">wordcloud</a>, there are also a few users that prefer Arch, btw.</p></blockquote><p>Rust programmers target a diverse set of platforms with their Rust programs. We saw a slight uptick in users targeting embedded and mobile platforms, but otherwise the distribution of platforms stayed mostly the same as last year. Since the WebAssembly target is quite diverse, we have split it into two separate categories this time. Based on the results it is clear that when using WebAssembly, it is mostly in the context of browsers (23%) rather than other use-cases (7%).</p><p>We cannot of course forget the favourite topic of many programmers: which IDE (developer environment) they use. Although Visual Studio Code still remains the most popular option, its share has dropped by 5pp this year. On the other hand, the Zed editor seems to have gained considerable traction recently. The small percentage of those who selected \"Other\" are using a wide range of different tools: from CursorAI to classics like Kate or Notepad++. Special mention to the 3 people using \"ed\", that's quite an achievement.</p><blockquote><p>You can also take a look at the linked <a href=\"https://blog.rust-lang.org/images/2025-02-13-rust-survey-2024/what-ide-do-you-use-wordcloud.png\">wordcloud</a> that summarizes open answers to this question (the \"Other\" category), to see what other editors are also popular.</p></blockquote><p>We were excited to see that more and more people use Rust at work for the majority of their coding, 38% vs 34% from last year. There is a clear upward trend in this metric over the past few years.</p><p>The usage of Rust within companies also seems to be rising, as 45% of respondents answered that their organisation makes non-trivial use of Rust, which is a 7pp increase from 2023.</p><p>Once again, the top reason employers of our survey respondents invested in Rust was the ability to build relatively correct and bug-free software. The second most popular reason was Rust‚Äôs performance characteristics. 21% of respondents that use Rust at work do so because they already know it, and it's thus their default choice, an uptick of 5pp from 2023. This seems to suggest that Rust is becoming one of the baseline languages of choice for more and more companies.</p><p>Similarly to the previous year, a large percentage of respondents (82%) report that Rust helped their company achieve its goals. In general, it seems that programmers and companies are quite happy with their usage of Rust, which is great!</p><p>In terms of technology domains, the situation is quite similar to the previous year. Rust seems to be especially popular for creating server backends, web and networking services and cloud technologies. It also seems to be gaining more traction for embedded use-cases.</p><blockquote><p>You can scroll the chart to the right to see more domains. Note that the Automotive domain was not offered as a closed answer in the 2023 survey (it was merely entered through open answers), which might explain the large jump.</p></blockquote><p>It is exciting to see the continued growth of professional Rust usage and the confidence so many users feel in its performance, control, security and safety, enjoyability, and more!</p><p>As always, one of the main goals of the State of Rust survey is to shed light on challenges, concerns, and priorities on Rustaceans‚Äô minds over the past year.</p><p>We have asked our users about aspects of Rust that limit their productivity. Perhaps unsurprisingly, slow compilation was at the top of the list, as it seems to be a perennial concern of Rust users. As always, there are efforts underway to improve the speed of the compiler, such as enabling the <a href=\"https://blog.rust-lang.org/2023/11/09/parallel-rustc.html\">parallel frontend</a> or switching to a <a href=\"https://blog.rust-lang.org/2024/05/17/enabling-rust-lld-on-linux.html\">faster linker by default</a>. We invite you to test these improvements and let us know if you encounter any issues.</p><p>Other challenges included subpar support for debugging Rust and high disk usage of Rust compiler artifacts. On the other hand, most Rust users seem to be very happy with its runtime performance, the correctness and stability of the compiler and also Rust's documentation.</p><p>In terms of specific unstable (or missing) features that Rust users want to be stabilized (or implemented), the most desired ones were async closures and if/let while chains. Well, we have good news! Async closures will be stabilized in the next version of Rust (1.85), and if/let while chains will hopefully follow <a href=\"https://github.com/rust-lang/rust/pull/132833\">soon after</a>, once Edition 2024 is released (which will also happen in Rust 1.85).</p><p>Other coveted features are generators (both sync and async) and more powerful generic const expressions. You can follow the <a href=\"https://rust-lang.github.io/rust-project-goals/2025h1/goals.html\">Rust Project Goals</a> to track the progress of these (and other) features.</p><p>In the open answers to this question, people were really helpful and tried hard to describe the most notable issues limiting their productivity. We have seen mentions of struggles with async programming (an all-time favourite), debuggability of errors (which people generally love, but they are not perfect for everyone) or Rust tooling being slow or resource intensive (rust-analyzer and rustfmt). Some users also want a better IDE story and improved interoperability with other languages.</p><p>This year, we have also included a new question about the speed of Rust's evolution. While most people seem to be content with the status quo, more than a quarter of people who responded to this question would like Rust to stabilize and/or add features more quickly, and only 7% of respondents would prefer Rust to slow down or completely stop adding new features.</p><p>Interestingly, when we asked respondents about their main worries for the future of Rust, one of the top answers remained the worry that Rust will become too complex. This seems to be in contrast with the answers to the previous question. Perhaps Rust users still seem to consider the complexity of Rust to be manageable, but they worry that one day it might become too much.</p><p>We are happy to see that the amount of respondents concerned about Rust Project governance and lacking support of the Rust Foundation has dropped by about 6pp from 2023.</p><p>Each year, the results of the State of Rust survey help reveal the areas that need improvement in many areas across the Rust Project and ecosystem, as well as the aspects that are working well for our community.</p><p>If you have any suggestions for the Rust Annual survey, please <a href=\"https://github.com/rust-lang/surveys/issues\">let us know</a>!</p><p>We are immensely grateful to those who participated in the 2024 State of Rust Survey and facilitated its creation. While there are always challenges associated with developing and maintaining a programming language, this year we were pleased to see a high level of survey participation and candid feedback that will truly help us make Rust work better for everyone.</p><p>If you‚Äôd like to dig into more details, we recommend you to browse through the full <a href=\"https://raw.githubusercontent.com/rust-lang/surveys/main/surveys/2024-annual-survey/report/annual-survey-2024-report.pdf\">survey report</a>.</p>","contentLength":14399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Giampaolo Rodola: psutil: drop Python 2.7 support","url":"https://gmpy.dev/blog/2025/psutil-drop-python-27-support","date":1739401200,"author":"","guid":277,"unread":true,"content":"<p>About dropping Python 2.7 support in psutil, 3 years ago\n<a href=\"https://github.com/giampaolo/psutil/issues/2014#issuecomment-969263432\">I stated</a>:</p><blockquote><p>Not a chance, for many years to come. [Python 2.7] currently represents 7-10%\nof total downloads, meaning around 70k / 100k downloads per day.</p></blockquote><p>Only 3 years later, and to my surprise, <strong>downloads for Python 2.7 dropped to\n0.36%</strong>! As such, as of psutil 7.0.0, I finally decided to drop support for\nPython 2.7!</p><p>These are downloads per month:</p><div><pre><code></code></pre></div><p>According to <a href=\"https://archive.is/wip/knzql\">pypistats.org</a> Python 2.7 downloads\nrepresents the 0.28% of the total, around 15.000 downloads per day.</p><p>Maintaining 2.7 support in psutil had become increasingly difficult, but still\npossible. E.g. I could still run tests by using <a href=\"https://github.com/giampaolo/psutil/blob/fbb6d9ce98f930d3d101b7df5a4f4d0f1d2b35a3/setup.py#L76-L85\">old PYPI\nbackports</a>.\nGitHub Actions could still be\n<a href=\"https://github.com/giampaolo/psutil/blob/fbb6d9ce98f930d3d101b7df5a4f4d0f1d2b35a3/.github/workflows/build.yml#L77-L112\">tweaked</a>\nto run tests and produce 2.7 wheels on Linux and macOS. Not on Windows though,\nfor which I had to use a separate service (Appveyor). Still, the amount of\nhacks in psutil source code necessary to support Python 2.7 piled up over the\nyears, and became quite big. Some disadvantages that come to mind:</p><ul><li>Having to maintain a Python compatibility layers like\n  <a href=\"https://github.com/giampaolo/psutil/blob/fbb6d9ce98f930d3d101b7df5a4f4d0f1d2b35a3/psutil/_compat.py\">psutil/_compat.py</a>.\n  This translated in extra extra code and extra imports.</li><li>The C compatibility layer to differentiate between Python 2 and 3 (<code>#if\n  PY_MAJOR_VERSION &lt;= 3</code>, etc.).</li><li>Dealing with the string vs. unicode differences, both in Python and in C.</li><li>Inability to use modern language features, especially f-strings.</li><li>Inability to freely use s, which created a difference on how CONSTANTS\n  were exposed in terms of API.</li><li>Having to install a specific version of  and other (outdated)\n  <a href=\"https://github.com/giampaolo/psutil/blob/fbb6d9ce98f930d3d101b7df5a4f4d0f1d2b35a3/setup.py#L76-L85\">deps</a>.</li><li>Relying on the third-party Appveyor CI service to run tests and produce 2.7\n  wheels.</li><li>Running 4 extra CI jobs on every commit (Linux, macOS, Windows 32-bit,\n  Windows 64-bit) making the CI slower and more subject to failures (we have\n  quite a bit of flaky tests).</li><li>The distribution of 7 wheels specific for Python 2.7. E.g. in the previous\n  release I had to upload:</li></ul><div><pre><code>psutil-6.1.1-cp27-cp27m-macosx_10_9_x86_64.whl\npsutil-6.1.1-cp27-none-win32.whl\npsutil-6.1.1-cp27-none-win_amd64.whl\npsutil-6.1.1-cp27-cp27m-manylinux2010_i686.whl\npsutil-6.1.1-cp27-cp27m-manylinux2010_x86_64.whl\npsutil-6.1.1-cp27-cp27mu-manylinux2010_i686.whl\npsutil-6.1.1-cp27-cp27mu-manylinux2010_x86_64.whl\n</code></pre></div><p>The removal was done in\n<a href=\"https://github.com/giampaolo/psutil/pull/2481\">PR-2841</a>, which removed around\n1500 lines of code (nice!). . In doing so, in the doc I\nstill made the promise that the 6.1.* serie will keep supporting Python 2.7\nand will receive  (no new features). It will be\nmaintained in a specific <a href=\"https://github.com/giampaolo/psutil/tree/python2\">python2\nbranch</a>. I explicitly kept\nthe\n<a href=\"https://github.com/giampaolo/psutil/blob/fbb6d9ce98f930d3d101b7df5a4f4d0f1d2b35a3/setup.py\">setup.py</a>\nscript compatible with Python 2.7 in terms of syntax, so that, when the tarball\nis fetched from PYPI, it will emit an informative error message on . The user trying to install psutil on Python 2.7 will see:</p><div><pre><code>$pip2installpsutil\nAsofversion.0.0psutilnolongersupportsPython.7.\nLatestversionsupportingPython.7ispsutil.1.X.\nInstallitwith:.\n</code></pre></div><p>As the informative message states, users that are still on Python 2.7 can still\nuse psutil with:</p><div><pre><code>pip2 install psutil==6.1.*\n</code></pre></div>","contentLength":2975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kay Hayen: Nuitka Release 2.6","url":"https://nuitka.net/posts/nuitka-release-26.html","date":1739401200,"author":"","guid":278,"unread":true,"content":"<li><p> Path normalization to native Windows format was required\nin more places for the  variant of .</p><p>The  function doesn‚Äôt normalize to native Win32\npaths with MSYS2, instead using forward slashes. This required manual\nnormalization in additional areas. (Fixed in 2.5.1)</p></li><li><p> Fix, give a proper error when extension modules asked to\ninclude failed to be located. instead of a proper error message.\n(Fixed in 2.5.1)</p></li><li><p>Fix, files with illegal module names (containing ) in their\nbasename were incorrectly considered as potential sub-modules for\n. These are now skipped. (Fixed in 2.5.1)</p></li><li><p> Improved stability by preventing crashes when stubgen\nencounters code it cannot handle. Exceptions from it are now ignored.\n(Fixed in 2.5.1)</p></li><li><p> Addressed a crash that occurred when encountering\nassignments to non-variables. (Fixed in 2.5.1)</p></li><li><p> Fixed a regression introduced in 2.5 release that could\nlead to segmentation faults in exception handling for generators.\n(Fixed in 2.5.2)</p></li><li><p> Corrected an issue where dictionary copies of large\nsplit directories could become corrupted. This primarily affected\ninstance dictionaries, which are created as copies until updated,\npotentially causing problems when adding new keys. (Fixed in 2.5.2)</p></li><li><p> Removed the assumption that module dictionaries\nalways contain only strings as keys. Some modules, like\n on macOS, use non-string keys. (Fixed in 2.5.2)</p></li><li><p> Ensured that the  option correctly\naffects the C compilation process. Previously, only individual\ndisables were applied. (Fixed in 2.5.2)</p></li><li><p> Fixed a crash that could occur during compilation\nwhen unary operations were used within binary operations. (Fixed in\n2.5.3)</p></li><li><p> Corrected the handling of\n<code></code>, which could lead to crashes. (Fixed\nin 2.5.4)</p></li><li><p> Resolved a segmentation fault occurring at runtime\nwhen calling  with only keyword arguments.\n(Fixed in 2.5.5)</p></li><li><p> Harmless warnings generated for x64 DLLs on arm64 with\nnewer macOS versions are now ignored. (Fixed in 2.5.5)</p></li><li><p> Addressed a crash in Nuitka‚Äôs dictionary code that\noccurred when copying dictionaries due to internal changes in Python\n3.13. (Fixed in 2.5.6)</p></li><li><p> Improved onefile mode signing by applying\n to the signature of binaries, not just\napp bundles. (Fixed in 2.5.6)</p></li><li><p> Corrected an issue where too many paths were added as\nextra directories from the Nuitka package configuration. This\nprimarily affected the  package, which currently relies\non the  import hack. (Fixed in 2.5.6)</p></li><li><p> Prevented crashes on macOS when creating onefile\nbundles with Python 2 by handling negative CRC32 values. This issue\nmay have affected other versions as well. (Fixed in 2.5.6)</p></li><li><p> Restored the functionality of code provided in\n, which was no longer being applied due to a\nregression. (Fixed in 2.5.6)</p></li><li><p> Suppressed the app bundle mode recommendation when it is\nalready in use. (Fixed in 2.5.6)</p></li><li><p> Corrected path normalization when the output directory\nargument includes ‚Äú~‚Äù.</p></li><li><p> GitHub Actions Python is now correctly identified as a\nHomebrew Python to ensure proper DLL resolution. (Fixed in 2.5.7)</p></li><li><p> Fixed a reference leak that could occur with\nvalues sent to generator objects. Asyncgen and coroutines were not\naffected. (Fixed in 2.5.7)</p></li><li><p> The  scan now correctly handles\ncases where both a package init file and competing Python files\nexist, preventing compile-time conflicts. (Fixed in 2.5.7)</p></li><li><p> Resolved an issue where handling string constants in\nmodules created for Python 3.12 could trigger assertions, and modules\ncreated with 3.12.7 or newer failed to load on older Python 3.12\nversions when compiled with Nuitka 2.5.5-2.5.6. (Fixed in 2.5.7)</p></li><li><p> Corrected the tuple code used when calling certain\nmethod descriptors. This issue primarily affected a Python 2\nassertion, which was not impacted in practice. (Fixed in 2.5.7)</p></li><li><p> Updated resource readers to accept multiple\narguments for <code></code>, and correctly handle\n and  as keyword-only arguments.</p></li><li><p> The platform encoding is no longer used to decode\n logs. Instead,  is used, as it is sufficient for\nmatching filenames across log lines and avoids potential encoding\nerrors. (Fixed in 2.5.7)</p></li><li><p> Requests to statically link libraries for \nare now ignored, as these libraries do not exist. (Fixed in 2.5.7)</p></li><li><p> Fixed a memory leak affecting the results of\nfunctions called via specs. This primarily impacted overloaded hard\nimport operations. (Fixed in 2.5.7)</p></li><li><p> When multiple distributions for a package are found,\nthe one with the most accurate file matching is now selected. This\nimproves handling of cases where an older version of a package (e.g.,\n) is overwritten with a different variant (e.g.,\n), ensuring the correct version is used for\nNuitka package configuration and reporting. (Fixed in 2.5.8)</p></li><li><p> Prevented a potential crash during onefile\ninitialization on Python 2 by passing the directory name directly\nfrom the onefile bootstrap, avoiding the use of  which\nmay not be fully loaded at that point. (Fixed in 2.5.8)</p></li><li><p> Preserved necessary  environment variables on\nWindows for packages that require loading DLLs from those locations.\nOnly  entries not pointing inside the installation prefix are\nremoved. (Fixed in 2.5.8)</p></li><li><p> Corrected the  check to function\nproperly when distribution names and package names differ. (Fixed in\n2.5.8)</p></li><li><p> Improved package name resolution for Anaconda\ndistributions by checking conda metadata when file metadata is\nunavailable through the usual methods. (Fixed in 2.5.8)</p></li><li><p> Normalized the downloaded gcc path to use native Windows\nslashes, preventing potential compilation failures. (Fixed in 2.5.9)</p></li><li><p> Restored static libpython functionality on Linux by\nadapting to a signature change in an unexposed API. (Fixed in 2.5.9)</p></li><li><p> Prevented  from being resurrected when a\nfinalizer is attached, resolving memory leaks that could occur with\n in the presence of exceptions. (Fixed in 2.5.10)</p></li><li><p> Suppressed the gcc download prompt that could appear during\n output on Windows systems without MSVC or with an\nimproperly installed gcc.</p></li><li><p>Ensured compatibility with monkey patched  or \nfunctions, which are used in some testing scenarios.</p></li><li><p> Improved the determinism of the JSON statistics\noutput by sorting keys, enabling reliable build comparisons.</p></li><li><p> Fixed a memory leak in  with finalizers,\nwhich could lead to significant memory consumption when using\n and encountering exceptions.</p></li><li><p> Optimized empty generators (an optimization result) to\navoid generating unused context code, eliminating C compilation\nwarnings.</p></li><li><p> Fixed a reference leak affecting the  value\nin . While typically , this could lead to\nobservable reference leaks in certain cases.</p></li><li><p> Improved handling of  and \nresurrection, preventing memory leaks with  and\n, and ensuring correct execution of  code in\ncoroutines.</p></li><li><p> Corrected the handling of  objects\nresurrecting during deallocation. While not explicitly demonstrated,\nthis addresses potential issues similar to those encountered with\ncoroutines, particularly for old-style coroutines created with the\n decorator.</p></li><li><p> Fixed a potential crash during runtime trace collection by\nensuring timely initialization of the output mechanism.</p></li>","contentLength":6936,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I made my own OS from scratch because I was bored","url":"https://jotalea.com.ar/misc/jotaleaos/","date":1739393712,"author":"Jotalea","guid":320,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43029686"},{"title":"Show HN: yknotify ‚Äì Notify when YubiKey needs touch on macOS","url":"https://github.com/noperator/yknotify","date":1739391899,"author":"noperator","guid":319,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43029385"},{"title":"Show HN: A no-build fullstack SSR TypeScript web framework","url":"https://jsr.io/@fullsoak/fullsoak","date":1739390092,"author":"thesephi","guid":318,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43029089"},{"title":"Oracle's Ellison Calls for Governments To Unify Data To Feed AI","url":"https://developers.slashdot.org/story/25/02/12/1857208/oracles-ellison-calls-for-governments-to-unify-data-to-feed-ai?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739386560,"author":"msmash","guid":368,"unread":true,"content":"Oracle co-founder and chairman Larry Ellison said governments should consolidate all national data for consumption by AI models, calling this step the \"missing link\" for them to take full advantage of the technology. From a report: Fragmented sets of data about a population's health, agriculture, infrastructure, procurement and borders should be unified into a single, secure database that can be accessed by AI models, Ellison said in an on-stage interview with former British Prime Minister Tony Blair at the World Government Summit in Dubai. \n\nCountries with rich population data sets, such as the UK and United Arab Emirates, could cut costs and improve public services, particularly health care, with this approach, Ellison said. Upgrading government digital infrastructure could also help identify wastage and fraud, Ellison said. IT systems used by the US government are so primitive that it makes it difficult to identify \"vast amounts of fraud,\" he added, pointing to efforts by Elon Musk's team at the Department of Government Efficiency to weed it out.","contentLength":1065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fine-tune LLMs with synthetic data for context-based Q&A using Amazon Bedrock","url":"https://aws.amazon.com/blogs/machine-learning/fine-tune-llms-with-synthetic-data-for-context-based-qa-using-amazon-bedrock/","date":1739382250,"author":"Sue Cha","guid":234,"unread":true,"content":"<p>There‚Äôs a growing demand from customers to incorporate generative AI into their businesses. Many use cases involve using pre-trained large language models (LLMs) through approaches like Retrieval Augmented Generation (RAG). However, for advanced, domain-specific tasks or those requiring specific formats, model customization techniques such as fine-tuning are sometimes necessary. <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> provides you with the ability to customize leading foundation models (FMs) such as Anthropic‚Äôs Claude 3 Haiku and Meta‚Äôs Llama 3.1.</p><p>Amazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available through an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. Amazon Bedrock offers a serverless experience, so you can get started quickly, privately customize FMs with your own data, and integrate and deploy them into your applications using AWS tools without having to manage any infrastructure.</p><p>Fine-tuning is a supervised training process where labeled prompt and response pairs are used to further train a pre-trained model to improve its performance for a particular use case. One consistent pain point of fine-tuning is the lack of data to effectively customize these models. Gathering relevant data is difficult, and maintaining its quality is another hurdle. Furthermore, fine-tuning LLMs requires substantial resource commitment. In such scenarios, synthetic data generation offers a promising solution. You can create synthetic training data using a larger language model and use it to fine-tune a smaller model, which has the benefit of a quicker turnaround time.</p><p>In this post, we explore how to use Amazon Bedrock to generate synthetic training data to fine-tune an LLM. Additionally, we provide concrete evaluation results that showcase the power of synthetic data in fine-tuning when data is scarce.</p><p>The solution comprises two main steps:</p><ol><li>Generate synthetic data using the Amazon Bedrock InvokeModel API.</li><li>Fine-tune using an Amazon Bedrock custom model.</li></ol><p>For synthetic data generation, we use a larger language model (such as <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropic‚Äôs Claude 3 Sonnet on Amazon Bedrock</a>) as the teacher model, and a smaller language model (such as Anthropic‚Äôs Claude Instant 1.2 or Claude 3 Haiku on Amazon Bedrock) as the student model for fine-tuning. We use the larger teacher model to generate new data based on its knowledge, which is then used to train the smaller student model. This concept is similar to knowledge distillation used in deep learning, except that we‚Äôre using the teacher model to generate a new dataset from its knowledge rather than directly modifying the architecture of the student model.</p><p>The following diagram illustrates the overall flow of the solution.</p><p>Finally, we share our experiment results, where we compare the performance of the model fine-tuned with synthetic data to the baseline (not fine-tuned) model and to a model fine-tuned with an equal amount of original training data.</p><p>To generate synthetic data and fine-tune models using Amazon Bedrock, you first need to create an <a href=\"https://aws.amazon.com/iam/\" target=\"_blank\" rel=\"noopener\">AWS Identity and Access Management (IAM)</a> service role with the appropriate permissions. This role is used by Amazon Bedrock to access the necessary resources on your behalf.</p><p>If you‚Äôre running this code using an <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a> notebook instance, edit the IAM role that‚Äôs attached to the notebook (for example, AmazonSageMaker-ExecutionRole-XXX) instead of creating a new role. Follow <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html\" target=\"_blank\" rel=\"noopener\">Create a service role for model customization</a> to modify the trust relationship and add the S3 bucket permission. Additionally, on the role‚Äôs tab, create the following inline policies:</p><ol><li>Policy name: bedrock-customization</li></ol><div><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel\",\n                \"bedrock:ListModelCustomizationJobs\",\n                \"bedrock:DeleteCustomModel\",\n                \"bedrock:CreateModelCustomizationJob\",\n                \"bedrock:StopModelCustomizationJob\",\n                \"bedrock:ListCustomModels\",\n                \"bedrock:GetCustomModel\",\n                \"bedrock:GetModelCustomizationJob\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre></div><ol start=\"2\"><li>Policy name: iam-pass-role</li></ol><div><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": [\n                \"${sagemaker-execution-role-arn}\"\n            ]\n        }\n    ]\n}</code></pre></div><p>The final permission policies for the SageMaker execution role should look like the following, which include AmazonSageMaker-ExecutionPolicy, AmazonSageMakerFullAccess, bedrock-customization, and iam-pass-role.</p><h2>Generate synthetic data using the Amazon Bedrock InvokeModel API</h2><p>We use the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock InvokeModel API</a> to generate synthetic data for fine-tuning. You can use the API to programmatically send an inference (text generation) request to the model of your choice. All you need is a well-crafted prompt tailored for data synthesis. We used the following sample prompt for our use case:</p><div><pre><code>PROMPT = \"\"\"\nYou are an AI assistant who is an expert in Amazon services. Your task is to understand a system that takes in a list of documents, and based on that, answers a question by providing citations for the documents that it referred the answer from.\n\nYour job is to generate three new Question/Answer pairs, emulating the tone, style, and grammar of the original data provided.\n\nHere is the original data :\nInput Documents and Question : {document}\\n\\nQuestion: {question}\nOutput Answer : {answer}\n\nStrictly return a jsonl with the keys (question, answer, topic). Every topic should be different. The answers should be in the exact same format as the original. The question and the answer should be different in content from the original data provided, and all questions should be diverse and different from each other. Do not answer in any other format. The response should be parsable as a jsonl.\n\"\"\"</code></pre></div><p>The goal of our use case was to fine-tune a model to generate a relevant and coherent answer based on a given reference document and a question. RAG is a popular technique used for such Q&amp;A tasks; however, one significant challenge with RAG is the potential for retrieving unrelated or irrelevant documents, which can lead to inaccurate responses. You can apply fine-tuning to guide the model to better focus on the relevance of the documents to the question instead of using the provided documents without context to answer the question.</p><p>Our dataset includes Q&amp;A pairs with reference documents regarding AWS services. Each sample has up to five reference documents as context, and a single-line question follows. The following table shows an example.</p><table border=\"1px\" cellpadding=\"10px\"><tbody><tr><td width=\"677\"><p>Step 1: Prepare to work with AWS CodeStar projects</p><p>In this step, you create an AWS CodeStar service role and an Amazon EC2 key pair, so that you can begin creating and working with AWS CodeStar projects. If you have used AWS CodeStar before, skip ahead to Step 2</p><p>Step 2: Create a Project in AWS CodeStar.</p><p>For this step, follow the instructions in Setting Up AWS CodeStar in the AWS CodeStar User Guide. Do not create a new AWS account, IAM user, or IAM group as part of those instructions. Use the ones you created or identified in Team Setup for AWS Cloud9. When you finish following those instructions, return to this topic.</p><p>Before you can start using AWS CodeStar, you must complete the following steps.</p><p>Step 1: Create an account</p><p>Step 2: Create the AWS CodeStar Service Role</p><p>Step 3: Configure the User‚Äôs IAM Permissions</p><p>Step 4: Create an Amazon EC2 Key Pair for AWS CodeStar Projects</p><p>Step 5: Open the AWS CodeStar Console</p><p>How Do I Get Started with AWS CodeStar?</p><p>To get started with AWS CodeStar:</p><p>Prepare to use AWS CodeStar by following the steps in Setting Up AWS CodeStar.</p><p>Experiment with AWS CodeStar by following the steps in the Getting Started with AWS CodeStar tutorial.</p><p>Share your project with other developers by following the steps in Add Team Members to an AWS CodeStar Project.</p><p>Integrate your favorite IDE by following the steps in Use an IDE with AWS CodeStar.</p><p>Step 2: Create a project in AWS CodeStar</p><p>In this step, you create a project in AWS CodeStar.</p><p>If you already have a project in AWS CodeStar you want to use, skip ahead to Step 3: Create an AWS Cloud9 Development Environment and Connect It to the Project.</p><p>For this step, follow the instructions in Create a Project in AWS CodeStar in the AWS CodeStar User Guide. In the AWS CodeStar create project wizard, when you get to the Setup tools page or Connect to your source repository page, choose Skip, and then return to this topic.</p><p>Create a Project in AWS CodeStar</p><p>You use the AWS CodeStar console to create a project. If you use a project template, it sets up the required resources for you. The template also includes sample code that you can use to start coding.</p><p>To create a project, sign in to the AWS Management Console with an IAM user that has the AWSCodeStarFullAccess policy or equivalent permissions. For more information, see Setting Up AWS CodeStar.</p><p>Note: You must complete the steps in Setting Up AWS CodeStar before you can complete the procedures in this topic.</p><p>Create a Project in AWS CodeStar (Console)</p><p>Create a Project in AWS CodeStar (AWS CLI)</p><p>Answer the following question to the best of your ability. Use and paraphrase the information in the context, if it answers the question. Otherwise, rely on your knowledge and memory.</p></td></tr><tr><td width=\"677\">How do I get started with AWS CodeStar?</td></tr><tr><td width=\"677\"><p>To get started with AWS CodeStar, follow these steps:</p><p>1.&nbsp;Prepare to work with AWS CodeStar projects:</p><p>a. Create an AWS CodeStar service role by following the instructions in ‚ÄúSetting Up AWS CodeStar‚Äù (Document 2) in the AWS CodeStar User Guide [1].</p><p>b. Create an Amazon EC2 key pair for AWS CodeStar projects, as described in the same guide [1].</p><p>2.&nbsp;Create a project in AWS CodeStar:</p><p>a. Sign in to the AWS Management Console with an IAM user that has the AWSCodeStarFullAccess policy or equivalent permissions [2].</p><p>b. In the AWS CodeStar console, select one of the available project templates, which will automatically provision all the required resources for your project [3].</p><p>c. Once your project is provisioned, you can see your running application from the Application endpoints tile [3].</p><p>3.&nbsp;Connect to the AWS CodeCommit source repository for your project:</p><p>a. Use the steps provided in the CodeStar console to connect to the AWS CodeCommit source repository for your project [3].</p><p>b. Begin coding and making changes to your project\\‚Äôs source code.</p><p>4.&nbsp;Track and manage changes in the release process:</p><p>a. Use the project dashboard to track and manage changes in the release process [3].</p><p>b. View the most recent project activity on the dashboard.</p><p>5.&nbsp;Share your project with other developers:</p><p>a. Follow the steps in ‚ÄúAdd Team Members to an AWS CodeStar Project‚Äù to share your project with other developers [3].</p><p>6.&nbsp;Integrate your favorite IDE:</p><p>a. Follow the steps in ‚ÄúUse an IDE with AWS CodeStar‚Äù to integrate your preferred Integrated Development Environment (IDE) with AWS CodeStar [3].</p><p>[1] https://docs.aws.amazon.com/codestar/latest/userguide/setting-up.html</p><p>[2] https://docs.aws.amazon.com/codestar/latest/userguide/create-project.html</p><p>[3] https://docs.aws.amazon.com/codestar/latest/userguide/getting-started.html</p></td></tr></tbody></table><p>For data synthesis, we asked the model to generate three new Q&amp;A pairs per reference document. However, you can adjust the number as needed. The crucial part is to make the model think deeply about a variety of topics. Because the purpose of generating synthetic data is to enrich the training dataset, it‚Äôs more beneficial to have the model look at different parts of the documents and create Q&amp;A pairs with different topics than the original.</p><p>The following example shows how to generate synthetic data with the Amazon Bedrock InvokeModel API. We tested the preceding prompt with Anthropic‚Äôs Claude 3 Sonnet. If you want to test a different model, retrieve the corresponding model ID from <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock model IDs</a>, and replace the modelId variable in the function.</p><div><pre><code>import boto3\nimport json\n\nbedrock = boto3.client(service_name=\"bedrock-runtime\")\n\ndef generate_synthetic_data(document, question, answer):\n    \n    values = {\n        \"document\": document,\n        \"question\": question,\n        \"answer\": answer\n    }\n    \n    body = {\n        \"messages\": [{\n            \"role\": \"user\", \"content\": PROMPT.format(**values)\n        }],\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"max_tokens\": 2048,\n        \"temperature\" : 0.5\n    }\n    \n    response = bedrock.invoke_model(\n        body=json.dumps(body),\n        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n    \n    response_body = json.loads(response.get('body').read())\n    \n    return response_body['content'][0]['text']\n</code></pre></div><p>The preceding function returns three JSONL records in strings with question, answer, and topic as keys. The following parse_llm_output function loads the strings and uses regular expressions to retrieve the generated questions and answers. Then, the create_synthetic_samples function combines those two functionalities to produce the final synthetic training samples.</p><div><pre><code>import re\nimport pd\n\ndef parse_llm_output(jsonl_string):\n    \n    question_pattern = re.compile(r'\"question\":\\s*\"([^\"]+)\"')\n    answer_pattern = re.compile(r'\"answer\":\\s*\"(.*?)\"\\s*,\\s*\"topic\"') \n    questions = question_pattern.findall(jsonl_string)\n    answers = answer_pattern.findall(jsonl_string)\n    \n    return questions, answers\n\n\ndef create_synthetic_samples(row: pd.Series) -&gt; pd.DataFrame:\n\n    jsonl_string = generate_synthetic_data(row['document'], row['question'], row['answer'])\n    questions, answers = parse_llm_output(jsonl_string)\n    \n    return pd.DataFrame({\n        \"document\": [row['document']] * len(questions),\n        \"question\": questions,\n        \"answer\": answers\n    })\n\n\ndef to_customization_format(row):\n\n    msg = {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": f\"{row['document']}\\n\\nQuestion: {row['question']}\"},\n            {\"role\": \"assistant\", \"content\": row['answer']}\n        ]\n    }\n    \n    return msg\n</code></pre></div><p>The following script combines all of the preceding functions and gives you the final training set with both original and synthetic samples. We convert the samples into the format required by the customization job using the to_customization_format function and save them as train.jsonl. Assume the input data is a CSV file with three columns: document, question, and answer.</p><div><pre><code>import pandas as pd\n\n# Load original training samples\noriginal_train = pd.read_csv(input_df_path)\n\n# Create synthetic training samples\nsynthetic_train = pd.concat(original_train.apply(create_synthetic_samples, axis=1).tolist())\n\n# Combine original and synthetic samples\nfinal_train_df = pd.concat([original_train, synthetic_train])\n\n# Convert to the format required by the customization job\nfinal_train = final_train_df.apply(to_customization_format, axis=1).tolist()\n\n# Write to JSONL file    \nwith open('train.jsonl', 'w') as file:\n    for item in final_train:\n        json.dump(item, file)\n        file.write('\\n')\n</code></pre></div><h2>Fine-tune using an Amazon Bedrock custom model</h2><p>Now that you have the synthetic data generated by the teacher model along with your original data, it‚Äôs time to train the student model. We fine-tune the student model using the Amazon Bedrock custom model functionality.</p><p>Model customization is the process of providing training data to an FM to improve its performance for specific use cases. Amazon Bedrock offers three model customization methods as of this writing:</p><ul></ul><p>You can create your own custom model using any of these methods through the Amazon Bedrock console or API. For more information on supported models and AWS Regions with various customization methods, please see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html\" target=\"_blank\" rel=\"noopener\">User guide for model customization</a>. In this section, we focus on how to fine-tune a model using the API.</p><p>To create a fine-tuning job in Amazon Bedrock, complete the following prerequisite steps:</p><ol><li>Upload the jsonl file to the training data bucket.</li><li>Make sure that you have created an IAM role, as described in the <a href=\"https://aws.amazon.com/blogs/machine-learning/fine-tune-llms-with-synthetic-data-for-context-based-qa-using-amazon-bedrock/#_Prerequisites\" target=\"_blank\" rel=\"noopener\">Prerequisites</a></li></ol><p>When these steps are complete, run the following code to submit a new fine-tuning job. In our use case, the student model was Anthropic‚Äôs Claude Instant 1.2. At the time of writing, Anthropic‚Äôs Claude 3 Haiku is generally available, and we recommend following the rest of the code using Anthropic‚Äôs Claude 3 Haiku. For the release announcement, see <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/11/fine-tuning-anthropics-claude-3-haiku-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">Fine-tuning for Anthropic‚Äôs Claude 3 Haiku in Amazon Bedrock is now generally available</a>.</p><p>If you want to try different models, you must check the model provider‚Äôs terms of service yourself. Many providers restrict using their models to train competing models. For the latest model support information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/custom-model-supported.html\" target=\"_blank\" rel=\"noopener\">Supported Regions and models for model customization</a>, and replace baseModelIdentifier accordingly. Different models have different hyperparameters. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-hp.html\" target=\"_blank\" rel=\"noopener\">Custom model hyperparameters</a>.</p><div><pre><code>import boto3\nimport json\nimport time\n\nbedrock = boto3.client(service_name='bedrock')\n    \n# Set parameters\ncustomizationType = \"FINE_TUNING\"\nbaseModelIdentifier = \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k\"\nroleArn = \"${customization-role-arn}\"\njobName = \"${customization-job-name}\"\ncustomModelName = \"${customization-model-name}\"\nhyperParameters = {\n    \"epochCount\": \"1\",\n    \"batchSize\": \"96\",\n    \"learningRateMultiplier\": \"0.5\",\n }\ntrainingDataConfig = {\"s3Uri\": \"s3://${training-bucket}/train.jsonl\"}\noutputDataConfig = {\"s3Uri\": \"s3://${output-bucket}/myOutputData\"}\n\n# Create job\nresponse_ft = bedrock.create_model_customization_job(\n    jobName=jobName, \n    customModelName=customModelName,\n    roleArn=roleArn,\n    baseModelIdentifier=baseModelIdentifier,\n    hyperParameters=hyperParameters,\n    trainingDataConfig=trainingDataConfig,\n    outputDataConfig=outputDataConfig\n)\n\njobArn = response_ft.get('jobArn')\n\n# Check job status\nwhile True:\n    status = bedrock.get_model_customization_job(jobIdentifier=jobArn).get('status')\n    if status != 'InProgress':\n        print(status)\n        break\n    else:\n        print(status)\n    time.sleep(30)\n</code></pre></div><p>When the status changes to , your fine-tuned student model is ready for use. To run an inference with this custom model, you need to purchase . A flexible option is available for custom models, which can be turned off when not in use and billed by the hour. A cost estimate is provided on the console prior to purchasing provisioned throughput.</p><p>On the Amazon Bedrock console, choose in the navigation pane. Select the model you fine-tuned and choose <strong>Purchase provisioned throughput</strong>.</p><p>The model name and type are automatically selected for you. Select  for . After you make this selection, the estimated cost is shown. If you‚Äôre okay with the pricing, choose .</p><p>When the Provisioned Throughput becomes available, retrieve the ARN of the provisioned custom model and run the inference:</p><div><pre><code>import boto3\nimport json\n\nbedrock = boto3.client(service_name=\"bedrock-runtime\")\n\ndef run_student_model(document, question):\n    \n    values = {\n        \"document\": document,\n        \"question\": question,\n    }\n    \n    body = {\n        \"messages\": [{\n            \"role\": \"user\", \"content\": PROMPT.format(**values)\n        }],\n        \"max_tokens\": 2048,\n        \"temperature\" : 0.5\n    }\n    \n    response = bedrock.invoke_model(\n        body=json.dumps(body),\n        modelId=\"${provisioned_model_arn}\",\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n    \n    response_body = json.loads(response.get('body').read())\n    \n    return response_body['content'][0]['text']\n</code></pre></div><p>In this section, we share our experiment results to provide data points on how the synthetic data generated by a teacher model can improve the performance of a student model. For evaluation methods, we used an LLM-as-a-judge approach, where a judge model compares responses from two different models and picks a better response. Additionally, we conducted a manual evaluation on a small subset to assess whether the LLM-as-a-judge and human judges have aligned preferences.</p><p>We carried out controlled experiments where we compared four different models as follows: 1,500 synthetic training samples for the 4 model were generated by Anthropic‚Äôs Claude 3 Sonnet, and we created three synthetic samples per one original reference document (3 samples * 500 original reference documents = 1,500 synthetic samples).</p><table border=\"1px\" cellpadding=\"10px\"><tbody><tr><td width=\"634\">Anthropic‚Äôs Claude Instant without any customization</td></tr><tr><td width=\"634\">Anthropic‚Äôs Claude Instant fine-tuned with 500 original training samples</td></tr><tr><td width=\"169\"></td><td width=\"634\">Anthropic‚Äôs Claude Instant fine-tuned with 2,000 original training samples</td></tr><tr><td width=\"169\"></td><td width=\"634\">Anthropic‚Äôs Claude Instant fine-tuned with 500 original training samples plus 1,500 synthetic training samples</td></tr></tbody></table><p>LLM output evaluation is an important step in developing generative AI applications, but it is expensive and takes considerable time if done manually. An alternative solution to systematically evaluate output quality in large volume is the LLM-as-a-judge approach, where an LLM is used to evaluate another LLM‚Äôs responses.</p><p>For our use case, we used Anthropic‚Äôs Claude 3 Sonnet and Meta Llama 3 70B as the judges. We asked the LLM judges to compare outputs from two different models and choose one over the other or state a tie. The following chart summarizes the judges‚Äô decisions. Each number represents the percentage of times when the respective model was selected as providing a better answer, excluding tie cases. The test set contained 343 samples.</p><p>As shown in the preceding chart, the Anthropic‚Äôs Claude 3 Sonnet judge preferred the response from the fine-tuned model with synthetic examples over the Anthropic‚Äôs Claude Instant base model (84.8% preference) and the fine-tuned model with original 500 samples (72.3% preference). However, the judge concluded that the fine-tuned model with 2,000 original examples was preferred over the fine-tuned model with synthetic examples (32.3% preference). This aligns with the expectation that when large, high-quality original data is available, it‚Äôs better to use the large training data that accurately reflects the target data distribution.</p><p>The Meta Llama judge reached a similar conclusion. As shown in the preceding chart, it preferred the response from the fine-tuned model with synthetic samples over the Anthropic‚Äôs Claude Instant base model (75.6% preference) and the fine-tuned model with original 500 examples (76.4% preference), but the fine-tuned model with 2,000 original examples was the ultimate winner.</p><p>To complement the LLM-as-a-judge result, we conducted manual evaluation with two human judges. We asked the two human evaluators to perform the same pairwise comparison task as the LLM judge, but for 20 examples. The following chart summarizes the results.</p><p>As shown in the preceding chart, the two human evaluators reached a similar conclusion, reinforcing the LLM-as-a-judge result. The fine-tuned model with synthetic examples produced outputs that were more preferable than the Anthropic‚Äôs Claude Instant base model and the fine-tuned model with the original 500 examples; however, it didn‚Äôt outperform the fine-tuned model with the 2,000 original examples.</p><p>These comparative evaluation results from both the LLM judges and human judges strongly demonstrate the power and potential of using data synthesis when training data is scarce. Moreover, by using high-quality data from the teacher model, we can effectively train the student model, which is lightweight and cost-effective for deployment in a production environment.</p><h3>Amazon Bedrock evaluations</h3><p>Running LLM-as-a-judge and human evaluation has become much easier with Amazon Bedrock. Model evaluation on Amazon Bedrock allows you to evaluate, compare, and select the best FMs for your use case. Human evaluation workflows can use your own employees or an AWS-managed team as reviewers. For more information on how to set up a human evaluation workflow, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-human.html\" target=\"_blank\" rel=\"noopener\">Creating your first model evaluation that uses human workers</a>. The latest feature, LLM-as-a-judge, is now in preview and allows you to assess multiple quality dimensions including correctness, helpfulness, and responsible AI criteria such as answer refusal and harmfulness. For step-by-step instructions, see <a href=\"https://aws.amazon.com/blogs/aws/new-rag-evaluation-and-llm-as-a-judge-capabilities-in-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">New RAG evaluation and LLM-as-a-judge capabilities in Amazon Bedrock</a>.</p><p>Make sure to delete the following resources to avoid incurring cost:</p><ul><li>Provisioned throughput for the custom model</li><li>The training_bucket and output_bucket S3 buckets</li></ul><p>In this post, we explored how to use Amazon Bedrock to generate synthetic training data using a large teacher language model and fine-tune a smaller student model with synthetic data. We provided instructions on generating synthetic data using the Amazon Bedrock InvokeModel API and fine-tuning the student model using an Amazon Bedrock custom model. Our evaluation results, based on both an LLM-as-a-judge approach and human evaluation, demonstrated the effectiveness of synthetic data in improving the student model‚Äôs performance when original training data is limited.</p><p>Although fine-tuning with a large amount of high-quality original data remains the ideal approach, our findings highlight the promising potential of synthetic data generation as a viable solution when dealing with data scarcity. This technique can enable more efficient and cost-effective model customization for domain-specific or specialized use cases.</p><p>If you‚Äôre interested in working with the AWS Generative AI Innovation Center and learning more about LLM customization and other generative AI use cases, visit <a href=\"https://aws.amazon.com/ai/generative-ai/innovation-center/\" target=\"_blank\" rel=\"noopener\">Generative AI Innovation Center</a>.</p><p> is a Deep Learning Architect at the AWS Generative AI Innovation Center, where she specializes in model customization and optimization. She has extensive hands-on experience in solving customers‚Äô business use cases by utilizing generative AI as well as traditional AI/ML solutions. Sujeong holds a M.S. degree in Data Science from New York University.</p><p><strong><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/Arijit-Intro-2.jpg\" alt=\"\" width=\"100\" height=\"116\">Arijit Ghosh Chowdhury</strong> is a Scientist with the AWS Generative AI Innovation Center, where he works on model customization and optimization. In his role, he works on applied research in fine-tuning and model evaluations to enable GenAI for various industries. He has a Master‚Äôs degree in Computer Science from the University of Illinois at Urbana Champaign, where his research focused on question answering, search and domain adaptation.</p><p> is a Senior Applied Scientist at Amazon Generative AI Innovation Center where he helps expedite the variety of use cases of AWS customers. Before joining Amazon, Sungmin was a postdoctoral research fellow at Harvard Medical School. He holds Ph.D. in Computer Science from New York University. Outside of work, Sungmin enjoys hiking, reading and cooking.</p><p> is an Applied Scientist II at the AWS Generative AI Innovation Center, where she develops generative AI solutions for AWS customers. Her expertise encompasses designing and implementing innovative AI-driven and deep learning techniques, focusing on natural language processing, computer vision, multi-modal learning, and graph learning. Yiyue holds a Ph.D. in Computer Science from the University of Notre Dame, where her research centered on advanced machine learning and deep learning methodologies. Outside of work, she enjoys sports, hiking, and traveling.</p><p> is a Machine Learning Engineer at the AWS Generative AI Innovation Center, where he works on model customization and optimization for LLMs. He also builds tools to help his team tackle various aspects of the LLM development life cycle‚Äîincluding fine-tuning, benchmarking, and load-testing‚Äîthat accelerating the adoption of diverse use cases for AWS customers. He holds an M.S. degree in Computer Science from UC Davis.</p><p> is a Senior Manager of Model Customization at the AWS Generative AI Innovation Center. Her team specializes in helping customers develop differentiating Generative AI solutions using their unique and proprietary data to achieve key business outcomes. She holds a Ph.D in Physics from the University of Iowa, with a focus on astronomical X-ray analysis and instrumentation development. Outside of work, she can be found hiking, mountain biking, and skiing around the mountains in Colorado.</p>","contentLength":28436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Achieve ~2x speed-up in LLM inference with Medusa-1 on Amazon SageMaker AI","url":"https://aws.amazon.com/blogs/machine-learning/achieve-2x-speed-up-in-llm-inference-with-medusa-1-on-amazon-sagemaker-ai/","date":1739382093,"author":"Daniel Zagyva","guid":233,"unread":true,"content":"<p><em>This blog post is co-written with Moran Beladev, Manos Stergiadis, and Ilya Gusev from Booking.com.</em></p><p><a href=\"https://aws.amazon.com/what-is/large-language-model/\" target=\"_blank\" rel=\"noopener\">Large language models</a> (LLMs) have revolutionized the field of natural language processing with their ability to understand and generate humanlike text. Trained on broad, generic datasets spanning a wide range of topics and domains, LLMs use their parametric knowledge to perform increasingly complex and versatile tasks across multiple business use cases. Furthermore, companies are increasingly investing resources in customizing LLMs through few-shot learning and fine-tuning to optimize their performance for specialized applications.</p><p>However, the impressive performance of LLMs comes at the cost of significant computational requirements, driven by their large number of parameters and autoregressive decoding process which is sequential in nature. This combination makes achieving low latency a challenge for use cases such as real-time text completion, simultaneous translation, or conversational voice assistants, where subsecond response times are critical.</p><p>Researchers developed <a href=\"https://arxiv.org/abs/2401.10774\" target=\"_blank\" rel=\"noopener\">Medusa</a>, a framework to speed up LLM inference by adding extra heads to predict multiple tokens simultaneously. This post demonstrates how to use Medusa-1, the first version of the framework, to speed up an LLM by fine-tuning it on <a href=\"https://aws.amazon.com/sagemaker-ai/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker AI</a> and confirms the speed up with deployment and a simple load test. Medusa-1 achieves an inference speedup of around two times without sacrificing model quality, with the exact improvement varying based on model size and data used. In this post, we demonstrate its effectiveness with a 1.8 times speedup observed on a sample dataset.</p><h2>Introduction to Medusa and its benefits for LLM inference speed</h2><p>LLMs generate text in a sequential manner, which involves autoregressive sampling, with each new token conditional on the previous ones. Generating K tokens necessitates K sequential executions of the model. This token-by-token processing introduces an inherent latency and computational overhead because the model needs to perform a separate forward pass for each new token in the output sequence. The following diagram from <a href=\"https://www.researchgate.net/publication/371123751_Role-Play_with_Large_Language_Models\" target=\"_blank\" rel=\"noopener\">Role-Play with Large Language Models</a> illustrates this flow.</p><p>Speculative decoding tackles this challenge by using a smaller, faster draft model to generate multiple potential token continuations in parallel, which are then verified by a larger, more accurate target model. This parallelization speeds up text generation while maintaining the quality of the target model because the verification task is faster than autoregressive token generation. For a detailed explanation of the concept, refer to the paper <a href=\"https://arxiv.org/abs/2302.01318\" target=\"_blank\" rel=\"noopener\">Accelerating Large Language Model Decoding with Speculative Sampling</a>. The speculative decoding technique can be implemented using the <a href=\"https://aws.amazon.com/blogs/machine-learning/achieve-up-to-2x-higher-throughput-while-reducing-costs-by-50-for-generative-ai-inference-on-amazon-sagemaker-with-the-new-inference-optimization-toolkit-part-1/\" target=\"_blank\" rel=\"noopener\">inference optimization toolkit</a> on <a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Jumpstart</a>.</p><p>The paper <a href=\"https://arxiv.org/abs/2401.10774\" target=\"_blank\" rel=\"noopener\">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a> introduced Medusa as an alternative to speculative decoding. Instead of adding a separate draft model, it adds extra decoding heads to the LLM that generate candidate continuations simultaneously. These candidates are then evaluated in parallel using a tree-based attention mechanism. This parallel processing reduces the number of sequential steps needed, leading to faster inference times. The main advantage of Medusa over speculative decoding is that it eliminates the need to acquire and maintain a separate draft model while achieving higher speedups. For example, when tested on the <a href=\"https://arxiv.org/abs/2402.14762\" target=\"_blank\" rel=\"noopener\">MT-Bench dataset</a>, the paper reports that Medusa-2 (the second version of Medusa) speeds up inference time by 2.8 times. This outperforms speculative decoding, which only manages to speed up inference time by 1.5 times on the same dataset.</p><p>The Medusa framework currently supports Llama and Mistral models. Although it offers significant speed improvements, it does come with a memory trade-off (similar to speculative decoding). For instance, adding five Medusa heads to the 7-billion-parameter Mistral model increases the total parameter count by 750 million (150 million per head), which means these additional parameters must be stored in GPU memory, leading to a higher memory requirement. However, in most cases, this increase doesn‚Äôt necessitate switching to a higher GPU memory instance. For example, you can still use an  instance with 24 GB of GPU memory to host your 7-billion-parameter Llama or Mistral model with extra Medusa heads.</p><p>Training Medusa heads requires additional development time and computational resources, which should be factored into project planning and resource allocation. Another important limitation to mention is that the current framework, when deployed on an Amazon SageMaker AI endpoint, only supports a batch size of one‚Äîa configuration typically used for low-latency applications.</p><p>The following diagram from the original Medusa paper authors‚Äô <a href=\"https://github.com/FasterDecoding/Medusa\" target=\"_blank\" rel=\"noopener\">FasterDecoding repository</a> gives a visual Medusa framework overview.</p><p>There are two main variants of Medusa:</p><ol><li> ‚Äì Requires a two-stage approach where you first fine-tune your LLM and then add Medusa heads and train them on top of your frozen fine-tuned LLM</li><li> ‚Äì Introduced later as an improvement, fine-tunes both the additional heads and the backbone LLM parameters together, enabling potentially even further latency speedups</li></ol><p>The Medusa paper reports that across models of varying sizes, you can achieve inference speedups of around two times for Medusa-1 and around three times for Medusa-2. With Medusa-1, the predictions are identical to those of the originally fine-tuned LLM. In contrast, with Medusa-2, we might observe slightly different results compared to simple fine-tuning of the LLM because both the heads and the backbone LLM parameters are updated together. In this post, we focus on Medusa-1.</p><p>We cover the following steps in our solution:</p><ul><li>Load and prepare the dataset</li><li>Fine-tune an LLM using a SageMaker AI training job</li><li>Train Medusa heads on top of a frozen fine-tuned LLM using a SageMaker AI training job</li><li>Deploy the fine-tuned LLM with Medusa heads on a SageMaker AI endpoint</li><li>Demonstrate LLM inference speedup</li></ul><p>By following this solution, you can accelerate LLM inference in your applications, leading to faster response times and improved user experience.</p><p>To build the solution yourself, there are the following prerequisites:</p><h3>Load and prepare the dataset</h3><p>Now that you have cloned the <a href=\"https://github.com/aws-samples/speedup-llm-inference-with-medusa-framework-on-amazon-sagemaker\">GitHub repository</a> and opened the  notebook, you will load and prepare the dataset in the notebook. We encourage you to read this post while running the code in the notebook. For this post, we use a dataset called <a href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" target=\"_blank\" rel=\"noopener\">sql-create-context</a>, which contains samples of natural language instructions, schema definitions and the corresponding SQL query. It contains 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL queries answering the question using the CREATE statement as context. For demonstration purposes, we select 3,000 samples and split them into train, validation, and test sets.</p><p>You need to run the ‚ÄúLoad and prepare the dataset‚Äù section of the  to prepare the dataset for fine-tuning. We also included a data exploration script to analyze the length of input and output tokens. After data exploration, we prepare the train, validation, and test sets and upload them to <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3).</p><h3>Fine-tune an LLM using SageMaker AI training job</h3><p>We use the <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\" target=\"_blank\" rel=\"noopener\">Zephyr 7B Œ≤</a> model as our backbone LLM. Zephyr is a series of language models trained to act as helpful assistants, and Zephyr 7B Œ≤ is a fine-tuned version of <a href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" target=\"_blank\" rel=\"noopener\">Mistral-7B-v0.1</a>, trained on a mix of publicly available and synthetic datasets using <a href=\"https://arxiv.org/abs/2305.18290\" target=\"_blank\" rel=\"noopener\">Direct Preference Optimization</a>.</p><p>To launch a SageMaker AI training job, we need to use the PyTorch or Hugging Face estimator. SageMaker AI starts and manages all the necessary <a href=\"https://aws.amazon.com/ec2/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instances for us, supplies the appropriate containers, downloads data from our S3 bucket to the container and uploads and runs the specified training script, in our case . We select the hyperparameters based on the <a href=\"https://arxiv.org/pdf/2305.14314\" target=\"_blank\" rel=\"noopener\">QLoRA paper</a>, but we encourage you to experiment with your own combinations. To expedite the execution of this code, we set the number of epochs to 1. However, for better results, it‚Äôs generally recommended to set the number of epochs to at least 2 or 3.</p><div><pre><code>from sagemaker.pytorch.estimator import PyTorch\nfrom sagemaker.debugger import TensorBoardOutputConfig\nimport time\nimport os\n\ndef get_current_time():\n    return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n\ndef create_estimator(hyperparameters_dict, job_name, role, sess, train_scipt_path):\n    metric=[\n        {\"Name\": \"loss\", \"Regex\": r\"'loss':\\s*([0-9.]+)\"},\n        {\"Name\": \"epoch\", \"Regex\": r\"'epoch':\\s*([0-9.]+)\"},\n    ]\n\n    tensorboard_s3_output_path = os.path.join(\n       \"s3://\", sess.default_bucket(), job_name, 'tensorboard'\n    )\n    print(\"Tensorboard output path:\", tensorboard_s3_output_path)\n\n    tensorboard_output_config = TensorBoardOutputConfig(\n        s3_output_path=tensorboard_s3_output_path,\n        container_local_output_path=hyperparameters_dict['logging_dir']\n    )\n    estimator = PyTorch(\n        sagemaker_session    = sess,\n        entry_point          = train_scipt_path,    # train script\n        source_dir           = 'train',      # directory which includes all the files needed for training\n        instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job, \"local_gpu\" for local mode\n        metric_definitions   = metric,\n        instance_count       = 1,                 # the number of instances used for training\n        role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n        volume_size          = 300,               # the size of the EBS volume in GB\n        framework_version      = '2.1.0',             # the pytorch_version version used in the training job\n        py_version           = 'py310',           # the python version used in the training job\n        hyperparameters      =  hyperparameters_dict,  # the hyperparameters passed to the training job\n        disable_output_compression = True,        # not compress output to save training time and cost\n        tensorboard_output_config = tensorboard_output_config\n    )\n    return estimator\n    \n# hyperparameters, which are passed into the training job\nsft_hyperparameters = {\n  ### SCRIPT PARAMETERS ###\n  'train_dataset_path': '/opt/ml/input/data/train/train_dataset.json', # path where sagemaker will save training dataset\n  'eval_dataset_path': '/opt/ml/input/data/eval/eval_dataset.json', # path where sagemaker will save evaluation dataset\n  'model_id': model_id,\n  'max_seq_len': 256,                               # max sequence length for model and packing of the dataset\n  'use_qlora': True,                                 # use QLoRA model\n  ### TRAINING PARAMETERS ###\n  'num_train_epochs': 1,                             # number of training epochs\n  'per_device_train_batch_size': 1,                  # batch size per device during training\n  'gradient_accumulation_steps': 16,                  # number of steps before performing a backward/update pass\n  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n  'optim': \"adamw_8bit\",                             # use fused adamw 8bit optimizer\n  'logging_steps': 15,                               # log every 10 steps\n  'save_strategy': \"steps\",                          # save checkpoint every epoch\n  'save_steps': 15,\n  'save_total_limit': 2,\n  'eval_strategy': \"steps\",\n  'eval_steps': 15,\n  'learning_rate': 1e-4,                             # learning rate, based on QLoRA paper\n  'bf16': True,                                      # use bfloat16 precision\n  'max_grad_norm': 10,                              # max gradient norm based on QLoRA paper\n  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n  'output_dir': '/opt/ml/checkpoints/',              # Temporary output directory for model checkpoints\n  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n  'logging_dir': \"/opt/ml/output/tensorboard\"        # tensorboard logging directory\n}\n \nsft_job_name = f\"sft-qlora-text-to-sql-{get_current_time()}\"\ndata = {\n    'train': train_dataset_path,\n    'eval': eval_dataset_path\n}\n\nsft_estimator = create_estimator(sft_hyperparameters, sft_job_name, role, sess, \"fine_tune_llm.py\")\n\nsft_estimator.fit(job_name=sft_job_name, inputs=data, wait=False)</code></pre></div><p>When our training job has completed successfully after approximately 1 hour, we can use the fine-tuned model artifact for the next step, training the Medusa heads on top of it. To visualize the training metrics in Tensorboard, you can follow the guidance in this documentation: <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-htb-access-tb-data.html\" target=\"_blank\" rel=\"noopener\">Load and visualize output tensors using the TensorBoard application</a></p><h3>Train Medusa heads on top of frozen fine-tuned LLM using a SageMaker AI training job</h3><p>For training Medusa heads, we can reuse the functions previously mentioned to launch the training job. We selected hyperparameters based on a combination of what the Medusa paper reported and what we found to be best performing after a few experiments. We set the number of Medusa heads to 5 and used the 8-bit AdamW optimizer, as recommended by the paper. For simplicity, we maintained a constant learning rate of 1e-4 with a constant scheduler, similar to the previous fine-tuning step. Although the paper recommends an increased learning rate and a cosine scheduler, we found that our chosen combination of hyperparameters performed well on this dataset. However, we encourage you to experiment with your own hyperparameter settings to potentially achieve even better results.</p><div><pre><code># hyperparameters, which are passed into the training job\nmedusa_hyperparameters = {\n  ### SCRIPT PARAMETERS ###\n  'train_dataset_path': '/opt/ml/input/data/train/train_dataset.json', # path where sagemaker will save training dataset\n  'eval_dataset_path': '/opt/ml/input/data/eval/eval_dataset.json', # path where sagemaker will save evaluation dataset\n  'model_path': '/opt/ml/input/data/fine-tuned-model/',\n  'max_seq_len': 256,                               # max sequence length for model and packing of the dataset\n  'medusa_num_heads': 5,\n  ### TRAINING PARAMETERS ###\n  'num_train_epochs': 3,                             # number of training epochs\n  'per_device_train_batch_size': 1,                  # batch size per device during training\n  'gradient_accumulation_steps': 16,                  # number of steps before performing a backward/update pass\n  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n  'optim': \"adamw_8bit\",                             # use fused adamw 8bit optimizer\n  'logging_steps': 15,                               # log every 10 steps\n  'save_strategy': \"steps\",                          # save checkpoint every epoch\n  'save_steps': 15,\n  'save_total_limit':2,\n  'eval_strategy': \"steps\",\n  'eval_steps': 15,\n  'learning_rate': 1e-4,                             # learning rate\n  'bf16': True,                                      # use bfloat16 precision\n  'max_grad_norm': 10,                              # max gradient norm based on QLoRA paper\n  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n  'output_dir': '/opt/ml/checkpoints/',              # Temporary output directory for model checkpoints\n  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n  'logging_dir': \"/opt/ml/output/tensorboard\"        # tensorboard logging directory\n}\n\nmedusa_train_job_name = f\"medusa-text-to-sql-{get_current_time()}\"\ndata = {\n    'train': train_dataset_path,\n    'eval': eval_dataset_path,\n    'fine-tuned-model': fine_tuned_model_path\n}\n\nmedusa_estimator = create_estimator(medusa_hyperparameters, medusa_train_job_name, role, sess, \"train_medusa_heads.py\")\n\nmedusa_estimator.fit(job_name=medusa_train_job_name, inputs=data, wait=False)</code></pre></div><p>We found that after 3 epochs, the evaluation loss of Medusa heads was converging, which can be observed in the TensorBoard graph in the following image.</p><p>Besides the hyperparameters, the main difference is that we pass  as the training entrypoint, where we first add Medusa heads, then freeze the fine-tuned LLM, and we create custom MedusaSFTTrainer class, which is a subclass of the transformers SFTTrainer.</p><div><pre><code># Add medusa heads and freeze base model\nadd_medusa_heads(\n    model,\n    medusa_num_heads=script_args.medusa_num_heads,\n)\nfreeze_layers(model)\nmodel.config.torch_dtype = torch_dtype\nmodel.config.use_cache = False\n\nlogger.info(\"Finished loading model and medusa heads\")\n\ntokenizer = AutoTokenizer.from_pretrained(script_args.model_path, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n################\n# Training\n################\ntrainer = MedusaSFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    max_seq_length=script_args.max_seq_length,\n    tokenizer=tokenizer,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False,  # No need to add additional separator token\n    },\n    medusa_num_heads=script_args.medusa_num_heads,\n    medusa_heads_coefficient=script_args.medusa_heads_coefficient,\n    medusa_decay_coefficient=script_args.medusa_decay_coefficient,\n    medusa_scheduler=script_args.medusa_scheduler,\n    train_only_medusa_heads=script_args.train_only_medusa_heads,\n    medusa_lr_multiplier=script_args.medusa_lr_multiplier\n)\ntrainer.train()</code></pre></div><p>In the  function, we add the residual blocks of the Medusa heads, and also override the forward pass for our model to make sure not to train the frozen backbone LLM:</p><div><pre><code>def add_medusa_heads(\n    model,\n    medusa_num_heads,\n):\n    \"\"\"\n    Args:\n        model (nn.Module): The base language model to be used.\n        medusa_num_heads (int, optional): Number of additional tokens to predict\n    \"\"\"\n    hidden_size = model.lm_head.weight.shape[-1]\n    vocab_size = model.lm_head.weight.shape[0]\n    model.config.medusa_num_layers = 1\n    model.config.medusa_num_heads = medusa_num_heads\n    model.medusa_num_heads = medusa_num_heads\n    # Create a list of Medusa heads\n    model.medusa_heads = nn.ModuleList(\n        [\n            nn.Sequential(\n                ResBlock(hidden_size),\n                nn.Linear(hidden_size, vocab_size, bias=False),\n            )\n            for _ in range(medusa_num_heads)\n        ]\n    )\n\n    # Ensure medusa_head's dtype and device align with the base_model\n    model.medusa_heads.to(model.dtype).to(model.device)\n    logger.info(f\"Loading medusa heads in {str(model.dtype)} to device {model.device}\")\n\n    for i in range(medusa_num_heads):\n        # Initialize the weights of each medusa_head using the base model's weights\n        model.medusa_heads[i][-1].weight.data[:] = model.lm_head.weight.data[:]\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        train_only_medusa_heads: bool = False,\n    ):\n        \"\"\"Forward pass of the MedusaModel.\n        Returns:\n            torch.Tensor: A tensor containing predictions from all Medusa heads.\n            (Optional) Original predictions from the base model's LM head.\n        \"\"\"\n        maybe_grad = torch.no_grad() if train_only_medusa_heads else nullcontext()\n        with maybe_grad:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n            hidden_states = outputs[0]\n            medusa_logits = [self.lm_head(hidden_states)]\n        for i in range(self.medusa_num_heads):\n            medusa_logits.append(self.medusa_heads[i](hidden_states))\n        return torch.stack(medusa_logits, dim=0)\n\n    model.forward = types.MethodType(forward, model)</code></pre></div><p>After the model training is finished (which takes 1 hour), we prepare the model artefacts for deployment and upload it to Amazon S3. Your final model artifact contains both the original fine-tuned model from the previous step under the  prefix and the trained Medusa heads in a file named .</p><h3>Deploy the fine-tuned LLM with Medusa heads on a SageMaker AI endpoint</h3><p>First, we create a SageMaker AI HuggingFaceModel object and then deploy the model to an endpoint with the following function:</p><div><pre><code>import json\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n\n\ndef deploy_model(endpoint_name, instance_type, model_s3_path=None, hf_model_id=None):\n    llm_image = get_huggingface_llm_image_uri(\n      \"huggingface\",\n      version=\"2.2.0\",\n      session=sess,\n    )\n\n    print(f\"llm image uri: {llm_image}\")\n\n    model_data = None\n    if model_s3_path:\n        model_data = {'S3DataSource': {'S3Uri': model_s3_path, 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n        hf_model_id = \"/opt/ml/model\"\n    else:\n        assert hf_model_id, \"You need to provide either pretrained HF model id, or S3 model data to deploy\"\n    config = {\n      'HF_MODEL_ID': hf_model_id,  # path to where sagemaker stores the model\n      'SM_NUM_GPUS': json.dumps(1),  # Number of GPU used per replica\n      'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n      'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n    }\n\n    llm_model = HuggingFaceModel(\n      name=endpoint_name,\n      role=role,\n      image_uri=llm_image,\n      model_data=model_data,\n      env=config\n    )\n\n    deployed_llm = llm_model.deploy(\n      endpoint_name=endpoint_name,\n      initial_instance_count=1,\n      instance_type=instance_type,\n      container_startup_health_check_timeout=300,\n    )\n    return deployed_llm</code></pre></div><p>We deploy three LLMs on three SageMaker AI endpoints:</p><ol><li>Base LLM which isn‚Äôt fine-tuned</li><li>The LLM that we fine-tuned</li><li>The fine-tuned LLM that also has trained Medusa heads</li></ol><p>You can deploy the three models in parallel by using a function that we included in the notebook, or you can deploy the models one by one by running the code below:</p><div><pre><code>base_deployed_llm = deploy_model( f\"base-{get_current_time()}\", instance_type=\"ml.g5.4xlarge\", model_s3_path=None, hf_model_id=model_id )\nsft_deployed_llm = deploy_model( f\"sft-{get_current_time()}\", instance_type=\"ml.g5.4xlarge\", model_s3_path=fine_tuned_model_path )\nmedusa_deployed_llm = deploy_model( f\"medusa-{get_current_time()}\", instance_type=\"ml.g5.4xlarge\", model_s3_path=medusa_trained_model_path )\n</code></pre></div><p>After the status for each endpoint becomes , which should take around 15 minutes, we can invoke them for inference. We send the following input:</p><div><pre><code>‚ÄúYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA. SCHEMA: CREATE TABLE table_name_32 (time VARCHAR, heat VARCHAR, name VARCHAR)\nWhat was Liu Limin's time in heat 4?‚Äú</code></pre></div><p>We can observe the following responses:</p><ol><li>The base LLM response contains extra words that aren‚Äôt needed:</li></ol><div><pre><code>‚ÄúTo retrieve the time of Liu Limin in heat 4 based on the provided SCHEMA, we need to write a SQL query. Since the table name is not provided, let 's assume it's \"my_table\".  Assuming the table has a primary key (which is not mentioned in the SCHEMA), we can join the table with itself to compare the heat and name of Liu Limin in heat 4 with all the records in the table.  Here's the SQL query:  ``sql SELECT t1.time FROM my_table t1 JOIN my_table t2 ON t1.name = t2.name AND t2.heat = 4 WHERE t1.name = 'Liu Limin' AND t1.heat &lt;&gt; 4; `  Explanation:  1. We're selecting the time` column from the first table (t1) in the FROM clause. 2. We're joining the table with itself (my_table t1 JOIN my_table t2) to compare the name and heat of Liu Limin in heat 4 with all the records in the table. 3. We're filtering the results using the WHERE clause. We're selecting only the records where the name is 'Liu Limin' and the heat is not equal to 4 (i.e., not heat 4). This is to ensure that we're selecting the time of Liu Limin in heat 3.  Note: This query assumes that the table has a unique primary key. If the table doesn't have a primary key, you may need to add additional conditions to the JOIN and WHERE clauses to ensure that we're selecting the correct records.‚Äú</code></pre></div><ol start=\"2\"><li>The fine-tuned LLM response is improved significantly, and contains only the required output:</li></ol><div><pre><code>'SELECT time FROM table_name_32 WHERE heat = 4 AND name = \"liu limin\"'</code></pre></div><ol start=\"3\"><li>The fine-tuned LLM with trained Medusa heads provides the exact same response as the fine-tuned model, demonstrating that Medusa-1, by design, maintains the output (quality) of the original model:</li></ol><div><pre><code>'SELECT time FROM table_name_32 WHERE heat = 4 AND name = \"liu limin\"'</code></pre></div><h3>Demonstrate LLM inference speedup</h3><p>To measure the inference speed improvements, we compare the response times of the deployed fine-tuned LLM and the fine-tuned LLM with Medusa heads on 450 test observations with the following code:</p><div><pre><code>import time\nimport numpy as np\nfrom tqdm import tqdm\n\ndef request(sample, deployed_llm):\n    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n    outputs = deployed_llm.predict({\n      \"inputs\": prompt,\n      \"parameters\": {\n        \"max_new_tokens\": 512,\n        \"do_sample\": False,\n        \"return_full_text\": False,\n      }\n    })\n    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n\ndef predict(deployed_llm, test_dataset):\n    predicted_answers = []\n    latencies = []\n\n    for sample in tqdm(test_dataset):\n        start_time = time.time()\n        predicted_answer = request(sample[\"messages\"][:2], deployed_llm)\n        end_time = time.time()\n\n        latency = end_time - start_time\n        latencies.append(latency)\n        predicted_answers.append(predicted_answer)\n\n    # Calculate p90 and average latencies\n    p90_latency = np.percentile(latencies, 90)\n    avg_latency = np.mean(latencies)\n\n    print(f\"P90 Latency: {p90_latency:.2f} seconds\")\n    print(f\"Average Latency: {avg_latency:.2f} seconds\")\n\n    return predicted_answers</code></pre></div><p>First, we run predictions using the fine-tuned LLM:</p><div><pre><code>sft_predictions = predict(sft_deployed_llm, test_dataset)\nP90 Latency: 1.28 seconds\nAverage Latency: 0.95 seconds</code></pre></div><p>Then, we run predictions using the fine-tuned LLM with Medusa heads:</p><div><pre><code>medusa_predictions = predict(medusa_deployed_llm, test_dataset)\nP90 Latency: 0.80 seconds\nAverage Latency: 0.53 seconds</code></pre></div><p>The prediction runs should take around 8 and 4 minutes respectively. We can observe that the average latency decreased from 950 to 530 milliseconds, which is an improvement of 1.8 times. You can achieve even higher improvements if your dataset contains longer inputs and outputs. In our dataset, we only had an average of 18 input tokens and 30 output tokens.</p><p>We want to once again highlight that, with this technique, the output quality is fully maintained, and all the prediction outputs are the same. The model responses for the test set of 450 observations are the same for both with Medusa heads and without Medusa heads:</p><div><pre><code>match_percentage = sum(a[\"content\"] == b[\"content\"] for a, b in zip(sft_predictions, medusa_predictions)) / len(sft_predictions) * 100\nprint(f\"Predictions with the fine-tuned model with medusa heads are the same as without medusa heads: {match_percentage:.2f}% of test set \")\n\nPredictions with fine-tuned model with medusa heads are the same as without medusa heads: 100.00% of test set </code></pre></div><p>You might notice in your run that a few observations aren‚Äôt exactly matching, and you might get a 99% match due to small errors in floating point operations caused by optimizations on GPUs.</p><p>At the end of this experiment, don‚Äôt forget to delete the SageMaker AI endpoints you created:</p><div><pre><code>base_deployed_llm.delete_model()\nbase_deployed_llm.delete_endpoint()\nsft_deployed_llm.delete_model()\nsft_deployed_llm.delete_endpoint()\nmedusa_deployed_llm.delete_model()\nmedusa_deployed_llm.delete_endpoint()</code></pre></div><p>In this post, we demonstrated how to fine-tune and deploy an LLM with Medusa heads using the Medusa-1 technique on Amazon SageMaker AI to accelerate LLM inference. By using this framework and SageMaker AI scalable infrastructure, we showed how to achieve up to twofold speedups in LLM inference while maintaining model quality. This solution is particularly beneficial for applications requiring low-latency text generation, such as customer service chat assistants, content creation, and recommendation systems.</p><p>As a next step, you can explore fine-tuning your own LLM with Medusa heads on your own dataset and benchmark the results for your specific use case, using the provided <a href=\"https://github.com/aws-samples/speedup-llm-inference-with-medusa-framework-on-amazon-sagemaker\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_dzagyva.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior ML Engineer at AWS Professional Services. He specializes in developing scalable, production-grade machine learning solutions for AWS customers. His experience extends across different areas, including natural language processing, generative AI and machine learning operations.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_adokic.jpg\" alt=\"\" width=\"100\" height=\"129\"> is a Senior Data Scientist at AWS Professional Services. She enjoys supporting customers to build innovative AI/ML solutions on AWS and she is excited about business transformations through the power of data.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_mbeladev.png\" alt=\"\" width=\"100\" height=\"100\"> is a Senior ML Manager at Booking.com. She is leading the content intelligence track which is focused on building, training and deploying content models (computer vision, NLP and generative AI) using the most advanced technologies and models. Moran is also a PhD candidate, researching applying NLP models on social graphs.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_mstergiadis.jpg\" alt=\"\" width=\"100\" height=\"100\"> is a Senior ML Scientist at Booking.com. He specializes in generative NLP and has experience researching, implementing and deploying large deep learning models at scale.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_igusev.jpg\" alt=\"\" width=\"100\" height=\"158\"> is a Senior Machine Learning Engineer at Booking.com. He leads the development of the several LLM systems inside Booking.com. His work focuses on building production ML systems that help millions of travelers plan their trips effectively.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_lvandermaas-727x1024-1.jpg\" alt=\"\" width=\"100\" height=\"141\"> is a Machine Learning Engineer at AWS Professional Services. He works closely with customers building their machine learning solutions on AWS, specializes in natural language processing, experimentation and responsible AI, and is passionate about using machine learning to drive meaningful change in the world.</p>","contentLength":31295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-as-a-judge on Amazon Bedrock Model Evaluation","url":"https://aws.amazon.com/blogs/machine-learning/llm-as-a-judge-on-amazon-bedrock-model-evaluation/","date":1739381817,"author":"Adewale Akinfaderin","guid":232,"unread":true,"content":"<p>The evaluation of large language model (LLM) performance, particularly in response to a variety of prompts, is crucial for organizations aiming to harness the full potential of this rapidly evolving technology. The introduction of an  framework represents a significant step forward in simplifying and streamlining the model evaluation process. This approach allows organizations to assess their AI models‚Äô effectiveness using pre-defined metrics, making sure that the technology aligns with their specific needs and objectives. By adopting this method, companies can more accurately gauge the performance of their AI systems, making informed decisions about model selection, optimization, and deployment. This not only enhances the reliability and efficiency of AI applications, but also contributes to a more strategic and informed approach to technology adoption within the organization.</p><p><a href=\"https://aws.amazon.com/bedrock\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a>, a fully managed service offering high-performing foundation models from leading AI companies through a single API, has recently introduced two significant evaluation capabilities: <a href=\"https://aws.amazon.com/blogs/aws/new-rag-evaluation-and-llm-as-a-judge-capabilities-in-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">LLM-as-a-judge under Amazon Bedrock Model Evaluation and RAG evaluation for Amazon Bedrock Knowledge Bases</a>. Both features use the LLM-as-a-judge technique behind the scenes but evaluate different things. This blog post explores LLM-as-a-judge on Amazon Bedrock Model Evaluation, providing comprehensive guidance on feature setup, evaluating job initiation through both the console and Python SDK and APIs, and demonstrating how this innovative evaluation feature can enhance generative AI applications across multiple metric categories including quality, user experience, instruction following, and safety.</p><p>Before we explore the technical aspects and implementation details, let‚Äôs examine the key features that make LLM-as-a-judge on Amazon Bedrock Model Evaluation particularly powerful and distinguish it from traditional evaluation methods. Understanding these core capabilities will help illuminate why this feature represents a significant advancement in AI model evaluation.</p><h2>Key features of LLM-as-a-judge</h2><ol><li><strong>Automated intelligent evaluation</strong>: LLM-as-a-judge uses pre-trained models to evaluate responses automatically, providing human-like evaluation quality with up to 98% cost savings. The system dramatically reduces evaluation time from weeks to hours while maintaining consistent evaluation standards across large datasets.</li><li><strong>Comprehensive metric categories</strong>: The evaluation system covers four key metric areas: quality assessment (correctness, completeness, faithfulness), user experience (helpfulness, coherence, relevance), instruction compliance (following instructions, professional style), and safety monitoring (harmfulness, stereotyping, refusal handling).</li><li>: The feature integrates directly with Amazon Bedrock and remains compatible with existing Amazon Bedrock Model Evaluation features. Users can access the functionality through the AWS Management Console for Amazon Bedrock and quickly integrate their custom datasets for evaluation purposes.</li><li>: The system supports the evaluation of models hosted on Amazon Bedrock, custom fine-tuned models, and imported models. Users can seamlessly connect their evaluation datasets through <a href=\"https://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service (Amazon S3)</a> buckets, making the evaluation process streamlined and efficient.</li><li>: Amazon Bedrock provides pre-selected, high-quality evaluation models with optimized prompt engineering for accurate assessments. Users don‚Äôt need to bring external judge models, because the Amazon Bedrock team maintains and updates a selection of judge models and associated evaluation judge prompts.</li><li>: The feature enables organizations to perform comprehensive model evaluations at scale without the traditional costs and time investments associated with human evaluation. The automated process maintains high-quality assessments while significantly reducing operational overhead.</li></ol><p>These features create a powerful evaluation framework that helps organizations optimize their AI model performance while maintaining high standards of quality and safety, all within their secure AWS environment.</p><p>Now that you understand the key features of LLM-as-a-judge, let‚Äôs examine how to implement and use this capability within Amazon Bedrock Model Evaluation. This section provides a comprehensive overview of the architecture and walks through each component, demonstrating how they work together to deliver accurate and efficient model evaluations.</p><p>LLM-as-a-judge on Amazon Bedrock Model Evaluation provides a comprehensive, end-to-end solution for assessing and optimizing AI model performance. This automated process uses the power of LLMs to evaluate responses across multiple metric categories, offering insights that can significantly improve your AI applications. Let‚Äôs walk through the key components of this solution as shown in the following diagram:</p><p>LLM-as-a-judge on Amazon Bedrock Model Evaluation follows a streamlined workflow that enables systematic model evaluation. Here‚Äôs how each component works together in the evaluation process:</p><ul><li>: The process begins with a prepared dataset containing prompts that will be used to test the model‚Äôs performance. The evaluation can be conducted with or without ground truth responses‚Äîwhile including ground truth provides additional comparison points, it‚Äôs entirely optional and not required for successful evaluation.</li><li>: The prompt dataset is converted into JSONL format, which is specifically structured for LLM-as-a-judge evaluation jobs. This format promotes proper processing of evaluation data.</li><li>: The prepared JSONL file is uploaded to an S3 bucket, serving as the secure storage location for the evaluation data.</li><li>: The Amazon Bedrock LLM-as-a-judge model evaluation job processes the stored data, running comprehensive assessments across the selected metric categories (including quality, user experience, instruction following, and safety).</li><li><strong>Automated report generation</strong>: Upon completion, the system generates detailed evaluation reports containing metrics, scores, and insights at both aggregate and individual response levels.</li><li>: Data scientists or machine learning engineers analyze the generated reports to derive actionable insights and make informed decisions.</li></ul><p>With this solution architecture in mind, let‚Äôs explore how to implement LLM-as-a-judge model evaluations effectively, making sure that you get the most valuable insights from your assessment process.</p><p>To use the LLM-as-a-judge model evaluation, make sure that you have satisfied the following requirements:</p><ul><li>Selected  and  models enabled in Amazon Bedrock. You can confirm that the models are enabled for your account on the  page of the Amazon Bedrock console.</li><li>If you‚Äôre using a custom model instead of an on-demand model for your generator model, make sure that you have sufficient quota for running a <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html\" target=\"_blank\" rel=\"noopener\">Provisioned Throughput</a> during inference. \n  <ul><li>Go to the AWS Service Quotas console, and check the following quotas: \n    <ul><li>Model units no-commitment Provisioned Throughputs across custom models.</li><li>Model units per provisioned model for [your custom model name].</li><li>Both of these fields need to have enough quota to support your Provisioned Throughput model unit. Request a quota increase if necessary to accommodate your expected inference workload.</li></ul></li></ul></li></ul><p>When preparing your dataset for LLM-as-a-judge model evaluation jobs, each prompt must include specific key-value pairs. Here are the required and optional fields:</p><ul><li>: This key indicates the input for various tasks. It can be used for general text generation where the model needs to provide a response, question-answering tasks where the model must answer a specific question, text summarization tasks where the model needs to summarize a given text, or classification tasks where the model must categorize the provided text.</li><li><strong>referenceResponse (used for specific metrics with ground truth)</strong>: This key contains the ground truth or correct response. It serves as the reference point against which the model‚Äôs responses will be evaluated if it is provided.</li><li>: This key is used to generate evaluation scores reported by category, helping organize and segment evaluation results for better analysis.</li></ul><ul><li>Each line must be a valid JSON object</li><li>The file must use JSONL format</li><li>The dataset should be stored in an Amazon S3 bucket</li></ul><p>Example JSONL format without ground truth ( is optional):</p><div><pre><code>{\n    \"prompt\": \"What is machine learning?\"\n    \"category\": \"technical\"\n}\n{\n    \"prompt\": \"Summarize climate change impacts\",\n    \"category\": \"environmental\"\n}\n</code></pre></div><p>Example JSONL format with ground truth ( is optional):</p><div><pre><code>{\n    \"prompt\": \"What is machine learning?\",\n    \"referenceResponse\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses algorithms and statistical models to analyze and draw inferences from patterns in data, allowing computers to perform specific tasks without explicit instructions.\",\n    \"category\": \"technical\"\n}\n{\n    \"prompt\": \"Summarize climate change impacts\",\n    \"referenceResponse\": \"Climate change leads to rising global temperatures, extreme weather events, sea level rise, and disruption of ecosystems. These changes result in more frequent natural disasters, threats to food security, loss of biodiversity, and various public health challenges. The impacts affect agriculture, coastal communities, and vulnerable populations disproportionately.\",\n    \"category\": \"environmental\"\n}</code></pre></div><h2>Start an LLM-as-a-judge model evaluation job using the console</h2><p>You can use LLM-as-a-judge on Amazon Bedrock Model Evaluation to assess model performance through a user-friendly console interface. Follow these steps to start an evaluation job:</p><ol><li>In the Amazon Bedrock console, choose and then select . On the page, choose the </li></ol><ol start=\"2\"><li>Choose  and select <strong>Automatic: LLM-as-a-judge</strong>.</li><li>Enter a name and description and select an . This model will be used as a judge to evaluate the response of a prompt or model from your generative AI application.</li></ol><ol start=\"4\"><li>Choose  and select the model to be used for generating responses in this evaluation job.</li></ol><ol start=\"5\"><li>Select the metrics you want to use to evaluate the model response (such as helpfulness, correctness, faithfulness, relevance, and harmfulness).</li></ol><ol start=\"6\"><li>Select the  for  and for . You can use theoption.</li></ol><ol start=\"7\"><li>Select or create an IAM service role with the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-judge.html\">proper permissions</a>. This includes service access to Amazon Bedrock, the S3 buckets in the evaluation job, and the models being used in the job. If you create a new IAM role in the evaluation setup, the service will automatically give the role the proper permissions for the job. Specify the output S3 bucket and choose .</li></ol><ol start=\"8\"><li>You will be able to see the evaluation job is .&nbsp;Wait for the job status to change to .</li></ol><ol start=\"9\"><li>When complete, select the job to see its details. The following is the metrics summary (such as 0.83 for helpfulness, 1.00 for correctness, 1.00 for faithfulness, 1.00 for relevance, and 0.00 for harmfulness).</li></ol><ol start=\"10\"><li>To view generation metrics details, scroll down in the model evaluation report and choose any individual metric (like helpfulness or correctness) to see its detailed breakdown.</li></ol><ol start=\"11\"><li>To see each record‚Äôs prompt input, generation output, ground truth, and individual scores, choose a metric and select ‚ÄúPrompt details‚Äù. Hover over any individual score to view its detailed explanation.</li></ol><h2>Start an LLM-as-a-judge evaluation job using Python SDK and APIs</h2><p>To use the Python SDK for creating an LLM-as-a-judge model evaluation job, use the following steps. First, set up the required configurations:</p><div><pre><code>import boto3\nfrom datetime import datetime\n\n# Generate unique name for the job\njob_name = f\"Model-evaluation-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n\n# Configure your knowledge base and model settings\nevaluator_model = \"mistral.mistral-large-2402-v1:0\"\ngenerator_model = \"amazon.nova-pro-v1:0\"\nrole_arn = \"arn:aws:iam::&lt;YOUR_ACCOUNT_ID&gt;:role/&lt;YOUR_IAM_ROLE&gt;\"\n\n# Specify S3 locations for evaluation data and output\ninput_data = \"s3://&lt;YOUR_BUCKET&gt;/evaluation_data/input.jsonl\"\noutput_path = \"s3://&lt;YOUR_BUCKET&gt;/evaluation_output/\"\n\n# Create Bedrock client\nbedrock_client = boto3.client('bedrock')</code></pre></div><p>To create an LLM-as-a-judge model evaluation job:</p><div><pre><code>def create_llm_judge_evaluation(\n    client,\n    job_name: str,\n    role_arn: str,\n    input_s3_uri: str,\n    output_s3_uri: str,\n    evaluator_model_id: str,\n    generator_model_id: str,\n    dataset_name: str = None,\n    task_type: str = \"General\" # must be General for LLMaaJ\n):    \n    # All available LLM-as-judge metrics\n    llm_judge_metrics = [\n        \"Builtin.Correctness\",\n        \"Builtin.Completeness\", \n        \"Builtin.Faithfulness\",\n        \"Builtin.Helpfulness\",\n        \"Builtin.Coherence\",\n        \"Builtin.Relevance\",\n        \"Builtin.FollowingInstructions\",\n        \"Builtin.ProfessionalStyleAndTone\",\n        \"Builtin.Harmfulness\",\n        \"Builtin.Stereotyping\",\n        \"Builtin.Refusal\"\n    ]\n\n    # Configure dataset\n    dataset_config = {\n        \"name\": dataset_name or \"CustomDataset\",\n        \"datasetLocation\": {\n            \"s3Uri\": input_s3_uri\n        }\n    }\n\n    try:\n        response = client.create_evaluation_job(\n            jobName=job_name,\n            roleArn=role_arn,\n            applicationType=\"ModelEvaluation\",\n            evaluationConfig={\n                \"automated\": {\n                    \"datasetMetricConfigs\": [\n                        {\n                            \"taskType\": task_type,\n                            \"dataset\": dataset_config,\n                            \"metricNames\": llm_judge_metrics\n                        }\n                    ],\n                    \"evaluatorModelConfig\": {\n                        \"bedrockEvaluatorModels\": [\n                            {\n                                \"modelIdentifier\": evaluator_model_id\n                            }\n                        ]\n                    }\n                }\n            },\n            inferenceConfig={\n                \"models\": [\n                    {\n                        \"bedrockModel\": {\n                            \"modelIdentifier\": generator_model_id\n                        }\n                    }\n                ]\n            },\n            outputDataConfig={\n                \"s3Uri\": output_s3_uri\n            }\n        )\n        return response\n        \n    except Exception as e:\n        print(f\"Error creating evaluation job: {str(e)}\")\n        raise\n        \n # Create evaluation job\ntry:\n    llm_as_judge_response = create_llm_judge_evaluation(\n        client=bedrock_client,\n        job_name=job_name,\n        role_arn=ROLE_ARN,\n        input_s3_uri=input_data,\n        output_s3_uri=output_path,\n        evaluator_model_id=evaluator_model,\n        generator_model_id=generator_model,\n        task_type=\"General\"\n    )\n    print(f\"‚úì Created evaluation job: {llm_as_judge_response['jobArn']}\")\nexcept Exception as e:\n    print(f\"‚úó Failed to create evaluation job: {str(e)}\")\n    raise\n</code></pre></div><p>To monitor the progress of your evaluation job:</p><div><pre><code># Get job ARN based on job type\nevaluation_job_arn = llm_as_judge_response['jobArn']\n# Check job status\ncheck_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn) \nprint(f\"Job Status: {check_status['status']}\")</code></pre></div><p>You can also compare multiple foundation models to determine which one works best for your needs. By using the same evaluator model across all comparisons, you‚Äôll get consistent benchmarking results to help identify the optimal model for your use case.</p><div><pre><code># Generator Models\nGENERATOR_MODELS = [\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    \"amazon.nova-micro-v1:0\"\n]\n\n# Consistent Evaluator\nEVALUATOR_MODEL = \"anthropic.claude-3-haiku-20240307-v1:0\"\n\ndef run_model_comparison(\n    generator_models: List[str],\n    evaluator_model: str\n) -&gt; List[Dict[str, Any]]:\n    evaluation_jobs = []\n    \n    for generator_model in generator_models:\n        job_name = f\"llmaaj-{generator_model.split('.')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n        \n        try:\n            response = create_llm_judge_evaluation(\n                client=bedrock_client,\n                job_name=job_name,\n                role_arn=ROLE_ARN,\n                input_s3_uri=input_data,\n                output_s3_uri=f\"{output_path}/{job_name}/\",\n                evaluator_model_id=evaluator_model,\n                generator_model_id=generator_model,\n                task_type=\"General\"\n            )\n            \n            job_info = {\n                \"job_name\": job_name,\n                \"job_arn\": response[\"jobArn\"],\n                \"generator_model\": generator_model,\n                \"evaluator_model\": evaluator_model,\n                \"status\": \"CREATED\"\n            }\n            evaluation_jobs.append(job_info)\n            \n            print(f\"‚úì Created job: {job_name}\")\n            print(f\"  Generator: {generator_model}\")\n            print(f\"  Evaluator: {evaluator_model}\")\n            print(\"-\" * 80)\n            \n        except Exception as e:\n            print(f\"‚úó Error with {generator_model}: {str(e)}\")\n            continue\n            \n    return evaluation_jobs\n\n# Run model comparison\nevaluation_jobs = run_model_comparison(GENERATOR_MODELS, EVALUATOR_MODEL)</code></pre></div><h3>Correlation analysis for LLM-as-a-judge evaluations</h3><p>You can use the <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\" target=\"_blank\" rel=\"noopener\">Spearman‚Äôs rank correlation coefficient</a> to compare evaluation results between different generator models using LLM-as-a-judge in Amazon Bedrock. After retrieving the evaluation results from your S3 bucket, containing evaluation scores across various metrics, you can begin the correlation analysis.</p><p>Using , compute the correlation coefficient between pairs of generator models, filtering out constant values or error messages to have a valid statistical comparison. The resulting correlation coefficients help identify how similarly different models respond to the same prompts. A coefficient closer to 1.0 indicates stronger agreement between the models‚Äô responses, while values closer to 0 suggest more divergent behavior. This analysis provides valuable insights into model consistency and helps identify cases where different models might produce significantly different outputs for the same input.</p><div><pre><code>import json\nimport boto3\nimport numpy as np\nfrom scipy import stats\n\ndef read_and_organize_metrics_from_s3(bucket_name, file_key):\n    s3_client = boto3.client('s3')\n    metrics_dict = {}\n    \n    try:\n        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n        content = response['Body'].read().decode('utf-8')\n        \n        for line in content.strip().split('\\n'):\n            if line:\n                data = json.loads(line)\n                if 'automatedEvaluationResult' in data and 'scores' in data['automatedEvaluationResult']:\n                    for score in data['automatedEvaluationResult']['scores']:\n                        metric_name = score['metricName']\n                        if 'result' in score:\n                            metric_value = score['result']\n                            if metric_name not in metrics_dict:\n                                metrics_dict[metric_name] = []\n                            metrics_dict[metric_name].append(metric_value)\n        return metrics_dict\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\ndef get_spearmanr_correlation(scores1, scores2):\n    if len(set(scores1)) == 1 or len(set(scores2)) == 1:\n        return \"undefined (constant scores)\", \"undefined\"\n    \n    try:\n        result = stats.spearmanr(scores1, scores2)\n        return round(float(result.statistic), 4), round(float(result.pvalue), 4)\n    except Exception as e:\n        return f\"error: {str(e)}\", \"undefined\"\n\n# Extract metrics\nbucket_name = \"&lt;EVALUATION_OUTPUT_BUCKET&gt;\"\nfile_key1 = \"&lt;EVALUATION_FILE_KEY1&gt;\"\nfile_key2 = \"&lt;EVALUATION_FILE_KEY2&gt;\"\n\nmetrics1 = read_and_organize_metrics_from_s3(bucket_name, file_key1)\nmetrics2 = read_and_organize_metrics_from_s3(bucket_name, file_key2)\n\n# Calculate correlations for common metrics\ncommon_metrics = set(metrics1.keys()) &amp; set(metrics2.keys())\n\nfor metric_name in common_metrics:\n    scores1 = metrics1[metric_name]\n    scores2 = metrics2[metric_name]\n    \n    if len(scores1) == len(scores2):\n        correlation, p_value = get_spearmanr_correlation(scores1, scores2)\n        \n        print(f\"\\nMetric: {metric_name}\")\n        print(f\"Number of samples: {len(scores1)}\")\n        print(f\"Unique values in Model 1 scores: {len(set(scores1))}\")\n        print(f\"Unique values in Model 2 scores: {len(set(scores2))}\")\n        print(f\"Model 1 scores range: [{min(scores1)}, {max(scores1)}]\")\n        print(f\"Model 2 scores range: [{min(scores2)}, {max(scores2)}]\")\n        print(f\"Spearman correlation coefficient: {correlation}\")\n        print(f\"P-value: {p_value}\")\n    else:\n        print(f\"\\nMetric: {metric_name}\")\n        print(\"Error: Different number of samples between models\")</code></pre></div><h2>Best practices for LLM-as-a-judge implementation</h2><p>You can also compare multiple foundation models to determine which one works best for your needs. By using the same evaluator model across all comparisons, you‚Äôll get consistent, scalable results. The following best practices will help you establish standardized benchmarking when comparing different foundation models.</p><ul><li>Create diverse test datasets that represent real-world use cases and edge cases. For large workloads (more than 1,000 prompts), use stratified sampling to maintain comprehensive coverage while managing costs and completion time. Include both simple and complex prompts to test model capabilities across different difficulty levels.</li><li>Choose evaluation metrics that align with your specific business objectives and application requirements. Balance quality metrics (correctness, completeness) with user experience metrics (helpfulness, coherence). Include safety metrics when deploying customer-facing applications.</li><li>Maintain consistent evaluation conditions when comparing different models. Use the same evaluator model across comparisons for standardized benchmarking. Document your evaluation configuration and parameters for reproducibility.</li><li>Schedule regular evaluation jobs to track model performance over time. Monitor trends across different metric categories to identify areas for improvement. Set up performance baselines and thresholds for each metric.</li><li>Optimize batch sizes based on your evaluation needs and cost constraints. Consider using smaller test sets for rapid iteration and larger sets for comprehensive evaluation. Balance evaluation frequency with resource utilization.</li><li>Maintain detailed records of evaluation jobs, including configurations and results. Track improvements and changes in model performance over time. Document any modifications made based on evaluation insights. The optional job description field can help you here.</li><li>Use evaluation results to guide model selection and optimization. Implement feedback loops to continuously improve prompt engineering. Regularly update evaluation criteria based on emerging requirements and user feedback.</li><li>Design your evaluation framework to accommodate growing workloads. Plan for increased complexity as you add more models or use cases. Consider automated workflows for regular evaluation tasks.</li></ul><p>These best practices help establish a robust evaluation framework using LLM-as-a-judge on Amazon Bedrock. For deeper insights into the scientific validation of these practices, including case studies and correlation with human judgments, stay tuned for our upcoming technical deep-dive blog post.</p><p>LLM-as-a-judge on Amazon Bedrock Model Evaluation represents a significant advancement in automated model assessment, offering organizations a powerful tool to evaluate and optimize their AI applications systematically. This feature combines the efficiency of automated evaluation with the nuanced understanding typically associated with human assessment, enabling organizations to scale their quality assurance processes while maintaining high standards of performance and safety.</p><p>The comprehensive metric categories, flexible implementation options, and seamless integration with existing AWS services make it possible for organizations to establish robust evaluation frameworks that grow with their needs. Whether you‚Äôre developing conversational AI applications, content generation systems, or specialized enterprise solutions, LLM-as-a-judge provides the necessary tools to make sure that your models align with both technical requirements and business objectives.</p><p>We‚Äôve provided detailed implementation guidance, from initial setup to best practices, to help you use this feature effectively. The accompanying code samples and configuration examples in this post demonstrate how to implement these evaluations in practice. Through systematic evaluation and continuous improvement, organizations can build more reliable, accurate, and trustworthy AI applications.</p><p>We encourage you to explore LLM-as-a-judge capabilities in the Amazon Bedrock console and discover how automatic evaluation can enhance your AI applications. To help you get started, we‚Äôve prepared a Jupyter notebook with practical examples and code snippets that you can find on our <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/tree/main/evaluation-observe/bedrock-llm-as-judge-evaluation\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/wale_picture_blog.png\" alt=\"\" width=\"100\" height=\"100\"> is a Sr. Data Scientist‚ÄìGenerative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/ishan.jpg\" alt=\"\" width=\"100\" height=\"99\"> is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/Badgephoto.jpeg\" alt=\"\" width=\"100\" height=\"115\"> is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.</p>","contentLength":26547,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From concept to reality: Navigating the Journey of RAG from proof of concept to production","url":"https://aws.amazon.com/blogs/machine-learning/from-concept-to-reality-navigating-the-journey-of-rag-from-proof-of-concept-to-production/","date":1739381272,"author":"Vivek Mittal","guid":231,"unread":true,"content":"<p>Generative AI has emerged as a transformative force, captivating industries with its potential to create, innovate, and solve complex problems. However, the journey from a proof of concept to a production-ready application comes with challenges and opportunities. Moving from proof of concept to production is about creating scalable, reliable, and impactful solutions that can drive business value and user satisfaction.</p><p>One of the most promising developments in this space is the rise of <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" target=\"_blank\" rel=\"noopener\">Retrieval Augmented Generation (RAG)</a> applications. RAG is the process of optimizing the output of a foundation model (FM), so it references a knowledge base outside of its training data sources before generating a response.</p><p>The following diagram illustrates a sample architecture.</p><p>In this post, we explore the movement of RAG applications from their proof of concept or minimal viable product (MVP) phase to full-fledged production systems. When transitioning a RAG application from a proof of concept to a production-ready system, optimization becomes crucial to make sure the solution is reliable, cost-effective, and high-performing. Let‚Äôs explore these optimization techniques in greater depth, setting the stage for future discussions on hosting, scaling, security, and observability considerations.</p><p>The diagram below illustrates the tradeoffs to consider for a production-ready RAG application.</p><p>The success of a production-ready RAG system is measured by its quality, cost, and latency. Machine learning (ML) engineers must make trade-offs and prioritize the most important factors for their specific use case and business requirements. For example, consider the use case of generating personalized marketing content for a luxury fashion brand. The brand might be willing to absorb the higher costs of using a more powerful and expensive FMs to achieve the highest-quality classifications, because misclassifications could lead to customer dissatisfaction and damage the brand‚Äôs reputation. Consider another use case of generating personalized product descriptions for an ecommerce site. The retailer might be willing to accept slightly longer latency to reduce infrastructure and operational costs, as long as the generated descriptions remain reasonably accurate and compelling. The optimal balance of quality, cost, and latency can vary significantly across different applications and industries.</p><p>Let‚Äôs look into practical guidelines on how you can enhance the overall quality of your RAG workflow, including the quality of the retriever and quality of the result generator using <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> and other features of <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a>. Amazon Bedrock Knowledge Bases provides a fully managed capability that helps you implement the entire RAG workflow from ingestion to retrieval and prompt augmentation without having to build custom integrations to data sources and manage data flows.</p><p>An effective evaluation framework is crucial for assessing and optimizing RAG systems as they move from proof of concept to production. These frameworks typically include overall metrics for a holistic assessment of the entire RAG pipeline, as well as specific diagnostic metrics for both the retrieval and generation components. This allows for targeted improvements in each phase of the system. By implementing a robust evaluation framework, developers can continuously monitor, diagnose, and enhance their RAG systems, achieving optimal performance across quality, cost, and latency dimensions as the application scales to production levels. <a href=\"https://aws.amazon.com/bedrock/evaluations/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Evaluations</a> can help you evaluate your retrieval or end-to-end RAG workflow in Amazon Bedrock Knowledge Bases. In the following sections, we discuss these specific metrics in different phases of the RAG workflow in more detail.</p><p>For better retrieval performance, the way the data is stored in the <a href=\"https://aws.amazon.com/blogs/machine-learning/dive-deep-into-vector-data-stores-using-amazon-bedrock-knowledge-bases/\" target=\"_blank\" rel=\"noopener\">vector store</a> has a big impact. For example, your input document might include tables within the PDF. In such cases, using an FM to parse the data will provide better results. You can use <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-advanced-parsing\" target=\"_blank\" rel=\"noopener\">advanced parsing options</a> supported by Amazon Bedrock Knowledge Bases for parsing non-textual information from documents using FMs. Many organizations store their data in structured formats within data warehouses and data lakes. Amazon Bedrock Knowledge Bases offers a feature that lets you connect your RAG workflow to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-build-structured.html\" target=\"_blank\" rel=\"noopener\">structured data stores</a>. This fully managed out-of-the-box RAG solution can help you natively query structured data from where it resides.</p><p>Another important consideration is the way your source document is split up into chunks. If your document would benefit from inherent relationships within your document, it might be wise to use <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-hiearchical-chunking\" target=\"_blank\" rel=\"noopener\">hierarchical chunking</a>, which allows for more granular and efficient retrieval. Some documents benefit from <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-semantic-chunking\" target=\"_blank\" rel=\"noopener\">semantic chunking</a> by preserving the contextual relationship in the chunks, helping make sure that the related information stays together in logical chunks. You can also use your own custom <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-custom-transformation\" target=\"_blank\" rel=\"noopener\">chunking strategy</a> for your RAG application‚Äôs unique requirements.</p><p>RAG applications process user queries by searching across a large set of documents. However, in many situations, you might need to retrieve documents with specific attributes or content. You can use <a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-knowledge-bases-now-supports-metadata-filtering-to-improve-retrieval-accuracy/\" target=\"_blank\" rel=\"noopener\">metadata filtering</a> to narrow down search results by specifying inclusion and exclusion criteria. Amazon Bedrock Knowledge Bases now also supports <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-knowledge-bases-auto-generated-query-filters-improved-retrieval/\" target=\"_blank\" rel=\"noopener\">auto generated query filters</a>, which extend the existing capability of manual metadata filtering by allowing you to narrow down search results without the need to manually construct complex filter expressions. This improves retrieval accuracy by making sure the documents are relevant to the query.</p><p>Writing an effective query is just as important as any other consideration for generation accuracy. You can add a <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html\" target=\"_blank\" rel=\"noopener\">prompt</a> providing instructions to the FM to provide an appropriate answer to the user. For example, a legal tech company would want to provide instructions to restrict the answers to be based on the input documents and not based on general information known to the FM. <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-test-config.html\" target=\"_blank\" rel=\"noopener\">Query decomposition</a> by splitting the input query into multiple queries is also helpful in retrieval accuracy. In this process, the subqueries with less semantic complexity might find more targeted chunks. These chunks can then be pooled and ranked together before passing them to the FM to generate a response.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-rerank-api-accuracy-rag-applications/\" target=\"_blank\" rel=\"noopener\">Reranking</a>, as a post-retrieval step, can significantly improve response quality. This technique uses LLMs to analyze the semantic relevance between the query and retrieved documents, reordering them based on their pertinence. By incorporating reranking, you make sure that only the most contextually relevant information is used for generation, leading to more accurate and coherent responses.</p><p>Adjusting <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html\" target=\"_blank\" rel=\"noopener\">inference parameters</a>, such as temperature and top-k/p sampling, can help in further refining the output.</p><p>You can use Amazon Bedrock Knowledge Bases to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-test-config.html\" target=\"_blank\" rel=\"noopener\">configure and customize</a> queries and response generation. You can also improve the relevance of your query responses with a <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/rerank.html\" target=\"_blank\" rel=\"noopener\">reranker model</a> in Amazon Bedrock.</p><p>The key metrics for retriever quality are <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">context precision</a>, <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">context recall</a>, and <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">context relevance</a>. Context precision measures how well the system ranks relevant pieces of information from the given context. It considers the question, ground truth, and context. Context recall provides the percentage of ground truth claims or key information covered by the retrieved context. Context relevance measures whether the retrieved passages or chunks are relevant for answering the given query, excluding extraneous details. Together, these three metrics offer insight into how effectively the retriever is able to surface the most relevant and focused source material to support a high-quality response.</p><p>Generator quality can be assessed through several key metrics. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Context utilization</a> examines how effectively the generator uses relevant information from the provided source material. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Noise sensitivity</a> gauges the generator‚Äôs propensity to include inaccurate details from the retrieved content. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Hallucination</a> measures the extent to which the generator produces incorrect claims not present in the source data. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Self-knowledge</a> reflects the proportion of accurate statements generated that can‚Äôt be found in the retrieved chunks. Finally, <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">faithfulness</a> evaluates how closely the generator‚Äôs output aligns with the information contained in the source material.</p><p>For measuring the overall generation quality, the key metrics include measuring the <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#overall-metrics\" target=\"_blank\" rel=\"noopener\">precision</a>, <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#overall-metrics\" target=\"_blank\" rel=\"noopener\">recall</a>, and <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">answer similarity</a>. Precision suggests the proportion of the correct claims in model‚Äôs response, whereas recall suggests the proportion of the ground truth claims covered by the model‚Äôs response. Answer similarity compares the meaning and content of a generated answer with a reference or ground truth answer. It evaluates how closely the generated answer matches the intended meaning of the ground truth answer.</p><p>Establishing a feedback loop with an evaluation framework against these quality metrics allows for continuous improvement, where the system can learn from user interactions and refine its performance over time. By optimizing these quality metrics, the RAG system can be designed to deliver reliable, cost-effective, and high-performing results for users.</p><p>Implementing responsible AI practices is crucial for maintaining ethical and safe deployment of RAG systems. This includes using guardrails to filter harmful content, deny certain topics, mask sensitive information, and ground responses in verified sources to reduce hallucinations.</p><p>Cost considers the compute resources and infrastructure required to run the system, and latency evaluates the response times experienced by end-users. To optimize cost and latency, implement <a href=\"https://aws.amazon.com/blogs/database/improve-speed-and-reduce-cost-for-generative-ai-workloads-with-a-persistent-semantic-cache-in-amazon-memorydb/\" target=\"_blank\" rel=\"noopener\">caching strategies</a> to reduce the need for expensive model inferences. Efficient <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html\" target=\"_blank\" rel=\"noopener\">query batching</a>&nbsp;can also improve overall throughput and reduce resource usage. Balance performance and resource usage to find the ideal configuration that meets your application‚Äôs requirements.</p><p>Use tools like Amazon Bedrock Knowledge Bases so you can take advantage of fully managed support for the end-to-end RAG workflow. It supports many of the <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/07/knowledge-bases-amazon-bedrock-advanced-rag-capabilities/\" target=\"_blank\" rel=\"noopener\">advanced RAG capabilities</a> we discussed earlier. By addressing these optimization techniques, you can transition your RAG-powered proof of concept to a robust, production-ready system that delivers high-quality, cost-effective, and low-latency responses to your users.</p><p>In addition to the server or compute layer, you will also need to consider an orchestration tool, testing environments, and a continuous integration and delivery (CI/CD) pipeline to streamline your application deployment. Having a feedback loop established based on the quality metrics along with a CI/CD pipeline is an important first step to creating self-healing architectures.</p><p>As your application grows, you will need to make sure your infrastructure can scale to meet the increasing demand. This can involve containerization with Docker or choosing serverless options, implementing load balancing, setting up auto scaling, and choosing between on-premises, cloud, or hybrid solutions. It also includes unique scaling requirements of your frontend application and backend generative AI workflow, as well as the use of content delivery networks (CDNs) and disaster recovery and backup strategies.</p><p>The following is a sample architecture for a secure and scalable RAG-based web application. This architecture uses Amazon ECS for hosting the service, <a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\" rel=\"noopener\">Amazon CloudFront</a> as a CDN, <a href=\"https://aws.amazon.com/waf/\" target=\"_blank\" rel=\"noopener\">AWS WAF</a> as a firewall, and <a href=\"https://aws.amazon.com/memorydb/\" target=\"_blank\" rel=\"noopener\">Amazon MemoryDB</a> for providing a semantic cache.</p><p>By carefully considering these aspects of hosting and scaling your infrastructure, you can build a resilient and adaptable system to support your growing web application or service. Stay tuned for more detailed information on these topics in upcoming blog posts.</p><h2>Data privacy, security, and observability</h2><p>Maintaining data privacy and security is of utmost importance. This includes implementing security measures at each layer of your application, from encrypting data in transit to setting up robust authentication and authorization controls. It also involves focusing on compute and storage security, as well as network security. Compliance with relevant regulations and regular security audits are essential. Securing your generative AI system is another crucial aspect. By default, Amazon Bedrock Knowledge Bases encrypts the traffic using AWS managed <a href=\"http://aws.amazon.com/kms\" target=\"_blank\" rel=\"noopener\">AWS Key Management Service</a> (AWS KMS) keys. You can also choose customer managed KMS keys for more control over encryption keys. For more information on application security, refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/safeguard-a-generative-ai-travel-agent-with-prompt-engineering-and-amazon-bedrock-guardrails/\" target=\"_blank\" rel=\"noopener\">Safeguard a generative AI travel agent with prompt engineering and Amazon Bedrock Guardrails</a>.</p><p>Comprehensive logging, monitoring, and maintenance are crucial to maintaining a healthy infrastructure. This includes setting up structured logging, centralized log management, real-time monitoring, and strategies for system updates and migrations.</p><p>By addressing these critical areas, you can build a secure and resilient infrastructure to support your growing web application or service. Stay tuned for more in-depth coverage of these topics in upcoming blog posts.</p><p>To successfully transition a RAG application from a proof of concept to a production-ready system, you should focus on optimizing the solution for reliability, cost-effectiveness, and high performance. Key areas to address include enhancing retriever and generator quality, balancing cost and latency, and establishing a robust and secure infrastructure.</p><p>By using purpose-built tools like Amazon Bedrock Knowledge Bases to streamline the end-to-end RAG workflow, organizations can successfully transition their RAG-powered proofs of concept into high-performing, cost-effective, secure production-ready solutions that deliver business value.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/07/vivek_phonetool.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Solution Architect at Amazon Web Services, where he helps organizations architect and implement cutting-edge cloud solutions. With a deep passion for Generative AI, Machine Learning, and Serverless technologies, he specializes in helping customers harness these innovations to drive business transformation. He finds particular satisfaction in collaborating with customers to turn their ambitious technological visions into reality.</p><p>&nbsp;is a Sr. Enterprise Solutions Architect at AWS, experienced in Software Engineering, Enterprise Architecture, and AI/ML. He is deeply passionate about exploring the possibilities of generative AI. He collaborates with customers to help them build well-architected applications on the AWS platform, and is dedicated to solving technology challenges and assisting with their cloud journey.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/07/mani.jpeg\" alt=\"\" width=\"100\" height=\"133\">&nbsp;is a Tech Lead ‚Äì Generative AI Specialists, author of the book Applied Machine Learning and High-Performance Computing on AWS, and a member of the Board of Directors for Women in Manufacturing Education Foundation Board. She leads machine learning projects in various domains such as computer vision, natural language processing, and generative AI. She speaks at internal and external conferences such AWS re:Invent, Women in Manufacturing West, YouTube webinars, and GHC 23. In her free time, she likes to go for long runs along the beach.</p>","contentLength":15255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Game Bub ‚Äì open-source FPGA retro emulation handheld","url":"https://eli.lipsitz.net/posts/introducing-gamebub/","date":1739380285,"author":"elipsitz","guid":317,"unread":true,"content":"<p>I‚Äôm excited to announce the project I‚Äôve been working on for the last year and a half: , an open-source FPGA based retro emulation handheld, with support for Game Boy, Game Boy Color, and Game Boy Advance games.</p><p>Game Bub can play physical cartridges, as well as emulated cartridges using ROM files loaded from a microSD card. Game Bub also supports the <a href=\"https://en.wikipedia.org/wiki/Game_Link_Cable\">Game Link Cable</a> in both GB and GBA modes for multiplayer games. I designed the hardware with a number of bonus features, like video out (HDMI) via a custom dock, a rumble motor, real-time clock (for certain games). Additionally, the hardware is designed with extensibility in mind, allowing future software improvements to expand its capabilities.</p><p>Game Bub has a custom-designed 6 layer PCB featuring a Xilinx XC7A100T FPGA with integrated memory,  display, speakers, rechargable battery, GB/GBA cartridge slot, all packaged up in a custom 3D-printed enclosure.</p><p>Check out the instructions, code, and design files <a href=\"https://github.com/elipsitz/gamebub\">on GitHub</a>. Note that building a Game Bub unit is fairly complex. If you might be interested in buying a complete Game Bub kit, please <a href=\"https://forms.gle/m1FFUqpCde7x5u5AA\">fill out this form</a> to help me gauge interest.</p><p>I had a lot of fun implementing a Game Boy at the hardware level, and I started thinking about how far I could take the project. I was using a Pynq-Z2 development board, which was definitely the right way to get started, but it came with a lot of limitations.</p><p>I had to use an external monitor for audio/video, and an external gamepad for input, but a real Game Boy, of course, is a portable handheld. I also wanted to add Game Boy Advance support, but the memory architecture of the Pynq-Z2 had access latency that <a href=\"https://eli.lipsitz.net/posts/fpga-gameboy-emulator/#fixing-an-audio-bug-on-the-game-boy-color\">was just barely acceptable for the Game Boy</a>, and would have been completely unacceptable for the Game Boy Advance. I also wanted to make something less ‚Äúhacky‚Äù: a real device that I could play and give to people, not just a bare PCB.</p><p>Furthermore, while there are open-source FPGA retrogaming projects (e.g. <a href=\"https://en.wikipedia.org/wiki/MiSTer\">MiSTer</a>), there doesn‚Äôt appear to be anything open-source that supports physical Game Boy and Game Boy Advance cartridges, let alone an open-source handheld device.</p><p>Thus, I somewhat naively set out to design what would become by far my most complex electrical engineering and hardware design project to date.</p><p>I set out some goals for the project:</p><ul><li>Build a standalone, rechargable battery-powered FPGA handheld</li><li>Minimize cost and complexity by using off-the-shelf components wherever possible</li><li>Capable of playing Game Boy, Game Boy Color, and Game Boy Advance games</li><li>Capable of using physical cartridges, or emulating cartridges (reading ROM files off of a microSD card)</li><li>Easy to use: graphical menu and in-game overlay</li><li>Integrated display and speakers, with headphone support</li><li>Integrated peripherals (rumble, real-time clock, accelerometer) for emulated cartridges</li><li>HDMI video output support for playing on a big screen</li><li>Decent looking design with good ergonomics</li><li>Expansion opportunities in the future: support for more systems, Wi-Fi, etc.</li></ul><p>And finally, since I was building this project for fun and learning, I wanted to be able to fully understand every single component of the system. I wanted to use my own emulator cores (e.g. not just port them from <a href=\"https://mister-devel.github.io/MkDocs_MiSTer/\">MiSTer</a>), do my own board design, and write my own drivers to interface with peripherals.</p><h3>A brief rant about FPGA retrogaming<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#a-brief-rant-about-fpga-retrogaming\">#</a></h3><p>There‚Äôs a lot of misleading marketing and hype out there around FPGA retrogaming. Some claim that FPGA retrogaming devices are not emulators (because they supposedly ‚Äúact like [the system] at the gate level‚Äù), that they achieve ‚Äúperfect accuracy‚Äù, or that they‚Äôre superior to software emulators.</p><p>In my opinion, this is blatantly wrong and actively harmful. FPGA retrogaming devices are emulators: they pretend to be something they‚Äôre not. And they‚Äôre only as accurate as they‚Äôre programmed to be, since they‚Äôre recreations. An FPGA can make certain aspects of accuracy easier to achieve, but it doesn‚Äôt guarantee it.</p><p>Software emulators can be extremely accurate. Furthermore, perfect accuracy (if it‚Äôs even possible) is by no means a requirement to play an entire system‚Äôs library of games. Some people claim that FPGA emulators are the only way to ‚Äúpreserve‚Äù a system, but I‚Äôd argue that software emulators are a significantly more accessible (no special hardware needed!) way to further this goal.</p><p>I believe that FPGA emulators have only one real advantage over software emulators: they can more easily interface with original hardware, such as physical cartridges or other consoles via link cables.</p><p>I did this project not because I think that FPGA emulators are inherently better than software emulators, but because I think they‚Äôre interesting and fun to build.</p><p>I began work on the project by doing some initial research and sketching out a high level design.</p><p>My previous FPGA emulator project used a Xilinx Zynq chip, which integrates FPGA fabric (‚ÄúPL‚Äù) with a dual-core ARM processor running Linux (‚ÄúPS‚Äù). I implemented the entire emulator on the FPGA, and used the Linux system to configure the FPGA, render the UI, and load ROM files from the filesystem.</p><p>I decided to keep this same division of responsibilities: using the FPGA to do the core emulation, with a separate processor to do support tasks. However, to make the overall design easier to reason about, I decided to to use an FPGA-only chip (without any hard processor cores), and an external microcontroller (MCU) to do the tasks that the ARM cores did before.</p><p>The FPGA would consume input, directly interface to the game cartridges (through level shifters to support both the 3.3 volt GBA and 5 volt Game Boy), and output audio and video to the speakers and display. The MCU would handle the UI, read ROM files from the microSD card, initialize peripherals (display, DAC, IMU), handle power sequencing, and load the FPGA configuration.</p><p>I wanted to have Wi-Fi and Bluetooth support: Wi-Fi for software updates, and the possibility of emulating the <a href=\"https://en.wikipedia.org/wiki/Game_Boy_Advance_Wireless_Adapter\">Game Boy Advance Wireless Adapter</a>, and Bluetooth to support wireless game controllers (when connected to an external display). To reduce complexity (and avoid the need for careful RF design), I looked only for complete Wi-Fi/Bluetooth modules with integrated antennas.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-block-diagram.svg\" alt=\"An early block diagram I sketched out\"><figcaption><p>An early block diagram I sketched out</p></figcaption></figure><p>I also drew out rough sketches of what the final device might look like: placement of buttons, screen, speakers, ports, cartridge slot, and battery. I settled on a vertical Game Boy Color-esque design (as opposed to a horizontal Game Boy Advance-style design), because I felt that this would maximize the space in the back of the device for full-size Game Boy Color cartridges and a battery.</p><p>After sketching out the goals and high level design, I started component selection: picking out each non-trivial component of the system, evaluating features and requirements (e.g. how they communicate, power consumption and voltages needed).</p><p>Since I intended to have this manufactured and assembled at JLCPCB, I strongly preferred parts that were available in their part library. One technique I even used for narrowing down part choices was finding the relevant category in their part search, and sorting by their stock count.</p><p>I initially planned to use an <a href=\"https://en.wikipedia.org/wiki/RP2040\">RP2040</a> microcontroller, with a separate ESP32-WROOM module to support Wi-Fi and Bluetooth.</p><p>The ESP32 supports both Bluetooth Classic and LE, which is essential for supporting a wide range of controllers, and the RP2040 has USB host support, to support wired controllers.</p><p>During the schematic design process, I ended up simplifying the RP2040 + ESP32 combination to just a single ESP32-S3 module for a few reasons:</p><ul><li>I started running out of GPIOs on the RP2040, and I was dedicating 4 of them (2 for UART, 1 for reset, 1 for booting in firmware download mode) to communication with the ESP32. Plus, the ESP32-S3 has more GPIOs overall.</li><li>I wanted to write the MCU firmware in Rust, and the ESP32-S3 had support for the Rust standard library (via ESP-IDF and <a href=\"https://github.com/esp-rs/esp-idf-hal\">esp-idf-hal</a>). This seemed like it would be easier to get the software up and running.</li><li>Fewer components means easier routing and assembly</li><li>The ESP32-S3 has an SDIO module (for interfacing with the microSD card), and FAT filesystem support (via ESP-IDF). It would be possible to do this with the RP2040 PIO, but having a proper peripheral and driver for this makes it a lot easier.</li><li>The ESP32-S3 is more powerful than the RP2040, and would probably be able to render a smoother UI.</li></ul><p>However, the ESP32-S3 has one main disadvantage compared to the original ESP32: it doesn‚Äôt have Bluetooth Classic support, only LE. This would greatly limit the range of supported wireless controllers, but I believed the compromise was worth it. I also decided to scrap USB host support, because supporting USB-C dual role (switchable device or host) would have added a lot of additional complexity.</p><p>If the RP2350 microcontroller (the successor to the RP2040) had been available when I started this project, I may very well have chosen it, since it has even more power, PIO blocks, memory, and GPIO pins. I might have paired it with an RM2 radio module for Wi-Fi and Bluetooth.</p><p>I wanted a display that would support integer scaling for the Game Boy Advance, which has a 240x160 pixel screen. I was also looking for a screen roughly on the order of 3.0-3.5 inches wide (diagonal), to be comfortable to hold in the hand.</p><p>I found the ER-TFT035IPS-6 LCD module from <a href=\"https://www.buydisplay.com/\">EastRising</a>, with a 3.5 inch display, and a 320x480 pixel resolution. This allows for a 2x integer scale for the Game Boy Advance (and a 2x scale plus centering for the 160x144 Game Boy display). This checked off almost all of the boxes: integer scaling, a good size, available at a reasonable price, pretty good documentation (for the ILI9488 LCD controller).</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/er-tft035ips-6.jpg\" alt=\"ER-TFT035IPS-6 LCD module\" width=\"1000\" height=\"550\"><figcaption><p>ER-TFT035IPS-6 LCD module</p></figcaption></figure><p>The main issue, which actually ended up being fairly annoying, is that it‚Äôs a 320x480 display, not 480x320. Meaning, it‚Äôs oriented in portrait mode, not landscape. I rotated the device 90 degrees to fit in a landscape orientation, but this created two issues:</p><ul><li>In landscape orientation, the bottom of the display (containing the LCD driver chip and the flex cable) faces to the left or the right, which means that larger bazels are required on the left and right of the display to center the ‚Äúactive area‚Äù of the LCD within the handheld.</li><li>In landscape orientation, the display refreshes from left to right, not top to bottom.</li></ul><p>The problem with refreshing from left to right is that the Game Boy and Game Boy Advance (and almost every other system) refresh from top to bottom. This means that the display can‚Äôt be refreshed perfectly in sync with the game (zero buffering), and single buffering leads to unsightly diagonal tearing. Instead, I had to use triple buffering, where the game is writing to one framebuffer, the LCD driver is reading from another buffer, and there‚Äôs one spare swap buffer. This increases the amount of memory used ‚Äì and because it needed to be accessed by both the game and LCD driver simultaneously (dual port), it needed to be stored in internal block RAM in the FPGA, a scarce resource.</p><p>So, even though the Game Boy emulator uses &lt;10% of the total logic resources of the FPGA, and the Game Boy Advance uses around 30%, I had to use a large (more expensive, and power hungry) FPGA so that I had enough block RAM.</p><p>I also stuck a standard size HDMI port into the design, connected directly to the FPGA. HDMI has a few additional, non-video signals that need level shifting from 5V to 3.3V (I opted for discrete transistors), and it requires the source (me!) to supply a small amount of power.</p><p>I had never previously designed anything that used a lithium ion battery, so I had a fair amount of learning to do. <a href=\"https://learn.adafruit.com/li-ion-and-lipoly-batteries/overview\">Adafruit</a> was a helpful resource. I needed a way to charge the battery from USB power, and a way to measure how charged it is.</p><p>Lithium ion batteries can be dangerous if misused. Safely charging a battery is non-trivial, and requires a feedback loop and adjustable voltage sources. A dedicated IC seemed like the best way to do this. A lot of hobbyists use the ultra-cheap TP4056 1A battery charger, but I‚Äôd read about a lot of issues it has around safely charging the battery while using it. I decided instead to opt for the <a href=\"https://www.ti.com/lit/ds/symlink/bq24073.pdf\">TI BQ2407x</a> series of battery charger ICs. They seem to be widely used in commercial products, came with a comprehensive datasheet, and had a few critical features: programmable input and charge current limits, safety timers, and ‚Äúpower path management‚Äù for safely charging the battery while the device is on.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/lipo-discharge-curve.png\" alt=\"Typical discharge curve for a 3.7V lipo battery (source: Adafruit)\" width=\"579\" height=\"402\"><figcaption><p>Typical discharge curve for a 3.7V lipo battery (source: <a href=\"https://learn.adafruit.com/assets/979\">Adafruit</a>)</p></figcaption></figure><p>There are a few ways to measure the charge level of the battery, which generally relies on the fact that a lithium ion battery‚Äôs voltage depends on its charge level. A fully charged battery is about 4.2 volts, a battery with between 80% and 20% charge is about 3.7 volts, and below that a drained battery falls off pretty quickly to under 3.0 volts. If all you want is a coarse estimate of the battery level, you can use an ADC to read the voltage and estimate whether the battery is fully charged or nearly discharged. However, since the voltage curve is nearly flat between 20% and 80% charge (and is also dependent on the load), this can‚Äôt give the fine-grained battery percentage that we‚Äôre used to on phones and laptops. Instead, I opted for a discrete fuel gauge IC, the <a href=\"https://www.analog.com/en/products/max17048.html\">MAX17048</a>. It‚Äôs simple to integrate and inexpensive.</p><p>I decided to use a push button for the main power switch, because I needed to be able to do a graceful shutdown, where the microcontroller could save state (e.g. the current save file for an emulated cartridge) before it actually powered off.</p><p>I briefly considered using an ultra-low power, always on microcontroller to act as a custom <a href=\"https://en.wikipedia.org/wiki/Power_management_integrated_circuit\">PMIC</a> to provide power switch functionality (and perhaps avoid the need for a separate real-time clock IC, and even a battery gauge). While this would have been flexible and really cool, I figured it wasn‚Äôt worth the additional complexity.</p><p>The main system power ranges from about 3.4 V when the battery is discharged, to 4.2 V when the battery is fully charged, up to 5.0 V when the device is plugged in with USB.</p><p>The ESP32-S3 module required 3.3 V, and most of the other ICs in the system did too. The main exception is the FPGA, which requires a 1.0 V core power rail, a 1.8 V ‚Äúauxiliary‚Äù power rail, and a 3.3 V power rail for I/O. Moreover, according to the <a href=\"https://docs.amd.com/v/u/en-US/ds181_Artix_7_Data_Sheet\">Xilinx Artix-7 datasheet (DS181)</a>, these power rails need to be powered on in a particular sequence: for my use, this means 1.0 V, then 1.8 V, then 3.3 V. Additionally, I needed a 5.0 V supply to interface with Game Boy / Game Boy Color cartridges.</p><p>There are multi-rail power regulators available, and a lot of FPGA development boards use them. However, they all seemed to be expensive and difficult to purchase in low quantities. Instead, I opted for separate power regulators for each rail. I used <a href=\"https://en.wikipedia.org/wiki/Buck_converter\">buck converters</a> instead of <a href=\"https://en.wikipedia.org/wiki/Linear_regulator\">linear regulators</a> to maximize power efficiency.</p><p>I used the <a href=\"https://www.ti.com/product/TLV62585\">TLV62585</a> converter for the 3.3 V, 1.8 V, and 1.0 V rails. This is a simple, performant buck converter with a ‚Äúpower good‚Äù output, which is useful for power sequencing: you can connect the  output of one regulator to the  pin of the next regulator, to power on the rails in the desired order.</p><p>For the 5.0 V rail, I used the <a href=\"https://www.ti.com/product/TPS61022\">TPS61022</a> boost converter. This converter is way overkill for the 5.0 V rail (which might use 75mA ), but it was readily available, and conveniently compatible with the same 1¬µH inductor as the buck converters.</p><p>According to the FPGA datasheet, the XC7A100T consumes more than 100mW of static power. That is, it consumes that as long as it‚Äôs connected to power, even if it‚Äôs doing absolutely nothing. I figured I might want to support a low power sleep mode, so I decided to split the FPGA into a separate power domain with an explicit power enable signal from the MCU. I also used an <a href=\"https://www.diodes.com/datasheet/download/AP2191.pdf\">AP2191W</a> load switch for the FPGA‚Äôs 3.3 V rail to be able to keep the 1.0 V ‚Üí 1.8 V ‚Üí 3.3 V sequencing.</p><p>I wanted the device to have both speakers and a 3.5mm headphone jack. Ultimately, the FPGA generates an <a href=\"https://en.wikipedia.org/wiki/I%C2%B2S\">I2S</a> digital audio signal, and I needed a <a href=\"https://en.wikipedia.org/wiki/Digital-to-analog_converter\">DAC</a> to convert it to an analog audio signal, and then an amplifier to drive the speakers (or headphones). I wanted digital volume control (to support volume buttons, rather than a volume knob or slider), and I needed some way to switch the audio output between speakers and the headphones, depending on whether or not headphones are plugged in. With no real audio experience, this seemed like a daunting task.</p><p>While searching for multiple separate components, I stumbled upon the <a href=\"https://www.ti.com/product/TLV320DAC3101\">TLV320DAC3101</a>. It combines a stereo DAC with a speaker amplifier and a headphone driver. Additionally, it supports digital volume control, and headphone detection. I think this chip is a good example of how thoughtful component selection can simplify the overall design. Looking through the datasheet, it required a 1.8 V core voltage (unlike essentially every other component other than the FPGA) and a fair amount of configuration registers to set over I2C, but it had all of the features I needed.</p><p>I was originally planning to have just a single (mono) speaker, but I figured if I had a stereo DAC, I might as well put two in there. I chose the <a href=\"https://www.sameskydevices.com/product/audio/speakers/miniature-(10-mm~40-mm)/ces-20134-088pmb\">CES-20134-088PMB</a>, an enclosed microspeaker with a JST-SH connector. Having an enclosed speaker simplified audio design, because as it turns out, you can‚Äôt just stick a speaker to a board and expect it to sound okay (Same Sky, the manufacturer of that speaker, <a href=\"https://www.sameskydevices.com/blog/how-to-design-a-micro-speaker-enclosure\">has a blog post explaining some of the nuances</a>).</p><p>I prefer the feeling of clicky, tactile buttons (such as those found in the GBA SP, Nintendo DS (original), Nintendo 3DS, Switch) compared to ‚Äúmushy‚Äù membrane buttons (such as those found in the Game Boy Color, original GBA, and Nintendo DS Lite). I learned that the tactile switches used in the GBA SP are a <a href=\"https://tech.alpsalpine.com/e/products/detail/SKRRAAE010/\">widely available off-the-shelf part from Alps Alpine</a>. I used similar, but smaller buttons for the Start/Select/Home buttons, and a right-angle button from the same manufacturer for side volume and power buttons.</p><p>Although I only had plans to support Game Boy and Game Boy Advance (requiring a D-pad, A and B buttons, L and R shoulder buttons, and Start/Select), I opted to add two more ‚ÄúX‚Äù and ‚ÄúY‚Äù face buttons to leave the possibility open of supporting more systems in the future.</p><p>The L and R buttons posed an additional challenge ‚Äì I found numerous right-angle tactile buttons (to be soldered onto the back, facing towards the top). However, none of them seemed to have the actuator (the part of the button you make contact with) far enough away from the PCB to be easily pressed. At first, I thought about making a separate shoulder button board to move them at the correct distance, but then I started looking at what existing devices do for inspiration. The Game Boy Advance SP actually uses a more complex mechanism for the shoulder buttons: rather than a simple actuator like the face buttons, there‚Äôs a hinge with a <a href=\"https://en.wikipedia.org/wiki/Torsion_spring\">torsion spring</a> that hits the actuator at an angle. This is actually part of what makes the shoulder buttons pleasant to press: you don‚Äôt need to hit them from exactly the right direction, because they pivot. I ended up just going with a standard right-angle tactile button, opting to solve the problem with the mechanism in the enclosure.</p><figure><figcaption><p>GBA SP shoulder button mechanism</p></figcaption></figure><p>One of my main goals was to allow ROM files to be loaded from a microSD card, rather than only being able to be played from a physical cartridge. To do this, I‚Äôd need dedicated RAM for the FPGA to hold the game. Game Boy Advance games, typically, are a maximum of 32 MB. They don‚Äôt make SRAMs that large (and if they did, they‚Äôd be very expensive). Instead, I needed to use <a href=\"https://en.wikipedia.org/wiki/Dynamic_random-access_memory\">DRAM</a>.</p><p>Asynchronous SRAM is very simple: supply a read address to the address pins, and some amount of nanoseconds later, the data you‚Äôre reading appears on the data pins. DRAM is more complex: the simplest kind is ‚Äúsingle data rate synchronous DRAM‚Äù (SDR SDRAM, or just <a href=\"https://en.wikipedia.org/wiki/Synchronous_dynamic_random-access_memory\">SDRAM</a>, distinguishing it from the significantly more complex DDR SDRAM). However, even SDRAM is non-trivial to use. DRAM is organized into banks, rows, and columns, and accessing DRAM requires sending commands to ‚Äúactivate‚Äù (open) a row before reading out ‚Äúcolumns‚Äù, and then ‚Äúprecharging‚Äù (closing) a row. Handling all of this requires a DRAM controller (see this <a href=\"https://www.fpga4fun.com/SDRAM2.html\">simple description of the state machine</a> required). This isn‚Äôt terribly complex, but I was signing myself up for more work.</p><p>Alternatively, I could have chosen a PSRAM chip (essentially DRAM with an integrated controller to make it have a more SRAM-like interface). However, I couldn‚Äôt find a PSRAM part that I was happy with (cost, availability, interface), and so I ended up going with the inexpensive W9825G6KH 32MB 16-bit SDRAM.</p><p>I also decided to stick a 512 KiB SRAM chip in the design in case I ended up needing some more simple memory later, like for emulating the SRAM used for Game Boy cartridge save files. Despite being 1/64 the capacity, this chip was about 3x the cost of the SDRAM. This ended up being a wise decision, since a lot of my internal FPGA block ram was eaten up by the triple buffer for the display (see above).</p><h3>Cartridge and Link Ports<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#cartridge-and-link-ports\">#</a></h3><p>The cartridge slot and link ports are no-name parts from AliExpress, easily available for cheap. These seem to mostly be GBA SP compatible, and are often used as repair parts.</p><p>The Game Boy Advance can play both Game Boy [Color] and Game Boy Advance games. These run at different voltages and use different protocols, so the device needed some way of determining which type of cartridge is inserted.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gb-vs-gba-cartridge-bottom.jpg\" alt=\"GBA cartridge (top) vs GB cartridge (bottom)\" width=\"1440\" height=\"519\"><figcaption><p>GBA cartridge (top) vs GB cartridge (bottom)</p></figcaption></figure><p>The cartridges are physically different at the bottom: GBA cartridges (the top cartridge in the image) have a notch on either side. The GBA has a  that senses the absence of a notch on an inserted cartridge and switches the device into Game Boy Color mode.</p><p>I measured the size and position of this notch, and searched Digi-Key and Mouser for switches that met these constraints. In the end, I was only able to find a single switch that would work.</p><h3>Miscellaneous peripherals<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#miscellaneous-peripherals\">#</a></h3><p>I used the surprisingly cheap <a href=\"https://www.st.com/en/mems-and-sensors/lsm6ds3tr-c.html\">LSM6DS3TR-C</a> IMU from ST. This tiny IMU has a 3-axis accelerometer and gyroscope, more than sufficient for emulating the few GB/GBA cartridges that have motion controls.</p><p>For keeping track of time even when the device was off, I used the <a href=\"https://www.nxp.com/part/PCF8563T\">PCF8563T</a> real-time clock chip. I chose this because it was 1) I2C (no additional pins required), 2) cheap, and 3) readily available from JLCPCB. Interestingly, all of the real-time clock chips I found count in seconds/minutes/hours/days/months/years. This makes sense for a really simple device with minimal computational power. However, it‚Äôs annoying for my purposes, since all I really want is a timestamp I can pass to some other datetime library, and converting between the calendar time and a unix timestamp is non-trivial due to how the chips incompletely handle leap years.</p><p>I picked up a few cheap coin vibration motors to use for vibration support (for the rare cartridge that had a built-in vibration motor).</p><p>I also used a <a href=\"https://www.ti.com/product/TCA9535\">TCA9535</a> I2C I/O expander to connect the face buttons to the MCU. I ran out of pins, and while I  have used the FPGA as a sort of I/O expander, I figured I‚Äôd make it simpler for myself (and allow the buttons to be used even if the FPGA was powered off) by letting the MCU read them itself.</p><p>For this project, as with my previous ones, I used <a href=\"https://www.kicad.org/\">KiCad</a> to create my schematic and do PCB layout. I really can‚Äôt recommend KiCad enough: it‚Äôs a great program, intuitive to use, and it‚Äôs free and open source.</p><p>This was a very ambitious project for my level of electrical engineering experience, and creating the schematic took a couple of weeks. I spent a lot of time designing the circuit for each component, because I was afraid I‚Äôd do something wrong and end up with a stack of useless boards without the skills needed to debug them. A lot of the component selection actually happened in parallel with schematic design, as I found new requirements or problems and had to change components.</p><p>I gained a lot of experience reading component datasheets. It‚Äôs a really valuable skill, both for component selection and for creating designs that use the components. Nearly every datasheet has a ‚Äútypical application‚Äù section, where the manufacturer shows how the component would fit into a circuit. At minimum, this has power supply information (e.g. these voltages to these pins with these decoupling capacitors). For more complex components like the DAC, it also has information about power sequencing, different ways the device could be connected to the rest of the system, a register list, that sort of thing. Some components also included PCB layout recommendations. This information was all really helpful, and gave me a good deal of confidence that my board would work as long as I read through the datasheet and followed the manufacturer‚Äôs recommendations.</p><p>Then I got to the FPGA. Nearly every component has a single datasheet. Some of them have an additional application note or two. Particularly complex chips (like the ESP32-S3 microcontroller) have a separate datasheet, reference manual, and hardware design guide. The Xilinx Series 7 FPGAs have . Overviews, packaging and pinout, configuration guides, BGA design rules, power specifications, clocking resources, I/O specifications, PCB layout guides, design checklists‚Ä¶ even a 4MB Excel spreadsheet for estimating power consumption! And believe me, Xilinx didn‚Äôt just write documentation for fun: there‚Äôs so much documentation because the chip  this much documentation.</p><p>Designing with the FPGA was overwhelming, and  beyond my experience level. At several points I genuinely considered dropping the project altogether. Fortunately, I persevered, and gradually internalized a lot of the information. I also read through the schematics of any open-source Artix-7 development board I could get my hands on. Seeing what other people were doing gave me more confidence that I was doing the right thing.</p><p>Eventually, after I placed all of the components, connected them, ensured all of the nets were labeled, and ran KiCad‚Äôs electrical rules checker (ERC) to find obvious mistakes, I moved on to layout.</p><p>I did PCB layout at the same time as some of the initial enclosure CAD. The mechanics of how everything fit together influenced the placement of the display connector, cartridge slot, buttons, speakers, and connectors. After I came up with a plausible enclosure design, I placed some of the first key components onto the PCB and locked them into place while I did the rest of the routing.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-shell-design.png\" alt=\"Rough enclosure design to help with board layout\" width=\"2000\" height=\"1298\"><figcaption><p>Rough enclosure design to help with board layout</p></figcaption></figure><p>I first focused on components that would be hardest to route. Primarily, the FPGA: the package I was using (CSG324) is a <a href=\"https://en.wikipedia.org/wiki/Ball_grid_array\">BGA</a>, 18x18 with 0.8mm pitch between pins. ‚ÄúFanning out‚Äù all of the I/O signals requires careful routing, and at 0.8mm pitch, it‚Äôs difficult to do this routing with cheap PCB manufacturing techniques. I ended up being able to do this routing with a 6-layer PCB (three signal, two ground, one power), with 0.1mm track width and spacing, and 0.4/0.25mm vias. Fortunately, this is all within the realm of JLCPCB‚Äôs capabilities.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/bga-fanout.png\" alt=\"BGA fanout with thin traces and small vias\" width=\"1442\" height=\"1102\"><figcaption><p>BGA fanout with thin traces and small vias</p></figcaption></figure><p>As I routed signals out from the FPGA to other parts, I assigned those signals to the FPGA pins. Similarly, with the MCU, I assigned signals to pins in a way that made routing easier. Certain signals had restrictions (e.g. on the FPGA, the main 50 MHz clock signal can only go into certain pins, or the configuration bitstream can only go to certain pins, or certain pins are differential pairs for HDMI output), but overall, I had a lot of flexibility with pin assignment.</p><p>KiCad has a feature where it automatically backs up your project as you work on it. I changed the settings to save every 5 minutes and not delete old backups, which allowed me to generate this timelapse of my layout process:</p><figure><figcaption><p>Revision 1 board layout timelapse</p></figcaption></figure><p>Once I finished placing and routing all of the components, I ran the design rules checker (DRC) and fixed issues. I hesitated for a while before sending the PCB for manufacturing. I re-read the schematics, reviewed the layout, and eventually felt confident enough that I was done. I submitted the order to JLCPCB, and after a few questions by their engineers about component placement, they started manufacturing it.</p><h2>Board testing and bring-up<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#board-testing-and-bring-up\">#</a></h2><p>After two weeks or so, I received the assembled boards in the mail:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/rev1-initial-boards.jpg\" alt=\"An assembled board and an unassembled board\" width=\"1440\" height=\"1026\"><figcaption><p>An assembled board and an unassembled board</p></figcaption></figure><p>First, I probed the power rail test points with a multimeter to check for shorts. Then, I plugged the boards in for the first time, and pressed the power button. To my delight, the green LED turned on, indicating that the power button circuit, power path, and 3.3V regulator worked. The microcontroller USB enumerated, and I could see that it logged some errors (since I hadn‚Äôt flashed anything to it yet).</p><p>I intended to write the MCU firmware in Rust, but I did initial board testing and bring-up with <a href=\"https://micropython.org/\">MicroPython</a>. This would let me interactively type in Python and write basic scripts to communicate with the peripherals on the board and make sure I had connected everything correctly. I didn‚Äôt have to worry about writing efficient or well-organized code, and could just focus on functionality.</p><p>I flashed the MicroPython firmware image, and wrote a couple lines of Python to blink the LED. I powered on the FPGA power domain, and checked that the , , and  rails had the correct voltage.</p><p>Next, I wrote a simple bitstream for the FPGA that read the state of the buttons and produced a pattern on the shared signals between the FPGA and the MCU. I wrote simple Python code to configure the FPGA, loaded up the bitstream, and polled the signals from the FPGA. Pressing buttons changed the state, and confirmed that the FPGA was properly powered, and configurable from the MCU.</p><p>After I confirmed the FPGA worked, I started writing a simple display driver to initialize the LCD and push some pixels from the MCU over SPI. The initialization sequence uses a number of LCD-specific parameters (voltages, gamma correction, etc.), that I learned from the LCD manufacturer‚Äôs example code.</p><figure><figcaption><p>(Slowly) pushing pixels to the LCD</p></figcaption></figure><p>The LCD module‚Äôs controller, an ILI9488, has a few quirks: despite claiming that it supports 16-bit colors over SPI, it actually only supports 18-bit colors. This unfortunately meant that the MCU‚Äôs LCD driver would be more inefficient than I expected, since it has to expand 16-bit colors to 18-bit before sending them over the bus. This didn‚Äôt end up being a huge issue, however, because the FPGA is the one driving the display most of the time.</p><p>Another quirk (hardware bug?) is that the ILI9488 doesn‚Äôt stop driving its SPI output line, even when its chip-select signal is inactive. This means that the chip will interfere with any other communication on the bus‚Ä¶ including the FPGA, which sits on the same bus. I never actually needed to read any data back from the LCD (and even if I did, it supports <a href=\"https://en.wikipedia.org/wiki/Serial_Peripheral_Interface#Three-wire\">three-wire SPI</a>), so I just cut the trace between the LCD‚Äôs SDO line and the SPI bus.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/lcd-debugging.jpg\" alt=\"Debugging the LCD test code\" width=\"1440\" height=\"954\"><figcaption><p>Debugging the LCD test code</p></figcaption></figure><h3>Trouble with power domains<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#trouble-with-power-domains\">#</a></h3><p>I started trying to communicate with the I2C peripherals (I/O expander, RTC, etc.), and found that nothing was responding. A bit of probing with a logic analyzer revealed that the SCL/SDA lines were being held low, and that powering on the FPGA power domain let the lines be pulled high and communication to happen.</p><p>I deduced that this was due to the DAC, which had its IOVDD powered by , which likely caused its protection diodes to pull the IO lines (SCL and SDA) low:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dac-iovdd-issue.png\" alt=\"The problematic portion of the schematic\" width=\"1862\" height=\"656\"><figcaption><p>The problematic portion of the schematic</p></figcaption></figure><p>I tested out this theory by cutting the PCB traces connecting the DAC‚Äôs IOVDD and  with a knife. After this, I2C worked even with the FPGA power disabled. Then, I tested a possible fix by adding a wire to power the DAC‚Äôs IOVDD from the  rail. I confirmed that I could still talk to the other I2C devices, and once enabling FPGA power, that I could talk to the DAC too.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dac-iovdd-rework.jpg\" alt=\"DAC IOVDD rework\" width=\"1440\" height=\"1126\"></figure><p>While bringing up the LCD, I saw that the FPGA was also pulling down the shared SPI bus lines while it was unpowered. Not enough to prevent communication with the LCD, but it still wasn‚Äôt great. Between this and the DAC issue, I learned an important EE lesson: be careful when connecting components in different power domains together. A tristate buffer, such as the <a href=\"https://www.ti.com/product/SN74LVC1G125\">74LVC1G125</a>, could have helped here to isolate the buses.</p><p>Once I2C was working, I wrote some basic driver code for the fuel gauge, real-time clock, IMU, and I/O expander, just to check that they all worked correctly. I also checked that the MCU could read from and write to the attached microSD card.</p><h4>Audio and video output from the FPGA<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#audio-and-video-output-from-the-fpga\">#</a></h4><p>Next, I updated my testing FPGA bitstream to output a test pattern over the <a href=\"https://hackaday.com/2024/01/25/displays-we-love-hacking-parallel-rgb/\">LCD parallel interface (‚ÄúDPI‚Äù)</a>, and a test tone to the DAC over the I2S interface. Then, I began poking on the MCU side to configure the LCD controller and DAC appropriately.</p><p>With some amount of trial and error, I convinced the LCD to accept input from the FPGA. Most of the trial and error revolved around the rotation of the LCD module. Soon after, I configured the DAC properly, and it played the test tone from the FPGA over the speakers and the headphones.</p><figure><figcaption><p>WIP video output from the FPGA</p></figcaption></figure><p>At this point, much of the board was working, so I soldered on the rest of the components (cartridge slot, cartridge switch, link port, shoulder buttons).</p><p>With the cartridge slot in place, I had everything I needed to port over the Game Boy emulator from my last project. I did a quick-and-dirty port of the emulator, with some hacking around to connect the core to the audio, video, and the physical cartridge. I was able to play the first Game Boy game on the device far sooner than I was expecting:</p><figure><figcaption><p>Pokemon Silver running from cartridge</p></figcaption></figure><p>I spent the next month or so implementing things on the FPGA. I started on the SPI receiver implementation, so that the MCU and FPGA could communicate.</p><p>It was relatively straightforward to write <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/main/scala/platform/handheld/SpiReceiver.scala\">the initial version</a>, which 4x oversampled the SPI signals from the main system clock. For the Game Boy, that was ~8 MHz, for a maximum SPI speed of 2 MHz. The MicroPython ESP32-S3 SPI implementation supported only single SPI, so that allowed for a maximum transfer speed of 256 KB/s. This was sufficient to do most of my initial testing, but I later <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/main/scala/platform/handheld/SpiReceiverFifo.scala\">wrote an improved SPI receiver</a> to run with an internal 200 MHz clock (from a PLL that turned on and off with the chip-select signal to save power), communicating with the rest of the system via a pair of FIFOs. This added a lot of complexity and edge cases, but it greatly improved performance, allowing the bus to run at 40 MHz.</p><p>I wrote the SPI interface to the FPGA with memory-like semantics: each SPI transfer starts with a command byte, encoding whether it‚Äôs a read or write transfer, the size of each word in the transfer (8, 16, or 32 bits), and whether the ‚Äútarget address‚Äù should autoincrement as the transfer progresses. Then, a 32-bit address, followed by reading or writing the data. Each thing that the MCU might want to access (control registers, blocks of memory) are mapped into the 32-bit address space.</p><p>As with my previous FPGA project, I wrote almost all of the FPGA code in <a href=\"https://www.chisel-lang.org/\">Chisel</a>, a Scala-based HDL. The remaining bits were the top-level Verilog. Chisel made it really simple to parametrize, compose, and test the various modules that I wrote.</p><p>Once I had the SPI receiver working, I wrote controllers for the on-board SRAM and SDRAM. The SRAM was relatively simple (although I still got it slightly wrong at first). The SDRAM was a bit tricky, and even as I write this I‚Äôm not quite satisfied with its performance, and intend to rewrite it in the future.</p><p>I exposed the SRAM and SDRAM interfaces to the MCU via SPI, which allowed me to read and write to these pieces of memory from the MCU. I used this a lot for testing: writing patterns into memory and reading them back to ensure that read and write both worked.</p><p>Side note: SDRAM has to be continuously refreshed, otherwise the stored data decays over time. It depends on the chip, but typically each row has to be read and written back (or auto-refreshed, which does the same thing) at least once every 64 milliseconds to avoid losing state. What I found interesting, however, is that the data can actually persist for quite a bit longer. I discovered that when I was reconfiguring the FPGA between tests, most of the test data that I had previously written would still stick around even without being refreshed. In the first few seconds some bits would start flipping, and over the course of a few minutes, most of what was written was completely unintelligible.</p><p>With the SDRAM controller and SPI receiver written, I was then able to implement the ‚Äúemulated cartridge‚Äù part of the Game Boy emulator, where the MCU reads a ROM file off of the microSD card and sends it to the FPGA to be stored in SDRAM. Then, the FPGA ‚Äúemulates‚Äù a cartridge (rather than interfacing with a real physical cartridge). After a few stupid mistakes, I was able to run test ROMs and homebrew. As an added bonus, since I was using my own SDRAM controller directly, I didn‚Äôt have any of <a href=\"https://eli.lipsitz.net/posts/fpga-gameboy-emulator/#fixing-an-audio-bug-on-the-game-boy-color\">the performance issues I‚Äôd faced before</a> when accessing the ROM stored in memory.</p><h2>Writing the microcontroller firmware in Rust<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#writing-the-microcontroller-firmware-in-rust\">#</a></h2><p>By this point I had tested, in some form or another, all of the different components of the system. I‚Äôm really surprised that everything worked in my first board revision ‚Äì even the rework I did early on wasn‚Äôt actually required for functionality.</p><p>I decided now was a good time to start building an interactive GUI. Up until this point, I had just been running commands in the MicroPython REPL. However, I didn‚Äôt want to build a whole UI in Python just to throw it away later, so I also started working on the ‚Äúproduction‚Äù Rust firmware.</p><p>In the last few years, a lot of progress has been made towards making Rust on the ESP32 chips work well, even on the chips that use the Xtensa ISA. I followed the <a href=\"https://docs.esp-rs.org/book/\">Rust on ESP Book</a> and quickly had an environment set up. I opted for the <a href=\"https://docs.esp-rs.org/book/overview/using-the-standard-library.html\">‚ÄúRust with the Standard Library‚Äù</a> approach, so that I could benefit from <a href=\"https://idf.espressif.com/\">ESP-IDF</a>, especially the built-in support for USB and SD cards with the FAT filesystem.</p><p>I started porting over the drivers I had written in Python. I found embedded Rust to be a bit verbose in some cases, but overall pleasant to use and worth the (little) trouble.</p><p>I starting writing my own minimal GUI framework for basic menus. I poked around with the <a href=\"https://docs.rs/embedded-graphics/latest/embedded_graphics/\"></a> library, but soon found that the typical patterns I was expecting to use weren‚Äôt a great fit for Rust. I also started planning out different screens and realized that I probably actually wanted to use a more comprehensive UI framework.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-gui-main-menu.png\" alt=\"Early main menu screen\" width=\"480\" height=\"320\"></figure><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/early-gui-rom-select.png\" alt=\"Early rom select screen\" width=\"480\" height=\"320\"></figure><p>Ultimately, I settled on <a href=\"https://github.com/slint-ui/slint\">Slint</a>, a Rust-native declarative GUI framework with excellent support for embedded devices. Slint has a custom DSL to describe the UI and composable components. After a bit of practice I found myself to be really productive with it. I enjoyed using Slint, and I‚Äôd use it again in the future. The authors are responsive on GitHub, and the project has steadily improved over the year or so that I‚Äôve been using it.</p><p>There were a few rough edges for my use case, however:</p><ul><li>The built-in GUI elements and examples were all heavily oriented around mouse or touchscreen navigation. Game Bub only has buttons for navigation, however, so I had to make my own widgets (buttons, lists) that worked with key navigation. This involved a few hacks, because Slint‚Äôs focus handling was a little bit simplistic.</li><li>The built-in GUI styles looked (in my opinion) bad on a low DPI screen. Text was excessively anti-aliased and hard to read at small sizes. This was also fixed by building my own widgets.</li><li>Slint doesn‚Äôt have a great story around supporting different ‚Äúscreens‚Äù ‚Äì I had to build some of my own infrastructure to be able to support navigation between the main menu, games, rom select, settings, etc.</li></ul><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-main-menu.png\" alt=\"Main menu\" width=\"480\" height=\"320\"></figure><p>The GUI is rendered on the MCU, and then the rendered framebuffer is sent over to the FPGA. Slint supports partial rendering, where only the parts of the screen that have changed are updated, which improved performance. The FPGA maintains a copy of the framebuffer and ultimately is responsible for driving the display. This has a few advantages over driving the display directly from the MCU:</p><ul><li>Sending a framebuffer at 40 MHz QSPI to the FPGA is 16x faster than sending it to the LCD controller at 10 MHz (the fastest speed supported by the ILI9488)</li><li>The UI is rendered at 240x160 to improve performance and maintain the GBA aesthetic, but the LCD controller doesn‚Äôt have a scaler, so the MCU would have to send 4x the pixels. The FPGA can easily scale the UI framebuffer itself.</li><li>The FPGA can composite the emulator output with a semi-transparent ‚Äúoverlay‚Äù to support an in-game menu, volume / brightness bars, battery notifications, etc.</li><li>An external display (e.g. monitor or TV) can be driven by the FPGA via HDMI</li></ul><p>I spent some time making a variety of firmware improvements, mostly polish and quality-of-life. I added a settings screen to set the date and time, whether to use Game Boy (DMG) or Game Boy Color (CGB) mode when playing Game Boy games, etc.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-settings-early.png\" alt=\"Settings screen\" width=\"480\" height=\"320\"></figure><p>Then I improved the ROM select file browser, and added a battery level indicator.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-load-rom.png\" alt=\"Rom select screen\" width=\"480\" height=\"320\"></figure><p>I also got sick of having to take the microSD card out of the device and connect it to my computer through a series of adapters (microSD to SD to USB-A to USB-C), so I implemented a basic utility to expose the microSD card as a USB Mass Storage Device, using <a href=\"https://github.com/hathach/tinyusb\">TinyUSB</a> and the ESP32-S3‚Äôs USB-OTG capabilities.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/screenshot-usb-storage.png\" alt=\"USB Mass Storage screen\" width=\"480\" height=\"320\"></figure><p>It was a little bit more difficult than I expected, because USB Mass Storage requires the device to provide raw block access. This means that the filesystem has to be unmounted by the device, otherwise the device and host could conflict and corrupt the filesystem. The ESP32-S3 also only supports USB Full Speed, for a practical maximum transfer speed of ~600KB/sec. It‚Äôs really useful for transferring save files or updating the FPGA bitstreams, but less useful for transferring a large number of ROM files.</p><p>Later, I implemented <a href=\"https://gbdev.io/pandocs/MBC7.html\">MBC7</a> support in the Game Boy emulator for Kirby Tilt ‚Äôn Tumble, using the on-board accelerometer.</p><p>After I implemented a decent amount of software functionality, I decided to finish the enclosure design. The bare board just wasn‚Äôt cutting it anymore, and the taped LCD module/loose speakers/rubber-banded battery contraption was fragile.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gamebub-without-shell.jpg\" alt=\"Game Bub looking rough without an enclosure\" width=\"1440\" height=\"1724\"><figcaption><p>Game Bub looking rough without an enclosure</p></figcaption></figure><p>I came into this project without any CAD or 3D printing experience. I looked at a few different CAD software packages, and I ultimately settled on <a href=\"https://www.freecad.org/\">FreeCAD</a>, primarily because it was free and open source. I learned how to use the software with some video tutorials. FreeCAD, unfortunately, was a little bit rough around the edges and I ended up running into some annoying issues. Nevertheless, I powered through and finished the design.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/enclosure-rev1-cad.png\" alt=\"FreeCAD view of the enclosure and some buttons\" width=\"2000\" height=\"1524\"><figcaption><p>FreeCAD view of the enclosure and some buttons</p></figcaption></figure><p>I found parametric modeling, where the geometry of the model is defined by constraints and dimensions, to be intuitive. However overall, I found 3D CAD to be very time consuming. I think a large part of this is my inexperience, but thinking in three dimensions is a lot more difficult than, say, a 2D PCB layout. Creating a full assembly was even more difficult: I had to visualize how the front and rear pieces would fit together, where the screws would go, and how the buttons, screen, speaker, cartridge slot, battery, and ports would all fit in. This project definitely pushed the boundaries of my (previously non-existent) product design skills.</p><p>After finishing the design, I printed out the technical drawing at a 1:1 scale and physically placed the board and other components down as a final check. Then, I sent it to JLCPCB for manufacturing. I opted for <a href=\"https://en.wikipedia.org/wiki/Stereolithography\">SLA resin printing</a>, for high precision and a smooth finish.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/shell-rev1-technical-drawing.png\" alt=\"Enclosure technical drawing\" width=\"1440\" height=\"971\"><figcaption><p>Enclosure technical drawing</p></figcaption></figure><p>After a couple weeks, I got the finished enclosure and custom buttons back.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/enclosure-rev1-outside.jpg\" alt=\"Front and rear half, outside\" width=\"1440\" height=\"1009\"><figcaption><p>Front and rear half, outside</p></figcaption></figure><p>I put the buttons, speakers, and screen into the enclosure, screwed on the PCB, and put the whole thing together.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/assembling-front.jpg\" alt=\"Assembling the front side\" width=\"1440\" height=\"1544\"><figcaption><p>Assembling the front side</p></figcaption></figure><figure><figcaption><p>Game Bub, fully assembled and functional</p></figcaption></figure><p>I wasn‚Äôt sure how dimensionally accurate the 3D printing would be, so I added a lot of extra clearance around the buttons and ports. As it turned out, the printing was very precise, so the buttons rattled around a little in the oversized button holes.</p><p>It‚Äôs a little bit chunky (smaller than an original Game Boy, though!) and the ergonomics aren‚Äôt ideal, but I was really happy to finally have an enclosure. It actually started (sort of) looking like a real product, and I wasn‚Äôt constantly worried about breaking it anymore.</p><h2>Game Boy Advance support<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#game-boy-advance-support\">#</a></h2><p>I won‚Äôt go into all of the details of how I wrote the emulator here (this article is already long enough!). If you‚Äôre interested, my <a href=\"https://eli.lipsitz.net/posts/fpga-gameboy-emulator/\">previous article about my Game Boy FPGA emulator</a> goes into detail about the general process of writing an emulator, and for a high-level introduction to the Game Boy Advance (from a technical perspective), I recommend <a href=\"https://www.copetti.org/writings/consoles/game-boy-advance/\">Rodrigo Copetti‚Äôs article</a>. In general, I tried to implement the emulator the way it might actually have been implemented in the original hardware: each cycle of the FPGA corresponds to one actual hardware cycle (no cheating!).</p><p>As with the Game Boy, I did nearly all of my development with a simulator backed by <a href=\"https://www.veripool.org/verilator/\">Verilator</a> and <a href=\"https://www.libsdl.org\">SDL</a>. By the end of the development process, the simulator was running at about 8% of the real-time speed (on an M3 MacBook Air with excellent single-core performance), which was a bit painful.</p><p>The Game Boy Advance CPU, the <a href=\"https://en.wikipedia.org/wiki/ARM7#ARM7TDMI\">ARM7TDMI</a>, is significantly more complicated than the Game Boy‚Äôs SM83 (a <a href=\"https://en.wikipedia.org/wiki/Zilog_Z80\">Z80</a> / <a href=\"https://en.wikipedia.org/wiki/Intel_8080\">8080</a>-ish hybrid). However, in some ways, it was easier to understand and implement: the ARM7TDMI is much closer to a simple modern processor architecture, and it‚Äôs extensively documented by ARM. For example, the <a href=\"https://developer.arm.com/documentation/ddi0210/c/?lang=en\">ARM7TDMI Technical Reference Manual</a> has block diagrams and detailed cycle-by-cycle instruction timing descriptions.</p><p>I had a lot of fun <a href=\"https://github.com/elipsitz/gamebub/tree/handheld/fpga/src/main/scala/gba/cpu\">implementing the CPU</a>. The architecture has a three-stage pipeline (fetch, decode, execute) ‚Äì a division that feels natural when you implement it in hardware. The ARM7TDMI has two instruction sets: the standard 32-bit ARM instruction set, and the compressed 16-bit THUMB instruction set. I implemented the CPU the way it works in hardware, where the only difference between ARM and THUMB is the decode stage.</p><p>As I was implementing the CPU, I wrote <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/test/scala/gba/cpu/ARM7TDMISpec.scala\">test cases</a> for each instruction. Each test checks the functionality of the instruction: processor state, register values after, as well as the cycle-by-cycle behavior and interaction with the memory bus. This was helpful for catching regressions as I implemented more and more control logic. It was also really satisfying to be able to implement individual instructions, then write the tests, and check that everything worked.</p><p>Chisel made it easy to write out the <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/fpga/src/main/scala/gba/cpu/Control.scala\">CPU control logic</a>. The CPU control logic is a state machine that generates microarchitectural control signals (e.g. bus A should hold the value from the first read register, bus B should hold an immediate value, the memory unit should start fetching the computed address, etc.). Chisel allowed me to collect common functionality into functions (e.g.  to set up the signals to dispatch the next decoded instruction, or  to signal that the pipeline should be flushed and a new instruction should be fetched from the current program counter).</p><p>I found it helpful to draw out timing diagrams with <a href=\"https://wavedrom.com\">WaveDrom</a> when working through instructions, especially to deal with the pipelined memory bus.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/timing-arm7tdmi-branch.svg\" alt=\"My timing diagram of the ARM7TDMI branch instructions\"><figcaption><p>My timing diagram of the ARM7TDMI branch instructions</p></figcaption></figure><p>By mid-May (about a month later), I finished the CPU implementation (with occasional bug fixes after) and moved onto the rest of the system.</p><h3>PPU, MMIO, and everything else<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#ppu-mmio-and-everything-else\">#</a></h3><p>Over the next month and a half, I implemented the majority of the rest of the Game Boy Advance. The CPU interacts with the rest of the system via <a href=\"http://problemkaputt.de/gbatek-gba-i-o-map.htm\">memory-mapped IO (MMIO)</a> registers. Unlike the Game Boy CPU, which can only access memory a single byte at a time, the ARM7TDMI can make 8-bit, 16-bit, and 32-bit accesses. This complicates MMIO, and the different hardware registers and memory regions in the GBA respond to different access widths in different ways.</p><p>I started with the Picture Processing Unit (PPU), which produces the video output. The author of <a href=\"https://github.com/nba-emu/NanoBoyAdvance\">NanoBoyAdvance</a>, fleroviux, had helpfully documented the <a href=\"https://nba-emu.github.io/hw-docs/ppu/ppu.html\">PPU VRAM access patterns</a>, which gave a lot of insight into how the PPU might work internally. <a href=\"https://www.coranac.com/tonc/text/toc.htm\">Tonc</a> was also immensely helpful for implementing the PPU and testing individual pieces of functionality.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/tonc-test.png\" alt=\"(Sort of) running a Tonc PPU demo\" width=\"963\" height=\"699\"><figcaption><p>(Sort of) running a Tonc PPU demo</p></figcaption></figure><p>The PPU took a few weeks, and then I moved onto DMA, followed by hardware timers, and audio. Of course, as I‚Äôd try new tests, demos, and games, I‚Äôd uncover bugs and fix them.</p><figure><figcaption><p>Kirby  in Dream Land</p></figcaption></figure><p>Game Boy and Game Boy Advance cartridges use the same 32-pin connector. However, they work very differently. The Game Boy cartridge bus is asynchronous: the game outputs the 16-bit address (64 KiB address space) on one set of pins and lowers the  pin. Some time later, the 8-bit read data from the ROM stabilizes on a separate set of pins.</p><p>For the GBA, Nintendo extended the bus data width to 16-bit and the address space to 25-bit (32 MiB). However, they kept roughly the same set of pins, accomplishing this by multiplexing the 24 data/address pins: the console outputs the address (in increments of the data word size of 16-bits, for a 24-bit physical address), then lowers the  signal to ‚Äúlatch‚Äù the address in the cartridge. Then, each time the console pulses the  pin, the cartridge increments its latched address and outputs the next data over the same pins. This allows for a continuous read of sequential data without having to send a new address for each access. The GBA also allows games to <a href=\"http://problemkaputt.de/gbatek-gba-system-control.htm\">configure cartridge access timings</a> to support different ROM chips.</p><p>I had to do a lot of my own research here. Software emulators don‚Äôt need to care about the precise timing of the cartridge bus, so there wasn‚Äôt much documentation. To figure out the exact cycle-accurate timing, I used a <a href=\"https://www.saleae.com/\">Saleae</a> logic analyzer and connected it to the cartridge bus. I wrote a test program for the GBA to do different types of accesses (reads, writes, sequential, non-sequential, DMA) with different timing configurations.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/cartridge-bus-analyzer.jpg\" alt=\"Cartridge bus analysis setup\" width=\"1440\" height=\"1080\"><figcaption><p>Cartridge bus analysis setup</p></figcaption></figure><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/trace-gba-cartridge.png\" alt=\"Portion of a trace\" width=\"2582\" height=\"734\"></figure><p>After coming up with numerous scenarios (especially around the interaction between DMA and the CPU, and starting and stopping burst accesses), I came up with a consistent model for how cartridge accesses worked. I created some timing diagrams to help:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/cartridge-timing-diagram.svg\" alt=\"Timing diagram of a non-sequential access followed by a sequential access\"><figcaption><p>Timing diagram of a non-sequential access followed by a sequential access</p></figcaption></figure><p>Finally, I started implementing the cartridge controller state machine based on my observations, paired with an emulated cartridge implementation. With the emulated cartridge, I was able to properly run real games in the simulator.</p><p>I quickly implemented physical cartridge support, to be able to finally run it on the actual FPGA. I connected the signals, built a new bitstream, and‚Ä¶ it didn‚Äôt work at all. The Game Boy Advance boot screen ran, but it didn‚Äôt get any further than that. I implemented the emulated cartridge on the FPGA (reading ROM files from the SD card), and it worked! Which was great, but physical cartridges still didn‚Äôt.</p><p>I used the logic analyzer to observe how my emulator was interacting with the cartridge compared to how an actual GBA, and found numerous issues.</p><p>One of the first things I noticed was short <a href=\"https://en.wikipedia.org/wiki/Glitch#Electronics_glitch\">glitches</a> on the  line. I knew these had to be glitches (rather than incorrect logic), because they were 8 nanoseconds long, much shorter than the ~59.6ns clock period. Since the cartridge latches the address on a falling edge of , glitches cause it to latch an address when it shouldn‚Äôt, screwing up reads.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/cartridge-bus-glitches.png\" alt=\"Glitches on the cartridge bus\" width=\"2177\" height=\"600\"><figcaption><p>Glitches on the cartridge bus</p></figcaption></figure><p>Here, I learned an important lesson in digital design: output signals should come directly from flip-flops, with no logic in between.</p><p>After each flip-flop outputs a new value (on the rising edge of the clock), the signals propagate through the chip. As they propagate, taking different paths of different lengths throughout the chip, the output from each lookup table (LUT) is unstable. These values only stabilize near the end of the clock cycle (assuming the design met timing closure), and then each flip-flop stores the stable value at the next rising edge. If you output a signal from logic, this instability is visible from outside of the chip, manifesting as glitches in the output signal. If you instead output the signal from a flip-flop, it‚Äôll change only on each clock edge, remaining stable in the middle.</p><p>And of course, I had written the cartridge controller without thinking about this, and  of the output signals were generated from logic. I rewrote the controller to output everything from flip-flops, which had a series of cascading changes since all of the signals now had to be computed one clock cycle earlier than I expected.</p><p>There were other issues too ‚Äì part of the problem was that my emulated cartridge model was too permissive, and didn‚Äôt catch some fairly obvious incorrect behavior. After a few days of intensive debugging with the logic analyzer, I got to the point where I could play games from physical cartridges.</p><figure><figcaption><p>Metroid: Zero Mission running from the cartridge</p></figcaption></figure><h4>Cartridge prefetch buffer<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#cartridge-prefetch-buffer\">#</a></h4><p>The ARM7TDMI has a single shared instruction and data memory bus. As a result, a long series of sequential memory accesses is rare. Even a linear piece of code without branches that includes ‚Äúload‚Äù or ‚Äústore‚Äù instructions would produce a series of non-sequential memory accesses, as the CPU fetches an instruction from one location, loads a register from a different location, and then goes back to fetching the next instruction.</p><p>This poses a real performance issue on the GBA, because every non-sequential access from the cartridge incurs a multi-cycle penalty. Nintendo attempted to mitigate this somewhat with the ‚Äúprefetch buffer‚Äù (<a href=\"https://mgba.io/2015/06/27/cycle-counting-prefetch/#game-pak-prefetch\">read this post by endrift, the author of mGBA, for more details</a>) which attempts to keep a cartridge read burst active between CPU accesses. Without emulating the prefetch buffer, some games lag (I noticed this the most in Mario Kart Super Circuit, and some rooms of Metroid: Zero Mission).</p><p>The prefetch buffer, while simple in theory, is not well documented and has a lot of corner cases and weird interactions. Emulator developers often start by taking a shortcut: making all cartridge accesses take a single cycle when the prefetch buffer is enabled. This wouldn‚Äôt work for me, since I actually had to interface with the physical cartridge.</p><p>So, I set out to do some more research to figure out exactly how the prefetch buffer worked. After making some educated guesses and tests, I came up with a reasonable model of how it might work.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/prefetch-notes.jpg\" alt=\"Notes about the prefetch state machine\" width=\"1440\" height=\"1744\"><figcaption><p>Notes about the prefetch state machine</p></figcaption></figure><p>Actually implementing it took a lot of work, and I kept stumbling upon more and more corner cases. Eventually I got to the point where all games appeared to run at full speed, and most importantly, didn‚Äôt randomly crash. My implementation isn‚Äôt perfect: there are still a few <a href=\"https://github.com/mgba-emu/suite\">mGBA test suite</a> timing tests I don‚Äôt pass, but it‚Äôs certainly sufficient to play games.</p><ul><li>: standard duplex SPI, used for communicating with accessories</li><li>: custom multi-drop UART-like protocol, used to link up to four GBAs together for multiplayer games</li><li>: the Nintendo N64 and GameCube controller protocol, used to connect to a GameCube</li><li>: duplex <a href=\"https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter\">UART</a> with flow control, </li><li>: controlling the four pins individually as <a href=\"https://en.wikipedia.org/wiki/General-purpose_input/output\">GPIO</a>, </li></ul><p>The timing of these isn‚Äôt well documented, so I did my own research.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gba-link-multiplayer-transfer.png\" alt=\"A multiplayer mode transfer with no attached consoles\" width=\"2012\" height=\"558\"><figcaption><p>A  mode transfer with no attached consoles</p></figcaption></figure><p>I did a lot of testing with examples from the <a href=\"https://github.com/afska/gba-link-connection\">gba-link-connection</a> library, intended for homebrew GBA games, but helpful for testing the different transfer modes in a controlled environment.</p><figure><figcaption><p>Multiplayer Mario Kart with Game Bub and a GBA</p></figcaption></figure><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/animal-crossing-gamecube-link.jpg\" alt=\"Game Bub linked to a GameCube playing Animal Crossing\" width=\"1440\" height=\"1801\"><figcaption><p>Game Bub linked to a GameCube playing Animal Crossing</p></figcaption></figure><p>During the emulator development, I had used various test ROMs (mentioned before) to test basic functionality in isolation. As my emulator became mature enough to run commercial games, however, I started to shift some of my focus to accuracy-focused test ROMs.</p><p>These test ROMs (such as the <a href=\"https://github.com/mgba-emu/suite\">mGBA test suite</a>) generally test really specific hardware quirks and timing. For example, they might test what happens when you run an instruction that ARM calls ‚Äúunpredictable‚Äù, or the exact number of cycles it takes to service an interrupt in specific scenarios, or <a href=\"https://bmchtech.github.io/post/multiply/\">the value of the ‚Äúcarry‚Äù flag after performing a multiplication</a>. These are the kinds of things that don‚Äôt actually matter for playing games, but present a fun challenge and a way to ‚Äúscore‚Äù your emulator against others. This also highlights the collaborative nature of the emulation development community: people sharing their research and helping each other out.</p><p>I won‚Äôt talk about all of the tests here (for my emulator‚Äôs test results, <a href=\"https://github.com/elipsitz/gamebub/blob/handheld/docs/accuracy.md\">see this page</a>). But I do want to mention the <a href=\"https://tcrf.net/AGS_Aging_Cartridge\"></a>. This is an official test cartridge from Nintendo, likely used as part of a factory test or RMA procedure. Apparently, Nintendo has  used it to test their emulators (e.g. their GBA emulator on the Nintendo Switch). This test has generally been considered to be difficult to pass (it tests some specific hardware quirks), but it‚Äôs easier now that the tests have been <a href=\"https://github.com/DenSinH/AGSTests/\">thoroughly reverse engineered and documented</a>. Still, passing it is a nice milestone:</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/agb-aging-cartridge.png\" alt=\"Passing the AGB Aging Cartridge\" width=\"964\" height=\"700\"><figcaption><p>Passing the AGB Aging Cartridge</p></figcaption></figure><h2>Second hardware revision<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#second-hardware-revision\">#</a></h2><p>Towards the end of 2024, approximately one year after I originally designed Game Bub, I decided to make a second hardware revision. Over the past year, I had been keeping track of all of the things I would want to change in a future revision. Since the first version of Game Bub miraculously worked without any major issues, this list was primarily minor issues and ergonomics changes.</p><p>I fixed the <a href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#trouble-with-power-domains\">minor I2C power issues</a>, removed the <a href=\"https://en.wikipedia.org/wiki/Reference_designator\">reference designators</a> from the PCB silkscreen (they looked messy with the dense board, and I didn‚Äôt use them for anything anyway), and changed around some test points. I improved the rumble circuit to be more responsive, and switched to a PCB-mounted vibration motor.</p><p>The first version of Game Bub was fairly thick, measuring 12.9mm at the top and 21.9mm on the bottom. The thickness of the rear enclosure was dictated by the thickness of Game Boy cartridges, but I made several changes to the front. I moved the  (8.5mm!) link port to the back, and removed the HDMI port (more on that later). I changed the headphone jack (5.0mm tall ‚Äì no wonder they started getting removed from phones) to a mid-mount one that sunk into the PCB and reduced the overall height.</p><p>I also switched from an  module (3.1mm depth) to an  (2.4mm depth). I should have done this from the beginning, I just didn‚Äôt even know the ESP32-S3-MINI existed. This had the side effect of giving me 3 more GPIOs, which allowed me to put the FPGA and LCD on separate SPI busses, avoiding the minor issue of an unpowered FPGA interfering with LCD communication, and allowed for faster boot because the LCD could be configured at the same time as the FPGA.</p><p>I switched the speakers, from the fully-enclosed <a href=\"https://www.sameskydevices.com/product/audio/speakers/miniature-(10-mm~40-mm)/ces-20134-088pmb\">CES-20134-088PMB</a> to the <a href=\"https://www.sameskydevices.com/product/audio/speakers/miniature-(10-mm~40-mm)/cms-160903-18s-x8\">CMS-160903-18S-X8</a>. I made this change primarily for ease of assembly. The first speaker had a wire connector that plugged into the board, and I found it difficult to connect during assembly without having the wire interfere with buttons. The new speaker is smaller and has a spring contact connector, so it just presses against the PCB as the device is assembled. This required some speaker enclosure design ‚Äì an unenclosed speaker in free air sounds quiet and tinny.</p><p>I reworked the layout of the face buttons and D-pad to match the spacing of the Nintendo DSi. This allowed me to use the silicone membranes from the DSi for an improved button feel and reduced rattling. I was also hoping to use the plastic buttons from the DSi (which were higher quality compared to my 3D printed buttons), but even with the new thinner design, the buttons weren‚Äôt quite tall enough to be easily pressed.</p><p>I created another timelapse of my modifications to produce the second version of the PCB:</p><figure><figcaption><p>Revision 2 board layout timelapse</p></figcaption></figure><p>For the second revision of the enclosure, I switched to <a href=\"https://en.wikipedia.org/wiki/Fusion_360\">Fusion 360</a> for the CAD work. While I would have preferred to keep using FreeCAD, I found that it was making it harder for me to be productive. Fusion 360 has a free version for hobbyists (with some limitations that have gradually increased over time), and overall I‚Äôve found it very pleasant to use.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/enclosure-rev2-cad.png\" alt=\"Fusion 360 view of the second enclosure, fully assembled\" width=\"1774\" height=\"1472\"><figcaption><p>Fusion 360 view of the second enclosure, fully assembled</p></figcaption></figure><p>Unlike with the first revision, I waited until I had a final design for both the enclosure and the PCB before getting anything manufactured. This let me go back and forth, making small modifications to each of them as needed.</p><p>I wanted to make the end result look more polished and professional, so I contracted a factory to produce custom LCD cover glass, made out of 0.7mm thick tempered glass with a black silkscreen. It was relatively expensive for a low quantity order, but I‚Äôm really happy with how it turned out.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/lcd-cover-glass.jpg\" alt=\"Custom LCD cover glass with adhesive backing\" width=\"1440\" height=\"1369\"><figcaption><p>Custom LCD cover glass with adhesive backing</p></figcaption></figure><h3>Manufacturing and assembly<a hidden=\"\" aria-hidden=\"true\" href=\"https://eli.lipsitz.net/posts/introducing-gamebub/#manufacturing-and-assembly\">#</a></h3><p>I got the PCBs manufactured and assembled, this time with black solder mask to look .</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/pcb-rev-2.jpg\" alt=\"Assembled PCB, revision 2\" width=\"1440\" height=\"1073\"><figcaption><p>Assembled PCB, revision 2</p></figcaption></figure><p>I had two enclosures made. The first was black PA-12 Nylon, printed with <a href=\"https://en.wikipedia.org/wiki/Multi-jet_fusion\">MJF</a>. Nylon is strong and durable, and the MJF 3D printing technology produces a slightly grainy surface that‚Äôs really pleasant to hold in your hand.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/nylon-closeup.jpg\" alt=\"Closeup of the nylon grainy texture\" width=\"1440\" height=\"894\"><figcaption><p>Closeup of the nylon grainy texture</p></figcaption></figure><p>The second one was made of transparent resin (SLA, like before). This lets me show off the PCB that I worked so hard on, and evokes the transparent electronics trend from the 90s.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gamebub-transparent.jpg\" alt=\"Transparent Game Bub\" width=\"1440\" height=\"1920\"></figure><p>Assembly was a lot easier this time around: the silicone membranes held the face buttons in place, the speakers had a spring contact instead of wires, and the shoulder button assembly was better. In the first revision, I had excessively large tolerances because I wasn‚Äôt sure how precise the 3D printing would be. In the second version, I was able to shrink these.</p><p>The final product looked and felt a lot better, too. The edges were more rounded, and the device was thinner and easier to hold. The buttons felt  better to press and didn‚Äôt rattle around, and the cover glass over the LCD added polish.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/gamebubs-side-by-side.jpg\" alt=\"First revision (left), second revision (center and right)\" width=\"1440\" height=\"742\"><figcaption><p>First revision (left), second revision (center and right)</p></figcaption></figure><p>I previously mentioned that I removed the full-size HDMI port from the first revision. I had first planned to change it to a mini-HDMI or micro-HDMI port to reduce the size, but I was worried about durability.</p><p>What I  wanted to do was output video through the USB-C port, avoiding the need for any HDMI port at all. Unfortunately, I had already concluded earlier that I wouldn‚Äôt be able to output <a href=\"https://en.wikipedia.org/wiki/DisplayPort\">DisplayPort</a> video signals from the FPGA, which meant that I couldn‚Äôt use the standard USB-C DisplayPort alternate mode.</p><p>However, an idea struck me towards the end of 2024: I didn‚Äôt actually  to use the DisplayPort alt-mode. The USB-C connector, in addition to the USB 2.0 D+/D- pins, has four differential pairs (for USB superspeed). Conveniently, HDMI  uses four differential pairs. The USB specification allows for vendor-specific alt-modes, so I could just implement my own, outputting the HDMI signal directly from the FPGA over the additional pins. Then I could build a custom dock that takes those pins and connects them to the data lines of an HDMI port.</p><p>According to the USB specification, alternate modes must be negotiated by both sides first, using the USB-C Power Delivery (USB-PD) protocol, to prevent them from interfering with devices that aren‚Äôt expecting them. I don‚Äôt actually have a USB-PD controller in Game Bub (too much added complexity), so I took a shortcut: have a microcontroller in the dock communicate with the Game Bub over regular USB and perform a handshake before enabling HDMI output from the FPGA. Once Game Bub detects that it‚Äôs been disconnected from the dock, it can just switch back to using the internal display.</p><p>I realized that the dock also presents another opportunity for controller support. I originally wanted to build wireless controller support into the handheld, but the ESP32-S3 only supports Bluetooth Low Energy, and the majority of controllers use Bluetooth Classic. Fortunately, the <a href=\"https://en.wikipedia.org/wiki/Raspberry_Pi#Raspberry_Pi_Pico\">Raspberry Pi Pico W</a> (with an RP2040 MCU) supports both types of Bluetooth, so I just decided to use that as the microcontroller on the dock. Game controllers connect to the dock over Bluetooth, and the Pico sends the controller inputs to the device. I wired up the  and  USB-C pins as a direct connection between the FPGA and the dock for low latency input.</p><p>The RP2040 acts as the USB host, and Game Bub only needs to be a device. I also added a USB hub chip and some additional USB ports on the back of the dock to allow for wired controller support too. Just like with wireless controllers, the dock handles the direct controller communication, and just passes inputs back to the main Game Bub unit.</p><p>Since the dock is so simple (comparatively), it only took about a day to design and lay out.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dock-pcb.jpg\" alt=\"Assembled dock PCB\" width=\"1440\" height=\"1016\"></figure><p>I had also hoped to use the dock to solve another problem around HDMI output: HDMI sinks (monitors, TVs) pull the HDMI data lines up to 3.3 volts, and can actually backfeed power to the HDMI source. For Game Bub, this meant that a powered-off unit would turn itself on when connected over HDMI. I used a HDMI buffer chip in the dock to try to alleviate this problem, but the chip I used wasn‚Äôt actually properly suited to this use-case and interfered with video output, so I had to carefully rework the board to bypass the chip. I‚Äôll have to fix it in a later revision.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/dock-rework.jpg\" alt=\"Bypassing the HDMI buffer chip\" width=\"1440\" height=\"1359\"><figcaption><p>Bypassing the HDMI buffer chip</p></figcaption></figure><p>After the rework, HDMI output worked! The rest of the features are still a work in progress.</p><figure><img loading=\"lazy\" src=\"https://eli.lipsitz.net/posts/introducing-gamebub/media/bare-gamebub-in-dock.jpg\" alt=\"Game Bub PCB on the dock, connected to an external monitor\" width=\"1440\" height=\"2116\"><figcaption><p>Game Bub PCB on the dock, connected to an external monitor</p></figcaption></figure><p>Congratulations on reading this far! This writeup ended up being incredibly long, even with a lot of details left out.</p><p>I‚Äôm proud of what I accomplished over the last year and a half: I met all of my goals to produce a polished handheld FPGA retrogaming device. I pushed my electrical engineering and product design skills to the limit, and learned a lot in the process. Professional product and hardware designers deserve  respect.</p><p>I deliberately designed this project with lots of possible extension opportunities to keep me occupied for a long time. I worked hard to get to the point where I‚Äôm comfortable sharing Game Bub with the world, but I still have a long list of TODOs for the future.</p><p>In the near term, I‚Äôm going to work on finishing the dock, implementing wireless controller support (and maybe wired). I plan to use the <a href=\"https://github.com/ricardoquesada/bluepad32\">Bluepad32</a> library to do so.</p><p>I also want to improve the accuracy of my Game Boy Advance emulator: my goal here is to someday pass the entire <a href=\"https://github.com/mgba-emu/suite\">mGBA test suite</a>. I hope that I can contribute back to the wonderful  community with my emulator, and I plan to write-up some of my research around the GBA cartridge interface and link port.</p><p>I have a long list of mostly minor changes to make to the MCU firmware: improving UI render performance, bits of polish like low battery notifications, eliminating display glitching when reloading the FPGA, and that sort of thing. I also plan to add more utilities, like a cartridge dumper and save backup/restore feature.</p><p>Some day, I want to emulate the <a href=\"https://en.wikipedia.org/wiki/Game_Boy_Advance_Wireless_Adapter\">Game Boy Advance Wireless Adapter</a> over Wi-Fi, e.g. with <a href=\"https://docs.espressif.com/projects/esp-idf/en/stable/esp32s3/api-reference/network/esp_now.html\">ESP-NOW</a>. This won‚Äôt be compatible with the original wireless adapter, unfortunately, since that uses raw 2.4 GHz modulation rather than Wi-Fi.</p><p>I designed Game Bub with extremely low production volumes in mind, using off-the-shelf commodity parts to keep the overall cost down. However, there are a few things I would have liked to be able to do, but are only possible with much higher volumes:</p><ul><li>A better LCD module (likely custom): native landscape mode to avoid the need for triple-buffering. Ideally a 720x480 resolution display, to allow for 3x GBA scaling and filter effects.</li><li>High-quality injection molded case and buttons: 3D printing is great for low volume production, but an injection molded case would be great. It would be more precise (allowing for tighter tolerances), stronger, and allow for significantly more color options.</li><li>Custom battery pack: or at least customizing the length of the connector wire. The current solution is hacky and doesn‚Äôt make the best use of internal space, due to limited off-the-shelf battery options.</li><li>Smaller BGA parts for SRAM and SDRAM to free up board space (and move internal signals to 1.8 volts): this is actually something that would be possible in smaller volumes too, if I were willing to send parts from Mouser or DigiKey to JLCPCB for assembly.</li></ul>","contentLength":69379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43027335"},{"title":"EuroPython Society: Board Report for January 2025","url":"https://www.europython-society.org/board-report-for-january-2025/","date":1739372917,"author":"","guid":276,"unread":true,"content":"<p>The top priority for the board in January was finishing the hiring of our event manager. We‚Äôre super excited to introduce Ane≈æka M√ºller! Ane≈æka is a freelance event manager and a longtime member of the Czech Python community. She‚Äôs a member of the Pyvec board, co-organizes PyLadies courses, PyCon CZ, Brno Pyvo, and Brno Python Pizza. She‚Äôll be working closely with the board and OPS team, mainly managing communication with service providers. Welcome onboard! </p><p>Our second priority was onboarding teams. We‚Äôre happy that we already have the Programme team in place‚Äîthey started early and launched the Call for Proposals at the beginning of January. We‚Äôve onboarded a few more teams and are in the process of bringing in the rest.</p><p>Our third priority was improving our grant programme in order to support more events with our limited budget and to make it more clear and transparent. We went through past data, came up with a new proposal, discussed it, voted on it, and have already published it on our <a href=\"https://www.europython-society.org/changes-in-the-grants-programme-for-2025/\">blog</a>. </p><ul><li>Updating onboarding/offboarding checklists for Volunteers and Board Members</li><li>Various infrastructure updates including new website deployment and self-hosted previews for Pull Requests to the website.</li><li>Setting up EPS AWS account.</li><li>Working out the Grant Guidelines update for 2025</li><li>Attending PyConWeb and FOSDEM</li><li>Reviewing updates to the Sponsors setup and packages for 2025</li><li>More documentation, sharing know-how and reviewing new proposals.</li></ul><ul><li>Brand strategy: Analysis of social media posts from previous years and web analytics. Call with a European open-source maintainer and a call with a local events organizer about EP content.</li><li>Comms &amp; design: Call for proposal announcements, EP 2024 video promotions, speaker mentorship, and newsletter. Video production - gathering videos from speakers, video post-production, and scheduling them on YouTube shorts, and social media.</li><li>Event management coordination: Calls with the event manager and discussions about previous events.</li><li>Grants: Work on new grant guidelines and related comms.</li><li>Team onboarding: Calls with potential comms team members and coordination.</li><li>PR: Delivering a lightning talk at FOSDEM.</li></ul><ul><li>Offboarding the old board</li><li>Onboarding new team members</li><li>Administrative work on Grants</li></ul><ul><li>Worked on the Grants proposal</li><li>Follow-up with team members</li><li>Community outreach: FOSDEM</li></ul><ul><li>Working on various infrastructure updates, mostly related to the website.</li><li>Reviewing Pull Requests for the website and the internal bot</li><li>Working on the infrastructure team proposal.</li></ul><ul><li>Timeline: Discussion with the Programme Team, and planning to do the same with the other teams.</li><li>Visa Request letter: Setup and Test Visa Request Automation for the current year</li><li>Team selection discussion with past volunteers</li></ul>","contentLength":2708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python Morsels: Avoid over-commenting in Python","url":"https://www.pythonmorsels.com/avoid-comments/","date":1739372739,"author":"","guid":275,"unread":true,"content":"<h2>Documenting instead of commenting</h2><p>Here is a comment I would not write in my code:</p><div><pre><code></code></pre></div><p>That comment seems to describe what this code does... so why would I  write it?</p><p>I do like that comment, but I would prefer to write it as a <a href=\"https://www.pythonmorsels.com/docstrings/\" target=\"_blank\">docstring</a> instead:</p><div><pre><code></code></pre></div><p>Documentation strings are for conveying the purpose of function, class, or module, typically at a high level.\nUnlike comments, they can be read by Python's built-in  function:</p><div><pre><code></code></pre></div><p>Docstrings are also read by other documentation-oriented tools, like <a href=\"https://www.sphinx-doc.org\" target=\"_blank\">Sphinx</a>.</p><h2>Non-obvious variables and values</h2><p>Here's a potentially helpful comment:</p>","contentLength":557,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta‚Äôs New AI: Outrageously Good!","url":"https://www.youtube.com/watch?v=m6aaQoPv5r8","date":1739372626,"author":"Two Minute Papers","guid":521,"unread":true,"content":"<article>‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nüìù The paper \"VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models\" is available here:\nhttps://hila-chefer.github.io/videojam-paper.github.io/\n\nVs Veo2: https://x.com/TomLikesRobots/status/1888279188336963725\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu</article>","contentLength":1174,"flags":null,"enclosureUrl":"https://www.youtube.com/v/m6aaQoPv5r8?version=3","enclosureMime":"","commentsUrl":null},{"title":"5 LLM Prompting Techniques Every Developer Should Know","url":"https://www.kdnuggets.com/5-llm-prompting-techniques-every-developer-should-know","date":1739372427,"author":"Bala Priya C","guid":587,"unread":true,"content":"<article>Want to make the most out of large language models? Check out these prompting techniques you can start using today.</article>","contentLength":115,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/llm-prompting.png","enclosureMime":"","commentsUrl":null},{"title":"Real Python: Python Keywords: An Introduction","url":"https://realpython.com/python-keywords/","date":1739368800,"author":"","guid":184,"unread":true,"content":"<p>Python keywords are reserved words with specific functions and restrictions in the language. Currently, Python has thirty-five keywords and four soft keywords. These keywords are always available in Python, which means you don‚Äôt need to import them. Understanding how to use them correctly is fundamental for building Python programs.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li>There are  and  in Python.</li><li>You can get a list of all keywords using  from the  module.</li><li> in Python act as keywords only in specific contexts.</li><li> are keywords that have been deprecated and turned into functions in Python 3.</li></ul><p>In this article, you‚Äôll find a basic introduction to all Python keywords and soft keywords along with other resources that will be helpful for learning more about each keyword.</p><div><p> Test your knowledge with our interactive ‚ÄúPython Keywords: An Introduction‚Äù quiz. You‚Äôll receive a score upon completion to help you track your learning progress:</p><div><div><a href=\"https://realpython.com/quizzes/python-keywords/\"></a><p>In this quiz, you'll test your understanding of Python keywords and soft keywords. These reserved words have specific functions and restrictions in Python, and understanding how to use them correctly is fundamental for building Python programs.</p></div></div></div><p>Python keywords are special reserved words that have specific meanings and purposes and can‚Äôt be used for anything but those specific purposes. These keywords are always available‚Äîyou‚Äôll never have to import them into your code.</p><p>Python keywords are different from Python‚Äôs <a href=\"https://docs.python.org/3/library/functions.html\">built-in functions and types</a>. The built-in functions and types are also always available, but they aren‚Äôt as restrictive as the keywords in their usage. </p><p>An example of something you  do with Python keywords is assign something to them. If you try, then you‚Äôll get a . You won‚Äôt get a  if you try to assign something to a built-in function or type, but it still isn‚Äôt a good idea. For a more in-depth explanation of ways keywords can be misused, check out <a href=\"https://realpython.com/invalid-syntax-python/#misspelling-missing-or-misusing-python-keywords\">Invalid Syntax in Python: Common Reasons for SyntaxError</a>.</p><p>There are <a href=\"https://docs.python.org/3/reference/lexical_analysis.html#keywords\">thirty-five keywords</a> in Python. Here‚Äôs a list of them, each linked to its relevant section in this tutorial:</p><p>Two keywords have additional uses beyond their initial use cases. The  keyword is also <a href=\"https://realpython.com/atom.xml#the-else-keyword-used-with-loops\">used with loops</a> and <a href=\"https://realpython.com/atom.xml#the-else-keyword-used-with-try-and-except\">with  and </a> in addition to in conditional statements. The  keyword is most commonly used in  statements, but also used <a href=\"https://realpython.com/atom.xml#the-as-keyword-used-with-with\">with the  keyword</a>.</p><p>The list of Python keywords and soft keywords has changed over time. For example, the  and  keywords weren‚Äôt added until Python 3.7. Also, both  and  were keywords in Python 2.7 but were turned into built-in functions in Python 3 and no longer appear in the keywords list.</p><p>As mentioned above, you‚Äôll get an error if you try to assign something to a Python keyword. Soft keywords, on the other hand, aren‚Äôt that strict. They syntactically act as keywords only in certain conditions.</p><p>This new capability was made possible thanks to the introduction of the <a href=\"https://realpython.com/python39-new-features/#a-more-powerful-python-parser\">PEG parser</a> in Python 3.9, which changed how the interpreter reads the source code.</p><p>Leveraging the PEG parser allowed for the introduction of <a href=\"https://realpython.com/structural-pattern-matching/\">structural pattern matching</a> in Python. In order to use intuitive syntax, the authors picked , , and  for the pattern matching statements. Notably,  and  are widely used for this purpose in many other programming languages.</p><p>To prevent conflicts with existing Python code that already used , , and  as variable or function names, Python developers decided to introduce the concept of soft keywords.</p><p>Currently, there are four  in Python:</p><p>You can use the links above to jump to the soft keywords you‚Äôd like to read about, or you can continue reading for a guided tour.</p><h2>Value Keywords: , , </h2><p>There are three Python keywords that are used as values. These values are <a href=\"https://python-patterns.guide/gang-of-four/singleton/\">singleton</a> values that can be used over and over again and always reference the exact same object. You‚Äôll most likely see and use these values a lot.</p><p>There are a few terms used in the sections below that may be new to you. They‚Äôre defined here, and you should be aware of their meaning before proceeding:</p>","contentLength":4020,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EuroPython Society: Changes in the Grants Programme for 2025","url":"https://www.europython-society.org/changes-in-the-grants-programme-for-2025/","date":1739366190,"author":"","guid":274,"unread":true,"content":"<ul><li>We are increasing transparency and reducing ambiguity in the guidelines.</li><li>We would like to support more events with our limited budget</li><li>We‚Äôve introduced caps for events in order to make sure all grants are fairly given and we can support more communities.</li><li>We‚Äôve set aside 10% of our budget for the local community. </li></ul><p>The EPS introduced a Grant Programme in 2017. Since then, we have granted almost EUR 350k through the programme, partly via EuroPython Finaid and by directly supporting other Python events and projects across Europe. In the last two years, the Grant Programme has grown to EUR 100k per year, with even more requests coming in.</p><p>With this growth come new challenges in how to distribute funds fairly so that more events can benefit. Looking at data from the past two years, we‚Äôve often been close to or over our budget. The guidelines haven‚Äôt been updated in a while. As grant requests become more complex, we‚Äôd like to simplify and clarify the process, and better explain it on our website.</p><p>We would also like to acknowledge that EuroPython, when traveling around Europe, has an additional impact on the host country, and we‚Äôd like to set aside part of the budget for the local community.</p><p>The Grant Programme is also a primary funding source for EuroPython Finaid. To that end, we aim to allocate 30% of the total Grant Programme budget to Finaid, an increase from the previous 25%.</p><ul><li>We‚Äôve updated the <a href=\"https://www.europython-society.org/grants\">text on our website</a>, and split it into multiple sub-pages to make it easier to navigate. The website now includes a checklist of what we would like to see in a grant application, and a checklist for the Grants Workgroup ‚Äì so that when you apply for the Grant you already know the steps that it will go through later and when you can expect an answer from us.</li><li>We looked at the data from previous years, and size and timing of the grant requests. With the growing number and size of the grants, to make it more accessible to smaller conferences and conferences happening later in the year, we decided to introduce max caps per grant and split the budget equally between the first and second half of the year. We would also explicitly split the total budget into three categories ‚Äì 30% goes to the EuroPython finaid, 10% is reserved for projects in the host country. The remaining 60% of the budget goes to fund other Python Conferences. This is similar to the split in previous years, but more explicit and transparent.</li></ul><p>Using 2024 data, and the budget available for Community Grants (60% of total), we‚Äôve simulated different budget caps and found a sweet spot at 6000EUR, where we are able to support all the requests with most of the grants being below that limit. For 2025 we expect to receive a similar or bigger number of requests.</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><img src=\"https://www.europython-society.org/content/images/2025/02/image-1.png\" alt=\"alt\" width=\"1418\" height=\"496\"><p>We are introducing a special 10% pool of money to be used on projects in the host country (in 2025 that‚Äôs again Czech Republic). This pool is set aside at the beginning of the year, with one caveat that we would like to deploy it in the first half of the year. Whatever is left unused goes back to the Community Pool to be used in second half of the year.</p><ul><li>Fairer Funding: By spreading our grants out during the year, conferences that happen later won‚Äôt miss out.</li><li>Easy to Follow: Clear rules and deadlines cut down on confusion about how much you can get and what it‚Äôs for.</li><li>Better Accountability: We ask for simple post-event reports so we can see where the money went and what impact it made.</li><li>Stronger Community: Funding more events grows our Python network across Europe, helping everyone learn, connect, and collaborate.</li></ul>","contentLength":3581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 5 Freelancer Websites Better Than Fiverr and Upwork","url":"https://www.kdnuggets.com/top-5-freelancer-websites-better-fiverr-upwork","date":1739365235,"author":"Abid Ali Awan","guid":586,"unread":true,"content":"<article>Discover freelancing platforms that care about you, not just your money, offering low commission rate, better policies, and higher earning potential.</article>","contentLength":149,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/awan_top_5_freelancer_websites_better_fiverr_upwork_4.png","enclosureMime":"","commentsUrl":null},{"title":"Real Python: Quiz: Python Keywords: An Introduction","url":"https://realpython.com/quizzes/python-keywords/","date":1739361600,"author":"","guid":183,"unread":true,"content":"<p>Python keywords are reserved words with specific functions and restrictions in the language. These keywords are always available in Python, which means you don‚Äôt need to import them. Understanding how to use them correctly is fundamental for building Python programs.</p>","contentLength":269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The difference between metrics and scorers","url":"https://www.youtube.com/watch?v=67KMDSitw1M","date":1739356864,"author":"probabl","guid":530,"unread":true,"content":"<article>You can easily write custom metric functions for scikit-learn, but you need to take an extra step if you want to use those metrics in a hyperparameter search. To add a number there, you need to use a scorer instead. The goal of this video is to explain the difference and to show how to leverage the `make_scorer` helper utility here.\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nBluesky: https://bsky.app/profile/probabl.bsky.social\nTwitter: https://x.com/probabl_ai\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":719,"flags":null,"enclosureUrl":"https://www.youtube.com/v/67KMDSitw1M?version=3","enclosureMime":"","commentsUrl":null},{"title":"Zato Blog: Modern REST API Tutorial in Python","url":"https://zato.io/en/blog/modern-rest-api-tutorial-in-python.html","date":1739347200,"author":"","guid":273,"unread":true,"content":"<div>\n  2025-02-12, by Dariusz Suchojad\n<img src=\"https://upcdn.io/kW15bqq/raw/root/static/blog/authors/dsuch.webp\" alt=\"\" width=\"35\"></div><p>Great APIs don't win theoretical arguments - they just prefer to work reliably and to make developers' lives easier.</p><p>Here's a tutorial on what building production APIs is really about: creating interfaces that are practical in usage,\nwhile keeping your systems maintainable for years to come.</p>","contentLength":326,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kushal Das: pass using stateless OpenPGP command line interface","url":"https://kushaldas.in/posts/pass-using-stateless-openpgp-command-line-interface.html","date":1739337973,"author":"","guid":272,"unread":true,"content":"<p><a href=\"https://kushaldas.in/posts/using-openpgp-card-tool-git-with-git.html\">Yesterday</a> I wrote about how\nI am using a different tool for  signing and verification. Next, I\nreplaced my  usage. I have a <a href=\"https://kushaldas.in/github.com/kushaldas/password-store/compare/master...card\">small\npatch</a> to use\nstateless OpenPGP command line interface (SOP). It is an implementation\nagonostic standard for handling OpenPGP messages. You can read the whole <a href=\"https://datatracker.ietf.org/doc/draft-dkg-openpgp-stateless-cli/\">SPEC\nhere</a>.</p><pre><code>cargo install rsop rsop-oct\n</code></pre><p>And copied the bash script from my repository to the path somewhere.</p><p>The  binary from  follows the same SOP standard but uses the\ncard to signing/decryption. I stored my public key in\n<code>~/.password-store/.gpg-key</code> file, which is in turn used for encryption.</p><p>Here nothing changed related my daily  usage, except the number of time I am typing my  :)</p>","contentLength":669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust 586","url":"https://this-week-in-rust.org/blog/2025/02/12/this-week-in-rust-586/","date":1739336400,"author":"TWiR Contributors","guid":401,"unread":true,"content":"<p>This week's crate is <a href=\"https://github.com/virust-ai/esp32-mender-client\">esp32-mender-client</a>, a client for ESP32 to execute firmware updates and remote commands.</p><p>Thanks to <a href=\"https://users.rust-lang.org/t/crate-of-the-week/2704/1399\">Kelvin</a> for the self-suggestion!</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.  The following\nRFCs would benefit from user testing before moving forward:</p><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p><em>No Calls for participation were submitted this week.</em></p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p><em>No Calls for papers or presentations were submitted this week.</em></p><p>A relatively neutral week, with lots of real changes but most small in\nmagnitude. Most significant change is rustdoc's move of JS/CSS minification to\nbuild time which cut doc generation times on most benchmarks fairly\nsignificantly.</p><p>3 Regressions, 5 Improvements, 1 Mixed; 2 of them in rollups\n32 artifact comparisons made in total</p><ul><li><em>No RFCs were approved this week.</em></li></ul><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><ul><li><em>No Cargo Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Team Proposals entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Reference RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><p>Rusty Events between 2025-02-12 - 2025-03-12 ü¶Ä</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>Just because things are useful doesn't mean they are magically sound.</p></blockquote>","contentLength":2396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.24 arrives","url":"https://golangweekly.com/issues/542","date":1739318400,"author":"","guid":597,"unread":true,"content":"<li><p><a href=\"https://golangweekly.com/link/165684/rss\">DB Fiddle</a> is a handy online database 'sandbox' for playing with various versions of MySQL, Postgres, and SQLite direct from the browser.</p></li>","contentLength":136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta SAM 2.1 is now available in Amazon SageMaker JumpStart","url":"https://aws.amazon.com/blogs/machine-learning/meta-sam-2-1-is-now-available-in-amazon-sagemaker-jumpstart/","date":1739315351,"author":"Marco Punio","guid":230,"unread":true,"content":"<p><em>This blog post is co-written with George Orlin from Meta.</em></p><p>Today, we are excited to announce that Meta‚Äôs <a href=\"https://ai.meta.com/sam2/\" target=\"_blank\" rel=\"noopener\">Segment Anything Model (SAM) 2.1</a> vision segmentation model is publicly available through <a href=\"https://aws.amazon.com/sagemaker/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart</a> to deploy and run inference. Meta SAM 2.1 provides state-of-the-art video and image segmentation capabilities in a single model. This cutting-edge model supports long-context processing, complex segmentation scenarios, and fine-grained analysis, making it ideal for automating processes for various industries such as medical imaging in healthcare, satellite imagery for environment monitoring, and object segmentation for autonomous systems. Meta SAM 2.1 is well suited for zero-shot object segmentation and accurate object detection based on simple prompts such as point coordinates and bounding boxes in a frame for video tracking and image masking.</p><p>This model was predominantly trained on AWS, and AWS will also be the first cloud provider to make it available to customers. In this post, we walk through how to discover and deploy the Meta SAM 2.1 model using SageMaker JumpStart.</p><p>Meta SAM 2.1 is a state-of-the-art vision segmentation model designed for high-performance computer vision tasks, enabling advanced object detection and segmentation workflows. Building upon its <a href=\"https://segment-anything.com/\" target=\"_blank\" rel=\"noopener\">predecessor</a>, version 2.1 introduces enhanced segmentation accuracy, robust generalization across diverse datasets, and scalability for production-grade applications. These features enable AI researchers and developers in computer vision, image processing, and data-driven research to improve tasks that require detailed analysis segmentation across multiple fields.</p><p>Meta SAM 2.1 has a streamlined architecture that is optimized for integration with popular model-serving frameworks like TorchServe and can be deployed on <a href=\"https://aws.amazon.com/sagemaker-ai\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker AI</a> to power real-time or batch inference pipelines. Meta SAM 2.1 empowers organizations to achieve precise segmentation outcomes in vision-centric workflows with minimal configuration and maximum efficiency.</p><p>Meta SAM 2.1 offers multiple variants‚ÄîTiny, Small, Base Plus, and Large‚Äîavailable now on SageMaker JumpStart, balancing model size, speed, and segmentation performance to cater to diverse application needs.</p><h2>SageMaker JumpStart overview</h2><p>SageMaker JumpStart offers access to a broad selection of publicly available foundation models (FMs). These pre-trained models serve as powerful starting points that can be deeply customized to address specific use cases. You can now use state-of-the-art model architectures, such as language models, computer vision models, and more, without having to build them from scratch.</p><p>With SageMaker JumpStart, you can deploy models in a secure environment. Models hosted on JumpStart can be provisioned on dedicated SageMaker Inference instances, including <a href=\"https://aws.amazon.com/machine-learning/trainium/\" target=\"_blank\" rel=\"noopener\">AWS Trainium</a> and <a href=\"https://aws.amazon.com/machine-learning/inferentia/\" target=\"_blank\" rel=\"noopener\">AWS Inferentia</a> based instances, and are isolated within your virtual private cloud (VPC). This enforces data security and compliance, because the models operate under your own VPC controls, rather than in a shared public environment. After deploying an FM, you can further customize and fine-tune it using the extensive capabilities of SageMaker AI, including SageMaker Inference for deploying models and container logs for improved observability. With SageMaker AI, you can streamline the entire model deployment process.</p><p>Make sure you have the following prerequisites to deploy Meta SAM 2.1 and run inference:</p><h2>Discover Meta SAM 2.1 in SageMaker JumpStart</h2><p>SageMaker JumpStart provides FMs through two primary interfaces: SageMaker Studio and the <a href=\"https://github.com/aws/sagemaker-python-sdk\" target=\"_blank\" rel=\"noopener\">SageMaker Python SDK.</a> This provides multiple options to discover and use hundreds of models for your specific use case.</p><p>SageMaker Studio is a comprehensive IDE that offers a unified, web-based interface for performing all aspects of the machine learning (ML) development lifecycle. From preparing data to building, training, and deploying models, SageMaker Studio provides purpose-built tools to streamline the entire process. In SageMaker Studio, you can access SageMaker JumpStart to discover and explore the extensive catalog of FMs available for <a href=\"https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/\" target=\"_blank\" rel=\"noopener\">deployment to inference capabilities on SageMaker Inference</a>.</p><p>You can access the SageMaker JumpStart UI through either <a href=\"https://aws.amazon.com/sagemaker/unified-studio/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Unified Studio</a> or SageMaker Studio. To deploy Meta SAM 2.1 using the SageMaker JumpStart UI, complete the following steps:</p><p>In SageMaker Unified Studio, on the menu, choose .</p><p>If you‚Äôre already on the SageMaker Studio console, choose  in the navigation pane.</p><p>You will be prompted to create a project, after which you can begin deployment.</p><p>Alternatively, you can use the SageMaker Python SDK to programmatically access and use SageMaker JumpStart models. This approach allows for greater flexibility and integration with existing AI/ML workflows and pipelines. By providing multiple access points, SageMaker JumpStart helps you seamlessly incorporate pre-trained models into your AI/ML development efforts, regardless of your preferred interface or workflow.</p><h2>Deploy Meta SAM 2.1 for inference using SageMaker JumpStart</h2><p>On the SageMaker JumpStart landing page, you can discover the public pre-trained models offered by SageMaker AI. You can choose the Meta model provider tab to discover the Meta models available.</p><p>If you‚Äôre using SageMaker Studio and don‚Äôt see the SAM 2.1 models, update your SageMaker Studio version by shutting down and restarting. For more information about version updates, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-apps.html\" target=\"_blank\" rel=\"noopener\">Shut down and Update Studio Classic Apps</a>.</p><p>You can choose the model card to view details about the model such as license, data used to train, and how to use. You can also find two buttons,  and , which help you use the model.</p><p>When you choose , you should be prompted to the next screen to choose an endpoint name and instance type to initiate deployment.</p><p>Upon defining your endpoint settings, you can proceed to the next step to use the model.</p><h2>Deploy Meta SAM 2.1 vision segmentation model for inference using the Python SDK</h2><p>When you choose , model deployment will start. Alternatively, you can deploy through the example notebook by choosing . The notebook provides end-to-end guidance on how to deploy the model for inference and clean up resources.</p><p>To deploy using a notebook, you start by selecting an appropriate model, specified by the . You can deploy any of the selected models on SageMaker AI.</p><p>You can deploy a Meta SAM 2.1 vision segmentation model using SageMaker JumpStart with the following SageMaker Python SDK code:</p><div><pre><code>from sagemaker.jumpstart.model import JumpStartModel \nmodel = JumpStartModel(model_id = \"meta-vs-sam-2-1-hiera-tiny\") \npredictor = model.deploy()</code></pre></div><p>This deploys the model on SageMaker AI with default configurations, including default instance type and default VPC configurations. You can change these configurations by specifying non-default values in <a href=\"https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.jumpstart.model.JumpStartModel\" target=\"_blank\" rel=\"noopener\">JumpStartModel</a>. After it‚Äôs deployed, you can run inference against the deployed endpoint through the SageMaker predictor. There are three tasks that are available with this endpoint: automatic mask generator, image predictor, and video predictor. We provide a code snippet for each later in this post. To use the predictor, a certain payload schema needs to be followed. The endpoint has sticky sessions enabled, so to start inference, you need to send a  payload:</p><div><pre><code>def start_session(asset_type, asset_path):\n\n    asset_base64 = None\n    \n     with open(image_path, 'rb') as f:\n            asset_base64 = base64.b64encode(f.read()).decode('utf-8')\n    \n    response = predictor.invoke_endpoint(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n                    \"type\": \"start_session\",\n                    \"input_type\": asset_type,\n                    \"path\": asset_base64 \n                }),\n        SessionId=\"NEW_SESSION\",\n    )\n    \n    session_id = response.headers.get(\"x-amzn-sagemaker-new-session-id\")\n    \n    return session_id\n</code></pre></div><p>The  invocation needs an input media type of either image or video and the base64 encoded data of the media. This will launch a session with an instance of the model and load the media to be segmented.</p><p>To close a session, send a  invocation:</p><div><pre><code>def close_session(session_id):\n    response = predictor.invoke_endpoint(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n                    \"type\": \"close_session\",\n                    \"session_id\": session_id\n                }),\n        SessionId=session_id,\n    )\n    \n    session_id = response.headers.get(\"x-amzn-sagemaker-closed-session-id\")\n    \n    return session_id\n</code></pre></div><p>If <code>x-amzn-sagemaker-closed-session-id</code> exists as a header, then the session has been successfully closed.</p><p>To continue a session and retrieve the session ID of the existing session, the response header will have the <code>x-amzn-sagemaker-session-id</code> key with the current session ID for any operation that is not  or . Operations that aren‚Äôt  or  need to be invoked with a response stream. This is due to the size of the resulting payload being larger than what SageMaker real-time endpoints can return.</p><p>This is a basic example of interacting with the SAM 2.1 SageMaker JumpStart endpoint with sticky sessions. The following examples for each of the tasks reference these operations without repeating them. The returned data is of mime type JSONL. For more complete examples, refer to the example notebooks for Meta SAM 2.1 on SageMaker Jumpstart.</p><h2>Recommended instances and benchmarks</h2><p>The following table lists all the Meta SAM 2.1 models available in SageMaker JumpStart along with the , default instance types, and maximum number of total tokens (sum of number of input tokens and number of generated tokens) supported for each of these models. For increased context length, you can modify the default instance type in the SageMaker JumpStart UI.</p><table border=\"1px\" cellpadding=\"5px\"><tbody><tr></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-tiny</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-small</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-base-plus</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-large</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr></tbody></table><h2>Meta SAM 2.1 use cases: Inference and prompt examples</h2><p>After you deploy the model using SageMaker JumpStart, you should be able to see a reference Jupyter notebook that references the parser and helper functions needed to begin using Meta SAM 2.1. After you follow those cells in the notebook, you should be ready to begin using the model‚Äôs vision segmentation capabilities.</p><p>Meta SAM 2.1 offers support for three different tasks (automatic mask generator, image predictor, video predictor) to generate masks for various objects in images, including object tracking in videos. In the following examples, we demonstrate how to use the automatic mask generator and image predictor on a JPG of a truck. This  file is stored in the  bucket; you can access it with the following code:</p><div><pre><code>s3_bucket = f\"jumpstart-cache-prod-{region}\"\nkey_prefix = \"inference-notebook-assets\"\n\ndef download_from_s3(key_filenames):\n    for key_filename in key_filenames:\n        s3.download_file(s3_bucket, f\"{key_prefix}/{key_filename}\", key_filename)\n        \ntruck_jpg = \"truck.jpg\"\n\n#Download images.\ndownload_from_s3(key_filenames=[truck_jpg])\ndisplay(Image(filename=truck_jpg))\n</code></pre></div><p>After you have your image and it is encoded, you can create masks for objects in the image. For use cases where you want to generate masks for every object in the image, you can use the automatic mask generator task.</p><p>The automatic mask generator is great for AI researchers for computer vision tasks and applications such as medical imaging and diagnostics to automatically segment regions of interest like tumors or specific organs to provide more accurate diagnostic support. Additionally, the automatic mask generator can be particularly useful in the autonomous vehicle space, in which it can segment out elements in a camera like pedestrians, vehicles, and other objects. Let‚Äôs use the automatic mask generator to generate masks for all the objects in .</p><p>The following code is the prompt to generate masks for your base64 encoded image:</p><div><pre><code># Start session\nsession_id = start_session(\"image\", truck_jpg)\n    \n# Generate and visualize masks with basic parameters\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"generate_automatic_masks\",\n            \"session_id\": session_id,\n            \"points_per_side\": 32,\n            \"min_mask_region_area\": 100\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n    \n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\nmasks = parser.get_responses()\n\n# End session\nend_session(session_id)\n</code></pre></div><p>We receive the following output (parsed and visualized).</p><p>Additionally, you can choose which objects in the provided image you want to create a mask for by adding points within that object for Meta SAM 2.1 to create. A use case for the image predictor can be valuable for tasks related to design and modeling by automating processes that typically require manual efforts. For example, the image predictor can automate turning 2D images into 3D models by analyzing 2D images of blueprints, sketches, or floor plans and generating preliminary 3D models. This is one of many examples of how the image predictor can act as a bridge between 2D and 3D construction across many different tasks. We use the following image with the points that we used to prompt Meta SAM 2.1 for masking the object.</p><p>The following code is used to prompt Meta SAM 2.1 and plot the coordinates:</p><div><pre><code># Start session\nsession_id = start_session(\"image\", truck_jpg)\n\npoints = [\n            {\"type\": \"point\", \"coordinates\": [500, 375], \"label\": 1},\n            {\"type\": \"point\", \"coordinates\": [1125, 625], \"label\": 1}\n         ]\n    \n# Add multiple points\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"add_points\",\n            \"session_id\": session_id,\n            \"points\": [p[\"coordinates\"] for p in points],\n            \"labels\": [p[\"label\"] for p in points],\n            \"clear_old_points\": clear_old_point,\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\n# Intermediate Response\nmasks = parser.get_responses()\n    \nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"predict\",\n            \"session_id\": session_id,\n            \"multimask_output\": True,\n            \"return_logits\": True\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\nmasks = parser.get_responses()\n\n# End session\nend_session(session_id)\n</code></pre></div><p>We receive the following output (parsed and visualized).</p><p>We now demonstrate how to prompt Meta SAM 2.1 for object tracking on video. One use case would be for ergonomic data collection and training purposes. You can use the video predictor to analyze the movement and posture of humans in real time, serving as a way to reduce injury and improve performance by setting alarms for bad posture or movements. Let‚Äôs start by accessing the  file [1] from the  S3 bucket defined in the following code:</p><div><pre><code>basketball_mp4 = \"basketball-layup.mp4\"\n\n#Download video\ndownload_from_s3(key_filenames=[basketball_mp4])\ndisplay(Video(filename=basketball_mp4))</code></pre></div><p>The following code shows how you can set up the prompt format to track objects in the video. The first object will use coordinates to track and not track, and the second object will track one coordinate.</p><div><pre><code># Start session\nsession_id = start_session(\"video\", basketball_mp4)\n\n# Object 1\nprompts1 = [\n        {\"type\": \"point\", \"coordinates\": [1478, 649], \"label\": 1},\n        {\"type\": \"point\", \"coordinates\": [1433, 689], \"label\": 0},\n    ]\n    \n# Extract points and labels\npoints = []\nlabels = []\nfor prompt in prompts1:\n    if prompt[\"type\"] == \"point\":\n        points.append(prompt[\"coordinates\"])\n        labels.append(prompt[\"label\"])\n\nrequest = {\n        \"type\": \"add_points\",\n        \"session_id\": session_id,\n        \"frame_index\": 0,\n        \"object_id\": 1,\n        \"points\": points,\n        \"labels\": labels,\n        \"clear_old_points\": True,\n    }\n    \n# Add multiple points\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps(request),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\n# Intermediate Response\nmasks = parser.get_responses()\n\n# Object 2\nprompts2 = [{\"type\": \"point\", \"coordinates\": [1433, 689], \"label\": 1}]\n\n# Extract points and labels\npoints = []\nlabels = []\nfor prompt in prompts2:\n    if prompt[\"type\"] == \"point\":\n        points.append(prompt[\"coordinates\"])\n        labels.append(prompt[\"label\"])\n\nrequest = {\n        \"type\": \"add_points\",\n        \"session_id\": session_id,\n        \"frame_index\": 0,\n        \"object_id\": 2,\n        \"points\": points,\n        \"labels\": labels,\n        \"clear_old_points\": True,\n    }\n    \n# Add multiple points\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps(request),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\n# Intermediate Response\nmasks = parser.get_responses()\n    \nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"propagate_in_video\",\n            \"session_id\": session_id,\n            \"start_frame_index\": 0,\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\nmasks = parser.get_responses()\n\n# End session\nend_session(session_id)\n</code></pre></div><p>We receive the following output (parsed and visualized).</p><p>Here we can see that Meta SAM 2.1 Tiny was able to successfully track the objects based off the coordinates that were provided in prompt.</p><p>To avoid incurring unnecessary costs, when you‚Äôre done, delete the SageMaker AI endpoints using the following code:</p><div><pre><code>predictor.delete_model()\npredictor.delete_endpoint()</code></pre></div><p>Alternatively, to use the SageMaker AI console, complete the following steps:</p><ol><li>On the SageMaker AI console, under in the navigation pane, choose</li><li>Search for the embedding and text generation endpoints.</li><li>On the endpoint details page, choose .</li><li>Choose  again to confirm.</li></ol><p>In this post, we explored how SageMaker JumpStart empowers data scientists and ML engineers to discover, access, and deploy a wide range of pre-trained FMs for inference, including Meta‚Äôs most advanced and capable models to date. Get started with SageMaker JumpStart and Meta SAM 2.1 models today. For more information about SageMaker JumpStart, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html\" target=\"_blank\" rel=\"noopener\">SageMaker JumpStart pretrained models</a> and <a href=\"https://aws.amazon.com/sagemaker/jumpstart/getting-started/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&amp;sagemaker-jumpstart-cards.sort-order=asc&amp;awsf.sagemaker-jumpstart-filter-product-type=*all&amp;awsf.sagemaker-jumpstart-filter-text=*all&amp;awsf.sagemaker-jumpstart-filter-vision=*all&amp;awsf.sagemaker-jumpstart-filter-tabular=*all&amp;awsf.sagemaker-jumpstart-filter-audio-tasks=*all&amp;awsf.sagemaker-jumpstart-filter-multimodal=*all&amp;awsf.sagemaker-jumpstart-filter-RL=*all\" target=\"_blank\" rel=\"noopener\">Getting started with Amazon SageMaker JumpStart</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/08/26/Marco-Punio.jpg\" alt=\"\" width=\"100\" height=\"134\"><a href=\"https://www.linkedin.com/in/marcpunio/\" target=\"_blank\" rel=\"noopener\">Marco Punio</a> is a Sr. Specialist Solutions Architect focused on generative AI strategy, applied AI solutions, and conducting research to help customers hyper-scale on AWS. As a member of the 3rd Party Model Provider Applied Sciences Solutions Architecture team at AWS, he is a Global Lead for the Meta ‚Äì AWS Partnership and technical strategy. Based in Seattle, WA, Marco enjoys writing, reading, exercising, and building applications in his free time.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/11/deepak-rupakula-100.jpg\" alt=\"\" width=\"100\" height=\"140\"><a href=\"https://www.linkedin.com/in/deepak-rupakula-042b956/\" target=\"_blank\" rel=\"noopener\">Deepak Rupakula</a> is a Principal GTM lead in the specialists group at AWS. He focuses on developing GTM strategy for large language models like Meta across AWS services like Amazon Bedrock and Amazon SageMaker AI. With over 15 years of experience in the tech industry, his experience includes leadership roles in product management, customer success, and analytics.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/11/harish-rao-100.jpg\" alt=\"\" width=\"100\" height=\"122\"><a href=\"https://www.linkedin.com/in/harishvs/\" target=\"_blank\" rel=\"noopener\">Harish Rao</a> is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He empowers customers to harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/11/baladithya-balamurugan-100.jpg\" alt=\"\" width=\"100\" height=\"120\"><a href=\"https://www.linkedin.com/in/baladithyabalamurugan/\" target=\"_blank\" rel=\"noopener\">Baladithya Balamurugan</a> is a Solutions Architect at AWS focused on ML deployments for inference and using AWS Neuron to accelerate training and inference. He works with customers to enable and accelerate their ML deployments on services such as Amazon SageMaker AI and Amazon EC2. Based in San Francisco, Baladithya enjoys tinkering, developing applications, and building his homelab in his free time.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/12/01/Banu-Nagasundaram.jpg\" alt=\"\" width=\"101\" height=\"140\"><a href=\"https://www.linkedin.com/in/banunagasundaram/\" target=\"_blank\" rel=\"noopener\">Banu Nagasundaram</a> leads product, engineering, and strategic partnerships for Amazon SageMaker JumpStart, SageMaker AI‚Äôs machine learning and generative AI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/10/naman.png\" alt=\"\" width=\"100\" height=\"133\"><a href=\"https://www.linkedin.com/in/namannandan/\" target=\"_blank\" rel=\"noopener\">Naman Nandan</a> is a software development engineer at AWS, specializing in enabling large-scale AI/ML inference workloads on Amazon SageMaker AI using TorchServe, a project jointly developed by AWS and Meta. In his free time, he enjoys playing tennis and going on hikes.</p>","contentLength":21819,"flags":null,"enclosureUrl":"https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-17926/basketball-layup-masked.mp4","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Mapping the Unix Magic Poster ‚Äì An Interactive Annotation Project","url":"https://drio.github.io/unixmagic/","date":1739312557,"author":"drio","guid":316,"unread":true,"content":"<p>I built this as a static site that lets us annotate the Unix Magic poster by placing markers on references and adding descriptions to explain them. I've added a few so far, but there's much more to document.</p><p>What I love about this approach is that contributions happen not just on the site itself but also through PRs, where we can discuss and refine the details of each reference. Feel free to send a PR!</p><p>Would love feedback, suggestions, and PRs from the community!</p>","contentLength":465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43019136"},{"title":"Falcon 3 models now available in Amazon SageMaker JumpStart","url":"https://aws.amazon.com/blogs/machine-learning/falcon-3-models-now-available-in-amazon-sagemaker-jumpstart/","date":1739312187,"author":"Niithiyn Vijeaswaran","guid":229,"unread":true,"content":"<h2>Overview of the Falcon 3 family of models</h2><p>The Falcon 3 family, developed by Technology Innovation Institute (TII) in Abu Dhabi, represents a significant advancement in open source language models. This collection includes five base models ranging from 1 billion to 10 billion parameters, with a focus on enhancing science, math, and coding capabilities. The family consists of Falcon3-1B-Base, Falcon3-3B-Base, Falcon3-Mamba-7B-Base, Falcon3-7B-Base, and Falcon3-10B-Base along with their instruct variants.</p><p>These models showcase innovations such as efficient pre-training techniques, scaling for improved reasoning, and knowledge distillation for better performance in smaller models. Notably, the Falcon3-10B-Base model achieves state-of-the-art performance for models under 13 billion parameters in zero-shot and few-shot tasks. The Falcon 3 family also includes various fine-tuned versions like Instruct models and supports different quantization formats, making them versatile for a wide range of applications.</p><p>Currently, SageMaker JumpStart offers the base versions of Falcon3-3B, Falcon3-7B, and Falcon3-10B, along with their corresponding instruct variants, as well as Falcon3-1B-Instruct.</p><h2>Get started with SageMaker JumpStart</h2><p>SageMaker JumpStart is a machine learning (ML) hub that can help accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select pre-trained foundation models (FMs), including Falcon 3 models. These models are fully customizable for your use case with your data.</p><p>Deploying a Falcon 3 model through SageMaker JumpStart offers two convenient approaches: using the intuitive SageMaker JumpStart UI or implementing programmatically through the SageMaker Python SDK. Let‚Äôs explore both methods to help you choose the approach that best suits your needs.</p><h2>Deploy Falcon 3 using the SageMaker JumpStart UI</h2><p>Complete the following steps to deploy Falcon 3 through the JumpStart UI:</p><h2>Deploy Falcon 3 programmatically using the SageMaker Python SDK</h2><p>For teams looking to automate deployment or integrate with existing MLOps pipelines, you can use the SageMaker Python SDK:</p><div><pre><code>from sagemaker.serve.builder.model_builder import ModelBuilder\nfrom sagemaker.serve.builder.schema_builder import SchemaBuilder\nfrom sagemaker.jumpstart.model import ModelAccessConfig\nfrom sagemaker.session import Session\nimport logging\n\nsagemaker_session = Session()\n\nartifacts_bucket_name = sagemaker_session.default_bucket()\nexecution_role_arn = sagemaker_session.get_caller_identity_arn()\n\n\njs_model_id = \"huggingface-llm-falcon-3-10B-base\"\n\ngpu_instance_type = \"ml.g5.12xlarge\"  \n\nresponse = \"Hello, I'm a language model, and I'm here to help you with your English.\"\n\nsample_input = {\n    \"inputs\": \"Hello, I'm a language model,\",\n    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n}\n\nsample_output = [{\"generated_text\": response}]\n\nschema_builder = SchemaBuilder(sample_input, sample_output)\n\nmodel_builder = ModelBuilder(\n    model=js_model_id,\n    schema_builder=schema_builder,\n    sagemaker_session=sagemaker_session,\n    role_arn=execution_role_arn,\n    log_level=logging.ERROR\n)\n\nmodel= model_builder.build()\n\npredictor = model.deploy(model_access_configs={js_model_id:ModelAccessConfig(accept_eula=True)}, accept_eula=True)</code></pre></div><p>Run inference on the predictor:</p><div><pre><code>predictor.predict(sample_input)</code></pre></div><p>To clean up the model and endpoint, use the following code:</p><div><pre><code>predictor.delete_model()\npredictor.delete_endpoint()</code></pre></div><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/05/30/NIITHIYN_ARP-2-1.jpg\" alt=\"\" width=\"100\" height=\"100\">&nbsp;is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is generative AI and AWS AI Accelerators. He holds a Bachelor‚Äôs degree in Computer Science and Bioinformatics.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/27/ml-17555-karpmar.jpg\" alt=\"Marc Karp\" width=\"100\" height=\"114\"> is an ML Architect with the Amazon SageMaker Service team. He focuses on helping customers design, deploy, and manage ML workloads at scale. In his spare time, he enjoys traveling and exploring new places.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/27/ml-17555-raghu.png\" alt=\"Raghu\" width=\"100\" height=\"100\"> is a Senior ML Solutions Architect with the Amazon SageMaker Service team. He focuses on helping customers build, deploy, and migrate ML production workloads to SageMaker at scale. He specializes in machine learning, AI, and computer vision domains, and holds a master‚Äôs degree in Computer Science from UT Dallas. In his free time, he enjoys traveling and photography.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/12/01/Banu-Nagasundaram.jpg\" alt=\"\" width=\"101\" height=\"140\"> leads product, engineering, and strategic partnerships for SageMaker JumpStart, SageMaker‚Äôs machine learning and GenAI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.</p>","contentLength":4499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GenAI Patterns: Query Rewriting","url":"https://martinfowler.com/articles/gen-ai-patterns/#query-rewrite","date":1739307480,"author":"Martin Fowler","guid":299,"unread":true,"content":"<p>Users often have difficulty writing the most effective queries.\n       and I explain <a href=\"https://martinfowler.com/articles/gen-ai-patterns/#query-rewrite\">Query Rewriting</a>:\n      getting an LLM to formulate alternative queries to send to a RAG's\n      retriever. </p>","contentLength":192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a virtual meteorologist using Amazon Bedrock Agents","url":"https://aws.amazon.com/blogs/machine-learning/building-a-virtual-meteorologist-using-amazon-bedrock-agents/","date":1739307184,"author":"Salman Ahmed","guid":228,"unread":true,"content":"<p>The integration of <a href=\"https://aws.amazon.com/ai/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a> capabilities is driving transformative changes across many industries. Although weather information is accessible through multiple channels, businesses that heavily rely on meteorological data require robust and scalable solutions to effectively manage and use these critical insights and reduce manual processes. This solution demonstrates how to create an AI-powered virtual meteorologist that can answer complex weather-related queries in natural language. We use various AWS services to deploy a complete solution that you can use to interact with an API providing real-time weather information. In this solution, we use <a href=\"https://aws.amazon.com/bedrock/agents/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Agents</a>.</p><p>Amazon Bedrock Agents helps to streamline workflows and automate repetitive tasks. Amazon Bedrock Agents can securely connect to your company‚Äôs data sources and augments the user‚Äôs request with accurate responses. You can use Amazon Bedrock Agents to architect an action schema tailored to your requirements, granting you control whenever the agent initiates the specified action. This versatile approach equips you to seamlessly integrate and execute business logic within your preferred backend service, fostering a cohesive combination of functionality and flexibility. There is also memory retention across the interaction allowing a more personalized user experience.</p><p>In this post, we present a streamlined approach to deploying an AI-powered agent by combining Amazon Bedrock Agents and a <a href=\"https://aws.amazon.com/what-is/foundation-models/\" target=\"_blank\" rel=\"noopener\">foundation model</a> (FM). We guide you through the process of configuring the agent and implementing the specific logic required for the virtual meteorologist to provide accurate weather-related responses. Additionally, we use various AWS services, including <a href=\"https://aws.amazon.com/amplify/\" target=\"_blank\" rel=\"noopener\">AWS Amplify</a> for hosting the front end, <a href=\"https://aws.amazon.com/lambda/\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> functions for handling request logic, <a href=\"https://aws.amazon.com/cognito/\" target=\"_blank\" rel=\"noopener\">Amazon Cognito</a> for user authentication, and <a href=\"https://aws.amazon.com/iam/\" target=\"_blank\" rel=\"noopener\">AWS Identity and Access Management</a> (IAM) for controlling access to the agent.</p><p>The diagram gives an overview and highlights the key components. The architecture uses Amazon Cognito for user authentication and Amplify as the hosting environment for our front-end application. Amazon Bedrock Agents forwards the details from the user query to the action groups, which further invokes custom Lambda functions. Each action group and Lambda function handles a specific task:</p><ol><li> ‚Äì Processes geographic coordinates (geo-coordinates) to get details about a specific location</li><li> Gathers weather information for the provided location</li><li> Obtains the current date and time</li></ol><p>You must have the following in place to complete the solution in this post:</p><h2>Deploy solution resources using AWS CloudFormation</h2><p>When you run the AWS CloudFormation template, the following resources are deployed (note that costs will be incurred for the AWS resources used):</p><ul><li>Lambda resources: \n  <ul><li>Function ‚Äì <code>&lt;Stack name&gt;-geo-coordinates-&lt;auto-generated&gt;</code></li><li>Function ‚Äì <code>&lt;Stack name&gt;-weather-&lt;auto-generated&gt;</code></li><li>Function ‚Äì <code>&lt;Stack name&gt;-date-time-&lt;auto-generated&gt;</code></li></ul></li><li>Amazon Bedrock Agents: virtual-meteorologist \n  <ul><li>Action groups (1) ‚Äì <code>obtain-latitude-longitude-from-place-name</code></li><li>Action groups (2) ‚Äì <code>obtain-weather-information-with-coordinates</code></li><li>Action groups (3) ‚Äì <code>get-current-date-time-from-timezone</code></li></ul></li></ul><p>After you deploy the CloudFormation template, copy the following from the tab on the <a href=\"https://console.aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener\">CloudFormation console</a> to be used during the configuration of your application after it‚Äôs deployed in AWS Amplify.</p><ul></ul><h2>Deploy the AWS Amplify application</h2><p>You need to manually deploy the Amplify application using the front-end code found on GitHub. Complete the following steps:</p><ol><li>Download the front-end code AWS-Amplify-Frontend.zip from <a href=\"https://github.com/aws-samples/virtual-meteorologist-using-amazon-bedrock-agents\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</li><li>Use the .zip file to manually <a href=\"https://docs.aws.amazon.com/amplify/latest/userguide/manual-deploys.html\" target=\"_blank\" rel=\"noopener\">deploy</a> the application in Amplify.</li><li>Return to the Amplify page and use the domain it automatically generated to access the application.</li></ol><h3>Use Amazon Cognito for user authentication</h3><p>Amazon Cognito is an identity service that you can use to authenticate and authorize users. We use Amazon Cognito in our solution to verify the user before they can use the application. We also use identity pool to provide temporary AWS credentials for the user while they interact with Amazon Bedrock API.</p><h3>Use Amazon Bedrock Agents to automate application tasks</h3><p>With Amazon Bedrock Agents, you can build and configure autonomous agents in your application. An agent helps your end users complete actions based on organization data and user input. Agents orchestrate interactions between FMs, data sources, software applications, and user conversations.</p><h3>Use action group to define actions that Amazon Bedrock agents perform</h3><p>An action group defines a set of related actions that an Amazon Bedrock agent can perform to assist users. When configuring an action group, you have options for handling user-provided information, including adding user input to the agent‚Äôs action group, passing data to a Lambda function for custom business logic, or returning control directly through the InvokeAgent response. In our application, we created three action groups to give the Amazon Bedrock agent these essential functionalities: retrieving coordinates for specific locations, obtaining current date and time information, and fetching weather data for given locations. These action groups enable the agent to access and process crucial information, enhancing its ability to respond accurately and comprehensively to user queries related to location-based services and weather conditions.</p><h3>Use Lambda for Amazon Bedrock action group</h3><p>As part of this solution, three Lambda functions are deployed to support the action groups defined for our Amazon Bedrock agent:</p><ol><li><strong>Location coordinates Lambda function</strong> ‚Äì This function is triggered by the <code>obtain-latitude-longitude-from-place-name</code> action group. It takes a place name as input and returns the corresponding latitude and longitude coordinates. The function uses a geocoding service or database to perform this lookup.</li><li><strong>Date and time Lambda function</strong> ‚Äì Invoked by the <code>get-current-date-time-from-timezone</code> action group, this function provides the current date and time information.</li><li><strong>Weather information Lambda function</strong> ‚Äì This function is called by the <code>obtain-weather-information-with-coordinates</code> action group. It accepts geo-coordinates from the first Lambda function and returns current weather conditions and forecasts for the specified area. This Lambda function used a weather API to fetch up-to-date meteorological data.</li></ol><p>Each of these Lambda functions receives an input event containing relevant metadata and populated fields from the Amazon Bedrock agent‚Äôs API operation or function parameters. The functions process this input, perform their specific tasks, and return a response with the required information. This response is then used by the Amazon Bedrock agent to formulate its reply to the user‚Äôs query. By using these Lambda functions, our Amazon Bedrock agent gains the ability to access external data sources and perform complex computations, significantly enhancing its capabilities in handling user requests related to location, time, and weather information.</p><h3>Use AWS Amplify for front-end code</h3><p>Amplify offers a development environment for building secure, scalable mobile and web applications. Developers can focus on their code rather than worrying about the underlying infrastructure. Amplify also integrates with many Git providers. For this solution, we manually upload our front-end code using the method outlined earlier in this post.</p><p>Navigate to the URL provided after you created the application in Amplify. Upon accessing the application URL, you‚Äôll be prompted to provide information related to Amazon Cognito and Amazon Bedrock Agents. This information is required to securely authenticate users and allow the front end to interact with the Amazon Bedrock agent. It enables the application to manage user sessions and make authorized API calls to AWS services on behalf of the user.</p><p>You can enter information with the values you collected from the CloudFormation stack outputs. You‚Äôll be required to enter the following fields, as shown in the following screenshot:</p><ul></ul><p>You need to sign in with your username and password. A temporary password was automatically generated during deployment and sent to the email address you provided when launching the CloudFormation template. At first sign-in attempt, you‚Äôll be asked to reset your password, as shown in the following video.</p><p>Now you can start asking questions in the application, for example, ‚ÄúCan we do barbecue today in Dallas, TX?‚Äù In a few seconds, the application will provide you detailed results mentioning if you can do barbecue in Dallas, TX. The following video shows this chat.</p><p>Here are a few sample queries to demonstrate the capabilities of your virtual meteorologist:</p><ol><li>‚ÄúWhat‚Äôs the weather like in New York City today?‚Äù</li><li>‚ÄúShould I plan an outdoor birthday party in Miami next weekend?‚Äù</li><li>‚ÄúWill it snow in Denver on Christmas Day?‚Äù</li><li>‚ÄúCan I go swimming on a beach in Chicago today?</li></ol><p>These queries showcase the agent‚Äôs ability to provide current weather information, offer advice based on weather forecasts, and predict future weather conditions. You can even ask a question related to an activity such as swimming, and it will answer based on the weather conditions if that activity is okay to do.</p><p>If you decide to discontinue using the virtual meteorologist, you can follow these steps to remove it, its associated resources deployed using AWS CloudFormation, and the Amplify deployment:</p><ol><li>Delete the CloudFormation stack: \n  <ol><li>On the AWS CloudFormation console, choose  in the navigation pane.</li><li>Locate the stack you created during the deployment process (you assigned a name to it).</li><li>Select the stack and choose .</li></ol></li><li>Delete the Amplify application and its resources. For instructions, refer to <a href=\"https://aws.amazon.com/getting-started/hands-on/build-web-app-s3-lambda-api-gateway-dynamodb/module-six/\" target=\"_blank\" rel=\"noopener\">Clean Up Resources</a>.</li></ol><p>This solution demonstrates the power of combining Amazon Bedrock Agents with other AWS services to create an intelligent, conversational weather assistant. By using AI and cloud technologies, businesses can automate complex queries and provide valuable insights to their users.</p><p>To learn more about Amazon Bedrock, refer to the following resources:</p><p>To learn more about the Anthropic‚Äôs Claude 3.5 Sonnet model, refer to the following resources:</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/salmanah.jpg\" alt=\"Salman Ahmed\" width=\"120\" height=\"160\"> is a Senior Technical Account Manager in AWS Enterprise Support. He enjoys helping customers in the travel and hospitality industry to design, implement, and support cloud infrastructure. With a passion for networking services and years of experience, he helps customers adopt various AWS networking services. Outside of work, Salman enjoys photography, traveling, and watching his favorite sports teams.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/sercast.jpg\" alt=\"Sergio Barraza\" width=\"120\" height=\"160\"> is a Senior Enterprise Support Lead at AWS, helping energy customers design and optimize cloud solutions. With a passion for software development, he guides energy customers through AWS service adoption. Outside work, Sergio is a multi-instrument musician playing guitar, piano, and drums, and he also practices Wing Chun Kung Fu.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/vatsravi.jpg\" alt=\"Ravi Kumar\" width=\"120\" height=\"160\"> is a Senior Technical Account Manager in AWS Enterprise Support who helps customers in the travel and hospitality industry to streamline their cloud operations on AWS. He is a results-driven IT professional with over 20 years of experience. In his free time, Ravi enjoys creative activities like painting. He also likes playing cricket and traveling to new places.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/goyalaws.jpg\" alt=\"Ankush Goyal\" width=\"120\" height=\"160\"> is a Enterprise Support Lead in AWS Enterprise Support who helps customers streamline their cloud operations on AWS. He is a results-driven IT professional with over 20 years of experience.</p>","contentLength":11490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python 3.14.0 alpha 5 is out","url":"https://pythoninsider.blogspot.com/2025/02/python-3140-alpha-5-is-out.html","date":1739302860,"author":"Hugo","guid":264,"unread":true,"content":"<p>Here comes the antepenultimate alpha.</p><p><strong>This is an early developer preview of Python\n3.14</strong></p><p>Python 3.14 is still in development. This release, 3.14.0a5, is the\nfifth of seven planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2025-05-06) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2025-07-22). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.14 are still being planned and\nwritten. Among the new major new features and changes so far:</p><p>The next pre-release of Python 3.14 will be the penultimate alpha,\n3.14.0a6, currently scheduled for 2025-03-14.</p><p>2025-01-29 marked the start of a new lunar year, the Year of the\nSnake üêç (and the Year of Python?).</p><p>For centuries, œÄ was often approximated as 3 in China. Some time\nbetween the years 1 and 5 CE, astronomer, librarian, mathematician and\npolitician Liu Xin (ÂäâÊ≠Ü) calculated œÄ as 3.154.</p><p>Around 130 CE, mathematician, astronomer, and geographer Zhang Heng\n(ÂºµË°°, 78‚Äì139) compared the celestial circle with the diameter of the\nearth as 736:232 to get 3.1724. He also came up with a formula for the\nratio between a cube and inscribed sphere as 8:5, implying the ratio of\na square‚Äôs area to an inscribed circle is ‚àö8:‚àö5. From this, he\ncalculated œÄ as ‚àö10 (~3.162).</p><p>Third century mathematician Liu Hui (ÂàòÂæΩ) came up with an algorithm\nfor calculating œÄ iteratively: calculate the area of a polygon inscribed\nin a circle, then as the number of sides of the polygon is increased,\nthe area becomes closer to that of the circle, from which you can\napproximate œÄ.</p><p>This algorithm is similar to the method used by Archimedes in the 3rd\ncentury BCE and Ludolph van Ceulen in the 16th century CE (see <a href=\"https://blog.python.org/2024/11/python-3140-alpha-2-released.html\">3.14.0a2\n  release notes</a>), but Archimedes only went up to a 96-sided polygon\n(96-gon). Liu Hui went up to a 192-gon to approximate œÄ as 157/50 (3.14)\nand later a 3072-gon for 3.14159.</p><p>Liu Hu wrote a commentary on the book The Nine Chapters on the\nMathematical Art which included his œÄ approximations.</p><p>In the fifth century, astronomer, inventor, mathematician,\npolitician, and writer Zu Chongzhi (Á•ñÊ≤ñ‰πã, 429‚Äì500) used Liu Hui‚Äôs\nalgorithm to inscribe a 12,288-gon to compute œÄ between 3.1415926 and\n3.1415927, correct to seven decimal places. This was more accurate than\nHellenistic calculations and wouldn‚Äôt be improved upon for 900\nyears.</p><p>Thanks to all of the many volunteers who help make Python Development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf-landing/\">Python Software\nFoundation</a>.</p><p>Regards from a remarkably snowless Helsinki,</p><p>Your release team, Hugo van KemenadeSteve Dower</p>","contentLength":2941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PyCoder‚Äôs Weekly: Issue #668: NumPy, Compiling Python 1.0, BytesIO, and More (Feb. 11, 2025)","url":"https://pycoders.com/issues/668","date":1739302200,"author":"","guid":271,"unread":true,"content":"<div><p> In this video course, you‚Äôll learn how to use NumPy by exploring several interesting examples. You‚Äôll read data from a file into an array and analyze structured arrays to perform a reconciliation. You‚Äôll also learn how to quickly chart an analysis &amp; turn a custom function into a vectorized function.</p></div><div><p> This tutorial will help you master Python string splitting. You‚Äôll learn to use , , and  to effectively handle whitespace, custom delimiters, and multiline text, which will level up your data parsing skills.</p></div><div><p> Python developers use Posit Package Manager to mirror public &amp; internally developed repos within their firewalls. Get reporting on known vulnerabilities to proactively address potential threats. High-security environments can even run air-gapped.</p></div><div><p> The author was recently invited with other senior devs to give a lightning talk on their personal development philosophy. This post captures those thoughts.</p></div><img src=\"https://pycoders.com/issues/668/open/feed\" width=\"1\" height=\"1\" alt=\"alt\"><p><em>[ Subscribe to üêç PyCoder‚Äôs Weekly üíå ‚Äì Get the best Python news, articles, and tutorials delivered to your inbox once a week <a href=\"https://pycoders.com/?utm_source=pycoders&amp;utm_medium=feed&amp;utm_campaign=footer\">&gt;&gt; Click here to learn more</a> ]</em></p>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon Q Business simplifies integration of enterprise knowledge bases at scale","url":"https://aws.amazon.com/blogs/machine-learning/amazon-q-business-simplifies-integration-of-enterprise-knowledge-bases-at-scale/","date":1739293892,"author":"Omar Elkharbotly","guid":227,"unread":true,"content":"<p>In this new era of emerging AI technologies, we have the opportunity to build AI-powered assistants tailored to specific business requirements. <a href=\"https://aws.amazon.com/q/business/\" target=\"_blank\" rel=\"noopener\">Amazon Q Business</a>, a new generative AI-powered assistant, can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in an enterprise‚Äôs systems.</p><p>Large-scale data ingestion is crucial for applications such as document analysis, summarization, research, and knowledge management. These tasks often involve processing vast amounts of documents, which can be time-consuming and labor-intensive. However, ingesting large volumes of enterprise data poses significant challenges, particularly in orchestrating workflows to gather data from diverse sources.</p><p>In this post, we propose an end-to-end solution using Amazon Q Business to simplify integration of enterprise knowledge bases at scale.</p><h2>Enhancing AWS Support Engineering efficiency</h2><p>The AWS Support Engineering team faced the daunting task of manually sifting through numerous tools, internal sources, and AWS public documentation to find solutions for customer inquiries. For complex customer issues, the process was especially time-consuming, laborious, and at times extended the wait time for customers seeking resolutions. To address this, the team implemented a chat assistant using Amazon Q Business. This solution ingests and processes data from hundreds of thousands of support tickets, escalation notices, public AWS documentation, re:Post articles, and AWS blog posts.</p><p>By using Amazon Q Business, which simplifies the complexity of developing and managing ML infrastructure and models, the team rapidly deployed their chat solution. The Amazon Q Business pre-built <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/connectors-list.html\" target=\"_blank\" rel=\"noopener\">connectors</a> like <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3), document retrievers, and upload capabilities streamlined data ingestion and processing, enabling the team to provide swift, accurate responses to both basic and advanced customer queries.</p><p>In this post, we propose an end-to-end solution using Amazon Q Business to address similar enterprise data challenges, showcasing how it can streamline operations and enhance customer service across various industries. First we discuss end-to-end large-scale data integration with Amazon Q Business, covering data preprocessing, security guardrail implementation, and Amazon Q Business best practices. Then we introduce the solution deployment using three <a href=\"http://aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a> templates.</p><p>The following architecture diagram represents the high-level design of a solution proven effective in production environments for AWS Support Engineering. This solution uses the powerful capabilities of Amazon Q Business. We will walk through the implementation of key components, including configuring enterprise data sources to build our knowledge base, document indexing and boosting, and implementing comprehensive security controls.</p><ul><li> ‚Äì An end-user who accesses Amazon Q Business applications with permissions granted by their administrator to perform their job duties</li><li> ‚Äì A user who manages Amazon Q Business resources and determines feature access for service users within the organization</li><li> ‚Äì A user responsible for creating and managing access policies for Amazon Q Business through <a href=\"https://aws.amazon.com/iam/identity-center/\" target=\"_blank\" rel=\"noopener\">AWS IAM Identity Center</a></li></ul><p>The following workflow details how a service user accesses the application:</p><ol><li>The service user initiates an interaction with the Amazon Q Business application, accessible through the web experience, which is an endpoint URL.</li><li>The service user‚Äôs permissions are authenticated using IAM Identity Center, an AWS solution that connects workforce users to AWS managed applications like Amazon Q Business. It enables end-user authentication and streamlines access management.</li><li>The authenticated service user submits queries in natural language to the Amazon Q Business application.</li><li>The Amazon Q Business application generates and returns answers drawing from the enterprise data uploaded to an S3 bucket, which is connected as a data source to Amazon Q Business. This S3 bucket data is continuously refreshed, making sure that Amazon Q Business accesses the most current information for query responses by using a retriever to pull data from the index.</li></ol><h2>Large-scale data ingestion</h2><p>Before ingesting the data to Amazon Q Business, the data might need transformation into <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/doc-types.html\" target=\"_blank\" rel=\"noopener\">formats</a> supported by Amazon Q Business. Furthermore, it might contain sensitive data or personally identifiable information (PII) requiring redaction. These <a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/data-ingestion-methods.html\" target=\"_blank\" rel=\"noopener\">data ingestion</a> challenges create a need to orchestrate tasks like transformation, redaction, and secure ingestion.</p><p>To facilitate orchestration, this solution incorporates <a href=\"https://aws.amazon.com/step-functions/\" target=\"_blank\" rel=\"noopener\">AWS Step Functions</a>. Step Functions provides a visual workflow service to orchestrate tasks and workloads resiliently and efficiently through built-in AWS integrations and error handling. The solution uses the Step Functions <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/state-map.html\" target=\"_blank\" rel=\"noopener\">Map state</a>, which allows for parallel processing of multiple items in a dataset, thereby efficiently orchestrating workflows and speeding up overall processing.</p><p>The following diagram illustrates an example architecture for ingesting data through an endpoint interfacing with a large corpus.</p><p>Step Functions orchestrates AWS services like <a href=\"https://aws.amazon.com/pm/lambda/\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> and organization APIs like DataStore to ingest, process, and store data securely. The workflow includes the following steps:</p><ol><li>The Prepare Map Input Lambda function prepares the required input for the Map state. For example, the Datastore API might require certain input like date periods to query data. This step can be used to define the date periods to be used by the Map state as an input.</li><li>The Ingest Data Lambda function fetches data from the Datastore API‚Äîwhich can be in or outside of the virtual private cloud (VPC)‚Äîbased on the inputs from the Map state. To handle large volumes, the data is split into smaller chunks to mitigate Lambda function overload. This enables Step Functions to manage the workload, retry failed chunks, and isolate failures to individual chunks instead of disrupting the entire ingestion process.</li><li>The fetched data is put into an S3 data store bucket for processing.</li><li>The Process Data Lambda function redacts sensitive data through <a href=\"https://aws.amazon.com/comprehend/\" target=\"_blank\" rel=\"noopener\">Amazon Comprehend</a>. Amazon Comprehend provides real-time APIs, such as <a href=\"https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectPiiEntities.html\" target=\"_blank\" rel=\"noopener\">DetectPiiEntities</a> and <a href=\"https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectEntities.html\" target=\"_blank\" rel=\"noopener\">DetectEntities</a>, which use natural language processing (NLP) machine learning (ML) models to identify text portions for redaction. When Amazon Comprehend detects PII, the terms will be redacted and replaced by a character of your choice (such as *). You can also use regular expressions to remove identifiers with predetermined formats.</li><li>Finally, the Lambda function creates two separate files: \n  <ol type=\"a\"><li>A sanitized data document in an Amazon Q Business <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/doc-types.html\" target=\"_blank\" rel=\"noopener\">supported format</a> that will be parsed to generate chat responses.</li><li>A JSON <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/s3-metadata.html\" target=\"_blank\" rel=\"noopener\">metadata</a> file for each document containing additional information to customize chat results for end-users and apply <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/metadata-boosting.html\" target=\"_blank\" rel=\"noopener\">boosting techniques</a> to enhance user experience (which we discuss more in the next section).</li></ol></li></ol><p>The following is the sample metadata file:</p><div><pre><code>{\n&nbsp;&nbsp; &nbsp;\"DocumentId\": \"qbusiness-ug.pdf.txt\",\n&nbsp;&nbsp; &nbsp;\"Attributes\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"_created_at\": \"2024-10-29T20:27:45+00:00\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"_last_updated_at\": \"2024-10-29T20:27:45+00:00\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"_source_uri\": \"https://docs.aws.amazon.com/pdfs/amazonq/latest/qbusiness-ug/qbusiness-ug.pdf\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"author\": \"AWS\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"services\": [\"Q Business\"]\n&nbsp;&nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;\"Title\": \"Amazon Q Business - User Guide\",\n&nbsp;&nbsp; &nbsp;\"ContentType\": \"plain/text\"\n}</code></pre></div><p>In the preceding JSON file, the  for each data document must be unique. All the other attributes are optional; however, the file has additional attributes like , , and  with values defined.</p><p>The two files are placed in a new S3 folder for Amazon Q to index. Additionally, the raw unprocessed data is deleted from the S3 bucket. You can further restrict access to documents uploaded to an S3 bucket for specific users or groups using <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/s3-user-management.html\" target=\"_blank\" rel=\"noopener\">Amazon S3 access control lists (ACLs)</a>.</p><p>Using the Amazon Q Business <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/supported-connectors.html\" target=\"_blank\" rel=\"noopener\">data source connector</a> feature, we integrated the S3 bucket with our application. This connector functionality enables the consolidation of data from multiple sources into a unified index for the Amazon Q Business application. The service offers various integration options, with Amazon S3 being one of the supported data sources.</p><p>When working with your specific dataset in Amazon Q Business, you can use relevance <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/metadata-boosting.html\" target=\"_blank\" rel=\"noopener\">tuning</a> to enhance the performance and accuracy of search results. This feature allows you to customize how Amazon Q Business prioritizes information within your ingested documents. For example, if your dataset includes product descriptions, customer reviews, and technical specifications, you can use relevance tuning to boost the importance of certain fields. You might choose to prioritize product names in titles, give more weight to recent customer reviews, or emphasize specific technical attributes that are crucial for your business. By adjusting these parameters, you can influence the ranking of search results to better align with your dataset‚Äôs unique characteristics and your users‚Äô information needs, ultimately providing more relevant answers to their queries.</p><p>For the metadata file used in this example, we focus on boosting two key metadata attributes:  and . By assigning higher weights to these attributes, we made sure documents with specific titles or services received greater prominence in the search results, improving their visibility and relevance for the users</p><p>The following code is the sample CloudFormation template snippet to enable higher weights to  and :</p><div><pre><code>BoostOverrideConfiguration:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Fn::Sub: |\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"nativeIndexConfiguration\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"indexId\": \"${QBusinessIndex.IndexId}\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"boostingOverride\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"_document_title\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"stringConfiguration\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"boostingLevel\": \"MEDIUM\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"services\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"stringListConfiguration\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"boostingLevel\": \"HIGH\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</code></pre></div><h3>Amazon Q Business guardrails</h3><p>Implementing robust security measures is crucial to protect sensitive information. In this regard, Amazon Q Business <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/guardrails.html\" target=\"_blank\" rel=\"noopener\">guardrails</a> or chat controls proved invaluable, offering a powerful solution to maintain data privacy and security.</p><p>Amazon Q Business guardrails provide configurable rules designed to control the application‚Äôs behavior. These guardrails act as a safety net, minimizing access, processing, or revealing of sensitive or inappropriate information. By defining boundaries for the application‚Äôs operations, organizations can maintain compliance with internal policies and external regulations. You can enable global- or topic-level controls, which control how Amazon Q Business responds to specific topics in chat.</p><p>The following is the sample CloudFormation template snippet to enable topic-level controls:</p><div><pre><code>TopicConfigurations:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: topic\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;rules:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- ruleType: CONTENT_BLOCKER_RULE\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ruleConfiguration:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;contentBlockerRule:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;systemMessageOverride: This message is blocked as it contains secure content\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;exampleChatMessages:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:ec2:us-east-1:123456789012:instance/i-abcdef123\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:ec2:us-west-2:123456789012:vpc/bpc-abcdef123\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:kms:eu-west-1:123456789012:key/12345678-1234-12345678-abc12345678\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- s3://bucket/prefix/file.csv\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:s3::::bucket-name</code></pre></div><p>This topic-level control blocks the Amazon Q Business chat conversation that has AWS service Amazon Resource Names (ARNs). When similar chat messages have been detected by the Amazon Q Business application, the system will block the responses and return the message ‚ÄúThis message is blocked as it contains secure content.‚Äù</p><p>For information about deploying the Amazon Q Business application with sample boosting and guardrails, refer to the <a href=\"https://github.com/aws-samples/amazon-q-business-simplified-integration-of-enterprise-knowledge-bases-at-scale\" target=\"_blank\" rel=\"noopener\">GitHub repo.</a></p><p>The following screenshot shows an example of the Amazon Q Business assistant chat landing page.<img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/QBusiness.jpg\" alt=\"Q Business landing Page\" width=\"1292\" height=\"660\"></p><p>The following screenshot illustrates the assistant‚Äôs behavior if a user includes text that matches one of the similarity-based examples specified in the guardrail topic control.</p><p>To enhance data security, you can deploy <a href=\"https://aws.amazon.com/macie/\" target=\"_blank\" rel=\"noopener\">Amazon Macie</a> classification jobs to scan for sensitive or PII data stored in S3 buckets. The following diagram illustrates a sample notification architecture to alert users on sensitive information that might be inadvertently stored. Macie uses machine learning to automatically discover, classify, and protect sensitive data stored in AWS. It focuses on identifying PII, intellectual property, and other sensitive data types to help organizations meet compliance requirements and protect their data from unauthorized access or breaches.</p><p>The workflow includes the following steps:</p><ol><li>Macie reviews the data store S3 bucket for sensitive information before being ingested.</li><li>An EventBridge rule invokes the Rectify &amp; Notify Lambda function.</li><li>The Lambda function processes the alert, remediates it by removing the affected files from the S3 bucket, and sends a notification using <a href=\"http://aws.amazon.com/sns\" target=\"_blank\" rel=\"noopener\">Amazon Simple Notification Service</a> (Amazon SNS) to the subscribed email addresses.</li></ol><p>This system enables rapid response to potential security alerts, allowing for immediate action to protect sensitive data.</p><p>The Macie detection and subsequent notification system can be demonstrated by uploading a new file to the S3 bucket, such as <code>sample-file-with-credentials.txt</code>, containing the PII data types monitored by Macie, such as fake temporary AWS credentials. After the file is uploaded to Amazon S3 and the scheduled Macie detection job discovers it, the Lambda function immediately removes the file and sends the following notification email to the SNS topic subscribers:</p><div><pre><code>Amazon Macie published a new Finding: \"The S3 object contains credentials data\"\nDescription: \"The S3 object contains credentials data such as AWS secret access keys or private keys.\"\nSeverity: {'score': 3, 'description': 'High'}\nType: SensitiveData:S3Object/Credentials\nCategory: CLASSIFICATION\nOrigin Type: \"SENSITIVE_DATA_DISCOVERY_JOB\"\nSensitive Data Categories: \"['CREDENTIALS']\"\nResources affected:\nBucket=\"&lt;BUCKET_NAME&gt;\",\nKey=\"processed/sample-file-with-credentials.txt\"\nTrying to delete S3 Object: &nbsp;s3://&lt;BUCKET_NAME&gt;/processed/sample-file-with-credentials.txt\nFile deletion succeeded.\n\n-------------\nFull Macie finding event:\n{\n&nbsp; &nbsp;...\n}</code></pre></div><p>Additionally, the findings are visible on the Macie console, as shown in the following screenshot.</p><h3>Additional recommendations</h3><p>To further enhance the security and reliability of the Amazon Q Business application, we recommend implementing the following measures. These additional security and logging implementations make sure the data is protected, alerts are sent in response to potential warnings, and timely actions can be taken for security incidents.</p><ul><li><strong>Amazon CloudWatch logging for Amazon Q Business </strong>‚Äì You can use <a href=\"https://aws.amazon.com/pm/cloudwatch/?gclid=EAIaIQobChMIyb-6xaHNiAMVjTrUAR3xqw4uEAAYASAAEgIAd_D_BwE&amp;trk=2dfe7cfe-88b0-4c42-844b-24167b0dc800&amp;sc_channel=ps&amp;ef_id=EAIaIQobChMIyb-6xaHNiAMVjTrUAR3xqw4uEAAYASAAEgIAd_D_BwE:G:s&amp;s_kwcid=AL!4422!3!658520966141!!!g!!!19852661915!149878722660\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a> logging for Amazon Q Business to save the logs for the data source connectors and document-level errors, focusing particularly on failed ingestion jobs. This practice is vital from a security perspective because it allows monitoring and quick identification of issues in the data ingestion process. By tracking failed jobs, potential data loss or corruption can be mitigated, maintaining the reliability and completeness of the knowledge base.</li><li><strong>Unauthorized access monitoring on Amazon S3</strong> ‚Äì You can implement EventBridge rules to <a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-log-s3-data-events.html\" target=\"_blank\" rel=\"noopener\">monitor mutating API actions on the S3 buckets</a>. These rules are configured to invoke SNS notifications when such actions are performed by unauthorized users. Enable <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html\" target=\"_blank\" rel=\"noopener\">Amazon S3 server access logging </a>to store detailed access records in a designated bucket, which can be analyzed using <a href=\"http://aws.amazon.com/athena\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a> for deeper insights. This approach provides real-time alerts for immediate response to potential security breaches, while also maintaining a detailed audit trail for thorough security analysis, making sure that only authorized entities can modify critical data.</li></ul><p>In the following sections, we walk through implementing the end-to-end solution. For this solution to work, the following prerequisites are needed:</p><ul><li>A new or existing AWS account that will be the data collection account</li></ul><h2>Configure the data ingestion</h2><p>In this post, we demonstrate the solution using publicly available documentation as our sample dataset. In your implementation, you can adapt this solution to work with your organization‚Äôs specific content sources, such as support tickets, JIRA issues, internal wikis, or other relevant documentation.</p><ul><li>Ingestion Lambda function</li><li>Processing Lambda function</li></ul><p>The data ingestion workflow in this example fetches and processes public data from the Amazon Q Business and <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a> official documentation in PDF format. Specifically, the Ingest Data Lambda function downloads the raw PDF documents, temporarily stores them in Amazon S3, and passes their Amazon S3 URLs to the Process Data Lambda function, which performs the PII redaction (if enabled) and stores the processed documents and their metadata to the S3 path indexed by the Amazon Q Business application.</p><p>You can adapt the Step Functions Lambda code for ingestion and processing according to your own internal data, making sure that the documents and metadata are in a valid format for Amazon Q Business to index, and are properly redacted for PII data.</p><h2>Configure IAM Identity Center</h2><p>You can only have one IAM Identity Center instance per account. If your account already has an Identity Center instance, skip this step and proceed to configuring the Amazon Q Business application.</p><p>You will need to add details for a user such as user name, email, first name, and surname.</p><p>After deploying the CloudFormation template, you will receive an email where you will need to accept the invitation and change the password for the user.</p><p>Before logging in, you will need to deploy the Amazon Q Business application.</p><h2>Configure the Amazon Q Business application</h2><p>You will need to add details such as the IAM Identity Center stack name deployed previously and the S3 bucket name provisioned by the data ingestion stack.</p><p>After you deploy the CloudFormation template, complete the following steps to manage user access:</p><ol><li>On the Amazon Q Business console, choose  in the navigation pane.</li><li>Choose the application you provisioned ().</li><li>Under , choose .</li><li>On the  tab, choose the user you specified when deploying the CloudFormation stack.</li><li>Under , choose  or .</li><li>Choose  and then </li></ol><p>Now you can log in using the user you have specified. You can find the URL for the web experience under .</p><p>If you are unable to log in, make sure that the user has been verified.</p><p>Before you can use the Amazon Q Business application, the data source needs to be synchronized. The application‚Äôs data source is configured to sync hourly. It might take some time to synchronize.</p><p>When the synchronization is complete, you should now be able to access the application and ask questions.</p><p>After you‚Äôre done testing the solution, you can delete the resources to avoid incurring additional charges. See the <a href=\"https://aws.amazon.com/q/business/pricing/\" target=\"_blank\" rel=\"noopener\">Amazon Q Business pricing page</a> for more information. Follow the instructions in the <a href=\"https://github.com/aws-samples/amazon-q-business-simplified-integration-of-enterprise-knowledge-bases-at-scale/blob/main/README.md\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> to delete the resources and corresponding CloudFormation templates. Make sure to delete the CloudFormation stacks provisioned as follows:</p><ol><li>Delete the Amazon Q Business application stack.</li><li>Delete the IAM Identity Center stack.</li><li>Delete the data ingestion</li><li>For each deleted stack, check for any resources that were skipped in the deletion process, such as S3 buckets.</li></ol><p>Delete any skipped resources on the console.</p><p>In this post, we demonstrated how to build a knowledge base solution by integrating enterprise data with Amazon Q Business using Amazon S3. This approach helps organizations improve operational efficiency, reduce response times, and gain valuable insights from their historical data. The solution uses AWS security best practices to promote data protection while enabling teams to create a comprehensive knowledge base from various data sources.</p><p>Whether you‚Äôre managing support tickets, internal documentation, or other business content, this solution can handle multiple data sources and scale according to your needs, making it suitable for organizations of different sizes. By implementing this solution, you can enhance your operations with AI-powered assistance, automated responses, and intelligent routing of complex queries.</p><p>Try this solution with your own use case, and let us know about your experience in the comments section.</p><p> is a Senior Cloud Support Engineer at AWS, specializing in Data, Machine Learning, and Generative AI solutions. With extensive experience in helping customers architect and optimize their cloud-based AI/ML/GenAI workloads, Omar works closely with AWS customers to solve complex technical challenges and implement best practices across the AWS AI/ML/GenAI service portfolio. He is passionate about helping organizations leverage the full potential of cloud computing to drive innovation in generative AI and machine learning.</p><p> is a Principal Cloud Support Engineer at AWS, focused on Networking and Generative AI solutions. He has deep expertise in resolving complex, cross-domain technical challenges through systematic problem-solving methodologies. With a customer-obsessed mindset, he leverages emerging technologies to drive innovation and deliver exceptional customer experiences.</p><p> is a Principal Cloud Support Engineer at AWS. She specializes in solving complex customer issues on the AWS Cloud, focusing on infrastructure-as-code, container orchestration, and generative AI technologies. She collaborates with teams across AWS to design solutions that enhance the customer experience. Outside of work, Bhavani enjoys cooking and traveling.</p><p> is a Senior Cloud Support Engineer at AWS, specialized in Machine Learning technologies and Generative AI solutions, helping customers operate and optimize their ML workloads. With a deep passion for driving performance improvements, he dedicates himself to empowering both customers and teams through innovative ML-enabled solutions. Away from his technical pursuits, Mattia embraces his passion for travel and adventure.</p><p> is a Senior Cloud Support Engineer at AWS who specializes in Serverless technologies and development within the AWS cloud. Kevin has a passion for creating solutions through code while ensuring it is built on solid infrastructure. Outside of work, Kevin enjoys art and sport.</p><p> is a Senior Principal Engineer leading AWS. Tipu supports customers with designing and optimizing their cloud technology strategy as a senior principal engineer in AWS Support &amp; Managed Services. For over 15 years, he has designed, operated and supported diverse distributed systems at scale with a passion for operational excellence. He currently works on generative AI and operational excellence.</p>","contentLength":23230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Faster distributed graph neural network training with GraphStorm v0.4","url":"https://aws.amazon.com/blogs/machine-learning/faster-distributed-graph-neural-network-training-with-graphstorm-v0-4/","date":1739293396,"author":"Theodore Vasiloudis","guid":226,"unread":true,"content":"<p>GraphStorm is a low-code enterprise graph machine learning (ML) framework that provides ML practitioners a simple way of building, training, and deploying graph ML solutions on industry-scale graph data. Although GraphStorm can run efficiently on single instances for small graphs, it truly shines when scaling to enterprise-level graphs in distributed mode using a cluster of <a href=\"http://aws.amazon.com/ec2\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instances or <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a>.</p><p>Today, AWS AI released GraphStorm v0.4. This release introduces integration with DGL-GraphBolt, a new graph storage and sampling framework that uses a compact graph representation and pipelined sampling to reduce memory requirements and speed up Graph Neural Network (GNN) training and inference. For the large-scale dataset examined in this post, the inference speedup is 3.6 times faster, and per-epoch training speedup is 1.4 times faster, with even larger speedups possible.</p><p>To achieve this, GraphStorm v0.4 with DGL-GraphBolt addresses two crucial challenges of graph learning:</p><ul><li> ‚Äì GraphStorm v0.4 provides compact and distributed storage of graph structure and features, which may grow in the multi-TB range. For example, a graph with 1 billion nodes with 512 features per node and 10 billion edges will require more than 4 TB of memory to store, which necessitates distributed computation.</li><li> ‚Äì In multi-layer GNNs, you need to sample neighbors of each node to propagate their representations. This can lead to exponential growth in the number of nodes sampled, potentially visiting the entire graph for a single node‚Äôs representation. GraphStorm v0.4 provides efficient, pipelined graph sampling.</li></ul><p>In this post, we demonstrate how GraphBolt enhances GraphStorm‚Äôs performance in distributed settings. We provide a hands-on example of using GraphStorm with GraphBolt on SageMaker for distributed training. Lastly, we share how to use <a href=\"https://aws.amazon.com/sagemaker/pipelines/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Pipelines</a> with GraphStorm.</p><h2>GraphBolt: Pipeline-driven graph sampling</h2><p>GraphBolt is a new data loading and graph sampling framework developed by the <a href=\"https://www.dgl.ai/\" target=\"_blank\" rel=\"noopener\">DGL</a> team. It streamlines the operations needed to sample efficiently from a heterogeneous graph and fetch the corresponding features. GraphBolt introduces a new, more compact graph structure representation for heterogeneous graphs, called fused Compressed Sparse Column (fCSC). This can reduce the memory cost of storing a heterogeneous graph by up to 56%, allowing users to fit larger graphs in memory and potentially use smaller, more cost-efficient instances for GNN model training.</p><p>GraphStorm v0.4 seamlessly integrates with GraphBolt, allowing users to take advantage of its performance improvements in their GNN workflows. The user just needs to provide the additional argument  when launching graph construction and training jobs.</p><p>A common model development process is to perform model exploration locally on a subset of your full data, and when you‚Äôre satisfied with the results, train the full-scale model. This setup allows for cheaper exploration before training on the full dataset. GraphStorm and SageMaker Pipelines allows you to do that by creating a model pipeline you can run locally to retrieve model metrics, and when you‚Äôre ready, run your pipeline on the full data on SageMaker, and produce models, predictions, and graph embeddings to use in downstream tasks. In the next section, we show how to set up such pipelines for GraphStorm.</p><p>We demonstrate such a setup in the following diagram, where a user can perform model development and initial training on a single EC2 instance, and when they‚Äôre ready to train on their full data, hand off the heavy lifting to SageMaker for distributed training. Using SageMaker Pipelines to train models provides several benefits, like reduced costs, auditability, and lineage tracking.</p><h2>Set up the environment for SageMaker distributed training</h2><p>Setting up your environment should take around 10 minutes. First, set up your Python environment to run the examples:</p><div><div><div><pre><code>conda init\neval $SHELL\n# Create a new env for the post\nconda create --name gsf python=3.10\nconda activate gsf\n\n# Install dependencies for local scripts\npip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu\npip install sagemaker boto3 ogb pyarrow\n# Verify installation, might take a few minutes for first run\npython -c \"import sagemaker; import torch\"\n\n# Clone the GraphStorm repository to access the example code\ngit clone https://github.com/awslabs/graphstorm.git ~/graphstorm</code></pre></div></div></div><h3>Build a GraphStorm SageMaker CPU image</h3><p>Next, build and push the GraphStorm PyTorch Docker image that you will use to run the graph construction, training, and inference jobs for smaller-scale data. Your role will need to be able to pull images from the <a href=\"https://gallery.ecr.aws/\" target=\"_blank\" rel=\"noopener\">Amazon ECR Public Gallery</a> and create <a href=\"http://aws.amazon.com/ecr/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Container Registry</a> (Amazon ECR) repositories and push images to your private ECR registry.</p><div><div><pre><code># Enter you account ID here\nACCOUNT_ID=&lt;aws-account-id&gt;\nREGION=us-east-1\n\ncd ~/graphstorm\nbash docker/build_graphstorm_image.sh --environment sagemaker --device cpu\nbash docker/push_graphstorm_image.sh -e sagemaker -r $REGION -a $ACCOUNT_ID -d cpu\n# This will create an ECR repository and push an image to\n# ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/graphstorm:sagemaker-cpu</code></pre></div></div><h3>Download and prepare datasets</h3><p>In this post, we use two citation datasets to demonstrate the scalability of GraphStorm. The Open Graph Benchmark (OGB) project hosts a number of graph datasets that can be used to benchmark the performance of graph learning systems. For a small-scale demo, we use the ogbn-arxiv dataset, and for a demonstration of GraphStorm‚Äôs large-scale learning capabilities, we use the ogbn-papers100M dataset.</p><h4>Prepare the ogbn-arxiv dataset</h4><p>Download the smaller-scale <a href=\"https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv\" target=\"_blank\" rel=\"noopener\">ogbn-arxiv</a> dataset to run a local test before launching larger-scale SageMaker jobs on AWS. This dataset has approximately 170,000 nodes and 1.2 million edges. Use the following code to download the data and prepare it for GraphStorm:</p><div><div><pre><code># Provide the S3 bucket to use for output\nBUCKET_NAME=&lt;your-s3-bucket&gt;</code></pre></div></div><div><pre><code>cd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\npython convert_arxiv_to_gconstruct.py \\\n--output-s3-prefix s3://$BUCKET_NAME/ogb-arxiv-input</code></pre></div><p>This will create the tabular graph data in Amazon S3, which you can verify by running the following code:</p><div><pre><code>aws s3 ls s3://$BUCKET_NAME/ogb-arxiv-input/ \nedges/\nnodes/\nsplits/\ngconstruct_config_arxiv.json</code></pre></div><p>Finally, upload GraphStorm training configuration files for arxiv to use for training and inference:</p><div><pre><code># Upload the training configurations to S3\naws s3 cp ~/graphstorm/training_scripts/gsgnn_np/arxiv_nc.yaml \\\ns3://$BUCKET_NAME/yaml/arxiv_nc_train.yaml\naws s3 cp ~/graphstorm/inference_scripts/np_infer/arxiv_nc.yaml \\\ns3://$BUCKET_NAME/yaml/arxiv_nc_inference.yaml</code></pre></div><h4>Prepare the ogbn-papers100M dataset on SageMaker</h4><p>The papers-100M dataset is a large-scale graph dataset, with 111 million nodes and 3.2 billion edges after adding reverse edges.</p><p>To download and preprocess the data as an <a href=\"https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Processing</a> step, use the following code. You can launch and let the job run in the background while proceeding through the rest of the post, and return to this dataset later. The job should take approximately 45 minutes to run.</p><div><pre><code># Navigate to the example code\ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\n\n# Build and push a Docker image to download and process the papers100M data\nbash build_and_push_papers100M_image.sh -a $ACCOUNT_ID -r $REGION\n\n# This creates an ECR repository and pushes an image to\n# $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/papers100m-processor\n\n# Run a SageMaker job to do the processing and upload the output to S3\nSAGEMAKER_EXECUTION_ROLE_ARN=&lt;your-sagemaker-execution-role-arn&gt;\naws configure set region $REGION\npython sagemaker_convert_papers100m.py \\\n--output-bucket $BUCKET_NAME \\\n--execution-role-arn $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--region $REGION \\\n--instance-type ml.m5.4xlarge \\\n--image-uri $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/papers100m-processor</code></pre></div><p>This will produce the processed data in <code>s3://$BUCKET_NAME/ogb-papers100M-input</code>, which can then be used as input to GraphStorm. While this job is running, you can create the GraphStorm pipelines.</p><h2>Create a SageMaker pipeline</h2><div><pre><code># Navigate to the example code\ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\n\nPIPELINE_NAME=\"ogbn-arxiv-gs-pipeline\"\n\nbash deploy_arxiv_pipeline.sh \\\n--account $ACCOUNT_ID\\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME \\\n--use-graphbolt false</code></pre></div><p>Running the preceding code will create a SageMaker pipeline configured to run three SageMaker jobs in sequence:</p><ul><li>A GConstruct job that converts the tabular file input to a binary partitioned graph on Amazon S3</li><li>A GraphStorm training job that trains a node classification model and saves the model to Amazon S3</li><li>A GraphStorm inference job that produces predictions for all nodes in the test set, and creates embeddings for all nodes</li></ul><p>To review the pipeline, navigate to <a href=\"https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/studio-landing\" target=\"_blank\" rel=\"noopener\">SageMaker AI Studio</a>, choose the domain and user profile you used to create the pipeline, then choose .</p><p>In the navigation pane, choose . There should be a pipeline named . Choose the pipeline, which will take you to the  tab for the pipeline. Choose  to view the pipeline steps.</p><h3>Run the SageMaker pipeline locally for ogbn-arxiv</h3><p>The ogbn-arxiv dataset is small enough that you can run the pipeline locally. Run the following command to start a local execution of the pipeline:</p><div><pre><code># Allow the local containers to inherit AWS credentials\nexport USE_SHORT_LIVED_CREDENTIALS=1\npython ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \\\n--pipeline-name ogbn-arxiv-gs-pipeline \\\n--region us-east-1 \\\n--local-execution | tee arxiv-local-logs.txt</code></pre></div><p>We save the log output to . You will use that later to analyze the training speed.</p><p>Running the pipeline should take approximately 5 minutes. When the pipeline is complete, it will print a message like the following:</p><div><pre><code>Pipeline execution 655b9357-xxx-xxx-xxx-4fc691fcce94 SUCCEEDED</code></pre></div><p>You can inspect the mean epoch and evaluation time using the provided  script and the log file you created:</p><div><pre><code>python analyze_training_time.py --log-file arxiv-local-logs.txt\n\nReading logs from file: arxiv-local-logs.txt\n\n=== Training Epochs Summary ===\nTotal epochs completed: 10\nAverage epoch time: 4.70 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 11\nAverage evaluation time: 1.90 seconds</code></pre></div><p>These numbers will vary depending on your instance type; in this case, these are values reported on an m6in.4xlarge instance.</p><h3>Create a GraphBolt pipeline</h3><p>Now you have established a baseline for performance, you can create another pipeline that uses the GraphBolt graph representation to compare the performance.</p><p>You can use the same pipeline creation script, but change two variables, providing a new pipeline name and setting  to :</p><div><pre><code># Deploy a GraphBolt-enabled pipeline\nPIPELINE_NAME_GB=\"ogbn-arxiv-gs-graphbolt-pipeline\"\nbash deploy_arxiv_pipeline.sh \\\n--account $ACCOUNT_ID \\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--use-graphbolt true\n\n# Execute the pipeline locally\npython ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--region us-east-1 \\\n--local-execution | tee arxiv-local-gb-logs.txt</code></pre></div><p>Analyzing the training logs, you can see the per-epoch time has dropped somewhat:</p><div><pre><code>python analyze_training_time.py --log-file arxiv-local-gb-logs.txt\n\nReading logs from file: arxiv-local-gb-logs.txt\n\n=== Training Epochs Summary ===\nTotal epochs completed: 10\nAverage epoch time: 4.21 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 11\nAverage evaluation time: 1.63 seconds</code></pre></div><p>For such a small graph, the performance gains are modest, around 13% per epoch time. With large data, the potential gains are much greater. In the next section, you will create a pipeline and train a model for papers-100M, a citation graph with 111 million nodes and 3.2 billion edges.</p><h2>Create a SageMaker pipeline for distributed training</h2><p>After the SageMaker processing job that prepares the papers-100M data has finished processing and the data is stored in Amazon S3, you can set up a pipeline to train a model on that dataset.</p><h3>Build the GraphStorm GPU image</h3><p>For this job, you will use large GPU instances, so you will build and push the GPU image this time:</p><div><pre><code>cd ~/graphstorm\n\nbash ./docker/build_graphstorm_image.sh --environment sagemaker --device gpu\n\nbash docker/push_graphstorm_image.sh -e sagemaker -r $REGION -a $ACCOUNT_ID -d gpu</code></pre></div><h3>Deploy and run pipelines for papers-100M</h3><p>Before you deploy your new pipeline, upload the training YAML configuration for papers-100M to Amazon S3:</p><div><pre><code>aws s3 cp \\\n~/graphstorm/training_scripts/gsgnn_np/papers100M_nc.yaml \\\ns3://$BUCKET_NAME/yaml/</code></pre></div><p>Now you are ready to deploy your initial pipeline for papers-100M:</p><div><pre><code># Navigate to the example code \ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt \nPIPELINE_NAME=\"ogb-papers100M-pipeline\" \nbash deploy_papers100M_pipeline.sh \\ \n    --account $ACCOUNT_ID \\\n    --bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n    --pipeline-name $PIPELINE_NAME \\ \n    --use-graphbolt false</code></pre></div><p>Run the pipeline on SageMaker and let it run in the background:</p><div><pre><code># Navigate to the example code\ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\n\nPIPELINE_NAME=\"ogb-papers100M-pipeline\"\nbash deploy_papers100M_pipeline.sh \\\n--account $ACCOUNT_ID \\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME \\\n--use-graphbolt false</code></pre></div><p>Your account needs to meet the required quotas for the requested instances. For this post, the defaults are set to four  for training jobs and one  instance for a processing job. To adjust your SageMaker service quotas, you can use the <a href=\"https://us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas\" target=\"_blank\" rel=\"noopener\">Service Quotas console</a>. To run both pipelines in parallel, i.e. without GraphBolt and with GraphBolt, you will need 8 x  and 2 x </p><p>Next, you can deploy and run another pipeline, with GraphBolt enabled:</p><div><pre><code># Deploy the GraphBolt-enabled pipeline\nPIPELINE_NAME_GB=\"ogb-papers100M-graphbolt-pipeline\"\nbash deploy_papers100M_pipeline.sh \\\n--account $ACCOUNT_ID\\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--use-graphbolt true\n\n# Execute the GraphBolt pipeline on SageMaker\npython ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--region us-east-1 \\\n--async-execution</code></pre></div><h3>Compare performance for GraphBolt-enabled training</h3><p>After both pipelines are complete, which should take approximately 4 hours, you can compare the training times for both cases.</p><p>On the page of the SageMaker console, there should be two new pipelines named  and <code>ogb-papers100M-graphbolt-pipeline</code>. Choose ogb-papers100M-pipeline, which will take you to the  tab for the pipeline. Copy the name of the latest successful execution and use that to run the training analysis script:</p><div><pre><code>python analyze_training_time.py \\\n--pipeline-name $PIPELINE_NAME\\\n--execution-name execution-1734404366941</code></pre></div><p>Your output will look like the following code:</p><div><pre><code>== Training Epochs Summary ===\nTotal epochs completed: 15\nAverage epoch time: 73.95 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 15\nAverage evaluation time: 15.07 seconds</code></pre></div><p>Now do the same for the GraphBolt-enabled pipeline:</p><div><pre><code>python analyze_training_time.py \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--execution-name execution-1734463209078</code></pre></div><p>You will see the improved per-epoch and evaluation times:</p><div><pre><code>== Training Epochs Summary ===\nTotal epochs completed: 15\nAverage epoch time: 54.54 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 15\nAverage evaluation time: 4.13 seconds</code></pre></div><p>Without loss in accuracy, the latest version of GraphStorm achieved a speedup of approximately 1.4 times faster per epoch for training, and a speedup of 3.6 times faster in evaluation time! Depending on the dataset, the speedups can be even greater, as shown by the DGL team‚Äôs <a href=\"https://www.dgl.ai/release/2024/03/06/release.html\" target=\"_blank\" rel=\"noopener\">benchmarking</a>.</p><p>This post showcased how GraphStorm 0.4, integrated with DGL-GraphBolt, significantly speeds up large-scale GNN training and inference, by 1.4 and 3.6 times faster, respectively, as measured on the papers-100M dataset. As shown in the <a href=\"https://www.dgl.ai/release/2024/01/26/release.html\" target=\"_blank\" rel=\"noopener\">DGL benchmarks</a>, even larger speedups are possible depending on the dataset.</p><p>We encourage ML practitioners working with large graph data to try GraphStorm. Its low-code interface simplifies building, training, and deploying graph ML solutions on AWS, allowing you to focus on modeling rather than infrastructure.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/04/badge-photo-small-copy.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior Applied Scientist at Amazon Web Services, where he works on distributed machine learning systems and algorithms. He led the development of GraphStorm Processing, the distributed graph processing library for GraphStorm and is a core developer for GraphStorm. He received his PhD in Computer Science from the KTH Royal Institute of Technology, Stockholm, in 2019.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/04/xiang.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior Applied Scientist at Amazon Web Services, where he develops deep learning frameworks including GraphStorm, DGL, and DGL-KE. He led the development of Amazon Neptune ML, a new capability of Neptune that uses graph neural networks for graphs stored in a Neptune graph database. He is now leading the development of GraphStorm, an open source graph machine learning framework for enterprise use cases. He received his PhD in computer systems and architecture at the Fudan University, Shanghai, in 2014.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/12/30/florian.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Principal Technical Product Manager at AWS AI/ML research supporting science teams like the graph machine learning group, and ML Systems teams working on large scale distributed training, inference, and fault resilience. Before joining AWS, Florian lead technical product management for automated driving at Bosch, was a strategy consultant at McKinsey &amp; Company, and worked as a control systems and robotics scientist‚Äîa field in which he holds a PhD.</p>","contentLength":17809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a Useful Voice-Activated Fully Local RAG System","url":"https://www.kdnuggets.com/creating-useful-voice-activated-fully-local-rag-system","date":1739293249,"author":"Cornellius Yudha Wijaya","guid":585,"unread":true,"content":"<article>This article will explore initiating the RAG system and making it fully voice-activated.</article>","contentLength":88,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/kdn-wijaya-creating-useful-voice-activated-rag.png","enclosureMime":"","commentsUrl":null},{"title":"Python Insider: Python 3.14.0 alpha 5 is out","url":"https://pythoninsider.blogspot.com/2025/02/python-3140-alpha-5-is-out.html","date":1739291158,"author":"","guid":270,"unread":true,"content":"<p>Here comes the antepenultimate alpha.</p><p><strong>This is an early developer preview of Python\n3.14</strong></p><p>Python 3.14 is still in development. This release, 3.14.0a5, is the\nfifth of seven planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2025-05-06) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2025-07-22). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.14 are still being planned and\nwritten. Among the new major new features and changes so far:</p><p>The next pre-release of Python 3.14 will be the penultimate alpha,\n3.14.0a6, currently scheduled for 2025-03-14.</p><p>2025-01-29 marked the start of a new lunar year, the Year of the\nSnake üêç (and the Year of Python?).</p><p>For centuries, œÄ was often approximated as 3 in China. Some time\nbetween the years 1 and 5 CE, astronomer, librarian, mathematician and\npolitician Liu Xin (ÂäâÊ≠Ü) calculated œÄ as 3.154.</p><p>Around 130 CE, mathematician, astronomer, and geographer Zhang Heng\n(ÂºµË°°, 78‚Äì139) compared the celestial circle with the diameter of the\nearth as 736:232 to get 3.1724. He also came up with a formula for the\nratio between a cube and inscribed sphere as 8:5, implying the ratio of\na square‚Äôs area to an inscribed circle is ‚àö8:‚àö5. From this, he\ncalculated œÄ as ‚àö10 (~3.162).</p><p>Third century mathematician Liu Hui (ÂàòÂæΩ) came up with an algorithm\nfor calculating œÄ iteratively: calculate the area of a polygon inscribed\nin a circle, then as the number of sides of the polygon is increased,\nthe area becomes closer to that of the circle, from which you can\napproximate œÄ.</p><p>This algorithm is similar to the method used by Archimedes in the 3rd\ncentury BCE and Ludolph van Ceulen in the 16th century CE (see <a href=\"https://blog.python.org/2024/11/python-3140-alpha-2-released.html\">3.14.0a2\n  release notes</a>), but Archimedes only went up to a 96-sided polygon\n(96-gon). Liu Hui went up to a 192-gon to approximate œÄ as 157/50 (3.14)\nand later a 3072-gon for 3.14159.</p><p>Liu Hu wrote a commentary on the book The Nine Chapters on the\nMathematical Art which included his œÄ approximations.</p><p>In the fifth century, astronomer, inventor, mathematician,\npolitician, and writer Zu Chongzhi (Á•ñÊ≤ñ‰πã, 429‚Äì500) used Liu Hui‚Äôs\nalgorithm to inscribe a 12,288-gon to compute œÄ between 3.1415926 and\n3.1415927, correct to seven decimal places. This was more accurate than\nHellenistic calculations and wouldn‚Äôt be improved upon for 900\nyears.</p><p>Thanks to all of the many volunteers who help make Python Development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf-landing/\">Python Software\nFoundation</a>.</p><p>Regards from a remarkably snowless Helsinki,</p><p>Your release team, Hugo van KemenadeSteve Dower</p>","contentLength":2941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real Python: Building a Python Command-Line To-Do App With Typer","url":"https://realpython.com/courses/build-command-line-todo-app-typer/","date":1739282400,"author":"","guid":182,"unread":true,"content":"<p>Building an application to manage your  can be an interesting project when you‚Äôre learning a new programming language or trying to take your skills to the next level. In this video course, you‚Äôll build a functional to-do application for the command line using Python and <a href=\"https://typer.tiangolo.com/\">Typer</a>, which is a relatively young library for creating powerful command-line interface (CLI) applications in almost no time.</p><p>With a project like this, you‚Äôll apply a wide set of core programming skills while building a real-world application with real features and requirements.</p><p><strong>In this video course, you‚Äôll learn how to:</strong></p><ul><li>Build a functional  with a  in Python</li><li>Use Typer to add , , and  to your to-do app</li><li>Test your Python to-do application with Typer‚Äôs  and </li></ul>","contentLength":734,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"10 Little-Known Python Libraries That Will Make You Feel Like a Data Wizard","url":"https://www.kdnuggets.com/10-little-known-python-libraries-data-wizard","date":1739278813,"author":"Josep Ferrer","guid":584,"unread":true,"content":"<article>In this article, I will introduce you to 10 little-known Python libraries every data scientist should know.</article>","contentLength":107,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Ferrer_10_little_known_libraries1.png","enclosureMime":"","commentsUrl":null},{"title":"Introducing the CrossValidationReport","url":"https://www.youtube.com/watch?v=R6dRAE83Y2c","date":1739277902,"author":"probabl","guid":529,"unread":true,"content":"<article>Skore version 0.6 introduces the `CrossValidationReport` that provides you with an `EstimatorReport` for each fold of your cross-validation, enabling you to inspect your estimator on each fold.\n\n------\nLinks:\n\nskore v0.6 documentation:\nhttps://skore.probabl.ai/0.6/index.html\n\nskore GitHub repository:\nhttps://github.com/probabl-ai/skore\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\nBluesky: https://bsky.app/profile/probabl.bsky.social\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":722,"flags":null,"enclosureUrl":"https://www.youtube.com/v/R6dRAE83Y2c?version=3","enclosureMime":"","commentsUrl":null},{"title":"Kushal Das: Using openpgp-card-tool-git with git","url":"https://kushaldas.in/posts/using-openpgp-card-tool-git-with-git.html","date":1739272360,"author":"","guid":269,"unread":true,"content":"<p>One of the power of Unix systems comes from the various small tools and how\nthey work together. One such new tool I am using for some time is for  &amp;  using OpenPGP and my Yubikey for the actual signing\noperation via\n<a href=\"https://crates.io/crates/openpgp-card-tool-git\">openpgp-card-tool-git</a>. I\nreplaced the standard  for this usecase with the  command from this\nproject.</p><h3>Installation &amp; configuration</h3><pre><code>cargo install openpgp-card-tool-git\n</code></pre><p>Then you will have to configuration your (in my case the global configuration) git configuration.</p><pre><code>git config --global gpg.program &lt;path to oct-git&gt;\n</code></pre><p>I am assuming that you already had it configured before for signing, otherwise\nyou have to run the following two commands too.</p><pre><code>git config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n</code></pre><p>Before you start using it, you want to save the pin in your system keyring.</p><p>Use the following command.</p><p>That is it, now your  will sign the commits using  tool.</p><p>In the next blog post I will show how to use the other tools from the \n<a href=\"https://crates.io/users/hko-s\">author</a> for various different OpenPGP oeprations.</p>","contentLength":1007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust vs. Go (Golang): Performance (Fastest Frameworks + PostgreSQL)","url":"https://www.youtube.com/watch?v=31R8Ef9A0iw","date":1739265918,"author":"Anton Putra","guid":536,"unread":true,"content":"<article>Go vs Rust Performance Comparison. Go (1.23.6) vs Rust (1.84.1).\n\nüî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)\n\nüçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&amp;si=p-UOaVM_6_SFx52H\n\nüëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience &amp; Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)\n‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra\n‚ñ∫  Twitter/X: https://twitter.com/antonvputra\n‚ñ∫  GitHub: https://github.com/antonputra\n‚ñ∫  Email: me@antonputra.com\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/244/lessons/244\n\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/429\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/431\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/433\n\n#rust #golang #devops</article>","contentLength":1179,"flags":null,"enclosureUrl":"https://www.youtube.com/v/31R8Ef9A0iw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Stateful vs. Stateless Architecture","url":"https://blog.algomaster.io/p/stateful-vs-stateless-architecture","date":1739263586,"author":"Ashish Pratap Singh","guid":750,"unread":true,"content":"<p>When a client interacts with a server, there are two ways to handle it:</p><ul><li><p> The client includes all necessary data in each request, so the server doesn‚Äôt store any prior information.</p></li><li><p> The server retains some data from previous requests, making future interactions dependent on past state.</p></li></ul><blockquote><p>In software systems,  refers to any data that persists across requests, such as user sessions, shopping carts, or authentication details.</p></blockquote><p>The choice between stateless and stateful architecture can affect scalability, performance, complexity, and cost.</p><p>In this article, we‚Äôll break down both the approaches, their advantages and trade-offs, and when to use each‚Äîwith real-world examples.</p><p>If you‚Äôre finding this newsletter valuable and want to deepen your learning, consider becoming a .</p><p>As a paid subscriber, you'll receive an <strong>exclusive deep-dive article</strong> every week, access to a structured100+topics and interview questions, and other .</p><p>In a , the system remembers client or process data () across multiple requests.</p><p>Once a client connects, the server holds on to certain details‚Äîlike user preferences, shopping cart contents, or authentication sessions‚Äîso the client doesn‚Äôt need to resend everything with each request.</p><p>Stateful systems typically store the state data in a database or in-memory storage.</p><blockquote><p> During online shopping, when you add items to your cart, the website remembers your selections. If you navigate away to browse more items and then return to your cart, your items are still there, waiting for you to check out.</p></blockquote><h2>Common Patterns in Stateful Architecture</h2><p>If you use  session storage (i.e., each app server keeps its own sessions locally), you can configure your load balancer for ‚Äústicky sessions.‚Äù </p><p>This means: Once a client is assigned to , all subsequent requests from that client are routed to .</p><blockquote><p>: If Server A fails, the user‚Äôs session data is lost or the user is forced to re-log in. Sticky sessions are also less flexible when scaling because you can‚Äôt seamlessly redistribute user traffic to other servers.</p></blockquote><h3>2. Centralized Session Store</h3><p>A more robust approach is to store session data in a  or  store (e.g., Redis). </p><ul><li><p>: All servers can access and update session data for any user. Any server can handle any request, because the session data is not tied to a specific server‚Äôs memory.</p></li></ul><blockquote><p>: You introduce network overhead and rely on an external storage. If the centralized storage fails, you lose session data unless you have a fallback strategy.</p></blockquote><ul><li><p><strong>Personalized Experiences:</strong> Stateful systems can deliver highly tailored interactions, as they remember user preferences and past actions.</p></li><li><p> Users can seamlessly resume activities where they left off, even if they disconnect and reconnect.</p></li><li><p> Certain operations can be faster because the server already possesses necessary data.</p></li></ul><ul><li><p> Maintaining state for a large number of users can become resource-intensive and complex, as each server needs to keep track of specific sessions.</p></li><li><p> Managing and synchronizing state across multiple servers (if needed) introduces additional challenges.</p></li><li><p> If a server holding a user's state fails, their session data might be lost.</p></li></ul><ul><li><p><strong>E-commerce Shopping Carts</strong> ‚Äì Stores cart contents and user preferences across multiple interactions, even if the user navigates away and returns.</p></li><li><p><strong>Video Streaming Services (Netflix, YouTube)</strong> ‚Äì Remembers user watch progress, recommendations, and session data for a seamless experience.</p></li><li><p><strong>Messaging Apps (WhatsApp, Slack)</strong> ‚Äì Maintains active user sessions and message history for real-time communication.</p></li></ul><p>In a  architecture, the server does  preserve client-specific data between individual requests.</p><ul><li><p>Each request is treated as , with no memory of previous interactions.</p></li><li><p>Every request must include <strong>all necessary information</strong> for processing.</p></li><li><p>Once the server responds, it <strong>discards any temporary data</strong> used for that request.</p></li></ul><blockquote><p>: Most  follow a stateless design. For instance, when you request weather data from a public API, you must provide all required details (e.g., location) in each request. The server processes it, sends a response, and forgets the interaction.</p></blockquote><h2>Common Patterns in Stateless Architecture</h2><h3>1. Token-Based Authentication (JWT)</h3><p>A very popular way to implement statelessness is through tokens, particularly  (JSON Web Tokens):</p><ol><li><p><strong>Client Authenticates Once</strong>: The user logs in using credentials (username/password) for the first time, and the server issues a signed .</p></li><li><p>: The client includes JWT token in each request (e.g., <code>Authorization: Bearer &lt;token&gt;</code> header).</p></li><li><p>: The server validates the token‚Äôs signature and any embedded claims (e.g., user ID, expiry time).</p></li><li><p>: The server does  need to store session data; it just verifies the token on each request.</p></li></ol><blockquote><p>Many APIs, including OAuth-based authentication systems, use JWTs to enable stateless, scalable authentication.</p></blockquote><p>Stateless architectures benefit from , ensuring that repeated requests produce the same result. This prevents inconsistencies due to network retries or client errors.</p><p> A  request with the same payload  updates the user‚Äôs data but doesn‚Äôt create duplicates.</p><blockquote><p>Idempotent APIsensures consistency and reliability, especially in distributed systems where requests might be retried automatically.</p></blockquote><ul><li><p> Stateless systems are inherently easier to scale horizontally. New servers can be added effortlessly, as they don't need to maintain any specific user sessions.</p></li><li><p> Since servers don't track state, the architecture is generally simpler and easier to manage.</p></li><li><p> The failure of a single server won't disrupt user sessions, as data isn't tied to specific servers.</p></li><li><p>With no session data stored on the server, you free up memory that would otherwise be reserved for session management.</p></li><li><p><strong>Easier to Cache Responses: </strong>Since requests are self-contained, caching layers (like CDNs) can more easily store and serve responses.</p></li></ul><ul><li><p> Stateless systems can't provide the same level of personalization or context awareness as stateful systems without additional effort (like using cookies or tokens).</p></li><li><p>The client must keep track of the authentication token or relevant data. If it loses the token, it must re-authenticate.</p></li><li><p> Every request needs to carry all the required information, potentially leading to larger payloads.</p></li></ul><ol><li><p><strong>Microservices Architecture: </strong>Each service handles requests independently, relying on external databases or caches instead of maintaining session data.</p></li><li><p><strong>Public APIs (REST, GraphQL): </strong>Clients send tokens with each request, eliminating the need for server-side sessions.</p></li><li><p>Tokens are securely stored on the device and sent with every request to authenticate users.</p></li><li><p>Stateless endpoints make caching easier since responses depend only on request parameters, not stored session data. A CDNcan cache and serve repeated requests, improving performance and reducing backend load.</p></li></ol><p>There's no one-size-fits-all answer when choosing between stateful and stateless architectures.</p><p>The best choice depends on your application‚Äôs needs, scalability goals, and user experience expectations.</p><h3><strong>When to Choose Stateful Architecture</strong></h3><p>Stateful systems are ideal when <strong>user context and continuity</strong> are critical. </p><p>Consider a stateful approach if your application:</p><ul><li><p>Requires personalization (e.g., user preferences, session history)</p></li><li><p>Needs real-time interactions (e.g., chat applications, multiplayer gaming)</p></li><li><p>Manages multi-step workflows (e.g., online banking transactions, checkout processes)</p></li><li><p>Must retain authentication sessions for security and convenience</p></li></ul><blockquote><p> A shopping cart in an e-commerce app should persist, so users don‚Äôt have to re-add items after refreshing the page.</p></blockquote><h3><strong>When to Choose Stateless Architecture</strong></h3><p>Stateless systems work best when <strong>scalability, simplicity, and resilience</strong> are top priorities. </p><p>Use a stateless approach if your application:</p><ul><li><p>Handles a high volume of requests and needs to scale efficiently</p></li><li><p>Doesn‚Äôt require storing client-specific data between requests</p></li><li><p>Needs fast, distributed processing without server dependencies</p></li><li><p>Must ensure reliability and failover readiness</p></li></ul><blockquote><p> A weather API doesn‚Äôt need to remember previous requests. Each query includes the location, and the response is processed independently.</p></blockquote><h3><strong>Hybrid Approaches: The Best of Both Worlds</strong></h3><p>Many modern applications  stateful and stateless components for flexibility.</p><p>This hybrid approach allows:</p><ul><li><p>Stateless APIs for core functionality, ensuring high scalability</p></li><li><p>Stateful sessions for personalization, improving user experience</p></li><li><p>External session stores (e.g., Redis) to manage state while keeping app servers stateless</p></li></ul><blockquote><p> A video streaming platform (e.g., Netflix) uses a stateless backend for streaming but retains stateful user sessions to track watch history and recommendations.</p></blockquote><p>If you found it valuable, hit a like ‚ù§Ô∏è and consider subscribing for more such content every week.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/how-i-mastered-data-structures-and-algorithms?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQ1NjU1MjUyLCJpYXQiOjE3MjE1MjE3MzEsImV4cCI6MTcyNDExMzczMSwiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.2cNY811YEugd5iH9XJQhakBzyahGqF7PcATBlFj5J2w&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you‚Äôre enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":8861,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/7e4801c3-e3aa-4ab6-8fe6-759af4a1f91a_1684x1196.png","enclosureMime":"","commentsUrl":null},{"title":"Django Weblog: DSF member of the month - Lily Foote","url":"https://www.djangoproject.com/weblog/2025/feb/10/dsf-member-of-the-month-lily-foote/","date":1739249491,"author":"","guid":268,"unread":true,"content":"<p>For February 2025, we welcome Lily Foote (<a href=\"https://fosstodon.org/@lilyf\">@lilyf</a>) as our DSF member of the month! ‚≠ê</p><p>Lily Foote is a contributor to Django core for many years, especially on the ORM. She is currently a member of the Django 6.x <a href=\"https://docs.djangoproject.com/en/dev/internals/organization/#steering-council\">Steering Council</a> and she has been a DSF member since March 2021. \nYou can learn more about Lily by visiting <a href=\"https://github.com/LilyFoote\">her GitHub profile</a>.</p><p>Let‚Äôs spend some time getting to know Lily better!</p><h4>Can you tell us a little about yourself (hobbies, education, etc)</h4><p>My name is Lily Foote and I‚Äôve been contributing to Django for most of my career. I‚Äôve also recently got into Rust and I‚Äôm excited about using Rust in Python projects. When I‚Äôm not programming, I love hiking, climbing and dancing (Ceilidh)! I also really enjoying playing board games and role playing games (e.g. Dungeons and Dragons).</p><h4>How did you start using Django?</h4><p>I‚Äôd taught myself Python in my final year at university by doing <a href=\"https://projecteuler.net/\">Project Euler</a> problems and then decided I wanted to learn how to make a website. Django was the first Python web framework I looked at and it worked really well for me.</p><h4>What other framework do you know and if there is anything you would like to have in Django if you had magical powers?</h4><p>I‚Äôve done a small amount with Flask and FastAPI. More than any new features, I think the thing that I‚Äôd most like to see is more long-term contributors to spread the work of keeping Django awesome.</p><h4>What projects are you working on now?</h4><p>The side project I‚Äôm most excited about is <a href=\"https://github.com/LilyFoote/django-rusty-templates\">Django Rusty Templates</a>, which is a re-implementation of Django‚Äôs templating language in Rust.</p><h4>Which Django libraries are your favorite (core or 3rd party)?</h4><h4>What are the top three things in Django that you like?</h4><p>Django Conferences, the mentorship program Djangonaut Space and the whole community!</p><h4>You have been a mentor multiple times with GSoC and Djangonaut Space program, what is required according to you to be a good mentor?</h4><p>I think being willing to invest time is really important. Checking in with your mentees frequently and being an early reviewer of their work. I think this helps keep their motivation up and allows for small corrections early on.</p><h4>Any advice for future contributors?</h4><p>Start small and as you get more familiar with Django and the process of contributing you can take on bigger issues. Also be patient with reviewers ‚Äì Django has high standards, but is mostly maintained by volunteers with limited time.</p><p>Yes! It‚Äôs a huge honour! Since January, we‚Äôve been meeting weekly and it feels like we‚Äôve hardly scratched the surface of what we want to achieve. The biggest thing we‚Äôre trying to tackle is how to improve the contribution experience ‚Äì especially evaluating new feature ideas ‚Äì without draining everyone‚Äôs time and energy.</p><h4>You have a lot of knowledge in the Django ORM, how did you start to contribute to this part?</h4><p>I added the Greatest and Least expressions in Django 1.9, with the support of one of the core team at the time. After that, I kept showing up (especially at conference sprints) and finding a new thing to tackle.</p><h4>Is there anything else you‚Äôd like to say?</h4><p><strong>Thank you for doing the interview, Lily!</strong></p>","contentLength":3110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.24 is released!","url":"https://go.dev/blog/go1.24","date":1739232000,"author":"Junyang Shao, on behalf of the Go team","guid":242,"unread":true,"content":"<div data-slug=\"/blog/go1.24\"><p>\n      Junyang Shao, on behalf of the Go team\n      11 February 2025\n      </p><p>Today the Go team is excited to release Go 1.24,\nwhich you can get by visiting the <a href=\"https://go.dev/dl/\">download page</a>.</p><p>Go 1.24 comes with many improvements over Go 1.23. Here are some of the notable\nchanges; for the full list, refer to the <a href=\"https://go.dev/doc/go1.24\">release notes</a>.</p><p>Several performance improvements in the runtime have decreased CPU overhead\nby 2‚Äì3% on average across a suite of representative benchmarks. These\nimprovements include a new builtin  implementation based on\n<a href=\"https://abseil.io/about/design/swisstables\" rel=\"noreferrer\" target=\"_blank\">Swiss Tables</a>, more efficient\nmemory allocation of small objects, and a new runtime-internal mutex\nimplementation.</p><ul><li>The  command now provides a mechanism for tracking tool dependencies for a\nmodule. Use  to add a  directive to the current module. Use\n to run the tools declared with the  directive.\nRead more on the <a href=\"https://go.dev/doc/go1.24#go-command\">go command</a> in the release notes.</li><li>The new  analyzer in  subcommand reports common mistakes in\ndeclarations of tests, fuzzers, benchmarks, and examples in test packages.\nRead more on <a href=\"https://go.dev/doc/go1.24#vet\">vet</a> in the release notes.</li></ul><h2>Standard library additions</h2><h2>Improved WebAssembly support</h2><p>Go 1.24 adds a new  directive for Go programs to export\nfunctions to the WebAssembly host, and supports building a Go program as a WASI\n<a href=\"https://github.com/WebAssembly/WASI/blob/63a46f61052a21bfab75a76558485cf097c0dbba/legacy/application-abi.md#current-unstable-abi\" rel=\"noreferrer\" target=\"_blank\">reactor/library</a>.\nRead more on <a href=\"https://go.dev/doc/go1.24#wasm\">WebAssembly</a> in the release notes.</p><p>Please read the <a href=\"https://go.dev/doc/go1.24\">Go 1.24 release notes</a> for the complete and\ndetailed information. Don‚Äôt forget to watch for follow-up blog posts that\nwill go in more depth on some of the topics mentioned here!</p><p>Thank you to everyone who contributed to this release by writing code and\ndocumentation, reporting bugs, sharing feedback, and testing the release\ncandidates. Your efforts helped to ensure that Go 1.24 is as stable as possible.\nAs always, if you notice any problems, please <a href=\"https://go.dev/issue/new\">file an issue</a>.</p></div>","contentLength":1768,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quansight Labs Blog: PEP 517 build system popularity","url":"https://labs.quansight.org/blog/pep-517-build-system-popularity","date":1739232000,"author":"","guid":266,"unread":true,"content":"<article>Analysis of PEP 517 build backends used in 8000 top PyPI packages</article>","contentLength":65,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seth Michael Larson: Building software for connection (#2: Consensus)","url":"https://sethmlarson.dev/building-software-for-connection-consensus?utm_campaign=rss","date":1739232000,"author":"","guid":267,"unread":true,"content":"<p>In the <a href=\"http://sethmlarson.dev/building-software-for-connection-local-first\">previous article</a> we concluded that a persistent always-on internet\nconnection isn't required for software to elicit feelings of connection between humans.</p><div><div><p>Building on this conclusion: let's explore how Animal Crossing software was able to intercommunicate without requiring\na centralized server and infrastructure and the trade-offs for these design decisions.</p></div></div><h2>Distributing digital goods without the internet</h2><p>Animal Crossing has over 1,000 unique items that need to be collected\nfor a complete <a href=\"https://nookipedia.com/wiki/Catalog\">catalog</a>, including furniture, wallpapers, clothing, parasols, and carpets.\nMany of these items are quite rare or were only programmed to be accessible\nthrough an official Nintendo-affiliated distribution such as a magazine or online contest.</p><p>Beyond official distributions, it's clear Animal Crossings' designer, Katsuya Eguchi,\nwanted players to  to complete their catalogs.\nThe game incentivized trading items between towns by assigning\none ‚Äú<a href=\"https://nookipedia.com/wiki/Fruit\">native fruit</a>‚Äù (Apple, Orange, Cherry, Peach, or Pear) and\nrandomly making a subset of items harder to find than others depending\non a <a href=\"https://nookipedia.com/wiki/Group\">hidden ‚Äúitem group‚Äù variable</a> (either A, B, or C).</p><p>Items could be exchanged between players when one player visits another town,\nbut this required physically bringing your memory card to another\nplayers' GameCube. The GameCube might have come with a handle, but the 'cube wasn't exactly a . Sharing a physical space isn't something you can do with everyone or on a regular basis.</p><div><div><p>So what did Katsuya Eguchi design for Animal Crossing? To allow for item distributions from magazines and contests and to make player-to-player item sharing easier Animal Crossing included a feature called ‚Äú<a href=\"https://nookipedia.com/wiki/Secret_code\">secret codes</a>‚Äù.</p><p>This feature worked by allowing players to exchange 28-character codes with Tom Nook for items. Players could also generate codes for their friends to ‚Äúsend‚Äù an item from their own game to a different town. Codes could be shared by writing them on a paper note, instant message, or text message.</p></div></div><h2>The forgotten durability of offline software</h2><div><div><p>\nThis <a href=\"https://www.reddit.com/r/Gamecube/comments/1gqfbae\">Reddit comment thread</a> from the GameCube subreddit was the initial inspiration for this entire series.\nThe post is about someone's niece who just started playing Animal Crossing for the first time.\nThe Redditor asked folks to send items to their nieces' town using the secret code system.\n</p><p>This ended up surprising many folks that this system \n in a game that was over 23 years old!\nFor reference, Nintendo Wi-Fi Connection and Nintendo Network were only available for 8 and 13 years respectively.\nBelow are a handful of the comments from the thread:</p><blockquote><ul><li>‚ÄúFor real does this still work lol?‚Äù</li></ul></blockquote></div></div><p>It's hard not to take these comments as indicators that something is\n with internet-connected software today. What had to go wrong for a \nsystem continuing to work to ? Many consumers' \nexperience with \nsoftware products\ntoday is that they become useless e-waste after some far-away service is \ndiscontinued a few years after purchase.</p><p>My intuition from this is that software that <em>requires centralized servers and infrastructure to function</em>\nwill have shorter lifetimes than software which is offline or only\nopportunistically uses online functionality.</p><p>I don't think this is particularly insightful,\nmore dependencies always means less resilience. But if we're building software for human connection then the software\nshould optimally only be limited by the <em>availability of humans to connect</em>.</p><h2>What is centralization good for?</h2><div><div><p>Animal Crossings' secret code system is far from perfect. The system is easily abusable, as the same secret codes can be\nreused over-and-over by the same user to duplicate items without ever expiring. The only limit was that 3 codes could be used per day.</p></div></div><p>Not long after Animal Crossing's release\nthe secret code algorithm was <a href=\"https://togenyanweb.appspot.com/Yokai/eplus/eplus.html\">reverse-engineered</a> so secret codes \nfor any item could be created for any town and recipient name as if they came from an official Nintendo distribution.\nThis was possible because the secret code system relied on \"<a href=\"https://en.wikipedia.org/wiki/Security_through_obscurity\">security through obscurity</a>\".</p><p>Could  be the answer to preventing these abuses?</p><p>The most interesting property that a centralized authority approach\nprovides is : forcing everyone to play by the same rules. By storing\nthe ‚Äúsingle source-of-truth‚Äù a central authority is able to prevent abuses\nlike the ones mentioned above.</p><p>For example, a centralized ‚Äúsecret code issuing server‚Äù could generate\nnew unique codes per-use and check each code's validity\nagainst a database to prevent users from generating their\nown illegitimate codes or codes being re-used multiple times.</p><p>The problem with\ncentralized consensus is it tends to be  to cover the entire software state.\nA centralized server can generate codes perfectly, but how can that same server\n that the items you're exchanging for codes were obtained legitimately? To know this\nthe server would <em>also need to track item legitimacy</em>, leading to software which requires\nan internet connection to operate.</p><p>This is optimal from a correctness perspective, but as was noted earlier,\nI suspect that if such a server was a mandatory part of the secret code system\nin Animal Crossing that the system <em>would likely not be usable today</em>.</p><p>This seems like a trade-off, <em>which future would you rather have?</em></p><h2>Redesigning Animal Crossing secret codes</h2><p>If I were designing Animal Crossings' secret code system with modern hardware, what would it look like?\nHow can we keep the offline fall-back while providing consensus and being less\nabusable, especially for official distributions.</p><p>I would likely use a <a href=\"https://en.wikipedia.org/wiki/Public-key_cryptography\">public-key cryptographic</a> system for official distributions,\nembedding a certificate that could be used to ‚Äúverify‚Äù that specific secret codes\noriginated from the expected centralized entity. Codes that are accepted would be\nrecorded to prevent reusing the same code multiple times in the same town.\nUsing public-key cryptography prevents the\nsystem from being reverse-engineered to distribute arbitrary items until the certificate\nprivate key was cracked.</p><p>For sharing items between players I would implement a system where each town\ngenerated a public and private key and the public key was shared to other towns\nwhenever the software was able to, such as when a player visited the other town.\nPlayers would only be able to send items to players that they have visited\n(which for Animal Crossing <em>required physical presence</em>, more on this later!)</p><p>Each sender could store a <a href=\"https://en.wikipedia.org/wiki/Cryptographic_nonce\">nonce</a> value for\neach potential recipient. Embedding that nonce into the secret code would allow\nthe recipients' software to verify that the specific code hadn't been used yet.\nThe nonce wouldn't have to be long to avoid simple reusing of codes.</p><p>Both above systems would require much more data to be embedded into each ‚Äúsecret\ncode‚Äù compared to the 28-character codes from the GameCube. For this I would\nuse QR codes to embed over 2KB of data into a single QR code. Funnily enough,\nAnimal Crossing New Leaf and onwards <a href=\"https://nookipedia.com/wiki/QR_code\">use QR code technology</a> for players to share design patterns.</p><p>This design is still abusable if users can modify their software or hardware\nbut doesn't suffer from the trivial-to-exploit flaws of Animal Crossing's secret code system.</p><h2>Decentralized global consensus?</h2><div><div><p>What if we could have the best of both worlds: we want consensus\nthat is both  and . At least today, we are out of luck.</p><p>Decentralized global consensus is <a href=\"https://bitcoin.org/bitcoin.pdf\">technologically feasible</a>, but the existing solutions\n(mostly blockchains)\nare expensive (both in energy and capital) and can't handle throughput on any sort of \nmeaningful scale.</p></div></div><p>There are many other decentralized consensus systems that \nare able to form ‚Äúpockets‚Äù of <em>useful peer-to-peer consensus</em> using a fraction of\nthe resources, such as email, <a href=\"https://en.wikipedia.org/wiki/BitTorrent\">BitTorrent</a>, <a href=\"https://activitypub.rocks/\">ActivityPub</a>, and <a href=\"https://nostr.com/\">Nostr</a>.\nThese systems are only possible by adding  or by only guaranteeing .</p><h2>When is global consensus needed?</h2><p>Obviously global consensus is important for certain classes of software like \nfinancial, civics, and infrastructure, but I wonder how the necessity\nof consensus in software changes for software with different risk\nprofiles.</p><p>For software which has fewer risks associated with misuse is there as much\nneed for global consensus?\nHow can  be designed to reduce risk and require\nless consensus to be effective? If global consensus and centralized \nservers become unnecessary, can we expect  to be usable \non much longer timescales, <em>essentially for as long as there are users?</em></p>","contentLength":8378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Brave Now Lets You Inject Custom JavaScript To Tweak Websites","url":"https://tech.slashdot.org/story/25/02/10/2348258/brave-now-lets-you-inject-custom-javascript-to-tweak-websites?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739231400,"author":"BeauHD","guid":367,"unread":true,"content":"Brave Browser version 1.75 introduces \"custom scriptlets,\" a new feature that allows advanced users to inject their own JavaScript into websites for enhanced customization, privacy, and usability. The feature is similar to the TamperMonkey and GreaseMonkey browser extensions, notes BleepingComputer. From the report: \"Starting with desktop version 1.75, advanced Brave users will be able to write and inject their own scriptlets into a page, allowing for better control over their browsing experience,\" explained Brave in the announcement. Brave says that the feature was initially created to debug the browser's adblock feature but felt it was too valuable not to share with users. Brave's custom scriptlets feature can be used to modify webpages for a wide variety of privacy, security, and usability purposes.\n \nFor privacy-related changes, users write scripts that block JavaScript-based trackers, randomize fingerprinting APIs, and substitute Google Analytics scripts with a dummy version. In terms of customization and accessibility, the scriptlets could be used for hiding sidebars, pop-ups, floating ads, or annoying widgets, force dark mode even on sites that don't support it, expand content areas, force infinite scrolling, adjust text colors and font size, and auto-expand hidden content.\n \nFor performance and usability, the scriptlets can block video autoplay, lazy-load images, auto-fill forms with predefined data, enable custom keyboard shortcuts, bypass right-click restrictions, and automatically click confirmation dialogs. The possible actions achievable by injected JavaScript snippets are virtually endless. However, caution is advised, as running untrusted custom scriptlets may cause issues or even introduce some risk.","contentLength":1745,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transforming credit decisions using generative AI with Rich Data Co and AWS","url":"https://aws.amazon.com/blogs/machine-learning/transforming-credit-decisions-using-generative-ai-with-rich-data-co-and-aws/","date":1739217934,"author":"Daniel Wirjo","guid":225,"unread":true,"content":"<p><em>This post is co-written with Gordon Campbell, Charles Guan, and Hendra Suryanto from RDC.&nbsp;</em></p><p>The mission of <a href=\"https://www.richdataco.com/\" target=\"_blank\" rel=\"noopener\">Rich Data Co (RDC)</a> is to broaden access to sustainable credit globally. Its software-as-a-service (SaaS) solution empowers leading banks and lenders with deep customer insights and AI-driven decision-making capabilities.</p><p>Making credit decisions using AI can be challenging, requiring data science and portfolio teams to synthesize complex subject matter information and collaborate productively. To solve this challenge, RDC used <a href=\"https://aws.amazon.com/ai/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a>, enabling teams to use its solution more effectively:</p><ul><li> ‚Äì Designed for data science teams, this agent assists teams in developing, building, and deploying AI models within a regulated environment. It aims to boost team efficiency by answering complex technical queries across the machine learning operations (MLOps) lifecycle, drawing from a comprehensive knowledge base that includes environment documentation, AI and data science expertise, and Python code generation.</li><li> ‚Äì Designed for portfolio managers and analysts, this agent facilitates natural language inquiries about loan portfolios. It provides critical insights on performance, risk exposures, and credit policy alignment, enabling informed commercial decisions without requiring in-depth analysis skills. The assistant is adept at high-level questions (such as identifying high-risk segments or potential growth opportunities) and one-time queries, allowing the portfolio to be diversified.</li></ul><p>In this post, we discuss how RDC uses generative AI on <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> to build these assistants and accelerate its overall mission of democratizing access to sustainable credit.</p><h2>Solution overview: Building a multi-agent generative AI solution</h2><p>We began with a carefully crafted evaluation set of over 200 prompts, anticipating common user questions. Our initial approach combined prompt engineering and traditional <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" target=\"_blank\" rel=\"noopener\">Retrieval Augmented Generation (RAG)</a>. However, we encountered a challenge: accuracy fell below 90%, especially for more complex questions.</p><p>To overcome the challenge, we adopted an agentic approach, breaking down the problem into specialized use cases. This strategy equipped us to align each task with the most suitable <a href=\"https://aws.amazon.com/what-is/foundation-models/\" target=\"_blank\" rel=\"noopener\">foundation model (FM)</a> and tools. Our multi-agent framework is orchestrated using <a href=\"https://langchain-ai.github.io/langgraph/\" target=\"_blank\" rel=\"noopener\">LangGraph</a>, and it consisted of:</p><ol><li> ‚Äì The orchestrator is responsible for routing user questions to the appropriate agent. In this example, we start with the data science or portfolio agent. However, we envision many more agents in the future. The orchestrator can also use user context, such as the user‚Äôs role, to determine routing to the appropriate agent.</li><li> ‚Äì The agent is designed for a specialized task. It‚Äôs equipped with the appropriate FM for the task and the necessary tools to perform actions and access knowledge. It can also handle multiturn conversations and orchestrate multiple calls to the FM to reach a solution.</li><li> ‚Äì Tools extend agent capabilities beyond the FM. They provide access to external data and APIs or enable specific actions and computation. To efficiently use the model‚Äôs context window, we construct a tool selector that retrieves only the relevant tools based on the information in the agent state. This helps simplify debugging in the case of errors, ultimately making the agent more effective and cost-efficient.</li></ol><p>This approach gives us the right tool for the right job. It enhances our ability to handle complex queries efficiently and accurately while providing flexibility for future improvements and agents.</p><p>The following image is a high-level architecture diagram of the solution.</p><h2>Data science agent: RAG and code generation</h2><p>To boost productivity of data science teams, we focused on rapid comprehension of advanced knowledge, including industry-specific models from a curated knowledge base. Here, RDC provides an integrated development environment (IDE) for Python coding, catering to various team roles. One role is model validator, who rigorously assesses whether a model aligns with bank or lender policies. To support the assessment process, we designed an agent with two tools:</p><ol><li> ‚Äì <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> powers our intelligent content retrieval through a streamlined RAG implementation. The service automatically converts text documents to their vector representation using <a href=\"https://aws.amazon.com/bedrock/amazon-models/titan/\" target=\"_blank\" rel=\"noopener\">Amazon Titan Text Embeddings</a> and stores them in <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\" target=\"_blank\" rel=\"noopener\">Amazon OpenSearch Serverless</a>. Because the knowledge is vast, it performs <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking.html\" target=\"_blank\" rel=\"noopener\">semantic chunking</a>, making sure that the knowledge is organized by topic and can fit within the FM‚Äôs context window. When users interact with the agent, <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> using OpenSearch Serverless provides fast, in-memory semantic search, enabling the agent to retrieve the most relevant chunks of knowledge for relevant and contextual responses to users.</li><li> ‚Äì With code generation, we selected <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropic‚Äôs Claude model on Amazon Bedrock</a> due to its inherent ability to understand and generate code. This tool is grounded to answer queries related to data science and can generate Python code for quick implementation. It‚Äôs also adept at troubleshooting coding errors.</li></ol><h2>Portfolio agent: Text-to-SQL and self-correction</h2><p>To boost the productivity of credit portfolio teams, we focused on two key areas. For portfolio managers, we prioritized high-level commercial insights. For analysts, we enabled deep-dive data exploration. This approach empowered both roles with rapid understanding and actionable insights, streamlining decision-making processes across teams.</p><p>Our solution required natural language understanding of structured portfolio data stored in <a href=\"https://aws.amazon.com/rds/aurora/\" target=\"_blank\" rel=\"noopener\">Amazon Aurora</a>. This led us to base our solution on a text-to-SQL model to efficiently bridge the gap between natural language and SQL.</p><p>To reduce errors and tackle complex queries beyond the model‚Äôs capabilities, we developed three tools using Anthropic‚Äôs Claude model on Amazon Bedrock for self-correction:</p><ol><li> ‚Äì Verifies and corrects SQL queries, addressing common issues such as data type mismatches or incorrect function usage</li><li> ‚Äì Validates query results, providing relevance and prompting retries or user clarification when needed</li><li> ‚Äì Engages users for additional information when queries are too broad or lack detail, guiding the interaction based on database information and user input</li></ol><p>These tools operate in an agentic system, enabling accurate database interactions and improved query results through iterative refinement and user engagement.</p><p>To improve accuracy, we tested model fine-tuning, training the model on common queries and context (such as database schemas and their definitions). This approach reduces inference costs and improves response times compared to prompting at each call. Using <a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart</a>, we <a href=\"https://aws.amazon.com/blogs/machine-learning/fine-tune-llama-3-for-text-generation-on-amazon-sagemaker-jumpstart/\" target=\"_blank\" rel=\"noopener\">fine-tuned Meta‚Äôs Llama model </a>by providing a set of anticipated prompts, intended answers, and associated context. Amazon SageMaker Jumpstart offers a cost-effective alternative to third-party models, providing a viable pathway for future applications. However, we didn‚Äôt end up deploying the fine-tuned model because we experimentally observed that prompting with Anthropic‚Äôs Claude model provided better generalization, especially for complex questions. To reduce operational overhead, we will also evaluate <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-build-structured.html\" target=\"_blank\" rel=\"noopener\">structured data retrieval on Amazon Bedrock Knowledge Bases</a>.</p><h2>Conclusion and next steps with RDC</h2><p>To expedite development, RDC collaborated with <a href=\"https://aws.amazon.com/startups\" target=\"_blank\" rel=\"noopener\">AWS Startups</a> and the <a href=\"https://aws.amazon.com/ai/generative-ai/innovation-center/\" target=\"_blank\" rel=\"noopener\">AWS Generative AI Innovation Center</a>. Through an iterative approach, RDC rapidly enhanced its generative AI capabilities, deploying the initial version to production in just 3 months. The solution successfully met the stringent security standards required in regulated banking environments, providing both innovation and compliance.</p><blockquote><p>‚ÄúThe integration of generative AI into our solution marks a pivotal moment in our mission to revolutionize credit decision-making. By empowering both data scientists and portfolio managers with AI assistants, we‚Äôre not just improving efficiency‚Äîwe‚Äôre transforming how financial institutions approach lending.‚Äù</p><p><em>‚ÄìGordon Campbell, Co-Founder &amp; Chief Customer Officer at RDC</em></p></blockquote><p>RDC envisions generative AI playing a significant role in boosting the productivity of the banking and credit industry. By using this technology, RDC can provide key insights to customers, improve solution adoption, accelerate the model lifecycle, and reduce the customer support burden. Looking ahead, RDC plans to further refine and expand its AI capabilities, exploring new use cases and integrations as the industry evolves.</p><p>For more information about how to work with RDC and AWS and to understand how we‚Äôre supporting banking customers around the world to use AI in credit decisions, contact your AWS Account Manager or visit <a href=\"https://www.richdataco.com/\" target=\"_blank\" rel=\"noopener\">Rich Data Co</a>.</p><p>For more information about generative AI on AWS, refer to the following resources:</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/wirjo.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Solutions Architect at AWS, focused on FinTech and SaaS startups. As a former startup CTO, he enjoys collaborating with founders and engineering leaders to drive growth and innovation on AWS. Outside of work, Daniel enjoys taking walks with a coffee in hand, appreciating nature, and learning new ideas.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/liuxuefeng.jpg\" alt=\"\" width=\"100\" height=\"133\"> leads a science team at the AWS Generative AI Innovation Center in the Asia Pacific regions. His team partners with AWS customers on generative AI projects, with the goal of accelerating customers‚Äô adoption of generative AI.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/iman.png\" alt=\"\" width=\"100\" height=\"133\"> is a computer scientist at the Generative AI Innovation Center at Amazon Web Services (AWS) working on Generative AI and complex multi-agents systems.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/rdc-gordon-campbell.png\" alt=\"\" width=\"100\" height=\"133\"> is the Chief Customer Officer and Co-Founder of RDC, where he leverages over 30 years in enterprise software to drive RDC‚Äôs leading AI Decisioning platform for business and commercial lenders. With a proven track record in product strategy and development across three global software firms, Gordon is committed to customer success, advocacy, and advancing financial inclusion through data and AI.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/rdc-charles-guan.png\" alt=\"\" width=\"100\" height=\"133\"> is the Chief Technology Officer and Co-founder of RDC. With more than 20 years of experience in data analytics and enterprise applications, he has driven technological innovation across both the public and private sectors. At RDC, Charles leads research, development, and product advancement‚Äîcollaborating with universities to leverage advanced analytics and AI. He is dedicated to promoting financial inclusion and delivering positive community impact worldwide.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/rdc-hendra-suryanto.png\" alt=\"\" width=\"100\" height=\"133\"> is the Chief Data Scientist at RDC with more than 20 years of experience in data science, big data, and business intelligence. Before joining RDC, he served as a Lead Data Scientist at KPMG, advising clients globally. At RDC, Hendra designs end-to-end analytics solutions within an Agile DevOps framework. He holds a PhD in Artificial Intelligence and has completed postdoctoral research in machine learning.</p>","contentLength":10851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build agentic AI solutions with DeepSeek-R1, CrewAI, and Amazon SageMaker AI","url":"https://aws.amazon.com/blogs/machine-learning/build-agentic-ai-solutions-with-deepseek-r1-crewai-and-amazon-sagemaker-ai/","date":1739215990,"author":"Surya Kari","guid":224,"unread":true,"content":"<p>AI agents are rapidly becoming the next frontier in enterprise transformation, with 82% of organizations planning adoption within the next 3 years. According to a <a href=\"https://www.capgemini.com/wp-content/uploads/2024/11/Generative-AI-in-Organizations-Refresh_25112024.pdf\" target=\"_blank\" rel=\"noopener\">Capgemini survey</a> of 1,100 executives at large enterprises, 10% of organizations already use AI agents, and more than half plan to use them in the next year. The recent release of the DeepSeek-R1 models brings state-of-the-art reasoning capabilities to the open source community. Organizations can build agentic applications using these reasoning models to execute complex tasks with advanced decision-making capabilities, enhancing efficiency and adaptability.</p><p>In this post, we dive into how organizations can use <a href=\"https://aws.amazon.com/sagemaker-ai/?gclid=CjwKCAiAtYy9BhBcEiwANWQQL-kwoCy7ZdAghF_qTtqKZCdOoS419cBEv70K_pLlN0NeCVYS8bV-JRoCPMQQAvD_BwE&amp;trk=8987dd52-6f33-407a-b89b-a7ba025c913c&amp;sc_channel=ps&amp;ef_id=CjwKCAiAtYy9BhBcEiwANWQQL-kwoCy7ZdAghF_qTtqKZCdOoS419cBEv70K_pLlN0NeCVYS8bV-JRoCPMQQAvD_BwE:G:s&amp;s_kwcid=AL!4422!3!724218586004!e!!g!!amazon%20sagemaker%20ai!11206038603!174643422154\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker AI</a>, a fully managed service that allows you to build, train, and deploy ML models at scale, and can build AI agents using CrewAI, a popular agentic framework and open source models like DeepSeek-R1.</p><h2>Agentic design vs. traditional software design</h2><p>Agentic systems offer a fundamentally different approach compared to traditional software, particularly in their ability to handle complex, dynamic, and domain-specific challenges. Unlike traditional systems, which rely on rule-based automation and structured data, agentic systems, powered by large language models (LLMs), can operate autonomously, learn from their environment, and make nuanced, context-aware decisions. This is achieved through modular components including reasoning, memory, cognitive skills, and tools, which enable them to perform intricate tasks and adapt to changing scenarios.</p><p>Traditional software platforms, though effective for routine tasks and horizontal scaling, often lack the domain-specific intelligence and flexibility that agentic systems provide. For example, in a manufacturing setting, traditional systems might track inventory but lack the ability to anticipate supply chain disruptions or optimize procurement using real-time market insights. In contrast, an agentic system can process live data such as inventory fluctuations, customer preferences, and environmental factors to proactively adjust strategies and reroute supply chains during disruptions.</p><p>Enterprises should strategically consider deploying agentic systems in scenarios where adaptability and domain-specific expertise are critical. For instance, consider customer service. Traditional chatbots are limited to preprogrammed responses to expected customer queries, but AI agents can engage with customers using natural language, offer personalized assistance, and resolve queries more efficiently. AI agents can significantly improve productivity by automating repetitive tasks, such as generating reports, emails, and software code. The deployment of agentic systems should focus on well-defined processes with clear success metrics and where there is potential for greater flexibility and less brittleness in process management.</p><h3>Generative AI on SageMaker AI</h3><p>SageMaker AI, a fully managed service, provides a comprehensive suite of tools designed to deliver high-performance, cost-efficient machine learning (ML) and generative AI solutions for diverse use cases. SageMaker AI empowers you to build, train, deploy, monitor, and govern ML and generative AI models through an extensive range of services, including notebooks, jobs, hosting, experiment tracking, a curated model hub, and MLOps features, all within a unified integrated development environment (IDE).</p><p>SageMaker AI simplifies the process for generative AI model builders of all skill levels to work with foundation models (FMs):</p><ul><li><a href=\"https://aws.amazon.com/sagemaker-ai/canvas/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Canvas</a> enables data scientists to seamlessly use their own datasets alongside FMs to create applications and architectural patterns, such as chatbots and Retrieval Augmented Generation (RAG), in a low-code or no-code environment.</li><li><a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart</a> offers a diverse selection of open and proprietary FMs from providers like Hugging Face, Meta, and Stability AI. You can deploy or fine-tune models through an intuitive UI or APIs, providing flexibility for all skill levels.</li></ul><p>With SageMaker AI, you can build generative AI-powered agentic workflows using a framework of your choice. Some of the key benefits of using SageMaker AI for fine-tuning and hosting LLMs or FMs include:</p><ul><li> ‚Äì SageMaker AI offers access to SageMaker JumpStart, a curated model hub where models with open weights are made available for seamless deployment through a few clicks or API calls. Additionally, for Hugging Face Hub models, SageMaker AI provides pre-optimized containers built on popular open source hosting frameworks such as <a href=\"https://github.com/vllm-project/vllm\" target=\"_blank\" rel=\"noopener\">vLLM</a>, <a href=\"https://github.com/triton-inference-server/server\" target=\"_blank\" rel=\"noopener\">NVIDIA Triton</a>, and H<a href=\"https://huggingface.co/docs/text-generation-inference/en/index\" target=\"_blank\" rel=\"noopener\">ugging Face Text Generation Inference (TGI).</a> You simply need to specify the model ID, and the model can be deployed quickly.</li><li><strong>Instance-based deterministic pricing</strong> ‚Äì SageMaker AI hosted models are billed based on instance-hours rather than token usage. This pricing model enables you to more accurately predict and manage generative AI inference costs while scaling resources to accommodate incoming request loads.</li><li><strong>Deployments with quantization</strong> ‚Äì SageMaker AI enables you to optimize models prior to deployment using advanced strategies such as quantized deployments (such as AWQ, GPTQ, float16, int8, or int4). This flexibility allows you to efficiently deploy large models, such as a 32-billion parameter model, onto smaller instance types like ml.g5.2xlarge with 24 GB of GPU memory, significantly reducing resource requirements while maintaining performance.</li><li><strong>Inference load balancing and optimized routing</strong> ‚Äì SageMaker endpoints support load balancing and optimized routing with various strategies, providing users with enhanced flexibility and adaptability to accommodate diverse use cases effectively.</li><li><strong>SageMaker fine-tuning recipes</strong> ‚Äì SageMaker offers <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-recipes.html\" target=\"_blank\" rel=\"noopener\">ready-to-use recipes</a> for quickly training and fine-tuning publicly available FMs such as Meta‚Äôs Llama 3, Mistral, and Mixtral. These recipes use <a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker HyperPod</a> (a SageMaker AI service that provides resilient, self-healing clusters optimized for large-scale ML workloads), enabling efficient and resilient training on a GPU cluster for scalable and robust performance.</li></ul><p>CrewAI provides a robust framework for developing multi-agent systems that integrate with AWS services, particularly SageMaker AI. CrewAI‚Äôs role-based agent architecture and comprehensive performance monitoring capabilities work in tandem with <a href=\"http://aws.amazon.com/cloudwatch\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a>.</p><p>The framework excels in workflow orchestration and maintains enterprise-grade security standards aligned with AWS best practices, making it an effective solution for organizations implementing sophisticated agent-based systems within their AWS infrastructure.</p><p>In this post, we demonstrate how to use CrewAI to create a multi-agent research workflow. This workflow creates two agents: one that researches on a topic on the internet, and a writer agent takes this research and acts like an editor by formatting it in a readable format. Additionally, we guide you through deploying and integrating one or multiple LLMs into structured workflows, using tools for automated actions, and deploying these workflows on SageMaker AI for a production-ready deployment.</p><p>The following diagram illustrates the solution architecture.</p><p>To follow along with the code examples in the rest of this post, make sure the following prerequisites are met:</p><ul><li><strong>Integrated development environment</strong> ‚Äì This includes the following: \n  <ul><li><strong>(Optional) Access to Amazon SageMaker Studio and the JupyterLab IDE</strong> ‚Äì We will use a Python runtime environment to build agentic workflows and deploy LLMs. Having access to a JupyterLab IDE with Python 3.9, 3.10, or 3.11 runtimes is recommended. You can also set up <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Studio</a> for single users. For more details, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html\" target=\"_blank\" rel=\"noopener\">Use quick setup for Amazon SageMaker AI</a>. Create a new SageMaker JupyterLab Space for a quick JupyterLab notebook for experimentation. To learn more, refer to Boost productivity on Amazon SageMaker Studio: Introducing JupyterLab Spaces and generative AI tools.</li><li> ‚Äì You can also follow along in your local IDE (such as PyCharm or VSCode), provided that Python runtimes have been configured for site to AWS VPC connectivity (to deploy models on SageMaker AI).</li></ul></li><li><strong>Permission to deploy models</strong> ‚Äì Make sure that your user execution role has the necessary permissions to deploy models to a SageMaker real-time endpoint for inference. For more information, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html\" target=\"_blank\" rel=\"noopener\">Deploy models for inference</a>.</li><li> ‚Äì The code used in this post is available in the following GitHub repo.</li></ul><h2>Simplified LLM hosting on SageMaker AI</h2><p>Before orchestrating agentic workflows with CrewAI powered by an LLM, the first step is to host and query an LLM using <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/image-classification/sagemaker.html\" target=\"_blank\" rel=\"noopener\">SageMaker real-time inference endpoints</a>. There are two primary methods to host LLMs on SageMaker AI:</p><ul><li>Deploy from SageMaker JumpStart</li><li>Deploy from Hugging Face Hub</li></ul><h2>Deploy DeepSeek from SageMaker JumpStart</h2><p>SageMaker JumpStart offers access to a diverse array of state-of-the-art FMs for a wide range of tasks, including content writing, code generation, question answering, copywriting, summarization, classification, information retrieval, and more. It simplifies the onboarding and maintenance of publicly available FMs, allowing you to access, customize, and seamlessly integrate them into your ML workflows. Additionally, SageMaker JumpStart provides solution templates that configure infrastructure for common use cases, along with executable example notebooks to streamline ML development with SageMaker AI.</p><p>The following screenshot shows an example of available models on SageMaker JumpStart.</p><p>To get started, complete the following steps:</p><ol><li>Run the following command in a Jupyter cell or the SageMaker Studio terminal:</li></ol><ol start=\"3\"><li>List all available LLMs under the Hugging Face or Meta JumpStart hub. The following code is an example of how to do this programmatically using the SageMaker Python SDK:</li></ol><div><pre><code>from sagemaker.jumpstart.filters import (And, Or)\nfrom sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n\n# generate a conditional filter to only select LLMs from HF or Meta\nfilter_value = Or(\n    And(\"task == llm\", \"framework == huggingface\"), \n    \"framework == meta\", \"framework == deekseek\"\n)\n\n# Retrieve all available JumpStart models\nall_models = list_jumpstart_models(filter=filter_value)</code></pre></div><p>For example, deploying the  model directly from SageMaker JumpStart requires only a few lines of code:</p><div><pre><code>from sagemaker.jumpstart.model import JumpStartModel\n\nmodel_id = \" deepseek-llm-r1\" \nmodel_version = \"*\"\n\n# instantiate a new JS meta model\nmodel = JumpStartModel(\n    model_id=model_id, \n    model_version=model_version\n)\n\n# deploy model on a 1 x p5e instance \npredictor = model.deploy(\n    accept_eula=True, \n    initial_instance_count=1, \n    # endpoint_name=\"deepseek-r1-endpoint\" # optional endpoint name\n)</code></pre></div><p>We recommend deploying your SageMaker endpoints within a VPC and a private subnet with no egress, making sure that the models remain accessible only within your VPC for enhanced security.</p><h2>Deploy DeepSeek from Hugging Face Hub</h2><p>Alternatively, you can deploy your preferred model directly from the <a href=\"https://huggingface.co/models\" target=\"_blank\" rel=\"noopener\">Hugging Face Hub</a> or the <a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/\" target=\"_blank\" rel=\"noopener\">Hugging Face Open LLM Leaderboard</a> to a SageMaker endpoint. Hugging Face LLMs can be hosted on SageMaker using a variety of supported frameworks, such as NVIDIA Triton, vLLM, and Hugging Face TGI. For a comprehensive list of supported deep learning container images, refer to the available <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Deep Learning Containers</a>. In this post, we use a <strong><em>DeepSeek-R1-Distill-Llama-70B</em></strong> SageMaker endpoint using the TGI container for agentic AI inference. We deploy the model from Hugging Face Hub using <a href=\"https://huggingface.co/blog/sagemaker-huggingface-llm\" target=\"_blank\" rel=\"noopener\">Amazon‚Äôs optimized TGI container</a>, which provides enhanced performance for LLMs. This container is specifically optimized for text generation tasks and automatically selects the most performant parameters for the given hardware configuration. To deploy from Hugging Face Hub, refer to the  or the following code snippet:</p><div><pre><code>import json\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\nimport os\nfrom datetime import datetime\n\n# Model configuration\nhub = {'HF_MODEL_ID':'deepseek-ai/DeepSeek-R1-Distill-Llama-70B', #Llama-3.3-70B-Instruct\n       'SM_NUM_GPUS': json.dumps(number_of_gpu),\n       'HF_TOKEN': HUGGING_FACE_HUB_TOKEN,\n       'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',  # Set to INFO level\n       'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True'  # configure CUDA memory to use expandable memory segments\n}\n# Create and deploy model\nhuggingface_model =   HuggingFaceModel(image_uri=get_huggingface_llm_image_uri(\"huggingface\", \nversion=\"2.3.1\"),\nenv=hub,\nrole=role,sagemaker_session=sagemaker_session)\npredictor = huggingface_model.deploy(\n               initial_instance_count=1,\n               instance_type=\"ml.p4d.24xlarge\"\n               endpoint_name=custom_endpoint_name,\n               container_startup_health_check_timeout=900)</code></pre></div><p>A new <em><strong>DeepSeek-R1-Distill-Llama-70B</strong></em>endpoint should be  in under 10 minutes. If you want to change the model from DeepSeek to another model from the hub, simply replace the following parameter or refer to the DeepSeek deploy example in the following <a href=\"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/reference/launcher.md\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>. To learn more about deployment parameters that can be reconfigured inside TGI containers at runtime, refer to the following <a href=\"https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/Deepseek/DeepSeek-R1-Llama8B-LMI-TGI-Deploy.ipynb\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>&nbsp;on TGI arguments.</p><div><pre><code>...\n\"HF_MODEL_ID\": \"deepseek-ai/...\", # replace with any HF hub models\n# \"HF_TOKEN\": \"hf_...\" # add your token id for gated models\n...</code></pre></div><p>For open-weight models deployed directly from hubs, we strongly recommend placing your SageMaker endpoints within a VPC and a private subnet with no egress, making sure that the models remain accessible only within your VPC for a secure deployment.</p><h2>Build a simple agent with CrewAI</h2><p>CrewAI offers the ability to create multi-agent and very complex agentic orchestrations using LLMs from several LLM providers, including SageMaker AI and Amazon Bedrock. In the following steps, we create a simple blocks counting agent to serve as an example.</p><p><strong>Create a blocks counting agent</strong></p><p>The following code sets up a simple blocks counter workflow using CrewAI with two main components:</p><ul><li><strong>Agent creation (blocks_counter_agent)</strong> ‚Äì The agent is configured with a specific role, goal, and capabilities. This agent is equipped with a tool called .</li><li><strong>Task definition (count_task)</strong> ‚Äì This is a task that we want this agent to execute. The task includes a template for counting how many of each color of blocks are present, where  will be replaced with actual color of the block. The task is assigned to .</li></ul><div><pre><code>from crewai import Agent, Task\nfrom pydantic import BaseModel, Field\n\n# 1. Configure agent\nblocks_counter_agent = Agent(\n    role=\"Blocks Inventory Manager\",\n    goal=\"Maintain accurate block counts\",\n    tools=[BlocksCounterTool],\n    verbose=True\n)\n\n# 2. Create counting task\ncount_task = Task(\n    description=\"Count {color} play blocks in storage\",\n    expected_output=\"Exact inventory count for specified color\",\n    agent=blocks_counter_agent\n)</code></pre></div><p>As you can see in the preceding code, each agent begins with two essential components: an agent definition that establishes the agent‚Äôs core characteristics (including its role, goal, backstory, available tools, LLM model endpoint, and so on), and a task definition that specifies what the agent needs to accomplish, including the detailed description of work, expected outputs, and the tools it can use during execution.</p><p>This structured approach makes sure that agents have both a clear identity and purpose (through the agent definition) and a well-defined scope of work (through the task definition), enabling them to operate effectively within their designated responsibilities.</p><p>Tools are special functions that give AI agents the ability to perform specific actions, like searching the internet or analyzing data. Think of them as apps on a smartphone‚Äîeach tool serves a specific purpose and extends what the agent can do. In our example,  helps the agent count the number of blocks organized by color.</p><p>Tools are essential because they let agents do real-world tasks instead of just thinking about them. Without tools, agents would be like smart speakers that can only talk‚Äîthey could process information but couldn‚Äôt take actual actions. By adding tools, we transform agents from simple chat programs into practical assistants that can accomplish real tasks.</p><p><strong>Out-of-the-box tools with CrewAI</strong>Crew AI offers a range of tools out of the box for you to use along with your agents and tasks. The following table lists some of the available tools.</p><table border=\"1px\" width=\"781\" cellpadding=\"10px\"><tbody><tr><td>For reading various file formats</td></tr><tr><td>For web content extraction</td></tr><tr><td>For searching YouTube channels</td></tr><tr><td>For searching PDF documents</td></tr><tr><td>For Python code interpretation</td></tr><tr></tr></tbody></table><p><strong>Build custom tools with CrewAI</strong>You can build custom tools in CrewAI in two ways: by subclassing BaseTool or using the @tool decorator. Let‚Äôs look at the following BaseTool subclassing option to create the BlocksCounterTool we used earlier:</p><div><pre><code>from crewai.tools import BaseTool\n\nclass BlocksCounterTool(BaseTool):\n    name = \"blocks_counter\" \n    description = \"Simple tool to count play blocks\"\n\n    def _run(self, color: str) -&gt; str:\n        return f\"There are 10 {color} play blocks available\"</code></pre></div><h2><strong>Build a multi-agent workflow with CrewAI, DeepSeek-R1, and SageMaker AI</strong></h2><p>Multi-agent AI systems represent a powerful approach to complex problem-solving, where specialized AI agents work together under coordinated supervision. By combining CrewAI‚Äôs workflow orchestration capabilities with SageMaker AI based LLMs, developers can create sophisticated systems where multiple agents collaborate efficiently toward a specific goal. The code used in this post is available in the following <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/tree/main/agents-with-sagemaker/deepseek_crewai_based_agent\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>.</p><p>Let‚Äôs build a research agent and writer agent that work together to create a PDF about a topic. We will use a DeepSeek-R1 Distilled Llama 3.3 70B model as a SageMaker endpoint for the LLM inference.</p><p><strong>Define your own DeepSeek SageMaker LLM (using LLM base class)</strong> The following code integrates SageMaker hosted LLMs with CrewAI by creating a custom inference tool that formats prompts with system instructions for factual responses, uses Boto3, an AWS core library, to call SageMaker endpoints, and processes responses by separating reasoning (before &lt;/think&gt;) from final answers. This enables CrewAI agents to use deployed models while maintaining structured output patterns.</p><div><pre><code># Calls SageMaker endpoint for DeepSeek inference\ndef deepseek_llama_inference(prompt: dict, endpoint_name: str, region: str = \"us-east-2\") -&gt; dict:\n    try:\n        # ... Response parsing Code...\n\n    except Exception as e:\n        raise RuntimeError(f\"Error while calling SageMaker endpoint: {e}\")\n\n# CrewAI-compatible LLM implementation for DeepSeek models on SageMaker.\nclass DeepSeekSageMakerLLM(LLM):\n    def __init__(self, endpoint: str):\n        # &lt;... Initialize LLM with SageMaker endpoint ...&gt;\n\n    def call(self, prompt: Union[List[Dict[str, str]], str], **kwargs) -&gt; str:\n        # &lt;... Format and return the final response ...&gt;</code></pre></div><p><strong>Name the DeepSeek-R1 Distilled endpoint</strong> Set the endpoint name as defined earlier when you deployed DeepSeek from the Hugging Face Hub:</p><div><pre><code>deepseek_endpoint = \"deepseek-r1-dist-v3-llama70b-2025-01-22\"</code></pre></div><p><strong>Create a DeepSeek inference tool</strong> Just like how we created the BlocksCounterTool earlier, let‚Äôs create a tool that uses the DeepSeek endpoint for our agents to use. We use the same BaseTool subclass here, but we hide it in the CustomTool class implementation in sage_tools.py in the tools folder. For more information, refer to the <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/tree/main/agents-with-sagemaker/deepseek_crewai_based_agent\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>.</p><div><pre><code>from crewai import Crew, Agent, Task, Process \n\n# Create the Tool for LLaMA inference\ndeepseek_tool = CustomTool(\n    name=\"deepseek_llama_3.3_70B\",\n    func=lambda inputs: deepseek_llama_inference(\n        prompt=inputs,\n        endpoint_name=deepseek_endpoint\n    ),\n    description=\"A tool to generate text using the DeepSeek LLaMA model deployed on SageMaker.\"\n)\n</code></pre></div><p> Just like the simple blocks agent we defined earlier, we follow the same template here to define the research agent. The difference here is that we give more capabilities to this agent. We attach a SageMaker AI based DeepSeek-R1 model as an endpoint for the LLM.</p><p>This helps the research agent think critically about information processing by combining the scalable infrastructure of SageMaker with DeepSeek-R1‚Äôs advanced reasoning capabilities.</p><p>The agent uses the SageMaker hosted LLM to analyze patterns in research data, evaluate source credibility, and synthesize insights from multiple inputs. By using the deepseek_tool, the agent can dynamically adjust its research strategy based on intermediate findings, validate hypotheses through iterative questioning, and maintain context awareness across complex information it gathers.</p><div><pre><code># Research Agent\n\nresearch_agent = Agent(\n    role=\"Research Bot\",\n    goal=\"Scan sources, extract relevant information, and compile a research summary.\",\n    backstory=\"An AI agent skilled in finding relevant information from a variety of sources.\",\n    tools=[deepseek_tool],\n    allow_delegation=True,\n    llm=DeepSeekSageMakerLLM(endpoint=deepseek_endpoint),\n    verbose=False\n)\n</code></pre></div><p> The writer agent is configured as a specialized content editor that takes research data and transforms it into polished content. This agent works as part of a workflow where it takes research from a research agent and acts like an editor by formatting the content into a readable format. The agent is used for writing and formatting, and unlike the research agent, it doesn‚Äôt delegate tasks to other agents.</p><div><pre><code>writer_agent = Agent(\n    role=\"Writer Bot\",\n    goal=\"Receive research summaries and transform them into structured content.\",\n    backstory=\"A talented writer bot capable of producing high-quality, structured content based on research.\",\n    tools=[deepseek_tool],\n    allow_delegation=False,\n    llm=DeepSeekSageMakerLLM(endpoint=deepseek_endpoint),\n    verbose=False\n)</code></pre></div><p><strong>Define tasks for the agents</strong> Tasks in CrewAI define specific operations that agents need to perform. In this example, we have two tasks: a research task that processes queries and gathers information, and a writing task that transforms research data into polished content.</p><p>Each task includes a clear description of what needs to be done, the expected output format, and specifies which agent will perform the work. This structured approach makes sure that agents have well-defined responsibilities and clear deliverables.</p><p>Together, these tasks create a workflow where one agent researches a topic on the internet, and another agent takes this research and formats it into readable content. The tasks are integrated with the DeepSeek tool for advanced language processing capabilities, enabling a production-ready deployment on SageMaker AI.</p><div><pre><code>research_task = Task(\n    description=(\n        \"Your task is to conduct research based on the following query: {prompt}.\\n\"\n    ),\n    expected_output=\"A comprehensive research summary based on the provided query.\",\n    agent=research_agent,\n    tools=[deepseek_tool]\n)\n\nwriting_task = Task(\n    description=(\n              \"Your task is to create structured content based on the research provided.\\n\"\"),\n    expected_output=\"A well-structured article based on the research summary.\",\n    agent=research_agent,\n    tools=[deepseek_tool]\n)\n</code></pre></div><p> A crew in CrewAI represents a collaborative group of agents working together to achieve a set of tasks. Each crew defines the strategy for task execution, agent collaboration, and the overall workflow. In this specific example, the sequential process makes sure tasks are executed one after the other, following a linear progression. There are other more complex orchestrations of agents working together, which we will discuss in future blog posts.</p><p>This approach is ideal for projects requiring tasks to be completed in a specific order. The workflow creates two agents: a research agent and a writer agent. The research agent researches a topic on the internet, then the writer agent takes this research and acts like an editor by formatting it into a readable format.</p><p>Let‚Äôs call the crew scribble_bots:</p><div><pre><code># Define the Crew for Sequential Workflow # \n\nscribble_bots = Crew( agents=[research_agent, writer_agent], \n       tasks=[research_task, writing_task], \n       process=Process.sequential # Ensure tasks execute in sequence)\n</code></pre></div><p><strong>Use the crew to run a task</strong> We have our endpoint deployed, agents created, and crew defined. Now we‚Äôre ready to use the crew to get some work done. Let‚Äôs use the following prompt:</p><div><pre><code>result = scribble_bots.kickoff(inputs={\"prompt\": \"What is DeepSeek?\"})</code></pre></div><p>Our result is as follows:</p><div><pre><code>**DeepSeek: Pioneering AI Solutions for a Smarter Tomorrow**\n\nIn the rapidly evolving landscape of artificial intelligence, \nDeepSeek stands out as a beacon of innovation and practical application. \nAs an AI company, DeepSeek is dedicated to advancing the field through cutting-edge research and real-world applications, \nmaking AI accessible and beneficial across various industries.\n\n**Focus on AI Research and Development**\n\n‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..\n</code></pre></div><p>Complete the following steps to clean up your resources:</p><ol><li>Delete your GPU DeekSeek-R1 endpoint:</li></ol><div><pre><code>import boto3\n\n# Create a low-level SageMaker service client.\nsagemaker_client = boto3.client('sagemaker', region_name=&lt;region&gt;)\n\n# Delete endpoint\nsagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n</code></pre></div><ol start=\"2\"><li>If you‚Äôre using a SageMaker Studio JupyterLab notebook, shut down the JupyterLab notebook instance.</li></ol><p>In this post, we demonstrated how you can deploy an LLM such as DeepSeek-R1‚Äîor another FM of your choice‚Äîfrom popular model hubs like SageMaker JumpStart or Hugging Face Hub to SageMaker AI for <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html\" target=\"_blank\" rel=\"noopener\">real-time inference</a>. We explored inference frameworks like Hugging Face TGI which helps streamline deployment while integrating built-in performance optimizations to minimize latency and maximize throughput. Additionally, we showcased how the SageMaker developer-friendly Python SDK simplifies endpoint orchestration, allowing seamless experimentation and scaling of LLM-powered applications.</p><p>Beyond deployment, this post provided an in-depth exploration of agentic AI, guiding you through its conceptual foundations, practical design principles using CrewAI, and the seamless integration of state-of-the-art LLMs like DeepSeek-R1 as the intelligent backbone of an autonomous agentic workflow. We outlined a sequential CrewAI workflow design, illustrating how to equip LLM-powered agents with specialized tools that enable autonomous data retrieval, real-time processing, and interaction with complex external systems.</p><p>Now, it‚Äôs your turn to experiment! Dive into our publicly available code on <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/tree/main/agents-with-sagemaker/deepseek_crewai_based_agent\" target=\"_blank\" rel=\"noopener\">GitHub</a>, and start building your own DeepSeek-R1-powered agentic AI system on SageMaker. Unlock the next frontier of AI-driven automation‚Äîseamlessly scalable, intelligent, and production-ready.</p><p>Special thanks to Giuseppe Zappia, Poli Rao, and Siamak Nariman for their support with this blog post.</p><p> is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the LLama family, and Qwen, focusing on their fine-tuning and optimization for specific scientific applications. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker, enabling the scaling of foundation models from development to production. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/06/26/Bobby-LIndsey.jpg\" alt=\"\" width=\"100\" height=\"141\">&nbsp;is a Machine Learning Specialist at Amazon Web Services. He‚Äôs been in technology for over a decade, spanning various technologies and multiple roles. He is currently focused on combining his background in software engineering, DevOps, and machine learning to help customers deliver machine learning workflows at scale. In his spare time, he enjoys reading, research, hiking, biking, and trail running.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/25/DSC03728_cleanup-PhotoRoom-1.jpeg\" alt=\"\" width=\"100\" height=\"140\"> is a Generative AI Specialist for third-party models at AWS, where he works with top-tier third-party foundation model (FM) providers to develop and execute joint Go-To-Market strategies, enabling customers to effectively train, deploy, and scale FMs to solve industry specific challenges. Karan holds a Bachelor of Science in Electrical and Instrumentation Engineering from Manipal University, a master‚Äôs in science in Electrical Engineering from Northwestern University and is currently an MBA Candidate at the Haas School of Business at University of California, Berkeley.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/08/29/Pranav-Profile-100.jpeg\" alt=\"\" width=\"100\" height=\"100\">&nbsp;is an AI/ML Specialist Solutions Architect at AWS. He focuses on helping customers build, train, deploy and migrate machine learning (ML) workloads to SageMaker. He previously worked in the semiconductor industry developing large computer vision (CV) and natural language processing (NLP) models to improve semiconductor processes using state of the art ML techniques. In his free time, he enjoys playing chess and traveling. You can find Pranav on&nbsp;<a href=\"https://www.linkedin.com/in/pranav-murthy-6bbb5773/\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://www.linkedin.com/in/pranav-murthy-6bbb5773/\" data-sk=\"tooltip_parent\">LinkedIn</a>.</p>","contentLength":29139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Transformer LLMs Work [Free Course]","url":"https://newsletter.languagemodels.co/p/how-transformer-llms-work-free-course","date":1739214020,"author":"Jay Alammar","guid":730,"unread":true,"content":"<p>Enroll for free now: https://bit.ly/4aRnn7Z Github Repo: https://github.com/HandsOnLLM/Hands-On-Large-Language-Models </p><p>We're ecstatic to bring you \"How Transformer LLMs Work\" -- a free course with ~90 minutes of video, code, and crisp visuals and animations that explain the modern Transformer architecture, tokenizers, embeddings, and mixture-of-expert models.</p><p> and I have developed a lot of the visual language over the last several years (tens of thousands of iterations for hundreds of figures) for the book. This was informed by many incredible colleagues at Cohere, C4AI, and the open source and open science ML community. But to have an opportunity to collaborate with the legendary Andrew Ng and the team at DeepLearning.ai we took them to the next level with animations and a concise narrative meant to enable technical learners to pick up an ML paper and understand the architecture description. </p><p>In this course, you'll learn how a transformer network architecture that powers LLMs works. You'll build the intuition of how LLMs process text and work with code examples that illustrate the key components of the transformer architecture. </p><p>Key topics covered in this course include: </p><ul><li><p>The evolution of how language has been represented numerically, from the Bag-of-Words model through Word2Vec embeddings to the transformer architecture that captures word meanings in full context. </p></li><li><p>How LLM inputs are broken down into tokens, which represent words or pieces before they are sent to the language model. </p></li><li><p>The details of a transformer and the three main stages, consisting of tokenization and embedding, the stack of transformer blocks, and the language model head. </p></li><li><p>The details of the transformer block, including attention, which calculates relevance scores followed by the feedforward layer, which incorporates stored information learned in training. </p></li><li><p>How cached calculations make transformers faster, how the transformer block has evolved over the years since the original paper was released, and how they continue to be widely used. </p></li><li><p>Explore an implementation of recent models in the Hugging Face transformer library. </p></li></ul><p>By the end of this course, you‚Äôll have a deep understanding of how LLMs process language and you'll be able to read through papers describing models and understand the details that are used to describe these architectures. This intuition will help improve your approach to building LLM applications.</p>","contentLength":2419,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8995a108-11cc-4edf-b69d-73006c087981_3050x1922.png","enclosureMime":"","commentsUrl":null},{"title":"Automate bulk image editing with Crop.photo and Amazon Rekognition","url":"https://aws.amazon.com/blogs/machine-learning/automate-bulk-image-editing-with-crop-photo-and-amazon-rekognition/","date":1739213429,"author":"Rahul Bhargava","guid":223,"unread":true,"content":"<p><a href=\"https://evolphin.com/creatives-collaborative-video-editing/\" target=\"_blank\" rel=\"noopener\">Evolphin Software</a>, Inc. is a leading provider of digital and media asset management solutions based in Silicon Valley, California. <a href=\"https://www.crop.photo/\" target=\"_blank\" rel=\"noopener\">Crop.photo</a> from Evolphin Software is a cloud-based service that offers powerful bulk processing tools for automating image cropping, content resizing, background removal, and listing image analysis.</p><p>Crop.photo is tailored for high-end retailers, ecommerce platforms, and sports organizations. The solution has created a unique offering for bulk image editing through its advanced AI-driven solutions. In this post, we explore how Crop.photo uses <a href=\"https://aws.amazon.com/rekognition/\" target=\"_blank\" rel=\"noopener\">Amazon Rekognition</a> to provide sophisticated image analysis, enabling automated and precise editing of large volumes of images. This integration streamlines the image editing process for clients, providing speed and accuracy, which is crucial in the fast-paced environments of ecommerce and sports.</p><h2>Automation: The way out of bulk image editing challenges</h2><p>Bulk image editing isn‚Äôt just about handling a high volume of images, it‚Äôs about delivering flawless results with speed at scale. Large retail brands, marketplaces, and sports industries process thousands of images weekly. Each image must be catalog-ready or broadcast-worthy in minutes, not hours.</p><p>The challenge lies not just in the quantity but in maintaining high-quality images and brand integrity. Speed and accuracy are non-negotiable. Retailers and sports organizations expect rapid turnaround without compromising image integrity.</p><p>This is where Crop.photo‚Äôs smart automations come in with an innovative solution for high-volume image processing needs. The platform‚Äôs advanced AI algorithms can automatically detect subjects of interest, crop the images, and optimize thousands of images simultaneously while providing consistent quality and brand compliance. By automating repetitive editing tasks, Crop.photo enables enterprises to reduce image processing time from hours to minutes, allowing creative teams to focus on higher-value activities.</p><h2>Challenges in the ecommerce industry</h2><p>The ecommerce industry often encounters the following challenges:</p><ul><li><strong>Inefficiencies and delays in manual image editing </strong>‚Äì Ecommerce companies rely on manual editing for tasks like resizing, alignment, and background removal. This process can be time-consuming and prone to delays and inconsistencies. A more efficient solution is needed to streamline the editing process, especially during platform migrations or large updates.</li><li><strong>Maintaining uniformity across diverse image types </strong>‚Äì Companies work with a variety of image types, from lifestyle shots to product close-ups, across different categories. Maintaining uniformity and professionalism in all image types is essential to meet the diverse needs of marketing, product cataloging, and overall brand presentation.</li><li><strong>Large-scale migration and platform transition </strong>‚Äì Transitioning to a new ecommerce platform involves migrating thousands of images, which presents significant logistical challenges. Providing consistency and quality across a diverse range of images during such a large-scale migration is crucial for maintaining brand standards and a seamless user experience.</li></ul><p>For a US top retailer, wholesale distribution channels posed a unique challenge. Thousands of fashion images need to be made for the marketplace with less than a day‚Äôs notice for flash sales. Their director of creative operations said,</p><blockquote><p><em>‚ÄúCrop.photo is an essential part of our ecommerce fashion marketplace workflow. With over 3,000 on-model product images to bulk crop each month, we rely on Crop.photo to enable our wholesale team to quickly publish new products on popular online marketplaces such as Macy‚Äôs, Nordstrom, and Bloomingdales. By increasing our retouching team‚Äôs productivity by over 70%, Crop.photo has been a game changer for us. Bulk crop images used to take days can now be done in a matter of seconds!‚Äù</em></p></blockquote><h2>Challenges in the sports industry</h2><p>The sports industry often contends with the following challenges:</p><ul><li><strong>Bulk player headshot volume and consistency </strong>‚Äì Sports organizations face the challenge of bulk cropping and resizing hundreds of player headshots for numerous teams, frequently on short notice. Maintaining consistency and quality across a large volume of images can be difficult without AI.</li><li><strong>Diverse player facial features </strong>‚Äì Players have varying facial features, such as different hair lengths, forehead sizes, and face dimensions. Adapting cropping processes to accommodate these differences traditionally requires manual adjustments for each image, which leads to inconsistencies and significant time investment.</li><li><strong>Editorial time constraints </strong>‚Äì Tight editorial schedules and resource limitations are common in sports organizations. The time-consuming nature of manual cropping tasks strains editorial teams, particularly during high-volume periods like tournaments, where delays and rushed work can impact quality and timing.</li></ul><p>An Imaging Manager at Europe‚Äôs Premier Football Organization expressed,</p><blockquote><p><em>‚ÄúWe recently found ourselves with 40 images from a top flight English premier league club needing to be edited just 2 hours before kick-off. Using the <a href=\"https://www.crop.photo/blog/bulk-headshots-for-sports\" target=\"_blank\" rel=\"noopener\">Bulk AI headshot cropping for sports</a> feature from Crop.photo, we had perfectly cropped headshots of the squad in just 5 minutes, making them ready for publishing in our website CMS just in time. We would never have met this deadline using manual processes. This level of speed was unthinkable before, and it‚Äôs why we‚Äôre actively recommending Crop.photo to other sports leagues.‚Äù</em></p></blockquote><p>Crop.photo uses Amazon Rekognition to power a robust solution for bulk image editing. Amazon Rekognition offers features like object and scene detection, facial analysis, and image labeling, which they use to generate markers that drive a fully automated image editing workflow.</p><p>The following diagram presents a high-level architectural data flow highlighting several of the AWS services used in building the solution.</p><p>The solution consists of the following key components:</p><ul><li> ‚Äì <a href=\"https://aws.amazon.com/cognito/\" target=\"_blank\" rel=\"noopener\">Amazon Cognito</a> is used for user authentication and user management.</li><li><strong>Infrastructure deployment</strong> ‚Äì Frontend and backend servers are used on <a href=\"https://aws.amazon.com/ecs/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Container Service</a> (Amazon ECS) for container deployment, orchestration, and scaling.</li><li><strong>Content delivery and caching</strong> ‚Äì <a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\" rel=\"noopener\">Amazon CloudFront</a> is used to cache content, improving performance and routing traffic efficiently.</li><li> ‚Äì Information about uploaded files and job execution is stored in <a href=\"https://aws.amazon.com/rds/aurora/\" target=\"_blank\" rel=\"noopener\">Amazon Aurora</a>.</li><li> ‚Äì <a href=\"https://aws.amazon.com/batch/\" target=\"_blank\" rel=\"noopener\">AWS Batch</a> processes thousands of images in bulk.</li><li> ‚Äì <a href=\"https://aws.amazon.com/sqs/\" target=\"_blank\" rel=\"noopener\">Amazon Simple Queue Service</a> (Amazon SQS) manages and queues jobs for processing, making sure they‚Äôre run in the correct order by AWS Batch.</li><li> ‚Äì Amazon Rekognition services analyze media files, including: \n  <ul><li>Face Analysis to generate headless crops.</li><li>Moderation to detect and flag profanity and explicit content.</li><li>Label Detection to provide context for image processing and focus on relevant objects.</li><li>Custom Labels to identify and verify brand logos and adhere to brand guidelines.</li></ul></li></ul><p>Amazon Rekognition is an AWS computer vision service that powers Crop.photo‚Äôs automated image analysis. It enables object detection, facial recognition, and content moderation capabilities:</p><ul><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/faces.html\" target=\"_blank\" rel=\"noopener\">Face detection</a> ‚Äì The Amazon Rekognition face detection feature automatically identifies and analyzes faces in product images. You can use this feature for face-based cropping and optimization through adjustable bounding boxes in the interface.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectLabelsImageProperties.html\" target=\"_blank\" rel=\"noopener\">Image color analysis</a> ‚Äì The color analysis feature examines image composition, identifying dominant colors and balance. This integrates with Crop.photo‚Äôs brand guidelines checker to provide consistency across product images.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/labels.html\" target=\"_blank\" rel=\"noopener\">Object detection</a> ‚Äì Object detection automatically identifies key elements in images, enabling smart cropping suggestions. The interface highlights detected objects, allowing you to prioritize specific elements during cropping.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/what-is.html\" target=\"_blank\" rel=\"noopener\">Custom label detection</a> ‚Äì Custom label detection recognizes brand-specific items and assets. Companies can train models for their unique needs, automatically applying brand-specific cropping rules to maintain consistency.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/text-detection.html\" target=\"_blank\" rel=\"noopener\">Text detection (OCR)</a> ‚Äì The OCR capabilities of Amazon Recognition detect and preserve text within images during editing. The system highlights text areas to make sure critical product information remains legible after cropping.</li></ul><p>Within the Crop.photo interface, users can upload videos through the standard interface, and the speech-to-text functionality will automatically transcribe any audio content. This transcribed text can then be used to enrich the metadata and descriptions associated with the product images or videos, improving searchability and accessibility for customers. Additionally, the brand guidelines check feature can be applied to the transcribed text, making sure that the written content aligns with the company‚Äôs branding and communication style.</p><p>The Crop.photo service follows a transparent <a href=\"https://help.crop.photo/en/articles/9752452-understanding-your-subscription-plan\" target=\"_blank\" rel=\"noopener\">pricing model</a> that combines unlimited automations with a flexible image credit system. Users have unrestricted access to create and run as many automation workflows as needed, without any additional charges. The service includes a range of features at no extra cost, such as basic image operations, storage, and behind-the-scenes processing.</p><p>For advanced AI-powered image processing tasks, like smart cropping or background removal, users consume image credits. The number of credits required for each operation is clearly specified, allowing users to understand the costs upfront. Crop.photo offers several subscription plans with varying image credit allowances, enabling users to choose the plan that best fits their needs.</p><h2>Results: Improved speed and precision</h2><p>The automated image editing capabilities of Crop.photo with the integration of Amazon Rekognition has increased speed in editing, with 70% faster image retouching for ecommerce. With a 75% reduction in manual work, the turnaround time for new product images is reduced from 2‚Äì3 days to just 1 hour. Similarly, the bulk image editing process has been streamlined, allowing over 100,000 image collections to be processed per day using <a href=\"https://aws.amazon.com/fargate\" target=\"_blank\" rel=\"noopener\">AWS Fargate</a>. Advanced AI-powered image analysis and editing features provide consistent, high-quality images at scale, eliminating the need for manual review and approval of thousands of product images.</p><p>For instance, in the ecommerce industry, this integration facilitates automatic product detection and precise cropping, making sure every image meets specific marketplace and brand standards. In sports, it enables quick identification and cropping of player facial features, including head, eyes, and mouth, adapting to varying backgrounds and maintaining brand consistency.</p><p>The following images are before and after pictures for an ecommerce use case.</p><p>For a <a href=\"https://www.crop.photo/case-studies/bulk-content-resize-beverage\" target=\"_blank\" rel=\"noopener\">famous wine</a> retailer in the United Kingdom, the integration of Amazon Rekognition with Crop.photo streamlined the processing of over 1,700 product images, achieving a 95% reduction in bulk image editing time, a confirmation to the efficiency of AI-powered enhancement.</p><p>Similarly, a top 10 <a href=\"https://www.crop.photo/blog/bulk-on-model-product-images-face-crop\" target=\"_blank\" rel=\"noopener\">global specialty retailer</a> experienced a transformative impact on their ecommerce fashion marketplace workflow. By automating the cropping of over 3,000 on-model product images monthly, they boosted their retouching team‚Äôs productivity by over 70%, maintaining compliance with the varied image standards of multiple online marketplaces.</p><p>These case studies illustrate the tangible benefits of integrating Crop.photo with Amazon Rekognition, demonstrating how automation and AI can revolutionize the bulk image editing landscape for ecommerce and sports industries.</p><p>Crop.photo, from AWS Partner Evolphin Software, offers powerful bulk processing tools for automating image cropping, content resizing, and listing image analysis, using advanced AI-driven solutions. Crop.photo is tailored for high-end retailers, ecommerce platforms, and sports organizations. Its integration with Amazon Rekognition aims to streamline the image editing process for clients, providing speed and accuracy in the high-stakes environment of ecommerce and sports. Crop.photo plans additional AI capabilities with <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> generative AI frameworks to adapt to emerging digital imaging trends, so it remains an indispensable tool for its clients.</p><p>To learn more about Evolphin Software and Crop.photo, visit their <a href=\"https://www.crop.photo/\" target=\"_blank\" rel=\"noopener\">website</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/28/Rahul-copy.png\" alt=\"\" width=\"100\" height=\"100\"> founder &amp; CTO of Evolphin Software and Crop.photo, is reshaping how brands produce and manage visual content at scale. Through Crop.photo‚Äôs AI-powered tools, global names like Lacoste and Urban Outfitters, as well as ambitious Shopify retailers, are rethinking their creative production workflows. By leveraging cutting-edge Generative AI, he‚Äôs enabling brands of all sizes to scale their content creation efficiently while maintaining brand consistency.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/28/vaishnavi-100-1.png\" alt=\"\" width=\"100\" height=\"127\"> is a Solutions Architect specializing in Cloud Security at AWS based in the San Francisco Bay Area. As a trusted technical advisor, Vaishnavi helps customers to design secure, scalable and innovative cloud solutions that drive both business value and technical excellence. Outside of work, Vaishnavi enjoys traveling and exploring different artisan coffee roasters.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/16/image-7-1.png\" alt=\"\" width=\"100\" height=\"100\"> is an Account Manager at AWS, who provides guidance to Evolphin Software and other organizations to help accelerate business outcomes leveraging AWS Technologies. John has a degree in Business Administration and Management with a concentration in Finance from Gonzaga University, and enjoys snowboarding in the Sierras in his free time.</p>","contentLength":13507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revolutionizing business processes with Amazon Bedrock and Appian‚Äôs generative AI skills","url":"https://aws.amazon.com/blogs/machine-learning/revolutionizing-business-processes-with-amazon-bedrock-and-appians-generative-ai-skills/","date":1739212621,"author":"Sunil Bemarkar","guid":222,"unread":true,"content":"<p><em>This blog post is co-written with Louis Prensky and Philip Kang from Appian.&nbsp;</em></p><p>The digital transformation wave has compelled enterprises to seek innovative solutions to streamline operations, enhance efficiency, and maintain a competitive edge. Recognizing the growing complexity of business processes and the increasing demand for automation, the integration of <a href=\"https://aws.amazon.com/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a> skills into environments has become essential. This strategic move addresses key challenges such as managing vast amounts of unstructured data, adhering to regulatory compliance, and automating repetitive tasks to boost productivity. Using robust infrastructure and advanced language models, these AI-driven tools enhance decision-making by providing valuable insights, improving operational efficiency by automating routine tasks, and helping with data privacy through built-in detection and management of sensitive information. For enterprises, this means achieving higher levels of operational excellence, significant cost savings, and scalable solutions that adapt to business growth. For customers, it translates to improved service quality, enhanced data protection, and a more dynamic, responsive service, ultimately driving better experiences and satisfaction.</p><p>Appian has led the charge by offering generative AI skills powered by a collaboration with <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> and <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropic‚Äôs Claude</a> large language models (LLMs). This partnership allows organizations to:</p><ul><li>Enhance decision making with valuable insights</li><li>Improve operational efficiency by automating tasks</li><li>Help protect data privacy through built-in detection and management of sensitive information</li><li>Maintain compliance with HIPAA and FedRAMP compliant AI skills</li></ul><p>Critically, by placing AI in the context of a wider environment, organizations can operationalize AI in processes that seamlessly integrate with existing software, pass work between digital workers and humans, and help achieve strong security and compliance.</p><p><a href=\"https://www.appian.com/\" target=\"_blank\" rel=\"noopener\">Appian</a>, an&nbsp;<a href=\"https://partners.amazonaws.com/partners/001E000000Rl0w0IAB/Appian\" target=\"_blank\" rel=\"noopener\">AWS Partner</a>&nbsp;with competencies in financial services, healthcare, and life sciences, is a leading provider of&nbsp;<a href=\"https://www.appian.com/low-code-basics/\" target=\"_blank\" rel=\"noopener\">low-code automation software</a> to streamline and optimize complex business processes for enterprises. The Appian AI Process Platform includes everything you need to design, automate, and optimize even the most complex processes, from start to finish. The world‚Äôs most innovative organizations trust Appian to improve their workflows, unify data, and optimize operations‚Äîresulting in accelerated growth and superior customer experiences.</p><p><a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.</p><p>Appian uses the robust infrastructure of Amazon Bedrock and Anthropic‚Äôs Claude LLMs to offer fully integrated, pre-built generative AI skills that help developers enhance and automate business processes using low-code development. These use case-driven tools automate common tasks in business processes, making AI-powered applications faster and easier to develop.</p><p>This blog post will cover how Appian AI skills build automation into organizations‚Äô mission-critical processes to improve operational excellence, reduce costs, and build scalable solutions. Additionally, we‚Äôll cover real-world examples of processes such as:</p><ul><li>A mortgage lender that used AI-driven data extraction to reduce mortgage processing times from 16 weeks to 10 weeks.</li><li>A financial services company that achieved a four-fold reduction in data extraction time from trade-related emails.</li><li>A legal institution that used AI to reduce attorney time spent on contract review, enabling them to focus on other, high-value work.</li></ul><h2>Current challenges faced by enterprises</h2><p>Modern enterprises face numerous challenges, including:</p><ul><li><strong>Managing vast amounts of unstructured data: </strong>Enterprises deal with immense volumes of data generated from various sources such as emails, documents, and customer interactions. Organizing, analyzing, and extracting valuable insights from unstructured data can be overwhelming without advanced AI capabilities.</li><li><strong>Help protect data privacy and compliance: </strong>With increasing regulatory requirements around data privacy and protection, organizations must safeguard sensitive information, such as personally identifiable information (PII). Manual processes for data redaction and compliance checks are often error-prone and resource-intensive.</li><li><strong>Streamlining repetitive and time-consuming tasks: </strong>Routine tasks such as data entry, document processing, and content classification consume significant time and effort. Automating these tasks can lead to substantial productivity gains and allow employees to focus on more strategic activities.</li><li><strong>Adapting to rapidly changing market conditions: </strong>In a fast-paced business environment, organizations need to be agile and responsive. This requires real-time data analysis and decision-making capabilities that traditional systems might not provide. AI helps businesses quickly adapt to industry changes and customer demands.</li><li><strong>Enhancing decision-making with accurate data insights: </strong>Making informed decisions requires access to accurate and timely data. However, extracting meaningful insights from large datasets can be challenging without advanced analytical tools. AI-powered solutions can process and analyze data at scale, providing valuable insights that drive better decision-making.</li></ul><h2>Appian AI service architecture</h2><p>The architecture of the generative AI skills integrates both the Amazon Bedrock and <a href=\"https://aws.amazon.com/textract/\" target=\"_blank\" rel=\"noopener\">Amazon Textract</a> scalable infrastructure with Appian‚Äôs process management capabilities. This generative AI architecture is designed with&nbsp;<a href=\"https://docs.appian.com/suite/help/24.3/private-ai.html\" target=\"_blank\" rel=\"noopener\">private AI</a>&nbsp;as the foundation and upholds those principles.</p><p>The key components of this architecture include:</p><ol><li><strong>Appian AI Process Platform instances</strong>: The frontend serves as the primary application environment where users interact with the system application to upload documents, initiate workflows, and view processed results.</li><li>: This service functions as an intermediary layer between the Appian instances and AWS AI services (Amazon Textract and Amazon Bedrock). This layer encapsulates the logic required to interact with the AWS AI services to manage API calls, data formatting, and error handling.</li><li>: This AWS service is used to automate the extraction of text and structured data from scanned documents and images and provide the extracted data in a structured format.</li><li>: This AWS service provides advanced AI capabilities using FMs for tasks such as text summarization, sentiment analysis, and natural language understanding. This helps enhance the extracted data with deeper insights and contextual understanding.</li></ol><p>Appian generative AI skills, powered by Amazon Bedrock with Anthropic‚Äôs Claude family of LLMs, are designed to jump-start the use of generative AI in your processes. The following figure showcases the diverse capabilities of Appian‚Äôs generative AI skills, demonstrating how they enable enterprises to seamlessly automate complex tasks.</p><p>Each new skill provides a pre-populated prompt template tailored to specific tasks, alleviating the need to start from scratch. Businesses can select the desired action and customize the prompt for a perfect fit, enabling the automation of tasks such as:</p><ul><li><strong>Content analysis and processing</strong>: With Appian‚Äôs generative AI skills, businesses can automatically generate, summarize, and classify content across various formats. This capability is particularly useful for managing large volumes of customer feedback, generating reports, and creating content summaries, significantly reducing the time and effort required for manual content processing.</li><li>: Organizations generate mountains of data and documents. Extracting this information manually can be both burdensome and error-prone. Appian‚Äôs AI skills can perform highly accurate text extraction from PDF files and scanned images and pull relevant data from both structured and unstructured data sources such as invoices, forms, and emails. This speeds up data processing and promotes higher accuracy and consistency.</li><li><strong>PII extraction and redaction</strong>: Identifying and managing PII within large datasets is crucial for data governance and compliance. Appian‚Äôs AI skills can automatically identify and extract sensitive information from documents and communication channels. Additionally, Appian supports plugins that can redact this content for further privacy. This assists your compliance efforts without extensive manual intervention.</li><li>: Appian‚Äôs AI skills can summarize documents to give users an overview before digging into the details. Whether it‚Äôs summarizing research papers, legal documents, or internal reports, AI can generate concise summaries, saving time and making sure that critical information is highlighted for quick review.</li></ul><p>The following figure shows an example of a prompt-builder skill used to extract unstructured data from a bond certificate.</p><p>Each AI skill offers pre-populated prompt templates, allowing you to deploy AI without starting from scratch. Each template caters to specific business needs, making implementation straightforward and efficient. Plus, users can customize these prompts to fit their unique requirements and operational needs.</p><p>In this solution, Appian Cloud seamlessly integrates and customizes Amazon Bedrock and Claude LLMs behind the scenes, abstracting complexity to deliver enterprise-grade AI capabilities tailored to its cloud environment. It provides pre-built, use case specific prompt templates for tasks like text summarization and data extractions, dynamically customized based on user inputs and business context. Using the scalability of the Amazon Bedrock infrastructure, Appian Cloud provides optimal performance and efficient handling of enterprise-scale workflows, all within a fully managed cloud service.</p><p>By addressing these complexities, Appian Cloud empowers businesses to focus solely on using AI to achieve operational excellence and business outcomes without the burdens of technical setup, integration challenges, or ongoing maintenance efforts.</p><p>Appian‚Äôs AI skills have proven effective across multiple industries. Here are a few real-world examples:</p><ul><li>: This organization automated the extraction of over 60 data fields from inconsistent document formats, reducing the process timeline from 16 weeks to 10 weeks and achieving 98.33% accuracy. The implementation of Appian‚Äôs generative AI skills allowed the mortgage processor to streamline their workflow, significantly cutting down on processing time and improving data accuracy, which led to faster loan approvals and increased customer satisfaction.</li><li>: A financial service company received over 1,000 loosely structured emails about trades. Manually annotating these emails led to significant human errors. With an Appian generative AI skill, the customer revamped the entity tagging process by automatically extracting approximately 40 data fields from unstructured emails. This resulted in a four-fold reduction in extraction time and achieved over 95% accuracy, improving the user experience compared to traditional ML extraction tools. The automated process not only reduced errors but also enhanced the speed and reliability of data extraction, leading to more accurate and timely trading decisions.</li><li>: A legal institution had to review negotiated contracts against the original contracts to determine whether the outlined risks had been resolved. This manual process was error prone and labor intensive. By deploying a generative AI skill, they automated the extraction of changes between contracts to find the differences and whether risks had been resolved. This streamlined the attorney review process and provided insights and reasoning into the differences found. The automated solution significantly reduced the time attorneys spent on contract review, allowing them to focus on more strategic tasks and improving the overall efficiency of the legal department.</li></ul><p>AWS and Appian‚Äôs collaboration marks a significant advancement in business process automation. By using the power of Amazon Bedrock and Anthropic‚Äôs Claude models, Appian empowers enterprises to optimize and automate processes for greater efficiency and effectiveness. This partnership sets a new standard for AI-driven business solutions, leading to greater growth and enhanced customer experiences. The ability to quickly deploy and customize AI skills allows businesses to stay agile and responsive in a dynamic environment.</p><p><a href=\"https://aws.amazon.com/marketplace/seller-profile?id=94a5a278-5f8c-4a9e-82e4-d3f0dd2ecbfe\" target=\"_blank\" rel=\"noopener\">Appian</a> solutions&nbsp;are available as software as a service (SaaS) offerings in&nbsp;<a href=\"https://aws.amazon.com/marketplace\" target=\"_blank\" rel=\"noopener\">AWS Marketplace</a>. Check out the <a href=\"https://docs.appian.com/suite/help/24.3/ai-skills-intro.html\" target=\"_blank\" rel=\"noopener\">Appian</a> website to learn more about how to use the AI skills.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/03/07/hsSunil.png\" alt=\"Sunil Bemarkar\" width=\"100\" height=\"125\">is a Senior Partner Solutions Architect at Amazon Web Services. He works with various Independent Software Vendors (ISVs) and Strategic customers across industries to accelerate their digital transformation journey and cloud adoption.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/01/28/John-Klacynski-100.jpeg\" alt=\"\" width=\"100\" height=\"100\">is a Principal Customer Solution Manager within the AWS Independent Software Vendor (ISV) team. In this role, he programmatically helps ISV customers adopt AWS technologies and services to reach their business goals more quickly.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/louis.jpg\" alt=\"\" width=\"100\" height=\"100\">is a Senior Product Manager at Appian. He is responsible for driving product strategy and feature design for AI Skills within Appian‚Äôs Cognitive Automation Group.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/01/Appian-Philip-Kang-100x100.jpeg\" alt=\"Philip Kang\" width=\"100\" height=\"100\">is a Principal Solutions Consultant in Partner Technology &amp; Innovation centers with Appian. In this role, he spearheads technical innovation with a focus on AI/ML and cloud solutions.</p>","contentLength":13619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I made a tiny book using a pen-plotter and AI","url":"https://muffinman.io/blog/the-tiny-book-of-great-joys/","date":1739209951,"author":"stankot","guid":315,"unread":true,"content":"<p>If you are interested in how I over-engineered the process of making a tiny book for my wife, using AI, a pen plotter, a 3D printer, and a lot of time, you are in the right place. The book is titled  , and here is how it turned out:</p><p>My wife is delighted with it, so it was worth all the effort.</p><p>This post will take you through the process. It will be a long one, but please stick around - I promise there will be a lot of pretty pictures.</p><p>Here is the outline of the post:</p><p>I had this idea for a while after seeing something similar somewhere on the internet.. Since then, I always wanted to make one for my wife - a physically small book with a bunch of small drawings of our memories together, inside jokes, and little things she likes.</p><p>I wanted the illustrations to be hand-drawn, and I had a plan to ask my friend to do them. But I knew he would refuse any kind of payment, so I felt bad adding more work to his plate. So I shelved the idea, but every now and then, it would pop up in my head.</p><p>Fast forward a few years - we got a kid, and our routine completely changed. We are enjoying it a lot, but it can be very exhausting, and every day seems identical to the last. That's why I decided I needed to do something for her to break the routine. The book idea seemed perfect - personal and handcrafted - so I gave it a try.</p><p>To be able to do everything myself, I went to create digital drawings and then draw them on paper using my trusty pen plotter.</p><p>With the idea in place, I moved on to creating the drawings - which turned out to be a challenge of its own.</p><p>For pen plotting, one needs vector files, so I started drawing in Figma. Unfortunately, I quickly realized that my drawing skills would not get me the result I had envisioned. Determined to do it this time, I decided to try using AI to generate images.</p><p>I got myself a Midjourney subscription and started playing with it. It took a lot of failed attempts to figure out how to get drawings that were simple and had a strong hand-drawn feel to them. Even then, I ended up editing every one of them, but more on that later.</p><p>One of the first images I was satisfied with (it didn't end up in the book, though):</p><p>It took a lot of time, but it was fun. Failed attempts were often quirky and funny, and I was learning how to use the tool. And it made me feel like a secret agent, doing it next to my wife, who had no idea what I was up to.</p><p>I may be wrong, but I think Midjourney wasn't built for the kind of illustrations I had in mind. I was after simple, hand-drawn illustrations that felt personal. Luckily, I found a style reference () that worked well for my case. I used it to generate almost all of the drawings that ended up in the book. For those who haven't used Midjourney - you can use images as style references to influence the style of images you want to generate.</p><p>Most of my images were generated using that  code and a  between 150 and 400 (it can go from 0 to 1000).</p><p>As for the prompts, these are the key terms I combined with the description and the style reference:</p><ul><li>isolated on white background</li></ul><p>It took me a lot of tries - between 10 and 30 attempts for each image you see in the book.</p><p>Once I solved the image generation part, I had to figure out how to turn them into vector files for plotting. The first thing I tried was something similar to halftone. As you can see below, in this process, the images completely lost the hand-drawn feel.</p><p>Then I remembered <a href=\"https://www.instagram.com/p/CNJ_ZBOHZKj/\">this plot</a> of Marble Machine X I did a while ago, for which I used AutoTrace to convert the original image to a vector file. The great thing about AutoTrace is that it supports \"centerline tracing\". And this time, I learned that Inkscape has a great AutoTrace plugin, which made it even easier to convert.</p><h3>What makes centerline tracing different</h3><p>Most of the tools that convert raster to vector images do it by outlining shapes. This is not suitable for plotting, as each line in the original image becomes a sausage-like shape. Centerline tracing, on the other hand, tries to draw a single line following the middle path through shapes. Don't worry if it sounds confusing; the example below should make things clearer.</p><p>Here is the image of Link from  generated by Midjourney:</p><p>After applying a common vectorization technique, we get this. As you can see, each line in the original drawing is now outlined, creating this messy-looking image.</p><p>But if we use centerline tracing, it suddenly looks a lot more like a drawing. It is not perfect, but don't worry - we are going to clean it up in the next step.</p><p>In the points where lines touch or cross, AutoTrace is not sure which line to follow and creates these funky-looking joints. Here is an exaggerated example to show you what I'm talking about. Input is the raster image at the top and the vectorized result is at the bottom:</p><p>But I found out that if I roughly separate these lines, I get a much better result.</p><p>Let's now apply this technique to the image of Link we've seen above. After separating lines (and some cleaning up) this is the image I ended up with. It is rough, but it is only used as an input for the tracing process, so it doesn't really matter. This was manual and somewhat tedious process, but I enjoyed it overall. It was a sort of meditation for me.</p><p>And finally, when we trace this image, we get a really nice and clean vector file perfect for plotting. <img alt=\"Very clean vectorized image of Link\" src=\"https://muffinman.io/img/tiny-book/link-05-after.png\"></p><p>Here is another example. We start with the image I generated using Midjourney:</p><p>After editing, removing details and separating lines, we get this one:</p><p>And the traced vector result:</p><p>You'll notice that in both examples I did  . I did that for pretty much all of the images, to fix things I wasn't able to polish using prompts. I also removed a lot of details to make sure images are crisp and readable at the small size.</p><p>All of this took a lot of experimentation, but it gave me a pretty solid workflow which I used to generate all of the images. The complete flow looks like this:</p><ul><li>Generate images using Midjourney.</li><li>Upscale them two times, because upscaled images were easier to edit and tracing was more precise.</li><li>Clean up, redraw and separate lines by hand using Gimp.</li><li>Use Inkscape plugin to run AutoTrace centerline tracing.</li></ul><p>It took me a while to generate all the images, and the fact that I was trying to keep it a secret from my wife didn't help. I think I did it over the span of two weeks, mostly in the evening after she would go to bed.</p><h3> never stood a chance!</h3><p>Before we continue I just want to show you two funky images of Link that really made me laugh:</p><p>With the drawings ready, I turned to the next crucial part - the text. I first wanted to write everything by hand, photograph it and then vectorize it in the same way I did with the images. But it was a hassle - I had to do a lot of editing for text to look as my handwriting.</p><p>Evil Mad Scientist, the maker of my pen plotter, has a fantastic tool called <a href=\"https://wiki.evilmadscientist.com/Hershey_Text\">Hershey Text</a>. It contains a bunch of single-line fonts ideal for plotting. I chose the EMS Elfin font as it looked playful and hand-drawn. I used it to write all of the text in the book and I think it turned out great.</p><p>The tricky part with bookbinding is that pages are not printed in order, but in a way that when you fold the sheets in half, you get the right order. I used Figma to design the layout, with a great care to make sure pages are in order after double-sided plotting.</p><p>Here is the layout laid out on A4 sized paper. Sorry for blurring the text, but a lot of it is very personal and I want to keep it for our eyes only.</p><p>Plotting is the part that went the smoothest, but not without hiccups. I usually use Pigma Micron blackliner markers. They use archival quality ink and they are literally indestructible. But this time, even the thinnest one I had was too thick for the book this small.</p><p>Here you can see the first  using markers of 0.2mm and 0.1mm thickness respectively. Lines got a bit smudged and looked much thicker than I expected. This was also the moment I realized I need to remove  from the images to make them readable at this size.</p><p>I needed to find a thinner pen.</p><h3>Technical pen to the rescue</h3><p>Blackliner markers were made as a more practical replacement for technical pens. But from what I've read, an old-school technical pen was the only thing capable of achieving super-fine lines I wanted. I went online and ordered Rotring Isograph 0.2mm. As soon as it arrived I sneaked out to my study and did another test plot using it. Oh boy, was I happy when I saw the result:</p><p>Lines were thin and crisp and at this point I was convinced the project will be a success!</p><p>All of the first plots were done on 120gsm printer paper. It is somewhat thick paper and drawings looked fantastic. Unfortunately, when I bound the pages together, the drawings and letters would get transferred on the opposite pages. I could probably get away with it, considering the whole hand made feel of the book. But I wanted it to be perfect.</p><p>A friend advised me to leave ink to dry for a few hours. I left each side to dry for 24 hours, but it smudged again. Next time I tried putting the plot (before cutting the pages) between two sheets of papers and pressing it with heavy books. I did that for more than 24 hours, but still after cutting and bounding the pages, they got smudged again. At this point I was becoming somewhat desperate. As the last resort I ordered different, 100gsm paper and to my relief it worked! Crisis averted!</p><p>In the final version you can still see tiny traces on a few pages, but these are barely visible and don't really bother me.</p><p>After plotting and cutting I was left with a stack of somewhat delicate pages. Now, it was finally time to turn them into a book.</p><p>As you can imagine, I had zero bookbinding experience. There are a lot of resources online, but two of them were crucial for my project as they were on how to bind tiny books:</p><p>After reading and watching these and a few generic articles on bookbinding, I gathered enough info to try doing it myself. I thought I was super clever because I 3D printed sides and spine of the book. I designed sewing holes in the spine so I can connect the pages directly to it without using glue. It was a decent idea, but it left a gap between two  . Still, I went with it for the first try.</p><p>I laid everything down on the canvas that the book would be wrapped in and started assembling it. But I made a crucial mistake - I used super glue. It dries quickly, it is stiff, and doesn't glue 3D printed plastic well and it dissolved the paper I used. Long story short, I made a mess. But I didn't stress too much, I just proclaimed that version is a prototype and used it as a learning experience.</p><p>I ordered proper bookbinding glue (PVA). While I was waiting for it, I focused on properly sewing the pages together.</p><p>The first time I sewed the pages together, I poked the holes by hand and they were somewhat uneven. Again, it was nothing major, but I didn't like it. So I designed and 3D printed a simple tool to help me drill the holes evenly.</p><p>The tool has two parts, and the pages fit snugly between them. Both top and bottom parts have holes, so I was able to put the needle through and poke perfectly even holes in the pages. I'm very proud of this silly contraption.</p><p>Here you can see all of the eight sheets with sewing holes.</p><p>Fun fact, I designed all 3D parts using JavaScript and <a href=\"https://replicad.xyz/\">Replicad</a> library. Here is <a href=\"https://studio.replicad.xyz/workbench?from-url=https://muffinman.io/img/tiny-book/model.js\">a link</a> if you want to play with the model in your browser.</p><p>But I ditched the 3D printed spine and used the technique called , which works great when you have only two signatures. It made signatures way more tight than when I connected them separately to the 3D printed spine.</p><p>When the glue arrived, I plotted everything again and took it from the top. I swapped 3D printed sides for cardboard. Using proper glue was a game changer. I had enough time to apply it before it hardened, and when it dried it stayed flexible. And when it got onto my fingers, it was easy to remove. Everything was much cleaner, and I finally managed to put it all together.</p><p>Unfortunately, I was rushing to finish the book, so I didn't take any photos of the process. But here are a few I do have:</p><p>If you are an experienced bookbinder and reading this, I'm sorry for the bookbinding crimes I probably committed. I promise I won't use super glue again.</p><p>It looked great! It was not perfect (more on that below), but I was super happy with how it turned out. It had a distinct handcrafted feel to it, the images turned out fantastic, and I think I really managed to bring out a personal touch with it.</p><p>On the day I finished the book and gave it to my wife, we were both exhausted (our kid was teething, and we had a very rough night), so I thought she would appreciate a little pick-me-up.</p><p>When I gave it to her, the first thing she asked was, \"Will I cry?\". She was brave, but it definitely got her all mushy and made her day. After reading, she carefully put it on the shelf, out of the reach of the little one.</p><p>Then I asked her if she ever suspected I was preparing a surprise for her, and she said that she had no idea. But she also said that she thought it was weird that I would often plot something and not brag about it to her afterwards. It was true, I love showing her my work, but luckily she didn't give it too much thought, and I was able to finish my secret project.</p><h3>One thing I would like to fix</h3><p>Like I mentioned, the book isn't perfect. The sides are a bit too large, so the pages seem too deep inside when the book is closed. For the same reason, the end pages turned out to be a bit short, which gives it a weird, uneven look. It is purely aesthetic, but I think it is the only thing keeping it from being perfect.</p><p>Lesson learned if I ever end up doing something similar.</p><p>It took way longer than it should have‚Äîit took me a month and a half to finish it. It took so long because I did it in secrecy, which meant working late in the evenings when my wife and kid were asleep. A bunch of little failures... ehm, I mean  also prolonged the project. And finally, I had to order multiple things, so I was blocked a few times while I was waiting for four different deliveries.</p><p>But the final assembly took me around two and a half hours from start to finish - plotting, cutting, sewing, and bookbinding. Mostly because I had already practiced all of them and defined the exact process.</p><p>It was so much fun. I love projects that span across multiple disciplines. This one touched AI, drawing, plotting, modeling, 3D printing, sewing, and bookbinding. I encountered a lot of little hiccups, but I also learned about all of them. Some of the errors I made could have been avoided if I had been more patient. But I hope you'll cut me some slack - I was super excited and eager to see how it would turn out, and I had limited time windows when I could do it in secrecy. Still, I need to take it as a lesson - being patient will help me save time when doing projects like this one.</p><p>The highlight for me was that I could do it without an illustrator. Love it or hate it, AI ended up being a fantastic tool that filled the gap in my skill set, which was crucial for making the book.</p><p>I hope you enjoyed this write-up as much as I enjoyed making the book and writing the post. And I do hope I inspired you to try making something of your own. If I did, please reach out on GitHub, I would love to see it.</p>","contentLength":15245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43003052"},{"title":"Beginner‚Äôs Guide to Subqueries in SQL","url":"https://www.kdnuggets.com/beginners-guide-subqueries-sql","date":1739206838,"author":"Nate Rosidi","guid":583,"unread":true,"content":"<article>Subqueries are popular tools for more complex data manipulation in SQL. If you‚Äôre a beginner on a quest to understand subqueries, this is the article for you.</article>","contentLength":160,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Rosidi_Beginners_Guide_to_Subqueries_in_SQL_5.png.png","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Global 3D topography explorer","url":"https://topography.jessekv.com/","date":1739202885,"author":"jessekv","guid":314,"unread":true,"content":"<div>\n                Click catchements or regions on the map to render them in 3D.\n                <a href=\"https://jessekv.com/post/watersheds/\" target=\"_blank\">Learn more</a></div>","contentLength":105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43001688"},{"title":"Python Morsels: Newlines and escape sequences in Python","url":"https://www.pythonmorsels.com/newlines-and-escape-sequences/","date":1739200649,"author":"","guid":265,"unread":true,"content":"<p>This string contains a newline character:</p><div><pre><code></code></pre></div><p>That's what  represents: a newline character.</p><p>If we print this string, we'll see that  becomes an  newline:</p><p>Why does Python represent a newline as ?</p><h2>Escape sequences in Python</h2><p>Every character in a Python ‚Ä¶</p>","contentLength":244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Micro Frontends with Angular and Native Federation","url":"https://blog.angular.dev/micro-frontends-with-angular-and-native-federation-7623cfc5f413?source=rss----447683c3d9a3---4","date":1739199751,"author":"Manfred Steyer","guid":770,"unread":true,"content":"<p>Enterprise-scale software systems are often implemented by several cross-functional teams. To enable such teams to provide new features efficiently, it is desirable to minimize the need for coordination between them. This calls for a modularization that verticalizes the system into low-coupled areas individual teams can take care&nbsp;of.</p><p>There are several possibilities for implementing such high-level modules (also called verticals). For instance, they can be implemented using a respective folder structure or in the form of several libraries in a Monorepo. Micro Frontends go one step further and designate a separate application for each vertical. This architectural style promises several advantages, such as a high amount of team autonomy, but it also comes with numerous challenges.</p><p>The first part of this article provides a critical overview of the benefits and drawbacks of Micro Frontends in the space of single-page applications. The second part discusses how such an architecture can be implemented with Native Federation, a community project built upon web standards that provide close integration with the Angular&nbsp;CLI.</p><h3>Motivation Behind Micro Frontends</h3><p>Like Micro Services, Micro Frontends promise several advantages, both technically and with respect to the organization. As applying Micro Frontend architectures results in several smaller applications, testing, performance tuning, and isolating faults in one part of the overarching system becomes more effortless, according to several&nbsp;sources.</p><p>However, the increased team autonomy was the main reason for applying this architectural style in the numerous cases I was involved in as a consultant. Individual teams are not blocked by waiting for other teams and can deploy separately anytime. This might not be a significant concern in a vast number of projects. Still, as soon as we talk about multi-team projects in a corporate environment with long communication paths and decision times, this aspect quickly becomes vital for the project‚Äôs success.</p><p>Teams can also make their own decisions that best fit their goals‚Ää‚Äî‚Ääarchitecturally and technology-wise. Mixing multiple client-side frameworks in the same application is considered an anti-pattern and should be avoided. However, it can help to create a migration path over to a new stack in the long run. The concern in corporate environments is that we find software solutions that usually outlive the average technology stack</p><p>Since Micro Frontends result in separate build processes, combining them with incremental builds, where only changed applications need to be rebuilt, has a massive potential for build-time improvements. For instance, the well-known <a href=\"https://nx.dev/\">Nx build system</a> provides this option. Interestingly, this feature can also be used without applying other aspects such as aligning teams with individual applications or separate deployments. There is debate about whether leveraging this tempting option automatically leads to micro frontend architectures.</p><p>A system consisting of several smaller applications can provide further organizational advantages: It‚Äôs easier to onboard new members and scale the development by adding further micro frontends. Team autonomy also leads to faster release&nbsp;cycles.</p><h3>Challenges to keep in&nbsp;mind</h3><p>Every architectural decision has consequences that need to be evaluated, and Micro Frontends are no exception. Besides the positive consequences outlined above, there are also several negative ones to consider.</p><p>For instance, individually developed Micro Frontends can diverge in UI/UX, leading to an inconsistent appearance. Also, loading several applications increases the number of bundles that need to be downloaded, adversely affecting loading times and increasing the memory pressure.</p><p>Splitting an application into low-coupled parts might be a best practice in general. However, it is often hard to define the boundaries between verticals clearly enough to implement them as individual applications. Also, while having several small applications at first glance simplifies the implementation, integrating them into an overarching solution brings additional complexity.</p><p>This leads to one of the biggest challenges I have seen in practice: we are moving away from a compile-time integration towards a runtime integration. This has severe consequences because we cannot easily foresee problems that might arise when individually developed and deployed applications start interacting at runtime. Besides the chance of technical conflicts, we also have to see that the current generation of SPA frameworks has not been built with such an operation mode in&nbsp;mind.</p><p>Instead, modern SPA frameworks, especially Angular, have been developed to focus on compile time optimizations. A powerful compiler leverages type checks to identify technical conflicts and emits efficient source code optimized for tree-shaking. Furthermore, the CLI in the Angular space provides a highly optimized build process. An off-label usage necessary for implementing Micro Frontends undermines some of these achievements.</p><h3>Angular Does Not Officially Support Micro Frontends</h3><p>For all the outlined reasons, the Angular team recommends checking if alternatives, like implementing the individual verticals in Monorepos, which can be compiled together, are suitable. For instance, Google adopted this approach years ago and manages all its products and libraries in a <a href=\"https://dl.acm.org/doi/10.1145/2854146\">single Monorepo</a>.</p><p>Of course, there are also ways to compensate for the disadvantages outlined here, and some of them, like establishing a design system to help with a consistent UI/UX or lazy loading individual system parts, might be needed in general. More details on such compensation strategies can be found in this <a href=\"https://www.angulararchitects.io/blog/consequences-of-micro-frontends-survey-results\">survey</a> of more than 150 Micro Frontend practitioners.</p><p>All architectural decisions have benefits and drawbacks and should be evaluated with those considerations if you are going to implement a solution. If such an evaluation reveals that Micro Frontends provide more advantages than alternatives for achieving your very goals, the following sections provide you a well-lit path for implementing this architectural pattern with&nbsp;Angular.</p><h3>Micro Frontends with Federation</h3><p><a href=\"https://module-federation.io/\">Module Federation</a> is a popular technology for implementing Micro Frontends and sharing dependencies. Shipped initially with webpack 5, it comes with a tooling-agnostic runtime and provides compile-time integration into  and . Besides the usage of the vite dev server, these technologies are currently not supported by the Angular CLI. However, promising community solutions like <a href=\"https://angular-rspack.dev/\"></a>and <a href=\"https://analogjs.org/\">AnalogJS</a> allow them to be used with Angular. <a href=\"https://nx.dev/\">Nx</a> and my <a href=\"https://www.npmjs.com/package/@angular-architects/module-federation\">CLI-plugin</a> provide an effortless integration.</p><p>Module Federation enables an application to load parts of other separately built and deployed applications lazily. The loading application is referred to as the host; the integrated ones are called&nbsp;remotes:</p><p>Federation, if permitted by the library version, can share dependencies like Angular or RxJS between the host and remotes. There are several configuration options for <a href=\"https://www.angulararchitects.io/blog/getting-out-of-version-mismatch-hell-with-module-federation/\">preventing version mismatches.</a> Since MF can only decide which dependencies to share at runtime, tree-shaking for shared parts is not possible.</p><p>To inform the host about the remotes and their shared dependencies, Module Federation creates a metadata file, the so-called remote entry, during the build. This file needs to be loaded into the&nbsp;host.</p><p>To fully decouple the idea of Federation from specific bundlers, I started the project <a href=\"https://www.npmjs.com/package/@angular-architects/native-federation\">Native Federation</a> several years ago. Its API surface is very similar to that of Module Federation. The focus is on portability and standards like ECMAScript modules and Import Maps. Its compile time acts as a wrapper around existing bundlers. For the communication with the bundler, it uses an exchangeable adapter:</p><p>The integration into the Angular CLI directly delegates to Angular‚Äôs  that leverages the fast bundler esbuild, and is the foundation for several current features like partial hydration. Because of its architecture, Native Federation can also be ported to further builders or other innovations the CLI might provide in the long&nbsp;run.</p><p>For integrating Micro Frontends built with Angular‚Äôs webpack-based builder, there is a <a href=\"https://www.angulararchitects.io/blog/combining-native-federation-and-module-federation/\">bridging solution</a> allowing the loading of such remotes into a Native Federation host. This solution enables the gradual adoption of the CLI‚Äôs new  and permits the sharing of dependencies between the two kinds of Federation. One of the features added recently is support for <a href=\"https://www.angulararchitects.io/blog/ssr-and-hydration-with-native-federation-for-angular/\">SSR and Hydration</a>, which is vital for performance-critical applications like public portals and web&nbsp;shops.</p><p>Native Federation for Angular is close to the CLI‚Äôs , but its compilation mode is for shared dependencies differs. While it works well for packages that align with <a href=\"https://angular.dev/tools/libraries/angular-package-format\">Angular‚Äôs Package Format</a>, which is the case for all libraries built with the CLI, other libraries might provide some challenges, especially older ones that still use CommonJS or older conventions for providing metadata.</p><h3>Using Native Federation in&nbsp;Angular</h3><p>For the setup Native Federation provides a schematic:</p><pre>ng add @angular-architects/native-federation --project mfe1 --port 4201 --type remote</pre><p>The switch  defines the kind of the application. Possible options are , , and . The latter is a host configured with a configuration file (federation manifest) during application start. This manifest informs the application of the locations of the remotes and can be switched out by another manifest during deployment:</p><pre>{    \"mfe1\" : \"http://localhost:4201/remoteEntry.json\"</pre><p>The key, in this case, is a short name the host uses to refer to the Micro Frontend. The value is the location of the remote entry with the metadata mentioned above. Alternatively, the manifest can be replaced by a service that informs the host of the current location of all deployed remotes and acts as a Micro Frontend registry.</p><p>The schematic configures the Native Federation builder delegation to the  and creates a configuration file :</p><pre>const { withNativeFederation, shareAll }     = require('@angular-architects/native-federation/config');<p>module.exports = withNativeFederation({</p>    './Component': './projects/mfe1/src/app/app.component.ts',    ...shareAll({}),    'rxjs/ajax',    'rxjs/testing',    // Add further packages you don't need at runtime</pre><p>The configuration assigns a unique name to the remote or host and defines which dependencies to share. Instead of providing an exhaustive list of all the dependencies to share, the configuration uses the helper function , which adds all dependencies found in the project‚Äôs . The skip list is used to opt out of sharing some of them or their secondary entry&nbsp;points.</p><p>Remotes also define exposed EcmaScript modules that can be loaded into the shell. For this, the  node maps the paths of the modules to short names such as&nbsp; in the example&nbsp;shown.</p><p>The schematic also adds code to initialize Native Federation to the . For the host, this code points to the federation manifest:</p><pre>import { initFederation } from '@angular-architects/native-federation';<p>initFederation('federation.manifest.json')</p>  .catch(err =&gt; console.error(err))<p>  .then(_ =&gt; import('./bootstrap'))</p>  .catch(err =&gt; console.error(err));</pre><p>After initializing federation, the file , also created by the schematic, is loaded. It contains the usual code for bootstrapping Angular, e.g., via  when the application uses Standalone Components.</p><p>To load a component or routing configuration exposed by a remote, traditional lazy loading is combined with Native Federation‚Äôs  function:</p><pre>import { loadRemoteModule } from '@angular-architects/native-federation';<p>export const APP_ROUTES: Routes = [</p>  [...]    path: 'flights',      loadRemoteModule('mfe1', './Component').then((m) =&gt; m.AppComponent),];</pre><p>Here,  is the key defined in the manifest, and&nbsp; points to the respective exposed module in the remote‚Äôs federation configuration.</p><p>More information on Native Federation can be found in this <a href=\"https://www.angulararchitects.io/blog/micro-frontends-with-modern-angular-part-1-standalone-and-esbuild/\">blog article</a> and in the project‚Äôs <a href=\"https://www.npmjs.com/package/@angular-architects/native-federation\">readme</a>, which also links to a tutorial.</p><p>Micro Frontends promise significant advantages for enterprise-scale applications, such as enhanced team autonomy and independent deployment. These benefits make this architectural style particularly appealing in multi-team corporate environments where streamlined communication and rapid development cycles are critical. Additionally, they support gradual migration to new technologies and optimize build times by leveraging incremental builds.</p><p>However, these advantages come with trade-offs. Micro Frontends can lead to inconsistent UI/UX, increased load times, and complex runtime integrations. Defining clear vertical boundaries and managing inter-application communication add to the challenge. Furthermore, frameworks like Angular, designed for compile-time optimization, face limitations in runtime integration scenarios. The Angular team, therefore, recommends alternatives such as splitting an application into libraries managed within a Monorepo, which aligns better with Angular‚Äôs strengths in type safety and efficient compilation.</p><p>Module Federation has emerged as a popular solution to address some challenges by enabling lazy loading and dependency sharing. Native Federation builds on these concepts with a focus on standards and portability. It provides a seamless integration into the Angular CLI and its performant esbuild-based , which is also the foundation for advanced features like SSR and hydration.</p><p><em>Together with this team at </em><a href=\"http://www.angulararchitects.io\"></a>,<em> Manfred Steyer helps companies around the globe establish maintainable Angular-based Architectures. He is a trainer, consultant, and Google Developer Expert (GDE) and writes for O‚ÄôReilly, the German Java Magazin, windows.developer, and Heise Developer. Furthermore, he regularly speaks at conferences.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7623cfc5f413\" width=\"1\" height=\"1\" alt=\"\">","contentLength":13726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Science Showdown: Which Tools Will Gain Ground in 2025","url":"https://www.kdnuggets.com/data-science-showdown-tools-gain-ground-2025","date":1739199602,"author":"Iv√°n Palomares Carrascosa","guid":582,"unread":true,"content":"<article>An analysis and discussion of the data science tools expected to gain prominence throughout the present year, and why.</article>","contentLength":118,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/kdn-tools-battle-2025.png","enclosureMime":"","commentsUrl":null},{"title":"Show HN: A unique generated maze to share with your valentine","url":"https://love.berk.es/","date":1739198155,"author":"berkes","guid":313,"unread":true,"content":"<dd><p> is an art project. It is so called <a href=\"https://en.wikipedia.org/wiki/Generative_art\">generative art</a>: I write the software, and the software creates art autonomously. \n            A nice introduction into generative art is <a href=\"https://www.youtube.com/watch?v=4Se0_w0ISYk\">this talk by Tim Holman</a>.\n          </p></dd><dd><p>\n          In a gallery in London, I came across gorgeaus screenprints by <a href=\"https://rbyrneart.com/\">Ricky Byrne</a>. I loved their use of color and hand-produced feel. Attention for color, layout, tension.\n          So I started experimenting with maze generation algorithms in Rust, with Nannou.<p>\n          In the process, I decided to make it a web app, for valentine, so everyone can create their own maze. And ported the Rust code to TypeScript in a tiny web app. </p><a href=\"https://www.youtube.com/watch?v=HyK_Q5rrcr4\">The Coding Train has a great tutorial set on maze generation</a>. I used the same common recursive backtracking algorithm, because the aesthetics are what I was looking for. </p></dd><dd><p>\n          The names you provide are used to generate <a href=\"https://en.wikipedia.org/wiki/Random_seed \">unique randomnes</a>. \n          This is used in a <a href=\"https://en.wikipedia.org/wiki/Maze_generation_algorithm\">maze generation algorithm</a> to create a maze. \n          I deliberately chose to animate the maze generation, to show the process. It is a slow process, but I think it is interesting to watch it carve out your maze. </p></dd><dd><p>\n          Saving, copying and sharing the maze is disabled until the maze is generated. Once the maze is generated, you can save, copy and share the maze. \n          Sharing, copying won't work on all browsers and is affected by some browser plugins or settings. The best result is on Chrome on Android. \n          </p></dd><dd><p>, the only parameter you can provide is the names. The maze is generated based on these names, and the algorithm is fixed. This is by design. Only the two names determine the artwork. \n            However, you can change the source code, see below.\n          </p></dd><dd><p>\n            The code is available on <a href=\"https://github.com/berkes/art/tree/main/lost-in-love\">GitHub</a>. Feel free to fork, change and improve it, or just have fun with it. </p></dd><dd><ul><li><a href=\"https://nannou.cc/\">Nannou</a> for the original maze generation</li><li><a href=\"https://vitejs.dev/\">Vite</a> for the build and web stuff</li></ul></dd>","contentLength":1890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000764"},{"title":"How to Join Strings in Python","url":"https://realpython.com/python-join-string/","date":1739196000,"author":"Real Python","guid":181,"unread":true,"content":"<p>Python‚Äôs built-in string method  lets you combine string elements from an iterable into a single string, using a separator that you specify. You call  on the separator, passing the iterable of strings to join.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li>You use  in Python to  with a .</li><li>A  is the piece of text you want <strong>inserted between each substring</strong>.</li><li>To join list elements, you <strong>call  on a separator string</strong>, passing the list as the argument.</li><li><strong>inserts the separator between each list element</strong> to form a single string.</li><li>The  method  that is the concatenation of the elements in the iterable, separated by the specified string.</li><li>For smaller string concatenation tasks, you can use the <strong>concatenation operator ()</strong> or  instead of .</li></ul><p>Python‚Äôs built-in <a href=\"https://docs.python.org/3/library/stdtypes.html#str.join\"></a> method gives you a quick and reliable way to combine multiple strings into a single string. Whether you need to format output or assemble data for storage,  provides a clean and efficient approach for joining strings from an <a href=\"https://realpython.com/python-iterators-iterables/\">iterable</a>.</p><p>In the upcoming sections, you‚Äôll learn the basic usage of  to concatenate strings effectively. You‚Äôll then apply that knowledge to real-world scenarios, from building CSV files to constructing custom log outputs. You‚Äôll also discover some surprising pitfalls and learn how to avoid them.</p><div><p> Test your knowledge with our interactive ‚ÄúHow to Join Strings in Python‚Äù quiz. You‚Äôll receive a score upon completion to help you track your learning progress:</p><div><div><a href=\"https://realpython.com/quizzes/python-join-string/\"></a><p>Test your understanding of Python's .join() string method for combining strings, handling edge cases, and optimizing performance.</p></div></div></div><h2>How to Join Strings in Python Using </h2><p>To use the string method , you call  on a separator string and pass an <a href=\"https://realpython.com/ref/glossary/iterable/\">iterable</a> of other strings as the argument. The method returns a single string, where it has inserted the separator string between each element of the iterable:</p><p>In this example, you joined a list of words into one sentence, separated by spaces.</p><p>At first glance, this usage might look a little backward. In many other <a href=\"https://realpython.com/python-strings/\">string operations</a>, you call the method on the main string that you want to manipulate. However, with , you call the method on the separator string, then pass the iterable of strings that you want to combine:</p><p>This example achieves the same result as the earlier one but splits the process into two steps. Defining  separately makes the code more readable and avoids the potentially odd-looking syntax of calling  directly on a short string literal.</p><div role=\"alert\"><p> Remember that  is a  method, which means that you‚Äôll need to call it on a  string object. Keeping that in mind may help you remember why you need to call it on the separator string.</p></div><p>You rarely see code that‚Äôs written in multiple steps where you assign the separator string to a variable, like you did in the example above.</p><p>In typical usage, you call  directly on the separator string, all in one line. This approach is more concise and highlights that any valid string can be your separator, whether it‚Äôs whitespace, a dash, or a multicharacter substring.</p><h3>Join With an Empty String</h3><p>What if you don‚Äôt want any separator at all, but just want to <a href=\"https://realpython.com/python-string-concatenation/\">concatenate</a> the items? One valid approach is to use an empty string () as the separator:</p><p>This code snippet concatenates the letters in the list, forming a single string . Using an empty string as the separator is a great way to assemble strings without a delimiter between them.</p><p>Since  can take any iterable of strings‚Äînot just lists‚Äîyou can even pass a string as an argument. Because strings are iterable, Python iterates over each character in that string, considering each character as a separate element:</p><p>By calling  on  and passing the string , you effectively place a comma between every single character in . This might not always be what you intend, but it‚Äôs a neat trick to keep in mind if you ever need to treat each character as a separate element.</p>","contentLength":3847,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: HTML visualization of a PDF file's internal structure","url":"https://github.com/desgeeko/pdfsyntax/blob/main/docs/browse.md","date":1739195573,"author":"desgeeko","guid":312,"unread":true,"content":"<p>Hi,\nI've just finished a rebuild of this function and added a lot of new features: info, page index, minimap, inverted index,... \nI think it may be useful for inspection, debugging or just as a learning resource showcasing the PDF file format.\nThis is a pet project and I would be happy to receive some feedback!\nRegards</p>","contentLength":320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000303"},{"title":"Using Gemini 2.0 Pro Locally","url":"https://www.kdnuggets.com/using-gemini-2-pro-locally","date":1739192434,"author":"Abid Ali Awan","guid":581,"unread":true,"content":"<article>Learn the easiest way to use a state-of-the-art Google experimental model locally.</article>","contentLength":82,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/awan_gemini_20_pro_locally_7.png","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Infinite horizontal arrays of text editors","url":"https://zeminary.com/arrays/app.html","date":1739150705,"author":"tsydenzhap","guid":311,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42995991"},{"title":"TypeScript: extracting parts of composite types via infer","url":"https://2ality.com/2025/02/typescript-infer-operator.html","date":1739145600,"author":"Dr. Axel Rauschmayer","guid":351,"unread":true,"content":"<p>In this blog post, we explore how we can extract parts of composite types via the  operator.</p><p>It helps if you are loosely familiar with conditional types. You can check out section <a href=\"https://exploringjs.com/tackling-ts/ch_computing-with-types-overview.html#conditional-types\">‚ÄúConditional types‚Äù</a> in ‚ÄúTackling TypeScript‚Äù&nbsp;to read up on them.</p>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Searchable library of free audiobooks","url":"https://booksearch.party/","date":1739137953,"author":"libridev","guid":310,"unread":true,"content":"<p>\n                If you're seeking free audiobooks and an efficient book finder, booksearch.party is an excellent resource. This platform aggregates a vast collection of audiobooks for free from various sources, including LibriVox, Project Gutenberg, and Lit2Go, into a user-friendly, searchable database.</p><p>LibriVox offers a wide range of public domain audiobooks, read by volunteers worldwide. Their catalog includes thousands of titles across various genres, all available for free.</p><p>Project Gutenberg provides over 60,000 free eBooks, many of which have been converted into audiobooks. This extensive collection includes classic literature and historical texts, all accessible without cost.</p><p>Lit2Go is a free online collection of public domain fiction, poetry, and nonfiction audiobooks. It offers a user-friendly interface where users can browse texts and listen to individual chapters.</p><p>By compiling these resources, booksearch.party serves as a comprehensive book finder, allowing users to easily search and access a wide array of free audiobooks. Whether you're interested in classic literature, historical documents, or educational materials, this platform simplifies the process of discovering and enjoying audiobooks at no cost.</p>","contentLength":1230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42994440"},{"title":"This Was CS50: Crying Poor, Yale To Stop Offering Harvard's Famed CS50 Course","url":"https://developers.slashdot.org/story/25/02/09/1922236/this-was-cs50-crying-poor-yale-to-stop-offering-harvards-famed-cs50-course?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739133240,"author":"EditorDavid","guid":366,"unread":true,"content":"Slashdot has been covering Harvard's legendary introductory programming course \"CS50\" since it began setting attendance records in 2014. \n\nBut now long-time Slashdot reader theodp brings some news about the course's fate over at Yale. From Yale's student newspaper:\n\n\nAfter a decade of partnership with Harvard, Yale's CS50 course will no longer be offered starting in fall 2025.... One of Yale's largest computer science courses, jointly taught with Harvard University, was canceled during a monthly faculty meeting after facing budgetary challenges. [Yale's endowment is $40+ billion]... Since Yale started offering the course in 2015, CS50 has consistently seen enrollment numbers in the hundreds and was often the department's largest class.... According to [Yale instructor Ozan] Erat, the original [anonymous] donation that made CS50 possible ended in June 2024, and the cost of employing so many undergraduate learning assistants for the course had become unsustainable. \n\ntheodp reminds us that CS50 and its progeny \"will continue to live on in all their glory in-person and online at Harvard and edX.\"","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: My first side project, streamlined book clubs on Slack","url":"https://booktalk.club/","date":1739107690,"author":"Papamanolis","guid":309,"unread":true,"content":"<p>Get personalized suggestions and find the</p><p>perfect reads for your team</p>","contentLength":68,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42990540"},{"title":"C++ on Steroids: Bjarne Stroustrup Presents Guideline-Enforcing 'Profiles' For Resource and Type Safety","url":"https://developers.slashdot.org/story/25/02/09/0636247/c-on-steroids-bjarne-stroustrup-presents-guideline-enforcing-profiles-for-resource-and-type-safety?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739102640,"author":"EditorDavid","guid":364,"unread":true,"content":"\"It is now 45+ years since C++ was first conceived,\" writes 74-year-old C++ creator Bjarne Stroustrup in an article this week for Communications of the ACM. But he complains that many developers \"use C++ as if it was still the previous millennium,\" in an article titled 21st Century C++ that promises \"the key concepts on which performant, type safe, and flexible C++ software can be built: resource management, life-time management, error-handling, modularity, and generic programming... \n\n\"At the end, I present ways to ensure that code is contemporary, rather than relying on outdated, unsafe, and hard-to-maintain techniques: guidelines and profiles.\"\n\nTo help developers focus on effective use of contemporary C++ and avoid outdated \"dark corners\" of the language, sets of guidelines have been developed. Here I focus on the C++ Core guidelines that I consider the most ambitious... My principal aim is a type-safe and resource-safe use of ISO standard C++. That is: \n\n- Every object is exclusively used according to its definition\n- No resource is leaked \nThis encompasses what people refer to as memory safety and much more. It is not a new goal for C++. Obviously, it cannot be achieved for every use of C++, but by now we have years of experience showing that it can be done for modern code, though so far enforcement has been incomplete... When thinking about C++, it is important to remember that C++ is not just a language but part of an ecosystem consisting of implementations, libraries, tools, teaching, and more. \nWG21 (and others) are working on \"profiles\" to enforce guidelines (though they're \"not yet available, except for experimental and partial versions\"). But Stroustrup writes that the C++ Core Guidelines \"use a strategy known as subset-of-superset.\"\n\n First: extend the language with a few library abstractions: use parts of the standard library and add a tiny library to make use of the guidelines convenient and efficient (the Guidelines Support Library, GSL).\n Next: subset: ban the use of low-level, inefficient, and error-prone features. \nWhat we get is \"C++ on steroids\": Something simple, safe, flexible, and fast; rather than an impoverished subset or something relying on massive run-time checking. Nor do we create a language with novel and/or incompatible features. The result is 100% ISO standard C++. Messy, dangerous, low-level features can still be enabled and used when needed. \nStroustrup writes that the C++ Core Guidelines focus on rules \"we hope that everyone eventually could benefit from.\"\n\nNo uninitialized variables\nNo range or nullptr violations\nNo resource leaks\nNo dangling pointers\nNo type violations\nNo invalidation\n\n\nBjarne Stroustrup answered questions from Slashdot readers in 2014...\n","contentLength":2744,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"#493: Quarto: Open-source technical publishing","url":"https://talkpython.fm/episodes/show/493/quarto-open-source-technical-publishing","date":1739088000,"author":"","guid":434,"unread":true,"content":"<article></article>","contentLength":0,"flags":null,"enclosureUrl":"https://talkpython.fm/episodes/download/493/quarto-open-source-technical-publishing.mp3","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Daily-notes.nvim ‚Äì fuzzy time journal and planning plugin","url":"https://github.com/fdavies93/daily-notes.nvim","date":1739083330,"author":"fdavies93","guid":308,"unread":true,"content":"<p>I wrote an nvim plugin that does fuzzy time parsing on plain english dates to help you create + organise periodic notes. I use it daily at work and home. Hope it's helpful to others. :)</p><p>note: not using NLP, LLMs or 'true' fuzzy parsing as per academic literature; just normal recursive descent parsing</p>","contentLength":300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42988946"},{"title":"TypeDoc: testing code examples in doc comments","url":"https://2ality.com/2025/02/testing-typedoc-examples.html","date":1739059200,"author":"Dr. Axel Rauschmayer","guid":350,"unread":true,"content":"<p>TypeDoc now lets us refer to parts of other files via . In this blog post, I explain how that works and why it‚Äôs useful.</p>","contentLength":122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Chez Scheme txtar port from Go","url":"https://git.sr.ht/~egtann/txtar/","date":1739039079,"author":"hellcow","guid":307,"unread":true,"content":"<p>txtar enables you to work with a simple text archive format compatible with\n<a href=\"https://golang.org/x/tools/txtar\" rel=\"nofollow noopener\">Go's txtar</a>. It concatenates files together\nand allows for a top-level comment.</p><pre><code>Math implementation\n-- math.h --\n#ifndef MATH_H\n#define MATH_H\n\nint add(int a, int b);\n\n#endif\n-- math.c --\n#include \"math.h\"\n\nint add(int a, int b) {\n    return a + b;\n}\n</code></pre><p>This format is easy for humans to read and write by hand and is perfect for\ntest data.</p><div><pre>\n$makeinstall\n\n\n$</pre></div><p>If you want to remove the library from your system, simply run .</p><div><pre></pre></div><p>You can obtain these libraries and many more via Thunderchez:</p><div><pre>$gitclonehttps://github.com/ovenpasta/thunderchez\n\n\n$/path/to/thunderchez\n</pre></div><p>All the exports of this library are documented with type expectations. I\nencourage you to scan the implementation.</p><p>A few common example usecases are presented below for convenience:</p><p>To use this library, simply import :</p><h4><a href=\"https://git.sr.ht/~egtann/txtar/#construct-an-archive-from-a-list-of-filenames\" rel=\"nofollow noopener\">#</a>Construct an archive from a list of filenames</h4><div><pre></pre></div><h4><a href=\"https://git.sr.ht/~egtann/txtar/#write-text-to-an-archive-file\" rel=\"nofollow noopener\">#</a>Write text to an archive file</h4><div><pre></pre></div><h4><a href=\"https://git.sr.ht/~egtann/txtar/#retrieve-a-file-from-an-archive\" rel=\"nofollow noopener\">#</a>Retrieve a file from an archive</h4><div><pre></pre></div><p>Copyright (C) 2025 Evan Tann, ParaVoce LLC</p><p>This program is free software: you can redistribute it and/or modify it under\nthe terms of the GNU Affero General Public License as published by the Free\nSoftware Foundation, either version 3 of the License, or (at your option) any\nlater version.</p><p>This program is distributed in the hope that it will be useful, but WITHOUT ANY\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\nPARTICULAR PURPOSE. See the GNU Affero General Public License for more details.</p>","contentLength":1478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42984962"},{"title":"Show HN: FlashSpace ‚Äì fast, open-source, macOS Spaces replacement","url":"https://github.com/wojciech-kulik/FlashSpace","date":1739035163,"author":"wojciech-kulik","guid":306,"unread":true,"content":"<p>I've recently launched a new open-source project aimed at enhancing the experience of switching between Spaces/workspaces on macOS. The built-in Spaces feature is often slow and unfriendly. This project is designed to boost your productivity :). Enjoy!</p>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42984420"},{"title":"Rust vs. Go (Golang): Performance 2025","url":"https://www.youtube.com/watch?v=CsKNTwS9kic","date":1739014569,"author":"Anton Putra","guid":535,"unread":true,"content":"<article>Go vs Rust Performance Comparison. Go (1.23.6) vs Rust (1.84.1).\n\nüî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)\n\nüçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&amp;si=p-UOaVM_6_SFx52H\n\nüëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience &amp; Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)\n‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra\n‚ñ∫  Twitter/X: https://twitter.com/antonvputra\n‚ñ∫  GitHub: https://github.com/antonputra\n‚ñ∫  Email: me@antonputra.com\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/243/lessons/243\n\n#rust #golang #devops</article>","contentLength":968,"flags":null,"enclosureUrl":"https://www.youtube.com/v/CsKNTwS9kic?version=3","enclosureMime":"","commentsUrl":null},{"title":"Google's 7-Year Slog To Improve Chrome Extensions Still Hasn't Satisfied Developers","url":"https://developers.slashdot.org/story/25/02/07/2246202/googles-7-year-slog-to-improve-chrome-extensions-still-hasnt-satisfied-developers?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738977900,"author":"BeauHD","guid":363,"unread":true,"content":"The Register's Thomas Claburn reports: Google's overhaul of Chrome's extension architecture continues to pose problems for developers of ad blockers, content filters, and privacy tools. [...] While Google's desire to improve the security, privacy, and performance of the Chrome extension platform is reasonable, its approach -- which focuses on code and permissions more than human oversight -- remains a work-in-progress that has left extension developers frustrated.\n \nAlexei Miagkov, senior staff technology at the Electronic Frontier Foundation, who oversees the organization's Privacy Badger extension, told The Register, \"Making extensions under MV3 is much harder than making extensions under MV2. That's just a fact. They made things harder to build and more confusing.\" Miagkov said with Privacy Badger the problem has been the slowness with which Google addresses gaps in the MV3 platform. \"It feels like MV3 is here and the web extensions team at Google is in no rush to fix the frayed ends, to fix what's missing or what's broken still.\" According to Google's documentation, \"There are currently no open issues considered a critical platform gap,\" and various issues have been addressed through the addition of new API capabilities.\n \nMiagkov described an unresolved problem that means Privacy Badger is unable to strip Google tracking redirects on Google sites. \"We can't do it the correct way because when Google engineers design the [chrome.declarativeNetRequest API], they fail to think of this scenario,\" he said. \"We can do a redirect to get rid of the tracking, but it ends up being a broken redirect for a lot of URLs. Basically, if the URL has any kind of query string parameters -- the question mark and anything beyond that -- we will break the link.\" Miagkov said a Chrome developer relations engineer had helped identify a workaround, but it's not great. Miagkov thinks these problems are of Google's own making -- the company changed the rules and has been slow to write the new ones. \"It was completely predictable because they moved the ability to fix things from extensions to themselves,\" he said. \"And now they need to fix things and they're not doing it.\"","contentLength":2187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TypeScript: the satisfies operator","url":"https://2ality.com/2025/02/satisfies-operator.html","date":1738972800,"author":"Dr. Axel Rauschmayer","guid":349,"unread":true,"content":"<p>TypeScript‚Äôs  operator lets us check the type of a value (mostly) without influencing it. In this blog post, we examine how exactly it works and where it‚Äôs useful.</p>","contentLength":167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeking Purity","url":"http://lucumr.pocoo.org/2025/2/8/seeking-purity","date":1738972800,"author":"Armin Ronacher","guid":305,"unread":true,"content":"<p>The concept of purity ‚Äî historically a guiding principle in social and\nmoral contexts ‚Äî is also found in passionate, technical discussions.  By\nthat I mean that purity in technology translates into adherence to a set\nof strict principles, whether it be functional programming, test-driven\ndevelopment, serverless architectures, or, in the case of Rust, memory\nsafety.</p><div><p>Rust positions itself as a champion of memory safety, treating it as a\nnon-negotiable foundation of good software engineering.  I love Rust: it's\nprobably my favorite language.  It probably won't surprise you that I have\nno problem with it upholding memory safety as a defining feature.</p><p>Rust aims to achieve the goal of memory safety via safe abstractions, a\ncompile time borrow checker and a type system that is in service of those\nsafe abstractions.  It comes as no surprise that the Rust community is\nalso pretty active in codifying a new way to <a href=\"https://www.ralfj.de/blog/2020/12/14/provenance.html\">reason about pointers</a>.  In many ways,\nRust pioneered completely new technical approaches and it it widely\nheralded as an amazing innovation.</p><p>However, as with many movements rooted in purity, what starts as a\ntechnical pursuit can evolve into something more ideological.  Similar to\nhow moral purity in political and cultural discourse can become charged,\nso does the discourse around Rust, which has been dominated by the pursuit\nof memory safety.  Particularly within the core Rust community itself,\ndiscussion has moved beyond technical merits into something akin to\nideological warfare.  The fundamental question of ‚ÄúIs this code memory\nsafe?‚Äù, has shifted to ‚ÄúWas it made memory safe in the  way?‚Äù.\nThis distinction matters because it introduces a purity test that values\nmethodology over outcomes.  Safe C code, for example, is often dismissed\nas impossible, not necessarily because it  impossible, but because it\nlacks the strict guarantees that Rust's borrow checker enforces.\nSimilarly, using Rust‚Äôs  blocks is increasingly frowned upon,\ndespite their intended purpose of enabling low-level optimizations when\nnecessary.</p><p>This ideological rigidity creates significant friction when Rust\ninterfaces with other ecosystems (or gets introduced there), particularly\nthose that do not share its uncompromising stance.  For instance, the role\nof Rust in the Linux kernel has been a hot topic.  The Linux kernel\noperates under an entirely different set of priorities.  While memory\nsafety is important there is insufficient support for adopting Rust in\ngeneral.  The kernel is an old project and it aims to remain maintainable\nfor a long time into the future.  For it to even consider a rather young\nprogramming language should be seen as tremendous success for Rust and\nalso for how open Linus is to the idea.</p><p>Yet that introduction is balanced against performance, maintainability,\nand decades of accumulated engineering expertise.  Many of the kernel\ndevelopers, who have found their own strategies to write safe C for\ndecades, are not accepting the strongly implied premise that their work is\ninherently flawed simply because it does not adhere to Rust's strict\npurity rules.</p><p>Tensions rose when a kernel developer advocating for Rust's inclusion took\nto social media to push for changes in the Linux kernel development\nprocess.  The public shaming tactic failed, <a href=\"https://lkml.org/lkml/2025/2/6/1292\">leading the developer to\nconclude</a>:</p><blockquote>\n‚ÄúIf shaming on social media does not work, then tell me what does,\nbecause I'm out of ideas.‚Äù</blockquote><p>It's not just the kernel where Rust's memory safety runs up against the\ncomplexities of the real world.  Very similar feelings creep up in the\ngaming industry where people love to do wild stuff with pointers.  You do\nnot need large disagreements to see the purist approach create some\nfriction.  A <a href=\"http://lucumr.pocoo.org/2025/2/4/fat-rand/\">recent post of mine</a> for instance\ntriggered some discussions about the trade-offs between more dependencies,\nand moving unsafe to centralized crates.</p><p>I really appreciate that Rust code does not crash as much.  That part of\nRust, among many others, makes it very enjoyable to work with.  Yet I am\nentirely unconvinced that memory safety should trump everything, at least\nat this point in time.</p><p>What people want in the Rust in Linux situation is for the project leader\nto come in to declare support for Rust's call for memory safety above all.\nTo make the detractors go away.</p></div><div><h2>Python's Migration Lesson</h2><p>Hearing this call and discussion brings back memories.  I have lived\nthrough a purity driven shift in a community before.  The move from Python\n2 to Python 3 started out very much the same way.  There was an almost\nreligious movement in the community to move to Python 3 in a ratcheting\nmotion.  The idea that you <a href=\"http://lucumr.pocoo.org/2013/5/21/porting-to-python-3-redux/\">could maintain code bases that support both 2\nand 3</a> were initially very\nloudly rejected.  I took a lot of flak at the time (and for years after)\nfor advocating for a more pragmatic migration which burned me out a lot.\nThat feedback came both in person and online and it largely pushed me away\nfrom Python for a while.  Not getting behind the Python 3 train was seen\nas sabotaging the entire project.  However, a decade later, I feel\nsomewhat vindicated that it was worth being pragmatic about that\nmigration.</p><p>At the root of that discourse was a idealistic view of how Unicode could\nwork in the language and that you can move an entire ecosystem at once.\nBoth those things greatly clashed with the lived realities in many\nprojects and companies.</p><p>I am a happy user of Python 3 today.  This migration has also taught me\nthe important lesson not be too stuck on a particular idea.  It would have\nbeen very easy to pick one of the two sides of that debate.  Be stuck on\nPython 2 (at the risk of forking), or go all in on Python 3 no questions\nasked.  It was the path in between that was quite painful to advocate for,\nbut it was ultimately the right path.  I wrote about <a href=\"http://lucumr.pocoo.org/2016/11/5/be-careful-about-what-you-dislike/\">my lessons of that\nmigration a in 2016</a> and\nI think most of this still rings true.  That was motivated by even years\nlater people still reaching out to me who did not move to Python 3, hoping\nfor me to embrace their path.  Yet Python 3 has changed!  Python 3 is a\nmuch better language than it was when it first released.  It is a great\nlanguage because it's used by people solving real, messy problems and\nbecause it over time found answers for what to do, if you need to have\nboth Python 2 and 3 code in the wild.  While the world of Python 2 is\nlargely gone, we are still in a world where Unicode and bytes mix in\ncertain contexts.</p></div><div><p>Fully committing to a single worldview can be easier because you stop\nquestioning everything ‚Äî you can just go with the flow.  Yet truths often\nreside on both sides.  Allowing yourself to walk the careful middle path\nenables you to learn from multiple perspectives.  You will face doubts and\nopen yourself up to vulnerability and uncertainty.  The payoff, however,\nis the ability to question deeply held beliefs and push into the unknown\nterritory where new things can be found.  You can arrive at a solution\nthat isn't a complete rejection of any side.  There is genuine value in\nwhat Rust offers‚Äîjust as there was real value in what Python 3 set out to\naccomplish.  But the Python 3 of today isn't the Python 3 of those early,\nideological debates; it was shaped by a messy, slow, often contentious,\nyet ultimately productive transition process.</p><p>I am absolutely sure that in 30 years from now we are going to primarily\nprogram in memory safe languages (or the machines will do it for us) in\nenvironments where C and C++ prevail.  That glimpse of a future I can\nvisualize clearly.  The path to there however?  That's a different story\naltogether.  It will be hard, it will be impure.  Maybe the solution will\nnot even involve Rust at all ‚Äî who knows.</p><p>We also have to accept that not everyone is ready for change at the same\npace. Forcing adoption when people aren't prepared only causes the\npendulum to swing back hard.  It's tempting to look for a single authority\nto declare ‚Äúthe one true way,‚Äù but that won't smooth out the inevitable\ncomplications.  Indeed, those messy, incremental challenges are part of how\nreal progress happens.  In the long run, these hard-won refinements tend\nto produce solutions that benefit all sides‚Äîif we‚Äôre patient enough to let\nthem take root.  The painful and messy transition is here to stay, and\nthat's exactly why, in the end, it works.</p></div>","contentLength":8311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Governing the ML lifecycle at scale, Part 4: Scaling MLOps with security and governance controls","url":"https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-4-scaling-mlops-with-security-and-governance-controls/","date":1738959913,"author":"Jia (Vivian) Li","guid":221,"unread":true,"content":"<p>Data science teams often face challenges when transitioning models from the development environment to production. These include difficulties integrating data science team‚Äôs models into the IT team‚Äôs production environment, the need to retrofit data science code to meet enterprise security and governance standards, gaining access to production grade data, and maintaining repeatability and reproducibility in machine learning (ML) pipelines, which can be difficult without a proper platform infrastructure and standardized templates.</p><p>This post, part of the ‚ÄúGoverning the ML lifecycle at scale‚Äù series (<a href=\"https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-1-a-framework-for-architecting-ml-workloads-using-amazon-sagemaker/\" target=\"_blank\" rel=\"noopener\">Part 1</a>, <a href=\"https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-2-multi-account-foundations/\" target=\"_blank\" rel=\"noopener\">Part 2</a>, <a href=\"https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-3-setting-up-data-governance-at-scale/\" target=\"_blank\" rel=\"noopener\">Part 3</a>), explains how to set up and govern a multi-account ML platform that addresses these challenges. The platform provides self-service provisioning of secure environments for ML teams, accelerated model development with predefined templates, a centralized model registry for collaboration and reuse, and standardized model approval and deployment processes.</p><p>An enterprise might have the following roles involved in the ML lifecycles. The functions for each role can vary from company to company. In this post, we assign the functions in terms of the ML lifecycle to each role as follows:</p><ul><li> ‚Äì Provision accounts for ML development teams, govern access to the accounts and resources, and promote standardized model development and approval process to eliminate repeated engineering effort. Usually, there is one lead data scientist for a data science group in a business unit, such as marketing.</li><li> ‚Äì Perform data analysis, model development, model evaluation, and registering the models in a model registry.</li><li> ‚Äì Develop model deployment pipelines and control the model deployment processes.</li><li> ‚Äì Review the model‚Äôs performance including documentation, accuracy, bias and access, and provide final approval for models to be deployed.</li><li> ‚Äì Define a standardized process for creating development accounts that conform to the company‚Äôs security, monitoring, and governance standards; create templates for model development; and manage the infrastructure and mechanisms for sharing model artifacts.</li></ul><p>This ML platform provides several key benefits. First, it enables every step in the ML lifecycle to conform to the organization‚Äôs security, monitoring, and governance standards, reducing overall risk. Second, the platform gives data science teams the autonomy to create accounts, provision ML resources and access ML resources as needed, reducing resource constraints that often hinder their work.</p><p>Additionally, the platform automates many of the repetitive manual steps in the ML lifecycle, allowing data scientists to focus their time and efforts on building ML models and discovering insights from the data rather than managing infrastructure. The centralized model registry also promotes collaboration across teams, enables centralized model governance, increasing visibility into models developed throughout the organization and reducing duplicated work.</p><p>Finally, the platform standardizes the process for business stakeholders to review and consume models, smoothing the collaboration between the data science and business teams. This makes sure models can be quickly tested, approved, and deployed to production to deliver value to the organization.</p><p>Overall, this holistic approach to governing the ML lifecycle at scale provides significant benefits in terms of security, agility, efficiency, and cross-functional alignment.</p><p>In the next section, we provide an overview of the multi-account ML platform and how the different roles collaborate to scale MLOps.</p><p>The following architecture diagram illustrates the solutions for a multi-account ML platform and how different personas collaborate within this platform.</p><p>There are five accounts illustrated in the diagram:</p><ul><li><strong>ML Shared Services Account</strong> ‚Äì This is the central hub of the platform. This account manages templates for setting up new ML Dev Accounts, as well as <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-whatis.html\" target=\"_blank\" rel=\"noopener\">SageMaker Projects</a> templates for model development and deployment, in <a href=\"https://aws.amazon.com/servicecatalog\" target=\"_blank\" rel=\"noopener\">AWS Service Catalog</a>. It also hosts a model registry to store ML models developed by data science teams, and provides a single location to approve models for deployment.</li><li> ‚Äì This is where data scientists perform their work. In this account, data scientists can create new <a href=\"https://aws.amazon.com/sagemaker/notebooks/\" target=\"_blank\" rel=\"noopener\">SageMaker notebooks</a> based on the needs, connect to data sources such as <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3) buckets, analyze data, build models and create model artifacts (for example, a container image), and more. The SageMaker projects, provisioned using the templates in the ML Shared Services Account, can speed up the model development process because it has steps (such as connecting to an S3 bucket) configured. The diagram shows one ML Dev Account, but there can be multiple ML Dev Accounts in an organization.</li><li> ‚Äì This is the test environment for new ML models, where stakeholders can review and approve models before deployment to production.</li><li> ‚Äì This is the production account for new ML models. After the stakeholders approve the models in the ML Test Account, the models are automatically deployed to this production account.</li><li> ‚Äì This account hosts data governance services for data lake, central feature store, and fine-grained data access.</li></ul><p>Key activities and actions are numbered in the preceding diagram. Some of these activities are performed by various personas, whereas others are automatically triggered by AWS services.</p><ol><li>ML engineers create the pipelines in Github repositories, and the platform engineer converts them into two different Service Catalog portfolios: ML Admin Portfolio and SageMaker Project Portfolio. The ML Admin Portfolio will be used by the lead data scientist to create AWS resources (for example, SageMaker domains). The SageMaker Project Portfolio has SageMaker projects that data scientists and ML engineers can use to accelerate model training and deployment.</li><li>The platform engineer shares the two Service Catalog portfolios with workload accounts in the organization.</li><li>Data engineer prepares and governs datasets using services such as Amazon S3, AWS Lake Formation, and Amazon DataZone for ML.</li><li>The lead data scientist uses the ML Admin Portfolio to set up SageMaker domains and the SageMaker Project Portfolio to set up SageMaker projects for their teams.</li><li>Data scientists subscribe to datasets, and use SageMaker notebooks to analyze data and develop models.</li><li>Data scientists use the SageMaker projects to build model training pipelines. These SageMaker projects automatically register the models in the model registry.</li><li>The lead data scientist approves the model locally in the ML Dev Account.</li><li>This step consists of the following sub-steps: \n  <ol type=\"a\"><li>&nbsp;After the data scientists approve the model, it triggers an event bus in Amazon EventBridge that ships the event to the ML Shared Services Account.</li><li>The event in EventBridge triggers the AWS Lambda function that copies model artifacts (managed by SageMaker, or Docker images) from the ML Dev Account into the ML Shared Services Account, creates a model package in the ML Shared Services Account, and registers the new model in the model registry in the ML Shared Services account.</li></ol></li><li>ML engineers review and approve the new model in the ML Shared Services account for testing and deployment. This action triggers a pipeline that was set up using a SageMaker project.</li><li>The approved models are first deployed to the ML Test Account. Integration tests will be run and endpoint validated before being approved for production deployment.</li><li>After testing, the governance officer approves the new model in the CodePipeline.</li><li>After the model is approved, the pipeline will continue to deploy the new model into the ML Prod Account, and creates a SageMaker endpoint.</li></ol><p>The following sections provide details on the key components of this diagram, how to set them up, and sample code.</p><h3>Set up the ML Shared Services Account</h3><p>The ML Shared Services Account helps the organization standardize management of artifacts and resources across data science teams. This standardization also helps enforce controls across resources consumed by data science teams.</p><p>The ML Shared Services Account has the following features:</p><p><strong>Service Catalog portfolios</strong> ‚Äì This includes the following portfolios:</p><ul><li> ‚Äì This is intended to be used by the project admins of the workload accounts. It is used to create AWS resources for their teams. These resources can include SageMaker domains, Amazon Redshift clusters, and more.</li><li><strong>SageMaker Projects Portfolio</strong> ‚Äì This portfolio contains the SageMaker products to be used by the ML teams to accelerate their ML models‚Äô development while complying with the organization‚Äôs best practices.</li><li> ‚Äì This is the centralized place for ML models developed and approved by different teams. For details on setting this up, refer to Part 2 of this series.</li></ul><p>The following diagram illustrates this architecture.</p><p>As the first step, the cloud admin sets up the ML Shared Services Account by using one of the blueprints for customizations in <a href=\"https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-1-a-framework-for-architecting-ml-workloads-using-amazon-sagemaker/\" target=\"_blank\" rel=\"noopener\">AWS Control Tower</a> account vending, as described in Part 1.</p><p>In the following sections, we walk through how to set up the ML Admin Portfolio. The same steps can be used to set up the SageMaker Projects Portfolio.</p><h3>Bootstrap the infrastructure for two portfolios</h3><p>After the ML Shared Services Account has been set up, the ML platform admin can bootstrap the infrastructure for the ML Admin Portfolio using <a href=\"https://github.com/aws-samples/data-and-ml-governance-workshop\" target=\"_blank\" rel=\"noopener\">sample code in the GitHub repository</a>. The code contains <a href=\"http://aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a> templates that can be later deployed to create the SageMaker Projects Portfolio.</p><p>Complete the following steps:</p><ol><li>Clone the GitHub repo to a local directory:<div><pre><code>git clone https://github.com/aws-samples/data-and-ml-governance-workshop.git</code></pre></div></li><li>Change the directory to the portfolio directory:<div><pre><code>cd data-and-ml-governance-workshop/module-3/ml-admin-portfolio</code></pre></div></li><li>Install dependencies in a separate Python environment using your preferred Python packages manager:<div><pre><code>python3 -m venv env\nsource env/bin/activate pip \ninstall -r requirements.txt</code></pre></div></li><li>Bootstrap your deployment target account using the following command:<div><pre><code>cdk bootstrap aws://&lt;target account id&gt;/&lt;target region&gt; --profile &lt;target account profile&gt;</code></pre></div><p>If you already have a role and AWS Region from the account set up, you can use the following command instead:</p></li><li>Lastly, deploy the stack:<div><pre><code>cdk deploy --all --require-approval never</code></pre></div></li></ol><p>When it‚Äôs ready, you can see the MLAdminServicesCatalogPipeline pipeline in AWS CloudFormation.</p><p>Navigate to AWS CodeStar Connections of the Service Catalog page, you can see there‚Äôs a connection named ‚Äúcodeconnection-service-catalog‚Äù. If you click the connection, you will notice that we need to connect it to GitHub to allow you to integrate it with your pipelines and start pushing code. Click the ‚ÄòUpdate pending connection‚Äô to integrate with your GitHub account.</p><p>Once that is done, you need to create empty GitHub repositories to start pushing code to. For example, you can create a repository called ‚Äúml-admin-portfolio-repo‚Äù. Every project you deploy will need a repository created in GitHub beforehand.</p><h3>Trigger CodePipeline to deploy the ML Admin Portfolio</h3><p>Complete the following steps to trigger the pipeline to deploy the ML Admin Portfolio. We recommend creating a separate folder for the different repositories that will be created in the platform.</p><ol><li>Get out of the cloned repository and create a parallel folder called :<div><pre><code>cd ../../.. # (as many .. as directories you have moved in)\nmkdir platform-repositories</code></pre></div></li><li>Clone and fill the empty created repository:<div><pre><code>cd platform-repositories\ngit clone https://github.com/example-org/ml-admin-service-catalog-repo.git\ncd ml-admin-service-catalog-repo\ncp -aR ../../ml-platform-shared-services/module-3/ml-admin-portfolio/. .</code></pre></div></li><li>Push the code to the Github repository to create the Service Catalog portfolio:<div><pre><code>git add .\ngit commit -m \"Initial commit\"\ngit push -u origin main</code></pre></div></li></ol><p>After it is pushed, the Github repository we created earlier is no longer empty. The new code push triggers the pipeline named cdk-service-catalog-pipeline to build and deploy artifacts to Service Catalog.</p><p>It takes about 10 minutes for the pipeline to finish running. When it‚Äôs complete, you can find a portfolio named ML Admin Portfolio on the Portfolios page on the Service Catalog console.</p><p>Repeat the same steps to set up the SageMaker Projects Portfolio, make sure you‚Äôre using the sample code (sagemaker-projects-portfolio) and create a new code repository (with a name such as sm-projects-service-catalog-repo).</p><h3>Share the portfolios with workload accounts</h3><p>You can share the portfolios with workload accounts in Service Catalog. Again, we use ML Admin Portfolio as an example.</p><h3>Confirm available portfolios in workload accounts</h3><p>If the sharing was successful, you should see both portfolios available on the Service Catalog console, on the  page under .</p><p>Now that the service catalogs in the ML Shared Services Account have been shared with the workloads OU, the data science team can provision resources such as SageMaker domains using the templates and set up SageMaker projects to accelerate ML models‚Äô development while complying with the organization‚Äôs best practices.</p><p>We demonstrated how to create and share portfolios with workload accounts. However, the journey doesn‚Äôt stop here. The ML engineer can continue to evolve existing products and develop new ones based on the organization‚Äôs requirements.</p><p>The following sections describe the processes involved in setting up ML Development Accounts and running ML experiments.</p><h3>Set up the ML Development Account</h3><p>The ML Development account setup consists of the following tasks and stakeholders:</p><ol><li>The team lead requests the cloud admin to provision the ML Development Account.</li><li>The cloud admin provisions the account.</li><li>The team lead uses shared Service Catalog portfolios to provisions SageMaker domains, set up IAM roles and give access, and get access to data in Amazon S3, or Amazon DataZone or AWS Lake Formation, or a central feature group, depending on which solution the organization decides to use.</li></ol><p><a href=\"https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-3-setting-up-data-governance-at-scale/\" target=\"_blank\" rel=\"noopener\">Part 3</a> in this series described multiple ways to share data across the organization. The current architecture allows data access using the following methods:</p><ul><li><strong>Option 1: Train a model using Amazon DataZone</strong> ‚Äì If the organization has Amazon DataZone in the central governance account or data hub, a data publisher can create an Amazon DataZone project to publish the data. Then the data scientist can subscribe to the Amazon DataZone published datasets from <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Studio</a>, and use the dataset to build an ML model. Refer to the <a href=\"https://github.com/aws-samples/data-and-ml-governance-workshop/blob/main/module-4/bank-marketing-datzone-athena-access.ipynb\" target=\"_blank\" rel=\"noopener\">sample code</a> for details on how to use subscribed data to train an ML model.</li><li><strong>Option 2: Train a model using Amazon S3</strong> ‚Äì Make sure the user has access to the dataset in the S3 bucket. Follow the sample code to run an ML experiment pipeline using data stored in an S3 bucket.</li><li><strong>Option 3: Train a model using a data lake with Athena</strong> ‚Äì Part 2 introduced how to set up a data lake. Follow the sample code to run an ML experiment pipeline using data stored in a data lake with Amazon Athena.</li><li><strong>Option 4: Train a model using a central feature group</strong> ‚Äì Part 2 introduced how to set up a central feature group. Follow the sample code to run an ML experiment pipeline using data stored in a central feature group.</li></ul><p>You can choose which option to use depending on your setup. For options 2, 3, and 4, the SageMaker Projects Portfolio provides project templates to run ML experiment pipelines, steps including data ingestion, model training, and registering the model in the model registry.</p><p>In the following example, we use option 2 to demonstrate how to build and run an ML pipeline using a SageMaker project that was shared from the ML Shared Services Account.</p><p>It takes a few minutes to create the project.</p><p>After the project is created, a SageMaker pipeline is triggered to perform the steps specified in the SageMaker project. Choose  in the navigation pane to see the pipeline.You can choose the pipeline to see the Directed Acyclic Graph (DAG) of the pipeline. When you choose a step, its details show in the right pane.</p><p>The last step of the pipeline is registering the model in the current account‚Äôs model registry. As the next step, the lead data scientist will review the models in the model registry, and decide if a model should be approved to be promoted to the ML Shared Services Account.</p><p>The lead data scientist should review the trained ML models and approve the candidate model in the model registry of the development account. After an ML model is approved, it triggers a local event, and the event buses in EventBridge will send model approval events to the ML Shared Services Account, and the artifacts of the models will be copied to the central model registry. A model card will be created for the model if it‚Äôs a new one, or the existing model card will update the version.</p><p>The following architecture diagram shows the flow of model approval and model promotion.</p><p>After the previous step, the model is available in the central model registry in the ML Shared Services Account. ML engineers can now deploy the model.</p><p>If you had used the <a href=\"https://github.com/aws-samples/data-and-ml-governance-workshop/tree/main/module-3\" target=\"_blank\" rel=\"noopener\">sample code</a> to bootstrap the SageMaker Projects portfolio, you can use the <strong>Deploy real-time endpoint from ModelRegistry ‚Äì Cross account, test and prod</strong> option in SageMaker Projects to set up a project to set up a pipeline to deploy the model to the target test account and production account.</p><p>The pipeline for deployment is ready. The ML engineer will review the newly promoted model in the ML Shared Services Account. If the ML engineer approves model, it will trigger the deployment pipeline. You can see the pipeline on the CodePipeline console.</p><p>The pipeline will first deploy the model to the test account, and then pause for manual approval to deploy to the production account. ML engineer can test the performance and Governance officer can validate the model results in the test account. If the results are satisfactory, Governance officer can approve in CodePipeline to&nbsp;deploy&nbsp;the model to production account.</p><p>This post provided detailed steps for setting up the key components of a multi-account ML platform. This includes configuring the ML Shared Services Account, which manages the central templates, model registry, and deployment pipelines; sharing the ML Admin and SageMaker Projects Portfolios from the central Service Catalog; and setting up the individual ML Development Accounts where data scientists can build and train models.</p><p>The post also covered the process of running ML experiments using the SageMaker Projects templates, as well as the model approval and deployment workflows. Data scientists can use the standardized templates to speed up their model development, and ML engineers and stakeholders can review, test, and approve the new models before promoting them to production.</p><p>This multi-account ML platform design follows a federated model, with a centralized ML Shared Services Account providing governance and reusable components, and a set of development accounts managed by individual lines of business. This approach gives data science teams the autonomy they need to innovate, while providing enterprise-wide security, governance, and collaboration.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/vivian-li.png\" alt=\"\" width=\"100\" height=\"130\"> is a Senior Solutions Architect in AWS, with specialization in AI/ML. She currently supports customers in financial industry. Prior to joining AWS in 2022, she had 7 years of experience supporting enterprise customers use AI/ML in the cloud to drive business results. Vivian has a BS from Peking University and a PhD from University of Southern California. In her spare time, she enjoys all the water activities, and hiking in the beautiful mountains in her home state, Colorado.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/Ram-Vittal.png\" alt=\"\" width=\"100\" height=\"126\"> is a Principal ML Solutions Architect at AWS. He has over 3 decades of experience architecting and building distributed, hybrid, and cloud applications. He is passionate about building secure, scalable, reliable AI/ML and big data solutions to help enterprise customers with their cloud adoption and optimization journey to improve their business outcomes. In his spare time, he enjoys riding motorcycle and walking with his dogs.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/cere-1.png\" alt=\"\" width=\"100\" height=\"94\"> is a GenAI Evaluation Specialist and Solutions Architect at AWS. He assists customers across industries and regions in operationalizing and governing their generative AI systems at scale, ensuring they meet the highest standards of performance, safety, and ethical considerations. Bringing a unique perspective to the field of AI, Alessandro has a background in quantum physics and research experience in quantum communications and quantum memories. In his spare time, he pursues his passion for landscape and underwater photography.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/Alberto.jpg\" alt=\"\" width=\"100\" height=\"134\">&nbsp;is a DevOps Consultant in Professional Services at AWS. He helps accelerate customers‚Äô journeys to the cloud and achieve their digital transformation goals. In his free time, he enjoys playing sports, especially basketball and padel, spending time with family and friends, and learning about technology.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/Sovik-100-1.jpg\" alt=\"\" width=\"100\" height=\"104\">&nbsp;is an AI/ML and Generative AI senior solution architect with AWS. He has extensive experience designing end-to-end machine learning and business analytics solutions in finance, operations, marketing, healthcare, supply chain management, and IoT. He has double masters degrees from the University of South Florida, University of Fribourg, Switzerland, and a bachelors degree from the Indian Institute of Technology, Kharagpur. Outside of work, Sovik enjoys traveling, taking ferry rides, and watching movies.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/viktor-1.jpg\" alt=\"\" width=\"100\" height=\"100\">&nbsp;is a Senior Machine Learning Engineer within AWS Professional Services, leading teams to build advanced machine learning solutions in the cloud. He‚Äôs passionate about making AI impactful, overseeing the entire process from modeling to production. In his spare time, he enjoys surfing, cycling, and traveling.</p>","contentLength":21699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The case for sans-io","url":"https://fasterthanli.me/articles/the-case-for-sans-io","date":1738954381,"author":"Amos Wenger","guid":18,"unread":true,"content":"<p data-bo=\"208\">The most popular option to decompress ZIP files from the Rust programming\nlanguage is a crate simply named <a href=\"https://lib.rs/crates/zip\">zip</a> ‚Äî At the time of this writing, it has 48\nmillion downloads. It‚Äôs fully-featured, supporting various compression methods,\nencryption, and even supports writing zip files.</p><p data-bo=\"528\">However, that‚Äôs not the crate  uses to read ZIP files. Some\napplications benefit from using asynchronous I/O, especially if they decompress\narchives that they download from the network.</p><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#character-encoding-differences\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#platform-differences\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#the-end-of-central-directory-record\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#boundary-confusion\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#not-doing-any-i-o-at-all\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#the-structure-of-rc-zip\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#bringing-io-uring-into-it\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#plugging-rc-zip-into-monoio\"></a><a href=\"https://fasterthanli.me/articles/the-case-for-sans-io#closing-words\"></a>","contentLength":471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerate your Amazon Q implementation: starter kits for SMBs","url":"https://aws.amazon.com/blogs/machine-learning/accelerate-your-amazon-q-implementation-starter-kits-for-smbs/","date":1738949354,"author":"Nneoma Okoroafor","guid":220,"unread":true,"content":"<p>Whether you‚Äôre a small or medium-sized business (SMB) or a managed service provider at the beginning of your cloud journey, you might be wondering how to get started. Questions like ‚ÄúAm I following best practices?‚Äù, ‚ÄúAm I optimizing my cloud costs?‚Äù, and ‚ÄúHow difficult is the learning curve?‚Äù are quite common. AWS is here to provide a concept called starter kits.</p><p>Starter kits are complete, deployable solutions that address common, repeatable business problems. They deploy the services that make up a solution according to best practices, helping you optimize costs and become familiar with these kinds of architectural patterns without a large investment in training. Most of all, starter kits save you time‚Äîtime that can be better spent on your business or with your customers.</p><p>In this post, we showcase a starter kit for <a href=\"https://aws.amazon.com/q/business/\" target=\"_blank\" rel=\"noopener\">Amazon Q Business</a>. If you have a repository of documents that you need to turn into a knowledge base quickly, or simply want to test out the capabilities of Amazon Q Business without a large investment of time at the console, then this solution is for you.</p><p>Amazon Q Business is a generative AI-powered assistant that can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in your enterprise systems. It empowers employees to be more creative, data-driven, efficient, prepared, and productive.</p><p>The following diagram illustrates the solution architecture.</p><p>The workflow involves the following steps:</p><ol><li>Upon successful authentication, the user can access the Amazon Q web UI and ask a question.</li><li>Amazon Q retrieves relevant information from its index, which is populated using data from the connected data sources (Amazon S3 and a web crawler).</li><li>Amazon Q then generates a response using its internal large language model (LLM) and presents it to the user through the Amazon Q web UI.</li><li>The user can provide feedback on the response through the Amazon Q web UI.</li></ol><p>Before deploying the solution, make sure you have the following in place:</p><ul><li>‚Äì You will need an active <a href=\"https://aws.amazon.com/\" target=\"_blank\" rel=\"noopener\">AWS account</a> with the necessary permissions to deploy CloudFormation stacks and create the required resources.</li><li> ‚Äì Make sure you have an existing S3 bucket that will be used as the data source for Amazon Q. To create a S3 bucket, refer to <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html\" target=\"_blank\" rel=\"noopener\">Create your first S3 bucket</a>.</li><li> ‚Äì <a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html\" target=\"_blank\" rel=\"noopener\">Configure AWS IAM Identity Center</a> in your AWS environment. You will need to provide the necessary details, such as the IAM Identity Center instance Amazon Resource Name (ARN), during the deployment process.</li></ul><h2>Deploy the solution using AWS CloudFormation</h2><p>Complete the following steps to deploy the CloudFormation template:</p><ol start=\"9\"><li>On the  page, leave everything as default, select <strong>I acknowledge that AWS CloudFormation might create IAM resources</strong> and and choose .</li></ol><p>To validate the Amazon Q solution is functioning as expected, perform the following tests:</p><ol><li>Test data ingestion: \n  <ol type=\"a\"><li>Upload a test file to the S3 bucket.</li><li>Verify that the file is successfully ingested and processed by Amazon Q.</li><li>Check the Amazon Q web experience UI for the processed data.</li></ol></li><li>Test web crawler functionality:</li><li>Verify that the web crawler is able to retrieve and ingest the data from the website.</li><li>Make sure the data is displayed correctly in the Amazon Q web experience UI.</li></ol><p>To clean up, delete the CloudFormation stack and the S3 bucket you created.</p><p>The Amazon Q starter kit provides a streamlined solution for SMBs to use the power of generative AI and intelligent question-answering. By automating the deployment and integration with key data sources, this kit eases the complexity of setting up Amazon Q, empowering businesses to quickly unlock insights and improve productivity.</p><p>If your SMB has a repository of documents that need to be transformed into a valuable knowledge base, or you simply want to explore the capabilities of Amazon Q, we encourage you to take advantage of this starter kit. Get started today and experience the transformative benefits of enterprise-grade question-answering tailored for your business needs, and let us know what you think in the comments. To explore more generative AI use cases, refer to <a href=\"https://aws.amazon.com/machine-learning/ai-use-cases/?use-cases.sort-by=item.additionalFields.priority&amp;use-cases.sort-order=asc&amp;awsf.use-case-area=*all&amp;awsf.industry=*all&amp;awsf.business-function=*all&amp;awsf.business-outcome=*all&amp;awsf.type=*all\" target=\"_blank\" rel=\"noopener\">AI Use Case Explorer</a>.</p><p> is a Partner Solutions Architect focused on AI/ML and generative AI. Nneoma is passionate about providing guidance to AWS Partners on using the latest technologies and techniques to deliver innovative solutions to customers.</p><p> is a Partner Solutions Architect with Amazon Web Services. He primarily serves consulting partners, providing architectural guidance and recommendations for new and existing workloads. Outside of work, he enjoys playing soccer, golf, and spending time with family and friends.</p><p> is a Partner Solutions Architect focused on helping AWS Distribution Partners and their Seller Partners build and grow their AWS practices. Jason is passionate about building solutions for MSPs and VARs in the small business space. Outside the office, Jason is an avid traveler and enjoys offshore fishing.</p>","contentLength":4928,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building the future of construction analytics: CONXAI‚Äôs AI inference on Amazon EKS","url":"https://aws.amazon.com/blogs/machine-learning/building-the-future-of-construction-analytics-conxais-ai-inference-on-amazon-eks/","date":1738948894,"author":"Tim Krause","guid":219,"unread":true,"content":"<p><em>This is a guest post co-written with Tim Krause, Lead MLOps Architect at CONXAI.</em></p><p><a href=\"https://www.conxai.com/\" target=\"_blank\" rel=\"noopener\">CONXAI Technology GmbH</a> is pioneering the development of an advanced AI platform for the Architecture, Engineering, and Construction (AEC) industry. Our platform uses advanced AI to empower construction domain experts to create complex use cases efficiently.</p><p>Construction sites typically employ multiple CCTV cameras, generating vast amounts of visual data. These camera feeds can be analyzed using AI to extract valuable insights. However, to comply with GDPR regulations, all individuals captured in the footage must be anonymized by masking or blurring their identities.</p><p>Our AI solution is offered in two forms:</p><ul><li><strong>Model as a service (MaaS)</strong> ‚Äì Our AI model is accessible through an API, enabling seamless integration. Pricing is based on processing batches of 1,000 images, offering flexibility and scalability for users.</li><li><strong>Software as a service (SaaS)</strong> ‚Äì This option provides a user-friendly dashboard, acting as a central control panel. Users can add and manage new cameras, view footage, perform analytical searches, and enforce GDPR compliance with automatic person anonymization.</li></ul><p>Our AI model, fine-tuned with a proprietary dataset of over 50,000 self-labeled images from construction sites, achieves significantly greater accuracy compared to other MaaS solutions. With the ability to recognize more than 40 specialized object classes‚Äîsuch as cranes, excavators, and portable toilets‚Äîour AI solution is uniquely designed and optimized for the construction industry.</p><p>Initially, CONXAI started with a small cloud provider specializing in offering affordable GPUs. However, it lacked essential services required for machine learning (ML) applications, such as frontend and backend infrastructure, DNS, load balancers, scaling, blob storage, and managed databases. At that time, the application was deployed as a single monolithic container, which included Kafka and a database. This setup was neither scalable nor maintainable.</p><p>After migrating to AWS, we gained access to a robust ecosystem of services. Initially, we deployed the all-in-one AI container on a single <a href=\"https://aws.amazon.com/ec2/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instance. Although this provided a basic solution, it wasn‚Äôt scalable, necessitating the development of a new architecture.</p><p>Our top reasons for choosing AWS were primarily driven by the team‚Äôs extensive experience with AWS. Additionally, the initial cloud credits provided by AWS were invaluable for us as a startup. We now use AWS managed services wherever possible, particularly for data-related tasks, to minimize maintenance overhead and pay only for the resources we actually use.</p><p>At the same time, we aimed to remain cloud-agnostic. To achieve this, we chose Kubernetes, enabling us to deploy our stack directly on a customer‚Äôs edge‚Äîsuch as on construction sites‚Äîwhen needed. Some customers are potentially very compliance-restrictive, not allowing data to leave the construction site. Another opportunity is federated learning, training on the customer‚Äôs edge and only transferring model weights, without sensitive data, into the cloud. In the future, this approach might lead to having one model fine-tuned for each camera to achieve the best accuracy, which requires hardware resources on-site. For the time being, we use Amazon EKS to offload the management overhead to AWS, but we could easily deploy on a standard Kubernetes cluster if needed.</p><p>Our previous model was running on TorchServe. With our new model, we first tried performing inference in Python with Flask and PyTorch, as well as with BentoML. Achieving high inference throughput with high GPU utilization for cost-efficiency was very challenging. Exporting the model to ONNX format was particularly difficult because the OneFormer model lacks strong community support. It took us some time to identify why the OneFormer model was so slow in the ONNX Runtime with NVIDIA Triton. We ultimately resolved the issue by converting ONNX to TensorRT.</p><p>Defining the final architecture, training the model, and optimizing costs took approximately 2‚Äì3 months. Currently, we improve our model by incorporating increasingly accurate labeled data, a process that takes around 3‚Äì4 weeks of training on a single GPU. Deployment is fully automated with GitLab CI/CD pipelines, Terraform, and Helm, requiring less than an hour to complete without any downtime. New model versions are typically rolled out in shadow mode for 1‚Äì2 weeks to provide stability and accuracy before full deployment.</p><p>The following diagram illustrates the solution architecture.</p><p>The architecture consists of the following key components:</p><ul><li>The S3 bucket (1) is the most important data source. It is cost-effective, scalable, and provides almost unlimited blob storage. We encrypt the S3 bucket, and we delete all data with privacy concerns after processing took place. Almost all microservices read and write files from and to Amazon S3, which ultimately triggers (2) <a href=\"https://aws.amazon.com/eventbridge/\" target=\"_blank\" rel=\"noopener\">Amazon EventBridge</a> (3). The process begins when a customer uploads an image on Amazon S3 using a presigned URL provided by our API handling user authentication and authorization through <a href=\"https://aws.amazon.com/cognito/\" target=\"_blank\" rel=\"noopener\">Amazon Cognito</a>.</li><li>The S3 bucket is configured in such a way that it forwards (2) all events into EventBridge.</li><li>TriggerMesh is a Kubernetes controller where we use  (6). It abstracts the infrastructure automation and automatically creates an <a href=\"https://aws.amazon.com/sqs/\" target=\"_blank\" rel=\"noopener\">Amazon Simple Queue Service</a> (Amazon SQS) (5) processing queue, which acts as a processing buffer. Additionally, it creates an EventBridge rule (4) to forward the S3 event from the event bus into the SQS processing queue. Finally, TriggerMesh creates a Kubernetes Pod to poll events from the processing queue to feed it into the Knative broker (7). The resources in the Kubernetes cluster are deployed in a private subnet.</li><li>The Knative trigger (9) polls the Knative broker based on a specific  and forwards it accordingly to the KServe  (10).</li><li>KServe is a standard model inference platform on Kubernetes that uses Knative Serving as its foundation and is fully compatible with Knative Eventing. It also pulls models from a model repository into the container before the model server starts, eliminating the need to build a new container image for each model version.</li><li>We use KServe‚Äôs ‚ÄúCollocate transformer and predictor in same pod‚Äù feature to maximize inference speed and throughput because containers within the same pod can communicate over localhost and the network traffic never leaves the CPU.</li><li>After many performance tests, we achieved best performance with the NVIDIA Triton Inference Server (11) after converting our model first into ONNX and then into TensorRT.</li><li>Our transformer (12) uses Flask with Gunicorn and is optimized for the number of workers and CPU cores to maintain GPU utilization over 90%. The transformer gets a  with the reference of the image Amazon S3 path, downloads it, and performs model inference over HTTP. After getting back the model results, it performs preprocessing and finally uploads the processed model results back to Amazon S3.</li><li>We use <a href=\"https://karpenter.sh/\" target=\"_blank\" rel=\"noopener\">Karpenter</a> as the cluster auto scaler. Karpenter is responsible for scaling the inference component to handle high user request loads. Karpenter launches new EC2 instances when the system experiences increased demand. This allows the system to automatically scale up computing resources to meet the increased workload.</li></ul><p>All this divides our architecture mainly in AWS managed data service and the Kubernetes cluster:</p><ul><li>The S3 bucket, EventBridge, and SQS queue as well as Amazon MSK are all fully managed services on AWS. This keeps our data management effort low.</li><li>We use Amazon EKS for everything else. TriggerMesh, , Knative Broker, Knative Trigger, KServe with our Python transformer, and the Triton Inference Server are also within the same EKS cluster on a dedicated EC2 instance with a GPU. Because our EKS cluster is just used for processing, it is fully stateless.</li></ul><p>From initially having our own highly customized model, transitioning to AWS, improving our architecture, and introducing our new Oneformer model, CONXAI is now proud to provide scalable, reliable, and secure ML inference to customers, enabling construction site improvements and accelerations. We achieved a GPU utilization of over 90%, and the number of processing errors has dropped almost to zero in recent months. One of the major design choices was the separation of the model from the preprocessing and postprocessing code in the transformer. With this technology stack, we gained the ability to scale down to zero on Kubernetes using the Knative serverless feature, while our scale-up time from a cold state is just 5‚Äì10 minutes, which can save significant infrastructure costs for potential batch inference use cases.</p><p>The next important step is to use these model results with proper analytics and data science. These results can also serve as a data source for generative AI features such as automated report generation. Furthermore, we want to label more diverse images and train the model on additional construction domain classes as part of a continuous improvement process. We also work closely with AWS specialists to bring our model in <a href=\"https://aws.amazon.com/ai/machine-learning/inferentia/\" target=\"_blank\" rel=\"noopener\">AWS Inferentia</a> chipsets for better cost-efficiency.</p><p><em>To learn more about the services in this solution, refer to the following resources:</em></p><p> is Lead MLOps Architect at CONXAI. He takes care of all activities when AI meets infrastructure. He joined the company with previous Platform, Kubernetes, DevOps, and Big Data knowledge and was training LLMs from scratch.</p><p> is a Solutions Architect at AWS, working with startup customers, and leveraging his expertise to help startup customers design their workloads on AWS.</p>","contentLength":9714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Untold Studios empowers artists with an AI assistant built on Amazon Bedrock","url":"https://aws.amazon.com/blogs/machine-learning/how-untold-studios-empowers-artists-with-an-ai-assistant-built-on-amazon-bedrock/","date":1738947969,"author":"Olivier Vigneresse","guid":218,"unread":true,"content":"<p><a href=\"https://untoldstudios.tv/\" target=\"_blank\" rel=\"noopener\">Untold Studios</a> is a tech-driven, leading creative studio specializing in high-end visual effects and animation. Our commitment to innovation led us to a pivotal challenge: how to harness the power of machine learning (ML) to further enhance our competitive edge while balancing this technological advancement with strict data security requirements and the need to streamline access to our existing internal resources.</p><p>To give our artists access to technology, we need to create good user interfaces. This is a challenge, especially if the pool of end-users are diverse in terms of their needs and technological experience. We saw an opportunity to use large language models (LLMs) to create a natural language interface, which makes this challenge easier and takes care of a lot of the heavy lifting.</p><p>This post details how we used <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> to create an AI assistant (Untold Assistant), providing artists with a straightforward way to access our internal resources through a natural language interface integrated directly into their existing Slack workflow.</p><p>The Untold Assistant serves as a central hub for artists. Besides the common AI functionalities like text and image generation, it allows them to interact with internal data, tools, and workflows through natural language queries.</p><p>For the UI, we use Slack‚Äôs built-in features rather than building custom frontends. Slack already provides applications for workstations and phones, message threads for complex queries, emoji reactions for feedback, and file sharing capabilities. The implementation uses Slack‚Äôs event subscription API to process incoming messages and Slack‚Äôs Web API to send responses. Users interact with the Untold Assistant through private direct messages or by mentioning it (@-style tagging) in channels for everybody to see. Because our teams already use Slack throughout the day, this eliminates context switching and the need to adopt new software. Every new message is acknowledged by a gear emoji for immediate feedback, which eventually changes to a check mark if the query was successful or an X if an error occurred. The following screenshot shows an example.</p><p>With the use of <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropic‚Äôs Claude 3.5 Sonnet model on Amazon Bedrock</a>, the system processes complex requests and generates contextually relevant responses. The serverless architecture provides scalability and responsiveness, and secure storage houses the studio‚Äôs vast asset library and knowledge base. Key AWS services used include:</p><p>The following diagram illustrates the solution architecture.</p><p>The main components for this application are the Slack integration, the Amazon Bedrock integration, the Retrieval Augmented Generation (RAG) implementation, user management, and logging.</p><p>We use a two-function approach to meet Slack‚Äôs 3-second acknowledgment requirement. The incoming event from Slack is sent to an endpoint in API Gateway, and Slack expects a response in less than 3 seconds, otherwise the request fails. The first Lambda function, with reserved capacity, quickly acknowledges the event and forwards the request to the second function, where it can be handled without time restrictions. The setup handles time-sensitive responses while allowing for thorough request processing. We call the second function directly from the first function without using an event with <a href=\"http://aws.amazon.com/sns\" target=\"_blank\" rel=\"noopener\">Amazon Simple Notification Service</a> (Amazon SNS) or a queue with <a href=\"https://aws.amazon.com/sqs/\" target=\"_blank\" rel=\"noopener\">Amazon Simple Queue Service</a> (Amazon SQS) in between to keep the latency as low as possible.</p><h3>Amazon Bedrock integration</h3><p>Our Untold Assistant uses Amazon Bedrock with Anthropic‚Äôs Claude 3.5 Sonnet model for natural language processing. We use the model‚Äôs function calling capabilities, enabling the application to trigger specific tools or actions as needed. This allows the assistant to handle both general queries and complex specialized queries or run tasks across our internal systems.</p><p>Our RAG setup uses Amazon Bedrock connectors to integrate with Confluence and Salesforce, tapping into our existing knowledge bases. For other data sources without a pre-built connector available, we export content to Amazon S3 and use the Amazon S3 connector. For example, we export pre-chunked asset metadata from our asset library to Amazon S3, letting Amazon Bedrock handle embeddings, vector storage, and search. This approach significantly decreased development time and complexity, allowing us to focus on improving user experience.</p><p>We map Slack user IDs to our internal user pool, currently in DynamoDB (but designed to work with <a href=\"https://aws.amazon.com/cognito/\" target=\"_blank\" rel=\"noopener\">Amazon Cognito</a>). This system tailors the assistant‚Äôs capabilities to each user‚Äôs role and clearance level, making sure that it operates within the bounds of each user‚Äôs authority while maintaining functionality. The access to data sources is controlled using tools. Every tool encapsulates a data source and the LLM‚Äôs access to tools is restricted by the user and their role.</p><p>Additionally, if a user tells the assistant something that should be remembered, we store this piece of information in a database and add it to the context every time the user initiates a request. This could be, for example, ‚ÄúKeep all your replies as short as possible‚Äù or ‚ÄúIf I ask for code it‚Äôs always Python.‚Äù</p><p>We use the built-in integration with <a href=\"http://aws.amazon.com/cloudwatch\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a> in Lambda to track system performance and error states. For monitoring critical errors, we‚Äôve set up direct notifications to a dedicated Slack channel, allowing for immediate awareness and response. Every query and tool invocation is logged to DynamoDB, providing a rich dataset that we use to analyze usage patterns and optimize the system‚Äôs performance and functionality.</p><h2>Function calling with Amazon Bedrock</h2><p>Like Anthropic‚Äôs Claude, most modern LLMs support function calling, which allows us to extend the capabilities of LLMs beyond merely generating text. We can provide a set of function specifications with a description of what the function is going to do and the names and descriptions of the function‚Äôs parameters. Based on this information, the LLM decides if an incoming request can be solved directly or if the best next step to solve the query would be a function call. If that‚Äôs the case, the model returns the name of the function to call, as well as the parameters and values. It‚Äôs then up to us to run the function and initiate the next step. Agents use this system in a loop to call functions and process their output until a success criterion is reached. In our case, we only implement a single pass function call to keep things simple and robust. However, in certain cases, the function itself uses the LLM to process data and format it nicely for the end-user.</p><p>Function calling is a very useful feature that helps us convert unstructured user input into structured automatable instructions. We anticipate that over the next couple of months, we will add many more functions to extend the AI assistant‚Äôs capabilities and increase its usefulness. Although frameworks like LangChain offer comprehensive solutions for implementing function calling systems, we opted for a lightweight, custom approach tailored to our specific needs. This decision allowed us to maintain a smaller footprint and focus on the essential features for our use case.</p><p>The following is a code example of using the AiTool base class for extendability.</p><p>All that‚Äôs required to add a new function is creating a class like the one in our example. The class will automatically be discovered and the respective specification added to the request to the LLM if the user has access to the function. All the required information to create the function specification is extracted from the code and docstrings:</p><ul><li> ‚Äì The ID of the function</li><li> ‚Äì A message that‚Äôs sent to the user through Slack for immediate feedback before the function is run</li><li><strong>EXCLUSIVE_ACCESS_DEPARTMENTS</strong> ‚Äì If set, only users of the specified departments have access to this tool</li></ul><p>The tool in this example updates the user memory. For example, the query ‚ÄúRemember to always use Python as a programming language‚Äù will trigger the execution of this tool. The LLM will extract the info string from the request, for example, ‚Äúcode should always be Python.‚Äù If the existing user memory that is always added to the context already contains a memory about the same topic (for example, ‚Äúcode should always be Java‚Äù), the LLM will also provide the memory ID and the existing memory will be overwritten. Otherwise, a new memory with a new ID is created.</p><h2>Key features and benefits</h2><p>Slack serves as a single entry point, allowing artists to query diverse internal systems without leaving their familiar workflow. The following features are powered by function calling using Anthropic‚Äôs Claude:</p><ul><li>Various knowledge bases for different user roles (Confluence, Salesforce)</li><li>Internal asset library (Amazon S3)</li><li>Image generation powered by Stable Diffusion</li><li>User-specific memory and preferences (for example, default programming languages, default dimensions for image generation, detail level of responses)</li></ul><p>By eliminating the need for additional software or context switching, we‚Äôve drastically reduced friction in accessing critical resources. The system is available around the clock for artist queries and tasks, and our framework for function calling with Anthropic‚Äôs Claude allows for future expansion of features.</p><p>The LLM‚Äôs natural language interface is a game changer for user interaction. It‚Äôs inherently more flexible and forgiving compared to traditional interfaces, capable of interpreting unstructured input, asking for missing information, and performing tasks like date formatting, unit conversion, and value extraction from natural language descriptions. The system adeptly handles ambiguous queries, extracting relevant information and intent. This means artists can focus on their creative process rather than worrying about precise phrasing or navigating complex menu structures.</p><p>Security and control are paramount in our AI adoption strategy. By keeping all data within the AWS ecosystem, we‚Äôve eliminated dependencies on third-party AI tools and mitigated associated risks. This approach allows us to maintain tight control over data access and usage. Additionally, we‚Äôve implemented comprehensive usage analytics, providing insights into adoption patterns and areas for improvement. This data-driven approach makes sure we‚Äôre continually refining the tool to meet evolving artist needs.</p><p>The Untold Assistant currently handles up to 120 queries per day, with about 10‚Äì20% of them calling additional tools, like image generation or knowledge base search. Especially for new users who aren‚Äôt too familiar with internal workflows and applications yet, it can save a lot of time. Instead of searching in several different Confluence spaces and Slack channels or reaching out to the technology team, they can just ask the Untold Assistant, which acts as a virtual member of the support team. This can cut down the time from minutes to only a few seconds.</p><p>Overall, the Untold Assistant, rapidly developed and deployed using AWS services, has delivered several benefits:</p><ul><li>Enhanced discoverability and usage of previously underutilized internal resources</li><li>Significant reduction in time spent searching for information</li><li>Streamlined access to multiple internal systems with an authorization system from a central entry point</li><li>Reduced load on the support and technology team</li><li>Increased speed of adoption of new technologies by providing a framework for user interaction</li></ul><p>Building on this success, we‚Äôre expanding functionality through additional function calls. A key planned feature is render job error analysis for artists. This tool will automatically fetch logs from recent renders, analyze potential errors using the capabilities of Anthropic‚Äôs Claude, and provide users with explanations and solutions by using both internet resources and our internal knowledge base of known errors.</p><p>Additionally, we plan to analyze the saved queries using <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html\" target=\"_blank\" rel=\"noopener\">Amazon Titan Text Embeddings</a> and agglomerative clustering to identify semantically similar questions. When the cluster frequency exceeds our defined threshold (for example, more than 10 similar questions from different users within a week), we enhance our knowledge base or update onboarding materials to address these common queries proactively, reducing repetitive questions and improving the assistant‚Äôs efficiency.</p><p>These initial usage metrics and the planned technical improvements demonstrate the system‚Äôs positive impact on our workflows. By automating common support tasks and continuously improving our knowledge base through data-driven analysis, we reduce the technology team‚Äôs support load while maintaining high-quality assistance. The modular architecture allows us to quickly integrate new tools as needs arise, to keep up with the astonishing pace of the progress made in AI and ML.</p><p>The Untold Assistant demonstrates how Amazon Bedrock enables rapid development of sophisticated AI applications without compromising security or control. Using function calling and pre-built connectors in Amazon Bedrock eliminated the need for complex vector store integrations and custom embedding pipelines, reducing our development time from months to weeks. The modular architecture using Python classes for tools makes the system highly maintainable and extensible.</p><p>By automating routine technical tasks and information retrieval, we‚Äôve freed our artists to focus on creative work that drives business value. The solution‚Äôs clean separation between the LLM interface and business logic, built entirely within the AWS ecosystem, enables quick integration of new capabilities while maintaining strict data security. The LLM‚Äôs ability to interpret unstructured input and handle ambiguous queries creates a more natural and forgiving interface compared to traditional menu-driven systems. This foundation of technical robustness and improved artist productivity positions us to rapidly adopt emerging AI capabilities while keeping our focus on creative innovation.</p><p>To explore how to streamline your company‚Äôs workflows using Amazon Bedrock, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\" target=\"_blank\" rel=\"noopener\">Getting started with Amazon Bedrock</a>. If you have questions or suggestions, please leave a comment.</p><p> is a Solutions Architect at AWS. Based in England, he primarily works with SMB Media an&amp;d Entertainment customers. With a background in security and networking, Olivier helps customers achieve success on their cloud journey by providing architectural guidance and best practices; he is also passionate about helping them bring value with Machine Learning and Generative AI use-cases.</p><p> is a Lead R&amp;D Developer at Untold Studios with a focus on cloud infrastructure and emerging technologies. After earning his PhD in Germany, where he collaborated with industry leaders like BMW and Audi, he has spent the past decade implementing software solutions, with a particular emphasis on cloud technology in recent years. At Untold Studios, he leads infrastructure optimisation and AI/ML initiatives, leveraging his technical expertise and background in research to drive innovation in the Media &amp; Entertainment space.</p><p> is an Account Manager at AWS who specialises in accelerating the cloud journey of Media &amp; Entertainment customers. He has been helping customers at AWS for the past 4.5 years. Max has been particularly involved with customers in the visual effect space, guiding them as they explore generative AI.</p>","contentLength":15446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Real Python Podcast ‚Äì Episode #238: Charlie Marsh: Accelerating Python Tooling With Ruff and uv","url":"https://realpython.com/podcasts/rpp/238/","date":1738929600,"author":"Real Python","guid":180,"unread":true,"content":"<p>Are you looking for fast tools to lint your code and manage your projects? How is the Rust programming language being used to speed up Python tools? This week on the show, we speak with Charlie Marsh about his company, Astral, and their tools, uv and Ruff.</p>","contentLength":256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Your YouTube Channel to Learn System Design üî•","url":"https://newsletter.systemdesign.one/p/system-design-youtube-channel","date":1738928139,"author":"Neo Kim","guid":55,"unread":true,"content":"<p>I created a YouTube channel to help you learn system design.</p><p>‚Ä¢ System design fundamentals.</p><p>‚Ä¢ System design interview preparation tips.</p><p>‚Ä¢ Simplified engineering case studies with visuals.</p><p>‚Ä¢ System design deep dives with real-world software architecture.</p><p>You'll receive a new video every 10 days.</p><p>I'll make it your main video channel for system design over time.</p><p>And I want to help you become good at work + ace system design interviews:</p><p>Please show your support,</p>","contentLength":461,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/83516d21-e16e-4561-861e-e83b9bf60224_1280x720.gif","enclosureMime":"","commentsUrl":null},{"title":"Protect your DeepSeek model deployments with Amazon Bedrock Guardrails","url":"https://aws.amazon.com/blogs/machine-learning/protect-your-deepseek-model-deployments-with-amazon-bedrock-guardrails/","date":1738895392,"author":"Satveer Khurpa","guid":217,"unread":true,"content":"<p>The rapid advancement of generative AI has brought powerful publicly available large language models (LLMs), such as <a href=\"https://aws.amazon.com/blogs/aws/deepseek-r1-models-now-available-on-aws/\" target=\"_blank\" rel=\"noopener\">DeepSeek-R1</a>, to the forefront of innovation. The DeepSeek-R1 models are now accessible through <a href=\"https://aws.amazon.com/bedrock/marketplace/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Marketplace</a> and <a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart</a>, and distilled variants are available through <a href=\"https://aws.amazon.com/bedrock/custom-model-import/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Custom Model Import</a>. According to <a href=\"https://www.deepseek.com/\" target=\"_blank\" rel=\"noopener\">DeepSeek AI</a>, these models offer strong capabilities in reasoning, coding, and natural language understanding. However, their deployment in production environments‚Äîlike all models‚Äîrequires careful consideration of data privacy requirements, appropriate management of bias in output, and the need for robust monitoring and control mechanisms.</p><p>Organizations adopting open source, open weights models such as DeepSeek-R1 have important opportunities to address several key considerations:</p><ul><li>Making sure to protect sensitive information</li><li>Fostering responsible content generation practices</li><li>Striving for compliance with relevant industry regulations</li></ul><p>These concerns become particularly critical in highly regulated industries such as healthcare, finance, and government services, where data privacy and content accuracy are paramount.</p><p>This blog post provides a comprehensive guide to implementing robust safety protections for DeepSeek-R1 and other open weight models using Amazon Bedrock Guardrails. We‚Äôll explore:</p><ul><li>How to use the security features offered by Amazon Bedrock to protect your data and applications</li><li>Practical implementation of guardrails to prevent prompt attacks and filter harmful content</li><li>Implementing a robust defense-in-depth strategy</li></ul><p>By following this guide, you‚Äôll learn how to use the advanced capabilities of DeepSeek models while maintaining strong security controls and promoting ethical AI practices. Whether developing customer-facing generative AI applications or internal tools, these implementation patterns will help you meet your requirements for secure and responsible AI. By following this step-by-step approach, organizations can deploy open weights LLMs such as DeepSeek-R1 in line with best practices for AI safety and security.</p><h2>DeepSeek models and deployment on Amazon Bedrock</h2><p>DeepSeek AI, a company specializing in open weights foundation AI models, recently launched their <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\" target=\"_blank\" rel=\"noopener\">DeepSeek-R1 models</a>, which according to their <a href=\"https://arxiv.org/pdf/2501.12948\" target=\"_blank\" rel=\"noopener\">paper</a> have shown outstanding reasoning abilities and performance in industry benchmarks. According to third-party evaluations, these models consistently achieve top three rankings across various metrics, including quality index, scientific reasoning and knowledge, quantitative reasoning, and coding (HumanEval).</p><p>The company has further developed their portfolio by releasing six dense models derived from DeepSeek-R1, built on Llama and Qwen architectures, which they‚Äôve made open weight models. These models are now accessible through AWS generative AI solutions: DeepSeek-R1 is available through <a href=\"https://aws.amazon.com/blogs/machine-learning/deepseek-r1-model-now-available-in-amazon-bedrock-marketplace-and-amazon-sagemaker-jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Marketplace and SageMaker Jumpstart</a>, while the Llama-based distilled versions can be implemented through <a href=\"https://aws.amazon.com/blogs/machine-learning/deploy-deepseek-r1-distilled-llama-models-with-amazon-bedrock-custom-model-import/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Custom Model Import</a>.</p><p>Amazon Bedrock offers comprehensive <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/security.html\" target=\"_blank\" rel=\"noopener\">security features</a> to help secure hosting and operation of open source and open weights models while maintaining data privacy and regulatory compliance. Key features include data encryption at rest and in transit, fine-grained access controls, secure connectivity options, and various <a href=\"https://aws.amazon.com/compliance/services-in-scope/\" target=\"_blank\" rel=\"noopener\">compliance</a> certifications. Additionally, Amazon Bedrock provides guardrails for content filtering and sensitive information protection to support responsible AI use. AWS enhances these capabilities with extensive platform-wide security and compliance measures:</p><p>Organizations should customize these security settings based on their specific compliance and security needs when deploying to production environments. AWS conducts vulnerability scanning of all model containers as part of its security process and accepts only models in <a href=\"https://github.com/huggingface/safetensors\" target=\"_blank\" rel=\"noopener\">Safetensors</a> format to help prevent unsafe code execution.</p><h2>Amazon Bedrock Guardrails</h2><p>Amazon Bedrock Guardrails provides configurable safeguards to help safely build generative AI applications at scale. Amazon Bedrock Guardrails can also be integrated with other Amazon Bedrock tools including <a href=\"https://aws.amazon.com/bedrock/agents/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Agents</a> and <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> to build safer and more secure generative AI applications aligned with responsible AI policies. To learn more, see the <a href=\"https://aws.amazon.com/ai/responsible-ai/\" target=\"_blank\" rel=\"noopener\">AWS Responsible AI</a> page.</p><p>Amazon Bedrock Guardrails can be used in two ways. First, it can be integrated directly with the InvokeModel and Converse API call, where guardrails are applied to both input prompts and model outputs during the inference process. This method is suitable with models hosted on Amazon Bedrock through the Amazon Bedrock Marketplace and Amazon Bedrock Custom Model Import. Alternatively, the ApplyGuardrail API offers a more flexible approach, allowing for <a href=\"https://aws.amazon.com/blogs/machine-learning/implement-model-independent-safety-measures-with-amazon-bedrock-guardrails/\" target=\"_blank\" rel=\"noopener\">independent evaluation of content</a> without invoking a model. This second method is useful for assessing inputs or outputs at various stages of an application, working with custom or third-party models outside of Amazon Bedrock. Both approaches enable developers to implement safeguards customized to their use cases and aligned with responsible AI policies, ensuring secure and compliant interactions in generative AI applications.</p><h4>Key Amazon Bedrock Guardrails policies</h4><p>Amazon Bedrock Guardrails provides the following configurable guardrail policies to help safely build generative AI applications at scale:</p><ul><li>Content filters \n  <ul><li>Adjustable filtering intensity for harmful content</li><li>Predefined categories: Hate, Insults, Sexual Content, Violence, Misconduct, and Prompt Attacks</li><li>Multi-modal content including text and images (preview)</li></ul></li><li>Topic filters \n  <ul><li>Capability to restrict specific topics</li><li>Prevention of unauthorized topics in both queries and responses</li></ul></li><li>Word filters \n  <ul><li>Blocks specific words, phrases, and profanity</li><li>Custom filters for offensive language or competitor references</li></ul></li><li>Sensitive information filters \n  <ul><li>Personally identifiable information (PII) blocking or masking</li><li>Support for custom regex patterns</li><li>Probabilistic detection for standard formats (such as SSN, DOB, and addresses)</li></ul></li><li>Contextual grounding checks \n  <ul><li>Hallucination detection through source grounding</li><li>Query relevance validation</li></ul></li><li>Automated Reasoning checks for hallucination prevention (gated preview)</li></ul><p>Model-agnostic implementation:</p><ul><li>Compatible with all Amazon Bedrock foundation models</li><li>Supports fine-tuned models</li><li>Extends to external custom and third-party models through the ApplyGuardrail API</li></ul><p>This comprehensive framework helps customers implement responsible AI, maintaining content safety and user privacy across diverse generative AI applications.</p><ol><li><ul><li>Create a guardrail with specific policies tailored to your use case and configure the policies.</li></ul></li></ol><ol start=\"2\"><li><strong> Integration with InvokeModel API</strong><ul><li>Call the Amazon Bedrock InvokeModel API with the guardrail identifier in your request.</li><li>When you make the API call, Amazon Bedrock applies the specified guardrail to both the input and output.</li></ul></li></ol><ol start=\"3\"><li><strong> Guardrail evaluation process</strong></li></ol><ol><li><ol type=\"a\"><li>: Before sending the prompt to the model, the guardrail evaluates the user input against the configured policies.</li><li>: For improved latency, the input is evaluated in parallel for each configured policy.</li><li>: If the input violates any guardrail policies, a pre-configured blocked message is returned, and the model inference is discarded.</li><li>: If the input passes the guardrail checks, the prompt is sent to the specified model for inference.</li><li>: After the model generates a response, the guardrail evaluates the output against the configured policies.</li><li>: If the model response violates any guardrail policies, it will be either blocked with a pre-configured message or have sensitive information masked, depending on the policy.</li><li>: If the output passes all guardrail checks, the response is returned to the application without modifications</li></ol></li></ol><p>Before setting up guardrails for models imported using the Amazon Bedrock Custom Model Import feature, make sure you meet these prerequisites:</p><ul><li>An AWS account with access to Amazon Bedrock along with the necessary IAM role with the required permissions. For centralized access management, we recommend that you use <a href=\"https://aws.amazon.com/iam/identity-center/\" target=\"_blank\" rel=\"noopener\">AWS IAM Identity Center</a>.</li><li>Make sure that a custom model is already imported using the Amazon Bedrock Custom Model Import service. For illustration, we‚Äôll use DeepSeek-R1-Distill-Llama-8B, which can be imported using <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Custom Model Import</a>. You have two options for deploying this model: \n  </li></ul><p>You can create the guardrail using the AWS Management Console as explained in this <a href=\"https://aws.amazon.com/blogs/machine-learning/safeguard-a-generative-ai-travel-agent-with-prompt-engineering-and-amazon-bedrock-guardrails/\" target=\"_blank\" rel=\"noopener\">blog post</a>. Alternatively, you can follow this <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/blob/main/responsible_ai/bedrock-guardrails/guardrails_custom_model_import.ipynb\" target=\"_blank\" rel=\"noopener\">notebook</a> for a programmatic example of how to create the guardrail in this solution. This notebook does the following :</p><ol><li>Install the required dependencies</li><li>Create a guardrail using the boto3 API and filters to meet the use case mentioned previously.</li><li>Configure the tokenizer for the imported model.</li><li>Test Amazon Bedrock Guardrails using prompts that show various Amazon Bedrock guardrail filters in action.</li></ol><p>This approach integrates guardrails into both the user inputs and the model outputs. This makes sure that any potentially harmful or inappropriate content is intercepted during both phases of the interaction. For open weight distilled models imported using Amazon Bedrock Custom Model Import, Amazon Bedrock Marketplace, and Amazon SageMaker JumpStart, critical filters to implement include those for prompt attacks, content moderation, topic restrictions, and sensitive information protection.</p><h2>Implementing a defense-in-depth strategy with AWS services</h2><p>While Amazon Bedrock Guardrails provides essential content and prompt safety controls, implementing a comprehensive defense-in-depth strategy is crucial when deploying any foundation model, especially open weights models such as DeepSeek-R1. For detailed guidance on defense-in-depth approaches aligned with OWASP Top 10 for LLMs, see our previous <a href=\"https://aws.amazon.com/blogs/machine-learning/architect-defense-in-depth-security-for-generative-ai-applications-using-the-owasp-top-10-for-llms/\" target=\"_blank\" rel=\"noopener\">blog post</a> on architecting secure generative AI applications.</p><ul><li>Developing organizational resiliency by starting with security in mind</li><li>Building on a secure cloud foundation using AWS services</li><li>Applying a layered defense strategy across multiple trust boundaries</li><li>Addressing the OWASP Top 10 risks for LLM applications</li><li>Implementing security best practices throughout the AI/ML lifecycle</li><li>Using AWS security services in conjunction with AI and machine learning (AI/ML)-specific features</li><li>Considering diverse perspectives and aligning security with business objectives</li><li>Preparing for and mitigating risks such as prompt injection and data poisoning</li></ul><p>The combination of model-level controls (guardrails) with a defense-in-depth strategy creates a robust security posture that can help protect against:</p><ul><li>Data exfiltration attempts</li><li>Unauthorized access to fine-tuned models or training data</li><li>Potential vulnerabilities in model implementation</li><li>Malicious use of AI agents and integrations</li></ul><p>We recommend conducting thorough threat modeling exercises using <a href=\"https://aws.amazon.com/blogs/security/threat-modeling-your-generative-ai-workload-to-evaluate-security-risk/\" target=\"_blank\" rel=\"noopener\">AWS guidance for generative AI workloads</a> before deploying any new AI/ML solutions. This helps align security controls with specific risk scenarios and business requirements.</p><p>Implementing safety protection for LLMs, including DeepSeek-R1 models, is crucial for maintaining a secure and ethical AI environment. By using Amazon Bedrock Guardrails with the Amazon Bedrock InvokeModel API and the ApplyGuardrails API, you can help mitigate the risks associated with advanced language models while still harnessing their powerful capabilities. However, it‚Äôs important to recognize that model-level protections are just one component of a comprehensive security strategy.</p><p>The strategies outlined in this post address several key security concerns that are common across various open weights models hosted on Amazon Bedrock using Amazon Bedrock Custom Model Import, Amazon Bedrock Marketplace, and through Amazon SageMaker JumpStart. These include potential vulnerabilities to prompt injection attacks, the generation of harmful content, and other risks identified in recent assessments. By implementing these guardrails alongside a defense-in-depth approach, organizations can significantly reduce the risk of misuse and better align their AI applications with ethical standards and regulatory requirements.</p><p>As AI technology continues to evolve, it‚Äôs essential to prioritize safety and responsible use of generative AI. Amazon Bedrock Guardrails provides a configurable and robust framework for implementing these safeguards, allowing developers to customize protection measures according to their specific use cases and organizational policies. We strongly recommend conducting thorough threat modeling of your AI workloads using AWS guidance to evaluate security risks and implementing appropriate controls across your entire technology stack.</p><p>Remember to regularly review and update not only your guardrails but all security controls to address new potential vulnerabilities and help maintain protection against emerging threats in the rapidly evolving landscape of AI security. While today we focus on DeepSeek-R1 models, the AI landscape is continuously evolving with new models emerging regularly. Amazon Bedrock Guardrails, combined with AWS security services and best practices, provides a consistent security framework that can adapt to protect your generative AI applications across various open weights models, both current and future. By treating security as a continuous process of assessment, improvement, and adaptation, organizations can confidently deploy innovative AI solutions while maintaining robust security controls.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/06/Satveer-1.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Sr. WW Specialist Solutions Architect, Bedrock at Amazon Web Services. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer‚Äôs deep understanding of generative AI technologies allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/wale_picture_blog.png\" alt=\"\" width=\"100\" height=\"100\"> is a Sr. Data Scientist‚ÄìGenerative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/03/17/rodzanto.jpg\" alt=\"\" width=\"100\" height=\"127\"> is a Principal Generative AI Specialist Solutions Architect at Amazon Web Services. He helps companies of all sizes solve their challenges, embrace innovation, and create new business opportunities with Amazon Bedrock. Apart from work, he loves to spend time with his family and play sports with his friends.</p>","contentLength":14671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oracle dabbles in some JavaScript comedy","url":"https://javascriptweekly.com/issues/722","date":1738886400,"author":"","guid":600,"unread":true,"content":"<li><p>üç™ <a href=\"https://javascriptweekly.com/link/165495/rss\">CookieConsent 3.1</a> ‚Äì A lightweight, pure JS, GDPR-compliant cookie consent mechanism with which to  meet regulatory&nbsp;requirements.</p></li><li><p><a href=\"https://javascriptweekly.com/link/165496/rss\">Happy DOM 17.0</a> ‚Äì Cross-runtime JS implementation of a web browser &nbsp;UI. Now supports ES&nbsp;modules.</p></li><li><p><a href=\"https://javascriptweekly.com/link/165498/rss\">get-value 4.0</a> ‚Äì Use property paths () get a nested value from an object.</p></li><li><p><a href=\"https://javascriptweekly.com/link/165499/rss\">mp4-muxer 5.2</a> ‚Äì MP4 multiplexer in pure TypeScript with support for the WebCodecs API, video and&nbsp;audio.</p></li><li><p><a href=\"https://javascriptweekly.com/link/165503/rss\">Wasp 0.16</a> ‚Äì <a href=\"https://javascriptweekly.com/link/165504/rss\">Wasp</a> is a Rails-like framework using Node, React &amp;&nbsp;Prisma.</p></li><li><p><a href=\"https://javascriptweekly.com/link/165507/rss\">RxDB 16.5</a> ‚Äì Offline-first, reactive database for JS&nbsp;apps.</p></li>","contentLength":548,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From PDFs to Insights: Structured Outputs from PDFs with Gemini 2.0","url":"https://www.philschmid.de/gemini-pdf-to-data","date":1738886400,"author":"","guid":16,"unread":true,"content":"<article>Learn how to extract structured data from PDFs with Gemini 2.0 and Pydantic.</article>","contentLength":76,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Design a Real-Time Gaming Leaderboard - System Design Interview","url":"https://blog.algomaster.io/p/design-real-time-gaming-leaderboard","date":1738852955,"author":"Ashish Pratap Singh","guid":749,"unread":true,"content":"<p>A  is a ranked list of players, typically sorted by a specific metric such as score, points or level.</p><p> In a  leaderboard, updates happen almost instantly:</p><ol><li><p>A player‚Äôs score changes (e.g., after scoring a point or defeating an opponent).</p></li><li><p>The system updates that player‚Äôs rank immediately.</p></li><li><p>Other players can see the updated position without waiting or refreshing.</p></li></ol><p>This real-time aspect makes the user experience more dynamic and engaging. </p><p>However, it also introduces significant , such as:</p><ul><li><p>Efficiently retrieving Top-N players (e.g., Top 10 or Top 100).</p></li><li><p>Allowing players to quickly find their own rank without scanning the entire leaderboard.</p></li></ul><p>In this article, we will explore how to design a , , and  that can support above queries and enhance the user experience.</p><p>Before diving into the design, lets clearly define the functional and non-functional requirements of our real-time gaming leaderboard..</p><ul><li><p>: Display top N players (e.g., top 10, top 100) on the leaderboard and update it in real-time.</p></li><li><p>: Allow a player to query their current rank without scanning the entire leaderboard.</p></li><li><p> Provide the ability to retrieve a ‚Äúslice‚Äù of the leaderboard around a specific player (e.g., ranks 45 to 55 if the player is rank 50).</p></li><li><p> Players can view past game scores and historical leaderboards for previous matches.</p></li></ul><h3>Non-Functional Requirements</h3><ul><li><p> Score changes should reflect immediately in the leaderboard.</p></li><li><p>: Leaderboard queries should return the results in milliseconds.</p></li><li><p>: System should support thousands of concurrent players submitting scores and fetching rankings.</p></li></ul><h3>Approach to Designing the System</h3><p>The most challenging aspects of building a real-time leaderboard is . Choosing the right storage system is critical to ensuring that queries can be executed efficiently without performance bottlenecks.</p><p>To simplify the design process, we will follow below approach:</p><ol><li><p>Clearly define the input/output structure of leaderboard queries and updates.</p></li><li><p><strong>Define the High-Level Architecture: </strong>Identify core system components and their interactions.</p></li><li><p>Choose the appropriate storage model optimized for fast leaderboard lookups and real-time updates.</p></li></ol><p>To support real-time leaderboard operations, we define a set of  that allow players to update scores, retrieve rankings, and query nearby ranks efficiently.</p><p>Updates a player's score incrementally</p><p>: <code>POST /leaderboard/score/update</code></p><pre><code><code>{\n  \"playerId\": \"player123\",\n  \"scoreDelta\": 50\n}</code></code></pre><blockquote><p>We will use relative score updates () rather than absolute updates.</p></blockquote><pre><code><code>{\n  \"playerId\": \"player123\",\n  \"updatedScore\": 1000,\n  \"currentRank\": 10\n}</code></code></pre><p>Retrieves the top-N players from the leaderboard, ranked by their scores.</p><p>: <code>GET /leaderboard/top?n=10</code></p><p>:  = number of top players to fetch (default 10, max 100, etc.)</p><pre><code><code>{\n  \"leaderboardId\": \"global\",\n  \"topPlayers\": [\n    { \"playerId\": \"playerA\", \"score\": 1500, \"rank\": 1 },\n    { \"playerId\": \"playerB\", \"score\": 1490, \"rank\": 2 },\n    // ...\n  ]\n}</code></code></pre><p>Allows a player to retrieve their current rank without scanning the entire leaderboard.</p><p>: <code>GET /leaderboard/rank/{playerId}</code></p><pre><code><code>{\n  \"playerId\": \"player123\",\n  \"score\": 1000,\n  \"rank\": 10\n}</code></code></pre><p>Retrieves players ranked around a given player, allowing for comparison with competitors of similar skill levels.</p><p>: <code>GET /leaderboard/nearby/{playerId}?range=5</code></p><p>:  indicates how many ranks above and below to fetch (e.g., 5 above, 5 below).</p><pre><code><code>{\n  \"playerId\": \"player123\",\n  \"startRank\": 45,\n  \"endRank\": 55,\n  \"players\": [\n    { \"playerId\": \"playerX\", \"score\": 1020, \"rank\": 44 },\n    { \"playerId\": \"player123\", \"score\": 1000, \"rank\": 45 },\n    { \"playerId\": \"playerZ\", \"score\": 995,  \"rank\": 46 },\n    // ...\n  ]\n}</code></code></pre><p> If a player is ranked 50th, this API allows them to see players ranked 45-55 for a competitive comparison.</p><h3>2.5 WebSockets for Real-Time Updates</h3><p>While REST APIs are good for on-demand queries, WebSockets or Server-Sent Events (SSEs) can push real-time leaderboard updates to subscribed users.</p><p>Players would subscribe to leaderboard updates, and receive updates instantly without polling.</p>","contentLength":3933,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc07561-4165-42b9-a375-caaa33a2b6e3_2208x1668.png","enclosureMime":"","commentsUrl":null},{"title":"OpenAI‚Äôs Deep Research: Unexpected Game Changer!","url":"https://www.youtube.com/watch?v=Mhk-otOlo1c","date":1738852757,"author":"Two Minute Papers","guid":520,"unread":true,"content":"<article>‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nüìù Deep research:\nhttps://openai.com/index/introducing-deep-research/\n\nLambda DeepSeek instructions: https://docs.lambdalabs.com/education/large-language-models/deepseek-r1-ollama/\n\n- Open Deep Research: https://opendeepresearch.vercel.app/\n- Revisiting the McKinley Tariff of 1890 through the Lens of Modern Trade Theory\nhttps://kevinbryanecon.com/o3McKinley.pdf\n- Deep Research by  Tyler Cowen February 4, 2025\nhttps://marginalrevolution.com/marginalrevolution/2025/02/deep-research.html\n\nComplex tax situation: https://x.com/PatriceBTC/status/1886529037474127951\nDaily briefing: https://x.com/mckaywrigley/status/1886521397767467302\nPaper: https://x.com/Afinetheorem/status/1886206439582015870/photo/1\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu</article>","contentLength":1626,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Mhk-otOlo1c?version=3","enclosureMime":"","commentsUrl":null},{"title":"The DeepSeek Series: A Technical Overview","url":"https://martinfowler.com/articles/deepseek-papers.html","date":1738851420,"author":"Martin Fowler","guid":298,"unread":true,"content":"<p>The appearance of DeepSeek Large-Language Models has caused a lot of\n      discussion and angst since their latest versions appeared at the beginning\n      of 2025. But much of the value of DeepSeek's work comes from the papers\n      they have published over the last year. \n      provides <a href=\"https://martinfowler.com/articles/deepseek-papers.html\">an overview of these papers</a>, highlighting three main arcs in this\n      research: a focus on improving cost and memory efficiency, the use of HPC\n      Co-Design to train large models on limited hardware, and the development\n      of emergent reasoning from large-scale reinforcement learning</p>","contentLength":582,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quiz: Python \"for\" Loops: The Pythonic Way","url":"https://realpython.com/quizzes/python-for-loop/","date":1738843200,"author":"Real Python","guid":178,"unread":true,"content":"<p>By working through this quiz, you‚Äôll revisit how to iterate over items in a data collection, how to use  for a predefined number of iterations, and how to use  for index-based iteration.</p>","contentLength":188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quiz: How to Join Strings in Python","url":"https://realpython.com/quizzes/python-join-string/","date":1738843200,"author":"Real Python","guid":179,"unread":true,"content":"<p>By working through this quiz, you‚Äôll review how to use  to combine strings with a specified separator and handle non-string data in iterables. You‚Äôll also revisit how  compares to other concatenation methods and how it‚Äôs optimized in CPython.</p>","contentLength":248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust vs. Go (Golang): Performance (Only Standard Library)","url":"https://www.youtube.com/watch?v=2e9U5sQ835Y","date":1738836482,"author":"Anton Putra","guid":534,"unread":true,"content":"<article>Go vs Rust Performance Comparison.\n\nüî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)\n\nüçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&amp;si=p-UOaVM_6_SFx52H\n\nüëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience &amp; Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)\n‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra\n‚ñ∫  Twitter/X: https://twitter.com/antonvputra\n‚ñ∫  GitHub: https://github.com/antonputra\n‚ñ∫  Email: me@antonputra.com\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/242/lessons/242\n\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/422\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/423\nPR to improve Rust - https://github.com/antonputra/tutorials/pull/424\n\n#deno #bun #nodejs</article>","contentLength":1146,"flags":null,"enclosureUrl":"https://www.youtube.com/v/2e9U5sQ835Y?version=3","enclosureMime":"","commentsUrl":null},{"title":"Remembering the order of the train_test_split","url":"https://www.youtube.com/watch?v=wb4bKSHiDik","date":1738836414,"author":"probabl","guid":528,"unread":true,"content":"<article>The page for the `train_test_split` function is the most visited page on the scikit-learn website. The reason, we suspect, is because folks often forget the order of arrays that come out. We know that `X_train` comes first, but then what? Is it `y_train` or `X_test`? This video explains the reasoning and also does a deep dive on the implementation.\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\nBluesky: https://bsky.app/profile/probabl.bsky.social\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":735,"flags":null,"enclosureUrl":"https://www.youtube.com/v/wb4bKSHiDik?version=3","enclosureMime":"","commentsUrl":null},{"title":"Oracle Starts Laying Mines In JavaScript Trademark Battle","url":"https://developers.slashdot.org/story/25/02/05/2345236/oracle-starts-laying-mines-in-javascript-trademark-battle?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738805400,"author":"BeauHD","guid":362,"unread":true,"content":"The Register's Thomas Claburn reports: Oracle this week asked the US Patent and Trademark Office (USPTO) to partially dismiss a challenge to its JavaScript trademark. The move has been criticized as an attempt to either stall or water down legal action against the database goliath over the programming language's name. Deno Land, the outfit behind the Deno JavaScript runtime, filed a petition with the USPTO back in November in an effort to make the trademarked term available to the JavaScript community. This legal effort is led by Node.js creator and Deno Land CEO Ryan Dahl, summarized on the JavaScript.tm website, and supported by more than 16,000 members of the JavaScript community. It aims to remove the fear of an Oracle lawsuit for using the term \"JavaScript\" in a conference title or business venture.\n \n\"Programmers working with JavaScript have formed innumerable community organizations,\" the website explains. \"These organizations, like the standards bodies, have been forced to painstakingly avoid naming the programming language they are built around -- for example, JSConf. Sadly, without risking a legal trademark challenge against Oracle, there can be no 'JavaScript Conference' nor a 'JavaScript Specification.' The world's most popular programming language cannot even have a conference in its name.\" [...] In the initial trademark complaint, Deno Land makes three arguments to invalidate Oracle's ownership of \"JavaScript.\" The biz claims that JavaScript has become a generic term; that Oracle committed fraud in 2019 when it applied to renew its trademark; and that Oracle has abandoned its trademark because it does not offer JavaScript products or services.\n \nOracle's motion on Monday focuses on the dismissal of the fraud claim, while arguing that it expects to prevail on the other two claims, citing corporate use of the trademarked term \"in connection with a variety of offerings, including its JavaScript Extension Toolkit as well as developer's guides and educational resources, and also that relevant consumers do not perceive JavaScript as a generic term.\" The fraud claim follows from Deno Land's assertion that the material Oracle submitted in support of its trademark renewal application has nothing to do with any Oracle product. \"Oracle, through its attorney, submitted specimens showing screen captures of the Node.js website, a project created by Ryan Dahl, Petitioner's Chief Executive Officer,\" the trademark cancellation petition says. \"Node.js is not affiliated with Oracle, and the use of screen captures of the 'nodejs.org' website as a specimen did not show any use of the mark by Oracle or on behalf of Oracle.\"\n \nOracle contends that in fact it submitted two specimens to the USPTO -- a screenshot from the Node.js website and another from its own Oracle JavaScript Extension Toolkit. And this, among other reasons, invalidates the fraud claim, Big Red's attorneys contend. \"Where, as here, Registrant 'provided the USPTO with [two specimens]' at least one of which shows use of the mark in commerce, Petitioner cannot plausibly allege that the inclusion of a second, purportedly defective specimen, was material,\" Oracle's motion argues, adding that no evidence of fraudulent intent has been presented. Beyond asking the court to toss the fraud claim, Oracle has requested an additional thirty days to respond to the other two claims.","contentLength":3388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Read-only accessibility in TypeScript","url":"https://2ality.com/2025/02/typescript-readonly.html","date":1738800000,"author":"Dr. Axel Rauschmayer","guid":348,"unread":true,"content":"<p>In this blog post, we look at how can make things ‚Äúread-only‚Äù in TypeScript ‚Äì mainly via the keyword .</p>","contentLength":108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Running sklearn models in Zig","url":"https://zig.news/vinybrasil/running-sklearn-models-in-zig-2do1","date":1738795860,"author":"Vin√≠cyus Brasil","guid":755,"unread":true,"content":"<p>Scikit-learn models became the industry standard for creating machine learning models. Although the library offers many conveniences, one becomes tied to Python APIs for serving such models. These frameworks (such as FastAPI, for example) have various scalability issues, as Anton demonstrates in this video.\nTo overcome this limitation, one solution is to use Python's C library to run the models, while the rest of the API can be built using another framework.</p><p>In this blogpost we'll implement this strategy. We'll create an application that uses Python's C API to run the models with the <a href=\"https://github.com/zigzap/zap\">Zap</a> library handling the requests, which is a Zig <code>blazing fast microframework for web applications</code>. The full code to the post can be found in this <a href=\"https://github.com/vinybrasil/zig-sklearn\">repository</a> and this post was originally posted on my <a href=\"https://vinybrasil.github.io/blog/zig-sklearn/\">blog</a>.</p><h2>\n  \n  \n  Creating the model and the shared object\n</h2><p>So let's say we got this simple script called  that creates a Logistic Regression, in which the model takes a numpy array with two dimensions and returns the value 1 or 0.  The model is saved in a pickle file called .</p><div><pre><code></code></pre></div><p>The full library we are going to create just loads the pickle file and returns it in a list. It will attend by the name of  and it  module have the following code:</p><div><pre><code></code></pre></div><p>Quite simple, right? The next step is to compile it to a shared object, the kind of file that contains the code we just wrote. \nTheoretically this step is not needed but I think its way easier to work with the library this way. </p><p>To compile the library we can use Cython. The following  file will translate our library to C and generate the shared object.</p><div><pre><code></code></pre></div><p>Note that both of the files generated, libpredict.so and model.pkl, need to be moved to the same folder where we'll build the zig executable.</p><h2>\n  \n  \n  Running Python code in Zig\n</h2><p>To interact with the Python C API, we gotta load the header file that includes the Python functions. The  file starts with:</p><div><pre><code></code></pre></div><p>The syntax to call Python is in this <a href=\"https://docs.python.org/3/c-api/index.html\">documentation</a>. For exemple,\nto print a string, we first need to initialize the interpreter and use the function PyRun_SimpleString </p><div><pre><code></code></pre></div><p>The main.zig file will start this way. First, we initialize the interpreter. Then, load the module  and extracts from it a pointer to the function  from the . To call the function we need to convert the zig array  to a Python List also using the C API and that's the reason for the function  to exist. Finally, the function can be called using the converted list.</p><div><pre><code></code></pre></div><p>Note the loadMod() function appends the current path and from it import the  file so the loadFunc function can link a pointer to the predict function.</p><div><pre><code></code></pre></div><p>The prepareList function just creates a Python list with PyList_New and appends to it the values from the  arrayh.</p><div><pre><code></code></pre></div><p>The evaluteResult function iterates through all of the values of the Python function's response and the returns the last one. I've written the code this way because maybe what you are interested is the return of the predict_proba() function of the sklearn model, so it's easier to adapt the code.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Creating the API with Zap\n</h2><p>I've chosen Zap because it's a  micro webframework that can handle a lot of requests simultaneously and also it's quite intuitive.</p><p>To create the API, we need a Router object to map the routes to the functions that handle the requests. Our only route here is the  that receives a json payload with the fields  and  to send to the Python function. We'll also need a HttpListener object to listen to the requests on the port 3000. The struct PredictorPackage is where all of the handling of the request is made;</p><div><pre><code></code></pre></div><p>Inside the PredictorPackage struct, the function predictValue captures the body of the request, parses it with std.json.Parsed and then calls the Python function with it. Then, it stringify it and responds the request with a resultPrediction object.</p><div><pre><code></code></pre></div><p>After building the project () and moving the libpredict.so and the model.pkl to the same folder as the executable,\nwe can test the API with cURL:</p><div><pre><code>curl  POST http://localhost:3000/predict </code></pre></div><p>It should return the following response:</p><p>And that's it. Fell free to contact me via Linkedin or to open a PR on the GitHub repo if you find something wrong. Keep on learning :D</p>","contentLength":4121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revolutionizing software testing: Introducing LLM-powered bug catchers","url":"https://engineering.fb.com/2025/02/05/security/revolutionizing-software-testing-llm-powered-bug-catchers-meta-ach/","date":1738780251,"author":"","guid":470,"unread":true,"content":"<p><a href=\"https://arxiv.org/pdf/2501.12862\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://arxiv.org/abs/2402.04380\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://arxiv.org/pdf/2501.12862\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://web.eecs.umich.edu/~weimerw/2022-481F/readings/mutation-testing.pdf\" target=\"_blank\" rel=\"noopener\"></a></p><ol></ol><p><a href=\"https://arxiv.org/pdf/2501.12862\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://engineering.fb.com/2022/07/27/developer-tools/programming-languages-endorsed-for-server-side-use-at-meta/\" target=\"_blank\" rel=\"noopener\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gemini 2.0 is now available to everyone","url":"https://deepmind.google/discover/blog/gemini-2-0-is-now-available-to-everyone/","date":1738771200,"author":"","guid":474,"unread":true,"content":"<article>We‚Äôre announcing new updates to Gemini 2.0 Flash, plus introducing Gemini 2.0 Flash-Lite and Gemini 2.0 Pro Experimental.</article>","contentLength":123,"flags":null,"enclosureUrl":"https://lh3.googleusercontent.com/jOZg_YMBxrm-cz0kTZPehtQ29D90UZ21AXq9E54u3RZ67VEoXNy2KEsmD3oPgCDCIBw39ydczc8VQHH3QlUzIjdsgSPwIFdAEHXiHhsnUIKqmhppSQ=w528-h297-n-nu-rw","enclosureMime":"","commentsUrl":null},{"title":"GenAI Patterns: RAG Limitations and Hybrid Retriever","url":"https://martinfowler.com/articles/gen-ai-patterns/#RagInPractice","date":1738767780,"author":"Martin Fowler","guid":297,"unread":true,"content":"<p>Today  and I outline four\n      limitations to the simple RAG from yesterday, and the pattern that\n      addresses the first of these: Hybrid Retriever. This tackles the\n      inefficiencies of embeddings-based search by combining it with other\n      search techniques.</p>","contentLength":269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Split a String in Python","url":"https://realpython.com/python-split-string/","date":1738764000,"author":"Real Python","guid":177,"unread":true,"content":"<p>Python‚Äôs  method lets you divide a string into a list of substrings based on a specified delimiter. By default,  separates at whitespace, including spaces, tabs, and newlines. You can customize  to work with specific delimiters using the  parameter, and control the amount of splits with .</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li>You  in Python using <strong> without arguments</strong>.</li><li>Python‚Äôs  method can split on  when you pass a character or string as an argument.</li><li>You <strong>limit splits using </strong> to control the number of substrings Python extracts.</li><li> into individual lines, excluding or including line breaks with the  parameter.</li><li> uses  for splitting strings based on complex patterns.</li></ul><p>Exploring these methods will level up your text-processing capabilities and enable you to confidently tackle real-world data parsing challenges.</p><div><p> Test your knowledge with our interactive ‚ÄúHow to Split a String in Python‚Äù quiz. You‚Äôll receive a score upon completion to help you track your learning progress:</p><div><div><a href=\"https://realpython.com/quizzes/python-split-string/\"></a><p>In this quiz, you'll test your understanding of Python's .split() method. This method is useful for text-processing and data parsing tasks, allowing you to divide a string into a list of substrings based on a specified delimiter.</p></div></div></div><h2>How to Split a String in Python Using </h2><p>The  method in Python is a versatile tool that allows you to divide a string into a list of substrings based on a specified delimiter. By default,  separates a string at each occurrence of whitespace, which includes spaces, tabs, and newlines. This makes it particularly handy when dealing with plain text where words are separated by spaces:</p><p>As you can see,  divides the sentence into individual words, removing the spaces in the process. It‚Äôs important to note that  also strips newlines by default, which might not be immediately obvious:</p><p>The  method treats the newline characters as whitespace, and splits the text into individual words rather than lines. This is a crucial point to remember when <a href=\"https://realpython.com/python-split-string/#split-strings-by-lines-with-splitlines\">working with multiline strings</a>.</p><p>The simplicity of  makes it a powerful tool for string manipulation. Whether you‚Äôre processing text files or parsing user input, using  can streamline your workflow.</p><p>When you‚Äôre faced with the task of cleaning up messy text, you may want to pair  with . You can learn more about using  by expanding the collapsible section below:</p><div><div data-parent=\"#collapse_cardddefa9\"><div><p>You may encounter code that uses  in combination with another string method, :</p><p>In this example, you‚Äôre dealing with text that contains whitespace characters at the beginning and end of . You may see results like this when working with data that you received through <a href=\"https://realpython.com/web-scraping-with-scrapy-and-mongodb/\">web scraping</a>, copy-paste artifacts <a href=\"https://realpython.com/creating-modifying-pdf/\">from PDFs</a>, or even regular <a href=\"https://realpython.com/python-keyboard-input/\">user input</a>.</p><p>Combining  and  is a common and straightforward way to break potentially messy text into manageable <a href=\"https://realpython.com/natural-language-processing-spacy-python/#tokens-in-spacy\">tokens</a>.</p><p>The idea is that you first remove superfluous whitespace characters from the beginning and end of the string using . This operation returns a new string object without that whitespace. You then <a href=\"https://en.wikipedia.org/wiki/Method_chaining\">chain</a> to this cleaned output, allowing it to break the text into elements‚Äîin this case, individual words.</p><p>If you‚Äôre only dealing with messy whitespace, however, then you don‚Äôt need to use  to get to a clean result:</p><p>Because  splits on any whitespace and ignores extra leading and trailing whitespace, the result is the same as when you first explicitly remove leading and trailing whitespace with .</p><p>Nevertheless, there may be advantages to adding  into the mix:</p><ul><li> You clearly communicate to other developers that you intend to apply data cleaning‚Äîand <a href=\"https://realpython.com/zen-of-python/\">readability counts</a>!</li><li> You may later need to update your code to split on a different delimiter, in which case  won‚Äôt remove extra whitespace automatically. A well-designed additional step of explicitly removing whitespace with  can prevent surprises.</li></ul><p>If you‚Äôre only splitting on whitespace, then you likely won‚Äôt need to use . However, due to developer habits and a preference for explicitness, you may still come across this combination of string methods in the wild.</p></div></div></div><p>When you use , it returns a list of substrings. This means that you can iterate over the result, access individual elements using indexing, or <a href=\"https://realpython.com/python-assignment-operator/#unpacking-iterables\">unpack the iterable</a> into separate variables.</p><p>For example, when you‚Äôre dealing with user input, you might want to extract specific pieces of information:</p><p>In this code snippet, you take a string containing a name, surname, and age, and split it into a list of three separate strings. Then, you unpack the list into three descriptive variables. Finally, you use an <a href=\"https://realpython.com/python-f-strings/\">f-string</a> to format the output.</p><div role=\"alert\"><p> Extracting data pieces like this is useful when you‚Äôre dealing with structured data where you know the position of each element beforehand, and you can rely on consistent use of whitespace.</p></div><p>In this section, you‚Äôve learned how to use Python‚Äôs  method to divide strings into smaller parts based on whitespace. This method is invaluable when working with plain text data, allowing you to extract and manipulate information. By understanding the default behavior of , including its treatment of newlines, you‚Äôll be well-equipped to handle a variety of string manipulation tasks in your Python projects.</p><p>In the next section, you‚Äôll explore how to customize the behavior of  by specifying different delimiters, enabling you to tackle more complex string splitting scenarios.</p><h2>Split With Different Delimiters Using </h2><p>By default,  uses any whitespace as a delimiter to separate a string into a list of substrings. However, many real-world scenarios require splitting strings using other delimiters. This is where the  parameter comes into play.</p><p>Imagine that you‚Äôre parsing data from a CSV (comma-separated values) file. CSV files are a popular format for storing tabular data, where each line represents a row and each value within a line is separated by a comma.</p>","contentLength":5783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust 585","url":"https://this-week-in-rust.org/blog/2025/02/05/this-week-in-rust-585/","date":1738731600,"author":"TWiR Contributors","guid":400,"unread":true,"content":"<p>This week's crate is <a href=\"https://github.com/orhun/ratzilla\">ratzilla</a>, a library for building terminal-themed web applications with Rust and WebAssembly.</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.  The following\nRFCs would benefit from user testing before moving forward:</p><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p><em>No Calls for papers or presentations were submitted this week.</em></p><p>A very quiet week with performance of primary benchmarks showing no change over all.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr><td align=\"center\">Improvements ‚úÖ  (secondary)</td></tr><tr></tr></tbody></table><p>5 Regressions, 2 Improvements, 5 Mixed; 6 of them in rollups\n49 artifact comparisons made in total</p><ul><li><em>No RFCs were approved this week.</em></li></ul><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><ul><li><em>No RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Cargo Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Team Proposals entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Reference RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><p>Rusty Events between 2025-02-05 - 2025-03-05 ü¶Ä</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>If your rust code compiles and you don't use \"unsafe\", that is a pretty good certification.</p></blockquote><p>Thanks to <a href=\"https://users.rust-lang.org/t/twir-quote-of-the-week/328/1657\">ZiCog</a> for the suggestion!</p>","contentLength":2292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"crates.io: development update","url":"https://blog.rust-lang.org/2025/02/05/crates-io-development-update.html","date":1738713600,"author":"Tobias Bieniek","guid":240,"unread":true,"content":"<p>Back in July 2024, we published a <a href=\"https://blog.rust-lang.org/2024/07/29/crates-io-development-update.html\">blog post</a> about the ongoing development of crates.io. Since then, we have made a lot of progress and shipped a few new features. In this blog post, we want to give you an update on the latest changes that we have made to crates.io.</p><p>In <a href=\"https://rust-lang.github.io/rfcs/3660-crates-io-crate-deletions.html\">RFC #3660</a> we proposed a new feature that allows crate owners to delete their crates from crates.io under certain conditions. This can be useful if you have published a crate by mistake or if you want to remove a crate that is no longer maintained. After the RFC was accepted by all team members at the end of August, we began implementing the feature.</p><p>We created a new API endpoint <code>DELETE /api/v1/crates/:name</code> that allows crate owners to delete their crates and then created the corresponding user interface. If you are the owner of a crate, you can now go to the crate page, open the \"Settings\" tab, and find the \"Delete this crate\" button at the bottom. Clicking this button will lead you to a confirmation page telling you about the potential impact of the deletion and requirements that need to be met in order to delete the crate:</p><p>As you can see from the screenshot above, a crate can only be deleted if either: the crate has been published for less than 72 hours or the crate only has a single owner, and the crate has been downloaded less than 500 times for each month it has been published, and the crate is not depended upon by any other crate on crates.io.</p><p>These requirements were put in place to prevent abuse of the deletion feature and to ensure that crates that are widely used by the community are not deleted accidentally. If you have any feedback on this feature, please let us know!</p><p>Around the holiday season we started experimenting with generating an <a href=\"https://www.openapis.org/\">OpenAPI</a> description for the crates.io API. This was a long-standing request from the community, and we are happy to announce that we now have an experimental OpenAPI description available at <a href=\"https://crates.io/api/openapi.json\">https://crates.io/api/openapi.json</a>!</p><p>Please note that this is still considered work-in-progress and e.g. the stability guarantees for the endpoints are not written down and the response schemas are also not fully documented yet.</p><p>You can view the OpenAPI description in e.g. a Swagger UI at <a href=\"https://petstore.swagger.io/\">https://petstore.swagger.io/</a> by putting <code>https://crates.io/api/openapi.json</code> in the top input field. We decided to not ship a viewer ourselves for now due to security concerns with running it on the same domain as crates.io itself. We may reconsider whether to offer it on a dedicated subdomain in the future if there is enough interest.</p><p>The OpenAPI description is generated by the <a href=\"https://github.com/juhaku/utoipa\">utoipa</a> crate, which is a tool that can be integrated with the <a href=\"https://github.com/tokio-rs/axum\">axum</a> web framework to automatically generate OpenAPI descriptions for all of your endpoints. We would like to thank <a href=\"https://github.com/juhaku\">Juha Kukkonen</a> for his great work on this tool!</p><h2><a href=\"https://blog.rust-lang.org/2025/02/05/crates-io-development-update.html#support-form-and-report-crate-button\" aria-hidden=\"true\"></a>Support form and \"Report Crate\" button</h2><p>Since the crates.io team is small and mostly consists of volunteers, we do not have the capacity to manually monitor all publishes. Instead, we rely on you, the Rust community, to help us catch malicious crates and users. To make it easier for you to report suspicious crates, we added a \"Report Crate\" button to all the crate pages. If you come across a crate that you think is malicious or violates the <a href=\"https://www.rust-lang.org/policies/code-of-conduct\">code of conduct</a> or our <a href=\"https://crates.io/policies\">usage policy</a>, you can now click the \"Report Crate\" button and fill out the form that appears. This will send an email to the crates.io team, who will then review the crate and take appropriate action if necessary. Thank you to crates.io team member <a href=\"https://github.com/eth3lbert\">@eth3lbert</a> who worked on the majority of this.</p><p>If you have any issues with the support form or the \"Report Crate\" button, please let us know. You can also always email us directly at <a href=\"mailto:help@crates.io\">help@crates.io</a> if you prefer not to use the form.</p><p>We have added a new feature that allows you to receive email notifications when a new version of your crate is published. This can be useful in detecting unauthorized publishes of your crate or simply to keep track of publishes from other members of your team.</p><p>This feature was another <a href=\"https://github.com/rust-lang/crates.io/issues/815\">long-standing feature request</a> from our community, and we were happy to finally implement it. If you'd prefer not to receive publish notifications, then you can go to your account settings on crates.io and disable these notifications.</p><p>These were some of the more visible changes to crates.io over the past couple of months, but a lot has happened \"under the hood\" as well.</p><ul><li><p><a href=\"https://rust-lang.github.io/rfcs/3691-trusted-publishing-cratesio.html\">RFC #3691</a> was opened and accepted to implement \"Trusted Publishing\" support on crates.io, similar to other ecosystems that adopted it. This will allow you to specify on crates.io which repository/system is allowed to publish new releases of your crate, allowing you to publish crates from CI systems without having to deal with API tokens anymore.</p></li><li><p>Slightly related to the above: API tokens created on crates.io now expire after 90 days by default. It is still possible to disable the expiry or choose other expiry durations though.</p></li><li><p>The crates.io team was one of the first projects to use the <a href=\"https://diesel.rs/\">diesel</a> database access library, but since that only supported synchronous execution it was sometimes a little awkward to use in our codebase, which was increasingly moving into an async direction after our migration to <a href=\"https://github.com/tokio-rs/axum\">axum</a> a while ago. The maintainer of diesel, <a href=\"https://github.com/weiznich\">Georg Semmler</a>, did a lot of work to make it possible to use diesel in an async way, resulting in the <a href=\"https://github.com/weiznich/diesel_async\">diesel-async</a> library. Over the past couple of months we incrementally ported crates.io over to  queries, which now allows us to take advantage of the internal query pipelining in  that resulted in some of our API endpoints getting a 10-15% performance boost. Thank you, Georg, for your work on these crates!</p></li><li><p>Whenever you publish a new version or yank/unyank existing versions a couple of things need to be updated. Our internal database is immediately updated, and then we synchronize the sparse and git index in background worker jobs. Previously, yanking and unyanking a high number of versions would each queue up another synchronization background job. We have now implemented automatic deduplication of redundant background jobs, making our background worker a bit more efficient.</p></li><li><p>The final big, internal change that was just merged last week is related to the testing of our frontend code. In the past we used a tool called <a href=\"https://miragejs.com/\">Mirage</a> to implement a mock version of our API, which allowed us to run our frontend test suite without having to spin up a full backend server. Unfortunately, the maintenance situation around Mirage had lately forced us to look into alternatives, and we are happy to report that we have now fully migrated to the \"Industry standard API mocking\" package <a href=\"https://mswjs.io/\">msw</a>. If you want to know more, you can find the details in the \"small\" <a href=\"https://github.com/rust-lang/crates.io/pull/10393\">migration pull request</a>.</p></li></ul><p>We hope you enjoyed this update on the development of crates.io. If you have any feedback or questions, please let us know on <a href=\"https://rust-lang.zulipchat.com/#narrow/stream/318791-t-crates-io\">Zulip</a> or <a href=\"https://github.com/rust-lang/crates.io/discussions\">GitHub</a>. We are always happy to hear from you and are looking forward to your feedback!</p>","contentLength":6967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"sync/v2 and the 'v2'-ification of Go's standard library","url":"https://golangweekly.com/issues/541","date":1738713600,"author":"","guid":596,"unread":true,"content":"<p> ‚Äî It uses Linux-only network namespaces for now, but this is a Go-powered process-scoped HTTP tracer you can run  root privileges. It even decrypts TLS traffic by generating a CA on the fly. Handy for debugging or seeing if dependencies or apps are ‚Äòphoning&nbsp;home,‚Äô&nbsp;perhaps..</p><p>Monastic Academy for the Preservation of Life on Earth </p>","contentLength":338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data logs: The latest evolution in Meta‚Äôs access tools","url":"https://engineering.fb.com/2025/02/04/security/data-logs-the-latest-evolution-in-metas-access-tools/","date":1738699218,"author":"","guid":469,"unread":true,"content":"<ul></ul><p><a href=\"https://www.facebook.com/help/384437594328726#data-logs\"></a><a href=\"https://www.facebook.com/help/212802592074644\"></a><a href=\"https://www.facebook.com/help/1700142396915814\"></a><a href=\"https://www.facebook.com/help/256333951065527\"></a></p><ul><li aria-level=\"1\"><a href=\"https://www.facebook.com/help/256333951065527\"></a></li><li aria-level=\"1\"><a href=\"https://www.facebook.com/help/794535777607370#advertiser-choices\"></a></li><li aria-level=\"1\"><a href=\"https://engineering.fb.com/2019/12/02/security/data-transfer-project/\"></a></li><li aria-level=\"1\"><a href=\"https://about.fb.com/news/2020/03/data-access-tools/\"></a></li><li aria-level=\"1\"><a href=\"https://www.facebook.com/help/943858526073065\"></a></li></ul><p><a href=\"https://atscaleconference.com/workflowsfacebook-powering-developer-productivity-and-automation-at-facebook-scale/\"></a></p><p><a href=\"https://www.youtube.com/watch?v=4T-MCYWrrOw\"></a><a href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\"></a></p><h2>Lessons learned from building data logs</h2><h2>Making data consistently understandable and explainable</h2><p><a href=\"https://transparency.meta.com/en-gb/ncmec-q2-2023/\"></a></p><p><a href=\"https://docs.hhvm.com/hack/built-in-types/enum\"></a></p><p><a href=\"https://about.fb.com/news/2020/06/introducing-manage-activity/\"></a></p>","contentLength":94,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python 3.13.2 and 3.12.9 now available!","url":"https://pythoninsider.blogspot.com/2025/02/python-3132-and-3129-now-available.html","date":1738699080,"author":"Thomas Wouters","guid":263,"unread":true,"content":"<p>A small release day today! That is to say the releases are relatively \nsmall; the day itself was of average size, as most days are.</p><div><p>Python 3.13‚Äôs second maintenance release. About 250 changes went into \nthis update, and can be yours for free if you just upgrade now.</p><p>Python 3.12‚Äôs  maintenance release already. Just 180 changes for 3.12, but it‚Äôs still worth upgrading.</p><p>Thanks to all of the many volunteers who help make Python Development\n and these releases possible! Please consider supporting our efforts by \nvolunteering yourself or through organization contributions to the \nPython Software Foundation.</p><p>Regards from your tireless, tireless release team,\nThomas Wouters \nSteve Dower </p></div>","contentLength":690,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Updating the Frontier Safety Framework","url":"https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/","date":1738687260,"author":"","guid":473,"unread":true,"content":"<article>Our next iteration of the FSF sets out stronger security protocols on the path to AGI</article>","contentLength":85,"flags":null,"enclosureUrl":"https://lh3.googleusercontent.com/0fu18H8X3miSAuwcVJ7Zulis_LZAL7F4bIFU7FYFA2dGx3Rm3HHlm5N202B0dtKBuS7iI5SD1QgpFPuU-O3TPzb7iG1Ns-loZzinRB3M3X3W-MAgIQ=w528-h297-n-nu-rw","enclosureMime":"","commentsUrl":null},{"title":"Injecting Domain Expertise Into Your AI System","url":"https://www.topbots.com/injecting-domain-expertise-into-ai-system/","date":1738683457,"author":"Janna Lipenkova","guid":593,"unread":true,"content":"<p>When starting their AI initiatives, many companies are trapped in silos and treat AI as a purely technical enterprise, sidelining domain experts or involving them too late. They end up with generic AI applications that miss industry nuances, produce poor recommendations, and quickly become unpopular with users. By contrast, AI systems that deeply understand industry-specific processes, constraints, and decision logic have the following benefits:</p><ul><li>&nbsp;‚Äî The more domain knowledge AI incorporates, the less manual effort is required from human experts.</li><li>&nbsp;‚Äî Experts disengage from AI systems that feel too generic. AI must speak their language and align with real workflows to gain trust.</li><li>A&nbsp;<strong>sustainable competitive moat</strong>&nbsp;‚Äî As AI becomes a commodity, embedding proprietary expertise is the most effective way to build defensible AI systems (cf.&nbsp;<a href=\"https://www.topbots.com/competitive-advantage-with-ai/\" target=\"_blank\" rel=\"noreferrer noopener\">this article</a>&nbsp;to learn about the building blocks of AI‚Äôs competitive advantage).</li></ul><p>Domain experts can help you connect the dots between the technicalities of an AI system and its real-life usage and value. Thus, they should be key stakeholders and co-creators of your AI applications. This guide is the first part of my series on expertise-driven AI. Following my&nbsp;<a href=\"https://www.topbots.com/building-ai-products-with-a-holistic-mental-model/\" target=\"_blank\" rel=\"noreferrer noopener\">mental model of AI systems</a>, it provides a structured approach to embedding deep domain expertise into your AI.</p><p>Throughout the article, we will use the use case of supply chain optimisation (SCO) to illustrate these different methods. Modern supply chains are under constant strain from geopolitical tensions, climate disruptions, and volatile demand shifts, and AI can provide the kind of dynamic, high-coverage intelligence needed to anticipate delays, manage risks, and optimise logistics. However, without domain expertise, these systems are often disconnected from the realities of life. Let‚Äôs see how we can solve this by integrating domain expertise across the different components of the AI application.</p><p>AI is only as domain-aware as the data it learns from. Raw data isn‚Äôt enough ‚Äî it must be curated, refined, and contextualised by experts who understand its meaning in the real world.</p><h2>Data understanding: Teaching AI what matters</h2><p>While data scientists can build sophisticated models to analyse patterns and distributions, these analyses often stay at a theoretical, abstract level. Only domain experts can validate whether the data is complete, accurate, and representative of real-world conditions.</p><p>In supply chain optimisation, for example, shipment records may contain missing delivery timestamps, inconsistent route details, or unexplained fluctuations in transit times. A data scientist might discard these as noise, but a logistics expert could have real-world explanations of these inconsistencies. For instance, they might be caused by weather-related delays, seasonal port congestion, or supplier reliability issues. If these nuances aren‚Äôt accounted for, the AI might learn an overly simplified view of supply chain dynamics, resulting in misleading risk assessments and poor recommendations.</p><p>Experts also play a critical role in assessing the completeness of data. AI models work with what they have, assuming that all key factors are already present. It takes human expertise and judgment to identify blind spots. For example, if your supply chain AI isn‚Äôt trained on customs clearance times or factory shutdown histories, it won‚Äôt be able to predict disruptions caused by regulatory issues or production bottlenecks.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Run joint Exploratory Data Analysis (EDA) sessions with data scientists and domain experts to identify missing business-critical information, ensuring AI models work with a complete and meaningful dataset, not just statistically clean data.</p><p>One common pitfall when starting with AI is integrating too much data too soon, leading to complexity, congestion of your data pipelines, and blurred or noisy insights. Instead, start with a couple of high-impact data sources and expand incrementally based on AI performance and user needs. For instance, an SCO system may initially use historical shipment data and supplier reliability scores. Over time, domain experts may identify missing information ‚Äî such as port congestion data or real-time weather forecasts ‚Äî and point engineers to those data sources where it can be found.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Start with a minimal, high-value dataset (normally 3‚Äì5 data sources), then expand incrementally based on expert feedback and real-world AI performance.</p><p>AI models learn by detecting patterns in data, but sometimes, the right learning signals aren‚Äôt yet present in raw data. This is where data annotation comes in ‚Äî by labelling key attributes, domain experts help the AI understand what matters and make better predictions. Consider an AI model built to predict supplier reliability. The model is trained on shipment records, which contain delivery times, delays, and transit routes. However, raw delivery data alone doesn‚Äôt capture the full picture of supplier risk ‚Äî there are no direct labels indicating whether a supplier is ‚Äúhigh risk‚Äù or ‚Äúlow risk.‚Äù</p><p>Without more explicit learning signals, the AI might make the wrong conclusions. It could conclude that all delays are equally bad, even when some are caused by predictable seasonal fluctuations. Or it might overlook early warning signs of supplier instability, such as frequent last-minute order changes or inconsistent inventory levels.</p><p>Domain experts can enrich the data with more nuanced labels, such as supplier risk categories, disruption causes, and exception-handling rules. By introducing these curated learning signals, you can ensure that AI doesn‚Äôt just memorise past trends but learns meaningful, decision-ready insights.</p><p>You shouldn‚Äôt rush your annotation efforts ‚Äî instead, think about a structured annotation process that includes the following components:</p><ul><li>&nbsp;Establish clear, standardized rules for labeling data to ensure consistency. For example, supplier risk categories should be based on defined thresholds (e.g., delivery delays over 5 days + financial instability = high risk).</li><li>&nbsp;Involve several domain experts to reduce bias and ensure objectivity, particularly for subjective classifications like risk levels or disruption impact.</li><li>&nbsp;Capture both direct and contextual factors, such as annotating not just shipment delays but also the cause (customs, weather, supplier fault).</li><li>&nbsp;Regularly audit and refine annotations based on AI performance ‚Äî if predictions consistently miss key risks, experts should adjust labelling strategies accordingly.</li></ul><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Define an annotation playbook with clear labelling criteria, involve at least two domain experts per critical label for objectivity, and run regular annotation review cycles to ensure AI is learning from accurate, business-relevant insights.</p><p>So far, our AI models learn from real-life historical data. However, rare, high-impact events ‚Äî like factory shutdowns, port closures, or regulatory shifts in our supply chain scenario ‚Äî may be underrepresented. Without exposure to these scenarios, AI can fail to anticipate major risks, leading to overconfidence in supplier stability and poor contingency planning. Synthetic data solves this by creating more datapoints for rare events, but expert oversight is crucial to ensure that it reflects plausible risks rather than unrealistic patterns.</p><p>Let‚Äôs say we want to predict supplier reliability in our supply chain system. The historical data may have few recorded supplier failures ‚Äî but that‚Äôs not because failures don‚Äôt happen. Rather, many companies proactively mitigate risks before they escalate. Without synthetic examples, AI might deduce that supplier defaults are extremely rare, leading to misguided risk assessments.</p><p>Experts can help generate synthetic failure scenarios based on:</p><ul><li>&nbsp;‚Äî Simulating supplier collapses triggered by economic downturns, regulatory shifts, or geopolitical tensions.</li><li>&nbsp;‚Äî Training AI on unrecorded early warning signs, like financial instability or leadership changes.</li><li>&nbsp;‚Äî Creating ‚Äúwhat-if‚Äù events, such as a semiconductor supplier suddenly halting production or a prolonged port strike.</li></ul><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Work with domain experts to define high-impact but low-frequency events and scenarios, which can be in focus when you generate synthetic data.</p><p>Data makes domain expertise shine. An AI initiative that relies on clean, relevant, and enriched domain data will have an obvious competitive advantage over one that takes the ‚Äúquick-and-dirty‚Äù shortcut to data. However, keep in mind that working with data can be tedious, and experts need to see the outcome of their efforts ‚Äî whether it‚Äôs improving AI-driven risk assessments, optimising supply chain resilience, or enabling smarter decision-making. The key is to make data collaboration intuitive, purpose-driven, and directly tied to business outcomes, so experts remain engaged and motivated.</p><p>Once AI has access to high-quality data, the next challenge is ensuring it generates useful and accurate outputs. Domain expertise is needed to:</p><ol><li>Define&nbsp;&nbsp;aligned with business priorities</li><li>Ensure AI&nbsp;&nbsp;industry-specific data</li><li>Continuously validate AI‚Äôs&nbsp;<strong>outputs and recommendations</strong></li></ol><p>Let‚Äôs look at some common AI approaches and see how they can benefit from an extra shot of domain knowledge.</p><h2>Training predictive models from scratch</h2><p>For structured problems like supply chain forecasting, predictive models such as classification and regression can help anticipate delays and suggest optimisations. However, to make sure these models are aligned with business goals, data scientists and knowledge engineers need to work together. For example, an AI model might try to minimise shipment delays at all costs, but a supply chain expert knows that fast-tracking every shipment through air freight is financially unsustainable. They can formulate additional constraints on the model, making it prioritise critical shipments while balancing cost, risk, and lead times.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Define clear objectives and constraints with domain experts before training AI models, ensuring alignment with real business priorities.</p><p>While predictive models trained from scratch can excel at very specific tasks, they are also rigid and will ‚Äúrefuse‚Äù to perform any other task. GenAI models are more open-minded and can be used for highly diverse requests. For example, an LLM-based conversational widget in an SCO system can allow users to interact with real-time insights using natural language. Instead of sifting through inflexible dashboards, users can ask,&nbsp;<em>‚ÄúWhich suppliers are at risk of delays?‚Äù</em>&nbsp;or&nbsp;<em>‚ÄúWhat alternative routes are available?‚Äù</em>&nbsp;The AI pulls from historical data, live logistics feeds, and external risk factors to provide actionable answers, suggest mitigations, and even automate workflows like rerouting shipments.</p><p>But how can you ensure that a huge, out-of-the-box model like ChatGPT or Llama understands the nuances of your domain? Let‚Äôs walk through the LLM triad ‚Äî a progression of techniques to incorporate domain knowledge into your LLM system.</p><p>As you progress from left to right, you can ingrain more domain knowledge into the LLM ‚Äî however, each stage also adds new technical challenges (if you are interested in a systematic deep-dive into the LLM triad, please check out chapters 5‚Äì8 of my book&nbsp;<a href=\"https://www.manning.com/books/the-art-of-ai-product-management\" target=\"_blank\" rel=\"noreferrer noopener\">The Art of AI Product Management</a>). Here, let‚Äôs focus on how domain experts can jump in at each of the stages:</p><ol><li>&nbsp;out-of-the-box LLMs might seem like a generic approach, but with the right intuition and skill, domain experts can fine-tune prompts to extract the extra bit of domain knowledge out of the LLM. Personally, I think this is a big part of the fascination around prompting ‚Äî it puts the most powerful AI models directly into the hands of domain experts without any technical expertise. Some key prompting techniques include:</li></ol><ul><li>&nbsp;Incorporate examples to guide the model‚Äôs responses. Instead of just asking&nbsp;<em>‚ÄúWhat are alternative shipping routes?‚Äù</em>, a well-crafted prompt includes sample scenarios, such as&nbsp;<em>‚ÄúExample of past scenario: A previous delay at the Port of Shenzhen was mitigated by rerouting through Ho Chi Minh City, reducing transit time by 3 days.‚Äù</em></li><li><strong>Chain-of-thought prompting:</strong>&nbsp;Encourage step-by-step reasoning for complex logistics queries. Instead of&nbsp;<em>‚ÄúWhy is my shipment delayed?‚Äù</em>, a structured prompt might be&nbsp;<em>‚ÄúAnalyse historical delivery data, weather reports, and customs processing times to determine why shipment #12345 is delayed.‚Äù</em></li><li><strong>Providing further background information:</strong>&nbsp;Attach external documents to improve domain-specific responses. For example, prompts could reference real-time port congestion reports, supplier contracts, or risk assessments to generate data-backed recommendations. Most LLM interfaces already allow you to conveniently attach additional files to your prompt.</li></ul><p><strong>2. RAG (Retrieval-Augmented Generation)</strong>: While prompting helps guide AI, it still relies on pre-trained knowledge, which may be outdated or incomplete. RAG allows AI to retrieve real-time, company-specific data, ensuring that its responses are grounded in current logistics reports, supplier performance records, and risk assessments. For example, instead of generating generic supplier risk analyses, a RAG-powered AI system would pull real-time shipment data, supplier credit ratings, and port congestion reports before making recommendations. Domain experts can help select and structure these data sources and are also needed when it comes to testing and evaluating RAG systems.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Work with domain experts to curate and structure knowledge sources ‚Äî ensuring AI retrieves and applies only the most relevant and high-quality business information.</p><p>: While prompting and RAG inject domain knowledge on-the-fly, they do not inherently embed supply domain-specific workflows, terminology, or decision logic into your LLM. Fine-tuning adapts the LLM to think like a logistics expert. Domain experts can guide this process by creating high-quality training data, ensuring AI learns from real supplier assessments, risk evaluations, and procurement decisions. They can refine industry terminology to prevent misinterpretations (<em>e.g., AI distinguishing between ‚Äúbuffer stock‚Äù and ‚Äúsafety stock‚Äù</em>). They also align AI‚Äôs reasoning with business logic, ensuring it considers cost, risk, and compliance ‚Äî not just efficiency. Finally, they evaluate fine-tuned models, testing AI against real-world decisions to catch biases or blind spots.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">: In LLM fine-tuning, data is the crucial success factor. Quality goes over quantity, and fine-tuning on a small, high-quality dataset can give you excellent results. Thus, give your experts enough time to figure out the right structure and content of the fine-tuning data and plan for plenty of end-to-end iterations of your fine-tuning process.</p><h2>Encoding expert knowledge with neuro-symbolic AI</h2><p>Every machine learning algorithm gets it wrong from time to time. To mitigate errors, it helps to set the ‚Äúhard facts‚Äù of your domain in stone, making your AI system more reliable and controllable. This combination of machine learning and deterministic rules is called neuro-symbolic AI.</p><p>For example, an explicit knowledge graph can encode supplier relationships, regulatory constraints, transportation networks, and risk dependencies in a structured, interconnected format.</p><p>Instead of relying purely on statistical correlations, an AI system enriched with knowledge graphs can:</p><ul><li>Validate predictions against domain-specific rules (e.g., ensuring that AI-generated supplier recommendations comply with regulatory requirements).</li><li>Infer missing information (e.g., if a supplier has no historical delays but shares dependencies with high-risk suppliers, AI can assess its potential risk).</li><li>Improve explainability by allowing AI decisions to be traced back to logical, rule-based reasoning rather than black-box statistical outputs.</li></ul><p>How can you decide which knowledge should be encoded with rules (&nbsp;AI), and which should be learned dynamically from the data (&nbsp;AI)? Domain experts can help youpick those bits of knowledge where hard-coding makes the most sense:</p><ul><li>Knowledge that is relatively stable over time</li><li>Knowledge that is hard to infer from the data, for example because it is not well-represented</li><li>Knowledge that is critical for high-impact decisions in your domain, so you can‚Äôt afford to get it wrong</li></ul><p>In most cases, this knowledge will be stored in separate components of your AI system, like decision trees, knowledge graphs, and ontologies. There are also some methods to integrate it directly into LLMs and other statistical models, such as&nbsp;<a href=\"https://www.lamini.ai/blog/lamini-memory-tuning\" rel=\"noreferrer noopener\" target=\"_blank\">Lamini‚Äôs memory fine-tuning</a>.</p><h2>Compound AI and modular workflows</h2><p>Generating insights and turning them into actions is a multi-step process. Experts can help you model workflows and decision-making pipelines, ensuring that the process followed by your AI system aligns with their tasks. For example, the following pipeline shows how the AI components we considered so far can be combined into a modular workflow for the mitigation of shipment risks:</p><p>Experts are also needed to calibrate the ‚Äúlabor distribution‚Äù between humans in AI. For example, when modelling decision logic, they can set thresholds for automation, deciding when AI can trigger workflows versus when human approval is needed.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Involve your domain experts in mapping your processes to AI models and assets, identifying gaps vs. steps that can already be automated.</p><p>Especially in B2B environments, where workers are deeply embedded in their daily workflows, the user experience must be seamlessly integrated with existing processes and task structures to ensure efficiency and adoption. For example, an AI-powered supply chain tool must align with how logistics professionals think, work, and make decisions. In the development phase, domain experts are the closest ‚Äúpeers‚Äù to your real users, and picking their brains is one of the fastest ways to bridge the gap between AI capabilities and real-world usability.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Involve domain experts early in UX design to ensure AI interfaces are intuitive, relevant, and tailored to real decision-making workflows.</p><h2>Ensuring transparency and trust in AI decisions</h2><p>AI thinks differently from humans, which makes us humans skeptical. Often, that‚Äôs a good thing since it helps us stay alert to potential mistakes. But distrust is also one of the biggest barriers to AI adoption. When users don‚Äôt understand why a system makes a particular recommendation, they are less likely to work with it. Domain experts can define how AI should explain itself ‚Äî ensuring users have visibility into confidence scores, decision logic, and key influencing factors.</p><p>For example, if an SCO system recommends rerouting a shipment, it would be irresponsible on the part of a logistics planner to just accept it. She needs to see the ‚Äúwhy‚Äù behind the recommendation ‚Äî is it due to supplier risk, port congestion, or fuel cost spikes? The UX should show a breakdown of the decision, backed by additional information like historical data, risk factors, and a cost-benefit analysis.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/26a0.png\" alt=\"‚ö†\"><strong>Mitigate overreliance on AI</strong>: Excessive dependence of your users on AI can introduce bias, errors, and unforeseen failures. Experts should find ways to calibrate AI-driven insights vs. human expertise, ethical oversight, and strategic safeguards to ensure resilience, adaptability, and trust in decision-making.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Work with domain experts to define key explainability features ‚Äî such as confidence scores, data sources, and impact summaries ‚Äî so users can quickly assess AI-driven recommendations.</p><h2>Simplifying AI interactions without losing depth</h2><p>AI tools should make complex decisions easier, not harder. If users need deep technical knowledge to extract insights from AI, the system has failed from a UX perspective. Domain experts can help strike a balance between simplicity and depth, ensuring the interface provides actionable, context-aware recommendations while allowing deeper analysis when needed.</p><p>For instance, instead of forcing users to manually sift through data tables, AI could provide pre-configured reports based on common logistics challenges. However, expert users should also have on-demand access to raw data and advanced settings when necessary. The key is to design AI interactions that are efficient for everyday use but flexible for deep analysis when required.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">&nbsp;Use domain expert feedback to define default views, priority alerts, and user-configurable settings, ensuring AI interfaces provide both efficiency for routine tasks and depth for deeper research and strategic decisions.</p><h2>Continuous UX testing and iteration with experts</h2><p>AI UX isn‚Äôt a one-and-done process ‚Äî it needs to evolve with real-world user feedback. Domain experts play a key role in UX testing, refinement, and iteration, ensuring that AI-driven workflows stay aligned with business needs and user expectations.</p><p>For example, your initial interface may surface too many low-priority alerts, leading to alert fatigue where users start ignoring AI recommendations. Supply chain experts can identify which alerts are most valuable, allowing UX designers to prioritize high-impact insights while reducing noise.</p><p><img src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/2705.png\" alt=\"‚úÖ\">: Conduct&nbsp;<a href=\"https://www.nngroup.com/articles/thinking-aloud-the-1-usability-tool/\" rel=\"noreferrer noopener\" target=\"_blank\">think-aloud sessions</a>&nbsp;and have domain experts verbalize their thought process when interacting with your AI interface. This helps AI teams uncover hidden assumptions and refine AI based on how experts actually think and make decisions.</p><p>Vertical AI systems must integrate domain knowledge at every stage, and experts should become key stakeholders in your AI development:</p><ul><li>They refine data selection, annotation, and synthetic data.</li><li>They guide AI learning through prompting, RAG, and fine-tuning.</li><li>They support the design of seamless user experiences that integrate with daily workflows in a transparent and trustworthy way.</li></ul><p>An AI system that ‚Äúgets‚Äù the domain of your users will not only be useful and adopted in the short- and middle-term, but also contribute to the competitive advantage of your business.</p><p>Now that you have learned a bunch of methods to incorporate domain-specific knowledge, you might be wondering how to approach this in your organizational context. Stay tuned for my next article, where we will consider the practical challenges and strategies for implementing an expertise-driven AI strategy!</p><p><em>Note: Unless noted otherwise, all images are the author‚Äôs.</em></p><p><em>This article was originally published on&nbsp;<a href=\"https://medium.com/towards-data-science/injecting-domain-expertise-into-your-ai-system-792febff48f0\" target=\"_blank\" rel=\"noreferrer noopener\">Towards Data Science</a>&nbsp;and re-published to TOPBOTS with permission from the author.</em></p><h2>Enjoy this article? Sign up for more AI updates.</h2><p>We‚Äôll let you know when we release more articles like this one.</p>","contentLength":22573,"flags":null,"enclosureUrl":"https://www.topbots.com/wp-content/uploads/2025/02/figure4.png","enclosureMime":"text/html","commentsUrl":null},{"title":"Panel at goto Copenhagen: \"Where is SW development Going","url":"https://www.youtube.com/watch?v=86-Dy5U2p5Y","date":1738682580,"author":"Martin Fowler","guid":296,"unread":true,"content":"<p> was on a panel at goto Copenhagen last September with Holly Cummings,\n      Trisha Gee, Dave Farley, and Daniel Terhorst-North. We discussed the\n      current state of software development and where it was heading. Given the\n      timing, there was much discussion about the role AI would play in our\n      profession's future.</p>","contentLength":328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GenAI Patterns: Retrieval Augmented Generation (RAG)","url":"https://martinfowler.com/articles/gen-ai-patterns/#rag","date":1738681020,"author":"Martin Fowler","guid":295,"unread":true,"content":"<p>A pre-trained GenAI model lacks recent and specific information about a\n      domain.  and I explain how Retrieval\n      Augmented Generation (RAG) can fill that gap.</p>","contentLength":166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"1 Simple Technique to Scale Microservices Architecture üöÄ","url":"https://newsletter.systemdesign.one/p/how-to-scale-microservices","date":1738677848,"author":"Neo Kim","guid":54,"unread":true,"content":"<p>Get the powerful template to approach system design for FREE on newsletter sign-up:</p><p><em>You will find references at the bottom of this page if you want to go deeper.</em></p><p>Once upon a time, there was a tech startup.</p><p>They tracked food deliveries in real-time.</p><p>Yet they had only a few customers; so, a simple monolith architecture was enough.</p><p>Until one day, an influencer with a massive following shared their app, and traffic skyrocketed.</p><p>So they moved to microservices architecture for scale.</p><p>Although it temporarily solved their scalability issues, there were newer problems.</p><ol><li><p>Implementing new features became difficult because of the dependencies between microservices.</p></li><li><p>The operational complexity increased due to different programming languages across microservices.</p></li></ol><p>Authorization can make or break your application‚Äôs security and scalability. From managing dynamic permissions to implementing fine-grained access controls, the challenges grow as your requirements and users scale.</p><p>This eBook will guide you through the 6 key requirements all authorization layers should include to avoid technical debt:</p><ul><li><p>Architectural and design considerations for building a scalable and secure authorization layer.</p></li><li><p>20+ technologies, approaches, and standards to consider for your permission management systems.</p></li><li><p>Practical insights, based on 500+ interviews with engineers, architects, and IAM professionals.</p></li></ul><p>Learn how to create an authorization solution that evolves with your business needs, while avoiding technical debt.</p><h2>How to Scale Microservices</h2><p>They wanted to reduce the maintenance effort and scale microservices quickly.</p><p>So they relied on automation.</p><p>A microservice must be scaled to handle changing traffic.</p><p>Yet scaling manually is slow and causes errors.</p><p>So they use <a href=\"https://en.wikipedia.org/wiki/Infrastructure_as_code\">infrastructure as code</a> to keep things simple; it‚Äôs used to deploy new instances based on demand.  means managing and provisioning infrastructure using code.</p><p>Ready for the next technique?</p><p>It‚Äôs difficult to understand and maintain APIs without a proper naming convention.</p><p>Yet maintaining consistent naming for APIs across microservices is hard. </p><p><em>So they generate APIs using code and store API definitions as JSON in a separate Git repository.</em></p><p>Here are the benefits of code-generated APIs:</p><ul><li><p>API paths can be validated using <a href=\"https://www.sonarsource.com/learn/linter/\">linters</a>.</p></li><li><p>API naming conventions and request-response structures are standardized.</p></li><li><p>API version changes can be tracked through tagging.</p></li><li><p>It ensures API operations are defined.</p></li><li><p>User-friendly API paths can be set up as default.</p></li></ul><p>Also it standardizes APIs built using different programming languages. Put simply, code generation makes it look like a single person wrote APIs across the system.</p><p>Besides code generation ensures health checks are included in each microservice.</p><ul><li><p>A health check URL is included in the API specification of the microservice.</p></li></ul><p>An unhealthy service is removed from production to avoid service interruptions.</p><p>A change to database schema shouldn‚Äôt break microservices depending on it. </p><p>Yet sharing a single database across microservices introduces tight coupling and failure risk.</p><p>So they set up separate databases for each microservice.</p><p>But maintaining many databases is a huge effort; so, they create databases and tables using code.</p><p>Here are its main benefits:</p><ul><li><p>Consistent database names.</p></li><li><p>Ensure proper database indexes exist from the start.</p></li><li><p>Easy to maintain and operate the database.</p></li></ul><p>Besides a microservice shouldn't talk to a database owned by another microservice. Instead, communication happens only via the microservice‚Äôs API owning the database. Otherwise, the data changes aren't safe as they bypass business logic.</p><p>: use code generation and automation to scale microservices quickly. It reduces maintenance efforts and helps with version control.</p><p>üëã  -Do you want to level up at work and better understand technical discussions?</p><p>My mission is to help you go from 0 to 1 in system design by spending less than 37 minutes a month.</p><ul><li><p>I launched the newsletter deep dives last month.</p></li></ul><ul><li><p>Yet future deep dives, starting this month, are available only to paid subscribers.</p></li><li><p>And subscription fees will be higher than the current fees.</p></li></ul><p>So<strong> pledge now to get access at a very low price.</strong></p><p><em>‚ÄúThis newsletter is an amazing resource for learning system design.‚Äù </em>Alex</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 123,001+ readers strong, very close to 124k. Let‚Äôs try to reach 124k readers by 10 February. Consider sharing this post with your friends and get rewards.</p>","contentLength":4531,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/84c96d29-cf27-47f0-9e1c-78ca1fa5f461_1280x720.gif","enclosureMime":"","commentsUrl":null},{"title":"NumPy Techniques and Practical Examples","url":"https://realpython.com/courses/numpy-techniques-practical-examples/","date":1738677600,"author":"Real Python","guid":176,"unread":true,"content":"<p>The <a href=\"https://numpy.org/doc/stable/reference/index.html#python-api\">NumPy library</a> is a Python library used for scientific computing. It provides you with a multidimensional array object for storing and analyzing data in a wide variety of ways. In this video course, you‚Äôll see examples of some features NumPy provides that aren‚Äôt always highlighted in other tutorials. </p><p><strong>In this video course, you‚Äôll learn how to:</strong></p><ul><li>Create  from data stored in files</li><li>Identify and remove  from a NumPy array</li><li>Use  to reconcile the differences between datasets</li><li>Analyze and chart specific parts of  data</li><li>Create  versions of your own functions</li></ul>","contentLength":554,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quiz: How to Split a String in Python","url":"https://realpython.com/quizzes/python-split-string/","date":1738670400,"author":"Real Python","guid":175,"unread":true,"content":"<p>Python‚Äôs  method lets you divide a string into a list of substrings based on a specified delimiter. By default,  separates at whitespace, including spaces, tabs, and newlines. You can customize  to work with specific delimiters using the  parameter, and control the amount of splits with .</p>","contentLength":291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI o3-mini - Thinking AI for Free‚Ä¶For Everyone!","url":"https://www.youtube.com/watch?v=oKx2xQQl--Q","date":1738668613,"author":"Two Minute Papers","guid":519,"unread":true,"content":"<article>‚ù§Ô∏è Check out Weights &amp; Biases and sign up for a free demo here: https://wandb.me/papers\n\no3 mini: https://openai.com/index/openai-o3-mini/\nOpenAI Deep Research: https://openai.com/index/introducing-deep-research/\n\nü§ù Interested in sponsoring us? Click here: https://www.jotform.com/241831324457354\n\nSources:\nhttps://x.com/hxiao/status/1885522459329520089?s=46\nhttps://x.com/techikansh/status/1885429093862187008?s=46\nhttps://x.com/rlancemartin/status/1885748894220554445?s=46\nhttps://x.com/buccocapital/status/1885792154129219959?s=46\nhttps://x.com/_akhaliq/status/1885733581651050586?s=46\nhttps://x.com/_akhaliq/status/1885833163764646267?s=46\nhttps://x.com/aidan_mclau/status/1886078444855034055?s=46\nhttps://x.com/cj_zzzz/status/1885740906034196725?s=46\nhttps://x.com/bbssppllvv/status/1886136914446630978?s=46\nhttps://x.com/hxiao/status/1885700308720091280\n\nOpen source Deep Research-ish thing (unofficial, of course):\nhttps://x.com/nickscamara_/status/1886287956291338689?s=46\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu</article>","contentLength":1814,"flags":null,"enclosureUrl":"https://www.youtube.com/v/oKx2xQQl--Q?version=3","enclosureMime":"","commentsUrl":null},{"title":"#419 Is your back end popular?","url":"https://pythonbytes.fm/episodes/show/419/is-your-back-end-popular","date":1738656000,"author":"","guid":445,"unread":true,"content":"<article></article>","contentLength":0,"flags":null,"enclosureUrl":"https://pythonbytes.fm/episodes/download/419/is-your-back-end-popular.mp3","enclosureMime":"","commentsUrl":null},{"title":"What are ACID Transactions in Databases?","url":"https://blog.algomaster.io/p/what-are-acid-transactions-in-databases","date":1738647067,"author":"Ashish Pratap Singh","guid":748,"unread":true,"content":"<p>Imagine you‚Äôre running an e-commerce application. </p><p>A customer places an order, and your system needs to deduct the item from inventory, charge the customer‚Äôs credit card, and record the sale in your accounting system‚Äîall at once. </p><p>What happens if the payment fails but your inventory count has already been reduced? Or if your application crashes halfway through the process?</p><p>This is where  come into play. They ensure that all the steps in such critical operations happen reliably and consistently.</p><p>ACID is an acronym that refers to the set of 4 key properties that define a transaction: <strong>Atomicity, Consistency, Isolation,</strong> and </p><p>In this article, we‚Äôll dive into what each of the ACID properties mean, why they are important, and how they are implemented in databases.</p><p>If you‚Äôre finding this newsletter valuable and want to deepen your learning, consider becoming a .</p><p>As a paid subscriber, you'll receive an <strong>exclusive deep-dive article</strong> every week, access to a structured100+topics and interview questions, and other .</p><p>A  in the context of databases is a sequence of one or more operations (such as inserting, updating, or deleting records) that the database treats as . It either fully succeeds or fully fails, with no in-between states.</p><p>When you send money to a friend, two things happen:</p><ol><li><p>Money is deducted from your account.</p></li><li><p>Money is added to their account.</p></li></ol><p>These two steps form . If either step fails, both are canceled.</p><p>Without transactions, databases could end up in inconsistent states. </p><ul><li><p>: Your money is deducted, but your friend never receives it.</p></li><li><p>: Two people booking the last movie ticket at the same time.</p></li></ul><p>Transactions solve these problems by enforcing rules like  (Atomicity, Consistency, Isolation, Durability).</p><p>Now, lets looks at each of the ACID properties.</p><p>Atomicity ensures that a transaction‚Äîcomprising multiple operations‚Äîexecutes as a unit of work: it either  succeeds (commits) or  fails (rolls back). </p><p>If any part of the transaction fails, the entire transaction is rolled back, and the database is restored to a state exactly as it was before the transaction began.</p><blockquote><p>In a money transfer transaction, if the credit step fails, the debit step cannot be allowed to stand on its own. This prevents inconsistent states like ‚Äúmoney disappearing‚Äù from one account without showing up in another.</p></blockquote><p>Atomicity abstracts away the complexity of manually undoing changes if something goes wrong.</p><h2>How Databases Implement Atomicity</h2><p>Databases use two key mechanisms to guarantee atomicity.</p><h4><strong>1. Transaction Logs (Write-Ahead Logs)</strong></h4><ul><li><p>Every operation is recorded in a  before it‚Äôs applied to the actual database table.</p></li><li><p>If a failure occurs, the database uses this log to  incomplete changes.</p></li></ul><p>Once the WAL entry is safely on disk, the database proceeds with modifying the in-memory pages that contain rows for  and .</p><p>When the operations succeed:</p><ol><li><p>The database marks  as  in the transaction log.</p></li><li><p>The newly updated balances for A and B will eventually get flushed from memory to their respective data files on disk.</p></li></ol><p>If the database crashes  the log entry is written but  the data files are fully updated, the WAL provides a way to recover:</p><ul><li><p>On restart, the database checks the WAL.</p></li><li><p>It sees  was committed.</p></li><li><p>It reapplies the  operations to ensure the final balances are correct in the data files.</p></li></ul><p>If the transaction had not committed (or was marked as ‚Äúin progress‚Äù) at the time of the crash, the database would  those changes using information in the log, leaving the table as if the transaction never happened.</p><h4><strong>2. Commit/Rollback Protocols</strong></h4><ul><li><p>Databases provide commands like , , and </p></li><li><p>Any changes made between  and  are considered ‚Äúin-progress‚Äù and won‚Äôt be permanently applied unless the transaction commits successfully.</p></li><li><p>If any step fails, or if you explicitly issue a , all changes since the start of the transaction are undone.</p></li></ul><p> in the context of ACID transactions ensures that any transaction will bring the database from one valid state to another valid state‚Äînever leaving it in a broken or ‚Äúinvalid‚Äù state.</p><p>It means that all the data integrity constraints, such as  (no duplicate IDs),  (related records must exist in parent tables), and (age can‚Äôt be negative), are satisfied before and after the transaction.</p><p>If a transaction tries to violate these rules, it will not be committed, and the database will revert to its previous state.</p><p>You have two tables in an e-commerce database:</p><ol><li><p> (with columns: , , etc.)</p></li><li><p> (with columns: , , , etc.)</p></li></ol><ul><li><p>: You can‚Äôt place an order for a product if  is greater than the  in the  table.</p></li></ul><ul><li><p>If the product‚Äôs  was 8 (less than what we‚Äôre trying to order), the database sees that the new value would be  which breaks the consistency rule (it should not go negative).</p></li><li><p>The transaction fails or triggers a rollback, preventing the database from ending in an invalid state.</p></li></ul><h2>How to Implement Consistency</h2><ol><li><p><strong>Database Schema Constraints</strong></p><ul><li><p>, , , ,  constraints, and other schema definitions ensure no invalid entries are allowed.</p></li></ul></li><li><p><strong>Triggers and Stored Procedures</strong></p><ul><li><p>Triggers can automatically check additional rules whenever rows are inserted, updated, or deleted.</p></li><li><p>Stored procedures can contain logic to validate data before committing.</p></li></ul></li><li><p><strong>Application-Level Safeguards</strong></p><ul><li><p>While the database enforces constraints at a lower level, applications often add extra checks‚Äîlike ensuring business rules are followed or data is validated before it even reaches the database layer.</p></li></ul></li></ol><p> ensures that concurrently running transactions do not interfere with each other‚Äôs intermediate states.</p><p>Essentially, while a transaction is in progress, its updates (or intermediate data) remain invisible to other ongoing transactions‚Äîgiving the illusion that each transaction is running sequentially, one at a time.</p><p>Without isolation, two or more transactions could read and write partial or uncommitted data from each other, causing incorrect or inconsistent results.</p><p>With isolation, developers can reason more reliably about how data changes will appear to other transactions.</p><p>To understand how isolation works, it helps to see what can go wrong without proper isolation.  Common concurrency anomalies include:</p><ol><li><ul><li><p>Transaction A reads data that Transaction B has modified but not yet committed.</p></li><li><p>If Transaction B then rolls back, Transaction A ends up holding an invalid or ‚Äúdirty‚Äù value that never truly existed in the committed state.</p></li></ul></li><li><ul><li><p>Transaction A reads the same row(s) multiple times during its execution but sees different data because another transaction updated or deleted those rows in between A‚Äôs reads.</p></li></ul></li><li><ul><li><p>Transaction A performs a query that returns a set of rows. Another transaction inserts, updates, or deletes rows that match A‚Äôs query conditions.</p></li><li><p>If A re-runs the same query, it sees a different set of rows (‚Äúphantoms‚Äù).</p></li></ul></li></ol><p>Databases typically allow you to choose an , which balances data correctness with performance.</p><p>Higher isolation levels provide stronger data consistency but can reduce system performance by increasing the wait times for transactions. </p><p>Let's explore the four common isolation levels:</p><ol><li><ul><li><p>Allows dirty reads; transactions can see uncommitted changes.</p></li><li><p>Rarely used, as it can lead to severe anomalies.</p></li></ul></li><li><ul><li><p>A transaction sees only data that has been committed at the moment of reading.</p></li><li><p>Prevents dirty reads, but non-repeatable reads and phantom reads can still occur.</p></li></ul></li><li><ul><li><p>Ensures if you read the same rows multiple times within a transaction, you‚Äôll get the same values (unless you explicitly modify them).</p></li><li><p>Prevents dirty reads and non-repeatable reads, but phantom reads may still happen (depending on the database engine).</p></li></ul></li><li><ul><li><p>The highest level of isolation, acting as if all transactions happen sequentially one at a time.</p></li><li><p>Prevents dirty reads, non-repeatable reads, and phantom reads.</p></li><li><p>Most expensive in terms of performance and concurrency because it can require more locking or more conflict checks.</p></li></ul></li></ol><h2>How Databases Enforce Isolation</h2><ul><li><p><strong>Pessimistic Concurrency Control</strong></p><ul><li><p>Rows or tables are locked so that no other transaction can read or write them until the lock is released.</p></li><li><p>Can lead to blocking or deadlocks if multiple transactions compete for the same locks.</p></li></ul></li></ul><h4>2. MVCC (Multi-Version Concurrency Control)</h4><ul><li><p><strong>Optimistic Concurrency Control</strong></p><ul><li><p>Instead of blocking reads, the database keeps multiple versions of a row.</p></li><li><p>Readers see a consistent snapshot of data (like a point-in-time view), while writers create a new version of the row when updating.</p></li><li><p>This approach reduces lock contention but requires carefully managing row versions and cleanup (vacuuming in PostgreSQL, for example).</p></li></ul></li></ul><ul><li><p>A form of MVCC where each transaction sees data as it was at the start (or a consistent point) of the transaction.</p></li><li><p>Prevents non-repeatable reads and dirty reads. Phantom reads may still occur unless the isolation level is fully serializable.</p></li></ul><p> ensures that once a transaction has been committed, the changes it made will survive, even in the face of power failures, crashes, or other catastrophic events. </p><p>In other words, once a transaction says ‚Äúdone,‚Äù the data is permanently recorded and cannot simply disappear.</p><h2>How Databases Ensure Durability</h2><h4>1. Transaction Logs (Write-Ahead Logging)</h4><p>Most relational databases rely on a  to preserve changes before they‚Äôre written to the main data files:</p><ol><li><p>: The intended operations (updates, inserts, deletes) are recorded in the WAL on durable storage (disk).</p></li><li><p>: Once the WAL entry is safely persisted, the database can mark the transaction as committed.</p></li><li><p><strong>Apply Changes to Main Data Files</strong>: The updated data eventually gets written to the main files‚Äîpossibly first in memory, then flushed to disk.</p></li></ol><p>If the database crashes, it uses the WAL during :</p><ul><li><p>: Any committed transactions not yet reflected in the main files are reapplied.</p></li><li><p>: Any incomplete (uncommitted) transactions are rolled back to keep the database consistent.</p></li></ul><h4>2. Replication / Redundancy</h4><p>In addition to WAL, many systems use replication to ensure data remains durable even if hardware or an entire data center fails.</p><ul><li><p>: Writes are immediately copied to multiple nodes or data centers. A transaction is marked committed only if the primary and at least one replica confirm it‚Äôs safely stored.</p></li><li><p>: Changes eventually sync to other nodes, but there is a (small) window where data loss can occur if the primary fails before the replica is updated.</p></li></ul><p>Regular  provide a safety net beyond logs and replication. In case of severe corruption, human error, or catastrophic failure:</p><ul><li><p>: Capture the entire database at a point in time.</p></li><li><p><strong>Incremental/Differential Backups</strong>: Store changes since the last backup for faster, more frequent backups.</p></li><li><p>: Ensures backups remain safe from localized disasters, allowing you to restore data even if hardware is damaged.</p></li></ul><p>If you found it valuable, hit a like ‚ù§Ô∏è and consider subscribing for more such content every week.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/scalability?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQwMDc4MDk0LCJpYXQiOjE3Mzc1MzgxODMsImV4cCI6MTc0MDEzMDE4MywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.xroFXQDDEPvo2FWnnt-G2Ji9MzYIDtJ68NRQX6sT8x8&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you‚Äôre finding this newsletter helpful and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":10887,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9ceb65c-70e6-4f3e-9511-f6bc5da93d13_1308x1086.png","enclosureMime":"","commentsUrl":null},{"title":"Tutorial: publishing ESM-based npm packages with TypeScript","url":"https://2ality.com/2025/02/typescript-esm-packages.html","date":1738627200,"author":"Dr. Axel Rauschmayer","guid":347,"unread":true,"content":"<p>During the last two years, ESM support in TypeScript, Node.js and browsers has made a lot of progress. In this blog post, I explain my modern setup that is relatively simple ‚Äì compared to what we had to do in the past:</p>","contentLength":220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fat Rand: How Many Lines Do You Need To Generate A Random Number?","url":"http://lucumr.pocoo.org/2025/2/4/fat-rand","date":1738627200,"author":"Armin Ronacher","guid":304,"unread":true,"content":"<p>I recently wrote <a href=\"http://lucumr.pocoo.org/2025/1/24/build-it-yourself/\">about dependencies in Rust</a>.  The feedback, both within and outside\nthe Rust community, was very different.  A lot of people, particularly\nsome of those I greatly admire expressed support.  The Rust community, on\nthe other hand, was very dismissive on on Reddit and Lobsters.</p><p>Last time, I focused on the  crate, but I also want to\nshow you a different one that I come across once more: .  It has a\nsimilarly out-of-whack value-to-dependency ratio, but in a slightly\ndifferent way.  More than , you are quite likely to use\nit.  If for instance if you want to generate a random UUID, the \ncrate will depend on it.  Due to its nature it also has a high security\nexposure.</p><p>I don't want to frame this as ‚Äú is a bad crate‚Äù.  It's not a bad\ncrate at all!  It is however a crate that does not appear very concerned\nabout how many dependencies it has, and I want to put this in perspective:\nof all the dependencies and lines of codes it pulls in, how many does it\nactually use?</p><p>As the name implies, the  crate is capable of calculating random\nnumbers.  The crate itself has seen a fair bit of churn: for instance 0.9\nbroke backwards compatibility with 0.8.  So, as someone who used that\ncrate, I did what a responsible developer is supposed to do, and upgraded\nthe dependency.  After all, I don't want to be the reason there are two\nversions of  in the dependency tree.  After the upgrade, I was\nsurprised how fat that dependency tree has become over the last nine\nmonths.</p><p>Today, this is what the dependency tree looks like for the default feature\nset on macOS and Linux:</p><pre>x v0.1.0 (/private/tmp/x)\n‚îî‚îÄ‚îÄ rand v0.9.0\n    ‚îú‚îÄ‚îÄ rand_chacha v0.9.0\n    ‚îÇ   ‚îú‚îÄ‚îÄ ppv-lite86 v0.2.20\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ zerocopy v0.7.35\n    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ byteorder v1.5.0\n    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ zerocopy-derive v0.7.35 (proc-macro)\n    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ proc-macro2 v1.0.93\n    ‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ unicode-ident v1.0.16\n    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ quote v1.0.38\n    ‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ proc-macro2 v1.0.93 (*)\n    ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ syn v2.0.98\n    ‚îÇ   ‚îÇ               ‚îú‚îÄ‚îÄ proc-macro2 v1.0.93 (*)\n    ‚îÇ   ‚îÇ               ‚îú‚îÄ‚îÄ quote v1.0.38 (*)\n    ‚îÇ   ‚îÇ               ‚îî‚îÄ‚îÄ unicode-ident v1.0.16\n    ‚îÇ   ‚îî‚îÄ‚îÄ rand_core v0.9.0\n    ‚îÇ       ‚îú‚îÄ‚îÄ getrandom v0.3.1\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ cfg-if v1.0.0\n    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ libc v0.2.169\n    ‚îÇ       ‚îî‚îÄ‚îÄ zerocopy v0.8.14\n    ‚îú‚îÄ‚îÄ rand_core v0.9.0 (*)\n    ‚îî‚îÄ‚îÄ zerocopy v0.8.14\n</pre><p>About a year ago, it looked like this:</p><pre>x v0.1.0 (/private/tmp/x)\n‚îî‚îÄ‚îÄ rand v0.8.5\n    ‚îú‚îÄ‚îÄ libc v0.2.169\n    ‚îú‚îÄ‚îÄ rand_chacha v0.3.1\n    ‚îÇ   ‚îú‚îÄ‚îÄ ppv-lite86 v0.2.17\n    ‚îÇ   ‚îî‚îÄ‚îÄ rand_core v0.6.4\n    ‚îÇ       ‚îî‚îÄ‚îÄ getrandom v0.2.10\n    ‚îÇ           ‚îú‚îÄ‚îÄ cfg-if v1.0.0\n    ‚îÇ           ‚îî‚îÄ‚îÄ libc v0.2.169\n    ‚îî‚îÄ‚îÄ rand_core v0.6.4 (*)\n</pre><p>So, let's investigate what all these dependencies do. The current version\npulls in quite a lot.</p><div><p>If you want to audit the entire dependency chain, you end up with\nmaintainers that form eight distinct groups:</p><ol><li>: rust core + various externals</li><li>: rust core + Alex Crichton</li><li> and : rust nursery + rust-random</li><li> and : Google (via two ICs there, Google\ndoes not publish)</li><li>: Andrew Gallant</li><li>, , , : David Tolnay</li></ol><p>If I also cared about WASM targets, I'd have to consider even more\ndependencies.</p></div><div><p>So let's vendor it.  How much code is there?  After removing all tests, we\nend up with  vendored taking up  disk\nspace.  Tokei reports .</p><p>Now this is a bit misleading, because like many times most of this is\nwithin .  But how much of  does \nneed?  A single function:</p><div><pre>: : -&gt; </pre></div><p>For that single function (and the information which DLL it needs link\ninto), we are compiling and downloading megabytes of .\nLonger term <a href=\"https://rust-lang.github.io/rfcs/2627-raw-dylib-kind.html\">this might not be necessary</a>, but today\nit is.</p><p>On Unix, it's harder to avoid  because it tries multiple APIs.\nThese are mostly single-function APIs, but some non-portable constants\nmake  difficult to avoid.</p><p>Beyond the platform dependencies, what else is there?</p><ul><li> (the 's picked default randon number generator)\nalone comes to 3,587 lines of code including 168 unsafe blocks.  If\nthe goal of using  was to avoid , there is still\na ton of  remaining.</li><li>The combination of , , , and\n comes to 49,114 lines of code.</li><li> clocks in at 3,000 lines of code.</li><li>The pair of  and  together?  14,004 lines\nof code.</li></ul><p>All of these are great crates, but do I need all of this just to generate a random number?</p></div><div><p>The Rust developer community <a href=\"https://www.reddit.com/r/rust/comments/1igjiip/rand_now_depends_on_zerocopy/\">on Reddit</a>\ndoesn't seem very concerned.  The main sentiment is that  now uses less\n so that's benefit enough.  While the total amount of unsafe\nprobably did not go down, that moved unsafe is is now in a common crate\nwritten by people that know how to use unsafe ().  There is\nalso the sentiment that all of this doesn't matter anyways, because we\nwill will all soon depend on  everywhere anyways, as more and\nmore dependencies are switching over to it.</p><p>Maybe this points to Rust not having a large enough standard library.\nPerhaps features like terminal size detection and random number generation\nshould be included.  That at least is what people pointed out on Twitter.</p><p>We already treat crates like , , and  as if they\nwere part of the standard library.  The difference is that I can trust the\nstandard library as a whole‚Äîit comes from a single set of authors, making\nauditing easier.  If these external, but almost standard crates were more\ncautious about dependencies and make it more of a goal to be auditable, we\nwould all benefit.</p><p>Or maybe this is just how Rust works now.  That would make me quite sad.</p><p> it looks like there is some appetite in  to improve on\nthis.</p><ul><li>a stripped down version of  (which does not require \nor most of the rust-crypto ecosystem) might replace :\n<a href=\"https://github.com/rust-random/rand/issues/934\">PR #934</a>.</li><li>if you use Rust 1.71 or later,  becomes mostly a\nno-op if you compile with .</li></ul><p><em>Edit: This post originally incorrectly said that getrandom depends on\nwindows-sys.  That is incorrect, it only depends on windows-targets.</em></p></div>","contentLength":6046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NetChoice Sues To Block Maryland's Kids Code, Saying It Violates the First Amendment","url":"https://yro.slashdot.org/story/25/02/03/2028246/netchoice-sues-to-block-marylands-kids-code-saying-it-violates-the-first-amendment?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738616400,"author":"BeauHD","guid":361,"unread":true,"content":"NetChoice has filed (PDF) its 10th lawsuit challenging state internet regulations, this time opposing Maryland's Age-Appropriate Design Code Act. The Verge's Lauren Feiner reports: NetChoice has become one of the fiercest -- and most successful -- opponents of age verification, moderation, and design code laws, all of which would put new obligations on tech platforms and change how users experience the internet. [...] NetChoice's latest suit opposes the Maryland Age-Appropriate Design Code Act, a rule that echoes a California law of a similar name. In the California litigation, NetChoice notched a partial win in the Ninth Circuit Court of Appeals, which upheld the district court's decision to block a part of the law requiring platforms to file reports about their services' impact on kids. (It sent another part of the law back to the lower court for further review.)\n \nA similar provision in Maryland's law is at the center of NetChoice's complaint. The group says that Maryland's reporting requirement lets regulators subjectively determine the \"best interests of children,\" inviting \"discriminatory enforcement.\" The reporting requirement on tech companies essentially mandates them \"to disparage their services and opine on far-ranging and ill-defined harms that could purportedly arise from their services' 'design' and use of information,\" NetChoice alleges. NetChoice points out that both California and Maryland have passed separate online privacy laws, which NetChoice Litigation Center director Chris Marchese says shows that \"lawmakers know how to write laws to protect online privacy when what they want to do is protect online privacy.\"\n \nSupporters of the Maryland law say legislators learned from California's challenges and \"optimized\" their law to avoid questions about speech, according to Tech Policy Press. In a blog analyzing Maryland's approach, Future of Privacy Forum points out that the state made some significant changes from California's version -- such as avoiding an \"express obligation√¢ to determine users' ages and defining the \"best interests of children.\" The NetChoice challenge will test how well those changes can hold up to First Amendment scrutiny. NetChoice has consistently maintained that even well-intentioned attempts to protect kids online are likely to backfire. Though the Maryland law does not explicitly require the use of specific age verification tools, Marchese says it essentially leaves tech platforms with a no-win decision: collect more data on users to determine their ages and create varied user experiences or cater to the lowest common denominator and self-censor lawful content that might be considered inappropriate for its youngest users. And similar to its arguments in other cases, Marchese worries that collecting more data to identify users as minors could create a \"honey pot\" of kids' information, creating a different problem in attempting to solve another.","contentLength":2938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Precision Time Protocol handles leap seconds","url":"https://engineering.fb.com/2025/02/03/production-engineering/how-precision-time-protocol-ptp-handles-leap-seconds/","date":1738602034,"author":"","guid":468,"unread":true,"content":"<p><a href=\"https://engineering.fb.com/2022/07/25/production-engineering/its-time-to-leave-the-leap-second-in-the-past/\"></a><a href=\"https://engineering.fb.com/2022/11/21/production-engineering/precision-time-protocol-at-meta/\"></a></p><p><a href=\"https://engineering.fb.com/2022/11/21/production-engineering/precision-time-protocol-at-meta/\" target=\"_blank\" rel=\"noopener\"></a></p><h2></h2><p><a href=\"https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/\" target=\"_blank\" rel=\"noopener\"></a></p><h2></h2><p><a href=\"https://en.wikipedia.org/wiki/Coordinated_Universal_Time\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://engineering.fb.com/2022/11/21/production-engineering/precision-time-protocol-at-meta/#client\" target=\"_blank\" rel=\"noopener\"></a><i></i></p><p><a href=\"https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://www.nytimes.com/2022/11/19/science/time-leap-second-bipm.html\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python for Loops: The Pythonic Way","url":"https://realpython.com/python-for-loop/","date":1738591200,"author":"Real Python","guid":174,"unread":true,"content":"<p>Python‚Äôs  loop allows you to iterate over the items in a collection, such as lists, tuples, strings, and dictionaries. The  loop syntax declares a loop variable that takes each item from the collection in each iteration. This loop is ideal for repeatedly executing a block of code on each item in the collection. You can also tweak  loops further with features like , , and .</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li> iterates over items in a data collection, allowing you to execute code for each item.</li><li>To , you use the  construct.</li><li>To <strong>repeat code a number of times</strong> without processing the data of an iterable, use the  construct.</li><li>To do , you can use <code>for index, value in enumerate(iterable):</code> to access both index and item.</li></ul><p>In this tutorial, you‚Äôll gain practical knowledge of using  loops to traverse various collections and learn Pythonic looping techniques. Additionally, you‚Äôll learn how to handle exceptions and how to use asynchronous iterations to make your Python code more robust and efficient.</p><div><p> Test your knowledge with our interactive ‚ÄúPython \"for\" Loops: The Pythonic Way‚Äù quiz. You‚Äôll receive a score upon completion to help you track your learning progress:</p><div><div><a href=\"https://realpython.com/quizzes/python-for-loop/\"></a><p>In this quiz, you'll test your understanding of Python's for loop. You'll revisit how to iterate over items in a data collection, how to use range() for a predefined number of iterations, and how to use enumerate() for index-based iteration.</p></div></div></div><h2>Getting Started With the Python  Loop</h2><p>In programming, loops are control flow statements that allow you to repeat a given set of operations a number of times. In practice, you‚Äôll find two main types of loops:</p><ol><li><a href=\"https://en.wikipedia.org/wiki/For_loop\"> loops</a> are mostly used to iterate a  number of times, which is common when you‚Äôre processing data collections with a specific number of data items.</li><li><a href=\"https://en.wikipedia.org/wiki/While_loop\"> loops</a> are commonly used to iterate an  number of times, which is useful when the number of iterations depends on a given condition.</li></ol><p>Python has both of these loops and in this tutorial, you‚Äôll learn about . In Python, you‚Äôll generally use  loops when you need to iterate over the items in a data collection. This type of loop lets you traverse different data collections and run a specific group of statements  or  each item in the input collection.</p><p>In Python,  loops are <a href=\"https://docs.python.org/3/reference/compound_stmts.html\">compound statements</a> with a  and a  that runs a predefined number of times. The basic syntax of a  loop is shown below:</p><p>In this syntax,  is the loop variable. In each iteration, this variable takes the value of the current item in , which represents the data collection you need to iterate over. The loop body can consist of one or more statements that must be indented properly.</p><p>Here‚Äôs a more detailed breakdown of this syntax:</p><ul><li> is the keyword that initiates the loop header.</li><li> is a variable that holds the current item in the input iterable.</li><li> is a keyword that connects the loop variable with the iterable.</li><li> is a data collection that can be iterated over.</li><li> consists of one or more statements to execute in each iteration.</li></ul><p>Here‚Äôs a quick example of how you can use a  loop to iterate over a list:</p><p>In this example,  is the loop variable, while the  list is the target collection. Each time through the loop,  takes on a successive item from . In this loop, the body consists of a call to <a href=\"https://realpython.com/python-print/\"></a> that displays the value on the screen. This loop runs once for each item in the target iterable. The way the code above is written is the <a href=\"https://realpython.com/learning-paths/writing-pythonic-code/\">Pythonic</a> way to write it.</p><p>However, what‚Äôs an <a href=\"https://realpython.com/python-iterators-iterables/#getting-to-know-python-iterables\">iterable</a> anyway? In Python, an  is an object‚Äîoften a data collection‚Äîthat can be iterated over. Common examples of iterables in Python include <a href=\"https://realpython.com/python-list/\">lists</a>, <a href=\"https://realpython.com/python-tuple/\">tuples</a>, <a href=\"https://realpython.com/python-strings/\">strings</a>, <a href=\"https://realpython.com/python-dicts/\">dictionaries</a>, and <a href=\"https://realpython.com/python-sets/\">sets</a>, which are all built-in data types. You can also have custom classes that support iteration.</p><div role=\"alert\"><p> Python has both iterables and iterators. Iterables support the  consisting of the <a href=\"https://realpython.com/python-magic-methods/\">special method</a>. Similarly, iterators support the  that‚Äôs based on the  and  special methods.</p><p>Both iterables and iterators can be iterated over. All iterators are iterables, but not all iterables are iterators. Python iterators play a fundamental role in  loops because they drive the iteration process.</p></div><p>You can also have a loop with multiple loop variables:</p><p>In this loop, you have two loop variables,  and . Note that to use this syntax, you just need to provide a tuple of loop variables. Also, you can have as many loop variables as you need as long as you have the correct number of items to <a href=\"https://realpython.com/python-tuple/#packing-and-unpacking-tuples\">unpack</a> into them. You‚Äôll also find this pattern useful when iterating over <a href=\"https://realpython.com/python-for-loop/#collections-dictionaries-and-sets\">dictionary</a> items or when you need to do <a href=\"https://realpython.com/python-for-loop/#looping-over-several-iterables-in-parallel\">parallel iteration</a>.</p><p>Sometimes, the input iterable may be empty. In that case, the loop will run its header once but won‚Äôt execute its body:</p>","contentLength":4646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deno vs. Node.js vs. Bun: Performance Comparison 2025","url":"https://www.youtube.com/watch?v=DpDHPoStZZ8","date":1738587725,"author":"Anton Putra","guid":533,"unread":true,"content":"<article>Node.js vs Deno vs Bun Performance\n\nüî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)\n\nüçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&amp;si=p-UOaVM_6_SFx52H\n\nüëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience &amp; Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)\n‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra\n‚ñ∫  Twitter/X: https://twitter.com/antonvputra\n‚ñ∫  GitHub: https://github.com/antonputra\n‚ñ∫  Email: me@antonputra.com\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Timestamps ‚è∞  ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n0:00 Intro\n0:42 1st Test - Static\n4:26 2nd Test - PostgreSQL\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/241/lessons/241\n‚ñ∫ Bun's memory leak fix - https://github.com/oven-sh/bun/issues/17063\n\n#deno #bun #nodejs</article>","contentLength":1129,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DpDHPoStZZ8?version=3","enclosureMime":"","commentsUrl":null},{"title":"Should We Sing the Praises of Agile, or Bury It?","url":"https://it.slashdot.org/story/25/02/02/1948240/should-we-sing-the-praises-of-agile-or-bury-it?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738528440,"author":"EditorDavid","guid":360,"unread":true,"content":"\"Stakeholders must be included\" throughout an agile project \"to ensure the evolving deliverables meet their expectations,\" according to an article this week in Communications of the ACM. \n\nBut long-time Slashdot reader theodp complains it's a \"gushing how-to-make-Agile-even-better opinion piece.\"\n\nLike other pieces by Agile advocates, it's long on accolades for Agile, but short on hard evidence justifying why exactly Agile project management \"has emerged as a critical component for firms looking to improve project delivery speed and flexibility\" and the use of Agile approaches is being expanded across other departments beyond software development. Indeed, among the three examples of success offered in the piece to \"highlight the effectiveness of agile methods in navigating complex stakeholder dynamics and achieving project success\" is Atlassian's use of agile practices to market and develop its products, many of which are coincidentally designed to support Agile practices and teams (including Jira). How meta. \n\nCiting \"recent studies,\" the piece concludes its call for stakeholder engagement by noting that \"59% of organizations measure Agile success by customer or user satisfaction.\" But that is one of those metrics that can create perverse incentives. Empirical studies of user satisfaction and engagement have been published since the 1970's, and sadly one of the cruel lessons learned from them is that the easiest path to having satisfied users is to avoid working on difficult problems. Keep that in mind when you ponder why difficult user stories seem to languish forever in the Kanban and Scrum Board \"Ice Box\" column, while the \"Complete\" column is filled with low-hanging fruit. Sometimes success does come easy! So, are you in the Agile-is-Heaven or Agile-is-Hell camp?","contentLength":1798,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build a Dice-Rolling Application With Python","url":"https://realpython.com/python-dice-roll/","date":1738504800,"author":"Real Python","guid":170,"unread":true,"content":"<p>In this tutorial, you‚Äôll learn how to create a Python dice roll simulator. The tutorial guides you through building a text-based user interface (TUI) application that simulates rolling dice using Python‚Äôs  module. You‚Äôll learn to gather and validate user input, use  for dice rolling, and display results with ASCII art.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li>To simulate  events, you can use  .</li><li>To get the , you use the built-in  function.</li><li>To display dice in Python, you generate <strong>ASCII art representations</strong> of dice faces and use .</li><li>To manipulate , you use methods such as  and .</li></ul><p>Building small projects, like a <a href=\"https://en.wikipedia.org/wiki/Text-based_user_interface\">text-based user interface (TUI)</a> dice-rolling application, will help you level up your Python programming skills. You‚Äôll learn how to gather and validate the user‚Äôs input, import code from modules and packages, write functions, use  loops and conditionals, and neatly display output by using strings and the <a href=\"https://realpython.com/python-print/\"></a> function.</p><p>Click the link below to download the entire code for this dice-rolling application and follow along while you build the project yourself:</p><p>In this step-by-step project, you‚Äôll build an application that runs dice-rolling simulations. The app will be able to roll up to six dice, with each die having six faces. After every roll, the application will generate an ASCII diagram of dice faces and display it on the screen. The following video demonstrates how the app works:</p><p>When you run your dice-rolling simulator app, you get a prompt asking for the number of dice you want to roll. Once you provide a valid integer from 1 to 6, inclusive, then the application simulates the rolling event and displays a diagram of dice faces on the screen.</p><p>Your dice-rolling simulator app will have a minimal yet user-friendly <a href=\"https://en.wikipedia.org/wiki/Text-based_user_interface\">text-based user interface (TUI)</a>, which will allow you to specify the number of six-sided dice that you‚Äôd like to roll. You‚Äôll use this TUI to roll the dice at home without having to fly to Las Vegas.</p><p>Here‚Äôs a description of how the app will work internally:</p><div><table><thead><tr></tr></thead><tbody><tr><td>Prompt the user to choose how many six-sided dice to roll, then read the user‚Äôs input</td><td>A call to  with appropriate arguments</td></tr><tr><td>Parse and validate the user‚Äôs input</td><td>A user-defined function called </td></tr><tr><td>Run the dice-rolling simulation</td><td>A user-defined function called </td></tr><tr><td>Generate an ASCII diagram with the resulting dice faces</td><td>A user-defined function called <code>generate_dice_faces_diagram()</code></td></tr><tr><td>Display the diagram of dice faces on the screen</td><td>A call to  with appropriate arguments</td></tr></tbody></table></div><p>Keeping these internal workings in mind, you‚Äôll code three custom functions to provide the app‚Äôs main features and functionalities. These functions will define your code‚Äôs public <a href=\"https://realpython.com/python-api/\">API</a>, which you‚Äôll call to bring the app to life.</p><p>To organize the code of your dice-rolling simulator project, you‚Äôll create a single file called  in a directory of your choice in your file system. Go ahead and create the file to get started!</p><p>You should be comfortable with the following concepts and skills before you start building this dice-rolling simulation project:</p><p>If you don‚Äôt have all of the prerequisite knowledge before starting this coding adventure, then that‚Äôs okay! You might learn more by going ahead and getting started! You can always stop and review the resources linked here if you get stuck.</p><h2>Step 1: Code the TUI of Your Python Dice-Rolling App</h2><p>In this step, you‚Äôll write the required code to ask for the user‚Äôs input of how many dice they want to roll in the simulation. You‚Äôll also code a Python function that takes the user‚Äôs input, validates it, and returns it as an integer number if the validation was successful. Otherwise, the function will ask for the user‚Äôs input again.</p><p>To download the code for this step, click the following link and navigate to the  folder:</p>","contentLength":3757,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build a Quiz Application With Python","url":"https://realpython.com/python-quiz-application/","date":1738504800,"author":"Real Python","guid":171,"unread":true,"content":"<p>In this tutorial, you‚Äôll build a Python quiz application for the terminal. You‚Äôll start by developing a basic app capable of asking questions, collecting answers, and checking correctness. As you progress, you‚Äôll enhance the app by adding features like user-friendly interfaces, storing questions in external files, handling multiple correct answers, and providing hints and explanations.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li> is a nice beginner project as it involves basic Python concepts like data structures and control flow.</li><li>The  helps you to interact with the user in the terminal.</li><li>You can <strong>structure questions and answers</strong> in data files.</li><li> are suitable for storing data in small games due to their readability and ease of integration with Python.</li></ul><p>The word  was <a href=\"https://www.merriam-webster.com/dictionary/quiz#first-known\">first used</a> in 1781 to mean . Nowadays, it‚Äôs mostly used to describe short tests of trivia or expert knowledge with questions like the following:</p><blockquote><p>When was the first known use of the word ?</p></blockquote><p>By following along in this step-by-step project, you‚Äôll build an application that can test a person‚Äôs expertise on a range of topics. You can use this project to strengthen your own knowledge or to challenge your friends to a fun battle of wits.</p><p>The quiz application is a comprehensive project for anyone comfortable with the basics of Python. Throughout the tutorial, you‚Äôll get all the code you need in separate, bite-size steps. You can also find the full source code of the application by clicking on the link below:</p><p>Whether you‚Äôre an eccentric person or not, read on to learn how to create your own quiz.</p><h2>Demo: Your Python Quiz Application</h2><p>In this step-by-step project, you‚Äôll build a terminal application that can quiz you and your friends on a range of topics:</p><p>You first choose a topic for your questions. Then, for each question, you‚Äôll choose an answer from a set of alternatives. Some questions may have multiple correct answers. You can access a hint to help you along the way. After answering a question, you‚Äôll read an explanation that can provide more context for the answer.</p><p>You‚Äôll start by creating a basic Python quiz application that‚Äôs only capable of asking a question, collecting an answer, and checking whether the answer is correct. From there, you‚Äôll add more and more features in order to make your app more interesting, user-friendly, and fun.</p><p>You‚Äôll build the quiz application iteratively by going through the following steps:</p><ol><li>Create a basic application that can ask multiple-choice questions.</li><li>Make the app more user-friendly by improving how it looks and how it handles user errors.</li><li>Refactor the code to use functions.</li><li>Separate question data from source code by storing questions in a dedicated data file.</li><li>Expand the app to handle multiple correct answers, give hints, and provide explanations.</li><li>Add interest by supporting different quiz topics to choose from.</li></ol><p>As you follow along, you‚Äôll gain experience in starting with a small script and expanding it. This is an important skill in and of itself. Your favorite program, app, or game probably started as a small <a href=\"https://en.wikipedia.org/wiki/Proof_of_concept\">proof of concept</a> that later grew into what it is today.</p><p>In this tutorial, you‚Äôll build a quiz application using Python‚Äôs basic building blocks. While working through the steps, it‚Äôs helpful if you‚Äôre comfortable with the following concepts:</p><p>If you‚Äôre not confident in your knowledge of these prerequisites, then that‚Äôs okay too! In fact, going through this tutorial will help you practice these concepts. You can always stop and review the resources linked above if you get stuck.</p><p>In this step, you‚Äôll learn how to create a program that can ask questions and check answers. This will be the foundation of your quiz application, which you‚Äôll improve upon in the rest of the tutorial. At the end of this step, your program will look like this:</p>","contentLength":3825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top Python Game Engines","url":"https://realpython.com/top-python-game-engines/","date":1738504800,"author":"Real Python","guid":172,"unread":true,"content":"<p>You can use several Python game engines for crafting video games using your existing Python skills. Popular Python game engines are Pygame, Arcade, and Ren‚ÄôPy, each offering unique features. In this tutorial, you‚Äôll learn how to install and use them, and how these engines differ from traditional stand-alone game engines.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li><strong>Pygame, Arcade, adventurelib, and Ren‚ÄôPy</strong> are some of the top Python game engines, each with unique features.</li><li>Python game engines are  for Python developers but may require more effort for  support compared to other engines.</li><li> is designed for beginners, offering simplified game development requiring less manual code than Pygame.</li><li> with Python game engines isn‚Äôt straightforward and requires additional effort for optimization and compatibility.</li><li>You can use Python to  using frameworks like .</li></ul><p>Using Python, and a host of great Python game engines, makes crafting great computer games much easier than in the past. In this tutorial, you‚Äôll explore several of these game engines, learning what you need to start crafting your own Python video games!</p><p>To get the most out of this tutorial, you should be well-versed in Python programming, including <a href=\"https://realpython.com/python3-object-oriented-programming/\">object-oriented programming</a>. An understanding of basic game concepts is helpful, but not necessary.</p><p>Ready to dive in? Click the link below to download the source code for all the games that you‚Äôll be creating:</p><h2>Python Game Engines Overview</h2><p>Game engines for Python most often take the form of Python libraries, which can be installed in a variety of ways. Most are available on <a href=\"https://pypi.org/\">PyPI</a> and can be installed with <a href=\"https://realpython.com/what-is-pip/\"></a>. However, a few are available only on GitHub, GitLab, or other code sharing locations, and they may require other installation steps. This article will cover installation methods for all the engines discussed.</p><p>Python is a general purpose programming language, and it‚Äôs used for a variety of tasks other than writing computer games. In contrast, there are many different stand-alone game engines that are tailored specifically to writing games. Some of these include:</p><p>These stand-alone game engines differ from Python game engines in several key aspects:</p><ul><li> Languages like C++, C#, and JavaScript are popular for games written in stand-alone game engines, as the engines themselves are often written in these languages. Very few stand-alone engines support Python.</li><li><strong>Proprietary scripting support:</strong> In addition, many stand-alone game engines maintain and support their own scripting languages, which may not resemble Python. For example, Unity uses C# natively, while Unreal works best with C++. </li><li> Many modern stand-alone game engines can produce games for a variety of platforms, including mobile and dedicated game systems, with very little effort. In contrast, porting a Python game across various platforms, especially mobile platforms, can be a major undertaking.</li><li> Games written using a stand-alone game engine may have different licensing options and restrictions, based on the engine used.</li></ul><p>So why use Python to write games at all? In a word, Python. Using a stand-alone game engine often requires you to learn a new programming or scripting language. Python game engines leverage your existing knowledge of Python, reducing the learning curve and getting you moving forward quickly.</p><p>There are many game engines available for the Python environment. The engines that you‚Äôll learn about here all share the following criteria:</p><ul><li>They‚Äôre relatively popular engines, or they cover aspects of gaming that aren‚Äôt usually covered.</li><li>They‚Äôre currently maintained.</li><li>They have good documentation available.</li></ul><p>For each engine, you‚Äôll learn about:</p><ul><li>Basic concepts, as well as assumptions that the engine makes</li><li>Major features and capabilities</li><li>Two game implementations, to allow for comparison</li></ul><p>Where appropriate, you should install these game engines in a <a href=\"https://realpython.com/python-virtual-environments-a-primer/\">virtual environment</a>. Full source code for the games in this tutorial is available for download at the link below and will be referenced throughout the article:</p><p>With the source code downloaded, you‚Äôre ready to begin.</p><p>When people think of Python game engines, the first thought many have is <a href=\"https://www.pygame.org\">Pygame</a>. In fact, there‚Äôs already <a href=\"https://realpython.com/pygame-a-primer/\">a great primer on Pygame</a> available at Real Python.</p><p>Written as a replacement for the stalled PySDL library, Pygame wraps and extends the <a href=\"https://www.libsdl.org/\">SDL library</a>, which stands for . SDL provides cross-platform access to your system‚Äôs underlying multimedia hardware components, such as sound, video, mouse, keyboard, and joystick. The cross-platform nature of both SDL and Pygame means that you can write games and rich multimedia Python programs for every platform that supports them!</p>","contentLength":4655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Develop Data Visualization Interfaces in Python With Dash","url":"https://realpython.com/python-dash/","date":1738504800,"author":"Real Python","guid":173,"unread":true,"content":"<p>Dash is a popular Python framework for creating interactive data visualization interfaces. With Dash, you build web applications using only Python, without needing advanced web development skills. It integrates seamlessly with technologies like Flask, React.js, and Plotly.js to render user interfaces and generate charts.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li> is an open-source framework for building data visualization interfaces using Python.</li><li> include interactive dashboards for data analysis and visualization tasks.</li><li>You can  of a Dash app using CSS, either inline or with external files.</li><li>You can  on PythonAnywhere, a platform offering free hosting for Python web apps.</li></ul><p><a href=\"https://dash.plotly.com/introduction\">Dash</a> gives data scientists the ability to showcase their results in interactive web applications. You don‚Äôt need to be an expert in <a href=\"https://realpython.com/learning-paths/become-python-web-developer/\">web development</a>. In this tutorial, you‚Äôll explore how to create, style, and deploy a Dash application, transforming a basic dashboard into a fully interactive tool.</p><p>You can download the source code, data, and resources for the sample application that you‚Äôll make in this tutorial by clicking the link below:</p><p>Dash is an open-source framework for building data visualization interfaces. Released in 2017 as a Python library, it‚Äôs grown to include implementations for R, Julia, and F#. Dash helps data scientists build analytical web applications without requiring advanced web development knowledge.</p><p>Three technologies constitute the core of Dash:</p><ol><li> supplies the web server functionality.</li><li> renders the user interface of the web page.</li><li> generates the charts used in your application.</li></ol><p>But you don‚Äôt have to worry about making all these technologies work together. Dash will do that for you. You just need to write Python, R, Julia, or F# and sprinkle in a bit of CSS.</p><p><a href=\"https://plotly.com/\">Plotly</a>, a Canada-based company, built Dash and supports its development. You may know the company from the <a href=\"https://plotly.com/graphing-libraries/\">popular graphing libraries</a> that share its name. The company released Dash as open source under an <a href=\"https://tldrlegal.com/license/mit-license\">MIT license</a>, so you can use Dash at no cost.</p><p>Plotly also offers a commercial companion to Dash called <a href=\"https://plotly.com/dash/\">Dash Enterprise</a>. This paid service provides companies with support services such as hosting, deploying, and handling authentication on Dash applications. But these features live outside of Dash‚Äôs open-source ecosystem.</p><p>Dash will help you build dashboards quickly. If you‚Äôre used to analyzing data or building data visualizations using Python, then Dash will be a useful addition to your toolbox. Here are a few examples of what you can make with Dash:</p><p>This is just a tiny sample. If you‚Äôd like to see other interesting use cases, then go check out the <a href=\"https://dash.gallery/\">Dash App Gallery</a>.</p><p>If you feel comfortable with the requirements and want to learn how to use Dash in your next project, then continue to the following section!</p><h2>Get Started With Dash in Python</h2><p>In this tutorial, you‚Äôll go through the end-to-end process of building a dashboard using Dash. If you follow along with the examples, then you‚Äôll go from a bare-bones dashboard on your local machine to a styled dashboard deployed on <a href=\"https://www.pythonanywhere.com/\">PythonAnywhere</a>.</p><h3>How to Set Up Your Local Environment</h3><p>To develop your app, you‚Äôll need a new directory to store your code and data. You‚Äôll also need a clean Python <a href=\"https://realpython.com/python-virtual-environments-a-primer/\">virtual environment</a>. To create those, execute the commands below, choosing the version that matches your operating system:</p>","contentLength":3358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Slashdot Asks: Do You Remember Your High School's 'Computer Room'?","url":"https://developers.slashdot.org/story/25/02/02/0233216/slashdot-asks-do-you-remember-your-high-schools-computer-room?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738463760,"author":"EditorDavid","guid":359,"unread":true,"content":"Bill Gates' blog has been updated with short videos about his upcoming book, including one about how his school ended up with an ASR-33 teletype that could connect their Seattle classroom to a computer in California. \"The teachers faded away pretty quickly,\" Gates adds, \"But about six of us stayed hardcore. One was Paul Allen...\" ‚Äî the future co-founder of Microsoft. And the experience clearly meant a lot to Gates. \"Microsoft just never would've happened without Paul ‚Äî and this teletype room.\" \n\nIn a longer post thanking his \"brilliant\" teachers, Gates calls his teletype experience \"an encounter that would shape my entire future\" and \"opened up a whole new world for me.\" Gates also thanks World War II Navy pilot and Boeing engineer Bill Dougall, who \"was instrumental in bringing computer access to our school, something he and other faculty members pushed for after taking a summer computer class... The fascinating thing about Mr. Dougall was that he didn't actually know much about programming; he exhausted his knowledge within a week. But he had the vision to know it was important and the trust to let us students figure it out.\" \n\nGates shared a similar memory about the computer-room's 20-something overseer Fred Wright, who \"intuitively understood that the best way to get students to learn was to let us explore on our own terms. There was no sign-up sheet, no locked door, no formal instruction.\"\n\nInstead, Mr. Wright let us figure things out ourselves and trusted that, without his guidance, we'd have to get creative... Some of the other teachers argued for tighter regulations, worried about what we might be doing in there unsupervised. But even though Mr. Wright occasionally popped in to break up a squabble or listen as someone explained their latest program, for the most part he defended our autonomy... \nMr. Wright gave us something invaluable: the space to discover our own potential.\n \n\nAny Slashdot readers have a similarly impactful experience? Share your own thoughts and memories in the comments. \n\nDo you remember your high school's computer room?","contentLength":2089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Guide to Modern Python String Formatting Tools","url":"https://realpython.com/python-formatted-output/","date":1738418400,"author":"Real Python","guid":168,"unread":true,"content":"<p>In modern Python, you have f-strings and the  method to approach the tasks of interpolating and formatting strings.\nThese tools help you embed variables and expressions directly into strings, control text alignment, and use custom format specifiers to modify how values appear. You can apply these techniques to create well-structured and readable Python code.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li> in Python involves embedding variables and expressions into strings.</li><li>You create an  in Python by prepending a string literal with an  or  and using  to include variables or expressions.</li><li>You can use variables in Python‚Äôs  by placing them inside curly braces and passing them as .</li><li> in Python control how values appear when formatted, using components like <strong>fill, align, sign, width, and type</strong>.</li><li>You align text in Python string formatting using the  component, which can justify text to the  within a specified width.</li></ul><p>When working with strings in Python, you can leverage these formatting techniques to create dynamic and readable output. To get the most out of this tutorial, you should know the basics of Python programming and the <a href=\"https://realpython.com/python-strings/\">string</a> data type.</p><div><p> Test your knowledge with our interactive ‚ÄúA Guide to Modern Python String Formatting Tools‚Äù quiz. You‚Äôll receive a score upon completion to help you track your learning progress:</p></div><p>Python has developed different string interpolation and formatting tools over the years. If you‚Äôre getting started with Python and looking for a quick way to format your strings, then you should use <a href=\"https://realpython.com/python-f-strings/\">Python‚Äôs f-strings</a>.</p><p>If you need to work with older versions of Python or legacy code, then it‚Äôs a good idea to learn about the other formatting tools, such as the  method.</p><p>In this tutorial, you‚Äôll learn how to format your strings using f-strings and the  method. You‚Äôll start with <a href=\"https://realpython.com/ref/glossary/f-string/\">f-strings</a> to kick things off, which are quite popular in modern Python code.</p><h2>Using F-Strings for String Interpolation</h2><p>Python has a string formatting tool called , which stands for <strong>formatted string literals</strong>. F-strings are string literals that you can create by prepending an  or  to the literal. They allow you to do string interpolation and formatting by inserting variables or expressions directly into the literal.</p><h3>Creating F-String Literals</h3><p>Here you‚Äôll take a look at how you can create an f-string by prepending the string literal with an  or :</p><p>Using either  or  has the same effect. However, it‚Äôs a more common practice to use a lowercase  to create f-strings.</p><p>Just like with regular string literals, you can use single, double, or triple quotes to define an f-string:</p><p>Up to this point, your f-strings look pretty much the same as regular strings. However, if you create f-strings like those in the examples above, you‚Äôll get complaints from your code <a href=\"https://realpython.com/python-code-quality/#linters\">linter</a> if you have one.</p><p>The remarkable feature of f-strings is that you can embed Python <a href=\"https://realpython.com/python-variables/\">variables</a> or <a href=\"https://realpython.com/python-operators-expressions/\">expressions</a> directly inside them. To insert the variable or expression, you must use a , which you create using a pair of curly braces.</p><h3>Interpolating Variables Into F-Strings</h3><p>The variable that you insert in a replacement field is evaluated and converted to its <a href=\"https://realpython.com/python-repr-vs-str/\">string representation</a>. The result is interpolated into the original string at the replacement field‚Äôs location:</p><p>In this example, you‚Äôve interpolated the  variable into your string. Note that Python treats anything outside the curly braces as a regular string.</p>","contentLength":3408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build a Tic-Tac-Toe Game With Python and Tkinter","url":"https://realpython.com/tic-tac-toe-python/","date":1738418400,"author":"Real Python","guid":169,"unread":true,"content":"<p>Developing a tic-tac-toe game in Python using Tkinter combines programming logic with graphical user interface design (GUI). This tutorial guides you through creating the game logic and a Tkinter-based GUI to produce a fully functional tic-tac-toe game. You‚Äôll learn how to manage player moves, detect winning combinations, and build an interactive interface with Tkinter widgets.</p><p><strong>By the end of this tutorial, you‚Äôll understand that:</strong></p><ul><li> can be used to implement the  for a tic-tac-toe game.</li><li> provides tools to build a game  using labels and buttons.</li><li>You can connect the  and  to create a fully interactive application.</li></ul><p>Playing computer games is a great way to unwind or challenge yourself, and it‚Äôs also fun and educational to build your own. In this project, you‚Äôll use the Tkinter GUI framework from Python‚Äôs standard library to create a game interface, while applying the model-view-controller pattern and an object-oriented approach to organize your code. For more on these concepts, check out the links in the <a href=\"https://realpython.com/tic-tac-toe-python/#prerequisites\">prerequisites</a>.</p><p>To download the entire source code for this project, click the link in the box below:</p><h2>Demo: A Tic-Tac-Toe Game in Python</h2><p>In this step-by-step project, you‚Äôll build a tic-tac-toe game in Python. You‚Äôll use the <a href=\"https://realpython.com/python-gui-tkinter/\">Tkinter</a> tool kit from the Python <a href=\"https://docs.python.org/3/library/index.html\">standard library</a> to create the game‚Äôs GUI. In the following demo video, you‚Äôll get a sense of how your game will work once you‚Äôve completed this tutorial:</p><p>Your tic-tac-toe game will have an interface that reproduces the classic three-by-three game board. The players will take turns making their moves on a shared device. The game display at the top of the window will show the name of the player who gets to go next.</p><p>If a player wins, then the game display will show a winning message with the player‚Äôs name or mark ( or ). At the same time, the winning combination of cells will be highlighted on the board.</p><p>Finally, the game‚Äôs  menu will have options to reset the game if you want to play again or to exit the game when you‚Äôre done playing.</p><p>If this sounds like a fun project to you, then read on to get started!</p><p>Your goal with this project is to create a tic-tac-toe game in Python. For the game interface, you‚Äôll use the Tkinter GUI tool kit, which comes in the standard Python installation as an <a href=\"https://peps.python.org/pep-0206/#batteries-included-philosophy\">included battery</a>. </p><p>The tic-tac-toe game is for two players. One player plays  and the other plays . The players take turns placing their marks on a grid of three-by-three cells. If a given player gets three marks in a row horizontally, vertically, or diagonally, then that player wins the game. The game will be tied if no one gets three in a row by the time all the cells are marked.</p><p>With these rules in mind, you‚Äôll need to put together the following game components:</p><ul><li>The game‚Äôs , which you‚Äôll build with a class called </li><li>The game‚Äôs , which you‚Äôll manage using a class called </li></ul><p>The game board will work as a mix between  and  in a model-view-controller design. To build the board, you‚Äôll use a Tkinter window, which you can create by instantiating the <a href=\"https://realpython.com/python-gui-tkinter/#building-your-first-python-gui-application-with-tkinter\"></a> class. This window will have two main components:</p><ol><li> Shows information about the game‚Äôs status</li><li> Represents previous moves and available spaces or cells</li></ol><p>You‚Äôll create the game display using a <a href=\"https://realpython.com/python-gui-tkinter/#displaying-text-and-images-with-label-widgets\"></a> widget, which allows you to display text and images.</p><p>For the grid of cells, you‚Äôll use a series of <a href=\"https://realpython.com/python-gui-tkinter/#displaying-clickable-buttons-with-button-widgets\"></a> widgets arranged in a <a href=\"https://realpython.com/python-gui-tkinter/#the-grid-geometry-manager\">grid</a>. When a player clicks one of these buttons, the game logic will run to process the player‚Äôs move and determine whether there‚Äôs a winner. In this case, the game logic will work as the , which will manage the data, logic, and rules of your game.</p><p>Now that you have a general idea of how to build your tic-tac-toe game, you should check out a few knowledge prerequisites that‚Äôll allow you to get the most out of this tutorial.</p><p>To complete this tic-tac-toe game project, you should be comfortable or at least familiar with the concepts and topics covered in the following resources:</p>","contentLength":3946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond DeepSeek: An Overview of Chinese AI Tigers and Their Cutting-Edge Innovations","url":"https://www.topbots.com/chinese-ai-tigers-overview/","date":1738335232,"author":"Kate Koidan","guid":592,"unread":true,"content":"<p>The recent disruption caused by DeepSeek‚Äôs R1 model sent shockwaves through the AI community, demonstrating that Chinese AI advancements may have been underestimated. The model‚Äôs performance, rivaling some of the most advanced offerings from OpenAI and Anthropic at a fraction of the cost, signaled a new era of competition in artificial intelligence.</p><p>However, DeepSeek is not the only Chinese company making waves in AI. While industry giants like Alibaba, Tencent, Baidu, and ByteDance continue to lead the charge, a new generation of AI startups ‚Äì often referred to as the ‚ÄúChinese AI Tigers‚Äù ‚Äì is emerging as formidable players. These startups are pushing the limits of generative AI, challenging global incumbents with state-of-the-art models and breakthrough innovations.</p><p>In this article, we will explore DeepSeek and five of the most influential Chinese AI startups: Moonshot AI, Zhipu AI, Baichuan AI, MiniMax, and 01.AI. Each of these companies has developed cutting-edge AI models and solutions that are shaping the future of artificial intelligence, both in China and beyond.</p><h2>DeepSeek: A Research-Driven AI Powerhouse in China</h2><p>Founded in May 2023, <a href=\"https://www.deepseek.com/\" target=\"_blank\" rel=\"noreferrer noopener\">DeepSeek</a> is an AI company based in Hangzhou, operating as an independent entity under High-Flyer, a leading Chinese quantitative hedge fund. Unlike many AI startups chasing commercialization, DeepSeek prioritizes technical innovation, running more like a research lab than a traditional business. The company is <a href=\"https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas\" target=\"_blank\" rel=\"noreferrer noopener\">focused</a> on developing artificial general intelligence (AGI) through breakthroughs in mathematics, coding, and multimodal AI capabilities.</p><h3>Technical Innovation &amp; Model Efficiency</h3><p>DeepSeek has distinguished itself through cost-effective AI solutions and pioneering architectural advancements. Its research efforts have led to the deployment of novel techniques, including Multi-Head Latent Attention (MLA), sparse Mixture-of-Experts (MoE), and FP8 mixed precision training. These innovations significantly reduce memory requirements and computational costs, allowing DeepSeek to achieve state-of-the-art performance with fewer resources than industry giants like OpenAI, Meta, and Anthropic.</p><p>DeepSeek V3, its most advanced model, <a href=\"https://sg.news.yahoo.com/meet-deepseek-chinese-start-changing-093000172.html\" target=\"_blank\" rel=\"noreferrer noopener\">boasts</a> 671B parameters and was trained in just 55 days at a cost of $5.58 million ‚Äì an order of magnitude more efficient than its Western counterparts. The company has also committed to open-source development, furthering its influence in the AI research community.</p><ul><li> (May 2024): Introduced the MLA architecture, reducing inference costs and intensifying competition in China‚Äôs AI market.</li><li> (December 2024): A 671B-parameter model trained in record time, outperforming Llama 3.1 and Qwen 2.5 while matching GPT-4o and Claude 3.5 Sonnet.</li><li> (January 2025): A <a href=\"https://x.com/deepseek_ai/status/1881318130334814301\" target=\"_blank\" rel=\"noreferrer noopener\">reasoning model</a> based on DeepSeek-V3 that rivals OpenAI‚Äôs o1 in mathematics and coding benchmarks. Open-sourced and free for commercial use, it delivers performance comparable to GPT models at <a href=\"https://venturebeat.com/ai/deepseek-r1s-bold-bet-on-reinforcement-learning-how-it-outpaced-openai-at-3-of-the-cost/\" target=\"_blank\" rel=\"noreferrer noopener\">just 3% of the cost</a>.</li><li> (January 2025): A <a href=\"https://www.reuters.com/technology/deepseeks-janus-pro-ai-model-beats-rivals-image-generation-2025-01-27/\" target=\"_blank\" rel=\"noreferrer noopener\">vision model</a> capable of understanding and generating images from text prompts, outperforming OpenAI‚Äôs DALL-E 3 and Stable Diffusion 3 in multiple image generation benchmarks.</li><li>: Optimized smaller-scale versions that retain high efficiency and strong performance.</li></ul><h3>Leadership &amp; Talent Strategy</h3><p>DeepSeek is led by CEO Liang Wenfeng, a former AI researcher and founder of High-Flyer Quant. Known for his technical idealism and open-source advocacy, Liang <a href=\"https://xyzlabs.substack.com/p/in-depth-behind-deepseeks-success\">believes</a> AGI could be achieved within the next decade. The company‚Äôs talent acquisition strategy is unconventional, focusing on hiring fresh graduates and early-career researchers driven by curiosity rather than experience. This approach fosters a culture of bottom-up innovation, with flexible resource allocation to encourage groundbreaking research.</p><p>Unlike many AI startups dependent on external venture capital, DeepSeek is entirely funded by High-Flyer Quant. The hedge fund‚Äôs <a href=\"https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas\" target=\"_blank\" rel=\"noreferrer noopener\">strategic investment</a> in over 10,000 GPUs before U.S. sanctions has provided DeepSeek with a significant computational advantage.</p><p>While primarily research-driven, DeepSeek has achieved profitability through API revenue. Its aggressive pricing ‚Äì offering API access at just 1/53rd the cost of Claude 3.5 Sonnet ‚Äì has <a href=\"https://xyzlabs.substack.com/p/in-depth-behind-deepseeks-success\" target=\"_blank\" rel=\"noreferrer noopener\">triggered a price war</a> in the AI industry, challenging established players.</p><p>DeepSeek‚Äôs chatbot technology demonstrates strong performance, delivering factual responses with linked resources. By combining technical breakthroughs, open-source accessibility, and an unconventional approach to AI development, DeepSeek is emerging as one of China‚Äôs most formidable AI contenders.</p><h2>Moonshot AI: Pushing the Boundaries of Long-Context LLMs</h2><p><a href=\"https://www.moonshot.cn/\" target=\"_blank\" rel=\"noreferrer noopener\">Moonshot AI</a> (YueZhiAnMian) is a Beijing-based artificial intelligence company founded in March 2023 by AI researchers Yang Zhilin, Zhou Xinyu, and Wu Yuxin. The company‚Äôs name draws inspiration from Pink Floyd‚Äôs album <em>The Dark Side of the Moon</em>, reflecting CEO Yang Zhilin‚Äôs admiration for the band. Moonshot AI specializes in developing LLMs capable of processing extensive text inputs, with a strong emphasis on long-form context and response capabilities.</p><ul><li> (October 2023): A breakthrough AI assistant capable of handling up to 200,000 Chinese characters in a single input, making it a leader in long-form text processing. Available via chat interface and API.</li><li> (January 2025): The latest <a href=\"https://x.com/Kimi_ai_/status/1881332472748851259\" target=\"_blank\" rel=\"noreferrer noopener\">o1-level multimodal model</a>, achieving state-of-the-art performance on short-chain-of-thought (CoT) benchmarks. It significantly outperforms GPT-4o and Claude 3.5 Sonnet on AIME, MATH-500, and LiveCodeBench, with improvements reaching up to +550%.</li><li><strong>Moonshot-V1-Vision-Preview</strong> (January 2025): A multimodal image understanding model that excels in OCR text extraction, image recognition, and advanced visual processing.</li></ul><p>CEO Yang Zhilin, a co-founder of Recurrent AI, previously worked at Meta AI and Google Brain. He <a href=\"https://techcrunch.com/2024/02/21/moonshot-ai-funding-china/\" target=\"_blank\" rel=\"noreferrer noopener\">earned</a> a PhD in computer science from Carnegie Mellon University under the advisement of Ruslan Salakhutdinov. A key contributor to Transformer-XL, Yang‚Äôs expertise in extending language model context windows has been instrumental in shaping Moonshot AI‚Äôs platform.</p><p>Moonshot AI <a href=\"https://canvasbusinessmodel.com/blogs/how-it-works/moonshot-ai-how-it-works\" target=\"_blank\" rel=\"noreferrer noopener\">generates</a> revenue through subscription-based services, pay-per-use API access, and licensing its AI technologies.</p><p>Moonshot AI‚Äôs chatbot delivers high-quality responses with factual accuracy and linked sources. With its expertise in long-context language processing and a strong funding base, Moonshot AI is poised to become a dominant player in China‚Äôs AI landscape, challenging global incumbents with its cutting-edge models.</p><h2>Zhipu AI: Advancing AI with Multimodal and Enterprise Solutions</h2><p>Founded in 2019 as a spin-off from Tsinghua University, <a href=\"https://www.zhipuai.cn/en/\" target=\"_blank\" rel=\"noreferrer noopener\">Zhipu AI</a> (Beijing Zhipu Huazhang Technology) has emerged as a key player in China‚Äôs AI landscape. The company focuses on developing advanced LLMs and multimodal AI applications for both consumer and enterprise use cases, with a <a href=\"https://zoonop.com/articles/zhipu-ai\" target=\"_blank\" rel=\"noreferrer noopener\">mission</a> to ‚Äúteach machines to think like humans.‚Äù</p><ul><li>: An open-source, end-to-end speech large language model, <a href=\"https://technode.com/2025/01/09/meet-chinas-top-six-ai-unicorns-who-are-leading-the-wave-of-ai-in-china/\" target=\"_blank\" rel=\"noreferrer noopener\">released</a> in October 2024, capable of human-like interactions with customizable tone, emotion, and dialect.</li><li>: A 130B-parameter code-generation model.</li><li>: A text-to-image model that generates high-quality, realistic, and novel images from textual descriptions.</li><li>: A <a href=\"https://www.scmp.com/tech/tech-trends/article/3284343/chinas-zhipu-ai-says-its-app-can-operate-your-smartphone-you\" target=\"_blank\" rel=\"noreferrer noopener\">voice-command-driven AI agent</a> designed for smartphone task automation, capable of handling complex requests like ordering from local stores based on user shopping history.</li><li>: A <a href=\"https://bigmodel.cn/\" target=\"_blank\" rel=\"noreferrer noopener\">cloud-based AI service</a> providing enterprise access to Zhipu‚Äôs suite of AI models, including GLP-4-Plus, CogView-3-Plus, CogVideoX, and GLM-4V-Plus.</li></ul><p>Zhipu AI‚Äôs <a href=\"https://datainnovation.org/2024/12/zhipu-ai-chinas-generative-trailblazer-grappling-with-rising-competition/\" target=\"_blank\" rel=\"noreferrer noopener\">leadership team</a> is anchored by CEO Zhang Peng, a Tsinghua University alumnus. The company‚Äôs foundation is deeply rooted in academia, having been co-founded by Tang Jie and Li Juanzi, both esteemed professors at Tsinghua University. Tang Jie, who serves as the Chief Scientist, is also the vice director of the Beijing Academy of Artificial Intelligence, highlighting his significant influence in China‚Äôs AI research community.</p><p>Zhipu AI has attracted significant investment from major Chinese technology firms and venture capitalists. In 2023, the company <a href=\"https://technode.com/2025/01/09/meet-chinas-top-six-ai-unicorns-who-are-leading-the-wave-of-ai-in-china/\" target=\"_blank\" rel=\"noreferrer noopener\">raised RMB 2.5 billion (‚âà$350 million)</a> in funding, with backers including Meituan, Ant Group, Alibaba, Tencent, Xiaomi, Sequoia Capital, and GL Ventures. A December 2024 funding round secured an <a href=\"https://global.chinadaily.com.cn/a/202412/17/WS67611416a310f1265a1d35ff.html\" target=\"_blank\" rel=\"noreferrer noopener\">additional $420 million</a>, valuing the company at over $2.8 billion.</p><p>The company‚Äôs primary revenue stream <a href=\"https://global.chinadaily.com.cn/a/202412/17/WS67611416a310f1265a1d35ff.html\" target=\"_blank\" rel=\"noreferrer noopener\">comes</a> from API access through the Zhipu MaaS Platform. In 2024, API revenue surged more than 30-fold year-over-year, with daily token consumption increasing 150-fold. The MaaS platform has attracted over 700,000 enterprise and developer users.</p><h3>Challenges &amp; U.S. Trade Blacklist</h3><p>In October 2024, Zhipu AI was <a href=\"https://www.ft.com/content/c61b361e-d6fd-407a-8737-55a341b9fa4d\" target=\"_blank\" rel=\"noreferrer noopener\">added to the U.S. trade blacklist</a>, limiting its access to American technology due to concerns about its ties to the Chinese military. Despite these restrictions, Zhipu AI has remained resilient, maintaining strong domestic growth and continued support from major Chinese investors.</p><p>While Zhipu AI faces stiff competition from other Chinese AI tigers, it continues to solidify its position in the Chinese AI ecosystem through a strong enterprise focus and cutting-edge multimodal models. However, its chatbot‚Äôs performance lags behind competitors, often struggling with precise, nuanced responses and occasionally mixing English and Chinese in its outputs. Nonetheless, its rapid growth and financial backing make it a formidable contender in China‚Äôs AI race.</p><h2>Baichuan AI: China‚Äôs OpenAI Challenger</h2><p>Founded in April 2023, <a href=\"https://www.baichuan-ai.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Baichuan AI</a> (Beijing Baichuan Intelligent Technology Co., Ltd.) is a Beijing-based artificial intelligence company led by Wang Xiaochuan, the former CEO of Sogou. The company positions itself as China‚Äôs equivalent of OpenAI, focusing on AI-driven solutions in healthcare, education, and finance. One of its core missions is leveraging AI to tackle systemic issues such as the global shortage of skilled doctors.</p><ul><li>: Baichuan-7B, Baichuan-13B, Baichuan2-7B, and Baichuan2-13B.</li><li>:\n<ul><li>: A multimodal model with extended context and integrated search capabilities.</li><li>: Uses a MoE (Mixture of Experts) architecture to optimize costs and significantly enhance speed.</li><li>: Tailored for high-frequency enterprise use, improving response time and efficiency.</li></ul></li><li>: Competing with local AI offerings like Baidu‚Äôs Ernie Bot and Moonshot AI‚Äôs Kimi, Baixiaoying has gained recognition for its superior search capabilities, functioning ‚Äúlike a professional‚Äù according to company claims.</li></ul><p>Baichuan AI is led by founder and CEO Wang Xiaochuan, who previously helmed Sogou, once China‚Äôs second-largest search engine before its acquisition by Tencent. The company‚Äôs core team <a href=\"https://siliconangle.com/2024/07/25/alibaba-backed-chinese-ai-startup-baichuan-raises-691m/\" target=\"_blank\" rel=\"noreferrer noopener\">comprises</a> AI experts from leading global tech firms, including Google, Tencent, Baidu, Huawei, Microsoft, and ByteDance, bringing together deep expertise in AI development and deployment.</p><p>Baichuan AI has received strong backing from major Chinese technology investors. In October 2023, the company raised <a href=\"https://en.jiemian.com/article/10249669.html\" target=\"_blank\" rel=\"noreferrer noopener\">$300 million in a Series A1</a> funding round, led by Alibaba, Tencent, and Xiaomi. This was followed by a Series A round in July 2024, which secured an <a href=\"https://siliconangle.com/2024/07/25/alibaba-backed-chinese-ai-startup-baichuan-raises-691m/\" target=\"_blank\" rel=\"noreferrer noopener\">additional $691 million</a>, bringing its valuation to $2.7 billion.</p><p>Baichuan AI‚Äôs business model <a href=\"https://platform.baichuan-ai.com/homePage\" target=\"_blank\" rel=\"noreferrer noopener\">revolves around API monetization</a>, offering enterprise access to its proprietary models. Additionally, it provides domain-specific AI solutions across healthcare, finance, education, and entertainment. The company also customizes AI solutions for enterprise clients, leveraging its foundational models to address specific industry needs.</p><p>With a clear ambition to rival OpenAI, Baichuan AI is rapidly expanding its technological footprint in China. Its robust funding, seasoned leadership, and advanced AI models position it as a key player in China‚Äôs AI landscape.&nbsp;</p><h2>MiniMax: Advancing Multimodal AI Solutions</h2><p>Founded in December 2021, <a href=\"https://minimaxi.com/en\" target=\"_blank\" rel=\"noreferrer noopener\">MiniMax</a> is a cutting-edge artificial intelligence company dedicated to developing large-scale AI models and multimodal technologies. The company integrates text, voice, and vision capabilities into its AI solutions, positioning itself as a key player in China‚Äôs AI landscape.</p><ul><li> (June 2023): An AI companion app allowing users to engage with virtual characters, including digital recreations of celebrities. The app <a href=\"https://www.wsj.com/tech/ai/one-of-americas-hottest-entertainment-apps-is-chinese-owned-04257355\" target=\"_blank\" rel=\"noreferrer noopener\">gained significant traction</a> in international markets, particularly among U.S. teenagers, ranking as the fifth most downloaded free entertainment app in the U.S.</li><li> (March 2024): A multimodal large language model consumer platform offering AI-generated text and music features. <a href=\"https://finance.yahoo.com/news/chinese-ai-tiger-minimax-launches-093000322.html\" target=\"_blank\" rel=\"noreferrer noopener\">Expanded</a> in September 2024 with , a text-to-video model producing high-resolution videos from text prompts.</li><li>:\n<ul><li>: A 456B-parameter <a href=\"https://techcrunch.com/2025/01/15/chinese-ai-company-minimax-releases-new-models-it-claims-are-competitive-with-the-industrys-best/\" target=\"_blank\" rel=\"noreferrer noopener\">text-only model</a> that surpasses Google‚Äôs Gemini 2.0 Flash on MMLU and SimpleQA benchmarks. With a 4M-token context window, it processes over 3 million words in a single input ‚Äì approximately five times the length of .</li><li>: A <a href=\"https://techcrunch.com/2025/01/15/chinese-ai-company-minimax-releases-new-models-it-claims-are-competitive-with-the-industrys-best/\" target=\"_blank\" rel=\"noreferrer noopener\">multimodal model</a> integrating text and visual inputs, excelling in ChartQA benchmarks. While it rivals Anthropic‚Äôs Claude 3.5 Sonnet, it falls short of OpenAI‚Äôs GPT-4o, Google‚Äôs Gemini 2.0 Flash, and InternVL2.5 in some evaluations.</li><li>: An advanced <a href=\"https://techcrunch.com/2025/01/15/chinese-ai-company-minimax-releases-new-models-it-claims-are-competitive-with-the-industrys-best/\" target=\"_blank\" rel=\"noreferrer noopener\">speech synthesis model</a> supporting 17 languages, including English and Chinese. It enables voice cloning from a mere 10-second sample and is available exclusively via MiniMax‚Äôs API and Hailuo AI platform.</li></ul></li></ul><p>MiniMax was <a href=\"https://thebambooworks.com/minimax-scores-big-new-funds-but-likely-to-need-more-in-expensive-ai-race/\" target=\"_blank\" rel=\"noreferrer noopener\">founded</a> by Yan Junjie and Zhou Yucong, both former SenseTime employees. Yan led deep learning toolchain development at SenseTime, while Zhou spearheaded its algorithms R&amp;D team. The leadership team also <a href=\"https://thebambooworks.com/minimax-scores-big-new-funds-but-likely-to-need-more-in-expensive-ai-race/\">includes</a> Yang Bin, a former Uber AI researcher known for his work in autonomous driving technologies.</p><p>MiniMax has attracted substantial investment from leading Chinese technology firms. In June 2023, the company raised <a href=\"https://www.verdict.co.uk/minimax-raising-250m-for-ai/\" target=\"_blank\" rel=\"noreferrer noopener\">over $250 million</a>, backed by a Tencent-led entity, bringing its valuation to $1.2 billion. A subsequent funding round in March 2024, led by Alibaba Group, <a href=\"https://www.cdomagazine.tech/aiml/alibaba-leads-600-mn-investment-round-for-chinese-ai-startup-minimax\" target=\"_blank\" rel=\"noreferrer noopener\">secured $600 million</a>, raising its valuation to $2.5 billion. Investors also include Hillhouse Investment, HongShan, IDG Capital, and Tencent.</p><p>MiniMax operates under a hybrid business model that balances open-source development, API services, and product innovation. The company releases select AI models with licensing restrictions to prevent their use in developing competing AI systems. It also provides API access to its AI capabilities, allowing third-party developers to integrate MiniMax‚Äôs technologies into their applications. Additionally, MiniMax develops both B2B solutions and consumer-facing applications, such as AI-powered role-playing platforms and text-to-video generators, positioning itself as a versatile player in the AI ecosystem.</p><p>MiniMax has faced <a href=\"https://techcrunch.com/2025/01/15/chinese-ai-company-minimax-releases-new-models-it-claims-are-competitive-with-the-industrys-best/\" target=\"_blank\" rel=\"noreferrer noopener\">regulatory challenges</a>, including the removal of Talkie from Apple‚Äôs App Store in December 2024 due to concerns over unauthorized AI avatars of public figures. Additionally, the company has been accused of using copyrighted content in its training data by British TV channels and Chinese streaming service iQIY. Despite these hurdles, MiniMax continues to innovate and expand its AI capabilities, solidifying its position in China‚Äôs AI sector.</p><h2>01.AI: Industry-Specific AI Company Led by Kai-Fu Lee</h2><p>Founded in March 2023, <a href=\"http://01.ai\" target=\"_blank\" rel=\"noreferrer noopener\">01.AI</a> is a Beijing-based artificial intelligence startup led by Kai-Fu Lee. The company focuses on developing smaller, industry-specific AI models while balancing both open-source and proprietary solutions.</p><ul><li> (November 2023): Released three model variants ‚Äì Yi-6B, Yi-9B, and Yi-34B ‚Äì designed for text generation, understanding, and complex reasoning. These models serve as the foundation for 01.AI‚Äôs large-scale language model offerings.</li><li> (Early 2024): The company‚Äôs first closed-source model, built for high-performance reasoning and content creation.</li><li>: A specialized coding model excelling in code completion, editing, and long-context modeling, supporting up to 128K tokens. It also demonstrates strong mathematical reasoning.</li><li> (May 2024): A personal AI workspace and productivity tool aimed at enhancing user efficiency.</li></ul><p>01.AI is led by Kai-Fu Lee, a renowned AI pioneer and former executive at Microsoft and Google. Lee is also the co-founder of Sinovation Ventures, a leading Chinese venture capital firm. The company has a team of over 100 AI specialists and experienced business professionals, many of whom previously worked with Lee at top tech firms.</p><p>01.AI has secured significant investments, <a href=\"https://tracxn.com/d/companies/01.ai/__-gDw3QA2cxhEHCuampmbLXvEk9cBAO1Opo3mdn774Kg#funding-and-investors\" target=\"_blank\" rel=\"noreferrer noopener\">raising $300 million</a> in November 2023 from Tencent, Xiaomi, Alibaba Cloud, and Sinovation Ventures, which brought its valuation to $1 billion. In August 2024, the company <a href=\"https://36kr.com/p/2894555774769792\" target=\"_blank\" rel=\"noreferrer noopener\">reportedly</a> secured additional funding worth hundreds of millions from an international strategic investor, a Southeast Asian consortium, and other institutions.</p><p>The company operates under a dual approach, balancing open-source accessibility with proprietary solutions. Its smaller models are open-source, allowing for academic and commercial use while maintaining strong performance across various AI benchmarks. In parallel, 01.AI offers API access to its closed-source models, providing six <a href=\"https://www.turingpost.com/p/01ai\" target=\"_blank\" rel=\"noreferrer noopener\">enterprise-optimized APIs</a>, including Yi-Large, Yi-Large-Turbo API, Yi-Medium API, Yi-Medium-200K API, Yi-Vision API, and Yi-Spark API. Additionally, the company develops consumer-facing AI applications such as Wanzhi, an AI productivity tool, and RuYi, a digital human solution, further expanding its reach in both enterprise and individual user markets.</p><p>In August 2024, 01.AI <a href=\"https://www.turingpost.com/p/01ai\" target=\"_blank\" rel=\"noreferrer noopener\">announced</a> that its Wanzhi productivity app had reached 10 million users, generating over 100 million yuan ($13.8 million) in revenue. With a strong balance between open-source development and enterprise-driven AI solutions, 01.AI is positioning itself as a key player in China‚Äôs AI sector, offering scalable and specialized AI tools for a range of industries.</p><h2>Conclusion: The Future of China‚Äôs AI Innovators</h2><p>The rapid advancements made by DeepSeek and the Chinese AI Tigers highlight the country‚Äôs growing prominence in artificial intelligence. These startups are not only competing with global tech giants but also setting new benchmarks in efficiency, multimodal capabilities, and enterprise applications. As China continues to invest heavily in AI research and development, the next wave of innovations from these companies could reshape the global AI landscape.</p><h2>Enjoy this article? Sign up for more AI updates.</h2><p>We‚Äôll let you know when we release more overview articles like this one.</p>","contentLength":18583,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Real Python Podcast ‚Äì Episode #237: Testing Your Python Code Base: Unit vs. Integration","url":"https://realpython.com/podcasts/rpp/237/","date":1738324800,"author":"Real Python","guid":167,"unread":true,"content":"<p>What goes into creating automated tests for your Python code? Should you focus on testing the individual code sections or on how the entire system runs? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.</p>","contentLength":269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A WebAssembly compiler in 192 bytes","url":"https://javascriptweekly.com/issues/721","date":1738281600,"author":"","guid":599,"unread":true,"content":"<a href=\"https://javascriptweekly.com/link/165114/rss\"><img src=\"https://res.cloudinary.com/cpress/image/upload/c_limit,w_480,h_480,q_auto/copm/7de26641.png\" width=\"140\" height=\"140\"></a><p> ‚Äî Take your knowledge to the next level with the most loved JavaScript course in the industry. Deepen your understanding of the most important aspects of JavaScript. This highly rated video course goes under the hood, looking at callbacks, higher-order functions, object-oriented JS, and&nbsp;more.</p>","contentLength":297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"gRPC vs REST vs GraphQL: Comparison & Performance","url":"https://www.youtube.com/watch?v=uH0SxYdsjv4","date":1738266883,"author":"Anton Putra","guid":532,"unread":true,"content":"<article>gRPC vs REST vs GraphQL Performance Benchmark.\n\nüî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)\n\nüçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&amp;si=p-UOaVM_6_SFx52H\n\nüëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience &amp; Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)\n‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra\n‚ñ∫  Twitter/X: https://twitter.com/antonvputra\n‚ñ∫  Instagram: https://www.instagram.com/aputrabay\n‚ñ∫  GitHub: https://github.com/antonputra\n‚ñ∫  Email: me@antonputra.com\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/240/lessons/240\n\n#python #golang #nodejs</article>","contentLength":1004,"flags":null,"enclosureUrl":"https://www.youtube.com/v/uH0SxYdsjv4?version=3","enclosureMime":"","commentsUrl":null},{"title":"Oracle Faces Java Customer Revolt After 'Predatory' Pricing Changes","url":"https://developers.slashdot.org/story/25/01/30/1617210/oracle-faces-java-customer-revolt-after-predatory-pricing-changes?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738255260,"author":"msmash","guid":358,"unread":true,"content":"Nearly 90% of Oracle Java customers are looking to abandon the software maker's products following controversial licensing changes made in 2023, according to research firm Dimensional Research. \n\nThe exodus reflects growing frustration with Oracle's shift to per-employee pricing for its Java platform, which critics called \"predatory\" and could increase costs up to five times for the same software, Gartner found. The dissatisfaction runs deepest in Europe, where 92% of French and 95% of German users want to switch to alternative providers like Bellsoft Liberica, IBM Semeru, or Azul Platform Core.","contentLength":602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Slow, flaky, and failing","url":"https://bitfieldconsulting.com/posts/slow-flaky-failing","date":1738242900,"author":"John Arundel","guid":17,"unread":true,"content":"<blockquote><p><em>If you find yourself working on a project with quite a few broken\nwindows, it‚Äôs all too easy to slip into the mindset of ‚ÄúAll the rest of\nthis code is crap, I‚Äôll just follow suit.‚Äù</em>\n‚ÄîDavid Thomas &amp; Andrew Hunt, <a href=\"https://amzn.to/3yGUnhD\">‚ÄúThe\nPragmatic Programmer: Your Journey to Mastery‚Äù</a></p></blockquote><p>It‚Äôs one minute to ship time, and you hit ‚Äúpush‚Äù on the very last\ncommit. There it goes: the build is running. Every second counts now,\nand you watch the test output with increasing impatience. Why do the\ntests take so darned ?</p><p>And then, to your horror, the first red lights start to appear. ‚ÄúBut\nthese were passing before, and I haven‚Äôt touched that code!‚Äù you wail.\nIt‚Äôs no good: your co-workers are already giving you the stink eye for\nbreaking the build and holding them up.</p><p>You‚Äôre not going to ship today, for one simple reason: your tests are\nslow, flaky, and failing. So what the hell?</p><p>Flaky tests sometimes fail, sometimes pass, regardless of whether the\nsystem is correct. There are many reasons for flaky tests, so let‚Äôs look\nat a couple of them, with some possible solutions.</p><p> can be a source of flakiness, as you probably\nknow from experience. In particular, fixed sleeps in tests are a bad\nidea (see the next section for more about these). Eliminate these\nwherever possible and replace them with code that only waits as long as\nstrictly necessary.</p><p>When you need to  timing itself, use the shortest\npossible interval. For example, don‚Äôt test a timer function with a\none-second duration when one millisecond would work just as well.</p><p>In some tests, as crazy as it sounds, the  can\naffect the test. One way to eliminate this cause of flakiness is by\nturning , and if necessary injecting a fake\n function to return a canned time of day.</p><p>Flakiness can also sometimes arise from .\nSome data structures in Go are inherently unordered: maps, for example.\nComparing these needs special care.</p><p>For example, iterating over a map comparing its elements is not good\nenough: the iteration order of maps is unspecified in Go. Instead, we\ncan use the  function to compare maps regardless\nof iteration order:</p><pre><code></code></pre><p>On the other hand, slices  inherently ordered, and so\n requires this:</p><pre><code></code></pre><p>But sometimes we don‚Äôt actually care about the order. Maybe we get\nthese results from some concurrent computations, and we don‚Äôt know what\norder they will show up in. We just want to know that we \nthe right results.</p><p>To compare two slices for equal , then, regardless\nof order, we can use  to sort them before the\ncomparison:</p><pre><code></code></pre><p>Whatever the cause of a flaky test suite, it‚Äôs a serious problem.\nLeft untreated, it will continuously erode value from the tests, until\neventually they become useless and ignored by all. It should be a red\nflag to hear something like ‚ÄúOh yeah, that test just fails\nsometimes.‚Äù</p><p>As soon as you hear that, you know that the test has become useless.\nDelete it, if the flakiness really can‚Äôt be fixed. Thou shalt not suffer\na flaky test to live. As soon as it starts flaking, it stops being a\nuseful source of feedback, and bad tests are worse than no tests.</p><p>A  test is not the same thing as a flaky test: a\nbrittle test fails when you change something unrelated, whereas a flaky\ntest fails when it feels like it. Fixing brittle tests is usually a\nmatter of decoupling entangled components, or simply reducing the scope\n(and thus sharpening the focus) of the test.</p><p>On the other hand, flaky tests can require some time and effort to\nfind the underlying cause and address it. Only do this if the test is\nreally worth it; if not, just delete it.</p><p>What if some tests aren‚Äôt just flaky, but fail all the time, because\nbugs aren‚Äôt being fixed? This is a very dangerous situation, and without\nprompt action the tests will rapidly become completely useless.</p><p>Why? Because if tests are allowed to fail for a while without being\nfixed, people soon stop trusting them, or indeed paying \nattention to them: ‚ÄúOh yeah, that test always fails.‚Äù</p><p>We can never have any failing tests, just as we can never have any\nbugs:</p><p>As soon as any test starts failing, fixing it should be everyone‚Äôs\ntop priority. No one is allowed to deploy any code change that‚Äôs not\nabout fixing this bug. Once you let one failing test slip through the\nnet, all the other tests become worthless.</p><p>This so-called  sounds radical, but\nit really isn‚Äôt. After all, what‚Äôs the alternative?</p><blockquote><p><em>The very first version of Microsoft Word for Windows was\nconsidered a ‚Äúdeath march‚Äù project. Managers were so insistent on\nkeeping to the schedule that programmers simply rushed through the\ncoding process, writing extremely bad code, because bug-fixing was not a\npart of the formal schedule.</em><em>Indeed, the schedule became merely a checklist of features waiting\nto be turned into bugs. In the post-mortem, this was referred to as\n‚Äúinfinite defects methodology‚Äù.</em>\n‚ÄîJoel Spolsky, <a href=\"https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/\">‚ÄúThe\nJoel Test: 12 Steps to Better Code‚Äù</a></p></blockquote><p>Fixing bugs now is cheaper, quicker, and makes more business sense\nthan fixing them later. The product should be ready to ship at all\ntimes, without bugs.</p><p>If you already  a large backlog of bugs, or failing\ntests, but the company‚Äôs still in business, then maybe those bugs aren‚Äôt\nreally that critical after all. The best way out may be to declare\nvoluntary : just close all old bugs, or delete\nall failing tests. Bugs that people  care about will pretty\nsoon be re-opened.</p><p>My book <a href=\"https://bitfieldconsulting.com/books/tests\">The Power of Go: Tests</a> is all\nabout how to write  tests: not just box-ticking\nexercises to satisfy some bureaucratic manager, but tests that really\nadd value to the code, and make your work easier and more enjoyable.</p><p>Even the world‚Äôs greatest test suite does us no good, though, if it\ntakes too long to run. How long is too long? Well, if we‚Äôre running\ntests every few minutes, clearly even a few minutes is too long. We\nsimply won‚Äôt run the tests often enough to get the fast feedback we need\nfrom them.</p><blockquote><p><em>By running the test suite frequently, at least several times a\nday, you‚Äôre able to detect bugs soon after they are introduced, so you\ncan just look in the recent changes, which makes it much easier to find\nthem.</em>\n‚ÄîMartin Fowler, <a href=\"https://martinfowler.com/bliki/SelfTestingCode.html\">‚ÄúSelf-Testing\nCode‚Äù</a></p></blockquote><p>One way or the other, then, we don‚Äôt want to be more than about five\nminutes away from passing tests. So, again, how long is \nlong for a test suite to run?</p><p>Kent Beck suggests that ten minutes is a psychologically significant\nlength of time:</p><blockquote><p><em>The equivalent of 9.8 m/s¬≤ is the ten-minute test suite. Suites\nthat take longer than ten minutes inevitably get trimmed, or the\napplication tuned up, so the suite takes ten minutes again.</em>\n‚ÄîKent Beck, <a href=\"https://amzn.to/3OR9pqg\">‚ÄúTest-Driven Development\nby Example‚Äù</a></p></blockquote><p>We may perhaps call this psychological limit the .\nBeyond the ten-minute mark, the problem is so obvious to everybody that\npeople are willing to put effort into speeding up the test suite. Below\nthat time, people will probably grumble but put up with it.</p><p>That certainly doesn‚Äôt mean that a ten-minute test suite is okay:\nit‚Äôs not, for the reasons we‚Äôve discussed. Let‚Äôs look at a few simple\nways to reduce the overall run-time of the test suite to something more\nmanageable.</p><ol type=\"1\"><li><p>. The inability to run certain\ntests in parallel is usually a design smell. Refactor so that each test\nhas its own world, touches no global state, and can thus run in\nparallel. Adding parallelism to a suite that doesn‚Äôt have it should\nspeed it up by about an order of magnitude.</p></li><li><p><strong>Eliminate unnecessary I/O</strong>. Once you go off the\nchip, things get slow. Do everything on the chip as far as possible,\navoiding I/O operations such as network calls or accessing disk files.\nFor example, you could use an  as an in-memory\nfilesystem, and memory-backed s and\ns instead of real files.</p></li><li><p>. Instead of calling some\nremote API, call a local fake instead. Local networking happens right in\nthe kernel, and while it‚Äôs still not , it‚Äôs a lot faster\nthan actually going out onto the wire.</p></li><li><p><strong>Share fixtures between tests</strong>. Any time you have\nsome expensive fixture setup to do, such as loading data into a\ndatabase, try to share its cost between as many tests as possible, so\nthat they can all use it. If necessary, do the setup in a single test\nand then run a bunch of subtests against it.</p><p>However, we need to be careful that the tests don‚Äôt then become flaky\nas a result of too much fixture sharing. A flaky test is worse than a\nslow test.</p></li><li><p>. A test that can‚Äôt proceed until\nsome concurrent operation has completed should use the ‚Äúwait for\nsuccess‚Äù pattern (loop and retry, with a tiny delay, until the operation\nhas completed). This minimises wasted time, whereas a long fixed sleep\nmaximises it (or causes flaky tests, which is also bad).</p></li><li><p><strong>Throw hardware at the problem</strong>. When you‚Äôve made\nthe test suite as fast as it can go and it‚Äôs still slow, just run it on\na faster computer. If the tests are mostly CPU-bound, rent a 256-core\ncloud machine and have it pull and run the tests on demand. CPU time\ncosts a lot less than programmer time, especially since hiring cheap\nprogrammers is a false economy.</p></li><li><p>. This is a last resort,\nbut it might come to that. If you have a few tests that simply\n be speeded up any more, and they‚Äôre dragging down the\nrest of the suite, extract them to a separate ‚Äúslow test‚Äù suite, and run\nit on a schedule. Every night, perhaps; certainly no less frequently\nthan that. Even nightly isn‚Äôt great, but it‚Äôs better than not running\ntests at all.</p></li></ol>","contentLength":9340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Local Diffusion Inference with Stable-Diffusion.cpp and Flux.1","url":"https://medium.com/@kyodo-tech/local-diffusion-inference-with-stable-diffusion-cpp-and-flux-1-5d1841fff9cf?source=rss-ac02ab142942------2","date":1738238284,"author":"Kyodo Tech","guid":111,"unread":true,"content":"<p><a href=\"https://github.com/leejet/stable-diffusion.cpp\">Stable-Diffusion.cpp</a> is designed for efficient, high-performance inference of Stable Diffusion models, especially on low-resource environments where traditional deep learning pipelines struggle. We provide an examination of SDcpp, its components, and the technical concepts that enable efficient inference of Stable Diffusion models on lower-end hardware such as CPUs and lightweight GPUs, such as Apple M-series chips. In particular, we‚Äôre looking at <a href=\"https://github.com/black-forest-labs/flux\">Flux.1</a>, a text-to-image model developed by <a href=\"https://blackforestlabs.ai/\">Black Forest&nbsp;Labs</a>.</p><h3>Introduction to Diffusion Models and Stable Diffusion</h3><p>Diffusion models, specifically Denoising Diffusion Probabilistic Models (DDPMs), have emerged as powerful generative frameworks capable of producing high-fidelity images from random noise through iterative denoising processes. Stable Diffusion extends the DDPM framework by incorporating text conditioning, allowing for the generation of images based on descriptive prompts. However, the computational demands of such models typically necessitate high-memory, high-bandwidth GPU resources, limiting their accessibility and scalability.</p><p>SDcpp addresses these limitations by providing an optimized inference framework that leverages techniques such as quantization, memory-efficient sampling, and optimized latent space representations to facilitate the deployment of Stable Diffusion models on devices with constrained computational resources. This paper delves into the architectural and algorithmic components of SDcpp, highlighting the methodologies that underpin its efficiency and performance.</p><p>The architecture is composed of several interdependent modules, each contributing to the inference process. First, it is <strong>loading the required models</strong> (1), including text encoders, the diffusion model, and the VAE decoder. It then performs  (2), tokenizing the input prompt and generating text embeddings using a CLIP-based model and an additional T5XXL text conditioning model. Next, it <strong>computes the conditioning graph</strong> (3), transforming embeddings to create a guidance vector for the diffusion process. The system then  (4) by iteratively refining a randomly initialized latent tensor using a noise schedule and a chosen sampling method, progressively steering it towards a structured latent representation in a number of . This step is the most compute-intensive, consuming roughly 90% of the total processing time. Once sampling completes, the pipeline <strong>decodes the latent representation</strong> (5) using a VAE model to reconstruct pixel-space information. Finally, it  (6) as an image file, completing the text-to-image generation process.</p><h4>Diffusion Model Core Components</h4><p>The diffusion model within SDcpp is the principal generative engine, responsible for transforming Gaussian noise into coherent images through a series of denoising . More steps allow finer details and smoother outputs but increase inference time, as each step requires additional computation. This process is governed by the DDPM framework, wherein the model iteratively refines the image by removing noise in a controlled manner. The diffusion model encompasses three primary components:</p><p><strong>UNet (Denoising Network):</strong> the UNet architecture employs an encoder-decoder structure with skip connections, enabling multi-scale feature extraction and reconstruction. The network predicts and removes noise at each denoising step, progressively enhancing image&nbsp;quality.</p><p><strong>Text Encoder (Conditioning Module):</strong> The text encoder converts textual prompts into latent embeddings that guide the denoising process. SDcpp supports multiple text encoders, including CLIP-L, CLIP-G, and T5XXL, each tailored for different model versions and memory considerations.</p><p><strong>Scheduler (Noise Controller):</strong> The scheduler orchestrates the noise removal strategy, dictating the temporal progression of denoising steps. It determines the rate and manner in which noise is reduced, balancing computational efficiency and image fidelity.</p><p>A recent model, <a href=\"https://huggingface.co/black-forest-labs/FLUX.1-schnell\"></a>&nbsp;, is optimized for low-resource environments, featuring a compact UNet architecture that accelerates inference by reducing the number of denoising steps. Flux.1 is designed for memory-efficient quantization and compatibility with the GGUF format, making it suitable for deployment on lightweight hardware. It reduces inference steps to only 4 steps and can achieve fast inference.</p><h4>Variational Autoencoder (VAE)</h4><p>VAEs facilitating the encoding of images into a lower-dimensional latent space. This reduction significantly diminishes computational and memory overhead while retaining high-resolution image details. Latent space is a <strong>compact mathematical representation</strong> of an image. Think of it as a highly efficient shorthand where only the most essential details are stored. Instead of storing every pixel, the image is encoded as numbers that describe its features. This makes it possible to work with images at a much lower computational cost. Stable Diffusion doesn‚Äôt generate images pixel by pixel. Instead, it creates images in latent space, where information is stored in a more compact and manageable way. Once the model finishes generating an image in this space, the VAE  it back into a high-resolution format that we can&nbsp;see.</p><p>Different VAEs can influence how well the final image looks. A generic VAE works for most images, but a model trained for a specific style, like anime or realistic photography, may use a custom VAE to better match its training data. A poorly matched VAE can cause color shifts, blurriness, or unwanted artifacts.</p><p>Technically, any model can use any VAE, but results vary. A VAE trained on cartoon-style images won‚Äôt reconstruct a photorealistic image correctly. This is why some models provide their own VAEs‚Ää‚Äî‚Ääthey have been optimized to maintain details that matter most for a specific&nbsp;style.</p><h4>LoRA (Low-Rank Adaptation)</h4><p>LoRA facilitates model customization without the need to adjust the entire diffusion model. By modifying only a small subset of parameters (~10 million), LoRA significantly reduces memory usage compared to full model fine-tuning. Multiple LoRAs can be combined with weighted influences on the final image, enabling diverse stylistic and feature modifications. For instance, combining two LoRAs with different weights can be executed using the syntax &lt;lora:styleA:0.6&gt; &lt;lora:styleB:0.8&gt;. To optimize performance, it is advisable to minimize the number and strength of LoRAs, especially in memory-limited environments.</p><h4>Training and Inference File&nbsp;Formats</h4><p>The  is the standard PyTorch checkpoint format, that stores full precision weights and optimizer states. Supports training and fine-tuning but can execute arbitrary Python code, posing security risks. <strong>SafeTensors (.safetensors)</strong> is a safer alternative to CKPT, with structured, immutable storage and no executable code. Loads faster but still used mainly for training.</p><p><strong>GGUF (GPT-Generated Unified Format)</strong> is a binary model format introduced by llama.cpp, succeeding GGML. Initially designed for language models (like LLaMA models), GGUF has been adopted in diffusion models to enable efficient, quantized inference without reformatting model files into proprietary formats (e.g.,&nbsp;.ggml or&nbsp;.gguf conversions required previously). Traditional formats (.safetensors,&nbsp;.ckpt,&nbsp;.bin) store full-precision FP16/FP32 model weights, consuming significant VRAM. GGUF introduces quantization at multiple levels, storing models in 2-bit to 16-bit precision while maintaining compatibility with&nbsp;SDcpp.</p><p>Use GGUF for efficient inference on low-resource devices, prefer&nbsp;.safetensors for training.</p><p>In our experiments,  demonstrates an extremely fast denoising process, with images at 1 and 2 steps appearing nearly identical, suggesting that most structural information is determined in the very first iterations. By 4 steps, images diverge significantly, and by 6 steps, they change even more, indicating that additional steps introduce large variations rather than just refining details. Interestingly, <strong>Euler and Euler_a produce the best results</strong>, likely because they follow a balanced noise reduction trajectory, allowing structure to form naturally without over-smoothing or excessive deviations. Euler introduces noticeable changes between 3, 4, and 6 steps, including shifts in pose, face, and hair color, implying that it follows a latent space trajectory where small step changes can drastically impact the final result. Euler_a, being an ancestral sampler, adds controlled randomness at each step, further amplifying these changes. In contrast, <strong>LCM performs poorly with Flux 1</strong>, excessively blurring images, likely due to its tuning for models that require more iterative refinement, making it unsuitable for Flux 1‚Äôs fast-converging architecture. We observed that <strong>4 steps often produce better images</strong> than 5 or 6 steps, as extra iterations introduce unnecessary alterations instead of improving details, reinforcing that Flux 1 is optimized for extremely low-step inference. This is a stark contrast to Stable Diffusion models like SD 1.5 or SDXL, which typically require 20‚Äì50 steps for high-quality outputs. These findings highlight that Flux 1‚Äôs unique training favors fewer steps with the right sampler, making Euler and Euler_a better choices at around 4 steps, while models like LCM fail due to excessive denoising.</p><h4>Denoising Schedules and Sampling Integration</h4><p>Denoising schedules define a series of noise levels that guide reverse diffusion. These schedules set discrete or continuous noise decay steps, essentially parameterizing the stochastic differential equation used to reverse the diffusion process. Discrete schedules apply abrupt noise transitions, forcing the model to rapidly shift between latent states. In contrast, smooth schedules such as karras or exponential distribute noise reduction evenly, supporting more accurate latent&nbsp;updates.</p><p>Sampling methods numerically integrate these latent trajectories. Euler-type samplers update the latent state with fixed time step approximations, benefiting from stable transitions when paired with smooth noise decay. Latent Consistency Models (LCM) depend on gradual noise reduction; abrupt schedules can induce over-smoothing and detail loss. Our observations indicate that matching a sampling method‚Äôs sensitivity to noise decay with an appropriate denoising schedule is key to preserving image structure while ensuring efficient inference.</p><p>Flux.1‚Äôs behavior indicates that its training regime emphasizes rapid structure formation in early steps. Euler‚Äôs robustness across discrete, exponential, and karras schedules suggests that its integration of abrupt noise transitions still aligns well with Flux.1‚Äôs latent dynamics. LCM, being more sensitive to the noise decay, only works well with smoother schedules (exponential and karras) that provide gradual latent&nbsp;updates.</p><p>The failures with AYS and GITS imply that these schedules impose noise trajectories incompatible with Flux.1‚Äôs fast-convergence design. Their latent paths likely diverge from the model‚Äôs learned reverse diffusion trajectory, causing the state to collapse into uniform or noisy outputs rather than structured images. This sensitivity reinforces that Flux.1 is optimized for few-step denoising with specific noise decay characteristics, and departures from that‚Ää‚Äî‚Ääespecially with schedules that enforce alternative trajectory regularity‚Ää‚Äî‚Ääcan disrupt the image generation process.</p><p>Flux.1 departs from prior diffusion models by leveraging a T5-style text encoder instead of CLIP, allowing it to process full sentences fluently rather than relying on discrete token hierarchies. Unlike Stable Diffusion, where Booru-style tags and keyword emphasis are required for precision, <strong>Flux.1 understands natural language</strong> natively, reducing reliance on rigid token structures. Long, descriptive prompts yield better scene coherence, material accuracy, and relational awareness than fragmented keyword&nbsp;lists.</p><p>Trigger words are optional rather than mandatory, as the model activates concepts contextually rather than requiring explicit token matching. However, structured elements‚Ää‚Äî‚Ääshort modifiers or category cues‚Ää‚Äî‚Ääcan refine specificity without overpowering Flux.1‚Äôs broader language comprehension. The model is particularly strong at multi-concept blending, which traditionally caused token interference in CLIP-based architectures. Negative prompting is also more effective, reducing the unintended blending of attributes.</p><p>Flux.1 performs best with contextual and relational descriptions, leveraging semantic depth over isolated tokens. While Booru tags still function, hybrid prompting‚Ää‚Äî‚Äämixing structured elements with full sentences‚Ää‚Äî‚Ääoffers the best balance between control and generative adaptability.</p><p>LoRA functions differently in Flux.1 compared to Stable Diffusion. Since Flux.1 uses a T5-based encoder, LoRA activation does not require fixed trigger words but instead responds dynamically to natural descriptions. This allows LoRAs to integrate seamlessly into prompts without rigid dependencies.</p><p>Key parameters such as network dimension, alpha scaling, and captioning density influence performance. Higher network dimension strengthens adaptation but risks overfitting, while alpha scaling balances LoRA integration with the base model. <strong>Flux.1‚Äôs LoRAs benefit from mixed captioning strategies‚Ää‚Äî‚Äävarying between short tags and natural descriptions‚Ää‚Äî‚Ääensuring activation without over-constraining the generative process.</strong> Unlike SDXL, which often requires explicit weight scaling (1.2‚Äì1.5x) to activate LoRAs, Flux.1 adapts LoRA strength more organically within context, reducing the need for manual weight adjustments.</p><p>To maximize LoRA performance in Flux.1, descriptive phrasing should replace rigid token triggers, using context-driven prompts to activate learned modifications naturally. Captions should balance specificity with generalization, ensuring LoRAs enhance rather than dominate the generative output.</p><h3>Optimization Strategies for Low-Resource Inference</h3><p>Quantization is a pivotal technique in reducing model size and memory usage by lowering the precision of model weights. SDcpp supports various quantization levels, each offering a different balance between memory efficiency and image&nbsp;quality:</p><ul><li> Reduces memory usage by approximately 75% compared to full precision (FP16) while maintaining good image quality. It introduces a slight computational overhead due to dequantization but is optimal for low-end hardware.</li><li> Offers slightly better precision than Q4 at the cost of requiring twice the memory. It is suitable for systems with more VRAM where higher image fidelity is&nbsp;desired.</li></ul><p>For lower-end hardware, Q4_K provides a good balance of speed and quality, while  is better for systems with more VRAM&nbsp;(&gt;16GB).</p><h4>Quantization Impact of Model&nbsp;Parts</h4><p>We‚Äôre loading a model, vae, clip, and t5, and can choose quantized parts for each. The model itself must be quantized due to memory constraints, but the VAE, CLIP, and T5 can remain in higher precision. We tested a q4_K model with safetensors in higher precision, and a fully quantized version where all parts were&nbsp;q4_0.</p><p>We observed <strong>a 2.5x slowdown with the fully quantized run</strong>, despite reduced memory usage. Likely due to increased dequantization overhead, precision mismatch-induced stalls, and reduced SIMD efficiency on the Apple M2. While background processes may have contributed to higher involuntary context switches (3.3M vs. 1.48M), they do not fully explain the slowdown. The quantization format (q4_K vs. q4_0) influences computational efficiency rather than raw inference speed, with q4_K employing grouped quantization, which improves data locality and reduces dequantization frequency, while q4_0 likely applies simpler per-tensor quantization, increasing compute overhead. The hybrid approach‚Äîquantizing only the diffusion model while keeping VAE, CLIP, and T5 in higher precision‚Äîseems to better utilize hardware by avoiding unnecessary precision conversion costs and leveraging FP computation optimizations. These results suggest that full quantization may degrade performance when not optimized for architectural strengths, particularly on FP-optimized hardware like the&nbsp;M2.</p><p>The sampling method determines the strategy by which noise is removed during the denoising process. Ancestral samplers introduce random variations at each step, leading to more diverse outputs, while fixed-step samplers follow a deterministic trajectory, yielding consistent results.</p><p>SDcpp integrates several sampling algorithms optimized for different performance criteria:</p><ul><li> A simple and fast method effective with a lower number of denoising steps, suitable for rapid inference.</li><li> Prioritizes image quality, requiring more steps and memory. It is ideal for applications where high detail is essential.</li><li><strong>Latent Consistency Models (LCM):</strong> Optimizes speed by drastically reducing the number of required denoising steps (e.g., 4‚Äì8 instead of 20‚Äì30), making it highly suitable for low-end systems. Users can specify the sampling method using the --sampling-method flag, such as --sampling-method lcm.</li></ul><p>Memory management is critical in ensuring that diffusion models run efficiently on constrained hardware. SDcpp employs several techniques to optimize memory&nbsp;usage:</p><ul><li> By processing the VAE decoder in smaller, manageable segments, VAE tiling minimizes peak memory&nbsp;usage.</li><li> Flash Attention optimizes memory allocation during UNet computations. On CUDA-enabled GPUs, it offers significant memory savings and speed improvements. Flash Attention can be activated via the --diffusion-fa flag.</li></ul><p>In our experiments on an Apple M2, enabling --vae-tiling reduced memory usage but increased execution time, suggesting a trade-off between efficiency and speed. When --diffusion-fa (Flash Attention) was enabled, execution time did not significantly improve. The combination of both options (vae-tiling and diffusion-fa) resulted in the  but also the , confirming that while these settings help on low-VRAM devices, they introduce performance overhead. The fastest run occurred with both options disabled, though it had the highest memory footprint.</p><p>Image resolution directly impacts both memory consumption and inference speed. Generating images at a resolution of 1280√ó640 requires approximately 2.5 times the memory of generating images at 512√ó512. To accommodate low-resource environments, users can opt for intermediate resolutions such as 768√ó384 or 1024√ó512, balancing image quality and computational efficiency.</p><h4>LoRA Delay and Conditioning Graph Recalculation</h4><p>When LoRA modules are applied, the conditioning graph must be recalculated due to LoRA‚Äôs modification of internal activations, which can reintroduce dependencies on structured tagging if the LoRA was trained on datasets requiring them. The conditioner parses and reweights prompt segments dynamically, adjusting token influence based on LoRA-induced shifts in the embedding space, ensuring compatibility between the altered diffusion model state and the input text. This recalibration process accounts for the delay seen post-LoRA application and before sampling, as the system must harmonize LoRA-induced parameter shifts with Flux.1‚Äôs inherently context-aware text encoding.</p><h4>Classifier-Free Guidance&nbsp;Scale</h4><p>CFG scale (Classifier-Free Guidance Scale) can be set with the --cfg-scale argument. It defaults to 7.0 and controls how strictly the model follows the text prompt, balancing adherence and creativity. Higher values (e.g., 10-15) force the model to generate images that closely match the prompt but can introduce artifacts, while lower values (e.g., 1-5) allow more randomness and artistic interpretation. It impacts performance slightly because higher CFG values increase the weight difference between conditional (prompted) and unconditional (random) predictions, requiring additional computation to balance them. However, the effect on inference speed is minor compared to factors like resolution, model architecture, and sampling&nbsp;steps.</p><p>Flux.1 seem to have been trained with  as target setting, meaning its internal weighting and contrast are optimized for that value. Lowering CFG to  reduces guidance too much, allowing excessive noise influence, resulting in darker, underexposed images. Raising it to 2.0 strengthens prompt adherence but overemphasizes highlights and contrast, making images appear overly bright. Unlike standard Stable Diffusion, where 7.0 is the default for balanced outputs, Flux.1‚Äôs tuning makes , with deviations causing unintended brightness shifts.</p><p>To analyze the impact of CFG scale on image generation, we generated eight images using CFG 1.0 and 2.0 across the Euler, LCM, iPNDM, and Euler_a samplers with 4 steps and a fixed seed of 42, using the prompt ‚Äúa beautiful girl in futuristic Tokyo, neon lights,‚Äù comparing how different guidance levels influence brightness, contrast, and prompt adherence.</p><h4>Memory-Efficient Inference</h4><p>Running Flux.1 (schnell) on an Apple M2 (16GB RAM), we achieve inference without a GPU, completing a 256√ó256 image in 216.3s with 9.28GB RAM usage. The UNet required just 96.59MB, and VAE decoding used 416MB, demonstrating the efficiency of quantized GGUF&nbsp;models.</p><p>Our analysis of Stable-Diffusion.cpp and Flux.1 demonstrates that efficient local diffusion inference is feasible even on low-resource hardware, provided that model selection, quantization, and sampling strategies are optimized. Flux.1‚Äôs extremely low-step denoising makes it uniquely suited for fast inference, especially when paired with Euler and Euler_a samplers, which balance structure formation and controlled randomness. In contrast, models like LCM introduce excessive smoothing, making them unsuitable for Flux.1‚Äôs fast convergence.</p><p>We also observed that 4 steps yield optimal results with Flux.1, as additional iterations introduce unnecessary alterations rather than improving image fidelity. While our results indicate that full quantization introduces dequantization overhead and computational stalls, <em>external factors such as background system load may have influenced the extent of the observed slowdown</em>. Memory-efficient techniques such as VAE tiling and Flash Attention can reduce hardware requirements but may introduce performance trade-offs. Users should select Q4_K or similar quantization for lower-end hardware, as grouped quantization schemes appear to mitigate some of the performance penalties associated with fully quantized inference.</p><p>These findings highlight that with the right model architecture, quantization, and sampler choice, high-quality image generation is possible on constrained devices without relying on high-end GPUs. Future improvements may focus on further refining model architectures like Flux.1 and optimizing quantization methods for even faster, memory-efficient inference.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d1841fff9cf\" width=\"1\" height=\"1\" alt=\"\">","contentLength":22810,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The ups and downs of stratified cross-validation","url":"https://www.youtube.com/watch?v=EpK9pWZGjbE","date":1738237726,"author":"probabl","guid":527,"unread":true,"content":"<article>When you use GridSearchCV in scikit-learn will do a cross-validated hyperparameter search. This is neat, but there are some worthwhile details involved when it comes to selection the cross-validation segments.\n\nLink to discussion on Github:\nhttps://github.com/scikit-learn/scikit-learn/issues/4757#issuecomment-2419933195\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\nBluesky: https://bsky.app/profile/probabl.bsky.social\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":706,"flags":null,"enclosureUrl":"https://www.youtube.com/v/EpK9pWZGjbE?version=3","enclosureMime":"","commentsUrl":null},{"title":"#492: Great Tables","url":"https://talkpython.fm/episodes/show/492/great-tables","date":1738224000,"author":"","guid":433,"unread":true,"content":"<article></article>","contentLength":0,"flags":null,"enclosureUrl":"https://talkpython.fm/episodes/download/492/great-tables.mp3","enclosureMime":"","commentsUrl":null},{"title":"Design YouTube - System Design Interview","url":"https://blog.algomaster.io/p/design-youtube-system-design-interview","date":1738215950,"author":"Ashish Pratap Singh","guid":747,"unread":true,"content":"<p>With over  monthly active users,  is the second most visited website in the world‚Äîtrailing only Google.</p><p>As a , it enables users to upload, watch, and interact with video content, while handling hundreds of millions of daily visitors, managing petabytes of data, and ensuring real-time video delivery across the globe.</p><p>In this article, we‚Äôll explore the <strong>system design of a large-scale video streaming service like YouTube</strong> that can accommodate hundreds of millions of daily users and billions of views, all while maintaining low latency and high availability.</p><p>We‚Äôll walk through every step of the design‚Äîfrom requirements and high-level architecture to database and API design‚Äîbefore diving deep into core use cases. </p><p>The concepts covered here are equally applicable to other large-scale video platforms such as  and .</p><p>Before diving into the design, lets outline the functional and non-functional requirements.</p><ul><li><p>Users should be able to  video files.</p></li><li><p>Uploaded videos must be  into multiple resolutions (e.g., 240p, 360p, 720p, 1080p) to support different network conditions and devices.</p></li></ul><ul><li><p>Users should be able to  videos in real-time with <strong>adaptive bitrate streaming</strong> to adjust quality based on network conditions.</p></li></ul><ul><li><p>Users can  for videos by title, tags, or description.</p></li><li><p>Users can  and  on videos.</p></li><li><p>Users should be able to create and subscribe to .</p></li></ul><h3><strong>Non-Functional Requirements:</strong></h3><ol><li><p> The system should support millions of concurrent users and thousands of video uploads per minute.</p></li><li><p> Core features like video upload, playback, and search should have minimal downtime.</p></li><li><p> Fast video streaming with minimal buffering and near-instantaneous search results.</p></li><li><p> Video files must be stored reliably, with redundancy mechanisms to prevent data loss due to hardware failures.</p></li><li><p> Optimize storage and bandwidth costs.</p></li></ol><ul><li><p><strong>Daily Active Users (DAU):</strong> 10 million</p></li><li><p> ~100,000 videos/day</p></li><li><p><strong>Average Videos Watched per User per Day:</strong> 5 videos</p></li><li><p> 500 MB.</p></li><li><p> 1 KB.</p></li></ul><ul><li><p>: 100,000 videos / day * 500 MB / video = 50 TB / day</p></li><li><p><strong>Daily Video Metadata Storage</strong>: 100,000 * 1KB = 100MB / day</p></li></ul><h4><strong>Network Bandwidth Estimation:</strong></h4><ul><li><p>10 million users √ó 5 videos/user = 50 million views/day </p></li><li><p><strong>Daily Bandwidth Requirements (without compression &amp; caching) </strong>: 50 million views * 500 MB / day = 25 PB / day</p></li></ul><p>Given the high storage and bandwidth requirements, leveraging  is the most practical approach:</p><ol><li><p><strong>Content Delivery Network (CDN):</strong> To cache frequently accessed videos closer to users and reduce latency.</p></li><li><p><strong>Blob Storage (e.g., AWS S3):</strong> To store video files reliably with redundancy.</p></li></ol><p>We can break the architecture of YouTube into two primary components: </p><ul><li><p> ‚Äì Handles video playback, and delivery.</p></li><li><p><strong>Video Upload &amp; Processing</strong> ‚Äì Manages user uploads, transcoding, and metadata storage.</p></li></ul><h2>3.1 Video Streaming Architecture</h2>","contentLength":2703,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23342c70-b11d-48c3-b30b-63ea245e5d2a_2026x1322.png","enclosureMime":"","commentsUrl":null},{"title":"Announcing Rust 1.84.1","url":"https://blog.rust-lang.org/2025/01/30/Rust-1.84.1.html","date":1738195200,"author":"The Rust Release Team","guid":239,"unread":true,"content":"<p>The Rust team has published a new point release of Rust, 1.84.1. Rust is a\nprogramming language that is empowering everyone to build reliable and\nefficient software.</p><p>If you have a previous version of Rust installed via rustup, getting Rust\n1.84.1 is as easy as:</p><p>If you don't have it already, you can <a href=\"https://www.rust-lang.org/install.html\">get </a> from the\nappropriate page on our website.</p><p>1.84.1 resolves a few regressions introduced in 1.84.0:</p><p>It also includes several fixes for those building Rust from source:</p><p>Many people came together to create Rust 1.84.1. We couldn't have done it\nwithout all of you. <a href=\"https://thanks.rust-lang.org/rust/1.84.1/\">Thanks!</a></p>","contentLength":567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I Use AI: Meet My Promptly Hired Model Intern","url":"http://lucumr.pocoo.org/2025/1/30/how-i-ai","date":1738195200,"author":"Armin Ronacher","guid":303,"unread":true,"content":"<p>After Musk's acquisition of Twitter, many people I respect and follow\nmoved to Bluesky.  I <a href=\"https://bsky.app/profile/mitsuhiko.at\">created an account there</a> and made an honest attempt of\nmaking it my primary platform.  Sadly, I found Bluesky to be surprisingly\nhostile towards AI content.  There is an almost religious resistance to AI\non there, at least in whatever corner of the platform I ended up in.</p><p>Despite these challenges, some individuals on both Twitter and Bluesky\nhave shown genuine curiosity about my AI usage.  In this post, I want to\nshare how I use Large Language Models and why I find them so helpful.</p><p>Before moving on, I want to include an an important disclaimer: I am by no\nmeans an expert in AI; I'm mostly an enthusiastic user.  Absolutely\nnothing here is novel!  What I do here is pretty boring which to some\ndegree is the point.  I won't be covering underlying technology or my\nprogrammatic experience.  This is strictly about how I use AI as a ‚Äútechy\nconsumer‚Äù.</p><p>In addition, as you read through this article you will probably figure out\nrather quickly that the way I use AI ‚Äî despite being used in part for\ncontent creation ‚Äî does not really impact intellectual property much.\nThat said, I'm curious and open to discussions about how we should be\ndealing with this problem.  Particularly on Bluesky a lot of the\nnegativity towards AI is related to watering down of copyrights and human\ncreation.  I don't know the answers to these things, but I think we need\nto have a productive dialog here rather than wishing for the technology to\ngo away.</p><div><p>In short: AI makes me significantly more productive.  I recently read\n<a href=\"https://x.com/aarondfrancis/\">Aaron Francis</a>'s Tweet about how he gets\na week's worth of productivity out of a day now thanks to AI.  I'm not\nanywhere close to that, but I use AI a lot now.  It has become\nindispensable for me for both content writing as well as programming.</p><p>Moreover, a common misconception is that AI is still at the level it was\nwhen ChatGPT first launched.  Many people tried it early, saw its\nlimitations, and never returned.  However, AI is evolving rapidly. If you\nhaven't kept up, you risk drawing inaccurate conclusions based on outdated\nimpressions.  In some sense I believe people who never tried to use AI\nyet, are in a better position to judge than the folks who used it two\nyears ago and never returned.</p></div><div><p>I work with a variety of AI tools, mostly because of professional\ncuriosity and to a smaller degree because each tool excels at something\nslightly different.  Here are the ones I use most often:</p><ul><li><a href=\"https://openwebui.com/\">Open WebUI</a>.  In short this is a Python web\napp that offers a chat interface similar to ChatGPT.\nUnlike ChatGPT, however, it lets you talk to different models.  First and\nforemost, I use this to talk to local models hosted by Ollama, but\nsecondarily I also use it to interface with other remote services like\nOpenAI, Anthropic and DeepSeek.</li><li><a href=\"https://github.com/simonw/llm\">Simon's llm</a>.  This is a command line\ntool with plenty of plugins that lets you prompt different models.  Think\nof it as a command-line version of Open WebUI.  It's particularly useful\nfor quick scripting and basic automation.</li><li><a href=\"https://ollama.com/\">Ollama</a>.  This allows me to run models locally\non my MacBook Pro M1 Max.  With the 64GB of RAM it has, it's a pretty\npotent machine for basic inference despite it being three years old.\nI'll explain later why I use local models.</li><li><a href=\"https://www.cursor.com/\">Cursor</a>.  It is a fork of Visual Studio\nCode with AI-based auto completion and code generation built-in.  It's\nmy go-to for programming with AI assistance at the moment.</li><li><a href=\"https://chatgpt.com/\">ChatGPT</a>.  Like probably most AI users, I use\nChatGPT, particularly on my phone.  I pay for the Plus subscription\nbecause I use it enough to get a lot of value out of it.  One\nsignificant use of this for me is in fact the voice mode (more on that\nlater).</li></ul><p>It's probably worth mentioning that you can get most of the benefits of this from just paying for a single AI\ntool.  I think as one expands their use, as especially as one gets better at writing prompts,\nthe desire naturally grows to use more tooling.  As for which models\n(and services) to use day to day I don't have a particular strong strategy\nand preferences change quickly.  For instance after DeepSeek's R1 release,\nI started exploring it quite a bit for programming ‚Äî and it's doing a\nphenomenal job at it ‚Äî&nbsp; and as of writing that's just a few days old.</p><p>If you want to run models locally, Apple Silicon machines currently offer\nsome of the best ‚Äúbang for your buck‚Äù in terms of performance, power\nusage, and money.  With <a href=\"https://tailscale.com/\">tailscale</a>, I can even\naccess my MacBook's Open WebUI interface from my phone, as long as it is\npowered on.</p></div><div><p>One frequent concern I hear is ‚Äúyou cannot trust LLMs‚Äù as they tend to\nhallucinate.  I get this in particular when I explain that I frequently\nuse this as a replacement for Google!  However, I approach the risk of\nhallucination the same way I would when seeking advice from another human:\npeople can and are routinely wrong, and you learn to cross-check\nselectively.</p><p>I treat AI as I would a collaborator or a pretty good intern but I remain\nresponsible for the final outcome.  In this case the intern also happens\nto get better month by month as models improve.  And a bit like a human,\nthat digital intern has morals and wants to be argued with.  Except, of\ncourse, that some of those AI interns <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/187oidh/deepseek_coder_7b_33b_thinks_its_trained_by_openai/\">don't want to talk about China</a>,\nwhile others get a stroke <a href=\"https://www.reddit.com/r/ChatGPT/comments/1h3rz4l/david_mayer_is_not_the_only_one_jonathan_zittrain/\">if you talk about certain people</a>.\nBut regardless of how good they get, in the end, it's my fault and my\nfault alone if I do the wrong thing.  I won't blame the AI and I need to\nspot check.</p><p>However, the logical conclusion of this is not that it's wrong all the time\nand you need to check everything, or that you cannot trust it at all.\nIt's similar to how you engage in a technical discussion with others about\na problem.  I have seen more than one situation where the conventional\nwisdom in the room is just wrong for a few minutes, until someone points\nout that we had it wrong.</p><p>Another major advantage is that AI tools are relatively open.  You can run\nmodels locally and integrate them with scripts.  Even the famous OpenAI\nwhich is not at all open is much more open than a Google search is.  For\ninstance, you can create a simple script for grammar-checking right from\nyour command line.</p><p>In other words, you  integrate it locally and nobody stops you.  By\ncontrast, many, many years ago I had a tool on my computer that allowed me\nto issue web searches and extract text from results.  That has stopped\nworking such a long time ago that I almost forgot about it.  It has\nstopped working because there is basically no competition in search, and\nGoogle does not want me to use it like that.</p><p>For instance, you can create a simple script for grammar checking right\nfrom your command line:</p><div><pre>phi4:latest\n ping -q -c1 google.com &gt;/dev/nullclaude-3-5-sonnet-latest\ncat\nllm -m  -s </pre></div><p>This script can automatically switch between a local model ( via\nOllama) and a remote one () based on internet\nconnectivity.  With a command like  in Vim, I can fix up\nsentences with a single step.</p><p>Or you can manipulate the contents of the clipboard like this:</p><pre>pbpaste | llm-spell | pbcopy &amp;&amp; say \"AI is done\"\n</pre></div><div><p>I don't let AI write my articles.  As a non-native Speaker, mistakes and\nlinguistic quirks are part of me.  That said, I do rely on AI tools for\nhelping me write.</p><p>Often, after I have made my first pass through a page, I ask an LLM to\nread through it and give me comments.  I have a Apple Note with various\nprompts I can use or I just come up with what I need in the moment.  The\nprocess is not particularly scripted out.  I basically talk to the LLM to\nfigure out what it thinks of the text, etc.</p><p>Here are some of the things I use AI for when writing:</p><ul><li> I compare the AI‚Äôs suggested revisions side by\nside with my original text and pick the changes I prefer.</li><li> AI often helps me see when my writing is too wordy.\nIn the days before AI, I often ended up with super long articles that\ndid not read well and that I did not publish.  Models like o1 are very\nhelpful in identifying things that don't need to be said.</li><li><strong>Writing Notes and finding key points:</strong> Here, I ask the AI to read\nthrough a draft ‚Äúlike a Computer Science 101 student‚Äù and take notes.\nThis helps me see if what it absorbed matches what I intended to\nconvey.</li><li>  I have a few prompts that asks the AI to\n‚Äúroast‚Äù or criticize my article, as if commenting on Reddit, Twitter,\nor Hacker News.  Even though these critiques seem shallow, they can\nsting, and they often highlight weaknesses in my argument or lack of\nclarity.  Even if they don't necessarily impact the writing, they\nprime me for some of the feedback I inevitably receive.</li><li> If I worry there's too much jargon, I use AI to\nresolve acronyms and point out technical terms I've used without\nexplanation, helping me make the text more accessible.</li></ul></div><div><p>This is probably the bulk of what I do and it falls into three major\ncategories.</p><ul><li><strong>Debugging and summarizing:</strong> Throwing stack traces and errors at an\nAI is usually a good first step in finding out where I should look,\nwhat might be wrong.  Sometimes it fixes a problem entirely by itself.\nI also sometimes just throw a long bug report or issue at the thing to\ntry to reason through it.</li><li> Sometimes you have an output, but you don't\nhave the input.  As an example I have a little side project to write a\ncommand line tool help generator, and AI is really good at taking the\noutput, ‚Äúparsing‚Äú it and creating a structure as input.  That is a\ngood first step to then generate out the code that should match again\nthat output.</li><li> lastly it just programs really well.  It can do\nsingle code snippets, it can generate complex stuff.  Particularly for\nprototyping it's amazing.</li></ul><p>Now it's not great at all languages.  I find it performs exceptionally\nwell at Python, somewhat okay at JavaScript and horrible at Rust.\nHowever, particularly if I wanna solve an algorithmic problem, I can often\niterate in Python until I have what I want, then ask it to translate it to\nRust.  That's from my experience at least a decent way to get going\nquickly on some harder problems.</p></div><div><p>Pair programming is fantastic, but not always possible if you're\nprogramming alone at home.  An LLM can serve as a stand-in discussion\npartner&nbsp;‚Äî&nbsp;a poor man's interlocutor.  Thankfully it's actually not all\nthat poor, because it's really good at telling you what stuff is called.\nThis is most fun if you do it with voice mode, but it also works quite\nwell in text.</p><p>Here a motivating example from voice mode:</p><blockquote><p> So, do you remember if you have, like, , it always copies from\nthe start, but if you keep concatenating to the same buffer, it's really\ninefficient. So, remember what the Big-O of that is if you do it in a loop,\nand wasn't there, like, a humorous name for this kind of algorithm?</p><p> You‚Äôre thinking of the ‚ÄúSchlemiel the Painter‚Äôs algorithm.‚Äù It‚Äôs a\nhumorous way of describing the inefficiency of repeatedly appending to a\nstring using operations like , which copies from the start each\ntime. The Big O complexity for this operation is O(n^2) if done repeatedly\nin a loop, since each  operation takes time proportional to the\ncurrent length of the string.</p></blockquote><p>It's also fun and educational for my kids.  They ask a lot of questions,\nand ChatGPT can tailor its explanations to their age level.  If the\nexplanation is still too advanced, I can immediately ask it to simplify\nfurther.  If it was too dumbed down, I can make it go the other way.  Now\nagain, there is always a risk that it gets it wrong, but that is okay.\nPart of all of this is to convey to the children the idea that we need to\nvalidate it and that being critical is important.</p><p>What makes ChatGPT so enjoyable here is that it's able to keep the\nconversation going ‚Äî it has state.  Answered in large metric numbers?  Can\nalways ask it to scale to elephants or soccer fields.</p><p>ChatGPT is also incredibly helpful when having to work with multiple\nlanguages.  For a recent example, my kids have Greek friends and we tried\nto understand the difference between some Greek words that came up.  I\nhave no idea how to write it, Google translate does not understand my\nattempts of pronouncing them either.  However, ChatGPT does.  If I ask it\nin voice mode what ‚Äúpa-me-spee-tee‚Äù in Greek means it knows what I tried\nto mumble and replies in a helpful manner.</p><p>Lastly the use on the go.  Sometimes I'm just not in a position where I\ncan actually write it down on a keyboard, but if I basically talk to\nChatGPT, it can transcribe it down and it will also remove some of my\n‚Äúum's‚Äù and it will just be there and I can copy-paste it later and edit\nit.  (To prove a point, I transcribed the previous sentence with the\nmobile app).  This is a very different experience than transcribing on iOS\nnatively.  It understands enough context of what I'm babbling that it will\nstart and stop transcribing.  Now this is not perfect, but pretty damn\ngood and incredibly helpful.</p><p>The multilingual aspect is particularly helpful because our family is\nmultilingual.  Being able to fluidly switch between German, Russian and\nEnglish is such a refreshing experience.</p></div><div><p>I mentioned earlier that I don't let LLMs write my texts.  I also don't\nuse AI to make illustrations, though I have in the past.  The reason is\nthat there is a certain style that goes along with these illustrations,\nwhich is just incredibly off-putting.  They are noticeably AI-generated,\nand typically quite low-quality.  People have taken to calling these\nimages ‚ÄúAI slop‚Äù and I personally respond really badly to it.  When\nsomeone throws me a pull request, an email or a text message that is\nobviously AI-generated without disclosing this, I immediately have a very\nlow opinion of them.</p><p>Slop like hallucinations are a problem, but they are only a problem if you\ndon't use your brain.  Even the worst slop can be the foundation of\nreally good content.  I'm a horrible artist, but I can use Illustrator.\nEven an AI slop image can help me trace the person in the pose I wanted.\nLikewise you can throw your notes into a document and let the AI imagine a\nstory around it.  You probably can't use that story right away, but you\ncan use it as potential inspiration.</p></div><div><p>AI tools, at their best, feel less like disruptive and dark magic and more\nlike a natural extension of the creative process as long as you see them\nas curious collaborators.  My approach isn't about outsourcing thinking,\nbut augmenting it: using LLMs to accelerate grunt work, untangle mental\nknots, and prototype ideas faster.  Skepticism is healthy, but dismissing\nAI outright risks missing its potential as a multiplier for those willing\nto engage critically.</p></div>","contentLength":14540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mini-R1: Reproduce Deepseek R1 ‚Äûaha moment‚Äú a RL tutorial","url":"https://www.philschmid.de/mini-deepseek-r1","date":1738195200,"author":"","guid":15,"unread":true,"content":"<article>Reproduce Deepseek R1 ‚Äûaha moment‚Äú and train an open model using reinforcement learning trying to teach it self-verification and search abilities all on its own to solve the Countdown Game.</article>","contentLength":193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Carving Out Your Competitive Advantage With AI","url":"https://www.topbots.com/competitive-advantage-with-ai/","date":1738176170,"author":"Janna Lipenkova","guid":591,"unread":true,"content":"<p>When I talk to corporate customers, there is often this idea that AI, while powerful, won‚Äôt give any company a lasting competitive edge. After all, over the past two years, large-scale LLMs have become a commodity for everyone. I‚Äôve been thinking a lot about how companies can shape a competitive advantage using AI, and a recent article in the Harvard Business Review (<a href=\"https://hbr.org/2024/09/ai-wont-give-you-a-new-sustainable-advantage\" rel=\"noreferrer noopener\" target=\"_blank\">AI Won‚Äôt Give You a New Sustainable Advantage</a>) inspired me to organize my thoughts around the topic.</p><p>Indeed, maybe one day, when businesses and markets are driven by the invisible hand of AI, the equal-opportunity hypothesis might ring true. But until then, there are so many ways ‚Äî big and small ‚Äî for companies to differentiate themselves using AI. I like to think of it as a complex ingredient in your business recipe ‚Äî the success of the final dish depends on the cook who is making it. The magic lies in how you combine AI craft with strategy, design, and execution.</p><p>In this article, I‚Äôll focus on real-life business applications of AI and explore their key sources of competitive advantage. As we‚Äôll see, successful AI integration goes far beyond technology, and certainly beyond having the trendiest LLM at work. It‚Äôs about finding AI‚Äôs unique sweet spot in your organization, making critical design decisions, and aligning a variety of stakeholders around the optimal design, deployment, and usage of your AI systems. In the following, I will illustrate this using the mental model we developed to structure our thinking about AI projects (cf.&nbsp;<a href=\"https://www.topbots.com/building-ai-products-with-a-holistic-mental-model/\" target=\"_blank\" rel=\"noreferrer noopener\">this article</a>&nbsp;for an in-depth introduction).</p><p>AI is often used to automate existing tasks, but the more space you allow for creativity and innovation when selecting your AI use cases, the more likely they will result in a competitive advantage. You should also prioritize the unique needs and strengths of your company when evaluating opportunities.</p><h2>Identifying use cases with differentiation potential</h2><p>When we brainstorm AI use cases with customers, 90% of them typically fall into one of four buckets ‚Äî productivity, improvement, personalization, and innovation. Let‚Äôs take the example of an airline business to illustrate some opportunities across these categories:</p><p>Of course, the first branch ‚Äî productivity and automation ‚Äî looks like the low-hanging fruit. It is the easiest one to implement, and automating boring routine tasks has an undeniable efficiency benefit. However, if you‚Äôre limiting your use of AI to basic automation, don‚Äôt be surprised when your competitors do the same. In our experience, strategic advantage is built up in the other branches. Companies that take the time to figure out how AI can help them offer something different, not just faster or cheaper, are the ones that see long-term results.</p><p>As an example, let‚Äôs look at a&nbsp;<a href=\"https://drive.google.com/file/d/1y1JN2rRM14CflkGkNvm9Ul62Ri1o5gd9/view\" rel=\"noreferrer noopener\" target=\"_blank\">project we recently implemented with the Lufthansa Group</a>. The company wanted to systematize and speed up its innovation processes. We developed an AI tool that acts as a giant sensor into the airline market, monitoring competitors, trends, and the overall market context. Based on this broad information, the tool now provides tailored innovation recommendations for Lufthansa. There are several aspects that cannot be easily imitated by potential competitors, and certainly not by just using a bigger AI model:</p><ul><li>Understanding which information exactly is needed to make decisions about new innovation initiatives</li><li>Blending public data with unique company-specific knowledge</li><li>Educating users at company scale on the right usage of the data in their assessment of new innovation initiatives</li></ul><p>All of this is novel know-how that was developed in tight cooperation between industry experts, practitioners, and a specialized AI team, involving lots of discovery, design decisions, and stakeholder alignment. If you get all of these aspects right, I believe you are on a good path toward creating a sustainable and defensible advantage with AI.</p><h2>Finding your unique sweet spot for value creation</h2><p>Value creation with AI is a highly individual affair. I recently experienced this firsthand when I challenged myself to build and launch an end-to-end AI app on my own. I‚Äôm comfortable with Python and don‚Äôt massively benefit from AI help there, but other stuff like frontend? Not really my home turf. In this situation, AI-powered code generation worked like a charm. It felt like flowing through an effortless no-code tool, while having all the versatility of the underlying ‚Äî and unfamiliar ‚Äî programming languages under my fingertips. This was my very own, personal sweet spot ‚Äî using AI where it unlocks value I wouldn‚Äôt otherwise tap into, and sparing a frontend developer on the way. Most other people would not get so much value out of this case:</p><ul><li>A professional front-end developer would not see such a drastic increase in speed.</li><li>A person without programming experience would hardly ever get to the finish line. You must understand how programming works to correctly prompt an AI model and integrate its outputs.</li></ul><p>While this is a personal example, the same principle applies at the corporate level. For good or for bad, most companies have some notion of strategy and core competence driving their business. The secret is about finding the right place for AI in that equation ‚Äî a place where it will complement and amplify the existing skills.</p><p>Data is the fuel for any AI system. Here, success comes from curating high-quality, focused datasets and continuously adapting them to evolving needs. By blending AI with your unique expertise and treating data as a dynamic resource, you can transform information into long-term strategic value.</p><h2>Managing knowledge and domain expertise</h2><p>To illustrate the importance of proper knowledge management, let‚Äôs do a thought experiment and travel to the 16th century. Antonio and Bartolomeo are the best shoemakers in Florence (which means they‚Äôre probably the best in the world). Antonio‚Äôs family has meticulously recorded their craft for generations, with shelves of notes on leather treatments, perfect fits, and small adjustments learned from years of experience. On the other hand, Bartolomeo‚Äôs family has kept their secrets more closely guarded. They don‚Äôt write anything down; their shoemaking expertise has been passed down verbally, from father to son.</p><p>Now, a visionary named Leonardo comes along, offering both families a groundbreaking technology that can automate their whole shoemaking business ‚Äî if it can learn from their data. Antonio comes with his wagon of detailed documentation, and the technology can directly learn from those centuries of know-how. Bartolomeo is in trouble ‚Äî without written records, there‚Äôs nothing explicit for the AI to chew on. His family‚Äôs expertise is trapped in oral tradition, intuition, and muscle memory. Should he try to write all of it down now ‚Äî is it even possible, given that most of his work is governed intuitively? Or should he just let it be and go on with his manual business-as-usual? Succumbing to inertia and uncertainty, he goes for the latter option, while Antonio‚Äôs business strives and grows with the help of the new technology. Freed from daily routine tasks, he can get creative and invent new ways to make and improve shoes.</p><p>Beyond explicit documentation, valuable domain expertise is also hidden across other data assets such as transactional data, customer interactions, and market insights. AI thrives on this kind of information, extracting meaning and patterns that would otherwise go unnoticed by humans.</p><p>Data doesn‚Äôt need to be big ‚Äî on the contrary, today, big often means noisy. What‚Äôs critical is the quality of the data you‚Äôre feeding into your AI system. As models become more sample-efficient ‚Äî i.e., able to learn from smaller, more focused datasets ‚Äî the kind of data you use is far more important than how much of it you have.</p><p>In my experience, the companies that succeed with AI treat their data ‚Äî be it for training, fine-tuning, or evaluation ‚Äî like a craft. They don‚Äôt just gather information passively; they&nbsp;&nbsp;and edit it, refining and selecting data that reflects a deep understanding of their specific industry. This careful approach gives their AI sharper insights and a more nuanced understanding than any competitor using a generic dataset. I‚Äôve seen firsthand how even small improvements in data quality can lead to significant leaps in AI performance.</p><h2>Capturing the dynamics with the data flywheel</h2><p>Data needs to evolve along with the real world. That‚Äôs where DataOps comes in, ensuring data is continuously adapted and doesn‚Äôt drift apart from reality. The most successful companies understand this and regularly update their datasets to reflect changing environments and market dynamics. A power mechanism to achieve this is the&nbsp;. The more your AI generates insights, the better your data becomes, creating a self-reinforcing feedback loop because users will come back to your system more often. With every cycle, your data sharpens and your AI improves, building an advantage that competitors will struggle to match. To kick off the data flywheel, your system needs to demonstrate some initial value to start with ‚Äî and then, you can bake in some additional incentives to nudge your users into using your system on a regular basis.</p><p>Now, let‚Äôs dive into the ‚Äúintelligence‚Äù component. This component isn‚Äôt just about AI models in isolation ‚Äî it‚Äôs about how you integrate them into larger intelligent systems. Big Tech is working hard to make us believe that AI success hinges on the use of massive LLMs such as the GPT models. Good for them ‚Äî bad for those of us who want to use AI in real-life applications. Overrelying on these heavyweights can bloat your system and quickly become a costly liability, while smart system design and tailored models are important sources for differentiation and competitive advantage.</p><h2>Toward customization and efficiency</h2><p>Mainstream LLMs are generalists. Like high-school graduates, they have a mediocre-to-decent performance across a wide range of tasks. However, in business, decent isn‚Äôt enough. You need to send your AI model to university so it can specialize, respond to your specific business needs, and excel in your domain. This is where fine-tuning comes into play. However, it‚Äôs important to recognize that mainstream LLMs, while powerful, can quickly become slow and expensive if not managed efficiently. As Big Tech boasts about larger model sizes and longer context windows ‚Äî i.e., how much information you can feed into one prompt ‚Äî smart tech is quietly moving towards efficiency. Techniques like prompt compression reduce prompt size, making interactions faster and more cost-effective.&nbsp;<strong>Small language models (SLMs)</strong>&nbsp;are another trend (Figure 4). With up to a couple of billions of parameters, they allow companies to safely deploy task- and domain-specific intelligence on their internal infrastructure (<a href=\"http://anacode.de/tpost/sz7xgnigh1-bigger-is-not-always-better-unlocking-ef\" rel=\"noreferrer noopener\" target=\"_blank\">Anacode</a>).</p><p>But before fine-tuning an LLM, ask yourself whether generative AI is even the right solution for your specific challenge. In many cases, predictive AI models ‚Äî those that focus on forecasting outcomes rather than generating content ‚Äî are more effective, cheaper, and easier to defend from a competitive standpoint. And while this might sound like old news, most of AI value creation in businesses actually happens with predictive AI.</p><h2>Crafting compound AI systems</h2><p>AI models don‚Äôt operate in isolation. Just as the human brain consists of multiple regions, each responsible for specific capabilities like reasoning, vision, and language, a truly intelligent AI system often involves multiple components. This is also called a ‚Äúcompound AI system‚Äù (<a href=\"https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/\" rel=\"noreferrer noopener\" target=\"_blank\">BAIR</a>). Compound systems can accommodate different models, databases, and software tools and allow you to optimize for cost and transparency. They also enable faster iteration and extension ‚Äî modular components are easier to test and rearrange than a huge monolithic LLM.</p><p>Take, for example, a customer service automation system for an SME. In its basic form ‚Äî calling a commercial LLM ‚Äî such a setup might cost you a significant amount ‚Äî let‚Äôs say $21k/month for a ‚Äúvanilla‚Äù system. This cost can easily scare away an SME, and they will not touch the opportunity at all. However, with careful engineering, optimization, and the integration of multiple models, the costs can be reduced by as much as 98% (<a href=\"https://arxiv.org/abs/2305.05176\" rel=\"noreferrer noopener\" target=\"_blank\">FrugalGPT</a>). Yes, you read it right, that‚Äôs&nbsp;&nbsp;‚Äî a staggering difference, putting a company with stronger AI and engineering skills at a clear advantage. At the moment, most businesses are not leveraging these advanced techniques, and we can only imagine how much there is yet to optimize in their AI usage.</p><h2>Generative AI isn‚Äôt the finish line</h2><p>While generative AI has captured everyone‚Äôs imagination with its ability to produce content, the real future of AI lies in reasoning and problem-solving. Unlike content generation, reasoning is nonlinear ‚Äî it involves skills like abstraction and generalization which generative AI models aren‚Äôt trained for.</p><p>AI systems of the future will need to handle complex, multi-step activities that go far beyond what current generative models can do. We‚Äôre already seeing early demonstrations of AI‚Äôs reasoning capabilities, whether through language-based emulations or engineered add-ons. However, the limitations are apparent ‚Äî past a certain threshold of complexity, these models start to hallucinate. Companies that invest in crafting AI systems designed to handle these complex, iterative processes will have a major head start. These companies will thrive as AI moves beyond its current generative phase and into a new era of smart, modular, and reasoning-driven systems.</p><p>User experience is the channel through which you can deliver the value of AI to users. It should smoothly transport the benefits users need to speed up and perfect their workflows, while inherent AI risks and issues such as erroneous outputs need to be filtered or mitigated.</p><h2>Optimizing on the strengths of humans and AI</h2><p>In most real-world scenarios, AI alone can‚Äôt achieve full automation. For example, at my company<a href=\"http://www.equintel.de/\" rel=\"noreferrer noopener\" target=\"_blank\">&nbsp;Equintel</a>, we use AI to assist in the ESG reporting process, which involves multiple layers of analysis and decision-making. While AI excels at large-scale data processing, there are many subtasks that demand human judgment, creativity, and expertise. An ergonomic system design reflects this labor distribution, relieving humans from tedious data routines and giving them the space to focus on their strengths.</p><p>This strength-based approach also alleviates common fears of job replacement. When employees are empowered to focus on tasks where their skills shine, they‚Äôre more likely to view AI as a supporting tool, not a competitor. This fosters a win-win situation where both humans and AI thrive by working together.</p><p>Every AI model has an inherent failure rate. Whether generative AI hallucinations or incorrect outputs from predictive models, mistakes happen and accumulate into the dreaded ‚Äúlast-mile problem.‚Äù Even if your AI system performs well 90% of the time, a small error rate can quickly become a showstopper if users overtrust the system and don‚Äôt address its errors.</p><p>Consider a bank using AI for fraud detection. If the AI fails to flag a fraudulent transaction and the user doesn‚Äôt catch it, the resulting loss could be significant ‚Äî let‚Äôs say $500,000 siphoned from a compromised account. Without proper trust calibration, users might lack the tools or alerts to question the AI‚Äôs decision, allowing fraud to go unnoticed.</p><p>Now, imagine another bank using the same system but with proper trust calibration in place. When the AI is uncertain about a transaction, it flags it for review, even if it doesn‚Äôt outright classify it as fraud. This additional layer of trust calibration encourages the user to investigate further, potentially catching fraud that would have slipped through. In this scenario, the bank could avoid the $500,000 loss. Multiply that across multiple transactions, and the savings ‚Äî along with improved security and customer trust ‚Äî are substantial.</p><p>Success with AI requires more than just adopting the latest technologies ‚Äî it‚Äôs about identifying and nurturing the individual sweet spots where AI can drive the most value for your business. This involves:</p><ul><li>Pinpointing the areas where AI can create a significant impact.</li><li>Aligning a top-tier team of engineers, domain experts, and business stakeholders to design AI systems that meet these needs.</li><li>Ensuring effective AI adoption by educating users on how to maximize its benefits.</li></ul><p>Finally, I believe we are moving into a time when the notion of competitive advantage itself is shaken up. While in the past, competing was all about maximizing profitability, today, businesses are expected to balance financial gains with sustainability, which adds a new layer of complexity. AI has the potential to help companies not only optimize their operations but also move toward more sustainable practices. Imagine AI helping to reduce plastic waste, streamline shared economy models, or support other initiatives that make the world a better place. The real power of AI lies not just in efficiency but in the potential it offers us to reshape whole industries and drive both profit and positive social impact.</p><p><em>Note: Unless noted otherwise, all images are the author‚Äôs.</em></p><p><em>This article was originally published on&nbsp;<a href=\"https://towardsdatascience.com/carving-out-your-competitive-advantage-with-ai-a4babb931076\" target=\"_blank\" rel=\"noreferrer noopener\">Towards Data Science</a>&nbsp;and re-published to TOPBOTS with permission from the author.</em></p><h2>Enjoy this article? Sign up for more AI updates.</h2><p>We‚Äôll let you know when we release more articles like this one.</p>","contentLength":17671,"flags":null,"enclosureUrl":"https://www.topbots.com/wp-content/uploads/2025/01/figure5.png","enclosureMime":"text/html","commentsUrl":null},{"title":"NVIDIA's New AI: 150x Faster Virtual Worlds!","url":"https://www.youtube.com/watch?v=jyhu1VipWpo","date":1738169284,"author":"Two Minute Papers","guid":518,"unread":true,"content":"<article>‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nüìù The paper \"InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds\" is available here:\nhttps://instantsplat.github.io/\nTry it out (hopefully works): https://huggingface.co/spaces/kairunwen/InstantSplat\n\nClouds paper: https://arcanous98.github.io/projectPages/gaussianVolumes.html\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu\n\n#nvidia</article>","contentLength":1223,"flags":null,"enclosureUrl":"https://www.youtube.com/v/jyhu1VipWpo?version=3","enclosureMime":"","commentsUrl":null},{"title":"Embeddings in GenAI Products","url":"https://martinfowler.com/articles/gen-ai-patterns/#embedding","date":1738166100,"author":"Martin Fowler","guid":293,"unread":true,"content":"<p>GenAI systems, like many modern AI approaches, have to handle vast\n      quantities of data, and find similarities between elements in an image or\n      chunk of words.  and I describe a key tool\n      to do this - <a href=\"https://martinfowler.com/articles/gen-ai-patterns/#embedding\">Embeddings</a> - transforming large data blocks into\n      numeric vectors so that embeddings near each other represent related\n      concepts</p>","contentLength":353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Forms with Flask","url":"https://blog.miguelgrinberg.com/post/dynamic-forms-with-flask","date":1738163041,"author":"Miguel Grinberg","guid":57,"unread":true,"content":"<p>A common need in web applications is to create a form that allows the user to enter a list of items, with the number of items not known in advance. This is a pattern often used when entering user information, specifically for phone numbers or addresses, but has a lot of other uses as well. In the example below you can see how the user dynamically adds more phone numbers to the form by clicking the \"Add another\" link.</p><p>Implementing this with Flask is surprisingly tricky, as it requires a combination of back and front end techniques working together. In this article I will show you two possible solutions, a basic one that uses only Flask and a more complete one for the <a href=\"https://flask-wtf.readthedocs.io/en/1.2.x/\">Flask-WTF</a> form handling extension.</p>","contentLength":708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bliki: Forest And Desert","url":"https://martinfowler.com/bliki/ForestAndDesert.html","date":1738155600,"author":"Martin Fowler","guid":294,"unread":true,"content":"<div><img src=\"https://martinfowler.com/bliki/images/forest-desert.png\"></div><p>The Forest and the Desert is a metaphor for thinking about software\n  development processes, developed by Beth Andres-Beck and hir father Kent Beck.\n  It posits that two communities of software developers have great difficulty\n  communicating to each other because they live in very different contexts, so\n  advice that applies to one sounds like nonsense to the other.</p><p>The desert is the common world of software development, where bugs are\n  plentiful, skill isn't cultivated, and communications with users is difficult.\n  The forest is the world of a well-run team that uses something like <a href=\"https://martinfowler.com/bliki/ExtremeProgramming.html\">Extreme Programming</a>, where developers swiftly put changes into\n  production, protected by their tests, code is invested in to keep it healthy,\n  and there is regular contact with The Customer.</p><p>Clearly Beth and Kent prefer The Forest (as do I). But the metaphor is more\n  about how description of The Forest and the advice for how to work there often\n  sounds nonsensical to those whose only experience is The Desert. It reminds us\n  that any lessons we draw about software development practice, or architectural\n  patterns, are governed by the context that we experienced them. It is possible\n  to change Desert into Forest, but it's difficult - often requiring people to do\n  things that are both hard and counter-intuitive. (It seems sadly easier for\n  The Forest to submit to desertification.) </p><p>In this framing I'm definitely a Forest Dweller, and seek with Thoughtworks\n  to cultivate a healthy forest for us and our clients. I work to explain The Forest to Desert\n  Dwellers, and help my fellow Forest Dwellers to make their forest even more\n  plentiful.</p><div><p>Kent Beck supplied the image, which he may have painstakingly drew pixel by\n    pixel. Or not.</p></div>","contentLength":1742,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust 584","url":"https://this-week-in-rust.org/blog/2025/01/29/this-week-in-rust-584/","date":1738126800,"author":"TWiR Contributors","guid":399,"unread":true,"content":"<p>This week's crate is <a href=\"https://github.com/riberk/embed_it\">embed_it</a>, a crate that helps you to embed assets into your binary and generates structs / trait implementations for each file or directory.</p><p>Thanks to <a href=\"https://users.rust-lang.org/t/crate-of-the-week/2704/1390\">Riberk</a> for the self-suggestion!</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.  The following\nRFCs would benefit from user testing before moving forward:</p><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p>Relatively quiet week, with one large-ish regression that will be reverted.\n<a href=\"https://github.com/rust-lang/rust/pull/132666\">#132666</a> produced a nice perf. win, by skipping\nunnecessary work. This PR actually reversed a regression caused by a <a href=\"https://github.com/rust-lang/rust/pull/131984\">previous PR</a>.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr><td align=\"center\">Improvements ‚úÖ  (secondary)</td></tr><tr></tr></tbody></table><p>2 Regressions, 3 Improvements, 2 Mixed; 4 of them in rollups\n45 artifact comparisons made in total</p><ul><li><em>No RFCs were approved this week.</em></li></ul><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><ul><li><em>No RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Cargo Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Team Proposals entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Reference RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><ul><li><em>No New or Updated RFCs were created this week.</em></li></ul><p>Rusty Events between 2025-01-29 - 2025-02-26 ü¶Ä</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>I have experience in multiple styles of MMA gained from fighting the borrow checker, if that counts.</p></blockquote>","contentLength":2690,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A continued focus on the benefits of Go 1.24","url":"https://golangweekly.com/issues/540","date":1738108800,"author":"","guid":595,"unread":true,"content":"<li><p><a href=\"https://golangweekly.com/link/164971/rss\">SRCL</a> is a neat new suite of React components for creating webapps with a monospaced, terminal-style aesthetic.</p></li><li><p>If you ever need to work with JavaScript server-side, <a href=\"https://golangweekly.com/link/164972/rss\">Bun 1.2</a> is a huge release for an increasingly popular, high performance -based Node-a-like. It 'just works' for a lot of stuff where Node gives me&nbsp;headaches.</p></li><li><p><a href=\"https://golangweekly.com/link/164974/rss\">Take a look at every byte</a> that goes over the network when making a real Postgres query over TLS . Everything is annotated to help you understand the&nbsp;complexity.</p></li>","contentLength":483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computing with tuple types in TypeScript","url":"https://2ality.com/2025/01/typescript-tuples.html","date":1738108800,"author":"Dr. Axel Rauschmayer","guid":346,"unread":true,"content":"<p>JavaScript‚Äôs Arrays are so flexible that TypeScript provides two different kinds of types for handling them:</p><ul><li>Array types for arbitrary-length sequences of values that all have the same type ‚Äì e.g.: </li><li>Tuple types for fixed-length sequences of values where each one may have a different type ‚Äì e.g.: <code>[number, string, boolean]</code></li></ul><p>In this blog post, we look at the latter ‚Äì especially how to compute with tuples at the type level.</p>","contentLength":427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a Scalable Flask Web Application From Scratch","url":"https://realpython.com/courses/create-scalable-flask-web-app/","date":1738072800,"author":"Real Python","guid":166,"unread":true,"content":"<p>Flask is a powerful and flexible micro web framework for Python, ideal for both small and large web projects. It provides a straightforward way to get a web application up and running, with all the features that you need to get started.</p><p>Over the course of this video course, you‚Äôll explore the process of creating a boilerplate for a Flask web project. This boilerplate will serve as a great starting point for any scalable Flask web app that you wish to develop in the future, from basic web pages to complex web applications.</p><p><strong>In this video course, you‚Äôll learn how to:</strong></p><ul><li>Create a </li><li>Add  with blueprints</li></ul>","contentLength":601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Emerging Patterns in Building GenAI Products","url":"https://martinfowler.com/articles/gen-ai-patterns/","date":1738068180,"author":"Martin Fowler","guid":292,"unread":true,"content":"<p>Everyone is fascinated about using generative AI these days, and my\n      colleagues are no exception. Some of them have had the opportunity to put\n      these system into practice, both as proof-of-concept, and more importantly\n      as production system. I've known  for\n      many years as a technology leader in India, he's been assembling the\n      lessons we've learned and I've worked with him to describe them as\n      patterns.</p><p>In this first installment, we look the limits of the base case of\n      Direct Prompting, and how we might assess the capability of a system using\n      Evals.</p>","contentLength":595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Long Polling vs WebSockets","url":"https://blog.algomaster.io/p/long-polling-vs-websockets","date":1738038640,"author":"Ashish Pratap Singh","guid":746,"unread":true,"content":"<p>Whether you are playing an online game or chatting with a friend‚Äîupdates appear in real-time without hitting .</p><p>Behind these seamless experiences lies a critical engineering decision: <strong>how to push real-time updates from servers to clients</strong>.</p><p>The traditional HTTP model was designed for request-response: <em>\"Client asks, server answers.\". </em>But in many real-time systems, the server needs to talk first and more often.</p><p>This is where  and  come into play‚Äîtwo popular methods for achieving real-time updates.</p><p>In this article, we‚Äôll explore these two techniques, how they work, their pros and cons, and use cases.</p><p>If you‚Äôre finding this newsletter valuable and want to deepen your learning, consider becoming a .</p><p>As a paid subscriber, you'll receive an <strong>exclusive deep-dive article</strong> every week, access to a structured100+topics and interview questions, and other .</p><p>HTTP, the backbone of the web, follows a :</p><ol><li><p>The client (e.g., a browser or mobile app) sends a request to the server.</p></li><li><p>The server processes the request and sends back a response.</p></li></ol><p>This model is simple and works for many use-cases, but it has limitations:</p><ul><li><p> With plain HTTP, the server cannot proactively push data to the client. The client has to request the data periodically.</p></li><li><p> HTTP is stateless, meaning each request stands alone with no persistent connection to the server. This can be problematic if you need continuous exchange of data.</p></li></ul><p>To build truly real-time features‚Äîlive chat, financial tickers, or gaming updates‚Äîyou need a mechanism where the server can instantly notify the client when something changes.</p><p> is a technique that mimics real-time behavior by keeping HTTP requests open until the server has data.</p><p>Long Polling is an enhancement over traditional polling. In regular polling, the client repeatedly sends requests at fixed intervals (e.g., every second) to check for updates. This can be wasteful if no new data exists. </p><p>Long Polling tweaks this approach: the client asks the server for data and then ‚Äúwaits‚Äù until the server has something new to return or until a timeout occurs.</p><h3>How Does Long Polling Work?</h3><ol><li><p> to the server, expecting new data.</p></li><li><p><strong>Server holds the request open</strong> until it has an update or a timeout is reached.</p><ul><li><p>If there's new data, the server immediately responds.</p></li><li><p>If there‚Äôs no new data and the timeout is reached, the server responds with an empty or minimal message.</p></li></ul></li><li><p>Once the client receives a response‚Äînew data or a timeout‚Äîit <strong>immediately sends a new request</strong> to the server to keep the connection loop going.</p></li></ol><ul><li><p>Simple to implement (uses standard HTTP).</p></li><li><p>Supported universally since it uses standard HTTP, and it works reliably through firewalls and proxies.</p></li></ul><ul><li><p>Higher latency after each update (client must re-establish connection).</p></li><li><p>Resource-heavy on servers (many open hanging requests).</p></li></ul><ul><li><p>Simple chat or comment systems where real-time but slightly delayed updates (near real-time) are acceptable.</p></li><li><p>Notification systems for less frequent updates (e.g., Gmail‚Äôs \"new email\" alert).</p></li><li><p>Legacy systems where WebSockets aren‚Äôt feasible.</p></li></ul><h3>Code Example (JavaScript)</h3><p> provide a <strong>full-duplex, persistent connection</strong> between the client and the server.</p><p>Once established, both parties can send data to each other at any time, without the overhead of repeated HTTP requests.</p><ol><li><p> Client sends an HTTP request with .</p></li><li><p>: If supported, the server upgrades the connection to WebSocket (switching from  to ). After the handshake, client and server keep a TCP socket open for communication.</p></li><li><p><strong>Full-Duplex Communication:</strong> Once upgraded, data can be exchanged bidirectionally in real time until either side closes the connection.</p></li></ol><ol><li><p>Ultra-low latency (no repeated handshakes).</p></li><li><p>Lower overhead since there‚Äôs only one persistent connection rather than repeated HTTP requests.</p></li><li><p>Scalable for real-time applications that need to support large number of concurrent users.</p></li></ol><ol><li><p>More complex setup (requires the client and server to support WebSocket).</p></li><li><p>Some proxies and firewalls may not allow WebSocket traffic.</p></li><li><p>Complexity in implementation and handling reconnections/errors.</p></li><li><p>Server resource usage might grow if you have a large number of concurrent connections.</p></li></ol><ol><li><p>Live chat and collaboration tools (Slack, Google Docs, etc.).</p></li><li><p>Multiplayer online games with real-time state synchronization.</p></li><li><p>Live sports/financial dashboards that need to push frequent updates.</p></li></ol><h3>Code Example (JavaScript)</h3><p>Both methods achieve real-time updates, but your choice depends on your project‚Äôs requirements:</p><ol><li><ul><li><p> is easier to implement using standard libraries. Any environment that supports HTTP can handle it, often without extra packages.</p></li><li><p> require a bit more setup and a capable proxy environment (e.g., support in Nginx or HAProxy). However, many frameworks (e.g., Socket.io) simplify the process significantly.</p></li></ul></li><li><p><strong>Scalability and Performance</strong></p><ul><li><p> can become resource-intensive with a large number of simultaneous clients, due to multiple open connections waiting on the server side.</p></li><li><p> offer a more efficient, persistent connection and scale better for heavy, frequent data streams.</p></li></ul></li><li><ul><li><p> fits scenarios where data updates aren‚Äôt super frequent. If new data arrives every few seconds or minutes, long polling might be enough.</p></li><li><p> are better for high-frequency updates or two-way communication (e.g., multiple participants editing a document or interacting in a game).</p></li></ul></li><li><ul><li><p> typically works even in older networks or those with strict firewalls.</p></li><li><p> might face issues in certain corporate or older mobile environments, though this is less of a problem as the standard becomes more widespread.</p></li></ul></li></ol><blockquote><p>While both achieve real-time communication, WebSockets are generally more efficient for truly real-time applications, while Long Polling can be simpler to implement for less demanding scenarios.</p></blockquote><h4><strong>1. Server-Sent Events (SSE)</strong></h4><ul><li><p>Allows the server to push messages to the client over HTTP.</p></li><li><p>It's simpler than WebSockets for one-way communication, but not full-duplex.</p></li><li><p>Best suited for use cases like news feeds, real-time notifications, and status updates.</p></li></ul><ul><li><p>Commonly used in IoT for lightweight publish-subscribe messaging.</p></li><li><p>Specialized for device-to-device or device-to-server communication with minimal overhead.</p></li></ul><h4><strong>3. Libraries like Socket.io</strong></h4><ul><li><p>Provides an abstraction over WebSockets for easier real-time communication.</p></li><li><p>Automatically falls back to long polling if WebSockets are unsupported.</p></li><li><p>Ensures cross-browser compatibility with robust and reliable performance.</p></li></ul><p>If you found it valuable, hit a like ‚ù§Ô∏è and consider subscribing for more such content every week.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/scalability?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQwMDc4MDk0LCJpYXQiOjE3Mzc1MzgxODMsImV4cCI6MTc0MDEzMDE4MywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.xroFXQDDEPvo2FWnnt-G2Ji9MzYIDtJ68NRQX6sT8x8&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you‚Äôre enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":6609,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/90181ae6-4dcb-456f-a43a-7824d97c740b_2002x1472.png","enclosureMime":"","commentsUrl":null},{"title":"UK Council Sells Assets To Fund Ballooning $50 Million Oracle Project","url":"https://news.slashdot.org/story/25/01/28/004213/uk-council-sells-assets-to-fund-ballooning-50-million-oracle-project?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738023600,"author":"BeauHD","guid":357,"unread":true,"content":"West Sussex County Council is using up to $31 million from the sale of capital assets to fund an Oracle-based transformation project, originally budgeted at $3.2 million but now expected to cost nearly $50 million due to delays and cost overruns. The project, intended to replace a 20-year-old SAP system with a SaaS-based HR and finance system, has faced multiple setbacks, renegotiated contracts, and a new systems integrator, with completion now pushed to December 2025. The Register reports: West Sussex County Council is taking advantage of the so-called \"flexible use of capital receipts scheme\" introduced in 2016 by the UK government to allow councils to use money from the sale of assets such as land, offices, and housing to fund projects that result in ongoing revenue savings. An example of the asset disposals that might contribute to the project -- set to see the council move off a 20-year-old SAP system -- comes from the sale of a former fire station in Horley, advertised for $3.1 million.\n \nMeanwhile, the delays to the project, which began in November 2019, forced the council to renegotiate its terms with Oracle, at a cost of $3 million. The council had expected the new SaaS-based HR and finance system to go live in 2021, and signed a five-year license agreement until June 2025. The plans to go live were put back to 2023, and in the spring of 2024 delayed again until December 2025. According to council documents published this week [PDF], it has \"approved the variation of the contract with Oracle Corporation UK Limited\" to cover the period from June 2025 to June 2028 and an option to extend again to the period June 2028 to 2030. \"The total value of the proposed variation is $2.96 million if the full term of the extension periods are taken,\" the council said.","contentLength":1792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Takes on MongoDB with PostgreSQL-Based Document Database","url":"https://developers.slashdot.org/story/25/01/27/2023232/microsoft-takes-on-mongodb-with-postgresql-based-document-database?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1738009320,"author":"msmash","guid":356,"unread":true,"content":"Microsoft has launched an open-source document database platform built on PostgreSQL, partnering with FerretDB as a front-end interface. The solution includes two PostgreSQL extensions: pg_documentdb_core for BSON optimization and pg_documentdb_api for data operations. \n\nFerretDB CEO Peter Farkas said the integration with Microsoft's DocumentDB extension has improved performance twentyfold for certain workloads in FerretDB 2.0. The platform carries no commercial licensing fees or usage restrictions under its MIT license, according to Microsoft.","contentLength":550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Illustrated DeepSeek-R1","url":"https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1","date":1738008946,"author":"Jay Alammar","guid":729,"unread":true,"content":"<p>DeepSeek-R1 is the latest resounding beat in the steady drumroll of AI progress. For the ML R&amp;D community, it is a major release for reasons including: </p><ol><li><p>It is an open weights model with smaller, distilled versions and </p></li><li><p>It shares and reflects upon a training method to reproduce a reasoning model like OpenAI O1. </p></li></ol><p>In this post, we‚Äôll see how it was built.</p><p><em>: <a href=\"https://zhuanlan.zhihu.com/p/21175143007\">Chinese</a>, <a href=\"https://tulip-phalange-a1e.notion.site/DeepSeek-R1-189c32470be2801c94b6e5648735447d\">Korean</a>, <a href=\"https://gist.github.com/gsamil/0a5ca3bf44e979151e6c5d33345ede16\">Turkish</a> (Feel free to translate the post to your language and send me the link to add here)</em></p><ul><li><p>Recap: How LLMs are trained</p></li><li><p>DeepSeek-R1 Training Recipe</p></li><li><p>1- Long chains of reasoning SFT Data</p></li><li><p>2- An interim high-quality reasoning LLM (but worse at non-reasoning tasks).</p></li><li><p>3- Creating reasoning models with large-scale reinforcement learning (RL) </p><ul><li><p>3.1- Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero)</p></li><li><p>3.2- Creating SFT reasoning data with the interim reasoning model</p></li><li><p>3.3- General RL training phase </p></li></ul></li></ul><h2>Recap: How LLMs are trained</h2><p>Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.</p><p>The following figure, from Chapter 12 of our book shows the general recipe of creating a high-quality LLM over three steps:</p><p>1) The language modeling step where we train the model to predict the next word using a massive amount of web data. This step results in a base model.</p><p>2) a supervised fine-tuning step that makes the model more useful in following instructions and answering questions. This step results in an instruction tuned model or a supervised fine -tuning / SFT model.</p><p>3) and finally a preference tuning step which further polishes its behaviors and aligns to human preferences, resulting in the final preference-tuned LLM which you interact with on playgrounds and apps.</p><h2>DeepSeek-R1 Training Recipe</h2><p>DeepSeek-R1 follows this general recipe. The details of that first step come from a <a href=\"https://arxiv.org/pdf/2412.19437v1\">previous paper for the DeepSeek-V3 model</a>. R1 uses the  model (not the final DeepSeek-v3 model) from that previous paper, and still goes through an SFT and preference tuning steps, but the details of how it does them are what's different.</p><p>There are three special things to highlight in the R1 creation process.</p><h3>1- Long chains of reasoning SFT Data</h3><p>This is a large number of long chain-of-thought reasoning examples (600,000 of them). These are very hard to come by and very expensive to label with humans at this scale. Which is why the process to create them is the second special thing to highlight</p><h3>2- An interim high-quality reasoning LLM (but worse at non-reasoning tasks).</h3><p>This data is created by a precursor to R1, an unnamed sibling which specializes in reasoning. This sibling is inspired by a third model called (that we‚Äôll discuss shortly). It is significant not because it‚Äôs a great LLM to use, but because creating it required so little labeled data alongside large-scale reinforcement learning resulting in a model that excels at solving reasoning problems. </p><p>The outputs of this unnamed specialist reasoning model can then be used to train a more general model that can also do other, non-reasoning tasks, to the level users expect from an LLM.</p><h3>3- Creating reasoning models with large-scale reinforcement learning (RL) </h3><p>This happens in two steps:</p><h4><strong>3.1 Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero)</strong></h4><p>Here, RL is used to create the interim reasoning model. The model is then used to  generate the SFT reasoning examples. But what makes creating this model possible is an earlier experiment creating an earlier model called .</p><p>R1-Zero is special because it is able to excel at reasoning tasks without having a labeled SFT training set. Its training goes directly from a pre-trained base model through a RL training process (no SFT step). It does this so well that it‚Äôs competitive with o1.</p><p>This is significant because data has always been the fuel for ML model capability. How can this model depart from that history? This points to two things:</p><p>1- Modern base models have crossed a certain threshold of quality and capability (this base model was trained on 14.8 trillion high-quality tokens).</p><p>2- Reasoning problems, in contrast to general chat or writing requests, can be automatically verified or labeled. Let‚Äôs show this with an example. </p><h5>Example: Automatic Verification of a Reasoning Problem</h5><p>This can be a prompt/question that is a part of this RL training step:</p><blockquote><p>Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start.</p></blockquote><p>A question like this lends itself to many ways of automatic verification. Say we present this this to the model being trained, and it generates a completion:</p><ul><li><p>A software linter can check if the completion is proper python code or not</p></li><li><p>We can execute the python code to see if it even runs</p></li><li><p>Other modern coding LLMs can create unit tests to verify the desired behavior (without being reasoning experts themselves). </p></li><li><p>We can go even one step further and measure execution time and make the training process prefer more performant solutions over other solutions ‚Äî even if they‚Äôre correct python programs that solve the issue.</p></li></ul><p>We can present a question like this to the model in a training step, and generate multiple possible solutions.</p><p>We can automatically check (with no human intervention) and see that the first completion is not even code. The second one is code, but is not python code. The third is a possible solution, but fails the unit tests, and the forth is a correct solution.</p><p>These are all signals that can be directly used to improve the model. This is of course done over many examples (in mini-batches) and over successive training steps.</p><p>These reward signals and model updates are how the model continues improving on tasks over the RL training process as seen in Figure 2 from the paper.</p><p>Corresponding with the improvement of this capability is the length of the generated response, where the model generates more thinking tokens to process the problem.</p><p>This process is useful, but the R1-Zero model, despite scoring high on these reasoning problems, confronts other issues that make it less usable than desired. </p><blockquote><p>Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing.</p></blockquote><p>R1 is meant to be a more usable model. So instead of relying completely on the RL process, it is used in two places as we‚Äôve mentioned earlier in this section:</p><p>1- creating an interim reasoning model to generate SFT data points</p><p>2- Training the R1 model to improve on reasoning and non-reasoning problems (using other types of verifiers)</p><h4>3.2 Creating SFT reasoning data with the interim reasoning model</h4><p>To make the interim reasoning model more useful, it goes through an supervised fine-tuning (SFT) training step on a few thousand examples of reasoning problems (some of which are generated and filtered from R1-Zero). The paper refers to this as cold start data‚Äù</p><blockquote><p>Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators.</p></blockquote><p>But wait, if we have this data, then why are we relying on the RL process? It‚Äôs because of the scale of the data. This dataset might be 5,000 examples (which is possible to source), but to train R1, 600,000 examples were needed. This interim model bridges that gap and allows to synthetically generate that extremely valuable data.</p><p>If you‚Äôre new to the concept of Supervised Fine-Tuning (SFT), that is the process that presents the model with training examples in the form of prompt and correct completion. This figure from chapter 12 shows a couple of SFT training examples:</p><h4>3.3 General RL training phase </h4><p>This enables R1 to excel at reasoning as well as other non-reasoning tasks. The process is similar to the the RL process we‚Äôve seen before. But since it extends to non-reasoning applications, it utilizes a helpfulnes and a safety reward model (not unlike the Llama models) for prompts that belong to these applications.</p><p>In terms of model dimension size and other hyperparameters, they look like this:</p><p>More details about the model architecture are presented in their two earlier papers:</p><p>With this, you should now have the main intuitions to wrap your head around the DeepSeek-R1 model. </p><p>Other suggested resources are:</p>","contentLength":8875,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F623a9dbf-c76e-438c-ba69-43ae9613ebbe_2930x1496.png","enclosureMime":"","commentsUrl":null},{"title":"#418 I'm a tea pot","url":"https://pythonbytes.fm/episodes/show/418/im-a-tea-pot","date":1737964800,"author":"","guid":444,"unread":true,"content":"<article></article>","contentLength":0,"flags":null,"enclosureUrl":"https://pythonbytes.fm/episodes/download/418/im-a-tea-pot.mp3","enclosureMime":"","commentsUrl":null},{"title":"Zig Cross Compiling","url":"https://zig.news/david_vanderson/zig-cross-compiling-37en","date":1737944648,"author":"David Vanderson","guid":754,"unread":true,"content":"<p>My first cross-compiling attempt with zig was great.</p><p>Both run Linux, and I had already successfully compiled on the phone itself, but the edit/debug cycle was too long.</p><p>So first up, find out what the phone target is, by running  which produces tons of output, but at the bottom:</p><div><pre><code> \"native\": {\n  \"triple\": \"aarch64-linux.4.19.325...4.19.325-gnu.2.39\",\n  \"cpu\": {\n   \"arch\": \"aarch64\",\n   \"name\": \"cortex_a78\",\n</code></pre></div><div><pre><code>$zig build -Dtarget=aarch64-linux.4.19.325...4.19.325-gnu.2.39\n</code></pre></div><div><pre><code>/home/dvanderson/.cache/zig/p/122004e82d4a0c61a9c414539c1f87bb125cb2b293573af77b153ea3903cb209b65b/library/aesce.c:182:18: \nerror: always_inline function 'vaesimcq_u8' requires target feature 'aes', \nbut would be inlined into function 'mbedtls_aesce_inverse_key' \nthat is compiled without support for 'aes'\n</code></pre></div><p>It looks like I need to also pass some , so again using the output of  from above:</p><div><pre><code>$zig build -Dtarget=aarch64-linux.4.19.325...4.19.325-gnu.2.39 -Dcpu=cortex_a78\n</code></pre></div><p>Note that this is all with zig 0.13, and there are a few issues that might change how to pass the target/cpu/libc/abi information in the future.</p>","contentLength":1081,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Michigan Law Requires High Schools to Offer CS Classes","url":"https://news.slashdot.org/story/25/01/26/1547204/new-michigan-law-requires-high-schools-to-offer-cs-classes?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1737912840,"author":"EditorDavid","guid":355,"unread":true,"content":"The state of Michigan will now require each public high school in the state to offer at least one computer science course to its students. \"This bill aligns Michigan with a majority of the country,\" according to the state's announcement, which says the bill \"advances technological literacy\" and ensures their students \"are well-equipped with the critical thinking skills necessary for success in the workforce.\" \nSlashdot reader theodp writes:\n\n\n\nFrom the Michigan House Fiscal Agency Analysis: \"Supporters of the bill say that increasing access to computer science courses for students in schools should be a priority of the state in order to ensure that students can compete for the types of jobs that have good pay and will be needed in the coming decades.\" \nThat analysis goes on to report that testifying in favor of the bill were tech-giant backed nonprofit Code.org (Microsoft is a $30 million Code.org donor), Amazon and AWS (Amazon is a $30+ million Code.org donor), the tech-supported Computer Science Teachers Association (CSTA), and the lobbying organization TechNet, whose members include Amazon, Apple, Google, Meta, and OpenAI). \nIt's not clear how many high schools in Michigan are already teaching CS courses, but this still raises a popular question for discussion. Should high schools be required to teach at least one CS course?","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oracle and US Investors (Including Microsoft) Discuss Taking Control of TikTok in the US","url":"https://tech.slashdot.org/story/25/01/26/0037255/oracle-and-us-investors-including-microsoft-discuss-taking-control-of-tiktok-in-the-us?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1737858840,"author":"EditorDavid","guid":354,"unread":true,"content":"A plan to keep TikTok available in the U.S. \"involves tapping software company Oracle and a group of outside investors,\" reports NPR, \"to effectively take control of the app's global operations, according to two people with direct knowledge of the talks...\" \n\n\"[P]otential investors who are engaged in the talks include Microsoft.\"\n\nUnder the deal now being negotiated by the White House, TikTok's China-based owner ByteDance would retain a minority stake in the company, but the app's algorithm, data collection and software updates will be overseen by Oracle, which already provides the foundation of TikTok's web infrastructure... \"The goal is for Oracle to effectively monitor and provide oversight with what is going on with TikTok,\" said the person directly involved in the talks, who was not authorized to speak publicly about the deliberations. \"ByteDance wouldn't completely go away, but it would minimize Chinese ownership....\" Officials from Oracle and the White House held a meeting on Friday about a potential deal, and another meeting has been scheduled for next week, according to the source involved in the discussions, who said Oracle is interested in a TikTok stake \"in the tens of billions,\" but the rest of the deal is in flux... \n\n\nUnder a law passed by Congress and upheld by the Supreme Court, TikTok must execute what is known as \"qualified divestiture\" from ByteDance in order to stay in business in the U.S... A congressional staffer involved in talks about TikTok's future, who was not authorized to speak publicly, said binding legal agreements from the White House ensuring ByteDance cannot covertly manipulate the app will prove critical in winning lawmakers' approval. \"A key part is showing there is no operational relationship with ByteDance, that they do not have control,\" the Congressional staffer said. \"There needs to be no backdoors where China can potentially gain access....\" \n\n\nChinese regulators, who have for years opposed the selling of TikTok, recently signaled that they would not stand in the way of a TikTok ownership change, saying acquisitions \"should be independently decided by the enterprises and based on market principles.\" The statement, at first, does not seem to say much, but negotiators in the White House believe it indicates that Beijing is not planning to block a deal that gives American investors a majority-stake position in the company.\n \n\n\"Meanwhile, Apple and Google still have not returned TikTok to app stores...\"","contentLength":2485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring prefect for scheduling data workloads","url":"https://www.youtube.com/watch?v=hkm5OfiiorE","date":1737765055,"author":"probabl","guid":526,"unread":true,"content":"<article>It is easy to train a model in a notebook, but if we want to do anything with it in production we probably want to be able to train it regularly. There's good old cron, but it's 2025 now so maybe it's time to explore something a bit more modern. That's why in this livestream we will be exploring Prefect.\n\nWebsite: https://probabl.ai/\nDiscord: https://discord.probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\nIf you're keen to see more videos like this, you can follow us over at  @probabl_ai.\n\n#probabl</article>","contentLength":723,"flags":null,"enclosureUrl":"https://www.youtube.com/v/hkm5OfiiorE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Podcast with Luca Rossi","url":"https://refactoring.fm/p/growing-the-development-forest-with","date":1737741660,"author":"Martin Fowler","guid":291,"unread":true,"content":"<p>Luca Rossi hosts a podcast (and newsletter) called Refactoring, so it's\n      obvious that we have some interests in common. The tile comes from me\n      leaning heavily on Beth Anders-Beck and Kent Beck's metaphor of <a href=\"https://refactoring.fm/bliki/ForestAndDesert.html\">The Forest and The Desert</a>. We talk\n      about the impact of AI on software development, the metaphor of technical\n      debt, and the current state of agile software development.</p>","contentLength":397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bringing Jetpack Compose to Instagram for Android","url":"https://engineering.fb.com/2025/01/24/android/bringing-jetpack-compose-to-instagram-for-android/","date":1737739853,"author":"","guid":467,"unread":true,"content":"<p>Introducing a new Android UI framework like Jetpack Compose into an existing app is more complicated than importing some AARS and coding away. What if your app has specific performance goals to meet? What about existing design components, integrations with navigation, and logging frameworks?</p><p>On this episode of the Meta Tech Podcast <a href=\"https://www.threads.net/@passy_\" target=\"_blank\" rel=\"noopener\">Pascal Hartig</a> is joined by Summer, a software engineer whose team handles large-scale migrations for Instagram. Summer walks through the various thoughtful and intricate phases that Instagram goes through to ensure that developers have the best possible experience when working on our codebases. She also discusses balancing all of this with Meta‚Äôs infrastructure teams, who have to maintain multiple implementations at once.</p><p>Learn how Meta approaches the rollout of a new framework and more!</p><p>Download or listen to the podcast episode below:</p><p>\nYou can also find the episode wherever you get your podcasts, including:</p><p>The&nbsp;<a href=\"https://insidefacebookmobile.libsyn.com/\" target=\"_blank\" rel=\"noopener\">Meta Tech Podcast</a>&nbsp;is a podcast, brought to you by Meta, where we highlight the work Meta‚Äôs engineers are doing at every level ‚Äì from low-level frameworks to end-user features.</p><p>And if you‚Äôre interested in learning more about career opportunities at Meta visit the&nbsp;<a href=\"https://www.metacareers.com/?ref=engineering.fb.com\" target=\"_blank\" rel=\"noopener\">Meta Careers</a>&nbsp;page.</p>","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Real Python Podcast ‚Äì Episode #236: Simon Willison: Using LLMs for Python Development","url":"https://realpython.com/podcasts/rpp/236/","date":1737720000,"author":"Real Python","guid":165,"unread":true,"content":"<p>What are the current large language model (LLM) tools you can use to develop Python? What prompting techniques and strategies produce better results? This week on the show, we speak with Simon Willison about his LLM research and his exploration of writing Python code with these rapidly evolving tools.</p>","contentLength":302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How we accidentally built a better build system for OCaml","url":"https://blog.janestreet.com/how-we-accidentally-built-a-better-build-system-for-ocaml-index/","date":1737676800,"author":"","guid":606,"unread":true,"content":"<p>A ‚Äúbuild system‚Äù is one of the most important tools in a developer‚Äôs\ntoolbox. Roughly, it figures out how to create runnable programs from\na bunch of different source files by calling out to the compiler,\nsetting up and executing test suites, and so on. Because you interact\nwith it daily, above all it has to be <a href=\"https://xkcd.com/303/\">fast</a> ‚Äì\nbut it also has to be flexible.</p>","contentLength":359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bun's on a roll with v1.2","url":"https://javascriptweekly.com/issues/720","date":1737676800,"author":"","guid":598,"unread":true,"content":"<a href=\"https://javascriptweekly.com/link/164760/rss\"><img src=\"https://res.cloudinary.com/cpress/image/upload/c_limit,w_480,h_480,q_auto/copm/329e3853.png\" width=\"193\" height=\"143\"></a><p> ‚Äî A fast and flexible DataGrid for building modern web apps. Key features and virtualized rendering are included in the core grid module. Pick &amp; choose special features to keep your app small. Built for JavaScript, extended to Angular, React, and&nbsp;Vue.</p><p>Wijmo From MESCIUS inc. </p>","contentLength":279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"JavaScript Temporal is coming","url":"https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/","date":1737676800,"author":"brian-smith","guid":89,"unread":true,"content":"<article>A new way to handle dates and times is being added to JavaScript. Let's take a look at Temporal, what problems it solves, the current state, and what you'll find in the new documentation about it on MDN.\n</article>","contentLength":204,"flags":null,"enclosureUrl":"https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/featured.png","enclosureMime":"","commentsUrl":null},{"title":"Template literal types in TypeScript: parsing during type checking and more","url":"https://2ality.com/2025/01/template-literal-types.html","date":1737676800,"author":"Dr. Axel Rauschmayer","guid":345,"unread":true,"content":"<p>In this blog post, we take a closer look at template literal types in TypeScript: While their syntax is similar to JavaScript‚Äôs template literals, they operate at the type level. Their use cases include:</p><ul><li>Static syntax checking for string literals</li><li>Transforming the casing of property names (e.g. from hyphen case to camel case)</li><li>Concisely specifying large string literal union types</li></ul>","contentLength":379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build It Yourself","url":"http://lucumr.pocoo.org/2025/1/24/build-it-yourself","date":1737676800,"author":"Armin Ronacher","guid":302,"unread":true,"content":"<p>Another day, another <a href=\"http://lucumr.pocoo.org/2016/3/24/open-source-trust-scaling/\">rant</a><a href=\"http://lucumr.pocoo.org/2022/1/10/dependency-risk-and-funding/\">about</a><a href=\"http://lucumr.pocoo.org/2024/3/26/rust-cdo/\">dependencies</a>. from me.  This time I will ask you that we\nstart and support a vibe shift when it comes to dependencies.</p><p>You're probably familiar with the concept of ‚Äúdependency churn.‚Äù  It's that\nnever-ending treadmill of updates, patches, audits, and transitive\ndependencies that we as developers love to casually install in the name of\nproductivity.  Who doesn't enjoy waiting for yet another \njust so you can get that fix for a bug you don't even have?</p><p>It's a plague in most ecosystems with good packaging solutions.\nJavaScript and Rust are particularly badly affected by that.  A brand new\nTokio project drags in 28 crates, a new Rocket project balloons that to\n172, and a little template engine like MiniJinja can exist with just a\nsingle dependency ‚Äî while its CLI variant slurps up 142.</p><p>If that doesn't sound like a big deal, let's consider <a href=\"https://crates.io/crates/terminal_size\">terminal_size</a>.  It is a crate that does\nexactly what its name suggests: it figures out your terminal dimensions.\nThe underlying APIs it uses have effectively been stable since the earliest days of computing\nterminals‚Äîwhat, 50 years or so? And yet, for one function, terminal-size\nmanages to introduce three or four additional crates, depending on your\noperating system.  That triggers a whole chain reaction, so you end up\ncompiling thousands of other functions just to figure out if your terminal\nis 80x25 or 120x40.  That crate had 26 releases.  My own version of that\nthat I have stuck away in a project from 10 years ago still works without\na single update.  Because shocker: nothing about figuring out terminal\nsizes has changed.  </p><p>So why does  have so many updates if it's so stable?\nBecause it's build on top of platform abstraction libraries that\nconstantly churn, so it needs to update to avoid code duplication and\nblowing up compile times even more.</p><p>But ‚Äúbig supply chain‚Äù will tell you that you must do it this way.  Don't\nyou dare to copy paste that function into your library.  Or don't you date\nto use ‚Äúunsafe‚Äù yourself.  You're not qualified enough to write unsafe\ncode, let the platform abstraction architects do that.  Otherwise someone\n<a href=\"https://github.com/geiger-rs/cargo-geiger\">will slap you</a>.  There are\nentire companies who are making a living of supplying you with the tools\nneeded to deal with your dependency mess.  In the name of security, we're\npushed to having dependencies and keeping them up to date, despite most of\nthose dependencies being the primary source of security problems.</p><p>The goal of code in many ways should be to be written in a way that it\ndoes not need updates.  It should eventually achieve some level of\nstability.  In the Rust ecosystem stable code is punished.  If you have a\nperfectly working dependency but you have a somewhat inactive bug tracker,\nRUSTSEC will come by and <a href=\"http://lucumr.pocoo.org/2024/3/26/rust-cdo/\">give you a chunk rating</a>.</p><p>But there  a simpler path.  You write code yourself.  Sure, it's more\nwork up front, but once it's written, it's done. No new crates, no waiting\nfor upsteam authors to fix that edge case.  If it's broken for you, you\nfix it yourself.  Code that works doesn't necessarily need the\nmaintenance treadmill.  Your code has a corner case?  Who cares.  This is\nthat vibe shift we need in the Rust world: celebrating fewer dependencies\nrather than more.</p><p>We're at a point in the most ecosystems where pulling in libraries is not\njust the default action, it's seen positively: ‚ÄúLook how modular and\ncomposable my code is!‚Äù  Actually, it might just be a symptom of never\nwanting to type out more than a few lines.</p><p>Now one will make the argument that it takes so much time to write all of\nthis.  It's 2025 and it's faster for me to have ChatGPT or Cursor whip up\na dependency free implementation of these common functions, than it is for\nme to start figuring out a dependency.  And it makes sense as for many\nsuch small functions the maintenance overhead is tiny and much lower than\nactually dealing with constant upgrading of dependencies.  The code is just\na few lines and you also get the benefit of no longer need to compile\nthousands of lines of other people's code for a single function.</p><p>But let's face it: corporate code review culture has also has infected\nOpen Source software.  Companies are more likely to reward engineers than\nscold them for pulling in that new ‚Äúshiny library‚Äù that solves the problem\nthey never actually had.  That creates problems, so dependabot and friends\nwere born.  Today I just dread getting dependabot pull requests but on\nprojects but I have to accept it.  I'm part of an ecosystem with my stuff\nand that ecosystem is all about churn, churn, churn.  In companies you can\nalso keep entire internal engineering teams busy with vendoring\ndependencies, internal audits and upgrading things throughout the company.</p><p>Fighting this fight is incredibly hard!  Every new hire has been trained\non the idea that dependencies are great, that code reuse is great.  That\nhaving old code sitting around is a sign of bad engineering culture.</p><p>It's also hard to fight this in Open Source.  Years ago I wrote <a href=\"https://crates.io/crates/sha1_smol\">sha1-smol</a> which originally was just called\n.  It became the standard crate to calculate SHA1 hashes.\nEventually I was pressured to donate that package name to rust-crypto and\nto depend on the rest of the crypto ecosystem as it was so established.\nIf you want to use the new sha1 crate, you get to enjoy 10 dependencies.\nBut there was just no way around it, because that name in the registry is\nprecious and people also wanted to have trait compatibility.  It feels\ntiring to be the only person in a conversation pushing to keep the churn\ndown and dependencies low.</p><p>It's time to have a new perspective: we should give kudos to engineers who\nwrite a small function themselves instead of hooking in a transitive web\nof crates.  We should be suspicious of big crate graphs.  Celebrated are\nthe minimal dependencies, the humble function that just quietly does the\njob, the code that doesn't need to be touched for years because it was\ndone right once.</p><p>And sure, it's not black and white.  There are the important libraries\nthat solve hard problems.  Graphics libraries that abstract over complex\ndrivers, implementations of protocols like HTTP and QUIC.  I won't be able\nto get rid of tokio and I have no desire to.  But when you end up using\none function, but you compile hundreds, some alarm bell should go off.</p><p>We need that vibe shift.  To celebrate building it yourself when it's\nappropriate to do so.  To give credit to library authors who build low to\nno-dependency Open Source libraries.</p><p>For instance minijinja celebrates it in the readme:</p><pre>$ cargo tree\nminimal v0.1.0 (examples/minimal)\n‚îî‚îÄ‚îÄ minijinja v2.6.0 (minijinja)\n    ‚îî‚îÄ‚îÄ serde v1.0.144\n</pre><p>And it has a PR to eventually <a href=\"https://github.com/mitsuhiko/minijinja/pull/539\">get rid of the last dependency</a>.  And sometime this\nyear I will make it my goal to go ahead proudly and trim down all that fat\nin my projects.</p>","contentLength":6833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepSeek: New Free AI That Changes Everything!","url":"https://www.youtube.com/watch?v=btvB56PkvwE","date":1737625587,"author":"Two Minute Papers","guid":517,"unread":true,"content":"<article>‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nTry it out (choose DeepSeek as your model): https://huggingface.co/chat/\nOfficial (read the privacy policy below before you use this one): https://www.deepseek.com/\n\nRun it at home:\nhttps://www.reddit.com/r/selfhosted/comments/1i6ggyh/got_deepseek_r1_running_locally_full_setup_guide/\nhttps://lmstudio.ai/\n\nLinks:\nhttps://x.com/ivanfioravanti/status/1881565759123702140\nhttps://eu.jotform.com/tables/241831324457354\nhttps://x.com/deepseek_ai/status/1881318130334814301\nhttps://x.com/nobody_qwert/status/1881620406710452535\nhttps://github.com/bytedance/UI-TARS\nhttps://github.com/bytedance/UI-TARS-desktop\nhttps://x.com/arcprize/status/1881761991116927066\nhttps://x.com/awnihannun/status/1881412271236346233\nhttps://x.com/thegenioo/status/1881720731458797793\nhttps://x.com/awnihannun/status/1881386796266946743\nhttps://chat.deepseek.com/downloads/DeepSeek%20Privacy%20Policy.html\nhttps://github.com/MoonshotAI/Kimi-k1.5\nhttps://github.com/bytedance/UI-TARS-desktop\nhttps://x.com/_akhaliq/status/1881886333431628001\nhttps://x.com/awnihannun/status/1882105560201617903\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\n#deepseek\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu</article>","contentLength":1980,"flags":null,"enclosureUrl":"https://www.youtube.com/v/btvB56PkvwE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Playing with the classification report","url":"https://www.youtube.com/watch?v=765qaIk30Rs","date":1737623293,"author":"probabl","guid":525,"unread":true,"content":"<article>In this video we will play around with a confusion matrix widget that will help us understand how the numbers in the classification report in scikit-learn are created. The classification report is a great utility, but it can help to remind oneself of what the numbers really mean. \n\nScikit-learn documentation:\nhttps://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html\n\nAppendix with notebooks:\nhttps://github.com/probabl-ai/youtube-appendix/tree/main/16-metrics\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\nBluesky: https://bsky.app/profile/probabl.bsky.social\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":879,"flags":null,"enclosureUrl":"https://www.youtube.com/v/765qaIk30Rs?version=3","enclosureMime":"","commentsUrl":null},{"title":"Master the Art of REST API Design","url":"https://blog.algomaster.io/p/master-the-art-of-rest-api-design","date":1737610478,"author":"Ashish Pratap Singh","guid":745,"unread":true,"content":"<p> is one of the most crucial steps in  and a key topic of discussion in .</p><p>A well-designed <a href=\"https://blog.algomaster.io/p/whats-an-api\">API</a> allows developers to easily integrate with a system while ensuring scalability and security.</p><p>Over the years, various  have emerged, including <strong>REST, GraphQL, gRPC, Webhooks and SOAP,</strong> each designed to address different needs.</p><p>However,  continue to dominate web development due to their simplicity, scalability, flexibility, widespread adoption and alignment with HTTP standards.</p><p>In this article, we will dive into covering:</p><ul><li><p> for building a well-structured, scalable, and secure RESTful API.</p></li><li><p><strong>Performance optimization techniques</strong> to enhance API efficiency and response times.</p></li></ul><p>REST (<strong>Representational State Transfer</strong>) is an architectural style for designing web services that enable communication between clients (e.g., web browsers, mobile apps) and servers over the .</p><p>REST uses HTTP methods (GET, POST, PUT, DELETE, etc.) to <strong>retrieve, create, update, and delete</strong> resources.</p><p>To build a well-designed REST APIyou must first understand the fundamentals of theHTTP protocol.</p><h3>1. HTTP Methods (Verbs) in REST APIs</h3><p>HTTP provides a set of  that define the type of operation to be performed on a resource.</p><p>In RESTful architectures, these methods typically map to CRUD operations:</p><p>It‚Äôs essential to use the correct HTTP method to make your API clear and intuitive. For example,  signals a read-only request to developers and should never modify server data, while  indicates data creation or an action that results in a change.</p><h3>2. REST is Resource-Oriented</h3><p>In RESTful API design, data is represented as , and each resource is identified by a <strong>Uniform Resource Identifier (URI)</strong>.</p><ul><li><p>‚Üí A collection (or list) of books</p></li><li><p> ‚Üí A specific book with ID 123</p></li></ul><p>An  is a combination of:</p><ul><li><p>An HTTP method (GET, POST, PUT etc.)</p></li><li><p>A resource URI (, )</p></li></ul><p>Each endpoint represents a specific operation on a resource.</p><ul><li><p> ‚Üí Fetch all books</p></li><li><p> ‚Üí Create a new book</p></li><li><p> ‚Üí Delete the book with ID 123</p></li></ul><p>Using clear and consistent endpoints helps developers quickly understand how to interact with your API.</p><h3>4. HTTP Status Codes: Understanding API Responses</h3><p>Each API response includes an , which indicates the result of the request.</p><p>Using meaningful status codes is important for helping consumers of your API understand why a request might have failed and how they can fix or retry it.</p><p>Common status codes include:</p><ul><li><p>: The request was successfully received and processed.</p><ul><li><p>: The request succeeded.</p></li><li><p>: A new resource was successfully created.</p></li><li><p>: The request succeeded, but there is no content to return.</p></li></ul></li><li><p>: Further action is needed to complete the request (e.g., a different endpoint or resource location).</p></li><li><p>: There was an error in the request sent by the client.</p><ul><li><p>: The request was malformed or invalid.</p></li><li><p>: Authentication is required or has failed.</p></li><li><p>: The client does not have permission to access the resource.</p></li><li><p>: The requested resource does not exist.</p></li><li><p>: Rate limit exceeded.</p></li></ul></li><li><p>: The server encountered an error while processing the request.</p><ul><li><p><strong>500 Internal Server Error</strong>: A general error occurred on the server.</p></li><li><p>: The server is currently unable to handle the request, often due to maintenance or overload.</p></li></ul></li></ul><h2><strong>1. Define Clear Resource Naming Conventions</strong></h2><p>Using a <strong>consistent, intuitive, and hierarchical structure</strong> for API endpoints improves both readability and usability. The goal is to help developers quickly understand how to interact with your API without extensive documentation.</p><p>Since REST is resource-oriented, focus on  (nouns) rather than  (verbs) for your endpoints. The HTTP methods (, , etc.) already describe the action, so using verbs in the URL are redundant.</p><pre><code><code>GET /getAllUsers\nPOST /createNewOrder\nDELETE /removeProduct/123</code></code></pre><pre><code>GET /users\nPOST /orders\nDELETE /products/123</code></pre>","contentLength":3656,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64780231-6a91-4d72-8ea2-159d4cd71f4e_1504x1056.png","enclosureMime":"","commentsUrl":null},{"title":"How Meta discovers data flows via lineage at scale","url":"https://engineering.fb.com/2025/01/22/security/how-meta-discovers-data-flows-via-lineage-at-scale/","date":1737608445,"author":"","guid":466,"unread":true,"content":"<ul><li aria-level=\"1\"><a href=\"https://engineering.fb.com/2025/01/22/security/how-meta-discovers-data-flows-via-lineage-at-scale/#learnings\"></a></li></ul><p><a href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\" target=\"_blank\" rel=\"noopener\"></a></p><ul><li> involves collecting various code and data assets (e.g., web endpoints, data tables, AI models) used across Meta.</li><li> expresses data assets in structural detail (e.g., indicating that a data asset has a field called ‚Äúreligion‚Äù).</li><li> labels data to describe its content (e.g., specifying that the identity column contains religion data).</li></ul><h2></h2><ul><li aria-level=\"1\"><b>Scalable data flow discovery</b></li><li aria-level=\"1\"><b>Efficient rollout of privacy controls</b></li><li aria-level=\"1\"><b>Continuous compliance verification</b></li></ul><h2></h2><ol><li aria-level=\"1\"><b>Collecting data flow signals</b></li><li aria-level=\"1\"><b>Identifying relevant data flows</b></li></ol><h3></h3><p><a href=\"https://about.fb.com/news/2020/10/privacy-matters-facebook-dating/\"></a></p><p><a href=\"https://engineering.fb.com/2021/10/20/security/static-analysis-award/'\"></a></p><ol></ol><table border=\"1\"><tbody><tr></tr><tr></tr><tr><td></td></tr><tr><td></td></tr></tbody></table><h3></h3><p><a href=\"https://research.facebook.com/publications/presto-sql-on-everything/\"></a><a href=\"https://spark.apache.org/\"></a></p><h3></h3><p><a href=\"https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/\"></a><a href=\"https://pytorch.org/\"></a><a href=\"https://engineering.fb.com/2016/05/09/core-infra/introducing-fblearner-flow-facebook-s-ai-backbone/\"></a></p><h2></h2><ol><li aria-level=\"1\"><b>Exclude and include candidates: </b></li></ol><ul><li aria-level=\"1\"><b>Focus on lineage early and reap the rewards</b></li><li aria-level=\"1\"><b>Build lineage consumption tools to gain engineering efficiency</b></li><li aria-level=\"1\"><b>Integrate lineage with systems to scale the coverage</b></li><li aria-level=\"1\"><b>Measurement improves our outcomes</b></li></ul><h2></h2><ul><li aria-level=\"1\"><b>Improving consumption experience</b></li></ul><p><i></i></p>","contentLength":744,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"December Project Goals Update","url":"https://blog.rust-lang.org/2025/01/23/Project-Goals-Dec-Update.html","date":1737590400,"author":"David Wood and Niko Matsakis","guid":238,"unread":true,"content":"<p>Our big goal for this period was async closures, and we are excited to announce that work there is done! Stable support for async closures <a href=\"https://github.com/rust-lang/rust/pull/132706#issuecomment-2540470500\">landed on nightly on Dec 12</a> and it will be included in Rust 1.85, which ships on Feb 20. Big kudos to <a href=\"https://github.com/compiler-errors\">compiler-errors</a> for driving that.</p><p>For our other goals, we made progress, but there remains work to be done:</p><ul><li><strong>Return Type Notation (RTN)</strong> is implemented and we had a <a href=\"https://blog.rust-lang.org/inside-rust/2024/09/26/rtn-call-for-testing.html\">call for experimentation</a> but it has not yet reached stable. This will be done as part of our 2025H1 goal.</li><li>Async Functions in Traits (and Return Position Impl Trait in Trait) are currently not consided  compatible. We would eventually like to have first-class  support, but as an intermediate step we created a procedural macro crate <a href=\"https://crates.io/crates/dynosaur\"></a> that can create wrappers that enable . We are planning a comprehensive blog post in 2025H1 that shows how to use this crate and lays out the overall plan for async functions in traits.</li><li>Work was done to prototype an <strong>implementation for async drop</strong> but we didn't account for reviewing bandwidth. <a href=\"https://github.com/nikomatsakis\">nikomatsakis</a> has done initial reads and is working with PR author to get this done in 2025H1. To be clear though the scope of this is an experiment with the goal of uncovering implementation hurdles. There remains significant language design work before this feature would be considered for stabilization (we don't even have an RFC, and there are lots of unknowns remaining).</li><li>We have had fruitful discussions about the trait for  but do not have widespread consensus, that's on the docket for 2025H1.</li></ul><p>We largely completed our goal to stabilize the language features used by the Rust for Linux project. In some cases a small amount of work remains. Over the last six months, we...</p><ul><li>stabilized the  macro to get the offset of fields;</li><li> stabilization PR and reference updates are up, excluding the \"output\" feature.</li><li>completed the majority of the work for arbitrary self types, which is being used by RfL and just needs documentation before stabilisation</li></ul><p>We also began work on compiler flag stabilization with <a href=\"https://github.com/rust-lang/rfcs/pull/3716\">RFC 3716</a>, which outlines a scheme for stabilizing flags that modify the target ABI.</p><p>The final release of Rust 2024 is confirmed for February 20, 2025 as part of Rust 1.85. Rust 1.85 is currently in beta. Feedback from the nightly beta and crater runs has been actively addressed, with adjustments to migrations and documentation to enhance user experience.</p><p>Big shout-outs to <a href=\"https://github.com/traviscross\">TC</a> and  <a href=\"https://github.com/ehuss/\">Eric Huss</a> for their hard work driving this program forward.</p><p>Over the last six months a number of internal refactorings have taken place that are necessary to support a  prototype.</p><p>One refactoring is that we have changed how we represent const arguments in the compiler to allow for adding a separate representation for the kinds of const arguments that  will add.</p><p>Another big refactoring is that we have changed the API surface for our representation of const arguments in the type system layer, there is no longer a way to evaluate a const argument without going through our general purpose type system logic. This was necessary to ensure that we correctly handle equality of the kinds of const arguments that  will support.</p><p>With all of these pre-requisite refactorings completed, a feature gate has been added to the compiler (<code>feature(min_generic_const_args)</code>) that uses the new internal representation of const arguments. We are now beginning to implement the actual language changes under this feature gate.</p><p>Over the course of the last six months...</p><ul><li>cargo semver-checks began to include generic parameters and bounds in its schema, allowing for more precise lints;</li><li>cargo manifest linting was implemented and merged, allowing for lints that look at the cargo manifest;</li><li>building on cargo manifest linting, the  lint was added, which identifies breakage caused by the removal of a package feature.</li></ul><p>In addition, we fleshed out a design sketch for the changes in rustdoc's JSON support that are needed to support cross-crate item linting. This in turn requires compiler extensions to supply that information to rustdoc.</p><ul><li>Progress was made on adding const traits and implementation in the compiler, with improvements being carefully considered.  was constified in <a href=\"https://github.com/rust-lang/rust/pull/133237\">rust#133237</a> and / in <a href=\"https://github.com/rust-lang/rust/pull/133260\">rust#133260</a>.</li></ul><ul><li>Over the last six months, we created a lang-team experiment devoted to this issue and <a href=\"https://github.com/spastorino\">spastorino</a> began work on an experimental implementation. <a href=\"https://github.com/joshtriplett\">joshtriplett</a> authored <a href=\"https://github.com/rust-lang/rfcs/pull/3680\">RFC 3680</a>, which has received substantial feedback. The current work is focused on identifying \"cheaply cloneable\" types and making it easy to create closures that clone them instead of moving them.</li></ul><ul><li>Alternatives to sandboxed build scripts are going to be investigated instead of continuing this project goal into 2025h1 - namely, declaratively configuring system dependencies with <a href=\"https://crates.io/crates/system-deps\"></a>, using an approach similar to code-checker <a href=\"https://crates.io/crates/cargo-acl\">Cackle</a> and its sandbox environment <a href=\"https://github.com/containers/bubblewrap\">Bubblewrap</a>, or fully-sandboxed build environments like Docker or Nix.</li></ul><ul><li>Significant speedups have been achieved, reducing the slowest crate resolution time from over 120 seconds to 11 seconds, and decreasing the time to check all crates from 178 minutes to 71.42 minutes.</li><li>Performance improvements have been made to both the existing resolver and the new implementation, with the lock file verification time for all crates reduced from 44.90 minutes to 32.77 minutes (excluding some of the hardest cases).</li></ul><ul><li>Our pull request adding example searches and adding a search button has been added to the agenda for the rustdoc team next meeting.</li></ul><ul><li>The  stabilization is now stable in version 1.84, with a new update blogpost published.</li><li>Significant progress was made on bootstrap with . We're now able to compile rustc and cargo, enabling try-builds and perf runs.</li></ul><ul><li>An optimisation for the  lint is open, benchmarked, and currently under review.</li><li>Help is needed on any issue marked with , especially on issue #13714.</li></ul><ul><li>Over the course of this goal, Nadrieril wrote and posted the never patterns RFC as an attempt to make progress without figuring out the whole picture, and the general feedback was \"we want to see the whole picture\". Next step will be to write up an RFC that includes a clear proposal for which empty patterns can and cannot be omitted. This is 100% bottlenecked on my own writing bandwidth (reach out if you want to help!). Work will continue but the goal won't be resubmitted for 2025h1.</li></ul><ul><li><a href=\"https://github.com/amandasystems\">Amanda</a> has made progress on removing placeholders, focusing on lazy constraints and early error reporting, as well as investigating issues with rewriting type tests; a few tests are still failing, and it seems error reporting and diagnostics will be hard to keep exactly as today.</li><li><a href=\"https://github.com/lqd\">@lqd</a> has opened PRs to land the prototype of the location-sensitive analysis. It's working well enough that it's worthwhile to land; there is still a lot of work left to do, but it's a major milestone, which we hoped to achieve with this project goal.</li></ul><ul><li>A fix stopping cargo-script from overriding the release profile was posted and merged.</li><li>Help is wanted for writing frontmatter support in rustc, as rustfmt folks are requesting it to be represented in the AST.</li></ul><ul><li>RFC is done, waiting for all rustdoc team members to take a look before implementation can start.</li></ul><ul><li><a href=\"https://github.com/SparrowLii\">SparrowLii</a> proposed a 2025H1 project goal to continue stabilizing the parallel front end, focusing on solving reproducible deadlock issues and improving parallel compilation performance.</li><li>The team discussed solutions to avoid potential deadlocks, finding that disabling work-stealing in rayon's subloops is effective, and will incorporate related modifications in a PR.</li></ul><ul><li>Progress on  continued despite a busy schedule, with a focus on improving suggestions and addressing architectural challenges.</li><li>A new API was designed in collaboration with <a href=\"https://github.com/epage\">epage</a>, aiming to align  more closely with  for easier contribution and integration.</li></ul><ul><li>The project goal slate for 2025h1 has been posted <a href=\"https://github.com/rust-lang/rfcs/pull/3764#issuecomment-2593395122\">as an RFC</a> and is waiting on approval from project team leads.</li></ul><ul><li>Another pull request was merged with only one remaining until a working MVP is available on nightly.</li><li>Some features were removed to simplify upstreaming and will be added back as single PRs.</li><li>Will start work on  feature of LLVM/Enzyme which allows Array of Struct and Struct of Array vectorisation.</li><li>There's been a push to add a AMD GPU target to the compiler which would have been needed for the LLVM offload project.</li></ul><ul><li>We have written and verified around 220 safety contracts in the verify-rust-std fork.</li><li>3 out of 14 challenges have been solved.</li><li>We have successfully integrated Kani in the repository CI, and we are working on the integration of 2 other verification tools: VeriFast and Goto-transcoder (ESBMC)</li></ul><ul><li>There wasn't any progress on this goal, but building a community around a-mir-formality is still a goal and future plans are coming.</li></ul><p>The following goals have not received updates in the last month:</p>","contentLength":8756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to align open LLMs in 2025 with DPO and and synthetic data","url":"https://www.philschmid.de/rl-with-llms-in-2025-dpo","date":1737590400,"author":"","guid":14,"unread":true,"content":"<article>Learn how to align LLMs using Hugging Face TRL and RLHF through Direct Preference Optimization (DPO) and on-policy synthetic data.</article>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Codemods in other languages","url":"https://martinfowler.com/articles/codemods-api-refactoring.html#CodemodsInOtherLanguages","date":1737561240,"author":"Martin Fowler","guid":290,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing the EstimatorReport","url":"https://www.youtube.com/watch?v=2rjkrR7CjHc","date":1737557343,"author":"probabl","guid":524,"unread":true,"content":"<article>Skore version 0.6 introduces a new EstimatorReport, which can be seen as a wrapper around an estimator that automatically detects all the relevant metrics and charts. The goal of the project is to be a useful sidekick for scikit-learn and you can expect more utilities to be released soon. \n\nLinks:\nskore v0.6 documentation:\nhttps://skore.probabl.ai/0.6/index.html\n\nskore GitHub repository:\nhttps://github.com/probabl-ai/skore</article>","contentLength":426,"flags":null,"enclosureUrl":"https://www.youtube.com/v/2rjkrR7CjHc?version=3","enclosureMime":"","commentsUrl":null},{"title":"How Bluesky Works ü¶ã","url":"https://newsletter.systemdesign.one/p/how-does-bluesky-work","date":1737551587,"author":"Neo Kim","guid":53,"unread":true,"content":"<p>Get the powerful template to approach system design for FREE on newsletter sign-up:</p><p><em>This post outlines Bluesky architecture; you will find references at the bottom of this page if you want to go deeper.</em></p><p><em>Note: I wrote this post after reading their engineering blog and documentation.</em></p><p>Once upon a time, Twitter had only a few million users.</p><p>And each user interaction went through their centralized servers.</p><p>So content moderation was easy.</p><p>Yet their growth rate was explosive and became one of the most visited sites in the world.</p><p>So they added automation and human reviewers to scale moderation.</p><p>Although it temporarily solved their content moderation issues, there were newer problems. Here are some of them:</p><ul><li><p>The risk of error increases if a single authority decides the moderation policies.</p></li><li><p>There‚Äôs a risk of bias when a single authority makes moderation decisions.</p></li><li><p>Managing moderation at scale needs a lot of effort; it becomes a bottleneck.</p></li></ul><p>So they set up <a href=\"https://bsky.app/\">Bluesky</a>: a research initiative to build a decentralized social network.</p><p>A decentralized architecture distributes control across many servers.</p><p>Here are 3 popular decentralized architectures in distributed systems:</p><ul><li><p>Federated architecture: client-server model; but different people run parts of the system and parts communicate with each other.</p></li><li><p>Peer-to-Peer architecture: there‚Äôs no difference between client and server; each device acts as both.</p></li><li><p>Blockchain architecture: distributed ledger for consensus and trustless interactions between servers.</p></li></ul><p><em>Bluesky uses a federated architecture</em><em>for its familiar client-server model, reliability, and convenience.</em> So each server could be run by different people and servers communicate with each other over HTTP.</p><p>Think of the federated network as email; a person with Gmail can communicate with someone using Protonmail.</p><p>Yet building a decentralized social network at scale is difficult.</p><p>So smart engineers at Bluesky used simple ideas to solve this hard problem.</p><p><em>I wrote a summary of this post (save it for later):</em></p><p>They created a decentralized open-source framework to build social networking apps, called Authenticated Transfer Protocol (), and built Bluesky on top of it.</p><p>Put simply, Bluesky doesn‚Äôt run separate servers; instead, ATProto servers distribute messages to each other.</p><p>A user‚Äôs data is shared across apps built on ATProto; Bluesky is one of the apps. So if a user switches between apps on ATProto, such as a photo-sharing app or blogging app, they don‚Äôt lose their followers (<a href=\"https://en.wikipedia.org/wiki/Social_graph\">social graph)</a>.</p><p>Imagine ATProto as Disneyland Park and Bluesky as one of its attractions. A single ticket is enough to visit all the attractions in the park. And if you don‚Äôt like one of the attractions, try another one in the park.</p><p>Here‚Äôs how Bluesky works:</p><p>A <a href=\"https://docs.bsky.app/docs/tutorials/creating-a-post\">post</a> is a short status update by a user; it includes text and images. </p><p>The text content and timestamp of the post are stored in a . <em>Think of the repository as a collection of data published by a single user.</em><a href=\"https://www.sqlite.org/\">SQLite</a> is used as its data layer for simplicity; each repository gets a separate SQLite database.</p><p>The data records are encoded in <a href=\"https://cbor.io/\">CBOR</a>, a compact binary format, before storing it in SQLite for low costs.</p><p>Repositories of different users are stored on a ; they set up many data servers for scale.</p><p><em>A data server exposes HTTP to handle client requests. </em>Put simply, the data server acts as a proxy for all client interactions. Also it manages user authentication and authorization. The data server includes tooling to automatically apply updates in the federated architecture.</p><p>They run 6 million user repositories on a single data server at 150 USD per month.</p><p>Think of the user repository as a Git repo and the data server as GitHub. It‚Äôs easy to move a Git repo from GitHub to GitLab. Similarly, a user repository is movable from one data server to another.</p><p>Besides it‚Äôs possible to use <a href=\"https://docs.bsky.app/showcase?tags=client\">alternative clients</a> for Bluesky. Yet it‚Äôs necessary to maintain a standard data schema for interactions. So separate  and API endpoints are defined for each app on ATProto, including Bluesky.</p><p>A user's repository doesn‚Äôt store information about actions performed by their followers such as comments or likes on their post. Instead, it‚Äôs stored only in the repository of the follower who took the action.</p><p>A post is shown to the user‚Äôs followers.</p><p>Yet it‚Äôs expensive to push updates to each follower‚Äôs repository. So information is collected from every data server using the .</p><p>The crawler doesn‚Äôt index data but forwards it.</p><ul><li><p>The crawler subscribes for updates on the data server: new posts, likes, or comments.</p></li><li><p>The data server notifies the crawler about updates in real time over <a href=\"https://en.wikipedia.org/wiki/WebSocket\">websockets</a>.</p></li><li><p>The crawler collects information from data servers and generates a .</p></li></ul><p>Consider the generated stream as a log over websockets; put simply, the crawler combines each user‚Äôs actions into a single TCP connection.</p><p>A user's post is shown to followers only after counting the likes, comments, and reposts on it.</p><p>Yet the stream doesn‚Äôt contain this information. So the stream‚Äôs data is aggregated using the ; it transforms raw data into a consumable form by processing it. Imagine the index server as a data presentation layer.</p><p>The index server is built using the <a href=\"https://go.dev/\">Go</a> language for concurrency. A NoSQL database, <a href=\"https://www.scylladb.com/\">ScyllaDB</a>, is used as its data layer for horizontal scalability.</p><p>A reference to the user's post ID is added to the follower‚Äôs repository when they like or repost a post. So the total number of likes and reposts is calculated by crawling every user repository and adding the numbers.</p><p>Here‚Äôs the workflow for displaying a post:</p><ul><li><p>A user‚Äôs request is routed via their data server to the index server.</p></li><li><p>The data server finds the people a user follows by looking at their repository.</p></li><li><p>The index server creates a list of post IDs in reverse chronological order.</p></li><li><p>The index server expands the list of post IDs to full posts with content.</p></li></ul><p>The index server then responds to the client.</p><p>In short, a user repository stores primary data, while the index server stores derived data from repositories.</p><p>The index server is the most read-heavy service; so, its results are cached using <a href=\"https://en.wikipedia.org/wiki/Redis\">Redis</a>, an in-memory storage, for performance.</p><p>JSON Web Token () is used for authentication between Bluesky services.</p><p>Media files, such as images and videos, are stored on the data server‚Äôs disk for simplicity. A cryptographic ID () is used to reference the media files in the repository. The index server fetches the media files from the data server on user request and caches them on the content delivery network () for efficiency.</p><p>A user updates only their repository when they follow someone. </p><p>Their repository adds a reference to the user‚Äôs unique decentralized identifier () to indicate follow.</p><p>The number of followers for a user is found by indexing every repository. This is similar to how Google finds inbound links to a web page; all documents on the web are crawled.</p><p>A user account includes a handle based on the domain name (); it keeps things simple.</p><p>And each user automatically gets a handle from the ‚Äòbsky.social‚Äô subdomain upon account creation. Yet posts are stored using DID, and the user handle is displayed along with the posts. So changes to a user's handle don‚Äôt affect their previous posts.</p><p>A user‚Äôs DID is immutable, but the handle is mutable; put simply, a user‚Äôs handle is reassignable to a custom domain name.</p><p>Here‚Äôs how a user handle with a custom domain name is verified on Bluesky:</p><ul><li><p>The user enters their custom domain name in the Bluesky account settings.</p></li><li><p>Bluesky generates a unique text value for the user: public key.</p></li><li><p>The user stores this value in the <a href=\"https://www.cloudflare.com/en-gb/learning/dns/dns-records/dns-txt-record/\">DNS TXT record</a> of the custom domain name.</p></li></ul><p>The index server then periodically checks the DNS TXT record and validates the user handle. Imagine  as a text data field for domain name settings.</p><h3>3. User Timeline and Feed</h3><p>A user‚Äôs timeline is created by arranging their posts in reverse chronological order.</p><p>Yet a user‚Äôs repository doesn‚Äôt contain information about likes and comments received on a post. So the request is sent to the index server; it returns the user‚Äôs timeline with aggregated data.</p><p>A feed is created from posts by people a user follows.</p><p>Bluesky supports <a href=\"https://bsky.app/feeds\">feeds</a> with custom logic, and there are 50K+ custom feeds available. </p><p>Consider the  as a filter for a specific list of keywords or users.</p><ul><li><p>The crawler generates a stream from data servers.</p></li><li><p>The feed generator filters, sorts, and ranks the content based on custom logic.</p></li><li><p>The feed generator creates a list of post IDs.</p></li></ul><p>The index server then populates the feed‚Äôs content on a user request.</p><p> is used to fetch the feed; it offers better performance. </p><p>It includes an extra parameter in API requests and responses. The cursor parameter points to a specific item in the dataset; for example, the post‚Äôs timestamp to fetch feed until a specific post.</p><ul><li><p>A sequential unique column, such as the post's timestamp, is chosen for pagination.</p></li><li><p>The user requests include the cursor parameter to indicate the result‚Äôs offset.</p></li><li><p>The index server uses the cursor parameter to paginate the result dataset.</p></li><li><p>The index server responds with the cursor parameter for future requests.</p></li></ul><p>The client decides the cursor parameter‚Äôs window size based on its viewport. The cursor-based pagination scales well as it prevents a full table scan in the index server.</p><p>Imagine a  as metadata to categorize content. Besides it‚Äôs possible to apply a label manually. A user chooses to hide or show a warning for posts with a specific label.</p><p>This preference is stored on their data server.</p><p>Here‚Äôs how Bluesky moderation works:</p><ul><li><p>The moderation service consumes the stream from the crawler.</p></li><li><p>The moderation service analyzes the content and assigns a label to it.</p></li><li><p>The index server stores the content along with its label.</p></li><li><p>The user requests are routed via the data server to the index server.</p></li><li><p>The data server includes label IDs in HTTP request headers before forwarding them.</p></li><li><p>The index server applies the label setting in the response.</p></li></ul><p>Also a data server operator does basic moderation. </p><p>The data server filters out the muted users before responding to the client.</p><p>Put simply, a user can post about anything, but the content is moderated before being shown to the public.</p><p>Bluesky allows a user to see how many people they follow also follow a specific user: <a href=\"https://www.ernberck.com/social-proof-explained/\">social proof</a>.</p><p>For example, when I visit <a href=\"https://bsky.app/profile/jay.bsky.team\">Jay‚Äôs</a> Bluesky profile, it shows how many people I follow also follow her.</p><p><em>Let‚Äôs dive into the different approaches they used to build this feature.</em></p><ul><li><p>Find the people I follow by querying the database.</p></li><li><p>Do separate parallel queries for each user to find the people they follow.</p></li><li><p>Check if any of them also follow Jay.</p></li></ul><p>But this approach won‚Äôt scale as the number of parallel queries increases with the number of people a user follows.</p><p>A scalable approach is to convert it into a set intersection problem:</p><ul><li><p>Set 1 tracks the people I follow.</p></li><li><p>Set 2 tracks the people who follow Jay.</p></li></ul><p>The intersection of these sets gives the expected result.</p><p>An in-memory graph service prevents expensive database queries and performs intersections quickly. Yet <a href=\"https://redis.io/docs/latest/develop/data-types/sets/\">Redis Sets</a> don‚Äôt use different CPU cores at once. So here‚Äôs how they implemented a minimum viable product:</p><ul><li><p>Each user has a 32-character DID.</p></li><li><p>The DID values are <a href=\"https://en.wikipedia.org/wiki/String_interning\">converted</a> into uint64 to reduce memory usage.</p></li><li><p>Each user‚Äôs DID maintains 2 sets: people they follow and people who follow them.</p></li></ul><p>But it still consumes a lot of memory and takes extra time to start.</p><p>So they optimized the graph service by implementing it using <a href=\"https://vikramoberoi.com/posts/a-primer-on-roaring-bitmaps-what-they-are-and-how-they-work/\">Roaring Bitmaps</a>.</p><p><em>Yet let‚Äôs take a step back and learn Bitmaps to better understand Roaring Bitmaps.</em></p><p>A  represents binary states using bit arrays.</p><p>Imagine Bluesky has only 100 users.</p><p>The index value of the people I follow is set to 1 on the Bitmap; the people I follow are then found by walking the Bitmap and recording the index of non-zero bits. A constant time lookup tells whether I follow a specific user. Although Bitmaps do faster bitwise operations, it‚Äôs inefficient for sparse data. </p><p>A 100-bit long Bitmap is needed even if I follow only a single user; so, it won‚Äôt scale for Bluesky‚Äôs needs.</p><p>It turns consecutive 0s and 1s into a number for reduced storage. For example, if I follow the last 10 users, only the last 10 indices are marked by 1. With Run-Length Encoding, it‚Äôs stored as 90 0s and 10 1s, thus low storage costs.</p><p>But this approach won‚Äôt scale for randomly populated data, and a lookup walks the entire Bitset to find an index.</p><p>Ready for the best technique? .</p><p>Think of it as compressed Bitmaps, but 100 times faster. <em>A Roaring Bitmap splits data into containers and each container uses a different storage mechanism.</em></p><ul><li><p>The dense data is stored in a container using Bitmap; it uses a fixed-size bit array.</p></li><li><p>The data with a large contiguous range of integers are stored in a container using run-length encoding; which reduces storage costs.</p></li><li><p>The sparse data is stored in a container as integers in a sorted array; it reduces storage needs.</p></li></ul><p>Put simply, Roaring Bitmaps use different containers based on data sparsity.</p><p>The set intersection is done in parallel and a container is converted into a different format for the set intersection. Also the graph data is stored and transferred over the network in Roaring Bitmap‚Äôs <a href=\"https://github.com/dgraph-io/sroar\">serialization format</a>.</p><p>Many instances of graph service are run for high availability and <a href=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/\">rolling updates</a>.</p><p>Bluesky supports short videos up to 90 seconds long.</p><p>The video is streamed through HTTP Live Streaming (). Think of HLS as a group of text files; it‚Äôs a standard for adaptive bitrate video streaming.</p><p>A client dynamically changes video quality based on network conditions.</p><ul><li><p>A video is encoded into different quality levels: 480p, 720p, 1080p, and so on.</p></li><li><p>Each encoded video is split into small segments of 5 seconds long.</p></li></ul><p>The client checks the network conditions and requests video segments of the right quality.</p><p>HLS uses Playlist files to manage encoded video segments. </p><p>Imagine the  as a text file, and there are 2 types of Playlists:</p><ul><li><p>Master playlist: list of all video quality levels available.</p></li><li><p>Media playlist: list of all video segments for a specific video quality.</p></li></ul><p>First, the client downloads the Master playlist to find available quality levels. Second, it selects a Media playlist based on network conditions. Third, it downloads the video segments in the sequence defined on the Media playlist.</p><p>The videos and playlists are cached in CDN to reduce costs and handle bandwidth needs. The stream requests are routed to CDN via <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/302\">302 HTTP redirects</a>.</p><p>The video views are tracked by <a href=\"https://systemdesign.one/distributed-counter-system-design/\">counting</a> requests to its Master playlist file.</p><p>And the response for the Master playlist includes a Session ID; it‚Äôs included in future requests for Media playlists to track the user session.</p><p>Besides the video seconds watched by a user are found by checking the last fetched video segment.</p><p>A video subtitle is stored in text format and has a separate Media playlist file.</p><p>The Master playlist includes a reference to subtitles; here‚Äôs how it works:</p><ul><li><p>The client finds the available subtitles by querying the Master playlist.</p></li><li><p>The client downloads a specific subtitle's Media playlist based on the language selected by the user.</p></li></ul><p>The client then downloads the subtitle segments defined in the Media Playlist.</p><p>Subscribe to get simplified system design case studies delivered to your inbox:</p><h3>Bluesky Quality Attributes</h3><p>A user‚Äôs data is cryptographically signed before saving it in the data server; it shows data authorship.</p><p>The <a href=\"https://www.baeldung.com/cs/merkle-trees\">Merkle search tree</a> is used to check the integrity of data, such as posts, transferred over the network.</p><p>Think of the  as an efficient data structure to check data integrity. It doesn‚Äôt store original data; instead, it has only cryptographic hashes representing data.</p><p>Besides the crawler keeps a copy of each user‚Äôs repository; it verifies the Merkle search tree signatures on data updates.</p><p>The crawler generates a stream to update the index server.</p><p>Yet the stream increases bandwidth and infrastructure costs. So the events are validated on the crawler and services, such as the feed generator, are connected to a trusted crawler. Besides Merkle search tree blocks are <a href=\"https://github.com/bluesky-social/jetstream\">converted</a> to JSON objects, which lack verifiability and signatures, to reduce stream size.</p><p>Put simply, Merkle search tree verification overhead is avoided by consuming the stream from a trustable crawler.</p><p>The crawler transfers the stream to the index server over websockets; <a href=\"https://facebook.github.io/zstd/\">zstd</a> compresses websocket traffic for performance.</p><p>A user, with a massive following, needs a data server with only tiny computing resources as the index server displays aggregated data.</p><p>Every change to a user's profile is propagated via the index server; which, means <a href=\"https://systemdesign.one/consistency-patterns/#eventual-consistency\">eventual consistency</a>.</p><p>Yet a user viewing their profile right after an update should see the latest changes. So Bluesky offers <em>read-after-write consistency for profile updates</em>; here‚Äôs how:</p><ul><li><p>All client requests are routed via the data server to the index server.</p></li><li><p>The response headers from the index server are compared against the data server. </p></li><li><p>The response is updated to include missing records before returning it to the client.</p></li></ul><p>This read-after-write behavior applies only to a user viewing their profile; others see the changes eventually.</p><p>A popular user's post notifies many users, which results in many requests to view the post at once.</p><p>The results from the index server are cached to reduce the database load. Yet a popular user's post creates a <a href=\"https://en.wikipedia.org/wiki/Thundering_herd_problem\">thundering herd problem</a>.</p><ul><li><p>A view request, the first one, from a follower reaches the cache server.</p></li><li><p>The cache server queries the database for post content.</p></li><li><p>Requests from others reach the database while the cache waits for the first response.</p></li></ul><p>It groups requests for the same data within a short period. Then forwards only a single request to the database; thus, preventing the thundering herd problem.</p><p>The data server notifies the crawler about data changes in repositories over websockets.</p><p>Yet network interruptions occur, so the crawler periodically checks repositories. It then compares the data against its local replica to find data changes.</p><p>Also requests are rate-limited to keep the system operational.</p><p><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\">HTTP 429 (Too Many Requests)</a> is returned in response headers when the rate limit threshold is exceeded. Interactions between servers are also rate-limited to avoid abuse in the decentralized architecture. A new data server‚Äôs request limit capacity is increased over time, based on trust, to keep the crawler running smoothly.</p><p>Besides a user is only allowed up to 1,666 requests an hour to reduce bot activity.</p><p>Bluesky has more than <a href=\"https://blueskyusercount.com/\">28 million users</a>; it became an independent company in 2021.</p><p>Although anybody can host a server on ATProto, most Bluesky services are now run by a single company.</p><p>ATProto and Bluesky look like an interesting new approach to social networks.</p><p>üëã  -Do you want to level up at work and better understand technical discussions?</p><p>My mission is to help you go from 0 to 1 in system design by spending less than 37 minutes a month.</p><p>This is the first deep dive article in this newsletter; it's a free preview.</p><ul><li><p>Yet future deep dives, starting this February, are available only to paid subscribers.</p></li><li><p>And subscription fees will be way higher than the current fees.</p></li></ul><p>So<strong> pledge now to get access at a very low price.</strong></p><p><em>\"This newsletter gave me ideas on how to build scalable systems.\"</em> Aditya, Google.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p>Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y‚Äôall are the best.</p>","contentLength":19389,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/449c6406-d812-4547-bc2a-8e55c325ce3c_1280x720.gif","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust 583","url":"https://this-week-in-rust.org/blog/2025/01/22/this-week-in-rust-583/","date":1737522000,"author":"TWiR Contributors","guid":398,"unread":true,"content":"<p>This week's crate is <a href=\"https://crates.io/crates/resvg\">resvg</a>, an SVG rendering library.</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.  The following\nRFCs would benefit from user testing before moving forward:</p><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p>A very quiet week for performance, with small improvements essentially on all benchmarks.</p><p>0 Regression, 1 Improvement, 2 Mixed; 0 of them in rollups\n40 artifact comparisons made in total</p><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><ul><li><em>No Cargo Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Team Proposals entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Reference RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><p>Rusty Events between 2025-01-22 - 2025-02-19 ü¶Ä</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>Memory safety issues mean you can‚Äôt trust what you‚Äôre seeing in your source code anymore.</p></blockquote>","contentLength":2032,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust 2024 in beta channel","url":"https://blog.rust-lang.org/2025/01/22/rust-2024-beta.html","date":1737504000,"author":"TC & Eric Huss","guid":237,"unread":true,"content":"<p>The next edition, Rust 2024, has entered the beta channel.  It will live there until 2025-02-20, when Rust 1.85 and Rust 2024 will be released as stable.</p><p>We're really happy with how Rust 2024 has turned out, and we're looking forward to putting it in your hands.</p><p>You can get a head start in preparing your code for the new edition, and simultaneously help us with final testing of Rust 2024, by following these steps within a project:</p><ol><li>Run <code>cargo +beta fix --edition</code>.</li><li>Set  and, if needed, , in .</li><li>Run , address any remaining warnings, and then run other tests.</li></ol><p>More details on how to migrate can be found <a href=\"https://doc.rust-lang.org/nightly/edition-guide/editions/transitioning-an-existing-project-to-a-new-edition.html\">here</a> and within each of the <a href=\"https://doc.rust-lang.org/nightly/edition-guide/rust-2024/\">chapters</a> describing the changes in Rust 2024.  For more on the changes themselves, see the <a href=\"https://doc.rust-lang.org/nightly/edition-guide/\">Edition Guide</a>.</p><p>If you encounter any problems or see areas where we could make the experience better, tell us about it by <a href=\"https://github.com/rust-lang/rust/issues/new/choose\">filing an issue</a>.</p>","contentLength":851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.24 Release Candidate 2","url":"https://golangweekly.com/issues/539","date":1737504000,"author":"","guid":594,"unread":true,"content":"<li><p><a href=\"https://golangweekly.com/link/164583/rss\">VHS 0.9</a> ‚Äì CLI 'home video recorder'. <em>\"Write terminal GIFs as code for integration testing and demoing your CLI&nbsp;tools.\"</em> v0.9 adds a  keyword for waiting for certain conditions before continuing&nbsp;.</p></li><li><p>ü§ñ <a href=\"https://golangweekly.com/link/164584/rss\">Mods 1.7</a> ‚Äì Charm's pipeline-able AI CLI client. Now with improved GitHub Copilot support and OpenAI o1&nbsp;support.</p></li><li><p>‚òéÔ∏é <a href=\"https://golangweekly.com/link/164585/rss\">phonenumbers 1.5</a> ‚Äì Go port of Google's libphonenumber library for parsing and validating phone&nbsp;numbers.</p></li><li><p><a href=\"https://golangweekly.com/link/164586/rss\">GoReleaser 2.6</a> ‚Äì Build/release binaries for multiple&nbsp;platforms. v2.6 gains Bun and Deno support.</p></li><li><p><a href=\"https://golangweekly.com/link/164588/rss\">Notify 1.3</a> ‚Äì Library to send notifications to various&nbsp;services.</p></li><li><p><a href=\"https://golangweekly.com/link/164591/rss\">sqlc 1.28</a> ‚Äì Generate type-safe Go code from&nbsp;SQL.</p></li>","contentLength":646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pre-RFC - Rename annotations","url":"https://poignardazur.github.io//2025/01/22/rename-annotations/","date":1737504000,"author":"Olivier Faure","guid":743,"unread":true,"content":"<p>Let‚Äôs imagine you‚Äôre writing a crate.</p><p>Your crate has a single  file, with two modules and an arbitrary number of items:</p><div><div><pre><code></code></pre></div></div><p>After a while, you decide that Foobar should really be exported from the  module. It‚Äôs a breaking change, but you‚Äôre fine with releasing a new major version:</p><div><div><pre><code></code></pre></div></div><p>Any user previously importing from your crate will get this error when they bump the version number:</p><div><div><pre><code>error[E0432]: unresolved import `best_crate::bar::Foobar`\n  --&gt; src/lib.rs:12:5\n   |\n12 | use best_crate::bar::Foobar;\n   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`\n   |\nhelp: consider importing this struct instead\n   |\n12 | use best_crate::foo::Foobar;\n   |     ~~~~~~~~~~~~~~~~~~~~~~~~\n</code></pre></div></div><p>This isn‚Äôt ideal, but at least there‚Äôs the ‚Äúconsider importing this instead‚Äù message giving these users an easy way to fix this.</p><p>But now let‚Äôs say you decide that ‚ÄúFoobar‚Äù is a terrible name, and your struct should really be named ‚ÄúFoofoo‚Äù instead for consistency:</p><div><div><pre><code></code></pre></div></div><p>Now your users will get this message:</p><div><div><pre><code>error[E0432]: unresolved import `best_crate::bar::Foobar`\n  --&gt; src/lib.rs:12:5\n   |\n12 | use best_crate::bar::Foobar;\n   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`\n</code></pre></div></div><p>No message to help them figure out what to use instead.</p><p>Rust should have an attribute to inform the compiler that an item previously existed, but has been moved and/or renamed:</p><div><div><pre><code></code></pre></div></div><p>You might want to put the attribute on the ‚Äúnew‚Äù item instead, in which case the syntax would be:</p><div><div><pre><code></code></pre></div></div><p>You‚Äôd probably want both, for cases where either the module of origin is removed, or the destination is no longer in the same crate (e.g. because you‚Äôve split your crate into sub-crates).</p><p>This diagnostic could help the compiler give more helpful error messages:</p><div><div><pre><code>error[E0432]: unresolved import `best_crate::bar::Foobar`\n  --&gt; src/lib.rs:12:5\n   |\n12 | use best_crate::bar::Foobar;\n   |     ^^^^^^^^^^^^^^^^^^^^^^^ no `Foobar` in `best_crate::bar`\n   |\nhelp: this item has been renamed to `best_crate::foo::Foofoo`\n   |\n12 | use best_crate::foo::Foofoo;\n   |     ~~~~~~~~~~~~~~~~~~~~~~~~\n</code></pre></div></div><p>Because the compiler is  that this is the correct move and not just guessing based on name similarity,  and similar tools would be able to automatically apply the rename.</p><p>Rename annotations would be helpful as a ‚Äúgrace period‚Äù after a crate‚Äôs major version change, but they would also be useful for purely internal refactors, using  to change  directives throughout your codebase.</p><p>All in all, this feels like a pretty useful feature which, thanks to the  namespace‚Äôs relaxed constraints, could be implemented relatively swiftly in the Rust toolchain.</p>","contentLength":2615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strobelight: A profiling service built on open source technology","url":"https://engineering.fb.com/2025/01/21/production-engineering/strobelight-a-profiling-service-built-on-open-source-technology/","date":1737478854,"author":"","guid":465,"unread":true,"content":"<ul></ul><h2></h2><p><a href=\"https://docs.ebpf.io/\" target=\"_blank\" rel=\"noopener\"></a></p><ul></ul><p><a href=\"https://research.facebook.com/publications/holistic-configuration-management-at-facebook/\" target=\"_blank\" rel=\"noopener\"></a></p><pre><code>add_continuous_override_for_offcpu_data(\n    \"my_awesome_team\", // the team that owns this service\n    Type.SERVICE_ID,\n    \"my_awesome_service\",\n    30_000, // desired samples per hour\n)\n</code></pre><p><a href=\"https://github.com/bpftrace/bpftrace\" target=\"_blank\" rel=\"noopener\"></a></p><h2></h2><h2></h2><h3></h3><p><a href=\"https://lwn.net/Articles/680985/\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://ieeexplore.ieee.org/document/10444807\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://research.facebook.com/publications/bolt-a-practical-binary-optimizer-for-data-centers-and-beyond/\"></a></p><h2></h2><p><a href=\"https://www.brendangregg.com/flamegraphs.html\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://learn.microsoft.com/en-us/windows-hardware/test/wpt/stack-tags\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://github.com/YtnbFirewings/gsym\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://github.com/libbpf/blazesym\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html\" target=\"_blank\" rel=\"noopener\"></a></p><h2></h2><p><a href=\"https://research.facebook.com/publications/scuba-diving-into-data-at-facebook/\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://perfetto.dev/\" target=\"_blank\" rel=\"noopener\"></a></p><p><a href=\"https://www.facebook.com/atscaleevents/videos/996197807391867/\"></a></p><p><a href=\"https://github.com/facebookincubator/strobelight\" target=\"_blank\" rel=\"noopener\"></a></p><p><i></i></p>","contentLength":188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI‚Äôs New ChatGPT Learned from 100,000 Conversations!","url":"https://www.youtube.com/watch?v=9Q0qPTKVTJE","date":1737476150,"author":"Two Minute Papers","guid":516,"unread":true,"content":"<article>‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nüìù The paper is available here:\nhttps://openai.com/index/openai-o1-system-card/\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Balfanz, Alex Haro, B Shang, Benji Rabhan, Gaston Ingaramo, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Martin, Michael Albrecht, Michael Tedder, Owen Skarpness, Richard Sundvall, Taras Bobrovytsky,, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu</article>","contentLength":1048,"flags":null,"enclosureUrl":"https://www.youtube.com/v/9Q0qPTKVTJE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Exploring Python's tuple Data Type With Examples","url":"https://realpython.com/courses/exploring-tuple-data-type-examples/","date":1737468000,"author":"Real Python","guid":164,"unread":true,"content":"<p>In Python, a  is a built-in data type that allows you to create  of values. The values or items in a tuple can be of any type. This makes tuples pretty useful in those situations where you need to store heterogeneous data, like that in a database record, for example.</p><p>Through this tutorial, you‚Äôll dive deep into Python tuples and get a solid understanding of their key features and use cases. This knowledge will allow you to write more efficient and reliable code by taking advantage of tuples.</p><p><strong>In this video course, you‚Äôll learn how to:</strong></p><ul><li> the items in an existing tuple</li><li>, , , and  tuples</li><li>, , and  existing tuples</li><li>Explore other  and common  of tuples</li></ul>","contentLength":649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quiz: How to Deal With Missing Data in Polars","url":"https://realpython.com/quizzes/how-to-deal-with-missing-data-in-polars/","date":1737460800,"author":"Real Python","guid":163,"unread":true,"content":"<p>By working through the questions, you‚Äôll review your understanding of dealing with  values in Polars and also expand on what you learned in the tutorial.</p><p>You‚Äôll need to do some research outside of the tutorial to answer all the questions. Embrace this challenge and let it take you on a learning journey.</p>","contentLength":307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"#417 Bugs hide from the light","url":"https://pythonbytes.fm/episodes/show/417/bugs-hide-from-the-light","date":1737446400,"author":"","guid":443,"unread":true,"content":"<article></article>","contentLength":0,"flags":null,"enclosureUrl":"https://pythonbytes.fm/episodes/download/417/bugs-hide-from-the-light.mp3","enclosureMime":"","commentsUrl":null},{"title":"What's an API?","url":"https://blog.algomaster.io/p/whats-an-api","date":1737437464,"author":"Ashish Pratap Singh","guid":744,"unread":true,"content":"<p>API stands for <strong>Application Programming Interface</strong>.</p><p>At its core, an API is a  that takes an  and gives you predictable </p><p>Think of an API as a  that enables applications to interact <strong>without needing direct access to each other's code or database</strong>.</p><p>Almost every digital service you use today‚Äîsocial media, e-commerce, online banking, ride-hailing apps‚Äîall of them are a bunch of APIs working together.</p><ul><li><p> ‚Äì If you provide a city name as input (), the API returns the <strong>current temperature, humidity, and weather conditions</strong>.</p></li><li><p> ‚Äì If you provide a <strong>pickup and destination address</strong>, the API finds the  and calculates the estimated fare.</p></li><li><p> ‚Äì If you provide a list of numbers (), the API returns the  ().</p></li></ul><p>When engineers build APIs, they clearly define <strong>what inputs the API accepts</strong> and , ensuring consistent behavior across different applications.</p><p>APIs follow a simple  model:</p><ul><li><p>A client (such as a web app or mobile app) makes a request to an API.</p></li><li><p>The API (hosted on an API server) processes the request, interacts with the necessary databases or services, and prepares a response.</p></li><li><p>The API sends the response back to the client in a structured format (usually JSON or XML).</p></li></ul><p>Every API requires , and passing incorrect data can result in errors.</p><p>For example: If you tried putting your name into the Google Maps API as an input, that wouldn‚Äôt work very well.</p><p>Some APIs also <strong>require inputs in a specific format</strong>.</p><p>Example: The  might need the input as  instead of .</p><p>APIs often  to ensure they are correct before processing them, which helps maintain .</p><p>Just as APIs require , they also return .</p><p>For example, the  always returns <strong>coordinates in the same format</strong>.</p><pre><code>{   \"latitude\": 40.6892,   \"longitude\": -74.0445 }</code></pre><p>If the API can‚Äôt find the location, it provides an error response explaining why.</p><pre><code>{   \"error\": \"Invalid address format\",   \"code\": 400 }</code></pre><p>If you‚Äôre finding this newsletter valuable and want to deepen your learning, consider becoming a .</p><p>As a paid subscriber, you'll receive an <strong>exclusive deep-dive article</strong> every week, access to a structured100+topics and interview questions, and other .</p><p>The apps you use every day‚Äîwhether it's <strong>Gmail, Instagram, Uber, or Spotify</strong>‚Äîare essentially <strong>a collection of APIs with a polished user interface (UI) on top</strong>.</p><p>Most applications follow the <strong>frontend/backend architecture</strong>, where:</p><ul><li><p>The  consists of APIs that handle <strong>data processing, business logic, and communication with databases</strong>.</p></li><li><p>The  is a <strong>graphical user interface (GUI)</strong> that interacts with these APIs, making applications user-friendly and accessible <strong>without requiring users to write code</strong>.</p></li></ul><p>Let‚Äôs break this down with a real-world example: .</p><p>Before the Uber app existed as a sleek, user-friendly experience, the company first built <strong>the core APIs that power ride-hailing services:</strong></p><ul><li><p>Calculating Fares &amp; Routes</p></li><li><p>Matching Riders &amp; Drivers</p></li></ul><p>These APIs run on Uber‚Äôs servers, forming the . Every time you request a ride, track your driver, or make a payment, these backend APIs handle the request.</p><p> are responsible for optimizing these APIs, improving ride-matching algorithms, securing transactions, and ensuring a smooth experience for millions of users.</p><p>The backend APIs handle , but they ‚Äîwhich isn't practical for everyday users. That‚Äôs why companies build a <strong>frontend (user interface)</strong> on top of these APIs, allowing users to interact with the system .</p><p>When you enter your pickup &amp; destination address, the frontend sends an API request to  and displays available cars.</p><p>Once the trip is complete, the frontend may call the process payment API to display the receipt.</p><p>APIs come in different forms depending on , , and .</p><h3>1. Open APIs (Public APIs)</h3><p>Open APIs, also known as , are accessible to external developers with minimal restrictions.</p><p>Companies provide these APIs to encourage  to integrate their services and build new applications on top of them.</p><h4><strong>Example: YouTube Data API</strong></h4><p>Normally, when you use the , it makes  to fetch your video feed, search for content, or post comments. However, YouTube also provides a  that allows developers to access some of this functionality .</p><p>For example, the  allows developers to fetch video results based on a keyword. If you send a request to the API with <code>\"machine learning tutorial\"</code> as the search term, it will return a structured response (JSON format) containing a list of relevant videos, including <strong>titles, descriptions, thumbnails, and video links</strong>.</p><p>This is incredibly useful because it enables developers to build custom applications on top of YouTube.</p><h3>2. Internal APIs (Private APIs)</h3><p>, also known as , are designed xclusively for internal use within an organization. Unlike Open APIs, these are not accessible to external developers and are used to facilitate seamless communication between different internal systems within a company.</p><p>Let‚Äôs take  as an example. When you place an order, you might assume that a single system processes your request. In reality,  (order processing, inventory, payment, logistics etc..) work together behind the scenes to fulfill your order efficiently.</p><p>Each of these APIs , but they communicate through well-defined protocols to ensure a smooth and efficient process.</p><p>Internal APIs allow companies to break down their applications into <strong>smaller, manageable services</strong>, making it easier to scale. Developers can  across different projects, reducing  and speeding up development.</p><p>The first two types of APIs we discussed‚Äî<strong>Open APIs and Internal APIs</strong>‚Äîare functional and serve  like fetching weather data or booking a ride.</p><p>But there‚Äôs another category of APIs that developers use daily:  (also called  or ).</p><p>These APIs don‚Äôt connect different applications; instead, they provide predefined functions within a programming language or framework to make development easier.</p><p>Python‚Äôs built-in list API</p><p>When working with lists, Python provides a set of <strong>built-in functions (methods) to manipulate data</strong>.</p><pre><code>numbers = [5, 3, 8, 1, 4] numbers.sort()  # API call to sort the list  fruits = [\"apple\", \"banana\"] fruits.append(\"orange\")  # API call to add an element  fruits.pop()  # API call to remove the last element</code></pre><p>Instead of writing sorting algorithms from scratch, developers can use  or  in Python.</p><p>Code APIs are not just limited to built-in programming language functions. Take , an AI/ML library. It provides a  for training machine learning models without needing to implement complex mathematical operations from scratch.</p><p>For example, creating a  using TensorFlow's API is as simple as:</p><pre><code>import tensorflow as tf model = tf.keras.Sequential([tf.keras.layers.Dense(64, activation=\"relu\")])</code></pre><p>Programming APIs abstract away complexity so that developers can focus on building solutions rather than reinventing the wheel.</p><p>APIs communicate using different <strong>protocols and architectures</strong> that define how requests are sent, how responses are formatted, and how data is exchanged between systems.</p><h3>1. REST (Representational State Transfer)</h3><p>REST is the most widely used API communication method today. It is <strong>lightweight, stateless, and scalable</strong>, making it perfect for web services and mobile applications. </p><p>REST APIs follow a set of design principles and use  (GET, POST, PUT, DELETE) to perform operations.</p><p>REST APIs are based on , and each resource is accessed through a . The API follows the , meaning the client sends a request, and the server processes it and sends a response.</p><h4>Example: REST API for a Bookstore</h4><p><strong>Retrieve a list of books (GET Request):</strong></p><pre><code>GET https://api.bookstore.com/books</code></pre><pre><code>[   { \"id\": 1, \"title\": \"Clean Code\", \"author\": \"Robert C. Martin\" },   { \"id\": 2, \"title\": \"The Pragmatic Programmer\", \"author\": \"Andrew Hunt\" } ]</code></pre><h3>2. SOAP (Simple Object Access Protocol)</h3><p>SOAP is an older API communication method that <strong>relies on XML-based messaging</strong>. </p><p>Unlike REST, which is lightweight, SOAP is more structured and secure, making it ideal for banking, healthcare, and enterprise applications.</p><p>SOAP messages are sent using  and require a <strong>WSDL (Web Services Description Language) file</strong>, which defines the API's available functions and request structure.</p><h4><strong>Example: SOAP API for a Banking Service</strong></h4><p> Fetching account balance</p><pre><code>&lt;soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:bank=\"http://bank.example.com/\"&gt;    &lt;soapenv:Header/&gt;    &lt;soapenv:Body&gt;       &lt;bank:GetAccountBalance&gt;          &lt;bank:accountNumber&gt;123456&lt;/bank:accountNumber&gt;       &lt;/bank:GetAccountBalance&gt;    &lt;/soapenv:Body&gt; &lt;/soapenv:Envelope&gt; </code></pre><pre><code>&lt;soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"&gt;    &lt;soapenv:Body&gt;       &lt;bank:GetAccountBalanceResponse&gt;          &lt;bank:balance&gt;5000.00&lt;/bank:balance&gt;       &lt;/bank:GetAccountBalanceResponse&gt;    &lt;/soapenv:Body&gt; &lt;/soapenv:Envelope&gt;</code></pre><p>GraphQL is an alternative to REST that <strong>allows clients to request exactly the data they need</strong>, making it more efficient for modern applications. Unlike REST, which requires multiple API calls to fetch related data, GraphQL can <strong>fetch all necessary data in a single request</strong>.</p><p>Instead of predefined endpoints, GraphQL exposes a , and the client sends queries to request specific fields.</p><h4>Example: Fetching a user's profile and their recent posts in a single request.</h4><pre><code>{   user(id: 123) {     name     email     posts {       title       likes     }   } }</code></pre><pre><code>{   \"data\": {     \"user\": {       \"name\": \"Alice\",       \"email\": \"alice@example.com\",       \"posts\": [         { \"title\": \"Hello World\", \"likes\": 100 },         { \"title\": \"GraphQL is Amazing!\", \"likes\": 200 }       ]     }   } }</code></pre><p>gRPC (Google Remote Procedure Call) is a <strong>high-performance API communication method</strong> that uses <strong>Protocol Buffers (Protobuf)</strong> instead of JSON or XML, making it faster and more efficient.</p><p>gRPC uses  instead of text-based formats, reducing payload size and it supports , meaning the client and server can send data at the same time.</p><p>Using an API might seem complex at first, but it follows a simple  pattern.</p><p>Here‚Äôs a guide on<strong> how to find, access, and interact with an API</strong> step by step:</p><h3><strong>Step 1: Find an API to Use</strong></h3><p>Before using an API, you need to  for your needs. APIs are available for different services like weather data, finance, social media, etc.</p><p><strong>Official API Documentation:</strong></p><h3><strong>Step 2: Read the API Documentation</strong></h3><p>API documentation explains <strong>how to use the API, available endpoints, authentication, and response formats</strong>.</p><p>The OpenWeatherMap API allows users to fetch real-time weather data. Here's a breakdown of its key components:</p><pre><code>https://api.openweathermap.org/data/3.0/weather?q=city_name&amp;appid=YOUR_API_KEY</code></pre><ul><li><p>: City name (e.g., )</p></li><li><p>: API Key (required for access)</p></li></ul><h3><strong>Step 3: Get API Access (API Key / Authentication)</strong></h3><p>Most APIs  to prevent unauthorized access and manage usage limits.</p><h4><strong>Common Authentication Methods:</strong></h4><ul><li><p>A unique key provided by the API service</p></li><li><p>Secure login via Google, Github, etc.</p></li><li><p>Token-based authentication</p></li><li><p>Username + password (Base64 encoded)</p></li></ul><p><strong>Example: Getting an API Key (OpenWeather API)</strong></p><ul><li><p>Sign up at https://home.openweathermap.org/users/sign_up.</p></li><li><p>Go to the  section and generate a key.</p></li><li><p>Use the API key in requests:</p></li></ul><pre><code>GET https://api.openweathermap.org/data/2.5/weather?q=London&amp;appid=YOUR_API_KEY</code></pre><h3><strong>Step 4: Test the API Using Postman or cURL</strong></h3><p>Before writing code,  to see how it responds.</p><h4><strong>Option 1: Using Postman (Recommended for Beginners)</strong></h4><ul><li><p>Click , enter the API endpoint URL (<code>https://api.openweathermap.org/data/3.0/weather?q=London&amp;appid=YOUR_API_KEY</code>).</p></li><li><p>Select  as the HTTP method.</p></li><li><p>Click  and view the response in .</p></li></ul><h4><strong>Option 2: Using cURL (For Command Line Users)</strong></h4><p>You can also test APIs directly from the  using .</p><pre><code>curl -X GET \"https://api.openweathermap.org/data/3.0/weather?q=New+York&amp;appid=YOUR_API_KEY\"</code></pre><h3><strong>Step 5: Write Code to Call the API</strong></h3><p>Now that you‚Äôve tested the API, it‚Äôs time to <strong>integrate it into your application</strong>.</p><h4><strong>Example: Calling an API in Python</strong></h4><pre><code>import requests  url = \"https://api.openweathermap.org/data/3.0/weather?q=New York&amp;appid=YOUR_API_KEY\" response = requests.get(url)  if response.status_code == 200:     data = response.json()     print(f\"Temperature: {data['main']['temp']}¬∞C\") else:     print(\"Error:\", response.status_code)</code></pre><ul><li><p> ‚Äì Sends an API request.</p></li><li><p> ‚Äì Converts response to JSON.</p></li><li><p><code>if response.status_code == 200</code> ‚Äì Checks if the request was successful.</p></li></ul><h3><strong>Step 6: Handle Errors &amp; Rate Limits</strong></h3><p>APIs <strong>don‚Äôt always return perfect responses</strong>. You should handle:</p><ul><li><p> (e.g., wrong city name).</p></li><li><p> (e.g., expired API keys).</p></li><li><p> (e.g., exceeding request limits).</p></li></ul><h4><strong>Example: Handling API Errors in Python</strong></h4><pre><code>if response.status_code == 200:     data = response.json()     print(f\"Weather: {data['weather'][0]['description']}\") elif response.status_code == 401:     print(\"Error: Invalid API key\") elif response.status_code == 404:     print(\"Error: City not found\") else:     print(f\"Unexpected error: {response.status_code}\")</code></pre><h3>Step 7: Use API Responses in Your Application</h3><p>Once you fetch data from an API, you can <strong>display it dynamically in a web or mobile app</strong>.</p><p> You can build a weather dashboard using the OpenWeatherMap API.</p><ul><li><p>Fetch live weather data from the API.</p></li><li><p>Parse and extract relevant details (temperature, humidity, condition).</p></li><li><p>Display the weather report in a user-friendly format.</p></li></ul><p>If you found it valuable, hit a like ‚ù§Ô∏è and consider subscribing for more such content every week.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/scalability?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQwMDc4MDk0LCJpYXQiOjE3Mzc1MzgxODMsImV4cCI6MTc0MDEzMDE4MywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.xroFXQDDEPvo2FWnnt-G2Ji9MzYIDtJ68NRQX6sT8x8&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you‚Äôre finding this newsletter helpful and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":13212,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/ae14a376-92a3-4ff6-a329-8e4f2a7ac9b5_1546x1074.png","enclosureMime":"","commentsUrl":null},{"title":"ECMAScript proposal: RegExp escaping","url":"https://2ality.com/2025/01/regexp-escape.html","date":1737417600,"author":"Dr. Axel Rauschmayer","guid":344,"unread":true,"content":"<p>The ECMAScript proposal <a href=\"https://github.com/tc39/proposal-regex-escaping\">‚ÄúRegExp escaping‚Äù</a> (by Jordan Harband and Kevin Gibbons) specifies a function  that, given a string , creates an escaped version that matches  ‚Äì if interpreted as a regular expression.</p><p>This proposal is currently at stage 3.</p>","contentLength":251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FastAPI vs Go (Golang) vs Node.js: Performance & Price","url":"https://www.youtube.com/watch?v=shAELuHaTio","date":1737313381,"author":"Anton Putra","guid":531,"unread":true,"content":"<article>Python vs Go (Golang) vs NodeJS Performance Comparison\n\nüî¥ To support my channel, I'd like to offer Mentorship/On-the-Job Support/Consulting (me@antonputra.com)\n\nüçø Benchmarks: https://youtube.com/playlist?list=PLiMWaCMwGJXmcDLvMQeORJ-j_jayKaLVn&amp;si=p-UOaVM_6_SFx52H\n\nüëã AWS is expensive - Infra Support Fund: https://buymeacoffee.com/antonputra\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Experience &amp; Location üíº ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  I‚Äôm a Senior Software Engineer at Juniper Networks (13+ years of experience)\n‚ñ∫  Located in San Francisco Bay Area, CA (US citizen)\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Connect with me üëã ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫  LinkedIn: https://www.linkedin.com/in/anton-putra\n‚ñ∫  Twitter/X: https://twitter.com/antonvputra\n‚ñ∫  Instagram: https://www.instagram.com/aputrabay\n‚ñ∫  GitHub: https://github.com/antonputra\n‚ñ∫  Email: me@antonputra.com\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Timestamps ‚è∞  ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n0:00 Intro\n1:42 Why Measure Latency from the Client Side?\n3:38 1st Test\n8:04 2nd Test\n\n‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨ Source Code üìö ‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨‚ñ¨\n‚ñ∫ Original Source Code: https://github.com/antonputra/tutorials/tree/239/lessons/239\n\n#python #golang #nodejs</article>","contentLength":1159,"flags":null,"enclosureUrl":"https://www.youtube.com/v/shAELuHaTio?version=3","enclosureMime":"","commentsUrl":null},{"title":"NVIDIA‚Äôs New AI: Huge Game Changer!","url":"https://www.youtube.com/watch?v=4TpwfbUjCYs","date":1737308176,"author":"Two Minute Papers","guid":515,"unread":true,"content":"<article>‚ù§Ô∏è Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\nüìù The paper \"CLoSD - Closing the Loop between Simulation and\nDiffusion for multi-task character control\" is available here:\nhttps://guytevet.github.io/CLoSD-page/\n\nüìù My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\nüôè We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nAlex Balfanz, Alex Haro, B Shang, Benji Rabhan, Gaston Ingaramo, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Martin, Michael Albrecht, Michael Tedder, Owen Skarpness, Richard Sundvall, Taras Bobrovytsky,, Thomas Krcmar, Tybie Fitzhugh, Ueli Gallizzi.\nIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Fel√≠cia Zsolnai-Feh√©r - http://felicia.hu\n\n#nvidia</article>","contentLength":1141,"flags":null,"enclosureUrl":"https://www.youtube.com/v/4TpwfbUjCYs?version=3","enclosureMime":"","commentsUrl":null},{"title":"Encryption at Rest with SQLAlchemy","url":"https://blog.miguelgrinberg.com/post/encryption-at-rest-with-sqlalchemy","date":1737301725,"author":"Miguel Grinberg","guid":56,"unread":true,"content":"<p>In this tutorial I'm going to show you how to extend SQLAlchemy so that you can define database columns that are stored encrypted. In the solution I'm going to share, the columns that are designated as encrypted will transparently encrypt and decrypt themselves as data moves to and from the database.</p><p>For this tutorial I'm going to use SQLAlchemy version 2, the latest and greatest. All the techniques I will present can be adapted to SQLAlchemy 1.x if you need to work with legacy versions. If you are interested in updating your knowledge of SQLAlchemy, I have a <a href=\"https://amzn.to/3S0diwc\">SQLAlchemy 2 book</a> that can help.</p>","contentLength":597,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TypeScript enums: use cases and alternatives","url":"https://2ality.com/2025/01/typescript-enum-patterns.html","date":1737244800,"author":"Dr. Axel Rauschmayer","guid":343,"unread":true,"content":"<p>In this blog post, we take a closer look at TypeScript enums:</p><ul><li>What are their use cases?</li><li>What are the alternatives if we don‚Äôt want to use them?</li></ul><p>The blog post concludes with recommendations for what to use when.</p>","contentLength":209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automatic Server Reloading in Rust on Change: What is listenfd/systemfd?","url":"http://lucumr.pocoo.org/2025/1/19/what-is-systemfd","date":1737244800,"author":"Armin Ronacher","guid":301,"unread":true,"content":"<p>When I developed <a href=\"https://werkzeug.palletsprojects.com/\">Werkzeug</a> (and\nlater <a href=\"https://flask.palletsprojects.com/\">Flask</a>), the most\nimportant part of the developer experience for me was enabling fast, automatic\nreloading.  Werkzeug (and with it Flask), this is achieved by using two\nprocsses at all times.  The parent process holds on to the file descriptor\nof the socket on which the server listens, and a subprocess picks up that\nfile descriptor.  That subprocess restarts when it detects changes.  This\nensures that no matter what happens, there is no window where the browser\nreports a connection error.  At worst, the browser will hang until the\nprocess finishes reloading, after which the page loads successfully.  In\ncase the inner process fails to come up during restarts, you get an error\nmessage.</p><p>A few years ago, I wanted to accomplish the same experience for working\nwith Rust code which is why I wrote <a href=\"https://github.com/mitsuhiko/systemfd\">systemfd</a> and <a href=\"https://github.com/mitsuhiko/listenfd\">listenfd</a>.  I however realized that I\nnever really wrote here about how they work and disappointingly I think\nthose crates, and a good auto-reloading experience in Rust are largely\nunknown.</p><div><p>Firstly one needs to monitor the file system for changes.  While in theory\nI could have done this myself, there was already a tool that could do\nthat.</p><p>At the time there was <a href=\"https://crates.io/crates/cargo-watch\">cargo watch</a>.  Today one might instead use it\ntogether with the more generic <a href=\"https://github.com/watchexec/watchexec\">watchexec</a>.  Either one monitor your\nworkspace for changes and then executes a command.  So you can for\ninstance tell it to restart your program.  One of these will work:</p><pre>watchexec -r -- cargo run\ncargo watch -x run\n</pre><p>You will need a tool like that to do the watching part.  At this point I\nrecommend the more generic  which you can find on <a href=\"https://github.com/watchexec/watchexec/blob/main/doc/packages.md\">homebrew and\nelsewhere</a>.</p></div><div><p>But what about the socket?  The solution to this problem I picked comes\nfrom <a href=\"https://en.wikipedia.org/wiki/Systemd\">systemd</a>.  Systemd has a\n‚Äúprotocol‚Äù that standardizes passing file descriptors from one process to\nanother through environment variables.  In systemd parlance this is called\n‚Äúsocket activation,‚Äù as it allows systemd to only launch a program if\nsomeone started making a request to the socket.  This concept was\noriginally introduced by Apple as part of launchd.</p><p>To make this work with Rust, I created two crates:</p><ul><li><a href=\"https://github.com/mitsuhiko/systemfd\">systemfd</a> is the command\nline tool that opens sockets and passes them on to other programs.</li><li><a href=\"https://crates.io/crates/listenfd\">listenfd</a> is a Rust crate that\naccepts file descriptors from systemd or .</li></ul><p>It's worth noting that systemfd is not exclusivly useful to Rust.  The\nsystemd protocol can be implemented in other languages as well, meaning\nthat if you have a socket server written in Go or Python, you can also use\nsystemfd.</p><p>So here is how you use it.</p><p>First you need to add  to your project:</p><p>Then, modify your server code to accept sockets via listenfd before\nfalling back to listening itself on ports provided through command-line\narguments or configuration files.  Here is an example using  in\naxum:</p><div><pre>::::::::-&gt; -&gt; ::::::::::::::::</pre></div><p>The key point here is to accept socket 0 from the environment as a TCP\nlistener and use it if available.  If the socket is not provided (e.g.\nwhen launched without systemd/), the code falls back to opening a\nfixed port.</p></div><div><p>Finally you can use  /  together with :</p><pre>systemfd --no-pid -s http::8888 -- watchexec -r -- cargo run\nsystemfd --no-pid -s http::8888 -- cargo watch -x run\n</pre><p>This is what the parameters mean:</p><ul><li> needs to be first it's the program that opens the sockets.</li><li> is a flag prevents the PID from being passed.  This is necessary\nfor  to accept the socket.  This is a departure of the socket\npassing protocol from systemd which otherwise does not allow ports to be\npassed through another program (like ).  In short: when the\nPID information is not passed, then listenfd will accept the socket\nregardless.  Otherwise it would only accept it from the direct parent\nprocess.</li><li> tells  to open one TCP socket on port 8888.\nUsing  instead of  is a small improvement that will cause\nsystemfd to print out a URL on startup.</li><li> makes  restart the process when something\nchanges in the current working directory.</li><li> is the program that watchexec will start and re-start onm\nchanges.  In Rust this will first compile the changes and then run the\napplication.  Because we put  in, it will try to first accept\nthe socket from .</li></ul><p>The end result is that you can edit your code, and it will recompile\nautomatically and restart the server without dropping any requests.  When\nyou run it, and perform changes, it will look a bit like this:</p><pre>$ systemfd --no-pid -s http::5555 -- watchexec -r -- cargo run\n~&gt; socket http://127.0.0.1:5555/ -&gt; fd #3\n[Running: cargo run]\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.02s\n     Running `target/debug/axum-test`\n[Running: cargo run]\n   Compiling axum-test v0.1.0 (/private/tmp/axum-test)\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.52s\n     Running `target/debug/axum-test`\n</pre><p>For easier access, I recommend putting this into a  or similar\nso you can just run  and it runs the server in watch mode.</p><p>To install  you can use curl to bash:</p><pre>curl -sSfL https://github.com/mitsuhiko/systemfd/releases/latest/download/systemfd-installer.sh | sh\n</pre></div><div><p>Now how does this work on Windows?  The answer is that  and\n have a custom, proprietary protocol that also makes socket\npassing work on Windows.  That's a more complex system which involves a\nlocal RPC server.  However the system does also support Windows and the\ndetails about how it works are largely irrelevant for you as a user\n‚Äî&nbsp;unless you want to implement that protocol for another programming\nlanguage.</p></div><div><p>I really enjoy using this combination, but it can be quite frustrating to\nrequire so many commands, and the command line workflow isn't optimal.\nIdeally, this functionality would be better integrated into specific Rust\nframeworks like axum and provided through a dedicated cargo plugin.  In a\nperfect world, one could simply run , and everything\nwould work seamlessly.</p><p>However, maintaining such an integrated experience is a much more involved\neffort than what I have.  Hopefully, someone will be inspired to further\nenhance the developer experience and achieve deeper integration with Rust\nframeworks, making it more accessible and convenient for everyone.</p></div>","contentLength":6094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"More Python parallel stuff with joblib-modal","url":"https://www.youtube.com/watch?v=0SXyIcsT4tg","date":1737160389,"author":"probabl","guid":523,"unread":true,"content":"<article>One of the probabl colleagues has been working on an integration between joblib (which scikit-learn uses for parallel processing) and Modal (a cloud-provider known for it's likeable configuration for compute resources in Python). In this livestream we will explore a very early version and show you what's possible now and what is different. \n\nThe notebook from this episode can be found here:\nhttps://github.com/probabl-ai/youtube-appendix/blob/main/00-livestream/modal-jobs.ipynb\n\nWebsite: https://probabl.ai/\nDiscord: https://discord.probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\nIf you're keen to see more videos like this, you can follow us over at  @probabl_ai.</article>","contentLength":889,"flags":null,"enclosureUrl":"https://www.youtube.com/v/0SXyIcsT4tg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Patterns of use of Vello crate","url":"https://poignardazur.github.io//2025/01/18/vello-analysis/","date":1737158400,"author":"Olivier Faure","guid":742,"unread":true,"content":"<p>This document tries to establish patterns among a list of crates and Github projects using the <a href=\"https://github.com/linebender/vello/\">Vello</a> renderer.</p><p>The crate list isn‚Äôt meant to be exhaustive, but it‚Äôs pretty large: I‚Äôve sifted through maybe 40 or so reverse dependencies of the Vello repository, up to the point where most of the READMEs I read were along the lines of ‚ÄúWIP: quick experiment paint shapes with Vello‚Äù.</p><p>Don‚Äôt expect anything groundbreaking. My main focus is on common patterns among people using Vello‚Äôs <a href=\"https://docs.rs/vello/latest/vello/struct.Scene.html\"></a> API; most of this is going to be pretty dry.</p><p>I‚Äôve mostly noticed two types of projects with Vello as a direct dependency:</p><ul><li> bridging Vello with some other format or framework, or providing wrapper functions for Vello‚Äôs API. Ex: , , etc.</li><li> that renders specific things with Vello.</li></ul><p>(Though in practice, a few blurred the lines, and a lot of projects that I put in one category or the other mostly ended up using Vello for example code or for very basic painting.)</p><p>The numbers (18 engine projects, 13 app-like projects) aren‚Äôt too surprising if you‚Äôre familiar with the ‚ÄúRust has 50 game engines and 3 games‚Äù stereotype. They also match my ‚Äúgut feeling‚Äù reading the code, where it feels like a lot more projects use Vello in a very systematized way, as some kind of middleware or optional backend than as a plug-and-play dependency to paint a bunch of shapes.</p><p>For instance, I saw almost no project calling the  function more than five times.</p><p>This is confounded by the fact that I only looked for direct dependents of Vello; I might have found more app projects looking for dependents of Masonry or bevy_vello. Maybe the painting-privitive-heavy code is in the dependents of one of those engine code projects I cited.</p><p>Graphite, for example, is a primitive-heavy 2D editor, but its use of Vello is bottlenecked through a middleware layer with its own internal representation.</p><p>In general, though, my impression is that the stereotype is mostly true.</p><p>Another interesting pattern is that code actually using Vello to paint things was often in amateur stub projects, which is good for our purposes: it tells us how people who have little experience with Vello end up using it.</p><h3>Fill and stroke arguments</h3><p> and  are the most used methods by a very wide margin.</p><p>As a reminder, their prototype is:</p><div><div><pre><code></code></pre></div></div><p>A lot code calling them looks like this:</p><div><div><pre><code></code></pre></div></div><p>Note the heavy usage of default values:</p><ul><li> is the default fill setting for most paint APIs.</li><li> creates a stroke with rount joins and caps, no dashes, and the given width.</li><li> is set to  for both methods.</li><li> is set to  for both methods.</li></ul><p>These patterns can be found throughout the projects I‚Äôve linked. In total, I‚Äôve counted:</p><ul><li>16 projects using  with ,  and .</li><li>10 projects using  with ,  and .</li><li>7 projects using  with , a transform and .</li><li>3 projects using  with , a transform and .</li></ul><p>Projects that used all the arguments of  or  were rare, and were generally written as middleware code passing these arguments from another source. For example:</p><div><div><pre><code></code></pre></div></div><p>These projects tended to have one instance code calling each Scene method in the entire repository.</p><p>Most of the projects I‚Äôve looked at used the  and  API exclusively.\nFew of them used , , , etc.</p><p>In total, I‚Äôve counted about a dozen projects using any of these APIs.</p><p>Those that did tended to be the ‚Äúrender any arbitrary SVG‚Äù kinds of projects.</p><p>Based on the above, I‚Äôd recommend having Vello export the following API:</p><div><div><pre><code></code></pre></div></div><ul><li> and  use the minimum number of arguments.</li><li> and  use an additional  argument.</li><li> and  use the full API.</li></ul><p>With this API, the  code I quoted would look like this:</p><div><div><pre><code></code></pre></div></div><p>This would also let us remove most of the helpers in  in Masonry.</p>","contentLength":3583,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Real Python Podcast ‚Äì Episode #235: Principles for Considering Your Python Tooling","url":"https://realpython.com/podcasts/rpp/235/","date":1737115200,"author":"Real Python","guid":162,"unread":true,"content":"<p>What are the principles you should consider when making decisions about which Python tools to use? What anti-patterns get in the way of making the right choices for your team? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.</p>","contentLength":292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Service Reliability Mathematics","url":"https://addyosmani.com/blog/service-reliability/","date":1737072000,"author":"","guid":78,"unread":true,"content":"<article>Service reliability is often reduced to a simple percentage but the reality is far more nuanced than those decimal points suggest. Lets explore what these numbers actually mean.</article>","contentLength":177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to use Anthropic MCP Server with open LLMs, OpenAI or Google Gemini","url":"https://www.philschmid.de/mcp-example-llama","date":1737072000,"author":"","guid":12,"unread":true,"content":"<article>How to use Anthropic MCP Server with open LLMs, OpenAI or Google Gemini</article>","contentLength":71,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bite: How Deepseek R1 was trained","url":"https://www.philschmid.de/deepseek-r1","date":1737072000,"author":"","guid":13,"unread":true,"content":"<article>5 Minute Read on how Deepseek R1 was trained using Group Relative Policy Optimization (GRPO) and RL-focused multi-stage training approach.</article>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measuring productivity impact with Diff Authoring Time","url":"https://engineering.fb.com/2025/01/16/developer-tools/measuring-productivity-impact-with-diff-authoring-time/","date":1737046800,"author":"","guid":464,"unread":true,"content":"<p>Do types actually make developers more productive? Or is it just more typing on the keyboard? To answer that question we‚Äôre revisiting <a href=\"https://engineering.fb.com/2024/10/25/developer-tools/diff-authoring-time-dat-measuring-developer-productivity-meta/\" target=\"_blank\" rel=\"noopener\">Diff Authoring Time (DAT)</a> ‚Äì how Meta measures how long it takes to submit changes to a codebase.</p><p>DAT is just one of the ways we measure developer productivity and this latest episode of the Meta Tech Podcast takes a look at two concrete use cases for DAT, including a type-safe mocking framework in Hack.</p><p>Tune in to learn how we leverage metrics to run experiments on productivity in our internal codebase at Meta.</p><p>Download or listen to the podcast episode below:</p><p>\nYou can also find the episode wherever you get your podcasts, including:</p><p>The&nbsp;<a href=\"https://insidefacebookmobile.libsyn.com/\" target=\"_blank\" rel=\"noopener\">Meta Tech Podcast</a>&nbsp;is a podcast, brought to you by Meta, where we highlight the work Meta‚Äôs engineers are doing at every level ‚Äì from low-level frameworks to end-user features.</p><p>And if you‚Äôre interested in learning more about career opportunities at Meta visit the&nbsp;<a href=\"https://www.metacareers.com/?ref=engineering.fb.com\" target=\"_blank\" rel=\"noopener\">Meta Careers</a>&nbsp;page.</p>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}