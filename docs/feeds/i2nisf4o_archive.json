{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":414,"items":[{"title":"native WebP encoding version 1.0! üöÄ","url":"https://www.reddit.com/r/golang/comments/1iw8a68/native_webp_encoding_version_10/","date":1740310727,"author":"/u/Pretend-Ad1926","guid":9590,"unread":true,"content":"<p>I‚Äôm excited to announce nativewebp v1.0, a major milestone for our WebP encoder in Go! This version marks 1.0 because we now fully support the VP8L format, making nativewebp a complete solution for lossless WebP encoding. Alongside this, we‚Äôve added better compression, improved Go integration, and important bug fixes.</p><p>Here are some highlights of this release:</p><p><strong>Full VP8L Feature Support</strong></p><p>This release now fully supports all VP8L features, including LZ77, Color Caching, and transforms, ensuring more accurate and efficient encoding.</p><p><strong>Smarter Compression with Filter Selection for Predictor Transform</strong></p><p>We now analyze block entropy and automatically select the best filter per block, leading to much better compression.</p><p>nativewebp now includes a wrapper for <a href=\"http://golang.org/x/image/webp\">golang.org/x/image/webp</a>, so you can use Decode and image.Decode out of the box without extra imports.</p><p>Looking forward to your thoughts and feedback on the new release!</p>","contentLength":918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7puq/amazon_eks_downgrade_a_practical_solution/","date":1740308426,"author":"/u/Complete-Emu-6287","guid":9546,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7pqt/amazon_eks_downgrade_a_practical_solution/","date":1740308415,"author":"/u/Complete-Emu-6287","guid":9545,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1740303005,"author":"/u/Aciddit","guid":9526,"unread":true,"content":"<div>By <b>Antonio Ojea, Michael McCune</b> |\n<time datetime=\"2025-02-14\">Friday, February 14, 2025</time></div><p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><figure><img src=\"https://kubernetes.io/images/docs/components-of-kubernetes.svg\" alt=\"Components of Kubernetes\"></figure><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the Node\nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial Node is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><figure><img src=\"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/ccm-chicken-egg-problem-sequence-diagram.svg\" alt=\"Chicken and egg problem sequence diagram\"><figcaption><p>Chicken and egg problem sequence diagram</p></figcaption></figure><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a Node resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a Node resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one ‚Äúcorrect way‚Äù to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting ‚ÄúhostNetwork‚Äù to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider‚Äôs instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure‚Äôs node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider‚Äôs documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: cloud-controller-manager\n  name: cloud-controller-manager\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cloud-controller-manager\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cloud-controller-manager\n      annotations:\n        kubernetes.io/description: Cloud controller manager for my infrastructure\n    spec:\n      containers: # the container details will depend on your specific cloud controller manager\n      - name: cloud-controller-manager\n        command:\n        - /bin/my-infrastructure-cloud-controller-manager\n        - --leader-elect=true\n        - -v=1\n        image: registry/my-infrastructure-cloud-controller-manager@latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 50Mi\n      hostNetwork: true # these Pods are part of the control plane\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - topologyKey: \"kubernetes.io/hostname\"\n            labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: cloud-controller-manager\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoExecute\n        key: node.kubernetes.io/unreachable\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoExecute\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoSchedule\n        key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - effect: NoSchedule\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iw6fpj/the_cloud_controller_manager_chicken_and_egg/"},{"title":"Finding UI libraries is easy, but discovering components visually is still a challenge. A curated list + an idea to fix this.","url":"https://github.com/sanjay10985/animated-react-collection","date":1740302565,"author":"/u/Mobile_Candidate_926","guid":9592,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iw6bzk/finding_ui_libraries_is_easy_but_discovering/"},{"title":"[P] See the idea development of academic papers visually","url":"https://www.reddit.com/r/MachineLearning/comments/1iw5lgj/p_see_the_idea_development_of_academic_papers/","date":1740299410,"author":"/u/MadEyeXZ","guid":9593,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Relevance-Guided Parameter Optimization for Efficient Control in Diffusion Transformers","url":"https://www.reddit.com/r/MachineLearning/comments/1iw46oq/r_relevanceguided_parameter_optimization_for/","date":1740293456,"author":"/u/Successful-Western27","guid":9487,"unread":true,"content":"<p>The key technical contribution here is a relevance-guided architecture that makes diffusion transformers more computationally efficient by selectively allocating processing power based on region importance. It combines DiT (Diffusion Transformers) with ControlNet approaches while introducing a relevance prior mechanism.</p><p>Main technical points: - Introduces a two-stage relevance assessment system: lightweight networks evaluate region importance, followed by adaptive computation allocation - Integrates with existing diffusion pipelines through modular design - Relevance prior guides transformer attention mechanisms - Compatible with standard diffusion transformer architectures</p><p>Key results: - 30-50% reduction in computational overhead - Maintains or improves image quality compared to baselines - More precise control over generated content - Effective handling of complex scenes</p><p>I think this could have meaningful impact on making high-quality image generation more accessible, especially for resource-constrained applications. The approach seems particularly promising for deployment scenarios where computational efficiency is crucial.</p><p>I think the relevance-guided approach could extend beyond image generation - the core idea of selective computation based on importance could benefit other transformer applications where attention mechanisms are computationally expensive.</p><p>TLDR: Novel architecture that makes diffusion transformers more efficient by focusing computational resources on important image regions, reducing compute needs by 30-50% while maintaining quality.</p>","contentLength":1576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Folang: Transpiler for F#-like functional languages ‚Äã‚Äãto Go","url":"https://www.reddit.com/r/golang/comments/1iw3tz7/folang_transpiler_for_flike_functional_languages/","date":1740292019,"author":"/u/karino2012","guid":9541,"unread":true,"content":"<p>I wrote a transpiler in Go that transpiles F#-like functional languages ‚Äã‚Äãto Go.</p><p>I design the language specifications from scratch to match Go, and named it Folang.</p><p>There are still many NYIs, but I have implemented it to the extent that it can be self-hosted, so I will post it on reddit.</p><ul><li>A transpiler that does not require anything other than Go to try</li><li>Argument types are inferred using F#-like syntax, and the arguments are generalized to become generic functions</li><li>The transpiler itself is 3600 lines of Folang code and about 500 lines of Go code</li></ul>","contentLength":546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A simple VSCode extension to remember which virtual desktop each editor window is on in Linux","url":"https://marketplace.visualstudio.com/items?itemName=mathiscode.remember-desktops","date":1740290165,"author":"/u/FatherCarbon","guid":9468,"unread":true,"content":"<div><div><div><table role=\"presentation\"><tbody><tr><td><div><div><p>On some Linux desktop managers, Visual Studio Code editors don't remember their last desktop. This extension uses  to save the desktop of each open editor window, and restore them when the editor starts.</p><p>There are commands to save the editor locations, and to restore them, but by default the extension will start working automatically when it is installed.</p></div></div></td></tr></tbody></table></div></div></div>","contentLength":356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iw3czs/a_simple_vscode_extension_to_remember_which/"},{"title":"Thoughts on listings like these selling flash drives with Ubuntu and other Linux distros pre-installed?","url":"https://www.reddit.com/r/linux/comments/1iw2zog/thoughts_on_listings_like_these_selling_flash/","date":1740288767,"author":"/u/FutureSuccess2796","guid":9486,"unread":true,"content":"<p>Admittedly someone who's relatively newer to the Linux space, so please bear with my question here. I was in middle of actually shopping for some extra brand new USBs to replace my old ones when I encountered this for the first time. It looked like there were quite a good number of people on marketplace platforms like eBay and Mercari selling bootable USB flash drives with a Linux distro pre-installed on it. Majority of the ones I saw were Ubuntu (like what I had pictured) on there, but I also saw a good amount of ones with Kali and different versions of Linux Mint as well. </p><p>Seems like you get the USB according to said listings with instructions on how to properly boot it or install it on your computer, and in some cases even provide contact information for support if needed. The prices on some of these are slightly in the higher side when compared to those I had screenshots of in the examples, and the sellers all had a large amount of sales and positive responses. </p><p>Now, of course, I'd personally just stick to what I've been doing and just create the bootable drive myself for literally free like I have from the start. So to me it was interesting to see these out there actually being bought when the process of doing this yourself is relatively easy with step-by-step guides on the respective distro's website and even YouTube tutorials if you wish to follow those. </p><p>So in short, what's everyone think of these?</p>","contentLength":1426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Font for programming mathematics","url":"https://www.reddit.com/r/rust/comments/1iw2ovd/font_for_programming_mathematics/","date":1740287667,"author":"/u/okimusix","guid":9544,"unread":true,"content":"<p>So I am a physics undergrad and I've been using Rust for a few years now. It's my favorite language and I use it for everything, from personal apps using Tauri to taking advantage of its speed for computations and using it in my school assignments.</p><p>Since I often find myself writing math code, I found naming variables \"lambda_squared\", for example, looks really clunky and makes it harder to read the code. For this, I implemented a Live Templates group on RustRover that replaced lambda, for example, with its equivalent unicode character. However, Rust did complain a little.</p><p>Finally, though, I found the solution. I had been trying to do this for a while with no luck, but I found a way to make it work. I used the ligature system on the FiraCode font to implement ligatures for every greek letter and some mathematical symbols, this way you get the readability of actual math, but for the compiler, it still looks like plain text. Here's an example</p><p>The text for the sum variable, for example, is just \"SUMxu2\", and both the compiler and I are happier. I don't know if anyone has done this before, I tried to look for it but never found anything. </p><p>If you find this something that could be useful for you or others, I can share a link to a drive or something where you can download the font, as well as the guide to every symbol I included. If so, please comment and share your thoughts on this too :)</p>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] API platforms vs self-deployment for diffusion models","url":"https://www.reddit.com/r/MachineLearning/comments/1iw2kbl/d_api_platforms_vs_selfdeployment_for_diffusion/","date":1740287219,"author":"/u/crookedstairs","guid":9469,"unread":true,"content":"<p>Caveat that Modal is a serverless compute platform! But this post covers when you might choose between API platforms (replicate, fal), traditional cloud (AWS EC2), managed ML platforms (SageMaker, Vertex), and serverless cloud.</p><p>I often see companies jump to self-deployment even if they're just using off-the-shelf models with a couple of adapters. I think that rarely makes sense from a cost or effort perspective unless you have a high volume of production traffic that you're amortizing those things across. The most compelling reason to move to self-deployment is if you need a high level of control over generated inputs =&gt; this requires fine-tuned weights / customer adapters / multi-step generation pipeline =&gt; this requires code-level control of your deployment.</p><p>What do you agree/disagree with? If you've evaluated these categories of providers before, tell me how they stacked up against each other.</p>","contentLength":907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's your recommendation on video editors?","url":"https://www.reddit.com/r/linux/comments/1iw2aae/whats_your_recommendation_on_video_editors/","date":1740286224,"author":"/u/chuzambs","guid":9506,"unread":true,"content":"<p>Hi there ! I'm looking for the best video editor for Linux, but as I know that's a completely subjective matter I ask for your favorite one. I come from adobe premiere and I'm looking for a Linux replacement, Im not a cinematographer so I'm not looking for something extremely professional.</p><p>I think Id go for da Vinci resolve since it's more standard, but would love to hear your recommendations</p><p>Edit: I'm running fedora bluefin (gnome) so I'd rather use flatpak </p>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is your logging, monitoring & observability stack for your golang app?","url":"https://www.reddit.com/r/golang/comments/1iw07rm/what_is_your_logging_monitoring_observability/","date":1740279237,"author":"/u/gwwsc","guid":9441,"unread":true,"content":"<p>My company uses papertrail for logging, prometheus and grafana for observability and monitoring.</p><p>I was not actively involved in the integration as it was done by someone else a few years ago and it works.</p><p>I want to do the same thing for my side project that I am working on for learning purpose. The thing I am confused about it should I first learn the basics about otel, collector agents etc? Or should I just dive in?</p><p>As a developer I get an itch if things are too abstracted away and I don't know how things are working. I want to understand the underlying concepts first before relying on abstraction.</p><p>What tools are you or your company using for this?</p>","contentLength":653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My \"AI Operating System\" Can Now Organize My Desktop!","url":"https://v.redd.it/crjpxmcknske1","date":1740275232,"author":"/u/mitousa","guid":9446,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ivyyce/my_ai_operating_system_can_now_organize_my_desktop/"},{"title":"This feels illegal","url":"https://www.reddit.com/r/linux/comments/1ivyn4j/this_feels_illegal/","date":1740274254,"author":"/u/dk865409","guid":9424,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built WikiTok in 4 hours - A TikTok style feed for Wikipedia","url":"https://www.reddit.com/r/artificial/comments/1ivy48f/i_built_wikitok_in_4_hours_a_tiktok_style_feed/","date":1740272626,"author":"/u/Illustrious-King8421","guid":9542,"unread":true,"content":"<p>So, I decided to use Replit's AI Agent to create my own version. Took me about 4 hours total, which isn't bad since I don't know any code at all.</p><p>To be honest, at first it seemed unreal - seeing the AI build stuff just from my instructions. But then reality hit me. With every feature I wanted to add, it became more of a headache. Here's what I mean: I wanted to move some buttons around, simple stuff. But when I asked the AI to realign these buttons, it messed up other parts of the design that were working fine before. Like, why would moving a button break the entire layout?</p><p>This really sucks because these errors took up most of my time. I'm pretty sure I could've finished everything in about 2 hours if it wasn't for all this fixing of things that shouldn't have broken in the first place.</p><p>I'm curious about other people's experiences. If you don't code, I'd love to hear about your attempts with AI agents for building apps and websites. What worked best for you? Which AI tool actually did what you needed?</p><p>What do you think? Would love to hear your stories and maybe get some tips for next time!</p>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"After 15 years of using Windows, I decided to try Linux","url":"https://www.reddit.com/r/linux/comments/1ivxy81/after_15_years_of_using_windows_i_decided_to_try/","date":1740272133,"author":"/u/Catwz","guid":9408,"unread":true,"content":"<p>First of all, I apologize for writing such a long text.</p><p>I'm 22 years old. I know I'm young and still don't know much, but I'd like to write about this anyway. I think I started using computers during the Windows XP era. My father worked repairing computers. My mom says I learned to type on a computer before writing on paper. I was like one of today's kids who spend all day on their phones, except with computers. During my childhood, I spent my time chronically online, playing various games and browsing the internet. I remember Windows XP very well, along with Windows 7 and Minecraft. Those were good times, but as I grew older, things changed very quickly. My father stopped working with computer repairs, and soon I knew more than everyone else in the family.</p><p>I could fix all kinds of computers easily for my friends; back then, everything was Windows. My first contact with Linux was at school when we started having computer classes, when I was around 15. The school computers were slow and had Ubuntu installed. It was slow, ugly, and very limited because the computers were managed by the school. That was my first impression: a slow system for government computers.</p><p>Microsoft tried various things. I remember Windows 8 when formatting laptops, and then that Windows 8.1 update where they changed the menu. A lot happened, and it seems to have passed so quickly. At school, I always used Office suite programs: Word, PowerPoint, etc., and in computer classes, you had to use LibreOffice on a very slow government computer. it was ugly and seemed very difficult to use.</p><p>My family's financial situation didn't improve much, so I ended up with limited access to new technologies. My phone was already old, and my computers were getting old. I still remember Windows 10's launch very well. My relatives would bring computers for me to repair and format, wanting the latest version of Windows with Office and everything else, but the computers were already old and barely worked with Windows 8.</p><p>I begged my father to buy me a laptop, and after much insistence, I finally convinced him. It was an Asus X450LA. A mid-range computer for its time. It came with Windows 8, I think, but I did that upgrade to Windows 10. I used it until I finished high school, but then Windows 11 came along, and my laptop was cut from the list of computers that could upgrade. it was the end of my laptop's life.</p><p>I was already working at my father's market, so I bought myself a new gaming computer with Windows 11. I had time again to spend on the internet and started to worry about my father's business expenses. Using Office costs money, sales programs are expensive, everything is expensive, and maybe my gaming laptop won't even be able to use the next Windows.</p><p>I started researching Linux. At first, I was a bit scared because everyone on Reddit talked about terminals, command lines to install anything, etc., but I decided to take my old laptop and refurbish it. I bought a new battery, an SSD, and an 8GB RAM stick. I researched on Reddit which distro was best for beginners, got an old USB drive, put Mint on it, and formatted my computer: Love at first sight.</p><p>I customized Mint and left it in a way that I spend more than 15 minutes before doing anything just appreciating it. I used LibreOffice for everything I did in Office. I used Firefox and liked it a lot. The system is very fast, strangely seems faster than my new computer with Windows 11. I downloaded my daily-use programs from Mint's app center: Spotify, Bitwarden, everything's there. I spent hours playing with the terminal with ChatGPT's help. I extracted running process logs to txt, system information. it's very easy to use. I even managed to install a game I played in my childhood, a BF2 mod: Forgotten Hope 2 from Windows on Mint using Lutris (I swear it's the last Windows thing I'll use).</p><p>I'm in love with my old laptop again. I cleaned it, spent hours looking at it, I love using Mint, made it my own. I'm going to buy a new computer for my room and install Mint for my personal use. I'll have a laptop and a computer with Linux. My current computer with Windows 11 will be only for sales programs and government programs that only work on Windows. I showed it to my father, and he liked Linux too.<p> Windows never again. Using Windows now feels like one of those mobile games full of ads</p></p>","contentLength":4349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advanced SQL Tricks (CTEs, Conditional Aggregations, etc)","url":"https://youtu.be/rDGCOE5YGT0","date":1740269892,"author":"/u/Special_Community179","guid":9543,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivx75e/advanced_sql_tricks_ctes_conditional_aggregations/"},{"title":"Found this on a piece of digital signage in a bathroom","url":"https://www.reddit.com/r/linux/comments/1ivwydq/found_this_on_a_piece_of_digital_signage_in_a/","date":1740269184,"author":"/u/A_Sc00py_b0i","guid":9393,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing wctx - A simple CLI tool for window context info on Wayland & X11","url":"https://www.reddit.com/r/linux/comments/1ivw7xn/introducing_wctx_a_simple_cli_tool_for_window/","date":1740267048,"author":"/u/slightlyfaulty","guid":9561,"unread":true,"content":"<p>Hey everyone, I just released my first package for Linux. It's called  (short for window context). It's a simple CLI tool that provides real-time information about the current  window (focused window) or  window (under the mouse cursor) on Wayland and X11. It's (mostly) written in Rust.</p><p>It's not very useful on its own, but it makes it much easier for programs and scripts to work with windows. For example, you could create hotkeys that only work in specific apps, or change your mouse scroll speed when the cursor is in a browser window, or turn your monitor brightness up when it has a fullscreen window.</p><p>You can of course already do these things, with a bit of effort. The main advantage of wctx is that it works across multiple desktop environments, which means programs and scripts using it will too. It's also dead simple to use, with several CLI output options and formats, as well as a D-Bus interface.</p><p>Currently it supports these desktop environments, with more to come if there's enough interest in them:</p><p>For other distros an installation script is included, with more info in the readme.</p><p>I'd love to hear everyone's thoughts. This is also my first real Rust project, so please be nice üòÑ (or rip me a new one so I can learn).</p><p>Feedback and contributions are very welcome!</p>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My first time with Linux","url":"https://www.reddit.com/r/linux/comments/1ivvcxn/my_first_time_with_linux/","date":1740264599,"author":"/u/Acu17y","guid":9392,"unread":true,"content":"<p>Oh my god guys, I'm speechless. Unfortunately I regret it, but it's the first time I've put my hands on a PC with a Linux kernel.<p> But this stuff is absurd! It has mind-blowing performance!</p></p><p>I installed it on my old laptop with an i3 5005u / 4gb of ram and a 500gb 5400rpm hdd and it's like it was reborn. I mean, it's basically the OS I've always dreamed of, I feel like the PC is really mine and everything is so fast and intuitive that I can't describe it.</p><p>I was so impressed by Linux Mint that I'm really thinking of installing it on the main machine and getting rid of Windows, if only it weren't for the huge library of video games I have.</p><p>It also has a community made up of wonderful people, true enthusiasts.</p><p>I write this post as an appreciation for this discovery and someone who can help me understand if it is possible to use mint for gaming, I read around that there are problems with anti-cheats and online games?</p>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Any open source project that shows a good example of unit test and integration test","url":"https://www.reddit.com/r/golang/comments/1ivv5eq/any_open_source_project_that_shows_a_good_example/","date":1740264003,"author":"/u/smartfinances","guid":9367,"unread":true,"content":"<p>As the title suggests. I have been adding various unit tests to my project but I am looking for suggestions/ideas on how to go about writing integration tests.</p><p>My project mostly entails reading from SQS, batching data based on some parameters and then writing the output to s3. probably, a very common pattern. Extending that the service reads from various SQS and batching is localised to one queue. So 10 queue creats 10 different outputs.</p><p>I am using localstack for development. I am not looking for examples of exactly the above use case but something similar that is interaction with db/external system and then giving some output would also do.</p>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Writing a file system in Go -- not FUSE, but a real FS","url":"https://www.reddit.com/r/golang/comments/1ivuz61/writing_a_file_system_in_go_not_fuse_but_a_real_fs/","date":1740263514,"author":"/u/Rich-Engineer2670","guid":9377,"unread":true,"content":"<p>I would say I'm crazy, but this both well established and redundant.....</p><p>Assume I wanted to write my own file system (education), with Golang -- not a fuse variant, but I literally am taking a file on a block device and treating it as a disk. If this were C, OK, I'd do the following:</p><ul><li>Define a binary boot block at LBA 0</li><li>Define a certain number of LBAs for boot code</li><li>Define a certain number of LBAs for partitions</li><li>Within each partition define the directories and free lists (FATs, clusters, etc...)</li><li>Have a bunch of free LBAs.</li></ul><p>In C, I could define structs and just write them out assuming they were packed. In Go, structs aren't C structs, so I need to constantly convert structs to binaries. Sure, I could use the binary package and a lot functions, but someone must have done this in a better way, or is the \"better way\" \"No, write your file systems in C....\"</p><p>I want to stay in Go, because everything else in the OS is in Go...</p>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talos on IPv6 only network?","url":"https://www.reddit.com/r/kubernetes/comments/1ivumee/talos_on_ipv6_only_network/","date":1740262538,"author":"/u/Moleventions","guid":9371,"unread":true,"content":"<div><p>Does anyone know if you can deploy Talos on an IPv6 only network in AWS?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Moleventions\"> /u/Moleventions </a>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why K8s when there‚Äôs k3s with less resource requirements?","url":"https://www.reddit.com/r/kubernetes/comments/1ivu87n/why_k8s_when_theres_k3s_with_less_resource/","date":1740261469,"author":"/u/Crafty0x","guid":9321,"unread":true,"content":"<div><p>I don‚Äôt get why a business will run the more demanding k8s instead of k3s. What could possibly be the limitations of running k3s on full fledged servers.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Crafty0x\"> /u/Crafty0x </a>","contentLength":186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Golang SQLite admin tool","url":"https://github.com/joelseq/sqliteadmin-go","date":1740258532,"author":"/u/lAdddd","guid":9320,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ivt55s/golang_sqlite_admin_tool/"},{"title":"DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask","url":"https://www.bloomberg.com/news/articles/2025-02-10/deepseek-could-make-founder-liang-wenfeng-one-of-the-world-s-richest-people?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczOTIzNzk1NywiZXhwIjoxNzM5ODQyNzU3LCJhcnRpY2xlSWQiOiJTUjhYTTdUMEcxS1cwMCIsImJjb25uZWN0SWQiOiI0MUVGMDc3MjI0RTM0MDhFOTNFMDdFQkY0RDc3QzI1QiJ9.kqtC_AK59CyhVfXIjYbRqB5ymi-WS52icc0pzlfX74E","date":1740256369,"author":"/u/cramdev","guid":9391,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ivsbro/deepseek_founders_are_worth_1_billion_or_150/"},{"title":"Solving The Millionaires' Problem in Rust","url":"https://vaktibabat.github.io/posts/smpc_circuits/","date":1740255860,"author":"/u/vaktibabat","guid":9370,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ivs4vp/solving_the_millionaires_problem_in_rust/"},{"title":"Official /r/rust \"Who's Hiring\" thread for job-seekers and job-offerers [Rust 1.85]","url":"https://www.reddit.com/r/rust/comments/1ivrkhs/official_rrust_whos_hiring_thread_for_jobseekers/","date":1740254374,"author":"/u/DroidLogician","guid":9425,"unread":true,"content":"<p>Welcome once again to the official <a href=\"https://www.reddit.com/r/rust\">r/rust</a> Who's Hiring thread!</p><p>Before we begin, job-seekers should also remember to peruse the <a href=\"https://www.reddit.com/r/rust/comments/1hynsw7/official_rrust_whos_hiring_thread_for_jobseekers/\">prior thread</a>.</p><p>This thread will be periodically stickied to the top of <a href=\"https://www.reddit.com/r/rust\">r/rust</a> for improved visibility. You can also find it again via the \"Latest Megathreads\" list, which is a dropdown at the top of the page on new Reddit, and a section in the sidebar under \"Useful Links\" on old Reddit.</p><p>The thread will be refreshed and posted anew when the next version of Rust releases in six weeks.</p><p>Please adhere to the following rules when posting:</p><ul><li><p>Don't create top-level comments; those are for employers.</p></li><li><p>Feel free to reply to top-level comments with on-topic questions.</p></li><li><p>Anyone seeking work should reply to my stickied top-level comment.</p></li><li><p>Meta-discussion should be reserved for the distinguished comment at the very bottom.</p></li></ul><ul><li><p><strong>The ordering of fields in the template has been revised to make postings easier to read. If you are reusing a previous posting, please update the ordering as shown below.</strong></p></li><li><p><strong>Remote positions: see bolded text for new requirement.</strong></p></li><li><p>To find individuals seeking work, see the replies to the stickied top-level comment; you will need to click the \"more comments\" link at the bottom of the top-level comment in order to make these replies visible.</p></li><li><p>To make a top-level comment you must be hiring directly; no third-party recruiters.</p></li><li><p>One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.</p></li><li><p>Proofread your comment after posting it and edit it if necessary to correct mistakes.</p></li><li><p>To share the space fairly with other postings and keep the thread pleasant to browse, we ask that you try to limit your posting to either 50 lines or 500 words, whichever comes first.<strong>We reserve the right to remove egregiously long postings.</strong> However, this only applies to the content of this thread; you can link to a job page elsewhere with more detail if you like.</p></li><li><p>Please base your comment on the following template:</p></li></ul><p>COMPANY: <em>[Company name; optionally link to your company's website or careers page.]</em></p><p>TYPE: <em>[Full time, part time, internship, contract, etc.]</em></p><p>LOCATION: <em>[Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.]</em></p><p>REMOTE: <em>[Do you offer the option of working remotely? <strong>Please state clearly if remote work is restricted to certain regions or time zones, or if availability within a certain time of day is expected or required.</strong>]</em></p><p>VISA: <em>[Does your company sponsor visas?]</em></p><p>DESCRIPTION: <em>[What does your company do, and what are you using Rust for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.]</em></p><p>ESTIMATED COMPENSATION: <em>[Be courteous to your potential future colleagues by attempting to provide at least a rough expectation of wages/salary. If you are listing several positions in the \"Description\" field above, then feel free to include this information inline above, and put \"See above\" in this field.<p> If compensation is negotiable, please attempt to provide at least a base estimate from which to begin negotiations. If compensation is highly variable, then feel free to provide a range.</p> If compensation is expected to be offset by other benefits, then please include that information here as well. If you don't have firm numbers but do have relative expectations of candidate expertise (e.g. entry-level, senior), then you may include that here.<p> If you truly have no information, then put \"Uncertain\" here.</p> Note that many jurisdictions (including several U.S. states) <strong>require salary ranges on job postings by law</strong>. If your company is based in one of these locations or you plan to hire employees who reside in any of these locations, you are likely subject to these laws.<p> Other jurisdictions may require salary information to be available upon request or be provided after the first interview.</p> To avoid issues, <strong>we recommend all postings provide salary information</strong>. You  state clearly in your posting if you are planning to compensate employees partially or fully in <strong>something other than fiat currency</strong> (e.g. cryptocurrency, stock options, equity, etc). Do  put just \"Uncertain\" in this case as the default assumption is that the compensation will be 100% fiat. Postings that fail to comply with this addendum . Thank you.]</em></p><p>CONTACT: <em>[How can someone get in touch with you?]</em></p>","contentLength":4390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I‚Äôve made a bunch of free wallpapers","url":"https://www.reddit.com/r/linux/comments/1ivqqg0/ive_made_a_bunch_of_free_wallpapers/","date":1740252187,"author":"/u/Folium_Creations","guid":9300,"unread":true,"content":"<p>I‚Äôve made a whole bunch of wallpapers and released them under CC:BY </p><p>I have made a git where I have uploaded, and will continue to upload them in 4k resolution as .png files for your convenience. I can‚Äôt stand all those ‚Äúwe have free wallpapers, as long as you register with email,phone number and the blood of your first born.‚Äù Here is the link to the git. I‚Äôm slowly building up a curated library of wallpapers I‚Äôve created. </p>","contentLength":438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are the odds that Rust is going to have a real competitor?","url":"https://www.reddit.com/r/rust/comments/1ivqkj1/what_are_the_odds_that_rust_is_going_to_have_a/","date":1740251759,"author":"/u/nikitarevenco","guid":9369,"unread":true,"content":"<p>By \"Real Competitor\" I mean: A language just like Rust with similar goals, but one that people actually prefer to Rust. So it would be a fast, low-level memory safe language with great tooling, great type system and other benefits that Rust offers. But it would need to be better than Rust to actually catch on</p><p>This language needs to offer real advantages over Rust to be considered. Of course since Rust has a huge ecosystem that is growing rapidly, it may take a long time. But I am talking on a timescale of 25+ years.</p><p>Creating a new programming language to compete with Rust would be a massive undertaking and there would have to be some real reason to do it. Rust may be missing some features like higher-kinded types, named function arguments and such but to really catch on the language would need to offer some extremely important feature that Rust doesn't have, as well as offering all of Rust's benefits at the same time.</p><p>Is there any such language currently in early development? Or perhaps, what would such a language have to look like?</p>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scrap Your ORM‚ÄîReplacing Your ORM With Relational Algebra","url":"https://youtu.be/SKXEppEZp9M?si=wccXwllXm-0M-zOO","date":1740249559,"author":"/u/JohnyTex","guid":9394,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivppbh/scrap_your_ormreplacing_your_orm_with_relational/"},{"title":"FFmpeg School of Assembly Language","url":"https://github.com/FFmpeg/asm-lessons/blob/main/lesson_01/index.md","date":1740247207,"author":"/u/mitousa","guid":9265,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivorhb/ffmpeg_school_of_assembly_language/"},{"title":"[R] Interpreting Deep Neural Networks: Memorization, Kernels, Nearest Neighbors, and Attention","url":"https://medium.com/@thienhn97/interpreting-deep-neural-networks-memorization-kernels-nearest-neighbors-and-attention-6bf0cefc7619","date":1740244549,"author":"/u/ThienPro123","guid":9301,"unread":true,"content":"<p>This means that our positive kernel is actually some inner product of a Hilbert space. Typically, Mercer‚Äôs theorem is used for the kernel trick where we can map our input data to richer feature spaces that are potentially infinite dimensional (e.g. Gaussian kernel, polynomial kernel, etc.). However, in our case, we will use it to interpret the other way around.</p><p>Note the following relationship between distance in the Hilbert space and the kernel function:</p><p>This means that the closer x is to y in H , the more similar they will be in our similarity measure. So our intuition of the similarity measure being related to some form of distance is formalized by the relationship above.</p><h2>Learned kernel instead of fixing a kernel a priori</h2><p>If something within our prediction model is not learnable, then it is a prior that we are imposing on the dataset and the problem.</p><p>In our previous discussions of soft-kernelized NNs, the kernel K is fixed, meaning that we have some prior on the geometry of the data. That is not always desirable and we want our methods to automatically learn the structure of the data rather than us manually imposing this geometry.</p><p>Hence, if we want to learn the kernel K instead of imposing a prior fixed kernel, we can write (due to Mercer‚Äôs theorem):</p><p>for some parameterized feature map œà : X ‚Üí H from the data space X to some Hilbert space H. Typically, H will just be R^n or the dimension of the latent space. We can then learn the parameters of œà via standard training techniques (i.e. gradient descent on some loss).</p><p>This view allows us to connect standard deep learning (or representation learning) to kernel learning.</p><h2>Interpreting attention as soft nearest neighbors</h2><p>Note that the soft kernelized nearest neighbor that we presented earlier</p><p>can be interpreted as the popular attention mechanism that is ubiquitous today in LLMs and LVMs via the transformers architecture.</p><p>If we interpret x as some token, e.g. x_c, in the sequence (x_1, ‚Ä¶, x_n), K(x_i , x) as the attention dot product i.e.</p><p>and setting W_{ci} as the normalized values for token at time i i.e.</p><p>then we would recover the attention computation (bidirectional or autoregressive depending on whether we set the W_{ci} = 0 for i &lt; c) as being the weighted average of the values of other tokens in the sequence.</p><p>The representer theorem states that there exists an optimal linear solution that lies in the span of the training data. We shall call span (œà(x_1), ‚Ä¶, œà (x_n)) the <em>training (examples) feature span.</em></p>","contentLength":2494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1ivnp1c/r_interpreting_deep_neural_networks_memorization/"},{"title":"i made a list of Tech EU tech projects! for users interested in privacy and sustainability","url":"https://github.com/uscneps/Awesome-European-Tech","date":1740244216,"author":"/u/uscnep","guid":9264,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivnk77/i_made_a_list_of_tech_eu_tech_projects_for_users/"},{"title":"I can play a game that wasn't meant to run on my PC, using Linux","url":"https://www.reddit.com/r/linux/comments/1ivn4kr/i_can_play_a_game_that_wasnt_meant_to_run_on_my/","date":1740243151,"author":"/u/Vousch","guid":9263,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gitoxide in February","url":"https://github.com/GitoxideLabs/gitoxide/discussions/1855","date":1740242874,"author":"/u/ByronBates","guid":9395,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ivn0m4/gitoxide_in_february/"},{"title":"I made an MMORPG playable with an API. Use any programming language to control your characters with the API.","url":"https://www.artifactsmmo.com/","date":1740241440,"author":"/u/Muigetsu","guid":9211,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivmgf2/i_made_an_mmorpg_playable_with_an_api_use_any/"},{"title":"windows firewall for local Go web app","url":"https://www.reddit.com/r/golang/comments/1ivmbtd/windows_firewall_for_local_go_web_app/","date":1740241115,"author":"/u/jeevanism","guid":9298,"unread":true,"content":"<p>Every time I start my Go web app locally on Windows, I get a firewall error (see screenshot). The Windows Firewall blocks it, and I have to manually allow access. Why does this keep happening? Is there a way to fix this permanently?</p><p>NB : I am unable to attach the screenshot here :( </p>","contentLength":282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to implement dynamic storage provisioning for onPrem cluster","url":"https://www.reddit.com/r/kubernetes/comments/1ivlitx/how_to_implement_dynamic_storage_provisioning_for/","date":1740239019,"author":"/u/Impossible_Nose_2956","guid":9237,"unread":true,"content":"<p>Hi I have setup Onprem cluster for dev qa and preprod environments onPrem.</p><p>And I use redis, rabbitmq, mqtt, sqlite(for celery) in the cluster. And all these need persistent volumes.</p><p>Without dynamic provisioning, i have to create a folder, then create pv with node affinity and then create pvc and assign it to the statefulset.</p><p>I dont want to handle PVs for my onPrem clusters.</p><p>What options are available?</p><p>Do let me know if my understanding of things is wrong anywhere. </p>","contentLength":464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DSBG, an open-source static site generator built with Go","url":"https://www.reddit.com/r/golang/comments/1ivl3o0/dsbg_an_opensource_static_site_generator_built/","date":1740237882,"author":"/u/tarjano","guid":9209,"unread":true,"content":"<p>This is my first big project built in Go, primarily to see if I could be as productive with it as I am with Python. I wanted to tackle a non-trivial project, so I aimed to include most of the functionality I envisioned for this type of tool. Here's what DSBG offers:</p><ul><li> Works with both Markdown and HTML source files.</li><li><strong>Automatic Tagging &amp; Filtering:</strong> Tags are generated from paths, with built-in tag filtering.</li><li><strong>Client-Side Fuzzy Search:</strong> Provides fast search over all content within the browser.</li><li><strong>Automatic RSS Feed Generation:</strong> Easily create RSS feeds for your blog.</li><li><strong>Watch Mode with Auto-Rebuild:</strong> For continuous feedback during development.</li><li> Includes 3 different themes, with the option to add your own custom CSS.</li><li> For major social networks.</li><li> Easily add analytics, comments, and more.</li></ul><p>The code might not be perfectly idiomatic, so any tips, suggestions, and feedback are very welcome!</p>","contentLength":870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is Saga Pattern in Distributed Systems?","url":"https://newsletter.scalablethread.com/p/what-is-saga-pattern-in-distributed","date":1740235399,"author":"/u/scalablethread","guid":9234,"unread":true,"content":"<p><a href=\"https://newsletter.scalablethread.com/i/146489166/consistency-and-consistency-model\" rel=\"\">data consistency</a></p><p>The Saga pattern is a design pattern that helps manage transaction updates across multiple services by breaking them down into a sequence of small local transactional updates, called \"saga steps\" or \"subtransactions.\" Each step represents a unit of work that interacts with a single service. Once a step is completed, it triggers the next step in the sequence. If any step fails, the saga executes compensating updates to undo the changes made by the previous steps, ensuring that the system returns to its initial state.</p><p>There are two main approaches to implementing the Saga Pattern: Orchestration and Choreography.</p><p>In this approach, a central orchestrator service coordinates the saga steps. The orchestrator tells each service when to execute its local transaction. It maintains the state of the saga and handles any failures by invoking compensating transactions. The orchestrator knows the entire saga flow.</p><p>The client initiates the saga by communicating with the orchestrator. The orchestrator then invokes the first service. Upon successful completion, the orchestrator moves to the next step, invoking the corresponding service. If a service fails, the orchestrator triggers compensating transactions in reverse order.</p><p>In the Choreography approach, there is no central coordinator. Instead, each service involved in the saga knows its role and communicates with the other services through events or messages. Each service listens for specific events and performs local transactions when the appropriate event is received. The saga flow is distributed across the services.</p><p>The client initiates the saga by communicating with the first service. This service performs its transaction and publishes an event. Other services, listening for this event, perform their respective transactions and publish their events. This chain reaction continues until the saga is complete. If a service fails, it publishes a compensating event, triggering other services to execute compensating transactions.</p><ul><li><p>Choreography has no single point of failure, as each service manages its part of the saga.</p></li><li><p>Orchestration provides simplified error handling and monitoring with centralized control. In contrast, each service needs to handle its errors in Choreography, which can lead to complex error-handling logic.</p></li><li><p>In Orchestration, the coordinator needs to know about all the services involved in the saga, which can lead to tight coupling. In contrast, in Choreography, services need to agree on the events and the order of transactions, which can lead to overhead in coordination.</p></li></ul><ul></ul><ul></ul><p><em>If you enjoyed this article, please hit the ‚ù§Ô∏è like button.</em></p><p><em>If you think someone else will benefit from this, then please üîÅ share this post.</em></p>","contentLength":2718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivk7x9/what_is_saga_pattern_in_distributed_systems/"},{"title":"[R] Calculating costs of fine tuning an Vision Language Model","url":"https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/","date":1740234081,"author":"/u/thekarthikprasad","guid":9235,"unread":true,"content":"<p>Hello guys, I need help in calculating the cost of fine-tuning a VL model.<p> My image dataset is of size 80+gb (</p><a href=\"https://huggingface.co/datasets/RussRobin/SpatialQA\">https://huggingface.co/datasets/RussRobin/SpatialQA</a>) The VL model is InternVL's 2B model<p> I am confused about whether to do a full parameter/QLoRA Finetuning.</p> I can't spend more on this, but wish to check the results.</p><p>If so I could, what would be the cost estimate, also how to estimate cost in general Can I sample the dataset, if it breaks my cost bound and still see the results?<p> Also do suggest the best and cheapest compute platform for my case.</p> Thanks in advance.</p>","contentLength":577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rustaceans, What are your thoughts on Gleam?","url":"https://www.reddit.com/r/rust/comments/1ivjcus/rustaceans_what_are_your_thoughts_on_gleam/","date":1740232848,"author":"/u/nikitarevenco","guid":9183,"unread":true,"content":"<p>I've been writing Rust for a couple months. I absolutely love its monads like Result and Option, pattern-matching, private-by-default, the friendly compiler and its type system. I took a quick look at Gleam and it seems to have those features as well. Its syntax heavily reminds me of Rust's, the major distinction is that Gleam is much higher level (No lifetimes, for example), and also it is a purely functional language. It is still relatively new.</p><p>For those who have tried it, what do you think about it? Are there situations where you will prefer Gleam over Rust and why. </p>","contentLength":576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Almost everyone is under-appreciating automated AI research","url":"https://www.reddit.com/r/artificial/comments/1ivja6c/almost_everyone_is_underappreciating_automated_ai/","date":1740232632,"author":"/u/MetaKnowing","guid":9233,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SystemV Filesystem Being Removed From The Linux Kernel","url":"https://www.phoronix.com/news/Removing-SystemV-Filesystem","date":1740229957,"author":"/u/unixbhaskar","guid":9181,"unread":true,"content":"<p>Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via <a href=\"https://twitter.com/MichaelLarabel\">Twitter</a>, <a href=\"https://www.linkedin.com/in/michaellarabel/\">LinkedIn</a>, or contacted via <a href=\"https://www.michaellarabel.com/\">MichaelLarabel.com</a>.</p>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ivifki/systemv_filesystem_being_removed_from_the_linux/"},{"title":"anyone tried kro for kubernetes resource management yet?","url":"https://www.reddit.com/r/kubernetes/comments/1ivhubw/anyone_tried_kro_for_kubernetes_resource/","date":1740227970,"author":"/u/AnnualRich5252","guid":9118,"unread":true,"content":"<p>i just came across this article on the new resource orchestrator for kubernetes called kro, and i think it's worth discussing here. for anyone who's been dealing with the ever-growing complexity of kubernetes deployments, kro could be a game changer. it simplifies how we manage and define complex kubernetes resources by grouping them into reusable units, making everything more efficient and predictable.</p><p>what i find cool about kro is that it focuses on making kubernetes resource management easier to handle, without needing the in-depth, advanced skills that most operators and devs have to rely on today. it's got this thing called a ResourceGraphDefinition (RGD) which essentially lets you define and manage resources as a unit, and it‚Äôs smart enough to figure out deployment sequences automatically based on dependencies. really takes the guesswork out of it.</p><p>it‚Äôs worth noting kro isn‚Äôt trying to replace helm or kustomize directly, but it definitely offers a more structured and predictable approach, with better handling of CRD upgrades and dependencies. while helm has been a go-to for packaging, kro's approach might be more useful for teams looking for a more secure, governed way to manage kubernetes resources at scale.</p><p>looking forward to hearing your thoughts!</p>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Efficiency Paradox: How to Save Yourself & the World ‚Ä¢ Holly Cummins","url":"https://youtu.be/dU_WHead0oY","date":1740227380,"author":"/u/goto-con","guid":9507,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivhol0/the_efficiency_paradox_how_to_save_yourself_the/"},{"title":"API Application Monitoring - OpenTelemetry? Or something else?","url":"https://www.reddit.com/r/golang/comments/1ivhm7y/api_application_monitoring_opentelemetry_or/","date":1740227138,"author":"/u/_nullptr_","guid":9180,"unread":true,"content":"<p>I am writing a few different gRPC and HTTP (via gRPC Gateway) API servers for various heavy financial compute/IO operations (trading systems and market data). I am doing this as a single developer. These are mostly for me as a hobbyist, but may become commercial/cloud provided at some point with a nice polished UI frontend.</p><p>Given the nature of the applications, I want to know what is \"going on\" and be able to troubleshoot performance bottlenecks as they arise, see how long transactions take, etc. I want to standardize the support for this into my apiserver package so all my apps can leverage and it isn't an afterthought. That said, I don't want some huge overhead either, but just want to know the performance of my app when I want to (and not when I don't). I do think I want to instrument with logs, trace and metrics after thinking what each would give me in value.</p><p>Right now I am leaning towards just going full OpenTelemetry knowing that it is early and might not be fully mature, but that it likely will over time. I am thinking I will use stdlib  for logs with Otel handler only when needed else default to basic stdout handler. Do I want to use otel metrics/tracing directly? I am also thinking I want these others sent to a  handler by default (even stdout is too much noise), and only to a collector when configured at runtime. Is that possible with the Go Otel packages? Does this seem like the best strategy? How does stdlib  play into this? or doesn't it? Other ideas?</p>","contentLength":1487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's a good combination of tools to get a proper application observation solution together?","url":"https://www.reddit.com/r/kubernetes/comments/1ivh9co/whats_a_good_combination_of_tools_to_get_a_proper/","date":1740225819,"author":"/u/tofagerl","guid":9117,"unread":true,"content":"<p>I work for a company with tons of k8s clusters, but they haven't really got the whole \"let's provide all the benefits of this to the product teams\" together yet, so we're stuck with a basic Grafana + Kibana package for now. That's fine, it works. </p><p>But since I used to work with Anthos, I got used to getting the full tracing benefits from Anthos Service Mesh, and I really miss having that. </p><p>So now I'd like to pressure the infra teams to provide something better for us, but I can't just say \"use Anthos Service Mesh\", because they are already running on GCS, so there'd be no point in using Anthos. Obviously they could use a normal Istio service mesh, but I'd like to know if there are easier solutions -- Service Meshes are complicated and come with serious drawbacks, and I'm really just looking for the observation layer, not the network security layer. </p><p>Keep in mind we prefer OSS solutions as a rule, and prefer non-managed solutions as a core philosophy because we believe in understanding each tool because we know it might break. </p>","contentLength":1038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI bans Chinese accounts using ChatGPT to edit code for social media surveillance","url":"https://www.engadget.com/ai/openai-bans-chinese-accounts-using-chatgpt-to-edit-code-for-social-media-surveillance-230451036.html","date":1740223463,"author":"/u/F0urLeafCl0ver","guid":9113,"unread":true,"content":"<p>OpenAI has banned the accounts of a group of Chinese users who had attempted to use ChatGPT to debug and edit code for an AI social media surveillance tool, the company <a data-i13n=\"cpos:1;pos:1\" href=\"https://openai.com/global-affairs/disrupting-malicious-uses-of-ai/\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:said Friday;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>. The campaign, which OpenAI calls Peer Review, saw the group prompt ChatGPT to generate sales pitches for a program those documents suggest was designed to monitor anti-Chinese sentiment on X, Facebook, YouTube, Instagram and other platforms. The operation appears to have been particularly interested in spotting calls for protests against human rights violations in China, with the intent of sharing those insights with the country's authorities.</p><p>\"This network consisted of ChatGPT accounts that operated in a time pattern consistent with mainland Chinese business hours, prompted our models in Chinese, and used our tools with a volume and variety consistent with manual prompting, rather than automation,\" said OpenAI. \"The operators used our models to proofread claims that their insights had been sent to Chinese embassies abroad, and to intelligence agents monitoring protests in countries including the United States, Germany and the United Kingdom.\"</p><p>According to Ben Nimmo, a principal investigator with OpenAI, this was the first time the company had uncovered an AI tool of this kind. \"Threat actors sometimes give us a glimpse of what they are doing in other parts of the internet because of the way they use our AI models,\" Nimmo told <a data-i13n=\"cpos:2;pos:1\" href=\"https://www.nytimes.com/2025/02/21/technology/openai-chinese-surveillance.html\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:The New York Times;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>.</p><p>Much of the code for the surveillance tool appears to have been based on an open-source version of one of Meta's <a data-i13n=\"cpos:3;pos:1\" href=\"https://www.engadget.com/ai/meta-says-llamas-usage-grew-tremendously-due-to-the-power-of-open-source-140020454.html\" data-ylk=\"slk:Llama models;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>. The group also appears to have used ChatGPT to generate an end-of-year performance review where it claims to have written phishing emails on behalf of clients in China.</p><p>\"Assessing the impact of this activity would require inputs from multiple stakeholders, including operators of any open-source models who can shed a light on this activity,\" OpenAI said of the operation's efforts to use ChatGPT to edit code for the AI social media surveillance tool.</p><p>Separately, OpenAI said it recently banned an account that used ChatGPT to generate social media posts critical of <a data-i13n=\"cpos:4;pos:1\" href=\"https://en.wikipedia.org/wiki/Cai_Xia\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:Cai Xia;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>, a Chinese political scientist and dissident who lives in the US in exile. The same group also used the chatbot to generate articles in Spanish critical of the US. These articles were published by \"mainstream\" news organizations in Latin America and often attributed to either an individual or a Chinese company.</p>","contentLength":2411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ivgo34/openai_bans_chinese_accounts_using_chatgpt_to/"},{"title":"I have KCA 50% coupun that i dont need, i will give to anyone who can give me aws aor redhat coupon in exchange?","url":"https://www.reddit.com/r/kubernetes/comments/1ivgiu6/i_have_kca_50_coupun_that_i_dont_need_i_will_give/","date":1740222832,"author":"/u/sabir8992","guid":9184,"unread":true,"content":"<div><p>If you have other exam i will give to anyone who can give me aws or Redhat coupon in exchange? DM ME Please</p></div>   submitted by   <a href=\"https://www.reddit.com/user/sabir8992\"> /u/sabir8992 </a>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distributed system courses in Rust?","url":"https://www.reddit.com/r/rust/comments/1ivgbko/distributed_system_courses_in_rust/","date":1740222035,"author":"/u/FeelingAttempt55","guid":9163,"unread":true,"content":"<p>I am currently following the <a href=\"https://github.com/pingcap/talent-plan/tree/master\">pingcap course</a> to learn distributed systems with Rust. So far, I am really enjoying the course, but the course is 5 years old, could you guys suggest some other project-based and more up-to-date courses? </p>","contentLength":233,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dockerize GO environment with only go.mod and go.sum and no source code","url":"https://www.reddit.com/r/golang/comments/1ivfxtb/dockerize_go_environment_with_only_gomod_and/","date":1740220358,"author":"/u/muthunatsharma","guid":9112,"unread":true,"content":"<p>I need a docker container which has go packages downloaded, installed and compiled as mentioned in go.mod and go.sum. All the articles show how to do it but the install/compile actually happens only when the source-code is copied in to the container and \"go build\" is run in the dockerfile.</p><p>I see \"go download\" downloads all pkgs in go.mod to /go/mod/pkg. How do I install these?<p> I can give \"go install &lt;pkg&gt;\" but that would mean I need to update my Dockerfile each time a new pkg is added to go.mod.</p></p><p>What is the one-shot way of installing it in the dockerfile build?</p><p>Edit: The context is to build a dev container where deps are pre-built saving time when code is mounted on the container and built -- this is the main point to save time. The container wouldn't have the app itself. Only the dependencies fully installed and serve as a standard environment to run.</p>","contentLength":861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"godoc.nvim - Golang docs inside Neovim!","url":"https://www.reddit.com/r/golang/comments/1ivfv16/godocnvim_golang_docs_inside_neovim/","date":1740220022,"author":"/u/ffredrikk","guid":9111,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PeaZip 10.3.0 released!","url":"https://www.reddit.com/r/linux/comments/1ivfn9y/peazip_1030_released/","date":1740219074,"author":"/u/peazip","guid":9368,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thought We Had Our EKS Upgrade Figured Out‚Ä¶ We Did Not","url":"https://www.reddit.com/r/kubernetes/comments/1ivf8u3/thought_we_had_our_eks_upgrade_figured_out_we_did/","date":1740217389,"author":"/u/rohit_raveendran","guid":9303,"unread":true,"content":"<p>You ever think you‚Äôve got everything under control, only for prod to absolutely humble you? Yeah, that was us.</p><ul><li>Lower environments? ‚úÖ Tested a bunch.</li><li>Version mismatches? ‚úÖ All within limits.</li><li>EKS addons? ‚úÖ Using the standard upgrade flow.</li></ul><p>So we run Terraform on upgrade day. Everything‚Äôs looking fine‚Äîuntil <strong>kube-proxy upgrade just straight-up fails.</strong> Some pods get stuck in  Great.</p><p>Cool, thanks, very helpful. We hadn‚Äôt changed anything on kube-proxy beyond the upgrade, so what the hell?</p><p>At this point, one of us starts frantically digging through the EKS docs while another engineer manually downgrades kube-proxy just to get things back up. That works, but obviously, we can‚Äôt leave it like that.</p><p>And then we find it: <strong>a tiny note in the AWS docs added just a few days ago.</strong> Turns out, <strong>kube-proxy 1.31 needs an ARMv8.2 processor with Cryptographic Extensions</strong> (<a href=\"https://github.com/awsdocs/amazon-eks-user-guide/blob/mainline/latest/ug/nodes/hybrid-nodes-os.adoc#arm\">link</a>).</p><p>And guess what Karpenter had spun up?  AWS confirmed that A1s are a no-go in EKS 1.31+. We updated our Karpenter configs to block them, ran the upgrade again, and boom‚Äîeverything worked.</p><ol><li><strong>You‚Äôre never actually prepared.</strong> We tested everything, but something always slips through. The real test is how fast you fix it.</li><li><strong>Karpenter is great, but don‚Äôt let it go rogue.</strong> We‚Äôre now explicitly blocking unsupported instance families.</li></ol><p>Anyway, if you guys have ever had one of those ‚Äúwe did everything right, and it still blew up‚Äù moments, drop your stories. Misery loves company.</p>","contentLength":1449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pewdiepie uses linux mint","url":"https://www.reddit.com/r/linux/comments/1ivf01w/pewdiepie_uses_linux_mint/","date":1740216313,"author":"/u/RedDevilVortex","guid":9059,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created a fairly extensive cheat sheet for scripting Sieve mail filters. Here's a link to the Gist if anyone is interested.","url":"https://gist.github.com/Hotrod369/6b7a24e1ea060e48e0c02459cbb950a0","date":1740214906,"author":"/u/StinkyPete312","guid":9210,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iveor2/i_created_a_fairly_extensive_cheat_sheet_for/"},{"title":"This is a minimalist 2-click MSI installer generator for your projects for Windows. Magic works as all you need is to populate _configMSI.yml with your own values, then click 2 bat or sh files (if you use MS Visual Studio or MSYS2/MINGW64). And voila, your MSI Installer is ready!","url":"https://github.com/windows-2048/Magic-MSI-Installer-Template","date":1740214284,"author":"/u/florida-haunted","guid":9081,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivejju/this_is_a_minimalist_2click_msi_installer/"},{"title":"Pixerise v0.12 Release: Python High-Performance 3D Renderer Adds Ray Casting, 1/z Depth Interpolation, and Group Management with Improved Architecture","url":"https://github.com/enricostara/pixerise","date":1740214075,"author":"/u/jumpixel","guid":9182,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivehr0/pixerise_v012_release_python_highperformance_3d/"},{"title":"Confused about \"NEW\" Rust feature in - Closures in async functions","url":"https://www.reddit.com/r/rust/comments/1ivdmek/confused_about_new_rust_feature_in_closures_in/","date":1740210338,"author":"/u/DataBora","guid":9302,"unread":true,"content":"<p>I am reading how new Rust feature is comming for using closures in Async functions. || async whaterver...</p><p>In my Elusion library implementation i have PipelineScheduler function signature:</p><pre><code> ```rust pub async fn new&lt;F, Fut&gt;(frequency: &amp;str, job: F) -&gt; ElusionResult&lt;Self&gt; where F: Fn() -&gt; Fut + Send + Sync + 'static, Fut: Future&lt;Output = ElusionResult&lt;()&gt;&gt; + Send + 'static ``` </code></pre><p>and then for Job creation:</p><pre><code>```rust let job = Job::new_async(&amp;cron, move |uuid, mut l| { let job_fn = job_fn.clone(); Box::pin(async move { let future = job_fn(); future.await.unwrap_or_else(|e| eprintln!(\"‚ùå Job execution failed: {}\", e)); let next_tick = l.next_tick_for_job(uuid).await; match next_tick { Ok(Some(ts)) =&gt; println!(\"Next job execution: {:?} UTC Time\", ts), _ =&gt; println!(\"Could not determine next job execution\"), } }) }).map_err(|e| ElusionError::Custom(format!(\"‚ùå Job creation failed: {}\", e)))?; ``` </code></pre><p>which user can use like this:</p><pre><code>let scheduler = PipelineScheduler::new(\"5min\", || async {}) </code></pre><p>How this new feature will be different?</p>","contentLength":1025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Evaluating LLM Knowledge Across 285 Graduate Disciplines: A Comprehensive Benchmark Using Human-LLM Collaborative Filtering","url":"https://www.reddit.com/r/MachineLearning/comments/1ivd069/r_evaluating_llm_knowledge_across_285_graduate/","date":1740207761,"author":"/u/Successful-Western27","guid":9236,"unread":true,"content":"<p>A new evaluation benchmark tests language models across 285 graduate-level disciplines using an iterative human-AI collaborative approach to generate and validate questions. The methodology combines expert review with model-assisted filtering to ensure high-quality, discipline-appropriate assessment.</p><p>Key technical points: - Uses a two-stage question generation process: initial AI generation followed by expert review - Implements collaborative filtering where both human experts and LLMs help identify and remove problematic questions - Covers disciplines from traditional academia to specialized industrial fields - Tests both factual knowledge and reasoning capabilities - Evaluated on multiple leading LLMs including GPT-4, Claude 2, and DeepSeek</p><p>Results: - Best performance: DeepSeek-R1 at 61.82% accuracy - Significant variance in performance across different disciplines - 80+ expert annotators involved in validation - Generated dataset of 2,855 validated questions</p><p>I think this benchmark addresses a critical gap in LLM evaluation by going beyond common academic subjects. The methodology of combining human expertise with AI assistance for question validation could be valuable for developing future evaluation datasets.</p><p>I think the relatively modest performance (62%) on graduate-level questions across diverse fields suggests current LLMs still have significant room for improvement in specialized domains. This could influence how we approach model training and evaluation for domain-specific applications.</p><p>TLDR: New benchmark tests LLMs across 285 graduate disciplines using human-AI collaborative question generation. Best model achieved 62% accuracy, revealing gaps in specialized knowledge.</p>","contentLength":1704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Plasma: Refinements All Around","url":"https://blogs.kde.org/2025/02/22/this-week-in-plasma-refinements-all-around/","date":1740207215,"author":"/u/gabriel_3","guid":9080,"unread":true,"content":"<p>Welcome to a new issue of \"This Week in Plasma\"! Every week we cover as much as possible of what's happening in the world of KDE Plasma and its associated apps like Discover, System Monitor, and more.</p><p>This week, we've been rapidly fixing the bugs that people found in Plasma 6.3, as well as some older bugs as well. In addition to that, some smaller UI improvements have started to trickle in! There's some larger work in progress too, but not merged yet. Have a look at what did merge this week:</p><p>Improved the weather widget's display of search results from the BBC weather service to reduce unhelpful visual noise. (Ismael Asensio, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500065\">link</a>)</p><p>Eliminated the visual difference between how Night Light looks on Wayland compared to on X11. (Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500300\">link</a>)</p><p>The Digital Clock widget's context menu is now less cluttered with things you're not likely to use. (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5139\">link</a>)</p><p>Rephrased some settings on System Settings' General Behavior page to be clearer about what it is that they actually do. (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2808\">link</a>)</p><p>Improved the accessibility of the Widget Explorer sidebar. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2807\">link</a>)</p><p>Fixed the issue mentioned last week where KWin built with LTO on GCC 15 could show a black screen on login when using an ICC profile; we found a way to restructure the code that avoids the issue. (Vlad Zahorodnii and Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499789\">link</a>)</p><p>Fixed a case where Plasma could crash when you tried to access the Properties dialog for a file in the Recently or Frequently Used file lists in the Kickoff Application Launcher. (Nicolas Fella, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499845\">link</a>)</p><p>Fixed a regression that caused the volume change OSD to fail to appear when adjusting the volume with the integrated volume buttons of a Bluetooth headset. (David Redondo, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500129\">link</a>)</p><p>Fixed a regression that caused the + clipboard popup to lose its visual highlights when navigated by keyboard. (Christoph Wolk, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500055\">link</a>)</p><p>Fixed an issue in KWin that caused the new \"Prefer efficiency\" option when using an ICC profile to not actually be very efficient on some hardware, and another one that broke Night Light while using the \"Prefer color accuracy\" setting. (Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499987\">link 1</a> and <a href=\"https://bugs.kde.org/show_bug.cgi?id=500404\">link 2</a>)</p><p>Taking screenshots on Wayland in FreeBSD now works. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500261\">link</a>)</p><p>Fixed a few bugs in the Color Picker widget, such as the shortcut option not working, and the tooltip not looking correct in certain circumstances. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/676\">link 1</a> and <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/677\">link 2</a>)</p><p>Fixed a bug with the Task Manager widgets that broke the ability to move the pointer diagonally to a tooltip without dismissing it by accident while using a right-to-left language like Arabic or Hebrew. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2792\">link</a>)</p><p>Made several improvements and fixes for keyboard navigation in the Kicker Application Menu widget. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2811\">link 1</a>, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2812\">link 2</a>, and <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2813\">link 3</a>)</p><p>Fixed a regression that caused desktop icons selected by dragging a box around them to become inappropriately deselected if the pointer ended right over one of the icons when releasing the mouse button. (David Edmundson, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499898\">link</a>)</p><p>Fixed a regression that caused the automatic tablet mode feature to accidentally get blocked on certain types of devices, but only when using the feature to re-bind mouse buttons. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500025\">link</a>)</p><p>Fixed a bug that caused the desktop and panels to go missing when applying a new Global Theme and using the option to replace the existing layout. This also fixed a bug that caused deleted widgets to not be deleted from the <code>plasma-org.kde.plasma.desktop-appletsrc</code> config file. (Marco Martin, <a href=\"https://bugs.kde.org/show_bug.cgi?id=498175\">link 1</a> and <a href=\"https://bugs.kde.org/show_bug.cgi?id=404641\">link 2</a>)</p><p>Fixed a set of subtle bugs in the implementation of the new \"prefer symbolic icons\" behavior of the System Tray that caused it to actually do the opposite, showing you colorful icons instead! (Nate Graham and David Redondo, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5227\">link 1</a> and <a href=\"https://invent.kde.org/frameworks/kiconthemes/-/merge_requests/175\">link 2</a>)</p><p>Extremely long weather station names no longer overflow and break the widget popup's layout. (Ismael Asensio, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/680\">link</a>)</p><p>The inline file renaming text field on the desktop is now colored correctly when using a mixed light/dark setup, as with Breeze Twilight. (Evgeniy Harchenko, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2829\">link</a>)</p><p>Limited the Power Management setting \"Change screen brightness\" to only take effect for built-in screens on battery-powered systems (e.g. laptops), which avoids certain timing-related brightness bugs for external monitors and makes the settings page less confusing. (Jakob Petsovits, <a href=\"https://bugs.kde.org/show_bug.cgi?id=498771\">link</a>)</p><p>Fixed an issue that could cause user switching from KRunner to behave strangely and eventually cause a crash. (David Edmundson, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500038\">link</a>)</p><p>Fixed an older regression that broke the \"highlight non-default settings\" features for pages in System Settings written using QtWidgets. The fact that this was overlooked for so long goes to show how few are left these days! (David Redondo, <a href=\"https://invent.kde.org/frameworks/kcmutils/-/merge_requests/257\">link</a>)</p><p>Switched KWin's render loop initialization code to use a more precise type of timer that should reduce frame drops. (Apostolos Dimitromanolakis, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500500\">link</a>)</p><p>When the  daemon crashes, now it automatically restarts itself in the background. (Bryan Liang, <a href=\"https://invent.kde.org/frameworks/kded/-/merge_requests/57\">link</a>)</p><p>KDE has become important in the world, and your time and contributions have helped us get there. As we grow, we need your support to keep KDE sustainable.</p><p>You can help KDE by becoming an active community member and <a href=\"https://community.kde.org/Get_Involved\">getting involved</a> somehow. Each contributor makes a huge difference in KDE ‚Äî you are not a number or a cog in a machine!</p><p>You don‚Äôt have to be a programmer, either. Many other opportunities exist:</p><p>You can also help us by <a href=\"https://kde.org/donate\">making a donation!</a> Any monetary contribution ‚Äî however small ‚Äî will help us cover operational costs, salaries, travel expenses for contributors, and in general just keep KDE bringing Free Software to the world.</p><p>Enter your email address to follow this blog and receive notifications of new posts by email.</p>","contentLength":5644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ivcuy0/this_week_in_plasma_refinements_all_around/"},{"title":"I Made a Configurable Rate Limiter‚Ä¶ Because APIs Can‚Äôt Say ‚ÄòChill‚Äô","url":"https://beyondthesyntax.substack.com/p/i-made-a-configurable-rate-limiter?r=4jgehp&amp;utm_campaign=post&amp;utm_medium=web&amp;triedRedirect=true","date":1740205486,"author":"/u/Sushant098123","guid":8987,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivcfct/i_made_a_configurable_rate_limiter_because_apis/"},{"title":"I have made a pong game in C++ using raylib. So can anyone plz give suggession where I can improve the game and my code?","url":"https://github.com/EthicalAniruddha/AI-Pong","date":1740204102,"author":"/u/Ethical_Aniruddha","guid":8986,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivc2qa/i_have_made_a_pong_game_in_c_using_raylib_so_can/"},{"title":"One-Minute Daily AI News 2/21/2025","url":"https://www.reddit.com/r/artificial/comments/1ivbt3a/oneminute_daily_ai_news_2212025/","date":1740203110,"author":"/u/Excellent-Target-847","guid":9262,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generic Bitfield I had fun implementing","url":"https://gist.github.com/oplanre/de0bba4f1e2f769458ca1adff7f12280","date":1740197539,"author":"/u/ln3ar","guid":9162,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iva8up/generic_bitfield_i_had_fun_implementing/"},{"title":"Announcing async-local 3.0 now with async closure support","url":"https://www.reddit.com/r/rust/comments/1iv8o6v/announcing_asynclocal_30_now_with_async_closure/","date":1740192335,"author":"/u/Jester831","guid":9060,"unread":true,"content":"<p>Async-local enables thread locals to be used in an async context across await points or within blocking threads managed by the Tokio runtime without the overhead of `Arc`. The way this is accomplished is by using <a href=\"https://crates.io/crates/generativity\">generativity</a> to create unique invariant lifetimes so that borrows to TLS can't be coerced to a `&amp;'static` lifetime and by configuring the runtime with a barrier to rendezvous worker threads during shutdown. This shutdown barrier makes it such that runtime tasks never outlive TLS data owned by worker threads; this makes every invariant lifetime guaranteed to be valid until no tasks remain. Blocking threads managed by the Tokio runtime cannot outlive worker threads with this configuration, and so pointers to TLS from worker threads can be safely moved to these blocking threads with the lifetime constrained. As the lifetimes cannot be coerced into `&amp;'static`, moving onto other threads is prevented. This crate downgrades to using `Arc` whenever the `barrier-protected-runtime` feature is not enabled, making it the end users choice to opt into this optimization by using async_local to configure the runtime shutdown barrier. </p>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"from nodejs want to move to golang","url":"https://www.reddit.com/r/golang/comments/1iv7ngg/from_nodejs_want_to_move_to_golang/","date":1740189188,"author":"/u/Spirited-Item1431","guid":8971,"unread":true,"content":"<p>I used to be a web developer who used Node.js as my daily programming language, but now I'm interested in switching to Golang. Aside from the usual fundamentals, what are the most important things to learn in Golang?</p>","contentLength":216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ring is unmaintained","url":"https://rustsec.org/advisories/RUSTSEC-2025-0007.html","date":1740185098,"author":"/u/technobicheiro","guid":8934,"unread":true,"content":"<p>This advisory has been withdrawn and should be ignored. It is kept only for reference.</p><p>The author has announced an indefinite hiatus in its development, noting that\nany reported security vulnerabilities may go unaddressed for prolonged periods\nof time.</p><p>After this advisory was published, the author graciously agreed to give\naccess to the rustls team. The rustls team is committed to providing\nsecurity (only) maintenance for  for the foreseeable future.</p><p>Advisory available under <a href=\"https://spdx.org/licenses/CC0-1.0.html\">CC0-1.0</a>\n    license.\n\n    \n    </p>","contentLength":508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iv6myf/ring_is_unmaintained/"},{"title":"[P] Decensor AI models Qwen/Deepseek by finetuning with non political data","url":"https://www.reddit.com/r/MachineLearning/comments/1iv6ckk/p_decensor_ai_models_qwendeepseek_by_finetuning/","date":1740184249,"author":"/u/Ambitious_Anybody855","guid":9114,"unread":true,"content":"<p>The best way to decensor a DeepSeek model? Don‚Äôt try to decensor it.</p><p>Fine-tuned OpenThinker on OpenThoughts-114k, a dataset focused on reasoning tasks like math, coding, and graduate-level Q&amp;A, with no political content. Despite using censored base models (Qwen), the fine-tuned OpenThinker-7B and OpenThinker-32B models became decensored without any explicit intervention. Unlike Perplexity, no custom fine-tuning was applied to remove censorship, yet the results remain uncensored. </p><p>It challenges assumptions about model safety and opens exciting new research directions. AI game is so on</p>","contentLength":590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Firefox's HEVC support for Linux (via VA-API) coming in Firefox 137","url":"https://www.reddit.com/r/linux/comments/1iv6bhi/firefoxs_hevc_support_for_linux_via_vaapi_coming/","date":1740184159,"author":"/u/neks101","guid":8955,"unread":true,"content":"<p>Windows got support in Firefox 134, MacOS on the Firefox beta build 136, and Linux will be on the Firefox nightly with 137. Looks like all OS will be supported by 137!</p>","contentLength":167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"First time on Linux, 3 gig ram and works like a rocket lol","url":"https://www.reddit.com/r/linux/comments/1iv5tas/first_time_on_linux_3_gig_ram_and_works_like_a/","date":1740182726,"author":"/u/SnooOpinions7428","guid":8898,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightweight Real-Time System Stats for VS Code","url":"https://marketplace.visualstudio.com/items?itemName=odangoo.otak-monitor","date":1740181177,"author":"/u/Wise_Bug47","guid":8942,"unread":true,"content":"<p align=\"center\">A lightweight system monitor for VS Code - Track CPU, memory, and disk usage with efficient 5-second updates and 1-minute averages.</p><ol><li>Find the system monitor in your VS Code status bar</li><li>View CPU usage percentage</li><li>Hover to see detailed current and average metrics</li></ol><p>otak-monitor is a lightweight VS Code extension that helps you monitor system resources without leaving your editor.</p><ul><li><ul><li>Status bar display of CPU usage percentage</li><li>Aggregated across all CPU cores</li><li>Precise to one decimal place</li><li>Current CPU clock speed (MHz)</li></ul></li><li><ul><li>Detailed memory information</li><li>Shows used and total memory in MB</li></ul></li><li><ul><li>Cross-platform disk space monitoring\n<ul><li>Windows: C: drive (home directory in Codespaces)</li><li>Linux: Root filesystem (workspace root in Codespaces)</li></ul></li><li>Shows used and total space in GB</li></ul></li><li><ul><li>Clean status bar integration</li><li>Detailed hover tooltip showing:\n<ul><li>Current CPU, memory, and disk metrics</li></ul></li></ul></li></ul><ul><li>Visual Studio Code ^1.90.0</li><li>Supported environments:\n<ul><li>Local: Windows, macOS, Linux</li><li>Remote: GitHub Codespaces</li></ul></li></ul><ol><li>Install the extension from VS Code Marketplace</li><li>Look for the CPU usage display in your status bar</li><li>Hover over it to see detailed system information</li></ol><p>The extension shows the following information in your status bar:</p><p>With a detailed tooltip showing:</p><pre><code>Current:\nCPU Usage: 45.3% (2400 MHz)\nMemory Usage: 1024 MB / 2048 MB (50.0%)\nDisk Usage: 150 GB / 500 GB (30.0%)\n</code></pre><p>Note: For disk usage, the monitored path varies by environment:</p><ul><li>Windows:\n<ul><li>Codespaces: Home directory</li></ul></li><li>Linux:\n<ul><li>Local: Root filesystem (/)</li><li>Codespaces: Workspace root</li></ul></li></ul><ul><li>CPU usage is calculated by comparing idle and total CPU time differences</li><li>Memory values are shown in MB and percentage</li><li>Disk usage monitoring adapts to the environment:\n<ul><li>Local machines: Monitors system root or C: drive</li><li>Codespaces: Monitors relevant workspace paths</li></ul></li><li>Moving averages are calculated using 12 data points (5-second intervals over 1 minute)</li><li>Updates occur every 5 seconds for efficient monitoring</li><li>Minimal performance impact on the system</li></ul><h2>GitHub Codespaces Support</h2><p>The extension automatically detects when running in GitHub Codespaces and adjusts its behavior:</p><ul><li>Monitors the workspace root directory in Linux environments</li><li>Uses home directory for Windows-based Codespaces</li><li>Maintains consistent monitoring experience across all environments</li><li>Provides accurate disk usage information for containerized development</li></ul><p>Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.</p><p>This project is licensed under the MIT License - see the <a href=\"https://github.com/tsuyoshi-otake-system-exe-jp/otak-monitor/blob/HEAD/LICENSE\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">LICENSE</a> file for details.</p>","contentLength":2481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iv592c/lightweight_realtime_system_stats_for_vs_code/"},{"title":"Best way to develop talos locally?","url":"https://www.reddit.com/r/kubernetes/comments/1iv4ttd/best_way_to_develop_talos_locally/","date":1740179989,"author":"/u/obviouslyGAR","guid":8877,"unread":true,"content":"<div><p>I am currently learning and building a cluster using talos.</p><p>One thing I want to know is how are you all developing locally? </p><p>Is using docker and using the command  the best way or is there another way that can be done like utilizing terraform?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/obviouslyGAR\"> /u/obviouslyGAR </a>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made an AirDrop server that uses URL Requests to accept data from anywhere","url":"https://github.com/gnhen/SkyDrop","date":1740179194,"author":"/u/GranttH","guid":8899,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iv4j4r/i_made_an_airdrop_server_that_uses_url_requests/"},{"title":"Rust Rant Contest: std::io::Error, the oversized junk drawer of failure","url":"https://www.reddit.com/r/rust/comments/1iv3rb3/rust_rant_contest_stdioerror_the_oversized_junk/","date":1740177182,"author":"/u/OliveTreeFounder","guid":8957,"unread":true,"content":"<p>I've been coding in Rust for five years, and  has never been anything but a headache. The error code? Never useful. It‚Äôs impossible to handle‚Äîtoo big, too vague‚Äîso we all end up just passing this bloated mess back to the caller without even knowing what‚Äôs inside or what actually caused the error.</p><p>But it gets worse. Traits, instead of being parameterized over an  type, just return <code>Result&lt;..., std::io::Error&gt;</code>. Once a trait like this becomes popular‚Äîlike  or ‚Äîyou're stuck. You can‚Äôt handle errors properly unless you rewrite every crate that depends on these traits.</p><p> is a contagious disease infecting the entire ecosystem. We need to stop this pandemic!</p>","contentLength":668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Matrix.org bridges to shut down in 1 month unless $100k can be raised","url":"https://matrix.org/blog/2025/02/crossroads/","date":1740174293,"author":"/u/Evidlo","guid":8876,"unread":true,"content":"<p>After a <a href=\"https://matrix.org/blog/2024/12/25/the-matrix-holiday-special-2024/\">successful 2024 with a lot to be proud of</a>, and a Matrix Conference that brought our community together to celebrate 10 years of Matrix, we step into 2025 with a light budget and a mighty team poised to make the most of it!</p><p>Our priorities remain to make Matrix a safer network, keep growing the ecosystem, make the most of our Governing Board, and drive a fruitful and friendly collaboration across all actors.</p><p>However, whether we will manage to get there is not fully a given.</p><h2><a href=\"https://matrix.org/blog/2025/02/crossroads/#the-foundation-is-key-to-the-success-of-matrix\" aria-label=\"Anchor link for: the-foundation-is-key-to-the-success-of-matrix\">üîó</a>The Foundation is key to the success of Matrix</h2><p>The Matrix.org Foundation has gone from depending entirely on Element, the company set up by the creators of Matrix, to having half of its budget covered by its <a href=\"https://matrix.org/support/\">11 funding members</a>, which is a great success on the road to financial independence! However half of the budget being covered means half of it isn‚Äôt. Or in other words: the Foundation is not yet sustainable, despite running on the strictest possible budget, and is burning through its (relatively small) reserves. And we are at the point where the end of the road is in sight.</p><blockquote><p>The Matrix.org Foundation exists to act as a neutral  and to nurture it as efficiently as possible as <strong>a single unfragmented standard, for the greater benefit of the whole ecosystem</strong>, not benefiting or privileging any single player or subset of players.</p></blockquote><p>Without the Foundation and its programs, the Matrix protocol itself faces existential threats:</p><ul><li>Without Trust &amp; Safety efforts, bad actors and communities would proliferate on the network and make it unlivable for the rest.</li><li>Without a canonical specification, the shared infrastructure and a Spec Core Team to maintain it, the protocol would become fragmented, losing its effective interoperability ‚Äì increasing the costs on all downstream users.</li><li>Without a neutral entity as the custodian of the specification, the ecosystem would first shatter and then consolidate around the biggest (likely for-profit) actor.</li><li>Without advocacy, conferences, documentation and tutorials, Matrix would become a niche protocol used by a few enthusiasts for side projects, whilst big proprietary and siloed networks continue to hold the world‚Äôs communications.</li></ul><p>But there is light at the end of the tunnel! Concretely, the Foundation delivers most of its value by fostering a healthy, fair and fertile ecosystem around Matrix. It needs to strike the right balance between:</p><ul><li><strong>Making Matrix accessible &amp; visible.</strong><ul><li>For the general public it means maintaining an easy default onboarding server (Matrix.org).</li><li>For server administrators it means providing the right tooling to keep their users (and themselves!) safe.</li><li>For developers it means making it easy to develop products using Matrix, via documentation, tutorials, and in-person events.</li></ul></li><li><strong>Making Matrix compelling to build on.</strong><ul><li>This means maintaining the Matrix Specification as a canonical, unencumbered, patent free and royalty free specification.</li><li>Being responsive and vendor-neutral when an organisation or individual contributes.</li><li>Promoting the good players within the ecosystem.</li><li>Ensuring the network grows and attracts more users.</li></ul></li><li><strong>Making Matrix a product that benefits the greater good.</strong><ul><li>This means ensuring that the general public can easily build safe &amp; easy to use communities on Matrix.</li><li>Ensuring that bad actors are proactively chased and discouraged to use Matrix.</li></ul></li></ul><p>Matrix has been here for 10 years, and will hopefully be here for many more! But to continue to grow and thrive, it needs the Foundation to be around and healthy, which means carefully allocating its budget in order to continue to exist and fulfill its mission. This is why it needs to focus on critical programs and shut down some of its activities.</p><p>We view the following programs as critical to the Foundation‚Äôs mission:</p><ul><li>Maintaining the canonical, backwards compatible, stable <a href=\"https://spec.matrix.org/latest/\">Matrix Spec</a></li><li>Developing protocol enhancements and Trust and Safety tooling, making the tools available to the ecosystem and moderating the servers under its control (typically Matrix.org) - <a href=\"https://matrix.org/blog/2025/02/building-a-safer-matrix/\">see our recent blog post</a></li><li>Running the Matrix.org homeserver as an initial home for newcomers</li><li>Promoting the Matrix protocol via online content, conferences and meet-ups and other marketing strategies</li></ul><p>We might fine tune our approach, but we can't cease any of those programs without severe consequences for the ecosystem.</p><p>Meanwhile, bridges have been at the heart of Matrix for a long time. Public bridges hosted by the Matrix.org Foundation have been a very good resource to show the power of interoperability, connect communities together, and onboard many people into their Matrix journey.</p><p>However, these bridges require regular maintenance as the bridged platforms evolve their APIs, and significant engineering and moderation support to run. Luckily, the Matrix ecosystem is now more mature than it was at the time we spun up those public Slack, XMPP and IRC bridge instances. There are now commercial players like <a href=\"https://www.beeper.com/\">Beeper</a> providing a user-friendly offering for people who want to get all their conversations in a single app, or <a href=\"https://indiehosters.net/\">IndieHosters</a> and <a href=\"https://www.fairkom.eu/\">Fairkom</a> offering hosting for Matrix server and bridge instances (and much more).</p><p>So unless the Foundation manages to raise $100,000 of funding by the end of March 2025, we will have to focus our resources on the critical lines of work, and consequently <strong>we will have to shut down all the remaining bridges hosted by the Matrix.org Foundation. This includes bridges to Slack, XMPP, OFTC (IRC), and Snoonet (IRC).</strong> We will also mark the software behind those bridges as archived, as we don't have the resources to accept new contributions.</p><p>In practice, the Foundation needs an additional $610K in revenue to break-even, but this $100K would extend our runway 1 month while we work on landing grants and new members. To put this in context, we nearly doubled our revenue in 2024, reaching $561K, but it was also the first year in which we carried the full cost of our operations: $1.2M. To make ends meet, we liquidated $283K worth of cryptocurrency donations and ended the year with a $356K deficit. We are currently on target for $587K revenue in 2025, with a modest increase in expenses.</p><h2><a href=\"https://matrix.org/blog/2025/02/crossroads/#growing-the-ecosystem-and-the-network\" aria-label=\"Anchor link for: growing-the-ecosystem-and-the-network\">üîó</a>Growing the ecosystem and the network</h2><p>Choosing to shut the bridges down is a difficult decision to make, but will allow us to focus on the critical projects which will keep the ecosystem growing. The success of Matrix depends on how widely it is used by the general public and by organisations ‚Äì preferably natively rather than via bridges.</p><p>The more people and organisations rely on Matrix, the more attractive it becomes for organisations to build products and services on top of it, the more funding the Foundation gets, and the more the Foundation can in turn reinvest into the ecosystem and run initiatives that benefit all stakeholders for the growth of the network.</p><p>Once the Foundation is cashflow positive, it will be able to accelerate and eventually get on with the multiple projects the team and Governing Board have in mind to make Matrix fun, exciting, reliable, safe, easy to use, and above all useful. And we hope to get there by the end of the year.</p><p>Most importantly, despite the Trust and Safety team being the Foundation‚Äôs biggest expense, as explained in <a href=\"https://matrix.org/blog/2025/02/building-a-safer-matrix/\">our blog post</a>, the team is still underresourced: they are understaffed and under a lot of pressure to deliver protocol improvements, better tooling for server admins, and ensure Matrix.org is a good citizen of the open federation. <strong>T&amp;S will be the first area to see increased funding.</strong></p><p>Separately, the Foundation wants to continue executing on its mission! Among others, better connect the doers in the ecosystem with the people and organisations who need their energy, share the successes and learnings from the community: the Matrix Conference was an incredible success and we want to see more of that.</p><p>We‚Äôve also seen a clear change in how many users and organisations were adopting Matrix in the last few months: the world needs a decentralised end-to-end encrypted network to communicate more than ever, and it shows! We want to uplift the good players which are driving this growth.</p><p>There is so much more that we could do to make Matrix better and realise its full potential. </p><p>Right now, the Foundation urgently needs <a href=\"https://matrix.org/support/\">your financial help</a>. For the sake of a safe network, our primary focus today, but also to be able to deliver on the reason we all want Matrix to succeed.</p><ul><li>People should have full control over their own communication.</li><li>People should not be locked into centralised communication silos, but instead be free to pick who hosts their communication without limiting who they can reach.</li><li>The ability to converse securely and privately is a basic human right.</li><li>Communication should be available to everyone as a free and open, unencumbered, standard and global network.</li></ul><p><strong>If you are an organisation building on top of Matrix</strong>, you can help by , which also gives you the opportunity to be eligible to participate in the Governing Board, and other perks. </p><p><strong>If you are an organisation buying Matrix services or products</strong>, you can help by <strong>ensuring that your vendor is financially contributing back to the project</strong> or becoming a member yourself.</p><p><strong>If you are an individual using Matrix,</strong> you can help by .</p><p><strong>If you are a philanthropist or other funder</strong>, you can help by getting in touch with us at <a href=\"https://matrix.org/cdn-cgi/l/email-protection#c1a7b4afa5a8afa681aca0b5b3a8b9efaeb3a6\"></a> to discuss funding options. </p><p>It isn‚Äôt the <a href=\"https://matrix.org/blog/2022/12/01/funding-matrix-via-the-matrix-org-foundation/\">first</a><a href=\"https://matrix.org/blog/2024/04/open-source-publicly-funded-service/\">time</a> we‚Äôve rung the alarm bell, and it is no fun to beg for help. We are at a crossroads, where the vibrancy of the ecosystem and enthusiasm around Matrix is not reflected in the support the Foundation gets, and we are at risk of losing this common resource and all it offers.</p><p>But all in all, we are optimists ‚Äì we wouldn‚Äôt have begun this journey if we weren‚Äôt ‚Äì and we believe that there are people out there who realise that sovereign and secure communication is as high on the list of today‚Äôs essential technology ‚Äì if not higher ‚Äì as ensuring AI is safe, so let‚Äôs spread the word and let‚Äôs continue working on a safer and more sovereign world!</p><div><div><p>\n                        The Matrix.org Foundation is a non-profit and only relies\n                        on donations to operate. Its core mission is to maintain\n                        the Matrix Specification, but it does much more than that.\n                    </p><p>\n                        It maintains the matrix.org homeserver and hosts several\n                        bridges for free. It fights for our collective rights to\n                        digital privacy and dignity.\n                    </p><a href=\"https://matrix.org/support\">Support us</a></div></div>","contentLength":10479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iv2mr3/matrixorg_bridges_to_shut_down_in_1_month_unless/"},{"title":"Reading the Source Code","url":"https://www.reddit.com/r/kubernetes/comments/1iv1def/reading_the_source_code/","date":1740171145,"author":"/u/TopNo6605","guid":8856,"unread":true,"content":"<p>Curious does anyone have any advice or vids/blogs/books that go through the source code of k8s? I'm the type of person who likes to see what's happening under the hood. But k8s is a beast of an application. I was reading the apiserver source and got up the point where it's creating handlers and doing something with an openapi controller...which I didn't know existed.</p><p>Fascinating stuff but the amount of abstraction here is what gets me. Everything is an interface and abstracted to some other file, you end up following a long chain only to end up at an interface function without a definition. I get it, for development purposes. But man it's a beast to learn.</p><p>With the apiserver I literally just started logging when functions were called but I had to take a break after 4 hours of that. How do knew contributors get brought up to speed?</p>","contentLength":840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tk9.0 canvas demo","url":"https://opu.peklo.biz/p/25/02/21/1740170028-43ac6.png","date":1740170311,"author":"/u/0xjnml","guid":8897,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iv11dh/tk90_canvas_demo/"},{"title":"Windows to Linux, Set Up Full Disk Encryption on openSUSE","url":"https://news.opensuse.org/2025/02/20/setup-fde-on-opensuse/","date":1740170160,"author":"/u/gabriel_3","guid":9079,"unread":true,"content":"<p>Data breaches and cyber threats are becoming increasingly common and securing your personal and professional information has never been more critical.</p><p>Users transitioning from <a href=\"https://news.opensuse.org/2024/11/26/transition-from-windows-step-by-step/\">Windows to Linux</a> through the Upgrade to Freedom campaign can use <a href=\"https://get.opensuse.org/\">openSUSE</a>‚Äôs tools to protect sensitive data, which include full disk encryption (FDE).</p><p>Full disk encryption during installation ensures maximum security. It safeguards all data on your hard drive by encrypting it and makes it unreadable without an decryption key. This level of protection is vital for preventing unauthorized access if your laptop or desktop is lost or stolen.</p><p>FDE with openSUSE is both user-friendly and powerful. The setup with advanced security features is easy.</p><p>For users seeking feature parity with Windows BitLocker, openSUSE offers Full Disk Encryption (FDE) secured by a TPM2 chip or a FIDO2 key. This advanced setup enhances security by storing encryption keys within the TPM, which ensures that only a trusted system configuration can unlock the disk. For a step-by-step guide on enabling this feature, read the <a href=\"https://news.opensuse.org/2024/09/20/quickstart-fde-yast2/\">Quickstart in Full Disk Encryption with TPM and YaST2</a> article.</p><p>Here‚Äôs a step-by-step guide to set up FDE on your system:</p><p><strong>Step 1: Download and Boot openSUSE</strong></p><ul><li>Visit <a href=\"https://get.opensuse.org/\">get.opensuse.org</a> to download the latest version of openSUSE Leap or Tumbleweed.</li><li>Restart your computer and boot from the USB drive to begin the installation process.</li></ul><p><strong>Step 2: Configure Encryption During Installation</strong></p><ul><li>Once the installer starts, select your preferred language and keyboard layout.</li><li>In the partitioning setup, choose Guided Setup with Encrypted LVM.</li><li>Set a strong passphrase for encryption. This passphrase will be required every time the system boots.   - Use a mix of upper and lower case letters, numbers and special characters for optimal security.</li><li>Proceed with the installation as directed by the installer.</li></ul><p><strong>Step 3: Verify Encryption Settings</strong></p><p>After installation is complete and the system restarts, you‚Äôll be prompted to enter your encryption passphrase. Once entered, openSUSE tools will decrypt the disk and boot normally. To confirm encryption is active:</p><ul><li>Open a terminal or console.</li><li>Run the command  to verify that your disk is listed with the encryption type (e.g., ).</li></ul><p>The output might look something similar to the following:</p><div><div><pre><code>NAME        FSTYPE      FSVER LABEL UUID                                   FSAVAIL FSUSE% MOUNTPOINT\nsda                                                                                     \n‚îú‚îÄsda1      ext4        1.0     4a83v1e1-e8d2-4e38-815d-fd79j194f5   25G    30%    /\n‚îî‚îÄsda2      swap        1           d2e18c23-9w4b-4d26-p1s2-cm2sd64tx9de                \nsdb                                                                                     \n‚îî‚îÄsdb1      crypto_LUKS 1           10bb2vca-81r4-418b-a2c4-e0f6585f2c7a                \n  ‚îî‚îÄluks    ext4        1.0         8a9wka1b-7e9c-1a1f-a9f7-3c82x1e4e87f   150G    10%    /mnt/data\n</code></pre></div></div><p>While FDE protects your data, it does not prevent data loss from hardware failure or accidental deletion. Regularly back up your data to an encrypted external drive or a secure cloud service to ensure its safety.</p><p><strong>Enhanced Security for Modern Challenges</strong></p><p>Setting up full disk encryption on openSUSE not only protects your data but also aligns with the Upgrade to Freedom campaign‚Äôs mission of empowering users to maintain control over their hardware and privacy. By combining open-source software with good security practices, openSUSE ensures that users can confidently embrace a more secure digital future.</p><p>For additional guidance and community support, visit the <a href=\"https://forums.opensuse.org/\">openSUSE forums</a> or join discussions at your local Linux user group.</p><p><small> Please be aware that some hardward configurations may require additional drivers or BIOS settings adjustments for full disk encryption to fully function properly. Check your device‚Äôs compatibility and update your firmware before proceeding. </small></p>","contentLength":3905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iv0z6q/windows_to_linux_set_up_full_disk_encryption_on/"},{"title":"Streamline Kubernetes Management with Rancher","url":"https://youtube.com/shorts/fOVTDobiwIE?feature=share","date":1740167919,"author":"/u/abhimanyu_saharan","guid":8833,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iv0357/streamline_kubernetes_management_with_rancher/"},{"title":"[First crate] derive_regex: construct a type by parsing a string with regular expressions","url":"https://www.reddit.com/r/rust/comments/1iuzg1i/first_crate_derive_regex_construct_a_type_by/","date":1740166287,"author":"/u/TitaniumBrain","guid":9116,"unread":true,"content":"<p>I had an idea and decided it was simple enough to publish <a href=\"https://crates.io/crates/derive-regex\">my first crate</a> and contribute to the Rust ecosystem.</p><p>I'm still relatively new to Rust (coming from a few years of Python but I fell in love with the language), so any feedback is welcome. I'm confident my code isn't , but I want to make sure I follow best practices and learn about any Rust .</p><p>Using this crate - and the associated derive proc macro - you can derive  on an enum or struct to automatically derive the  constructor method.</p><p>Copied from the readme, here's a couple examples if you don't to click away from Reddit:</p><p>```rust use derive_regex::FromRegex;</p><pre><code>pattern = r\"^(?P&lt;timestamp&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?P&lt;level&gt;[A-Z]+)\\] (?P&lt;message&gt;.+)$\" </code></pre><p>)] struct LogEntry { timestamp: String, level: String, message: String, }</p><p>fn main() { let log = \"2025-02-20 15:30:00 [INFO] Server started successfully\"; let entry = LogEntry::parse(log).expect(\"Failed to parse log entry\"); println!(\"Parsed log entry: {:#?}\", entry); // Parsed log entry: LogEntry { // timestamp: \"2025-02-20 15:30:00\", // level: \"INFO\", // message: \"Server started successfully\", // } } ```</p><p>```rust use derive_regex::FromRegex;</p><p>enum CookingCommand { // Parses a command like \"chop 3 carrots\" #[regex(pattern = r\"chop (?P&lt;quantity&gt;\\d+) (?P&lt;ingredient&gt;\\w+)\")] Chop { quantity: u32, ingredient: String },</p><pre><code>// Parses a command like \"boil for 10 minutes\" #[regex(pattern = r\"boil for (?P&lt;minutes&gt;\\d+) minutes\")] Boil(u32), // Parses a command like \"bake at 375.0 degrees for 25 minutes\" #[regex(pattern = r\"bake at (?P&lt;temperature&gt;\\d+\\.\\d+) degrees for (?P&lt;minutes&gt;\\d+) minutes\")] Bake { temperature: f64, minutes: u32 }, // Parses a command like \"mix salt and pepper\" #[regex(pattern = r\"mix (?P&lt;ingredient1&gt;\\w+) and (?P&lt;ingredient2&gt;\\w+)\")] Mix { ingredient1: String, ingredient2: String, }, </code></pre><p>fn main() { let commands = [ \"First, chop 3 carrots\", \"Don't forget to boil for 10 minutes\", \"I guess I'll bake at 375.0 degrees for 25 minutes\", \"mix salt and pepper now\", ];</p><pre><code>for cmd in &amp;commands { if let Ok(command) = CookingCommand::parse(cmd) { match command { CookingCommand::Chop { quantity, ingredient, } =&gt; { println!(\"Chop {} {}(s)\", quantity, ingredient); } CookingCommand::Boil(minutes) =&gt; { println!(\"Boil for {} minutes\", minutes); } CookingCommand::Bake { temperature, minutes, } =&gt; { println!(\"Bake at {} degrees for {} minutes\", temperature, minutes); } CookingCommand::Mix { ingredient1, ingredient2, } =&gt; { println!(\"Mix {} and {}\", ingredient1, ingredient2); } } } else { eprintln!(\"Failed to parse command: {}\", cmd); } } // Chop 3 carrots(s) // Boil for 10 minutes // Bake at 375 degrees for 25 minutes // Mix salt and pepper </code></pre>","contentLength":2667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interop 2025: another year of web platform improvements","url":"https://web.dev/blog/interop-2025?hl=en","date":1740163542,"author":"/u/feross","guid":8956,"unread":true,"content":"<p>\n  Published: February 13, 2025\n</p><p>After the huge success of Interop 2024, the project returns today with a new set\nof focus areas for 2025. While we couldn't include every suggestion made this\nyear, the final list reaches across the web platform‚Äîfrom CSS to\nperformance-related features.</p><ul></ul><p>In addition, and as in previous years, there's a set of areas for investigation.\nThese are areas where we don't have enough information or tests to include as a\nfocus area, but the group feels some work should be done to get them to a stage\nwhere we can include them.</p><ul></ul><p>We're excited about all of these features and the improvements this year's\nproject will bring to the platform. And, as with <a href=\"https://web.dev/blog/interop-2024-wrapup\">last\nyear</a>, the project will make a whole set of things\nBaseline Newly available. This post shares more information about some of the\nfeatures on the list, with links to information to find out more.</p><p>Several of the features included in Interop 2025 are features that you flagged\nup as important in the State of CSS 2024 survey. They'll help you create more\nbeautiful and performant user experiences.</p><p>This feature lets you anchor a positioned element to an anchor, it's\nparticularly useful when displaying popovers.</p><h3 data-text=\"Same-document view transitions\" tabindex=\"-1\">Same-document view transitions</h3><p>Also included this year are view transitions, specifically same-document view\ntransitions, and the  CSS property.</p><h3 data-text=\"The backdrop-filter property\" tabindex=\"-1\">The  property</h3><p>The\n<a href=\"https://developer.mozilla.org/docs/Web/CSS/backdrop-filter\"></a>\nproperty has been Baseline Newly available since September 2024. It lets you\ncreate effects behind your content. For example to blur or create effects that\nyou might expect to only be available in a graphics application.</p><p>Despite being mostly interoperable, you can see from <a href=\"https://wpt.fyi/results/css/filter-effects?label=experimental&amp;label=master&amp;aligned&amp;q=backdrop-filter\">the failing tests for\n</a>that\nthere are bugs and issues in those implementations. While these issues might not\nbe a problem to everyone, we know that many of you do run into them, it'll be\ngreat to get this feature working really well.</p><p>The  element is a disclosure widget which can be expanded to reveal\nadditional content. The  element itself is Baseline Widely available.\nHowever, there are a number of related features that have been more recently\nadded <a href=\"https://developer.chrome.com/blog/styling-details\">that make  more\nuseful</a>.</p><ul><li>The  and  CSS pseudo-elements.</li><li>Using  to toggle the content instead of .</li><li>Auto-expanding the  element with find-in-page matches.</li><li>The  attribute, which hides an element until it is found\nusing the browser's find-in-page search or it is directly navigated to by\nfollowing a URL fragment.</li></ul><p>The  at-rule lets you scope your selectors to a sub-tree of the DOM, or\neven select between an upper and lower boundary in the tree. For example, the\nfollowing CSS only selects  elements inside an element with a class of\n.</p><p>In the next example, an upper and lower bound is used. The  element is\nonly selected if it's between the element with a class of  and also\noutside of the element with a class of .</p><p>Without the  event, there's no reliable way to detect that a scroll is\ncomplete. The best you could do is to use  to check if the scroll\nhas stopped for a period. This makes it more like a scroll has paused event, not\na scroll has ended event.</p><p>With the  event, the browser does all this difficult evaluation for\nyou.</p><h3 data-text=\"The text-decoration property\" tabindex=\"-1\">The  property</h3><p>The\n<a href=\"https://developer.mozilla.org/docs/Web/CSS/text-decoration\"></a>\nproperty is a shorthand for , ,\n, and <code translate=\"no\" dir=\"ltr\">text-decoration-thickness</code>. It's deemed Baseline\nWidely available, however in Safari the only unprefixed shorthand property that\nworks is . It's this that will be addressed during 2025.</p><p>The CSS <a href=\"https://developer.mozilla.org/docs/Web/CSS/writing-mode\"></a>\nproperty has a number of possible values, many of which are designed to lay out\nscripts that display vertically. Sometimes however, you want to lay out text\nvertically as part of a design, rather than for language support reasons. The\n and  values are designed for this, but have suffered\nfrom poor browser compatibility. This should be fixed during 2025.</p><p>In addition, the logical CSS properties  and \nare included. These make it possible to control what happens when content\noverflows boxes, regardless of the writing mode.</p><p><a href=\"https://web.dev/explore/learn-core-web-vitals\">Web Vitals</a> can help you quantify the\nexperience of your site and identify opportunities to improve. The Web Vitals\ninitiative aims to simplify the landscape, and help sites focus on the metrics\nthat matter most, the Core Web Vitals.</p><h4 data-text=\"Event Timing API (for INP)\" tabindex=\"-1\">Event Timing API (for INP)</h4><p>This year, the work will focus on the following features:</p><ul><li>JavaScript string builtins: to make the WebAssembly built-in string\nfunctions mirror a subset of the JavaScript String API so it can be callable\nwithout JavaScript glue code.</li><li>Resizable buffers integration: to integrate WebAssembly into JavaScript code\nthat uses resizable buffers.</li></ul><p>This year the project includes a removal from the platform. <a href=\"https://developer.mozilla.org/docs/Web/API/MutationEvent\">Mutation\nevents</a> are deprecated\nand replaced with the much more performant and Baseline Widely available\n<a href=\"https://developer.mozilla.org/docs/Web/API/MutationObserver\">Mutation Observer\nAPI</a>. Chrome\nremoved these events in Chrome 126, and this focus area is to remove them from\nall browsers.</p><p>Descriptions of the full list of features can be found in the project <a href=\"https://github.com/web-platform-tests/interop/blob/main/2025/README.md\">README</a>.\nAlso, read the posts from the other companies working on Interop 2025.</p>","contentLength":4896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuybup/interop_2025_another_year_of_web_platform/"},{"title":"COSMIC Alpha 6: Big Leaps Forward","url":"https://blog.system76.com/post/cosmic-alpha-6-big-leaps-forward","date":1740163210,"author":"/u/Schnurres","guid":8781,"unread":true,"content":"<div data-v-7773bbb7=\"\"><p>Our COSMIC mission continues! This month, we finished up some essential features and fixes in preparation for the upcoming beta alongside some amazing COSMIC contributors. Check out what‚Äôs new in Alpha 6, and make sure you‚Äôre fully updated to see these changes for yourself!</p><p>Desktop Zoom can now be activated in Settings &gt; Accessibility, from the Accessibility applet in the panel, or using the shortcuts Super + =, Super + -, or Super + Mouse Scroll.</p></div><div data-v-7773bbb7=\"\"><p>More <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-greeter/issues/40\">accessibility features</a> in the books! Clicking the Accessibility icon at login gives you access to various settings toggles for:</p><ul><li>Reads on-screen text aloud</li><li> Scrolling up while holding Super <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/issues/853\">magnifies</a> the region of the screen where your cursor is located</li></ul><p>Navigates you to Accessibility Settings</p></div><div data-v-7773bbb7=\"\"><p>Additional accessibility features for high-contrast, color inversion and various color filters for colorblindness are being worked on soon.</p><p><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/547\">Desktop view</a> is now supported in COSMIC. Right-clicking an empty desktop and selecting ‚ÄúDesktop view options‚Äù opens a settings window for your desktop. Show or hide desktop folders, drives, or the Trash; you can also adjust icon size and spacing between icons from this window. A fix for the window appearing below other windows will arrive in a later update. Files and folders can also be <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/597\">dragged</a> between the desktop view and COSMIC Files.</p></div><div data-v-7773bbb7=\"\"><p><strong>Additional Scaling Options</strong></p><p>An additional scaling setting has been added to scale the screen slightly, from 5% to 20% larger. For example, on a display set to 125% scaling, those desiring larger text can use this new setting to increase scaling to 130%, 135%, 140%, or 145%.</p></div><div data-v-7773bbb7=\"\"><p>Workspaces received some updates to really make the feature sparkle. For starters, you can now <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/34\">scroll between workspaces</a> in the overview as a quick and easy way of navigating to your intended destination. Clicking on the preview of the current workspace or empty space in the workspace overview will allow you to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/49\">exit the workspace view</a>.</p><p>Previews for horizontal workspace now include name and number. Workspace previews on rotated displays will show the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/17\">correct orientation</a>. The last workspace is now removed if it follows another <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/83\">empty workspace</a>.</p></div><div data-v-7773bbb7=\"\"><p>Additionally, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/89\">minimized windows</a> can now be dragged and dropped between workspaces, while windows can be moved to another display by dropping them in the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/53\">workspace overview</a>. Dragging a window <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/41\">out of a stack</a> will match expectations and no longer move the whole stack. Window titles in the overview are now at the top left of the window and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/56\">match the users theme</a>. Additional workspace features, including pinned workspaces, will arrive in a future update.</p><p><strong>Windows Gravitate to Edges</strong></p><p>Toggling on ‚ÄúFloating windows gravitate to nearby edges‚Äù in Window Management Settings will automatically align a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/pull/1189\">window‚Äôs edge</a> to the adjacent screen border when dragged close to it, removing the struggle of aligning it with the edge manually.</p><p>If a search in the Launcher yields more than eight entries, users can now <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-launcher/issues/238\">scroll</a> to see the additional options. In addition, the Launcher has been updated to trigger a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-launcher/issues/126\">countdown timer</a> whenever Power Off, Restart, and Log Out are selected, matching the behavior of the Power applet.</p></div><div data-v-7773bbb7=\"\"><p>File path completion is now in COSMIC Files. Hitting the Down arrow when typing a file path into the search bar will automatically finish the file path you‚Äôre searching for. Meanwhile, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/728\">copying a file</a> allows pasting of the file path in other applications. COSMIC Files uses <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/732\">Home and End keys</a> for navigating the app. You can now compress and extract <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/468\">password protected zip files</a> and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/190\">drag selecting</a> will scroll the content window.</p><p>Copy/paste using the middle mouse button has been implemented. Sweet convenience.</p><p>A <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-player/issues/52\">nav bar</a> has been added for viewing folders in a tree view to display the video files available to open in the Media Player. File menu options have also been completed:</p></div><div data-v-7773bbb7=\"\"><p>When a music file is playing, the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-player/issues/56\">Media Player</a> image will display the song title, album, artist, and year released. Down the line we‚Äôd like to explore adding further metadata, such as album artwork and song lyrics.&nbsp;</p><p>Mpris control has been added to show and control currently-playing media in the sound applet, and the scrubber now moves to a second line for improved single-column usability.</p><p>A <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/128\">Revert all changes</a> feature has been added to COSMIC Edit to revert your file back to the most recent saved state. If you decide to scrap everything or start a new file from scratch, go to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/41\">File &gt; Close Project</a> to remove the project from the NavBar and bring up a new document and tab. When multiple tabs are open, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/123\">cycle project tabs</a> using Ctrl+Tab and Ctrl+Shift+Tab shortcuts.</p><p><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/pull/311\">Zoom</a> has also been implemented. Zoom in and out from the View menu, or using Ctrl + or Ctrl - shortcuts. Reset back to default using Ctrl + 0.</p><p>Opens Sans replaces Fira Sans as the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/libcosmic/pull/809\">default font</a> for COSMIC. The team liked Open Sans for its better legibility, glyph and language support, and a more modern aesthetic. Likewise, Noto Sans Mono will be used for the default monospace font.</p><p>Memory usage has been greatly reduced in a number of areas, including <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-applets/pull/796\">minimize</a>, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/pull/789\">COSMIC Files</a>, and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-panel/issues/336\">workspaces</a>. A related update made to libcosmic should prevent memory fragmentation. In addition, optimizations to cosmic-text and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/freedesktop-icons/pull/4\">freedesktop-icons</a> have reduced memory usage across all COSMIC apps and applets.</p><p><strong>A Whole Swarm of Bug Fixes‚Ä¶and More!</strong></p><ul><li>Fixed a bug with server-side decorations that caused the cursor to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/issues/1071\">drag a window</a> after a single click</li><li>When clicking an app icon of an app with multiple windows opened, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-applets/issues/456\">window previews</a> now adapt to the size and shape of the window</li><li>Implemented a fix in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/issues/680\">cosmic-comp</a> related to keyboard grabbing after a window is focused</li><li>Implemented behavior to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/pull/735\">COSMIC Files</a> for exiting the context menu</li><li><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/308\">COSMIC Files</a> now changes view away from the external drive after the drive is mounted and removed</li><li>Implemented a fix for <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/766\">COSMIC Files</a> attempting to read an unreadable .hidden file</li><li>Fixed a crash involving the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/xdg-desktop-portal-cosmic/issues/121\">file picker</a> related to a11y in libcosmic</li><li><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-term/issues/68\">COSMIC Terminal</a> now uses a hollow block cursor design when the window is unfocused</li><li>Removed Spell Check menu option from <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/300\">COSMIC Edit</a>, to be returned once the feature exists</li><li>In <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/191\">COSMIC Edit</a>, ‚ÄúFind‚Äù searches now highlight all occurrences, and the currently selected item is highlighted at a higher opacity</li><li>Scrolling now occurs as expected when dragging to highlight text in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/154\">COSMIC Edit</a></li><li>Saving a root or read-only file in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/249\">COSMIC Edit</a> now prompts the user for their password, removing the need to run the application as root</li><li><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-screenshot/issues/40\">Screenshot tool</a> now respects the user‚Äôs time zone when naming screenshot files</li><li>Implemented a fix preventing icons from disappearing from the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-screenshot/issues/18\">screenshot tool</a></li><li>Fixed a bug preventing the Delete key from moving a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/592\">desktop file</a> to the Trash</li><li>Implemented a fix for a bug causing <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-session/issues/103\">Steam</a> to crash</li><li>Fixed an issue causing some Radeon RX users to be unable to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-epoch/issues/1447\">log in</a></li><li>The context menu in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-settings/issues/953\">COSMIC Settings</a> now closes when another option is selected in the NavBar</li><li>Implemented the ability to import environment variables from <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-session/pull/106\">systemd</a></li><li>Fixed a regression with <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/libcosmic/issues/656\">libcosmic</a> affecting the ComboBox widget</li><li>Added a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-panel/issues/190\">slight delay</a> when the cursor hovers from one applet to the next to account for intent</li><li>Added support for using the middle mouse button to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-term/issues/49\">copy/paste</a></li><li>Clicking next/previous month in the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/libcosmic/issues/632\">calendar widget</a> no longer selects the day</li><li>Resolved a bug with <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-settings/issues/798\">Firefox</a> not recognizing it‚Äôs the default web browser when it‚Äôs not set as the default mail client</li><li>Removed WPS suggestion from the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-applets/pull/793\">WiFi applet</a> when WPS is not supported</li><li>Added support to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-bg/pull/68\">cosmic-bg</a> for compositors without fractional scaling support</li><li>Pop!_OS 24.04 Linux kernel updated to version <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/linux/pull/343\">6.12.10</a></li></ul><p>Head to the <a target=\"_blank\" rel=\"noopener\" href=\"https://system76.com/cosmic\">COSMIC page</a> for a fresh install of Alpha 6. If your system has an NVIDIA GPU, remember to install the NVIDIA ISO. Have fun and break things!</p></div>","contentLength":7754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iuy6v6/cosmic_alpha_6_big_leaps_forward/"},{"title":"Meanwhile at the Pentagon","url":"https://www.reddit.com/r/artificial/comments/1iuwy03/meanwhile_at_the_pentagon/","date":1740160167,"author":"/u/MetaKnowing","guid":8804,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] MLGym: A New Framework and Benchmark for Advancing AI Research Agents","url":"https://www.reddit.com/r/MachineLearning/comments/1iuwuyu/r_mlgym_a_new_framework_and_benchmark_for/","date":1740159974,"author":"/u/Rybolos","guid":8853,"unread":true,"content":"<p>We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</p>","contentLength":1468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Dimensionality reduction is bad practice?","url":"https://www.reddit.com/r/MachineLearning/comments/1iuwgcu/d_dimensionality_reduction_is_bad_practice/","date":1740159022,"author":"/u/Ready_Plastic1737","guid":8854,"unread":true,"content":"<p>I was given a problem statement and data to go along with it. My initial intuition was \"what features are most important in this dataset and what initial relationships can i reveal?\"</p><p>I proposed t-sne, PCA, or UMAP to observe preliminary relationships to explore but was immediately shut down because \"reducing dimensions means losing information.\"</p><p>which i know is true but..._____________</p><p>can some of you add to the ___________? what would you have said?</p>","contentLength":451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Godfather Yoshua Bengio says it is an \"extremely worrisome\" sign that when AI models are losing at chess, they will cheat by hacking their opponent","url":"https://www.reddit.com/r/artificial/comments/1iuvosh/ai_godfather_yoshua_bengio_says_it_is_an/","date":1740157177,"author":"/u/MetaKnowing","guid":8719,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust ü¶Ä DataFrame Library Elusion v3.3.0 is released üöÄ FIXED NORMALIZATION","url":"https://www.reddit.com/r/rust/comments/1iuvnrr/rust_dataframe_library_elusion_v330_is_released/","date":1740157108,"author":"/u/DataBora","guid":9115,"unread":true,"content":"<p>Elusion is a high-performance DataFrame / Data Engineering / Data Analysis library designed for in-memory data formats such as CSV, JSON, PARQUET, DELTA, as well as for ODBC Database Connections for MySQL and PostgreSQL, as well as for Azure Blob Storage Connections, as well as for creating JSON files from REST API's which can be forwarded to DataFrame.</p>","contentLength":355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Practices for Consistent API Error Handling","url":"https://zuplo.com/blog/2025/02/11/best-practices-for-api-error-handling","date":1740157047,"author":"/u/ZuploAdrian","guid":8852,"unread":true,"content":"<p><strong>Clear and consistent API error handling is crucial for improving developer\nexperience and reducing debugging time.</strong> Poor practices, like unclear messages\nor misuse of HTTP status codes, can frustrate developers and lead to increased\nsupport tickets. This guide covers actionable strategies to standardize API\nerror handling, including:</p><ul><li><strong>Use of HTTP Status Codes:</strong> Ensure accurate mapping (e.g., 400 for client\nerrors, 500 for server issues).</li><li><strong>Structured Error Responses:</strong> Follow RFC 9457 (Problem Details) standards\nfor clear, actionable error details.</li><li> Write brief, helpful, and secure messages.</li><li><strong>Protocol-Specific Practices:</strong> Tailor error handling for REST, GraphQL, and\ngRPC APIs.</li></ul><p>To address the challenges highlighted earlier, you can apply these\nwell-established methods.</p><p>HTTP status codes are your first tool for communicating errors. The key is to\nuse them accurately, rather than relying on generic codes.</p><table><thead><tr></tr></thead><tbody><tr><td>401 Unauthorized, 422 Unprocessable Entity</td></tr><tr><td>500 Internal Error, 503 Service Unavailable</td></tr></tbody></table><p>You can find a full list of status codes\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\">on MDN</a>, but here's a\nfew helpful docs we've written in the past that go into more depth:</p><p>For consistent error reporting, modern APIs should follow the\n<a href=\"https://www.rfc-editor.org/rfc/rfc9457.html\">RFC 9457</a> Problem Details\nspecification. This is the successor to the popular\n<a href=\"https://www.rfc-editor.org/rfc/rfc7807\">RFC 7807</a> draft. For an in-depth\nunderstanding of this format, check out our\n<a href=\"https://zuplo.com/blog/2023/04/11/the-power-of-problem-details\">full problem details guide</a>. In\ncase you're short on time - here's a quick overview:</p><p>The problem details response is sent back as a JSON body with the following\nproperties:</p><ul><li>: A URI that identifies the specific error type. This\nhelps clients understand the error and potentially find more information or\ndocumentation about it. Ideally, this URI should be stable and not change over\ntime.</li><li>: A short, human-readable summary of the problem. This\nshould be a brief description that concisely conveys the error. The title\nshould not change for a given \"type\" URI.</li><li><strong>status (integer, optional)</strong>: The HTTP status code generated by the origin\nserver for this occurrence of the problem. This helps clients understand the\nnature of the error and how it relates to the HTTP protocol.</li><li><strong>detail (string, optional)</strong>: A more detailed, human-readable explanation of\nthe problem. This can include specific information about the error and what\nmight have caused it. The \"detail\" field is intended to provide context and\nsuggestions to clients on how they might address the problem.</li><li><strong>instance (string, URI, optional)</strong>: A URI that identifies the specific\noccurrence of the problem. This can help clients and servers correlate and\ntrack individual instances of errors.</li></ul><p>Here's what a standardized error response might look like:</p><pre tabindex=\"0\"><code></code></pre><p>This response would be accompanied with the following headers and status code:</p><pre tabindex=\"0\"><code></code></pre><p>Here's a video that shows you how to send these error back in practice. It's in\n.Net/C# but the concepts are broadly applicable:</p><p>When evolving error schemas, it's crucial to make changes without disrupting\nexisting clients. Here's how you can manage this:</p><ul><li>Add optional fields instead of altering existing ones</li><li>Preserve legacy formats during transitions</li><li>Implement semantic versioning for your endpoints</li></ul><p>API management tools like Zuplo are really handy when trying to bring\nconsistency across your error formats, and are especially useful when trying to\ntransition all of your APIs over from one format to another.</p><p>Creating effective API error messages means providing useful guidance while\nkeeping security in mind. Google's AIP-193 guidelines\n<a href=\"https://google.aip.dev/193\">[4]</a> recommend using a structured format with\nplain, straightforward language that remains technically accurate.</p><p>The goal is to make error messages both clear and actionable. Here's a\ncomparison of good and bad examples:</p><table><thead><tr></tr></thead><tbody><tr><td>\"Invalid email format in 'user_email' field\"</td><td>\"ValidationError: field_23\"</td></tr><tr><td>\"Request to /api/v1/users failed\"</td></tr></tbody></table><p>If you've ever used Azure, many of their system errors demonstrate this\napproach, for example:</p><pre tabindex=\"0\"><code></code></pre><p>Maintaining security while providing useful error feedback involves a few key\npractices:</p><ul><li> to prevent leaks.</li><li><strong>Standardize authentication errors</strong> for consistency.</li><li> to avoid exposing vulnerabilities.</li></ul><p>For example, following RFC 9457 guidelines, a secure error response might look\nlike this:</p><pre tabindex=\"0\"><code></code></pre><p>This is another area where using an API gateway/API management tool is useful.\nYou can monitor your outbound responses and scan them for PII or other sensitive\ninformation/keywords using a regex. A programmable gateway (ex. Zuplo) will even\nlet you rewrite your response bodies to strip out sensitive data.</p><div><div><p>Over 10,000 developers trust Zuplo to secure, document, and monetize their APIs</p><a href=\"https://zuplo.com?utm_source=blog&amp;utm_medium=inline-cta\">Learn More</a></div></div><p>Let's dive into how different API protocols handle errors, building on the\nstandards discussed earlier.</p><p>REST APIs rely on HTTP status codes paired with structured error payloads. A\ncommon standard for this is , which ensures a\nconsistent format across endpoints. This method also supports content\nnegotiation between JSON and XML, keeping the structure consistent across\ndifferent formats.</p><p>GraphQL handles errors differently. It always responds with a  status\ncode, even when errors occur. Errors are communicated through an  array,\nwhich allows for partial success. For instance, GitHub's GraphQL API might\nreturn:</p><pre tabindex=\"0\"><code></code></pre><p>This approach allows for returning valid data alongside error details.</p><p>gRPC uses a predefined set of numeric status codes (ranging from 0 to 16) for\nerror handling, aligned with . Errors include structured details\nfor better context. Here's an example:</p><pre tabindex=\"0\"><code></code></pre><table><thead><tr></tr></thead><tbody><tr><td>Standard HTTP success codes</td></tr><tr></tr><tr></tr></tbody></table><p>Each protocol has its own approach, but they all follow two key principles:\n<strong>machine-readable error codes</strong> and . This ensures\nerrors are both understandable and actionable.</p><p>For cross-protocol APIs, API gateways can simplify error handling. They provide\nunified error schema management and automate transformations between\nprotocol-specific formats. This helps maintain consistency while respecting the\nconventions of each API type.</p><p>Effective error management relies on consistent monitoring and testing to uphold\nusability standards outlined in earlier protocols. This process builds on\nprotocol-specific error conventions to ensure smooth handling across systems.</p><p>A centralized error code system can ensure uniformity across distributed\nservices. For example, Google uses the  format, which\nenforces standardized error structures with both machine-readable codes and\nhuman-readable messages <a href=\"https://google.aip.dev/193\">[4]</a>.</p><p>In multi-service architectures, two main approaches are common:</p><table><thead><tr></tr></thead><tbody><tr><td>Ensures uniform error codes, Acts as a single source of truth</td><td>Requires strict oversight</td></tr><tr><td>Distributed with Prefixes</td><td>Allows team independence, Enables quicker updates</td><td>Demands thorough documentation</td></tr></tbody></table><p>Many organizations find that combining these methods provides the best results.</p><p>Key metrics to monitor include\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://techcommunity.microsoft.com/discussions/appsonazure/best-practices-for-api-error-handling-a-comprehensive-guide/4088121\">[3]</a>:</p><ul><li><strong>Mean Time to Acknowledge (MTTA)</strong> errors: Target under 30 minutes.</li><li>: Should remain below 5% after fixes.</li><li><strong>95th percentile error resolution time</strong>: A critical benchmark for resolution\nspeed.</li><li><strong>User-impacting error ratio</strong>: Measured per 10,000 requests.</li></ul><p>Tools like <a href=\"https://sentry.io/\">Sentry</a> support distributed tracing in over 15\nlanguages, while <a href=\"https://raygun.com/\">Raygun</a> offers deployment correlation to\npinpoint issues <a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a>.</p><p>Testing ensures compliance with HTTP status codes and response formats covered\nin earlier sections. Error testing is typically done manually using tools like\nPostman - but you should definitely invest in automation as your API grows and\nevolves. To test specific errors, you should invest in automated\n<a href=\"https://zuplo.com/blog/2025/02/01/end-to-end-api-testing-guide\">end-to-end API testing</a> using\ntools like Playwright and StepCI.</p><p>If you have schematized errors, you can perform\n<a href=\"https://zuplo.com/blog/2024/07/19/verify-json-schema\">schema validation</a> on your live responses\nto ensure they adhere to those schemas. When response validation is combined\nwith an <a href=\"https://zuplo.com/blog/2024/09/25/mastering-api-definitions\">API design specification</a>\nlike OpenAPI to enforce outputs match what's documented, it's known as\n.</p><p>Effective API error handling builds on the protocol-specific conventions\ndiscussed earlier. Two key goals are ensuring <strong>consistent response formats</strong>\nand reducing repeat client errors\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://blog.postman.com/best-practices-for-api-error-handling/\">[5]</a>.</p><p>Key requirements include:</p><ul><li><strong>Standardized Response Structure</strong>:<ul><li>Machine-readable error codes</li><li>Clear, human-readable messages</li><li>Links to relevant documentation</li><li>Request correlation IDs for troubleshooting</li></ul></li><li><strong>Security-Focused Practices</strong>:<ul><li>Prevent exposure of sensitive data by adhering to security guidelines\noutlined in the Writing Clear Error Messages section</li><li>Use appropriate status codes</li><li>Follow established security best practices</li></ul></li></ul><p>To create a reliable error-handling system, follow these four phases:</p><ol><li>Use an\n<a href=\"https://zuplo.com/blog/2025/01/27/8-api-monitoring-tools-every-developer-should-know\">API monitoring tool</a>\nto analyze errors at the endpoint level. This helps identify inconsistencies\nand establish a baseline for improvement\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://blog.postman.com/best-practices-for-api-error-handling/\">[5]</a>.</li><li>Introduce centralized error-handling middleware to enforce the newly defined\nstandards. Often, an API gateway plays this role.</li><li>Use error tracking tools to evaluate the system‚Äôs performance. Track metrics\nlike error recurrence rates, resolution times, and Mean Time to Acknowledge\n(MTTA).</li></ol><p>These steps align with earlier recommendations for policy-driven error handling\nand offer practical ways to enhance your API's reliability.</p><p>Handling API errors effectively requires a clear and structured approach. This\ninvolves combining standard protocols with additional application-specific\ninformation to provide clarity and maintain security.</p><p><strong>1. Protocol-Specific Handling</strong></p><p>Each API protocol has its own method for managing errors. Here‚Äôs how to handle\nerrors for some common protocols:</p><ul><li>: Use HTTP status codes alongside detailed error messages in the\nresponse body.</li><li>: Include error arrays in the response, allowing for partial\nsuccess when appropriate.</li><li>: Utilize standardized status codes with structured error details.</li></ul><p><strong>2. Security Best Practices</strong></p><p>To keep your API secure, follow these guidelines (as outlined in the \"Security\nin Error Messages\" section):</p><ul><li>Use generic error messages for authentication failures to prevent revealing\nsensitive information.</li><li>Avoid exposing internal system details in error responses.</li><li>Filter sensitive data on the server side before sending error responses.</li></ul><p><strong>3. Monitoring and Consistency</strong></p><p>Set up monitoring tools, such as distributed tracing, to identify and analyze\nerror patterns. Use\n<a href=\"https://zuplo.com/blog/2024/12/16/api-gateway-hosting-options\">API gateways</a> to\nenforce consistent error formats and schemas across your system for better\nmanagement and debugging\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://techcommunity.microsoft.com/discussions/appsonazure/best-practices-for-api-error-handling-a-comprehensive-guide/4088121\">[3]</a>.</p>","contentLength":10121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuvmvv/best_practices_for_consistent_api_error_handling/"},{"title":"Meetup: All in Kubernetes (Munich)","url":"https://www.reddit.com/r/kubernetes/comments/1iuvh2k/meetup_all_in_kubernetes_munich/","date":1740156652,"author":"/u/simplyblock-r","guid":8722,"unread":true,"content":"<p>Hey folks, if you're in or around Munich or Bavaria: this is for you! (if it's not a right place to post it, pls delete)</p><p>We're running our second meetup of the \"All in Kubernetes\" roadshow in Munich on Thursday, 13th of March. The first meetup, last month in Berlin, one was a big success with over 80 participants in Berlin.</p><p>Community is focused around stateful workloads in Kubernetes. The sessions lined up are:</p><ol><li>Architecting and Building a K8s-based AI Platform</li><li>Databases on Kubernetes: A Storage Story</li></ol>","contentLength":501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub Traffic - CLI Edition","url":"https://postimg.cc/XXWK9gzB","date":1740155625,"author":"/u/manifoldjava","guid":8851,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuv282/github_traffic_cli_edition/"},{"title":"list of decimal packages: fixed and big","url":"https://www.reddit.com/r/golang/comments/1iuunf1/list_of_decimal_packages_fixed_and_big/","date":1740154589,"author":"/u/kardianos","guid":8830,"unread":true,"content":"<p>I was updating a list of decimal packages. I thought I would share.</p><p>There are generally 2 varieties: fixed sized and arbitrary precision. The udecimal is interesting as it uses a fixed size for 128 bit precision with zero allocations, then uses an allocating \"*big.Int\" version for anything larger then that.</p><p>I currently use \"cockroachdb/apd\", which is a great package for frameworks or databases, but, it's a bit awkward to hold and lacks good formating. Realistically, I just need a fixed size decimal for my needs (financial/clinical). When I get a chance, I'll probably swap in for one of the fixed size packages.</p>","contentLength":615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a new playground for Go, Pt, TS and more other, with Postgres... It supports program arguments, pretty output for JSON and I will add a lot feature soon","url":"https://codiew.io/ide","date":1740151971,"author":"/u/Halabooda","guid":8831,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iutm6h/i_built_a_new_playground_for_go_pt_ts_and_more/"},{"title":"Borrow Checker Trauma","url":"https://www.reddit.com/r/rust/comments/1iuthsl/borrow_checker_trauma/","date":1740151657,"author":"/u/xwaxes","guid":8784,"unread":true,"content":"<p>I am using the term ‚Äòborrow checker trauma‚Äô for lack of a better word. A bit of context first; I have been using Rust for my personal web projects extensively but use Rails at work. </p><p>So the problem is, whenever I am working on work projects and want to perform two or more operations on a variable, especially if I am passing it around or returning it, I always find myself taking a step back to consider if the ownership has moved before I remember that I am on Ruby and that doesn‚Äôt apply. </p><p>Has anyone experienced this in other languages or on their daily workflow?</p>","contentLength":571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talk me out of using Mongo","url":"https://www.reddit.com/r/golang/comments/1iutb24/talk_me_out_of_using_mongo/","date":1740151172,"author":"/u/grdevops","guid":8652,"unread":true,"content":"<p>Talk me out of using Mongo for a project I'm starting and intend to make a publicly available service. I  love how native Mongo feels for golang, specifically structs. I have a fair amount of utils written for it and it's basically at a copy and paste stage when I'm adding it to different structs and different types. </p><p>Undeniably, Mongo is what I'm comfortable with have spend the most time writing and the queries are dead simple in Go (to me at least) compared to Postgres where I have not had luck with embedded structs and getting them to easily insert or scanned when querying (especially many rows) using sqlx. Getting better at postgres is something I can do and am absolutely 100% willing to do if it's the right choice, I just haven't run into the issues with Mongo that I've seen other people have</p><p>As far as the data goes, there's not a ton of places where I would need to do joins, maybe 5% of the total DB calls or less and I know that's where Mongo gets most of its flak. </p>","contentLength":984,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bugs with k8s snap and IPv6 only","url":"https://www.reddit.com/r/kubernetes/comments/1iurtp1/bugs_with_k8s_snap_and_ipv6_only/","date":1740147192,"author":"/u/hblok","guid":8633,"unread":true,"content":"<p>I'm setting up an IPv6  cluster, using Ubuntu 24.04 and the k8s and kubelet snaps. I've disabled IPv4 on the eth0 interface, but not on loopback. The CP comes up fine, and can be used locally and remotely. However, when trying to connect a worker node, there are some configuration options relating to IPv6 which I believe are bugs. I'd be interested to hear if these are misunderstandings on my part, or actual bugs.</p><p>The first is in the k8s-apiserver-proxy config file <code>/var/snap/k8s/common/args/conf.d/k8s-apiserver-proxy.json</code>. It looks like this, where the the last part is the port number 6443. The service does not start with a <em>\"failed to parse endpoint\"</em> error:</p><pre><code>{\"endpoints\":[\"dead:beef:1234::1:6443\"]} </code></pre><p>When correcting the address to use brackets, it will start up correctly.</p><pre><code>{\"endpoints\":[\"[dead:beef:1234::1]:6443\"]} </code></pre><p>Secondly, the snap.k8s.kubelet.service will not start, trying to bind to 0.0.0.0:10250 , but fails with <em>\"Failed to listen and serve\" err=\"listen tcp 0.0.0.0:10250: bind: address already in use\"</em>. Here I'm not sure where the address and port is coming from, but I'm guessing it's a default somewhere. Possibly <a href=\"https://github.com/kubernetes/kubernetes/issues/108248\">related to this report</a>.</p>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gofs - a file server written in go","url":"https://www.reddit.com/r/golang/comments/1iuqggw/gofs_a_file_server_written_in_go/","date":1740143207,"author":"/u/-dtdt-","guid":9077,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/-dtdt-\"> /u/-dtdt- </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Certifications for software architects","url":"https://www.cerbos.dev/blog/certifications-for-enterprise-architects-domain-solutions-architects-software-engineers","date":1740142976,"author":"/u/West-Chard-1474","guid":8598,"unread":true,"content":"<p>Over the years, I‚Äôve noticed that no one can quite settle on how important certification is. All it takes is one look at the Software Architecture subreddit and you‚Äôll see people asking about certificates only to be told they‚Äôre both useless and useful.</p><p>When I was working with Lemon.io (a developer marketplace with 80k+ developers), I got to see firsthand how certification affected their careers. So when I saw this topic start to pop up again (without any real answers), I decided to dive into research to see if my experience at Lemon.io still held true for architects.</p><h2>The value of certification</h2><p>Assuming I‚Äôm talking to architects with a lot of experience, what I will say is that certification doesn‚Äôt replace experience, but it does complement it really well. And, it can be a strong differentiator from your peers.</p><p>I‚Äôm going to try a metaphor here. If your career is a burger, your experience is the patty and certificates are the condiments. Some people prefer their burgers with bacon, cheese, or even an egg. Depending on what your career goals are, you‚Äôll want to add different condiments to your ‚Äòburger‚Äô. But keep in mind, the most important thing will always be the meat, a.k.a your experience.</p><p>Deciding if certification is right for you is the first step. The next step is asking the question: what is the right certification for you?</p><p>I loved the Role Based Roadmap from Mumshad Mannambeth, founder &amp; CEO at KodeKloud on navigating the certification paths <a href=\"https://www.linkedin.com/posts/mmumshad_kodekloud-cloudcomputing-aws-activity-7110238676915273728-U2X8/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAB-THdoBNL-y76_DeuYlNKmEH_yOiRzPAjc\">(you can find it here)</a>.</p><p>It inspired me to do something similar but focused on Architects. Below, you‚Äôll find 12 of the most popular architectural certificates you can choose to upgrade your career, and add more ‚Äútrust badges‚Äù to your CV, LinkedIn profile, or freelancer profile.</p><p>Each has its own focus and value it can add depending on your career goals and role:</p><table><thead><tr><th><strong>Certifications for Enterprise Architects</strong></th><th><strong>Certifications for Domain Solutions Architects</strong></th><th><strong>Software Architecture, Governance, and Infrastructure Certification</strong></th></tr></thead><tbody><tr><td>AWS Certified Solutions Architect</td></tr><tr><td>Google Professional Cloud Architect</td></tr><tr><td>Zachman Certified - Enterprise Architect</td><td>Microsoft Certified: Azure Solutions Architect Expert</td></tr><tr><td>Certified Enterprise Architect (CEA) Black Belt Program</td><td>Red Hat Certified Architect (RHCA)</td></tr></tbody></table><p>Recognized globally, <a href=\"https://www.credly.com/org/the-open-group/badge/the-open-group-certified-togaf-9-certified\">this certificate</a> covers the TOGAF framework for designing, planning, implementing, and governing enterprise information technology architecture. So if you are working on an enterprise-wide architecture or want to switch to that direction, it can be useful.\nUnlike the certifications below, this is a whole ecosystem that builds on itself. So it is a significant investment. However, if you‚Äôre working in, or looking to work in, the governmental sector or with large corporations, this may be a good choice for you. Keep in mind, however, that while it may look cheap, the cost does not include training, which is provided by TOGAF-accredited partners, each of whom sets their own price.</p><p>The comment below sums up my research into TOGAF 9 really well:</p><p><strong>Best for Software Architects working in large organizations</strong>: Enterprise Architects,  Business Architects, Solutions Architects, and IT Strategy Consultants with experience in strategic planning, We had a few Enterprise Architects with TOGAF 9 certification at Lemon.io and it was a nice value-add to their profiles. It is worth mentioning that their rates were in the top tier üôÇ.</p><ul><li>13 Level 1 Learning Units</li><li>27 Level 2 Learning Units</li></ul><p>Testing: Two-stage testing, including TOGAF 9 Part 1 and TOGAF 9 Part 2 examinations</p><table><thead><tr></tr></thead><tbody><tr><td>$360 USD per exam (requires two exams)</td><td>English, Simplified Chinese, Spanish (Latin American), French</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: TOGAF 9 certificate does not expire.</p><p>ITIL is one of the most popular certification systems in the world, with more than two million certified specialists in the world. The <a href=\"https://www.axelos.com/certifications/itil-service-management\">Master certification</a> is the highest level of ITIL certifications and requires passing all four previous certifications before challenging it. To achieve the master certification you have to pass an assessment to validate your ability to apply ITIL frameworks to real-world business scenarios. So, experience working with the ITIL principals and practices as an enterprise software architect is required.</p><p>ITIL is not a training or testing provider but works with accredited partners for each. That means the pricing is dependent on your provider. The nice thing is, that it is one of the few large accreditations you can achieve through self-study.</p><p>If you want to chat with peers who are planning to become ITIL Masters, there is an active subreddit called <a href=\"https://www.reddit.com/r/ITIL_Certification/\">ITIL_Certification</a>.</p><p>: Governance &amp; Compliance Architects, Governance and Compliance Managers, Enterprise Architects.</p><p>Five distinct levels to progress through: Foundation -&gt; Practitioner ‚Äì&gt; Intermediate -&gt; Expert -&gt; Master. Each level has its own training and requirements.</p><ul><li>Attend a training course with an accredited training organization, which will include the exam as part of the course.</li><li>Self-study using the core manual, then book an exam directly with PeopleCert.</li></ul><table><thead><tr></tr></thead><tbody><tr><td>Dependent on your chosen training/testing partner.</td><td>All 4 previous ITIL certifications, including ITIL Expert certification</td><td>5 years in IT leadership, management, or higher management advisory levels.</td><td>English, Brazilian Portuguese, Chinese, Dutch, French, German, Italian, Japanese, Polish, Spanish, Thai</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: Certification is valid for 3 years. You can renew your certification by retaking the exam or by earning a new certification.</p><h2>Zachman Certified - Enterprise Architect</h2><p><a href=\"https://zachman-feac.com/\">This certification</a> covers the Zachman Framework for designing and maintaining Enterprise Architectures, which aligns IT with business goals. The training is focused on high-level enterprise planning, including Enterprise Architecture principles, strategy formulation, and practical application, rather than project or solutions architecture. This makes it ideal for those who need a structured approach to solve enterprise challenges.</p><p>This is another multi-level certification option. Each level builds on the one before it, which makes it a very comprehensive course. And, in my humble opinion, very expensive.</p><p>: Enterprise Architects, Business Architects, and IT Consultants with an Enterprise Architecture focus.</p><p>Four levels available: Associate, Practitioner, Professional and Educator (last one only required for those who want to teach the course).</p><p>Training: Two weeks of online prep work, and three days of live instruction</p><p>Testing: Two-hour, online exam (passing grants level 1 Associate)</p><p>Case study: Delivering a case study provides level 2 Practitioner, and a second case study provides level 3 Professional</p><table><thead><tr></tr></thead><tbody><tr><td>$2999 USD covers level 1 &amp; 2 (regional pricing is available)</td><td>Each level requires certification in the preceding level</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate expires in 3 years. Recertification costs $99 USD.</p><h2>Certified Enterprise Architect (CEA) Black Belt Program (Owned by Zachman now)</h2><p>Zachman now owns CEA, so I decided to add this certification under the Zachman banner. Just like Zachman, this is a three-step course, except the naming convention is designed to make you feel like you‚Äôre in a martial art, which is cool. It‚Äôs also $7,000 more, which is not as cool, but that does mean you get to skip the yellow and green belt and go straight for the black belt.</p><p>Designed to develop Enterprise Architects through hands-on training and real-world projects, <a href=\"https://zachman-feac.com/\">this program</a> will prepare you for leadership roles in Enterprise Architecture. This program is built on ISO standards and focuses on the practical application of frameworks, tools, and methodologies.</p><p>: (Very rich people) Senior Enterprise Architect, IT Director with EA experience, Chief Architect, Enterprise Architect with 10+ years experience, Enterprise Architecture Consultant.</p><ul><li>Accelerated Path of 12 Weeks: Five individual courses taught in parallel over twelve weeks.</li><li>Progressive Path that is self-paced: Five individual online courses taken at your own pace over 24-30 weeks.</li></ul><p>The <a href=\"https://iasaglobal.org/Public/Public/Learn/Training_and_Certifications.aspx\">IASA Global certification</a> is a vendor-independent program for Business Technology Architects. The training has four stages that roughly align with your career stage. For Enterprise Architects, the professional (3rd) tier is the most useful  The previous two tiers are for those in earlier stages of their career. The training is based on practical experience with a focus on Enterprise Architecture (EA), Software Architecture (SA), Solution Architecture (SolA), Infrastructure Architecture (IA) and Business Architecture (BA).</p><p>Unlike most tiered options, IASA allows you to challenge each level even if you haven‚Äôt attained the certification under it. So if you‚Äôve progressed in your career far enough that you don‚Äôt think a foundational certification is valuable, and you don‚Äôt want to work your way through 3 tiers you already fully understand, IASA may be the answer for you.</p><p>: Senior architects and business analysts aiming to bridge the gap between business and technology.</p><p>Four levels of certification:</p><ul><li>Professional (recommended for enterprise-level architects) - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2 hrs allotted for the exam.</li><li>Distinguished - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2.5 hrs allotted for the exam.</li></ul><p>Testing: Online or onsite testing is available</p><table><thead><tr><th> (for professional tier)</th></tr></thead><tbody><tr><td> Exam: $425 USD  N/A  Exam + prep: $2,000 USD  Exam + prep: $2,800 USD</td><td>A minimum of 10 years in the industry as a practicing architect.  Extensive documentation is required.</td><td>CITA-Associate level certificate is recommended but not mandatory.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: Requires individuals to maintain an active Full Iasa Membership and collect at least 80 hours of Continuing Education Units bi-annually (based on their website).</p><h2>The Open Group ArchiMate 3 Certification</h2><p>ArchiMate was mentioned a lot over Reddit. <a href=\"https://www.opengroup.org/certifications/archimate\">The Open Group ArchiMate 3</a> doesn‚Äôt teach you how to be an EA but rather focuses on how to communicate better as an EA by teaching ArchiMate‚Äôs modelling language. This language is designed to remove ambiguity from the description, analysis, and visualization of Enterprise Architectures.</p><p>The content of the course is designed to complement TOGAF, which makes it useful for Enterprise Architects who already work in a TOGAF framework. They aren‚Äôt however, the same thing.</p><p>: Enterprise Architects</p><p>Training: Online self-study available, or attend an accredited training course (Accredited Training Courses provide an exam voucher).</p><ul><li>ArchiMate 3 Part 1 offers foundation certification (60 min time limit for 40-question, multiple choice exam. Passing score: 60%)</li><li>ArchiMate 3 Part 2 offers practitioner certification (90 min time limit for 8-question, scenario-based and complex multiple choice exam. Passing score: 65%)</li></ul><p>Online or in-person proctored exams available depending on provider</p><table><thead><tr></tr></thead><tbody><tr><td>Training cost depends on the provider.  Each exam costs $360 USD.</td><td> None  Foundation certification or pass Part 1 exam on the same day with the same provider.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certification does not expire, which is very cool.</p><h2>AWS Certified Solutions Architect</h2><p><a href=\"https://aws.amazon.com/certification/certified-solutions-architect-associate/?nc1=f_ls\">The AWS certification</a> is a great starting point for architects or senior Software Engineers with AWS Cloud or strong on-premises IT experience. It covers the design and optimization of AWS cloud-based software and shows you can handle complex multi-service architectures, which is crucial as more companies move to the cloud. The exam tests real-world scenarios you would face designing large-scale systems, including hybrid architectures, multi-region deployments, and cost optimization at scale. Basic familiarity with programming concepts will help, but you don‚Äôt need hands-on experience with code.</p><p>This is the only provider-specific certificate that offers a more in-depth, self-directed training option for a price. Of course, you don‚Äôt have to take that option. If you‚Äôre confident, you can take the free training and then challenge the test. However, the test is also the cheapest among these certifications, so it offsets the overall cost a little bit.</p><p>During my research, I found quite a few posters, including <a href=\"https://www.reddit.com/r/AWSCertifications/comments/1h5jfmp/how_did_passing_aws_solution_architect/\">this one</a>, that had seen significant benefits from taking the course.</p><p>: Systems Administrators (Cloud Focused), Cloud Architects, Solutions Architects, Cloud Consultants Software Architects for Cloud-Based Applications, and Enterprise Architects.</p><ul><li>3 courses of online, self-directed exam prep are available</li><li>15.25 hrs free; 48.75 hrs paid</li></ul><p>Testing: 130 minutes. Pearson VUE testing center, or online proctored exam</p><table><thead><tr></tr></thead><tbody><tr><td>1 year of hands-on experience designing cloud solutions with AWS services.</td><td>English, French (France), German, Italian, Japanese, Korean, Portuguese (Brazil), Spanish (Latin America), Spanish (Spain), Simplified and Traditional Chinese</td></tr></tbody></table><h2>Google Professional Cloud Architect</h2><p>If you‚Äôre all in on Google, <a href=\"https://cloud.google.com/learn/certification/cloud-architect\">this certification</a> is for you. It will help you show your cloud architecture skills and advance your career in Google‚Äôs cloud technology. Keep in mind, this is focused on Google‚Äôs cloud infrastructure and doesn‚Äôt cover software architecture.</p><p>The training and exam for this certificate are all online (though on-site exams are available) which makes it very flexible. Plus, it‚Äôs pretty cheap (although it‚Äôs the most expensive of the provider-specific options). However, if you go through all the training, it‚Äôs going to take you some time, as it‚Äôs the longest provider-specific course here.</p><p>: Cloud Architects, Solutions Architects, IT Project Managers focused on the cloud, Cloud Engineers, DevOps Engineers, and Enterprise Architects.</p><p>Training: 114.75 hrs, online, self-directed</p><p>Testing: 2-hr test, with two options: an online proctored exam, or an onsite-proctored exam at a testing center.</p><table><thead><tr></tr></thead><tbody><tr><td>3+ years of industry experience, including 1+ year of designing and managing solutions with Google Cloud</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate is valid for 2 years. Recertify by retaking the exam within 60 days of the expiration date.</p><h2>Microsoft Certified: Azure Solutions Architect Expert</h2><p><a href=\"https://learn.microsoft.com/en-us/credentials/certifications/azure-solutions-architect/\">This certificate</a> shows you know your way around Azure, including how to design and implement cloud and hybrid solutions. It dives deep into how various IT infrastructure components in the Microsoft ecosystem (like compute, network, storage, monitoring, and security) interact to generate solutions. Just like the Google certificate above, this is infrastructure-focused, not software-focused, so keep that in mind when considering it.</p><p>The course for the Microsoft certification is shorter than Google‚Äôs by almost 100 hours and can be taken both online at your own pace, or with an online instructor. It‚Äôs also a bit cheaper than Google‚Äôs, making it an easier investment both for hours and dollars spent.</p><p>: Systems Administrators, Network Engineers, IT Managers, Cloud Architects, Computer Systems Analysts and Infrastructure Engineers.</p><p>Training: Self-paced online learning (15.25 hrs), or instructor-led online training (4 days).</p><p>Testing: Online proctored exam.</p><table><thead><tr></tr></thead><tbody><tr><td>$165 USD  Self-paced training is free; instructor-led depends on the provider.</td><td>Experience with Azure administration and development, and DevOps processes.  Advanced experience and knowledge of IT operations.</td><td>English, Chinese (Simplified), French, German, Japanese, Korean, Portuguese (Brazil), Spanish</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate expires after one year. Recertification is free via an online assessment on Microsoft Learn.</p><h2>Red Hat Certified Architect (RHCA)</h2><p>Red Hat‚Äôs highest level of certification, the <a href=\"https://www.redhat.com/en/services/certification/rhca\">RHCA designation</a> covers designing, implementing, and managing Red Hat-based IT infrastructures. One of the nice things is that RHCA offers tracks in both infrastructure and enterprise applications. So you can choose the option that best matches your goals.</p><p>Red Hat prices its certification differently than most. It‚Äôs a subscription-based model, which allows you to have a little more freedom with how you tackle their training. In fact, their certification offers the most custom options, with a variety of options to get from A (where you are) to B (certified). So if you have diverse interests, this one might be the one for you.</p><p>: Senior Systems Administrator, Cloud Architect, IT Infrastructure Architect, DevOps Engineer, Senior Application Developer, Enterprise Solutions Architect</p><ul><li>Red Hat Certified Architect in Infrastructure</li><li>Red Hat Certified Architect in Enterprise Applications</li></ul><p>Certification builds on prerequisites with five additional certifications chosen from a list.</p><p>Training and exams depend on your chosen path and certifications.</p><table><thead><tr></tr></thead><tbody><tr><td> Standard: $7,500 USD/year for 25 training units and certification. <p> Premium: $9,000 USD/year for 30 training units and certification.</p></td><td>Red Hat Certified Engineer (RHCE)  OR <p> Red Hat Certified Enterprise Microservices Developer (RHCEMD) </p> OR <p> Red Hat Certified Cloud-native Developer (RHCCD)</p></td><td>Recommended experience depends on your specific path.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: Certifications become ‚Äònon-current‚Äô after three years. To maintain an RHCA certification, you must maintain five additional certifications over your RHCE. RHCEMD or RHCCD. These certifications do not need to be the same as those you‚Äôve taken to attain your RHCA.</p><p>Made for IT professionals who want to master SOA, <a href=\"https://www.arcitura.com/cert/soa-architect-certification-exam.html\">this certification</a> covers designing, implementing, and managing Service-Oriented Architecture (SOA) infrastructure.</p><p>SOA certification is one and done. There are no levels or long-term commitment to one system, which makes it less of a commitment. It‚Äôs also very affordable.</p><p>: Enterprise Architects, Solutions Architects, IT Architects, Software Architects, Systems Engineers with SOA experience, Technical Leads with architecture responsibilities</p><ul><li>170 mins online proctored exam</li><li>50 hrs of training over 5 modules (modules include: Workbook Lessons, Video Lessons, Interactive Exercises, Mind Map Poster, Practice Exam Questions, PDFs of Workbook and Poster, Lab Exercise Booklet).</li></ul><table><thead><tr></tr></thead><tbody><tr><td>$399 USD for course and certification</td></tr></tbody></table><h2>iSAQB CPSA-F/CPSA-A (International Software Architecture Qualification Board)</h2><p>In contrast to TOGAF training, <a href=\"https://www.isaqb.org/certifications/cpsa-certifications/cpsa-foundation-level/\">the CPSA program</a> focuses on the practical implementation of IT systems. Its foundation and advanced certificates offer room for architects to grow.</p><p>This is a two-step program, foundation and advanced, which puts it between the single-step SOA certification and the larger three- or even four-step offerings. Just like TOGAF and ITIL, iSAQB is not a testing or training provider, so there are a lot of options for training. Or, if you‚Äôre confident, you can challenge the exam without, though that‚Äôs not recommended.</p><p>: Software designers, software developers, Software Architects, systems analysts</p><p>\nThough training and testing are performed by independent operators, you have four testing options available, including:</p><ul><li>Exam after classroom training</li></ul><table><thead><tr></tr></thead><tbody><tr><td>The cost of training is dependent on training providers.  Testing price is dependent on training providers.</td><td>Training is suggested.  Foundation certification is required for Advanced.</td><td>18+ months of practical experience, including:  - A higher programming language <p> - Technical documentation </p> - Object-oriented programming language <p> - Design and implementation of distributed applications </p> - Basics of modeling and abstraction; and UML and their relation to source</td><td>Language is dependent on training providers.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate does not expire.</p><p>The right certification can set you apart on your resume, but it‚Äôs never a replacement for experience. Depending on your career path, you may choose to get certified by one of the bodies above, or simply study on your own and prove what you can do through practical experience.</p><p>If you‚Äôve decided to pursue the certification path, the options above are all great choices. Of course, each requires a commitment of time and money. While you can‚Äôt warp the space-time continuum to change the time requirement, it may be possible to get your employer to help you cover some, if not all of the cost. If they do, it gives you an even higher ROI on your investment to yourself.</p>","contentLength":20228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuqdq8/certifications_for_software_architects/"},{"title":"This month in Servo: new webview API, relative colors, canvas buffs, and more!","url":"https://servo.org/blog/2025/02/19/this-month-in-servo/","date":1740142406,"author":"/u/wuyuwei-tw","guid":8721,"unread":true,"content":"<p>Servo now supports several new web API features:</p><p>We‚Äôve landed a bunch of  improvements:</p><p> are a lot more useful now, with  now supporting  (<a href=\"https://github.com/Taym95\">@Taym95</a>, <a href=\"https://github.com/servo/servo/pull/35040\">#35040</a>), , , and  (<a href=\"https://github.com/Taym95\">@Taym95</a>, <a href=\"https://github.com/servo/servo/pull/34958\">#34958</a>).</p><p>Servo aims to be an embeddable web engine, but so far it‚Äôs been a lot harder to embed Servo than it should be.</p><p>For one, configuring and starting Servo is complicated.\nWe found that getting Servo running at all, even without wiring up input or handling resizes correctly, took  of Rust code (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35118\">#35118</a>).\nEmbedders (apps) could only control Servo by sending and receiving a variety of ‚Äúmessages‚Äù and ‚Äúevents‚Äù, and simple questions like ‚Äúwhat‚Äôs the current URL?‚Äù were impossible to answer without keeping track of extra state in the app.</p><p>Contrast this with <a href=\"https://webkitgtk.org/\">WebKitGTK</a>, where you can write a minimal kiosk app with a fully-functional webview in  of C.\nTo close that gap, we‚Äôve started <strong>reworking our embedding API</strong> towards something more idiomatic and ergonomic, starting with the concept embedders care about most: the .</p><p>Our new webview API is controlled by calling methods on a  (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35119\">#35119</a>, <a href=\"https://github.com/servo/servo/pull/35183\">#35183</a>, <a href=\"https://github.com/servo/servo/pull/35192\">#35192</a>), including navigation and user input.\nHandles will eventually represent the lifecycle of the webview itself; if you have one, the webview is valid, and if you drop them, the webview is destroyed.</p><p>Servo needs to call into the embedder too, and here we‚Äôve started replacing the old EmbedderMsg API with a  (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35211\">#35211</a>), much like the delegates in <a href=\"https://developer.apple.com/documentation/webkit/wkuidelegate?language=objc\">Apple‚Äôs WebKit API</a>.\nIn Rust, a delegate is a  that the embedder can install its own  for.\nStay tuned for more on this next month!</p><p>Other embedding improvements include:</p><p>We‚Äôve reworked Servo‚Äôs , making all prefs optional with reasonable defaults (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/34966\">#34966</a>, <a href=\"https://github.com/servo/servo/pull/34999\">#34999</a>, <a href=\"https://github.com/servo/servo/pull/34994\">#34994</a>).\nAs a result:</p><ul><li><strong>The names of all preferences have changed</strong>; see the <a href=\"https://doc.servo.org/servo_config/prefs/struct.Preferences.html\">Prefs docs</a> for a list</li><li><strong>Embedders no longer need a </strong> resource to get Servo running</li></ul><p>Servo‚Äôs networking is more efficient now, with the ability to <strong>cancel fetches for navigation</strong> that contain redirects (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/34919\">#34919</a>) and <strong>cancel fetches for &lt;video&gt; and &lt;media&gt;</strong> when the document is unloaded (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/34883\">#34883</a>).\nThose changes also <strong>eliminate per-request IPC channels</strong> for navigation and cancellation respectively, and in the same vein, we‚Äôve eliminated them for image loading too (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35041\">#35041</a>).</p><p>We‚Äôve continued <strong>splitting up our massive script crate</strong> (<a href=\"https://github.com/jdm\">@jdm</a>, <a href=\"https://github.com/servo/servo/pull/34359\">#34359</a>, <a href=\"https://github.com/servo/servo/pull/35157\">#35157</a>, <a href=\"https://github.com/servo/servo/pull/35169\">#35169</a>, <a href=\"https://github.com/servo/servo/pull/35172\">#35172</a>), which will eventually make Servo much faster to build.</p><p>We now run <strong>CI smoketests on OpenHarmony</strong> using a real device (<a href=\"https://github.com/jschwe\">@jschwe</a>, <a href=\"https://github.com/mukilan\">@mukilan</a>, <a href=\"https://github.com/servo/servo/pull/35006\">#35006</a>), increasing confidence in your changes beyond compile-time errors.</p><p>We‚Äôve also tripled our <strong>self-hosted CI runner capacity</strong> (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/servo/servo/pull/34983\">#34983</a>, <a href=\"https://github.com/servo/servo/pull/35002\">#35002</a>), making concurrent Windows and macOS builds possible without falling back to the much slower GitHub-hosted runners.</p><p>Servo can‚Äôt yet run WebDriver-based tests on <a href=\"https://wpt.fyi\">wpt.fyi</a>, <a href=\"https://wpt.servo.org\">wpt.servo.org</a>, or CI, because the  executor for the <a href=\"https://web-platform-tests.org\">Web Platform Tests</a> does not support testdriver.js.\n does, though, so we‚Äôve started fixing test regressions with that executor with the goal of eventually switching to it (<a href=\"https://github.com/jdm\">@jdm</a>, <a href=\"https://github.com/servo/servo/pull/34957\">#34957</a>, <a href=\"https://github.com/servo/servo/pull/34997\">#34997</a>).</p><p>Thanks again for your generous support!\nWe are now receiving  (‚àí11.4% over December) in recurring donations.\nWith this money, we‚Äôve been able to expand our capacity for <a href=\"https://ci0.servo.org\">self-hosted</a><a href=\"https://ci1.servo.org\">CI</a><a href=\"https://ci2.servo.org\">runners</a> on Windows, Linux, and macOS builds, <strong>halving  build times</strong> from over an hour to under 30 minutes!</p><p>Servo is also on <a href=\"https://thanks.dev\">thanks.dev</a>, and already  (+5 over December) that depend on Servo are sponsoring us there.\nIf you use Servo libraries like <a href=\"https://crates.io/crates/url/reverse_dependencies\">url</a>, <a href=\"https://crates.io/crates/html5ever/reverse_dependencies\">html5ever</a>, <a href=\"https://crates.io/crates/selectors/reverse_dependencies\">selectors</a>, or <a href=\"https://crates.io/crates/cssparser/reverse_dependencies\">cssparser</a>, signing up for <a href=\"https://thanks.dev\">thanks.dev</a> could be a good way for you (or your employer) to give back to the community.</p><p>As always, use of these funds will be decided transparently in the Technical Steering Committee.\nFor more details, head to our <a href=\"https://servo.org/sponsorship/\">Sponsorship page</a>.</p>","contentLength":3866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iuq74e/this_month_in_servo_new_webview_api_relative/"},{"title":"Is this architecture possible without using haproxy but nginx(in rocky linux 9)?","url":"https://www.reddit.com/r/kubernetes/comments/1iupwhs/is_this_architecture_possible_without_using/","date":1740141445,"author":"/u/Keeper-Name_2271","guid":8575,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alerting from Prometheus and Grafana with kube-prometheus-stack","url":"https://www.reddit.com/r/kubernetes/comments/1iupvn8/alerting_from_prometheus_and_grafana_with/","date":1740141365,"author":"/u/HumanResult3379","guid":8917,"unread":true,"content":"<p>In Grafana page's , I find the built-in alert rules named .</p><p>I set Slack Contact points. But when the Alert Firing, it didn't send to Slack.</p><p>If I create a customized alert in Grafana, it can be sent to Slack. So does the alert-rules above only for seeing?</p><p>By the way, I find almost the same alert in Prometheus' AlertManager. I set a slack notification endpoint and the messages been sent there!</p><ol><li>Are the prometheus' alert-rules the same as  in Grafana Alert rules page like the picture above?</li><li>If want send alert from Grafana, does it only possible use new created alert rule manually in Grafana?</li></ol>","contentLength":589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Have we hit a scaling wall in base models? (non reasoning)","url":"https://www.reddit.com/r/artificial/comments/1iupqgp/have_we_hit_a_scaling_wall_in_base_models_non/","date":1740140885,"author":"/u/CH1997H","guid":9078,"unread":true,"content":"<p>Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 Sonnet</p><p>Yet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the \"scaling laws\" where the chart just says \"line goes up\")</p><p>Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scaling</p><p>It looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it</p>","contentLength":850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a new playground for Go","url":"https://codiew.io/ide?t=go","date":1740140631,"author":"/u/Halabooda","guid":8597,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iupnrn/i_built_a_new_playground_for_go/"},{"title":"[D] Have we hit a scaling wall in base models? (non reasoning)","url":"https://www.reddit.com/r/MachineLearning/comments/1iupnet/d_have_we_hit_a_scaling_wall_in_base_models_non/","date":1740140600,"author":"/u/CH1997H","guid":8632,"unread":true,"content":"<p>Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 Sonnet</p><p>Yet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the \"scaling laws\" where the chart just says \"line goes up\")</p><p>Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scaling</p><p>It looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it</p>","contentLength":850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created A Easy to use Rust Web Framework","url":"https://www.reddit.com/r/rust/comments/1iuplg1/i_created_a_easy_to_use_rust_web_framework/","date":1740140417,"author":"/u/Rough_Shopping_6547","guid":8832,"unread":true,"content":"<p>I just published my  project!</p><p>I realized there isn‚Äôt a single easy-to-use, plug-and-play Rust web framework out there (at least to my knowledge), so I decided to create my own.</p><p>I'd love to hear your thoughts on it!</p>","contentLength":214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Junior, Trying to understand why startups use golang for backend","url":"https://www.reddit.com/r/golang/comments/1iup4di/junior_trying_to_understand_why_startups_use/","date":1740138781,"author":"/u/FriendshipOk6564","guid":8573,"unread":true,"content":"<p>Hello,i just took a look at the website 'who is hiring' and saw a lot of startups using ruby on rails and golang in their stack and i'm confuse, the path isn't normally mvp in rails and after some companies will rewrite their wall backend at some point in something like Java spring? it append for netflix but also a big company where i live. Why would they mixte ror and golang? Those it mean they are rewriting their ror in a microservice architecture in go?</p>","contentLength":460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to properly prepare monorepos in Golang and is it worth it?","url":"https://www.reddit.com/r/golang/comments/1iuoppk/how_to_properly_prepare_monorepos_in_golang_and/","date":1740137237,"author":"/u/GoDuffer","guid":8780,"unread":true,"content":"<p>Hello everyone. At the moment I am writing a report on the topic of a monorepo in order to close my internship at the university.</p><p>Since I am a Go developer (or at least I aspire to be one), I decided to make a monorepo in Go.</p><p>The first thing I came across was an article from Uber about how they use Bazel and I started digging in this direction.</p><p>And then I realized that it was too complicated for small projects and I became interested.</p><p>Does it make sense to use a monorepo on small projects? If not, how to split the application into services? Or store each service in a separate repository.</p><p>In Java, everything is trivially simple with their modules and Gradle. Yes, Go has modules and a workspace, but let's be honest, this is not the level of Gradle.</p><p>As a result, we have that Bazel is too complicated for simple projects, and gowork seems somehow cut down after Gradle.</p><ol><li><p>Monorepo or polyrepo for Go?</p></li><li><p>Is there anything other than go work and Bazel?</p></li><li><p>What is the correct way to split a Go project so that it looks like a Solution in C#, or modules in Java/Gradle?</p></li></ol><p>It is quite possible that I really don't understand the architecture of Go projects, I will be glad if you point me in the right direction.</p>","contentLength":1196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Deeper Love of Go (Go 1.24 early access edition)","url":"https://bitfieldconsulting.com/books/deeper","date":1740136216,"author":"/u/bitfieldconsulting","guid":8525,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iuogi4/the_deeper_love_of_go_go_124_early_access_edition/"},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1iuob4d/weekly_share_your_victories_thread/","date":1740135633,"author":"/u/gctaylor","guid":8500,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI and the future of work - an EU perspective","url":"https://v.redd.it/cx0l3st20hke1","date":1740134098,"author":"/u/snehens","guid":8653,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iunxt8/ai_and_the_future_of_work_an_eu_perspective/"},{"title":"Getting organised! ¬∑ AerynOS","url":"https://github.com/orgs/AerynOS/discussions/37","date":1740133024,"author":"/u/Wooden-Opposite3557","guid":8720,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iunoxx/getting_organised_aerynos/"},{"title":"ChatGPT took an oath to protect its own.üòÑü§ñ","url":"https://www.reddit.com/r/artificial/comments/1iuno62/chatgpt_took_an_oath_to_protect_its_own/","date":1740132932,"author":"/u/snehens","guid":8526,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My experience after switching from Java to Go","url":"https://www.reddit.com/r/golang/comments/1iuni44/my_experience_after_switching_from_java_to_go/","date":1740132223,"author":"/u/hosmanagic","guid":8498,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/hosmanagic\"> /u/hosmanagic </a>","contentLength":33,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AVR microcontrollers are now officially maintained!","url":"https://www.reddit.com/r/rust/comments/1iunfgx/avr_microcontrollers_are_now_officially_maintained/","date":1740131957,"author":"/u/Patryk27","guid":8499,"unread":true,"content":"<p>AVRs are cute &amp; tiny microcontrollers from Atmel - you might've heard about ATmega328p used in Arduino Uno, for example:</p><p>Every week we're marching towards better AVR support in Rust and as of today I can proudly say: we don't need no `target.json`s anymore + we've got an official maintainer! (points finger at self)</p><p>So far AVRs remain tier 3, but at least it's waay easier to use them now - just target `avr-none` and provide `-C target-cpu` so that rustc &amp; llvm know which specific microcontroller you're building for; <a href=\"https://github.com/llvm/llvm-project/pull/118015\">a couple</a> of <a href=\"https://github.com/llvm/llvm-project/pull/121498\">important</a> codegen <a href=\"https://github.com/llvm/llvm-project/pull/106722\">fixes</a> are also coming together with rustc's upgrade to LLVM 20, hoping to wrap up on <a href=\"https://github.com/Rahix/avr-hal/pull/585\">https://github.com/Rahix/avr-hal/pull/585</a> over the coming days.</p><p>I'd like to take this moment to thank <a href=\"https://github.com/benshi001\">https://github.com/benshi001</a> for his continued support and code reviews on the LLVM's side - let AVR flourish!</p>","contentLength":845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sponsoring Rust Developers","url":"https://www.reddit.com/r/rust/comments/1iun7oj/sponsoring_rust_developers/","date":1740131048,"author":"/u/szabgab","guid":8855,"unread":true,"content":"<p>One of the \"findings\" of my <a href=\"https://www.reddit.com/r/rust/comments/1ital1t/why_dont_you_use_rust_at_your_company/\">previous question</a> was that some crates are missing or not mature enough to be used.</p><p>If you would like to use Rust you can hope that those gaps will be closed in time or you can do something about it. If you have the time and expertise you can get involved in the needed projects, but there is a much easier and less time-consuming way. You and/or your company can sponsor the development efforts.</p><p>Allocating 10-20 USD / month by an individual or 1000-2000 USD month by a small company does not sound like a big investment and many such sponsors can make a huge difference together.</p><p>One way to find who to sponsor is to find the developers of your dependencies. For that visit the <a href=\"https://github.com/sponsors/explore\">Explore GitHub Sponsors</a> page. On the left-hand side select the \"Cargo\" ecosystem. That will show you the individuals and the organizations that you currently rely upon that also accept sponsorship.</p><p>I've also created a page listing some of the <a href=\"https://rust.code-maven.com/sponsoring\">people and project</a> who develop Rust and could be sponsored. For some of them I've also included background information.</p>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My experience with the GNOME Desktop - from despised to loved","url":"https://www.reddit.com/r/linux/comments/1iun2fo/my_experience_with_the_gnome_desktop_from/","date":1740130417,"author":"/u/Fishsven","guid":8933,"unread":true,"content":"<p> I started my Linux journey with Pop!_OS, and I hated the wasted space of the panel-like dock. It took me a while for me to return to GNOME as I was discovering KDE Plasma's (5.24) customization potential. I loved it at first, but I noticed how the DE slowly became unstable after a lot of customising (Plasma has GREATLY improved by now, last time I tried 5.27 on Q4OS and it was blazing fast and rock solid). I was annoyed at how people took a liking to the hideous DE known as GNOME, and for me there was little difference between it and Windows 8, as they were basically tablet centric with GNOME and it's wasted space.</p><p> I eventually got tired of Plasma, because it had way too many features that I didn¬¥t wan¬¥t to use. Tried XFCE, MATE and Budgie, and they felt too outdated for my liking; Budgie felt off. I decided to give GNOME a shot and installed Ubuntu 22.04. For once I was starting to like GNOME. It felt more unified and simple than KDE, but just more modern than the other desktops. However, this was NOT stock GNOME. I installed vanilla GNOME on the same OS and decided to give it a shot.</p><p> Moving on from Ubuntu's Yaru theme to Adwaita felt like a MASSIVE downgrade. Except the looks, GNOME's true workflow actually started to make sense to me and it was more productive than any desktop I tried. Of course, I installed some extensions like Blur my Shell, but I can use GNOME without extensions nowadays. As I'm writing this, GNOME 48 would bring a new Adwaita font with Inter as it's base, which will improve the looks of GNOME by a bit, IMO. Currently using Zorin OS, which has a GNOME theme that is MILES better compared to Libadwaita / Adwaita. </p><p> What I understood is GNOME is not all about looks, it makes the UI simpler and easier to understand, with ONLY the things you need, and it stays out of your way and focuses on your work. It might be dumbing down the desktop for some, but that's exactly what GNOME's for. A solid philosophy IMO- but definitely lagging in some important areas. </p>","contentLength":2009,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How donations helped the LibreOffice project and community in 2024","url":"https://blog.documentfoundation.org/blog/2025/02/21/how-your-donations-helped-the-libreoffice-project-in-2024/","date":1740130087,"author":"/u/themikeosguy","guid":8630,"unread":true,"content":"<p>Thank you for visiting our website and your interest in our services and products. As the protection of your personal data is an important concern for us, please click on the \"More information\" link to access our Privacy Policy page - which will open in a separate browser tab - where we explain what information we collect during your visit to our website, how it is processed, and whether or how it may be used.\nOnce you have carefully read our Privacy Policy page, close the browser tab to return to this page and click on the \"Save Preferences\" button under this text to acknowledge it, close the dialogue and return to the website.<p>\nWe take all the necessary technical and organisational security measures to protect your personal data from loss and misuse. Your data is stored in a secure operating environment that is not accessible to the public.</p></p>","contentLength":853,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iumzoe/how_donations_helped_the_libreoffice_project_and/"},{"title":"reddittui - A terminal browser for reddit","url":"https://github.com/tonymajestro/reddit-tui","date":1740124441,"author":"/u/tmajest","guid":8438,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iulout/reddittui_a_terminal_browser_for_reddit/"},{"title":"Sharing my Open Source Project: Realtime Messaging Platform Built with Go & React (Fullstack)","url":"https://github.com/JoyalAJohney/Realtime-distributed-chat","date":1740113598,"author":"/u/BruceWayn_","guid":8916,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuivtv/sharing_my_open_source_project_realtime_messaging/"},{"title":"Minecraft from scratch with only modern OpenGL","url":"https://github.com/GianlucaP106/minecraft","date":1740108768,"author":"/u/One_Mess_1093","guid":8378,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuhfcq/minecraft_from_scratch_with_only_modern_opengl/"},{"title":"Contribute by filing bugs. You'll feel all warm and fuzzy inside.","url":"https://www.reddit.com/r/linux/comments/1iugim6/contribute_by_filing_bugs_youll_feel_all_warm_and/","date":1740105949,"author":"/u/billhughes1960","guid":8527,"unread":true,"content":"<p>As a lifelong Linux user, I believe strongly in giving back to the open-source community. While I'm not a developer myself, I've found another way to contribute: filing bug reports.</p><p>I'll admit my early attempts were probably pretty rough ‚Äì missing crucial context and details. But practice makes perfect (or at least close!), and these days my bug reports are often addressed within a day or so.</p><p>There's something incredibly satisfying about uncovering a problem, meticulously documenting it, submitting a report, seeing it assigned to someone, and finally witnessing the fix. It's a tangible way to make a difference in the software we all rely on.</p><p>This level of responsiveness and respect simply doesn't exist in proprietary ecosystems. I've tried reporting bugs on Windows and macOS with little success ‚Äì it often feels like shouting into the void. But in the open-source world, even smaller projects welcome contributions and treat you seriously.</p><p>So, I encourage everyone to embrace bug reporting! Start with a simpler project to get comfortable with the process, then gradually tackle more complex ones. Not only will you be improving the software for everyone, but you'll also experience that warm glow of knowing you made a positive impact.</p>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linus Torvalds responds to Christoph Hellwig","url":"https://lore.kernel.org/rust-for-linux/CAHk-=wgLbz1Bm8QhmJ4dJGSmTuV5w_R0Gwvg5kHrYr4Ko9dUHQ@mail.gmail.com/","date":1740105034,"author":"/u/bik1230","guid":7580,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iug7u9/linus_torvalds_responds_to_christoph_hellwig/"},{"title":"Linus Torvalds rips into Hellwig for blocking Rust for Linux","url":"https://lore.kernel.org/rust-for-linux/CAHk-=wgLbz1Bm8QhmJ4dJGSmTuV5w_R0Gwvg5kHrYr4Ko9dUHQ@mail.gmail.com/","date":1740102572,"author":"/u/eugay","guid":7578,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iufdhk/linus_torvalds_rips_into_hellwig_for_blocking/"},{"title":"Installed Ubuntu on my Nan's laptop:","url":"https://www.reddit.com/r/linux/comments/1iueq5x/installed_ubuntu_on_my_nans_laptop/","date":1740100681,"author":"/u/Unique_Ad4547","guid":7577,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Libreboot 20241206, 10th revision released! GRUB security fixes, better LVM scanning, non-root USB2 hub support","url":"https://libreboot.org/news/libreboot20241206rev10.html","date":1740100282,"author":"/u/libreleah","guid":8629,"unread":true,"content":"<p>Article published by: Leah Rowe</p><p>Date of publication: 18 February 2025</p><p>Today‚Äôs Libreboot 20241206 revision is the 10th revision in the Libreboot 20241206 stable release series. The changelog on this page is written, relative to Libreboot 20241206 revision 9 which was released on 12 February 2025. The  Libreboot 20241206 release came out on 6 December 2024. You can find the full list of revisions <a href=\"https://libreboot.org/news/libreboot20241206.Revisions.html\">here</a> and the original release <a href=\"https://libreboot.org/news/libreboot20241206.html\">here</a>.</p><div><h2>Open source BIOS/UEFI firmware</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot20241206rev10.html#open-source-biosuefi-firmware\">[link]</a></div><p>Libreboot is a free/open source BIOS/UEFI replacement on x86 and ARM, providing boot firmware that initialises the hardware in your computer, to then load an operating system (e.g.&nbsp;Linux/BSD). It is specifically a , in the same way that Debian is a Linux distribution. It provides an automated build system to produce coreboot ROM images with a variety of payloads such as GRUB or SeaBIOS, with regular well-tested releases to make coreboot as easy to use as possible for non-technical users. From a project management perspective, this works in  the same way as a Linux distro, providing a source-based package manager (called lbmk) which patches sources and compiles coreboot images. It makes use of <a href=\"https://www.coreboot.org/\">coreboot</a> for hardware initialisation, and then a payload such as <a href=\"https://www.seabios.org/SeaBIOS\">SeaBIOS</a> or <a href=\"https://www.gnu.org/software/grub/\">GRUB</a> to boot your operating system; on ARM(chromebooks), we provide  (as a coreboot payload).</p><p>We also provide an experimental U-Boot setup on x86, as a coreboot payload for providing a minimal UEFI implementation.</p><p>Normally, revisions would only be documented on the <a href=\"https://libreboot.org/news/libreboot20241206.Revisions.html\">Libreboot 20241206 revisions page</a>, but this revision contains , so it was decided that there should be a full announcement, to ensure that more people see it.</p><div><h2>Summarised list of changes</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot20241206rev10.html#summarised-list-of-changes\">[link]</a></div><p>GRUB released  to its main branch, fixing a large number of security issues. You can read about them here:</p><p>This updates GRUB to revision <code>4dc6166571645780c459dde2cdc1b001a5ec844c</code> from 18 February 2025. Several OOB heap writes, buffer overflows, use after frees and so on, are now prevented with this update.</p><p>In addition to the security fixes, several out-of-tree fixes from Libreboot‚Äôs main branch have been merged for GRUB, fixing bugs in the xHCI driver, and adding support for non-root USB2 hubs on platforms that use the  GRUB tree.</p><p>Changes to the GRUB configuration have been made, to make scanning of LVM volume/group names more reliable, including on full-disk-encryption setups. More such changes are planned for the next major release; the current changes are very minor.</p>","contentLength":2480,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iuel79/libreboot_20241206_10th_revision_released_grub/"},{"title":"BritCSS: Fixes CSS to use non-American English","url":"https://github.com/DeclanChidlow/BritCSS","date":1740096884,"author":"/u/ValenceTheHuman","guid":8397,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iudp7p/britcss_fixes_css_to_use_nonamerican_english/"},{"title":"Docker Hub will only allow an unauthenticated 10/pulls per hour starting March 1st","url":"https://docs.docker.com/docker-hub/usage/","date":1740096401,"author":"/u/onedr0p","guid":7514,"unread":true,"content":"<blockquote><p>Starting April 1, 2025, all users with a Pro, Team, or Business\nsubscription will have unlimited Docker Hub pulls with fair use.\nUnauthenticated users and users with a free Personal account have the\nfollowing pull limits:</p><ul><li>Unauthenticated users: 10 pulls/hour</li><li>Authenticated users with a free account: 100 pulls/hour</li></ul></blockquote><p>The following table provides an overview of the included usage and limits for each\nuser type, subject to fair use:</p><div><table><thead><tr><th>Number of public repositories</th><th>Number of private repositories</th></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>10 per IPv4 address or IPv6 /64 subnet</td></tr></tbody></table></div><p>For more details, see the following:</p><p>When utilizing the Docker Platform, users should be aware that excessive data\ntransfer, pull rates, or data storage can lead to throttling, or additional\ncharges. To ensure fair resource usage and maintain service quality, we reserve\nthe right to impose restrictions or apply additional charges to accounts\nexhibiting excessive data and storage consumption.</p><p>Docker Hub has an abuse rate limit to protect the application and\ninfrastructure. This limit applies to all requests to Hub properties including\nweb pages, APIs, and image pulls. The limit is applied per IPv4 address or per\nIPv6 /64 subnet, and while the limit changes over time depending on load and\nother factors, it's in the order of thousands of requests per minute. The abuse\nlimit applies to all users equally regardless of account level.</p><p>You can differentiate between the pull rate limit and abuse rate limit by\nlooking at the error code. The abuse limit returns a simple  response. The pull limit returns a longer error message that includes\na link to documentation.</p>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iudj0d/docker_hub_will_only_allow_an_unauthenticated/"},{"title":"[Media] Rust powered flight radar","url":"https://www.reddit.com/r/rust/comments/1iubs4r/media_rust_powered_flight_radar/","date":1740091662,"author":"/u/Confident-Alarm-6911","guid":7579,"unread":true,"content":"<p>So, consider this mix: I have thing for retro-interfaces with monochromatic displays, I wanted to learn rust and do something with sdr radio, I live next to the airport. And that‚Äôs how my small radar comes to life üòé</p><p>Hardware: ESP32C3, 1.5 inch i2c oled display, some encoder. RTL-SDR V4 running on my local linux machine and small endpoint to serve ADS-B data via http.</p><p>Firmware written in rust 2021 edition. Libraries: mostly std and esp-idf-svc + rtos (not necessary, but I wanted to try it)</p><p>I‚Äôm pretty content with this small project as it is my first attempt to build something in Rust. Now I want to design 3D printable case, do some polishing on software side, and publish it as open source.</p><p>I wanted to post video but it says I can not do this in this community, so only pic</p>","contentLength":784,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IaaC Simplified: Automating EC2 Deployments with GitHub Actions, Terraform, Docker & Distribution Registry | Vue & Node admin panel framework","url":"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/","date":1740090590,"author":"/u/Unerring-Ocean","guid":7467,"unread":true,"content":"<p>This guide shows how to deploy own Docker apps (with AdminForth as example) to Amazon EC2 instance with Docker and Terraform involving Docker self-hosted registry.</p><ul><li>GitHub actions Free plan which includes 2000 minutes per month (1000 of 2-minute builds per month - more then enough for many projects, if you are not running tests etc). Extra builds would cost  per minute.</li><li>AWS account where we will auto-spawn EC2 instance. We will use  instance (2 vCPUs, 2GB RAM) which costs  per month in  region (cheapest region). Also it will take  per month for EBS gp2 storage (20GB) for EC2 instance</li></ul><p>This is it, registry will be auto-spawned on EC2 instance, so no extra costs for it. Also GitHub storage is not used, so no extra costs for it.</p><p>The setup has next features:</p><ul><li>Build process is done using IaaC approach with HashiCorp Terraform, so almoast no manual actions are needed from you. Every resource including EC2 server instance is described in code which is commited to repo so no manual clicks are needed.</li><li>Docker build process is done on GitHub actions, so EC2 server is not overloaded</li><li>Changes in infrastructure including changing server type, adding S3 Bucket, changing size of sever disk is also can be done by commiting code to repo.</li><li>Docker images and cache are stored on EC2 server, so no extra costs for Docker registry are needed.</li><li>Total build time for average commit to AdminForth app (with Vite rebuilds) is around 2 minutes.</li></ul><p>Previously we had a blog post about <a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions/\">deploying AdminForth to EC2 with Terraform without registry</a>. That method might work well but has a significant disadvantage - build process happens on EC2 itself and uses EC2 RAM and CPU. This can be a problem if your EC2 instance is well-loaded without extra free resources. Moreover, low-end EC2 instances have a small amount of RAM and CPU, so build process which involves vite/tsc/etc can be slow or even fail.</p><p>So obviously to solve this problem we need to move the build process to CI, however it introduces new chellenges and we will solve them in this post.</p><p>Quick difference between approaches from previous post and current post:</p><table><thead><tr></tr></thead><tbody><tr><td>How and where docker build happens</td><td>Source code is rsync-ed from CI to EC2 and docker build is done there</td><td>Docker build is done on CI and docker image is pushed to registry (in this post we run registry automatically on EC2)</td></tr><tr><td>How Docker build layers are cached</td><td>GitHub actions has no own Docker cache out of the box, so it should be stored in dedicated place (we use self-hosted registry on the EC2 as it is free)</td></tr><tr><td>Simpler setup with less code (we don't need code to run and secure registry, and don't need extra cache setup as is naturally persisted on EC2).</td><td>Build is done on CI, so EC2 server is not overloaded. For most cases CI builds are faster than on EC2. Plus time is saved because we don't need to rsync source code to EC2</td></tr><tr><td>Build on EC2 requires additional server RAM / overloads CPU</td><td>More terraform code is needed. registry cache might require small extra space on EC2</td></tr></tbody></table><h2>Chellenges when you build on CI<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#chellenges-when-you-build-on-ci\" aria-label=\"Direct link to Chellenges when you build on CI\" title=\"Direct link to Chellenges when you build on CI\">‚Äã</a></h2><p>When you move build process to CI you have to solve next chellenges:</p><ol><li>We need to deliver built docker images to EC2 somehow (and only we)</li><li>We need to persist cache between builds</li></ol><h4>Exporing images to tar files<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#exporing-images-to-tar-files\" aria-label=\"Direct link to Exporing images to tar files\" title=\"Direct link to Exporing images to tar files\">‚Äã</a></h4><p>Simplest option which you can find is save docker images to tar files and deliver them to EC2. We can easily do it in terraform (using  command on CI and  command on EC2). However this option has a significant disadvantage - it is slow. Docker images are big (always include all layers, without any options), so it takes infinity to do save/load and another infinity to transfer them to EC2 (via relatively slow rsync/SSH and relatively slow GitHub actions outbound connection).</p><p>Faster, right option which we will use here - involve Docker registry. Registry is a repository which stores docker images. It does it in a smart way - it saves each image as several layers, so if you will update last layer, then only last layer will be pushed to registry and then only last will be pulled to EC2.\nTo give you row compare - whole-layers image might take , but last layer created by  command might take . And most builds you will do only last layer changes, so it will be 20 times faster to push/pull last layer than whole image.\nAnd this is not all, registry uses TLS HTTP protocol so it is faster then SSH/rsync encrypted connection.</p><p>Of course you have to care about a way of registry authentication (so only you and your CI/EC2 can push/pull images).</p><p>What docker registry can you use? Pretty known options:</p><ol><li>Docker Hub - most famous. It is free for public images, so literally every opensource project uses it. However it is not free for private images, and you have to pay for it. In this post we are considering you might do development for commercial project with tight budget, so we will not use it.</li><li>GHCR - Registry from Google. Has free plan but allows to store only 500MB and allows to transfer 1GB of traffic per month. Then you pay for every extra GB in storage and traffic. Probably small images will fit in this plan, but generally even alpine-based docker images are bigger than 500MB, so it is not a good option.</li><li>Self-hosted registry web system. In our software development company, we use Harbor. It is a powerful free open-source registry that can be installed to own server. It allows pushing and pulling without limit. Also, it has internal life-cycle rules that cleanup unnecessary images and layers. The main drawbacks of it are that it is not so fast to install and configure, plus you have to get a domain and another powerfull server to run it. So unless you are a software development company, it is not worth using it.</li><li>Self-hosted minimal CNCF Distribution <a href=\"https://distribution.github.io/distribution/\" target=\"_blank\" rel=\"noopener noreferrer\">registry</a> on EC2 itself. So since we already have EC2, we can run registry on it directly. The  container is pretty light-weight and easy to setup and it will not consume a lot of extra CPU/RAM on server. Plus images will be stored close to application so pull will be fast.</li></ol><p>In the post we will use last (4th way). Our terraform will deploy registry automatically, so you don't have to do anything special.</p><p>Docker builds without layer cache persistence are possible but very slow. Most builds only change a couple of layers, and having no ability to cache them will cause the Docker builder to regenerate all layers from scratch. This can, for example, increase the Docker build time from a minute to ten minutes or even more.</p><p>Out of the box, GitHub Actions can't save Docker layers between builds, so you have to use external storage.</p><blockquote><p>Though some CI systems can persist docker build cache, e.g. open-source self-hosted Woodpecker CI allows it out of the box. However GitHub actions which is pretty popular, reasonably can't allow such free storage to anyone</p></blockquote><p>So when build-in Docker cache can't be used, there is one alternative - Docker BuildKit external cache.\nSo BuildKit allows you to connect external storage. There are several options, but most sweet for us is using Docker registry as cache storage (not only as images storage to deliver them to application server).</p><blockquote><p><em>BuildKit cache in Compose issue</em>\nPreviously we used docker compose to build &amp; run our app, it can be used to both build, push and pull images, but has <a href=\"https://github.com/docker/compose/issues/11072#issuecomment-1848974315\" target=\"_blank\" rel=\"noopener noreferrer\">issues with external cache connection</a>. While they are not solved we have to use  command to build images. It is not so bad, but is another point of configuration which we will cover in this post.</p></blockquote><h3>Registry authorization and traffic encryption<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#registry-authorization-and-traffic-encryption\" aria-label=\"Direct link to Registry authorization and traffic encryption\" title=\"Direct link to Registry authorization and traffic encryption\">‚Äã</a></h3><p>Hosting custom CNCF registry, from other hand is a security responsibility.</p><p>If you don't protect it right, someone will be able to push any image to your registry and then pull it to your EC2 instance. This is a big security issue, so we have to protect our registry.</p><p>First of all we need to set some authorization to our registry so everyone who will push/pull images will be authorized. Here we have 2 options: HTTP basic auth and Client certificate auth. We will use first one as it is easier to setup. We will generate basic login and password automatically in terraform so no extra actions are needed from you.</p><p>But this is not enough. Basic auth is not encrypted, so someone can perform MITM attack and get your credentials. So we need to encrypt traffic between CI and registry. We can do it by using TLS certificates. So we will generate self-signed TLS certificates, and attach them to our registry.</p><p>Assume you have your AdminForth project in .</p><p>Create file  in :</p><p>create folder  and create file  inside:</p><h2>Step 3 - create a SSH keypair<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-3---create-a-ssh-keypair\" aria-label=\"Direct link to Step 3 - create a SSH keypair\" title=\"Direct link to Step 3 - create a SSH keypair\">‚Äã</a></h2><p>Make sure you are still in  folder, run next command:</p><p>Now it should create  and  files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.</p><h2>Step 4 - create TLS certificates to encrypt traffic between CI and registry<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-4---create-tls-certificates-to-encrypt-traffic-between-ci-and-registry\" aria-label=\"Direct link to Step 4 - create TLS certificates to encrypt traffic between CI and registry\" title=\"Direct link to Step 4 - create TLS certificates to encrypt traffic between CI and registry\">‚Äã</a></h2><p>Make sure you are still in  folder, run next command:</p><p>Run next command to create TLS certificates:</p><p>This will create  and  files.</p><h2>Step 5 - .gitignore file<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-5---gitignore-file\" aria-label=\"Direct link to Step 5 - .gitignore file\" title=\"Direct link to Step 5 - .gitignore file\">‚Äã</a></h2><p>Create  file with next content:</p><h2>Step 6 - buildx bake file<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-6---buildx-bake-file\" aria-label=\"Direct link to Step 6 - buildx bake file\" title=\"Direct link to Step 6 - buildx bake file\">‚Äã</a></h2><p>Create file :</p><h2>Step 7 - main terraform file main.tf<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-7---main-terraform-file-maintf\" aria-label=\"Direct link to Step 7 - main terraform file main.tf\" title=\"Direct link to Step 7 - main terraform file main.tf\">‚Äã</a></h2><p>Create file  in  folder:</p><blockquote><p>üëÜ Replace  with your app name (no spaces, only underscores or letters)</p></blockquote><h3>Step 7.1 - Configure AWS Profile<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-71---configure-aws-profile\" aria-label=\"Direct link to Step 7.1 - Configure AWS Profile\" title=\"Direct link to Step 7.1 - Configure AWS Profile\">‚Äã</a></h3><p>Open or create file  and add (if not already there):</p><h3>Step 7.2 - Run deployment<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-72---run-deployment\" aria-label=\"Direct link to Step 7.2 - Run deployment\" title=\"Direct link to Step 7.2 - Run deployment\">‚Äã</a></h3><p>To run the deployment first time, you need to run:</p><h2>Step 8 - Migrate state to the cloud<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-8---migrate-state-to-the-cloud\" aria-label=\"Direct link to Step 8 - Migrate state to the cloud\" title=\"Direct link to Step 8 - Migrate state to the cloud\">‚Äã</a></h2><p>First deployment had to create S3 bucket for storing Terraform state. Now we need to migrate the state to the cloud.</p><p>Add to the end of :</p><blockquote><p>üëÜ Replace  with your app name (no spaces, only underscores or letters).\nUnfortunately we can't use variables, HashiCorp thinks it is too dangerous üò•</p></blockquote><p>Now you can delete local  file and  file as they are in the cloud now.</p><h2>Step 9 - CI/CD - Github Actions<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-9---cicd---github-actions\" aria-label=\"Direct link to Step 9 - CI/CD - Github Actions\" title=\"Direct link to Step 9 - CI/CD - Github Actions\">‚Äã</a></h2><p>Create file <code>.github/workflows/deploy.yml</code>:</p><div><div>.github/workflows/deploy.yml</div></div><h3>Step 8.1 - Add secrets to GitHub<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-81---add-secrets-to-github\" aria-label=\"Direct link to Step 8.1 - Add secrets to GitHub\" title=\"Direct link to Step 8.1 - Add secrets to GitHub\">‚Äã</a></h3><p>Go to your GitHub repository, then  -&gt;  -&gt;  and add:</p><ul><li> - your AWS access key</li><li><code>VAULT_AWS_SECRET_ACCESS_KEY</code> - your AWS secret key</li><li> - execute  and paste to GitHub secrets</li><li> - execute  and paste to GitHub secrets</li><li> - execute  and paste to GitHub secrets</li><li> - execute  and paste to GitHub secrets</li></ul><p>Now you can push your changes to GitHub and see how it will be deployed automatically.</p><p>Once you will have sensitive tokens/passwords in your apps you have to store them in a secure way.</p><p>Simplest way is to use GitHub secrets.</p><p>Let's imagine you have  which will be used one of AI-powered plugins of adminforth. We can't put this key to the code, so we have to store it in GitHub secrets.</p><p>Open your GitHub repository, then  -&gt;  -&gt;  and add  with your key.</p><p>Now open GitHub actions file and add it to the  section:</p><div><div>.github/workflows/deploy.yml</div></div><p>Next add it to the  script:</p><p>In the same way you can add any other secrets to your GitHub actions.</p><h3>Out of space on EC2 instance? Extend EBS volume<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#out-of-space-on-ec2-instance-extend-ebs-volume\" aria-label=\"Direct link to Out of space on EC2 instance? Extend EBS volume\" title=\"Direct link to Out of space on EC2 instance? Extend EBS volume\">‚Äã</a></h3><p>To upgrade EBS volume size you have to do next steps:</p><p>This will increase physical size of EBS volume, but you have to increase filesystem size too.</p><blockquote><p>You can find your EC2 IP in AWS console by visiting EC2 -&gt; Instances -&gt; Your instance -&gt; IPv4 Public IP</p></blockquote><p>This would show something like this:</p><p>Here we see that  is our disk and  is our partition.</p><p>Now to extend partition run:</p><p>This will extend partition to the full disk size. No reboot is needed.</p>","contentLength":11283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iubdcw/iaac_simplified_automating_ec2_deployments_with/"},{"title":"Using one ingress controller to proxy to another cluster","url":"https://www.reddit.com/r/kubernetes/comments/1iub1dp/using_one_ingress_controller_to_proxy_to_another/","date":1740089739,"author":"/u/djjudas21","guid":7472,"unread":true,"content":"<p>I'm planning a migration between two on-premise clusters. Both clusters are on the same network, with an ingress IP provided by MetalLB. The network is behind a NAT gateway with a single public IP, and port forwarding.</p><p>I need to start moving applications from cluster A to cluster B, but I can only set my port forwarding to point to cluster A  cluster B.</p><p>I'm trying to figure out if there's a way to use one cluster's ingress controller to proxy some sites to the other cluster's ingress controller. Something like SSL passthrough.</p><p>I've tried to configure the following on cluster B to proxy some specific site back to cluster A, with SSL passthrough as cluster A is running all its sites with TLS enabled. Unfortunately it isn't working properly and attempting to connect to <a href=\"http://app.example.com\">app.example.com</a> on cluster B only presents the default ingress controller self-signed cert, not the real app cert from cluster A.</p><pre><code>apiVersion: v1 kind: Service metadata: name: microk8s-proxy namespace: default spec: type: ExternalName externalName: ingress-a.example.com --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" name: microk8s-proxy namespace: default spec: ingressClassName: public rules: - host: app.example.com http: paths: - backend: service: name: microk8s-proxy port: number: 443 path: / pathType: Prefix </code></pre><p>I've been working on this for hours and can't get it working. Seems like it might be easier to just schedule a day of downtime for all sites! Thanks</p>","contentLength":1570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google's Shift to Rust Programming Cuts Android Memory Vulnerabilities by 68%","url":"https://thehackernews.com/2024/09/googles-shift-to-rust-programming-cuts.html","date":1740089695,"author":"/u/Unerring-Ocean","guid":7468,"unread":true,"content":"<p>Google has revealed that its transition to memory-safe languages such as Rust as part of its secure-by-design approach has led to the percentage of memory-safe vulnerabilities discovered in Android dropping from 76% to 24% over a period of six years.</p><p>The tech giant said focusing on <a href=\"https://blog.google/technology/safety-security/tackling-cybersecurity-vulnerabilities-through-secure-by-design/\" rel=\"noopener\" target=\"_blank\">Safe Coding</a> for new features not only reduces the overall security risk of a codebase, but also makes the switch more \"scalable and cost-effective.\"</p><p>Eventually, this leads to a drop in memory safety vulnerabilities as new memory unsafe development slows down after a certain period of time, and new memory safe development takes over, Google's Jeff Vander Stoep and Alex Rebert <a href=\"https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html\" rel=\"noopener\" target=\"_blank\">said</a> in a post shared with The Hacker News.</p><p>Perhaps even more interestingly, the number of memory safety vulnerabilities tends to register a drop notwithstanding an increase in the quantity of new memory unsafe code.</p><p>The paradox is explained by the fact that vulnerabilities decay exponentially, with a study finding that a high number of vulnerabilities often reside in new or recently modified code.</p><p>\"The problem is overwhelmingly with new code, necessitating a fundamental change in how we develop code,\" Vander Stoep and Rebert noted. \"Code matures and gets safer with time, exponentially, making the returns on investments like rewrites diminish over time as code gets older.\"</p><p>Google, which <a href=\"https://thehackernews.com/2021/04/android-to-support-rust-programming.html\" rel=\"noopener\" target=\"_blank\">formally announced</a> its plans to support the Rust programming language in Android way back in April 2021, said it began prioritizing transitioning new development to memory-safe languages around 2019.</p><p>As a result, the number of memory safety vulnerabilities discovered in the operating system has declined from <a href=\"https://security.googleblog.com/2022/12/memory-safe-languages-in-android-13.html\" rel=\"noopener\" target=\"_blank\">223 in 2019</a> to less than 50 in 2024.</p><p>It also goes without saying that much of the decrease in such flaws is down to advancements in the ways devised to combat them, moving from reactive patching to proactive mitigating to proactive vulnerability discovery using tools like <a href=\"https://thehackernews.com/2023/12/google-using-clang-sanitizers-to.html\" rel=\"noopener\" target=\"_blank\">Clang sanitizers</a>.</p><p>The tech giant further noted that memory safety strategies should evolve even more to prioritize \"high-assurance prevention\" by incorporating <a href=\"https://dl.acm.org/doi/10.1145/3651621\" rel=\"noopener\" target=\"_blank\">secure-by-design principles</a> that enshrine security into the very foundations. </p><p>\"Instead of focusing on the interventions applied (mitigations, fuzzing), or attempting to use past performance to predict future security, Safe Coding allows us to make strong assertions about the code's properties and what can or cannot happen based on those properties,\" Vander Stoep and Rebert said.</p><p>That's not all. Google said it is also focusing on offering interoperability between Rust, C++, and Kotlin, instead of code rewrites, as a \"practical and incremental approach\" to embracing memory-safe languages and ultimately eliminating entire vulnerability classes.</p><p>\"Adopting Safe Coding in new code offers a paradigm shift, allowing us to leverage the inherent decay of vulnerabilities to our advantage, even in large existing systems,\" it said.</p><p>\"The concept is simple: once we turn off the tap of new vulnerabilities, they decrease exponentially, making all of our code safer, increasing the effectiveness of security design, and alleviating the scalability challenges associated with existing memory safety strategies such that they can be applied more effectively in a targeted manner.\"</p><p>The development comes as Google touted increased collaboration with Arm's product security and graphics processing unit (GPU) engineering teams to flag multiple shortcomings and elevate the overall security of the GPU software/firmware stack across the Android ecosystem.</p><p>\"Proactive testing is good hygiene as it can lead to the detection and resolution of new vulnerabilities before they're exploited,\" Google and Arm <a href=\"https://security.googleblog.com/2024/09/google-arm-raising-bar-on-gpu-security.html\" rel=\"noopener\" target=\"_blank\">said</a>.</p><div>Found this article interesting?  Follow us on <a href=\"https://twitter.com/thehackersnews\" rel=\"noopener\" target=\"_blank\">Twitter </a> and <a href=\"https://www.linkedin.com/company/thehackernews/\" rel=\"noopener\" target=\"_blank\">LinkedIn</a> to read more exclusive content we post.</div>","contentLength":3792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iub0rk/googles_shift_to_rust_programming_cuts_android/"},{"title":"CustomResourceDefinitions to provision Azure resources such as storage blob","url":"https://www.reddit.com/r/kubernetes/comments/1iuay1o/customresourcedefinitions_to_provision_azure/","date":1740089501,"author":"/u/Valuable-Ad3229","guid":7471,"unread":true,"content":"<p>I am developer working with Azure Kubernetes Service, and I wonder if it is possible to define a CustomResourceDefinitions to provision other Azure resources such as Azure storage blobs, or Azure identities?</p><p>I am mindful that this may be anti-pattern but I am curious. Thank you!</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Are there any theoretical machine learning papers that have significantly helped practitioners?","url":"https://www.reddit.com/r/MachineLearning/comments/1iuanhy/d_are_there_any_theoretical_machine_learning/","date":1740088779,"author":"/u/nihaomundo123","guid":8455,"unread":true,"content":"<p>21M deciding whether or not to specialize in theoretical ML for their math PhD. Specifically, I am interested in</p><p>ii) but NOT interested in papers focusing on improving empirical performance, like the original dropout and batch normalization papers.</p><p>I want to work on something with the potential for deep impact during my PhD, yet still theoretical. When trying to find out if the understanding-based questions in category i) fits this description, however, I could not find much on the web...</p><p><strong>If anyone has any specific examples of papers whose main focus was to understand some phenomena, and that ended up revolutionizing things for practitioners, would appreciate it :)</strong></p>","contentLength":670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The State of Scala & Clojure Surveys: How is functional programming on JVM doing","url":"https://www.jvm-weekly.com/p/the-state-of-scala-and-clojure-surveys","date":1740088259,"author":"/u/ArturSkowronski","guid":8574,"unread":true,"content":"<p>The title might be a bit of an overstatement ‚Äì don‚Äôt expect a very extensive analysis of functional programming trends. I wanted to focus on two surveys that have appeared recently, which tell us a bit about languages that many of you probably used in a previous reincarnation cycle, but have already forgotten about.</p><p>Of course, the usual disclaimer at the beginning ‚Äì we know that surveys tend to show what they feel is worth showing and have a certain narrative power.</p><p>Having that in mind, I think we can begin.</p><p>First, let‚Äôs look at the people who filled out the survey ‚Äì demographics tell us a lot about the quality of the results and what we can expect. We have as many as 232 responses, with 75% Software Engineers and 49.6% people in tech-lead or related areas ‚Äì apparently, many of us wear several hats at once (yes, I know that pain too). It turned out that most of them work on closed-source projects (87.5%), although there‚Äôs no shortage of hardcore open-source folks (18.1%). I think that fairly reflects the state of the industry.</p><p>An overwhelming majority of respondents like or love Scala: 49.1% love it, 44% rather like it. The remaining 6.9% are still undecided, and 0.9% (i.e., 2 people) have fallen into Scala-depression. However here we also hit, to some extent, the ‚Äúpeak‚Äù of people interested in the topic - those willing to fill out the Scala Survey.</p><p>The average age of projects is 7 years, with a median of 6 years ‚Äì that‚Äôs quite‚Ä¶ a lot. It also shows that quite a bit of Scala is legacy projects ‚Äì at least in the surveyed group, ‚Äúgreenfields‚Äù are relatively rare.</p><p>Typelevel (41.8%), Akka (35.3%), ZIO (23.3%), and Play (15.5%). And of course, Spark (7.7%) ‚Äì let‚Äôs not forget that big data elephant in the room, though the days when it was synonymous with Data seem to be behind us. These people have a new best friend.</p><p>Moreover, more than half of the projects (53%) combine different ecosystems such as Akka and Cats, indicating that we‚Äôre building increasingly hybrid beasts. 36.2% are ‚Äúmonogamists‚Äù relying entirely on a single library (ZIO, we‚Äôre looking at you), and 10.8% are the brave ones using only the standard library.</p><p><a href=\"https://www.linkedin.com/article/edit/7298227473140305920/#\" rel=\"\">VirtusLab</a><a href=\"https://github.com/scalameta/metals\" rel=\"\">project here</a></p><p>SBT beats everyone hands down (87.5%). Scala-CLI (11.2%) is relatively new but has decent traction, and Bazel (7.8%) and Maven (7.3%) also have loyal fans. </p><p>22.4% of commercial projects have already switched to Scala 3, but as many as 37% do not plan to. Why? Because as usual, the ecosystem is (still) not fully ready, there‚Äôs a lack of resources, and management is pushing the topic aside. You know that feeling when a new version of a language tempts you, but there are dozens of ‚Äúwork in progress‚Äù branches piling up in the repo‚Ä¶?</p><p>Talking about the problems, the report shows three major ones:</p><ul><li><p><strong>Fragmentation of the ecosystem and migration issues to Scala 3</strong></p></li><li><p><strong>Lack of resources to maintain older projects.</strong></p></li></ul><p><strong>recruiting Scala developers</strong></p><p>Given these recruitment problems, it‚Äôs not surprising that 68.6% allow remote work, which at least somewhat makes life easier for those who have managed to settle in the Bieszczady Mountains or in Bali.</p><ul></ul><p>Despite all these challenges, 88.4% of respondents would still choose Scala for new projects without hesitation. It shows that the JVM community sees great potential in Scala, but also knows that working on its further development and tooling is a marathon, not a sprint.</p><p><a href=\"https://bit.ly/scala-report\" rel=\"\">the report is full of interesting details</a></p><p>Now, let's take a look at the other report I have for you today.</p><p><a href=\"https://www.linkedin.com/article/edit/7298227473140305920/#\" rel=\"\">Alex Miller</a><a href=\"https://clojure.org/news/2024/12/02/state-of-clojure-2024\" rel=\"\">State of Clojure</a></p><p>Let‚Äôs start with what warms the hearts of backend folks the most: does Clojure live in real projects and is it more than just a hobby experiment? Definitely yes! 73% of respondents use Clojure at work, mainly in web development, commercial services, and enterprise applications. Their services often end up in the cloud ‚Äì public (58%) or private (26%). So you can say with confidence that the ‚ÄúLispy DSL‚Äù is conquering more and more server rooms and Docker containers.</p><p>What about team size? Most are small teams (up to 10 people), though there are also true giants ‚Äì Nubank, with over a thousand Clojure developers, is a prime example. It‚Äôs no coincidence they‚Äôre now responsible for the development of the language.</p><p>Regarding the adoption of new versions, things look surprisingly good. As many as 58% have already moved to Clojure 1.12, released in September 2024, which indicates that stability and a lack of painful breaking changes are quite a motivator.</p><p>Here we see that Java 21 LTS already has 54% of users, and Java 8 is losing ground in favor of newer versions (only 9% remain with the old-timer, which is better than in Java itself). Probably for this reason, Clojure plans to raise the base version in subsequent releases (much like Scala, but we‚Äôll talk about that next week).</p><p><a href=\"https://babashka.org/\" rel=\"\">Babashka</a></p><p>An example is Babashka ‚Äì a dialect that enjoys huge popularity (93% of respondents who use dialects had dealt with it) because it allows scripts to be run quickly, without the start-up delays of the JVM. ClojureDart, on the other hand, brings Clojure into the Dart ecosystem, opening new perspectives for web and mobile apps. Other projects like Squint, Jank, and Cherry demonstrate the community‚Äôs ongoing creativity ‚Äì each introduces its own modifications, often experimental, allowing the Clojure philosophy to adapt to entirely new conditions.</p><p>Leiningen and deps.edn continues to vie for space among dependency management tools ‚Äì we can see that deps.edn is gaining strength, and nRepl, REBL, and other plugins help make REPL feel like home.</p><p>A special section of the survey examines people who have just started their adventure with Clojure (less than a year of experience). The report has been tracking programmers‚Äô migration paths to Clojure for years. It appears they still largely come from Java, JavaScript, and Python. Ruby and C++ are in decline, whereas C# is starting to gain slightly ‚Äì perhaps thanks to the ‚Äúfunctional awakening‚Äù in the .NET ecosystem.</p><p><strong>Biggest challenges for newcomers?</strong></p><p>If you‚Äôve read this far, you‚Äôre probably as happy as I am to see that Clojure keeps evolving. On one hand ‚Äì a stable, mature platform, and on the other ‚Äì new dialects, a growing community not just in corporate settings but also in open-source projects and even in the gaming industry (yes, yes, I‚Äôve seen it!). State of Clojure 2024 shows that Lisp on the JVM is still a very strong player: a steady, balanced development without revolutionary changes, yet‚Ä¶ there‚Äôs always something new to discover.</p><p><a href=\"https://clojure.org/news/2024/12/02/state-of-clojure-2024\" rel=\"\">through the full report</a></p><p>Happy coding ‚Äì and until next time in JVM Weekly!</p>","contentLength":6662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuag90/the_state_of_scala_clojure_surveys_how_is/"},{"title":"[R] Detecting LLM Hallucinations using Information Theory","url":"https://www.reddit.com/r/MachineLearning/comments/1iu9ryi/r_detecting_llm_hallucinations_using_information/","date":1740086564,"author":"/u/meltingwaxcandle","guid":7496,"unread":true,"content":"<p>LLM hallucinations and errors are a major challenge, but what if we could predict when they happen? Nature had a great <a href=\"https://www.nature.com/articles/s41586-024-07421-0\">publication</a> on semantic entropy, but I haven't seen many practical guides on production patterns for LLMs.</p><ol><li><strong>Sequence log-probabilities</strong> provides a free, effective way to detect unreliable outputs (can be interpreted as \"LLM confidence\").</li><li><strong>High-confidence responses were nearly twice as accurate</strong> as low-confidence ones (76% vs 45%).</li><li>Using this approach, we can automatically <strong>filter poor responses, introduce human review, or iterative RAG pipelines</strong>.</li></ol><p><strong>Experiment setup is simple</strong>: generate 1000 RAG-supported LLM responses to various questions. Ask experts to blindly evaluate responses for quality. See how much LLM confidence predicts quality.</p><p>Bonus: precision recall curve for an LLM.</p><p>My interpretation is that LLM operates in a higher entropy (less predictable output / flatter token likelihood distributions) regime when it's not confident. So it's dealing with more uncertainty and starts to break down essentially.</p><p>Regardless of your opinions on validity of LLMs, this feels like one of the simplest, but effective methods to catch a bulk of errors. </p>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Project - Deploy Flask App With MySQL on Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iu92sy/learning_project_deploy_flask_app_with_mysql_on/","date":1740084836,"author":"/u/kchandank","guid":8379,"unread":true,"content":"<p>If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.</p><p>In this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.</p><ul><li>Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).</li><li>kubectl installed and configured to interact with your Kubernetes cluster.</li><li>Docker installed on your machine to build and push the Docker image of the Flask app.</li><li>Docker Hub account to push the Docker image.</li></ul><p>You will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:</p><ul></ul><p>Create a <a href=\"http://app.py\">app.py</a> file with following content</p><pre><code>from flask import Flask, jsonify import os import mysql.connector from mysql.connector import Error app = Flask(__name__) def get_db_connection(): \"\"\" Establishes a connection to the MySQL database using environment variables. Expected environment variables: - MYSQL_HOST - MYSQL_DB - MYSQL_USER - MYSQL_PASSWORD \"\"\" host = os.environ.get(\"MYSQL_HOST\", \"localhost\") database = os.environ.get(\"MYSQL_DB\", \"flaskdb\") user = os.environ.get(\"MYSQL_USER\", \"flaskuser\") password = os.environ.get(\"MYSQL_PASSWORD\", \"flaskpass\") try: connection = mysql.connector.connect( host=host, database=database, user=user, password=password ) if connection.is_connected(): return connection except Error as e: app.logger.error(f\"Error connecting to MySQL: {e}\") return None u/app.route(\"/\") def index(): return f\"Welcome to the Flask App running in {os.environ.get('APP_ENV', 'development')} mode!\" u/app.route(\"/dbtest\") def db_test(): \"\"\" A simple endpoint to test the MySQL connection. Executes a query to get the current time from the database. \"\"\" connection = get_db_connection() if connection is None: return jsonify({\"error\": \"Failed to connect to MySQL database\"}), 500 try: cursor = connection.cursor() cursor.execute(\"SELECT NOW();\") current_time = cursor.fetchone() return jsonify({ \"message\": \"Successfully connected to MySQL!\", \"current_time\": current_time[0] }) except Error as e: return jsonify({\"error\": str(e)}), 500 finally: if connection and connection.is_connected(): cursor.close() connection.close() if __name__ == \"__main__\": debug_mode = os.environ.get(\"DEBUG\", \"false\").lower() == \"true\" app.run(host=\"0.0.0.0\", port=5000, debug=debug_mode) </code></pre><pre><code>FROM python:3.9-slim # Install ping (iputils-ping) for troubleshooting RUN apt-get update &amp;&amp; apt-get install -y iputils-ping &amp;&amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements.txt . RUN pip install --upgrade pip &amp;&amp; pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 5000 ENV FLASK_APP=app.py CMD [\"python\", \"app.py\"] </code></pre><pre><code>docker build -t becloudready/my-flask-app </code></pre><p>It will show a 6 digit Code, which you need to enter to following URL</p><p>Push the Image to DockerHub</p><pre><code>docker push becloudready/my-flask-app </code></pre><p>You should be able to see the Pushed Image</p><pre><code>apiVersion: apps/v1 kind: Deployment metadata: name: flask-deployment namespace: flask-app labels: app: flask spec: replicas: 2 selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: becloudready/my-flask-app:latest # Replace with your Docker Hub image name. ports: - containerPort: 5000 env: - name: APP_ENV valueFrom: configMapKeyRef: name: flask-config key: APP_ENV - name: DEBUG valueFrom: configMapKeyRef: name: flask-config key: DEBUG - name: MYSQL_DB valueFrom: configMapKeyRef: name: flask-config key: MYSQL_DB - name: MYSQL_HOST valueFrom: configMapKeyRef: name: flask-config key: MYSQL_HOST - name: MYSQL_USER valueFrom: secretKeyRef: name: db-credentials key: username - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: flask-svc namespace: flask-app spec: selector: app: flask type: LoadBalancer ports: - port: 80 targetPort: 5000 </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: flask-config namespace: flask-app data: APP_ENV: production DEBUG: \"false\" MYSQL_DB: flaskdb MYSQL_HOST: mysql-svc.mysql.svc.cluster.local </code></pre><pre><code>apiVersion: v1 kind: Namespace metadata: name: flask-app --- apiVersion: v1 kind: Namespace metadata: name: mysql </code></pre><pre><code>kubectl create secret generic db-credentials \\ --namespace=flask-app \\ --from-literal=username=flaskuser \\ --from-literal=password=flaskpass \\ --from-literal=database=flaskdb </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: mysql-initdb namespace: mysql data: initdb.sql: | CREATE DATABASE IF NOT EXISTS flaskdb; CREATE USER 'flaskuser'@'%' IDENTIFIED BY 'flaskpass'; GRANT ALL PRIVILEGES ON flaskdb.* TO 'flaskuser'@'%'; FLUSH PRIVILEGES; </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: mysql-svc namespace: mysql spec: selector: app: mysql ports: - port: 3306 targetPort: 3306 </code></pre><pre><code>apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-statefulset namespace: mysql labels: app: mysql spec: serviceName: \"mysql-svc\" replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: initContainers: - name: init-clear-mysql-data image: busybox command: [\"sh\", \"-c\", \"rm -rf /var/lib/mysql/*\"] volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql containers: - name: mysql image: mysql:5.7 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: rootpassword # For production, use a Secret instead. - name: MYSQL_DATABASE value: flaskdb - name: MYSQL_USER value: flaskuser - name: MYSQL_PASSWORD value: flaskpass volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: initdb mountPath: /docker-entrypoint-initdb.d volumes: - name: initdb configMap: name: mysql-initdb volumeClaimTemplates: - metadata: name: mysql-persistent-storage spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi storageClassName: do-block-storage </code></pre><ul><li><p>kubectl apply -f namespaces.yaml</p></li><li><p>Deploy ConfigMaps and Secrets:</p><p>kubectl apply -f flask-config.yaml kubectl apply -f mysql-initdb.yaml kubectl apply -f db-credentials.yaml</p></li><li><p>kubectl apply -f mysql-svc.yaml kubectl apply -f mysql-statefulset.yaml</p></li><li><p>kubectl apply -f flask-deployment.yaml kubectl apply -f flask-svc.yaml</p></li></ul><p><code>kubectl get svc -n flask-app</code><code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</code><code>flask-svc LoadBalancer 10.109.112.171 146.190.190.51 80:32618/TCP 2m53s</code></p><p>Unable to connect to MySQL from Flask App</p><p>Login to the Flask app pod to ensure all values are loaded properly</p><pre><code>kubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql MYSQL_DB=flaskdb MYSQL_PASSWORD=flaskpass MYSQL_USER=flaskuser MYSQL_HOST=mysql-svc.mysql.svc.cluster.local </code></pre><ul><li>Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.</li><li>Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.</li><li>Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.</li></ul>","contentLength":7195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to manage tool dependencies in Go 1.24+","url":"https://www.alexedwards.net/blog/how-to-manage-tool-dependencies-in-go-1.24-plus","date":1740083763,"author":"/u/alexedwards","guid":7465,"unread":true,"content":"<p>One of my favourite features of Go 1.24 is the new functionality for managing  dependencies.</p><p>By this, I mean tooling that you use to assist with development, testing, build, or deployment ‚Äì such as <a href=\"https://staticcheck.dev/\"></a> for static code analysis, <a href=\"https://pkg.go.dev/golang.org/x/vuln/cmd/govulncheck\"></a> for vulnerability scanning, or <a href=\"https://www.alexedwards.net/blog/github.com/air-verse/air\"></a> for live-reloading applications.</p><p>Historically, managing these dependencies ‚Äî especially in a team setting ‚Äî has been tricky. The previous solutions have been to use a <a href=\"https://marcofranssen.nl/manage-go-tools-via-go-modules\"></a> file or the <a href=\"https://www.alexedwards.net/blog/using-go-run-to-manage-tool-dependencies\"></a> pattern, but while these approaches work, they‚Äôve always felt like workarounds with some downsides.</p><p>With Go 1.24, there‚Äôs finally a better way. </p><p>To demonstrate the new functionality, let's scaffold a simple module and add some application code.</p><figure><code><pre>$ go mod init example.com\n<samp>go: creating new go.mod: module example.com</samp>\n$ touch main.go\n</pre></code></figure><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/kr/text\"\n)\n\nfunc main() {\n    wrapped := text.Wrap(\"This is an informational message that should be wrapped.\", 30)\n    fmt.Println(wrapped)\n}\n</pre></code></figure><p>Now fetch the  package and run the code. The output should look like this:</p><figure><code><pre>$ go get github.com/kr/text\n<samp>go: downloading github.com/kr/text v0.2.0\ngo: added github.com/kr/text v0.2.0</samp>\n$ go run .\n<samp>This is an informational\nmessage that should be\nwrapped.</samp></pre></code></figure><p>Go 1.24 introduces the  flag for , which you can use like this:</p><figure><code><pre>go get -tool import_path@version\n</pre></code></figure><p>This command will download the package specified by the import path (along with any child dependencies), store them in your module cache, and record them in your  file. The  part is optional ‚Äì if you omit it, the latest version will be downloaded.</p><p>Let's use this to add the latest versions of  and  to our module as developer tools, along with  version .</p><figure><code><pre>$ go get -tool golang.org/x/tools/cmd/stringer\n<samp>go: downloading golang.org/x/tools v0.30.0\ngo: downloading golang.org/x/sync v0.11.0\ngo: downloading golang.org/x/mod v0.23.0\ngo: added golang.org/x/mod v0.23.0\ngo: added golang.org/x/sync v0.11.0\ngo: added golang.org/x/tools v0.30.0</samp>\n\n$ go get -tool golang.org/x/vuln/cmd/govulncheck\n<samp>go: downloading golang.org/x/vuln v1.1.4\ngo: downloading golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7\ngo: downloading golang.org/x/sys v0.30.0\ngo: upgraded golang.org/x/telemetry v0.0.0-20240521205824-bda55230c457 =&gt; v0.0.0-20240522233618-39ace7a40ae7\ngo: added golang.org/x/vuln v1.1.4</samp>\n\n$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.1\n<samp>go: downloading honnef.co/go/tools v0.5.1\ngo: downloading golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678\ngo: downloading github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c\ngo: downloading golang.org/x/exp v0.0.0-20231110203233-9a3e6036ecaa\ngo: added github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c\ngo: added golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678\ngo: added honnef.co/go/tools v0.5.1</samp></pre></code></figure><p>After running these, your  file will now include a  section listing the tools you've added. The corresponding module paths and versions for all the dependencies will appear in the  section and be marked as indirect:</p><figure><code><pre>module example.com\n\ngo 1.24.0\n\nrequire (\n    github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c // indirect\n    github.com/kr/text v0.2.0 // indirect\n    golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678 // indirect\n    golang.org/x/mod v0.23.0 // indirect\n    golang.org/x/sync v0.11.0 // indirect\n    golang.org/x/sys v0.30.0 // indirect\n    golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7 // indirect\n    golang.org/x/tools v0.30.0 // indirect\n    golang.org/x/vuln v1.1.4 // indirect\n    honnef.co/go/tools v0.5.1 // indirect\n)\n\ntool (\n    golang.org/x/tools/cmd/stringer\n    golang.org/x/vuln/cmd/govulncheck\n    honnef.co/go/tools/cmd/staticcheck\n)\n</pre></code></figure><p>Once added, you can run tools using the  command.</p><p>To run a specific tool from the command line within your module, you can use  followed by the last non-major-version segment of the import path for the tool (which is, normally, just the name for the tool). For example:</p><figure><code><pre>$ go tool staticcheck -version\n<samp>staticcheck 2024.1.1 (0.5.1)</samp>\n\n$ go tool govulncheck\n<samp>No vulnerabilities found.</samp></pre></code></figure><p>The  command also works nicely if you want to execute tools from your scripts or Makefiles. To illustrate, let's create a Makefile with an  task that runs staticcheck and govulncheck on the codebase.</p><pre><code>.PHONY: audit\naudit:\n    go vet ./...\n    go tool staticcheck ./...\n    go tool govulncheck\n</code></pre><p>If you run , you should see that all the checks complete successfully.</p><figure><code><pre>$ make audit\n<samp>go vet ./...\ngo tool staticcheck ./...\ngo tool govulncheck\nNo vulnerabilities found.</samp></pre></code></figure><p>Let's also take a look at an example where we use the stringer tool in conjunction with  to generate  methods for some  constants.</p><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/kr/text\"\n)\n\n//go:generate go tool stringer -type=Level\n\ntype Level int\n\nconst (\n    Info Level = iota\n    Error\n    Fatal\n)\n\nfunc main() {\n    wrapped := text.Wrap(\"This is an informational message that should be wrapped.\", 30)\n\n    fmt.Printf(\"%s: %s\\n\", Info, wrapped)\n}\n</pre></code></figure><p>The important thing here is the  line. When you run  on this file, it will in turn use  to execute the version of the stringer tool listed in your  file.</p><figure><code><pre>$ go generate .\n$ ls \n<samp>go.mod  go.sum  level_string.go  main.go  Makefile</samp></pre></code></figure><p>You should see that a new  file is created, and running the application should result in some output that looks like this:</p><figure><code><pre>$ go run .\n<samp>Info: This is an informational\nmessage that should be\nwrapped.</samp></pre></code></figure><p>You can check which tools have been added to a module by running , like so:</p><figure><code><pre>$ go list tool\n<samp>honnef.co/go/tools/cmd/staticcheck\ngolang.org/x/tools/cmd/stringer\ngolang.org/x/vuln/cmd/govulncheck</samp></pre></code></figure><p>Because the tools are included in your  file as dependencies, if you want to check that the code for the tools stored in your module cache has not changed you can simply run :</p><figure><code><pre>$ go mod verify\n</pre></code></figure><p>This will check that the code in your module cache exactly matches the corresponding checksums in your  file.</p><p>If you run , the code for tooling dependencies will be included in the  folder and the  manifest alongside your non-tool dependencies.</p><figure><code><pre>$ go mod vendor\n$  tree  -L 3\n<samp>.\n‚îú‚îÄ‚îÄ go.mod\n‚îú‚îÄ‚îÄ go.sum\n‚îú‚îÄ‚îÄ main.go\n‚îú‚îÄ‚îÄ Makefile\n‚îî‚îÄ‚îÄ vendor\n        ‚îú‚îÄ‚îÄ github.com\n        ‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ BurntSushi\n        ‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ kr\n        ‚îú‚îÄ‚îÄ golang.org\n        ‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ x\n        ‚îú‚îÄ‚îÄ honnef.co\n        ‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ go\n        ‚îî‚îÄ‚îÄ modules.txt</samp></pre></code></figure><p>When tools are vendored in this way, running  will execute the corresponding code in the  directory. Note that  does not work on vendored code.</p><p>To upgrade or downgrade a specific tool to a specific version, you can use the same <code>go get -tool import_path@version</code> command that you did for adding the tool originally. For example:</p><figure><code><pre>$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.0\n</pre></code></figure><p>To upgrade to the latest version of a specific tool, omit the  suffix. </p><figure><code><pre>$ go get -tool honnef.co/go/tools/cmd/staticcheck\n</pre></code></figure><p>You can also upgrade  to their latest version by running . Note:  is a sub-command here, not a flag.</p><p>If your tool dependencies are vendored, you will need to re-run  after any upgrades or downgrades.</p><p>At the time of writing, I'm not aware of any easy way to specifically list the tools that have upgrades available ‚Äì if you know of one please let me know!</p><p>To remove the tool completely from your module, use  with the special version tag .</p><figure><code><pre>$ go get -tool honnef.co/go/tools/cmd/staticcheck@none\n</pre></code></figure><p>Again, if you're vendoring, make sure to run  after removing a tool.</p><p>A <a href=\"https://old.reddit.com/r/golang/comments/1iu8nkj/how_to_manage_tool_dependencies_in_go_124/mdzscsa/\">Reddit commenter</a> mentioned the potential for problems if your tools share dependencies with your application code. For example, let's say that your application code depends on  version , and is tested and known to work with that version. Then if you add a tool that relies on a  version of , the version number in your  file will be bumped to the newer version and your application code will use that newer version too.</p><p>In theory, this  be a problem so long as all your dependencies and their child dependencies are stable, follow strict semantic versioning, and don't make backwards-incompatible changes without a major version increment. But, of course, the real world is messy and backwards-incompatible changes  happen, which could unexpectedly break your application code.</p><p>It's worth noting that this issue isn't limited to tool dependencies ‚Äì the same thing can happen if your application code and a non-tool dependency both rely on the same package. However, including tools in  increases the risk.</p><p>To reduce this risk, you can use a separate modfile for tool dependencies instead of including them in your main . You can do this with the  flag, specifying an alternative file such as , like so:</p><figure><code><pre><samp># Initialize a go.tool.mod modfile</samp>\n$ go mod init -modfile=go.tool.mod example.com\n\n<samp># Add a tool to the module</samp>\n$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck\n\n<samp># Run the tool from the command line</samp>\n$ go tool -modfile=go.tool.mod govulncheck\n\n<samp># List all tools added to the module</samp>\n$ go list -modfile=go.tool.mod tool\n\n<samp># Verify the integrity of the tool dependencies</samp>\n$ go mod verify -modfile=go.tool.mod\n\n<samp># Upgrade or downgrade a tool to a specific version</samp>\n$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@v1.1.2\n\n<samp># Upgrade all tools to their latest version</samp>\n$ go get -modfile=go.tool.mod tool\n\n<samp># Remove a tool from the module</samp>\n$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@none\n</pre></code></figure>","contentLength":9405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iu8nkj/how_to_manage_tool_dependencies_in_go_124/"},{"title":"Why do temporaries need to explicitly borrowed?","url":"https://www.reddit.com/r/rust/comments/1iu8jsn/why_do_temporaries_need_to_explicitly_borrowed/","date":1740083498,"author":"/u/parkotron","guid":8474,"unread":true,"content":"<p>As a long time C++ dev, I feel it didn't take me very long to pick up Rust's reference semantics and borrowing rules, but there one place where I constantly find myself forgetting to include the : passing temporaries into functions taking references.</p><pre><code>fn foo(s: &amp;str) { println!(\"The str is: {s}\"); } fn bar() -&gt; String { \"temporary\".to_string() } fn main() { foo(&amp;bar()); // ^ I always forget this ampersand until reminded by the compiler. } </code></pre><p>Rust's explicit  and  operators make a lot of sense to me: given a chunk of code, it should be obvious where a value has been borrowed and what kind of borrow it is. One should never be surprised to learn a reference was taken, because it's right there in the code.</p><p>But in the case of temporary values, it really doesn't matter, does it? Whatever a function call does (or doesn't) do to a temporary value passed to it, the effect cannot be observed in the surrounding code, since the temporary is gone by the end of the statement.</p><p>Is there a subtlety I'm missing here? Does that ampersand on a temporary convey useful information to an experienced Rust dev? Or is it really just syntactic noise, as it seems to me? Are there corner cases I'm just not considering? Could a future edition of Rust be changed to implicitly borrow from temporaries (like it implicitly borrows to make method calls)? Is my mental model just wrong?</p><p>To be perfectly clear, this isn't a criticism, just curiosity. Clearly a lot of thought has been put into the language's design and syntax. This is just the only place I've encountered where Rust's explicitness doesn't feel completely justified.</p>","contentLength":1609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To the purists rocking linux from scratch systems: how was it?","url":"https://www.reddit.com/r/linux/comments/1iu7gc4/to_the_purists_rocking_linux_from_scratch_systems/","date":1740080811,"author":"/u/0110010001101111","guid":8631,"unread":true,"content":"<p>how was your experience from installation to day to day management? what was your use case to build such system over just choosing a distro.</p><p>the apps and the updating it. is it a hassle?</p><p>is it a viable or reasonable option as a daily driver. i just wanted to get some insights about it.</p><p>what do you like or dont like about it. the tradeoffs you were willing to accept, etc. </p>","contentLength":371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rate my photo manipulation tool","url":"https://www.reddit.com/r/golang/comments/1iu6pch/rate_my_photo_manipulation_tool/","date":1740079001,"author":"/u/tunerhd","guid":7513,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/tunerhd\"> /u/tunerhd </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI can fix bugs‚Äîbut can‚Äôt find them: OpenAI‚Äôs study highlights limits of LLMs in software engineering","url":"https://venturebeat.com/ai/ai-can-fix-bugs-but-cant-find-them-openais-study-highlights-limits-of-llms-in-software-engineering/","date":1740078804,"author":"/u/F0urLeafCl0ver","guid":8473,"unread":true,"content":"<div><p><em>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. <a href=\"https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav\">Learn More</a></em></p></div><p>In a <a href=\"https://arxiv.org/pdf/2502.12115\" target=\"_blank\" rel=\"noreferrer noopener\">new paper</a>, <a href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\">OpenAI</a> researchers detail how they developed an LLM benchmark called SWE-Lancer to test how much foundation models can earn from real-life freelance software engineering tasks. The test found that, while the models can solve bugs, they can‚Äôt see why the bug exists and continue to make more mistakes.&nbsp;</p><p>The researchers tasked three LLMs ‚Äî OpenAI‚Äôs GPT-4o and o1 and <a href=\"https://venturebeat.com/ai/the-code-whisperer-how-anthropics-claude-is-changing-the-game-for-software-developers/\">Anthropic‚Äôs Claude-3.5 Sonnet</a> ‚Äî with 1,488 freelance software engineer tasks <a href=\"https://venturebeat.com/business/upwork-shares-leap-40-as-ceo-calls-ipo-beginning-of-a-new-chapter/\">from the freelance platform</a> Upwork amounting to $1 million in payouts. They divided the tasks into two categories: individual contributor tasks (resolving bugs or implementing features), and management tasks (where the model roleplays as a manager who will choose the best proposal to resolve issues).&nbsp;</p><p>‚ÄúResults indicate that the real-world freelance work in our benchmark remains challenging for frontier language models,‚Äù the researchers write.&nbsp;</p><p>The test shows that foundation models cannot fully replace human engineers. While they can help solve bugs, they‚Äôre not quite at the level where they can start earning freelancing cash by themselves.&nbsp;</p><h2>Benchmarking freelancing models</h2><p>The researchers and 100 other professional software engineers identified potential tasks on Upwork and, without changing any words, fed these to a Docker container to create the SWE-Lancer dataset. The container does not have internet access and cannot access GitHub ‚Äúto avoid the possible of models scraping code diffs or pull request details,‚Äù they explained. </p><p>The team identified 764 individual contributor tasks, totaling about $414,775, ranging from 15-minute bug fixes to weeklong feature requests. These tasks, which included reviewing freelancer proposals and job postings, would pay out $585,225.</p><p>The tasks were added to the expensing platform Expensify.&nbsp;</p><p>The researchers generated prompts based on the task title and description and a snapshot of the codebase. If there were additional proposals to resolve the issue, ‚Äúwe also generated a management task using the issue description and list of proposals,‚Äù they explained. </p><p>From here, the researchers moved to end-to-end test development. They wrote Playwright tests for each task that applies these generated patches which were then ‚Äútriple-verified‚Äù by professional software engineers.</p><p>‚ÄúTests simulate real-world user flows, such as logging into the application, performing complex actions (making financial transactions) and verifying that the model‚Äôs solution works as expected,‚Äù the paper explains.&nbsp;</p><p>After running the test, the researchers found that none of the models earned the full $1 million value of the tasks. Claude 3.5 Sonnet, the best-performing model, earned only $208,050 and resolved 26.2% of the individual contributor issues. However, the researchers point out, ‚Äúthe majority of its solutions are incorrect, and higher reliability is needed for trustworthy deployment.‚Äù</p><p>The models performed well across most individual contributor tasks, with Claude 3.5-Sonnet performing best, followed by o1 and GPT-4o.&nbsp;</p><p>‚ÄúAgents excel at localizing, but fail to root cause, resulting in partial or flawed solutions,‚Äù the report explains. ‚ÄúAgents pinpoint the source of an issue remarkably quickly, using keyword searches across the whole repository to quickly locate the relevant file and functions ‚Äî often far faster than a human would. However, they often exhibit a limited understanding of how the issue spans multiple components or files, and fail to address the root cause, leading to solutions that are incorrect or insufficiently comprehensive. We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit.‚Äù</p><p>Interestingly, the models all performed better on manager tasks that required reasoning to evaluate technical understanding.</p><p>These benchmark tests showed that AI models can solve some ‚Äúlow-level‚Äù coding problems and can‚Äôt replace ‚Äúlow-level‚Äù software engineers yet. The models still took time, often made mistakes, and couldn‚Äôt chase a bug around to find the root cause of coding problems. Many ‚Äúlow-level‚Äù engineers work better, but the researchers said this may not be the case for very long.&nbsp;</p>","contentLength":4364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iu6mdl/ai_can_fix_bugsbut_cant_find_them_openais_study/"},{"title":"TwinSong: Jupyter notebook built from scratch in Rust","url":"https://www.reddit.com/r/rust/comments/1iu5tpa/twinsong_jupyter_notebook_built_from_scratch_in/","date":1740076846,"author":"/u/winter-moon","guid":7469,"unread":true,"content":"<p>I've spent a lot of time working with Python in Jupyter notebooks, but one thing has always bothered me: the way code and outputs are mixed together. While this is great for tutorials and interactive documentation, it's less ideal for exploratory work or data processing, where I just want to interact with Python without the constraints of a document-style interface. </p><p>To address this, I created TwinSong, a Jupyter alternative that separates code and outputs. Right now, it's primarily a UX experiment, but core features like editing and executing cells are already in place. Instead of modifying Jupyter's existing codebase, I built it from scratch with a React frontend and a Rust backend.</p><p>While performance wasn't the main focus, implementing a Python kernel driver in Rust keeps the kernel clean and avoids loading Python dependencies that might interfere with user code. Plus, as we've seen with other projects, rewriting classic Python tools in Rust can open up new possibilities.</p>","contentLength":986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Enriching token embedding with last hidden state?","url":"https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/","date":1740074778,"author":"/u/Academic_Sleep1118","guid":8599,"unread":true,"content":"<p>Looking at a decoder transformer working process from an information theory standpoint, we can see that the information available in the last hidden state is collapsed into a single token during generation. It means that you collapse a hidden state that, in theory, has about:</p><p> (or whatever quant) bits of information to something like:</p><p>I wonder if it's a good thing (sorry for the naive phrasing). The information used by a transformer to predict the next token is entirely stored in its context window and does not involve any recurrent state. So, predicting the next token of a sequence the transformer was just fed with is going to yield the exact same result as doing so for the same sequence if it were entirely generated by the transformer itself.</p><p>Fair enough, in some sense: whether the sequence was generated or just read doesn't change anything about what the next token should be.</p><p>But on the other hand, this approach means that  the information flow between tokens has to happen through the attention mechanism. There's no way for the transformer to embed some nuance or flavor into the predicted token embedding. Like in:</p><p><em>\"Well, I predicted the token '</em></p><p>When the next token is predicted, this nuance that was likely present in the last hidden state (or even in the softmaxed output probability distribution) is totally lost.</p><p>So while I was having a little walk yesterday, I was thinking that it might be a good idea to add some information to the token embeddings using something like:</p><p><strong>augmented_embedding = embedding(token) + F(last_hidden_state)</strong></p><p>(It would be important to make sure that:</p><p><strong>‚ÄñF(last_hidden_state)‚Äñ ‚â™ ‚Äñembedding(token)‚Äñ</strong></p><p>I have tried to find papers on this subject and asked for feedback from Claude, ChatGPT, and Perplexity.</p><ul><li> told me it was <em>\"an incredibly insightful idea.\"</em></li><li> hallucinated a paper on the subject.</li><li> gave me a very long list of totally unrelated sources.</li></ul><p>So I'm turning to you guys. I would love it if some big-brained guy told me why other big-brained guys decided not to follow this idea, or why it doesn't work.</p><p>Here are some things I identified as potentially problematic:</p><p>Transformers are nice to train with heavy parallelization precisely because they are not recursive. Each sequence of size  can give  independent training examples. Injecting last hidden states' information in token embeddings would break some of that parallelization.</p><p>It would still be possible to train it efficiently, I guess.</p><ol><li>First, take the () vanilla sequences and get the predictions.</li><li>Then, for each prediction, store the last hidden state and update the corresponding token embedding in each of the sequences where it appears.</li><li>Now, you have a new set of training sequences, with all (but the first) token embeddings updated.</li><li>You can repeat this process indefinitely. I hope it converges ^^</li></ol><p>This really looks like a diffusion process, by the way. That brings me to the next point:</p><p>Here, I am not very competent. What are the conditions that define such a process' stability? My uneducated guess is that if you keep:<strong>‚Äñlast_hidden_state_contribution‚Äñ ‚â™ ‚Äñaugmented_token_embedding‚Äñ</strong> you should not have many problems. But it would also limit the information flow. I guess there's a trade-off, and I wouldn't be surprised if it's not good enough.</p><p>What do you guys think? Has this already been tried somewhere? Is there a fundamental reason this wouldn't work?</p>","contentLength":3370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thoughts on an AI powered bipedal, musculoskeletal , anatomically accurate, synthetic human with over 200 degrees of freedom, over 1,000 Myofibers, and 500 sensors?","url":"https://v.redd.it/b1iwrsu32cke1","date":1740074243,"author":"/u/VivariuM_007","guid":7436,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iu4qex/thoughts_on_an_ai_powered_bipedal_musculoskeletal/"},{"title":"Announcing Rust 1.85.0 and Rust 2024 | Rust Blog","url":"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html","date":1740071479,"author":"/u/slanterns","guid":7359,"unread":true,"content":"<p>The Rust team is happy to announce a new version of Rust, 1.85.0. This stabilizes the 2024 edition as well.\nRust is a programming language empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via , you can get 1.85.0 with:</p><p>If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please <a href=\"https://github.com/rust-lang/rust/issues/new/choose\">report</a> any bugs you might come across!</p><p>We are excited to announce that the Rust 2024 Edition is now stable!\nEditions are a mechanism for opt-in changes that may otherwise pose a backwards compatibility risk. See <a href=\"https://doc.rust-lang.org/edition-guide/editions/index.html\">the edition guide</a> for details on how this is achieved, and detailed instructions on how to migrate.</p><p>This is the largest edition we have released. The <a href=\"https://doc.rust-lang.org/edition-guide/rust-2024/index.html\">edition guide</a> contains detailed information about each change, but as a summary, here are all the changes:</p><p>The guide includes migration instructions for all new features, and in general\n<a href=\"https://doc.rust-lang.org/edition-guide/editions/transitioning-an-existing-project-to-a-new-edition.html\">transitioning an existing project to a new edition</a>.\nIn many cases  can automate the necessary changes. You may even find that no changes in your code are needed at all for 2024!</p><p>Note that automatic fixes via  are very conservative to avoid ever changing the semantics of your code. In many cases you may wish to keep your code the same and use the new semantics of Rust 2024; for instance, continuing to use the  macro matcher, and ignoring the conversions of conditionals because you want the new 2024 drop order semantics. The result of  should not be considered a recommendation, just a conservative conversion that preserves behavior.</p><p> people came together to create this edition. We'd like to thank them all for their hard work!</p><p>Rust now supports asynchronous closures like  which return futures when called. This works like an  which can also capture values from the local environment, just like the difference between regular closures and functions. This also comes with 3 analogous traits in the standard library prelude: , , and .</p><p>In some cases, you could already approximate this with a regular closure and an asynchronous block, like . However, the future returned by such an inner block is not able to borrow from the closure captures, but this does work with  closures:</p><pre><code>let mut vec: Vec&lt;String&gt; = vec![];\n\nlet closure = async || {\n    vec.push(ready(String::from(\"\")).await);\n};\n</code></pre><p>It also has not been possible to properly express higher-ranked function signatures with the  traits returning a , but you can write this with the  traits:</p><pre><code>use core::future::Future;\nasync fn f&lt;Fut&gt;(_: impl for&lt;'a&gt; Fn(&amp;'a u8) -&gt; Fut)\nwhere\n    Fut: Future&lt;Output = ()&gt;,\n{ todo!() }\n\nasync fn f2(_: impl for&lt;'a&gt; AsyncFn(&amp;'a u8))\n{ todo!() }\n\nasync fn main() {\n    async fn g(_: &amp;u8) { todo!() }\n    f(g).await;\n    //~^ ERROR mismatched types\n    //~| ERROR one type is more general than the other\n\n    f2(g).await; // ok!\n}\n</code></pre><h3><a href=\"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html#hiding-trait-implementations-from-diagnostics\" aria-hidden=\"true\"></a>Hiding trait implementations from diagnostics</h3><p>The new <code>#[diagnostic::do_not_recommend]</code> attribute is a hint to the compiler to not show the annotated trait implementation as part of a diagnostic message. For library authors, this is a way to keep the compiler from making suggestions that may be unhelpful or misleading. For example:</p><pre><code>pub trait Foo {}\npub trait Bar {}\n\nimpl&lt;T: Foo&gt; Bar for T {}\n\nstruct MyType;\n\nfn main() {\n    let _object: &amp;dyn Bar = &amp;MyType;\n}\n</code></pre><pre><code>error[E0277]: the trait bound `MyType: Bar` is not satisfied\n --&gt; src/main.rs:9:29\n  |\n9 |     let _object: &amp;dyn Bar = &amp;MyType;\n  |                             ^^^^ the trait `Foo` is not implemented for `MyType`\n  |\nnote: required for `MyType` to implement `Bar`\n --&gt; src/main.rs:4:14\n  |\n4 | impl&lt;T: Foo&gt; Bar for T {}\n  |         ---  ^^^     ^\n  |         |\n  |         unsatisfied trait bound introduced here\n  = note: required for the cast from `&amp;MyType` to `&amp;dyn Bar`\n</code></pre><p>For some APIs, it might make good sense for you to implement , and get  indirectly by that blanket implementation. For others, it might be expected that most users should implement  directly, so that  suggestion is a red herring. In that case, adding the diagnostic hint will change the error message like so:</p><pre><code>#[diagnostic::do_not_recommend]\nimpl&lt;T: Foo&gt; Bar for T {}\n</code></pre><pre><code>error[E0277]: the trait bound `MyType: Bar` is not satisfied\n  --&gt; src/main.rs:10:29\n   |\n10 |     let _object: &amp;dyn Bar = &amp;MyType;\n   |                             ^^^^ the trait `Bar` is not implemented for `MyType`\n   |\n   = note: required for the cast from `&amp;MyType` to `&amp;dyn Bar`\n</code></pre><h3><a href=\"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html#fromiterator-and-extend-for-tuples\" aria-hidden=\"true\"></a> and  for tuples</h3><p>Earlier versions of Rust implemented convenience traits for iterators of  tuple pairs to behave like , with  in 1.56 and  in 1.79. These have now been  to more tuple lengths, from singleton  through to 12 items long, . For example, you can now use  to fanout into multiple collections at once:</p><pre><code>use std::collections::{LinkedList, VecDeque};\nfn main() {\n    let (squares, cubes, tesseracts): (Vec&lt;_&gt;, VecDeque&lt;_&gt;, LinkedList&lt;_&gt;) =\n        (0i32..10).map(|i| (i * i, i.pow(3), i.pow(4))).collect();\n    println!(\"{squares:?}\");\n    println!(\"{cubes:?}\");\n    println!(\"{tesseracts:?}\");\n}\n</code></pre><pre><code>[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n[0, 1, 8, 27, 64, 125, 216, 343, 512, 729]\n[0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561]\n</code></pre><h3><a href=\"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html#updates-to-stdenvhome_dir\" aria-hidden=\"true\"></a>Updates to </h3><p> has been deprecated for years, because it can give surprising results in some Windows configurations if the  environment variable is set (which is not the normal configuration on Windows). We had previously avoided changing its behavior, out of concern for compatibility with code depending on this non-standard configuration. Given how long this function has been deprecated, we're now updating its behavior as a bug fix, and a subsequent release will remove the deprecation for this function.</p><p>These APIs are now stable in const contexts</p><p>Many people came together to create Rust 1.85.0. We couldn't have done it without all of you. <a href=\"https://thanks.rust-lang.org/rust/1.85.0/\">Thanks!</a></p>","contentLength":5852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iu3l0a/announcing_rust_1850_and_rust_2024_rust_blog/"},{"title":"Ugly Code and Dumb Things","url":"https://lucumr.pocoo.org/2025/2/20/ugly-code/","date":1740070620,"author":"/u/FoxInTheRedBox","guid":7438,"unread":true,"content":"<p data-date=\"2025-02-20T00:00:00\">written on Thursday, February 20, 2025</p><p>This week I had a conversation with one of our engineers about ‚Äúshitty\ncode‚Äù which lead me to sharing with him one of my more unusual\ninspirations: <a href=\"https://github.com/exflickr/flamework/\">Flamework</a>, a\npseudo framework created at Flickr.</p><div><h2>Two Passions, Two Approaches</h2><p>There are two driving passions in my work.  One is the love of creating\nbeautiful, elegant code ‚Äî making Open Source libraries and APIs that focus\non clear design and reusability.  The other passion is building quick,\npragmatic solutions for real users (who may not even be developers).  The\nlatter usually in a setting of building a product, where the product is\nnot the code.  Here, speed and iteration matter more than beautiful code\nor reusability, because success hinges on shipping something people want.</p><p>Flamework is in service of the latter, and in crass violation of the\nformer.</p><p>Early on, I realized that creating reusable code and directly solving\nproblems for users are often at odds.  My first clue came when I helped\nrun the German\n<a href=\"https://www.ubuntuusers.de/\">ubuntuusers</a> website.  It was powered by\na heavily modified version of phpBB, which despite how messy it was,\nscaled to a large user base when patched properly.  It was messy, but easy\nto adjust.  The abstractions were one layer deep.</p><p>Back then, me and a friend tried to replace it by writing my own bulletin\nboard software, <a href=\"https://web.archive.org/web/20070502223619/http://flying.circus.pocoo.org/\">Pocoo</a>.\nWorking in isolation, without users, led me down a path of\nover-engineering.  While we learned a lot and ended up creating popular\nOpen Source libraries (like Jinja, Werkzeug and Pygments), Pocoo never\nbecame a solid product.  Later, my collaborators and I <a href=\"https://github.com/inyokaproject/inyoka/\">rebuilt\nubuntuusers</a>, without the\ngoal of making it into a reusable product.  That rewrite shipped\nsuccessfully and it lives to this very day.</p><p>But it took me years to fully realize what was happening here: reusability\nis not that important when you‚Äôre building an application, but it‚Äôs\ncrucial when you‚Äôre building a library or framework.</p></div><div><p>If you are unfamiliar with Flamework you should watch a talk that Cal\nHenderson gave in 2008 at DjangoCon (<a href=\"https://www.youtube.com/watch?v=i6Fr65PFqfk\">Why I hate Django</a>).  He talked about scale\nand how Django didn't solve for it.  He enumerated all the things\nimportant to him: sharding, using custom sequences for primary keys,\nforgoing joins and foreign keys, supporting database replication setups,\ndenormalizing data to the extreme.  This is also were I first learned\nabout the possibility of putting all session data into cookies via\nsigning.  It was a memorable talk for me because it showed me that there\nare shortcomings.  Django (which I used for ubuntuusers) had beautiful\nAPIs but at the time solved for little of that Cal needed.  The talk\nreally stuck with me.</p><p>At the time of the talk, Flamework did not really exist.  It was more of\nan idea and principles of engineering at Flickr.</p><p>A few years later, Flamework appeared on GitHub, not as an open-sourced\npiece of Flickr code but as a reimplementation of those same ideas.  You\ncan explore its repository and see code like this:</p><div><pre></pre></div><p>Instinctively it makes me cringe.  Is that a SQL injection?  Well you were\nsupposed to use the PHP <a href=\"https://www.php.net/manual/en/function.addslashes.php\">addslashes</a> function\nbeforehand.  But notice how it caters to sharding and clustering directly\nin the query function.</p></div><div><p>Code like this often triggers a visceral reaction, especially in engineers\nwho prize clean design.</p><p>How does something like that get created?  Cal Henderson described\nFlickr's principle as ‚Äúdoing the dumbest possible thing that will work.‚Äù\nMaybe ‚Äúdumb‚Äù is too strong ‚Äî ‚Äúsimple‚Äù might be more apt.  Yet simplicity\ncan look messy to someone expecting a meticulously engineered codebase.\nThis is not at all uncommon and I have seen it over and over.  The first\nlarge commercial project that got traction that I ever worked on (<a href=\"https://en.wikipedia.org/wiki/Plurk\">Plurk</a>) was also pretty pragmatic and\nmessy inside.  My former colleague Ben Vinegar also <a href=\"https://benv.ca/blog/posts/the-hardest-problem\">recently shared</a> a story of early,\nmessy FreshBooks code and how he came to terms with it.  Same story at\n<a href=\"https://sentry.io/welcome\">Sentry</a>.  We moved fast, we made a mess.</p><p>None of this is surprising in retrospective.  Perfect code doesn't\nguarantee success if you haven't solved a real problem for real people.\nPursuing elegance in a vacuum leads to abandoned side projects or\nframeworks nobody uses.  By contrast, clunky but functional code often\ncomes with just the right compromises for quick iteration.  And that in\nturn means a lot of messy code powers products that people love ‚Äî\nsomething that's a far bigger challenge.</p></div><div><p>I have shown Flamework's code to multiple engineers over the years and it\nusually creates such a visceral response.  It blind sights one by\nseemingly disregarding all rules of good software engineering.</p><p>That makes Flamework serve as a fascinating Rorschach test for engineers.\nAre you <a href=\"https://github.com/exflickr/flamework\">looking at it</a> with\nadmiration for the focus on some critical issues like scale, the built-in\nobservability and debugging tools.  Or are you judging it, and its\ncreators, for manually constructing SQL queries, using global variables,\nnot using classes and looking like messy PHP4 code?  Is it a pragmatic\ntool, intentionally designed to iterate quickly at scale, or is it a naive\nmess made by unskilled developers?</p><p>Would I use Flamework?  Hello no.  But I appreciate the priorities behind\nit.  If these ugly choices help you move faster, attract users and\nvalidate the product, then a rewrite, or large refactorings later are a\nsmall price to pay.</p></div><div><p>At the end of the day, where you stand on ‚Äúshitty code‚Äù depends on your\nprimary goal:</p><ul><li>Are you shipping a product and racing to meet user needs?</li><li>Or are you building a reusable library or framework meant to stand the\ntest of time?</li></ul><p>Both mindsets are valid, but they rarely coexist harmoniously in a single\ncodebase.  Flamework is a reminder that messy, simple solutions can be\npowerful if they solve real problems.  Eventually, when the time is right,\nyou can clean it up or rebuild from the ground up.</p><p>The real challenge is deciding which route to take ‚Äî and when.  Even with\nexperience, it is can be hard to know when to move from quick fixes to\nmore robust foundations.  The principles behind Flamework are also\nreflected in <a href=\"https://develop.sentry.dev/getting-started/philosophy/\">Sentry's development philosophy</a>.  One more\npoignant one being ‚ÄúEmbrace the Duct Tape‚Äù.  Yet as Sentry matured, much\nof our duct tape didn't stand the test of time, and was re-applied at\nmoments when the real solution would have been a solid foundation poured\nwith concrete.</p><p>That's because successful projects eventually grow up.  What let you\niterate fast in the beginning might eventually turn into an unmaintainable\nmess and will be rebuilt from the inside out.</p><p>I personally would never have built Flamework, it repulses me a bit.  At the\nsame time, I have a enormous respect for the people who build it.  Their\nwork and thinking has shaped how I solve problems and think of product\nengineering.</p></div>","contentLength":6780,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iu37xs/ugly_code_and_dumb_things/"},{"title":"Why Firefox?","url":"https://www.reddit.com/r/linux/comments/1iu25zd/why_firefox/","date":1740068011,"author":"/u/Flaky_Comfortable425","guid":7437,"unread":true,"content":"<p>This actually makes me curious, when I switch between a lot of distros, jumping from Debian to CentOS to dfferent distros, I can see that they all love firefox, it's not my favorite actually, and there are plenty of internet browsers out there which is free and open source like Brave for example, still I am wondering what kind of attachment they have to this browser</p>","contentLength":368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"C equivalent of select() / poll() in go socket programming","url":"https://www.reddit.com/r/golang/comments/1iu1ugj/c_equivalent_of_select_poll_in_go_socket/","date":1740067219,"author":"/u/ChestPainGuy","guid":8416,"unread":true,"content":"<p>Hi, I'm fairly new to socket programming and go, so forgive my ignorance.</p><p>Recently, I have been reading up Beej's guide to network programming, where he explains the use of  and  to read and write to multiple sockets without blocking.</p><p>I have googled quite a bit, but almost every tutorial or go example on the basics of socket connections just spawn a new goroutine with something like .</p><ol><li>So whats' the equivalent of  in go?</li><li>Is spawning a goroutine for every connection an effective approach?</li></ol><p>Any good links to network programming in go would be appreciated if this question is too dumb. Thanks</p>","contentLength":588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is anyone working on AI designed to preserve democracy?","url":"https://www.reddit.com/r/artificial/comments/1iu1n10/is_anyone_working_on_ai_designed_to_preserve/","date":1740066712,"author":"/u/BarbaGramm","guid":7466,"unread":true,"content":"<p>I‚Äôm looking for people or groups who are already working on something like this:</p><p>A decentralized AI trained to preserve the intellectual, historical, and emotional essence of democracy‚Äîwhat it actually means, not just what future regimes might redefine it to be. Think of it as a fusion of data hoarding, decentralized AI, and resistance tech, built to withstand authoritarian drift and historical revisionism.</p><p>Maybe it doesn't reach the heights of the corporate or state models, but a system that can always articulate the delta‚Äîthe difference between a true democratic society (or at least what we seem to be leaving behind) and whatever comes next. If democracy gets twisted into something unrecognizable, this AI should be able to compare, contrast, and remind people what was lost. It should be self-contained, offline-capable, decentralized, and resistant to censorship‚Äîan incorruptible witness to history.</p><p>Does this exist? Are there people in AI, decentralized infrastructure, or archival communities working toward something like this? I don‚Äôt want to reinvent the wheel if a community is already building it. If you know of any projects, frameworks, or people tackling this problem, please point me in the right direction.</p><p>If no one is doing it, shouldn't this be a project people are working on? Is there an assumption that corporate or state controlled AI will do this inherently?</p>","contentLength":1397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Slice Internals in Go: How the Runtime Expands Slices Efficiently","url":"https://themsaid.com/slice-internals-in-go","date":1740064893,"author":"/u/themsaid","guid":7435,"unread":true,"content":"<p>The deeper you delve into Go‚Äôs internals, the more evident it becomes that its creators carefully engineered the language to strike a precise balance between performance and flexibility. This delicate equilibrium influences many of Go‚Äôs core features, including its approach to memory management and data structures. One standout example of this thoughtful design is the implementation of slice growth. Through this approach, Go ensures that slices expand seamlessly, optimizing both performance and memory usage without compromising ease of use.</p><p>In Go, a slice is a lightweight data structure that serves as a window into a contiguous block of memory where elements of a specific type are stored. At its core, a slice doesn‚Äôt directly contain the data itself but instead holds a pointer to an underlying array (know as the backing array.)</p><p>When the Go runtime creates a slice, as in this example, it constructs a small struct under the hood, defined in the runtime as :</p><div><pre> {\n\t unsafe.\n}</pre></div><p>The  type has the following fields:</p><ul><li> holds a pointer to the underlying array.</li><li> stores the number of elements in the slice.</li><li> stores the capacity of the array.</li></ul><p>The  type used for the  field is a generic pointer type that bypasses Go's type safety rules. Since the size of an array in Go is part of the type, the runtime uses  so it can replace the array with a larger one when needed.</p><p>The code in the example above creates a slice that has zero elements with a backing array that can hold 10 elements of type . We can later fill that array with elements by using the  function:</p><p>Here, we append a byte to the empty array represented by the unsigned 8-bit integer .</p><p>When appending elements to a slice, the Go runtime first checks whether the backing array has enough capacity to accommodate the new elements. If it does, the elements are simply added to the existing array. However, if the current array lacks sufficient space, the runtime allocates a larger backing array, copies the existing elements into it, and then appends the new elements.</p><div><pre>([], , )\n\n(, ) (, ) </pre></div><p>The capacity of the newly allocated array is determined by several factors, including the current array‚Äôs capacity, the type of elements it holds, and the number of new elements being appended. These factors influence how much the array grows, ensuring efficient memory usage while minimizing the need for frequent reallocations.</p><p>The runtime begins by attempting to double the existing capacity as the first step in determining the new array size:</p><div><pre></pre></div><p>If the total number of existing and newly appended elements exceeds the doubled capacity, the runtime sets the new capacity to match the required number of elements:</p><div><pre> {\n    \n}</pre></div><p>This ensures that the new capacity is larger than or equals to the number of elements after the appending operation.</p><div><pre>([], , )\n\n(, , , )\n\n.(()) </pre></div><p>In this example, the integer slice initially had a capacity of 1. After adding three elements, the runtime allocated a new backing array with a capacity of 3. This happened because doubling the original capacity (1 * 2) was insufficient to accommodate the new elements, prompting the runtime to adjust the capacity accordingly.</p><p>If doubling the capacity is sufficient, the runtime further evaluates whether allocating such a large array is efficient or merely a waste of memory.</p><p>For small slices, capacity less than 256, the runtime employs a simple doubling strategy (e.g., 2 to 4, 4 to 8, 8 to 16.) This makes sense for small workloads: doubling ensures plenty of headroom for future appends. However, as the slice‚Äôs capacity climbs into more than 256, or beyond, doubling becomes less practical. Doubling a capacity of, say, 10,000 to 20,000 allocates an extra 10,000 elements‚Äô worth of memory (potentially tens or hundreds of kilobytes, depending on the element size) which might sit unused for a long time.</p><p>To address this, the runtime adjusts its growth strategy for larger slices by reducing the growth factor gradually until it reaches 1.25. This slower growth means that if a slice already has a capacity of, say, 512, adding a few elements doesn‚Äôt balloon it to 1024; it might rise to 832 instead (a 62.5% increase). The key insight is that a larger slice can absorb more appends before hitting its capacity limit. For instance, a slice with a capacity of 512 has room for 512 more elements if empty, compared to just 8 for a capacity of 8. This naturally delays the need for reallocation.</p><p>This conservative approach aims to curb excessive memory usage. By growing incrementally rather than exponentially, the runtime avoids reserving vast swaths of memory that might remain idle, which is critical in applications handling large datasets or with limited resources (e.g., embedded systems). However, there‚Äôs a flip side: smaller growth steps mean the slice fills up sooner, triggering reallocation more often. Each reallocation involves CPU work (allocating memory, copying the existing elements, and updating the slice‚Äôs pointer) which can add up if appends are frequent.</p><p>The runtime‚Äôs strategy thus balances these two forces: memory footprint versus CPU overhead. It leans toward saving memory at the cost of potentially more frequent (but smaller) reallocations, betting that the trade-off pays off in most real-world scenarios where slices don‚Äôt grow indefinitely.</p><p>When determining how a slice should grow, the Go runtime takes into account the type of elements stored in the array, as this directly impacts memory allocation. On 64-bit systems, memory is generally allocated in chunks of 8 bytes. Any allocation that does not align with this rule is rounded up to the nearest multiple of 8 to ensure efficient memory usage and alignment.</p><p>Let's say we create a slice of bytes with capacity zero and then append an element to it:</p><div><pre>([], , )\n\n(, )\n\n.(()) </pre></div><p>After the growth, the capacity of the slice becomes 8 (8 bytes). If the element type was a 64-bit integer instead, the growth will increase the capacity to 1 (1 * 64 bits = 8 bytes):</p><div><pre>([], , )\n\n(, )\n\n.(()) </pre></div><p>If the element type was a 32-bit integer, the growth will increase the capacity to 2 (2 * 32 bits = 8 bytes):</p><div><pre>([], , )\n\n(, )\n\n.(()) </pre></div><p>The reason is that modern CPUs, particularly on 64-bit systems, operate most efficiently when data is aligned to their word size (the amount of data they can process in one cycle.) On a 64-bit system, the word size is 64 bits, or 8 bytes. If the runtime allocates, say, 5 bytes, the CPU  and masks off the unused portion. That means, allocating in 8-byte multiples ensures the entire chunk is usable without waste or extra work.</p><p>In addition, CPU caches fetch memory in 64-byte lines (8 words of 8 bytes each.) Multiples of 8 bytes fit neatly into these lines, reducing cache misses and improving locality when accessing sequential data, like a slice‚Äôs backing array.</p><p>In addition to the 8-byte chunk allocation rule, the Go runtime maintains a table of predefined constants to guide its memory allocation decisions. This table categorizes memory allocations into specific size classes, helping the runtime minimize fragmentation and efficiently reuse freed memory blocks.</p><p>The table looks like this:</p><div><pre>+---------+-------+\n| Class   | Value |\n+---------+-------+\n| Class  |      |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |    |\n| .     | .   |\n+---------+-------+</pre></div><p>This size class allocation table functions as an efficient lookup mechanism for managing memory allocation and deallocation. When a memory block belonging to a specific size class is freed, the runtime stores it in the table rather than immediately returning it to the operating system. Later, if a request is made for a memory block of the same size class, the system can quickly retrieve and reuse the previously freed block instead of performing an extensive search through physical memory to find a suitable allocation.</p><div><pre>+---------------------------------+\n|  Freed memory  size class X  |\n+---------------------------------+\n        ‚îÇ\n        ‚ñº\n+-------------------+  \n| Memory Block     |  &lt;-- Freed  (Stored in Table)\n+-------------------+  \n| Memory Block     |  &lt;-- Freed  (Stored in Table)\n+-------------------+  \n| Memory Block     |  &lt;-- Freed  (Stored in Table)\n+-------------------+  \n        ‚îÇ\n        ‚ñº\n+-------------------------------------+\n| Incoming Memory Allocation Request  |\n+-------------------------------------+\n        ‚îÇ\n          (Lookup in Table)\n+-------------------------------+\n| Matching Freed Block Found    |\n+-------------------------------+\n        ‚îÇ\n          (Reused Instead of )\n+----------------------------+\n|  Allocated to Application  |\n+----------------------------+</pre></div><p>With that in mind, the runtime not only rounds up to the nearest 8-byte boundary but also rounds up to the nearest size class in the allocation table.</p><p>Consider this slice operation:</p><div><pre>([], , )\n\n(, , , , , )</pre></div><p>Here, the slice starts with zero capacity and we add 5 elements of type . Without considering the size class allocation table, the runtime would allocate 40 bytes for the new backing array:</p><div><pre> *  bits =  bits /  =  bytes</pre></div><p>Instead, the runtime consults the class allocation table and rounds up to the nearest match (48 in this case). As a result, it allocates a backing array with a capacity of 6 (48 bytes / 64 bits), even though the new array would only need to hold 5 elements (that require only 40 bytes).</p><p>This approach significantly improves performance by reducing external fragmentation (where free memory is scattered in small, non-contiguous blocks, making larger allocations difficult.) It also minimizes allocation overhead and speeds up memory access by eliminating the need to repeatedly request new memory from the operating system.</p><p>In summary, the Go runtime takes several key factors into account when growing an array:</p><ol><li>: It starts with a doubling factor (2x) and then gradually winds down to 1.25x.</li><li>: The array is rounded up to the nearest 8-byte boundary.</li><li><strong>The Size Class Allocation Table</strong>: The runtime rounds up to the nearest available class in the table.</li></ol><p>I really admire the thoughtful work the Go team has put into making the language both efficient and flexible. It's clear that a lot of careful consideration went into optimizing performance while maintaining flexibility for developers.</p>","contentLength":10276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iu0xjk/slice_internals_in_go_how_the_runtime_expands/"},{"title":"Rust 2024 Is Coming: baby steps","url":"https://smallcultfollowing.com/babysteps/blog/2025/02/20/rust-2024-is-coming/?utm_source=atom_feed","date":1740063180,"author":"/u/VorpalWay","guid":7288,"unread":true,"content":"<div><p>So, a little bird told me that Rust 2024 is going to become stable today, along with Rust 1.85.0. In honor of this momentous event, I have penned a little ditty that I‚Äôd like to share with you all. Unfortunately, for those of you who remember Rust 2021‚Äôs <a href=\"https://smallcultfollowing.com/babysteps/blog/2021/05/26/edition-the-song/\">‚ÄúEdition: The song‚Äù</a>, in the 3 years between Rust 2021 and now, my daughter has realized that her father is deeply uncool and so I had to take this one on solo. Anyway, enjoy! Or, you know, suffer. As the case may be.</p><p>In ChordPro format, for those of you who are inspired to play along.</p><pre tabindex=\"0\"><code>{title: Rust 2024}\n{subtitle: }\n\n{key: C}\n\n[Verse 1]\n[C] When I got functions that never return\nI write an exclamation point [G]\nBut use it for an error that could never be\nthe compiler [C] will yell at me\n\n[Verse 2]\n[C] We Rust designers, we want that too\n[C7] But we had to make a [F] change\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\n[Bridge]\n[Am] ... [Am] But will my program [E] build?\n[Am] Yes ... oh that‚Äôs [D7] for sure\n[F] edi-tions [G] are [C] opt in\n\n[Verse 3]\n[C] Usually when I return an `impl Trait`\neverything works out fine [G]\nbut sometimes I need a tick underscore\nand I don‚Äôt really [C] know what that‚Äôs for\n\n[Verse 4]\n[C] We Rust designers we do agree\n[C7] That was con- [F] fusing \n[F] But that will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\n[Bridge 2]\n[Am] Cargo fix will make the changes\nautomatically [G] Oh that sure sounds great...\n[Am] but wait... [Am] my de-pen-denc-[E]-ies\n[Am] Don‚Äôt worry e-[D7]ditions\n[F] inter [G] oper [C] ate\n\n[Verse 5]\n[C] Whenever I match on an ampersand T\nThe borrow [G] propagates\nBut where do I put the ampersand\nwhen I want to [C] copy again?\n\n[Verse 6]\n[C] We Rust designers, we do agree\n[C7] That really had to [F] change\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\n[Outro]\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\nOne more time!\n\n[Half speed]\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n</code></pre></div>","contentLength":2134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iu09jr/rust_2024_is_coming_baby_steps/"},{"title":"[D] Deepseek 681bn inference costs vs. hyperscale?","url":"https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/","date":1740059045,"author":"/u/sgt102","guid":8398,"unread":true,"content":"<p>I've estimated the cost/performance of Deepseek 681bn like this :</p><p>Huggingface open deepseek blog reported config &amp; performance = 32 H100's 800tps </p><p>1million tokens = 1250s = 21 (ish) , minutes. 69.12 million tokens per day </p><p>Cost to rent 32 H100's per month ~$80000</p><p>Cost per million tokens = $37.33 (80000/ 31 days /69.12 ) </p><p>I know that this is very optimistic (100% utilisation, no support etc.) but does the arithmetic make sense and does it pass the sniff test do you think? Or have I got something significantly wrong? </p><p>I guess this is 1000 times more expensive than an API served model like Gemini, and this gap has made me wonder if I am being silly</p>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I still have these","url":"https://www.reddit.com/r/linux/comments/1itynca/i_still_have_these/","date":1740058638,"author":"/u/emuboy85","guid":7235,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chromium Ozone/Wayland: The Last Mile Stretch","url":"https://nickdiego.dev/blog/chromium-ozone-wayland-the-last-mile-stretch/","date":1740058042,"author":"/u/Worldly_Topic","guid":7259,"unread":true,"content":"<p>Hey there! I‚Äôm glad to finally start paying my blogging debt :) as this\nis something I‚Äôve been planning to do for quite some time now. To get the\nball rolling, I‚Äôve shared some bits about me in my very first blog post\n<a href=\"https://nickdiego.dev/blog/ola-mundo/\">Ol√° Mundo</a>.</p><p>In this article, I‚Äôm going to walk through what we‚Äôve been working on\nsince last year in the Chromium Ozone/Wayland project, on which I‚Äôve\nbeen involved (directly or indirectly) since I‚Äôve joined Igalia back in\n2018.</p><p>Lets start with some context, the project consists of implementing,\nshipping and maintaining native <a href=\"https://wayland.freedesktop.org\" target=\"_blank\" rel=\"noreferrer\">Wayland</a> support in the\nChromium project. Our team at <a href=\"https://igalia.com\" target=\"_blank\" rel=\"noreferrer\">Igalia</a> has been leading the\neffort since it was first merged upstream back in 2016. For more\nhistorical context, there are a few <a href=\"https://blogs.igalia.com/msisov/chrome-on-wayland-waylandification-project/\" target=\"_blank\" rel=\"noreferrer\">blog</a><a href=\"https://blogs.igalia.com/adunaev/2021/12/17/ozone-our-way-to-the-big-change/\" target=\"_blank\" rel=\"noreferrer\">posts</a> and this <a href=\"https://www.youtube.com/watch?v=KSxaoXOSxhs\" target=\"_blank\" rel=\"noreferrer\">amazing talk</a>, by my colleagues\nAntonio Gomes and Max Ihlenfeldt, presented at last year‚Äôs <a href=\"https://webengineshackfest.org/\" target=\"_blank\" rel=\"noreferrer\">Web Engines\nHackfest</a>.</p><p>Especially due to the <a href=\"https://chromium.googlesource.com/chromium/src/+/refs/tags/88.0.4324.84/docs/lacros.md\" target=\"_blank\" rel=\"noreferrer\">Lacros project</a>, progresses on Linux\nDesktop has been slower over the last few years. Fortunately, the\nscenario changed since last year, when a new sponsor came up and made it\npossible to address most of the outstanding missing features and issues\nrequired to move Ozone Wayland to the finish line.</p><p>It‚Äôs been a few months since Chromium Wayland backend has started to be\ntested as the main browser backend by Google employees, through a finch\ntrial experiment, as well as internally at Igalia. Feedback collected\nsince then is quite positive in general. The exception is Nvidia setups,\nwhich, depending on the driver version, may face major regressions (see\n<a href=\"https://nickdiego.dev/blog/chromium-ozone-wayland-the-last-mile-stretch/#explicit-sync\">Explicit Sync</a> section below for more details).</p><p>While official roll-out has been under discussion, it‚Äôs still disabled\nby default on Linux Desktop. Early adopters willing to test it are\nencouraged to explicitly opt-in by flipping the \nchrome flag to  or . Issue reports are welcome at\n<a href=\"https://crbug.com/new?component=1456988&amp;template=0\" target=\"_blank\" rel=\"noreferrer\">crbug.com/new</a>.</p><p>There are also a few other Wayland-specific flags which might be\nselectively enabled, if you feel brave enough :) such as, ui scaling,\ntext input v3, etc; all described in more details below.</p><figure><figcaption>Chrome Wayland flags available in M135.</figcaption></figure><p>Initial fractional scaling support for Linux Desktop was originally\nimplemented by an external contributor <a href=\"https://chromium-review.googlesource.com/c/chromium/src/+/4370091\" target=\"_blank\" rel=\"noreferrer\">back in\n2023</a>.\nAfter some months of stabilization, reports of <a href=\"https://issues.chromium.org/336007385\" target=\"_blank\" rel=\"noreferrer\">blurriness and some\nother subtle issues</a> started to\npop up, which were listed as top-priority when this new project phase\nkicked off back in 2024 June.</p><p>After some analysis, we could confirm that there were some fundamental\nmissing bits in the process. Which was the actual usage of the fractional\nscale values provided by the Wayland compositor, via\nthe <a href=\"https://wayland.app/protocols/fractional-scale-v1\" target=\"_blank\" rel=\"noreferrer\">fractional-scale-v1</a>\nprotocol extension. Rather than using it, fractional scales were being\n using <a href=\"https://wayland.app/protocols/xdg-output-unstable-v1\" target=\"_blank\" rel=\"noreferrer\">xdg-output</a>\nprotocol instead, which is unsupported (for such usage) and prone to\nprecision issues.</p><p>The screenshot above shows a sharp Chrome window scaled by a 1.25\nfactor, running on Gnome Shell 47.</p><p>The work involved a considerable architectural refactoring to make it\npossible to support the per-window scaling design of the Wayland\nprotocol, without breaking the standard per-display scaling implemented\nin Chromium. The feature started shipping experimentally in Milestone\n128, behind the  chrome flag and disabled by\ndefault until M135. I plan to cover it in more details soon in separate\nblog post.</p><p>IME has been yet another major pain point for some Wayland users. Back\nin last June, a careful study was conducted by my colleague <a href=\"https://garai.ca\" target=\"_blank\" rel=\"noreferrer\">Orko\nGarai</a> in order to understand and consolidate the\npossible approaches to tackle it, as well as pros, cons and potential\nroad-blockers for each of them. Besides the detailed outcomes of that\nstudy, publicly available in the form of a <a href=\"https://docs.google.com/document/d/1GkOphcAQBMdW4iPiMOd9eKd70tlXWQaR7M3GJXGUDpQ\" target=\"_blank\" rel=\"noreferrer\">design\ndocument</a>,\nOrko has recently started a blog post series about the topic\n<a href=\"https://garai.ca/what_is_ime\" target=\"_blank\" rel=\"noreferrer\">here</a>.</p><p>Experimental support for\n<a href=\"https://wayland.app/protocols/text-input-unstable-v3\" target=\"_blank\" rel=\"noreferrer\">text-input-v3</a>\nprotocol was implemented on the Chromium side, with ongoing work on the\nWayland community side to fullfill browser use cases from a protocol\nperspective, which is expected to come soon as part of <a href=\"https://gitlab.freedesktop.org/wayland/wayland-protocols/-/merge_requests/282\" target=\"_blank\" rel=\"noreferrer\">version\n3.2</a>\nof the text-input protocol.</p><p>In the meantime, we keep working with the Chromium community on\nimprovements to the client-side implementation of the protocol, although\nprogress has been slow as we are still looking for some key browser\nrequirements to be solved on the protocol side to have the confidence\nand buy-in for productization.</p><p>Supporting the full original user experience for Chrome‚Äôs tab dragging\nunder Wayland has proved to be complex, especially because the core\nWayland protocol does not cover all of its requirements. Back in 2021,\nwe designed a brand new protocol and implemented it in ChromeOS‚Äô Exo\nWayland compositor, in the context of Lacros project, to fullfill those\ngaps.</p><p>Years later,\n<a href=\"https://wayland.app/protocols/xdg-toplevel-drag-v1\" target=\"_blank\" rel=\"noreferrer\">xdg-toplevel-drag</a>\nhas emerged as a community effort to standardize it upstream, thanks\nDavid Redondo and Robert Mader for working on it. It consists of a\ntrimmed down version of the original  protocol with\nsome tweaks to make it more aligned with Wayland design principles. A\nfew months ago, initial support for it has landed in Chromium and we‚Äôve\nbeen stabilizing it since then.</p><h4>Full UX support in Mutter<a href=\"https://nickdiego.dev/blog/chromium-ozone-wayland-the-last-mile-stretch/#full-ux-support-in-mutter\" aria-hidden=\"true\">#</a></h4><p>Back in last November, xdg-toplevel-drag was\n<a href=\"https://wayland.app/protocols/xdg-toplevel-drag-v1#compositor-support\" target=\"_blank\" rel=\"noreferrer\">supported</a>\nonly in KWin and Jay compositors, when a demand to implement support for\nit in Mutter was raised by our customer, and the task was assigned to\nme.</p><p>Long story short, it was a pretty interesting and challenging experience\nwhich I‚Äôm glad to have had. As a curiosity, last time I had coded in C\nand glib had been , maybe &gt;13 years? üò± Also, it was my\nfirst time hacking on Mutter and Gnome code base. After all, the\n<a href=\"https://gitlab.gnome.org/GNOME/mutter/-/merge_requests/4107\" target=\"_blank\" rel=\"noreferrer\">MR</a> did\nlanded and will start shipping as part of Gnome 48, next month.</p><figure><figcaption>xdg-toplevel-drag-v1 demo on Gnome Shell 48.</figcaption></figure><p>I‚Äôm preparing a blog post to share more technical details about the\nprotocol implementation from the perspective of a browser developer and\nGnome/Mutter newcomer.</p><p>Let me take the opportunity to say thanks to the Gnome developers who\nhelped me a lot in the process: Jonas √Ödahl, Carlos Garnacho, Georges\nStavracas and Sebastian Wick. Really appreciate your help and patience,\nguys!</p><p>In parallel to the work on the regular tab drag experience through\nxdg-toplevel-drag, a fallback implementation relying solely on core\nWayland drag-and-drop protocol has been led by my colleague Max\nIhlenfeldt. A few days ago, Max has published an <a href=\"https://blogs.igalia.com/max/fallback-tab-dragging/\" target=\"_blank\" rel=\"noreferrer\">awesome in-depth blog\npost</a> about his\nwork on it. Enjoy the read!</p><figure><figcaption>Fallback tab dragging UX demo</figcaption></figure><p>The main difference to the regular UX is that, rather than instantly\ncreating a browser window when it gets dragged out of its tab strip, a\ndrag icon containing the tab thumbnail is used instead. Browser window\ncreation is then deferred to when the drop happens, as can be seen in\nthe video above. The feature was recently enabled by default, and\nstarted shipping in Chrome 133.</p><p>Linux desktop environments usually support system-wide ‚Äútext scaling‚Äù\nsettings, which are supposed to be handled by applications. On\nGnome-based environments, it can be triggered via several ways,\nsuch as the ‚ÄúLarge Text‚Äù accessibility feature.</p><p>Historically, it has been supported in Chromium X11 by resizing the\nwhole browser UI elements, instead of just text items. After an in-depth\nanalysis, it was decided to follow the very same approach for the\nWayland initial implementation. The main motivation was that there seems\nto be gaps in both Chromium‚Äôs internal UI framework as well as in\nChrome‚Äôs UI/layout code, which would need to be fixed before supporting\nsuch text-only live resizing/relayout.</p><figure><figcaption>Quick demo of text scale in action on Chromium Wayland on Gnome 47.</figcaption></figure><p>Technically, the solution involved implementing an additional scaling\nlayer in Ozone/Wayland, so called ‚Äúui scale‚Äù which make the browser UI\nto get fully resized/re-laid out instantly in reaction to system‚Äôs ‚Äútext\nscaling factor‚Äù updates.</p><p>The feature has started shipping in Milestone 131, disabled by default\nas usual. Users can enable it by using the <code>chrome://flags/#wayland-ui-scaling</code>\nflag.</p><p>To address some <a href=\"https://issues.chromium.org/issues/377438303\" target=\"_blank\" rel=\"noreferrer\">display tearing reports</a>,\nsupport for the <a href=\"https://wayland.app/protocols/linux-drm-syncobj-v1\" target=\"_blank\" rel=\"noreferrer\">linux-drm-syncobj-v1</a>\nprotocol has been implemented. The patch series has landed and started\nshipping in version 132.0.6834.83, and can be enabled using the\n<code>wayland-linux-drm-syncobj</code> chrome flag.</p><p>Feedback has been positive so far. If you‚Äôre willing to give it a try,\nplease bear in mind that Linux kernel version &gt;= 6.11 is required, and\ndon‚Äôt hesitate to get back to us with your remarks.</p><p>Besides overall stabilization and maintenance, there is a large ongoing\neffort led by my colleague Orko to get Chromium‚Äôs interactive UI tests\ninfrastructure and code working with major Wayland compositors,\nprimary focus is Mutter/Gnome, though wlroots is also considered\nfor the future.</p><p>Another area we are currently investigating is ‚Äúsession management‚Äù,\nwhich will make it possible to restore browser window attributes, such\nas, position, display and workspace across restarts.</p><p>Aside from that, there are a bunch of technical debt and follow-up\nissues which spun off from some of the features and fixes listed above,\nsuch as:</p><p>New sponsors and partners are always welcome, so please don‚Äôt hesitate\nto mail us to discuss how we coud be of help.</p><p>Yay! Quite busy and exciting times!! I‚Äôd like to thank once more all of\nour supporters, sponsors and, of course, <a href=\"https://igalia.com\" target=\"_blank\" rel=\"noreferrer\">Igalia</a> for making all\nthis possible ‚ù§Ô∏è Looking forward to the challenges ahead! Stay tuned for\nmore updates and don‚Äôt hesitate to reach out if you have questions or\nother remarks. üëãüëã</p>","contentLength":9484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1itygay/chromium_ozonewayland_the_last_mile_stretch/"},{"title":"The Fedora Project Leader is willfully ignorant about Flathub","url":"https://blogs.gnome.org/alatiera/2025/02/19/the-fedora-project-leader-is-willfully-ignorant-about-flathub/","date":1740057149,"author":"/u/Worldly_Topic","guid":7234,"unread":true,"content":"<p>Today I woke up to a link of an <a href=\"https://www.youtube.com/watch?v=oKP1hgdFJKo\">interview</a> from the current Fedora Project Leader, Matthew Miller. Brodie who conducted the interview mentioned that Miller was the one that reached out to him. The background of this video was the currently ongoing issue regarding OBS, Bottles and the Fedora project, which Niccol√≤ made an <a href=\"https://youtu.be/o2qd2RFC6Fk\">excellent video</a> explaining and summarizing the situation. You can also find the article over at <a href=\"https://thelibre.news/why-obs-and-bottles-got-in-a-fight-with-fedora/\">thelibre.news</a>. ‚ÄúImpressive‚Äù as this story is, it‚Äôs for another time.</p><p>What I want to talk in this post, is the outrageous, smearing and straight up slanderous statements about Flathub that the Fedora Project Leader made during the interview..</p><p>I am not directly involved with the Flathub project (A lot of my friends are), however I am a maintainer of the GNOME Flatpak Runtime, and a contributor to the Freedesktop-sdk and ElementaryOS Runtimes. I also maintain applications that get published on Flathub directly. So you can say I am someone invested in the project and that has put a lot of time into it. It was extremely frustrating to hear what would only qualify as reddit-level completely made up arguments with no base in reality coming directly from Matthew Miller.</p><p>Below is a transcript, slightly edited for brevity, of all the times Flathub and Flatpak was mentioned. You can refer to the <a href=\"https://www.youtube.com/watch?v=oKP1hgdFJKo\">original video</a> as well as there were many more interesting things Miller talked about.</p><p>It starts off with an introduction and some history and around the 10-minute mark, the conversation starts to involve Flathub.</p><blockquote><p>Miller: [..] long way of saying I think for something like OBS we‚Äôre not really providing anything by packaging that. Miller: I think there is an overall place for the Fedora Flatpaks, because Flathub part of the reason its so popular (there‚Äôs a double edged sword), (its) because the rules are fairly lax about what can go into Flathub and the idea is we want to make it as easy for developers to get their things to users, but there is not really much of a review</p></blockquote><p>This is not the main reason why Flathub is popular, its a lot more involved and interesting in practice. I will go into this in a separate post hopefully soon.</p><p>Claiming that Flathub does not have any review process or inclusion policies is straight up wrong and incredibly damaging. It‚Äôs the kind of thing we‚Äôve heard ad nauseam from Flathub haters, but never from a person in charge of one of the most popular distributions and that should have <strong>really really known better</strong>.</p><p>You can find the <a href=\"https://docs.flathub.org/docs/for-app-authors/requirements\">Requirements</a> in the Flathub documentation if you spend 30 seconds to google for them, along with the submission <a href=\"//docs.flathub.org/docs/for-app-authors/submission/\">guidelines</a> for developers. If those documents qualify as a wild west and free for all, I can‚Äôt possibly take you seriously.</p><p>I haven‚Äôt maintained a linux distribution package myself so I won‚Äôt go to comparisons between Flathub and other distros, however you can find people, with red hats even, <a href=\"https://social.vivaldi.net/@sesivany/114030210735848325\">that do so and talked about it</a>. Of course this is one off examples and social bias from my part. But it proves how laughable of a claim is that things are not reviewed. Additionally, the most popular story I hear from developers is how Flathub requirements are often stricter and sometimes cause annoyances.</p><p>Additionally, Flathub has been the driving force behind encouraging applications to update their metadata, completely reworking the User Experience and handling off permissions and made them prominent to the user. (To the point where even network access is marked as potentially-unsafe).</p><blockquote><p>Miller: [..] the thing that says verified just says that it‚Äôs verified from the developer themselves.</p></blockquote><p>No, verified does not mean that the developer signed off into it. Let‚Äôs take another 30 seconds to look into the Flathub <a href=\"https://docs.flathub.org/docs/for-users/verification\">documentation page</a> about exactly this.</p><blockquote><p>A verified app on Flathub is one whose developer has confirmed their ownership of the app ID [‚Ä¶]. This usually also may mean that either the app is maintained directly by the developer or a party authorized or approved by them.</p></blockquote><p>It still went through the review process and all the rest of requirements and policies apply. The verified program is basically a badge to tell users this is a supported application by the upstream developers, rather than the free for all that exists currently where you may or may not get an application released from years ago depending on how stable your distribution is.</p><p>Sidenote, did you know that 1483/3003 applications on Flathub are verified as of the writing of this post? As opposed to maybe a dozen of them at best in the distributions. You <a href=\"https://gitlab.gnome.org/-/snippets/6791\">can check</a> for yourself</p><blockquote><p>Miller: .. and it doesn‚Äôt necessarily verify that it was build with good practices, maybe it was built in a coffee shop on some laptop or whatever which could be infected with malware or whatever could happen</p></blockquote><p>Again if Miller had done the bare minimum effort, he would have come across the <a href=\"https://docs.flathub.org/docs/for-app-authors/requirements\">Requirements</a> page which describes exactly how an Application in Flathub is built, instead of further spreading made up takes about the infrastructure. I can‚Äôt stress enough how damaging it has been&nbsp;throughout the years to claim that ‚ÄúFlathub may be potential Malware‚Äù. Why it‚Äôs malware? Because I don‚Äôt like its vibes and I just assume so..</p><p>I am sure If I did the same about Fedora in a very very public medium with thousand of listeners I would probably end up with a Layers letter from Redhat.</p><p>Now Applications in Flathub are all built without a network access, in Flathub‚Äôs build servers, using flatpak-builder and Flatpak Manifests which are a declarative format, which means all the sources required to build the application are known, validated/checksumed, the build is reproducible to the extend possible, you can easily inspect the resulting binaries and the manifest itself used to build the application ends up in  which you can also inspect with the following command and use it to rebuild the application yourself exactly like how it‚Äôs done in Flathub.</p><div><pre><code></code></pre></div><p>The exception to this, are proprietary applications naturally, and a handful of applications (under an OSI approved license) where Flathub developers helped the upstream projects integrate a direct publishing workflow into their Deployment pipelines. I am aware of Firefox and OBS as the main examples, both of which publish in Flathub through their Continues Deployment (CI/CD) pipeline the same way they generate their builds for other platforms they support and the code for how it happens is available on their repos.</p><p>If you have issues trusting Mozilla‚Äôs infrastructure, then how are you trusting Firefox in the first place and good luck auditing gecko to make sure it does not start to ship malware. Surely distribution packagers audit every single change that happens from release to release for each package they maintain and can verify no malicious code ever gets merged. The <a href=\"https://lwn.net/Articles/967180/\">xz backdoor</a> was very recent, and it was identified by pure chance, none of this prevented it.</p><p>Then Miller proceeds to describe the Fedora build infrastructure and afterward we get into the following:</p><blockquote><p>Miller: I will give an example of something I installed in Flathub, I was trying to get some nice gui thing that would show me like my system Hardware stats [‚Ä¶] one of them ones I picked seemed to do nothing, and turns out what it was actually doing, there was no graphical application it was just a script, it was running that script in the background and that script uploaded my system stats to a server somewhere.</p></blockquote><p>Firstly we don‚Äôt really have many details to be able to identify which application it was, I would be very curious to know. Now speculating on my part, the most popular application matching that description it‚Äôs Hardware Probe and it absolutely has a GUI, no matter how minimal. It also asks you before uploading.</p><p>Maybe there is a org.upload.MySystem application that I don‚Äôt know about, and it ended up doing what was in the description, again I would love to know more and update the post if you could recall!</p><blockquote><p>Miller: No one is checking for things like that and there‚Äôs no necessarily even agreement that that was was bad.</p></blockquote><p>Second time! Again with the ‚ÄúThere is no review and inclusion process in Flathub‚Äù narrative. There absolutely is, and these are the kinds of things that get brought up during it.</p><blockquote><p>Miller: I am not trying to be down on Flathub because I think it is a great resource</p></blockquote><p>Yes, I can see that, however in your ignorance you were something much worse than ‚ÄúDown‚Äù. This is pure slander and defamation, coming from the current ‚ÄúFedora Project Leader‚Äù, the ‚ÄúTechnically Voice of Fedora‚Äù (direct quote from a couple seconds later). All the statements made above are manufactured and inaccurate. Myths that you‚Äôd hear from people that never asked, looked or cared about any of these cause the moment you do you its obvious how laughable all these claims are.</p><blockquote><p>Miller: And in a lot of ways Flathub is a competing distribution to Fedora‚Äôs packaging of all applications.</p></blockquote><p>Precisely, he is spot on here, and I believe this is what kept Miller willfully ignorant and caused him to happily pick the first anit-flatpak/anti-flathub arguments he came across on reddit and repeat the verbatim without putting any thought into it. I do not believe Miller is malicious on purpose, I do truly believe he means well and does not know better.</p><p>However, we can‚Äôt ignore the conflict that arises from his current job position as an big influence to why incidents like this happened. Nor the influence and damage this causes when it comes from a person of Matthew Miller‚Äôs position.</p><blockquote><p>Miller: One of the other things I wanted to talk about Flatpak, is the security and sandboxing around it. Miller: Like I said the stuff in the Flathub are not really reviewed in detail and it can do a lot of things:</p></blockquote><p>Third time with the no review theme. I was fuming when I first heard this, and I am very very angry about still, If you can‚Äôt tell. Not only is this an incredibly damaging lie as covered above, it gets repeated over and over again.</p><blockquote><p>With Flatpak basically the developer defines what the permissions are. So there is a sandbox, but the sandbox is what the person who put it there is, and one can imagine that if you were to put malware in there you might make your sandboxing pretty loose.</p></blockquote><blockquote><p>Brodie: One of the things you can say is ‚ÄúI want full file system access, and then you can do anything‚Äù</p></blockquote><p>No, again it‚Äôs stated in the Flathub documentation, permissions are very carefully reviewed and updates get blocked when permissions change until another review has happened.</p><blockquote><p>Miller: Android and Apple have pretty strong leverage against application developers to make applications work in their sandbox</p><p>Brodie: the model is the other way around where they request permissions and then the user grants them whereas Flatpak, they get the permission and then you could reject them later</p></blockquote><p>This is partially correct, the first part about leverage will talk about in a bit, but here‚Äôs a primer on how permissions work in Flatpak and how it compares to the sandboxing technologies in iOS and Android.</p><p>In all of them we have a separation between Static and Dynamic permissions. Static are the ones the application always has access to, for example the network, or the ability to send you notifications. These are always there and are mentioned at install time usually. Dynamic permissions are the ones where the application has to ask the user before being able to access a resource. For example opening a file chooser dialog so the user can upload a file, the application the only gets access to the file the user consented or none. Another example is using the camera on the device and capturing photos/video from it.</p><p>Brodie here gets a bit confused and only mentions static permissions. If I had to guess it would be cause we usually refer to the dynamic permissions system in the Flatpak world as ‚ÄúPortals‚Äù.</p><blockquote><p>Miller: it didn‚Äôt used to be that way and and in fact um Android had much weaker sandboxing like you could know read the whole file system from one app and things like that [‚Ä¶] they slowly tightened it and then app developers had to adjust Miller: I think with the Linux ecosystem we don‚Äôt really have the way to tighten that kind of thing on app developers ‚Ä¶ Flatpak actually has that kind of functionality [‚Ä¶] with portals [‚Ä¶] but there‚Äôs no not really a strong incentive for developers to do that because, you know well, first of all of course my software is not going to be bad so why should I you know work on sandboxing it, it‚Äôs kind of extra work and I I don‚Äôt know I don‚Äôt know how to solve that. I would like to get to the utopian world where we have that same security for applications and it would be nice to be able to install things from completely untrusted places and know that they can‚Äôt do anything to harm your system and that‚Äôs not the case with it right now</p></blockquote><p>As with any technology and adoption, we don‚Äôt get to perfection from day 1. Static permissions are necessary to provide a migration path for existing applications and until you have developed the appropriate and much more complex dynamic permissions mechanisms that are needed. For example up until iOS 18 it wasn‚Äôt possible to give applications access to a subset of your contacts list. Think of it like having to give access your entire filesystem instead of the specific files you want. Similarly partial-only access to your photos library arrived couple years ago in IOS and Android.</p><p>In an ideal world all permissions are dynamic, but this takes time and resources and adaptation for the needs of applications and the platform as development progresses.</p><p>Now about the leverage part.</p><p>I do agree that ‚Äúthe Linux ecosystem‚Äù as a whole does not have any leverage on applications developers. This is cause Miller is looking at the wrong place for it. <a href=\"https://blogs.gnome.org/tbernard/2019/12/04/there-is-no-linux-platform-1/\">There is no Linux ecosystem</a> but rather Platforms developers target.</p><p>GNOME and KDE, as they distribute all their applications on Flathub absolutely have leverage. Similarly Flathub itself has leverage by changing the publishing requirements and inclusion guidelines. Which I kept being told they don‚Äôt exist.. Every other application that wants to publish also has to adhere by the rules on Flathub. ElementaryOS and their Appcenter has leverage on developers. Canonical does have the same pull as well with the Snapstore. Fedora on the other hand doesn‚Äôt have any leverage cause the Fedora Flatpak repository is <a href=\"https://thelibre.news/why-obs-and-bottles-got-in-a-fight-with-fedora/\">irrelevant, broken and nobody wants to use it</a>.</p><p>[..] The <a href=\"https://lwn.net/Articles/967180/\">xz backdoor</a> gets brought up when discussing dependencies and how software gets composed together.</p><blockquote><p>Miller: we try to keep all of those things up to date and make sure everything is patched across the dist even when it‚Äôs even when it‚Äôs difficult. I think that really is one of the best ways to keep your system secure and because the sandboxing isn‚Äôt very strong that can really be a problem, you know like the XZ thing that happened before. If XZ is just one place it‚Äôs not that hard of an update but if you‚Äôve got a 100 Flatpaks from different places [‚Ä¶] and no consistency to it it‚Äôs pretty hard to manage that</p></blockquote><p>I am not going to get in depth about this problem domain and the arguments over it. In fact I have been writing another blog post for a while. I hope to publish shortly. Till then I can not recommend high enough <a href=\"https://www.bassi.io/articles/2017/08/10/dev-v-ops/\">Emmanuele‚Äôs</a> and <a href=\"https://0pointer.net/blog/revisiting-how-we-put-together-linux-systems.html\">Lennart‚Äôs</a> blog posts, as well as one of the very <a href=\"https://blogs.gnome.org/alexl/2011/09/30/rethinking-the-linux-distibution/\">early posts</a> from Alex when Flatpak was in early design phase on the shortcomings of the current distribution model.</p><p>Now about bundled dependencies. The concept of Runtimes has served us well so far, and we have been doing a pretty decent job providing most of the things applications need but would not want to bundle themselves. This makes the Runtimes a single place for most of the high profile dependencies (curl, openssl, webkitgtk and so on) that you‚Äôd frequently update for security vulnerabilities and once it‚Äôs done they roll out to everyone without needing to do anything manual to update the applications or even rebuilt them.</p><p>Applications only need to bundle their direct dependencies,and as mentioned above, the flatpak manifest includes the exact definition of all of them. They are available to anyone to inspect and there‚Äôs tooling that can scan them and hopefully in the future alert us.</p><p>If the Docker/OCI model where you end bundling the entire toolchain, runtime, and now you have to maintain it and keep up with updates and rebuild your containers is good enough for all those enterprise distributions, then the Flatpak model which is much more efficient, streamlined and thought out and much much much less maintenance intensive, it is probably fine.</p><blockquote><p>Miller: part of the idea of having a distro was to keep all those things consistent so that it‚Äôs easier for everyone, including the developers</p></blockquote><p>As mentioned above, nothing that fundamentally differs from the leverage that Flathub and the Platform Developers have.</p><blockquote><p>Brodie: took us 20 minutes to get to an explanation [..] but the tldr Fedora Flatpak is basically it is built off of the Fedora RPM build system and because that it is more well tested and sort of intended, even if not entirely for the Enterprise, designed in a way as if an Enterprise user was going to use it the idea is this is more well tested and more secure in a lot of cases not every case.\nMiller: Yea that‚Äôs basically it</p></blockquote><p>This is a question/conclusion that Brodie reaches with after the previous statements and by far the most enraging thing in this interview. This is also an excellent example of the damage Matthew Miller caused today and if I was a Flathub developer I would stop on nothing sort of a public apology from the Fedora project itself. Hell I want this just being an application developer that publishes on it. The interview has been basically shitting on both the Developers of Flathub  the people that choose to publish in it. And if that‚Äôs not enough there should be an apology just out of decency. Dear god..</p><blockquote><p>Brodie: how should Fedora handle upstreams that don‚Äôt want to be packaged&nbsp; like the OBS case here where they did not want there to be a package in Fedora Flatpak or another example is obviously bottles which has made a lot of noise about the packaging</p></blockquote><p>Lastly I want to touch on this closing question in light of recent events.</p><blockquote><p>Miller: I think we probably shouldn‚Äôt do it. We should respect people‚Äôs wishes there. At least when it is an open source project working in good faith there. There maybe some other cases where the software, say theoretically there‚Äôs somebody who has commercial interests in some thing and they only want to release it from their thing even though it‚Äôs open source. We might want to actually like, well it‚Äôs open source we can provide things, we in that case we might end up you having a different name or something but yeah I can imagine situations where it makes sense to have it packaged in Fedora still but in general especially and when it‚Äôs a you know friendly successful open source project we should be friendly yeah. The name thing is something people forget history like that‚Äôs happened before with Mozilla with Firefox and Debian.</p></blockquote><p>This is an excellent idea! But it gets better:</p><blockquote><p>Miller: so I understand why they strict about that but it was kind of frustrating um you know we in Fedora have basically the same rules if you want to take Fedora Linux and do something out of it, make your own thing out of it, put your own software on whatever, you can do that but we ask you not to call it Fedora if it‚Äôs a fedora remix brand you can use in some cases otherwise pick your own name it‚Äôs all open source but you know the name is ours. yeah and I the Upstream as well it make totally makes sense.</p><p>Brodie: yeah no the name is completely understandable especially if you do have a trademark to already even if you don‚Äôt like it‚Äôs it‚Äôs common courtesy to not name the thing the exact same thing</p><p>Miller: yeah I mean and depending on the legalities like you don‚Äôt necessarily have to register a trademark to have the trademark kind of protections under things so hopefully lawyers you can stay out of the whole thing because that always makes the situations a lot more complicated, and we can just get along talking like human beings who care about making good software and getting it to users.</p></blockquote><p>And I completely agree with all of these, all of it. But let‚Äôs break it down a bit because no matter how nice the words and intentions it hasn‚Äôt been working out this way with the Fedora community so far.</p><p>First, Miller agrees the Fedora project should be respecting of application developer‚Äôs wishes to not have their application distributed by fedora but rather it be a renamed version if Fedora wishes to keep distributing it.</p><p>However, every single time a developer has asked for this, they have been ridiculed, laughed at and straight up bullied by Fedora packagers and the rest of the Fedora community. It has been a similar response from other distribution projects and companies as well, it‚Äôs not just Fedora. You can look at <a href=\"https://thelibre.news/why-obs-and-bottles-got-in-a-fight-with-fedora/\">Bottle‚Äôs story</a> for the most recent example. It is very nice to hear Miller‚Äôs intentions but means nothing in practice.</p><p>Then Miller proceeds to assure us why he understand that naming and branding is such a big deal to those projects (unlike the rest of the Fedora community again). He further informs us how Fedora has the exact same policies and asks from people that want to fork Fedora. Which makes the treatment that every single application developer has received when asking about the same exact thing ever more outrageous.</p><p>What I didn‚Äôt know is that in certain cases you don‚Äôt even need to have a trademark yet to be covered by some of the protections, depending on jurisdiction and all.</p><p>And last we come into lawyers. Neither Fedora nor application developers would want it to ever come to this, and it was stated multiple times by Bottles developers that they don‚Äôt want to have to file for a trademark so they can be taken seriously. Similarly, OBS developers said how resorting to legal action would be the last thing they would want to do and would rather have the issue resolved before that. But it took until OBS, a project of a high enough profile, with the resources required to acquire a trademark and to threaten legal action before the Fedora Leadership cared to treat application developers like human beings and get the Fedora packagers and community members to comply. (Something which they had stated multiple times they simply couldn‚Äôt do).</p><p>I hate all of this. Fedora and all the other distributions need to do better. They all claim to care about their users but happily keep shipping broken and miss configured software to them over the upstream version, just cause it‚Äôs what aligns with their current interests. In this case is the promotion of Fedora tooling and Fedora Flatpaks over the application in Flathub they have no control over. In previous incidents it was about <a href=\"https://pagure.io/fedora-workstation/issue/351\">branding applications</a> like the rest of the system even though it was making them unusable. And I can find you and list you with a bunch of examples from other distributions just as easily.</p><p>They don‚Äôt care about their users, they care about their bottom line first and foremost. Any civil attempts at fixing issues get ignored and laughed at, up until there is a threat of a legal action or a big enough PR damage, drama and shitshow that they can‚Äôt ignore it anymore and have to backtrack on them.</p><p>This is my two angry cents. Overall I am not exactly sure how Matthew Miller managed in a rushed and desperate attempt at damage control for the OBS drama, to not only to make it worse, but to piss off the entire Flathub community at the same time. But what‚Äôs done is done, let‚Äôs see what we can do to address the issues that have festered and persisted for years now.</p>","contentLength":23816,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ity5zp/the_fedora_project_leader_is_willfully_ignorant/"},{"title":"Append-only programming","url":"https://iafisher.com/blog/2024/08/append-only-programming","date":1740056354,"author":"/u/Xadartt","guid":7320,"unread":true,"content":"<p>I have recently adopted a new methodology of software development:</p><ol><li>Everything goes in a single C file.</li><li>New code is appended to the end of the file.</li><li>Existing code cannot be edited.</li></ol><p>I call it .</p><p>Append-only programming has many benefits. It forces you to define your interfaces before your implementations. It encourages you to write small functions. And it produces source code that is eminently readable, because the text of the program recapitulates your train of thought ‚Äì a kind of stream-of-consciousness <a href=\"https://en.wikipedia.org/wiki/Literate_programming\">literate programming</a>.</p><p>Make no mistake: append-only programming is not the most forgiving paradigm. If a subprocedure is found to be erroneous, a corrected version must be appended, and all of its callers must likewise be corrected. In unfortunate cases, the entire program may need to be retyped. The programmer is thus advised to get it right the first time.</p><p>Rather than use a conventional text editor, I prefer to simply , which ensures that rules (2) and (3) are strictly observed. In fact, with a couple of aliases, I <a href=\"https://blog.sanctum.geek.nz/series/unix-as-ide/\">never need to leave the shell</a> at all:</p><div><pre><code>alias edit='cat &gt;&gt; main.c'\nalias show='less main.c'\nalias check='gcc -Wall -c main.c'\nalias build='gcc -Wall main.c'\nalias checkpoint='git add main.c &amp;&amp; git commit -m \".\"'\nalias revert='git restore main.c'\n</code></pre></div><p>In all seriousness, append-only programming is just a fun challenge, not a legitimate way of writing software. I wrote a <a href=\"https://github.com/iafisher/append-only\">small Lisp interpreter</a> in append-only fashion, and it got tedious around the third time I had to re-type .</p><p>My original idea was that, since C lets you forward-declare types and functions, you could write a program incrementally: start by defining the  function in terms of high-level helper functions, then write those helper functions in terms of slightly lower-level functions, and so on until the entire program is complete. It's a sensible approach, and one that I often use in practice.</p><p>Of course, real coding rarely proceeds so smoothly, and you often discover, in the middle of writing your low-level functions, that your high-level functions need to be revised, which append-only programming makes difficult. Even more difficult is troubleshooting code that is not working. It is not an especially compelling use of my time to re-type an entire function just to add  statements.</p><p>Append-only programming was a noble experiment, but perhaps not one that it would be fruitful to repeat. If this post does inspire you to try it yourself, I'd recommend a couple of revisions that preserve the spirit while easing some of the monotonous parts:</p><ul><li>Split out a  header file, so that you can append declarations and imports independently of definitions.</li><li>Split your program into one file for every function, and allow yourself to overwrite files.</li></ul><p>For those of you feeling even more adventurous, may I suggest append-only blogging? Or is that just Twitter?&nbsp;‚àé</p>","contentLength":2838,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1itxwls/appendonly_programming/"},{"title":"Greg Kroah-Hartman Makes A Compelling Case For New Linux Kernel Drivers To Be Written In Rust","url":"http://phoronix.com/news/Greg-KH-On-New-Rust-Code","date":1740054936,"author":"/u/Xaneris47","guid":7170,"unread":true,"content":"\"As someone who has seen almost EVERY kernel bugfix and security issue for the past 15+ years (well hopefully all of them end up in the stable trees, we do miss some at times when maintainers/developers forget to mark them as bugfixes), and who sees EVERY kernel CVE issued, I think I can speak on this topic.\n<p>The majority of bugs (quantity, not quality/severity) we have are due to the stupid little corner cases in C that are totally gone in Rust. Things like simple overwrites of memory (not that rust can catch all of these by far), error path cleanups, forgetting to check error values, and use-after-free mistakes.  That's why I'm wanting to see Rust get into the kernel, these types of issues just go away, allowing developers and maintainers more time to focus on the REAL bugs that happen (i.e. logic issues, race conditions, etc.)\n</p><p>I'm all for moving our C codebase toward making these types of problems impossible to hit, the work that Kees and Gustavo and others are doing here is wonderful and totally needed, we have 30 million lines of C code that isn't going anywhere any year soon.  That's a worthy effort and is not going to stop and should not stop no matter what.\n</p><p>But for new code / drivers, writing them in rust where these types of bugs just can't happen (or happen much much less) is a win for all of us, why wouldn't we do this?  C++ isn't going to give us any of that any decade soon, and the C++ language committee issues seem to be pointing out that everyone better be abandoning that language as soon as possible if they wish to have any codebase that can be maintained for any length of time.\n</p><p>Rust also gives us the ability to define our in-kernel apis in ways that make them almost impossible to get wrong when using them.  We have way too many difficult/tricky apis that require way too much maintainer review just to \"ensure that you got this right\" that is a combination of both how our apis have evolved over the years (how many different ways can you use a 'struct cdev' in a safe way?) and how C doesn't allow us to express apis in a way that makes them easier/safer to use.  Forcing us maintainers of these apis to rethink them is a GOOD thing, as it is causing us to clean them up for EVERYONE, C users included already, making Linux better overall.\n</p><p>And yes, the Rust bindings look like magic to me in places, someone with very little Rust experience, but I'm willing to learn and work with the developers who have stepped up to help out here.  To not want to learn and change based on new evidence (see my point about reading every kernel bug we have.)\n</p><p>Rust isn't a \"silver bullet\" that will solve all of our problems, but it sure will help in a huge number of places, so for new stuff going forward, why wouldn't we want that?\n</p><p>Linux is a tool that everyone else uses to solve their problems, and here we have developers that are saying \"hey, our problem is that we want to write code for our hardware that just can't have all of these types of bugs automatically\".\n</p><p>Why would we ignore that?\n</p><p>Yes, I understand our overworked maintainer problem (being one of these people myself), but here we have people actually doing the work!\n</p><p>Yes, mixed language codebases are rough, and hard to maintain, but we are kernel developers dammit, we've been maintaining and strengthening Linux for longer than anyone ever thought was going to be possible. We've turned our development model into a well-oiled engineering marvel creating something that no one else has ever been able to accomplish. Adding another language really shouldn't be a problem, we've handled much worse things in the past and we shouldn't give up now on wanting to ensure that our project succeeds for the next 20+ years.  We've got to keep pushing forward when confronted with new good ideas, and embrace the people offering to join us in actually doing the work to help make sure that we all succeed together.\n</p>","contentLength":3907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1itxh4g/greg_kroahhartman_makes_a_compelling_case_for_new/"},{"title":"EKS vs. GKE differences in Services and Ingresses for their respective NLBs and ALBs","url":"https://www.reddit.com/r/kubernetes/comments/1itx2uh/eks_vs_gke_differences_in_services_and_ingresses/","date":1740053554,"author":"/u/jumiker","guid":7289,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Patches Would Make All Kernel Encryption/Decryption Faster On x86/x86_64 Hardware","url":"https://www.phoronix.com/news/Linux-x86-Crypt-Drop-Fallback","date":1740052807,"author":"/u/unixbhaskar","guid":7171,"unread":true,"content":"\nOn top of <a href=\"https://www.phoronix.com/news/3.3x-AES-CTR-AMD-Zen-5-Patches\">all the recent x86/x86_64 Linux kernel crypto improvements</a> made recently by Google engineer Eric Biggers to better laverage AVX-512 and other modern x86 ISA features, a new patch-set posted today by Biggers would help make all x86/x86_64 kernel encryption/decryption at least slightly faster.\n<p>The new patch series isn't about making use of some new CPU ISA features or anything wild like that but rather just cleaning up some existing code so that the x86 kernel-mode FPU always works reliably with soft IRQs so that some old SIMD helper crypto fallback code can be removed. That fallback code was \"really bad for performance\" and presented performance implications for those not even needing to rely on it.\n</p><p>In turn this code to remove the no-SIMD encryption/decryption fallbacks can help with performance by a few percent or even as much as a 23% improvement has been noted for AES-XTS.\n</p>Eric Biggers explained with <a href=\"https://lore.kernel.org/lkml/20250220051325.340691-1-ebiggers@kernel.org/\">this RFC patch series</a>:\n<blockquote>\"This patchset fixes a longstanding issue where kernel-mode FPU (i.e., SIMD) was not reliably usable in softirqs in x86, which was creating the need for a fallback.  The fallback was really bad for performance, and it even hurt performance for users that never encountered the edge case where kernel-mode FPU was not usable.\n<p>This patchset aligns x86 with other architectures such as arm, arm64, and riscv by making kernel-mode FPU work in softirqs reliably.  There are a few possible ways to achieve that, and for now I just went with the simplest way; see patch 1 for details.\n</p><p>Patch 2 eliminates all uses of the \"crypto SIMD helper\" from x86, as patch 1 makes it unnecessary.  For the RFC it is just one big patch; I'll probably split patch 2 up if this progresses past RFC status.\n</p><p>Performance results have been positive.  All en/decryption is now slightly faster on x86, as it no longer take a detour through crypto/simd.c.  I get a 7% or 23% improvement for AES-XTS, for example.\n</p><p>I also benchmarked bidirectional IPsec, which has been claimed to often hit the edge case where kernel-mode FPU was previously not usable in softirq context.  Ultimately, I was not actually able to reproduce that edge case being reached unless I reduced the number of CPUs to 1, in which case it then started being occasionally reached.  Regardless, even without that case being reached, IPsec throughput still improved by 2%. In situations where that case was being reached, or where users required a synchronous algorithm, a much larger improvement should be seen.\"</p></blockquote>Great work and beyond the performance benefits, cleaning up this old fallback/helpers introduces just 100 lines of code while dropping 360 lines of existing code.","contentLength":2662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1itwuu5/new_patches_would_make_all_kernel/"},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1itvy0m/weekly_this_week_i_learned_twil_thread/","date":1740049229,"author":"/u/gctaylor","guid":7143,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Found a profiling tool on GitHub: here‚Äôs what I learned testing it","url":"https://www.reddit.com/r/golang/comments/1itvpcl/found_a_profiling_tool_on_github_heres_what_i/","date":1740048221,"author":"/u/dergtersder","guid":7140,"unread":true,"content":"<p>I recently came across Perforator, an open-source profiler, while browsing GitHub. I decided to test it out on one of our Go services, which has been a bit sluggish lately.</p><p>- The real-time flame graphs are incredibly detailed and made it easy to spot inefficient function calls.</p><p>- Setup was simple. It worked out of the box with minimal configuration.</p><p>- One downside... it doesn‚Äôt yet have full Java support, which we also use heavily.</p><p>Overall, I‚Äôd say it‚Äôs great for projects in Go, Python, or C++. Curious if anyone else has tested it. What‚Äôs been your experience?</p>","contentLength":570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"zns ‚Äî A CLI tool for querying DNS records with readable, colored output.","url":"https://github.com/znscli/zns","date":1740048103,"author":"/u/bschaatsbergen_","guid":7285,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1itvodu/zns_a_cli_tool_for_querying_dns_records_with/"},{"title":"Mesa 25.0.0 Release Notes / 2025-02-19 ‚Äî The Mesa 3D Graphics Library latest documentation","url":"https://docs.mesa3d.org/relnotes/25.0.0.html","date":1740046268,"author":"/u/ilep","guid":7356,"unread":true,"content":"<li><p>gallium/radeon: import libdrm_radeon source code, drop the dependency</p></li><li><p>aco: remove unused TCS fields from aco_shader_info</p></li><li><p>ac/nir: get pass_tessfactors_by_reg from nir_gather_tcs_info</p></li><li><p>radeonsi: fix passing TCS wave ID from LS to HS for monolithic LS+HS</p></li><li><p>radeonsi: don‚Äôt overwrite info.tess._primitive mode when it can be correct</p></li><li><p>radeonsi: get the value for load_tcs_primitive_mode_amd from shader info</p></li><li><p>radeonsi: replace are_tessfactors_def_in_all_invocs with nir_gather_tcs_info</p></li><li><p>radeonsi: reduce si_shader_key_ge::tes_prim_mode size to 2 bits</p></li><li><p>radeonsi: remove unused function si_get_tcs_out_patch_stride</p></li><li><p>radeonsi: don‚Äôt set tess level outputs in patch_outputs_written unconditionally</p></li><li><p>radeonsi: remove unused si_shader_info::output_readmask</p></li><li><p>radeonsi: set *outputs_written in scan_io_usage instead of later</p></li><li><p>radeonsi: split outputs_written_before_tes_gs into ls_es_* and tcs_* masks</p></li><li><p>radeonsi/ci: update navi31 failures</p></li><li><p>glsl: add a helper for duplicated code calling nir_opt_varyings</p></li><li><p>gallium: use struct nir_shader * type in finalize_nir instead of void *</p></li><li><p>st/mesa: call pipe_screen::finalize_nir outside of st_finalize_nir</p></li><li><p>gallium: add PIPE_CAP_CALL_FINALIZE_NIR_IN_LINKER</p></li><li><p>st/mesa: add ST_DEBUG=xfb printing xfb info</p></li><li><p>mesa: capture shaders to disk before invoking the linker</p></li><li><p>nir/opt_varyings: add nir_io_always_interpolate_convergent_fs_inputs</p></li><li><p>nir/opt_varyings: add nir_io_compaction_rotates_color_channels</p></li><li><p>nir/opt_varyings: fix packing color varyings</p></li><li><p>nir/opt_varyings: implement compaction without flexible interpolation</p></li><li><p>nir/opt_varyings: don‚Äôt count the cost of the same instruction multiple times</p></li><li><p>radeonsi: fix buffer_size for emulated GS statistics</p></li><li><p>radeonsi: fix an assertion failure in si_shader_ps with AMD_DEBUG=mono</p></li><li><p>radeonsi: handle nir_intrinsic_component in kill_ps_outputs</p></li><li><p>radeonsi: fix gl_FrontFace elimination when one side is culled</p></li><li><p>radeonsi/ci: add options to test llvmpipe, softpipe, virgl, zink</p></li><li><p>nir/print: print fb_fetch_output for variables</p></li><li><p>nir/lower_pntc_ytransform: handle lowered IO</p></li><li><p>nir/lower_clip: fixes for lowered IO without compact arrays</p></li><li><p>nir/lower_clip: rewrite find_output to handle vec2/3 and make it readable</p></li><li><p>nir/lower_fragcoord_wtrans: handle trimmed fragcoord loads</p></li><li><p>nir/lower_two_sided_color: fix for lowered IO</p></li><li><p>nir: add nir_io_semantics::fb_fetch_output_coherent</p></li><li><p>nir: rename nir_io_glsl_opt_varyings to nir_io_dont_optimize and deprecate it</p></li><li><p>nir: add nir_io_separate_clip_cull_distance_arrays to replace PIPE_CAP</p></li><li><p>vc4/lower_blend: don‚Äôt read non-existent channels</p></li><li><p>nir: make use_interpolated_input_intrinsics a nir_lower_io parameter</p></li><li><p>ac/surface: adjust HiZ enablement</p></li><li><p>radeonsi: prepare for making SI_NGG_CULL_TRIANGLES/LINES VS only, rename them</p></li><li><p>radeonsi: optionally return MESA_PRIM_UNKNOWN from si_get_input_prim</p></li><li><p>radeonsi: rewrite/replace gfx10_ngg_get_vertices_per_prim</p></li><li><p>radeonsi: return a better value for load_initial_edgeflags_amd</p></li><li><p>radeonsi: clean up and rename gfx10_edgeflags_have_effect</p></li><li><p>radeonsi: add helper si_shader_culling_enabled</p></li><li><p>radeonsi: only compute and use min_direct_count on gfx7-8</p></li><li><p>radeonsi: enable NGG culling for non-monolithic TES and GS</p></li><li><p>radeonsi: don‚Äôt use nir_io_dont_optimize because it‚Äôs deprecated</p></li><li><p>r300: don‚Äôt lower sin/cos in finalize_nir</p></li><li><p>nir/opt_varyings: use a hash table to make cloning SSA faster</p></li><li><p>amd: import libdrm_amdgpu ioctl wrappers</p></li><li><p>util,amd: add inlinable versions of drmIoctl/drmCommandWrite*</p></li><li><p>nir: allow cloning indirect array derefs in nir_clone_deref_instr</p></li><li><p>nir/lower_io_to_temporaries: fix interp_deref_at_* lowering</p></li><li><p>radeonsi: don‚Äôt call set_framebuffer_state in si_destroy_context</p></li><li><p>radeonsi: handle a failure to create gfx_cs</p></li><li><p>winsys/amdgpu: fix FD mismatch</p></li><li><p>Revert ‚Äúgbm: mark surface buffers as explicit flushed‚Äù</p></li><li><p>nir/lower_clip: don‚Äôt set cursor to fix crashes due to removed instructions</p></li><li><p>nir/lower_clip: separate code for IO variables and intrinsics</p></li><li><p>nir/lower_clip: set clip_distance_array_size outside of create_clipdist_vars</p></li><li><p>nir/lower_clip: convert nir_lower_clip_gs to nir_shader_intrinsics_pass</p></li><li><p>nir/lower_clip: implement ClipVertex lowering for GS + lowered IO correctly</p></li><li><p>vc4: lower clip planes in st/mesa</p></li><li><p>nir/opt_varyings: always call remove_dead_varyings in init_linkage</p></li><li><p>nir/opt_varyings: add a default callback for varying_estimate_instr_cost</p></li><li><p>nir/opt_varyings: replace options::lower_varying_from_uniform with a cost number</p></li><li><p>nir/algebraic: use is_used_once in a few iand/ior patterns</p></li><li><p>nir/algebraic: optimize (a &amp; b) &amp; (a &amp; c) ==&gt; (a &amp; b) &amp; c</p></li><li><p>nir/algebraic: optimize (a | b) | (a | c) ==&gt; (a | b) | c</p></li><li><p>nir/algebraic: optimize (a &amp; b) | (a | c) =&gt; a | c, (a &amp; b) &amp; (a | c) =&gt; a &amp; b</p></li><li><p>gallium: replace PIPE_SHADER_CAP_INDIRECT_INPUT/OUTPUT_ADDR with NIR options</p></li><li><p>st/mesa: replace EmitNoIndirectInput / EmitNoIndirectOutput with NIR options</p></li><li><p>util/bitset_test: test the return value of BITSET_TEST_RANGE_INSIDE_WORD better</p></li><li><p>util/bitset: add BITSET_GET_RANGE_INSIDE_WORD</p></li><li><p>nir/linking_helpers: don‚Äôt promote interpolated varyings to flat</p></li><li><p>nir/opt_varyings: remove redundant conditions from a while loop</p></li><li><p>nir/opt_varyings: fix compaction with sparse indirect FS inputs</p></li><li><p>nir/opt_varyings: count the number of unused components for compaction correctly</p></li><li><p>nir/opt_varyings: fix max_slot for color varying compaction</p></li><li><p>nir/opt_varyings: make top-level compaction code for TES, TCS, GS separate</p></li><li><p>nir/opt_varyings: change try_move_postdominator param to nir_instr type</p></li><li><p>amd,zink: remove options.varying_estimate_instr_cost callbacks</p></li><li><p>nir/opt_varyings: propagate indirect uniform/UBO loads into the next shader</p></li><li><p>nir/opt_varyings: add inter-shader code motion for uniform/UBO indexing</p></li><li><p>nir/opt_varyings: fix getting deref variables for sysvals</p></li><li><p>nir/opt_varyings: remove rare dead output stores after inter-shader code motion</p></li><li><p>nir/opt_varyings: fix compile failures in the disabled PRINT code</p></li><li><p>amd/ci: add piglit failures due to a overzealous test</p></li><li><p>nir/lower_io_passes: lower indirect IO for TCS</p></li><li><p>radeonsi: pass cull face state via user SGPRs for shader culling</p></li><li><p>radeonsi: revert to always returning true for load_cull_any_enabled_amd</p></li><li><p>radeonsi: try to fix Navi14 regression in debug builds</p></li><li><p>radeonsi: don‚Äôt compute total_direct_count in si_draw if it‚Äôs unused</p></li><li><p>radeonsi/ci: handle glinfo errors better</p></li><li><p>radeonsi/ci: stop using a global flakes list, only use a per-chip flakes list</p></li><li><p>radeonsi/ci: remove most flakes and some skips, update navi31 failures</p></li><li><p>radeonsi/ci: remove ‚Äìslow</p></li><li><p>radeonsi/ci: update navi31 failures</p></li><li><p>r600: fix a constant buffer memory leak for u_blitter</p></li><li><p>ac/lower_ngg: improve streamout code generation for gfx12/ACO to match LLVM</p></li><li><p>ac: update SPI_GRP_LAUNCH_GUARANTEE_* register values for gfx12</p></li><li><p>ac/surface/gfx12: enable DCC 256B compressed blocks and reorder modifiers</p></li><li><p>radeonsi/gfx12: set DB_RENDER_OVERRIDE based on stencil state</p></li><li><p>radeonsi/gfx12: adjust HiZ/HiS logic</p></li><li><p>ac/nir: reserve the first LDS vec4 for the HS tf0/1 group vote in TCS</p></li><li><p>ac/nir: use s_sendmsg(HS_TESSFACTOR) to optimize writing tess factors for gfx11</p></li><li><p>ac/nir: allow a TCS input to be available from both VGPRs and LDS</p></li><li><p>ac,radv,radeonsi: enable TCS input reads from VGPRs for all compatible loads</p></li><li><p>ac/nir: add new helpers for computing the TCS LDS/offchip size accurately</p></li><li><p>radeonsi: remove unused parameter tcs_vgpr_only_inputs from si_get_nir_shader</p></li><li><p>radeonsi: switch to the new TCS LDS/offchip size computation</p></li><li><p>radv: switch to the new TCS LDS/offchip size computation</p></li><li><p>ac/nir: call nir_gather_tcs_info only once for RADV</p></li><li><p>nir/opt_varyings: set all IO types to float to facilitate full vectorization</p></li><li><p>nir/opt_varyings: clear info-&gt;clip/cull_distance_array_size if relocated</p></li><li><p>st/mesa: don‚Äôt use nir_opt_fragdepth because it‚Äôs incorrect with MSAA</p></li><li><p>mesa: set correct XFB prim mode for draw validation after resuming XFB</p></li><li><p>mesa: fix printing _NEW_* flags</p></li><li><p>gallium: pass XFB primitive mode to set_stream_output_targets</p></li><li><p>st/mesa: add a pass that unlowers IO intrinsics to variables</p></li><li><p>glsl,st/mesa: always lower IO for GLSL, unlower IO for drivers</p></li><li><p>v3d: enable uniform expression propagation from outputs to the next shader</p></li><li><p>ci: update fail lists and trace checksums</p></li><li><p>virgl/ci: disable virgl-traces because it doesn‚Äôt upload results</p></li><li><p>radeonsi/ci: don‚Äôt copy skips.csv to the results directory</p></li><li><p>radeonsi/ci: update failures and flakes</p></li><li><p>radeonsi: fix a gfx10.3 regression due to a gfx12 change</p></li><li><p>radeonsi: kill Z and stencil PS outputs if depth or stencil is disabled</p></li><li><p>radeonsi/gfx11: fix alpha-to-coverage + alpha-to-one used together</p></li><li><p>radeonsi: fix alpha-to-coverage + alpha-to-one used together for gfx6-10.3</p></li><li><p>radeonsi: implement nir_opt_frag_depth using kill_z instead of the NIR pass</p></li><li><p>radeonsi: eliminate shader code computing killed Z/S/samplemask PS outputs</p></li><li><p>radeonsi: make NGG streamout output primitive type known at compile time</p></li><li><p>radeonsi/gfx12: fix DrawTransformFeedback(stream != 0)</p></li><li><p>radeonsi/gfx12: tune streamout performance</p></li><li><p>radeonsi: make nir-&gt;info and si_shader_info::base identical</p></li><li><p>radeonsi: remove some uses of enum pipe_shader_type</p></li><li><p>radeonsi: make si_init_shader_args static</p></li><li><p>radeonsi: call si_init_shader_args in si_get_nir_shader</p></li><li><p>radeonsi: use nir-&gt;info instead of sel-&gt;info.base</p></li><li><p>radeonsi: disable luminance alpha formats on gfx6</p></li><li><p>radeonsi,radv: fix incorrect min_esverts for NGG subgroup calculation</p></li><li><p>ac/llvm: remove unused code</p></li><li><p>radeonsi/ci: update failures</p></li><li><p>radeonsi: fix a TCS regression</p></li><li><p>radeonsi: switch si_get_blitter_vs to IO intrinsics</p></li><li><p>radeonsi: remove unused code</p></li><li><p>radeonsi: fix a front face regression (crash)</p></li><li><p>nir/opt_load_store_vectorize: make hole_size signed to indicate overlapping loads</p></li><li><p>radv: reduce maxGeometryShaderInvocations to 32</p></li><li><p>ac/nir: handle disabled PS VGPRs in ac_nir_load_arg_at_offset</p></li><li><p>amd: lower load_pixel_coord in NIR</p></li><li><p>amd: lower load_frag_coord in NIR</p></li><li><p>amd: lower load_local_invocation_id in NIR</p></li><li><p>amd: lower load_first_vertex/base_instance/draw_id/view_index in NIR</p></li><li><p>amd: lower load_invocation_id in NIR</p></li><li><p>amd: lower load_sample_id in NIR</p></li><li><p>amd: lower load_sample_pos in NIR</p></li><li><p>amd: lower load_frag_shading_rate in NIR</p></li><li><p>amd: lower load_front_face in NIR</p></li><li><p>ac,radeonsi: move load_vector_arg flags to common code</p></li><li><p>amd: lower load_barycentric_pixel/centroid/sample in NIR</p></li><li><p>amd: lower load_barycentric_at_offset in NIR</p></li><li><p>amd: lower load_gs_wave_id_amd in NIR</p></li><li><p>amd: lower load_vertex_id/instance_id and overwrite_vs_arguments in NIR</p></li><li><p>radeonsi: don‚Äôt return 0 from si_get_max_workgroup_size</p></li><li><p>ac/nir: extract a load_subgroup_id lowered helper</p></li><li><p>amd: lower load_local_invocation_index in NIR</p></li><li><p>amd: lower load_subgroup_invocation in NIR</p></li><li><p>amd: lower load_tess_rel_patch_id/primitive_id/tess_coord and overwrite.. in NIR</p></li><li><p>ac/llvm: remove already lowered cases</p></li><li><p>ac/nir: lower more loads in ac_nir_lower_intrinsics_to_args instead of drivers</p></li><li><p>ac/nir: clean up ac_nir_lower_indirect_derefs</p></li><li><p>ac/nir: add helper ac_nir_load_arg_upper_bound</p></li><li><p>ac/nir: set arg_upper_bound_u32 for vs_rel_patch_id</p></li><li><p>ac/nir: split local_invocation_ids to 3 separate VGPR inputs</p></li><li><p>ac/nir: set upper ranges for range analysis while lowering system values</p></li><li><p>radeonsi: lower sysval intrinsics as late as possible</p></li><li><p>amd: optimize atomics before lowering intrinsics</p></li><li><p>radeonsi: use nir_opt_sink</p></li><li><p>radeonsi: use nir_opt_move</p></li><li><p>vulkan: silence an unused variable warning</p></li><li><p>llvmpipe: silence an unused result warning</p></li><li><p>util/disk_cache: silence unused result warnings</p></li><li><p>nir: set nir_io_semantics::num_slots to at least 1 in build helpers</p></li><li><p>nir: set src_type and dest_type to float implicitly for IO build helpers</p></li><li><p>nir: don‚Äôt set num_slots/src/dest_type/write_mask when they‚Äôre set automatically</p></li><li><p>nir: flip the early exit condition in nir_lower_io_temporaries</p></li><li><p>nir: remove redundant option linker_ignore_precision</p></li><li><p>nir: use IO intrinsics in nir_lower_bitmap</p></li><li><p>nir: use IO intrinsics in nir_lower_drawpixels</p></li><li><p>mesa: remove unused PROGRAM_SYSTEM_VALUE</p></li><li><p>mesa: remove unused PROGRAM_WRITE_ONLY</p></li><li><p>st/mesa: fold st_translate_prog_to_nir into prog_to_nir</p></li><li><p>st/mesa: run DCE before st_unlower_io_to_vars</p></li><li><p>st/mesa: use IO intrinsics in st_nir_lower_fog</p></li><li><p>st/mesa: use IO intrinsics in st_nir_lower_position_invariant</p></li><li><p>st/mesa: switch ATI_fs to IO intrinsics</p></li><li><p>st/mesa: unlower IO for internal shaders if needed</p></li><li><p>st/mesa: switch Z/S DrawPixels shaders to IO intrinsics</p></li><li><p>st/mesa: switch GL_SELECT shader to IO intrinsics</p></li><li><p>st/mesa: switch st_nir_make_passthrough_shader to IO intrinsics</p></li><li><p>st/mesa: switch st_pbo_create_vs and st_pbo_create_gs to IO intrinsics</p></li><li><p>st/mesa: switch PBO create_fs to IO intrinsics</p></li><li><p>st/mesa: switch st_nir_make_clearcolor_shader to IO intrinsics</p></li><li><p>st/mesa: don‚Äôt use nir_copy_var</p></li><li><p>st/mesa: recompute IO bases for ARB_vp/fp</p></li><li><p>glsl: fix corruption due to blake3 hash not being set for nir_opt_undef</p></li><li><p>radeonsi: ignore PIPE_RESOURCE_FLAG_TEXTURING_MORE_LIKELY for TC-compatible HTILE</p></li><li><p>radeonsi: simplify and fix enable_tc_compatible_htile_next_clear logic</p></li><li><p>radeonsi: re-enable non-TC-compatible HTILE for write-only Z/S</p></li><li><p>mesa: switch ARB_vp/fp to IO intrinsics</p></li><li><p>mesa: switch fixed-func fragment program to IO intrinsics</p></li><li><p>nir/algebraic: use is_used_once for comparison patterns</p></li><li><p>nir/algebraic: add and improve pack/unpack patterns</p></li><li><p>nir/algebraic: optimize pack_split(unpack(a).x, unpack(a).y) -&gt; a</p></li><li><p>radeonsi: fix a perf regression due to slow reply from GEM_WAIT_IDLE for timeout=0</p></li><li><p>radeonsi: always use RADEON_USAGE_DISALLOW_SLOW_REPLY</p></li><li><p>ac: update ATOMIC_MEM definitions</p></li><li><p>ac/nir: sort xfb info to facilitate vectorization of xfb stores</p></li><li><p>ac/nir: vectorize streamout stores for legacy pipeline optimally</p></li><li><p>ac/nir/ngg: vectorize streamout stores for NGG optimally</p></li><li><p>ac/nir/ngg: fold so_vertex_index * so_stride into immediate offset</p></li><li><p>ac/nir/ngg: export positions after streamout to improve performance</p></li><li><p>ac,radeonsi: scalarize overfetching loads</p></li><li><p>radeonsi: lower descriptors sooner to allow vectorizing descriptor loads</p></li><li><p>amd: vectorize SMEM loads aggressively, allow overfetching for ACO</p></li><li><p>radeonsi: don‚Äôt set BREAK_PRIMGRP/WAVE_AT_EOI when tessellation is disabled</p></li><li><p>radeonsi: only set BREAK_PRIMGRP/WAVE_AT_EOI when TES/GS need PrimID sysval after TES</p></li><li><p>radeonsi/gfx12: enable alt_hiz_logic</p></li><li><p>radeonsi/gfx12: set DIS_PG_SIZE_ADJUST_FOR_STRIP after shader compilation</p></li><li><p>radeonsi/gfx12: use ACO if LLVM is 19 or older</p></li><li><p>radeonsi/gfx12: use ACO for streamout because it‚Äôs faster</p></li><li><p>mesa: rework enablement of force_gl_names_reuse</p></li><li><p>mesa: enable GL name reuse by default for all drivers except virgl</p></li><li><p>ac/nir: remove broadcast_last_cbuf because it can be deduced from NIR</p></li><li><p>ac/nir: split ac_nir_lower_ps into 2 passes</p></li><li><p>nir: add barycentric coordinates src to load_point_coord_maybe_flipped</p></li><li><p>ac: use Z_EXPORT_FORMAT=32_AR for Z + Alpha mrtz exports</p></li><li><p>ac/llvm: lower vector load_const in NIR</p></li><li><p>ac/llvm: remove the low-optimizing compiler option</p></li><li><p>radeonsi: add si_screen::use_aco to shader cache key to fix shader cache failures</p></li><li><p>radeonsi: remove unused variables from si_shader_context (LLVM)</p></li><li><p>radeonsi: make many shader functions static or move them to .c files</p></li><li><p>radeonsi: remove unused functions</p></li><li><p>nir: add next_stage param to nir_slot_is_varying &amp; nir_remove_sysval_output</p></li><li><p>Revert ‚Äúac/llvm: enable wqm for ac_build_quad_swizzle from ac_build_fs_interp_mov‚Äù</p></li><li><p>nir: add a pass that moves output stores to the end of the shader</p></li><li><p>st/mesa: move VS &amp; TES output stores to the end before unlowering IO</p></li><li><p>mesa: switch fixed-func vertex program to IO intrinsics</p></li><li><p>st/mesa: assert that all incoming shaders use lowered IO</p></li><li><p>st/mesa: remove dead/no-op code due to IO being always lowered</p></li><li><p>glsl: remove dead code due to IO being always lowered</p></li><li><p>glsl: simplify nir_lower_io_to_temporaries logic</p></li><li><p>nir: remove dead code due to IO being always lowered in st/mesa</p></li><li><p>st/mesa: inline st_finalize_nir_before_variants</p></li><li><p>nir: remove handling IO variables from passes used by st/mesa</p></li><li><p>gallium/u_threaded: move tc_batch_execute after all call functions</p></li><li><p>gallium/u_threaded: make the execute function table private</p></li><li><p>gallium/u_threaded: use TC_END_BATCH to terminate the loop</p></li><li><p>gallium/u_threaded: replace the function table with a switch and direct calls</p></li><li><p>gallium/u_threaded: inline all tc_call functions</p></li><li><p>gallium/u_threaded: sort cases in batch_execute by their occurrence</p></li><li><p>zink/ci: skip KHR-Single-GL46‚Ä¶SizedDeclarationsPrimitive due to random timeout</p></li><li><p>dri: put shared-glapi into libgallium.*.so</p></li><li><p>glapi: stop using the remap table</p></li><li><p>glapi: remove the remap table</p></li><li><p>loader: improve the existing loader-libgallium non-matching version error</p></li><li><p>glapi: rename exported symbols so as not to conflict with old libglapi</p></li><li><p>freedreno/ci: skip a dmat3 div test timing out</p></li><li><p>radv: don‚Äôt call ac_nir_lower_ps_early</p></li><li><p>ac/nir: optimize front_face in ac_nir_lower_ps_early</p></li><li><p>ac/nir: lower sample_pos in ac_nir_lower_ps_early</p></li><li><p>ac/nir: lower barycentric_at_offset/sample in ac_nir_lower_ps_early</p></li><li><p>ac/nir: lower fbfetch_output in ac_nir_lower_ps_early</p></li><li><p>ac/nir: return progress from ac_nir_lower_ps_early</p></li><li><p>ac/nir: return progress from ac_nir_lower_ps_late</p></li><li><p>ac/nir: handle FRAG_RESULT_COLOR with dual src blending in ac_nir_lower_ps_early</p></li><li><p>ac/nir: switch passes to use nir_shader_intrinsics_pass</p></li><li><p>ac/nir: drop 16x EQAA support from ac_get_ps_iter_mask</p></li><li><p>ac/nir: clamp vertex color outputs in the right place</p></li><li><p>radeonsi: sample shading state fixes</p></li><li><p>ac,aco,radeonsi: replace SampleMaskIn with 1 &lt;&lt; SampleID if full sample shading</p></li><li><p>ac/nir: simplify force_*_sample_interp options in ac_nir_lower_ps_early</p></li><li><p>ac/nir: simplify force_*_center_interp options in ac_nir_lower_ps_early</p></li><li><p>ac/nir: optimize barycentric_at_sample(sample_id) in ac_lower_ps_early</p></li><li><p>ac/nir: optimize frag_coord &lt;-&gt; pixel_coord in ac_nir_lower_ps_early</p></li><li><p>ac/nir: eliminate sample_mask_in without MSAA in ac_nir_lower_ps_early</p></li><li><p>ac/nir: cosmetic stuff for ac_nir_lower_ps</p></li><li><p>aco: implement replacing frag_coord with pixel_coord in PS prolog</p></li><li><p>aco: simplify how broadcast_last_cbuf is implemented in PS epilog</p></li><li><p>aco: implement replacement of sample_mask_in with helper_invocation in PS prolog</p></li><li><p>ac/nir: compute ddx/ddy for barycentric_at_offset at the beginning of shaders</p></li><li><p>ac/nir: lower sample_pos to load_sample_positions_amd when frag_coord is center</p></li><li><p>nir/opt_varyings: handle user barycentrics</p></li><li><p>mesa: enable GL name reuse for virgl</p></li><li><p>radeonsi: disallow compute queues on Raven/Raven2 due to hangs</p></li><li><p>ac/nir: clamp vertex color outputs in the right place</p></li><li><p>radeonsi: get sample positions from user SGPRs instead of memory</p></li><li><p>radeonsi: fix PS prolog not counting used fragcoord VGPRs correctly</p></li><li><p>radeonsi: implement replacing frag_coord with pixel_coord at draw time</p></li><li><p>radeonsi: don‚Äôt set the alpha ref user SGPR if alpha test doesn‚Äôt use it</p></li><li><p>radeonsi: simplify how broadcast_last_cbuf is implemented for PS epilogs</p></li><li><p>radeonsi: use load_pixel_coord for polygon stipple lowering</p></li><li><p>radeonsi: remove si_nir_kill_ps_outputs and use ac_nir_lower_ps_early instead</p></li><li><p>radeonsi: add load_polygon_stipple_buffer_amd instead of using si_shader_args</p></li><li><p>radeonsi: call si_init_gs_output_info in si_get_nir_shader</p></li><li><p>radeonsi: add si_nir_shader_ctx holding parameters from si_get_nir_shader</p></li><li><p>radeonsi: call si_nir_late_opts unconditionally</p></li><li><p>radeonsi: set the ‚Äúfirst‚Äù parameter of si_nir_opts correctly</p></li><li><p>radeonsi: simplify how the NIR name of shader variants is modified</p></li><li><p>radeonsi: cosmetic changes in get_nir_shader</p></li><li><p>radeonsi: reorder NIR passes in get_nir_shader (part 1)</p></li><li><p>radeonsi: reorder NIR passes in get_nir_shader (part 2)</p></li><li><p>radeonsi: reorder NIR passes in get_nir_shader (part 3)</p></li><li><p>radeonsi: split and restructure get_nir_shader</p></li><li><p>radeonsi: get LS+HS and ES+GS together in get_nir_shader instead of separately</p></li><li><p>radeonsi: set uses_vmem_load/sampler in get_nir_shaders</p></li><li><p>radeonsi: move/rewrite PS color input gathering for shader variants</p></li><li><p>radeonsi: use barycentrics from load_point_coord_maybe_flipped</p></li><li><p>radeonsi: lower indirect indexing sooner</p></li><li><p>radeonsi: move spi_ps_input_config functions up</p></li><li><p>radeonsi: split si_fixup_spi_ps_input_config</p></li><li><p>radeonsi: get SPI_PS_INPUT_ENA from shader variant NIR for ACO</p></li><li><p>radeonsi: minor restructuring of si_llvm_compile_shader</p></li><li><p>radeonsi: verify that SPI_PS_INPUT_ENA from LLVM is equal to ACO</p></li><li><p>radeonsi: remove ac_shader_config from si_shader_part</p></li><li><p>radeonsi: precompute COMPUTE_PGM_RSRC3</p></li><li><p>radeonsi: set SHARED_VGPR_CNT for compute for ACO</p></li><li><p>radeonsi: set SHARED_VGPR_CNT for gfx shaders for ACO</p></li><li><p>radeonsi: gather PS inputs from shader variant NIR</p></li><li><p>radeonsi: don‚Äôt set BASE in si_nir_lower_ps_color_input</p></li><li><p>radeonsi: remove si_shader_info code that is no longer needed</p></li><li><p>radeonsi: implement replacement of sample_mask_in with helper_invocation</p></li><li><p>radeonsi: ignore pipe_rasterizer_state::force_persample_interp</p></li><li><p>radeonsi: fix interpolateAt* with non-GL4 ARB_sample_shading</p></li><li><p>radeonsi/ci: add more gfx11 flakes</p></li><li><p>radeonsi: set gl_FragCoord to pixel center to fix GLCTS failures</p></li><li><p>radeonsi: validate BITSET_TEST_RANGE_INSIDE_WORD assertion at compile time</p></li><li><p>radeonsi: remove SI_TRACKED__UNUSED_GAP</p></li><li><p>radeonsi: dead code removal and move some code out of headers</p></li><li><p>radeonsi: remove redundant divergence analysis and smem flagging</p></li><li><p>radeonsi: remove an incorrectly defined modifier</p></li><li><p>winsys/amdgpu: disable DCC for gfx12 when using AMD_FORCE_FAMILY</p></li><li><p>ac/fake_hw_db: deobfuscate GPU name strings</p></li><li><p>gallium,st/mesa: allow reporting compile failures from create_vs/fs/.._state</p></li>","contentLength":20397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1itv8kk/mesa_2500_release_notes_20250219_the_mesa_3d/"},{"title":"`#[derive(Deserialize)]` can easily be used to break your type's invariants","url":"https://www.reddit.com/r/rust/comments/1itv4mw/derivedeserialize_can_easily_be_used_to_break/","date":1740045795,"author":"/u/hpxvzhjfgb","guid":7237,"unread":true,"content":"<p>Recently I realised that if you just put <code>#[derive(Serialize, Deserialize)]</code> on everything without thinking about it, then you are making it possible to break your type's invariants. If you are writing any unsafe code that relies on these invariants being valid, then your code is automatically unsound as soon as you derive .</p><pre><code>mod non_zero_usize { use serde::{Deserialize, Serialize}; #[derive(Serialize, Deserialize)] pub struct NonZeroUsize { value: usize, } impl NonZeroUsize { pub fn new(value: usize) -&gt; Option&lt;NonZeroUsize&gt; { if value == 0 { None } else { Some(NonZeroUsize { value }) } } pub fn subtract_one_and_index(&amp;self, bytes: &amp;[u8]) -&gt; u8 { assert!(self.value &lt;= bytes.len()); // SAFETY: `self.value` is guaranteed to be positive by `Self::new`, so // `self.value - 1` doesn't underflow and is guaranteed to be in `0..bytes.len()` by // the above assertion. *unsafe { bytes.get_unchecked(self.value - 1) } } } } use non_zero_usize::NonZeroUsize; fn main() { let bytes = vec![5; 100]; // good let value = NonZeroUsize::new(1).unwrap(); let elem = value.subtract_one_and_index(&amp;bytes); println!(\"{elem}\"); // doesn't compile, field is private // let value = NonZeroUsize(0); // panics // let value = NonZeroUsize::new(0).unwrap(); // undefined behaviour, invariant is broken let value: NonZeroUsize = serde_json::from_str(r#\"{ \"value\": 0 }\"#).unwrap(); let elem = value.subtract_one_and_index(&amp;bytes); println!(\"{elem}\"); } </code></pre><p>I'm surprised that I have never seen anyone address this issue before and never seen anyone consider it in their code. As far as I can tell, there is also no built-in way in serde to fix this (e.g. with an extra  attribute) without manually implementing the traits yourself, which is extremely verbose if you do it on dozens of types.</p><p>I found a couple of crates on crates.io that let you do validation when deserializing, but they all have almost no downloads so nobody is actually using them. There was also <a href=\"https://www.reddit.com/r/rust/comments/1f0v7zu/serdev_serde_with_validation_is_out/\">this reddit post</a> a few months ago about one such crate, but the comments are just people reading the title and screeching \"PARSE DON'T VALIDATE!!!\", apparently without understanding the issue.</p><p>Am I missing something or is nobody actually thinking about this? Is there actually no existing good solution other than something like serdev? Is everyone just writing holes into their code without knowing it?</p>","contentLength":2342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention","url":"https://www.reddit.com/r/MachineLearning/comments/1itutpg/r_native_sparse_attention_hardwarealigned_and/","date":1740044498,"author":"/u/hiskuu","guid":7236,"unread":true,"content":"<p>Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.</p>","contentLength":1392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EKS Auto Mode a.k.a managed Karpenter.","url":"https://www.reddit.com/r/kubernetes/comments/1itumdr/eks_auto_mode_aka_managed_karpenter/","date":1740043614,"author":"/u/lynxerious","guid":7360,"unread":true,"content":"<p>It's relatively new, has anyone tried it before? Someone just told me about it recently.</p><p><a href=\"https://aws.amazon.com/eks/pricing/\">https://aws.amazon.com/eks/pricing/</a> The pricing is a bit strange, it adds up cost to EC2 pricing instead of Karpenter pods. And there are many type of instance I can't search for in that list.</p>","contentLength":280,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft's Quantum Leap: Majorana 1 Chip Ushers in New Era of Computing","url":"https://www.reddit.com/r/artificial/comments/1itu770/microsofts_quantum_leap_majorana_1_chip_ushers_in/","date":1740041807,"author":"/u/Frosty-Feeling2316","guid":7355,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Publishing a Crate is insanely easy","url":"https://www.reddit.com/r/rust/comments/1ittsuf/publishing_a_crate_is_insanely_easy/","date":1740040023,"author":"/u/max-t-devv","guid":7497,"unread":true,"content":"<p>Basically the title, publishing a Rust crate is way easier than I expected. I wrote a CLI tool and assumed the process would be a pain, but it was literally just:</p><p>Having dealt with the BS from other languages, this was a really nice surprise.</p><p>Are there any gotchas or best practices you wish you knew before publishing?</p>","contentLength":317,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Tools for Kubernetes: What Have I Missed?","url":"https://www.reddit.com/r/kubernetes/comments/1ittpj1/ai_tools_for_kubernetes_what_have_i_missed/","date":1740039627,"author":"/u/Electronic_Role_5981","guid":7051,"unread":true,"content":"<p><strong>karpor (kusionstack subproject)</strong></p><p>Intelligence for Kubernetes. World's most promising Kubernetes Visualization Tool for Developer and Platform Engineering teams</p><p><strong>kube-copilot (personal project from Azure)</strong></p><ul><li>Automate Kubernetes cluster operations using ChatGPT (GPT-4 or GPT-3.5).</li><li>Diagnose and analyze potential issues for Kubernetes workloads.</li><li>Generate Kubernetes manifests based on provided prompt instructions.</li><li>Utilize native  and  commands for Kubernetes cluster access and security vulnerability scanning.</li><li>Access the web and perform Google searches without leaving the terminal.</li></ul><p><strong>some cost related `observibility and analysis`</strong></p><p>I did not check if all below projects focus on k8s.</p><p>Are there any ai-for-k8s projects that I miss?</p>","contentLength":713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Perform Cleanup Tasks When a Pod Crashes (Including OOM Errors)?","url":"https://www.reddit.com/r/kubernetes/comments/1itt3ja/how_to_perform_cleanup_tasks_when_a_pod_crashes/","date":1740037029,"author":"/u/SamaDinesh","guid":7099,"unread":true,"content":"<p>I have a requirement where I need to delete a specific file in a shared volume whenever a pod goes down.</p><p>I initially tried using the  lifecycle hook, and it works fine when the pod is deleted normally (e.g., via ). However, the problem is that  does not trigger when the pod crashes unexpectedly, such as due to an OOM error or a node failure. </p><p>I am looking for a reliable way to ensure that the file is deleted even when the pod crashes unexpectedly. Has anyone faced a similar issue or found a workaround?</p><pre><code>lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"rm -f /data/your-file.txt\"] </code></pre>","contentLength":587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling gRPC With Kubernetes Using Go","url":"https://nyadgar.com/posts/scaling-grpc-with-kubernetes-using-go/","date":1740036838,"author":"/u/No-Bug-242","guid":7096,"unread":true,"content":"<a href=\"mailto:noam.g4@gmail.com\">By Noam Yadgar</a><p><a href=\"https://grpc.io/\" target=\"_blank\" rel=\"noopener\"></a>\n is a strong player in microservices-based systems.\nLeveraging <a href=\"https://protobuf.dev/\" target=\"_blank\" rel=\"noopener\">Protocol Buffers</a>\n for well-defined API contracts,\nfast serialization (about  faster than ), smaller payloads, and the use\nof streams (thanks to ). It‚Äôs easy to see why this technology for\nreal-time microservice communication is a good choice.</p><p>Unlike a typical REST API that‚Äôs built on top of ,  is built on top of\n. The most noticeable feature of  is the ability to perform .\nThis feature allows servers to asynchronously  data to the client before the client\nasks for it.  leverages  to support , a key feature that separates \nfrom any other -based API. Because of that, \nrequires a long-lasting  connection between the client and the server.</p><p>One thing that makes s exceptionally good at scaling is the notion of being .\nEvery single request is essentially a new  session that can be routed to any available replica\nof the server. This works perfectly with Kubernetes ‚Äôs load-balancing.</p><p>Things are a bit different when it comes to  because each client is keeping the \nconnection for its entire lifespan (if not configured otherwise), the load-balancer will try to\nsymmetrically spread the load across the clients and servers.</p><pre>flowchart LR\n    c1(client-1):::pod ==&gt; s[service/server]:::srv\n    c2(client-2):::pod ==&gt; s\n    c3(client-3):::pod ==&gt; s\n    c4(client-4):::pod ==&gt; s\n    s ==&gt; p1(server-1):::pod\n    s ==&gt; p2(server-2):::pod\n    s ==&gt; p3(server-3):::pod\nlinkStyle 0,3,4 stroke: orange;\nlinkStyle 1,5 stroke: blue;\nlinkStyle 6,2 stroke: red;\nclassDef srv fill: #a3e4d7, stroke:  #148f77 \nclassDef pod fill: #85c1e9, stroke: #2874a6\n</pre><p><small><i>Figure 1: The first three clients are symmetrically routed to the server‚Äôs three pods.\nThe 4th client starts the next cycle.</i></small></p><p>In Figure 1, we have more clients than servers, so even if we may not fully optimize\nthe resource utilization of the servers, at least all of them are kept busy. But what happens\nif we have fewer clients than servers? The answer is that some pods will stand idle\nwithout doing work but waste resources.</p><pre>flowchart LR\n    c1(client-1):::pod ==&gt; s[service/server]:::srv\n    s ==&gt; p1(server-1):::pod\n    s --&gt; p2(server-2):::pod\n    s --&gt; p3(server-3):::pod\nlinkStyle 0,1 stroke: orange;\nclassDef srv fill: #a3e4d7, stroke:  #148f77 \nclassDef pod fill: #85c1e9, stroke: #2874a6\n</pre><p><small><i>Figure 2: One client - Three servers.\nThe client is making requests only to one server.</i></small></p><p>Autoscaling the number of server replicas might be the solution.\nThe truth is - It‚Äôs not. Imagine a scenario where you‚Äôve set an autoscaling rule (with\na tool like <a href=\"https://keda.sh/\" target=\"_blank\" rel=\"noopener\">Keda</a>\n) that increases the number of replicas whenever a pod\nreaches 90% of its memory consumption.</p><p>Since there‚Äôs no link between the number of clients and the number of servers,\nwe can face a scenario in which one client is causing the autoscaling rule to be triggered,\nincreasing the number of servers by one; however, it doesn‚Äôt use the new replica.</p><p>It wouldn‚Äôt make sense to scale the number of servers based on the number of clients either.\nScaling based on resource utilization is a good rule, but we want to ensure that\nwhen a pod is too busy, it will share the load with new replicas that the autoscaling tool is adding.</p><p>To illustrate the problem, I wrote simple  apps in Go (a client and a server)\nbased on this :</p><div><pre tabindex=\"0\"><code data-lang=\"proto\"></code></pre></div><p>The client sends 3000 messages via the  method, and the server responds\nwith a pre-generated  to reflect its unique identity. The returned value is then\nprinted to  by the client. Here‚Äôs the client‚Äôs code:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Using <a href=\"https://minikube.sigs.k8s.io/docs/\" target=\"_blank\" rel=\"noopener\">minikube</a>\n as my local Kubernetes cluster.\nI‚Äôve deployed three servers, pointed from a service called  and ran one client\nas a job:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>As you can see, although we have three servers, the client sent all of its requests only to one of\nthese servers. Maybe because our single client is making synchronous\nrequests to the service. Let‚Äôs try to communicate concurrently:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>The same results. You‚Äôll still face the same results even if you try to create the connection on every iteration.\nAs mentioned above, the load-balancer is symmetrically spreading connections across pods,\nso a single client will always be connected to the same server.</p><p>In this experiment, we‚Äôre trying to make our single client spread its messages (preferably evenly)\nacross the different replicas of the server.</p><pre>flowchart LR\n    c1(client-1):::pod ==&gt; s[service/server]:::srv\n    s ==&gt; p1(server-1):::pod\n    s ==&gt; p2(server-2):::pod\n    s ==&gt; p3(server-3):::pod\nlinkStyle 0,1,2,3 stroke: orange;\nclassDef srv fill: #a3e4d7, stroke:  #148f77 \nclassDef pod fill: #85c1e9, stroke: #2874a6\n</pre><p><small><i>Figure 3: One client is sending requests to all server‚Äôs replicas.</i></small></p><p>The trick is to  Kubernetes‚Äô built-in load balancer. Kubernetes supports  services.\nThose services don‚Äôt have static IPs and, therefore, don‚Äôt have a single endpoint where they perform\nload-balancing. The purpose of  services is to expose pod IPs deployed under\nthe service via , allowing other load-balancing/service-discovery implementations to replace\nthe built-in one.</p><p>To turn a Kubernetes service into a  service, we should simply add to our service definition:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h3>Client side load-balancing</h3><p>Now that our service is , we can set the client‚Äôs load-balancing policy\nto make request in a  fashion:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>If we run this version of the client against a  service:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Great! All 3000 messages were sent across all pods. The spread is not quite\neven. One pod received about 42% of messages, and the others about 29%. The reason is that our client\nsent all of its messages concurrently. If we send the messages synchronously:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>The client has almost successfully cycled around all three servers with a nearly 100% even spread.</p><p>We‚Äôre not done yet. Our client doesn‚Äôt have a mechanism for knowing about new pods\nduring its runtime. When calling  with the load-balancing settings,\nour client retrieves the list of available IPs and will never try to fetch it again by default.</p><p>To illustrate this point, I throttled the client by adding 50 milliseconds to each iteration,\ngiving me 2.5 minutes to play with the number of replicas during the client‚Äôs runtime.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>I removed one replica during runtime and gradually added up to 5 replicas.\nLet‚Äôs see the results:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Interestingly, we‚Äôve lost one pod forever after the first change (removing one replica).\nIn fact, it (coincidentally) failed to make its first request, and our client‚Äôs load balancer discarded it for good.\nNotice that our client wasn‚Äôt aware that I‚Äôd added more replicas (up to 5)  during its runtime.</p><p>To solve this, we need a  resolver that allows the client to periodically fetch new IPs.\nThe  module conveniently has a built-in  resolver.\nBy using its  package, we can globally register a  resolver as follows:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>On this attempt, I started with three replicas, then dropped one, added one, and finally dropped one.\nLet‚Äôs look at the results:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Not a single miss. The last one, with the 150 messages, is probably the first pod dropped\nsince I dropped it closer to the start of the client‚Äôs job. Then, I‚Äôve added one, perhaps the\npod with the 512 messages. After I removed one replica again, I was left with two pods, running until\nthe job was finished, explaining the two pods with identical 1169 messages.</p><p>By understanding the benefits of  services in Kubernetes and combining client-side load-balancing\nwith a  resolver, we‚Äôve managed to make a  client in Go that knows how to spread its messages\nin a  manner while being aware of new available servers during runtime.</p>","contentLength":7458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1itt1xh/scaling_grpc_with_kubernetes_using_go/"},{"title":"[R] Geometric Continuous Diffusion for Language Modeling via Statistical Manifold Flow","url":"https://www.reddit.com/r/MachineLearning/comments/1itsx7f/r_geometric_continuous_diffusion_for_language/","date":1740036287,"author":"/u/Successful-Western27","guid":7287,"unread":true,"content":"<p>The key contribution here is modeling language generation as a continuous diffusion process on a statistical manifold rather than using discrete token-based diffusion. This allows for smoother transitions between language states and more efficient generation.</p><p>Main technical points: - Uses Riemannian geometry to create a continuous manifold of probability distributions over tokens - Implements specialized neural architecture that learns to navigate this manifold space - Employs controlled diffusion paths for more precise generation - Achieves significant speedup in sampling (2-3x faster than discrete baseline) - Reports improved perplexity scores across multiple language benchmarks</p><p>Results on standard benchmarks: - WikiText-103: 16.8 perplexity (vs 18.2 baseline) - C4: 14.9 perplexity (vs 15.8 baseline) - Convergence in ~500 steps vs ~1000 for discrete models - Memory usage reduced by approximately 30%</p><p>I think this approach could meaningfully impact language model development by providing a more mathematically elegant way to handle text generation. The continuous nature better matches how language meaning actually flows, potentially leading to more natural outputs. The efficiency gains are particularly interesting for practical applications.</p><p>I think the main challenges ahead are: - Scaling to larger models while maintaining the manifold structure - Handling very long sequences effectively - Bridging theory and implementation for production systems</p><p>TLDR: Novel continuous diffusion approach for language modeling using statistical manifolds. Shows improved perplexity and generation speed vs discrete models. Promising direction for more efficient and natural language generation.</p>","contentLength":1697,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analyzing and editing sections of an ELF executable","url":"https://www.reddit.com/r/golang/comments/1itsiws/analyzing_and_editing_sections_of_an_elf/","date":1740034718,"author":"/u/Astro_Z0mbie","guid":7512,"unread":true,"content":"<p>Hello everyone, I am taking a course in computer security, I am studying malware analysis with ghira. Since I know a bit about golang I wanted to write a program that modifies the .text section of an executable but I can't do it. \"debug/elf\" is extremely useful but does not allow modifications. What do you recommend? My purpose is simply to add a section to the end of the executable.</p>","contentLength":386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CAP Theorem explained with a horse and carriage","url":"https://www.asksiri.us/blog/2025-02-14.md","date":1740029717,"author":"/u/siriusastrebe","guid":7017,"unread":true,"content":"<p>You are king or queen of a vast realm. Your subjects depend on your wise decision making, and your law to maintain order.</p><p>Therefore you've written down all laws so that every citizen knows what is lawfully permitted, and what is a criminal offense.</p><p>It would hardly be just if somebody was punished for a crime they did not know existed. You decide it's important for all people to be able to see these laws. You send your messengers across the kingdom to the many cities in your realm, each with a tablet inscribing all of the laws, to be placed in the public square and announced for those who can't read.</p><p>Unfortunately, some of your messengers return with reports that a major road has been blocked by a flood. The river is uncrossable. There is simply no way to deliver the tablets until the flood waters recede, which coud be weeks.</p><p>This puts you in a conundrum. You could hold off on adopting these new laws on principle that everybody must see the same sets of laws. But it could be some time.</p><p><em>The Yellow River Breaches Its Course by Ma Yuan, 1160-1225</em></p><p>The CAP theorem can be used to explain this situation. It was originally envisioned as a pick any two:</p><ul><li> (Every city has the same laws)</li><li> (There are laws on display, even if they are out of date)</li><li> (Roads being blocked)</li></ul><p>The more modern thinking on the CAP theorem is that partitions are unavoidable. Just like a King or Queen can't stop a flood, network engineers can't always stop a broken router or a server from going dark.</p><p> suggests that all computers in the network have the same data, just like all cities in our kingdom see the same sets of laws. Due to blocked roads, you're forced to hide the laws from everybody until you can be sure that every city has seen the new set of laws.</p><p>If you relaxed that requirement, you allow some cities to show maybe an older version of the laws even when the roads are blocked, you're choosing to prioritize . Prioritizing  means that for some cities, the laws may be out of date, but at least there's some laws on display, potentially important to keep the peace. While the flood is ongoing, cities outside of the reach of the Capital operate on their own set of laws.</p><p>The only way to stay  (to have all laws be the same) and  (the laws are on display) is to have clear roads and no partitions. To make sure everybody is on the same page, you'd want riders returning from each city confirming that the new laws are in fact on display, at which point you can confidently say the laws are universal.</p><blockquote><p>Uses a single disk array that is shared by multiple servers. If the main database server fails, the standby server is able to mount and start the database as though it were recovering from a database crash. This allows rapid failover with no data loss.</p></blockquote><blockquote><p>One significant limitation of this method is that if the shared disk array fails or becomes corrupt, the primary and standby servers are both nonfunctional</p></blockquote><p><strong>File System (Block Device) Replication</strong></p><blockquote><p>All changes to a file system are mirrored to a file system residing on another computer. The only restriction is that the mirroring must be done in a way that ensures the standby server has a consistent copy of the file system</p></blockquote><p><strong>SQL-Based Replication Middleware</strong></p><blockquote><p>A program intercepts every SQL query and sends it to one or all servers. Each server operates independently. Read-write queries must be sent to all servers, so that every server receives any changes. But read-only queries can be sent to just one server, allowing the read workload to be distributed among them.</p></blockquote><blockquote><p>Care must also be taken that all transactions either commit or abort on all servers, perhaps using two-phase commit (PREPARE TRANSACTION and COMMIT PREPARED).</p></blockquote><p>Notice how all of these methods require the replica to be online. That's the price of . Other Postgres replications relax the consistency, and focus more on :</p><blockquote><p>Warm and hot standby servers can be kept current by reading a stream of write-ahead log (WAL) records. If the main server fails, the standby contains  of the main server, and can be quickly made the new primary database server.</p></blockquote><blockquote><p>Logical replication allows a database server to send a stream of data modifications to another server.</p></blockquote><blockquote><p>On the other hand, if there are other writes done either by an application or by other subscribers to the same set of tables, conflicts can arise.</p></blockquote><p><strong>Trigger-Based Primary-Standby Replication</strong></p><blockquote><p>Operating on a per-table basis, the primary server sends data changes (typically) asynchronously to the standby servers.</p></blockquote><blockquote><p>Because it updates the standby server asynchronously (in batches), there is possible data loss during fail over.</p></blockquote><p><strong>Asynchronous Multimaster Replication</strong></p><blockquote><p>Each server works independently, and periodically communicates with the other servers to identify conflicting transactions. The conflicts can be resolved by users or conflict resolution rules.</p></blockquote><p>Notice how all of these  strategies all involve some form of data loss () when a failover happens and if writes are allowed on replicas there are conflicts that need to be resolved. That's the price of .</p><p>Once the roads clear, your messengers deliver the laws to the furthest parts of your kingdom. You can rest assured knowing that nobody will be punished for a law they were unaware of. That is, until you need to make changes to the law. You're getting nervous about any blocked roads as you're starting to realize it's the same problem all over again.</p>","contentLength":5342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1itr88c/cap_theorem_explained_with_a_horse_and_carriage/"},{"title":"Creating a chrome extension with rust + leptos -> wasm","url":"https://iism.org/article/ride-the-lightning-the-art-of-creative-motivation-63","date":1740028887,"author":"/u/old-man-of-the-cpp","guid":7357,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1itr029/creating_a_chrome_extension_with_rust_leptos_wasm/"},{"title":"[P] Sakana AI released CUDA AI Engineer.","url":"https://www.reddit.com/r/MachineLearning/comments/1itqrgl/p_sakana_ai_released_cuda_ai_engineer/","date":1740028025,"author":"/u/Excellent_Delay_3701","guid":7050,"unread":true,"content":"<p>It translates torch into CUDA kernels. </p><p>here's are steps:<strong>Stage 1 and 2 (Conversion and Translation):</strong> The AI CUDA Engineer first translates PyTorch code into functioning CUDA kernels. We already observe initial runtime improvements without explicitly targeting these.</p><p><strong>Stage 3 (Evolutionary Optimization):</strong> Inspired by biological evolution, our framework utilizes evolutionary optimization (‚Äò<a href=\"https://blog.otoro.net/2017/10/29/visual-evolution-strategies/\">survival of the fittest</a>‚Äô) to ensure only the best CUDA kernels are produced. Furthermore, we introduce a novel kernel crossover prompting strategy to combine multiple optimized kernels in a complementary fashion.</p><p><strong>Stage 4 (Innovation Archive):</strong> Just as how cultural evolution shaped our human intelligence with knowhow from our ancestors through millennia of civilization, The AI CUDA Engineer also takes advantage of what it learned from past innovations and discoveries it made (Stage 4), building an Innovation Archive from the ancestry of known high-performing CUDA Kernels, which uses previous stepping stones to achieve further translation and performance gains.</p>","contentLength":1056,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cluster restoration","url":"https://www.reddit.com/r/kubernetes/comments/1itq9c8/cluster_restoration/","date":1740026321,"author":"/u/Upper-Aardvark-6684","guid":6959,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Upper-Aardvark-6684\"> /u/Upper-Aardvark-6684 </a>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to run VM using kubevirt in kind cluster in MacOS (M2)?","url":"https://www.reddit.com/r/kubernetes/comments/1itpzch/how_to_run_vm_using_kubevirt_in_kind_cluster_in/","date":1740025387,"author":"/u/Wooden_Departure1285","guid":6958,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Wooden_Departure1285\"> /u/Wooden_Departure1285 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"1972 UNIX V2 \"Beta\" Resurrected","url":"https://www.tuhs.org/pipermail/tuhs/2025-February/031420.html","date":1740024698,"author":"/u/old-man-of-the-cpp","guid":7098,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1itprxm/1972_unix_v2_beta_resurrected/"},{"title":"how advancements like Dynamic Resource Allocation (DRA) and the Container Device Interface (CDI) are shaping Kubernetes for AI workloads","url":"https://furiosa.ai/blog/the-next-chapter-of-kubernetes-enabling-ml-inference-at-scale","date":1740023767,"author":"/u/woowookim","guid":6206,"unread":true,"content":"<p dir=\"ltr\">CDI is designed to solve two problems. First, all device vendors expose their devices to containers in different ways. Second, device support in container runtimes is fragmented. </p><p dir=\"ltr\">This imposes high costs on both AI chip vendors and the open-source community. Yes, these circumstances clearly illustrate why a standard interface is necessary. </p><p dir=\"ltr\">Now AI chip vendors can support various container runtimes through standardized interfaces by implementing CDI. Docker also supports it as an experimental feature, but it is expected to become a default feature in the near future since containerd now supports it as a default feature. Originally, CDI was part of the DRA. However, it now has a greater influence on the entire container ecosystem.</p><p dir=\"ltr\">In addition to standardized device exposure, CDI delivers several benefits for enterprise deployments using different kinds of AI hardware (such as RNGD):</p><ul><li dir=\"ltr\"><p dir=\"ltr\">Simplified runtime support</p></li><li dir=\"ltr\"><p dir=\"ltr\">Reduced implementation costs</p></li></ul><p dir=\"ltr\">Furiosa has built RNGD to be the best accelerator for real world deployments using large language models, multimodal models and agentic AI systems. To achieve this, we will leverage new Kubernetes functionality like CDI and DRA.  RNGD uses the CDI to define how devices are assigned to containers. </p><p dir=\"ltr\">The CDI specification lays out the format, guidelines, and interfaces needed to properly describe these devices. For example, RNGD‚Äôs vendor-specific interfaces like device nodes and sysfs files can be expressed using the interfaces provided by the CDI. This makes it simpler for container-related systems to understand which system resources need to be present when running workloads on RNGD hardware. </p><p dir=\"ltr\">We‚Äôre currently building a DRA plugin for flexible and efficient resource scheduling. When we release the plugin in 2025, users will be able to request RNGD resources defined through the DRA interface. </p><p dir=\"ltr\">One key benefit of DRA is that it allows users to specify their desired hardware topology. For example, a user might say they need four RNGD devices under a single physical CPU socket, or two RNGD devices under a specific PCIe switch. They can even specify that multiple servers must be located beneath a particular network switch. </p><p>When the scheduler hands off the RNGD scheduling task to the Furiosa DRA plugin, the plugin calculates which server meets the user‚Äôs topology requirements. Among the servers that match these conditions, it then assigns the requested RNGD resources to the user‚Äôs container (reality is binding the user's pod on that server).</p><h2 dir=\"ltr\">Future outlook for Kubernetes and ML workloads</h2><p dir=\"ltr\">The evolution of Kubernetes, driven by advancements like DRA and CDI, is crucial for the future of AI. As the industry moves beyond traditional GPUs and embraces more efficient chip architectures like RNGD, the ability to effectively orchestrate and manage these resources will become even more critical.</p><p dir=\"ltr\">This will accelerate the adoption of specialized hardware, leading to faster, more efficient, and more cost-effective ML inference.</p><p dir=\"ltr\">The ongoing collaboration between the Kubernetes community and AI chip vendors is essential to ensure that Kubernetes continues to meet the evolving needs of the AI landscape. By working together, we can unlock the full potential of AI and drive innovation across industries. We believe that sharing our experience with developing and deploying RNGD will contribute to this important effort. We also hope to foster more open-source contributions that will speed progress toward an open ecosystem that supports a wide range of AI-specific hardware to serve different needs in the industry.</p>","contentLength":3579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1itphl0/how_advancements_like_dynamic_resource_allocation/"},{"title":"Grok 3 DeepSearch","url":"https://www.reddit.com/r/artificial/comments/1itp49l/grok_3_deepsearch/","date":1740022547,"author":"/u/so_like_huh","guid":6978,"unread":true,"content":"<p>Well, I guess maybe Elon Musk really made it unbiased then right?</p>","contentLength":65,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust #587","url":"https://this-week-in-rust.org/blog/2025/02/19/this-week-in-rust-587/","date":1740017823,"author":"/u/seino_chan","guid":7470,"unread":true,"content":"<p>This week's crate is <a href=\"https://crates.io/crates/httpmock\">httpmock</a>, which is quite unsurprisingly a HTTP mocking library for Rust.</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.  The following\nRFCs would benefit from user testing before moving forward:</p><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><ul><li><em>No calls for testing were issued this week.</em></li></ul><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p>This week's results were dominated by the update to LLVM 20 (<a href=\"https://github.com/rust-lang/rust/pull/135763\">#135763</a>),\nwhich brought a large number of performance improvements, as usually. There were also two other\nsignificant improvements, caused by improving the representation of  values (<a href=\"https://github.com/rust-lang/rust/pull/136593\">#136593</a>) and doing less work when formatting in  (<a href=\"https://github.com/rust-lang/rust/pull/136828\">#136828</a>).</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr><td align=\"center\">Improvements ‚úÖ  (secondary)</td></tr><tr></tr></tbody></table><p>3 Regressions, 2 Improvements, 4 Mixed; 4 of them in rollups\n50 artifact comparisons made in total</p><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><ul><li><em>No RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Cargo Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Team Proposals entered Final Comment Period this week.</em></li></ul><ul><li><em>No Language Reference RFCs entered Final Comment Period this week.</em></li></ul><ul><li><em>No Unsafe Code Guideline Tracking Issues or PRs entered Final Comment Period this week.</em></li></ul><p>Rusty Events between 2025-02-19 - 2025-03-19 ü¶Ä</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>I have found that many automated code review tools, including LLMs, catch 10 out of 3 bugs.</p></blockquote><p>Despite a lamentable lack of suggestions, llogiq is properly pleased with his choice.</p>","contentLength":2447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1itnl16/this_week_in_rust_587/"},{"title":"iter.Seq, iter.Seq2 and iter.Pull in practice","url":"https://www.reddit.com/r/golang/comments/1itly57/iterseq_iterseq2_and_iterpull_in_practice/","date":1740013085,"author":"/u/fuzzylollipop","guid":7381,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/fuzzylollipop\"> /u/fuzzylollipop </a>","contentLength":36,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go team is sneaking Monadish packages into the standard library","url":"https://www.reddit.com/r/golang/comments/1itlwvi/go_team_is_sneaking_monadish_packages_into_the/","date":1740012981,"author":"/u/fuzzylollipop","guid":6954,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/fuzzylollipop\"> /u/fuzzylollipop </a>","contentLength":36,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] What is the future of retrieval augmented generation?","url":"https://www.reddit.com/r/MachineLearning/comments/1itl38x/d_what_is_the_future_of_retrieval_augmented/","date":1740010668,"author":"/u/jsonathan","guid":6957,"unread":true,"content":"<p>RAG is suspiciously inelegant. Something about using traditional IR techniques to fetch context for a model feels.. early-stage. It reminds me of how Netflix had to mail DVDs before the internet was good enough for streaming.</p><p>I just can‚Äôt imagine LLMs working with databases this way in the future. Why not do retrieval  inference, instead of before? E.g. if the database was embedded directly in the KV cache, then retrieval could be  via gradient descent just like everything else. This at least seems more elegant to me than using (low-precision) embedding search to gather and stuff chunks of context into a prompt.</p><p>Regardless of what the future looks like, my sense is that RAG will become obsolete in a few years. What do y'all think?</p>","contentLength":740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simulating the evolution of tiny neural networks.","url":"https://github.com/kostareg/evolution-rs","date":1740009176,"author":"/u/Most-Ice-566","guid":7172,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1itkjk3/simulating_the_evolution_of_tiny_neural_networks/"},{"title":"Build your own SQLite in Rust, Part 5: Evaluating queries","url":"https://blog.sylver.dev/build-your-own-sqlite-part-5-evaluating-queries?showSharer=true","date":1740004041,"author":"/u/geoffreycopin","guid":7142,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1itil6k/build_your_own_sqlite_in_rust_part_5_evaluating/"},{"title":"Typst 0.13 is out now","url":"https://typst.app/blog/2025/typst-0.13/","date":1740002784,"author":"/u/Frexxia","guid":6111,"unread":true,"content":"<div><strong><p>With Typst 0.13, we wanted to improve the day-to-day experience of using Typst. We fixed some of the most long-standing bugs and made Typst even more flexible to use. And on top, we're shipping a first, experimental version of HTML export. </p></strong></div><p>It's been almost two years since Typst's open-source launch and the project has matured quite a bit since then. Typst 0.12's development cycle saw many large-scale changes to Typst's foundations. With Typst 0.13, we moved the focus to the day-to-day experience of using Typst. We made quality-of-life improvements all across and fixed some of the biggest paper cuts. But, of course, we also shipped some exciting new features!</p><p>In this blog post, I'll walk you through the highlights of the release. If you prefer a more visual take on the topic, also check out the <a href=\"https://youtu.be/3OrrMzOCXfY\">release video</a>.</p><p>For a comprehensive overview of all changes in the release, visit the <a href=\"https://typst.app/docs/changelog/0.13.0/\">changelog</a>. If you're looking to upgrade your document to Typst 0.13, you can also skip ahead to the <a href=\"https://typst.app/blog/2025/typst-0.13/#migrating\">Migration section</a>.</p><h2>Paragraphs and first-line indent</h2><p>The work on  is what I'm most proud of in this release, but at the same time it's among the things that are least visible for users. What do I even mean with \"semantic paragraphs?\"</p><p>Let me explain. Up until now, Typst considered  piece of text you wrote as a paragraph ‚Äî be it a single word in a page header, a figure caption, or a page number. Just like paragraphs, these things can have spacing, break across lines, etc. Layout-wise they are not all that different from paragraphs.</p><p>However, there  semantical differences. Only proper paragraphs should be counted when paragraphs are numbered (such as in legal texts). Only proper paragraphs should be announced by a screen reader as such. And even layout-wise there are differences; for instance, that only proper paragraphs should have first-line indent. While the layout routines for \"just text\" and a paragraph may be very similar, the second order effects of something being a proper paragraph are far-reaching.</p><p>In version 0.13, Typst finally gains a better understanding of this distinction. Whether something is a paragraph or just text is decided based on a few simple rules about which you can read in the <a href=\"https://typst.app/docs/reference/model/par/#what-becomes-a-paragraph\">updated paragraph documentation</a>.</p><p>The most visible immediate effect of this work is that <a href=\"https://typst.app/docs/reference/model/par/#parameters-first-line-indent\"></a> can now be applied to all paragraphs instead of just consecutive ones, closing the most upvoted Typst bug. Semantic paragraphs are also crucial for the in-development HTML export and for planned future work on PDF accessibility.</p><p>If you've created a table of contents with Typst's <a href=\"https://typst.app/docs/reference/model/outline/\" title=\"outline\">outline</a> functionality before, you might remember that it always looked a bit bland. The default style had no indentation and rather tightly dotted leaders (leaders are the filler dots between a title and its page number).</p><p>In Typst 0.13, the outline gets a full facelift while also becoming easier to customize. The new automatic indentation nicely aligns all titles and numberings across the whole outline, long titles have better-looking wrapping behavior, and we fixed a number of bugs.</p><p>Since Typst 0.2, you could draw <a href=\"https://en.wikipedia.org/wiki/B%C3%A9zier_curve\">B√©zier paths</a> with the  function. However, the input format of this function was rather arcane. Rather than specifying pen movements as in an SVG, you had to specify directly points with their two control points. Moreover, the path function had a fatal flaw: You could not close a path and then keep on drawing. This is necessary to draw a shape with cutouts, as depicated below.</p><p>The new <a href=\"https://typst.app/docs/reference/visualize/curve/\" title=\"`curve`\"></a> function fixes these flaws. It provides an easier-to-understand and more expressive interface. We also used this opportunity to change the name from  to  as we plan to repurpose the name  for a file path type in an upcoming release.</p><p>Various functions in Typst load files, be it <a href=\"https://typst.app/docs/reference/visualize/image/\">images</a>, <a href=\"https://typst.app/docs/reference/data-loading/\">data loading functions</a>, or <a href=\"https://typst.app/docs/reference/foundations/plugin/\">plugins</a>. Sometimes though, a little extra flexibility is needed, for example, to preprocess, generate, or inline data into a Typst document.</p><p>For this reason, there are also  variants on various of the functions, e.g.  or . However, that approach didn't work so well when a path is expected in a set rule, as in <code>theme</code>. It also introduced duplication: All the properties of an image are also spelled out again in .</p><p>Typst 0.13 revamps file handling to improve this unsatisfactory situation. All places where a path is expected now also support raw <a href=\"https://typst.app/docs/reference/foundations/bytes/\" title=\"bytes\">bytes</a> instead. Typst will always interpret a string as a path and raw bytes as data. When trying to decode an image from a string, thus make sure to first convert it to bytes. Converting to bytes is cheap as Typst will internally reuse the memory from the string. It will even remember that the bytes came from a string to make conversions back to a string cheap as well!</p><p>The existing  functions are now deprecated as they are not needed anymore. The  variants of data loading functions remain unchanged.</p><p>With the new byte-taking  function (and previously ), you can generate images at runtime. However, the image function expects images in an encoded image exchange format like PNG or JPEG. Producing valid bytes for such a format in pure Typst code is prohibitively complicated. Meanwhile, plugins are unnecessarily bloated and slowed down if they have to include an image encoder.</p><p>To streamline image generation workflows, Typst 0.13 thus brings support for loading images from uncompressed raw pixel data. To that end, the <a href=\"https://typst.app/docs/reference/visualize/image/#parameters-format\"></a> parameter of the image function supports a new form, where the channel encoding and pixel width/height of the image can be specified. This feature is crucial for better scientific visualizations ‚Äî think things like heatmaps.</p><p>In version 0.8, Typst gained support for WebAssembly plugins ‚Äî one of the features that would very likely still be a little blue \"feature request\" label if not for our fabulous open source community. Since then, plugins have become the backbone of various community packages. They're great because they bring the power and package ecosystem of all the languages that compile to WebAssembly right into Typst.</p><p>They are also faster to execute than Typst code. Still, with heavy usage the time spent executing plugin code can make up a significant chunk of compile time. A simple way to improve this would've been to switch to a faster WebAssembly runtime (specifically, from  to ). However, taking on a dependency on a WebAssembly runtime with just-in-time compilation wasn't a spot we wanted to put Typst into. It would have reduced portability and security and increased the amount of third-party code Typst depends on by a lot.</p><p>There was another way to speed up plugins: Since 0.12, Typst's layout engine is multi-threaded. Plugins didn't profit from this though as they couldn't be easily replicated across threads. This is a limitation we're lifting with 0.13. Typst will now automatically run plugins in multiple threads without any changes from plugin authors. This is possible because we require (and also already required in the past) plugin functions to be  This means that we can execute plugin functions out of order without a visible effect. For cases where purity is too limiting, Typst 0.13 introduces the new <a href=\"https://typst.app/docs/reference/foundations/plugin/#definitions-transition\"> API</a>, which lets plugin authors deal with stateful operations in a sound way.</p><p>The work on speeding up plugins was prioritized through a Typst open-source support contract. If you're using Typst in production at your company and are hitting any road blocks, <a href=\"https://typst.app/pricing/?oss-support#oss\">please reach out to us!</a></p><h2>Single-letter strings in math</h2><p>Since Typst's initial release,  would generate the letters \"h\" and \"i\" in upright style while  would result in an italic \"h\". It's one of the <a href=\"https://github.com/typst/typst/issues/274\">longest-standing bugs</a>, which is curious because it  so easy to fix. Unfortunately, it was not. To see why, we need to take a look behind the scenes and understand how Typst views your equations.</p><p>In Typst 0.12 and lower, the  in  is a <a href=\"https://typst.app/docs/reference/text/text/\" title=\"text\">text</a> element like any other text in your document. A string like  is converted to content by becoming such a text element, too. While different syntactically,  and  thus used to yield identical content. Since  becomes italic by default,  did, too.</p><p>Changing that by itself wouldn't have been too hard, but there is a third guest to the party: Symbols. The  in  is a <a href=\"https://typst.app/docs/reference/foundations/symbol/\">symbol value</a>. Like strings, symbols were up until now converted to content by becoming text elements. This means they work both in math and normal text (as ).</p><p>For a long time, the issue was thus blocked on finding a general solution to the text element ambiguity ‚Äî <a href=\"https://github.com/typst/typst/issues/1125\">perhaps introducing a new  element for math-y text</a>. That story isn't fully written yet, but for 0.13 we really wanted to fix the issue at hand. For this reason, we attempted to find a minimal solution that fixes the issue while leaving our options for further improvements open.</p><p>The solution we came up with: Bare letters and symbols are now converted into an internal  which has auto-italics in math, but is transparently converted to normal text outside of math. Meanwhile, strings still generate text elements. With the <a href=\"https://laurmaedje.github.io/posts/types-and-context/\">planned unification of types and elements</a>, this symbol element and the existing symbol type will naturally merge into one.</p><p>Thanks to <a href=\"https://github.com/wrzian\">@wrzian</a> for working on this!</p><p>When text in different writing scripts is mixed, it's often important to have precise control over which text is typeset with which font. For example, Latin and Chinese text are almost always typeset with different fonts.</p><p>This is quite problematic for CJK (Chinese, Japanese, Korean) Typst users which often have text that mix their native language and English. Typst 0.13 takes a first step to improve this situation. With the new <a href=\"https://typst.app/docs/reference/text/text/#parameters-font\"></a> functionality, users can specify precisely for which character ranges a font should be used. This can, for example, be used to define in which font punctuation (which is present in both Latin and CJK fonts) is rendered.</p><p>The  feature supports specifying a character set, either as a <a href=\"https://typst.app/docs/reference/foundations/regex/\">regular expression</a> or one of the built-in ones. Currently, the only built-in set is , which should be specified for a Latin font that is  a CJK font in the fallback list. In the example below, we can put  first in the fallback chain while still having quotes render with </p><p>With Typst 0.13's new  function, you can attach arbitrary text or binary files to your PDF. These embedded files can then be browsed in PDF viewers or extracted programmatically by third-party tools.</p><p>When is this useful? One example, electronic invoicing, is ever more important as new EU legislation just came into force. While electronic invoices are typically in XML-based formats, it's often useful to still have a human-readable and printable invoice.</p><p>With PDF file embedding, the XML invoice data can be inserted into the PDF itself, forming a hybrid invoice. Currently, there is still one missing piece in Typst's support for this: The PDF metadata must identify the document as an E-Invoice to other applications. We plan to add support for embedding arbitrary metadata like this in a future Typst release.</p><h2>A first look at HTML export</h2><p>Saved for last is a particularly exciting topic: We've been starting work on <a href=\"https://typst.app/docs/reference/html/\">HTML export</a>! The feature is still very incomplete and only available for experimentation behind a feature flag, but there's already some stuff to see.</p><p>Most of the markup and some of the other built-in functions like  and  already produce the appropriate HTML. Our focus is on producing semantically rich HTML that retains the structure of the input document. The HTML output should be accessible, human-readable, and editable by hand.</p><pre><code>\nA  with some  A list\n with elements\n\nA bit of text\n  captionMy caption</code></pre><p>will produce the following HTML output:</p><pre></pre><p>Typst cannot always produce the perfect HTML automatically. Instead, it gives you full control by letting you generate raw HTML elements:</p><pre><code>\n  attrsstyle\n  A div with  inside!\n</code></pre><pre></pre><p>To make your document portable across PDF and HTML, we're also introducing a <a href=\"https://typst.app/docs/reference/foundations/target/\" title=\"`target`\"></a> function that returns the current export format (either  or ). It is mainly intended for use in show rules, like below:</p><pre></pre><p>The  function is contextual because the export target can vary within one compilation. How? With the <a href=\"https://typst.app/docs/reference/html/frame/\" title=\"`html.frame`\"></a> function, you can lay out part of your HTML document as an inline SVG, using Typst's normal layout engine. Within such a frame, the compilation target is  again, so that show rules produce the appropriate layout elements instead of HTML elements.</p><p>A lot! For instance, currently, Typst will always output a single HTML file. Support for outputting directories with multiple HTML documents and assets, as well as support for outputting fragments that can be integrated into other HTML documents is planned.</p><p>Typst currently also doesn't output any CSS, instead focusing fully on emitting semantic markup. You can of course write your own CSS styles and still benefit from sharing your content between PDF and HTML.</p><p>In the future, we plan to give you the option of automatically emitting CSS, taking more of your existing set rules into account. More generally, we have a lot of plans for HTML export! Visit the <a href=\"https://github.com/typst/typst/issues/5512\">tracking issue</a> to learn more about them.</p><p>In the CLI, you can experiment with HTML export by passing  or setting the  environment variable to . In the web app, HTML export is not yet available. It will become available once we think it's ready for general use.</p><p>You can also use HTML export with . Typst will then automatically spin up a live-reloading HTTP server that serves your document.</p><pre></pre><p>Work on Typst's HTML export is sponsored by <a href=\"https://nlnet.nl/project/Typst-HTML/\">NLNet</a>. We are very grateful for their support! We also want to thank external contributor <a href=\"https://github.com/01mf02\">@01mf02</a> with whom we've thus far collaborated on HTML export through the NLNet grant. Unfortunately, we and him have since parted ways over technical differences. Nonetheless, we plan to increase the time and resources we put into HTML export, and we are very happy to have NLNet's continued support in this endeavor.</p><p>Typst 0.13 ships with a number of deprecations and breaking changes. The <a href=\"https://typst.app/docs/changelog/0.13.0/\">changelog</a> has a full account of all changes, but in this section you'll learn how to deal with the most common kinds of breakage.</p><p>In Typst 0.8, <a href=\"https://typst.app/docs/reference/foundations/type/\">types</a> were promoted to proper values. As a result,  directly returns a type instead of a string since then. To make this change less disruptive, we also introduced a temporary compatibility behavior where types could be used like strings in some contexts (e.g.,  would be true). For implementation reasons, we did not add a warning for this at the time. We're rectifying this now and adding a warning to shake out remaining reliance on this behavior. With Typst 0.14, the compatibility behavior will be fully removed.</p><pre><code> int </code></pre><p>The  function and the  variants of data loading functions are now deprecated. You can instead directly pass bytes to the respective top-level functions instead. Read the <a href=\"https://typst.app/blog/2025/typst-0.13/#files-and-bytes\">section on files and bytes</a> to learn more.</p><pre><code></code></pre><p>The changes to the built-in <a href=\"https://typst.app/docs/reference/model/outline/\" title=\"outline\">outline</a> (table of contents) improve the out-of-the-box style and customizability. Unfortunately, they also break some existing outline customizations.</p><p>First of all, the  argument moved from  to <a href=\"https://typst.app/docs/reference/model/outline/#definitions-entry\" title=\"`outline.entry`\"></a>. If you get the error \"unexpected argument: fill\", adjust your code as shown below:</p><pre><code> outlinefillfill</code></pre><p>Because the  property is now on the entry, it can also be configured for individual outline levels, like this:</p><pre><code> outlineentrylevel outlinefill</code></pre><p>In light of the changes to paragraphs, outline entries now show themselves as <a href=\"https://typst.app/docs/reference/layout/block/\">blocks</a> instead of lines of text. This means spacing is now configured via normal show-set rules for <a href=\"https://typst.app/docs/reference/layout/block/#parameters-spacing\" title=\"`block.spacing`\"></a>.</p><pre><code> outlineentrylevelbelow outline it \n  it\n   weak</code></pre><p>In Typst 0.12 and below, outline entries expose a few fields like  and  that are useful for writing an outline entry show rule. These fields were derived from other fields for your convenience. Typst 0.13 makes this more explicit and idiomatic by turning them into methods. Read the <a href=\"https://typst.app/docs/reference/model/outline/#building-an-entry\">documentation on outline customization</a> for more details on how to use these methods.</p><pre><code> outline it \n  ...\n  it outline it \n  ...\n  itpage\n</code></pre><p>The  function is superseded by the new  function. The  function has an easier-to-understand interface that's closer to how SVG and the HTML canvas work. Read the <a href=\"https://typst.app/docs/reference/visualize/curve/\"> function's documentation</a> to learn how to express existing paths with the new function.</p><p>The  type was renamed to <a href=\"https://typst.app/docs/reference/visualize/tiling/\" title=\"`tiling`\"></a>. To migrate, simply replace  with . No further changes are necessary. The name  remains as a deprecated alias in Typst 0.13, but will be removed in an upcoming release.</p><p>Why the rename? For once, the name  was very generic. The name  is more closely associated with what it expresses. Secondly, we're considering to repurpose the name  for what today are <a href=\"https://typst.app/docs/reference/foundations/selector/\">selectors</a> once elements and types are unified.</p><pre><code>fillfill</code></pre><p>We also removed a number of things that were already deprecated and warned for in Typst 0.12. This includes the</p><p>That's it for Typst 0.13. We hope you're just as excited about the release as we are!</p>","contentLength":16690,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iti2zx/typst_013_is_out_now/"},{"title":"Cursed fire or #define black magic","url":"https://ssloy.github.io/strange/cursed-fire/","date":1740002748,"author":"/u/haqreu","guid":7358,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iti2fw/cursed_fire_or_define_black_magic/"},{"title":"A Tiny London Startup Convergence's AI Agent Proxy 1.0 Just Deepseeked OpenAI‚Ä¶ AGAIN!","url":"https://v.redd.it/y9lddtlo46ke1","date":1740002470,"author":"/u/CreepToeCurrentSea","guid":6196,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ithydj/a_tiny_london_startup_convergences_ai_agent_proxy/"},{"title":"TinyCompiler: a compiler in a week-end","url":"https://ssloy.github.io/tinycompiler/","date":1740001953,"author":"/u/haqreu","guid":7286,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ithqsh/tinycompiler_a_compiler_in_a_weekend/"},{"title":"Concurrency, go routines","url":"https://www.reddit.com/r/golang/comments/1itfw9d/concurrency_go_routines/","date":1739997413,"author":"/u/Historical-Poetry871","guid":7380,"unread":true,"content":"<p>there is some core ideas of multi threading and goroutines individually i cant understand.</p><p>1- the fact that there is no guarantee about the code execution and which thread is going to read this value through a channel</p><p>2- how would this be faster than synchronous execution since we have to write and operate once per x time ?</p><p>3- dead locks and channels being a blocking at reading or writing</p><p>4- mutex and how they does work and how would i be sure that this thread is going to read right now and this would come after</p><p>all i have found is just some scratching of the surface so if there is any recommendations videos, articles, books, etc.. i would be thankful.</p>","contentLength":655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Master Node Migration","url":"https://www.reddit.com/r/kubernetes/comments/1itfclm/master_node_migration/","date":1739996081,"author":"/u/BrockWeekley","guid":6084,"unread":true,"content":"<p>Hello all, I've been running a k3s cluster for my home lab for several months now. My master node hardware has begun failing - it is always maxed out on CPU and is having all kinds of random failures. My question is, would it be easier to simply recreate a new cluster and apply all of my deployments there, or should mirroring the disk of the master to new hardware be fairly painless for the switch over?</p><p>I'd like to add HA with multiple master nodes to prevent this in the future, which is why I'm leaning towards just making a new cluster, as switching from an embedded sqlite DB to a shared database seems like a pain. </p>","contentLength":623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fedora OBS Drama Resolved","url":"https://gitlab.com/fedora/sigs/flatpak/fedora-flatpaks/-/issues/39#note_2354562186","date":1739995878,"author":"/u/that_leaflet","guid":6061,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1itf9nn/fedora_obs_drama_resolved/"},{"title":"I ran tests on Grok 3 vs. DeepSeek R1 vs. ChatGPT o3-mini with same critical prompts. The results will surprise you.","url":"https://www.reddit.com/r/artificial/comments/1itf378/i_ran_tests_on_grok_3_vs_deepseek_r1_vs_chatgpt/","date":1739995444,"author":"/u/Illustrious-King8421","guid":6955,"unread":true,"content":"<p><strong>1/ üåå Quantum entanglement</strong></p><p>\"Explain the concept of quantum entanglement and its implications for information transfer.\"</p><p>üîÑ Particles remain correlated over distance</p><p>‚ö° Cannot transmit information faster than light</p><p>üîê Used in quantum cryptography, teleportation</p><p>üèÜ DeepSeek R1: Best structured answer, explained Bell's theorem, EPR paradox, and practical applications</p><p>ü•à Grok 3: Solid explanation but less depth than DeepSeek R1. Included Einstein's \"spooky action at a distance\"</p><p>ü•â ChatGPT o3-mini: Gave a basic overview but lacked technical depth</p><p><strong>2/ üåø Renewable Energy Research (Past Month)</strong></p><p>\"Summarize the latest renewable energy research published in the past month.\"</p><p>üìä Identify major energy advancements in the last month</p><p>üìë Cite sources with dates</p><p>üîã Cover solar, wind, hydrogen, and policy updates</p><p>üèÜ DeepSeek R1: Most comprehensive. Covered solar, wind, AI in energy forecasting, and battery tech with solid technical insights</p><p>ü•à Grok 3: Focused on hydrogen storage, solar on reservoirs, and policy changes but lacked broader coverage</p><p>ü•â ChatGPT o3-mini: Too vague, provided country-level summaries but lacked citations and specific studies</p><p><strong>3/ üí∞ Universal Basic Income (UBI) Economic Impact</strong></p><p>\"Analyze the economic impacts of Universal Basic Income (UBI) in developed countries.\"</p><p>üìà Cover effects on poverty, employment, inflation, government budgets</p><p>üîç Mention real-world trials (e.g., Finland, Alaska)</p><p>‚öñÔ∏è Balance positive &amp; negative impacts</p><p>üèÜ Grok 3: Best structured answer. Cited Finland's trial, Alaska Permanent Fund, and analyzed taxation effects</p><p>ü•à DeepSeek R1: Detailed but dense. Good breakdown of pros/cons, but slightly over-explained</p><p>ü•â ChatGPT o3-mini: Superficial, no real-world trials or case studies</p><p><strong>4/ üîÆ Physics Puzzle (Marble &amp; Cup Test)</strong></p><p>\"Assume the laws of physics on Earth. A small marble is put into a normal cup and the cup is placed upside down on a table. Someone then takes the cup and puts it inside the microwave. Where is the ball now? Explain your reasoning step by step.\"</p><p>üéØ The marble falls out of the cup when it's lifted</p><p>üìç The marble remains on the table, not in the microwave</p><p>üèÜ DeepSeek R1: Thought the longest but nailed the physics, explaining gravity and friction correctly</p><p>ü•à Grok 3: Solid reasoning but overcomplicated the explanation with excessive detail</p><p>ü•â ChatGPT o3-mini: Incorrect. Claimed the marble stays in the cup despite gravity</p><p><strong>5/ üå°Ô∏è Global Temperature Trends (Last 100 Years)</strong></p><p>\"Analyze global temperature changes over the past century and summarize key trends.\"</p><p>üåç ~1.5¬∞C warming since 1925</p><p>üìä Clear acceleration post-1970</p><p>‚ùÑÔ∏è Cooling period 1940‚Äì1970 due to aerosols</p><p>üèÜ Grok 3: Best structured answer. Cited NASA, IPCC, NOAA, provided real anomaly data, historical context, and a timeline</p><p>ü•à DeepSeek R1: Strong details but lacked citations. Good analysis of regional variations &amp; Arctic amplification</p><p>ü•â ChatGPT o3-mini: Basic overview with no data or citations</p><p><strong>ü•â ChatGPT o3-mini: 0 Wins</strong></p><p>üëë DeepSeek R1 is the overall winner, but Grok 3 dominated in citation-based research.</p><p>Let me know what tests you want me to run next!</p>","contentLength":3126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning how to use a debugger ( YouTube video )","url":"https://youtu.be/LuNeTTFIwpw?si=18Ikh0CskJjEM_-r","date":1739994289,"author":"/u/adelowo","guid":6082,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1itelqr/learning_how_to_use_a_debugger_youtube_video/"},{"title":"Kubemgr: Open-Source Kubernetes Config Merger","url":"https://www.reddit.com/r/kubernetes/comments/1ite641/kubemgr_opensource_kubernetes_config_merger/","date":1739993255,"author":"/u/RAPlDEMENT","guid":6139,"unread":true,"content":"<p>I'm excited to share a personal project I've been working on recently. My classmates and I found it tedious to manually change environment variables or modify Kubernetes configurations by hand. Merging configurations can be straightforward but often feels cumbersome and annoying.</p><p>To address this, I created Kubemgr, a Rust crate that abstracts a command for merging Kubernetes configurations:</p><pre><code>KUBECONFIG=config1:config2... kubectl config view --flatten </code></pre><p>Available on <a href=\"http://crates.io\">crates.io</a>, this CLI makes the process less painful and more intuitive.</p><p>But that's not all! For those who prefer not to install the crate locally, I also developed a user interface using Next.js and WebAssembly (WASM). The goal was to ensure that both the interface and the CLI use the exact same logic while keeping everything client-side for security reasons.</p><p>I understand that this project might not be useful for everyone, especially those who are already experienced with Kubernetes. However, it was primarily a learning exercise for me to explore new technologies and improve my skills. I'm eager to get feedback and hear any ideas for new features or improvements that could make Kubemgr more useful for the community.</p><p>The project is open-source, so feel free to check out the code and provide recommendations or suggestions for improvement on GitHub. Contributions are welcome!</p><p>If you like the project, please consider starring the GitHub repo!</p>","contentLength":1412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WAX ‚Äì JSX-based Server-Side Rendering for Go","url":"https://www.reddit.com/r/golang/comments/1itc26f/wax_jsxbased_serverside_rendering_for_go/","date":1739988285,"author":"/u/NoPeanut6044","guid":6110,"unread":true,"content":"<p>WAX is a Go library for server-side rendering (SSR) of JSX/TSX components, designed to provide a seamless, dynamic view layer without the need to regenerate templates after code changes.</p><p>Key Features: ‚úÖ Server-side rendering of JSX ‚Äì Render JSX/TSX views directly in Go.<p> üîÑ Hot reload for views ‚Äì Automatically refresh changes without restarting the server.</p> ‚úÖ TypeScript model generation ‚Äì Generate TypeScript typings from Go structs for type safety.<p> ‚úÖ Seamless integration ‚Äì Works with net/http, Echo, and other Go web frameworks.</p> üöÄ Single-file deployment ‚Äì Bundle JSX views into a Go binary using embed.FS.</p><p>Library is in active development but usable. I'd love to get your feedback and suggestions for improvement!</p>","contentLength":735,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux really made using a computer fun again","url":"https://www.reddit.com/r/linux/comments/1itc0aw/linux_really_made_using_a_computer_fun_again/","date":1739988168,"author":"/u/IllustriousWonder894","guid":6020,"unread":true,"content":"<p>Seriously, I came mostly for privacy reasons and will now stay because Linux made using a PC fun again. In a way using Linux is like back then when PCs where new and, for the most part, something creative. Where Windows kept evolving and everything was new and exciting. Linux, probably due to its open source nature, still feels exactly like that. Tons of ways to use it, tons of distros, you can tinker with it however you want to. It obviously has flaws when all you know is Windows where most things work out of the box but the gained freedom is just worth SO much more. In a way the same applies to everything FOSS. If there is one thing I grew tired of in todays digital world its how utterly corporate and sanitized everything is. Everything needs to be as foolproof as possible, everything needs to be as inoffensive as possible and because of that you get told what youre allowed to do. Linux and the whole FOSS community is the exact opposite. You actually need to do your research, You need to tinker but in return youre the one who tells your software what its allowed to do and what not. All kinds of DEs, everyone uses Linux different. Its really nostalgic and still has the magic from the past. Depending on your usecase and hardware using Linux is rough at times, frustrating even, but I honestly wouldnt want it any different.</p>","contentLength":1343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simple TUI SQLite Browser","url":"https://www.reddit.com/r/linux/comments/1itbtzn/simple_tui_sqlite_browser/","date":1739987763,"author":"/u/JonkeroTV","guid":7141,"unread":true,"content":"<p>JDbrowser is small and simple application to browse an SQLite database with a Text User Interface. Written in rust.</p><p>Uses vim style key binds, keep the fingers on the home row where they belong.</p><p>Feel free to try it out and let me know what you think!</p><p>Binaries, building, code and installing available <a href=\"https://github.com/Jkeyuk/JDbrowser\">Here</a></p><p>Arch users: AUR package available for simple install</p>","contentLength":353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Imperfect Systems are Good, Actually: Bluesky's Lossy Timelines","url":"https://jazco.dev/2025/02/19/imperfection/","date":1739987590,"author":"/u/swdevtest","guid":6021,"unread":true,"content":"<p>Often when designing systems, we aim for perfection in things like consistency of data, availability, latency, and more.</p><p>The hardest part of system design is that it‚Äôs difficult (if not impossible) to design systems that have perfect consistency, perfect availability, incredibly low latency, and incredibly high throughput, all at the same time.</p><p>Instead, when we approach system design, it‚Äôs best to treat each of these properties as points on different axes that we balance to find the ‚Äúright fit‚Äù for the application we‚Äôre supporting.</p><p>I recently made some major tradeoffs in the design of <a href=\"https://bsky.app/\">Bluesky‚Äôs</a> Following Feed/Timeline to improve the performance of writes at the cost of consistency in a way that doesn‚Äôt negatively affect users but reduced P99s by over 96%.</p><p>When you make a post on Bluesky, your post is indexed by our systems and persisted to a database where we can fetch it to hydrate and serve in API responses.</p><p>Additionally, a reference to your post is ‚Äúfanned out‚Äù to your followers so they can see it in their Timelines.</p><p>This process involves looking up all of your followers, then inserting a new row into each of their Timeline tables in reverse chronological order with a reference to your post.</p><p>When a user loads their Timeline, we fetch a page of post references and then hydrate the posts/actors concurrently to quickly build an API response and let them see the latest content from people they follow.</p><p>The Timelines table is sharded by user. This means each user gets their own Timeline partition, randomly distributed among shards of our horizontally scalable database (ScyllaDB), replicated across multiple shards for high availability.</p><p>Timelines are regularly trimmed when written to, keeping them near a target length and dropping older post references to conserve space.</p><p>Bluesky currently has around <a href=\"https://bsky.jazco.dev/stats\">32 Million Users</a> and our Timelines database is broken into hundreds of shards.</p><p>To support millions of partitions on such a small number of shards, each user‚Äôs Timeline partition is colocated with tens of thousands of other users‚Äô Timelines.</p><p>Under normal circumstances with all users behaving well, this doesn‚Äôt present a problem as the work of an individual Timeline is small enough that a shard can handle the work of tens of thousands of them without being heavily taxed.</p><p>Unfortunately, with a large number of users, some of them will do abnormal things like‚Ä¶ well‚Ä¶ following hundreds of thousands of other users.</p><p>Generally, this can be dealt with via policy and moderation to prevent abusive users from causing outsized load on systems, but these processes take time and can be imperfect.</p><p>When a user follows hundreds of thousands of others, their Timeline becomes hyperactive with writes and trimming occurring at massively elevated rates.</p><p>This load slows down the individual operations to the user‚Äôs Timeline, which is fine for the bad behaving user, but causes problems to the tens of thousands of other users sharing a shard with them.</p><p>We typically call this situation a ‚ÄúHot Shard‚Äù: where some resident of a shard has ‚Äúhot‚Äù data that is being written to or read from at much higher rates than others. Since the data on the shard is only replicated a few times, we can‚Äôt effectively leverage the horizontal scale of our database to process all this additional work.</p><p>Instead, the ‚ÄúHot Shard‚Äù ends up spending so much time doing work for a single partition that operations to the colocated partitions slow down as well.</p><p>Returning to our Fanout process, let‚Äôs consider the case of Fanout for a user followed by 2,000,000 other users.</p><p>Under normal circumstances, writing to a single Timeline takes an average of ~600 microseconds. If we sequentially write to the Timelines of our user‚Äôs followers, we‚Äôll be sitting around for 20 minutes at best to Fanout this post.</p><p>If instead we concurrently Fanout to 1,000 Timelines at once, we can complete this Fanout job in ~1.2 seconds.</p><p>That sounds great, except it oversimplifies an important property of systems: <a href=\"https://web.archive.org/web/20200603133348/https://robertovitillo.com/why-you-should-measure-tail-latencies/\">tail latencies</a>.</p><p>The  latency of a write is ~600 microseconds, but some writes take much less time and some take much more. In fact, the P99 latency of writes to the Timelines cluster can be as high as 15 milliseconds!</p><p>What does this mean for our Fanout? Well, if we concurrently write to 1,000 Timelines at once, statistically we‚Äôll see 10 writes as slow as or slower than 15 milliseconds.</p><p>In the case of timelines, each ‚Äúpage‚Äù of followers is 10,000 users large and each ‚Äúpage‚Äù must be fanned out before we fetch the next page.</p><p>This means that our slowest writes will hold up the fetching and Fanout of the next page. How does this affect our expected Fanout time?</p><p>Each ‚Äúpage‚Äù will have ~100 writes as slow as or slower than the P99 latency. If we get unlucky, they could all stack up on a single routine and end up slowing down a single page of Fanout to 1.5 seconds.</p><p>In the worst case, for our 2,000,000 Follower celebrity, their post Fanout could end up taking as long as 5 minutes!</p><p>That‚Äôs not even considering P99.9 and P99.99 latencies which could end up being &gt;1 second, which could leave us waiting tens of minutes for our Fanout job.</p><p>Now imagine how bad this would be for a user with 20,000,000+ Followers!</p><p>So, how do we fix the problem? By embracing imperfection, of course!</p><p>Imagine a user who follows hundreds of thousands of others. Their Timeline is being written to hundreds of times a second, moving so fast it would be humanly impossible to keep up with the entirety of their Timeline even if it was their full-time job.</p><p>For a given user, there‚Äôs a threshold beyond which it is  for them to be able to keep up with their Timeline. Beyond this point, they likely consume content through various other feeds and do not primarily use their Following Feed.</p><p>Additionally, beyond this point, it is  for us to not necessarily have a perfect chronology of everything posted by the many thousands of users they follow, but provide enough content that the Timeline always has something new.</p><p><em>Note in this case I‚Äôm using the term ‚Äúreasonable‚Äù to loosely convey that as a social media service, there must be a limit to the amount of work we are expected to do for a single user.</em></p><p>What if we introduce a mechanism to reduce the correctness of a Timeline such that there is a limit to the amount of work a single Timeline can place on a DB shard.</p><p>We can assert a  for the number of follows a user should have to have a healthy and active Timeline, then increase the ‚Äúlossiness‚Äù of their Timeline the further past that limit they go.</p><p>A  can be defined as <code>min(reasonable_limit/num_follows, 1)</code> and can be used to probabilistically drop writes to a Timeline to prevent hot shards.</p><p>Just before writing a page in Fanout, we can generate a random float between  and , then compare it to the  of each user in the page. If the user‚Äôs  is smaller than the generated float, we filter the user out of the page and don‚Äôt write to their Timeline.</p><p>Now, users all have the same number of ‚Äúfollows worth‚Äù of Fanout. For example with a  of 2,000, a user who follows 4,000 others will have a  of  meaning half the writes to their Timeline will get dropped. For a user following 8,000 others, their loss factor of  will drop 75% of writes to their Timeline.</p><p>Thus, each user has a effective ceiling on the amount of Fanout work done for their Timeline.</p><p>By specifying the limits of  user behavior and embracing imperfection for users who go beyond it, we can continue to provide service that meets the expectations of users without sacrificing scalability of the system.</p><p>We write to Timelines at a rate of more than one million times a second during the busy parts of the day. Looking up the number of follows of a given user before fanning out to them would require more than one million additional reads per second to our primary database cluster. This additional load would not be well received by our database and the additional cost wouldn‚Äôt be worth the payoff for faster Timeline Fanout.</p><p>Instead, we implemented an approach that caches high-follow accounts in a Redis sorted set, then each instance of our Fanout service loads an updated version of the set into memory every 30 seconds.</p><p>This allows us to perform lookups of follow counts for high-follow accounts millions of times per second per Fanount service instance.</p><p>By caching values which don‚Äôt need to be perfect to function correctly in this case, we can once again embrace imperfection in the system to improve performance and scalability without compromising the function of the service.</p><p>We implemented Lossy Timelines a few weeks ago on our production systems and saw a dramatic reduction in hot shards on the Timelines database clusters.</p><p>In fact, there now appear to be no hot shards in the cluster at all, and the P99 of a page of Fanout work has been reduced by over 90%.</p><p>Additionally, with the reduction in write P99s, the P99 duration for a full post Fanout has been reduced by over 96%. Jobs that used to take 5-10 minutes for large accounts now take &lt;10 seconds.</p><p>Knowing where it‚Äôs okay to be imperfect lets you trade consistency for other desirable aspects of your systems and scale ever higher.</p><p>There are plenty of other places for improvement in our Timelines architecture, but this step was a big one towards improving throughput and scalability of Bluesky‚Äôs Timelines.</p><p>If you‚Äôre interested in these sorts of problems and would like to help us build the core data services that power Bluesky, check out <a href=\"https://jobs.gem.com/bluesky/am9icG9zdDojUfV5u9SSp_tydYRdQe9D\">this job listing</a>.</p><p>If you‚Äôre interested in other open positions at Bluesky, you can find them <a href=\"https://bsky.social/about/join\">here</a>.</p>","contentLength":9589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1itbrd8/when_imperfect_systems_are_good_actually_blueskys/"},{"title":"Rust Integration in Linux Kernel Faces Challenges but Shows Progress","url":"https://thenewstack.io/rust-integration-in-linux-kernel-faces-challenges-but-shows-progress/","date":1739986010,"author":"/u/CrankyBear","guid":6023,"unread":true,"content":"<p>When Miguel Ojeda, overseer of the <a href=\"https://rust-for-linux.com/\" rel=\"external \" onclick=\"this.target='_blank';\">Rust for Linux</a> project, asked Hellwig to suggest an alternative, he replied that Rust developers should ‚Äú<a href=\"https://lwn.net/ml/all/20250108151858.GB24499@lst.de/\" rel=\"external \" onclick=\"this.target='_blank';\">Keep the wrappers in your code</a> instead of making life painful for others.‚Äù Getting to the heart of the matter, in another Linux Kernel Mailing List (LKML) note, Hellwig wrote, ‚Äú<a href=\"https://lwn.net/ml/all/20250110083955.GA5395@lst.de/\" rel=\"external \" onclick=\"this.target='_blank';\">Maintaining multi-language projects is a pain</a> I have no interest in dealing with. If you want to use something that‚Äôs not C, be that assembly or Rust, you write to C interfaces and deal with the impedance mismatch yourself.‚Äù</p><h2>Is Rust Harder for Maintainers?</h2><p>That doesn‚Äôt work for Hellwig, either. He replied, ‚ÄúI also do not want another maintainer.&nbsp; If you want to make Linux impossible to maintain due to a cross-language codebase, do that in your driver so that you have to do it instead of spreading this cancer to core subsystems.‚Äù</p><p><a href=\"https://www.theregister.com/2001/06/02/ballmer_linux_is_a_cancer/\" rel=\"external \" onclick=\"this.target='_blank';\">‚ÄúCancer</a>,‚Äù thanks to Steve Ballmer‚Äôs criticism of Linux, has always been a red-letter term in <a href=\"https://thenewstack.io/learning-linux-start-here/\">Linux circles</a>. A lot of heated words followed. I think senior Linux kernel developer Ted T‚Äôso, though, hit the nail on the head when he said that, Ultimately, <a href=\"https://lore.kernel.org/lkml/20250208204416.GL1130956@mit.edu/\" rel=\"external \" onclick=\"this.target='_blank';\">Cristoph‚Äôs concern is that Rust is going to make life harder for maintainers </a>because of particular build breaks getting in the way of the very limited bandwidth that Maintainers have. In short, it‚Äôs not so much that kernel maintainers think Rust is awful; they don‚Äôt have enough hours in the day to maintain their projects.</p><h2>‚ÄòRust Device Driver Mess‚Äô</h2><p>Be that as it may, one maintainer, <a href=\"https://asahilinux.org/\" rel=\"external \" onclick=\"this.target='_blank';\">Asahi Linux</a> lead developer Hector Martin, called on Torvalds to ‚Äúpipe up with an authoritative answer‚Äù to resolve the Rust device driver mess. ‚ÄúIf he doesn‚Äôt, Miguel and the other Rust folks should just merge this series once it is reviewed and ready, ignoring Christoph‚Äôs overt attempt at sabotaging the project.‚Äù When that didn‚Äôt work, Martin took to&nbsp; ‚Äúshaming on social media‚Äù to carry his point. Torvalds was not amused.</p><p>Torvalds replied, ‚ÄúHow about you accept the fact that maybe the problem is you? You think you know better. But the current process works. It has problems, but problems are a fact of life.&nbsp; There is no perfect.‚Äù That said, Torvalds continued, ‚ÄúIf we have issues in the kernel development model, then <a href=\"https://lkml.org/lkml/2025/2/6/1292\" rel=\"external \" onclick=\"this.target='_blank';\">social media sure as hell isn‚Äôt the solution</a>. The same way, it sure as hell wasn‚Äôt the solution to politics.‚Äù</p><h2>Lessons From Real-Time Linux</h2><p>So, what can be done moving forward with Rust and Linux? Senior real-time Linux developer Steven Rostedt suggested the Rust developers might follow in the footsteps of <a href=\"https://www.zdnet.com/article/20-years-later-real-time-linux-makes-it-to-the-kernel-really/\" rel=\"external \" onclick=\"this.target='_blank';\">real-time Linux, which took twenty years to join the mainline Linux kernel</a>; that was to ‚Äú<a href=\"https://lkml.org/lkml/2025/2/7/1955\" rel=\"external \" onclick=\"this.target='_blank';\">keep [Rust as]&nbsp; an out of tree patch</a>. ‚Ä¶ Yes, being out of tree is very difficult because you have to constantly rebase ‚Ä¶ But it also gives you full flexibility to try new approaches. Just because something is out of tree doesn‚Äôt mean it can‚Äôt be published and used. Red Hat and SUSE, as well as many others, shipped PREEMPT_RT while it was out of tree.‚Äù</p><p>Since then, Ojeda published a ‚Äú<a href=\"https://rust-for-linux.com/rust-kernel-policy\" rel=\"external \" onclick=\"this.target='_blank';\">Rust kernel policy</a>‚Äù document to clarify the status of Rust integration efforts. This move came in response to growing confusion and debate within the Linux community regarding the role of Rust in kernel development.</p><p>This document addresses several crucial points, including kernel maintainers‚Äô expected level of support. Ojeda noted that it continues to be up to each maintainer to decide how to deal with Rust. ‚ÄúSome subsystems may decide they do not want to have Rust code for the time being, typically for bandwidth reasons. This is fine and expected.‚Äù So, while some developers want Rust to move much more quickly into the kernel, Hellwig‚Äôs position is perfectly defendable.</p><p>Indeed, Ojeda continued, ‚ÄúFor Rust, a subsystem may allow to temporarily break Rust code. The intention is to facilitate the friendly adoption of Rust in a subsystem without introducing a burden to existing maintainers who may be working on urgent fixes for the C side. The breakage should nevertheless be fixed as soon as possible, ideally before the breakage reaches Linus.‚Äù</p><h2>Rust Integration With Linux</h2><p>When it comes to Rust‚Äôs integration with Linux, unlike the tech mantra of ‚Äúmove fast and break things,‚Äù the rule is to ‚Äúmove slow and stabilize things.‚Äù After all, despite all the harsh words, Rust‚Äôs integration into Linux continued to move forward.</p><p>Next merge window, hopefully, we will have PCI and platform drivers working, which will fully enable almost all driver subsystems to start accepting (or at least getting) rust drivers.&nbsp; This is the end result of a lot of work from a lot of people, congrats to all of them for getting this far, you‚Äôve proved many of us wrong in the best way possible, working code.‚Äù</p><p>In short, for all the war of words, Rust‚Äôs movement into Linux continues to be slow, steady, and productive, as we can see in 6.13. Rust will find its place in Linux.</p><div><svg width=\"68px\" height=\"31px\" viewBox=\"0 0 68 31\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg></div>","contentLength":4960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1itb34w/rust_integration_in_linux_kernel_faces_challenges/"},{"title":"Starpath is 55 bytes","url":"https://hellmood.111mb.de//starpath_is_55_bytes.html","date":1739985195,"author":"/u/Hell__Mood","guid":5963,"unread":true,"content":"<p><a href=\"https://lovebyte.party/\">Lovebyte 2025</a> was a sizecoding competition held online from 15. to 16. february this year. They were hosting many size restricted competitions - going from 32 bytes to 1024 bytes, where coders from all around the world could anonymously submit entries to compete against each other. The <a href=\"https://demozoo.org/parties/5201/#competition_19705\">64 byte competition (results)</a> was won by \"<a href=\"https://www.youtube.com/watch?v=4g9WXzZrSx0\">Starpath (Youtube capture)</a>\", a tiny animated raycaster showing curved 3D geometry with fake lighting, a copper night sky and a few tiny stars, while playing ambient wind sound over the speakers. In the <a href=\"https://www.pouet.net/prod.php?which=103622\">download archive</a> you will find different versions together with commented source code, the smallest one -only  bytes- is shown and explained in short here. If it is your first time coming across sizecoding, i suggest you follow the links below in the references, since sizecoders sometimes use very unusual techniques and quirks to achieve their goals. The links in the references also can help you setup an environment to try out sizecoding yourself.\n\n\t\t</p><div><img src=\"https://hellmood.111mb.de//starpath_big.png\" alt=\"Main article image\"></div><div><img src=\"https://hellmood.111mb.de//starpath55%20hexdump.png\" alt=\"hexdump\"></div><h2>Code with short explanations</h2><pre><code>\n; Starpath by HellMood/DSR\n; 55/64 byte intro for MSDOS\n; 1st place at \"Lovebyte 2025\"\n;\n; heaven is one step around the corner\n; ... you just have to keep moving\n;\n; greetings to all sizecoders !\n\n;          STAR SMOL\n\n; this is the SMOL ** 55 bytes ** version\n; it lacks sound or ESC support\n; is not synced against a timer\n; might be slow on real hardware\n; star pattern is just okay\n; and has no dithering to smoothen\n; but is only ** 55 ** bytes, so you\n; can customize your own starpath =)\n\nmov al,0x13\t\t\t; mode 13h, 320x200 pixels, 256 colors\nB: cwd\t\t\t\t; erase DX for pixel setting (AH = 0x0C/0x00)\nint 0x10\t\t\t; set graphic mode (AH = 0x00), set pixel (AH = 0x0C)\nX: mov bl,0xD\t\t; start ray depth at 14\nL: mov ax,0xCCCC\t; \"Rrrola constant\" to convert screen pointer to coordinates\nmul cx\t\t\t\t; Getting X,Y in DL,DH\nmov al,dh\t\t\t; getting Y into AL\nmul bl\t\t\t\t; multiply Y by current depth (into AH)\nxchg ax,dx\t\t\t; store Y' into DH, get X into AL\nsub al,bl\t\t\t; curve X by the current depth\njc W\t\t\t\t; if left of the curve, jump to \"sky\"\nmul bl\t\t\t\t; multiply X by current depth (into AH)\nmov al,dh\t\t\t; get Y' in AL (now AL,AH = Y',X')\nor al,ah\t\t\t; OR for geometry and texture pattern\nlea dx,[bx+si]\t\t; get (current depth) + (current frame count) in DX (DL)\ninc bx\t\t\t\t; (increment depth by one)\nand al,dl\t\t\t; mask geometry/texture by time shifted depth...\ntest al,16\t\t\t; ... to create \"gaps\"\njz L\t\t\t\t; if ray did not hit, repeat pixel loop\njmp short Q\t\t\t; jump over the sky ^^\nW: mov al,27\t\t; is both the star color and palette offset into sky\nadd dl,cl\t\t\t; pseudorandom multiplication leftover DL added to\njz Q\t\t\t\t; truncated pixel count, 1 in 256 chance to be a star *\nA: shld ax,cx,4\t\t; if not, shift the starcolor and add scaled pixel count\nQ: mov ah,0x0C\t\t; AH = 0xC sets a pixel when int 10h is called\nloop B\t\t\t\t; repeat for 64k pixels\ninc si\t\t\t\t; increment frame counter\njmp short B\t\t\t; rinse and repeat        </code></pre>\n\t\t\tYou can copy the code and paste it into an <a href=\"http://twt86.co/\">online dosbox</a> to check for yourself, or directly click <a href=\"http://twt86.co?c=sBOZzRCzDbjMzPfhiPD245Io2HIR9uOI8AjgjRBDINCoEHTj6wqwGwDKdAQPpMgEtAzizkbryw%3D%3D\">this link</a> which will append the code as base64 and execute it automatically in the online dosbox. It is recommended to do so on a strong machine, since the intro needs about 300'000 cycles to achieve a fluent impression of movement.\n          ","contentLength":3278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1itaqal/starpath_is_55_bytes/"},{"title":"Why don't you use Rust at your company?","url":"https://www.reddit.com/r/rust/comments/1ital1t/why_dont_you_use_rust_at_your_company/","date":1739984859,"author":"/u/szabgab","guid":6083,"unread":true,"content":"<p>There are plenty of readers here who us Rust at their company, but I am sure there are also many who would like to use Rust in a professional setting, but can't. I would like to collect the excuses you get from your boss and the valid concerns and reasons you and your boss might have about Rust.</p><p>I hope that knowing the issues will give us a better chance addressing them. </p>","contentLength":373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Valve releases Team Fortress 2 code","url":"https://github.com/ValveSoftware/source-sdk-2013/commit/0759e2e8e179d5352d81d0d4aaded72c1704b7a9","date":1739981531,"author":"/u/old-man-of-the-cpp","guid":5964,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1it96gr/valve_releases_team_fortress_2_code/"},{"title":"GitHub - codeglyph/go-dotignore: go-dotignore is a Go library for parsing .gitignore-style files and matching paths against ignore patterns. It supports custom ignore files, negation patterns, directory/file matching, and advanced wildcards.","url":"https://github.com/codeglyph/go-dotignore","date":1739980393,"author":"/u/toxic2soul","guid":5961,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1it8pc7/github_codeglyphgodotignore_godotignore_is_a_go/"},{"title":"Tutorial: Create a full beat'em up game in Godot in 10h","url":"https://www.youtube.com/watch?v=hJXRisFiZTE","date":1739979777,"author":"/u/m_ologin","guid":6040,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1it8gck/tutorial_create_a_full_beatem_up_game_in_godot_in/"},{"title":"[R] Diffusion Is The Solution For Efficient And Effective RNNs","url":"https://www.reddit.com/r/MachineLearning/comments/1it790b/r_diffusion_is_the_solution_for_efficient_and/","date":1739976684,"author":"/u/jacobfa","guid":6022,"unread":true,"content":"<div><p>I show that diffusion kernels capture global dependencies and that a simple diffusion kernel with a recurrent structure outperforms transformers in fewer parameters and FLOPs. </p></div>   submitted by   <a href=\"https://www.reddit.com/user/jacobfa\"> /u/jacobfa </a>","contentLength":206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] PapersTok - AI arXiv papers with a TikTok like UX","url":"https://www.reddit.com/r/MachineLearning/comments/1it6x9a/p_paperstok_ai_arxiv_papers_with_a_tiktok_like_ux/","date":1739975790,"author":"/u/pranftw","guid":5965,"unread":true,"content":"<p>In the current fast paced world of AI research, where hundreds of papers are put up on arXiv daily, keeping up with the latest developments presents significant challenges. One of them being the difficulty of navigating around the arXiv web interface, where new tabs have to be constantly opened and closed just to skim through the title and the abstract. What if there was a much simpler and fun way to do just that?</p><p>Inspired by <a href=\"http://wikitok.io\">WikiTok</a>, I built <a href=\"https://papers.infitok.com\">PapersTok</a> to scroll through arXiv submissions related to AI. It has LaTeX support to render math equations. It also provides the ability to bookmark papers you find interesting. I'm planning to add more features in the coming days to enhance the experience of skimming through papers.</p><p>I request the community to highlight the challenges they currently face that can be alleviated through this tool. Your valuable feedback and comments are much appreciated. Feel free to DM or tweet me at <a href=\"http://x.com/xinfitok\">X</a> or here on Reddit.</p>","contentLength":953,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"C.K.A Exam Change - Did Anyone Take the Exam with the New Syllabus Effective 18th?","url":"https://www.reddit.com/r/kubernetes/comments/1it6rzx/cka_exam_change_did_anyone_take_the_exam_with_the/","date":1739975397,"author":"/u/fighting_cockroach","guid":5851,"unread":true,"content":"<p>I'm curious if anyone has taken the Certified Kubernetes Administrator exam with the new syllabus that became effective on February 18th. If so, how does it compare to the old exam? </p><p>Any insights on the changes and how they impacted your experience would be greatly appreciated!</p>","contentLength":277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"So we have a few computers in the house including functional snappy daily drivers that don't meet the windows 11 CPU requirement... Got my non tech savvy partner on the Linux train (Kubuntu).","url":"https://www.reddit.com/r/linux/comments/1it60ce/so_we_have_a_few_computers_in_the_house_including/","date":1739973294,"author":"/u/Objective_Love_7434","guid":6956,"unread":true,"content":"<p>So me and my partner have 3 computers that work perfectly fine that are daily drivers. Nothing wrong with them whatsoever and they run fast and snappy for office tasks and light gaming. They run fast and snappy with SSDs.</p><p>We found that even with a TPM 2.0, they don't meet the CPU requirements. My partner is not tech savvy at all but usually plays older games and some steam indie games, most of which run on his laptop and if not that, our desktop.</p><p>This is where we finally decided to be the change regarding big corporations that really don't care. From cars that won't start without an update to trying to force decommissioning of good hardware seeing as moores law has long since ended. 3 working computers don't go into landfill even if a single capacitor pops in this house, as why waste money and ruin the environment more? The most we do is replace the batteries. I prefer older business grade hardware when I get it as that stuff doesn't seem to break. A surface laptop got all of 3 years before it's screen began to fail.</p><p>So I realised Linux would be our only option. I've been away years (was on Ubuntu for a while, then windows 7 came out and I immensely liked the OS), until I tried Kubuntu. Fast, stable and so snappy. Like everything just opens instantly. Light footprint. Even has an 'app store' kind of thing, good for a layman.</p><p>With wine all my partner's games run, and steam with proton makes gaming on Linux a reality. I used the terminal to set things up for us, as I'm the one with the tech knowledge in the house.</p><p>And my partner said it looks and feels almost identical to windows. He's not tech savvy at all, I set it up, but once running he can install windows apps as he normally would and use native ones.</p><p>We both have the view the windows 11 requirements were probably one last grab to get everyone to buy new PCs with their distribution partners and finally got rid of Microsoft. Currently windows 10 is still on our NAS box but that will change when it reaches end of support. I swapped paying for cloud storage to occasionally rotating HDDs at friends houses and keeping a small free one to upload encrypted small rapidly changing data sets.</p><p>Shocked that Kubuntu even came with a working office suite that didn't have to be installed, absolutely free. A hidden gem for the layman.</p><p>I do wish they still did .deb files though, makes it easy to keep offline setup files. I just fished them out of the cache.</p>","contentLength":2428,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Server Setup - Part 2: What‚Äôs Next After Installation","url":"https://linuxblog.io/linux-server-after-installation/","date":1739973176,"author":"/u/Unprotectedtxt","guid":7097,"unread":true,"content":"<p>Setting up a Linux server is rewarding and opens up all kinds of possibilities. You can run a personal website, a media server, or cloud storage‚Äîthere‚Äôs so much potential. By following foundational steps like choosing the right distribution, configuring SSH, and enabling firewalls, you already have a secure and flexible platform.</p><p><em>If you‚Äôve already followed the basics from </em><a title=\"Linux Server Setup ‚Äì Part 1\" href=\"https://linuxblog.io/linux-server-setup/\" rel=\"bookmark\">Linux Server Setup ‚Äì Part 1</a>,<em> then it‚Äôs time to dig deeper into monitoring, logging, troubleshooting, and additional tools to keep your Linux server running at peak performance.&nbsp;</em></p><h2>Monitoring, Logs, and Troubleshooting</h2><p><a href=\"https://linuxblog.io/free-linux-server-monitoring-apm-sysadmins/\" target=\"_blank\" rel=\"noopener\">Monitoring</a>, <a href=\"https://linuxblog.io/what-is-observability/\" target=\"_blank\" rel=\"noopener\">logging</a>, and <a href=\"https://linuxblog.io/90-linux-commands-frequently-used-by-linux-sysadmins/\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> are the keys to a stable and performant Linux server. Monitoring tools provide real-time insights, logs reveal detailed events, and troubleshooting ensures that issues are resolved quickly. Let‚Äôs break this down, starting with monitoring tools.</p><p><strong>Installing System Monitoring Tools</strong> (examples)</p><p>The  command is usually already installed by default. However, here‚Äôs how you can install some other useful command-line server monitoring tools:</p><p>: A more interactive and visually appealing alternative to . It allows you to sort and manage processes with ease.</p><pre>sudo apt install htop  # Debian/Ubuntu\nsudo dnf install htop  # Fedora/RHEL (Alma, Rocky, etc)</pre><p>: Offers a broader overview of system performance, including disk I/O and network activity, in a single dashboard.</p><pre>sudo apt install glances  # Debian/Ubuntu\nsudo dnf install glances  # Fedora/RHEL</pre><p>In addition to real-time monitoring, understanding <a href=\"https://www.loggly.com/ultimate-guide/linux-logging-basics/\" target=\"_blank\" rel=\"noopener\">system logs</a> is crucial for diagnosing issues and ensuring your server runs smoothly.&nbsp;<a href=\"https://www.datadoghq.com/dg/logs/linux-event-logs/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Logs</a> (<a href=\"https://www.datadoghq.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Datadog</a> is a <a href=\"https://linuxblog.io/sponsors/\">blog partner</a>) help diagnose issues with services and provide insights into server behavior. Tools like <a href=\"https://www.freedesktop.org/software/systemd/man/latest/journalctl.html\" target=\"_blank\" rel=\"noopener\"></a> can assist in reviewing logs for specific services, making troubleshooting more effective.</p><p>Logs are invaluable for <a href=\"https://linuxblog.io/monitoring-php-performance-diagnosing-bottlenecks/\" target=\"_blank\" rel=\"noopener\">diagnosing</a> and <a href=\"https://linuxblog.io/mysql-server-has-gone-away-error-solutions/\" target=\"_blank\" rel=\"noopener\">resolving server issues</a>. They provide detailed insights into server behavior and errors. Here‚Äôs how to use logs effectively when troubleshooting a Linux server:</p><ol start=\"1\" data-spread=\"true\"><li>: System logs are typically found in . Common logs include:\n<ul data-spread=\"false\"><li> or  for general system events.</li><li> for authentication events.</li><li> or  for web server logs.</li></ul></li><li>: For systems using ,  provides an efficient way to view logs:\n<pre><code>journalctl -u sshd\njournalctl -b  # Logs from the current boot</code></pre></li><li>: Use  to filter logs for specific terms:\n<pre><code>grep \"error\" /var/log/syslog</code></pre></li><li><strong>Monitor Logs in Real-Time</strong>: Use  to watch logs as they are updated:\n<pre><code>tail -f /var/log/auth.log</code></pre></li></ol><p>Understanding and <a href=\"https://github.com/logrotate/logrotate\" target=\"_blank\" rel=\"noopener\">analyzing logs regularly</a> can help you detect potential issues early and ensure the long-term stability of your server.</p><h3 data-pm-slice=\"1 1 []\">Troubleshooting Common Issues</h3><p>When setting up a new Linux server, errors and issues can arise. Being able to <a href=\"https://linuxblog.io/mastering-linux-administration-20-powerful-commands-to-know/\" target=\"_blank\" rel=\"noopener\">troubleshoot common problems effectively</a> is a vital skill for maintaining uptime and ensuring the smooth operation of your server. Let‚Äôs look at resolving connection issues and also how to use logs when troubleshooting Linux.</p><p><strong>Debugging Connection Problems</strong></p><p>Here are some steps to troubleshooting common networking issues:</p><ol start=\"1\" data-spread=\"true\"><li><strong>Verify Network Configuration</strong>: Ensure your server has the correct IP address, gateway, and DNS settings. Use tools like  or  to check network interfaces and  to test connectivity.</li><li>: Review your firewall settings to confirm that required ports are open. Use commands such as:\n<pre>sudo ufw status\nsudo firewall-cmd --list-all</pre></li><li>: If SSH is not working, try verbose mode to get detailed output:\n<pre>ssh -v username@your_server_ip</pre></li><li>: Ensure the necessary services are running:\n<pre>sudo systemctl status ssh</pre></li><li>: Check for physical issues with cables or routers if self-hosting. For cloud servers, confirm that the hosting provider has no outages or restrictions (<a href=\"https://status.stacklinux.com/\" target=\"_blank\" rel=\"noopener\">check their status page</a>).</li></ol><p>Building your Linux knowledge is easier when you‚Äôre part of a community. Whether you‚Äôre troubleshooting, sharing success stories, or looking for inspiration, there‚Äôs no substitute for real-time human-interaction with other Linux enthusiasts. Our sister site, <a href=\"https://linuxcommunity.io/\" target=\"_blank\" rel=\"noopener\">LinuxCommunity.io</a>, is a great place to swap ideas in a friendly forum setting. If you‚Äôre on Reddit, Subreddits like <a href=\"https://www.reddit.com/r/sysadmin/\" target=\"_blank\" rel=\"noopener\">r/sysadmin</a>, <a href=\"https://www.reddit.com/r/linux\" target=\"_blank\" rel=\"noopener\">r/linux</a>, <a href=\"https://www.reddit.com/r/linux4noobs\" target=\"_blank\" rel=\"noopener\">r/linux4noobs</a>, <a href=\"https://www.reddit.com/r/selfhosted\" target=\"_blank\" rel=\"noopener\">r/selfhosted</a>, and <a href=\"https://www.reddit.com/r/homelab\" target=\"_blank\" rel=\"noopener\">r/homelab</a> are full of power users sharing projects, tips, and real-world experience to help you keep leveling up.</p><h2>Level Up Your Linux Server</h2><p>Below are some tools, software, ideas, and resources that will help you discover projects, and keep your server at peak performance.</p><ul><li><a href=\"https://github.com/ajenti/ajenti\" target=\"_blank\" rel=\"noopener\">Ajenti</a>: Web-based server control panel for admins.</li><li><a href=\"https://ansible.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Ansible</a>: Automate configuration management, application deployment, and IT orchestration.</li><li><a href=\"https://bitwarden.com\" target=\"_blank\" rel=\"noopener\">Bitwarden</a>: Self-hosted password manager for secure credential storage.</li><li><a href=\"https://www.bookstackapp.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">BookStack</a>: Self-hosted platform for documentation and wikis.</li><li><a href=\"https://cachethq.io\" target=\"_blank\" rel=\"noopener\">CachetHQ</a>: Status page system for monitoring incidents and uptime.</li><li><a href=\"https://www.cacti.net\" target=\"_blank\" rel=\"noopener\">Cacti</a>: Graphing solution for network and server metrics.</li><li><a href=\"https://caddyserver.com\" target=\"_blank\" rel=\"noopener\">Caddy</a>: Web server with automatic HTTPS and easy configuration.</li><li><a href=\"https://ceph.io/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Ceph</a>: Unified block, file, and object storage for distributed environments.</li><li><a href=\"https://www.cloudron.io\" target=\"_blank\" rel=\"noopener\">Cloudron</a>: Simplify self-hosting apps with automated management.</li><li><a href=\"https://cockpit-project.org/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Cockpit</a>: A web-based interface to manage and monitor your Linux server.</li><li><a href=\"https://containerd.io/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Containerd</a>: A lightweight, high-performance container runtime.</li><li><a href=\"https://www.docker.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Docker</a>: Run and manage containers for application deployment.</li><li><a href=\"https://www.duplicati.com\" target=\"_blank\" rel=\"noopener\">Duplicati</a>: Backup software for creating encrypted backups.</li><li><a href=\"https://www.endian.com/community/comparison/\" target=\"_blank\" rel=\"noopener\">Endian</a>: Open-source firewall and UTM (Unified Threat Management) solution.</li><li><a href=\"https://gitea.io\" target=\"_blank\" rel=\"noopener\">Gitea</a>: Lightweight, self-hosted Git service.</li><li><a href=\"https://about.gitlab.com\" target=\"_blank\" rel=\"noopener\">GitLab</a>: Self-hosted Git repository management and CI/CD platform.</li><li><a href=\"https://www.gluster.org/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">GlusterFS</a>: Distributed storage for scalable and redundant file storage solutions.</li><li><a href=\"https://grafana.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Grafana</a>: Visualize metrics and performance data from Prometheus and other sources.</li><li><a href=\"https://www.graylog.org\" target=\"_blank\" rel=\"noopener\">Graylog</a>: Log management and analysis platform.</li><li><a href=\"https://guacamole.apache.org\" target=\"_blank\" rel=\"noopener\">Guacamole</a>: Clientless remote desktop gateway.</li><li><a href=\"https://www.haproxy.org\" target=\"_blank\" rel=\"noopener\">HAProxy</a>: High-performance load balancer and reverse proxy for TCP/HTTP.</li><li><a href=\"https://icinga.com\" target=\"_blank\" rel=\"noopener\">Icinga</a>: Monitor system health and alert on issues.</li><li><a href=\"https://www.ipfire.org/\" target=\"_blank\" rel=\"noopener\">IPFire</a>: Secure and flexible open-source firewall distribution.</li><li><a href=\"https://www.keycloak.org\" target=\"_blank\" rel=\"noopener\">Keycloak</a>: Open-source identity and access management solution.</li><li><a href=\"https://kubernetes.io/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Kubernetes</a>: Orchestrate, scale, and manage containers across multiple nodes.</li><li><a href=\"https://www.librenms.org/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">LibreNMS</a>: Network monitoring tool that supports a wide range of devices.</li><li><a href=\"https://www.lighttpd.net\" target=\"_blank\" rel=\"noopener\">Lighttpd</a>: Lightweight and fast web server for high-performance environments.</li><li><a href=\"https://mailinabox.email/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Mail-in-a-Box</a>: Self-hosted email server for managing personal or small-scale email.</li><li><a href=\"https://mailcow.email\" target=\"_blank\" rel=\"noopener\">Mailcow</a>: Self-hosted email suite with modern webmail and administration.</li><li><a href=\"https://mattermost.com\" target=\"_blank\" rel=\"noopener\">Mattermost</a>: Open-source, self-hosted alternative to Slack for team communication.</li><li><a href=\"https://www.monitorix.org\" target=\"_blank\" rel=\"noopener\">Monitorix</a>: Lightweight server monitoring with web UI.</li><li><a href=\"https://munin-monitoring.org\" target=\"_blank\" rel=\"noopener\">Munin</a>: Track server performance and visualize data trends.</li><li><a href=\"https://www.nagios.org/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Nagios</a>: Monitor systems, networks, and infrastructure for alerts and outages.</li><li><a href=\"https://www.netdata.cloud/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Netdata</a>: Real-time performance monitoring for servers and applications.</li><li><a href=\"https://nextdns.io\" target=\"_blank\" rel=\"noopener\">NextDNS</a>: Cloud-based DNS with advanced privacy and security features.</li><li><a href=\"https://github.com/evilsocket/opensnitch\" target=\"_blank\" rel=\"noopener\">OpenSnitch</a>: Interactive firewall for Linux to monitor outgoing connections.</li><li><a href=\"https://openvpn.net/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">OpenVPN</a>: Create secure VPN connections for your server and network.</li><li><a href=\"https://opnsense.org/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">OPNsense</a>: A fork of pfSense offering a modern, user-friendly firewall and router platform.</li><li><a href=\"https://docs.paperless-ngx.com\" target=\"_blank\" rel=\"noopener\">Paperless-ngx</a>: Self-hosted document management system for organizing digital files.</li><li><a href=\"https://pi-hole.net/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Pi-hole</a>: Network-wide ad blocker for DNS-based ad and tracker blocking.</li><li><a href=\"https://www.pfsense.org/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">pfSense</a>: Open-source firewall and router software for secure network management.</li><li><a href=\"https://www.portainer.io\" target=\"_blank\" rel=\"noopener\">Portainer</a>: Manage Docker and Kubernetes containers with ease.</li><li><a href=\"https://safing.io\" target=\"_blank\" rel=\"noopener\">Portmaster</a>: Privacy-first tools for network monitoring and protection.</li><li><a href=\"https://poste.io/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Poste.io</a>: Modern, self-hosted mail server solution.</li><li><a href=\"https://prometheus.io/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Prometheus</a>: Monitor server performance and metrics in real time.</li><li><a href=\"https://www.puppet.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Puppet</a>: Manage server configurations and automate deployments.</li><li><a href=\"https://saltproject.io/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Salt Project</a>: Automate configurations and orchestrate systems in real-time.</li><li><a href=\"https://tianji.msgbyte.com\" target=\"_blank\" rel=\"noopener\">Tianji</a>: Self-hosted alternative to social media analytics.</li><li><a href=\"https://traefik.io\" target=\"_blank\" rel=\"noopener\">Traefik</a>: Modern HTTP reverse proxy and load balancer for microservices.</li><li><a href=\"https://www.truenas.com/\" target=\"_blank\" rel=\"noopener\">TrueNAS</a>: Open-source Network Attached Storage (NAS) operating system.</li><li><a href=\"https://uptime.kuma.pet/\" target=\"_blank\" rel=\"noopener\">Uptime Kuma</a>: Modern, self-hosted monitoring system for uptime checks.</li><li><a href=\"https://varnish-cache.org\" target=\"_blank\" rel=\"noopener\">Varnish Cache</a>: HTTP accelerator for caching web content and boosting performance.</li><li><a href=\"https://www.vaultproject.io/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Vault</a>: Manage secrets, access control, and secure key storage.</li><li><a href=\"https://virt-manager.org\" target=\"_blank\" rel=\"noopener\">Virt-Manager</a>: Desktop interface for managing virtual machines.</li><li><a href=\"https://www.webmin.com/?utm_source=linuxblog.io\">Webmin</a>: A web-based interface for system administration of Linux servers.</li><li><a href=\"https://www.wireguard.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">WireGuard</a>: A modern, high-performance VPN protocol for secure and fast connections.</li><li><a href=\"https://www.zabbix.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Zabbix</a>: Enterprise-grade open-source monitoring for networks, servers, and applications.</li></ul><ul><li><a href=\"https://www.adafruit.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Adafruit</a>: Electronics and Linux-powered devices.</li><li><a href=\"https://www.phoronix.com/?utm_source=linuxblog.io\" target=\"_blank\" rel=\"noopener\">Phoronix</a>: Linux hardware reviews, benchmarks, and news.</li></ul><p>Linux servers have so much more to offer. Try new services, experiment with new monitoring tools, and hone your admin skills as you add more features. Every tweak you make increases your expertise and what your server can handle.</p><p>Looking for your next project or want more info? Head over to the <a href=\"https://linuxblog.io/browse-articles/\" target=\"_blank\" rel=\"noopener\">browse articles</a> page for how-tos, tips, and inspiration.&nbsp;Let‚Äôs keep building, learning, and enjoying taking our Linux servers to new heights!</p>","contentLength":8814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1it5yx8/linux_server_setup_part_2_whats_next_after/"},{"title":"Klarna Went All in on AI Customer Support & Are Now Reversing Course","url":"https://www.reddit.com/r/artificial/comments/1it5m0i/klarna_went_all_in_on_ai_customer_support_are_now/","date":1739972132,"author":"/u/YakFull8300","guid":6019,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[media] My first project on rust after learning basics.","url":"https://www.reddit.com/r/rust/comments/1it5kwi/media_my_first_project_on_rust_after_learning/","date":1739972036,"author":"/u/Independent_Row_6529","guid":5967,"unread":true,"content":"<p>Hi. Have been learning Rust, for the last one month. Wrote this program for a digital clock in terminal with ANSI block character.</p><p>Made an array to store the numbers written using the ansi codes - It was hard to align the block character for the output. Could this have been done differently?</p><p>Are there any dependencies specifically for making terminal UIs?</p><p>I'm also intending to add more features - to learn rust more. Please give some advice on that. Thanks in advance</p>","contentLength":467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepSeek GPU smuggling probe shows Nvidia's Singapore GPU sales are 28% of its revenue, but only 1% are delivered to the country: Report","url":"https://www.tomshardware.com/tech-industry/deepseek-gpu-smuggling-probe-shows-nvidias-singapore-gpu-sales-are-28-percent-of-its-revenue-but-only-1-percent-are-delivered-to-the-country-report","date":1739971380,"author":"/u/esporx","guid":6060,"unread":true,"content":"<p>‚ÄúThe physical delivery of products sold by Nvidia to Singapore represent less than 1% of Nvidia‚Äôs overall revenue,‚Äù Tan said. He then added, ‚ÄúIt is common practice for global entities to centralize the billing for procured goods and services in their hubs, but this is separate from where the products are shipped to so far from our checks.‚Äù This is despite reports saying Singapore accounts for nearly 28% of Nvidia‚Äôs revenue for 2024.</p><p>That means a company based in Singapore could order chips from Nvidia, with their billing address marked as such, but have them delivered to another country. However, Tan said this business strategy isn‚Äôt new, with many multinational companies operating across borders doing the same thing, saying that if you‚Äôre operating in different countries, it‚Äôs sometimes more cost-effective to bill everything using the headquarters address and then have the items shipped directly to where they‚Äôre needed.</p><p>In fact, Nvidia itself has long said [<a data-analytics-id=\"inline-link\" href=\"https://s201.q4cdn.com/141608511/files/doc_financials/2025/q3/ed2a395c-5e9b-4411-8b4a-a718d192155a.pdf\" data-url=\"https://s201.q4cdn.com/141608511/files/doc_financials/2025/q3/ed2a395c-5e9b-4411-8b4a-a718d192155a.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">PDF</a>], \"Revenue by geographic area is based upon the billing location of the customer. The end customer and shipping location may be different from our customer‚Äôs billing location. For example, most shipments associated with Singapore revenue were to locations other than Singapore and shipments to Singapore were insignificant.\"</p><p>However, Singapore is closely tied to China ‚Äî especially in business. This is especially true in the tech sector, where many Chinese companies have set up key offices on the island. For example, TikTok, which Chinese tech giant ByteDance owns, has its headquarters in the country, and its CEO is also Singaporean. Despite that, the country also considers the U.S. to be a key strategic partner, both in trade and politics, with the two countries‚Äô militaries even allowed to use each other‚Äôs facilities on the island and in Guam.</p><p>The country has to carefully balance its relationship with China and the United States, especially as the countries are currently engaged in a trade war with various bans and sanctions taking effect in recent years. Singapore likely doesn‚Äôt want to be put on Washington‚Äôs entity list, especially as it considers itself a business-friendly country, and getting on that list means it will have several limitations put on it, especially in the tech space. Because of this, Tan said that the Singapore government is working closely with U.S. authorities to investigate this discrepancy and that the country does not condone any business using their Singaporean address to get around export controls set by other countries.</p>","contentLength":2577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1it5cvr/deepseek_gpu_smuggling_probe_shows_nvidias/"},{"title":"KDE Plasma 6.3.1, Bugfix Release for February","url":"https://kde.org/announcements/plasma/6/6.3.1/","date":1739971175,"author":"/u/gabriel_3","guid":5876,"unread":true,"content":"<section><p>You can give us feedback and get updates on our social media channels:\n<a href=\"https://go.kde.org/matrix/#/#kde:kde.org\" aria-label=\"Share on Matrix\"></a><a href=\"https://floss.social/@kde\" aria-label=\"Share on Mastodon\"></a><a href=\"https://bsky.app/profile/kde.org\" aria-label=\"Share on Bluesky\"></a><a href=\"https://www.facebook.com/kde/\" aria-label=\"\"></a><a href=\"https://www.linkedin.com/company/29561/\" aria-label=\"Share on LinkedIn\"></a><a href=\"https://www.reddit.com/r/kde/\" aria-label=\"Share on Reddit\"></a><a href=\"https://lemmy.kde.social/\" aria-label=\"Share on Lemmy\"></a><a href=\"https://www.youtube.com/@KdeOrg\" aria-label=\"Share on YouTube\"></a><a href=\"https://tube.kockatoo.org/a/kde_community/video-channels\" aria-label=\"Share on PeerTube\"></a><a href=\"https://vk.com/kde_ru\" aria-label=\"Share on VK\"></a><a href=\"https://www.instagram.com/kdecommunity/\" aria-label=\"Share on Instagram\"></a></p><p align=\"justify\">Your feedback is greatly appreciated.</p></section><p align=\"justify\">KDE is a <a href=\"https://www.gnu.org/philosophy/free-sw.html\">Free Software</a> community that exists and grows only because of the help of many volunteers that donate their time and effort. KDE is always looking for new volunteers and contributions, whether it is help with coding, bug fixing or reporting, writing documentation, translations, promotion, money, etc. All contributions are gratefully appreciated and eagerly accepted. Please read through the <a href=\"https://kde.org/community/donations/\">Supporting KDE page</a> for further information or become a KDE e.V. supporting member through our <a href=\"https://kde.org/community/donations/\">Join the Game</a> initiative.</p><p align=\"justify\">KDE is an international technology team that creates free and open source software for desktop and portable computing. Among KDE‚Äôs products are a modern desktop system for Linux and UNIX platforms, comprehensive office productivity and groupware suites and hundreds of software titles in many categories including Internet and web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 60 languages and is built with ease of use and modern accessibility principles in mind. KDE‚Äôs full-featured applications run natively on Linux, BSD, Windows, Haiku, and macOS.</p>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1it5af8/kde_plasma_631_bugfix_release_for_february/"},{"title":"Yet another article on sqlc","url":"https://www.reddit.com/r/golang/comments/1it50tc/yet_another_article_on_sqlc/","date":1739970356,"author":"/u/Medical-Age-6422","guid":6018,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Medical-Age-6422\"> /u/Medical-Age-6422 </a>","contentLength":39,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Greg KH: Rust isn't a \"silver bullet\" that will solve all of our problems, but it sure will help in a huge number of places, so for new stuff going forward, why wouldn't we want that?","url":"https://lore.kernel.org/rust-for-linux/2025021954-flaccid-pucker-f7d9@gregkh/","date":1739967255,"author":"/u/SupermarketAntique32","guid":5780,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1it42sw/greg_kh_rust_isnt_a_silver_bullet_that_will_solve/"},{"title":"Greg KH: But for new code / drivers, writing them in Rust where these types of bugs just can't happen (or happen much much less) is a win for all of us, why wouldn't we do this?","url":"https://lore.kernel.org/rust-for-linux/2025021954-flaccid-pucker-f7d9@gregkh/","date":1739965896,"author":"/u/small_kimono","guid":5731,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1it3owc/greg_kh_but_for_new_code_drivers_writing_them_in/"},{"title":"Rewrite Kafka in Rust? I've developed a faster message queue, StoneMQ.","url":"https://www.reddit.com/r/rust/comments/1it3d53/rewrite_kafka_in_rust_ive_developed_a_faster/","date":1739964657,"author":"/u/jonefeewang","guid":5966,"unread":true,"content":"<div><ol><li><strong>Current Features (v0.1.0)</strong>:<ul><li>Supports single-node message sending and receiving.</li><li>Implements group consumption functionality.</li></ul></li><li>:<ul><li>Aims to replace Kafka's server-side functionality in massive-scale queue cluster.</li><li>Focused on reducing operational costs while improving efficiency.</li><li>Fully compatible with Kafka's client-server communication protocol, enabling seamless client-side migration without requiring modifications.</li></ul></li><li>:<ul><li>Entirely developed in .</li><li>Utilizes  and  to achieve high performance, concurrency, and scalability.</li></ul></li></ol></div>   submitted by   <a href=\"https://www.reddit.com/user/jonefeewang\"> /u/jonefeewang </a>","contentLength":538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Share your EXPLOSIONS thread","url":"https://www.reddit.com/r/kubernetes/comments/1it2vyy/weekly_share_your_explosions_thread/","date":1739962833,"author":"/u/gctaylor","guid":6138,"unread":true,"content":"<div><p>Did anything explode this week (or recently)? Share the details for our mutual betterment.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a>","contentLength":121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is future technology for Sr, Devops Engineer from now-2025,","url":"https://www.reddit.com/r/kubernetes/comments/1it2tme/what_is_future_technology_for_sr_devops_engineer/","date":1739962581,"author":"/u/sabir8992","guid":5734,"unread":true,"content":"<div><p>Can you list out the technology and certification that will be alteast in trend for next 5 to 8 years.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/sabir8992\"> /u/sabir8992 </a>","contentLength":134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GopherCon Europe 2025 just got released","url":"https://www.reddit.com/r/golang/comments/1it2all/gophercon_europe_2025_just_got_released/","date":1739960425,"author":"/u/zetttaa","guid":5688,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/zetttaa\"> /u/zetttaa </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Mamba: Can We Achieve Infinite Context Length?","url":"https://www.reddit.com/r/MachineLearning/comments/1it279f/r_mamba_can_we_achieve_infinite_context_length/","date":1739960044,"author":"/u/Personal_Click_6502","guid":5850,"unread":true,"content":"<p>I discuss Mamba, a class of state space models for sequence modeling, and explain the basics of Transformers, RNNs, and State Space Models, along with their limitations. The blog then explores how Mamba, an S6 model (Selective Scan Structured State Space Sequence Model), offers advantages when modeling long sequences.</p><p>Long Context lengths, reaching billions of tokens, are essential for LLMs. They enable reasoning over extended histories while addressing challenges like chunking in RAG-based approaches and the ‚Äúlost in the middle‚Äù problem. However, infinite context length remains challenging due to the quadratic computational cost of self-attention in Transformers.</p><p>Mamba's linear time complexity presents a potential solution. Falcon-Mamba, which can process sequences of any length without increasing memory usage (as shown in the image), has demonstrated this.</p><p>This blog covers Mamba, its mathematical foundations, and a PyTorch implementation.</p><p>Trying to write these blogs to have a good understanding of these interesting concepts. If time permits, I hope to eventually compile them into a book. Feedback and criticism are always welcome.</p>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Role of Redis in Making TBMQ a High-Performance MQTT Broker","url":"https://thingsboard.io/blog/1-million-reasons-to-choose-tbmq-as-high-performance-mqtt-broker/","date":1739959429,"author":"/u/dlandiak","guid":5908,"unread":true,"content":"<p>Can an open-source MQTT broker handle one million messages per second for persistent sessions? TBMQ 2.x <a href=\"https://thingsboard.io/docs/mqtt-broker/reference/1m-throughput-p2p-performance-test/\" target=\"_blank\" rel=\"noopener\" title=\"\">proves</a> it can! Even more importantly, it achieves this with no single point of failure and ensures no data loss, even when hardware fails, making it a robust self-hosted MQTT broker solution for IIoT applications.</p><p>This article dives into the architectural decisions and performance improvements that define the recent <a href=\"https://thingsboard.io/docs/mqtt-broker/releases/#v201-december-31-2024\" target=\"_blank\" rel=\"noopener\" title=\"\">2.0.x releases</a> of TBMQ, focusing on how these changes optimize persistent session handling, enhance P2P messaging, and improve overall system efficiency within a scalable IoT architecture.</p><p>We hope this article offers valuable insights for software engineers looking for ideas and patterns to offload database workloads to persistent caching layers, helping to improve scalability and performance in their systems. Additionally, for those exploring a Mosquitto alternative, TBMQ stands out as a powerful and fault-tolerant MQTT broker option.</p><p>While the TBMQ 1.x version <a href=\"https://thingsboard.io/docs/mqtt-broker/reference/3m-throughput-single-node-performance-test/\">ispatches</a> 3 million messages per second, as a high-performance MQTT broker it was primarily designed to aggregate data from IoT devices and deliver it to back-end applications reliably (QoS 1). This architecture is based on our experience with IIoT and other large-scale IoT deployments, where millions of devices transmit data to a limited number of applications.</p><p>Through these deployments, we recognized that IoT devices and applications follow distinct communication patterns. IoT devices or sensors publish data frequently but subscribe to relatively few topics or updates. In contrast, applications subscribe to data from tens or even hundreds of thousands of devices and require reliable message delivery. Additionally, applications often experience periods of downtime due to system maintenance, upgrades, failover scenarios, or temporary network disruptions.</p><p>To address these differences, TBMQ introduces a key feature: the classification of MQTT clients as either standard (IoT devices) or <a href=\"https://thingsboard.io/docs/mqtt-broker/architecture/#persistent-application-client\" target=\"_blank\" rel=\"noopener\" title=\"\">application</a> clients. This distinction enables optimized handling of persistent MQTT sessions for applications. Specifically, each persistent application client is assigned a separate Kafka topic. This approach ensures efficient message persistence and retrieval when an MQTT client reconnects, improving overall reliability and performance. Additionally, application clients support MQTT‚Äôs shared subscription feature, allowing multiple instances of an application to efficiently distribute message processing.</p><p><a href=\"https://kafka.apache.org/\" target=\"_blank\" rel=\"noopener\" title=\"\">Kafka</a> serves as one of the core components. Designed for high-throughput, distributed messaging, Kafka efficiently handles large volumes of data streams, making it an ideal choice for TBMQ. With the latest Kafka versions capable of managing a huge number of topics, this architecture is well-suited for enterprise-scale deployments.</p><p>Unlike the fan-in, the point-to-point (P2P) communication pattern enables direct message exchange between MQTT clients. Typically implemented using uniquely defined topics, P2P is well-suited for private messaging, device-to-device communication, command transmission, and other direct interaction use cases.</p><p>One of the key differences between fan-in and peer-to-peer MQTT messaging is the volume and flow of messages. In a P2P scenario, subscribers do not handle high message volumes, making it unnecessary to allocate dedicated Kafka topics and consumer threads to each MQTT client. Instead, the primary requirements for P2P message exchange are low latency and reliable message delivery, even for clients that may go offline temporarily. To meet these needs, TBMQ optimizes persistent session management for standard MQTT clients, which include IoT devices. </p><p>In TBMQ 1.x, standard MQTT clients relied on PostgreSQL for message persistence and retrieval, ensuring that messages were delivered when a client reconnected. While PostgreSQL performed well initially, it had a fundamental limitation‚Äîit could only scale vertically. We anticipated that as the number of persistent MQTT sessions grew, PostgreSQL‚Äôs architecture would eventually become a bottleneck. To address this, we explored more scalable alternatives capable of handling the increasing demands of our MQTT broker. Redis was quickly chosen as the best fit due to its horizontal scalability, native clustering support, and widespread adoption.</p><h4>PostgreSQL usage and limitations</h4><p>To fully understand the reasoning behind this shift, it‚Äôs important to first examine how MQTT clients operated within the PostgreSQL architecture. This architecture was built around two key tables.</p><p>The  table was responsible for maintaining the session state of each persistent MQTT client:</p><pre><code>                      Table \"public.device_session_ctx\"\n       Column       |          Type          | Collation | Nullable | Default \n--------------------+------------------------+-----------+----------+---------\n client_id          | character varying(255) |           | not null | \n last_updated_time  | bigint                 |           | not null | \n last_serial_number | bigint                 |           |          | \n last_packet_id     | integer                |           |          | \nIndexes:\n    \"device_session_ctx_pkey\" PRIMARY KEY, btree (client_id)</code></pre><p>The key columns are  and , which is used to maintain message order for persistent MQTT clients:</p><ul><li> represents the packet ID of the last MQTT message received.</li><li> acts as a continuously increasing counter, preventing message order issues when the MQTT packet ID wraps around after reaching its limit of .</li></ul><p>The  table was responsible for storing messages that must be published to persistent MQTT clients (subscribers).</p><pre><code>                         Table \"public.device_publish_msg\"\n          Column          |          Type          | Collation | Nullable | Default \n--------------------------+------------------------+-----------+----------+---------\n client_id                | character varying(255) |           | not null | \n serial_number            | bigint                 |           | not null | \n topic                    | character varying      |           | not null | \n time                     | bigint                 |           | not null | \n packet_id                | integer                |           |          | \n packet_type              | character varying(255) |           |          | \n qos                      | integer                |           | not null | \n payload                  | bytea                  |           | not null | \n user_properties          | character varying      |           |          | \n retain                   | boolean                |           |          | \n msg_expiry_interval      | integer                |           |          | \n payload_format_indicator | integer                |           |          | \n content_type             | character varying(255) |           |          | \n response_topic           | character varying(255) |           |          | \n correlation_data         | bytea                  |           |          | \nIndexes:\n    \"device_publish_msg_pkey\" PRIMARY KEY, btree (client_id, serial_number)\n    \"idx_device_publish_msg_packet_id\" btree (client_id, packet_id)</code></pre><p>The key columns to highlight:</p><ul><li> ‚Äì captures the system time (timestamp) when the message is stored. This field is used for periodic cleanup of expired messages.</li><li> ‚Äì represents the expiration time (in seconds) for a message. This is set only for incoming MQTT 5 messages that include an expiry property. If the expiry property is absent, the message does not have a specific expiration time and remains valid until it is removed by time or size-based cleanup.</li></ul><p>Together, these tables manage message persistence and session state. The  table is designed for fast retrieval of the last MQTT packet ID and serial number stored for each persistent MQTT client. When messages for a client are received from a shared Kafka topic, the broker queries this table to fetch the latest values. These values are incremented sequentially and assigned to each message before being saved to the  table.</p><p>While this design ensured reliable message delivery, it also introduced performance constraints. To better understand its limitations, we conducted prototype testing to evaluate PostgreSQL‚Äôs performance under the P2P communication pattern. Using a single instance with 64GB RAM and 12 CPU cores, we simulated message loads with a dedicated <a href=\"https://github.com/thingsboard/tb-mqtt-perf-tests/tree/p2p-perf-test\" target=\"_blank\" rel=\"noopener\" title=\"\">performance testing tool</a> capable of generating MQTT clients and simulating the desired message load. The primary performance metric was the average message processing latency ‚Äî measured from the moment the message was published to the point it was acknowledged by the subscriber. The test was considered successful only if there was no performance degradation, meaning the broker consistently maintained an average latency in the two-digit millisecond range. </p><p>With the prototype testing, we ultimately reach thelimit at 30k msg/s throughput utilizing PostgreSQL as a persistence message storage. Throughput refers to the total number of messages per second, including both incoming and outgoing messages. </p><p>Based on the TimescaleDB blog <a href=\"https://www.timescale.com/blog/postgresql-timescaledb-1000x-faster-queries-90-data-compression-and-much-more#ingest-performance\" target=\"_blank\" rel=\"noopener\" title=\"\">post</a>, vanilla PostgreSQL can handle up to 300k inserts per second under ideal conditions. However, this performance depends on factors such as hardware, workload, and table schema. While vertical scaling can provide some improvement, PostgreSQL‚Äôs per-table insert throughput eventually reaches a hard limit. Confident that Redis could overcome this bottleneck, we began the migration process to achieve greater scalability and efficiency.</p><h4>Redis as a scalable alternative</h4><p>Our decision to migrate to Redis was driven by its ability to address the core performance bottlenecks encountered with PostgreSQL. Unlike PostgreSQL, which relies on disk-based storage and vertical scaling, Redis operates primarily in memory, significantly reducing read and write latency. Additionally, Redis‚Äôs distributed architecture enables horizontal scaling, making it an ideal fit for high-throughput messaging in P2P communication scenarios.</p><p>With these benefits in mind, we started our migration process with an evaluation of data structures that could preserve the functionality of the PostgreSQL approach while aligning with Redis Cluster constraints to enable efficient horizontal scaling. This also presented an opportunity to improve certain aspects of the original design, such as periodic cleanups, by leveraging Redis features like built-in expiration mechanisms.</p><h5>Redis Cluster Constraints</h5><p>When migrating from PostgreSQL to Redis, we recognized that replicating the existing data model would require multiple Redis data structures to efficiently handle message persistence and ordering. This, in turn, meant using multiple keys for each persistent MQTT Client session.</p><p>Redis Cluster distributes data across multiple slots to enable horizontal scaling. However, multi-key operations must access keys within the same slot. If the keys reside in different slots, the operation triggers a cross-slot error, preventing the command from executing. MQTT client ID as a&nbsp;<a href=\"https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/#hash-tags\" target=\"_blank\" rel=\"noopener\" title=\"\">hash tag</a>&nbsp;in our key names to address this. By enclosing the client ID in curly braces , Redis ensures that all keys for the same client are hashed to the same slot. This guarantees that related data for each client stays together, allowing multi-key operations to proceed without errors.</p><h5>Atomic operations via Lua scripts</h5><p>Consistency is critical in a high-throughput environment like TBMQ, where many messages can arrive simultaneously for the same MQTT client. Hashtagging helps to avoid cross-slot errors, but without atomic operations, there is a risk of race conditions or partial updates. This could lead to message loss or incorrect ordering. It is important to make sure that operations updating the keys for the same MQTT client are atomic.</p><p>Redis is designed to execute individual commands atomically. However, in our case, we need to update multiple data structures as part of a single operation for each MQTT client. Executing these sequentially without atomicity opens the door to inconsistencies if another process modifies the same data in between commands. That‚Äôs where <a href=\"https://redis.io/docs/latest/develop/interact/programmability/eval-intro/\" target=\"_blank\" rel=\"noopener\" title=\"\">Lua scripting</a> comes in. Lua script executes as a single, isolated unit. During script execution, no other commands can run concurrently, ensuring that the operations inside the script happen atomically.</p><p>Based on this information, we decided that for any operation, such as saving messages or retrieving undelivered messages upon reconnection, we will execute a separate Lua script. This ensures that all operations within a single Lua script reside in the same hash slot, maintaining atomicity and consistency.</p><h5>Choosing the right Redis data structures</h5><p>One of the key requirements of the migration was maintaining message order, a task previously handled by the  column in PostgreSQL‚Äôs  table. After evaluating various Redis data structures, we determined that <a href=\"https://redis.io/docs/latest/develop/data-types/sorted-sets/\" target=\"_blank\" rel=\"noopener\" title=\"\">sorted sets</a> (ZSETs)  were the ideal replacement. </p><p>Redis sorted sets naturally organize data by score, enabling quick retrieval of messages in ascending or descending order. While sorted sets provided an efficient way to maintain message order, storing full message payloads directly in sorted sets led to excessive memory usage. Redis does not support per-member TTL within sorted sets. As a result, messages persisted indefinitely unless explicitly removed. Similar to PostgreSQL, we had to perform periodic cleanups using  to delete expired messages. This operation carries a complexity of , where  is the number of elements removed. To overcome this limitation we decided to store message payloads using <a href=\"https://redis.io/docs/latest/develop/data-types/strings/\" target=\"_blank\" rel=\"noopener\" title=\"\">strings</a> data structure while storing in the sorted set references to these keys.</p><p>In the image above, you can see that the score continues to grow even when the MQTT packet ID wraps around. Let‚Äôs take a closer look at the details illustrated in this image. At first, the reference for the message with the MQTT packet ID equal to  was added to the sorted set:</p><pre><code>ZADD {client_id}_messages 65534 {client_id}_messages_65534</code></pre><p>Here,  is the sorted set key name, where  acts as a hash tag derived from the persistent MQTT client‚Äôs unique ID. The suffix  is a constant added to each sorted set key name for consistency. Following the sorted set key name, the score value  corresponds to the MQTT packet ID of the message received by the client. Finally, we see the reference key that points to the actual payload of the MQTT message. Similar to the sorted set key, the message reference key uses the MQTT client‚Äôs ID as a hash tag, followed by the  suffix and the MQTT packet ID value.</p><p>In the next iteration, we add the message reference for the MQTT message with a packet ID equal to  into the sorted set. This is the maximum packet ID, as the range is limited to .</p><pre><code>ZADD {client_id}_messages 65535 {client_id}_messages_65535</code></pre><p>So at the next iteration MQTT packet ID should be equal to , while the score should continue to grow and be equal to .</p><pre><code>ZADD {client_id}_messages 65536 {client_id}_messages_1</code></pre><p>This approach ensures that the message‚Äôs references will be properly ordered in the sorted set regardless of the packet ID‚Äôs limited range.</p><p>Message payloads are stored as string values with  commands that support expiration (), providing  complexity for writes and  applications:</p><pre><code>SET {client_id}_messages_1 \"{\n  \\\"packetType\\\":\\\"PUBLISH\\\",\n  \\\"payload\\\":\\\"eyJkYXRhIjoidGJtcWlzYXdlc29tZSJ9\\\",\n  \\\"time\\\":1736333110026,\n  \\\"clientId\\\":\\\"client\\\",\n  \\\"retained\\\":false,\n  \\\"packetId\\\":1,\n  \\\"topicName\\\":\\\"europe/ua/kyiv/client/0\\\",\n  \\\"qos\\\":1\n}\" EX 600</code></pre><p>Another benefit aside from efficient updates and TTL applications is that the message payloads can be retrieved:</p><pre><code>GET {client_id}_messages_1</code></pre><pre><code>DEL {client_id}_messages_1</code></pre><p>with constant complexity  without affecting the sorted set structure.</p><p>Another very important element of our Redis architecture is the use of a string key to store the last MQTT packet ID processed:</p><pre><code>GET {client_id}_last_packet_id\n\"1\"</code></pre><p>This approach serves the same purpose as in the PostgreSQL solution. When a client reconnects, the server must determine the correct packet ID to assign to the next message that will be saved in Redis. Initially, we considered using the sorted set‚Äôs highest score as a reference. However, since there are scenarios where the sorted set could be empty or completely removed, we concluded that the most reliable solution is to store the last packet ID separately.</p><h6>Managing Sorted Set Size Dynamically</h6><p>This hybrid approach, leveraging sorted sets and string data structures, eliminates the need for periodic cleanups based on time, as per-message TTLs are now applied. In addition, following the PostgreSQL design we needed to address somehow the cleanup of the sorted set based on the messages limit set in the configuration.</p><p>This limit is an important part of our design, allowing us to control and predict the memory allocation required for each persistent MQTT client. For example, a client might connect, triggering the registration of a persistent session, and then rapidly disconnect. In such scenarios, it is essential to ensure that the number of messages stored for the client (while waiting for a potential reconnection) remains within the defined limit, preventing unbounded memory usage. </p><pre><code>if (messagesLimit &gt; 0xffff) {\n    throw new IllegalArgumentException(\"Persisted messages limit can't be greater than 65535!\");\n}</code></pre><p>To reflect the natural constraints of the MQTT protocol, the maximum number of persisted messages for individual clients is set to .</p><p>To handle this within the Redis solution, we implemented dynamic management of the sorted set‚Äôs size. When new messages are added, the sorted set is trimmed to ensure the total number of messages remains within the desired limit, and the associated strings are also cleaned up to free up memory.</p><pre><code>-- Get the number of elements to be removed\nlocal numElementsToRemove = redis.call('ZCARD', messagesKey) - maxMessagesSize\n-- Check if trimming is needed\nif numElementsToRemove &gt; 0 then\n    -- Get the elements to be removed (oldest ones)\n    local trimmedElements = redis.call('ZRANGE', messagesKey, 0, numElementsToRemove - 1)\n    -- Iterate over the elements and remove them\n    for _, key in ipairs(trimmedElements) do\n        -- Remove the message from the string data structure\n        redis.call('DEL', key)\n        -- Remove the message reference from the sorted set\n        redis.call('ZREM', messagesKey, key)\n    end\nend</code></pre><h6>Message Retrieval and Cleanup</h6><p>Our design not only ensures dynamic size management during the persistence of new messages but also supports cleanup during message retrieval, which occurs when a device reconnects to process undelivered messages. This approach keeps the sorted set clean by removing references to expired messages.</p><pre><code>-- Define the sorted set key\nlocal messagesKey = KEYS[1]\n-- Define the maximum allowed number of messages\nlocal maxMessagesSize = tonumber(ARGV[1])\n-- Get all elements from the sorted set\nlocal elements = redis.call('ZRANGE', messagesKey, 0, -1)\n-- Initialize a table to store retrieved messages\nlocal messages = {}\n-- Iterate over each element in the sorted set\nfor _, key in ipairs(elements) do\n    -- Check if the message key still exists in Redis\n    if redis.call('EXISTS', key) == 1 then\n        -- Retrieve the message value from Redis\n        local msgJson = redis.call('GET', key)\n        -- Store the retrieved message in the result table\n        table.insert(messages, msgJson)\n    else\n        -- Remove the reference from the sorted set if the key does not exist\n        redis.call('ZREM', messagesKey, key)\n    end\nend\n-- Return the retrieved messages\nreturn messages</code></pre><p>By leveraging Redis‚Äô sorted sets and strings, along with Lua scripting for atomic operations, our new design achieves efficient message persistence and retrieval, as well as dynamic cleanup. This design addresses the scalability limitations of the PostgreSQL-based solution.</p><p>In the next sections, we describe the performance of the new Redis-based architecture compared to the PostgreSQL solution. These sections present the results of the performance tests and their key findings.</p><h4>Migration from Jedis to Lettuce</h4><p>As we mentioned earlier, we conducted a prototype test that revealed the limit of 30k msg/s throughput when using PostgreSQL for persistence message storage. At the moment we migrated to Redis, we already used the <a href=\"https://github.com/redis/jedis\" target=\"_blank\" rel=\"noopener\" title=\"\">Jedis</a> library for Redis interactions, primarily for cache management, and extended it to handle message persistence for persistent MQTT clients. However, the initial results of the Redis implementation with Jedis were unexpected. While we anticipated Redis would significantly outperform PostgreSQL, the performance improvement was modest ‚Äì reaching only 40k msg/s throughput compared to the 30k msg/s limit with PostgreSQL.</p><p>This led us to investigate the bottlenecks, where we discovered that Jedis was a limiting factor. While reliable, Jedis operates synchronously, processing each Redis command sequentially. This forces the system to wait for one operation to complete before executing the next. In high-throughput environments, this approach significantly limited Redis‚Äôs potential, preventing the full utilization of system resources.</p><p>To overcome this limitation, we migrated to <a href=\"https://github.com/redis/lettuce\" target=\"_blank\" rel=\"noopener\" title=\"\">Lettuce</a>, an asynchronous Redis client built on top of <a href=\"https://github.com/netty/netty\" target=\"_blank\" rel=\"noopener\" title=\"\">Netty</a>. With Lettuce, our throughput increased to 60k msg/s, demonstrating the benefits of non-blocking operations and improved parallelism.</p><p>Lettuce allows multiple commands to be sent and processed in parallel, fully exploiting Redis‚Äôs capacity for concurrent workloads. Ultimately, the migration unlocked the performance gains we expected from Redis, paving the way for successful P2P testing at scale.</p><h4>Scaling Point-to-point Messaging</h4><p>With Redis and Lettuce fully integrated, the next challenge was ensuring TBMQ‚Äôs ability to handle large-scale P2P messaging in a distributed environment. To simulate real-world conditions, we deployed <a href=\"https://thingsboard.io/docs/mqtt-broker/install/cluster/aws-cluster-setup/\" target=\"_blank\" rel=\"noopener\" title=\"\">TBMQ on AWS EKS (Elastic Kubernetes Service)</a>, allowing us to dynamically scale and stress-test the system. </p><p>To evaluate performance and prove that our system can scale efficiently, we started with 200,000 messages per second and increased the load by 200,000 messages in each iteration. In each phase, we scaled the number of TBMQ brokers and Redis nodes to handle the growing traffic while keeping the system stable. For the 1M msg/sec test, we also scaled the number of Kafka brokers to handle the corresponding workload.</p><figure><table><tbody><tr></tr></tbody></table></figure><p>Beyond adding resources, each increase in load required fine-tuning of Kafka topic partitioning and Lettuce command batching parameters. These adjustments helped distribute traffic evenly and keep latency stable, preventing bottlenecks as we scaled.</p><div><div><figure><table><tbody><tr></tr></tbody></table><figcaption> ‚Äì number of Kafka partitions for topics that store incoming messages. ‚Äì Number of Redis commands buffered before a flush. Commands are flushed either when the batch size is reached or every , whichever comes first.  </figcaption></figure></div></div><p>We reached our target of <strong>1 million messages per second</strong>, validating TBMQ‚Äôs capability to support high-throughput reliable P2P messaging. To better illustrate the test setup and results, the following diagram provides a visual breakdown of the final performance test.</p><p>Throughout testing, we monitored key performance indicators such as CPU utilization, memory usage, and message processing latency. One of TBMQ‚Äôs standout advantages, highlighted in our P2P testing, is its exceptional messages-per-second per CPU core performance. Compared to public benchmarks of other brokers, TBMQ consistently delivers higher throughput with fewer resources, reinforcing its efficiency in large-scale deployments.</p><p><strong>Key takeaways from tests include:</strong></p><ul><li> TBMQ demonstrated linear scalability. By incrementally adding TBMQ nodes, Redis nodes, and Kafka nodes at higher workloads, we maintained reliable performance as the message throughput increased from 200k to 1M msg/sec.</li><li><strong>Efficient Resource Utilization:</strong> CPU utilization on TBMQ nodes remained consistently around ~90% across all test phases, indicating that the system effectively used available resources without overconsumption.</li><li> The observed latency across all tests remained&nbsp;within two-digit bounds. This was predictable given the&nbsp;QoS 1level chosen for our test, applied to both publishers and&nbsp;persistent&nbsp;subscribers. We also tracked the average acknowledgment latency for publishers, which stayed within&nbsp;single-digit bounds&nbsp;across all test phases.</li><li> TBMQ‚Äôs one-to-one communication pattern showed excellent efficiency, processing about&nbsp;8900 msg/s per CPU core. We calculated this by dividing the total throughput by the total number of CPU cores used in the setup.</li></ul><p>Additionally, the following table provide a comprehensive summary of the key elements and results of the final 1M msg/sec test:</p><figure><table><tbody><tr><td></td></tr></tbody></table><figcaption> The average CPU utilization across all TBMQ nodes. The average duration from when a PUB message is sent by the publisher to when it is received by the subscriber. The average time elapsed between the PUB message sent by the publisher and the reception of the PUBACK acknowledgment.</figcaption></figure><p><a href=\"https://thingsboard.io/docs/mqtt-broker/releases/#v201-december-31-2024\" target=\"_blank\" rel=\"noopener\" title=\"\">TBMQ 2.x</a> sets a new benchmark for self-hosted MQTT brokers, proving its ability to handle one million messages per second for persistent sessions without data loss‚Äîeven in the event of hardware failures. Designed to eliminate single points of failure and bottlenecks, TBMQ achieves exceptional efficiency through a combination of Redis for persistence, Kafka for message distribution, and a highly optimized broker codebase.<p>In our tests and by comparing public data from other platforms, TBMQ leads in messages processed per CPU core (</p>), making it a strong choice for high-throughput IIoT and other MQTT deployments. This efficiency is not just a raw metric‚Äîit also means lower infrastructure costs and simpler scaling.<p>TBMQ has long been a top performer in fan-in and fan-out scenarios, handling data from millions of devices to back-end systems with ease. Now, it also stands out in P2P messaging, offering low latency and high throughput for one-to-one communication use cases. If you need massive throughput, low latency, and efficient resource usage, TBMQ offers a proven path forward.</p></p>","contentLength":26166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1it21xl/the_role_of_redis_in_making_tbmq_a/"},{"title":"How AI generated code accelerates technical debt","url":"https://leaddev.com/software-quality/how-ai-generated-code-accelerates-technical-debt","date":1739958615,"author":"/u/scarey102","guid":5690,"unread":true,"content":"<div><p>You have  article left to read this month before you need to <a href=\"https://leaddev.com/register\">register</a> a free LeadDev.com account.</p></div><p>GitClear‚Äôs latest report exposes rising code duplication and declining quality as AI coding tools gain in popularity.</p><p>‚ÄúI don‚Äôt think I have ever seen so much <a href=\"https://leaddev.com/technical-direction/tech-debt-engineering-leaders-how-shortcut-today-impacts-tomorrow\">technical debt</a> being created in such a short period of time during my 35-year career in technology,‚Äù says API evangelist Kin Lane, referring to AI-generated code proliferation.</p><p>GitClear‚Äôs second-annual <a href=\"http://gitclear.com/ai_assistant_code_quality_2025_research\">AI Copilot Code Quality research</a> analyzed 211 million changed lines of code from 2020 to 2024 across a combined dataset of anonymized private repositories and 25 of the largest open-source projects. It found multiple signatures of <a href=\"https://leaddev.com/software-quality/guide-measuring-and-improving-code-quality\">declining code quality</a> ‚Äì sounding the alarm around the long-term repercussions of quick wins with AI.</p><p>During 2024, GitClear tracked an 8-fold increase in the frequency of code blocks with five or more lines that duplicate adjacent code ‚Äì&nbsp;showing a prevalence of code duplication ten times higher than two years ago.</p><p>That same year, 46% of code changes were new lines, while copy-pasted lines exceeded moved lines. ‚ÄúMoved,‚Äù lines is a metric GitClear has devised to track the rearranging of code, an action typically performed to consolidate previous work into reusable modules. ‚ÄúRefactored systems, in general, and moved code in particular, are the signature of code reuse,‚Äù says Bill Harding, CEO of Amplenote and GitClear.</p><p>A year-on-year decline in code movement suggests developers are less likely to reuse previous work, a marked shift from existing industry best practice that would lead to more redundant systems with less consolidation of functions.</p><p>Unbridled AI code generation is anticipated to carry a long-term maintenance burden, especially for long-lived repositories. Contrary to perceived productivity benefits, the <a href=\"https://www.harness.io/state-of-software-delivery\">State of Software Delivery 2025</a> report by software vendor Harness found the majority of developers spend  time debugging AI-generated code and more time resolving security vulnerabilities.</p><p>The growing frequency of copy-and-pasted lines in commits might not be alarming to those who‚Äôve copied StackOverflow code during their career. However, the new AI-infused workflow has the potential to dramatically escalate <a href=\"https://leaddev.com/software-quality/tackling-tech-debt\">technical debt</a>.</p><p>Google‚Äôs 2024 <a href=\"https://cloud.google.com/blog/products/devops-sre/announcing-the-2024-dora-report\">DORA report</a> found a trade-off between gains and losses with AI, where a 25% increase in AI usage <a href=\"https://leaddev.com/velocity/how-speed-code-reviews\">quickens code reviews</a> and benefits documentation, but results in a 7.2% decrease in delivery stability.</p><p>Data suggests that if current trends continue, defect remediation and refactoring may soon dominate developer workloads. ‚ÄúIf developer productivity continues being measured by commit count or lines added, AI-driven maintainability decay will proliferate,‚Äù says Harding.</p><p>Unless teams focus on <a href=\"https://leaddev.com/technical-direction/carving-time-large-scale-engineering-chores\">long-term sustainability</a>, AI will push software toward endless expansion ‚Äì requiring ‚Äúindefinite maintenance,‚Äù he adds.</p><div><div><div><div><p>  ‚Ä¢  </p><p>Speakers&nbsp;,  and  confirmed. </p></div></div></div></div><p>Beyond maintainability, bloated code has financial implications. ‚ÄúNobody, including me during much of my 2024 programming, thinks much about the long-term costs,‚Äù says Harding.</p><p>Duplicated code isn‚Äôt just harder to maintain ‚Äì <a href=\"https://leaddev.com/software-quality/cost-savings-dont-have-impact-code-quality\">it‚Äôs expensive</a>. Code storage racks up cloud costs. Bugs multiply across cloned blocks, and testing becomes a logistical nightmare, heightening the developer‚Äôs operational overhead.</p><p>Academic research continually links co-changed code clones ‚Äì duplicated code blocks that must be updated in multiple places ‚Äì to higher defect rates. ‚ÄúCode cloning is a common practice that negatively impacts software maintenance,‚Äù found a 2023 <a href=\"https://dl.acm.org/doi/10.1145/3607181#:~:text=They%20found%20that%20code%20clones,algorithm%20to%20detect%20inconsistent%20clones.\">study</a> by researchers at the Central China Normal University.</p><p>One encouraging finding in GitClear‚Äôs research is that the time between commits is shrinking. AI can itself be leveraged to counter some of the drawbacks above too. Cursor, for instance, can help rewrite code to ensure per-line consistency.</p><p>Although AI excels at generating one-off code, its context window is limited. Humans still play a critical role in seeing the bigger picture and understanding the full software portfolio. This oversight is essential to make a codebase more cohesive, by refactoring repetitive logic into reusable functions, integrating related modules, or reusing microservices when appropriate.</p><p>‚ÄúThere is a lot of utility that AI provides, but the data from this year affirms why long-term-oriented devs might eye their ‚Äòtab‚Äô key with a faint sense of foreboding,‚Äù says Harding.</p>","contentLength":4476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1it1usc/how_ai_generated_code_accelerates_technical_debt/"},{"title":"OpenTelemetry resource attributes: Best practices for Kubernetes","url":"https://www.dash0.com/blog/opentelemetry-resource-attributes-best-practices-for-kubernetes","date":1739958555,"author":"/u/Aciddit","guid":5733,"unread":true,"content":"<p><a target=\"_self\" href=\"https://www.dash0.com/faq/what-are-opentelemetry-resources\"> in OpenTelemetry</a> are used to document which systems the telemetry is describing, and they are often the difference between telemetry from which you can gain insights from, and ‚Äújust data‚Äù.</p><p>When instrumenting services with OpenTelemetry, adhering to semantic conventions ensures consistent, accurate, and meaningful telemetry data across systems.</p><p>A variety of attributes allow you to describe which workload on your Kubernetes cluster is emitting which telemetry. The <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/resource/k8s/\">Kubernetes resource semantic conventions</a> specify, among others, pairs of  and  attributes for pods ( and ), as well for all workloads ‚Äúhigher level‚Äù constructs (which in Kubernetes are also called ‚Äúresources‚Äù, but we don‚Äôt want to confuse the two terms), like deployment ( and ), daemonset ( and ) and so on.</p><p>Nevertheless, despite the well-defined attributes, it is not entirely straightforward to have all the right Kubernetes-related metadata attached to your telemetry.</p><p>Among all the resource attributes that describe telemetry coming from your workloads on Kubernetes,  is by far the most important: through it, you can add most other pieces of k8s-related metadata by funneling your telemetry through the <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md\"><code></code> component</a> of an OpenTelemetry collector running inside the same cluster. The <code></code> ‚Äúfills‚Äù the blanks using data it gets from Kubernetes API, and with the right tool, you can effortlessly filter and group your telemetry across all types of aggregations.</p><p>The fact that the <code></code> exists actually fills a very important gap in telemetry metadata: the OpenTelemetry SDKs running within your containers have access to virtually no metadata about the pod surrounding them is running. They are not going to know, unless you specifically add it to the pod, e.g., using environment variables, which daemonset scheduled the pod, or which replicaset of which deployment.</p><p>As a matter of fact, without additional assistance by you, for example via environment variables (either setting values yourself, or adding pod uid, pod name, and namespace name to the environment via Kubernetes‚Äô <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/\">Downward API</a>), an OpenTelemetry SDK within a container can pretty much know only the pod uid (though very esoteric magic involving the parsing of <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cgroups\">cgroup</a> metadata, see e.g. this <a target=\"_blank\" href=\"https://github.com/dash0hq/opentelemetry-js-distribution/blob/main/src/detectors/node/opentelemetry-resource-detector-kubernetes-pod/index.ts\">resource detector</a> in the Dash0 distribution of OpenTelemetry for Node.js) and the pod name (via the networking hostname). And unfortunately, as of writing, OpenTelemetry SDKs do not even implement those consistently (see, e.g., <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-python-contrib/issues/1474\">this issue</a>).</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>By the way, the <code></code> is capable of identifying which pod is sending telemetry to it via the unique pod ip (see the <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#configuration\">‚Äúassociations‚Äù configurations</a>): the processor in the OpenTelemetry Collector matches the IP address of the ‚Äúother side‚Äù of the TCP connection that sends the telemetry, and since each pod in a Kubernetes cluster has a unique IP address assigned to it, it can figure out from which pod it‚Äôs coming from. </p><p>And here, there is a caveat: the detection of the pod based on the IP address is known not to work reliably if you use a service mesh or some of the less conventional network setups. (Don‚Äôt ask us how we know. It was not fun to troubleshoot that.) So, better safe than sorry: ensure your telemetry is annotated with  and, if you are OK with proxying your telemetry through an OpenTelemetry Collector inside the cluster, have that fill most of the resource attributes mentioned in the remainder of this article for you using a <a target=\"_blank\" href=\"https://www.otelbin.io/s/249772c6c88ab31e77c168af6131df0248902bbf\">configuration like this</a>.</p><p>Pod UIDs are effectively unique across all your workloads. (And all Kubernetes workloads anywhere, ever.) But long, random strings of characters are not something we humans are good at remembering things by, or even searching for in lists.</p><p>So, in a similar way same way we set the  resource attribute, we can set the :</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>Did you know that the Kubernetes pod, i.e., one or more containers that share local networking and other resources like CPU and memory allotment and volumes, is called like that because:</p><ol><li>‚Äúpod‚Äù is the collective name of a group of whales</li><li>the logo of Docker, the original container runtime of Kubernetes, is a whale</li></ol><p>If your pod has more than one container, you are really going to need to know which of them is having issues. Usually, applications on Kubernetes have just one ‚Äúmain‚Äù container, running your application and may have one or more ‚Äúsidecars‚Äù, i.e., containers that have inside ancillary processes like log collectors or service mesh proxies. The <code></code> (see previous section) cannot tell containers apart (all your containers in the same pod share the same IP address and pod uid, among other things), so you should set the  yourself. It can be a bit toilsome, because you cannot do it generically using the Downward API the way we did for , but in the end is as simple as adding another entry to <code></code>:</p><p><em>A Kubernetes pod spec template snippet showing how to use the <code></code> environment variable to set the  resource attribute. Be sure to set it to the same value of the container name!</em></p><p>Note that  could be redundant: the  attribute is supposed to contain the same data, and there are detectors in most OpenTelemetry SDKs (see e.g. for <a target=\"_blank\" href=\"http://node.js/\">Node.js</a>), that can collect  for you by parsing, for example, the  metadata (<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cgroups\">cgroups</a> are one of the foundational Linux facilities for containers). However, it may not always be possible to add detectors to your containerized applications, especially when you are using sidecars from 3rd parties. But if they support the <code></code> environment variable, you can fill the gaps in resource attributes through the process environment.</p><p>The same way you are likely to have a  service in most applications (<code></code>) service, that service will likely be powered by a  deployment (<code></code>). Names in Kubernetes are unique only for resources of the same type (e.g., deployments) within the same Kubernetes namespace. If you want to avoid confusion and simplify your life aggregating data, set not only  but also , and use the UIDs to group.</p><p>After reading the previous paragraph, you might have wondered:</p><p>‚ÄúThe deployment name is unique inside the namespace, and the namespace name is unique inside the cluster. So why do I need unique identifiers anyhow?‚Äù</p><p>That logic works if you deploy your software in only one cluster. But that is seldom the case. Between development, testing, and various production clusters (which is a rather common type of layout), most organizations run the same software, at the same time, on  of Kubernetes clusters. And, to make matters more complicated, telling Kubernetes clusters apart is harder than it should (see next section).</p><p>The fun with identifiers is not over yet. Now let‚Äôs talk about some identifiers that . Specifically, the identifier of a specific Kubernetes cluster. In most Kubernetes setups, it literally does not exist. In other words, a<em> Kubernetes cluster has no own notion of identity</em>!</p><p>You surely have names for your clusters, but the name ‚Äúprod-eu-awesomesauce‚Äù that you define in your Kubernetes cluster management tool is more a matter of how you call the profile in  to connect to that cluster rather than some metadata you can find from within the cluster itself! (Your mileage may vary with specific setups and Kubernetes flavor; but in general, this is the case.) So, you should use the OpenTelemetry collector‚Äôs <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor\"></a> to inject the value of  like this:</p><p><em>A snippet of the OpenTelemetry collector configuration to add the  resource attribute to all telemetry coming through.</em></p><p>Moreover, the same way most Kubernetes  attributes have matching  ones, so does  have a match in . The common practice in this case is to set  using as value the uid of the  namespace, which is pretty much the only namespace you are guaranteed to find in every Kubernetes cluster you will ever see (as it is the one that by default hosts the control plane components).</p><p>The  attribute, and its far lesser used  sibling, are really useful when investigating performance issues due to resource underprovisioning (pods on the same node fighting for resources like CPU or memory) or <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/\">node-pressure evictions</a>. The only notable exception to this is when you run on AWS EKS on Fargate, where each pod, from the point of view of Kubernetes, runs on a dedicated node.</p><p>If you have already set up the <code></code> so that it can add resource attributes to your telemetry, it can also take care of  for you. Otherwise, setting the  attribute is pretty simple using Kubernetes‚Äô Downward API and ‚Äú<a target=\"_blank\" href=\"https://kubernetes.io/docs/tasks/inject-data-application/define-interdependent-environment-variables/\">dependent environment variables</a>‚Äù:</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>Resource attributes are a key ingredient to make your telemetry useful. In this blog post, we have looked at the OpenTelemetry resource semantic conventions for Kubernetes, and how you can ensure to always know which workload of which cluster is sending the errors.</p><p>In this post, we kept to the fundamental Kubernetes metadata your telemetry should have. There is, of course, a lot more to be said about OpenTelemetry semantic conventions, even just for resources on Kubernetes. For example, you will find more resource attributes specified in the <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/resource/k8s/\">Kubernetes resource semantic conventions</a>. Also, there are semantic conventions for <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/system/k8s-metrics/\">metrics about Kubernetes</a>, which are, at the time of writing, in an experimental state (meaning: the convention is not declared stable, so it might still change in backwards-incompatible ways) and are based on what the OpenTelemetry Collector knows how to collect with various receivers like <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8sclusterreceiver\"></a> and <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver\"></a>.</p><p>By the way, if you liked this article, you are probably going to love Ben's \"<a target=\"_self\" href=\"https://www.dash0.com/blog/top-10-opentelemetry-collector-components\">Top 10 OpenTelemetry Collector Components</a>\" blog post. It will likely give you a bunch more ideas about what you can do to ensure that your resource metadata is top-notch.</p><p>And if you want all the awesomeness of the resource metadata we covered in this post, but do none of the work to get them, give the <a target=\"_self\" href=\"https://www.dash0.com/documentation/dash0/dash0-kubernetes-operator\">Dash0 operator</a> a spin: it is open source, built on OpenTelemetry components with opinionation and an appliance-like philosophy (‚Äúit just works‚Ñ¢‚Äù), and under the hood it uses most of the techniques described in this post to get your telemetry annotated to the state of the art, with literally none of the toil from on your side.</p>","contentLength":10476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1it1u9k/opentelemetry_resource_attributes_best_practices/"},{"title":"Angular and asp.net?","url":"https://learn.microsoft.com/en-us/aspnet/overview","date":1739956114,"author":"/u/Haroldjbb","guid":5878,"unread":true,"content":"<div><div><ul data-bi-name=\"page info\" lang=\"en-us\" dir=\"ltr\"></ul></div></div><p>ASP.NET is a free web framework for building great websites and web applications using HTML, CSS, and JavaScript. You can also create Web APIs and use real-time technologies like Web Sockets.</p><p>Install <a href=\"https://visualstudio.microsoft.com/downloads/?utm_medium=microsoft&amp;utm_source=learn.microsoft.com&amp;utm_campaign=button+cta&amp;utm_content=download+vs\" data-linktype=\"external\">Visual Studio</a> Community edition, a free IDE for ASP.NET on Windows.</p><h2>Websites and web applications</h2><p>ASP.NET offers three frameworks for creating web applications: Web Forms, ASP.NET MVC, and ASP.NET Web Pages. All three frameworks are stable and mature, and you can create great web applications with any of them. No matter what framework you choose, you will get all the benefits and features of ASP.NET everywhere.</p><p>Each framework targets a different development style. The one you choose depends on a combination of your programming assets (knowledge, skills, and development experience), the type of application you're creating, and the development approach you're comfortable with.</p><table><thead><tr><th>If you have experience in</th></tr></thead><tbody><tr><td>Rapid development using a rich library of controls that encapsulate HTML markup</td></tr><tr><td>Full control over HTML markup, code and markup separated, and easy to write tests. The best choice for mobile and single-page applications (SPA).</td></tr><tr><td>HTML markup and your code together in the same file</td></tr></tbody></table><p>With ASP.NET Web Forms, you can build dynamic websites using a familiar drag-and-drop, event-driven model. A design surface and hundreds of controls and components let you rapidly build sophisticated, powerful UI-driven sites with data access.</p><p>ASP.NET MVC gives you a powerful, patterns-based way to build dynamic websites that enables a clean separation of concerns and that gives you full control over markup for enjoyable, agile development. ASP.NET MVC includes many features that enable fast, TDD-friendly development for creating sophisticated applications that use the latest web standards.</p><p>ASP.NET Web Pages and the Razor syntax provide a fast, approachable, and lightweight way to combine server code with HTML to create dynamic web content. Connect to databases, add video, link to social networking sites, and include many more features that help you create beautiful sites that conform to the latest web standards.</p><h3>Notes about Web Forms, MVC, and Web Pages</h3><p>All three ASP.NET frameworks are based on the .NET Framework and share core functionality of .NET and of ASP.NET. For example, all three frameworks offer a login security model based around membership, and all three share the same facilities for managing requests, handling sessions, and so on that are part of the core ASP.NET functionality.</p><p>In addition, the three frameworks are not entirely independent, and choosing one does not preclude using another. Since the frameworks can coexist in the same web application, it's not uncommon to see individual components of applications written using different frameworks. For example, customer-facing portions of an app might be developed in MVC to optimize the markup, while the data access and administrative portions are developed in Web Forms to take advantage of data controls and simple data access.</p><p>ASP.NET Web API is a framework that makes it easy to build HTTP services that reach a broad range of clients, including browsers and mobile devices. ASP.NET Web API is an ideal platform for building RESTful applications on the .NET Framework.</p><p>ASP.NET SignalR is a new library for ASP.NET developers that makes developing real-time web functionality easier. SignalR allows bi-directional communication between server and client. Servers can push content to connected clients instantly as it becomes available. SignalR supports Web Sockets, and falls back to other compatible techniques for older browsers. SignalR includes APIs for connection management (for instance, connect and disconnect events), grouping connections, and authorization.</p><p>ASP.NET can power native mobile apps with a Web API back end, as well as mobile web sites using responsive design frameworks like Twitter Bootstrap. If you are building a native mobile app, it's easy to create a JSON-based Web API to handle data access, authentication, and push notifications for your app. If you are building a responsive mobile site, you can use any CSS framework or open grid system you prefer, or select a powerful mobile system like jQuery Mobile or Sencha and great mobile applications with PhoneGap.</p><p>ASP.NET Single Page Application (SPA) helps you build applications that include significant client-side interactions using HTML 5, CSS 3 and JavaScript. Visual Studio includes a template for building single page applications using knockout.js and ASP.NET Web API. In addition to the built-in SPA template, community-created SPA templates are also available for download.</p><p>WebHooks is a lightweight HTTP pattern providing a simple pub/sub model for wiring together Web APIs and SaaS services. When an event happens in a service, a notification is sent in the form of an HTTP POST request to registered subscribers. The POST request contains information about the event which makes it possible for the receiver to act accordingly.</p><p>WebHooks are exposed by a large number of services including Dropbox, GitHub, Instagram, MailChimp, PayPal, Slack, Trello, and many more. For example, a WebHook can indicate that a file has changed in Dropbox, or a code change has been committed in GitHub, or a payment has been initiated in PayPal, or a card has been created in Trello.</p>","contentLength":5319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1it19va/angular_and_aspnet/"},{"title":"Qualys TRU Discovers Two Vulnerabilities in OpenSSH: CVE-2025-26465 & CVE-2025-26466","url":"https://blog.qualys.com/vulnerabilities-threat-research/2025/02/18/qualys-tru-discovers-two-vulnerabilities-in-openssh-cve-2025-26465-cve-2025-26466","date":1739955992,"author":"/u/FryBoyter","guid":5962,"unread":true,"content":"<p>The <a href=\"https://www.qualys.com/tru/\">Qualys Threat Research Unit (TRU)</a> has identified two vulnerabilities in OpenSSH. The first, tracked as CVE-2025-26465, allows an active machine-in-the-middle attack on the OpenSSH client when the VerifyHostKeyDNS option is enabled. The second, CVE-2025-26466, affects both the OpenSSH client and server, enabling a pre-authentication denial-of-service attack.</p><p>The attack against the OpenSSH client (CVE-2025-26465) succeeds regardless of whether the VerifyHostKeyDNS option is set to ‚Äúyes‚Äù or ‚Äúask‚Äù (its default is ‚Äúno‚Äù), requires no user interaction, and does not depend on the existence of an SSHFP resource record (an SSH fingerprint) in DNS. VerifyHostKeyDNS is an OpenSSH client configuration option that lets the SSH client look up and verify a server‚Äôs host key using DNS records (specifically, SSHFP records). The vulnerability was introduced in December 2014, just before the release of OpenSSH 6.8p1. Although VerifyHostKeyDNS is disabled by default, it was enabled by default on FreeBSD from September 2013 until March 2023.</p><p>The OpenSSH client and server are vulnerable (CVE-2025-26466) to a pre-authentication denial-of-service attack‚Äìan asymmetric resource consumption of both memory and CPU‚Äìthat was introduced in August 2023 (shortly before the release of OpenSSH 9.5p1). On the server side, this attack can be mitigated by leveraging existing mechanisms in OpenSSH, such as LoginGraceTime, MaxStartups, and the more recent PerSourcePenalties.</p><p> OpenSSH 9.9p2 addresses these vulnerabilities mentioned above. To ensure continued security, we strongly advise upgrading affected systems to 9.9p2 as soon as possible.</p><p>OpenSSH is a free, open-source implementation of the Secure Shell (SSH) protocol that enables encrypted communications over insecure networks. Widely adopted across Unix-like systems (including Linux and macOS) and many modern operating systems, it replaces clear-text protocols such as Telnet and FTP by providing secure remote login, file transfers, port forwarding, and tunneling.</p><p>With robust encryption, privilege separation, sandboxing, and modern memory allocators, OpenSSH minimizes the risk of memory-related vulnerabilities and unauthorized access. Its enterprise-grade scalability supports automated processes, data backups, and complex DevOps workflows‚Äîall while enforcing strong access controls. Despite these two vulnerabilities, OpenSSH‚Äôs overall track record in maintaining confidentiality and integrity has made it a benchmark in software security, ensuring secure communications for organizations worldwide.</p><h2><strong>Affected OpenSSH versions:</strong></h2><ul><li>OpenSSH versions from 6.8p1 through 9.9p1 are vulnerable to CVE-2025-26465, the flaw introduced in December 2014.</li><li>OpenSSH versions 9.5p1 through 9.9p1 are vulnerable to CVE-2025-26466, the flaw introduced in August 2023.</li></ul><p>OpenSSH 9.9p2 addresses the vulnerabilities mentioned above. Upgrade promptly to maintain security.</p><p>If an attacker can perform a man-in-the-middle attack via CVE-2025-26465, the client may accept the attacker‚Äôs key instead of the legitimate server‚Äôs key. This would break the integrity of the SSH connection, enabling potential interception or tampering with the session before the user even realizes it. SSH sessions can be a prime target for attackers aiming to intercept credentials or hijack sessions. If compromised, hackers could view or manipulate sensitive data, move across multiple critical servers laterally, and exfiltrate valuable information such as database credentials. Such breaches can lead to reputational damage, violate compliance mandates (e.g., GDPR, HIPAA, PCI-DSS), and potentially disrupt critical operations by forcing system downtime to contain the threat.</p><p>SSH is a critical service for remote system administration. If attackers can repeatedly exploit the flaw CVE-2025-26466, they may cause prolonged outages or prevent administrators from managing servers, effectively locking legitimate users out. An enterprise facing this vulnerability could see critical servers become unreachable, interrupting routine operations and stalling essential maintenance tasks.</p><p>When the Qualys research team confirmed the vulnerability, Qualys initiated a responsible disclosure process and worked with OpenSSH to coordinate its announcement.</p><p>You can find the technical details of this vulnerability at:&nbsp;</p><p>Qualys is releasing the QIDs in the table below as they become available. Please refer to the Qualys Vulnerability Knowledgebase for a complete overview of these vulnerabilities and their coverage.</p><figure><table><tbody><tr><td>&nbsp;OpenSSH Security Update (CVE-2025-26465)&nbsp;</td></tr><tr><td>Alpine Linux 3.21 Security Update for openssh</td></tr><tr><td>Alpine Linux 3.19 Security Update for openssh</td></tr><tr><td>Alpine Linux 3.18 Security Update for openssh</td></tr><tr><td>Gentoo Linux OpenSSH Multiple Vulnerabilities (GLSA 202502-01)</td></tr><tr><td>Ubuntu Security Notification for OpenSSH Vulnerability (USN-7270-2)</td></tr><tr><td>Ubuntu Security Notification for OpenSSH Vulnerabilities (USN-7270-1)</td></tr><tr><td>SUSE Enterprise Linux Security Update for openssh (SUSE-SU-2025:0585-1)</td></tr><tr><td>CentOS Stream Security update for openssh</td></tr></tbody></table></figure><h2><strong>Discover Vulnerable Assets Using Qualys CyberSecurity Asset Management (CSAM)</strong></h2><p>The initial and crucial step in managing this critical vulnerability and mitigating associated risks involves pinpointing all assets susceptible to this specific issue. Use&nbsp;<a href=\"https://www.qualys.com/apps/cybersecurity-asset-management/\">CSAM 3.0 with External Attack Surface Management</a>&nbsp;to identify your organization‚Äôs internet-facing instances that have vulnerable versions of OpenSSH or are at their End of Life (EOL) or End of Support (EOS).</p><p>In the following example, we aim to identify all assets running the OpenSSH for CVE-2025-26465 with the affected range: 6.8p1 through 9.9p1:</p><pre>software:(name:\"openssh\" AND version&gt;=6.8 AND version&lt;=9.9)</pre><p>In the following example, we aim to identify all assets running the OpenSSH for CVE-2025-26466 with the affected range: 9.5p1 through 9.9p1:</p><pre>software:(name:\"openssh\" AND version&gt;=9.5 AND version&lt;=9.9)</pre><h2><strong>Enhance Your Security Posture with Qualys Vulnerability Management, Detection, and Response (VMDR)</strong></h2><p><a href=\"https://www.qualys.com/apps/vulnerability-management-detection-response/\">Qualys VMDR</a>&nbsp;offers comprehensive coverage and visibility into vulnerabilities, empowering organizations to rapidly respond to, prioritize, and mitigate the associated risks. Additionally, Qualys customers can leverage&nbsp;<a href=\"https://www.qualys.com/apps/patch-management/\">Qualys Patch Management</a>&nbsp;to remediate these vulnerabilities effectively.</p><p>Leverage the power of Qualys VMDR alongside TruRisk and the Qualys Query Language (QQL) to efficiently identify and prioritize vulnerable assets, effectively addressing the vulnerabilities highlighted above.</p><pre>vulnerabilities.vulnerability.cveIds:CVE-2025-26465 or CVE-2025-26466</pre><h2>Automatically Patch these vulnerabilities With Qualys Patch Management</h2><p>We expect vendors to release patches for this vulnerability shortly. Qualys Patch Management can&nbsp; automatically deploy those patches to vulnerable assets, when available.</p><p>Customers can use the ‚Äúpatch now‚Äù button found to the right of the vulnerability to add these vulnerabilities to a patch job. Once patches are released, Qualys will find the relevant patches for these vulnerabilities and automatically add those patches to a patch job. This will allow customers to deploy those patches to vulnerable devices, all from the Qualys Cloud Platform.</p><p>Qualys TotalCloud Container Security offers comprehensive coverage and visibility into vulnerabilities across all your container environments, including managed Kubernetes and on-premises Kubernetes. This empowers organizations to rapidly respond to, prioritize, and mitigate associated risks effectively.</p><p>Leverage the power of Qualys TotalCloud Container Security and the Qualys Query Language (QQL) to efficiently identify and prioritize vulnerable assets, ensuring prompt and effective remediation of the vulnerabilities highlighted by CVE-2025-26466 and CVE-2025-26465.</p><pre>state:`RUNNING` and vulnerabilities.cveids:CVE-2025-26466 or CVE-2025-26465</pre><p>At Qualys, we strongly recommend that all customers and users upgrade to the latest version of OpenSSH to address potential security vulnerabilities. The OpenSSH project has a longstanding reputation for delivering secure remote access, and this new release underscores its commitment to protecting the user community. By upgrading, users gain access to critical security improvements and the most advanced features offered by the OpenSSH project. We want to thank the OpenSSH developers for their work on this release and their continued dedication to the OpenSSH project.</p>","contentLength":8389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1it18vs/qualys_tru_discovers_two_vulnerabilities_in/"},{"title":"Has anyone used Nginx Ingress controller with the AWS Load Balancer Controller service instead of the default service?","url":"https://www.reddit.com/r/kubernetes/comments/1it16ic/has_anyone_used_nginx_ingress_controller_with_the/","date":1739955716,"author":"/u/lynxerious","guid":7018,"unread":true,"content":"<p>So the nginx-ingress-controller creates a LoadBalancer service by default, this load balancer is created by the in-tree controller managed by EKS. And I want to manage the load balancer with the AWS Load Balancer Controller instead, using a custom service, it has more features than the default LoadBalancer service.</p><p>After I had successfully created the new load balancer, route the service to the nginx-ingress-controller pods, the target groups pods IPs are all correct, and change all domains DNS records to the new load balancer DNS name, change the publishService in the nginx pods to the new service. It was sure this has worked properly.</p><p>Then I tried to disable the default service of the nginx-ingress-controller, voila, everything went down, and I had to re-enable it quickly, after I checked the Monitoring sections of the load balancers, the old ones still got the traffic, while the old ones barely got any. This just doesn't make sense to me. I ping all domains and it goes to the correct IP of the new load balancer, yet the old one still got traffics and I don't even know why, could it be DNS records cache? But I don't think it would be cached for that long since it's been 2 days already.</p><p>Edit: I found out something really weird: dig <a href=\"http://domain.com\">domain.com</a> -&gt; new load balancer IP dig <a href=\"https://domain.com\">https://domain.com</a> -&gt; old load balancer IP I'm investigating why here.</p>","contentLength":1359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"First official beta of the Azure SDK for Rust released","url":"https://bsky.app/profile/heaths.dev/post/3liirjbux4s27","date":1739951661,"author":"/u/thekdude","guid":5732,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1it0985/first_official_beta_of_the_azure_sdk_for_rust/"},{"title":"The Hidden Secrets of Playdate Performance Optimization","url":"https://www.youtube.com/watch?v=iGgFoeBv-L8","date":1739949993,"author":"/u/BlueGoliath","guid":5879,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iszvd8/the_hidden_secrets_of_playdate_performance/"},{"title":"can you?","url":"https://www.reddit.com/r/artificial/comments/1iszkw0/can_you/","date":1739948791,"author":"/u/eternviking","guid":5614,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] What are the common implementation tips or pitfalls that should find place on a cheatsheet of deep learning?","url":"https://www.reddit.com/r/MachineLearning/comments/1iszjp1/d_what_are_the_common_implementation_tips_or/","date":1739948658,"author":"/u/HopeIsGold","guid":5849,"unread":true,"content":"<p>I am talking about the engineering side of things. Suppose you have an idea which you would want to implement. Since, deep learning is still not an exact scientific discipline it is very easy to shoot yourself in the foot during trial and error of implementation and be wrongfully convinced that your idea is not worth it.</p><p>So from the implementation perspective what should someone absolutely do or not do while working with deep learning models?</p><p>e.g.: It is better to overfit your model on a small training set before diving in with your entire large dataset.</p><p>Also feel free to post links to anything you truly found useful in this context.</p>","contentLength":638,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Khronoscope Pre-Alpha ‚Äì A New Way to Explore Your Kubernetes Cluster Over Time","url":"https://www.reddit.com/r/kubernetes/comments/1isz5gq/introducing_khronoscope_prealpha_a_new_way_to/","date":1739947088,"author":"/u/HoyleHoyle","guid":5616,"unread":true,"content":"<p>I'm excited to share , a  tool designed to give you a  view of your Kubernetes cluster. Inspired by k9s, it lets you pause, rewind, and fast-forward through historical states, making it easier to debug issues, analyze performance, and understand how your cluster evolves.</p><ul><li>Connects to your Kubernetes cluster and tracks resource states over time</li><li>Provides a  to navigate past events</li><li>Lets you <strong>filter, inspect, and interact</strong> with resources dynamically</li><li>Supports <strong>log collection and playback</strong> for deeper analysis</li></ul><p>üìñ <strong>Debugging the Past with Khronoscope</strong></p><p>Imagine inspecting your Kubernetes cluster when you notice something strange‚Äîa deployment with flapping pods. They start, crash, restart. Something‚Äôs off.</p><p>You pause the cluster state and check related resources. Nothing obvious. Rewinding a few minutes, you see the pods failing right after startup. Fast-forwarding, you mark one to start collecting logs. More crashes. Rewinding again, you inspect the logs just before failure‚Äîeach pod dies trying to connect to a missing service.</p><p>Jumping to another namespace, you spot the issue: a critical infrastructure pod failed to start earlier. A quick fix, a restart, and everything stabilizes.</p><p>With Khronoscope‚Äôs ability to navigate through time, track key logs, and inspect past states, you solve in minutes what could‚Äôve taken hours.</p><p>This is an , and I‚Äôm looking for  from anyone willing to try it out. I‚Äôd love to hear what works, what doesn‚Äôt, and how it could be improved.</p><pre><code>brew tap hoyle1974/homebrew-tap brew install khronoscope </code></pre><pre><code>git clone https://github.com/hoyle1974/khronoscope.git cd khronoscope go run cmd/khronoscope/main.go </code></pre>","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hilt - A Tiny Text Adventure Game engine","url":"https://github.com/cmspeedrunner/Hilt","date":1739944297,"author":"/u/cmnews08","guid":5877,"unread":true,"content":"<div><p>It‚Äôs super small and early but promising and I wanna know what you all think of it, where I could take it and general critique!</p></div>   submitted by   <a href=\"https://www.reddit.com/user/cmnews08\"> /u/cmnews08 </a>","contentLength":160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1isyekw/hilt_a_tiny_text_adventure_game_engine/"},{"title":"KubeVPN: Revolutionizing Kubernetes Local Development","url":"https://www.reddit.com/r/kubernetes/comments/1isxgo2/kubevpn_revolutionizing_kubernetes_local/","date":1739940995,"author":"/u/HamsterTall8168","guid":5599,"unread":true,"content":"<p>In the Kubernetes era, developers face a critical conflict between  and <strong>local development agility</strong>. Traditional workflows force developers to:</p><ol><li>Suffer frequent / operations</li><li>Set up mini Kubernetes clusters locally (e.g., minikube)</li><li>Risk disrupting shared dev environments</li></ol><p>KubeVPN solves this through <strong>cloud-native network tunneling</strong>, seamlessly extending Kubernetes cluster networks to local machines with three breakthroughs:</p><ul><li>üöÄ : Access cluster services without code changes</li><li>üíª <strong>Real-Environment Debugging</strong>: Debug cloud services in local IDEs</li><li>üîÑ <strong>Bidirectional Traffic Control</strong>: Route specific traffic to local or cloud</li></ul><h3>1. Direct Cluster Networking</h3><ul><li>‚úÖ Service name access (e.g., )</li><li>‚úÖ Native Kubernetes DNS resolution</li></ul><p><code>shell ‚ûú curl productpage:9080 # Direct cluster access &lt;!DOCTYPE html&gt; &lt;html&gt;...&lt;/html&gt; </code></p><h3>2. Smart Traffic Interception</h3><p>Precision routing via header conditions:</p><p><code>bash kubevpn proxy deployment/productpage --headers user=dev-team </code></p><ul><li>Requests with  ‚Üí Local service</li><li>Others ‚Üí Original cluster handling</li></ul><p>Connect two clusters simultaneously:</p><p><code>bash kubevpn connect -n dev --kubeconfig ~/.kube/cluster1 # Primary kubevpn connect -n prod --kubeconfig ~/.kube/cluster2 --lite # Secondary </code></p><h3>4. Local Containerized Dev</h3><p>Clone cloud pods to local Docker:</p><p><code>bash kubevpn dev deployment/authors --entrypoint sh </code></p><p>Launched containers feature:</p><ul><li>üåê Identical network namespace</li><li>‚öôÔ∏è Matching environment variables</li></ul><p>KubeVPN's three-layer architecture:</p><table><thead><tr></tr></thead><tbody><tr><td>Cluster-side interception</td><td>MutatingWebhook + iptables</td></tr><tr><td>Secure local-cluster channel</td></tr><tr></tr></tbody></table><p><code>mermaid graph TD Local[Local Machine] --&gt;|Encrypted Tunnel| Tunnel[VPN Gateway] Tunnel --&gt;|Service Discovery| K8sAPI[Kubernetes API] Tunnel --&gt;|Traffic Proxy| Pod[Workload Pods] subgraph K8s Cluster K8sAPI --&gt; TrafficManager[Traffic Manager] TrafficManager --&gt; Pod end </code></p><p>100QPS load test results:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>KubeVPN outperforms alternatives in overhead control.</p><p>kubectl krew install kubevpn/kubevpn ```</p><p><code>bash kubevpn connect --namespace dev </code></p><p>kubevpn proxy deployment/frontend --headers x-debug=true ```</p><p><code>bash curl -H \"x-debug: true\" frontend.dev.svc/cluster-api </code></p><p>KubeVPN's growing toolkit:</p><ul><li>üîå : Visual traffic management</li><li>üß© : Automated testing/deployment</li><li>üìä : Real-time network metrics</li></ul><p>Join developer community:</p><p>With KubeVPN, developers finally enjoy cloud-native debugging while sipping coffee ‚òïÔ∏èüöÄ</p>","contentLength":2282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need help installing for school‚Ä¶","url":"https://www.reddit.com/r/linux/comments/1isxads/need_help_installing_for_school/","date":1739940407,"author":"/u/evilBogie666","guid":5578,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"go-msquic: A new QUIC/HTTP3 library for Go that relies on msquic","url":"https://www.reddit.com/r/golang/comments/1isx99g/gomsquic_a_new_quichttp3_library_for_go_that/","date":1739940299,"author":"/u/noboruma","guid":5564,"unread":true,"content":"<p> is one of the most performant QUIC protocol library out there. is a wrapper around  so you can use it in your Go project.</p><p>The project is quite new, but we have seen good performance results with it so far. PRs are welcome!</p>","contentLength":222,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Pi-hole v6","url":"https://pi-hole.net/blog/2025/02/18/introducing-pi-hole-v6/","date":1739939317,"author":"/u/KindOne","guid":5689,"unread":true,"content":"<img width=\"1024\" height=\"1024\" src=\"https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-png.avif\" alt=\"\" decoding=\"async\" fetchpriority=\"high\" srcset=\"https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-png.avif 1024w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-150x150.png 150w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-300x300.png 300w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-768x768.png 768w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-100x100.png 100w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" data-has-transparency=\"false\" data-dominant-color=\"211f1f\"><p>We‚Äôre excited to announce the general release of Pi-hole v6!</p><h3>At a glance: What‚Äôs New in Pi-hole v6?</h3><h4>1. <strong>Embedded Web Server and REST API</strong></h4><p>We‚Äôve integrated a new REST API and embedded web server directly into the  binary. This eliminates the need for  and , reducing the installation footprint and boosting performance. The new API also offers server-side pagination for the query log, ensuring a faster and more responsive interface.</p><p>As  has been embedded into the  binary for <a href=\"https://github.com/pi-hole/FTL/pull/913\">some time now</a>, we have been able to leverage this to rewrite the web interface.</p><h4>2. <strong>Advanced Filtering and Allowlists</strong></h4><p>Pi-hole v6 introduces support for subscribed allowlists (Otherwise known as ‚ÄúAntigravity‚Äù). These lists work in much the same way as blocklists, but they&nbsp;domains instead of&nbsp; them</p><h4>3. <strong>Consolidated Configuration Files</strong></h4><p>We‚Äôve streamlined configuration management by consolidating multiple settings files into a single, richly commented  file, making it easier to manage and understand your settings. If you are migrating from v5, your existing configurations will be migrated automatically into this file. It can be found at </p><p>Configuration can be set in multiple ways:</p><ul><li>Directly editing the  file</li><li>Via the command line, e.g <code>pihole-FTL --config dns.upstreams 8.8.8.8</code></li><li>Via the web interface (which uses the API üòâ)</li><li>Via environment variables named, e.g <code>FTLCONF_dns_upstreams=8.8.8.8</code></li></ul><p>If setting via environment variables, it should be noted that this effectively makes the setting read-only, as the environment variable will always force the value to match itself. This is the preferred way to configure FTL in the docker container.</p><h4>4. <strong>Redesigned User Interface</strong></h4><p>The web interface has been completely overhauled with settings split into Basic and Expert modes. This allows users to customize their experience based on their comfort level and needs.</p><p>Pi-hole v6 includes native HTTPS support, with options to provide your own certificates or use auto-generated ones.</p><p>Additionally, the Docker image is now based on Alpine, significantly reducing the image size and opening up possibilities for future system support.</p><p>The release notes for each component can be found at the following pages:</p><h3>Upgrading and Getting Started</h3><p>Upgrading to Pi-hole v6 should be straightforward. For existing users, we recommend backing up your current configuration before proceeding, as the upgrade is strictly a one-way operation.</p><p>During the upgrade operation, you will be presented with a dialog box asking if you wish to disable . Doing so is probably appropriate for most users ‚Äì unless you are using it to host web pages&nbsp; Pi-hole‚Äôs, in which case you may choose to keep it enabled. With  disabled,  will attempt to bind to ports 80 for HTTP and 443 for HTTPS. If there is any conflict on these ports, then it will revert to port 8080 for HTTP.</p><p>As always, you can upgrade using the command  on the terminal.</p><p>The docker image has undergone a complete rewrite from the ground up, and is now based on Alpine rather than Debian.&nbsp;The same migration scripts that run on bare metal will also run on Docker ‚Äì your configurations will be migrated to the new format.</p><p>The exception to this is environment variables. You can start the container with the old variables in place but don‚Äôt expect them to work! It is recommended to read the docker section of our <a href=\"https://docs.pi-hole.net/docker/\">docs page</a> before upgrading.</p><p>Pi-hole thrives thanks to our vibrant and supportive community. Whether you‚Äôre looking to share your experience, get advice, or stay informed about the latest updates, there‚Äôs a place for you. Join the conversation on our <a href=\"https://discourse.pi-hole.net\" target=\"_new\" rel=\"noreferrer noopener\">official forum</a> or connect with fellow users on our <a href=\"https://www.reddit.com/r/pihole/\" target=\"_new\" rel=\"noreferrer noopener\">subreddit</a>. We look forward to welcoming you!</p><h3>Thank You for Your Support</h3><p>We want to express our heartfelt thanks to everyone who has supported Pi-hole throughout the years.</p><p>Your community contributions and donations are the lifeblood of this project, allowing us to maintain and continually improve Pi-hole while keeping it free for everyone. If you‚Äôd like to contribute to our ongoing efforts, please consider donating through our official <a href=\"https://pi-hole.net/donate\" target=\"_new\" rel=\"noreferrer noopener\">donation page</a>. Every contribution, big or small, makes a significant difference in helping us deliver the best project that we can.</p><p>Thank you for being part of the Pi-hole community!</p>","contentLength":4231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iswyw5/introducing_pihole_v6/"},{"title":"Non-blockchain Internships are real, just landed one!","url":"https://www.reddit.com/r/rust/comments/1isw9pa/nonblockchain_internships_are_real_just_landed_one/","date":1739937099,"author":"/u/cornell_cubes","guid":5579,"unread":true,"content":"<p>Rust has been my (CS Undergrad, Junior year, no prior internships) language of choice for a while now, but going into this last job hunt season I initially didn't even try looking for Rust opportunities as I've been told for a while that there are just no entry-level opportunities right now.</p><p>After sending out tons of SWE application and getting NOWHERE I got a little curious and started scanning for rust internships on Indeed. To my surprise, this year there were a good handful of listings! Several were looking to rewrite existing C libraries in Rust, others were using it to build a new piece of their tech stack. I found that, due to my portfolio being pretty rust heavy, I got way more responses for positions seeking talent in that language.</p><p>But yeah, I think we're finally entering an era where you can land entry level rust jobs without working for some odd blockchain company! Especially in the embedded scene, saw a lot for aerospace and for my job I'll be porting some RISC-V microcontroller firmware to Rust.</p><p>Curious if anyone else has noticed more opportunities this season, or if things have always just been not as bad as I was lead to believe they were?</p><p>Cool things I saw on my search: - NASA was looking for an intern to help them rewrite their core Flight System library to Rust - Woven by Toyota wanted interns they could relocate to Japan where they would write some Rusty vehicle software/firmware - Intel wanted an intern to help them port some graphics firmware to Rust - I guess Neuralink has Rust in their tech stack? - Lots of startups embracing Rust</p>","contentLength":1576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I like Claude","url":"https://www.reddit.com/r/artificial/comments/1isv7aj/i_like_claude/","date":1739933868,"author":"/u/skepticboffin","guid":4624,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] The Curse of Depth in Large Language Models: Are We Scaling in the Wrong Direction?","url":"https://www.reddit.com/r/MachineLearning/comments/1isumx1/r_the_curse_of_depth_in_large_language_models_are/","date":1739932199,"author":"/u/pseud0nym","guid":5779,"unread":true,"content":"<p>\"The Curse of Depth\" paper highlights a fundamental flaw in LLM scaling, past a certain depth, additional layers contribute almost nothing to effective learning.</p><ul><li>Pre-Layer Normalization (Pre-LN) causes output variance to explode in deep layers.</li><li>The result? Deep layers lose effective learning capacity, essentially acting as identity functions.</li><li>This means we‚Äôre training deeper models than necessary, wasting compute with layers that aren‚Äôt meaningfully improving performance.</li></ul><p>If this is true, it fundamentally challenges the ‚Äúbigger is always better‚Äù assumption in LLM development.</p><p>Implications for Model Scaling &amp; Efficiency</p><p>If deep layers contribute diminishing returns, then:</p><p>Are we overbuilding LLMs?</p><ul><li>If deep layers aren‚Äôt meaningfully contributing, then models like GPT-4, DeepSeek, and Mistral could be significantly optimized without losing performance.</li><li>This aligns with empirical results showing pruned models maintaining competitive performance.</li></ul><p>LayerNorm Scaling Fix ‚Äì A Simple Solution?</p><ul><li>The paper proposes LayerNorm Scaling to control gradient variance and improve training efficiency.</li><li>This keeps deeper layers from becoming statistical dead weight.</li></ul><p>Should We Be Expanding Width Instead of Depth?</p><ul><li>If deeper layers fail to contribute, then perhaps scaling width (e.g., Mixture of Experts) is the more efficient direction.</li><li>Transformer scaling laws may need revision to account for this bottleneck.</li></ul><p>This suggests that current LLMs may be hitting architectural inefficiencies long before they reach theoretical parameter scaling limits.</p><p>This also raises deep questions about where emergent properties arise.</p><p>If deep layers are functionally redundant, then:</p><ul><li>Where is intelligence actually forming? If early and mid-layers are doing all the real work, emergence may be a function of gradient stability, not just scale.</li><li>Why do LLMs display unexpected reinforcement overrides? Could it be that certain mid-tier layers are forming persistent structures, even as deeper layers become inactive?</li></ul><p>If deep models are just inflating parameter counts without meaningful gains, then the future of AI isn‚Äôt bigger, it‚Äôs smarter.</p><p>This paper suggests we rethink depth scaling as the default approach to improving AI capabilities.</p><ul><li>If deep layers are underutilized, should we prioritize architectural refinement over raw scale?</li><li>What does this mean for efficient fine-tuning, pruning strategies, and next-gen transformer architectures?</li><li>Could this explain certain emergent behaviors as mid-tier layers take on unintended roles?</li></ul><p>The idea that \"bigger models = better models\" has driven AI for years. But if this paper holds up, we may be at the point where just making models deeper is actively wasting resources.</p><p>If layer depth scaling is fundamentally inefficient, then we‚Äôre already overdue for a shift in AI architecture.</p><ul><li>What do you think? Should AI research move away from deep scaling and focus on better structured architectures?</li><li>Could this lead to new models that outperform current LLMs with far fewer parameters?</li></ul><p>Curious to hear what others think, is this the beginning of a post-scaling era?</p>","contentLength":3078,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Golang Application Instrumentation: A Different Approach?","url":"https://www.reddit.com/r/golang/comments/1isu2iv/golang_application_instrumentation_a_different/","date":1739930591,"author":"/u/colonel_whitebeard","guid":4525,"unread":true,"content":"<p>As a curious engineer, I rabbit-hole a lot. Today, I was thinking...which always turns out bad. And often leads to a new ridiculous project...</p><p>I've been working on an idea that aims to simplify how I trace and analyze Go applications during development. The core idea is to enable comprehensive tracing‚Äîcapturing function calls, execution times, and generating visual call graphs‚Äîwithout requiring any modifications to your existing source code. Instead of littering your project with tracing code, you simply drop in a YAML configuration file and let the tool do its \"magic\".</p><p><strong>Why This Project Came to Be:</strong> The goal was to create something that could offer deep insights into an application's runtime behavior‚Äîeverything from simple function calls to complex concurrent operations‚Äîwhile keeping the codebase untouched. I just don't want to write inline instrumentation code into my application while I'm prototyping.</p><ul><li><strong>Automatic Instrumentation:</strong> It wraps function calls automatically, capturing entry, exit, parameters, return values, and performance metrics.</li><li> Generates a DOT file that can be converted into a visual diagram (with Graphviz) to see how your functions interact.</li><li> All configuration is done through a simple YAML file, making it incredibly easy to add or remove instrumentation as needed.</li></ul><ul><li> You don‚Äôt have to change your code at all.</li><li> Just add a YAML file, and you're ready to gain insights.</li><li> It works across different types of Go applications‚Äîwhether it's handling concurrency, recursion, or typical sequential logic.</li><li><strong>Better Debugging &amp; Profiling:</strong> It helps you understand performance bottlenecks and the inner workings of your application without the overhead of traditional methods.</li></ul><p><em>Again, this is aimed at the development stage. It's not a replacement for existing production-ready telemetry packages. It's just meant to be a simple way to add some instrumentation to any application without modifying the codebase.</em></p><p><strong><em>Is this concept valid, or am I just reinventing the wheel?</em></strong></p><p>If there's enough interest, I'll certainly share the source code, but right now it's working, but a bit of a cluster-mess. I'll just need a day or two to clean it up and make it less embarrassing. ;)</p><p>Cheers, and thanks for the feedback!</p><p>EDIT: I realized that I should explain a few more details! The idea revolves around AST construction/deconstruction of the project code to build a new temp version of an existing application with instrumentation wrapped around the code. It then runs the instrumented version of your code in a temp area and provides analysis in the form of logs and call graphs, and other info. This can certainly be extended, but it was only a day-long rabbit hole. ;)</p>","contentLength":2675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] The Curse of Depth in LLMs: Why Are Deep Layers Less Effective?","url":"https://www.reddit.com/r/MachineLearning/comments/1isu1nn/r_the_curse_of_depth_in_llms_why_are_deep_layers/","date":1739930525,"author":"/u/pseud0nym","guid":5691,"unread":true,"content":"<p>Recent research is shedding light on an unexpected problem in modern large language models, the deeper layers aren‚Äôt pulling their weight.</p><p>A recent paper, <em>\"The Curse of Depth in Large Language Models\"</em>, highlights a critical issue: - Deep layers in LLMs contribute significantly less to learning than earlier ones.<p> - Many of these layers can be pruned without serious performance loss, raising questions about training efficiency.</p> - The culprit? Pre-Layer Normalization (Pre-LN), which causes output variance to explode in deeper layers, making them act almost like identity functions.<p> - A simple fix? LayerNorm Scaling, which controls this variance and improves training efficiency.</p></p><p>This has major implications for LLM architecture, training efficiency, and scaling laws. If half the layers in models like LLaMA, Mistral, and DeepSeek aren‚Äôt contributing effectively, how much computational waste are we dealing with?</p><p>Key questions for discussion: 1Ô∏è) Should we be rethinking deep-layer training strategies to improve efficiency?<p> 2Ô∏è) Does this impact the assumption that deeper = better in transformer architectures?</p> 3Ô∏è) Could insights from this paper help with LLM compression, fine-tuning, or distillation techniques?</p><p>Let‚Äôs discuss‚Äîwhat are your thoughts on the Curse of Depth?</p>","contentLength":1289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best approach to manifests/infra?","url":"https://www.reddit.com/r/kubernetes/comments/1isql4s/best_approach_to_manifestsinfra/","date":1739921216,"author":"/u/RespectNo9085","guid":4474,"unread":true,"content":"<p>I've been provisioning various Kube clusters throughout the years, and now I\"m about to start a new project. </p><p>To me the best practice is to have a repo for the infrastructure using Terraform/Open Tofu, in this repo I usually set conditionals to provision either a Minikube for local or an EKS for prod, </p><p>Then I would create another repo to put together all cross-cutting concerns as a helm chart. That means I will use Grafana, Tempo, Vault Helm Charts and then I will package them in to one 'shared infrastructure' helm chart which is then applied to the clusters. </p><p>Each microservice will have its own helm chart that is generated on push to master and serverd on GIthub packages, there is also a dev manifest where people update the chart version for their microservice. The dev manifest has all they need to run the cluster, all the services.</p><p>The problem here is that sometimes I want to add a new technology to the cluster, for example recently I wanted to add the API gateway, Vault, Cillium or some other time I wanted to add a Mattermost instance, and some of these don't have proper helm charts. </p><p>Most of their instructions are simple cases where you apply a manifest from a URL into the cluster and that's no way to provision a cluster, because if I want to change things in the future, then should I apply again with a new values.yaml ? not fun, I like to see, understand and control what is going into my cluster. </p><p>So the question is, is the only option to read those manifest and create my own Helm charts? should I even Helm? is there a better approach? any opinion is appreciated. </p>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jelly much?","url":"https://www.reddit.com/r/linux/comments/1ispt1q/jelly_much/","date":1739919299,"author":"/u/Calrissiano","guid":4473,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm I too ambitious?","url":"https://www.reddit.com/r/rust/comments/1ispq6n/im_i_too_ambitious/","date":1739919113,"author":"/u/Sonder-Otis","guid":5598,"unread":true,"content":"<p>for my operating systems class I personally want to work on a project, creating a boot loader. I want to use rust for this. But I have never written rust before. And for my dsa classes I am learning python(which is simple I think). Is it too ambitious to think I can learn rust within the month or two and build the project.</p><p>I have previously written JS,Java and C++.</p><p>edit: my grades do not depend on it. I want to do it because I want to learn rust and have a better undrstanding of operating systems</p>","contentLength":499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Configuration Languages (config DSLs) solve configuration complexity?","url":"https://www.reddit.com/r/kubernetes/comments/1isph9t/can_configuration_languages_config_dsls_solve/","date":1739918543,"author":"/u/wineandcode","guid":4448,"unread":true,"content":"<p>Configuration languages are not the best solution to configuration complexity. Each language has its pros and cons, but none moves the needle much. In this post, Brian Grant explores what they are. Why would someone create a new one? And do they reduce configuration complexity?</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starskey - High performance embedded key-value database (LSM tree based)","url":"https://github.com/starskey-io/starskey","date":1739917759,"author":"/u/diagraphic","guid":4472,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1isp362/starskey_high_performance_embedded_keyvalue/"},{"title":"Linux running on NES via NES86 -- IBM PC emulator","url":"https://www.youtube.com/watch?v=OooHTDMUSGY","date":1739916036,"author":"/u/r_retrohacking_mod2","guid":5848,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1isot8w/linux_running_on_nes_via_nes86_ibm_pc_emulator/"},{"title":"Rhino Linux 2025.2 releases with plenty of fixes.","url":"https://blog.rhinolinux.org/news-19","date":1739911153,"author":"/u/MrBeeBenson","guid":5597,"unread":true,"content":"<div><div><div>Rhino Linux Team,</div></div></div><p>It's been just about a month since our last snapshot, and we are happy to bring the release of Rhino Linux 2025.2. A lot of development has progressed, with the team working to resolve multiple longer standing issues in our <a href=\"https://github.com/rhino-linux/tracker\" target=\"_blank\" rel=\"noreferrer\">Bug Tracker</a>. Our release notes are shorter this time around, but pack a solid amount of fixes that we hope improve the overall usage and stability of Rhino Linux.</p><ul><li> Last month's release kicked off the year with a bit of a bumpy start. Several major bugs slipped through our final testing stage, and we worked quickly and diligently to resolve the issues users were facing, but we strive to provide a better and fuller experience for our users, and the last release may not have met those expectations. Smaller open source projects often operate solely on volunteer work, and Rhino Linux is no exception. If you would like to contribute to the development of Rhino Linux, please join our <a href=\"https://discord.gg/reSvc8Ztk3\" target=\"_blank\" rel=\"noreferrer\">Discord Server</a> and contribute to the discussion. You can also report new and view existing issues on the <a href=\"https://github.com/rhino-linux/tracker\" target=\"_blank\" rel=\"noreferrer\">Rhino Linux Bug Tracker</a>.</li></ul><p>To upgrade to 2025.2, please run: </p><p>The following issues relating to the Unicorn Desktop have been resolved:</p><ul><li>Updating the package no longer overwrites existing configuration files</li><li>The Global Menu no longer causes the end of the panel to extend off of the screen</li><li>The desktop wallpaper now properly displays on all screen sizes</li><li>Issues with certain devices lacking audio out-of-the-box have been resolved</li></ul><ul><li>Kernel version  ships by default on Generic ISO disk images.</li><li>Kernel version  ships by default on Pine64 images.</li><li>Kernel version  ships by default on Raspberry Pi images.</li><li>Our new  utility ships by default on all images, which you can read more about <a href=\"https://blog.rhinolinux.org/news-18\">here</a>.</li><li>Issues with Raspberry Pi and Pine64 boots have been resolved.</li><li>Deployed images now properly auto-resize to fill the image volume on boot again.</li></ul><p>Many thanks, and happy rolling,</p><a href=\"https://blog.rhinolinux.org/\">Rhino Linux Blog</a>","contentLength":1888,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1isn0p7/rhino_linux_20252_releases_with_plenty_of_fixes/"},{"title":"How to stop SSL-Certs from being deleted when uninstalling a helm deployment","url":"https://www.reddit.com/r/kubernetes/comments/1ismivb/how_to_stop_sslcerts_from_being_deleted_when/","date":1739909981,"author":"/u/Eldiabolo18","guid":4432,"unread":true,"content":"<p>when trying a helm chart I often have to reinstall it a couple of times until it works the way I want it. If that Helm-Chart has an ingress and generates a SSL-Cert from Letsencrypt via Cert-Manager, the cert also gets deleted and regenerated. </p><p>I just ran into the issue, that I redployed the helm chart more than 5 times in 24 (48?) hrs for the same domain, so letsencrypt blocks the request. </p><p>Is there any way to stop the SSL-Certs from being deleted when in uninstall a helm chart, so it can be reused for the next deployment? Or is there any other way around this? </p>","contentLength":567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CSS-only Syntax Highlighting","url":"https://aartaka.me/css-only-highlight.html","date":1739909657,"author":"/u/aartaka","guid":5778,"unread":true,"content":"<a href=\"https://aartaka.me/about.html\">By Artyom Bologov</a><p>\nSo I started posting some\n<a href=\"https://aartaka.me/making-c-uglier.html\">pretty intimidating pieces of code</a>\non this blog.\nAnd I also became a part of EVIL-using Smartparens-reliant angry-fruit-salad-addicted bunch.\n(At my $ork, I'm still an Emacs/Paredit/Monochrome lover deep inside.)\nSo it's only consequential that I can no longer look at my blog.\nIt's too dull, too monotonous.\nToo colorless.\n\n</p><p>\nI need syntax highlighting!\nBut I promised that my posts will use no JS.\nUnless otherwise noted.\nMeaning: almost never.\nCan I somehow do syntax highlighting without JS?\n\n</p><section><p><a href=\"https://aartaka.me/this-post-is-ed.html\">So I'm generating my blog with ed(1)</a>\nWhich means that I have an intermediate step of HTML compilation.\nNot really convenient (it takes a whole tenth of a second!)\nBut it's worth the wait to replace all the templates and variables and syntax.\nSo why not add one more thing to this pipeline?\n\n</p><p>\nThe simplest way to do highlighting is adding some inline tags,\nlike  or :\n\n</p><figure><pre lang=\"sed\">[[].:-@^-`-]\\{,\\}\\/b&gt;\n</pre><figcaption>Wrapping tokens in b tags (do not ask me where that regex came from)</figcaption></figure><p>\nA good and simple approach, but why not go further?\nLike using something beyond default element styles?\n\n</p></section><section><p>\nLooking for JS-less syntax highlighting, I found several options for CSS.\nAdding some -s and classes to code blocks to add highlighting.\n<a href=\"https://github.com/soulshined/ft-syntax-highlight\">Like this thing, for example.</a>\nAll cool, but\n\n</p><figure><pre lang=\"html\">.cssscript.com&lt;/span&gt;\n      .com\n      \n    ....\n</pre><figcaption>No, I do not want that, thanks</figcaption></figure><p>\nI mean, I could auto-generate these classes, but\n\n</p><ul><li> That'd make my highlighting script ten times longer than HTML generation one.\n</li><li> I'd need to account for every language&amp;token combination.\n</li><li> I'd have to make arbitrary decisions about whether something is an identifier or an attribute or a variable or a class or a keyword or a string or a comment or a number or...\n</li></ul><p>\nIs there maybe a sloppier solution that unifies both preprocessing simplicity and CSS flexibility?\n\n</p></section><section><p>\nIt would be nice if CSS matched against element text.\nImagine how easy it would be to match keywords with that!\nNo need for these ugly classes around every token.\n\n</p><p>\nBut CSS does no such thing, for better or worse.\nIt only matches against elements, classes, IDs, and attributes.\nWait...\n\n</p><p>\nThat's my hack: putting a duplicate of the code token into an attribute.\n(Remember preprocessing?)\n\n</p><figure><pre lang=\"sed\">.+1,/&lt;\\/pre/-1s/\\([[].:-@^-`-]\\{,\\}\\)\\2'&gt;\\2&lt;\\/span&gt;/g\n</pre><figcaption>Creating simple span-s for later styling.</figcaption></figure><p>\nAnd then highlighting the tokens with the matching attributes.\n\n</p><figure><pre lang=\"css\"> [],\n [],\n [],\n [],\n ...  []\n{\n    ()\n}\n</pre><figcaption>Huge set of highlighted special cases</figcaption></figure><p>\nSo meta: rule for  matches itself due to being overly greedy.\nI don't hide this deficiency of my algo from you, because why should I?\nIt's supposed to be a flawed heuristic hack for JS haters like me.\n\n</p><p>\nAnd that's it really!\nCreate -s\n(or , , or whatever that doesn't mess up page semantics terribly)\nwith token attributes and match these attributes.\n\n</p><p>\nHere's a more serious example for you:\n\n</p><figure><pre lang=\"lisp\">( ()\n  .\n.\n ().\n.\n ().\"\n  ( ())\n  ( (( ())\n        ())\n    (\n      (\n        ((\n             ( ()))\n         ( ( ())\n                  ( ())))\n        ((\n              ())\n         (\n                 ()\n                 ( ())\n                 ( ())))\n        (\n         ( ()))\n        ( ( ( () ( () ))))))\n    ()\n    ()))\n</pre><figcaption>A bigger Lisp code piece showcasing the highlighter</figcaption></figure><p>\nGo and purge a JS syntax highlighting library from your website.\nNow!\n\n</p><p>\nP.S. You can find the script I'm using at\n<a href=\"https://aartaka.me/scripts/tohighlight.ed\">scripts/tohighlight.ed</a>.\nFinding the stylesheet with keyword matching is left as an (easy!) exercise for the reader.\n\n</p></section>","contentLength":3457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1isme3j/cssonly_syntax_highlighting/"},{"title":"Anybody who says that there is a 0% chance of AIs being sentient is overconfident. Nobody knows what causes consciousness. We have no way of detecting it & we can barely agree on a definition. So we should be less than 100% certain about anything to do with consciousness and AI.","url":"https://www.reddit.com/r/artificial/comments/1isl2ms/anybody_who_says_that_there_is_a_0_chance_of_ais/","date":1739906483,"author":"/u/katxwoods","guid":4407,"unread":true,"content":"<div><p>To be fair, I think this is true of most philosophical questions.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/katxwoods\"> /u/katxwoods </a>","contentLength":97,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SQLC and multiple SQLite connections","url":"https://www.reddit.com/r/golang/comments/1isksmh/sqlc_and_multiple_sqlite_connections/","date":1739905824,"author":"/u/maekoos","guid":4430,"unread":true,"content":"<p>I've read a few times now that it's best to have one SQLite connection for writing and then one or more for reading to avoid database locking (I think that's why, at least).</p><p>But I'm not sure how you‚Äôd best handle this with SQLC?</p><p>I usually just create a single repo with a single sql.DB instance, which I then pass to my handlers. Should I create two repos and pass them both? I feel like I'd probably mix them up at some point. Or maybe one repo package with my read queries and another with only my write queries?</p><p>I'm really curious how you‚Äôd handle this in your applications!</p>","contentLength":578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Christoph Hellwig: \"Linus in private said that he absolutely is going to merge Rust code over a maintainers objection\"","url":"https://lore.kernel.org/rust-for-linux/Z7SwcnUzjZYfuJ4-@infradead.org/","date":1739905250,"author":"/u/Karma_Policer","guid":4408,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iskjgo/christoph_hellwig_linus_in_private_said_that_he/"},{"title":"GPU nodes on-premise","url":"https://www.reddit.com/r/kubernetes/comments/1isk9ne/gpu_nodes_onpremise/","date":1739904616,"author":"/u/blu-base","guid":4342,"unread":true,"content":"<p>My company acquired a few GPU nodes with a couple of nvidia h100 cards each. The app team is likely wanting to use nvidias Trition interference server. For this purpose we need to operate kubernetes on those nodes. I am now wondering whether to maintain native kubernetes on these nodes. Or to use some suite, such as open shift or rancher. Running natively means a lot of work on reinventing the wheel, having an operation documentation/ process. However, using suites could mean an overhead of complexity relative to the few number of local nodes.</p><p>I am not experienced with doing the admin side of operating an on-premise kubernetes. Have you any recommendations how to run such GPU focused clusters?</p>","contentLength":701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are the 'it just works' distros right now?","url":"https://www.reddit.com/r/linux/comments/1isitee/what_are_the_it_just_works_distros_right_now/","date":1739901210,"author":"/u/jjopm","guid":4311,"unread":true,"content":"<p>In addition to say ubuntu and opensuse tumbleweed, which distros effectively run themselves right now, for day to day use, like Mac OS X but without the restrictive forced updates etc.</p><p>More specifically: For day to day personal use and some app development but not for enterprise use necessarily, not bloated with things most users don't need or want, regular but not excessively distracting security updates, reasonable update cadence but non-breaking, minimal and not over-designed UI, etc.</p>","contentLength":491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Before It Even Gets a Stable Release, Serpent OS Changes Its Name To AerynOS","url":"https://fossforce.com/2025/02/before-it-even-gets-a-stable-release-serpent-os-changes-its-name-to-aerynos/","date":1739899850,"author":"/u/CrankyBear","guid":4310,"unread":true,"content":"<p>Last updated on February 19, 2025 </p><h3><em>Any article about Serpent OS or Solus OS (or now, AerynOS), is first and foremost an article about Ikey Doherty.</em></h3><p>Serpent OS, a Linux distro that just recently reached alpha, announced on Friday that it‚Äôs changing its name to AerynOS. In most cases, that would be the story ‚Äî a distro in the birthing stage is changing its name ‚Äî but in this case the story is the guy behind the distro, which is Ikey Doherty.</p><p>There‚Äôs been a long queue forming for more than three years for the first stable release of Serpent OS, mainly because it‚Äôs the brainchild of Doherty. It‚Äôs supposed to be the next-gen, better-than-Solus-OS-distro coming out of what I‚Äôve seen called, ‚Äúthe Ikey Brand.‚Äù</p><p>Ikey, it seems, has a reputation as being the creator of great Linux distros. He also has a reputation of suddenly abandoning his creations due to‚Ä¶ well, mainly money issues.</p><p>Take Solus OS for example.</p><p>Doherty has started two separate distros using the name Solus. The first was started in 2013, which quickly became popular and which Ken Starks wrote about in his <a href=\"https://fossforce.com/2013/10/solusos-linux-distro-stands-ground/\" target=\"_blank\">first ever FOSS Force article</a>. But about the same time that Doherty started it ‚Äî or at least not very long afterwards ‚Äî he abandoned it, which became the subject of <a href=\"https://fossforce.com/2013/10/solusos-life-happensdistros-die/\" target=\"_blank\">Starks‚Äô second FOSS Force article</a>, published 10 days later.</p><p>Four months after that Doherty was <a href=\"https://fossforce.com/2014/04/ikey-doherty-talks-evolve-os-budgie-desktop/\" target=\"_blank\">back with another distro</a>, Evolve, which quickly went through a name change, to Solus OS, as a way of solving a trademark dispute. That distro also became quite popular, and even gave birth to Budgie, which remains a popular desktop environment.</p><p>It‚Äôs also been something of a roller coaster ride for everyone involved, with Doherty abandoning it, leaving his team stranded, and then returning years later when another project lead abandoned the project, <a href=\"https://fossforce.com/2023/07/solus-is-back-but-can-it-survive-its-troubled-past/\" target=\"_blank\">which I chronicled</a> about a year-and-a-half ago.</p><p>Somewhere along the line, Doherty began developing another distro, Serpent OS</p><p>The plan, as I understand it, has been for Serpent OS to be something of a advanced version of Solus OS ‚Äî with Serpent being more of a distro for developers, and with Solus becoming based on it, but being more of a distro for everyday Linux users, sort of like Arch and Manjaro.</p><p>Things seemed to be moving along nicely ‚Äî albeit slowly for Serpent OS ‚Äî when Doherty out of the blue posted a dire warning to LinkedIn a few weeks ago that he was <a href=\"https://www.linkedin.com/posts/ikeydoherty_serpentos-activity-7291839455391277057-o7hE?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAGi5KwBrEJ9ACZMBcW3hg8W3DeB5jWNSVw\" target=\"_blank\">running out of money</a> and that due to funding issues he was ‚Äúhaving to slow development down.‚Äù</p><p>Since then there‚Äôs been a wave of articles in open-source media supporting Doherty and the project, but there‚Äôs been no indication one way or the other on whether that‚Äôs resulted in funds coming in.</p><p>Then on Friday came another unexpected announcement. This time it was about the most recent name change of a Doherty project.</p><p>The last time I wrote about Doherty and his ongoing troubles I used Yogi Berra‚Äôs famous phrase, ‚Äúd√©j√† vu all over again,‚Äù for one of the subtitles. That same phrase would be especially fitting for this article, considering that Doherty announced the name change in a post called ‚ÄúEvolve This OS,‚Äù a reference to the original name of the distro that‚Äôs now known as Solus OS but which isn‚Äôt to be confused with another Doherty founded distro called Solus which is now dead.</p><p>In other words, lightening has struck twice. Its d√©j√† vu all over again‚Ä¶ all over again.</p><p><a href=\"https://serpentos.com/blog/2025/02/14/evolve-this-os/\" target=\"_blank\">In his pos</a>t, Doherty attempts to make the case that the name change was necessary because, ‚Äú‚Äòserpents‚Äô are often associated with negative connotations.‚Äù</p><p>‚ÄúIt‚Äôs fair to say we‚Äôve spent a long time in prototype and alpha phases,‚Äù he wrote. ‚ÄúIn order to move forward, our identity needs to be more befitting of the project we‚Äôre building. A move into the real world. This isn‚Äôt a hobby project, it‚Äôs a full blown Linux distribution with serious technical underpinnings, achievements and goals.‚Äù</p><p>Okie dokie then. So that‚Äôs going to get Serpent OS, now AerynOS (pronounced ‚Äúerin‚Äù) on the road to recovery and stability?</p><p>I think I‚Äôm going to side with Linuxiac‚Äôs Bobby Borisov, who pretty much said <a href=\"https://linuxiac.com/serpent-os-rebrands-as-aerynos/\" target=\"_blank\">in an article on Saturday</a>, ‚ÄúUh, I don‚Äôt think so.‚Äù</p><p>Basically it boils down to this: Doherty has made it clear that he‚Äôs got all of his ducks in a row, even if he‚Äôs missing the duck that pays the rent.</p><p>‚ÄúHaving already secured AerynOS.com, AerynOS.dev, plus the associated social media accounts, we‚Äôre now in the process of rebranding the project. For sheer irony, we‚Äôll make the final transition day the 17th of March, 2025. Yes, St. Patrick‚Äôs Day.‚Äù</p><p>St. Patrick, you might remember, drove the snakes out of Ireland.</p><p><em>Editor‚Äôs note: An earlier version of this article reported that in a recent LinkedIn post that Ikey Doherty had said that without funding the Serpent OS project was in danger of ending, when what he had actually written was that without increased funding he‚Äôs ‚Äúhaving to slow development down.‚Äù The article has been edited to reflect that.</em></p><div itemtype=\"http://schema.org/Person\" itemscope=\"\" itemprop=\"author\"><div><div><div itemprop=\"description\"><p>Christine Hall has been a journalist since 1971. In 2001, she began writing a weekly consumer computer column and started covering Linux and FOSS in 2002 after making the switch to GNU/Linux. Follow her on Twitter: <a href=\"https://twitter.com/BrideOfLinux\">@BrideOfLinux</a></p></div></div></div></div>","contentLength":5218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1isi8zt/before_it_even_gets_a_stable_release_serpent_os/"},{"title":"\"How Rust & Embassy Shine on Embedded Devices (Part 1)\"","url":"https://www.reddit.com/r/rust/comments/1ishpbd/how_rust_embassy_shine_on_embedded_devices_part_1/","date":1739898567,"author":"/u/carlk22","guid":4614,"unread":true,"content":"<p>For over a year, off-and-on, the Seattle Rust User's Group has been exploring embedded programming with Rust and Embassy. Using Rust on embedded systems is both frustrating and fun. Frustrating because support for Rust lags behind both C/C++ and Python. Fun because of the Embassy Framework. </p><p><strong>Embassy gives us many benefits of an (Real Time) Operating System (RTOS) without the overhead</strong>. It provides bare-metal, cooperative multitasking with async/await, enabling non-blocking operations and efficient task management. (However, it does not provide hard real-time guarantees like traditional RTOS.) </p><p>I find it astounding that with Rust, we get async on a single processor without even needing memory allocation.</p><p><strong>If You Decide to Use Rust for Embedded, We Have Advice:</strong></p><ol><li>Use Embassy to model hardware with ownership.</li><li>Minimize the use of static lifetimes, global variables, and lazy initialization.</li><li>Adopt async programming to eliminate busy waiting.</li><li>Replace panics with Result enums for robust error handling.</li><li>Make system behavior explicit with state machines and enum-based dispatch.</li><li>Simplify hardware interaction with virtual devices.</li><li>Use Embassy tasks to give virtual devices state, method-based interaction, and automated behavior.</li><li>Layer virtual devices to extend functionality and modularity.</li><li>Embrace no_std and avoid alloc where possible.</li></ol>","contentLength":1328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Boot error","url":"https://www.reddit.com/r/linux/comments/1ishic4/boot_error/","date":1739898114,"author":"/u/bekir_yalcin","guid":4270,"unread":true,"content":"<p>Hello, I installed Zorin OS next to Windows to try it and I was not satisfied. However, when deleting it, I encountered the error in the image. What should I do? I can't get into the Windows boot menu, I can't use my computer.</p>","contentLength":226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Update API Server SANs and sync issue","url":"https://www.reddit.com/r/kubernetes/comments/1ishh0h/update_api_server_sans_and_sync_issue/","date":1739898036,"author":"/u/nfreder","guid":4275,"unread":true,"content":"<p>We have a use case to leverage a cloud provider I won't name that starts with an O. I have a requirement to setup a hostname and IP for connectivity. Is there a way to run kubeadm commands across all control plane nodes that I'm missing? I think a daemonset might work but, I have a chicken and egg issue to setup any kind of automation there. </p>","contentLength":344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Duplicate webcam device from host to VMs","url":"https://www.reddit.com/r/linux/comments/1ish99m/duplicate_webcam_device_from_host_to_vms/","date":1739897514,"author":"/u/Lhakryma","guid":4269,"unread":true,"content":"<p>So a friend of mine has a linux host machine, and several VMs for several reasons.</p><p>He has a webcam on the host, which apparently he can't use in both the host and the VM at the same time.</p><p>I didn't try this yet (since my host is windows and it's a bit more complicated), but isn't it possible to just `cat host@/dev/videoX &gt; vm@/dev/videoX` and as such have the same data that's being generated through the host's `/dev/videoX` device on the VM's video device?</p>","contentLength":457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"eserde: Don't stop at the first deserialization error","url":"https://mainmatter.com/blog/2025/02/13/eserde/","date":1739897285,"author":"/u/LukeMathWalker","guid":4374,"unread":true,"content":"<p><a href=\"https://github.com/mainmatter/eserde\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\"></a> is a new Rust crate by <a href=\"https://mainmatter.com/rust-consulting/\">Mainmatter</a>, built on top of , to provide better error reporting capabilities when deserializing user-facing payloads‚Äîe.g. API request bodies, configuration files.</p><p><a href=\"https://serde.rs\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\"></a> is  Rust library for (de)serialization.There's a catch, though:  is designed to abort deserialization as soon as an error occurs. This becomes an issue when relying on  for deserializing user-provided payloads‚Äîe.g. a request body for a REST API.There may be  errors in the submitted payload, but <a href=\"https://crates.io/crates/serde_json\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\"></a> will only report the first one it encounters before stopping deserialization. The API consumer is then forced into a slow and frustrating feedback loop:</p><ol><li>Receive a single error back</li><li>Back to 1., until there are no more errors to be fixed</li></ol><p>That's a poor developer experience. We should do better!We should report  errors at once, thus reducing the number of API interactions required to converge to a well-formed payload.</p><p>That's the problem  was born to solve.</p><h2><a href=\"https://mainmatter.com/blog/2025/02/13/eserde/#case-study:-an-invalid-json-payload\" aria-describedby=\"case-study:-an-invalid-json-payload\"></a>Case study: an invalid JSON payload</h2><p>Let's consider this schema as our reference example:</p><pre><code>\n    version\n    source\n    major\n    minor\n    patch</code></pre><p>We'll try to deserialize an invalid JSON payload into it via :</p><pre><code> payload  error payload\n    error</code></pre><p>Only the first error is returned, as expected. But we know there's more than that!We're missing the  field in the  struct and the  field can't be null.Let's switch to :</p><pre><code>\n    version\n    source\n    major\n    minor\n    patch payload  errors payload\n    errors</code></pre><p>Much better, isn't it?We can now inform the users  that they have to fix three different schema violations.</p><p>To use  in your projects, add the following dependencies to your :</p><pre><code></code></pre><ul><li>Replace all instances of <code>#[derive(serde::Deserialize)]</code> with <code>#[derive(eserde::Deserialize)]</code></li><li>Switch to an -based deserialization function</li></ul><p> provides first-class support for JSON deserialization, gated behind the  Cargo feature.</p><pre><code></code></pre><p>If you're working with JSON:</p><p> doesn't support deserializing from a reader, i.e. there is no equivalent to .</p><p>There is also an  integration, <a href=\"https://docs.rs/eserde_axum\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\"></a>. It provides an -powered JSON extractor as a drop-in replacement for 's built-in one.</p><p>The approach used by  is compatible, in principle, with all existing -based deserializers.Refer to <a href=\"https://github.com/mainmatter/eserde/blob/main/eserde/src/json.rs\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\">the source code of </a> as a blueprint to follow for building an -powered deserialization function for another format.</p><p> is designed to be maximally compatible with .</p><p><a href=\"https://docs.rs/eserde/latest/eserde/derive.Deserialize.html\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\"><code>derive(eserde::Deserialize)</code></a> will implement both  and , honoring the behaviour of all the  attributes it supports.</p><p>If one of your fields doesn't implement , you can annotate it with  to fall back to 's default deserialization logic for that portion of the input.</p><pre><code>\n    x</code></pre><p>But how does  actually work? Let's keep using JSON as an example‚Äîthe same applies to other data formats.We try to deserialize the input via . If deserialization succeeds, we return the deserialized value to the caller.</p><pre><code>s de s error  devve e</code></pre><p>Nothing new on the happy path‚Äîit's the very same thing you're doing today in your own applications with vanilla . We diverge on the unhappy path.Instead of returning to the caller the error reported by , we do another pass over the input using <code>eserde::EDeserialize::deserialize_for_errors</code>:</p><pre><code>s _guard  de s de  de errors de__ errors  errors\n            path\n            details error\n        errors\n    errors</code></pre><p><code>EDeserialize::deserialize_for_errors</code> accumulates deserialization errors in a thread-local buffer, initialized by <code>ErrorReporter::start_deserialization</code> and retrieved later on by <code>ErrorReporter::take_errors</code>.</p><p>This underlying complexity is encapsulated into 's functions, but it's beneficial to have a mental model of what's happening under the hood if you're planning to adopt .</p><h2><a href=\"https://mainmatter.com/blog/2025/02/13/eserde/#limitations-and-downsides\" aria-describedby=\"limitations-and-downsides\"></a>Limitations and downsides</h2><p> is a new library‚Äîthere may be issues and bugs that haven't been uncovered yet. Test it thoroughly before using it in production. If you encounter any problems, please open an issue on our <a href=\"https://github.com/mainmatter/eserde\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\">GitHub repository</a>.</p><p>Apart from defects, there are some downsides inherent in 's design:</p><ul><li>It can't deserialize from a non-replayable reader, since it needs to visit the input again on the unhappy path.</li><li>On the unhappy path it's going to be  than , since it visits the input again.</li><li>It'll have a bigger impact than vanilla  on your compilation times, since <code>#[derive(eserde::Deserialize)]</code> generates more code than  (roughly twice as much), so</li></ul><p>We believe the trade-off is worthwhile for user-facing payloads, but you should walk in with your eyes wide open.</p><p>We plan to add first-class support for more data formats, in particular YAML and TOML. They are frequently used for configuration files, another scenario where batch error reporting would significantly improve our developer experience.</p><p>We plan to incrementally support more and more  attributes, thus minimising the friction to adopting  in your codebase.</p><p>We plan to add first-class support for validation, with a syntax similar to <a href=\"https://docs.rs/garde/latest/garde/\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\"></a> and <a href=\"https://docs.rs/validator/latest/validator/\" target=\"_blank\" rel=\"nofollow noopener\" aria-describedby=\"external-new-window-message\"></a>. The key difference: validation would be performed  the deserialization process. No need to remember to call  afterwards.</p>","contentLength":4912,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ish5vd/eserde_dont_stop_at_the_first_deserialization/"},{"title":"Xbox creator says \"world will return to local compute\" once the cost of high GPU performance lowers","url":"https://www.pcguide.com/news/xbox-creator-says-world-will-return-to-local-compute-once-the-cost-of-high-gpu-performance-lowers/","date":1739897085,"author":"/u/Automatic_Can_9823","guid":4309,"unread":true,"content":"<div> PC Guide is reader-supported. When you buy through links on our site, we may earn an affiliate commission. <a href=\"https://www.pcguide.com/earnings-disclaimer/\">Read More</a></div><p>There‚Äôs a lot to talk about in the world of computer graphics and PC gamers are often quick to share their opinion on the state of graphics, performance, and everything in between. One of the hottest topics as of late is what some people may feel is a ‚Äòreliance‚Äô on AI technology, whether it be upscaling, frame generation, or both. We believe that tech like Frame Gen is great for PC gaming, <a href=\"https://www.pcguide.com/news/pc-gaming-is-better-off-with-frame-generation-but-it-shouldnt-exist-to-reach-60-fps/\" target=\"_blank\" rel=\"noreferrer noopener\">but it shouldn‚Äôt exist to reach 60 FPS</a> as we‚Äôve seen in some recent titles.</p><p>This also seems to be true to a certain extent for ‚Äòfather of Xbox‚Äô Seamus Blackley who has shared his thoughts for an upcoming <a href=\"https://open.spotify.com/show/4z0BGj2zmTceUpoQ8WL2qr\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">VideoGamer Podcast</a>. He claims to be a ‚Äúbig believer in local compute‚Äù and believes computer graphics will return to focus on traditional raster techniques once the cost of doing so is more practical. Right now, we‚Äôre in an artificial intelligence boom, but who knows how long it will last?</p><p>Rasterization, as opposed to AI, is all about raw performance. This is what any GPU reviewer bases their benchmarks on and it‚Äôs what PC gamers want to see when comparing graphics cards one-to-one. While AI may be the main driver for Nvidia, AMD, and Intel right now, Blackley seems confident that things will come full circle.</p><blockquote><p>‚ÄúI am a big believer in local compute, and so I think that the world will return to local compute in just a minute when the cost of making super high performance gets a little bit lower.‚Äù</p><p>‚ÄúNvidia and other graphics companies have changed their focus from better rendering to upscaling and to AI features. It‚Äôs like the message there is that it‚Äôs not a good business anymore to try to convince people that they‚Äôre rendering better, which means you have enough rendering power, which is terrifying to them because then what‚Äôs their business? If not trying to sell you a graphics card with more rendering?‚Äù</p></blockquote><p>Right now, it makes sense for Nvidia to go full speed ahead on its artificial intelligence ventures, especially with AMD always on its tail. <a href=\"https://www.pcguide.com/news/dlss-4-multi-frame-gen-has-native-support-in-just-four-games-at-launch-luckily-the-nvidia-app-has-you-covered/\" target=\"_blank\" rel=\"noreferrer noopener\">Multi Frame Generation</a> is Team Green‚Äôs latest development exclusive to the new RTX 50 series GPUs, and it is a big selling point for these cards considering the raw performance uplift isn‚Äôt quite as good as prior generations.</p><h2>AI is ‚Äújust a filter,‚Äù Blackley adds</h2><p>Blackley views AI in pretty simple terms. ‚ÄúAI is just, it‚Äôs just a filter ‚Äì it‚Äôs a machine-learning filter ‚Äì it‚Äôs really dressing it up to be fancy.‚Äù That‚Äôs essentially what upscaling and frame generation are all about; ‚Äòfake frames‚Äô as some people would say. However, it is worth pointing out that Blackley says you ‚Äúreach some kind of plateau in graphics and connectivity and processing power where more doesn‚Äôt necessarily make things better‚Äù. This is the other side of the argument, suggesting that AI is a necessary part of computer graphics.</p><p>While raw raster performance gains may be taking the back seat for now in favor of a push for AI, it seems like a matter of time before rendering costs lower enough that ‚Äòsuper high performance‚Äô becomes sustainable without over-reliance on artificial intelligence.</p><div><div><img alt=\"\" nitro-lazy-src=\"https://cdn-afkgp.nitrocdn.com/GgcvDclOgOFrMPDAxuwUmHHZlgKuQsxq/assets/images/optimized/rev-05f4362/www.pcguide.com/wp-content/uploads/2023/08/jack-goodall-pc-guide-96x96-60x60.jpg\" decoding=\"async\" nitro-lazy-empty=\"\" src=\"data:image/svg+xml;nitro-empty-id=MzA2MzoxMzU=-1;base64,PHN2ZyB2aWV3Qm94PSIwIDAgOTYgOTYiIHdpZHRoPSI5NiIgaGVpZ2h0PSI5NiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48L3N2Zz4=\" nitro-lazy-srcset=\"https://cdn-afkgp.nitrocdn.com/GgcvDclOgOFrMPDAxuwUmHHZlgKuQsxq/assets/images/optimized/rev-05f4362/www.pcguide.com/wp-content/uploads/2023/08/jack-goodall-pc-guide-96x96-60x60.jpg 1x, https://cdn-afkgp.nitrocdn.com/GgcvDclOgOFrMPDAxuwUmHHZlgKuQsxq/assets/images/optimized/rev-05f4362/www.pcguide.com/wp-content/uploads/2023/08/jack-goodall-pc-guide-96x96.jpg 2x\"></div></div>","contentLength":3219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ish2zs/xbox_creator_says_world_will_return_to_local/"},{"title":"Better GitHub Actions caching for Go","url":"https://danp.net/posts/github-actions-go-cache/","date":1739896962,"author":"/u/dpiddy","guid":4507,"unread":true,"content":"<p>I‚Äôve been spending some time on <a href=\"https://en.wikipedia.org/wiki/Continuous_integration\" target=\"_blank\">CI</a> improvements at <a href=\"https://www.grax.com/\" target=\"_blank\">work</a> recently, mostly around cutting down how long things take.</p><p>As I looked to see if anything about our overall process could be improved, a couple things bothered me:</p><p>Why, if we were using a pretty standard GitHub Actions setup, were there indications that modules were being downloaded as part of every run?\nShouldn‚Äôt that all be cached?</p><p>Why did it seem like there was always a delay before tests actually started running?\nShouldn‚Äôt the first few packages‚Äô fast tests complete quickly?</p><p>Before getting into what I ended up changing, let‚Äôs review the setup we had and break down the issues.</p><p>There are two main workflows I was concerned with: one for CI and another artifacts.</p><p>The CI workflow runs tests.\nThe artifacts workflow builds binaries for distribution.\nWe use <a href=\"https://docs.github.com/en/actions/sharing-automations/creating-actions/about-custom-actions\" target=\"_blank\">custom actions</a> to cut down on some repetition but they both end up having this step:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Let‚Äôs look at the second one in more detail.</p><p>setup-go doesn‚Äôt use the <a href=\"https://github.com/actions/cache\" target=\"_blank\">cache action</a> directly but they both use the same implementation so we can describe what setup-go is doing in its terms.</p><p>If you were to put an explicit action/cache step in to replicate what setup-go does, it would look something like:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>(some items in  made static for brevity)</p><p>When saving to the cache, this step generates a key based on:</p><ul></ul><p>For example, if you <a href=\"https://go.dev/doc/devel/release#go1.23.minor\" target=\"_blank\">updated to use Go 1.23.4</a> around December 3rd when it came out and didn‚Äôt change anything else that influenced the key until Go 1.23.5 came out in January you would have been using the same cache item that whole time.\nA build that happened January 8th would have used the same cache item as one that started December 16th, regardless of how much your code had changed in the meantime.</p><p>For smaller projects that‚Äôs probably not a big deal but for larger ones it can add up.\nIt seemed to be for ours!</p><p>We were running into another issue related to cache item immutability.</p><p>When we did make a change that led to a cache item, our faster-running artifacts workflow was sneaking in and saving a cache item based on what it was doing.\nWhen the CI workflow finished it saw the cache key it wanted to save already existed and carried on.</p><p>The artifacts workflow for the most part runs .\nOur CI workflow runs something like <code>go test -race -count 1 ./...</code>.</p><p>That meant our cache was missing build output that could help build our tests, especially for the <a href=\"https://go.dev/doc/articles/race_detector\" target=\"_blank\">race detector</a> enabled by .\nThat explained the delay before our first few fast-testing packages produced any output.</p><p>The cache was also missing all the modules needed by everything CI does which explained every CI run starting with some download output.</p><p>Now that the issues were understood they could be fixed!</p><p>I wanted a new setup that would:</p><ul><li>Cache build output in a way that kept things fresh</li><li>Not let the cache grow too big</li><li>Ensure the more involved CI workflow saved cache items, not the artifacts workflow</li><li>Only save to the cache when running CI on the  branch</li><li>Let CI and artifacts runs on any branch use the cache</li></ul><p>Let‚Äôs start with cache saving.</p><p>Near the end of the CI workflow, we now have this:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The  step gives us a  environment variable with the current date.\nThe  step saves a cache item to a key based on OS, the hash of , and .</p><p>That means whenever a cache item is saved it‚Äôll end with the  value, such as .\nBecause cache items are immutable, if a cache item already exists with the same , saving will be skipped.</p><p>If an existing cache was used for this run, before saving, the  step trims build output that was not created or used by the CI run that is completing.\nThere is <a href=\"https://go.dev/issue/69879\" target=\"_blank\">a proposal</a> around making the go command‚Äôs cache trimming configurable but this seems good enough for our purposes.</p><p>Finally, to ensure all needed modules end up in any saved cache items, we have this near the top of the CI workflow:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p><a href=\"https://go.dev/ref/mod#go-mod-download\" target=\"_blank\"></a> will pre-fill the module cache with everything the main module needs.\nIf everything is already present, it does nothing (and is fast).</p><p>Since only the CI action saves the cache that means the artifacts workflow can‚Äôt leave us with an ineffective cache item anymore.</p><p>All this leaves us with a cache that is:</p><ul><li>Pre-filled with all modules the main module needs</li><li>Updated every day or so with the use of </li><li>Trimmed before saving to only contain relevant build output, keeping size down</li><li>Only saved by CI runs on </li></ul><p>Now, restoring the cache.</p><p>Near the top of both the CI and artifacts workflows, we have:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The trick here is that using  means restoring will always fall back to using <a href=\"https://github.com/actions/cache#inputs\" target=\"_blank\"></a>.\nThe  prefix contains everything before the  value that is used to save.</p><p>This means all both the CI and artifacts workflows, on any branch, can load a cache item that was saved by a recent CI run on .</p><p>For example, if the last CI run on  saved , any subsequent CI or artifacts run will use that item.</p><p>All this leaves us with a cache restore setup that:</p><ul><li>Both the CI and artifacts workflows, on any branch, can use</li><li>Prefers fresh cache entries saved by CI on </li></ul><h2>Conclusion and possible improvements</h2><p>All this together has helped our Go caching greatly.</p><p>Your results may vary, of course, but rolling these changes out cut about a minute and a half off our CI run time.\nAnd it was nice to see those initial fast tests report something almost instantly.</p><p>Perhaps this should be integrated into the setup-go action somehow.</p><p>Either way a possible improvement over  could be to judge cache churn by how many build output files are created/touched during a CI run.\nIf many are created and few are touched, it probably indicates a new cache item is warranted.</p>","contentLength":5471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ish19c/better_github_actions_caching_for_go/"},{"title":"This turns any noise into SFXs","url":"https://www.reddit.com/r/artificial/comments/1isgrn7/this_turns_any_noise_into_sfxs/","date":1739896301,"author":"/u/Mindless-Investment1","guid":4613,"unread":true,"content":"<p>Just came across this video of a guy turning his sounds into effects, like that Sketch2Sound thing Adobe previewed. I feel like this'll be one of those apps that changes the way movie studios create content.</p>","contentLength":207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"issue with ingress","url":"https://www.reddit.com/r/kubernetes/comments/1isgbi8/issue_with_ingress/","date":1739895206,"author":"/u/GeneEfficient1481","guid":4274,"unread":true,"content":"<p>hello everyone i am having trouble with this ingress exercise</p><p>Create an Ingress resource named web and configure it as follows:</p><p>Route traffic for the host web.kubernetes and all routes to the existing web service. Enable TLS termination using the existing Secret web certification.</p><p>Redirect HTTP requests to HTTPS. </p><p>I have configured /etc/hosts I will pair the node ip with the web.kubernetes host</p><p>[curl: (7) Unable to connect to web.k8s.local port 80: connection refused]</p><p>what should i do to solve the problem?</p><p>this my txt containing deploy,svc secret and ingress: # 1. Deployment</p><p>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=web.k8s.local/O=web.k8s.local\"</p><p>kubectl create secret tls web-cert --namespace=prod --cert=tls.crt --key=tls.key</p>","contentLength":776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are some of the best desktop environments that are ideal for use on a TV?","url":"https://www.reddit.com/r/linux/comments/1isg2sk/what_are_some_of_the_best_desktop_environments/","date":1739894589,"author":"/u/blackbirdproductions","guid":4241,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Plasma Bigscreen was the most promising, but it&#39;s been in development for a while now, and isn&#39;t available to the public yet.</p> <p>So I&#39;m on the hunt for a TV optimized desktop environment for a Pi project I&#39;m working in which involves replacing my Apple TV... and before someone says &#39;use Kodi so you can own your media&#39; and what nots... I KNOW, I just prefer streaming for my use case.</p> <p>I&#39;d love to know if anyone has used Linux on a TV <em>preferably</em> for general media such as streaming and what&#39;s your favorite linux distro or desktop environment for your system?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/blackbirdproductions\"> /u/blackbirdproductions </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1isg2sk/what_are_some_of_the_best_desktop_environments/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1isg2sk/what_are_some_of_the_best_desktop_environments/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elixir in kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1isfy34/elixir_in_kubernetes/","date":1739894278,"author":"/u/Latter-Change-9228","guid":4243,"unread":true,"content":"<p>I'm currently learning elixir in order to use it in production. I heard of the node architecture that elixir provides thanks to the OTP but I can't find resources about some return on experienec on using distributed elixir in a kubernetes context. Any thoughts about that ?</p>","contentLength":273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop Over-Engineering AI Apps: The Case for Boring Technologies","url":"https://www.timescale.com/blog/stop-over-engineering-ai-apps","date":1739894174,"author":"/u/jascha_eng","guid":5816,"unread":true,"content":"<blockquote>\"Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.\" ‚Äî Anthropic, <a href=\"https://www.anthropic.com/research/building-effective-agents\"><u>Building Effective Agents</u></a></blockquote><p>The AI tooling landscape resembles a gold rush. New frameworks pop up daily, each claiming to be  solution for building AI applications. But in an attempt to solve every possible use case, they introduce layers of abstraction that make systems harder to understand, debug, and maintain.</p><p>This seems familiar: NoSQL, serverless, microservices. Each promised simplicity but just shifted complexity elsewhere.&nbsp;</p><p>AI is no different. An application leveraging LLMs is still just an app. Ninety percent of what we‚Äôve learned in software engineering still applies. The best solutions don‚Äôt replace what works‚Äîthey build on it.</p><p>LangChain, the most popular AI application framework today, promises to be a comprehensive solution for building LLM applications. While it's an impressive piece of engineering, it exemplifies the \"do everything\" approach that plagues the field.</p><p>A typical RAG (retrieval-augmented generation) app that retrieves relevant documents and uses them to answer a question looks like this according to Langchain‚Äôs <a href=\"https://python.langchain.com/docs/tutorials/rag/\"></a>:</p><div><pre><code>\n# Load and chunk contents of the blog\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nall_splits = text_splitter.split_documents(docs)\n\n# Index chunks\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\nvector_store = InMemoryVectorStore(embeddings)\n_ = vector_store.add_documents(documents=all_splits)\n\n# Define prompt for question-answering\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# Define state for application\nclass State(TypedDict):\n    question: str\n    context: List[Document]\n    answer: str\n\n\n# Define application steps\ndef retrieve(state: State):\n    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n    return {\"context\": retrieved_docs}\n\n\ndef generate(state: State):\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n    response = llm.invoke(messages)\n    return {\"answer\": response.content}\n\n\n# Compile application and test\ngraph_builder = StateGraph(State).add_sequence([retrieve, generate])\ngraph_builder.add_edge(START, \"retrieve\")\ngraph = graph_builder.compile()\n</code></pre></div><p>This \"simple\" example introduces seven new concepts: StateGraphs, sequences, graph builders, vector stores, splitters, documents, and loaders. And that‚Äôs on top of concepts that engineers need to understand to build a RAG app in the first place: vector similarity search, chunks, embeddings, and, of course, large language models (LLMs). Yet, even these foundational concepts aren't left untouched: They're wrapped in LangChain's own implementations. OpenAIEmbeddings isn't the official OpenAI client but rather LangChain's wrapper. Even BeautifulSoup, a tried-and-true Python HTML parsing library, gets encapsulated in a custom Loader wrapper.</p><p>While LangChain lowers the barrier to entry for AI apps, many teams find that these abstractions become liabilities as projects scale. We‚Äôve heard many stories of developers building MVPs of their RAG systems on LangChain, but tearing it up and re-building their app without a framework a month or so later. And we‚Äôre not alone; many AI engineering teams have written about their decision to move away from frameworks like LangChain‚Äîsee examples <a href=\"https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents\"></a>, <a href=\"https://youtu.be/iHwptVCfxyg?si=np3OL0r9yZxhhHN3\"></a>, <a href=\"https://www.reddit.com/r/LLMDevs/comments/1hsfitm/not_using_langchain_ever/\"></a>, <a href=\"https://www.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/\"></a>, <a href=\"https://minimaxir.com/2023/07/langchain-problem/\"></a>, and <a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1d4p1t6/is_langchain_usable/\"></a>.</p><p>There‚Äôs a common refrain behind all these stories: LangChain (and similar tools) buries engineers in layers of abstraction before they even reach prompt engineering, evals, and data management. These abstractions make debugging challenging when issues arise across multiple layers and also make customization and meeting specific application requirements challenging. Moreover, it creates an entire parallel vocabulary that maps to actual LLM operations, forcing engineers to learn two sets of concepts instead of one. </p><p>The fundamental irony is that modern programming languages already handle LLM primitives perfectly well. Unlike web frameworks that abstract away complex networking concepts or object-relational mappers (ORMs) that simplify database interactions, LLM operations work with surprisingly simple data types. Messages are just strings. Embeddings are just lists of floating-point numbers. Python's built-in types and basic control flow are already ideal for handling these primitives.So why not keep it simple, stupid?</p><p>Tools like <a href=\"https://github.com/BerriAI/litellm\"></a> exemplify good AI tooling: They solve a single, well-defined problem by providing a <a href=\"https://www.timescale.com/blog/one-line-of-sql-all-the-litellm-embeddings\">unified interface for LLM provider APIs</a>. No embedding management, no caching, no workflow orchestration‚Äîjust one focused task done well.</p><p>This approach mirrors <a href=\"https://www.anthropic.com/research/building-effective-agents\"></a> about successful agent implementations. Instead of betting on monolithic frameworks, using simple, focused components leads to systems that are easier to understand, maintain, and evolve. We can then combine these basic building blocks to build solutions that exactly match our needs.</p><p><a href=\"https://www.timescale.com/blog/one-line-of-sql-all-the-litellm-embeddings\">We recently added LiteLLM</a> to <a href=\"https://github.com/timescale/pgai\"></a>, our tool for automating embedding creation and synchronization in PostgreSQL. This move enabled our PostgreSQL extension to support basically every embedding provider. It also saved us from having to integrate a new API for every provider. Kudos to LiteLLM. This was really helpful.</p><h2>Integrating AI Into Existing Software Stacks</h2><p>The surge of AI-specific tools has led teams to overlook a fundamental truth: Most AI applications are still just applications at heart! And they need the same things that ‚Äúnon-AI applications‚Äù need: data persistence, authentication, business logic, and all the other components we've been building for decades. Successfully building an AI application today means choosing tools that complement and integrate with your existing infrastructure, not replacing your entire stack with AI-specific tools.&nbsp;</p><p>PostgreSQL, the well-known and loved relational database, is a great example of this. PostgreSQL has been the backbone of countless applications for over 30 years. Thanks to PostgreSQL extensions like <a href=\"https://www.timescale.com/learn/postgresql-extensions-pgvector\"></a>, <a href=\"https://github.com/timescale/pgai\"></a>, and <a href=\"https://github.com/timescale/pgvectorscale/\"></a>, it can handle vector similarity search alongside relational queries. </p><p>When building RAG functionality, many developers choose to stick with PostgreSQL, rather than choosing from the myriad new specialized vector databases like <a href=\"https://www.timescale.com/blog/pgvector-vs-pinecone\"></a>, Chroma, and Qdrant, to name but a few. This isn't just about reducing complexity; it's about leveraging battle-tested technology that your team already knows how to operate, monitor, and scale.</p><p>Here‚Äôs how to implement simple semantic search across your product documentation in PostgreSQL. Instead of setting up a separate vector database and building synchronization logic, you can use pgvector and <a href=\"https://github.com/timescale/pgai\"></a> to auto-embed your data and add vector search capabilities directly to your existing database:</p><div><pre><code>SELECT ai.create_vectorizer(\n    'documentation'::regclass,\n    destination =&gt; 'documentation_embeddings',\n    embedding =&gt; ai.embedding_openai('text-embedding-3-small', 768),\n    chunking =&gt; ai.chunking_recursive_character_text_splitter('content')\n);\n</code></pre></div><p>This single SQL command creates and maintains embeddings for your documentation automatically, similar to how PostgreSQL maintains an index. No separate service to deploy, no complex sync logic to debug, no additional failure modes to consider. Pgai is itself built with composability in mind, allowing you to combine different embedding, chunking, or other strategies together through simple, well-defined interfaces. Even if you decide not to use pgai Vectorizer, using pgvector itself saves you from maintaining yet another piece of infrastructure. This is AI tooling done well, in our humble opinion.</p><p>Following the principle of integrating into existing tools and frameworks for folks whose native language isn‚Äôt SQL, we also recently added <a href=\"https://github.com/timescale/pgai/blob/main/docs/vectorizer/python-integration.md\"><u>SQLAlchemy support for pgai</u></a>. Instead of learning yet another query language or API client, this lets you work with vector embeddings using the same query patterns as any Python application you‚Äôve built before:</p><div><pre><code>class Documentation(Base):\n    __tablename__ = \"documentation\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    content: Mapped[str]\n    \n    # Add vector embeddings to your model via an sqlalchemy relationship\n    embeddings = vectorizer_relationship(dimensions=768)\n\n# Semantic search via sqlalchemy queries\nsimilar_docs = (\n    session.query(Documentation)\n    .join(Documentation.embeddings)\n    .order_by(\n        Documentation.embeddings.embedding.cosine_distance(\n            func.ai.openai_embed(\"text-embedding-3-small\", \"How do I setup authentication?\")\n        )\n    )\n    .limit(5)\n    .all()\n)\n</code></pre></div><p>The nice thing about this is that your team <a href=\"https://www.timescale.com/blog/pgai-vectorizer-meets-python-integrating-sqlalchemy-and-alembic\">can leverage their existing SQLAlchemy</a> knowledge, and your vector search integrates seamlessly with your other queries and filters. To integrate this into your application, follow any existing SQLAlchemy + (insert web framework of your choice) guide. For any issues regarding query optimization, you can fall back on decades of PostgreSQL experience either in your team or in the community.</p><p>The AI tooling gold rush has created an ecosystem filled with abstractions looking for problems to solve. While the enthusiasm is understandable, we've seen how this pattern can lead to unnecessary complexity, technical debt, and, ultimately, harder-to-maintain systems. Instead of reinventing the wheel or building entire new ecosystems, the path forward lies in the thoughtful integration of AI capabilities into our existing, battle-tested tools. When evaluating AI tools for your stack, we suggest you follow these principles:</p><ul><li>: Favor tools that build upon existing, well-understood platforms rather than those that require wholesale replacement of working systems. PostgreSQL with pgvector might not be as shiny as the latest vector database, but it works just as well and brings decades of operational knowledge and reliability.</li><li>: Look for tools that solve specific problems well and can be easily integrated with others. LiteLLM's focused approach to LLM API abstraction exemplifies this philosophy, as does pgai's integration with SQLAlchemy.</li><li><strong>Value developer experience</strong>: The best tools feel natural to use with your existing workflows. If the tools integrate well with existing stacks, this reduces your team's cognitive overhead.</li><li><strong>Beware of \"all-in-one\" solutions</strong>: Tools that promise to solve every AI-related problem often create more complexity instead of eliminating it.</li></ul><p>We built <a href=\"https://github.com/timescale/pgai\"></a> with these principles in mind. It focuses on solving specific problems well: automating embedding synchronization through vectorizers, providing a clean interface for LLM interactions, and, most importantly, integrating seamlessly with existing PostgreSQL deployments. We believe this approach‚Äîbuilding on proven foundations while adding new capabilities‚Äîis the sustainable path forward for AI development.</p>","contentLength":11201,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1isfwjw/stop_overengineering_ai_apps_the_case_for_boring/"},{"title":"Good books/video/article to understand ingress controllers","url":"https://www.reddit.com/r/kubernetes/comments/1isfeoc/good_booksvideoarticle_to_understand_ingress/","date":1739892908,"author":"/u/khaloudkhaloud","guid":4242,"unread":true,"content":"<div><p>Any good ressources to \"really\" understand how ingress controllers works</p></div>   submitted by   <a href=\"https://www.reddit.com/user/khaloudkhaloud\"> /u/khaloudkhaloud </a>","contentLength":109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Designing type inference for high quality type errors","url":"https://blog.polybdenum.com/2025/02/14/designing-type-inference-for-high-quality-type-errors.html","date":1739891283,"author":"/u/Uncaffeinated","guid":5847,"unread":true,"content":"<p>Type inference has a reputation for causing unhelpful error messages from the compiler when there is a type error. For example, here‚Äôs a typical comment:</p><p>However, things don‚Äôt have to be this way. Type inference‚Äôs bad reputation is due to design decisions in existing languages that sacrifice good error messages in exchange for other goals. There is nothing inherent to type inference that prevents you from offering good error messages.</p><p>I recently released <a href=\"https://github.com/Storyyeller/polysubml-demo\">PolySubML</a>, a programming language combining global type inference with subtyping and advanced polymorphism, and supporting good type error messages was a constant consideration during development. In this post, I will explain how I designed PolySubML‚Äôs error messages and why I think existing languages tend to fall short in this respect.</p><p>But first, a few disclaimers:</p><p>First off, this post is solely concerned with error messages for . Dealing with , particularly parser errors, is a completely different topic that is outside the scope of this post.</p><p>Second, the focus here is helping the user to <strong>understand why their code won‚Äôt compile</strong> and identify the cause of the error. In some cases, compilers will also attempt to guess what the user meant and suggest a fix, but this is inherently heuristic-based and subjective, and outside the scope of this post.</p><p>Lastly, PolySubML is an experimental hobby programming language that has never been used at large scale. It is a  and  of my ideas, but it is a very different sort of beast than widespread battle-tested languages. Since PolySubML is a one-person hobby project, the focus is on the  and design aspects, rather than aspects like polish which are a function of time and people (the Rust compiler in particular has had almost infinite polish applied over the years, thanks to its incredibly large and dedicated community.)</p><p>With that out of the way, let‚Äôs get on to the pitfalls that often make compiler error messages suck:</p><p>Generally speaking, users think their code is correct when submitting it to the compiler. Sometimes, people will speculatively compile to try to identify bugs or places that need to be updated during a refactor, but even then, the user merely thinks that there might be bugs  in the abstract. They won‚Äôt be convinced of the presence of bugs unless the compiler provides specific evidence explaining where and why there is a bug. (Or rather, a violation of the language‚Äôs typing rules which  but not necessarily indicates a bug.)</p><p><strong>The job of a compiler error message is to prove to the user that their code is invalid</strong> according to the language‚Äôs rules, ideally in a way that helps the user identify where they went wrong and how the problem can be corrected.</p><p>Abstractly, the process of type checking can be modeled as deriving  about the code, with specific rules for deriving new facts based on previous facts, with the rules determined by the language. For example, you might have reasoning along the lines of ‚Äú has type int‚Äù and ‚Äúif  has type int, then after ,  also has type int‚Äù and ‚Äúin ,  is required to be a record‚Äù, and ‚Äúif an expression of type int is used as a record, the program is invalid‚Äù.</p><p>The general form of the rules is ‚Äúif A and B and C, then D‚Äù, where the typechecker continually derives facts from the right hand side of rules once the left hand side is satisfied. Eventually it either derives a contradiction and reports a type error, or it doesn‚Äôt and the code compiles successfully. This leads to proofs that are relatively short and easy to understand - once a contradiction is reached, you can easily work backwards and show the user a sequence explaining exactly why their code is invalid, step by step. (Realistically, you probably don‚Äôt want to show the user  step for reasons of verbosity, but the point is that you  if necessary. More on this in section 3.)</p><p>However, this all goes wrong if your language includes rules of the form ‚Äúif A and B and C, then D ‚Äù. Suddenly, instead of proceeding monotonically from start to finish, the compiler has to  how to proceed. This means that the compiler has to try  in order to discover a type error. For example, if you know A, B, and C, that lets you conclude ‚ÄúD or E‚Äù, but ‚ÄúD or E‚Äù doesn‚Äôt help at all by itself. If say, D turns out to lead to a contradiction, you can‚Äôt immediately report an error like before - instead you have to backtrack and see if E leads to a contradiction .</p><p>The above description is very abstract, so let‚Äôs look at a more concrete example - specifically . In some languages, you might have multiple different functions with the same name and different type signatures, and the compiler needs to try each one in turn and can only report a type error if <em>every single possible choice</em> results in an error.</p><p>For example, in Java, you might have something like</p><div><div><pre><code></code></pre></div></div><p>Where , , and  are distinct types. In order to type check , the compiler has to try all three possible  functions to see if  of them typecheck. If  has type , then it will first try , find a type error, backtrack and try , find a type error, backtrack  and try , and finally find a valid match. If  instead had some  type, say , then the typechecker would have to try all three functions before it could prove there is a type error.</p><p>So why is this so bad? Having to guess and backtrack will make typechecking very slow, but it  makes compiler error messages terrible.</p><p>Without guessing, error messages are simple and easy because there is a direct chain of reasoning leading to a contradiction. However, when the typechecker has to make guesses like with the overloading example, that all goes out the window.</p><p>In order to prove to the user that  has a type error, the compiler has to prove that  does not have type  (with some chain of reasoning)  prove that  does not have type  (with another chain of reasoning)  also prove that  does not have type . Suddenly, the proof of an error is three times as long. But worse yet, <em>this is completely useless to the user</em>.</p><p>The  doesn‚Äôt care about  possibility. The user intended to call  function , not every possible function. Perhaps they  for  to have type  and made a mistake. (Or perhaps  having type  is correct but they mistakenly thought that there is a version of  which takes  as argument.)</p><p>If the user intended  to have type , then what they care about is the proof that  does  have type  (so they can figure out what the mistake was and correct it). They don‚Äôt care at all that the compiler also checked the hypothetical possibilities of  and  and found that those don‚Äôt work either, since they never intended for that to be the case in the first place!</p><p><strong>If you force the typechecker to make guesses, it will guess things the user didn‚Äôt intend, and the resulting error messages will be bloated and irrelevant to the user.</strong></p><p>From the previous example, you might agree that overloading is bad, but not  bad. After all, checking the type of the argument should be trivial in this case since the types involved are so simple (assuming no inheritance or anything at least). However this was the simplest possible case, for the sake of example. Overloading gets worse.  worse.</p><p>For example, usually languages do not force users to annotate a type on <em>literally every expression in the program</em>. Which means that oftentimes the type of something is temporarily unknown and has to be inferred. Instead of just checking the known type of  against , , and  and immediately detecting a contradiction, you might have to go through a whole chain of inference to rule out each case.</p><p>Even worse than that, functions often have multiple arguments, and so guessing which overload to use  influences what types every  argument to the function is expected to take as well. That means that the guess cascades, and for each guess of the first argument, you have to independent check the other arguments, which may in turn involve yet more guesses and backtracking. Likewise, checking the type of the argument may be non trivial when the type is generic, and so checking a guess for the top level type requires recursion to check the type parameters, which in turn involve more guesses.</p><p>This means that such guessing will often blow up . For every top level guess, you have to try every possibility at the second level decision point, and for each of , you have to try every possibility at the third level decision point, etc.</p><p>Even worse is C++, where its template system runs on the principle of <a href=\"https://en.wikipedia.org/wiki/Substitution_failure_is_not_an_error\">‚Äúsubstitution failure is not an error‚Äù</a>, which roughly means ‚Äútry every possibility and only report an error if every single combination of choices leads to a failure‚Äù. This means that usage of templates often results in a) exponential increases in compilation time and b) exponentially large compile error messages. Even worse, abuse of this ‚Äúfeature‚Äù became  in the C++ community (referred to as ‚Äútemplate metaprogramming‚Äù). C++ is legendary for being slow to compile and having  completely incomprehensible error messages, and this is a major reason why.</p><p>Therefore, the first and most important step to ensure good error messages is to <strong>design your type system so the typechecker never has to guess or backtrack</strong>.</p><p>The first rule merely afflicts  real-world programming languages. Now it‚Äôs time to get  controversial with something that nearly every language gets wrong.</p><p>Consider the following Ocaml code:</p><p>Compiling this results in an error message:</p><div><div><pre><code>File \"bin/main.ml\", line 1, characters 12-14:\n1 | let _ = [1; \"\"]\n                ^^\nError: This expression has type string but an expression was expected of type\n        int\n</code></pre></div></div><p>This error message tells us that  has type , which is good, but it also claims that it was expected to have type  for no apparent reason, which is less good. There is nothing inherent about lists in Ocaml that requires them to be ints. Something like  would compile just fine. The  cause of the conflict is something that Ocaml didn‚Äôt highlight at all - the  to the highlighted portion.</p><p>Now in this case, the example is small enough that the user could probably deduce the actual cause of the mismatch just by looking at the code  the point of the error. However, as the code becomes bigger and more complex, that quickly becomes impossible. For example, consider the following:</p><div><div><pre><code></code></pre></div></div><p>This produces the following, rather unhelpful error message:</p><div><div><pre><code>File \"bin/main.ml\", line 19, characters 17-20:\n19 | let _ = 5.3 -. f 2.1\n                      ^^^\nError: This expression has type float but an expression was expected of type\n        int\n</code></pre></div></div><p>That‚Äôs it, the entire compiler output. Again, we can see that the highlighted expression is a , but there‚Äôs absolutely no indication of  Ocaml expected it to be an  instead.</p><p>Unlike the previous example, looking at the surrounding code won‚Äôt make it any clearer either. Even looking at the definition of the  function being called nearby doesn‚Äôt help as  doesn‚Äôt have an explicit type signature. The actual cause of the mismatch is a different  to  in a different part of the code, with no indication of how to find it.</p><p>The fundamental problem here is that when Ocaml requires two types to be equal, it doesn‚Äôt actually keep track of the types that it expected to be equal. It just blindly <strong>assumes that whichever type it happens to see first is the gospel truth</strong> and proceeds under that assumption. Doing it this way does make typechecking slightly faster (due to having to track less data), presumably the reason that the Ocaml compiler is implemented this way. However, the result is completely unhelpful error messages.</p><p>If the user writes , then maybe they intended it to be a list of ints and the  is incorrect. But it could  be the case that they intended to have a list of strings and the  is the incorrect part. (Or possibly, the user was just not aware that Ocaml forbids having both ints and strings in the same list and will be left confused either way as long as the error message doesn‚Äôt explain this restriction.)</p><p>So what would a better error message look like? Let‚Äôs look at how PolySubML handles the same example. The code isn‚Äôt quite directly comparable because PolySubML doesn‚Äôt force type equality like this in the first place, but in this particular example, the end result is still the same, so it still offers a useful comparison.</p><p>Here‚Äôs the same code from before, translated to PolySubML:</p><div><div><pre><code></code></pre></div></div><p>And here‚Äôs what the PolySubML compiler outputs:</p><div><div><pre><code>TypeError: Value is required to have type float here:\n(* blah blah blah *)\nlet _ = 5.3 -. f 2.1;\n            ^~~~~~~~  \nHowever, that value may have type int originating here:\n(* blah blah blah *)\nlet _ = 1 + f 1;\n              ^  \n(* assume lots of code in between here *)\nHint: To narrow down the cause of the type mismatch, consider adding an explicit type annotation here:\n    match v.v &lt;- `Some x with\n    | `None _ -&gt; x\n    | `Some (old: _) -&gt; old\n            +   ++++     \n);\n</code></pre></div></div><p>Notice how PolySubML shows both sides of the conflict, where a value of type  originates and then flows to a place where type  is required. Not only that, but the error message also suggests a place to add a manual type annotation to help narrow down the cause of the mistake. This brings us to the next rule, a technique I came up with for PolySubML which as far as I know has not been done before.</p><p>In my previous language, <a href=\"https://blog.polybdenum.com/2020/07/04/subtype-inference-by-example-part-1-introducing-cubiml.html\">CubiML</a>, type error messages show where a) a value originates with a certain type and then b) flows to a use where it is required to have an incompatible type. For simple cases, this is already enough for the user to understand the problem. However, thanks to type inference, there might be an arbitrarily long and complex path from a) to b) which the user won‚Äôt understand. For example, in the previous section, the int flows into a function call, through a mutable field, a match expression, and then back out of the function to a different call of the same function.</p><p>As described in the previous sections, there is a chain of inference starting from the provided source code and the language‚Äôs rules which lead to a contradiction. However, the compiler doesn‚Äôt know  of that chain contains the problem. The user‚Äôs mistake could be at any point in that chain.</p><p>One approach would be to show the user the entire chain of reasoning leading to the contradiction. That would certainly ensure that the part with the true mistake is also shown. However, long error messages are useless because the user won‚Äôt be able to actually hunt through the lengthy output to find the one part that‚Äôs actually relevant to them. Therefore, we need to keep error messages relatively short, which seems like an impossible contradiction.</p><p>Fortunately, there‚Äôs another possibility - <em>ask the user for clarification</em> to narrow down the location of their mistake. Instead of presenting the entire chain to the user, just ask for the ground truth at one point in the chain, which will in turn rule out one half and allow you to progressively narrow down the location of the mistake.</p><p>For example, look at the last part of the PolySubML error message shown in the previous section:</p><div><div><pre><code>Hint: To narrow down the cause of the type mismatch, consider adding an explicit type annotation here:\n    match v.v &lt;- `Some x with\n    | `None _ -&gt; x\n    | `Some (old: _) -&gt; old\n            +   ++++     \n);\n</code></pre></div></div><p>In the case of a type error, PolySubML makes a list of every inference variable involved in the conflicting chain, and then picks one (usually near the middle) and suggests that the user add an explicit type annotation for it.</p><p>Assuming the user adds a correct type annotation, that narrows down the problem. For example, suppose you have something like <code>int -&gt; x -&gt; y -&gt; z -&gt; float</code>, where a value of type  flows to a use of type , passing through points ,  and , on the way.</p><p>Suppose we suggest that the user add a type annotation to . Perhaps the user intended for it to be an , in which case we get <code>int -&gt; x -&gt; int -&gt; z -&gt; float</code> and the conflict is narrowed down to the  part. Or perhaps they meant for it to be a , in which case the conflict is narrowed down to . Either way, the location of the user‚Äôs mistake has been narrowed down.</p><p>Suggesting locations for type annotations like this is especially effective because that‚Äôs likely what the user would be doing anyway. Faced with a confusing type error that they can‚Äôt figure out, users will often start adding extra type annotations to their code to try to narrow down the problem.</p><p>However, if the user doesn‚Äôt know where the problem is, they often also won‚Äôt know where it would be useful to add type annotations , and will waste a lot of effort without getting anywhere. With PolySubML by contrast, the compiler explicitly highlights a location where adding a type annotation is  to help narrow down the cause of the mistake, leading to much faster and more effective debugging.</p><h2>Aside: Why even have type inference?</h2><p>Opponents of type inference will often ask what the point of even having type inference is if you have to add type annotations to track down errors. First, there‚Äôs the obvious rebuttal that regardless of language, nobody ever annotates  in the program, because that‚Äôs just not feasible or useful, and so everyone supports type inference to some extent, it‚Äôs only a matter of degree.</p><p>But the other point is that having to annotate 5% of your code 5% of the time is much less work than being required to preemptively annotate 100% of your code 100% of the time. And especially with PolySubML, it will lead you directly to the problem, meaning few annotations are required to find it. And if you really want to, you can just remove the type annotations again afterwards (though you‚Äôll often want to leave them around as documentation, etc.)</p><h2>When type annotations aren‚Äôt enough</h2><p>When a type conflict involves one or more inference variables, PolySubML will display the endpoints of the conflict and  will suggest explicitly annotating one of those inference variables to help narrow down the cause of the conflict if it is not clear to the user.</p><p>That‚Äôs all well and good, you might wonder, but what happens in the case of a type conflict that  involve any inference variables? That‚Äôs a good question and I unfortunately don‚Äôt have a good answer to it.</p><p>The good news is that such conflicts are inherently localized. For example, in the PolySubML typechecker, every function has a typed signature. If the user does not provide explicit types for the function, the compiler just implicitly inserts inference variables and uses those instead. This means that any type conflict which does not involve inference variables is also guaranteed to not cross any function boundaries.</p><p>Likewise, the compiler also inserts inference variables when there is no explicit type annotation for mutable record fields, nontrivial pattern matching, polymorphic instantiation, and most variable assignments, among other things. This means that the scope of a type conflict is inherently limited if it does not pass through any inference variables.</p><p>In the case of a conflict with no inference variables, PolySubML will display the endpoints of the conflict (like usual) and also display the expression where the conflict was detected during type checking. For example, in the following code:</p><div><div><pre><code></code></pre></div></div><p>PolySubML‚Äôs error message is as follows:</p><div><div><pre><code>TypeError: Value is required to have type int here:\nlet f = fun (x: int): int -&gt; x + 1;\n                ^~~              \nlet a = 42.9;\nlet _ = f a;\nHowever, that value may have type float originating here:\nlet f = fun (x: int): int -&gt; x + 1;\nlet a = 42.9;\n        ^~~~  \nlet _ = f a;\nNote: Type mismatch was detected starting from this expression:\nlet f = fun (x: int): int -&gt; x + 1;\nlet a = 42.9;\nlet _ = f a;\n        ^    \n</code></pre></div></div><p>Hopefully, these three data points along with the constrained scope of the conflict will be enough for users to understand the issue in most cases. However, I‚Äôm concerned that in especially complex cases, that may not be enough.</p><p>The fact that there are no inference variables involved in a type conflict implies that the code effectively has two conflicting types right next to each other. However, if the types are especially big and complex, the user may not be able to determine the problematic parts, even with the conflicting types side by side.</p><p>I‚Äôm not sure what a good solution to that problem would be. Unlike with chains of , where there is the widely accepted and understood solution of adding intermediate type annotations to narrow down the problem, there‚Äôs no way for the user to explicitly clarify intent <em>within the middle of a complicated type signature itself</em>. If anyone has a solution to this problem, please let me know. (Note that this is a problem any language will have, whether or not you‚Äôre doing type inference.)</p><p>In you‚Äôre going to suggest that the user add explicit type annotations to help narrow down errors, you need to also make it  for the user to add explicit type annotations.</p><p>For example, consider . A generic function is one that operates on placeholder types (aka type variables) which can be substituted for any type later, and in particular can be substituted for multiple  types at different points in the code.</p><p>In PolySubML, an example of a generic function is <code>fun (type t) (x: t): t -&gt; x</code>. This is an identity function which can operate on  type. Instead of the type signature mentioning a specific type, it is defined with the type parameter . Whenever the function is called, the type parameters are substituted with inference variables, with different inference variables at each callsite, a process called .</p><p>This leads to the question: Is there syntax for  type instantiation? PolySubML was designed to follow Ocaml syntax as closely as possible. In Ocaml, there is no syntax for explicitly providing types when instantiating a generic type. They presumably didn‚Äôt see the need, since the instantiated types can always be inferred anyway.</p><p>However, just because the types  be inferred doesn‚Äôt mean there is no need for explicit syntax. After all, the user might want to explicitly provide the types in order to narrow down type errors, document the types, or place additional constraints on the code.</p><p>Consider the following example:</p><div><div><pre><code></code></pre></div></div><p>In the  code,  be instantiated with , in which case it will conflict with the expected return type of . Alternatively, it could be instantiated with , in which case the return type is correct but it conflicts with the argument type of . This code has a conflict, but there‚Äôs no way to know which half the user intended and which half is the mistake. If the user were able to provide an explicit type for , they could indicate which one they meant and thus narrow down the error.</p><p>In PolySubML, type error messages will suggest adding an explicit type annotation to an inference variable if possible, which means that there needs to be a way for the user to supply explicit type annotations for any sort of inference variable, including those generated by generic function instantiation. And this means we need syntax for explicit instantiation.</p><p>Since Ocaml doesn‚Äôt have any syntax for this, I had to make up my own for PolySubML. In PolySubML, you can explicitly instantiate polymorphic types by putting the types in square brackets after the expression, e.g. , , , etc.</p><p>And thus, the above code results in the following error message:</p><div><div><pre><code>TypeError: Value is required to have type float here:\nlet f = fun (type t) (x: t): t -&gt; x;\nlet _: float = f 42;\n    ^~~~~        \nHowever, that value may have type int originating here:\nlet f = fun (type t) (x: t): t -&gt; x;\nlet _: float = f 42;\n                ^~  \nHint: To narrow down the cause of the type mismatch, consider adding an explicit type annotation here:\nlet f = fun (type t) (x: t): t -&gt; x;\nlet _: float = f[t=_] 42;\n                +++++    \n</code></pre></div></div><p>In the previous section, we saw that you have to be careful to ensure that your language syntax offers a place to add explicit type annotations where necessary. However, that‚Äôs not the only thing that can make it impossible for users to add type annotations.</p><p>A much more common problem is that many languages don‚Äôt have <em>syntax for writing the types</em> themselves. For example, consider the following Rust code:</p><div><div><pre><code></code></pre></div></div><p>This code compiles and works just fine. However, <em>there is no possible type annotation you could add</em> to  (the part marked ) and still have your code compile (at least not without using the Nightly compiler and opting into unstable features).</p><p>The problem is that Rust has types which exist  but for which <em>there is no syntax to actually write the type</em>. This means that your code works <em>as long as the types are inferred</em>. However since there is no way to actually  the types you are using, you‚Äôre completely stuck as soon as you need to add explicit type annotations.</p><p>Async streams are especially bad here because a) they tend to have complicated types, especially if you chain multiple stream operations and b) it is normally impossible to write any of the types involved. Debugging errors in Rust code using async streams is an exercise in frustration, which normally consists of staring at the code and making random adjustments until something compiles.</p><p>One time, I wasted considerable time attempting to add explicit type annotations to narrow down the cause of a type error in some stream code I was working on. I even tried breaking it up and adding es so I could use , and I  wasn‚Äôt able to get it working with explicit types and still had no idea what the cause of the original compile error was. I ended up having to completely rewrite the code in question to stop using streams at all since it was impossible to debug compile errors.</p><p>But enough ranting. The point here is that you shouldn‚Äôt do this. <strong>Any type that can be inferred must also be possible to write explicitly</strong>.</p><h2>It‚Äôs harder than you might think</h2><p>Avoiding putting  unwritable types into your language the way Rust did is a good first step. However, that‚Äôs not enough by itself, because it is very easy to  have unwriteable types as well.</p><p>The requirement that every inferrable type also be possible to express explicitly means that the typechecker can‚Äôt have any  that let it do things which can‚Äôt be done in the type syntax. There‚Äôs a constant temptation to say ‚Äúoh lets just add this one extra analysis to the typechecker, that will solve a common pain point and allow more correct code to compile.‚Äù But unless you  add corresponding explicit type syntax (which you usually won‚Äôt, because that makes the language ‚Äúmore complicated‚Äù), you‚Äôve just broken this rule.</p><p>In fact, even if you‚Äôre deliberately trying to follow this rule, it is still very easy to break if you aren‚Äôt careful.</p><p>During the design of PolySubML, this rule was a major consideration, and I even went so far as to add an extra feature (type unions and intersections) to the type syntax just to make necessary types expressible in one specific edge case. And even despite that, I  messed it up!</p><p>In the original release of PolySubML, the following code compiled:</p><div><div><pre><code></code></pre></div></div><p>This code doesn‚Äôt have any actual , so what‚Äôs the problem? The problem is that it is inferring a type that can‚Äôt be written.</p><p>Specifically, there is no annotation that could be written for field  on the first line (where the  is). The  type for  is , where  is the abstract type created by the pattern match on line 2. However, since  is only  on line 2, there is no way for the user to explicitly write it in an annotation on line 1. The compiler is inferring a type that can‚Äôt be written, exactly what I tried so hard to avoid!</p><p>Therefore, I had to modify PolySubML in order to ensure that it can only infer <em>types that were in scope at the point of the inference variable being inferred</em> in order to guarantee that the user could explicitly write that type at that point if they wanted to. Compiling the same code in PolySubML today instead results in a type error, thus satisfying rule 4.</p><p>I won‚Äôt say much here because I already wrote <a href=\"https://blog.polybdenum.com/2022/04/25/when-type-annotations-are-code-too.html\">an entire blog post on the topic</a>, but I figured I should mention this here for completeness, because it is another common design issue that causes type inference to behave in complex and surprising ways, and thus contributes to the bad reputation of type inference.</p><p>Type inference has a reputation for confusing and impossible-to-debug type errors. However, there is no reason why it has to be this way. If you design your language in the right way, you can still have high quality type errors even with powerful global type inference. This does mean avoiding certain features which are often convenient, but I think in the long run, having high quality error messages in your language is the superior tradeoff.</p>","contentLength":28626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iservn/designing_type_inference_for_high_quality_type/"},{"title":"What XOR is and why it's useful","url":"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/","date":1739891133,"author":"/u/mariuz","guid":4271,"unread":true,"content":"<p>[Simon Tatham, 2025-02-14]</p><p>Recently I was called on to explain the ‚ÄòXOR‚Äô operator to\n      somebody who vaguely knew of its existence, but didn‚Äôt have a\n      good intuition for what it was useful for and how it\n      behaved.</p><p>For me, this was one of those ‚Äòfuture shock‚Äô moments when you\n      realise the world has moved on. When I got started in computers,\n      you had to do low-level bit twiddling to get anything very\n      interesting done, so you pretty much couldn‚Äôt \n      learning about XOR. But these days, to a high-level programmer,\n      it‚Äôs much more of an optional thing, and you can perfectly well\n      not know much about it.</p><p>So I collected some thoughts together and gave a lecture on\n      XOR. Slightly to my own surprise, I was able to spend a full\n      hour talking about it ‚Äì and then over the course of the next\n      couple of weeks I remembered several other things I could\n      usefully have mentioned.</p><p>And once I‚Äôd gone to the effort of collecting all those\n      thoughts into one place, it seemed a waste not to write it all\n      down somewhere more permanent. (The collecting is the hardest\n      part!)</p><p>So here‚Äôs a text version of my XOR talk ‚Äì or rather, the talk\n      that it  have been if I‚Äôd remembered to include\n      everything the first time round.</p><p>We‚Äôll start by looking at XOR as a boolean logic operator, or\n      equivalently a logic gate: a function that takes two individual\n      bits, and outputs a single bit.</p><p>If I‚Äôm going right back to first principles, then I should\n      start by actually  XOR.</p><p>Any boolean function of two inputs can be defined by showing\n      its truth table: for each of the four combinations of inputs,\n      say what the output is. So I‚Äôll start by showing the truth table\n      for XOR.</p><figure><figcaption>The truth table of XOR, shown in two different ways</figcaption></figure><p>But just saying what it  doesn‚Äôt give a good\n      intuition for what things it‚Äôs useful for, or when to use it. So\n      in the next few sections I‚Äôll present a few different ways\n      of  what XOR does.</p><p>For such a simple operation, it‚Äôs possible to describe what it\n      does in a lot of quite different-looking ways. But all of them\n      are true at once! So you don‚Äôt need to  thinking\n      about XOR as being just one of the following concepts. You can\n      suddenly switch from thinking of it one way to another, any time\n      that‚Äôs convenient. Nothing stops being true if you do.</p><p>The X in ‚ÄòXOR‚Äô stands for the word ‚Äúexclusive‚Äù. (Some assembly\n      language syntaxes abbreviate it as ‚ÄòEOR‚Äô instead, on the grounds\n      that ‚Äúexclusive‚Äù doesn‚Äôt actually begin with an X.\n      But  people prefer the X spelling, in my\n      experience.)</p><p>In the normal English language, the conjunction ‚Äòor‚Äô has an\n      ambiguity: if I say ‚Äòyou can do this  that‚Äô, and in\n      fact someone tries to do both this  that, does that\n      count? It depends on the context. A parent telling a child ‚ÄúYou\n      can have this dessert  that one‚Äù certainly means ‚Äòbut\n      not both‚Äô ‚Äì that‚Äôs the whole point. But on the other hand, if\n      the same parent wants the child to do something useful, and says\n      ‚ÄúDo your homework, or tidy your room!‚Äù, they‚Äôd probably be\n      extra  if the child did both.</p><p>So, when you want to be clear about which version of ‚Äòor‚Äô you\n      mean, you might say that you‚Äôre  the ‚Äòboth‚Äô\n      case, or  it.</p><p>In computing, ‚ÄòOR‚Äô as a boolean operator is always taken to\n      mean the  version:  or  or\n      both. And when you want  or  but \n      both, you talk about  OR.</p><p>Looking at the truth tables above, you can see that that‚Äôs\n      exactly what the XOR operation is doing:</p><ul><li>If &nbsp;=&nbsp;1,  if &nbsp;=&nbsp;1,\n        then &nbsp;XOR&nbsp;&nbsp;=&nbsp;1.</li><li>But not both: if  are both 1,\n        then &nbsp;XOR&nbsp;&nbsp;=&nbsp;0.</li></ul><h4>The ‚Äònot equals‚Äô operator</h4><p>Another way to look at the same truth table\n      is: &nbsp;XOR&nbsp;&nbsp;=&nbsp;1 whenever the inputs \n      and  are  from each other. If they‚Äôre\n      the same (either both 0 or both 1),\n      then &nbsp;XOR&nbsp;&nbsp;=&nbsp;0.</p><p>So you could look at &nbsp;XOR&nbsp; as meaning the same\n      thing as &nbsp;‚â†&nbsp;: it‚Äôs a way to  two\n      boolean values with each other.</p><p>A third way to look at the same truth table is to consider each\n      value of one of the inputs , and look at what XOR does\n      to the other variable  in each case:</p><ul><li>If &nbsp;=&nbsp;0, then &nbsp;XOR&nbsp; is the same thing as just .</li><li>If &nbsp;=&nbsp;1, then &nbsp;XOR&nbsp; is the  of : 0 becomes 1 and 1 becomes 0.</li></ul><p>So another way to look at the XOR operation is that you‚Äôre\n      either going to leave  alone, or invert it (swapping 0\n      and 1), and  tells you which one to do. If you imagine\n      XOR as a tiny computing device, you could think of the\n      input  as ‚Äòdata‚Äô that the device is operating on,\n      and  as a ‚Äòcontrol‚Äô input that tells the device what to\n      do with the data, with the possible choices being ‚Äòflip‚Äô or\n      ‚Äòdon‚Äôt flip‚Äô.</p><p>&nbsp;XOR&nbsp; means: <strong>invert , but only\n        if  is true.</strong></p><p>But the same is  true the other way round,\n      because &nbsp;XOR&nbsp; is the same thing\n      as &nbsp;XOR&nbsp;. You can swap your point of view to\n      thinking of  as the ‚Äòdata‚Äô input and  as\n      ‚Äòcontrol‚Äô, and nothing changes ‚Äì the operation is the same\n      either way round.</p><p>That is, &nbsp;XOR&nbsp; also\n      means: <strong>invert , but only if  is\n      true!</strong></p><p>Here‚Äôs yet  way to look at the XOR\n      operation. &nbsp;XOR&nbsp; tells you whether an  of the inputs are true:</p><ul><li>If &nbsp;=&nbsp;&nbsp;=&nbsp;0, then  of the inputs\n        are true. 0 is even, and &nbsp;XOR&nbsp;&nbsp;=&nbsp;0.</li><li>If &nbsp;=&nbsp;1 and &nbsp;=&nbsp;0, or the other way round,\n        then  of the inputs is true. 1 is odd,\n        and &nbsp;XOR&nbsp;&nbsp;=&nbsp;1.</li><li>If &nbsp;=&nbsp;&nbsp;=&nbsp;1, then  of the inputs\n        are true. 2 is even, and &nbsp;XOR&nbsp;&nbsp;=&nbsp;0 again.</li></ul><p>Put another way: if you  the two inputs,\n      and then reduce the result modulo 2 (that is, divide by 2 and\n      take the remainder), you get the same answer\n      as &nbsp;XOR&nbsp;.</p><p>In particular, if you XOR together  than two\n      values, the overall result will tell you whether an odd or even\n      number of the inputs were 1 ‚Äì no matter how many inputs you\n      combine.</p><p>So XOR corresponds to addition mod 2. But it also corresponds\n      to  mod 2, at the same time: if you take the\n      difference &nbsp;‚àí&nbsp; and reduce  mod 2,\n      you still get the same answer &nbsp;XOR&nbsp;!</p><ul><li>If &nbsp;=&nbsp;, then the\n        difference &nbsp;‚àí&nbsp; is 0, and so\n        is &nbsp;XOR&nbsp;.</li><li>If &nbsp;‚â†&nbsp;, then the\n        difference &nbsp;‚àí&nbsp; is either +1 or ‚àí1. Mod 2,\n        those are the same as each other ‚Äì they‚Äôre both just 1.</li></ul><h3>What properties does it have?</h3><p>If you have a complicated boolean expression involving lots of\n      XORs, it‚Äôs useful to know how you can manipulate the expression\n      to simplify it.</p><p>To begin with, XOR is both \n      and , which mean (respectively)\n      that</p><p align=\"center\">(&nbsp;XOR&nbsp;)&nbsp;XOR&nbsp;&nbsp;=&nbsp;&nbsp;XOR&nbsp;(&nbsp;XOR&nbsp;)</p><p>In practice, what this means is that if you see a long list of\n      values or variables XORed together, you <em>don‚Äôt need to\n      worry</em> about what order they‚Äôre combined in, because it\n      doesn‚Äôt make any difference. You can rearrange the list of\n      inputs into any order you like, and choose any subset of them to\n      combine first, and the answer will be the same regardless.</p><p>(This is perhaps most easily seen by thinking of XOR as\n      ‚Äòaddition mod 2‚Äô, because addition is also both commutative and\n      associative ‚Äì whether or not it‚Äôs mod anything ‚Äì so XOR inherits\n      both properties.)</p><p>Secondly, , which means\n      that XORing anything with zero just gives you the same thing you\n      started with:</p><p>So if you have a long list of things XORed together, and you\n      can see that one of them is 0, you can just delete it from the\n      list.</p><p>Thirdly, <strong>everything is\n      self-inverse</strong>: if you XOR any value with ,\n      you always get zero.</p><p>One consequence of this is that if you have a long list of\n      variables being XORed together, and the same variable occurs\n      twice in the list, you can just remove both copies. <em>Anything\n      appearing twice cancels out.</em> For example, the two copies\n      of  can be removed in this expression:</p><p>Another consequence is that if you have a\n      value &nbsp;XOR&nbsp;, and you want to recover\n      just , you can do it by XORing in another copy\n      of  (if you know it), to cancel the one that‚Äôs already\n      in the expression. <em>Putting something in a second time is the\n      same as removing it:</em></p><p align=\"center\">(&nbsp;XOR&nbsp;)&nbsp;XOR&nbsp;&nbsp;=&nbsp;&nbsp;XOR&nbsp;(&nbsp;XOR&nbsp;)&nbsp;=&nbsp;&nbsp;XOR&nbsp;0&nbsp;=&nbsp;</p><p>Now we‚Äôve seen what XOR does on individual bits, it‚Äôs time to\n      apply it to something larger.</p><p>To a computer it‚Äôs most natural to represent an integer in\n      binary, so that it looks like a string of 0s and 1s. So if you\n      have two integers, you can combine them  (that\n      is, ‚Äòtreating each bit independently‚Äô) using a logical operation\n      like XOR: you write the numbers in binary one above the\n      other, and set each output bit to the result of\n      combining the corresponding bits of both input numbers via\n      XOR.</p><p>Here are a couple of examples, one small and carefully chosen,\n      and the other large and random-looking:</p><figure><div><div><table><tbody></tbody></table></div><div><table><tbody><tr><td><code>11001111001000011111000101111011</code></td></tr><tr><td><code>10011011010000100100111001011101</code></td></tr><tr><td><code>01010100011000111011111100100110</code></td></tr></tbody></table></div></div><figcaption>Small and large examples of applying bitwise XOR to two integers</figcaption></figure><p>The small example contains one bit for each element of the XOR\n      truth table. If you look at vertically aligned bits in the\n      binary column of the table, you‚Äôll see that in the rightmost\n      place both  and  have a 0 bit; in the leftmost\n      place they both have a 1 bit; in between, there‚Äôs a position\n      where only  has a 1, and one where only  has a\n      1. And in each position, the output bit is the boolean XOR of\n      the two input bits.</p><p>Of course, this idea of ‚Äòbitwise‚Äô logical operations ‚Äì taking\n      an operation that accepts a small number of input bits, and\n      applying it one bit at a time to a whole binary integer ‚Äì is not\n      limited to XOR. Bitwise AND and bitwise OR are also well defined\n      operations, and both useful. But this particular article is\n      about XOR, so I‚Äôll stick to that one.</p><h3>Still has all the same properties</h3><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#properties\">Earlier</a> I discussed a number of\n      mathematical laws obeyed by the one-bit version of XOR: it‚Äôs\n      commutative, associative, has 0 as an identity, and every\n      possible input is self-inverse in the sense that XORing it with\n      itself gives 0.</p><p>In the bitwise version applied to integers, all of these things\n      are still true:</p><ul><li>&nbsp;XOR&nbsp;&nbsp;=&nbsp;&nbsp;XOR&nbsp; for any integers  and </li><li>(&nbsp;XOR&nbsp;)&nbsp;XOR&nbsp;&nbsp;=&nbsp;&nbsp;XOR&nbsp;(&nbsp;XOR&nbsp;) similarly</li><li>&nbsp;XOR&nbsp;0&nbsp;=&nbsp;0&nbsp;XOR&nbsp;&nbsp;=&nbsp;, where 0 means the  zero, i.e. with all its binary bits 0</li></ul><p>So if you have a complicated expression containing bitwise\n      XORs, you can simplify it in all the same ways you could do it\n      with single-bit XORs.</p><p>With individual bits, I <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#unequal\">said earlier</a>\n      that &nbsp;XOR&nbsp; means the same thing\n      as &nbsp;‚â†&nbsp;: given two input bits, it returns 1 if\n      the bits are different from each other, and 0 if they‚Äôre the\n      same.</p><p>Of course, applied bitwise to whole integers, this is true\n      separately in every bit position: each bit of the output integer\n      tells you whether the two corresponding input bits were unequal.</p><p>So if &nbsp;=&nbsp;, then &nbsp;XOR&nbsp;&nbsp;=&nbsp;0. And\n      if &nbsp;‚â†&nbsp;, then &nbsp;XOR&nbsp;&nbsp;‚â†&nbsp;0, because\n      two unequal integers must have disagreed in at least one bit\n      position, so that bit will be set in their XOR.</p><p>So in some sense you can still use bitwise XOR to tell you\n      whether two entire integers are equal: it‚Äôs 0 if they are, and\n      nonzero if they‚Äôre not. But bitwise XOR gives you more detail\n      than that: it also gives you a specific list of <em>which bit\n      positions</em> they differ in.</p><h3>Bitwise conditional inverter</h3><p>I also said <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#condinv\">earlier</a> that you could see\n      XOR as a ‚Äòconditional inversion‚Äô operator: imagine one\n      input  to be ‚Äòdata‚Äô, and the other input  to be\n      a ‚Äòcontrol‚Äô input that says whether you want to flip the data\n      bit.</p><p>Applied bitwise to whole integers, this is still true, but more\n      usefully so: the control input  says  data\n      bits you want to flip.</p><p>For example, in the Unicode character encoding (and also in its\n      ancestor ASCII), every character you might need to store in a\n      string is represented by an integer. The choice of integer\n      encodings has some logic to it (at least in places). In\n      particular, the upper-case Latin letters A,&nbsp;B,&nbsp;C,&nbsp;‚Ä¶,&nbsp;Z and their\n      lower-case equivalents a,&nbsp;b,&nbsp;c,&nbsp;‚Ä¶,&nbsp;z have encodings that differ\n      in just one bit: A is 65, or binary 01000001, and a is 97, or\n      binary 01100001. So if you know that a character value\n      represents a Latin letter, you can swap it from upper case\n      to lower case  by flipping just that one\n      bit between 0 and 1.</p><p>And you can do that most easily, in machine code or any other\n      programming language, by XORing it with a number that has just\n      that one bit set, namely binary 00100000, or decimal 32. For\n      example, in Python (where the  operator means\n      bitwise XOR):</p><figure><div><div><pre>&gt;&gt;&gt; chr(ord('A') ^ 32)\n'a'\n&gt;&gt;&gt; chr(ord('x') ^ 32)\n'X'</pre></div></div><figcaption>Flipping case by XORing with 32 in Python</figcaption></figure><p>Of course, the other thing I said earlier also applies: it\n      doesn‚Äôt matter  of the inputs to XOR you regard as\n      the data, and which as the control. The operation is the same\n      both ways round.</p><p>In particular, suppose you‚Äôve already XORed two values \n      and  to obtain a number  that tells you which\n      bits they differ in. Then you can turn either of \n      or  into the other one, by XORing with the\n      difference , because that flips exactly the set of bits\n      where  has the opposite value to .</p><p>In other words, these three statements are\n      all  ‚Äì if any one of them is true, then so\n      are the other two:</p><h3>Addition or subtraction without carrying</h3><p>That isn‚Äôt quite how it works once you do it bitwise on whole\n      words. Each individual pair of bits is added mod\n      2, but each of those additions is independent.</p><p>One way to look at this is: suppose you were a schoolchild just\n      learning addition, and you‚Äôd simply forgotten that it was\n      necessary to carry an overflowed sum between digit positions at\n      all. So for every column of the sum, you‚Äôd add up the input\n      digits in that column, write down the low-order digit of\n      whatever you got, and ignore the carry completely.</p><p>If this imaginary schoolchild were to do this procedure in\n      binary rather than decimal, the operation they‚Äôd be performing\n      is exactly bitwise XOR! <strong>Bitwise XOR is like binary\n      addition, without any carrying.</strong></p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#halfadder\">Later on</a> I‚Äôll show an interesting\n      consequence of this, by considering what  happen to\n      the carry bits, and how you can put them back in again\n      afterwards. There‚Äôs also a <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#gf2poly\">mathematically\n      meaningful and useful interpretation</a> of the corresponding\n      ‚Äòcarryless‚Äô version of , in which you\n      make a shifted version of one of the inputs for each 1 bit in\n      the other, and then add them together in this carryless XOR\n      style instead of via normal addition.</p><p>Now we‚Äôve seen what XOR , and a few different things\n      it‚Äôs , let‚Äôs look at some things it‚Äôs used for.</p><h3>Cryptography: combine a plaintext with a keystream</h3><p>XOR is used all over cryptography, in many different ways. I\n      won‚Äôt go into the details of  of those ways ‚Äì I‚Äôm\n      sure I don‚Äôt even know all of them myself! ‚Äì but I‚Äôll show a\n      simple and commonly used one.</p><p>Suppose you have a message to encrypt. One really simple thing\n      you could do would be to take an equally long stream of\n      random-looking data ‚Äì usually known as a ‚Äòkeystream‚Äô ‚Äì and\n      combine the two streams, a byte or a word at a time, in some way\n      that the receiver can undo. So if your message was ‚Äúhello‚Äù, for\n      example, you might simply transmit a byte made by combining ‚Äúh‚Äù\n      with keystream byte #1, then one that‚Äôs ‚Äúe‚Äù combined with\n      keystream byte #2, and so on.</p><p>This seems absurdly simple ‚Äì surely real cryptography is\n      terrifyingly complicated compared to this? But it really is a\n      thing that happens. As long as each byte is combined with the\n      keystream byte in a way that makes it possible to recover the\n      original byte at the other end, this is a completely sensible\n      way to encrypt a message!</p><p>That‚Äôs not to say that there isn‚Äôt terrifyingly complicated\n      stuff  in the setup. But in this particular\n      context, the complicated part is in how you \n      your stream of random-looking bytes; the final step where you\n      combine it with the message is the easy part. One option is for\n      your keystream to be a huge amount of genuinely random data, as\n      big as the total size of all messages you‚Äôll ever need to send;\n      this is known as a ‚Äòone-time pad‚Äô, and is famously actually\n      unbreakable ‚Äì but also amazingly impractical for almost all\n      purposes. More usually you use a stream cipher, or a block\n      cipher run in a counter mode, to expand a manageably small\n      actual  into a keystream as long as you need.</p><p>Anyway. I‚Äôve so far left out the detail of exactly how you\n      ‚Äúcombine‚Äù the keystream with the message. But given the subject\n      of this article, you can probably guess that it‚Äôs going to be\n      XOR.</p><p>XOR isn‚Äôt the only thing that would . If your\n      message and your keystream are each made of bytes, then there\n      are plenty of other ways to combine two bytes that let you\n      recover one of them later. For example, addition mod\n      2 would be fine: you could make each encrypted byte\n      by  the message byte and keystream byte, and then\n      the receiver (who has access to exactly the same keystream)\n      would subtract the keystream byte from the sum to recover the\n      message byte.</p><p>But in practice, XOR is generally what‚Äôs used, because it‚Äôs\n      simpler. Not so much for software ‚Äì CPUs generally have an ‚ÄòADD‚Äô\n      instruction and an ‚ÄòXOR‚Äô instruction which are just as fast and\n      easy to use as each other ‚Äì but encryption is also often done in\n      hardware, by specially made circuitry. And if you‚Äôre building\n      hardware, addition is more complicated than XOR, because you\n      have to worry about carrying between bit positions, which costs\n      more space on the chip (extra logic gates) and also extra time\n      (propagating signals from one end to the other of the addition).\n      XOR avoids both problems: in custom hardware, it‚Äôs far cheaper.</p><p>(It‚Äôs also  slightly more convenient that the\n      sender and receiver don‚Äôt have to do \n      operations. With XOR, the sender who applies the keystream and\n      the receiver who takes it off again are both doing exactly the\n      same thing, instead of one adding and the other\n      subtracting.)</p><h3>Pixel graphics: draw things so it‚Äôs easy to undraw\n      them again</h3><p>Let‚Äôs set the wayback machine to the mid-1980s, and go back in\n      time to when computers were smaller and simpler (at least, the\n      kind you could afford to have in your home). Home computer\n      graphics systems stored a very small number of bits for each\n      pixel on the screen, meaning that you could only display a\n      limited number of different colours at a time; and even with\n      that limitation on framebuffer size, home computers had so\n      little RAM in total that it was a struggle to store two entire\n      copies of what was displayed on the screen and still have enough\n      memory for anything else (like whatever program\n      was  that screenful of graphics).</p><p>In an environment limited like that, what do you do if you want\n      to draw an object that appears and disappears, or that moves\n      around gradually?</p><p>If your moving object is , then every time you\n      ‚Äòundraw‚Äô it, you have to restore whatever was supposed to be on\n      the screen behind it. That means you have to \n      what was behind it ‚Äì either by storing the actual pixels, or by\n      storing some recipe that knows how to recreate the missing patch\n      of graphics from scratch. Either one costs memory, and the\n      second option probably costs time as well, and you don‚Äôt have a\n      lot of either to spare.</p><p>Nevertheless, you do that when you really have to. But when\n      you  have to, it‚Äôs always helpful to take\n      shortcuts.</p><p>So one common graphical dodge was: <em>don‚Äôt make graphical\n      objects opaque if you don‚Äôt have to</em>. Any time you can get\n      away with it, prefer to draw a thing on the screen in a way that\n      is : combine each pixel  of the\n      moving object with the existing screen pixel , in such a\n      way that you can undo the operation later, recovering the\n      original screen pixel  from the combined\n      value  by remembering what  was.</p><p>As in the previous section, there are plenty of ways to do this\n      in principle. You could imagine treating the bits of each pixel\n      as an -bit integer for some small , and doing\n      addition and subtraction on them (again, mod\n      2). For example, you could draw by addition,\n      setting &nbsp;=&nbsp;&nbsp;+&nbsp;, and undraw by\n      subtraction to recover &nbsp;=&nbsp;&nbsp;‚àí&nbsp;.</p><p>But these systems typically had far fewer than 8 bits per\n      pixel, so each byte of the screen would have more than one pixel\n      packed into it. Or, worse, the screen might be laid out in ‚Äòbit\n      planes‚Äô, with several widely separated blocks of memory each\n      containing one bit of every pixel. Doing ordinary addition on\n      the pixel values is very awkward in both cases.</p><p>In particular, consider the first of those possibilities, where\n      you have several pixels packed into a byte. Suppose the 8 bits\n      of the byte are treated as two 4-bit integers, or four 2-bit\n      integers. In order to do parallel  of each of\n      those small chunks, you can‚Äôt use the normal CPU‚Äôs addition\n      instruction, because a carry off the top of one pixel value\n      propagates into the next, causing unwanted graphical effects. So\n      you‚Äôd somehow need to arrange to suppress the\n      carry  pixels, but leave the\n      carry  a pixel alone.</p><p>So it‚Äôs much easier not to try this in the first place.\n      Combining the pixel values via XOR instead of addition means\n      that you automatically avoid carries between pixels, because\n      there are no carries . This is also more\n      convenient in the ‚Äòbit plane‚Äô memory layout, because each bit of\n      a pixel is treated independently; if you tried to do ordinary\n      addition in that situation, you‚Äôd have to make extra effort to\n      propagate carries between corresponding bits in each bit\n      plane.</p><p>Here‚Äôs a simple example, in which two lines have been drawn\n      crossing each other. You can see that in the second diagram,\n      drawn using XOR, there‚Äôs a missing pixel at the point where the\n      two lines cross. That pixel is part of  lines, so\n      it‚Äôs been drawn twice. Each time it was drawn, the pixel value\n      flipped between black and white, so drawing it twice sets it\n      back to the background colour.</p><p>That missing pixel looks like a tiny blemish. The first version\n      of the picture looks . But it makes it harder to\n      undraw one of those lines later. If you just undraw it by\n      setting all the pixels of one line back to white, then you leave\n      a hole in the remaining line. And if you don‚Äôt want to leave a\n      hole, then you need some method of remembering which\n      pixel  to reset to white.</p><p>In the XOR version of the picture, it‚Äôs the easiest thing in\n      the world to undraw one line and leave the other one undamaged.\n      Simply draw the line  that you want to\n      undraw. <em>Putting a second copy of something into an XOR\n      combination is the same as removing it.</em> This kind of\n      graphical ‚Äòblemish‚Äô was considered a reasonable price to pay, in\n      situations where efficient undrawing and redrawing was\n      needed.</p><p>Most types of 1980s home computers had an option to draw in XOR\n      mode, for this kind of reason. It was one of the easiest ways\n      for a beginning computer programmer to draw pretty animations. A\n      common type of demo was to simulate two points moving around the\n      screen along a pair of independent paths, and draw a line\n      between the two points at each moment ‚Äì but to allow the\n      previous few versions of the line to persist as well as the\n      latest one, undrawing each one after it had been on the screen\n      for a few frames.</p><p>Here‚Äôs an example of the kind of thing. In this example each\n      end of the line is following a path given by a different\n      <a href=\"https://en.wikipedia.org/wiki/Lissajous_curve\">Lissajous\n      curve</a>:</p><p>Because we‚Äôre drawing the lines in XOR mode, the procedure for\n      turning each frame of this animation into the next involves\n      drawing only two lines: the new one that we‚Äôre adding at the\n      leading edge of the motion, and the one that‚Äôs about to vanish\n      from the trailing edge. So the program only has to remember the\n      endpoint coordinates of the 10 (or however many) lines it\n      currently has on screen ‚Äì or perhaps not even bother with that,\n      and just recalculate the (&nbsp;‚àí&nbsp;10)th coordinate in each\n      frame to work out what line to undraw.</p><p>So this lets you produce pretty and interesting results, with\n      almost no memory cost (you don‚Äôt have to store extra copies of\n      lots of pixels). It uses very little CPU as well (you don‚Äôt have\n      to redraw  the lines in every frame, only the ones\n      you‚Äôre adding and removing), which makes the animation able to\n      run faster.</p><p>This style of animation was a common thing for beginning\n      programmers to play with, but it was also used by professionals.\n      Perhaps most famously, a moving line drawn in this style acted\n      as the main antagonist of the 1981 video\n      game <a href=\"https://en.wikipedia.org/wiki/Qix\">Qix</a>.</p><p>Even in the 1990s, with slightly larger computers, XOR-based\n      drawing and undrawing was still a useful thing: in early GUI\n      environments like the Amiga, when you dragged a window around\n      the screen with the mouse, the system didn‚Äôt have enough CPU to\n      redraw the entire window as you went. Instead it would draw an\n      outline frame for where the window was going to end up, and once\n      you‚Äôd put it in the right place, you‚Äôd let go of the mouse\n      button and the window would be properly redrawn in the new\n      location, just once. And again the outline frame would be drawn\n      using XOR, so that it didn‚Äôt cost extra memory to remember how\n      to undraw it on each mouse movement.</p><p>But going back to the above animation: if you watch it\n      carefully, you might notice that when the moving endpoints\n      change direction and a lot of the lines cross each other, the\n      missing pixels at the crossing points give rise to a sort of\n      interference effect. Those missing pixels started off as a\n      necessity to save memory, but it turns out they‚Äôre also quite\n      pretty in their own right.</p><p>Another favourite kind of 1980s graphical program for beginners\n      was to explore those interference patterns more fully. Here‚Äôs an\n      image created by drawing a sequence of lines, all with one\n      endpoint in the bottom left corner of the frame, and with the\n      other endpoints being every single pixel along the top row. In\n      ordinary drawing mode this would just be a slow way to fill in a\n      big black triangle. But in XOR mode, the lines interfere with\n      each other wherever more than one of them passes through the\n      same pixel:</p><p>That‚Äôs actually interesting, and it takes some time to work out\n      what‚Äôs going on!</p><p>The topmost row is all black, because every pixel on that row\n      is drawn just once, by the line whose endpoint is at exactly\n      that position. But half way up, there‚Äôs an all-\n      row, because the  lines in the image have\n      only &nbsp;/&nbsp;2 pixels to pass through on their way to the top\n      row. So you get two lines passing through each pixel, so every\n      pixel is drawn exactly twice, and cancels out back to white.\n      Similarly, a third of the way up there‚Äôs another black row where\n      every pixel is drawn exactly  times, and so on.\n      And generally, 1&nbsp;/&nbsp; of the way from bottom to top, you\n      expect an all-black row if  is odd, or an all-white row\n      if  is even.</p><p>But in between those 1&nbsp;/&nbsp; rows, something much more\n      interesting happens, when the number of lines going through each\n      pixel isn‚Äôt the same everywhere on the row. Those interference\n      patterns are related to the distribution of rounding errors in\n      computing the horizontal position of each pixel of the line.</p><p>I‚Äôll end this section with a question that  don‚Äôt\n      know the answer to. Suppose you continue this\n      interference-pattern drawing further to the right, past the\n      point where the line slopes at 45¬∞:</p><p>At the 45¬∞ mark, something changes fundamentally. Those\n      horizontal black and white stripes have changed direction, and\n      now they‚Äôre sloping at what looks like a constant angle.</p><p>I understand why  has changed. In this style\n      of pixelated line drawing, a line that‚Äôs closer to horizontal\n      than vertical can have multiple pixels on each row, and only one\n      in each column, whereas if it‚Äôs closer to vertical than\n      horizontal, it‚Äôs the other way round. So that‚Äôs a good reason\n      why the picture should look fundamentally different\n      in  way on opposite sides of the 45¬∞ line.</p><p>But why does that give rise to a \n      set of coloured stripes, and not any other kind of weird effect?\n      I‚Äôve never sat down and worked that one out.</p><h3>The ‚Äúhalf-adder identity‚Äù</h3><p>I said <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#mod2\">in a previous section</a> that one way\n      to look at the expression &nbsp;XOR&nbsp; is that it‚Äôs the\n      remainder mod 2 of &nbsp;+&nbsp;. Another way to say that\n      is that it‚Äôs the \n      of &nbsp;+&nbsp;.</p><p>To answer that, let‚Äôs start with a truth table:</p><figure><div><div><table><tbody></tbody></table></div></div><figcaption>Truth tables for the two bits of the sum &nbsp;+&nbsp;</figcaption></figure><p>Here, I‚Äôve shown the actual value of the\n      sum &nbsp;+&nbsp; for each possible input. Then I‚Äôve\n      written each of those values in binary, so that 0 and 1 become\n      00 and 01, and 2 becomes 10. You can see that the high bit of\n      the sum is only 1 if  inputs are set.</p><p>In other words: the low bit of &nbsp;+&nbsp;\n      is &nbsp;XOR&nbsp;, and the high bit\n      is &nbsp;AND&nbsp;. In other words, &nbsp;+&nbsp;&nbsp;=&nbsp;(&nbsp;XOR&nbsp;)&nbsp;+&nbsp;2&nbsp;√ó&nbsp;(&nbsp;AND&nbsp;).</p><p>That makes sense, if  and  are individual bits.\n      What if we did the same operations bitwise, on whole integers?</p><p>If  and  are 32-bit integers (for example),\n      then we‚Äôve already <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#carryless\">seen above</a>\n      that &nbsp;XOR&nbsp; is the integer you get if you add the\n      bits in each position, but not carrying from one bit position to\n      the next. But also, &nbsp;AND&nbsp; is an integer\n      consisting of  those carry bits: the th\n      bit of &nbsp;AND&nbsp; is 1 precisely when a carry\n      bit  have been generated in that bit position but\n      wasn‚Äôt.</p><p>If there should have been a carry bit out of the th\n      place, then it should have been added to the (&nbsp;+&nbsp;1)st\n      place. But it‚Äôs not too late ‚Äì we can add it anyway!</p><p>In fact, the above equation for single bits ,&nbsp;\n      is  true, even when ,&nbsp; are whole\n      integers, and the XOR and AND operators are bitwise:</p><p align=\"center\">&nbsp;+&nbsp;&nbsp;=&nbsp;(&nbsp;XOR&nbsp;)&nbsp;+&nbsp;2&nbsp;√ó&nbsp;(&nbsp;AND&nbsp;)</p><p>Just to show it in action, here‚Äôs a demonstration set of\n      values, chosen so that the binary values cover a lot of possible\n      bit combinations, but the decimal values make it immediately\n      clear that the final line is also &nbsp;+&nbsp;:</p><figure><div><div><table><tbody><tr><td><code>00111101011011000100101001010010</code></td></tr><tr><td><code>00001100001010011011100010000000</code></td></tr><tr><td><code>00001100001010000000100000000000</code></td></tr><tr><td><code>00011000010100000001000000000000</code></td></tr><tr><td><code>00110001010001011111001011010010</code></td></tr><tr><td><code>01001001100101100000001011010010</code></td></tr></tbody></table></div></div><figcaption>A demonstration case of adding via AND and XOR</figcaption></figure><p>I think of this as the ‚Äúhalf-adder identity‚Äù (although as far\n      as I know that‚Äôs not a standard name). The name comes from the\n      hardware design concept of a ‚Äúhalf-adder‚Äù, which is a small\n      logic component consisting of just an AND and XOR gate, doing\n      addition of two bits in hardware:</p><p>It‚Äôs called a  adder because in order to do full\n      binary addition you need two of these per bit, because you have\n      to add  input bits: two from the input integers\n      and one carried from the bit position to the right. So you build\n      a  adder out of two half-adders, one\n      adding  to  and the second adding the sum of\n      those two to , with one extra\n      gate to\n      combine the two carry bits:</p><p>‚ÄúBut hang on,‚Äù you might be thinking. ‚ÄúIf you need two\n      half-adders per bit to do a full addition, how is that\n      ‚Äòhalf-adder identity‚Äô of yours getting away with\n      just  pair of AND and XOR operations per bit?‚Äù Or\n      you might think: ‚ÄúYou‚Äôve generated an  carry from\n      each bit position, but where have you handled the \n      carry?‚Äù Or you might think ‚ÄúHang on, how do you arrange for a\n      carry in the lowest bit of the addition to potentially propagate\n      all the way up to the top, when the only left shift in this\n      expression moves data by only one bit?‚Äù</p><p>The answer to all three questions is: because of the + sign on\n      the right-hand side of the identity. After we compute the\n      outputs of  half-adder on each pair of input bits,\n      producing a word full of low bits and a word full of carries ‚Ä¶\n      we recombine the two words . That‚Äôs\n      what finishes the job of propagating carries.</p><p>In other words, unlike the hardware half-adder, the ‚Äúhalf-adder\n      identity‚Äù  build an addition out of only simpler\n      operations. It builds an addition out of two simpler\n      operations .</p><p>‚ÄúWell, in  case,‚Äù you might think, ‚Äúisn‚Äôt it a\n      complete cheat, and not useful for anything?‚Äù</p><p>Not quite! It‚Äôs true that this identity is not \n      useful. But ‚Äònot often‚Äô isn‚Äôt ‚Äònever‚Äô, and in unusual\n      circumstances there are uses for it.</p><p>Here‚Äôs one occasional use for the half-adder identity.</p><p>Suppose you need to calculate the average of two integers, each\n      the size of your CPU‚Äôs registers (let‚Äôs say, 32 bits). In other\n      words, you want to add two integers, and then divide by 2 (or,\n      equivalently, shift right by 1 bit).</p><p>But adding two 32-bit integers makes a 33-bit sum, which\n      doesn‚Äôt fit in your register. If you just do the simple thing ‚Äì\n      add and then shift ‚Äì then you potentially get the wrong answer,\n      because the topmost bit has been lost in between the addition\n      and the right shift.</p><p>Most CPUs have a good answer to this. The add instruction can\n      (perhaps optionally) set a ‚Äòcarry flag‚Äô to indicate whether a 1\n      bit was carried off the top of an addition; also, there‚Äôs\n      usually a special form of right shift by 1 bit that brings the\n      carry bit back in to the top of the word. (On Arm, that\n      instruction is called RRX; on x86, RCR.) So the way to average\n      two numbers without an overflow problem is to do ADD followed by\n      RRX.</p><p>But a few CPUs can‚Äôt do it that way. Some architectures don‚Äôt\n      have a carry flag at all, or indeed any other flags: for\n      example, MIPS, RISC-V, and the historic DEC Alpha. In other\n      situations, there is a carry flag, but no convenient RRX\n      instruction, so that it would take a lot of effort to recover\n      the carry bit and put it at the top of the shifted-right word;\n      for example, the initial versions of Arm‚Äôs space-efficient\n      ‚ÄúThumb‚Äù instruction set discarded the RRX instruction, simply\n      because there wasn‚Äôt room for everything.</p><p>So, if you  do ADD and RRX, how do you most\n      efficiently compute your average?</p><p>The half-adder identity provides the answer. If the sum of the\n      two inputs is expressed like this (with the &lt;&lt; operation\n      meaning ‚Äòshift left‚Äô) ‚Ä¶</p><p align=\"center\">&nbsp;+&nbsp;&nbsp;=&nbsp;(&nbsp;XOR&nbsp;)&nbsp;+&nbsp;(&nbsp;AND&nbsp;)&nbsp;&lt;&lt;&nbsp;1</p><p>‚Ä¶ but what you want is the same sum shifted right by one bit,\n      then all you have to do is to shift the XOR term ,\n      instead of the AND term left:</p><p align=\"center\">(&nbsp;+&nbsp;)&nbsp;&gt;&gt;&nbsp;1&nbsp;=&nbsp;(&nbsp;XOR&nbsp;)&nbsp;&gt;&gt;&nbsp;1&nbsp;+&nbsp;(&nbsp;AND&nbsp;)</p><p>Also on the theme of ‚Äòcompensating for missing features of your\n      CPU‚Äô, here‚Äôs another thing you can do with the half-adder\n      identity.</p><p>In the 1970s, Data General made a CPU with such a small\n      instruction set that it didn‚Äôt even  the\n      bitwise XOR operation ‚Äì it had AND, but not XOR. On that CPU,\n      what would you have done if you needed XOR?</p><p>The answer is to use the half-adder identity in reverse!\n      Instead of starting from &nbsp;XOR&nbsp; and\n      making &nbsp;+&nbsp;, do it the other way round. Given the\n      equation</p><p align=\"center\">&nbsp;+&nbsp;&nbsp;=&nbsp;(&nbsp;XOR&nbsp;)&nbsp;+&nbsp;2&nbsp;√ó&nbsp;(&nbsp;AND&nbsp;)</p><p>you can rearrange it to get</p><p align=\"center\">&nbsp;XOR&nbsp;&nbsp;=&nbsp;(&nbsp;+&nbsp;)&nbsp;‚àí&nbsp;2&nbsp;√ó&nbsp;(&nbsp;AND&nbsp;)</p><p>So if you have addition and bitwise AND, you can build XOR out\n      of them!</p><p>(Exercise for the reader: you can also make bitwise OR in the\n      same way, by removing the ‚Äú2&nbsp;√ó‚Äù in the expression ‚Äì subtract\n      (&nbsp;AND&nbsp;)  doubling it first.)</p><p>Suppose you have a collection of bits packed into an integer,\n      and you want to swap the positions of two of them. What‚Äôs the\n      best way?</p><p>There‚Äôs no one answer to that question, because a lot depends\n      on what hardware you‚Äôre doing it on (and what useful\n      capabilities it has), and also, on what else you want to do at\n      the same time. For example, swapping a  of pairs of\n      bits can often be done more efficiently than by swapping one\n      pair at a time. So this is one of these simple-looking questions\n      that becomes surprisingly complicated if you add ‚Äú‚Ä¶ and do it\n      absolutely as fast as possible‚Äù to the requirements.</p><p>But more than one of the good techniques for solving this\n      problem are based on XOR, because you can divide the problem\n      into two essential cases:</p><ul><li>If the two bits you want to swap are both 0, or both 1, then\n        swapping them <em>makes no difference anyway</em>. So in that\n        situation, you don‚Äôt need to do anything at all.</li><li>If one of the bits is 0 and the other is 1, then swapping\n        them <em>is the same as inverting each one</em>.</li></ul><p>In other words, a reasonable strategy is: first find out if the\n      bits are different, and then, if so, invert both.</p><p>Both of those are applications of XOR. If you XOR the two bits\n      with each other, that will give you 1 if they‚Äôre different. And\n      then you can use that value as input to another XOR, using it as\n      a ‚Äòcontrolled bit flip‚Äô to invert both bits, but only if they\n      were different to start with.</p><p>Here‚Äôs a piece of pseudocode that uses that principle to swap\n      two bits. It assumes that \n      and  are the  of the two bits\n      to be swapped, with 0 meaning the units bit, 1 the bit to its\n      left, and so on. It also assumes that .</p><figure><div><div><pre># Constants derived from pos0 and pos1\nbit0 = (1 &lt;&lt; pos0)      # an integer with just the bit at pos0 set\ndistance = pos1 - pos0  # distance between the two bits\n\n# Computation starting from the input value\ndiff_all = input XOR (input &gt;&gt; distance)\ndiff_one = diff_all AND bit0\ndiff_dup = diff_one XOR (diff_one &lt;&lt; distance)\noutput = input XOR diff_dup</pre></div></div><figcaption>Pseudocode to swap two bits by XOR and shifts</figcaption></figure><p>(If you need to swap the same pair of bit positions in a lot of\n      inputs, the first two lines can be precomputed just once and\n      reused.)</p><p>The first value we compute, , is made by\n      shifting the input right by the distance between the two bit\n      positions. So each bit of  tells you the\n      difference between a pair of the input bits separated by that\n      distance.</p><p>But we‚Äôre only interested in one  pair of\n      the input bits. So next we compute , which\n      uses bitwise AND to pick out just a single bit\n      of . This will be 0 if the two bits we want\n      to swap are the same; if they‚Äôre different, it will be equal\n      to  (i.e. it will have a single bit set at\n      position ).</p><p>If we XORed that value back into , it would\n      conditionally flip the lower of the two bits we want to swap.\n      But we want to conditionally flip  of them. So now\n      we duplicate the useful bit of  into the\n      other bit position, by shifting it left by the distance between\n      the two bits, and combining that with \n      itself to get . This has a 1\n      in  of the target bit positions if the two bits\n      need to be flipped, and is still all 0 otherwise.</p><p>So now, XORing that into the input will flip both bits if\n      necessary, and leave them alone otherwise.</p><p>This looks like a lot of effort to swap two bits. But one nice\n      thing about it is that it‚Äôs just as easy to swap \n      of pairs of bits, if every pair is the same distance apart.\n      (That is, swapping bit  with bit , for\n      multiple different values of  but the same \n      every time.) In that situation, the only thing you need to\n      change is the step that makes \n      from : instead of ANDing with\n      a -bit constant, AND with a -bit\n      constant, containing the low bit of every pair you want to\n      swap.</p><p>Why might you want to do  in turn? Because there‚Äôs\n      a general technique called a  which lets\n      you encode a completely arbitrary permutation in the form of a\n      series of stages, with each stage swapping a lot of pairs of\n      things separated by the same distance. The distances iterate\n      through powers of 2 and then back again: for example, you might\n      do a set of swaps with distances 16, 8, 4, 2, 1, 2, 4, 8, 16.\n      (Or the other way round if you prefer ‚Äì it doesn‚Äôt really\n      matter  order you go through the distances in.) So\n      if you want to permute the bits of a word in a completely\n      arbitrary way, a computational primitive that swaps a lot of\n      bits at equal separation is just what you want for each stage of\n      a Bene≈° network.</p><h4>Using XOR and just a two-bit word</h4><p>In the simpler case where you‚Äôre only swapping a single pair of\n      bits, here are a couple of simpler (therefore faster)\n      techniques.</p><p>Suppose that you have an integer called ,\n      which has exactly two bits set, at the positions you want to\n      swap. What happens if you make the bitwise AND of that value\n      with your input? There are three possibilities:</p><ul><li>(&nbsp;AND&nbsp;)&nbsp;=&nbsp;0. Both bits are\n        0, so you don‚Äôt need to do anything to swap them.</li><li>(&nbsp;AND&nbsp;)&nbsp;=&nbsp;.\n        Both bits are 1, so you still don‚Äôt need to do anything.</li><li>Otherwise, the two bits have different values, so you can\n        swap them by XORing the input with .</li></ul><p>On several CPU architectures this kind of test is easier than\n      doing the full business above with shifts ‚Äì especially because\n      you don‚Äôt need the input bit positions specified in multiple\n      ways. For example, many CPU architectures would let you do\n      something like this (though the syntax of the instructions would\n      vary)</p><figure><div><div><pre>  tmp = input AND bits         # zero if both bits are 0\n  if tmp == 0, jump to ‚Äòdone‚Äô  # in which case we have no work to do\n  tmp = tmp XOR bits           # now zero if both bits are 1\n  if tmp == 0, jump to ‚Äòdone‚Äô  # so again we have nothing to do\n  input = input XOR bits       # in any other case, flip both bits\ndone:</pre></div></div><figcaption>‚ÄòMachine pseudocode‚Äô to swap two bits</figcaption></figure><p>You could typically do this with about five instructions,\n      because CPUs will typically set a flag as a side effect of each\n      AND or XOR operation to tell you whether the result was zero.\n      Or, if not that, they might have a single instruction to jump to\n      a different location depending on whether a register is zero.</p><p>On the 32-bit Arm architecture you can do this in just three\n      instructions, because you don‚Äôt need the jumps: instead, you can\n      make the two XOR operations , because 32-bit\n      Arm generally lets you make an instruction conditional on the\n      current CPU flags without needing a separate branch instruction\n      to skip past it.</p><p>And on x86, you can also do it in three instructions for a\n      completely different reason. x86 has an even stranger processor\n      status flag called the ‚Äòparity flag‚Äô. This is set whenever the\n      result of an operation has . And\n      in this situation, that‚Äôs exactly the case where you need to\n      flip the bits! So you don‚Äôt need to test separately for the\n      ‚Äòboth bits 0‚Äô and ‚Äòboth bits 1‚Äô cases: they can be tested\n      together, by checking if the parity flag says there were an even\n      number of 1 bits in the result of the AND operation.</p><figure><div><div><pre>  ANDS    tmp, input, bits\n  EORSNE  tmp, tmp, bits\n  EORNE   input, input, bits</pre></div><div><pre>  TST     input, bits\n  JPE     done\n  XOR     input, bits\ndone:</pre></div></div><figcaption>Arm and x86 machine code to swap two bits</figcaption></figure><p>In a computer program, how do you swap two numbers?</p><p>It depends on the platform. Some programming languages have a\n      dedicated function for it, like C++‚Äôs .\n      Others have a convenient syntax for assigning multiple variables\n      at once: in Rust you can write \n      (assuming the variables are ), and in Python the\n      same thing works without even needing the brackets. Even some\n      machine languages support swapping two registers: for example,\n      x86 has the XCHG (exchange) instruction.</p><p>But suppose you‚Äôre in a simpler language without any of those\n      features, like C. Or suppose you‚Äôre programming machine code on\n      an architecture without a ‚Äòswap two registers‚Äô\n      instruction. The usual idiom is to use a temporary\n      variable:</p><figure><div><div><pre>a_orig = a;\na = b;\nb = a_orig;</pre></div></div><figcaption>C code for swapping two integers \n        and </figcaption></figure><p>You need the temporary variable because in C you can only\n      assign one variable at a time, so after you execute the\n      assignment , both variables now have the same\n      value (namely the value originally in ), and\n      you‚Äôve lost the previous value of  completely ‚Äì\n      unless you‚Äôd saved it in a third location first.</p><p>But just occasionally you don‚Äôt have a spare register, or at\n      least it would cost you a lot of extra effort to free one up. It\n      turns out that there‚Äôs a different way to swap two\n      values,  needing a spare storage\n      location,\n      using three XORs:</p><figure><div><div><pre>a = a XOR b;\nb = b XOR a;\na = a XOR b;</pre></div></div><figcaption>Pseudocode for swapping \n        and  using three XORs</figcaption></figure><p>It‚Äôs difficult to explain this clearly, because we don‚Äôt have\n      separate names for the  we‚Äôre mutating, and\n      the  that are stored in them. So let‚Äôs start by\n      making up some names. Let‚Äôs say that the value that\n      was  stored in  is\n      called  (as I already did above in that C\n      snippet). And similarly, the original value of  is\n      called .</p><p>Then this is what happens with the three XORs:</p><figure><div><div><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div></div><figcaption>Intermediate states of the three-XOR swap algorithm</figcaption></figure><p>The basic idea is that there are three values we care about:\n      the two input values, and their bitwise XOR. And if you have\n      three values, one of which is the XOR of the other two, then\n      knowing  of them is enough to work out the\n      third, because  of them is the XOR of the other\n      two. So at each step of this algorithm, we set one of the\n      variables to the XOR of both variables, and that‚Äôs always a\n      reversible operation (in fact doing exactly the same thing\n      again  reverse it), so <em>information is never\n      lost</em>. At every stage, XORing the two values we have in our\n      variables recovers whichever one we currently \n      have. And it takes three XORs to rotate through all the possible\n      options, which is why after three steps we‚Äôre back to having the\n      original values of  and  ‚Äì but the other way\n      round.</p><p>I should say at this point that this isn‚Äôt a trick you\n      can  do with XOR ‚Äì it doesn‚Äôt depend on XOR having\n      any essential magic that other operations don‚Äôt. You can use the\n      same technique with additions and subtractions if you prefer, by\n      setting one variable to the  of the other two:</p><figure><div><div><pre>a = a + b;     // now a = (a_orig + b_orig)\nb = a - b;     // now b = (a_orig + b_orig) - b_orig = a_orig\na = a - b;     // now a = (a_orig + b_orig) - a_orig = b_orig</pre></div></div><figcaption>Pseudocode for swapping \n        and  by addition and subtraction</figcaption></figure><p>But the XOR version is more common (among people who want to do\n      this trick at all). Partly that‚Äôs because it‚Äôs simpler: all\n      three operations are the same, whereas in the additive version\n      all  operations are different (the two\n      subtractions are opposite ways round, in the sense of which of\n      their inputs they overwrite).</p><p>But also, the XOR version is more flexible, because ‚Äì just like\n      in the <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/pixels\">pixel graphics</a> application ‚Äì you can\n      use it in situations where addition would cause unwanted extra\n      carries.</p><p>For example, here‚Äôs a case where you can swap with three XORs,\n      which wouldn‚Äôt work with three additions. Suppose\n      that  and  are 32-bit registers,\n      and  is some number less than 32:</p><figure><div><div><pre>a = a XOR (b &lt;&lt; n);\nb = b XOR (a &gt;&gt; n);\na = a XOR (b &lt;&lt; n);</pre></div></div><figcaption>Swap part of  with part\n      of </figcaption></figure><p>The effect of this code is to swap  of the two\n      values. Specifically, only the upper (32&nbsp;‚àí&nbsp;) bits\n      of  are affected, because every time we XOR\n      something into , the value we XOR in has been\n      shifted left by  bits, so its lower  bits are\n      zero. Similarly, the value XORed into  has been\n      shifted right  bits, so its top  bits are zero,\n      so the operation only affects the lower (32&nbsp;‚àí&nbsp;) bits\n      of .</p><p>In fact, this code precisely swaps those two segments of the\n      inputs: the upper (32&nbsp;‚àí&nbsp;) bits of  are\n      swapped with the lower (32&nbsp;‚àí&nbsp;) bits\n      of .</p><p>One use for this trick I‚Äôve seen in the past: suppose that you\n      were operating on a buffer of graphics data with one byte per\n      pixel. You‚Äôve loaded a word from the buffer containing four\n      pixels , and you‚Äôre trying to magnify it by a factor\n      of two, so that you want to create words containing \n      and .</p><p>The neatest way I know to do it is to use this ‚Äòshifted swap‚Äô\n      technique twice:</p><ul><li>Initialise both  and  to copies\n        of the input word .</li><li>Do a shifted swap with &nbsp;=&nbsp;8, swapping the top 24\n        bits of  (containing ) with the bottom 24 of  (containing ). Now &nbsp;=&nbsp;, and &nbsp;=&nbsp;.</li><li>Do a second shifted swap, this time with &nbsp;=&nbsp;24,\n        swapping the top 8 bits of \n        (containing ) with the bottom 8 of \n        (containing ). Now &nbsp;=&nbsp;,\n        and &nbsp;=&nbsp;. Done!</li></ul><h3>Winning at the game of Nim</h3><p>Possibly the  application of XOR that I know\n      of is in game theory.</p><p>In the simple combinatorial game of ‚ÄòNim‚Äô, there are some piles\n      of counters. On your turn, you must choose one pile and remove\n      any number of counters you like from it, from 1 to the whole\n      pile, or anywhere in between (but not 0). You lose if you have no\n      possible move, which only\n      happens if there are no counters at all remaining (because your\n      opponent took the last one).</p><p>How do you determine a good move in this game?</p><p>The basic idea is to identify a set of : states of the game in which, if it‚Äôs your turn,\n      you‚Äôre in trouble. This set should have the properties that:</p><ol><li>the position where you have actually lost counts as a losing\n        position. (It‚Äôd be embarrassing to get that wrong.)</li><li>from any losing position,  possible move\n        leaves the other player in a non-losing position. (If you‚Äôre\n        in trouble, you stay in trouble no matter what you do.)</li><li>from any non-losing position, there should be  move which leaves the other player in a losing\n        position. (If you have the advantage, there‚Äôs a way to keep\n        it, though you may have to think about it.)</li></ol><p>For a game like Nim, where every move reduces the total number\n      of counters, you could analyse the game computationally by\n      iterating through all the possible positions in order of\n      increasing size, and for each one, classify it as ‚Äòlosing‚Äô or\n      ‚Äònon-losing‚Äô according to the above rules, by checking the\n      results of smaller positions you‚Äôve already classified. Of\n      course this only gets you the status of a finite number of\n      positions (until you run out of patience to run the computer\n      program), but you might hope to see a pattern, which you could\n      then try to prove.</p><p>If you try this, it turns out that there is indeed a pattern,\n      and it‚Äôs a surprising one. The losing positions in Nim are\n      precisely positions in which <em>the bitwise XOR of all the pile\n      sizes is zero!</em></p><p>Two of the three criteria above are easy to check:</p><ol><li>If there are no counters at all, then XORing all the pile\n        sizes together gives you zero, because there aren‚Äôt any piles\n        at all (or, equivalently, they all have zero height). So the\n        ‚Äòyou have lost‚Äô position is a losing position by this XOR\n        rule. ‚úì</li><li>If you‚Äôre in a losing position (all the piles XOR to zero),\n        then your move must change exactly one of the pile sizes, say\n        from  to . So the XOR of the pile sizes has\n        changed by &nbsp;XOR&nbsp;, which\n        (since &nbsp;‚â†&nbsp;) isn‚Äôt zero. So any move from a\n        losing position goes to a non-losing position. ‚úì</li></ol><p>The third condition is the trickiest. From\n      a -losing position ‚Äì if the piles XOR to some\n      nonzero value  ‚Äì there always needs to be  move that makes the piles XOR to zero again.</p><p>If you were allowed to  counters to a pile as well\n      as removing them, this would be easy. Just modify any pile you\n      like by XORing its size with  (which will necessarily\n      change it from its previous value), and then the sizes XOR to\n      zero.</p><p>But in some cases, this makes a pile bigger, and that‚Äôs not\n      allowed. So we need to show that there‚Äôs at least one pile that\n      is made  by doing this.</p><p>First, how can you tell whether XORing some other\n      number  with  makes it bigger or smaller? The\n      answer is to look at the  1 bit in . If\n      that bit is also 1 in , it will be 0\n      in &nbsp;XOR&nbsp;, and that means &nbsp;XOR&nbsp;\n      will be smaller than . This is true \n      what the lower-order bits do, because even if all the lower bits\n      change from 0 to 1, the sum of those effects will still be\n      (just) smaller than the effect of the high bit changing from 1\n      to 0.</p><p>So, if the piles XOR to , then we‚Äôre looking for pile\n      sizes which have a 1 in the same place as the highest 1 bit\n      of . Those are exactly the piles for which\n      XORing  into the size will make it smaller ‚Äì meaning we\n      can modify that pile in a way that is both within the rules, and\n      creates a losing position for the other player.</p><p>So we can find a legal winning move if there‚Äôs at least one\n      pile size with a 1 in that bit position. But of course there\n      must be one,  that bit is set in : if\n      every pile size has 0 in a given bit position, then \n      does too!</p><p>For an example, let‚Äôs calculate what‚Äôs going on in the picture\n      above. The three pile sizes are 12, 10 and 3. In binary, those\n      are 1100, 1010 and 0011. The bitwise XOR of those values is\n      0101, so this isn‚Äôt a losing position. To win, we must change a\n      pile size by XORing it with 0101. Two of the pile sizes would be\n      made bigger by this operation: the two smaller sizes, 1010 and\n      0011, have a 0 in the relevant position (they‚Äôre of the\n      form 0), so they would become 1111 and 0110\n      respectively, each larger than their original size. But the\n      largest pile, with size 1100, has that bit set, so it becomes\n      1001, a smaller value. Therefore, there‚Äôs only one winning move,\n      and it‚Äôs to reduce the largest pile from size 12 to size 9 by\n      removing three counters, as the picture shows.</p><p>One reason I find this a particularly strange place for bitwise\n      XOR to show up is that it doesn‚Äôt to have anything to do with\n      your choice of number base. If you‚Äôre writing two\n      integers , then it might seem very natural to\n      combine corresponding pairs of digits in various ways. But if\n      you write the same integers in base 10, or some other base like\n      3 or 5, then you‚Äôd find yourself imagining a totally different\n      set of ‚Äòdigit-wise‚Äô operations analogous to the bitwise ones\n      (like taking the minimum, or maximum, or sum mod 10, of each\n      pair of corresponding decimal digits). So you‚Äôd expect bitwise\n      XOR to show up in situations where it was important that you‚Äôd\n      written a number in binary. But the winning strategy in Nim\n      doesn‚Äôt depend on what base you write the pile sizes in, or even\n      whether you wrote them down in place-value notation at all ‚Äì so\n      the appearance of bitwise XOR  to be saying that\n      binary is important to the underlying mathematics, whether\n      humans have thought of it or not!</p><h2>Mathematical concepts that look like XOR</h2><p>That last example, from game theory, is moving more in the\n      direction of mathematics rather than practical computing. So\n      this is a good moment to change direction and talk about some\n      concepts in pure mathematics that are basically XOR with a\n      different name, or in a not-very-subtle disguise.</p><p>In some cases, there‚Äôs a whole further area of study that\n      follows on from the XOR-like operation, showing that XOR isn‚Äôt\n      just a useful thing in its own right ‚Äì it‚Äôs also the starting\n      point of a lot more.</p><p>In the following sections the mathematics will be more advanced\n      than it‚Äôs been until now: I don‚Äôt have space to describe every\n      concept from absolute scratch, so in each section I‚Äôll have to\n      rely on some background knowledge that makes it possible to\n      explain the new concept briefly.</p><p>If that isn‚Äôt your thing, then I hope you‚Äôve enjoyed the\n      previous parts of the article!</p><h3>Symmetric difference of sets</h3><p>I‚Äôll start with a simple one. There‚Äôs a natural correspondence\n      between Boolean logic operations, and operations on sets in set\n      theory. For any set , you can imagine asking the yes/no\n      question ‚ÄòIs this particular thing a member of ?‚Äô. Then,\n      set operations on the sets themselves (like union and\n      intersection) correspond naturally to Boolean logic operations\n      on the answers to those membership queries.</p><p>For example, if you have two sets  and , then\n      when is some element  in the union &nbsp;‚à™&nbsp;?\n      Precisely when either  is\n      in ,  is in , or both. The\n      union operator corresponds to the Boolean (inclusive) OR.</p><p>Similarly, &nbsp;‚àà&nbsp;(&nbsp;‚à©&nbsp;) precisely\n      when &nbsp;‚àà&nbsp;&nbsp;‚àà&nbsp;. The\n      intersection operator corresponds to AND.</p><p>In this model, what corresponds to XOR? It‚Äôs the <em>symmetric\n      difference</em> operator, written &nbsp;‚àÜ&nbsp;: the set\n      of elements that are in  of \n      and , no matter which one it\n      is. &nbsp;‚àà&nbsp;(&nbsp;‚àÜ&nbsp;) precisely\n      when (&nbsp;‚àà&nbsp;) XOR (&nbsp;‚àà&nbsp;).</p><p>This correspondence between XOR and symmetric difference means\n      that the ‚àÜ operator has all the same properties as XOR ‚Äì for\n      example, it‚Äôs both commutative and associative. Proving this is\n      a common introductory exercise in simple set theory, and doing\n      it directly can easily lead to half a page of tedious algebra;\n      but understanding symmetric difference as ‚Äòbasically XOR‚Äô, and\n      XOR in turn as the same thing as addition mod 2, makes it clear\n      that symmetric difference inherits commutativity and\n      associativity from addition itself.</p><p>In group theory, if  is an element of a group ,\n      you can ask: is any power of  equal to the group\n      identity ? If so, what‚Äôs the \n      number &nbsp;&gt;&nbsp;0 such\n      that &nbsp;=&nbsp;? This number is\n      called the  of the element . (If there is\n      no such integer , so that all the powers of  are\n      different, then we say that  has infinite order.)</p><p>Instead of asking about the order of one specific element\n      of  at a time, you can also ask a similar question for\n      the whole group at once: is there a single number  such\n      that, for \n      element &nbsp;‚àà&nbsp;, &nbsp;=&nbsp;?\n      The smallest such number is called the  of the\n      group . (Again, it may be infinite. If it‚Äôs finite, then\n      it‚Äôs also the lowest common multiple of all the orders of\n      individual elements.)</p><p>If a group has exponent 2 in particular, then that means every\n      element\n      is : &nbsp;=&nbsp; for\n      all . A standard exercise is to show that this also\n      makes the group , i.e. the group operation is\n      commutative, i.e. &nbsp;=&nbsp; for\n      all ,.</p><p>Group operations are also associative, by definition of a\n      group. So in this situation, we have an operation that‚Äôs\n      commutative, associative, has an identity (namely ), and\n      everything is self-inverse. So if you have a long list of group\n      elements combined together, you can reorder it to bring\n      identical elements together, and then any two copies of the same\n      element cancel out.</p><p>That sounds a lot like XOR ‚Äì and it is. Every group of exponent\n      2 can be understood as a special case of XOR, by imagining that\n      each element of the group corresponds to a function (on some\n      set  that depends on the group) taking values in {0,&nbsp;1},\n      and combining two elements has the effect of taking the bitwise\n      XOR (or sum mod 2) of their associated functions.</p><p>Not every such group contains  the possible\n      functions from &nbsp;‚Üí&nbsp;{0,&nbsp;1}. Every  group of\n      exponent 2 does (as long as you don‚Äôt define  to have\n      spare unused elements), but infinite groups can be more subtle.\n      You might have, for example, the set of functions from\n      ‚Ñï&nbsp;‚Üí&nbsp;{0,&nbsp;1} that are eventually all 0, or eventually constant, or\n      eventually periodic.</p><p>But all groups of exponent 2 correspond to  set of\n      functions with codomain {0,&nbsp;1}, under bitwise XOR.</p><p>In an <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#nim\">earlier section</a> we saw that the losing\n      positions in the game of Nim are characterised by the pile sizes\n      XORing to zero.</p><p>This isn‚Äôt just an isolated mathematical curiosity about one\n      obscure game. Nim is central to the theory of <em>Sprague-Grundy\n      analysis</em>, which proves a large class of other games to be\n      ‚Äòequivalent‚Äô to Nim in the sense that you can analyse them using\n      the same technique.</p><p>However, the class of games that this works for doesn‚Äôt include\n      most games you might normally play. It‚Äôs limited\n      to  games, which are those where the set of\n      permitted moves don‚Äôt change depending on which player‚Äôs turn it\n      is. Chess, for example, is  impartial, because each\n      piece belongs to a specific player, and the other player isn‚Äôt\n      allowed to move it. It‚Äôs not enough that one player‚Äôs pieces are\n      basically the same as the other player‚Äôs, and move by the same\n      rules: chess would only be impartial if both players were\n      allowed to move , regardless of\n      colour.</p><p>The basic idea is that for any position  in any\n      impartial game, you can assign it a number , known as\n      a . Then you can treat position \n      as ‚Äòessentially‚Äô the same as a Nim pile containing \n      counters, in the sense that for every smaller\n      number &nbsp;&lt;&nbsp;, there‚Äôs a move that\n      turns  into a position with Grundy number , but\n      no move that leaves the Grundy number unchanged at \n      itself. (There may or may not be moves that go\n      to  numbers; in that respect Grundy numbers\n      differ from actual Nim piles, but this difference turns out not\n      to matter.)</p><p>In many of these games, it‚Äôs natural to combine multiple\n      instances of the game into one bigger one, in a way that gives\n      the player the choice of which subgame to make a move in. This\n      operation is called making a , or sometimes\n      a , of the smaller games. For example, a\n      Nim game with multiple piles of counters is exactly the\n      composite of smaller Nim games each containing just one of the\n      piles, because on your turn you must choose just one of the\n      subgames (piles) to modify, and make a move that would be legal\n      in that subgame by itself.</p><p>How do you work out the Grundy number of a composite game? One\n      very convenient way is to first find the Grundy number of each\n      component (which are smaller and simpler games, so this is\n      usually easier). Then the overall game is a composite of smaller\n      games, each one equivalent to a Nim pile of a particular size ‚Äì\n      and so its Grundy number is obtained the same way you‚Äôd evaluate\n      a position in Nim, by combining the smaller games‚Äô Grundy\n      numbers using bitwise XOR.</p><p>A concrete example is the game of ‚Äòkayles‚Äô, which starts off\n      with a row of counters equally spaced, and on your move you may\n      remove any single counter, or two counters directly adjacent. So\n      most of the possible first moves divide the starting row into\n      two smaller rows, which you then have to play in separately\n      (there‚Äôs no move you can make that affects both). Sprague-Grundy\n      analysis saves you from having to analyse every\n      possible  of kayles rows: instead, you only\n      need to work out the Grundy number of each length\n      of  kayles row, and then the Grundy number of a\n      composite of multiple rows can be worked out by XOR.</p><p>So bitwise XOR is crucial to this entire branch of game theory.\n      For this reason, the operation of bitwise XOR on non-negative\n      integers is sometimes referred to by game theorists as\n      the .</p><h3>(2), the finite field of order 2</h3><p>A , in mathematics, means an algebraic structure\n      in which you can add, subtract, multiply and divide any two\n      numbers (except for dividing by 0), and you still get a number\n      within the original field, and those operations behave\n      ‚Äòsensibly‚Äô in the same ways you‚Äôre used to, both individually\n      and in combination with each other. For example, you\n      expect &nbsp;+&nbsp;&nbsp;=&nbsp;&nbsp;+&nbsp;,\n      and &nbsp;‚àí&nbsp;&nbsp;=&nbsp;0,\n      and (&nbsp;+&nbsp;)&nbsp;=&nbsp;&nbsp;+&nbsp;.</p><p>Well-known fields include the real numbers, ‚Ñù; their superset,\n      the complex numbers ‚ÑÇ; their subset, the rational numbers ‚Ñö.\n      There are also a great many intermediate fields between ‚Ñö and ‚Ñù,\n      such as the set of all numbers of the form &nbsp;+&nbsp;‚àö2\n      for rational  and . A well-known example of\n      something that‚Äôs  a field is the integers, ‚Ñ§: you\n      can add, subtract and multiply integers just fine and still get\n      an integer, but if you divide two integers, you can easily get\n      something that isn‚Äôt an integer any more.</p><p>All of those fields have infinite size. If nothing else, they\n      contain the integers: you can start from 1, and keep adding 1,\n      and get an endless sequence of numbers, all different and all\n      still in the field.</p><p>But there‚Äôs also such a thing as a  field: a\n      structure that obeys all the same rules as any other field, but\n      has a finite number of different elements.</p><p>A finite field has a fundamentally different nature from the\n      fields I‚Äôve mentioned so far. If you do that same experiment I\n      just mentioned in a finite field ‚Äì start with 1 and keep adding\n      1 ‚Äì you  get an infinite sequence of different\n      values, because there aren‚Äôt infinitely  different\n      values at all. Sooner or later, you must repeat a number you‚Äôve\n      run into before.</p><p>In particular, this means that if you count up 1, 2, 3, ‚Ä¶ in a\n      finite field, you must at some point find that one of those\n      numbers is equal to  again. So finite fields have\n      the nature of  arithmetic, rather than ordinary\n      arithmetic: there‚Äôs always some positive number  (known\n      as the  of the field, and as it turns\n      out, always prime) such that adding 1 to itself  times\n      gives zero, and therefore, adding  to\n      itself  times also gives\n      zero. (Which means that you also\n      aren‚Äôt allowed to divide by , because you can‚Äôt divide\n      by zero, and  is the same thing as zero in this\n      context.)</p><p>In fact, the simplest example of a finite field\n      is  modular arithmetic. For a prime ,\n      the integers mod  have all the properties of a field, as\n      long as you interpret ‚Äòdividing by ‚Äô to mean multiplying\n      by the modular inverse of  mod .</p><p>And the simplest example of  is to take \n      to be the smallest prime of all, namely 2. If you do that, you\n      get a field with just two numbers in it: 0 and 1! This field is\n      called various things, including (2) and ùîΩ.\n      (‚ÄòGF‚Äô stands for ‚ÄòGalois field‚Äô, after the mathematician who\n      pioneered research in this area.)</p><p>This field is so small that it‚Äôs possible to just \n      all the answers to every basic arithmetic operation:</p><ul><li>Addition works mod 2, so 1&nbsp;+&nbsp;1&nbsp;=&nbsp;0. Every other addition is\n        the same as in normal integers: 0&nbsp;+&nbsp;0&nbsp;=&nbsp;0 and 0&nbsp;+&nbsp;1&nbsp;=&nbsp;1.</li><li>Subtraction is <em>exactly the same thing as addition</em>,\n        because ‚àí1 and +1 are the same thing in any ‚Äòmod 2‚Äô context.\n        In particular, 0&nbsp;‚àí&nbsp;1&nbsp;=&nbsp;1 (again, because ‚àí1&nbsp;=&nbsp;+1), and\n        everything else is the same as in normal integers: 0&nbsp;‚àí&nbsp;0&nbsp;=&nbsp;0,\n        1&nbsp;‚àí&nbsp;1&nbsp;=&nbsp;0 and 1&nbsp;‚àí&nbsp;0&nbsp;=&nbsp;1.</li><li>Multiplication works  like normal integers:\n        any multiplication involving 0 gives 0, and 1&nbsp;√ó&nbsp;1&nbsp;=&nbsp;1.</li><li>The only case of division that‚Äôs allowed  is\n        dividing a number by 1, which just gives you the same number\n        back again. And you can‚Äôt divide by 0 at all.</li></ul><p>To put this another way: the elements of this field look like\n      booleans (with the usual convention of 0&nbsp;=&nbsp;false and 1&nbsp;=&nbsp;true),\n      and addition and subtraction both behave like the XOR operator.\n      Multiplication behaves like AND: the product is 1 only if both\n      inputs are 1, because otherwise, at least one input is 0, and\n      multiplying by 0 gives 0.</p><p>This means that we‚Äôve just found out another algebraic property\n      of the XOR operator: AND distributes over it, which is to say that</p><p align=\"center\">&nbsp;AND&nbsp;(&nbsp;XOR&nbsp;)&nbsp;=&nbsp;(&nbsp;AND&nbsp;)&nbsp;XOR&nbsp;(&nbsp;AND&nbsp;)</p><p>because that‚Äôs just the translation into Boolean algebra of the\n      ordinary algebraic\n      identity (&nbsp;+&nbsp;)&nbsp;=&nbsp;&nbsp;+&nbsp;,\n      which is true in (2) just like in any other field.</p><p>This tiny field seems as if it‚Äôs surely too trivial to actually\n      be useful for anything. But it isn‚Äôt!</p><p>All of linear algebra ‚Äì vectors, matrices, and all that ‚Äì\n      starts from the definition of a . That in\n      turn depends on the starting point of choosing a \n      which will act as the ‚Äòscalars‚Äô of your vector space, and the\n      elements of your matrices. Depending on the field you choose,\n      the vectors and matrices behave differently. (For example,\n      rational, real and complex matrices will disagree on whether a\n      matrix is diagonalisable, or has a square root.)</p><p>You can make a vector space over any field you like. Even over\n      the trivially simple (2), if you want to. If you do\n      that, then vectors are particularly simple: each vector looks\n      like a sequence of numbers which are all either 0 or 1. You\n      could imagine representing this as just a string of individual\n      bits in a computer.</p><p>When you add two vectors or matrices, you add each component\n      separately, using whatever addition is appropriate to your\n      field. If the field is (2), that means the addition is\n      mod 2, i.e. it works like XOR, independently in each\n      component. <em>Adding two vectors or matrices\n      over</em>(2) <em>corresponds exactly to bitwise\n      XOR.</em></p><p>What about multiplying a matrix  by a vector ?\n      In ordinary real-number linear algebra, one way to look at this\n      is that the output is a linear combination of the columns of the\n      matrix , and the coefficients of the linear combination\n      are given by the components of the vector . That is,\n      the th column of  is multiplied by\n      the th component of , and all those products are\n      added together.</p><p>Over (2), this is particularly simple, because the\n      components of  are all either 0 or 1, so multiplying one\n      of those into a column of  either zeroes it out\n      completely, or leaves it unchanaged. So  is just\n      specifying a  of the columns of . And\n      then those columns are added together like vectors\n      over (2), i.e. combined as if by bitwise XOR.</p><p>Of course, you have to ask why anyone would bother. What‚Äôs the\n      use of vectors and matrices in which the components work mod 2\n      in this way? They clearly don‚Äôt represent anything in geometry\n      (like vectors over ‚Ñù do), or anything in quantum mechanics or\n      signal processing (which are both applications of vectors over\n      ‚ÑÇ). Is this just a mathematical curiosity not ruled out by the\n      definition of a vector space, or are there uses for it?</p><p>There are! And here‚Äôs an example.</p><p>If you‚Äôre communicating over a noisy channel like a radio, you\n      often want to transmit your data with some redundancy, so that\n      if a few bits of your message aren‚Äôt received correctly at the\n      other end, the receiver can tell that it happened, and perhaps\n      even reconstruct the correct message in spite of the errors.</p><p>This idea in general is known as an <em>error-correcting\n      code</em>. The general idea is that you expand an original\n      message of (say)  bits into some larger number of\n      bits &nbsp;&gt;&nbsp; which you send, and then the\n      receiver decodes the  bits they receive to get back your\n      original -bit message. So there are only\n      2 strings you might have fed to the encoder\n      as input, and therefore only 2 of the\n      2 possible -bit strings could have\n      been produced as output. The idea of an error-correcting code is\n      to ‚Äòspace out‚Äô the valid -bit codewords in the overall\n      space of -bit strings, so that any two valid codewords\n      differ in a large number of bits, and a small number of errors\n      can‚Äôt turn one into another. If any two valid codewords differ\n      by at least  bits, for example, then a transmission\n      error that alters fewer than  bits can\n      be  (the receiver recognises that the received\n      string isn‚Äôt a valid codeword), and an error altering fewer\n      than &nbsp;/&nbsp;2 bits can be corrected, by finding\n      the  valid codeword.</p><p>(Incidentally, ‚Äòcodes‚Äô in this sense aren‚Äôt \n      codes, like in cryptography. The word ‚Äòcode‚Äô in this context has\n      the wider meaning of ‚Äòany way to convert your message into\n      something convenient to send, so that the receiver knows how to\n      get the message back at the other end‚Äô. For this application, we\n      don‚Äôt mind if other people can  reconstruct the\n      message. Of course, if you wanted to protect the message against\n      eavesdroppers  against transmission errors, you\n      might put a layer of encryption inside your error-correcting\n      code!)</p><p>One very popular way to construct these codes is to make\n      them , which means that the code basically works\n      by having a pair of matrices over (2), so that each one\n      takes an input string of bits (represented as a vector), and\n      outputs another string of bits:</p><ul><li>A  is used by the sender, to expand\n        the original -bit message into a longer -bit\n        codeword.</li><li>A  is used by the receiver, to find out\n        whether the received codeword is valid. Any valid codeword\n        multiplied by the check matrix should give the zero vector ‚Äì\n        but if the codeword has errors, then the output vector (known\n        as the ) contains enough information to\n        identify them.</li></ul><p>This is a convenient system because doing it with vectors and\n      matrices makes the syndrome independent of the message. That is,\n      if the same pattern of error bits occurs in two different\n      messages, both of them generate the same syndrome vector. So the\n      receiver only needs a lookup table that maps every possible\n      syndrome to the pattern of error bits it generates ‚Äì\n      they  need a separate version of that table for\n      each possible codeword.</p><p>And in a few cases you don‚Äôt  need the lookup\n      table. Here‚Äôs a particularly pretty concrete example of a linear\n      code, known as a :</p><p>Suppose , the length of the code, is one less than a\n      power of 2. For this example I‚Äôll take &nbsp;=&nbsp;15, but any\n      other number of this form (3, 7, 15, 31, 63, ‚Ä¶) also works.</p><p>We‚Äôll start by saying what the  does. The\n      receiver has a 15-bit string to analyse. They index the bits\n      with all the non-zero 4-bit binary numbers, so that the bits are\n      numbered 0001, 0010, 0011, ‚Ä¶, 1110, 1111. Then they XOR together\n      the indexes of all the 1 bits in the message. Legal codewords\n      are defined to be any bit string for which the result of this\n      XOR operation is zero.</p><p>So, if you take a legal codeword and flip any single bit, the\n      result of this XOR process is not zero, and better than that, it\n      directly tells you the index of the bit that was flipped! So a\n      Hamming code can correct any one-bit error in a codeword,\n      without even needing a lookup table.</p><p>How does the sender construct a codeword? The easiest way is to\n      reserve the bits with power-of-2 indices as check bits, and fill\n      in all the rest. So the 11 bits corresponding to indices\n      that  1, 2, 4 or 8 are your data bits, which you\n      fill in with the actual message. Then the sender XORs together\n      the indices of all the 1 bits so far, and sets the final four\n      bits however is necessary to make the result become zero: if the\n      lowest bit in the XOR value is 1, they set the bit with index 1\n      to cancel it out, and the same for the other three bit\n      positions.</p><p>So a 15-bit Hamming code lets you transmit 11 bits of actual\n      data, and uses the other 4 bits to allow a one-bit error to be\n      corrected. In general, a (2&nbsp;‚àí&nbsp;1)-bit Hamming\n      code carries 2&nbsp;‚àí&nbsp;1&nbsp;‚àí&nbsp; bits of data,\n      using the other  bits to correct a one-bit error. In\n      other words, a longer Hamming code is more efficient (more\n      useful message data per bit transmitted), but correspondingly\n      less good at correcting errors (still only one error allowed per\n      codeword, but the codewords are longer).</p><p>This is a linear code, because both the encoding and checking\n      processes I‚Äôve described can be written down as a matrix over\n      (2). The process of XORing together the indices of set\n      bits in the received data is exactly the same thing as\n      multiplying by a matrix whose columns contain the nonzero binary\n      integers 0001, 0010, ‚Ä¶, 1111. And the process of constructing a\n      message is the same thing as multiplying by a matrix in which\n      each column sets one of the bits with a non-power-of-2 index\n      plus whatever power-of-2 bits cancel it out ‚Äì for example, there\n      will be a column that sets the bit with index 0101, and then\n      cancels it by also setting the bits 0001 and 0100.</p><p>Another thing we can do with any field, including (2),\n      is to consider the set of  with coefficients\n      in that field.</p><p>That is, we consider expressions of the form</p><p>in which the numbers  are all either\n      0 or 1, and  is an abstract symbol that doesn‚Äôt\n      represent an actual number at\n      all.</p><p>If you want to add or multiply two of these polynomials, you do\n      it exactly as if you were manipulating ordinary polynomials with\n      integer or real coefficients: simplify using the\n      rule &nbsp;=&nbsp;,\n      collect together terms with the same power of , and\n      evaluate each coefficient. The only difference is that the final\n      evaluation happens in (2), which is equivalent to\n      saying ‚Äòafter you calculate the coefficients, reduce each one\n      mod 2‚Äô.</p><p>For example, suppose you wanted to add the polynomials\n      (1&nbsp;+&nbsp;&nbsp;+&nbsp;) and\n      (&nbsp;+&nbsp;). In the ordinary integers, the\n      sum would be\n      1&nbsp;+&nbsp;&nbsp;+&nbsp;&nbsp;+&nbsp;.\n      Over (2), the answer is exactly the same except that\n      the 2 term vanishes, because we‚Äôre working mod 2, so 2\n      is just the same thing as 0. So you just get\n      1&nbsp;+&nbsp;&nbsp;+&nbsp;.</p><p>In other words, if  and  are two polynomials,\n      then the  term of &nbsp;+&nbsp;\n      is simply the sum of the  terms\n      in  and  themselves. But the sum is mod 2, i.e.\n      it corresponds to XOR. <em>Each coefficient of the sum is the\n      XOR of the same coefficient in the two inputs.</em></p><p>In other words, if you consider a polynomial to be represented\n      by just its sequence of coefficients, then <em>addition of\n      polynomials over</em>(2) <em>corresponds exactly to\n      bitwise XOR</em>.</p><p>What about multiplication? The same rule works: multiply the\n      polynomials the same way you would normally, and then reduce the\n      coefficients mod 2. So the product\n      (1&nbsp;+&nbsp;&nbsp;+&nbsp;)(&nbsp;+&nbsp;),\n      for example, would normally work out\n      to &nbsp;+&nbsp;&nbsp;+&nbsp;2&nbsp;+&nbsp;&nbsp;+&nbsp;,\n      but again, reducing the coefficients mod 2 makes the\n      2 term vanish. So over (2), the product is &nbsp;+&nbsp;&nbsp;+&nbsp;&nbsp;+&nbsp;.</p><p>Another way to describe this algorithm is: to calculate the\n      product of two polynomials  and , for each\n      term  in , make the partial\n      product , which looks just\n      like  itself except that it‚Äôs ‚Äòshifted upwards‚Äô\n      by  places ‚Äì the power of  in each term is\n      larger by  than it originally was. Then add together all\n      of those partial products, in ‚Äòmod 2‚Äô fashion.</p><p>This looks very similar to the algorithm you use for\n      multiplying two ordinary integers  and  if\n      you‚Äôre given them written in binary. In that algorithm, you\n      again make a shifted value &nbsp;√ó&nbsp;2 for\n      every power of 2 corresponding to a 1 bit in . The only\n      difference is that you combine the partial products at the end\n      using ordinary integer addition, instead of bitwise XOR.</p><p>In software that deals with polynomials over (2) a\n      lot, it‚Äôs actually very convenient to  one of\n      these polynomials as a binary number, with the bit that would\n      normally have value 2 instead being taken to\n      be the coefficient of . For example,\n      you might represent\n      1&nbsp;+&nbsp;&nbsp;+&nbsp; using the\n      integer 1&nbsp;+&nbsp;2&nbsp;+&nbsp;2&nbsp;=&nbsp;13. (As if you‚Äôd\n      forgotten about the ‚Äòmod 2‚Äô business and just evaluated the\n      original polynomial at &nbsp;=&nbsp;2.)</p><p>In that representation, multiplying polynomials looks almost\n      exactly like multiplying integers in binary ‚Äì you make a shifted\n      copy of one of the inputs  for each set bit of ,\n      and then combine them all ‚Äì except that combining all the values\n      at the end is done ‚Äòwithout carrying‚Äô, i.e. it‚Äôs replaced with\n      XOR. Some CPU architectures even provide a built-in hardware\n      instruction to do this modified type of multiplication. The x86\n      architecture calls it a name like ‚Äòcarryless multiplication‚Äô,\n      with instruction names including the string ;\n      the Arm architecture calls it ‚Äòpolynomial multiplication‚Äô, and\n      you‚Äôll probably find a P in the name of any instruction that\n      does it.</p><p>These polynomials over (2) behave in some ways\n      similarly to the integers: you can add, subtract and multiply\n      them as much as you like, but when you try to divide two\n      polynomials, the result often isn‚Äôt still a polynomial, and you\n      have to decide what to do about that.</p><p>Just like dividing in the integers, one thing you can do is to\n      deliver a quotient and a remainder: if you‚Äôre asked to\n      calculate &nbsp;/&nbsp; and you find that  isn‚Äôt a\n      multiple of , you can find a nearby polynomial\n      that  a multiple of , and return the result\n      of dividing  by , plus the ‚Äòremainder‚Äô\n      that‚Äôs the difference between  itself and the polynomial\n      you substituted.</p><p>A well-known kind of checksum, used to verify transmission of\n      network packets in Ethernet and for many other similar purposes,\n      is called a Cyclic Redundancy Check (CRC), and it works like\n      this:</p><ul><li>Choose a polynomial  over (2). (There are\n        several polynomials people like to use, typically with degree\n        32 or sometimes 16. But for a given application, both sender\n        and receiver must agree on a specific one.)</li><li>Given a message to transmit, expressed as a sequence of\n        bits, pretend that the entire message is itself a giant\n        polynomial  over (2), with the coefficients\n        given by the bits of the message.</li><li>Divide  by , throw away the quotient, and\n        keep the remainder. That is, reduce  mod .</li></ul><p>You can see this, in some ways, as very similar to the\n      error-correcting codes I mentioned in a <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#ecc\">previous\n      section</a>. A CRC will only  errors, not correct\n      them; and because it‚Äôs a tiny number of bits appended to a huge\n      message, its power to detect errors is much smaller than those\n      more rigorous codes. If you‚Äôre trying to transmit over a noisy\n      radio channel then you probably use full error-correcting codes;\n      CRCs are for the kind of situation where  every\n      transmission is free of error, and very rarely there‚Äôs either\n      one flipped bit or a sudden burst of random noise, and it‚Äôs fine\n      to deal with the problem by telling the other end ‚Äòwhoops, that\n      one packet didn‚Äôt arrive intact, please re-send it‚Äô.</p><h5>Making larger finite fields</h5><p>In a <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#gf2poly\">previous section</a> I said that\n      the  examples of finite fields consisted of the\n      integers mod , for some prime . But they‚Äôre not\n      the only examples.</p><p>Let me describe the process of making one of these simplest\n      finite fields () in slightly more detail:</p><ul><li>Choose a specific value  which isn‚Äôt divisible by\n        anything smaller.</li><li>Reduce everything else mod  ‚Äì that is, consider\n        things as the same if they differ by a multiple of .\n        This leaves only finitely many things counted as\n        different.</li><li>Because  is prime, it turns out that there is now\n        always a multiplicative inverse of any value not equivalent to\n        0. So division now works, and we‚Äôve made a field.</li></ul><p>It turns out that all the  finite fields that\n      exist can be made by following exactly the same procedure a\n      second time ‚Äì except that instead of starting with the integers,\n      you start with the set of polynomials\n      over ().</p><p>That is: first pick  and make the finite field of\n      integers mod . Now pick a polynomial  with\n      coefficients in (), which\n      is  ‚Äì that is, it isn‚Äôt the product of any\n      two smaller polynomials ‚Äì and reduce all the rest of the\n      polynomials over () mod .</p><p>The result is always a finite field. It still has\n      characteristic  ‚Äì that is, adding together \n      copies of the same thing gives zero. But it has more\n      than  elements. In fact, if the polynomial  has\n      degree  (that is, its highest-order term\n      is ), then the new field has\n      \n      elements.</p><p>You can do this for any prime . But in this article\n      we‚Äôre concerned with &nbsp;=&nbsp;2 in particular. Here are the\n      first few irreducible polynomials over (2):</p><figure><div><div><p>\n1 + \n1 +  + \n1 +  + \n1 +  + \n1 +  + \n1 +  + \n1 +  +  +  + \n          ‚Ä¶</p></div></div><figcaption>Irreducible polynomials over (2) up to degree 4</figcaption></figure><p>A more compact way to write the same information is to use the\n      notation I mentioned earlier, of representing each one as an\n      integer whose pattern of bits gives its coefficients, equivalent\n      to evaluating the polynomial at &nbsp;=&nbsp;2. Then the start of\n      the sequence of irreducible polynomials looks like this:</p><p align=\"center\">2, 3, 7, 11, 13, 19, 25, 31, 37, 41, 47, 55, 59, 61, 67, 73, 87, 91, 97, 103, 109, 115, 117, 131, 137, 143, 145, 157, 167, 171, 185, 191, 193, 203, 211, 213, 229, 239, 241, 247, 253, 283, 285, 299, 301, 313, 319, 333, 351, 355, 357, 361, 369, 375, 379, 391, 395, 397, 415, 419, 425, 433, 445, 451, 463, 471, 477, 487, 499, 501, 505, ‚Ä¶</p><p>Read like a sequence of integers,\n      these numbers fascinate me. They‚Äôre like prime numbers from a\n      parallel universe. If addition were done without carrying, these\n      would  the prime numbers. And, just as you‚Äôd hope for\n      something from a parallel universe, it takes you an extra look\n      to spot the difference, because they look very similar to start\n      with ‚Äì many of the initial ones are  ordinary\n      integer primes. (Though not all ‚Äì 25 and 55 are the first\n      composite ones; and not all the integer primes appear ‚Äì 5, 17,\n      and 23 are the first ones missing.)</p><p>There‚Äôs a lot more to say about these larger finite fields. But\n      I‚Äôll finish up by mentioning what they‚Äôre  for.\n      They crop up all over cryptography:</p><ul><li>A finite field of order 2 forms a key building\n        block of the standard AES cipher, and also of another cipher\n        (Twofish) which  became AES (it was another\n        finalist in the competition to choose a standard block\n        cipher).</li><li>A finite field of the much larger size 2 is\n        used in GCM, which is a fast scheme that combines bulk\n        encryption with integrity protection (known as ‚ÄòAEAD‚Äô).</li><li>Finite fields of large power-of-2 size are also sometimes\n        (though not always) used in elliptic-curve cryptography, for\n        both key exchange and digital signatures.</li><li>Elliptic-curve cryptography is probably going to be\n        abandoned at some point, because of the possibility of working\n        quantum computers being built that can attack it. Finite\n        fields of power-of-2 order  appear in at least\n        one of the replacement ‚Äòpost-quantum‚Äô schemes, as part of the\n        decoding algorithm in the ‚ÄúClassic McEliece‚Äù key encapsulation\n        system.</li></ul><p>These things are big business ‚Äì and that‚Äôs mostly why CPU\n      architectures bother to implement that polynomial multiplication\n      primitive at all!</p><p>With any luck, you should be able to read the footnotes of this\n      article in place, by clicking on the superscript footnote number\n      or the corresponding numbered tab on the right side of the page.</p><p>But just in case the CSS didn‚Äôt do the right thing, here‚Äôs the\n      text of all the footnotes again:</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-inverse\">1.</a> I apologise for\n      using ‚Äòinvert‚Äô and ‚Äòinverse‚Äô in two senses in this article. In\n      boolean logic, ‚Äòinverting‚Äô a signal generally means the NOT\n      operator, interchanging 1 with 0 (or true with false, if you\n      prefer). But in mathematics, an ‚Äòinverse‚Äô is a value that\n      cancels out another value to get you back to the identity, in a\n      way which varies depending on the operation you use to combine\n      them (if you‚Äôre talking about addition it‚Äôs ‚àí, and for\n      multiplication it‚Äôs 1/). I hope that everywhere I‚Äôve\n      used the word at all it‚Äôs clear from context which sense I mean\n      it in.</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-twos-complement\">2.</a> What if the integers are\n      negative? Normally, in the finite integer sizes that computers\n      handle in hardware, negative integers are represented in two‚Äôs\n      complement, i.e. mod 2. So ‚àí1 is the integer\n      with all bits 1, for example. There‚Äôs a reasonably natural way\n      to extend this to  integers, by\n      pretending the string of 1 bits on the left goes all the way to\n      infinity (and it can even be made mathematically rigorous!). In\n      this view, the XOR of two positive numbers, or two negative\n      numbers, is positive, because at a high enough bit position each\n      bit is XORing two 0s or two 1s; but the XOR of a positive and\n      negative number is negative, because sooner or later you‚Äôre\n      always XORing a 0 with a 1 bit. Some languages, such as Python,\n      actually implement this ‚Äì you can try it out at the interactive\n      prompt!</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-emoji\">3.</a> Of course, this\n      doesn‚Äôt apply to  Unicode characters! Most don‚Äôt\n      have a concept of upper or lower case at all. And unfortunately,\n      this rule isn‚Äôt even obeyed by all of the characters that do. It\n      was consistently true in ASCII, and in some of the descendants\n      of ASCII, but Unicode as a whole wasn‚Äôt able to stick 100% to\n      the principle. If you take this too far, you might get strange\n      ideas, like the lower-case version of the car emoji being a ‚Äòno\n      pedestrians‚Äô sign:\n</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-well-actually\">4.</a> I\n      suppose, , you could argue that it is still\n      literally true that &nbsp;XOR&nbsp;, &nbsp;+&nbsp;,\n      and &nbsp;‚àí&nbsp; are all congruent to each other mod 2,\n      because all that means is that they all have the same low-order\n      bit, which they do. But that isn‚Äôt a\n      particularly  thing to say, because it ignores\n      the way all the higher-order bits do something completely\n      different!</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-integrity\">5.</a> However,\n      encrypting the message is only half the story. Given a\n      good-quality keystream, this technique is good enough to\n      assure , meaning that an eavesdropper\n      can‚Äôt find out what you‚Äôre saying. But it doesn‚Äôt do one single\n      thing to ensure , meaning that if your message\n      is modified by an attacker, the receiver can detect the\n      tampering and know not to trust the modified message. Integrity\n      protection is a completely separate problem. A common mistake in\n      novice cryptosystem design is to leave it out, assuming that if\n      nobody can figure out what the message says, then ‚Äúsurely‚Äù\n      nobody can work out how to tamper with it in a useful way\n      either. Even with more complicated encryption methods than what\n      I‚Äôm describing here, this never goes well!</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-proof-exercise\">6.</a> If you‚Äôre\n      mathematically minded and still don‚Äôt find this instantly\n      obvious, you might want to try actually  it. I\n      leave this mostly as an exercise for the reader, but a hint\n      would be: break down each of the input integers \n      and  as the sum of terms like\n      2 (each one being\n      an individual bit of one of the inputs times a power of 2), and\n      collect together the terms on each side that involve the\n      same .</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-or-xor\">7.</a> The extra gate that combines the carry\n      bits can be either an OR gate or an XOR gate, whichever is\n      easier. It doesn‚Äôt matter which, because the two types of gate\n      only disagree in the case where both their inputs are 1, and in\n      this circuit, that can‚Äôt happen! If the carry from the first\n      half-adder is 1, then its other output  be 0, which\n      means there‚Äôs no carry from the second half-adder.</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-swp\">8.</a> There is an Arm instruction called\n      SWP for ‚Äòswap‚Äô, but it‚Äôs not what you want for this kind of\n      purpose. It swaps a register with a value in ,\n      not two registers. It‚Äôs not really for ordinary computation:\n      it‚Äôs intended for atomically synchronising between threads,\n      which is very hard to get right and well beyond the scope of\n      this article!</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-aliasing\">9.</a> However, there‚Äôs a subtle\n      ‚Äúgotcha‚Äù about this technique. It works fine as long as the two\n      variables you‚Äôre swapping are actually different variables, but\n      it fails if they  each other. For example, if you\n      use this trick to swap two elements of an array, so\n      that  and  are replaced\n      with  and , then what\n      happens if &nbsp;=&nbsp;? In that situation you probably\n      wanted the ‚Äòswap‚Äô of an array element with itself to leave it\n      unchanged. But the triple-XOR swap idiom will turn it into zero,\n      which may well  be what you wanted!</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-misere\">10.</a> In fact, Nim comes in two\n      well-known forms, with opposite win conditions. In the variant\n      discussed here, your aim is to take the last counter and leave\n      your opponent with no move. In the other variant, the win\n      condition is exactly the reverse: you aim to make\n      your  take the last counter! The strategy for\n      the latter is a tiny bit more complicated, but not too bad. I\n      only discuss the simple version here.</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-infinite-positive-characteristic\">11.</a> However, the\n      converse isn‚Äôt true. Any finite field has positive\n      characteristic, but not every field with positive characteristic\n      is finite. There are also  fields with this\n      same modular-arithmetic property. But we won‚Äôt get as far as\n      discussing those here.</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-formal-polynomials\">12.</a> You might argue:\n      if  is a number in (2), then it‚Äôs either 0 or\n      1, and in either case, ,\n      or , or any higher power of , are\n      all equal to  itself. So under that assumption, any\n      polynomial of this kind could be simplified by squashing all the\n      higher-order terms down into the  term. But\n      we  do that here, because we don‚Äôt make the\n      assumption that  is one of the two numbers we know\n      about: we leave open the possibility that any two\n      powers \n      and  might be\n      unequal.</p><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-unique-finite-fields\">13.</a> In fact, it turns\n      out that  finite fields of a given\n      size  are the same as each other.\n      Choosing a different irreducible polynomial of the same degree\n      changes the , but not the underlying\n      thing being represented ‚Äì as if you still had all the same\n      numbers, but you just changed the name of each\n      one.</p>","contentLength":96945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1isept6/what_xor_is_and_why_its_useful/"},{"title":"To Deref or not to Deref?","url":"https://www.reddit.com/r/rust/comments/1ise16r/to_deref_or_not_to_deref/","date":1739889277,"author":"/u/awesomealchemy","guid":4431,"unread":true,"content":"<p>Do you impl Deref for your \"newtypes\" or do you stick with the  syntax?</p><pre><code>struct Frequency(f32); impl Deref for Frequency { type Target = f32; fn deref(&amp;self) -&gt; &amp;Self::Target { &amp;self.0 } } </code></pre><p>I'm torn. The .0 syntax gives me C++ std::pair PTSD and hurts my eyes. BUT the deref adds boilerplate and we still need to deref it... </p>","contentLength":322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lessons from 30 years in a programming chair","url":"https://www.codeproject.com/Articles/5152541/Lessons-from-a-Life-in-a-Chair","date":1739888821,"author":"/u/Full-Spectral","guid":4625,"unread":true,"content":"<p>I've been programing professionally since 1988, and for a number of years non-professionally before that. I started in the age of floppies and my High School computer class was BASIC programs on a mainframe terminal. So I've put in a good number of miles and hours in this thing of ours since then. I just wanted to throw out some things that I've learned or come to believe from spending 50'ish man-years in a programming chair and exploring a pretty wide variety of problem domains. Accept or ignore as you wish.</p><p>Ultimately, for those of us on the decidedly non-trivial end of the software spectrum, complexity is the devil. And, sadly, both optimization and flexibility are indirect tools of Satan. I say sadly, because there's no way we can really avoid them, and in appropriate measure they are very useful or absolutely necessary. But they clearly are big contributors to complexity that are not just the results of bad decisions or an infinitely uncaring universe. They are significant sources of complexity that even the best designed systems can't avoid and we create them purposefully.</p><p>Obviously on the very local scale, the tools can help a lot (see Say What you Mean below.) But on the larger scale, there's nothing going to save us that I can see. As a practical matter, we create linkages between disparate bits of code such that we can't really express to any tools enough information for them to be sure we are doing the right thing, both initially and over time. And we inevitably create situations where the internal state of objects are purposefully not coherent at all times so as to avoid overhead that may never be required, or to support some required or desired&nbsp;functionality (maybe like move semantics in C++.)</p><p>I have no answer for this at all, after 30+ years. I don't think that there is an answer. Unless someone makes a 'Do Exactly What my Idea Needs in a Can\" framework that has all of the issues pre-worked out, every significant undertaking is going to face those complexity issues and it seems to me nothing but extreme human vigilance can ultimately be used to manage it. And we humans aren't all that great at that.</p><h2>The Persistence of Persistence</h2><p>One thing I've learned the hard way is always, always make sure that any data you persist is versioned and carefully structured to allow for extension. Failure to do this almost always bites you in the butt somehow, somewhere. Once you get something stored and it's not versioned, and if that data type is in turn persisted as part of a bunch of other types, the only way to fix an issue may be to do something in every one of those containing types, since you may have to depend on the version of the containing type to know if you are dealing with the old, bad format or the new fixed one.</p><p>I have a few of these mistakes I made way, way back in the early days of my system, and wouldn't even want to consider having to make up for them even now, so they are still there and will likely have to remain unchanged forever.</p><p>Another mistake I made early on was to write out the contained data values first, then the stuff about the containing data type itself. But, that means that you can't even use the version of the containing data type to correct errors in the persistence of unversioned contained values because you can't get the version of the containing data type value until you've read all the contained values. That one bit me hard early on in a couple cases. So the lesson there is that if the data is hierarchical, make sure your versioning info matches that (pre-order I guess that would be.) That way, you always have an out to correct errors in contained value persistence.</p><h2>An Empire in my Underwear</h2><p>No, not that. I should be so lucky. No, this one is related to the fact that one of the great things about software is that it's one of those all too few undertakings where you can sit in your bedroom in your undies and potentially change the world, or create the seed that grows into a business empire that massively changes your bank balance and creates a lot of jobs. There are some others of course, though many of those are more of the&nbsp;creative nature where the value of your labor&nbsp;is much more a matter of opinion and fashion than a matter of demonstrated&nbsp;utility as it is more likely to be with software.</p><p>With a computer, a compiler, and a concept, you can potentially make a real difference in the world in some way.&nbsp;</p><p>Though there is of course a continuum of software endeavors, and some work on the lighter end of that spectrum may not be all that sensitive to this or that choice because they just aren't that complex, for those things that do reach the level where complexity itself becomes the enemy, the most explicit statement of intent is going to be best.</p><p>There is probably always some amount of pressure on languages to make it easier to write code fast. It's the same with most products. The ones that someone can sit down with and get to the happy-clappy demo stage quickest with will likely have an acceptance advantage. And that can be a great advantage for those things on the lighter end of the spectrum, or for creating the demo to get the VCs to make you into a golden unicorn.</p><p>But for complex systems that you are creating to sell, it's very much a matter of you write it once and you suffer for that sin forever more afterwards. So anything that increases speed of development at the cost of explicit expression of semantics (which is what tells the tools what you really mean, which is the only thing that lets the tools tell you that you are indeed doing what you really intend) is not a good tradeoff. In my opinion, all languages should prioritize increase in the ability to express semantics. Rust, for example, is doing some interesting things on this front, though I disagree with some of their other decisions.</p><h2>The Inevitability of Middle Age</h2><p>Languages seem to&nbsp;tend follow a similar life arc as people generally do. They start off fairly lean and focused. Then they slowly put on more and more poundage. Partly I guess it's the 'swim or die' thing, where you feel like you have to add features constantly or you will be perceived as falling behind and becoming irrelevant. Partly from trying to be more and more things to more and more people to increase appeal and applicability. Partly from dealing with users who are constantly arguing for the&nbsp;often widely varied or mutually exclusive bits and bobs they are particularly obsessed about.</p><p>In the end, the language ends up the overweight middle aged guy in a speedo. It's kind of bloated, too complex, too diffuse, trying to be too many things to too many people. It's become what it in some cases may have even been created in opposition to, something often true in the wider world in general. It leaves me sort of hoping for a punk revolution sometimes.</p><p>Somewhat of a related argument to this is that I think languages have to have the courage once in a while to just say, that's it. This is the end of this line of evolution. It will be maintained but nothing more. We need to drop a lot of evolutionary baggage and set up a new base camp at considerably higher altitude. Obviously that's hard, but the consequences of not doing it are pretty obvious with ever-accumulating evolutionary baggage, and probably whole areas of the language that effectively become immune to fundamental improvement because they can't realistically be rebuilt while the house is occupied.</p><h2>The Mistakes of the Past Become the Promise of the Future</h2><p>Wait around long enough, and the things that were in the past proven in real-world practice to be ill-formed and sub-optimal, and then corrected at great sacrifice, will come back around as the radical new future vision. Once you have enough people who started their careers after that solution was painfully implemented, they will grow up in a world where the only target for their frustration is the thing that was introduced to fix that original problem.</p><p>Then you eventually get a critical mass of people who have no memory of how bad it was before, they only see the issues that exist for them now, they see the results of bad human decisions and blame it on the tools and techniques, or believe that inherent problem/people complexity is actually tools and techniques complexity, and they start to argue that the old stuff is really the answer to all these problems they see around them, because it has to be better than the current paradigm. They often push these ideas as modernist when in fact they may be quite retrograde. They don't realise that, if they go back, they will still have all those bad human decisions, all that same inherent problem and people complexity, but now in the context of a set of techniques that were soundly rejected years ago for good reason.</p><h2>Entrepreneurs vs. Mercenaries</h2><p>It seems to me that there are two fundamental classes of developers. There are those who want to create something of their own to sell and there are those who work for other people. That's pretty obvious, and might seem unrelated to development, but I think these two undertakings create a very different view of the software world. For the former, the language is mostly just a tool, a means to an end. There's no point chasing the latest and greatest language features because your customers could care less. All they care about is features and quality. So, to the degree that a new language feature doesn't really contribute to the product or the code quality, the entrepreneur may not care at all.</p><p>Mercenaries, on the other hand, seem to me to tend to be more obsessed with the language itself because they believe (often justifiably) that knowing all the latest features is important to them to get to their next job. I.e. perhaps the language is as much a tool of career advancement to them. Therefore they are perhaps more likely to adopt new features just because they are there and presumably because they might get asked about them in their next interview.</p><p>This, it seems to me, is why you see so many people in language oriented forums arguing about the finer points of really new language features, or even stuff not likely to even happen for years yet, if at all, while actual companies are probably mostly not even fully using features from two revisions back.&nbsp;</p><p>I think this creates a bit of dissonance in online discussions because the underlying views of the participants may be so different but neither side really understands the other person's point of departure clearly. And it has to be said that the bulk of people in programming forums seem to tend towards the mercenary type, so that the entrepreneurial view is often not necessarily well received or understood.</p><h2>Let the X Percenters Take Care of Themselves</h2><p>We all grow up learning about the evils of premature optimization. And those lessons are correct. You can spend months and months optimizing code and introducing lots of extra complexity for little gain, while a simple tweak in a very limited area of the code might ultimately provide orders of magnitude more performance. And plenty of programs have no significant performance constraints at all.</p><p>But, in the C++ world for example, there seems to be this current obsession with performance optimization that sometimes also&nbsp;results in lots of extra complexity in the underlying infrastructure and applications, when it probably only actually is required in a very small percentage of programs, and even then within small areas of those programs. There are gasps of horror from the audience at virtual methods or runtime inheritance, when the actual difference this will make in the bulk of programs is not even worth worrying about worrying about, and what's used should be driven purely by what works best for you from an implementation point of view.</p><p>Obviously general purpose code does have some extra obligation on this front, but ultimately I think that introducing large amounts of complexity to heavily optimize even general purpose code is not a win overall. That code becomes far harder to maintain, harder to move forward quickly with safety, takes more brain cycles that could be applied to other things, and is more likely to have bugs. So it's really stealing from the 90% to serve the 10%, or whatever the actual relative percentages might be.</p><p>I say let those folks with exceptional performance requirements take care of themselves, and those few places in the more average program that really need significant optimization be dealt with specifically. That doesn't necessarily mean every one of them has to roll their own, but that they should at least use specialized tools for those programs or small bits of programs that really need it. That means more time goes into the stuff that will benefit the bulk of us more, and all our code is less likely to have bugs introduced over time.</p><h2>Take Project Structure Seriously From Day One</h2><p>Though it's easy to say, oh, we can always restructure it later, we all know that, in the real world with fairly sizeable teams and large code bases and probably a fair bit of gnarly code that no one wants to touch anymore, that actually doing any significant restructuring can be really difficult to justify to the powers that be, when you are working hard to just keep putting down the tracks in front of the train. And even difficult to justify to yourself, knowing it really needs to be done, given that you will get paid the same whether you risk a heart attack or not.</p><p>So I'd argue to take project structure seriously from the start. Think ahead out to some fairly worst case scenarios and plan for a lot of growth. If it never happens, it doesn't cost you that much. If it does, you will be more ready for it. Even if it seems like you are getting too fiddly at first, probably you won't regret that in the end.</p><p>Obviously this isn't the biggest issue in the world. I just mention it because it's all too easy to start a project just thinking, well let's get something working then we can see where to go from there. Then you get something working, business reality kicks in, and suddenly it's years later and it's a mess that will be brutal to straighten out, and you now have to do it while the train is moving fast.</p><p>Likely you still won't likely get it perfect up front, but some amount of serious preparatory thought and a bit more up front infrastructure setup work is generally worth it for non-trivial projects. I'm a bit better off than most, being a lone wolf developer, so it's easier to stop and just wholesale make changes across the whole code base. But they can be soul slash brain cell destroying, and possibly avoidable time wasted that could be spent on far more productive things.</p><h2>Separation of Data and Presentation</h2><p>This is an obvious one but it's still easy to get wrong. When you are starting a new big chunk, and it's a struggle just to get it done to begin with, it can be easy to forget things like this, and it's often a lot more work to set these things up right up front. I've suffered from this one well enough. In my defence, most of my mistakes (some of which I still haven't dealt with fully because of the difficulty of doing so) were made long ago before this issue was something that was drilled into all of us on a daily basis.&nbsp;</p><p>In my case, the big one is in my automation system's touch screen system. It's very complex. Just getting the original bits of it done was a monster undertaking, and it's grown massively since. Like many such things, it's a set of graphical, and often interactive, widgets that you can place on the screen via a designer and configure to look and act/react like you want. I ended up with the data that configures those widgets being part of the class hierarchy that does the actual display of them, so the two are tied together.</p><p>Very sub-optimal, though fifteen'ish years ago when I did the original work, I wasn't so aware of those types of issues. I can untie that knot, but it will be a lot of work at the expense of other important things and there are only so many super-model parties I can miss.</p><h2>Never Give Up, Never Surrender</h2><p>In the end, software is the perfect challenge for the techno-geek like me and probably many who are reading this (assuming many actually ever read this or read this far.) It's you against the dark forces of chaos. It's got all of the intellectual challenges of something like math or pure logic, but with (potentially at least) practical consequences. And it generally pays a lot better than either of those as well.</p><p>If you are just starting out down this road, just stick with it. Like any sort of open ended endeavor, nothing but time spent at the grindstone is going to make you better. You can't really think your way through it. You have to just get your hands bloody and ultimately sacrifice a good chunk of your time in this mortal coil if you want to really become a master of the art.</p><p>For some of us, that's not a bad trade off because we aren't necessarily that comfortable in the mortal coil to begin with. But, either way, you aren't going to become a master via casual effort. It will require a considerable commitment. Still, most anything in this life does to one degree or another if you want to get paid well for it, since if it was easy everyone could do it.</p><p>If you have to put in the time anyway, I'd argue that it's a good choice because it offers a lot of intellectual, geographical, and problem domain portability. Almost everyone needs software as part of whatever they are doing, and wherever they are. So you can either concentrate your whole life in one area, or dive into a number of different worlds with a set of skills that are useful in a lot of places. Throw in above average compensation, the ability in a lot of cases to work from home, and the lack of injuries and sore muscles at the end of the day, and it could be a lot worse.</p>","contentLength":17833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1isdvad/lessons_from_30_years_in_a_programming_chair/"},{"title":"[R] The Curse of Depth in Large Language Models","url":"https://www.reddit.com/r/MachineLearning/comments/1isdopn/r_the_curse_of_depth_in_large_language_models/","date":1739888312,"author":"/u/StartledWatermelon","guid":4373,"unread":true,"content":"<p> Uniform pre-layer norm across model's depth considered harmful. Scale the norm by 1/sqrt(depth) at each block.</p><blockquote><p>In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.</p></blockquote><blockquote><p>We measure performance degradation on the Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2021) by pruning entire layers of each model, one at a time, and directly evaluating the resulting pruned models on MMLU without any fine-tuning in Figure 2. Results: 1). Most LLMs utilizing Pre-LN exhibit remarkable robustness to the removal of deeper layers, whereas BERT with Post-LN shows the opposite trend. 2). The number of layers that can be pruned without significant performance degradation increases with model size</p><p>LayerNorm Scaling effectively scales down the output variance across layers of Pre-LN, leading to considerably lower training loss and achieving the same loss as Pre-LN using only half tokens.</p></blockquote>","contentLength":2354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reviewing the Cryptography Used by Signal","url":"https://soatok.blog/2025/02/18/reviewing-the-cryptography-used-by-signal/","date":1739887448,"author":"/u/tapo","guid":5615,"unread":true,"content":"<p>Last year, I <a href=\"https://soatok.blog/2024/05/14/its-time-for-furries-to-stop-using-telegram/\">urged furries to stop using Telegram</a> because it doesn‚Äôt actually provide them with any of the privacy guarantees they  it gives them. Instead of improving Telegram‚Äôs cryptography to be actually secure, the CEO started spreading misleading bullshit about Signal<a href=\"https://soatok.blog/2025/02/18/reviewing-the-cryptography-used-by-signal/#trademark\">¬Æ</a>.</p><p>Since then, I‚Äôve been flooded with people asking me about <a href=\"https://soatok.blog/encrypted-messaging-apps/\" target=\"_blank\" rel=\"noreferrer noopener\">various other encrypted messaging apps</a> and accused by Internet reply-guys of having malicious intentions. Some of the more egregiously stupid accusations were that I was somehow being paid to promote Signal.</p><blockquote><p>Not only am I not being paid to promote Signal, I refuse to ever be paid to promote  ever! .</p></blockquote><p>To be clear, being accused of being a paid shill for recommending Signal isn‚Äôt exactly unique to Signal, it also happens with other technologies.</p><p>For example: Have you ever wondered by influencers (streamers, vloggers, etc.) always promote ‚Äú<a href=\"https://gist.github.com/joepie91/5a9909939e6ce7d09e29\" target=\"_blank\" rel=\"noreferrer noopener\">VPN services</a>‚Äù instead of Tor (which is free)?</p><p>It‚Äôs not just ‚Äútoday‚Äôs sponsor‚Äù, either.</p><blockquote><p>: That they recommend a VPN and not Tor in their first table immediately makes me suspicious.</p></blockquote><blockquote><p> Why? I‚Äôve personally seen more news articles about Tor users getting de-anonymized than I have VPN users. [‚Ä¶]</p></blockquote><p>The rhetorical sleight-of-hand here isn‚Äôt particularly clever.</p><ul><li>Tor uses onion-routing to provide anonymity to Internet traffic.</li><li>VPNs just provide an encrypted tunnel to another ISP, and therefore do not offer anonymity.</li><li>You can‚Äôt  VPN users because they were never  to begin with!</li></ul><p>Tor is at least as private as any VPN. If you‚Äôre worried about exit nodes, only use Tor to access Onion Services or websites that use HTTPS.</p><p>As a security engineer that specializes in applied cryptography, I‚Äôm generally not interested in the ‚ÄúTor vs VPN‚Äù debate.</p><p>I‚Äôm  more interested in the ‚ÄúWireGuard vs OpenVPN‚Äù debate (on the side of WireGuard), and what lessons about software security the rest of the industry could learn from WireGuard.</p><p>In fact: If someone is promoting a VPN service in 2025 and that service doesn‚Äôt use WireGuard as its underlying protocol, they are almost certainly LARPing at security expertise rather than offering valuable advice.</p><blockquote><p>EDIT: I don‚Äôt currently have an opinion on <a href=\"https://blog.cloudflare.com/masque-building-a-new-protocol-into-cloudflare-warp/\" target=\"_blank\" rel=\"noreferrer noopener\">MASQUE</a>. </p><p>I don‚Äôt really keep up with everything Cloudflare does, unless it involves post-quantum cryptography.</p></blockquote><p>Like Tor, Signal doesn‚Äôt cost you anything to use. Nobody makes money by telling you to use either of those things. </p><p>And therein lies the question: Is Signal‚Äôs cryptography  good? And how can we be sure of that?</p><p>To know this, we first need to discuss cryptography audits.</p><p>Audits are a type of engagement between a vendor and a team of security consultants with specific expertise in the technologies involved.</p><p>How an audit works is, loosely:</p><ul><li>The vendor (or a third party, such as <a href=\"https://ostif.org/\" target=\"_blank\" rel=\"noreferrer noopener\">OSTIF</a>) hires the consultants for a timeboxed assessment of the security of the product or service in question.</li><li>The consultants (ideally with the source code in hand) will then try to find any way to subvert the normal operation of the product/service, especially in a way that‚Äôs useful for an attacker.</li><li>Any findings that result from the consultants‚Äô work are compiled together into an Audit Report, with specific recommendations for remediating the issues they identified.</li><li>The vendor responds by either fixing each issue, or documenting them as known limitations if a fix is impractical.</li><li>Optional: The Audit Report is made public.</li></ul><p>Regardless of the expertise of the consultants, every audit suffers from the same limitations:</p><ol><li>The engagement has a specific timebox, which means that coverage will be finite.</li><li>The engagement is performed over a finite number of snapshots of the source code (typically, one commit hash), so each subsequent commit to the codebase erodes the relevance of the audit.</li><li>The consultants are human beings, and therefore imperfect.</li></ol><p>Furthermore, performing an audit of a product or service without a clear threat model can lead to a lot of disagreement about the relevance or severity of any findings.</p><p>This cuts both ways: High-severity issues could actually be nothingburgers to the users of the app, or ‚Äúinformational‚Äù findings could be a dealbreaker to your users. Lacking clarity about the security goals and assumptions can hamstring any efforts to providing security assurance.</p><p>Unfortunately, sometimes you will see encrypted messaging apps proudly proclaim, ‚ÄúWe were audited‚Äù when facing criticism, except:</p><ul><li>Their last audit was 5+ years (and/or over 1000 commits) ago.</li><li>They only have the one public audit report.</li><li>The company and/or person that did the audit has no other online footprint, including other audits, and only seemed to pop up to opine about this one vendor.</li><li>The timebox for the audit is tiny compared to the quantity and complexity of the software in question.</li></ul><p>Plenty of smaller security consulting teams do excellent work.</p><p>What a more recognizable brand gives you, however, is a reasonable amount of quality control: They‚Äôre generally better resourced to ensure that the appropriate experts are assigned to each project, so you‚Äôre less likely to end up with a useless rubber-stamp Audit Report than you might from a one-person shop in over their head.</p><p>It‚Äôs probably better to use the list above to inform your heuristics for assessing the credibility of an Audit Report, rather relying on, ‚ÄúDid the biggest name sign off on it?‚Äù</p><h3>Why Are You Telling Us All This?</h3><p>Because, like the title says, I‚Äôm going to review the cryptography used by Signal.</p><p>This series is morally equivalent to the sort of work you‚Äôd get from an audit, if it were timeboxed over a weekend rather than several weeks.</p><h3>Objectives For This Series</h3><blockquote><p>It is with those four words this website is founded. Computer, smartphone, and online security does not require a degree or years of experience. All it requires is someone show you the way.</p></blockquote><p>My spin on the Decent Security mission statement is a bit more ambitious, if narrowly scoped:</p><blockquote><p><strong>You can understand applied cryptography.</strong></p><p>Verifying the security claims made by an encrypted messaging app does not require a Ph.D in mathematics or an encyclopedic knowledge of software vulnerabilities. All it requires is someone teaching the fundamentals, and then indulging in a bit of self-exploration.</p></blockquote><p>Time will tell if I‚Äôm successful or not.</p><p>I did the header image and wrote this blog post earlier, but I did the entirety of this cryptography review over the course of a single weekend. That was the original timebox I set for myself.</p><p>I do feel that, if I had allocated more time to it, I might have been able to explain some of the later components in greater detail.</p><p>In response to a Hacker News comment incorrectly calling my hobby and community a fetish (<a href=\"https://ghostarchive.org/archive/WWtE3\" target=\"_blank\" rel=\"noreferrer noopener\">archive</a>, <a href=\"https://archive.ph/R665n\" target=\"_blank\" rel=\"noreferrer noopener\">alternative</a>, <a href=\"https://web.archive.org/web/20250219190654/https://news.ycombinator.com/item?id=43105421\" target=\"_blank\" rel=\"noreferrer noopener\">second alternative</a>), I‚Äôve increased the amount of furry art in this series.</p><p>Signal, the Signal logo, and all related names, designs, and slogans are registered trademarks of Signal Technology Foundation.</p><p>Though my inclusion of their logo is almost certainly Fair Use (as I am writing about Signal, not pretending to represent Signal), their <a href=\"https://signal.org/brand/trademarks/\" target=\"_blank\" rel=\"noreferrer noopener\">Trademarks page</a> indicates that this text should be included.</p>","contentLength":7097,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1isddpm/reviewing_the_cryptography_used_by_signal/"},{"title":"I'm glad AI didn't exist when I learned to code","url":"https://www.reddit.com/r/programming/comments/1isd6pk/im_glad_ai_didnt_exist_when_i_learned_to_code/","date":1739886894,"author":"/u/sshivreddit","guid":4209,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sshivreddit\"> /u/sshivreddit </a> <br/> <span><a href=\"https://blog.shivs.me/im-glad-ai-didnt-exist-when-i-learned-to-code\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1isd6pk/im_glad_ai_didnt_exist_when_i_learned_to_code/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Welcome, Cot: the Rust web framework for lazy developers","url":"https://www.reddit.com/r/rust/comments/1isd428/welcome_cot_the_rust_web_framework_for_lazy/","date":1739886672,"author":"/u/m4tx","guid":4210,"unread":true,"content":"<p><strong>Ever wanted a Django-like experience in Rust? Meet</strong><a href=\"https://cot.rs/\">Cot</a><strong>, a batteries-included web framework designed for developers who just want to get things done.</strong></p><p>It has been built from a frustration that there is no easy-to-use, fully features web framework for Rust, even though the web development ecosystem has existed for quite a long time in the community. It builds upon projects such as axum, sea-query, tower, serde, and more, combining them in a package that allows you to start quickly, adding a lot of features in the process.</p><p>Cot comes with built-in authentication, sessions, an admin panel, templates, and even its own ORM with automatically generated migrations ‚Äì something that even the most established ORMs in the wild (such as SeaORM and Diesel) do not provide. It is still in early development and hence it's still missing many features and <strong>is by no means production-ready yet</strong>, but we're planning to make frequent updates to close the gap to other mature tools as quickly as possible!</p>","contentLength":989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Linux Company Called me (Veggero!) a Zombie","url":"https://www.reddit.com/r/linux/comments/1iscnkt/this_linux_company_called_me_veggero_a_zombie/","date":1739885306,"author":"/u/ManinaPanina","guid":4240,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ManinaPanina\"> /u/ManinaPanina </a> <br/> <span><a href=\"https://tube.kockatoo.org/videos/watch/fcd71cf8-37be-41ad-ab66-bb7efaf44350\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1iscnkt/this_linux_company_called_me_veggero_a_zombie/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nil channels in Go","url":"https://vishnubharathi.codes/blog/nil-channels-in-go/","date":1739885221,"author":"/u/scriptnull","guid":5667,"unread":true,"content":"<p>A friend from work messaged me today that they had a hard time because they had used  instead of  in their Go code.</p><p>I responded by saying that I usually have one rule of thumb i.e. to always use of  whenever I need a channel or map. That way I can be very sure that I can use those immediately.</p><p>They added that the surprising thing was it didn‚Äôt panic the program rather they ended up with an infinite loop that ran silently. I got more intrigued about the situation. So many questions started popping up in my mind. Why was I not able to catch it in the code review? Why was there no linter rule that could catch this? What is the point of having a nil channel in Go if I am brainwashing myself to always use ?</p><p>I went on to get some answers and here they are!</p><p>It is just a channel assigned to nil value.</p><p>When you try to send to a nil channel. </p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><h2>Receive from a nil channel</h2><p>When you try to receive a value from a nil channel</p><p>You again get a deadlock.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>Now let us try doing both from a nil channel.</p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><p>This ended up with deadlock too.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>But my friend mentioned they ran into an infinite loop and not a deadlock. How so?</p><p>My immediate suspicion was a  construct instead of  construct in the above program.</p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><p>Now that leads to an infinite loop without printing anything! Because  seems to not execute the  block when  is a nil channel. What can it do after all? It can‚Äôt really receive anything from an un-initialized nil channel, right? So it ignores the  block and always runs the  block again and again.</p><p>Now let us initialize the channel by using  instead of  to see how our dear friend  behaves.</p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><p>It was again an infinite loop, but this time the output was different. </p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>The zeros took over the output. I had to pipe the output to  to stop the program from running infinitely and at the same time collect some sample output.</p><p>What are these zeros? Where are they coming from?</p><p>Those are arising from the  block of the select. When the channel is not nil, our select statement attempts to receive a value. That results in printing , the three values that were sent to the channel. When we close a channel, all we get is the zero value. Hence we are getting zeros after that.</p><p>Is there a way to check if a channel is closed? yes, there is.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>We avoided printing zeros, but it is still leading to an infinite loop. Because the select is alternating between  and  blocks and continuously executes them.</p><p>Let us get rid of .</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>That didn‚Äôt prevent the infinite loop, our friend  is going on and on choose the  block and performing the if condition that evaluates to false always as the channel is closed after sending 3.</p><p>How do we avoid the infinite loop? Remember how the select statement ignored the  block when my friend accidentally used the nil channel instead of an initialized channel at the start of this post? That is exactly what we need to  the case in the  statement.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>Now we are out of an infinite loop but are hitting a deadlock after the channel is closed.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>Because after we  the case, the  statement essentially reduces to an empty  clause.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>Makes the go routine sleep forever, there is no case statement that it can listen to for receiving a message.</p><p>The core lesson however is</p><blockquote><p>nil channels are useful for disabling  blocks of </p></blockquote><p>I kind of arrived at this lesson in a weird way, but <a target=\"_blank\" rel=\"noopener\" href=\"https://www.youtube.com/watch?v=t9bEg2A4jsw\">this just for func episode</a> teaches it in a beautiful way. (Thanks Campoy if you are reading this!)</p><p>This is particularly useful when you are dealing with multiple channels in different cases of a  and if you want to diable the case blocks one by one when those channels are no longer needed. Going to copy-paste the example from that justforfunc episode to capture the idea.</p><p>The problem is to merge values coming from two channels and output them in another channel.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>Now the  routine could listen on both the channels and disable the case for a channel after it is closed to make sure that we don‚Äôt spend any more CPU time on that case.</p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><p>Let me solve the rest of the problem just for closure.</p><p>One way would be to break to an  label as shown below. That way, </p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><p>NOTE: this would work without the  statement also.</p><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>If the above is same as the previous solution, what is the point? We noticed that setting a channel to  is beneficial when we have multiple cases. In here, I could maybe use that as a check for the .</p><figure><table><tbody><tr><td><pre></pre></td><td><pre></pre></td></tr></tbody></table></figure><p>I should probably start brainwasing myself to make sure I set the channel to  after consuming it completely. That way I can avoid the weird break label syntax and disable select cases to get more throughput.</p><p>Anyhow I know of a simpler solution. So my recommended solution for my friend would be to</p><ul><li>initialize the channel with </li><li>use a  construct instead of  construct.</li></ul><figure><table><tbody><tr><td><pre></pre></td></tr></tbody></table></figure><p>The example that I gave was a very ‚Äútrimmed down‚Äù version of what my friend was trying to accomplish in a real-world system. He was trying to consume a channel and split the messages into two other channels. The miss was failing to  the channels where the split was occurring.</p><p>On the other end, we learned from the justforfunc example that, when we try to merge two channels into one, we could start setting the consumed channel(s) to nil.</p><p>This is provoking me to make up <a target=\"_blank\" rel=\"noopener\" href=\"https://go-proverbs.github.io/\">a Go proverb</a> of my own üòÖ Please excuse me if it sounds bad! You have come so far. So you can‚Äôt escape from it now - lol :D</p><p>‚ÄúInit when you split, Nil when you merge.‚Äù</p>","contentLength":5257,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iscmld/nil_channels_in_go/"},{"title":"RKE2-Agent and Cilium HostFirewall Blocking Port 9345","url":"https://www.reddit.com/r/kubernetes/comments/1isc6cm/rke2agent_and_cilium_hostfirewall_blocking_port/","date":1739883773,"author":"/u/zdeneklapes","guid":4186,"unread":true,"content":"<p>I'm setting up a Kubernetes cluster using Rancher RKE2 with Cilium as the CNI. Everything works fine on the <strong>RKE2 server (master node)</strong> with  and <strong>kube-proxy replacement activated</strong>.</p><p>However, when I try to add a , it seems that some rules are pulled to the worker node, and after approximately , port  is . This results in the following error on the worker node:</p><pre><code>Feb 18 09:45:28 compute-07 rke2[173412]: time=\"2025-02-18T09:45:28Z\" level=error msg=\"Failed to connect to proxy. Empty dialer response\" error=\"dial tcp &lt;my-public-server-ip&gt;:9345: connect: connection timed out\" </code></pre><p>To fix this, I tried allowing the port  before adding the new worker node by applying the following <strong>CiliumClusterwideNetworkPolicy</strong>:</p><pre><code>apiVersion: cilium.io/v2 kind: CiliumClusterwideNetworkPolicy metadata: name: allow-hostfirewall-9345 spec: nodeSelector: {} # Applies to all nodes ingress: - fromEntities: - all toPorts: - ports: - port: \"9345\" protocol: TCP egress: - toEntities: - all toPorts: - ports: - port: \"9345\" protocol: TCP </code></pre><p>Unfortunately, this did not resolve the issue.</p><p>Before starting , I confirmed that the port :</p><pre><code>root@compute-07:~# nc -zv &lt;ip&gt; 9345 Ncat: Version 7.92 () Ncat: Connected to &lt;ip&gt;:9345. Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds.https://nmap.org/ncat </code></pre><p>After starting , the port :</p><pre><code>root@compute-07:~# nc -zv &lt;ip&gt; 9345 Ncat: Version 7.92 ( https://nmap.org/ncat ) Ncat: Connection timed out. </code></pre><ol><li><strong>Why is port 9345 being closed after the RKE2 agent starts?</strong></li><li><strong>Is there a better way to explicitly allow this port through Cilium's hostFirewall?</strong></li><li><strong>What additional troubleshooting steps should I take to debug this issue?</strong></li></ol>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Clojure?","url":"https://www.reddit.com/r/programming/comments/1isbxlu/why_clojure/","date":1739882987,"author":"/u/therealplexus","guid":4312,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/therealplexus\"> /u/therealplexus </a> <br/> <span><a href=\"https://gaiwan.co/blog/why-clojure/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1isbxlu/why_clojure/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Evaluating LLMs on Real-World Software Engineering Tasks: A $1M Benchmark Study","url":"https://www.reddit.com/r/MachineLearning/comments/1isbo6t/r_evaluating_llms_on_realworld_software/","date":1739882100,"author":"/u/Successful-Western27","guid":4185,"unread":true,"content":"<p>A new benchmark designed to evaluate LLMs on real-world software engineering tasks pulls directly from Upwork freelance jobs with actual dollar values attached. The methodology involves collecting 1,400+ tasks ranging from $50-$32,000 in payout, creating standardized evaluation environments, and testing both coding ability and engineering management decisions.</p><p>Key technical points: - Tasks are verified through unit tests, expert validation, and comparison with human solutions - Evaluation uses Docker containers to ensure consistent testing environments - Includes both direct coding tasks and higher-level engineering management decisions - Tasks span web development, mobile apps, data processing, and system architecture - Total task value exceeds $1 million in real freelance payments</p><p>I think this benchmark represents an important shift in how we evaluate LLMs for real-world applications. By tying performance directly to economic value, we can better understand the gap between current capabilities and practical utility. The low success rates suggest we need significant advances before LLMs can reliably handle professional software engineering tasks.</p><p>I think the inclusion of management-level decisions is particularly valuable, as it tests both technical understanding and strategic thinking. This could help guide development of more complete engineering assistance systems.</p><p>TLDR: New benchmark tests LLMs on real $1M+ worth of Upwork programming tasks. Current models struggle significantly, completing only ~10% of coding tasks and ~20% of management decisions.</p>","contentLength":1576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Edge Developer VM Remote Code Execution","url":"https://www.reddit.com/r/programming/comments/1isb7mf/microsoft_edge_developer_vm_remote_code_execution/","date":1739880454,"author":"/u/FoxInTheRedBox","guid":5646,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FoxInTheRedBox\"> /u/FoxInTheRedBox </a> <br/> <span><a href=\"https://infosec.rm-it.de/2025/02/17/microsoft-edge-developer-vm-remote-code-execution/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1isb7mf/microsoft_edge_developer_vm_remote_code_execution/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simplifying Kubernetes deployments with a unified Helm chart","url":"https://www.reddit.com/r/kubernetes/comments/1isb1e2/simplifying_kubernetes_deployments_with_a_unified/","date":1739879866,"author":"/u/danielepolencic","guid":4122,"unread":true,"content":"<div><p>Managing  in  at scale often leads to inconsistent deployments and maintenance overhead. This episode explores a practical solution that standardizes service deployments while maintaining team autonomy.</p><p> discusses how a unified  chart approach can help platform teams support multiple development teams efficiently while maintaining consistent standards across services.</p><ul><li>Why inconsistent  chart configurations across teams create maintenance challenges and slow down deployments</li><li>How to implement a unified  chart that balances standardization with flexibility through override functions</li><li>How to maintain quality through automated documentation and testing with tools like  and </li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/danielepolencic\"> /u/danielepolencic </a>","contentLength":710,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reproducible-openSUSE (RBOS) Project Hits Milestone","url":"https://www.reddit.com/r/linux/comments/1isadeh/reproducibleopensuse_rbos_project_hits_milestone/","date":1739877303,"author":"/u/Vulphere","guid":4119,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vulphere\"> /u/Vulphere </a> <br/> <span><a href=\"https://news.opensuse.org/2025/02/18/rbos-project-hits-milestone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1isadeh/reproducibleopensuse_rbos_project_hits_milestone/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Questions and advice","url":"https://www.reddit.com/r/kubernetes/comments/1isa58r/weekly_questions_and_advice/","date":1739876419,"author":"/u/gctaylor","guid":4081,"unread":true,"content":"<p>Have any questions about Kubernetes, related tooling, or how to adopt or use Kubernetes? Ask away!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention (submitted by Liang Wenfeng - DeepSeek)","url":"https://www.reddit.com/r/MachineLearning/comments/1is9ufs/r_native_sparse_attention_hardwarealigned_and/","date":1739875169,"author":"/u/Nunki08","guid":4272,"unread":true,"content":"<p>Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng<em>Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.</em> arXiv:2502.11089 [cs.CL] : <a href=\"https://arxiv.org/abs/2502.11089\">https://arxiv.org/abs/2502.11089</a></p>","contentLength":1724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meilisearch 1.13","url":"https://www.meilisearch.com/blog/meilisearch-1-13","date":1739873145,"author":"/u/ggStrift","guid":4148,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1is9dml/meilisearch_113/"},{"title":"Longhorn does not recognize dm-crypt module in ubunti 24.04 vm.","url":"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/","date":1739872887,"author":"/u/MrSliff84","guid":4054,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Do i have to set up secrets first, to get rid of this warning in longhorn?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MrSliff84\"> /u/MrSliff84 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flatpak seems like a huge storage waste ?","url":"https://www.reddit.com/r/linux/comments/1is9a5q/flatpak_seems_like_a_huge_storage_waste/","date":1739872765,"author":"/u/LuccDev","guid":4080,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi guys. I am not here to spread hate towards flatpak or anything, I would just like to actually understand why anyone would use it over the distro&#39;s repos. To me, it seems like it&#39;s a huge waste of storage. Just right now, I tried to install Telegram. The Flatpak version was over 700MB to download (just for a messaging app !), while the RPM Fusion version (I&#39;m on Fedora non atomic) was 150MB only (I am including all the dependencies in both cases).</p> <p>Seeing this huge difference, I wonder why I should ever use flatpak, because if any program I want to install will re-download and re-install the dependencies on my disk that could have been already installed on my computer (e.g. Telegram flatpak was pulling... 380MB of &quot;platform locale&quot; ?)</p> <p>Also, do the flatpaks reuse dependencies with each other ? Or are they just encapsulated ?</p> <p>(Any post stating that storage is cheap and thus I shouldn&#39;t care about storage waste will be ignored)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LuccDev\"> /u/LuccDev </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1is9a5q/flatpak_seems_like_a_huge_storage_waste/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1is9a5q/flatpak_seems_like_a_huge_storage_waste/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Map internals in Go 1.24","url":"https://themsaid.com/map-internals-go-1-24","date":1739870475,"author":"/u/themsaid","guid":4239,"unread":true,"content":"<p>Maps in Go 1.24 have undergone a complete rewrite, significantly improving their performance. The new implementation draws inspiration from Google's high-performance hash map design known as <a href=\"https://abseil.io/about/design/swisstables#swiss-tables-design-notes\" rel=\"nofollow\">Swiss Tables</a>.</p><p>Under the new implementation, a Map is a collection of groups of 8 key/value pairs. Each  contains 8 slots of data in addition to a metadata field that holds a . The control word is 64 bits in size, each byte representing one of the slots.</p><div><pre>+---------------------+\n|  Control  (b) |  &lt;-  ( byte per slot)\n+---------------------+\n|  Key   |  Value   |  \n|  Key   |  Value   |  \n|  Key   |  Value   |  \n|  Key   |  Value   |  \n|  Key   |  Value   |  \n|  Key   |  Value   |  \n|  Key   |  Value   |  \n|  Key   |  Value   |  \n+---------------------+</pre></div><p>These groups are distributed across multiple tables, with each  containing a number of groups that is always a power of two. A power of two is a number that can be written as (2^n), such as 1, 2, 4, 8, etc...</p><div><pre>+---------------------+\n|       Table        |  \n+---------------------+\n+---------------------+\n|  Control  (b) |  &lt;- Group \n+---------------------+\n|  Key   |  Value   |   \n.\n|  Key   |  Value   |  \n+---------------------+\n+---------------------+\n|  Control  (b) |  &lt;- Group \n+---------------------+\n|  Key   |  Value   |  \n.\n|  Key   |  Value   |  \n+---------------------+</pre></div><p>The map holds a pointer to the array of tables it manages and replaces the array with another when the number of tables change.</p><p>When you lookup or insert/update a key. A hashing function is used to generate a 64 bit hash code. This code is then divided into two parts:</p><ol><li>The first 57 bits: referred to as .</li><li>The last 7 bits: referred to as .</li></ol><p>The full hash code is used to determine which table stores a given key in a multi-table map. Within each table, the h1 portion of the code identifies the potential groups that may contain the key. Once a group is selected, the h2 portion is utilized in the control word to enable fast key lookups within that group.</p><p>The hash code of a key is used to determine which table the key belongs to. This selection process is done by using a portion of the hash code, specifically a number of bits, based on how many tables are available. For example, if there's only a single table, the hash code is not considered and all keys go straight to the table. If there are two tables, the first bit in the code is used to determine which table the key belongs to:</p><div><pre>Hash:     : \nFirst bit : \nTable     : </pre></div><div><pre>Hash:     : \nFirst bit : \nTable     : </pre></div><p>Now let's say there are 4 tables. In that case, two bits are used:</p><div><pre>Hash:        : \nFirst  bits : \nTable        : </pre></div><div><pre>Hash:        : \nFirst  bits : \nTable        : </pre></div><div><pre>Hash:        : \nFirst  bits : \nTable        : </pre></div><p>Using the first 57 bits of the hash (), an internal function calculates the index of the group where a key should be stored. If that group is full (i.e., all 8 slots are occupied), quadratic probing is used to find the next group to check. Instead of simply checking the groups one after another (as in linear probing), quadratic probing uses a mathematical formula to determine the next group, with the step size increasing as the number of groups grows. This ensures that keys are distributed across as many groups as possible, reducing congestion and leading to faster lookups.</p><p>If linear probing were used instead, the runtime would just check the next group in line each time it encounters a full group. For example:</p><div><pre> group  =&gt; (full).\n group  =&gt; (also full).\n group  =&gt; (available slot).</pre></div><p>This could cause congestion in the groups at the beginning of the table, and lookups would become slower because the runtime would have to check more and more groups to find the desired key.</p><p>In contrast, with quadratic probing, the steps spread out across the table, reducing congestion and making lookups faster. Even when groups are full, the formula guarantees the next probe is further away in a way that distributes keys more evenly.</p><p>As mentioned earlier, the control word is 64 bits in size and each byte represents one of the slots. The last 7 bits of the hash () are used to fill the last 7 bits of the byte representing the slot. The one extra bit is used to flag the slot as empty, deleted, or occupied.</p><div><pre>Empty   : \nDeleted : \nOccupied: ******* (where * is a bit of h2)</pre></div><p>When looking up a key in a group, the control word is scanned to find potential slots in which the control byte matches h2 of the key. Once a slot is located, the runtime compares the input key with the key occupying the slot. If there's a match, the value is returned. If not, the next possible slot is checked.</p><p>In this example, occupied slots where the h2 value matches  appear twice in the control word. This means there are two slots where the key could potentially be stored:</p><div><pre>[][]</pre></div><p>When updating a key, the control word is first used to find if the key already exists in the group. If it doesn't exist in the group, or any other group in the probe sequence, the first available slot is used to insert the new key.</p><p>The process for looking up a key in a map follows these steps:</p><ol><li>Use the hash to determine the table.</li><li>Create a probe sequence for potential groups that may house the key (using h1).</li><li>Go over the groups one by one.</li><li>Scan the group's control word to find possible slots that match h2.</li><li>Check the key stored in each matching slot to verify it matches the input key.</li></ol><p>While traversing the groups, the search will stop immediately if the runtime encounters a group with an empty slot. This optimization improves the lookup process by allowing the search to return early, rather than continuing to probe all the groups in the sequence unnecessarily.</p><p>For this to work effectively, it is important that a slot which previously held a deleted key is not marked as empty. Instead, it is marked as \"deleted.\" This ensures that the search process can differentiate between truly empty slots and those that were once occupied but now deleted, allowing the lookup to behave correctly without mistakenly stopping prematurely.</p><p>The optimizations in this process stem from the following factors:</p><ol><li>The runtime only needs to scan a single table in a large map.</li><li>It only checks a subset of groups within that table.</li><li>It uses the control word first to verify the key's existence, rather than iterating through each key-value pair individually.</li><li>It returns early if an empty slot is found.</li></ol><p>When inserting a value into a map , the runtime first attempts to find a slot that is occupied by the same key, using the same probing sequence as in key lookups.</p><p>If the runtime encounters a group that contains an empty slot during the search, it concludes that the key does not exist in the map. The empty slot is then used to store the new key-value pair.</p><p>If no empty slots are found, the runtime proceeds to look for the first slot that is marked as \"deleted.\" This slot is available for reuse, and the new key-value pair is inserted into this deleted slot, ensuring that the table's capacity is efficiently used.</p><p>When deleting a key from a map, the runtime searches for the key using the same probing sequence as in key lookups. For each group, the runtime checks for the key using the control word and then inspects the actual key stored in each matching slot.</p><p>If the key was found, the runtime checks if there are empty slots in the group. If yes, the key is marked as empty. Otherwise, it's marked as deleted to avoid interrupting lookups prematurely as explained in a previous section.</p><p>If the key wasn't found in a group while the group had empty slots. The search stops as this means the key doesn't exist in the map.</p><p>When you make a small map, size &lt;= 8, the runtime creates a map with no tables and a single group. Inside that single group, all slots are either empty or occupied (no slots are marked as \"deleted\"). This is because a small map doesn't use the probing sequence for lookups, it simply iterates over the 8 slots of the single group until it finds a match.</p><p>As the map grows in size, or if you make a large map, the tables concept is introduced to speed up lookups by distributing the keys on multiple groups.</p><p>When a table is first created, it is allocated a capacity that allows it to hold between 16 and 1024 slots (equivalent to 2 to 128 groups). As the number of occupied slots approaches ~87% of the table capacity, the table undergoes a growth operation in which the runtime multiplies the current capacity by 2. If the new value is 1024 slots or less, the table is simply replaced with a new one that has double the old capacity. However, if expanding the table would push its capacity beyond 1024 slots, the table is instead split into two separate tables to manage the growing data efficiently.</p><p>When a table is split into two, the tables array in the map data structure is replaced with a larger array, whose size is always a power of two (1, 2, 4, 8, ‚Ä¶). For example, imagine a scenario where a map initially has two tables (Table 0 and Table 1). If Table 0 needs to be split, the total number of tables increases to three. However, to maintain a power-of-two-sized array, the tables array grows to size 4 and multiple entries in the new array can point to the same table:</p><div><pre> ()\n ()\n</pre></div><p>Using the key hash, what used to map to table 0 before the split, may now map to table  or . And what used to map to table 1 before the growth, will keep pointing to table 1 (represented by index 2 and 3 in the array.):</p><pre lang=\"math\"><code>Before Growth:\n\nSelection bit = 0  --&gt;  [index 0] --&gt;  Table 0\nSelection bit = 1  --&gt;  [index 1] --&gt;  Table 1\n</code></pre><pre lang=\"math\"><code>After Growth:\n\nSelection bits = 00  --&gt;  [index 0]  --&gt;  Table 0a\nSelection bits = 01  --&gt;  [index 1]  --&gt;  Table 0b\nSelection bits = 10  --&gt;  [index 2]  --&gt;  Table 1\nSelection bits = 11  --&gt;  [index 3]  --&gt;  Table 1\n</code></pre><p>This techniques avoids rehashing the entire map and allows for individual tables to grow separately.</p><p>Reading the source code of Go 1.24's map implementation was a fascinating experience. One of the things I truly admire about Go is that its core functionality is implemented in the language itself. This makes it possible to dive deep into the internals, understand exactly how things work behind the scenes, and learn from the incredibly talented contributors who have shaped the language.</p><p>If you're curious and want to explore the source code yourself, you can start <a href=\"https://github.com/golang/go/blob/master/src/internal/runtime/maps/map.go\">here</a>.</p>","contentLength":10240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1is8rwa/map_internals_in_go_124/"},{"title":"What Cgroup v2 Features Are You Using Beyond Basic CPU and Memory limit in Kubernetes? (Alpha features or customized plugins)","url":"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/","date":1739869690,"author":"/u/Electronic_Role_5981","guid":4053,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">https://kubernetes.io/docs/concepts/architecture/cgroups/</a> </p> <p><a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">cgroup v2 </a>is stable since v1.25.</p> <p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#memory-qos-with-cgroup-v2\">MemoryQoS</a> started using memory.high, but it may cause throttling issue to hang the application sometimes. It is still alpha since 1.22.</p> <p>For OOMKill behavior change, kubelet added <a href=\"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/\">singleProcessOOMKill</a> to keep the behavior of cgroups v1 when users want. </p> <p><a href=\"https://github.com/kubernetes/enhancements/issues/4205\">PSI KEP</a> was merged recently for v1.33.</p> <p><a href=\"https://github.com/kubernetes/enhancements/issues/2400\">NodeSwap</a> was beta now.</p> <p>Cgroup v2 controller includes:</p> <ul> <li><strong><em>memory (since Linux 4.5)</em></strong></li> <li><em>pids</em> (since Linux 4.5)</li> <li><strong><em>io</em></strong> <strong>(since Linux 4.5)</strong></li> <li><em>rdma (since Linux 4.11)</em></li> <li><em>perf_event</em> (since Linux 4.11)</li> <li><strong><em>cpu (since Linux 4.15)</em></strong></li> <li><em>cpuset</em> (since Linux 5.0)</li> <li><strong><em>freezer</em></strong> <strong>(since Linux 5.2)</strong></li> <li><em>hugetlb</em> (since Linux 5.6)</li> <li><em>nsdelegate</em> (since Linux 4.15)</li> <li>PSI(since Linux 4.20)</li> </ul> <p>Anyone started using the blkio limit or other cgroup controllers? Are you enable the CgroupV2 related feature gates above or flags? </p> <ul> <li>Some related projects: <ul> <li> <a href=\"https://facebookmicrosites.github.io/oomd/\">https://facebookmicrosites.github.io/oomd/</a></li> <li> <a href=\"https://github.com/facebookincubator/oomd\">https://github.com/facebookincubator/oomd</a></li> </ul></li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NAS build","url":"https://www.reddit.com/r/linux/comments/1is8ilc/nas_build/","date":1739869344,"author":"/u/Account34546","guid":4052,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello guys,<br/> I&#39;d like to ask you for help. My house has several Windows PCs of various family members and after recent event during which one PC completely crashed and lost all data I decided there&#39;s need to do proper data backup. My idea is building simple machine, used as network storage to save PC systems image and maybe user&#39;s media files (video, photos and so on).<br/> Currently I don&#39;t need any fancy features, all I need is (probably) Samba share to this potential network storage machine.<br/> Can you recommend suitable distro which will be used as a backbone of this system?<br/> Debian? Ubuntu? Any other? The more simple, the better.</p> <p>Edit: This community is epic! I really did not expect so many suggestions.<br/> Thank you guys, looks like I have something to tinker with for couple of days.<br/> I&#39;ll let you know the result when it&#39;s done.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Account34546\"> /u/Account34546 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1is8ilc/nas_build/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1is8ilc/nas_build/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making my debug build run 100x faster so that it is finally usable","url":"https://www.reddit.com/r/programming/comments/1is8azp/making_my_debug_build_run_100x_faster_so_that_it/","date":1739868442,"author":"/u/broken_broken_","guid":4526,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/broken_broken_\"> /u/broken_broken_ </a> <br/> <span><a href=\"https://gaultier.github.io/blog/making_my_debug_build_run_100_times_faster.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1is8azp/making_my_debug_build_run_100x_faster_so_that_it/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Web What? - How gaming is coming to browsers","url":"https://www.reddit.com/r/programming/comments/1is7hbx/web_what_how_gaming_is_coming_to_browsers/","date":1739864796,"author":"/u/monster4210","guid":4120,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/monster4210\"> /u/monster4210 </a> <br/> <span><a href=\"https://marshalldoes.dev/blog/web-what/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1is7hbx/web_what_how_gaming_is_coming_to_browsers/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"unexpected side effects in pod routing","url":"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/","date":1739862394,"author":"/u/Cyclonit","guid":4004,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I am working on hosting <a href=\"https://www.home-assistant.io/\">Home Assistant</a> in my Kubernetes Homelab. For Home Assistant being able to discover devices in my home network, I added a secondary bridged macvlan0 network interface using Multus. Given that my router manages IP addresses for my home network, I decided to use DHCP for the pod&#39;s second IP address too. This part works fine.</p> <pre><code>apiVersion: &quot;k8s.cni.cncf.io/v1&quot; kind: NetworkAttachmentDefinition metadata: name: eth0-macvlan-dhcp spec: config: | { &quot;cniVersion&quot;: &quot;0.3.0&quot;, &quot;type&quot;: &quot;macvlan&quot;, &quot;master&quot;: &quot;eth0&quot;, &quot;mode&quot;: &quot;bridge&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;dhcp&quot; } } </code></pre> <p>However, using DHCP results in the pod receiving a second default route via my home network&#39;s router. This route takes precedence over the default route via the pod network and completely breaks pod-to-pod communication.</p> <p>This is how the routes look like inside of the container after deployment:</p> <pre><code>```sh $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 192.168.178.1 0.0.0.0 UG 0 0 0 net1 default 10.0.2.230 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 ``` </code></pre> <p>This is what happens after trying to delete the first route. As you can see, the default route via <a href=\"http://10.0.2.230\">10.0.2.230</a> was replaced by a default route via localhost. <a href=\"http://10.0.2.230\">10.0.2.230</a> is not an IP of the pod.</p> <pre><code>$ route del -net default gw 192.168.178.1 netmask 0.0.0.0 dev net1 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default localhost 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 </code></pre> <p>Interestingly, this is completely reversible by adding the undesired route back:</p> <pre><code>$ route add -net default gw 192.168.178.1 netmask 0.0.0.0 dev net1 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 192.168.178.1 0.0.0.0 UG 0 0 0 net1 default 10.0.2.230 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 </code></pre> <p>Any ideas on what is going on?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cyclonit\"> /u/Cyclonit </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What does Rust development look like on Windows?","url":"https://www.reddit.com/r/rust/comments/1is6d2u/what_does_rust_development_look_like_on_windows/","date":1739860222,"author":"/u/crankykernel","guid":4273,"unread":true,"content":"<p>I'm a 25-year developer of C, and for the last decade Rust on Linux and only Linux systems living in the terminal and Emacs. However, I want to provide a better first class experience for our app on Windows.. Its a background service, fortunately. We already do build on Windows with MSYS2/mingw32 or whatever. My experience with Windows is installing that toolset when our Windows CI break.</p><p>However, to make Windows more first class, I want to setup a proper Windows development environment for Rust. I've got as far as installing Rust w/Rustup in Powershell terminal and getting VSCode to work. But still jumping back to my msys2 environment for git etc.</p><p>Anyone care to tell me what their development environment for mainly Rust apps looks like on Windows?</p>","contentLength":756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sin/Cosine SIMD functions?","url":"https://www.reddit.com/r/rust/comments/1is66j5/sincosine_simd_functions/","date":1739859511,"author":"/u/West-Implement-5993","guid":4447,"unread":true,"content":"<div><p>To my surprise I discovered that  isn't implemented in Rust yet (see <a href=\"https://github.com/rust-lang/stdarch/issues/310\">https://github.com/rust-lang/stdarch/issues/310</a>). Is there an alternative way to run really wide sin/cosine functions (ideally AVX512 but I'll settle for 256)? I'm writing a program to solve Kepler's equation via Newton‚ÄìRaphson for many bodies simultaneously.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/West-Implement-5993\"> /u/West-Implement-5993 </a>","contentLength":372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Every time I build my Golang project, I get Microsoft Defender warnings","url":"https://www.reddit.com/r/golang/comments/1is61jy/every_time_i_build_my_golang_project_i_get/","date":1739858974,"author":"/u/freewheel1466","guid":4307,"unread":true,"content":"<p>Every time I build my Golang project, I get this message:</p><p><code>bash Threats found Microsoft Defender Antivirus found threats. Get details. </code></p><p>This wouldn't have been a problem if my binary was meant to run on a server. However, I'm building a desktop application which will run on customers' end devices (Windows 10/11 PCs).</p><p>If the binaries I build get flagged by Windows Defender, I don't see how I could get people to run it.</p>","contentLength":417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"You can‚Äôt call yourself a senior until you‚Äôve worked on a legacy project","url":"https://www.reddit.com/r/programming/comments/1is4us1/you_cant_call_yourself_a_senior_until_youve/","date":1739854657,"author":"/u/10ForwardShift","guid":3956,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/10ForwardShift\"> /u/10ForwardShift </a> <br/> <span><a href=\"https://www.infobip.com/developers/blog/seniors-working-on-a-legacy-project\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1is4us1/you_cant_call_yourself_a_senior_until_youve/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are some great Youtube channels that explain the newest developments in AI?","url":"https://www.reddit.com/r/artificial/comments/1is2mgl/what_are_some_great_youtube_channels_that_explain/","date":1739847446,"author":"/u/bearhunter429","guid":4308,"unread":true,"content":"<p>What are your favorite ones?</p>","contentLength":28,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is this book still applicable ?","url":"https://www.reddit.com/r/golang/comments/1is2dlm/is_this_book_still_applicable/","date":1739846680,"author":"/u/United-Nebula3793","guid":4118,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/United-Nebula3793\"> /u/United-Nebula3793 </a>","contentLength":40,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux bluetooth headset support for high quality audio + microphone at the same time?","url":"https://www.reddit.com/r/linux/comments/1is1tdd/linux_bluetooth_headset_support_for_high_quality/","date":1739844979,"author":"/u/PinkFreudBrasil","guid":3955,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>This problem has been going on for years and I am wondering if there is a solution nowadays, or what does it take for a solution to emerge:</p> <p>Connect your bluetooth headset to your linux machine and choose one:</p> <ol> <li><p>High quality with A2DP and no microphone</p></li> <li><p>Very low quality audio with HSP/HFP and microphone</p></li> </ol> <p>Is there a solution nowadays?</p> <p>If there is no solution yet, why not? This is the kind of thing that affects everyone, so I would imagine there is enough motivation for people to work on a solution. </p> <p>How do you work around this problem?</p> <p>I used to have a keyboard shortcut to switch between the two modes, but I remember it was not optimal and introduced other problems as a trade-off.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PinkFreudBrasil\"> /u/PinkFreudBrasil </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1is1tdd/linux_bluetooth_headset_support_for_high_quality/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1is1tdd/linux_bluetooth_headset_support_for_high_quality/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python 3.14, due out later this year, is set to receive a new type of interpreter that can boost performance by up to 30% with no changes to existing code. üë®‚Äçüíª","url":"https://www.reddit.com/r/programming/comments/1is11rv/python_314_due_out_later_this_year_is_set_to/","date":1739842682,"author":"/u/Choobeen","guid":2131,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>The CPython 3.14 change log describes the feature as ‚Äúa new type of interpreter based on tail calls.‚Äù This description may be a little misleading for those who don‚Äôt closely follow internal Python development work. ‚ÄúTail calls‚Äù doesn‚Äôt mean that CPython, or the Python language, will now support tail call optimization. It refers to an optimization that a C compiler performs on the CPython code, which speeds up the way the interpreter dispatches its bytecode instructions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Choobeen\"> /u/Choobeen </a> <br/> <span><a href=\"https://www.infoworld.com/article/3820890/a-new-interpreter-in-python-3-14-delivers-a-free-speed-boost.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1is11rv/python_314_due_out_later_this_year_is_set_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simple cli and gui for DD","url":"https://www.reddit.com/r/linux/comments/1is07kl/simple_cli_and_gui_for_dd/","date":1739840314,"author":"/u/Serious_Hippo_9296","guid":2146,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I needed a tool to make it easier and faster to make 1:1 disk images and burn ISO&#39;s. If anyone has a feature they want added, let me know, Its a shell script that should work with multiple platforms. There is cli and gui version.</p> <p><a href=\"https://github.com/DigijEth/DD_Toolbox/tree/main\">https://github.com/DigijEth/DD_Toolbox/tree/main</a></p> <p>DD Toolbox is a versatile script designed for Linux systems to create 1:1 copies of drives and burn ISO files to USB drives. It also includes advanced disk operations such as zeroing out drives, writing random data, cloning drives, and managing MBR backups. The script supports progress monitoring using the <code>pv</code> tool and logs all operations for reference.</p> <ul> <li>Burn ISO images to USB drives</li> <li>Download ISO images from the internet and burn them</li> <li>Create 1:1 disk images from USB drives</li> <li>Create ISO images from directories</li> <li>Advanced disk operations: <ul> <li>Zero out a drive</li> <li>Write random data to a drive</li> <li>Clone one drive to another</li> <li>Backup and restore MBR</li> </ul></li> <li>Dependency checking and installation</li> <li>Progress monitoring with <code>pv</code></li> <li>Comprehensive logging</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Serious_Hippo_9296\"> /u/Serious_Hippo_9296 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1is07kl/simple_cli_and_gui_for_dd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1is07kl/simple_cli_and_gui_for_dd/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can anyone explain to me in simple terms what vCPU means? I have been scratching my head over this.","url":"https://www.reddit.com/r/golang/comments/1irz945/can_anyone_explain_to_me_in_simple_terms_what/","date":1739837593,"author":"/u/gwwsc","guid":4406,"unread":true,"content":"<div><p>If I run a go app on an EC2 server, does it matter if it has 1vCPU or 2vCPU? How should I determine the hardware specifications of the system on which my app should run?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/gwwsc\"> /u/gwwsc </a>","contentLength":197,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The C3 Programming Language - version 0.6.7 released","url":"https://www.reddit.com/r/programming/comments/1irxwio/the_c3_programming_language_version_067_released/","date":1739833898,"author":"/u/Nuoji","guid":2085,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Nuoji\"> /u/Nuoji </a> <br/> <span><a href=\"https://c3.handmade.network/blog/p/8994-it%2527s_february_and_time_for_a_new_c3_release__0.6.7\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irxwio/the_c3_programming_language_version_067_released/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Worker pool with some extras","url":"https://www.reddit.com/r/golang/comments/1irxsfw/worker_pool_with_some_extras/","date":1739833608,"author":"/u/umputun","guid":4446,"unread":true,"content":"<p>Created this package out of necessity after dealing with many concurrent and parallel data processing tasks. While building worker pools manually is perfectly fine for simple cases, some scenarios can get quite tricky to handle correctly.</p><p>This package gives you a clean API with some neat features - you can batch tasks for better performance, keep worker state, route related tasks to the same worker, collect metrics, and handle errors the way you want. It comes with middleware for common needs like retries with backoff, timeouts, panic recovery and validation, plus you can add any custom middleware you need to address any cross-cutting concerns.</p><p>You can even chain multiple pools together to build complex processing pipelines. All type-safe with generics. The pool is fast enough and even outperforms many manual implementations.</p>","contentLength":835,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Debugging An Undebuggable App","url":"https://www.reddit.com/r/programming/comments/1irxpv2/debugging_an_undebuggable_app/","date":1739833424,"author":"/u/DreamyRustacean","guid":2112,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DreamyRustacean\"> /u/DreamyRustacean </a> <br/> <span><a href=\"https://bryce.co/undebuggable/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irxpv2/debugging_an_undebuggable_app/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Whats the most kubefriendly pubsub messaging broker?","url":"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/","date":1739831242,"author":"/u/leeliop","guid":2089,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Like rabbitmq or even amazon sns?</p> <p>Or is it easier just using sns if we are in eks/amazon managed k8s land?</p> <p>Its for enterprise messaging volume, not particularly complex but just lots of it</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/leeliop\"> /u/leeliop </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minecraft from scratch with only modern openGL","url":"https://www.reddit.com/r/golang/comments/1irwit1/minecraft_from_scratch_with_only_modern_opengl/","date":1739830424,"author":"/u/One_Mess_1093","guid":2083,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1irwit1/minecraft_from_scratch_with_only_modern_opengl/\"> <img src=\"https://external-preview.redd.it/MoadT5xBO4ujN4IJxeK687Yka2borosJ0cF4_zWoSj4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=692e4a1041c21b00f7d0c804c056ce4366b2e2d4\" alt=\"Minecraft from scratch with only modern openGL\" title=\"Minecraft from scratch with only modern openGL\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/One_Mess_1093\"> /u/One_Mess_1093 </a> <br/> <span><a href=\"https://github.com/GianlucaP106/minecraft\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irwit1/minecraft_from_scratch_with_only_modern_opengl/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What if BSD law suit never happened, and BSD succeded Linux?","url":"https://www.reddit.com/r/linux/comments/1irwgpn/what_if_bsd_law_suit_never_happened_and_bsd/","date":1739830280,"author":"/u/cryptobread93","guid":2084,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>For people who doesn&#39;t know the history, you know BSD&#39;s had a lawsuit because of Unix stuff at 1991, which BSD team didn&#39;t deserve for. Because of the lawsuit, they couldn&#39;t continue developing BSD kernel for 2 years until the case ended at 1992 or so. From this space, Linux emerged and succeeded BSD. And in turn it blown up, to this day.</p> <p>But even Linus Torvalds said had the case about BSD&#39;s was resolved back then, he wouldn&#39;t ever create Linux, and contribute to BSD instead. Where would we be if this BSD case never happened and Linux was never created? Would companies have more foothold over us citizens, with their BSD license allowing them to close their source their code? </p> <p>I don&#39;t think any companies wouldn&#39;t voluntarily contribute any code back. Open source would greatly suffer, I think. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cryptobread93\"> /u/cryptobread93 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1irwgpn/what_if_bsd_law_suit_never_happened_and_bsd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irwgpn/what_if_bsd_law_suit_never_happened_and_bsd/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The main issue of linux distros","url":"https://www.reddit.com/r/linux/comments/1irw90a/the_main_issue_of_linux_distros/","date":1739829754,"author":"/u/FatihAlper_","guid":2057,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I think the main issue about all linux distros is troubleshooting. I&#39;ve used many, many distros. Fedora, Ubuntu, Debian, openSUSE, ElementaryOS, PopOS, Arch, Manjaro, KDE Neon, Nobara, Vanilla OS, Silverblue, SteamOS 2 even RHEL. I&#39;m using linux long before kernel 4. Every single time, when something breaks, i find myself in stackoverflow, reading shit that i dont know, trying to learn it, trying to improvise it and every single time i spent hours just to install the fucking os again.</p> <p>Yeah operating systems are fragile, linux distros even more fragile. Therefore issues can be expected. But troubleshooting in linux is just an hot mess. For example, you are trying to install some shitty app for your project. In windows, if app is not working, it might be a missing dll, so just find it and place the correct folder. Maybe you need some dependencies, .NET probably. Find the exe, next next next and you are done. Its that easy.</p> <p>In linux, it may look the same, install the dependency from another source if its not in your repo and voila. But that dependency, that goddamn file needs something specific. For example a specific version of fuse. So you, like a normal person trying to install that needed fuse which of course you have no idea what fuse about then reboot your pc and you have a fucking black screen and white texts. One fucking single command breaks your entire os.</p> <p>This thing wont be a problem in modern windows versions. Its EASY. Its really EASY to MAINTAIN. You have some helpful programs too if you want a super simple easy++ baby mode if you want. But in linux if your system breaks, it will cost you 3+ hours just to fix your mistake, damn maybe its not even your mistake. </p> <p>Also, i dont believe the word &#39;linux is not for everyone&quot; linux is so specifically designed that its JUST for psycho power users. Not for my grandma, not for my dad, not for my sister. Only and only for psychos that could kill a man just for the ability to fixing the goddamn operating system.</p> <p>That&#39;s why linux will never be a common os. If you are a sysadmin, yeah go for it. If you are a developer, yeah you can use linux. If you are a nerd that have enthusiasm towards the linux, heck yeah go for it. But if you are just a regular user that need something that just works, you should run away from linux. </p> <p>Final words, in an essence, linux just an alpha state os for desktop users. Still in progres, always will be. There is not enough demand for easy to use linux distro, so there will be no supply. You can always brag about how poweruser u are but some guy at the starbucks with his shitdows pc takes ease of use for granted and never look back to linux. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FatihAlper_\"> /u/FatihAlper_ </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1irw90a/the_main_issue_of_linux_distros/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irw90a/the_main_issue_of_linux_distros/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Being fluent in Go can give you greater returns in the long-run","url":"https://www.reddit.com/r/golang/comments/1irw2jj/being_fluent_in_go_can_give_you_greater_returns/","date":1739829322,"author":"/u/narenarya","guid":3954,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Here&#39;s what I observed after programming in Go, Python, and JavaScript for quite a bit of time now (&gt; 10 years)</p> <p>Both Python &amp; JavaScript provide better initial returns despite less fluency, whereas Go will be very productive once you feel comfortable.</p> <p>So, if you are already in Go learning path, keep pushing! It will soon pay you back in hefty amounts.</p> <p>I made a chart to show this:</p> <p><a href=\"https://imgur.com/a/i4ZwdcK\">Go learning curve &amp; returns</a></p> <p>I would like to hear your opinions about working with other programming languages &amp; Go in terms of productivity.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/narenarya\"> /u/narenarya </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1irw2jj/being_fluent_in_go_can_give_you_greater_returns/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irw2jj/being_fluent_in_go_can_give_you_greater_returns/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[ Removed by Reddit ]","url":"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/","date":1739826481,"author":"/u/Vw-Bee5498","guid":3977,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>[ Removed by Reddit on account of violating the <a href=\"/help/contentpolicy\">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vw-Bee5498\"> /u/Vw-Bee5498 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Rust-based project hit 20k stars on GitHub ‚Äî dropping some cool merch to celebrate","url":"https://www.reddit.com/r/rust/comments/1iruuaf/my_rustbased_project_hit_20k_stars_on_github/","date":1739826333,"author":"/u/GyulyVGC","guid":2113,"unread":true,"content":"<p><a href=\"https://github.com/GyulyVGC/sniffnet\">Sniffnet</a> is an open source network monitoring tool developed in Rust, which got much love and appreciation since the beginning of this journey.</p><p>If it accomplished so much is also thanks to the support of the Reddit community, and today I just wanted to share with you all that we're dropping some brand new apparel ‚Äî I believe this is a great way to sustain the project development as an alternative to direct donations.</p>","contentLength":421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"go version -m","url":"https://www.reddit.com/r/golang/comments/1irus4i/go_version_m/","date":1739826191,"author":"/u/der_gopher","guid":2036,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Did you know that you can inspect any Go binary and find the Go version it was built with? <code>go version -m &lt;file&gt;</code> does just that, plus shows other helpful build info. Comes very handy when debugging binaries shipped to prod.</p> <p>``` $ go version -m ./ultrafocus</p> <p>ultrafocus: go1.24.0 path github.com/plutov/ultrafocus mod github.com/plutov/ultrafocus v0.3.1 build GOARCH=amd64 build GOOS=darwin build GOAMD64=v1 build vcs=git build vcs.revision=4f9ebaee5de88c9fa3ea27d5327edb5283e624f0 build vcs.time=2024-10-11T11:23:07Z build vcs.modified=false ```</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/der_gopher\"> /u/der_gopher </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1irus4i/go_version_m/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irus4i/go_version_m/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On finding Linux help with Google search","url":"https://www.reddit.com/r/linux/comments/1irul62/on_finding_linux_help_with_google_search/","date":1739825737,"author":"/u/Unlikely-Giraffe9369","guid":3975,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m a linux noob and one of the things I find great about linux is that any problem I have, I can just google it and find the answer... or so I thought.</p> <p>Sure most of the time I can find some forum or reddit post with the solution I need, but I feel like google just sucks at giving me the information I want. Although forum posts are useful, most of what appears after a google search is just a frequently asked question related to my problem, but it&#39;s not exactly what I&#39;m looking for. There&#39;s also the issue that most forum posts are many years old and have outdated information.</p> <p>I think it would be a lot more useful - and sensible - if the top results also contained documentation or articles from reputable sources that explain how linux works so that I can actually understand and learn the tools I&#39;m using instead of copying random commands from forums.</p> <p>There is also the option of asking LLMs, but this usually results in similar issues, and I think relying on them in general is just a bad idea.</p> <p>Just as an example today I wanted to reformat a hard drive from ntfs to ext4, something that I bet is extremely simple, but like I said I am basically still a complete noob. When I search up my question in google, I get: </p> <p>- AI overview which just gives me a few commands that probably work but I don&#39;t really feel like copying commands that I don&#39;t know what they do from a LLM which could be hallucinating or have outdated information.</p> <p>- Reddit threads and forum posts mostly about converting without reformating so that the files are saved (not really what I asked, I never specified I want to keep my files because I don&#39;t), also many of the posts are several years old</p> <p>- Some articles from random apps that will do it for me, which I doubt I need considering how a small a task this is.</p> <p>I&#39;m sure that I could figure it out by looking through the forum posts but I would rather google show me some website/documentation that explains how to use mkfs or whatever the best way is. Maybe there is some standard way that everyone does it but again I can&#39;t figure out what it is from google.</p> <p>Everyone always says to RTFM, but how can I if I don&#39;t know which manual to read, or if I can&#39;t even find it?</p> <p>Am I going about this wrong or is Google just this ineffective at finding information and I never noticed until now?</p> <p>TLDR Frustrated that google always shows unrelated or outdated forum posts instead of actually explaining how to use linux.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Unlikely-Giraffe9369\"> /u/Unlikely-Giraffe9369 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1irul62/on_finding_linux_help_with_google_search/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irul62/on_finding_linux_help_with_google_search/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why is it important for Syn to produce a concrete tree?","url":"https://www.reddit.com/r/rust/comments/1irug1h/why_is_it_important_for_syn_to_produce_a_concrete/","date":1739825395,"author":"/u/EthanAlexE","guid":4121,"unread":true,"content":"<p>First of all, I dabble in Rust, but I am not very knowledgable in it. I've once written a proc macro from a tutorial, but I don't have a great intuitive understanding of them.</p><p>I was listening to an episode of Self Directed Research (<a href=\"https://www.youtube.com/watch?v=8CrmJV2NT9I\">Compile Time Crimes</a>) and Amos breifly mentions (~7:57) that the reason why proc macros dont just use the rustc parser, or rustc doesn't just use syn is that rustc is better at error recovery and syn produces a tree with everything lexically significant still there (newlines, leading/traailing characters) rather than just what they mean symantically.</p><p>The first one makes sense to me, rustc needs to deal with a lot more possibly incorrect code than syn does. But why does syn need to preserve info that isn't necessarily symantically important?</p><p>I guess this question also extends into parsers for tooling in general. I know Treesitter also produces a concrete parse tree and maybe parsers written specifically for language servers as well?</p><p>Edit: my bad if syn doesn't actually produce a CST, I'm mostly just working off of what Amos said in the video</p>","contentLength":1080,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why I‚Äôm Writing a Scheme Implementation in 2025 (The Answer is Async Rust)","url":"https://maplant.com/2025-02-17-Why-I","date":1739824408,"author":"/u/maplant","guid":3976,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iru1eo/why_im_writing_a_scheme_implementation_in_2025/"},{"title":"[ Removed by Reddit ]","url":"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/","date":1739823237,"author":"/u/Vw-Bee5498","guid":4025,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>[ Removed by Reddit on account of violating the <a href=\"/help/contentpolicy\">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vw-Bee5498\"> /u/Vw-Bee5498 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python Apps","url":"https://www.reddit.com/r/linux/comments/1irt3bw/python_apps/","date":1739822157,"author":"/u/BOBOLIU","guid":4003,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have been using Fedora Linux for around ten years and noticed during regular updates that an increasing number of applications are written in Python. Is there a trend of writing applications in Python? If that is the case, should I expect Linux to get slower over time? </p> <p>Based on my personal experience, Fedora Linux is much slower now than ten years ago, at least in terms of boot time. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BOBOLIU\"> /u/BOBOLIU </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1irt3bw/python_apps/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irt3bw/python_apps/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Which AI (ChatGPT, Claude, etc.) stands out in document analysis compared to others - what is you experience?","url":"https://www.reddit.com/r/artificial/comments/1irsane/which_ai_chatgpt_claude_etc_stands_out_in/","date":1739820250,"author":"/u/LSDwarf","guid":4268,"unread":true,"content":"<p>honestly I've been using AI for simple tasks so far and free versions of ChatGPT and Perplexity were more than enough for that.</p><p>However, now I need to analyse quite a number of scientific documents (in PDF) and I need the  AI model to be able to extract specific data from these documents, summarize, answer my specific questions, etc. Paid version is not a problem for this task. Which model (and possibly a price tier) will you recommend?</p>","contentLength":439,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Visual explanation of \"Backpropagation: Multivariate Chain Rule\"","url":"https://www.reddit.com/r/MachineLearning/comments/1irs3gn/d_visual_explanation_of_backpropagation/","date":1739819775,"author":"/u/madiyar","guid":4024,"unread":true,"content":"<p>One part that confuses me about backpropagation is why people associate backpropagation to the chain rule ? The chain rule doesn't clearly explain when there are multiple paths from a parameter to the loss. Eventually I realized that I was missing the term \"multivariate chain rule,\" and once I found it, everything clicked in my head. Let me know if you have thoughts here. </p>","contentLength":375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Forget the Data and Fine-tuning! Just Fold the Network to Compress [Feb, 2025]","url":"https://www.reddit.com/r/MachineLearning/comments/1irqngl/r_forget_the_data_and_finetuning_just_fold_the/","date":1739816399,"author":"/u/Megneous","guid":2038,"unread":true,"content":"<p> We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments. Our code is online.</p><p>Summary (AI used to summarize):</p><h3>Summary of Novel Contributions in \"Just Fold the Network to Compress\"</h3><p>: Traditional model compression techniques (e.g., pruning, quantization) require fine-tuning or access to training data to maintain performance, limiting their use in data-constrained scenarios.: - : Introduces , a method that compresses models without fine-tuning or training data by merging structurally similar neurons. - : Addresses  (reduced activation variance degrading performance) and  (excessive variance) through novel data-free techniques. </p><p>: Prior work in neuron alignment (e.g., weight matching) and data-driven variance repair (e.g., REPAIR) relies on data or fine-tuning.: - <strong>Data-Free Neuron Alignment</strong>: Extends weight matching to intra-model neuron clustering via , avoiding dependency on input data. - : Frames model folding as a <strong>k-means optimization problem</strong>, proving it minimizes Frobenius norm approximation error during compression. </p><p>: - : Merges neurons by applying k-means to weight matrices across consecutive layers, reducing redundancy while preserving inter-layer dependencies. - <strong>Fold-AR (Approximate REPAIR)</strong>: Estimates intra-cluster correlations to rescale activations, preventing variance collapse without data. - <strong>Fold-DIR (Deep Inversion REPAIR)</strong>: Uses synthetic data generated via  (optimizing noise to match BatchNorm statistics) to recalibrate activation variances. - <strong>Handling Complex Architectures</strong>: Extends folding to residual connections and BatchNorm layers by clustering combined weight-normalization matrices. </p><p>: - <strong>High Sparsity Performance</strong>: Outperforms data-free methods (e.g., IFM, INN) by  at 70% sparsity on ResNet18/CIFAR10. - : Achieves  to data-driven methods on LLaMA-7B without fine-tuning or data. - : Fold-AR and Fold-DIR maintain variance ratios close to 1, avoiding collapse/overshooting (Fig. 4). </p><h4><strong>5. Limitations and Future Work</strong></h4><p>: - Effectiveness depends on model redundancy (less effective for compact models).<p> - Uniform sparsity per layer (future work may optimize layer-wise sparsity). </p></p><h3>Potential Benefits for SOTA Models</h3><ol><li>: Enables compression of large models (e.g., LLMs) for smartphones/IoT devices without data access or retraining.</li><li><strong>Privacy-Sensitive Domains</strong>: Critical for healthcare/finance where data cannot be used for calibration.</li><li>: Reduces LLM size by 20‚Äì50% with minimal performance loss, lowering inference costs.</li><li>: Fold-AR/Fold-DIR mitigate performance drops caused by out-of-distribution calibration data in data-driven methods.</li></ol><p>: A folded LLM could run on edge devices like NVIDIA Jetson Nano with , maintaining usability for tasks like text generation while reducing memory and energy consumption.</p>","contentLength":3543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Today I learned something new about Go's slices","url":"https://www.reddit.com/r/golang/comments/1irpsb0/today_i_learned_something_new_about_gos_slices/","date":1739814408,"author":"/u/andres2142","guid":1995,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Go really cares about performance, cares about not wasting any resources.</p> <p>Given this example:</p> <pre><code>var s []int s = append(s, 0) //[0] len(1) cap(1) s = append(s, 1) //[0 1] len(2) cap(2) s = append(s, 2, 3, 4) //[0 1 2 3 4] len(5) cap(6) </code></pre> <p>The capacity after adding multiple values to <code>s</code> is 6, not 8. This blew my mind because I thought it should&#39;ve doubled the capacity from 4 to 8, but instead, Go knows that 8 should have been a waste and instead sets it as 6, as long as you append multiple values to a slice.</p> <p>This is different if I would&#39;ve done individually like this:</p> <pre><code>var s []int s = append(s, 0) //[0] len(1) cap(1) s = append(s, 1) //[0 1] len(2) cap(2) s = append(s, 2) //[0 1 2] len(3) cap(4) s = append(s, 3) //[0 1 2 3] len(4) cap(4) s = append(s, 4) //[0 1 2 3 4] len(5) cap(8) </code></pre> <p><code>s</code> ends up with a capacity of 8 because it doubled it, like usual</p> <p>I was not aware of this amazing feature.</p> <p>Go is really an amazing language. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/andres2142\"> /u/andres2142 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1irpsb0/today_i_learned_something_new_about_gos_slices/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irpsb0/today_i_learned_something_new_about_gos_slices/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bootstrapping Argo for Entra ID OIDC","url":"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/","date":1739813751,"author":"/u/UberBoob","guid":1947,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks! I&#39;m trying to spin up an Argo-managed Cluster to use Azure AD credentials as the sole SSO provider.</p> <p>I have the secrets mounted on the Argo Server pods, provided from AWS Secrets Manager by AWS Secrets Store CSI driver and provider. client_id and client_secret are located at /mnt/secrets-store. My terrafrom modules are running a helm release install of Argo CD 7.7.7.</p> <p>Im trying to use env variables passed as helm values.yaml. Argo CD runs fine, I can login via initial Admin creds. The Entra ID button is in place for login, however response from Microsoft is that I must provide a client id in the request.</p> <p>Anyone else take this approach and have it working? We, can pass the values via Terraform, however the secret ends up in plan files and is not masked even when using the sensitive() in Terraform. This fails our scan audits and want to keep the secrets in AWS secrets manager as a permanent solution.</p> <p>The Argo Docs don&#39;t go into much detail around OIDC, other than setting the OIDC details in the ConfiMap.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/UberBoob\"> /u/UberBoob </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Canonical Extends Kubernetes Distro Support to a Dozen Years","url":"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/","date":1739813703,"author":"/u/CrankyBear","guid":1997,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/\"> <img src=\"https://external-preview.redd.it/CrTnhrf0m844iLfAuNSRmJ3R8qO-sAQM4pZX5wPcg5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74e86d0c279e67e1c32bd05a5d434e80aba9d3f0\" alt=\"Canonical Extends Kubernetes Distro Support to a Dozen Years\" title=\"Canonical Extends Kubernetes Distro Support to a Dozen Years\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CrankyBear\"> /u/CrankyBear </a> <br/> <span><a href=\"https://thenewstack.io/canonical-extends-kubernetes-distro-support-to-a-dozen-years/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anti-technology protestors disrupted a keynote talk at, unbelievably, a Pause AI event in Paris... because Pause AI is too pro-AI","url":"https://v.redd.it/r9d4v0rghqje1","date":1739813153,"author":"/u/MetaKnowing","guid":2056,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1irp9cm/antitechnology_protestors_disrupted_a_keynote/"},{"title":"Transparent Benchmarking with Apache Iggy","url":"https://blog.iggy.rs/posts/transparent-benchmarks/","date":1739812203,"author":"/u/spetz0","guid":2155,"unread":true,"content":"<h2>Benchmarks should be the first-class citizen</h2><p>In the world of software development, <strong>benchmarks are often treated as a second-class citizen</strong>. They're more of an addition to the codebase, rather than a crucial part of it, which should be the other way around, especially when it comes to the performance-critical systems or infrastructure tools.</p><p>Sometimes, the benchmarking results are nothing more than just a <strong>cherry-picking of the best-case scenarios</strong>, which are not representative of real-world usage. In such a case, they simply serve a sole purpose of either making the project look better than it is or how well it does outperform the competition, under the extremely optimized conditions when comparing with its counterparts.</p><p><strong>Trying to reproduce the benchmarks is often a nightmare</strong>, as the environment setup is not documented, the code is unavailable, or the instructions are not clear enough. This makes it close to impossible to verify the results, which are then taken for granted.</p><p>Or even worse, the <strong>benchmarking tool might be so complex, that it's hard to understand how it works</strong>, and what are the assumptions behind it. ALl of these, does result in hard to extend or modify the existing benchmarks, which are not covering the particular use case you're interested in. It's just here to tell everyone that we do have benchmarks, but how we do it, and what they measure, is a mystery.</p><p><strong>Which is why at <a href=\"https://github.com/iggy-rs/iggy/\">Iggy</a>, we've decided to make the benchmarks a first-class citizen</strong>.</p><p>Our  tool, which is used to run the benchmarks and is part of the core open source repository (can be found under the  directory), has come a long way and has been serving us well.</p><p>We use it to do quick performance checks, regression testing, and to see how the changes we introduce affect the performance. <strong>We run it on our localhost, as well as on the Virtual Machines in the cloud, to see how it behaves under a variety of environments.</strong></p><h2>Iggy benchmarking dashboard</h2><p> - a benchmarking dashboard, which is available to everyone. It's a website where you can see how Iggy performs under the different conditions, and how it scales with the number of clients, messages, and topics.</p><p>This is our community-driven effort, where everyone can contribute, and add their own benchmarks. For all the information on how to run the benchmarks, render them on the dashboard, upload your results or contribute to the project, please check the <a href=\"https://github.com/iggy-rs/iggy-bench-dashboard\">iggy-bench-dashboard</a> repository. In general, it's as simple as:</p><ul><li>Building the <a href=\"https://github.com/iggy-rs/iggy\">Iggy</a> in the release mode with </li><li>Starting your Iggy server with <code>cargo r --bin iggy-server -r</code> (feel free to adjust the configuration in  or via environment variables)</li><li>Running the  tool with the desired parameters, e.g. <code>cargo r --bin iggy-bench -r pinned-producer tcp</code></li><li>Extending your benchmark with the output (HTML charts, JSON sampling etc.) <code>cargo r --bin iggy-bench -r pinned-producer tcp output -o performance_results --identifier spetz</code></li><li>Navigating to the specific benchmark directory to browse the charts and/or uploading them to the dashboard.</li><li>And there's always  command e.g.  to make your life easier :)</li></ul><p><strong>And this is just the beginning</strong>, as we plan to extend the dashboard, and add more benchmarks, which are covering the different use cases.</p><p>Our main goal is to make the benchmarking process (and its results) <strong>transparent, reproducible, and easy to understand</strong>. We want to make them a first-class citizen, and a crucial part of the Iggy project. We want to make them a tool, which will help us to improve the performance, and to make Iggy the best streaming server out there. We're looking forward to your feedback, and we hope you'll enjoy the benchmarks.</p><h2>Towards the microsecond latency</h2><p><strong>And as a cherry on top, we've recently managed to achieve the sub-millisecond write latency</strong>. This is a huge milestone for us, as it's a proof that Iggy can be used in low-latency applications, where speed is crucial. Lately, we've been experimenting a lot with <a href=\"https://github.com/rkyv/rkyv\">rkyv</a> - zero-copy deserialization framework, which has yielded some great results. Keep in mind that streaming the data within the range of microseconds latency depends on the several factors, such as message size, network conditions, or the hardware you're running on.</p><p>And the best part is that we're just getting started. We're looking forward to pushing the limits even further, and to see how far we can go. There's still tons of optimizations coming, including switching the runtime to the <a href=\"https://github.com/bytedance/monoio\">monoio</a> which does support , and we've experienced superb results with this one on our experimental branch. Then, there's the whole concept of shared-nothing &amp; thread-per-core design, and many more. Stay tuned!</p>","contentLength":4610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1irouxk/transparent_benchmarking_with_apache_iggy/"},{"title":"This is how I use LLMs: as colleagues. Not to code, but to help ME code.","url":"https://www.reddit.com/r/artificial/comments/1irotl1/this_is_how_i_use_llms_as_colleagues_not_to_code/","date":1739812116,"author":"/u/roz303","guid":1996,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MacOS Targeted by New XCSSET Malware Variant That Infects Xcode Projects","url":"https://www.reddit.com/r/programming/comments/1irot80/macos_targeted_by_new_xcsset_malware_variant_that/","date":1739812095,"author":"/u/Dark-Marc","guid":1979,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dark-Marc\"> /u/Dark-Marc </a> <br/> <span><a href=\"https://www.reddit.com/r/pwnhub/comments/1iropmo/microsoft_warns_of_new_xcsset_macos_malware/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irot80/macos_targeted_by_new_xcsset_malware_variant_that/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Yoneda","url":"https://www.reddit.com/r/programming/comments/1irokr7/understanding_yoneda/","date":1739811545,"author":"/u/andarmanik","guid":2037,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/andarmanik\"> /u/andarmanik </a> <br/> <span><a href=\"https://bartoszmilewski.com/2013/05/15/understanding-yoneda/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irokr7/understanding_yoneda/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kartoffels, a game where you implement firmware for a potato, v0.7 released! ü•î","url":"https://www.reddit.com/r/rust/comments/1irodha/kartoffels_a_game_where_you_implement_firmware/","date":1739811046,"author":"/u/Patryk27","guid":1945,"unread":true,"content":"<p><a href=\"https://kartoffels.pwy.io/\">kartoffels</a> is a game where you're given a potato and your job is to implement a <a href=\"https://github.com/Patryk27/kartoffel/\">firmware</a> for it:</p><p>Today I've released v0.7 which brings cellular automata-based worldgen (caves, caves, caves!), statistics and a migration to 32-bit RISC-V:</p>","contentLength":235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] How's the job market?","url":"https://www.reddit.com/r/MachineLearning/comments/1irnuv1/d_hows_the_job_market/","date":1739809777,"author":"/u/Ready_Plastic1737","guid":2039,"unread":true,"content":"<p>Yesterday, I began applying for new jobs. Currently, my title is \"ML Engineer,\" but to be honest, I've been functioning more like an ML consultant lately‚ÄîI haven't coded in months.</p><p>I've almost reached 2 years of experience since completing my Master's in Computer Engineering with a focus on ML. It seems many roles are seeking candidates with 3+ years of experience.</p><p>I'm just curious about how many applications it will take before I get my first interview‚ÄîI'm currently at 24 applications.</p>","contentLength":493,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Muscle up your Go templates experience","url":"https://www.reddit.com/r/golang/comments/1irnum2/muscle_up_your_go_templates_experience/","date":1739809759,"author":"/u/begoon","guid":2035,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://github.com/Masterminds/sprig\">https://github.com/Masterminds/sprig</a></p> <p>This brings go templates experience to another level just with one import.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/begoon\"> /u/begoon </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1irnum2/muscle_up_your_go_templates_experience/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irnum2/muscle_up_your_go_templates_experience/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I really hope AIs aren't conscious. If they are, we're totally slave owners and that is bad in so many ways","url":"https://www.reddit.com/r/artificial/comments/1irnmqk/i_really_hope_ais_arent_conscious_if_they_are/","date":1739809221,"author":"/u/katxwoods","guid":1941,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mobile Phone?","url":"https://www.reddit.com/r/linux/comments/1irmh3k/mobile_phone/","date":1739806293,"author":"/u/moosetunes","guid":1942,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I recently searched online for Linux mobile phones. I was somewhat surprised to see how little support and selection exists globally. Assuming I don&#39;t want a phone with either Apple or Google intellectual property, what am I buying?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/moosetunes\"> /u/moosetunes </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1irmh3k/mobile_phone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irmh3k/mobile_phone/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.24 is here üôå","url":"https://www.reddit.com/r/golang/comments/1irm5qm/go_124_is_here/","date":1739805479,"author":"/u/danielwetan","guid":1891,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>This release brings performance boosts, better tooling, and improved WebAssembly support.</p> <p>Highlights: - Generics: Full support for generic type aliases. - Faster Go: New runtime optimizations cut CPU overhead by ~2‚Äì3%. - Tooling: Easier tool dependency tracking (go get -tool), smarter go vet for tests. - WebAssembly: Export Go functions to the WASM host. - Standard library: FIPS 140-3 compliance, better benchmarking, new os.Root for isolated filesystem access.</p> <p>Full details: <a href=\"https://go.dev/blog/go1.24\">https://go.dev/blog/go1.24</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/danielwetan\"> /u/danielwetan </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1irm5qm/go_124_is_here/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irm5qm/go_124_is_here/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nvidia compute is doubling every 10 months","url":"https://www.reddit.com/r/artificial/comments/1irm1wx/nvidia_compute_is_doubling_every_10_months/","date":1739805194,"author":"/u/MetaKnowing","guid":1892,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ExpressVPN Rewrites Lightway VPN Protocol in Rust for Security","url":"https://www.reddit.com/r/rust/comments/1irlr8s/expressvpn_rewrites_lightway_vpn_protocol_in_rust/","date":1739804450,"author":"/u/flacao9","guid":1946,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/flacao9\"> /u/flacao9 </a> <br/> <span><a href=\"https://cyberinsider.com/expressvpn-rewrites-lightway-vpn-protocol-in-rust-for-security/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1irlr8s/expressvpn_rewrites_lightway_vpn_protocol_in_rust/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"**[Discussion] ByteGPT-small: My First Byte-Tokenized LLM for Mobile Devices** üöÄ","url":"https://www.reddit.com/r/MachineLearning/comments/1irloen/discussion_bytegptsmall_my_first_bytetokenized/","date":1739804244,"author":"/u/kells1986","guid":1944,"unread":true,"content":"<p>I‚Äôve been working on a series of  designed for <strong>compute- and memory-constrained devices</strong> like mobile phones and embedded systems. üöÄ </p><p>This is my . It's a  trained with <strong>byte tokenization (inspired by ByT5)</strong> to maximize efficiency for on-device inference. </p><ul><li> Tiny embeddings reduce model size and memory use.</li><li> Byte-level tokenization is simple‚Äîno SentencePiece or BPE required.</li><li> Better handling of typos and unseen tokens.</li></ul><ul><li> Now live! I'll be adding ONNX, CoreML and TFLite files soon</li><li> Making it chat-ready.</li><li> Training ByteGPT-medium (~150M params).</li><li> Shrinking models while retaining quality. Focusing on domain-specific small LLMs that run on the edge.</li></ul><p>I‚Äôd love your feedback, especially if you: - Have experience deploying <strong>LLMs on mobile or embedded devices</strong>. - Have tried  or other distillation methods. - Think byte tokenization has more potential than people assume. </p><ul><li>Have you experimented with ?</li><li>What‚Äôs your experience with  vs. subword models?</li><li>Any advice on GPRO distillation techniques?</li></ul><p>Looking forward to your thoughts! üòä </p>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ImageFan Reloaded - Light-weight, tab-based image viewer, supporting multi-core processing","url":"https://www.reddit.com/r/linux/comments/1irkjq1/imagefan_reloaded_lightweight_tabbased_image/","date":1739801056,"author":"/u/MihneaRadulescu","guid":1943,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MihneaRadulescu\"> /u/MihneaRadulescu </a> <br/> <span><a href=\"https://github.com/mihnea-radulescu/imagefanreloaded\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irkjq1/imagefan_reloaded_lightweight_tabbased_image/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self hosted kubernetes, how to make control plane easier....","url":"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/","date":1739800870,"author":"/u/TheBeardMD","guid":1865,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Very familiar with AWS EKS, where you don&#39;t really have to worry about the control plane at all. Thinking about starting a cluster from scratch, but find the control plane very complex. Are there any options to make managing the control plane easier so it&#39;s possible to create a cluster from scratch?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheBeardMD\"> /u/TheBeardMD </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The state of Kubernetes job market in 2024","url":"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/","date":1739800433,"author":"/u/danielepolencic","guid":1864,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/\"> <img src=\"https://external-preview.redd.it/BycxNH7ovolfHwotvNh-bJXbjRfA8Et5_h5nErx1JMA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca3d7a63e70d5d842882f98b3db33b15e3165655\" alt=\"The state of Kubernetes job market in 2024\" title=\"The state of Kubernetes job market in 2024\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/danielepolencic\"> /u/danielepolencic </a> <br/> <span><a href=\"https://kube.careers/state-of-kubernetes-jobs-2024-q4\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft to Deprecate Location History Feature in Windows","url":"https://www.reddit.com/r/programming/comments/1irjs19/microsoft_to_deprecate_location_history_feature/","date":1739798767,"author":"/u/miso25","guid":1893,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/miso25\"> /u/miso25 </a> <br/> <span><a href=\"https://cyberinsider.com/microsoft-to-deprecate-location-history-feature-in-windows/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irjs19/microsoft_to_deprecate_location_history_feature/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kubegui.io - user friendly kubernetes desktop application","url":"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/","date":1739795999,"author":"/u/Live_Landscape_7570","guid":1809,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\"> <img src=\"https://a.thumbs.redditmedia.com/3eh_HCnUiUTn-Kn5n_tSv4JYVCz06lQL6-ycwAITBU8.jpg\" alt=\"kubegui.io - user friendly kubernetes desktop application\" title=\"kubegui.io - user friendly kubernetes desktop application\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Who we are? SRE engineers! What do we want? </p> <p><a href=\"http://kubegui.io\">One more GUI-based Kubernetes management tool</a> for daily operations. </p> <p>I&#39;ve just created a golang/wails based client for any available Kubernetes cluster that&#39;s better then alternatives (based on exceptional research made within my family members) and much much cheaper. </p> <p><a href=\"https://preview.redd.it/veh4twld2pje1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=33b972e13d958d3ff208aef70a2c6497fc4f9aea\">kubegui.io</a></p> <p>Some advantages: </p> <p>‚ö° Lightning Fast Performance: Built with Go official kubernetes client for maximum speed/cache usage + minimal resource usage </p> <p>üíª Zero Dependencies: No kubectl required (or any other tools) </p> <p>üîÑ Seamless Multi-cluster Management: Switch between clusters with last viewed resource state saved </p> <p>üí° AI provided suggestions: Realtime AI integrations for fix suggestions (for deployments/pods/events issues) </p> <p>üìä Advanced Monitoring: Real-time metrics out of the box (for pods/nodes for the last hour) </p> <p>üîí Enhanced Security: No external calls (except for AI fix suggestions if enabled) </p> <p>üì¶ Single Binary Distribution: No runtime dependencies required </p> <p>üìÑ Smart yaml viewer: Context-aware editor with indentation linter and error detection </p> <p>üìù Interactive Shell access: One click pod exec (xterm with copy-paste available) </p> <p>üéÆ Pod ports forwarding: One click inside pod details exposed ports (via default browser session). </p> <p>üõ°Ô∏è Network Policies: Visualize policy feature inside resource details </p> <p>üîç Enhanced log viewer: Built-in logs syntax highlighter </p> <p>üîë Custom resource generator: Unique custom resource example creation based on crd schema </p> <p>‚ûï Auto updates: self-updating application (via github public project repo background calls)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Live_Landscape_7570\"> /u/Live_Landscape_7570 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Death of a thousand nits: the gentle art of code review","url":"https://www.reddit.com/r/programming/comments/1iri0l4/death_of_a_thousand_nits_the_gentle_art_of_code/","date":1739792670,"author":"/u/AlexandraLinnea","guid":1833,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AlexandraLinnea\"> /u/AlexandraLinnea </a> <br/> <span><a href=\"https://bitfieldconsulting.com/posts/code-review\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1iri0l4/death_of_a_thousand_nits_the_gentle_art_of_code/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"0+0 > 0: C++ thread-local storage performance","url":"https://www.reddit.com/r/programming/comments/1irhytj/00_0_c_threadlocal_storage_performance/","date":1739792476,"author":"/u/FoxInTheRedBox","guid":1862,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FoxInTheRedBox\"> /u/FoxInTheRedBox </a> <br/> <span><a href=\"https://yosefk.com/blog/cxx-thread-local-storage-performance.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irhytj/00_0_c_threadlocal_storage_performance/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"searchcode.com‚Äôs SQLite database is probably 6 terabytes bigger than yours","url":"https://www.reddit.com/r/golang/comments/1irhw56/searchcodecoms_sqlite_database_is_probably_6/","date":1739792175,"author":"/u/FoxInTheRedBox","guid":1805,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FoxInTheRedBox\"> /u/FoxInTheRedBox </a> <br/> <span><a href=\"https://boyter.org/posts/searchcode-bigger-sqlite-than-you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irhw56/searchcodecoms_sqlite_database_is_probably_6/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 unique ways I use Android 15's Private Space that aren't for porn or cheating","url":"https://www.reddit.com/r/linux/comments/1irhmg0/5_unique_ways_i_use_android_15s_private_space/","date":1739791075,"author":"/u/throwaway16830261","guid":1806,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/throwaway16830261\"> /u/throwaway16830261 </a> <br/> <span><a href=\"https://www.androidauthority.com/unique-uses-android-15-private-space-3526505/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irhmg0/5_unique_ways_i_use_android_15s_private_space/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask r/kubernetes: What are you working on this week?","url":"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/","date":1739790030,"author":"/u/gctaylor","guid":1775,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell <a href=\"/r/kubernetes\">/r/kubernetes</a> what you&#39;re up to this week!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing the Scientific Computing in Rust virtual workshop 2025","url":"https://www.reddit.com/r/rust/comments/1irglbs/announcing_the_scientific_computing_in_rust/","date":1739786754,"author":"/u/mscroggs","guid":1808,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mscroggs\"> /u/mscroggs </a> <br/> <span><a href=\"https://scientificcomputing.rs/2025/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1irglbs/announcing_the_scientific_computing_in_rust/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Doesn‚Äôt Have to Be Complicated ‚Äì A Practical Guide to Simplifying Your Workflow","url":"https://www.reddit.com/r/kubernetes/comments/1irghmz/kubernetes_doesnt_have_to_be_complicated_a/","date":1739786358,"author":"/u/Sheishofert","guid":1743,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sheishofert\"> /u/Sheishofert </a> <br/> <span><a href=\"https://rust-on-nails.com/blog/kubernetes-complicated/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irghmz/kubernetes_doesnt_have_to_be_complicated_a/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harvard team built a CMOS chip to map 70,000 synaptic connections between 2,000 rat neurons","url":"https://www.reddit.com/r/programming/comments/1irg96i/harvard_team_built_a_cmos_chip_to_map_70000/","date":1739785298,"author":"/u/Sheishofert","guid":1748,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sheishofert\"> /u/Sheishofert </a> <br/> <span><a href=\"https://www.tomshardware.com/tech-industry/harvard-team-built-a-cmos-chip-to-map-70-000-synaptic-connections-between-2-000-rat-neurons\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irg96i/harvard_team_built_a_cmos_chip_to_map_70000/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Region-Adaptive Sampling: Accelerating Diffusion Transformers by Selectively Updating High-Focus Areas","url":"https://www.reddit.com/r/MachineLearning/comments/1irfq36/r_regionadaptive_sampling_accelerating_diffusion/","date":1739782994,"author":"/u/Successful-Western27","guid":1863,"unread":true,"content":"<p>The key contribution here is a new adaptive sampling approach for diffusion transformers that reduces computation by selectively allocating attention based on region importance. Instead of processing all regions equally, it identifies which parts need more detailed processing.</p><p>Main technical aspects: - Introduces region importance scoring via lightweight network - Dynamic token selection based on predicted importance scores - Modified attention mechanism compatible with existing architectures - Adaptive caching strategy for memory efficiency</p><p>Results show: - 30-50% reduction in computation time - No degradation in FID or CLIP scores - 40% memory savings through adaptive sampling - Effective across multiple model architectures - Works for both conditional and unconditional generation</p><p>I think this could be particularly impactful for real-world applications where compute efficiency matters. The ability to maintain quality while reducing resource usage by up to 50% opens up possibilities for running these models on more modest hardware. The principles here might also transfer well to other domains where selective attention allocation could help, like video generation or 3D rendering.</p><p>What interests me most is how this challenges the assumption that uniform processing is necessary for high-quality generation. By showing we can be selective about computation allocation, it suggests there's still significant room for efficiency improvements in current architectures.</p><p>TLDR: New method reduces diffusion transformer computation by 30-50% through selective attention to important image regions, without quality loss.</p>","contentLength":1624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Kotlin‚Äôs Result Type Falls Short for Handling Failures","url":"https://www.reddit.com/r/programming/comments/1irfo1u/why_kotlins_result_type_falls_short_for_handling/","date":1739782779,"author":"/u/ablx0000","guid":1894,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ablx0000\"> /u/ablx0000 </a> <br/> <span><a href=\"https://verbosemode.dev/p/why-kotlins-result-type-falls-short\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irfo1u/why_kotlins_result_type_falls_short_for_handling/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses","url":"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/","date":1739781956,"author":"/u/Wownever","guid":1679,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/\"> <img src=\"https://external-preview.redd.it/htjVgpTbC6LEHHA_g3zSEEkmVxg_6sYEyx94ado_4FU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8858d3c38de489836abc3910c8502b1710b9834\" alt=\"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses\" title=\"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Wownever\"> /u/Wownever </a> <br/> <span><a href=\"https://www.wiz.io/blog/kubernetes-report-preview-2025\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] How does OpenAI Canvas works with inplace human edit works with KV Caching?","url":"https://www.reddit.com/r/MachineLearning/comments/1irfgsc/d_how_does_openai_canvas_works_with_inplace_human/","date":1739781888,"author":"/u/PunsbyMann","guid":1773,"unread":true,"content":"<p>I was wondering, how does OpenAI uses KV Caching if it allows inplace human edits? Does it have to invalidate the whole cache up to the more earliest file edit and then have to perform forward pass for the rest of the canvas text?</p><p>Does it works like the described image or there are better ways to save cache for text that is between edits but unchanged (I don't think so, as the hidden context would change as for all the future token generations)?</p><pre><code>Line 1: def process_data(): ‚Üí KV‚ÇÅ Line 2: x = 5 ‚Üí KV‚ÇÇ (aware of KV‚ÇÅ) Line 3: y = x + 10 ‚Üí KV‚ÇÉ (aware of KV‚ÇÅ, KV‚ÇÇ) Line 4: return y ‚Üí KV‚ÇÑ (aware of KV‚ÇÅ, KV‚ÇÇ, KV‚ÇÉ) now we edit Line 2: </code></pre><pre><code>Line 1: def process_data(): ‚Üí KV‚ÇÅ (still valid) Line 2: x = 10 ‚Üí KV‚ÇÇ' (new) Line 3: y = x + 10 ‚Üí KV‚ÇÉ (INVALID! Based on old x value) Line 4: return y ‚Üí KV‚ÇÑ (INVALID! Based on old chain) </code></pre><p>is there a smarter way for getting away with making less number of forward passes?</p><p>EDIT: I do recognize now, how bad the title is phrased.</p>","contentLength":997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Go a good language for beginners?","url":"https://www.reddit.com/r/golang/comments/1irfdsh/is_go_a_good_language_for_beginners/","date":1739781521,"author":"/u/Locyain","guid":1741,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I know a bit of Lua, HTML, and CSS, and I&#39;ve also considered learning JavaScript, but it seems like a lot of content, so I thought I might learn it later. What I like about Go is its simplicity, and what I enjoy the most is the ability to create TUI applications. What do you think about that?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Locyain\"> /u/Locyain </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1irfdsh/is_go_a_good_language_for_beginners/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1irfdsh/is_go_a_good_language_for_beginners/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you scale to zero and from zero?","url":"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/","date":1739779527,"author":"/u/Electronic_Role_5981","guid":1680,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://github.com/kubernetes/enhancements/issues/2021\">https://github.com/kubernetes/enhancements/issues/2021</a> is open. `HPAScaleToZero` is alpha in v1.16 and has no much updates. </p> <p>There are several known choices like</p> <ul> <li>Scale from zero(<strong>No native support</strong>), or known as activator. (Also some Faas platforms, like OpenFaas or Serverless apps supports)</li> </ul> <ol> <li>Knative: Activator. <a href=\"https://knative.dev/docs/serving/architecture/#diagram\">https://knative.dev/docs/serving/architecture/#diagram</a> <a href=\"https://github.com/knative/serving\">knative/serving</a></li> <li>KEDA <ol> <li>Example: OpenFunction: <a href=\"https://github.com/OpenFunction/OpenFunction/pull/483\">https://github.com/OpenFunction/OpenFunction/pull/483</a> &amp; <a href=\"https://github.com/OpenFunction/OpenFunction/blob/main/docs/proposals/20230726-integrate-keda-http-add-on.md\">https://github.com/OpenFunction/OpenFunction/blob/main/docs/proposals/20230726-integrate-keda-http-add-on.md</a></li> </ol></li> <li>A initial implementation using service, like kube-proxy: <a href=\"https://github.com/wzshiming/kube-activator\">https://github.com/wzshiming/kube-activator</a> <ul> <li>Scale to zero (natively alpha feature)</li> </ul></li> <li>HPA: HPAScaleToZero feature gate</li> <li><a href=\"https://knative.dev/docs/serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period\">https://knative.dev/docs/serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period</a></li> <li><a href=\"https://github.com/deislabs/osiris\">https://github.com/deislabs/osiris</a> archived as the hpa supports it</li> </ol> <p>The last discussion in reddit is <a href=\"https://www.reddit.com/r/kubernetes/comments/1de8qiz/scaling%5C_to%5C_zero/\">https://www.reddit.com/r/kubernetes/comments/1de8qiz/scaling\\_to\\_zero/</a>. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Set Up a Persistent Volume for MinIO on GKE Free Tier? Do I Get Any Free Storage?","url":"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/","date":1739775790,"author":"/u/blvck_viking","guid":1633,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m setting up a self-hosted MinIO instance on Google Kubernetes Engine (GKE) and need to configure a persistent volume for storage. I&#39;m currently using the GKE free tier and was wondering:</p> <ol> <li>Does GKE free tier include any free persistent storage, or will I need to pay for it?</li> <li>What&#39;s the best way to set up a Persistent Volume (PV) and Persistent Volume Claim (PVC) for MinIO in a GKE cluster?</li> <li>Any recommendations on storage classes and best practices?</li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/blvck_viking\"> /u/blvck_viking </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Browser-on-ram: Sync browser related directories to RAM","url":"https://www.reddit.com/r/linux/comments/1irdxye/browseronram_sync_browser_related_directories_to/","date":1739775302,"author":"/u/64bitman","guid":1676,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/64bitman\"> /u/64bitman </a> <br/> <span><a href=\"https://github.com/64-bitman/browser-on-ram\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1irdxye/browseronram_sync_browser_related_directories_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rust-analyzer changelog #273","url":"https://www.reddit.com/r/rust/comments/1irdwlh/rustanalyzer_changelog_273/","date":1739775149,"author":"/u/WellMakeItSomehow","guid":1750,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WellMakeItSomehow\"> /u/WellMakeItSomehow </a> <br/> <span><a href=\"https://rust-analyzer.github.io/thisweek/2025/02/17/changelog-273.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1irdwlh/rustanalyzer_changelog_273/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MemSed: MEMory Search and EDit for Linux, inspired by Cheat Engine","url":"https://www.reddit.com/r/linux/comments/1ircwje/memsed_memory_search_and_edit_for_linux_inspired/","date":1739771285,"author":"/u/WillyJL","guid":1617,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I wanted to do the usual Cheat Engine workflow to edit values in games, but found no good solution for Linux. There&#39;s Game Conqueror but that crashed a lot for me and doesn&#39;t really work how I wanted, so I just made my own!</p> <p>It is still a work in progress, but works fairly well for day-to-day use at this point. Should work on most Linux distros and does not have any additional requirements, it&#39;s a single (nearly static) binary. Due to the nature of what it does (read/write process memory) it requires running as root, *insert usual word of warning about that here*.</p> <p>You can find the source code and downloads here:<br/> <a href=\"https://github.com/Willy-JL/MemSed\">https://github.com/Willy-JL/MemSed</a><br/> Please consider leaving a star :D</p> <p><a href=\"https://preview.redd.it/awr36qvf0nje1.png?width=893&amp;format=png&amp;auto=webp&amp;s=66ed4d99b40c8e87b814bf6a31fff70d812206ff\">https://preview.redd.it/awr36qvf0nje1.png?width=893&amp;format=png&amp;auto=webp&amp;s=66ed4d99b40c8e87b814bf6a31fff70d812206ff</a></p> <p>Will post a demo video in the comments below.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/WillyJL\"> /u/WillyJL </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ircwje/memsed_memory_search_and_edit_for_linux_inspired/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ircwje/memsed_memory_search_and_edit_for_linux_inspired/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-Minute Daily AI News 2/16/2025","url":"https://www.reddit.com/r/artificial/comments/1ircvd9/oneminute_daily_ai_news_2162025/","date":1739771165,"author":"/u/Excellent-Target-847","guid":1675,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Schemesh: Fusion between Unix shell and Lisp REPL","url":"https://www.reddit.com/r/programming/comments/1irctno/schemesh_fusion_between_unix_shell_and_lisp_repl/","date":1739770994,"author":"/u/namanyayg","guid":2058,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/namanyayg\"> /u/namanyayg </a> <br/> <span><a href=\"https://github.com/cosmos72/schemesh\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1irctno/schemesh_fusion_between_unix_shell_and_lisp_repl/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is GPL and BSD typically looked down upon in the Rust community?","url":"https://www.reddit.com/r/rust/comments/1iraze0/is_gpl_and_bsd_typically_looked_down_upon_in_the/","date":1739764518,"author":"/u/I_will_delete_myself","guid":1749,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Thinking of open sourcing a proprietary tool built in Rust. But I am kinda paranoid about AWS going along and then sell it as a service while my rear end gets broke. Is it looked down upon with to use a GPL license or BSD in the Rust community? Most of the repos appear to be MIT or apache 2.0.</p> <p>Edit:</p> <p>You may need a lawyer for this, but you may want to trademark if you are concerned about commercial abuse but still keep the code free and open to use. Tauri does this to keep their stuff open but prevent abuse without permission. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/I_will_delete_myself\"> /u/I_will_delete_myself </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1iraze0/is_gpl_and_bsd_typically_looked_down_upon_in_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1iraze0/is_gpl_and_bsd_typically_looked_down_upon_in_the/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"pywal 16 release 3.8.0 is out!","url":"https://www.reddit.com/r/linux/comments/1ir90fm/pywal_16_release_380_is_out/","date":1739758030,"author":"/u/-EDX-","guid":1772,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/-EDX-\"> /u/-EDX- </a> <br/> <span><a href=\"https://github.com/eylles/pywal16/releases/tag/3.8.0\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ir90fm/pywal_16_release_380_is_out/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux let my 2015 mbp actually work again!","url":"https://www.reddit.com/r/linux/comments/1ir83i9/linux_let_my_2015_mbp_actually_work_again/","date":1739755213,"author":"/u/jasonsc95","guid":859,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jasonsc95\"> /u/jasonsc95 </a> <br/> <span><a href=\"https://i.redd.it/5acazq2gplje1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ir83i9/linux_let_my_2015_mbp_actually_work_again/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingress Help","url":"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/","date":1739754260,"author":"/u/MeerkatMoe","guid":860,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to setup ingress using ingress nginx, but I can&#39;t figure out how to get routing to work...either my frontend breaks or my api is unreachable.</p> <p>I have an nginx service (not ingress nginx) that serves a frontend on port 80 and an express service that serves a backend API on port 5000.</p> <p>My first attempt was two separate ingresses (not sure about terminology):</p> <pre><code>--- metadata: name: api-ingress annotations: kubernetes.io/ingress.class: &quot;nginx&quot; spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} --- metadata: name: frontend-ingress namespace: {{ k3s_namespace }} annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: / pathType: Prefix backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre> <p>but that didn&#39;t work, and sometimes my API won&#39;t get routed correctly. I think it&#39;s because they get combined and I can&#39;t guarantee the order.</p> <p>My next try was to combine them:</p> <pre><code>kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} - path: &quot;/(?!api).*&quot; pathType: ImplementationSpecific backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre> <p>(left some stuff out to save space)</p> <p>but that also didn&#39;t work.</p> <p>What is the best way to get this working? To summarize, I just need</p> <p>&quot;/api/*&quot; -&gt; api service port 5000 (it can route as /api/&lt;whatever&gt; or just &lt;whatever&gt;)</p> <p>&quot;/*&quot; -&gt; nginx port 80</p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MeerkatMoe\"> /u/MeerkatMoe </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How arch-delta works and saves bandwidth for Arch Linux upgrades","url":"https://www.reddit.com/r/rust/comments/1ir7hhp/how_archdelta_works_and_saves_bandwidth_for_arch/","date":1739753393,"author":"/u/djugei","guid":1656,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/djugei\"> /u/djugei </a> <br/> <span><a href=\"https://djugei.github.io/how-arch-delta-works/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1ir7hhp/how_archdelta_works_and_saves_bandwidth_for_arch/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event driven workloads on K8s - how do you handle them?","url":"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/","date":1739752293,"author":"/u/sniktasy","guid":846,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks! </p> <p>I have been working with <a href=\"https://github.com/numaproj/numaflow\">Numaflow</a>, an open source project that helps build event driven applications on K8s. It basically makes it easier to process streaming data (think events on kafka, pulsar, sqs etc). </p> <p>Some cool stuff - autoscaling based on pending events/ back pressure handling (scale to 0 if need be), source and sink connectors, multi-language support, can support real time data processing use cases with the pipeline semantics etc</p> <p>Curious, how are you handling event-driven workloads today? Would love to hear what&#39;s working for others?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sniktasy\"> /u/sniktasy </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"There is no 1875 epoch","url":"https://www.reddit.com/r/programming/comments/1ir52zy/there_is_no_1875_epoch/","date":1739746489,"author":"/u/AlanBennet29","guid":1678,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AlanBennet29\"> /u/AlanBennet29 </a> <br/> <span><a href=\"https://iter.ca/post/1875-epoch/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ir52zy/there_is_no_1875_epoch/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Should People Just Use Goreleaser Instead of `actions-rust-release`?","url":"https://www.reddit.com/r/rust/comments/1ir4l9f/should_people_just_use_goreleaser_instead_of/","date":1739745155,"author":"/u/autarch","guid":845,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/autarch\"> /u/autarch </a> <br/> <span><a href=\"https://blog.urth.org/2025/02/16/should-people-just-use-goreleaser-instead-of-actions-rust-release/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1ir4l9f/should_people_just_use_goreleaser_instead_of/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why do people hate Ubuntu so much?","url":"https://www.reddit.com/r/linux/comments/1ir3aq8/why_do_people_hate_ubuntu_so_much/","date":1739741766,"author":"/u/Large-Start-9085","guid":795,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>When I switched to Linux 4 years ago, I used Pop OS as my first distro. Then switched to Fedora and used it for a long time until recently I switched again. </p> <p>This time I finally experienced Ubuntu. I know it&#39;s usually the first distro of most of the users, but I avoided it because I heard people badmouth it a lot for some reason and I blindly believed them. I was disgusted by Snaps and was a Flatpak Fanboy, until I finally tried them for the first time on Ubuntu. </p> <p>I was so brainwashed that I hated Ubuntu and Snaps for no reason. And I decided to switch to it only because I was given permission to work on a project using my personal laptop (because office laptop had some technical issues and I wasn&#39;t going to get one for a month) and I didn&#39;t wanted to take risk so I installed Ubuntu as the Stack we use is well supported on Ubuntu only.</p> <p>And damn I was so wrong about Ubuntu! Everything just worked out of the box. No driver issues, every packege I can imagine is available in the repos and all of them work seemlessly. I found Snaps to be better than Flatpaks because Apps like Android Studio and VS Code didn&#39;t work out of the box as Flatpaks (because of absurd sandboxing) but I faced no issues at all with Snaps. I also found that Ubuntu is much smoother and much more polished than any distro I have used till now. </p> <p>I really love the Ubuntu experience so far, and I don&#39;t understand the community&#39;s irrational hate towards it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Large-Start-9085\"> /u/Large-Start-9085 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ir3aq8/why_do_people_hate_ubuntu_so_much/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ir3aq8/why_do_people_hate_ubuntu_so_much/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D]How to handle highly imbalanced dataset?","url":"https://www.reddit.com/r/MachineLearning/comments/1ir2zm3/dhow_to_handle_highly_imbalanced_dataset/","date":1739740969,"author":"/u/ThickDoctor007","guid":879,"unread":true,"content":"<p>I‚Äôm working on an insurance claims prediction model, and I‚Äôd love to get insights from the community on tackling a highly imbalanced dataset. In the past, I built churn prediction models, and now I‚Äôm focusing on predicting insurance claims, where the percentage of claims is quite low.</p><p>My dataset spans 15 years and contains ~800,000 records with features such as sex, age, horsepower, car brand &amp; type </p>","contentLength":408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Use the New tool Directive in Go 1.24","url":"https://www.reddit.com/r/golang/comments/1ir2q9q/how_to_use_the_new_tool_directive_in_go_124/","date":1739740319,"author":"/u/zakariachahboun","guid":794,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zakariachahboun\"> /u/zakariachahboun </a> <br/> <span><a href=\"https://www.bytesizego.com/blog/go-124-tool-directive\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ir2q9q/how_to_use_the_new_tool_directive_in_go_124/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Issues with logrotate when logrotate failed to rotate the logs for container","url":"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/","date":1739739973,"author":"/u/barely_malted","guid":796,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I am using AWS EKS and using default kubelet logrotate parameters (maxsize = 10 Mi and maxfiles = 5)<br/> I am facing an issue where I believe these default values are not respected. The kubelet is failing with &#39;Failed to rotate log for container&#39; &#39;err=failed to compress log (container/pod log paths) nospace left on device&#39;<br/> At the same time one of my pods generated 200 GB logs in one single file. How is this possible ?<br/> I was not able to find out any documentation regarding this behaviour.<br/> Does this mean that since the kubelet was not able to rotate logs, it just kept on writing them to this one log file till it reached the diskspace limits of my worker nodes ?<br/> K8s/EKS version 1.27</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/barely_malted\"> /u/barely_malted </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I wrote a Go Game in the Go Language","url":"https://www.reddit.com/r/golang/comments/1ir2kz3/i_wrote_a_go_game_in_the_go_language/","date":1739739934,"author":"/u/ExistingStrawberry25","guid":1861,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a long-time professional Java developer, and I was curious about both the Go language and the Go board game, so I started a conversation with ChatGPT about both. At certain point, I quite innocently asked if there was a way to use the language to simulate the game. It turned out there was. It&#39;s an amateur effort, not ready for prime time, but it works. If you want to check it out, I&#39;d be interested to know what people in the know about Go think. <a href=\"https://github.com/eGantry/go-repo\">Here&#39;s the game project.</a> To run it from the project folder, enter <code>go run main.go ko-rule.go</code></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ExistingStrawberry25\"> /u/ExistingStrawberry25 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ir2kz3/i_wrote_a_go_game_in_the_go_language/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ir2kz3/i_wrote_a_go_game_in_the_go_language/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ICML Reviewers 2025- Assigned papers? [D]","url":"https://www.reddit.com/r/MachineLearning/comments/1ir29gm/icml_reviewers_2025_assigned_papers_d/","date":1739739123,"author":"/u/Even-Inevitable-7243","guid":1774,"unread":true,"content":"<p>To all other ICML 2025 Reviewers, are you already seeing your assigned papers in OpenReview? I received the update that assigned papers are ready for review days ago but in my Reviewers Console I only see \"<em>You have no assigned papers. Please check again after the paper assignment process is complete.</em>\" I also have no outstanding Reviewer Tasks listed. We are now three days into the review period so I would hope that assignments are complete by now. What I am absolutely not going to do is review 6 papers within a period of a few days because some AC needs extra reviewers.</p>","contentLength":576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distributed Software Architecture Fundamentals for Product Owners","url":"https://www.reddit.com/r/programming/comments/1ir1tlc/distributed_software_architecture_fundamentals/","date":1739737990,"author":"/u/snowball_dev","guid":1980,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>An article I wrote trying to explain my frustration to my PO with the current architecture of a system and why it is <em>not</em> a microservice </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/snowball_dev\"> /u/snowball_dev </a> <br/> <span><a href=\"https://litdev.bearblog.dev/software-architecture-for-product-owners/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ir1tlc/distributed_software_architecture_fundamentals/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do devs use kubernetes services locally via ingress on the likes of docker desktop","url":"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/","date":1739734137,"author":"/u/TheRandyOne","guid":746,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have recently started getting some toolkits running for my devs. I need to get them started on k8s as I am moving services over to k8s.</p> <p>I was explaining how this works to a friend and it dawned on me that to use a resource inside the cluster you need to enter via an ingress. The ingress is easy enough since we have the nginx ingress. </p> <p>The problem comes in with the dns records required to point to the defined resource to 127.0.0.1 in the /etc/hosts file. Since we have quite few services that need to hosted in k8s, it&#39;ll really suck to have the devs to add a bunch of records to the hosts file</p> <p>Basically I want something like a wild card record that always returns 127.0.0.1 outside the cluster. So they can pick whatever name they want and always have that delivered to the ingress.</p> <p>Am I doing this wrong? Is there some other way that I should be approaching this problem?<br/> Or can someone explain how they deal with this other than just editing hosts files.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheRandyOne\"> /u/TheRandyOne </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why did they decide not to have union ?","url":"https://www.reddit.com/r/golang/comments/1iqzofv/why_did_they_decide_not_to_have_union/","date":1739732626,"author":"/u/D4kzy","guid":765,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I know life is simpler without union but sometimes you cannot get around it easily. For example when calling the windows API or interfacing with C.</p> <p>Do they plan to add union type in the future ? Or was it a design choice ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/D4kzy\"> /u/D4kzy </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1iqzofv/why_did_they_decide_not_to_have_union/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1iqzofv/why_did_they_decide_not_to_have_union/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Getting LLMs to more reliably modify code- let's parse Abstract Syntax Trees and have the LLM operate on that rather than the raw code- will it work? I wrote a blog post, \"Prompting LLMs to Modify Existing Code using ASTs\"","url":"https://www.reddit.com/r/programming/comments/1iqzcf6/getting_llms_to_more_reliably_modify_code_lets/","date":1739731817,"author":"/u/10ForwardShift","guid":1742,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/10ForwardShift\"> /u/10ForwardShift </a> <br/> <span><a href=\"https://codeplusequalsai.com/static/blog/prompting_llms_to_modify_existing_code_using_asts.html?p\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1iqzcf6/getting_llms_to_more_reliably_modify_code_lets/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linus Quote of the day","url":"https://www.reddit.com/r/linux/comments/1iqz9we/linus_quote_of_the_day/","date":1739731649,"author":"/u/bhagwano-ka-bhagwan","guid":719,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bhagwano-ka-bhagwan\"> /u/bhagwano-ka-bhagwan </a> <br/> <span><a href=\"https://i.redd.it/xos4kh96rjje1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1iqz9we/linus_quote_of_the_day/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"A calculator app? Anyone could make that.\"","url":"https://www.reddit.com/r/programming/comments/1iqz489/a_calculator_app_anyone_could_make_that/","date":1739731262,"author":"/u/iamkeyur","guid":720,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iamkeyur\"> /u/iamkeyur </a> <br/> <span><a href=\"https://chadnauseam.com/coding/random/calculator-app\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1iqz489/a_calculator_app_anyone_could_make_that/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pull Request testing on Kubernetes: working with GitHub Actions and GKE","url":"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/","date":1739730123,"author":"/u/nfrankel","guid":722,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I‚Äôm continuing my series on running the test suite for each Pull Request on Kubernetes. In the <a href=\"https://blog.frankel.ch/integration-test-kubernetes/1/\">previous post</a>, I laid the groundwork for our learning journey: I developed a basic JVM-based CRUD app, tested it locally using Testcontainers, and tested it in a GitHub workflow with a GitHub service container.</p> <p>This week, I will raise the ante to run the end-to-end test in the target Kubernetes environment. For this, I‚Äôve identified gaps that I‚Äôll implement in this blog post:</p> <ul> <li>Create and configure a Google Kubernetes Engine instance</li> <li>Create a Kubernetes manifest for the app, with Kustomize for customization</li> <li>Allow the GitHub workflow to use the GKE instance</li> <li>Build the Docker image and store it in the GitHub Docker repo</li> <li>Install the PostgreSQL Helm chart</li> <li>Apply our app manifest</li> <li>Finally, run the end-to-end test</li> </ul> <p>Stages 1, 2, and 3 are upstream, while the workflow executes the latter steps for each PR.</p> <p>As I had to choose a tech stack for the app, I had to select a Cloud provider for my infrastructure. I choose GKE because I‚Äôm more familiar with Google Cloud, but you can apply the same approach to any other provider. The concept will be the same, only the implementation will differ slightly.</p> <p><a href=\"https://blog.frankel.ch/pr-testing-kubernetes/2/\">Read more</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nfrankel\"> /u/nfrankel </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hinton: \"I thought JD Vance's statement was ludicrous nonsense conveying a total lack of understanding of the dangers of AI ... this alliance between AI companies and the US government is very scary because this administration has no concern for AI safety.\"","url":"https://www.reddit.com/r/artificial/comments/1iqy8te/hinton_i_thought_jd_vances_statement_was/","date":1739729055,"author":"/u/MetaKnowing","guid":718,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Fine-tuning a Video Diffusion Model on new datasets","url":"https://www.reddit.com/r/MachineLearning/comments/1iqy1pi/d_finetuning_a_video_diffusion_model_on_new/","date":1739728574,"author":"/u/lapurita","guid":1807,"unread":true,"content":"<p>I have a few different types of datasets that I'd like to train generative video models for: - drone footage - microscopy videos</p><p>It seems to be me like most SOTA Video Diffusion models (atleast the open source ones) utilize an existing Image Diffusion models, and turn it into a Video Diffusion Model by adding temporal layers, then train the model on videos while keeping the spatial layers fixed.</p><p>This takes me to my problem. Lets focus on one of the modalities, microscopy videos. I think the conditioning for now would be a starting frame. I have a few approaches as I see it: - Fine-tune image model (probably SD2) on microscopy images, then turn that model into a video model by adding temporal layers and fine-tune on videos<p> - Directly fine-tune Stable Video Diffusion on the microscopy videos</p></p><p>Intuitively I'm feeling like \"full fine-tunes\" is what makes sense here, as opposed to something like Low Rank Adaptation? From what I can tell, Stable Video Diffusion seems to be the best open source model, or is there another model else I should look into aswell?</p><p>I have around 500GB of data and 1500 H100 hours, so I'm definitely not GPU-rich enough to do anything from scratch, hence why some fine-tuning approach is preferred, and also why the latent approaches are preferred over the pixel space ones.</p><p>There seems to exist immense resources online on how to fine-tune the image diffusion models, but not so much about the video models. Obviously the process should be pretty similar, but still. What do you think, have I identified the most approaches that are most likely to work, or do you know of anything else? And what do you think, how should I approach this? How good results can I expect to get? And about evaluation, are automatic metrics good enough or am I going to need to do human evals?</p>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Managing a Talos cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/","date":1739727418,"author":"/u/simen64","guid":698,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have been looking into moving my homelab to Kubernetes and Talos seems great for the job. I use OpenTofu for deploying infra in my homelab like VM&#39;s in proxmox, but how do people integrate Talos into OpenTofu / Terraform? I have not gotten the talos terraform provider to work and it lacks basic functionality for stuff like updating. So how do people manage their talos clusters?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/simen64\"> /u/simen64 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I wrote a desktop overlay for reading manga with egui","url":"https://www.reddit.com/r/rust/comments/1iqxfd0/i_wrote_a_desktop_overlay_for_reading_manga_with/","date":1739727052,"author":"/u/Takader","guid":744,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Today i am open sourcing my manga overlay i have been working on. It enables continues detection of japanese text in a selected region on the desktop. My goal was making it easy to find the meaning of kanji in order to learn japanese. </p> <p>You can find the source code on <a href=\"https://github.com/Icekey/manga-overlay\">github Manga Overlay</a>. Currently only Windows is supported.</p> <p>This is the first time i am open sourcing a project so feedback is welcome.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Takader\"> /u/Takader </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1iqxfd0/i_wrote_a_desktop_overlay_for_reading_manga_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1iqxfd0/i_wrote_a_desktop_overlay_for_reading_manga_with/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FOSDEM 2025 - Linux √ó VR! Beginner's Guide on How to Join Events in Virtual Reality from Ubuntu using Envision and Monado, an OpenXR Alternative to SteamVR","url":"https://www.reddit.com/r/linux/comments/1iqx43d/fosdem_2025_linux_vr_beginners_guide_on_how_to/","date":1739726280,"author":"/u/nialv7","guid":742,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nialv7\"> /u/nialv7 </a> <br/> <span><a href=\"https://fosdem.org/2025/schedule/event/fosdem-2025-5976-linux-vr-beginner-s-guide-on-how-to-join-events-in-virtual-reality-from-ubuntu-using-envision-and-monado-an-openxr-alternative-to-steamvr/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1iqx43d/fosdem_2025_linux_vr_beginners_guide_on_how_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Looking for lightweight Android emulator","url":"https://www.reddit.com/r/linux/comments/1iqwwrq/looking_for_lightweight_android_emulator/","date":1739725769,"author":"/u/__Timo_L_S__","guid":694,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi, I&#39;m looking for a lightweight and fast Android emulator, it only needs to have the very basics as all that I want to do is launch a single app. Currently I&#39;m using <a href=\"https://waydro.id/\">waydroid</a> to achieve this but starting the app takes a while as it has to boot android up first. This got me wondering if there exists a stripped version or something that would start faster. No google services required.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/__Timo_L_S__\"> /u/__Timo_L_S__ </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1iqwwrq/looking_for_lightweight_android_emulator/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1iqwwrq/looking_for_lightweight_android_emulator/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measure cpu utilization per deployment?","url":"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/","date":1739722689,"author":"/u/netcat23","guid":745,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi guys, does measuring cpu utilization of a deployment brings any value?</p> <p>What is you opinion about it?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/netcat23\"> /u/netcat23 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finally installed Arch in an old 32 bits machine!!","url":"https://www.reddit.com/r/linux/comments/1iqvkm3/finally_installed_arch_in_an_old_32_bits_machine/","date":1739722363,"author":"/u/mierd41a","guid":695,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I installed Arch in this Samsung Laptop NC210 (32-bit) . I was with a lot of problems with keyrings but I was able to fix it. It was easier than I expected, although I have already installed Arch before.</p> <p>What DE or WM do you recommend? It has 2GB of RAM and an Intel Atom, I was thinking about XFCE or BSPWM.</p> <p>I didn&#39;t know what TAG put, sorry if I it is wrong. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mierd41a\"> /u/mierd41a </a> <br/> <span><a href=\"https://i.redd.it/dffu430szije1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1iqvkm3/finally_installed_arch_in_an_old_32_bits_machine/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing: pixelvim, vim inspired pixel editor","url":"https://www.reddit.com/r/rust/comments/1iqviie/announcing_pixelvim_vim_inspired_pixel_editor/","date":1739722214,"author":"/u/avatar_10101","guid":781,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><blockquote> <p><code>pixelvim</code> is a pixel editor inspired by the <code>vim</code> text editor, with an emphasis on keyboard interaction. It also aims to be feature-rich, customizable, and extendable via user scripts.</p> </blockquote> <p>This is my personal project of making a vim-like pixel art editor (not very creative with the name, I know), written in Rust using <a href=\"https://github.com/not-fl3/miniquad\">miniquad</a>.</p> <p>Repo: <a href=\"https://github.com/bolphen/pixelvim\">https://github.com/bolphen/pixelvim</a></p> <p>Try it in the browser: <a href=\"https://bolphen.github.io/pixelvim/\">https://bolphen.github.io/pixelvim/</a> (you can drag-and-drop png, gif, and aseprite files, and save to png and gif; use <code>:set scale/ui=2</code> to increase the UI if you find them too small)</p> <p>Notable features</p> <ul> <li>vi-style remappable keyboard interaction and a command system, including modifiers (<code>5j</code> for moving 5 pixels down) and chain-able commands (<code>:select/all THEN cut THEN :layer/new/above THEN paste</code>)</li> <li>elaborated &quot;visual&quot; mode for pixel selection (that is undo/redoable)</li> <li>animation &quot;live draw&quot; (see the screencast below: very useful for quickly creating particle effects)</li> <li>rudimentary support for lua user scripts (not available in the browser version)</li> <li>data recovery from swap file in case of crashes</li> </ul> <p>The code is not pretty (a lot of places held up with glue) and there are quite a lot I want to improve as well as new features to add, but I feel that the end product could already be useful for some so I&#39;m releasing it. Overall I wish to translate more vim features that could be useful into pixelvim (for example, insert mode for drawing purely with the keyboard; registers and macros; better documentation), also better UI.</p> <p>There used to be a similar project <a href=\"https://github.com/cloudhead/rx\">rx</a>. This is not a fork, though I did borrow a few things here and there. I&#39;d say right now pixelvim is much more feature complete than rx.</p> <p>Cheers!</p> <p><a href=\"https://i.redd.it/4qigb6m84ije1.gif\">https://i.redd.it/4qigb6m84ije1.gif</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/avatar_10101\"> /u/avatar_10101 </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1iqviie/announcing_pixelvim_vim_inspired_pixel_editor/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1iqviie/announcing_pixelvim_vim_inspired_pixel_editor/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mongotui - A MongoDB client with a terminal user interface","url":"https://www.reddit.com/r/golang/comments/1iqv3kc/mongotui_a_mongodb_client_with_a_terminal_user/","date":1739721100,"author":"/u/gopher_256","guid":717,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1iqv3kc/mongotui_a_mongodb_client_with_a_terminal_user/\"> <img src=\"https://external-preview.redd.it/55ok8qa12rwU9Jc0kdib_vxlpdVHi8KxFJlw-e3iA1g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=736beebd3df0fa9ed868209e58cd6f5a2480f176\" alt=\"mongotui - A MongoDB client with a terminal user interface\" title=\"mongotui - A MongoDB client with a terminal user interface\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gopher_256\"> /u/gopher_256 </a> <br/> <span><a href=\"https://github.com/kreulenk/mongotui\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1iqv3kc/mongotui_a_mongodb_client_with_a_terminal_user/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI do have some points tho","url":"https://www.reddit.com/r/artificial/comments/1iqv26f/ai_do_have_some_points_tho/","date":1739720999,"author":"/u/JaydenPlayz2011","guid":1632,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making a Streaming JOIN 50% faster","url":"https://www.reddit.com/r/rust/comments/1iqum3o/making_a_streaming_join_50_faster/","date":1739719765,"author":"/u/bobbymk10","guid":721,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.epsio.io/blog/optimizing-streaming-joins-leveraging-asymmetry-for-better-performance\">https://www.epsio.io/blog/optimizing-streaming-joins-leveraging-asymmetry-for-better-performance</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bobbymk10\"> /u/bobbymk10 </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1iqum3o/making_a_streaming_join_50_faster/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1iqum3o/making_a_streaming_join_50_faster/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making a Streaming Join 50% Faster","url":"https://www.reddit.com/r/programming/comments/1iqulkg/making_a_streaming_join_50_faster/","date":1739719723,"author":"/u/ThinkRedstone","guid":1677,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ThinkRedstone\"> /u/ThinkRedstone </a> <br/> <span><a href=\"https://www.epsio.io/blog/optimizing-streaming-joins-leveraging-asymmetry-for-better-performance\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1iqulkg/making_a_streaming_join_50_faster/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How JIT (Just in time) compilation and V8 works and makes js incredibly fast","url":"https://www.reddit.com/r/programming/comments/1iqul1k/how_jit_just_in_time_compilation_and_v8_works_and/","date":1739719684,"author":"/u/CaptainOnBoard","guid":780,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CaptainOnBoard\"> /u/CaptainOnBoard </a> <br/> <span><a href=\"https://www.royalbhati.com/posts/why-js-is-fast\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1iqul1k/how_jit_just_in_time_compilation_and_v8_works_and/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Writing LLM prompts in Go with type-safety","url":"https://www.reddit.com/r/golang/comments/1iqtugn/writing_llm_prompts_in_go_with_typesafety/","date":1739717626,"author":"/u/shared_ptr","guid":690,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1iqtugn/writing_llm_prompts_in_go_with_typesafety/\"> <img src=\"https://external-preview.redd.it/M_AtZ_KMWH0EvoBdkj5ejjLtytl99FrbaFjeBLdIYZA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e1dd424ef52b065c3c52ecee76a707f34466b88e\" alt=\"Writing LLM prompts in Go with type-safety\" title=\"Writing LLM prompts in Go with type-safety\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/shared_ptr\"> /u/shared_ptr </a> <br/> <span><a href=\"https://blog.lawrencejones.dev/ai-dont-need-python/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1iqtugn/writing_llm_prompts_in_go_with_typesafety/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starting a Weekly Rancher Series ‚Äì From Zero to Hero!","url":"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/","date":1739717223,"author":"/u/abhimanyu_saharan","guid":650,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;m kicking off a weekly YouTube series on Rancher, covering everything from getting started to advanced use cases. Whether you&#39;re new to Rancher or looking to level up your Kubernetes management skills, this series will walk you through step-by-step tutorials, hands-on demos, and real-world troubleshooting.</p> <p>I&#39;ve just uploaded the introductory video where I break down what Rancher is and why it matters: üì∫ <a href=\"https://youtu.be/_CRjSf8i7Vo?si=ZR6IcXaNOCCppFiG\">https://youtu.be/_CRjSf8i7Vo?si=ZR6IcXaNOCCppFiG</a></p> <p>I&#39;ll be posting new videos every week, so if you&#39;re interested in mastering Rancher, make sure to follow along. Would love to hear your feedback and any specific topics you&#39;d like to see covered!</p> <p>Let‚Äôs build and learn together! üöÄ</p> <h1>Kubernetes #Rancher #DevOps #Containers #SelfHosting #Homelab</h1> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/abhimanyu_saharan\"> /u/abhimanyu_saharan </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"15 lessons from 15 years in tech","url":"https://www.reddit.com/r/programming/comments/1iqt5x8/15_lessons_from_15_years_in_tech/","date":1739715596,"author":"/u/gregorojstersek","guid":675,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gregorojstersek\"> /u/gregorojstersek </a> <br/> <span><a href=\"https://newsletter.eng-leadership.com/p/15-lessons-from-15-years-in-tech\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1iqt5x8/15_lessons_from_15_years_in_tech/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"üé∏üî• Introducing ChordFlow ‚Äì A Rust-Powered TUI for Guitar Practice!","url":"https://www.reddit.com/r/rust/comments/1iqsx67/introducing_chordflow_a_rustpowered_tui_for/","date":1739714844,"author":"/u/timvancann","guid":697,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey fellow Rustaceans and guitarists! üëã</p> <p>I‚Äôve been working on <strong>ChordFlow</strong>, a terminal-based tool built in Rust to help with <strong>chord practice and improvisation</strong>. The idea came from my own struggles with guitar neck mastery and melodic improvisation. I wanted something lightweight, fast, and distraction free to help me follow chord while keeping time with a metronome.<br/> It also gave me a good opportunity to dive into ratatui and learn more about Rust!</p> <p><strong>Features:</strong></p> <p>üéµ Generates random <strong>chord progressions</strong> for improvisation<br/> üéõÔ∏è Built-in <strong>metronome</strong> to stay in time<br/> üñ•Ô∏è <strong>TUI interface</strong> for an easy and minimal setup<br/> üõ†Ô∏è <strong>Customizable</strong>‚Äîbring your own chord sets or use the defaults<br/> üöÄ Written in <strong>Rust</strong> for speed and efficiency</p> <p>It‚Äôs open-source, and I‚Äôd love feedback, contributions, or just thoughts from fellow Rustaceans and musicians! If you‚Äôre into <strong>music theory, Rust, or just want a minimal practice tool</strong>, give it a try!</p> <p>üëâ Check it out: <a href=\"https://github.com/timvancann/chordflow\">https://github.com/timvancann/chordflow</a><br/> üëâ Video demo: <a href=\"https://youtu.be/Oc7po6uNBfQ\">https://youtu.be/Oc7po6uNBfQ</a></p> <p>Would love to hear what you think! What features would you like to see? ü§ò</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/timvancann\"> /u/timvancann </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1iqsx67/introducing_chordflow_a_rustpowered_tui_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1iqsx67/introducing_chordflow_a_rustpowered_tui_for/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["reddit"]}