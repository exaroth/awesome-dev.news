{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":388,"items":[{"title":"I’ve released a game where players write real JavaScript code to battle other players online","url":"https://store.steampowered.com/app/1137320/Screeps_Arena/","date":1762030223,"author":"/u/artchiv","guid":324206,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olyo43/ive_released_a_game_where_players_write_real/"},{"title":"to transaction or not to transaction","url":"https://www.reddit.com/r/golang/comments/1olxt4z/to_transaction_or_not_to_transaction/","date":1762028100,"author":"/u/PancakeWithSyrupTrap","guid":324210,"unread":true,"content":"<p>Take this simplistic code:</p><p>func create(name string) error {</p><p>if err != nil { return err }</p><p>err := writeToDatabase(name)</p><p>if err != nil { return err}</p><p>func newDisk(name) error {</p><p>name, err := getDisk(name)</p><p>if err != nil { return err }</p><p>if name != \"\" { return nil }</p><p>if err != nil { return err}</p><p>This creates a disk and database record.</p><p>The `newDisk` function idempotently creates a disk. Why ? If writing a database record fails, there is an inconsistency. A real resource is created but there is no record of it. When client receives an error presumably it will retry, so a new disk will not be created and hopefully the database record is written. Now we are in a consistent state.</p><p>But is this a sensible approach ? In other words, shouldn't we guarantee we are always in a consistent state ? I'm thinking creating the disk and writing a database record should be atomic.</p>","contentLength":852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compile from a git repo but make changes","url":"https://www.reddit.com/r/golang/comments/1olx8jv/compile_from_a_git_repo_but_make_changes/","date":1762026706,"author":"/u/No-Confection8657","guid":324176,"unread":true,"content":"<p>I am running a VPS with ubuntu aarch64 and have go 1.25. I am trying to compile a program from a repo that is written in go but want to also implement a change from a pull request. The repo isn't mine, though I do have a fork of it on my git. </p><p>I installed task and followed the steps in the <a href=\"http://contributing.md/\">contributing.md</a> file. When I \"task deps\" it did spit out an error that was basically the same as when I was doing it passing go commands manually:</p><p>I decided to just try ignoring that and running \"task\" to build it. And it seemed to compile and I have successfully ran it.</p><p>Here is my issue now - I manually made the changes to the VERSION and internal/tgc/channel_manager.go files locally before running this but I think it just went ahead and used the original versions ignoring my changes</p><p>when I run teldrive version it spits out 1.7.0 and the changes to the version file is 1.7.1 - also the file that got generated is the exact same amount of bytes as the 1.7.0 release. So I think it just made the file with none of the changes I had manually input into the local copies of the files.</p><p>Then when I run task, it exits with the following error:</p><p>task: Failed to run task \"default\": task: Command \"go run scripts/release.go --version current\" failed: exit status 1</p><p>not sure what would cause this - when I look at that file, it seems to just reference the VERSION file to get the version number. and it simply says 1.7.1 instead of 1.7.0</p><p>Am I missing something obvious? Sorry for the long post, I am new at this.</p>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Multi-Platform Docker Builds with QEMU","url":"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/","date":1762024493,"author":"/u/Helpful_Geologist430","guid":324186,"unread":true,"content":"<p>One intriguing feature of containers and images is their multi-platform support. Docker uses Buildx, which is based on BuildKit, to enable multi-platform builds.</p><p>The old way (build for the current host’s platform, if you’re on an ARM CPU, you build an ARM image that won’t run on x86, and vice versa):</p><p>The new way: without a single care, build a multi-platform image that supports both x86 and ARM (and others):</p><div><div><code><table><tbody><tr><td><pre>docker buildx build  linux/amd64,linux/arm64 </pre></td></tr></tbody></table></code></div></div><p>How is this sorcery possible? Let’s take a look.</p><h2><a href=\"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/#but-first-what-are-containers\"></a></h2><p>Containers, under the hood, are simply processes isolated thanks to <a href=\"https://man7.org/linux/man-pages/man7/namespaces.7.html\">Linux’s namespaces</a>. The executables and files of these processes, packaged in layers, are compiled for a specific architecture.</p><p>Put differently, a container is a bundled runtime. This is what the <a href=\"https://specs.opencontainers.org/runtime-spec/runtime/?v=v1.0.2\">OCI runtime bundle</a> defines:</p><div><div><code><table><tbody><tr><td><pre>coolcontainer/\n├── config.json\n└── rootfs/\n    ├── bin/\n    ├── lib/\n    └── ...\n</pre></td></tr></tbody></table></code></div></div><p>An OCI runtime bundle (used to start a container) is obtained from an OCI image (Docker images are OCI-compliant).</p><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n</pre></td><td><pre>apt  umoci skopeo runc\n\n\nskopeo copy docker://alpine:latest oci:alpine:latest\n\nalpine/\n\numoci unpack  alpine:latest alpine-runtime-bundle\nalpine-runtime-bundle/\n\nrunc run  alpine-runtime-bundle mycoco\n    yo  /home/greeting\n    alpine-runtime-bundle/rootfs/home/greeting\n</pre></td></tr></tbody></table></code></div></div><p>This spec defines what’s needed to run a container. All OCI-compliant container solutions adhere to it (Docker, Podman, etc.). These files are then used to create a container process. The isolation is achieved through Linux namespaces. To the container, it feels like it’s running on its own filesystem, network, PID space, and so on, but in reality, it’s just a process, albeit a well-isolated one.</p><p>The reference implementation that takes an OCI runtime bundle and starts a container is  (Docker uses it under the hood). It takes all the information and layers in the bundle and creates the container process. Mounts, environment variables, and all kinds of options that can be specified when running a container are handled by .</p><p>This means that the executables and binaries are destined for a specific OS and architecture:</p><div><div><code><table><tbody><tr><td><pre>file  alpine-runtime-bundle/rootfs/bin/ls\nalpine-runtime-bundle/rootfs/bin/ls: ELF 64-bit LSB executable, ARM aarch64\n</pre></td></tr></tbody></table></code></div></div><p>So the  command inside the container layers is simply a regular executable built for ARM64.</p><p>You can’t just run an image built for an x86 CPU on an ARM CPU (out of the box). That’s where multi-platform images come into the picture.</p><p>An image that supports multiple architectures? Say what?</p><div><div><code><table><tbody><tr><td><pre>skopeo inspect  docker://docker.io/library/ubuntu:latest | jq | \n        ...\n</pre></td></tr></tbody></table></code></div></div><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n</pre></td><td><pre>\nskopeo copy  amd64  linux \n  docker://docker.io/library/nginx:latest \n  oci:nginx-amd64:latest\n\numoci unpack  nginx-amd64:latest nginx-amd64-runtime-bundle\n\n\nfile  nginx-amd64-runtime-bundle/rootfs/bin/ls\n</pre></td></tr></tbody></table></code></div></div><p>J’accuse! Intruder! An x86-64 binary on an ARM machine!</p><div><div><code><table><tbody><tr><td><pre>./alpine-runtime-bundle/rootfs/bin/ls\n\n./nginx-amd64-runtime-bundle/rootfs/bin/ls\n\nrunc run  nginx-amd64-runtime-bundle my-nginx-container\n</pre></td></tr></tbody></table></code></div></div><p>And that’s the heart of the problem when it comes to running containers across different platforms. What to do?</p><h2><a href=\"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/#qemu-and-binfmt-misc-to-the-rescue\"></a></h2><p>QEMU (Quick EMUlator) is quite the remarkable piece of software. It’s both an emulator and a virtualizer, and it also provides user-level emulation. Ehhh, what? Well, that’s what you get when you look up QEMU. Let’s put it in simpler terms:</p><ul><li><p>Emulator: It emulates hardware. It simulates entire systems (CPU, memory, disk, network, etc.) in software, meaning it exposes an interface to a guest program similar to actual hardware. Think about it: for an OS, all it sees is a bunch of CPU machine code that interacts with hardware and registers. If those registers and hardware behaviors are simulated in software, the OS is none the wiser and that’s exactly what QEMU does. You can simulate different CPUs (ARM, x86, RISC-V, etc.), run machine code instructions, and update state (registers, flags, program counter, etc.) as if you were running on real hardware, it’s just slower. By emulating CPUs in software, QEMU can run an OS built for the same or a different CPU architecture.</p></li><li><p>Virtualizer: Some CPUs offer hardware-assisted virtualization, basically, the CPU can differentiate between a guest and a host OS. This is a lot faster than using an emulator, but since you’re using the same CPU, you can only run a guest OS built for that CPU (for example, an x86 Linux guest on an x86 Linux host). This is supported in Linux through KVM. QEMU can make use of KVM, so when available, it’s better to use it for faster guest execution.</p></li><li><p>User-space emulation: This allows us to run a binary built for an architecture different from our machine’s by translating machine code and system calls on the fly. For instance,  works on an  CPU as if it were native. It’s truly magical, QEMU decodes ARM instructions and translates them into x86-64 ones, roughly:</p></li></ul><div><div><code><table><tbody><tr><td><pre>ARM code:          ADD R0, R1, R2\nQEMU intermediate: tcg_gen_add_i32(result, R1, R2)\nx86-64 host code:  mov eax,[R1]; add eax,[R2]; mov [R0],eax\n</pre></td></tr></tbody></table></code></div></div><p>So QEMU user-space emulation is the first piece of the cross-platform image puzzle.</p><p>The second piece is . It stands for <em>Binary Format Miscellaneous</em> (quite the name). The basic idea is that your Linux kernel knows how to run executables built for its own architecture. If you’re on an x86-64 CPU, your kernel can run x86-64 ELF files by default. It can’t run executables built for other architectures (like ARM) or other file types (like Windows  files or scripts).</p><p> is a kernel feature that allows us to specify an interpreter or program to handle certain files, based on their extension or on a magic byte sequence contained within the file. ARM Linux executables, for instance, have a distinguishable magic sequence:  We can configure  to use  whenever it encounters a file with that magic sequence. Similarly, we can configure it to use  when encountering files with a  extension.</p><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n</pre></td><td><pre> |  /proc/sys/fs/binfmt_misc/register\n\n\n./HelloWorld.jar \n +x /usr/local/bin/java-wrapper\n\n |  /proc/sys/fs/binfmt_misc/register\n\n./HelloWorld.jar\n</pre></td></tr></tbody></table></code></div></div><p>So,  lets the kernel specify a wrapper, interpreter, or command to run certain files based on their magic bytes or file extensions.</p><p>To recap:  allows us to execute binaries from other architectures, and  is the mechanism that maps those binaries (based on their magic bytes) to the appropriate QEMU user-space command.</p><p>In Docker’s <a href=\"https://docs.docker.com/build/building/multi-platform/#qemu\">documentation</a> about multi-platform builds, they explain that Docker Desktop supports multi-platform images with QEMU out of the box. (Docker Desktop is essentially a Linux VM tailored to run Docker, so it already has this configured.)</p><p>For Docker engine in Linux, we need to run:</p><div><div><code><table><tbody><tr><td><pre>docker run  tonistiigi/binfmt  all\n</pre></td></tr></tbody></table></code></div></div><p>This registers the  mappings (like we did above for Java and x86) but for all architectures.</p><p>The image <a href=\"https://github.com/tonistiigi/binfmt/blob/2062d3e3b27656ff1b19d762994567155b6fbdb2/cmd/binfmt/config.go#L22\">tonistiigi/binfmt</a> contains a Go binary that basically does what we demonstrated earlier, setting up mappings from ELF magic bytes to the appropriate QEMU binary for multiple architectures:</p><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n</pre></td><td><pre></pre></td></tr></tbody></table></code></div></div><p>In <a href=\"https://github.com/tonistiigi/binfmt/blob/2062d3e3b27656ff1b19d762994567155b6fbdb2/cmd/binfmt/main.go#L98\"></a>, we find this delightful snippet:</p><div><div><code><table><tbody><tr><td><pre></pre></td></tr></tbody></table></code></div></div><h2><a href=\"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/#the-final-piece-of-the-puzzle\"></a></h2><p>Ok, we know how foreign binaries are run. But how does it all tie together? How are we actually building these multi-platform images?</p><p>The Docker docs have a nice example:</p><div><div><code><table><tbody><tr><td><pre>\nFROM alpine\n\nRUN  /arch\n</pre></td></tr></tbody></table></code></div></div><div><div><code><table><tbody><tr><td><pre>docker buildx build linux/amd64,linux/arm64  letsgo:1.0 \n\ndocker run linux/arm64 letsgo:1.0  /arch\n\n\ndocker run linux/amd64 letsgo:1.0  /arch\n</pre></td></tr></tbody></table></code></div></div><p>It’s beautiful! So what happened exactly? By specifying <code>--platform=linux/amd64,linux/arm64</code>, we’re asking Docker to build two images, one for each platform. The pulled base layer () is platform-specific, and the binaries within it are built for each architecture. Let’s verify that:</p><div><div><code><table><tbody><tr><td><pre>docker run linux/arm64 letsgo:1.0 sh\napk add file \n\nfile  /bin/uname\n</pre></td></tr></tbody></table></code></div></div><p>Nice! The  runs , and that’s where the QEMU magic occurs. Under the hood, each binary in the image layers, compiled for its target architecture, is executed. Docker’s  instruction spawns a new process on the host (isolated within namespaces, but still just a process). Depending on the file’s magic bytes, the appropriate QEMU interpreter is invoked automatically via  and that RUN command works. If it was not for  and QEMU, we’d get a polite .</p><p>QEMU isn’t the only way to build multi-platform images. Docker Buildx supports using multiple builder nodes (a cluster), and you can use nodes with different architectures to build images natively for their respective platforms. There’s even a cloud offering built around this approach.</p><div><div><code><table><tbody><tr><td><pre>\ndocker buildx create  multiarch-builder unix:///var/run/docker.sock\n\ndocker buildx create  multiarch-builder ssh://user@arm64-host\ndocker buildx use multiarch-builder\n</pre></td></tr></tbody></table></code></div></div><p>For compilers that support cross-compilation (compiling code on one platform, the host, to create an executable for a different platform ,the target), like Go, which does so natively by specifying  and , you can build directly for each target architecture without relying on emulation. For example, in a Dockerfile build stage you might run:</p><div><div><code><table><tbody><tr><td><pre>RUN  go build  server </pre></td></tr></tbody></table></code></div></div><p>Then, you can simply copy the resulting binary into the runtime stage. Since the Go compiler supports cross-compilation, there’s no need to use QEMU here. Instead, we rely on the  and  environment variables provided automatically by Docker Buildx.</p>","contentLength":9470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olwc2d/understanding_multiplatform_docker_builds_with/"},{"title":"[P] Flow Matching: A visual introduction","url":"https://peterroelants.github.io/posts/flow_matching_intro/","date":1762020413,"author":"/u/Xochipilli","guid":324174,"unread":true,"content":"<p>I've been working with flow matching models for video generation for a while, and recently went back to my old notes from when I was first learning about them. I cleaned them up and turned them into this blog post.</p><p>Hopefully it’s useful for anyone exploring flow matching for generative modeling. Writing it certainly helped solidify my own understanding.</p>","contentLength":356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1olumno/p_flow_matching_a_visual_introduction/"},{"title":"DigitalOcean is chasing me for $0.01: What it taught me about automation","url":"https://linuxblog.io/digitalocean-1-cent-automation/","date":1762014573,"author":"/u/modelop","guid":324115,"unread":true,"content":"<p>There are three kinds of emails that can ruin a quiet Saturday: a security warning, an outage alert, and, apparently, a repeat reminder that you owe a cloud provider one cent, yes, $0.01. I’ve been using DigitalOcean since 2013. Personally, I don’t use it often, but I log in several times a week to support clients hosted there.</p><h2>A chuckle and twelve years of cloud love</h2><p>Over the past twelve years I have set up and managed countless droplets, and DigitalOcean’s support and uptime have been excellent; this isn’t that kind of post.</p><p>It’s a lighthearted look at what happens when automation churns out more notifications than the situation may warrant, and why even a penny‑sized bill can teach us bigger lessons about design and efficiency.</p><p>On Saturday, 25 October 2025, an email with the subject <em>“Payment required: Your pre‑payment has been used”</em> arrived in my inbox. It informed me that my prepaid credit was insufficient to cover the month’s usage and urged me to “make another payment or add an alternate payment method.”</p><p>There was just one catch: the outstanding balance was $0.01. I chuckled, and went on with my day only to receive the exact message two more times over the coming days. By the time Saturday rolled around, my inbox looked like this:</p><p>The inbox search screenshot above shows the cadence: identical “Payment required” messages on October 25th, 28th, and 31st, 2025, followed by an email on November 1, 2025, titled <em>“Your 2025‑10 invoice is available.”</em></p><p>The invoice email (screenshot also above) contains a table that lists the usage charges for October as $0.01, notes that the payment method will only be charged if the balance exceeds $3.00, and invites me to “View Invoice.” Here’s what those other three messages look like:</p><p>My immediate reaction was a bit of a chuckle, but by the fourth email, I was more curious than anything: Why<em> does an automated billing system send four emails about a 1-cent balance? </em><a href=\"https://docs.digitalocean.com/platform/billing/invoices/\" target=\"_blank\" rel=\"noopener\">DigitalOcean’s billing documentation</a> notes that invoices are generated monthly. In my case, the system sent several “action required” emails, maybe because I don’t have a payment method saved? But in any case, I rarely use my personal DigitalOcean account beyond just quick tests:</p><p>This experience of multiple emails for 1 cent owed, prompted me to think about the hidden costs of excessive email notifications and how we can design billing and alerting in a more thoughtful way.</p><h2>The True Cost of an Email</h2><p>Email feels free because individuals don’t pay per message, but providers do. A 2025 breakdown of email marketing costs notes that the typical cost for a business to send emails is <a href=\"https://www.mobiloud.com/blog/how-much-does-email-marketing-cost\" target=\"_blank\" rel=\"noopener\">$1–$2 per thousand messages</a>, translating to roughly $0.001–$0.002 per email. Amazon’s Simple Email Service charges $0.10 per 1,000 emails for outbound messages (sending or receiving) and a few cents per gigabyte for attachments.</p><p>This cost is likely less for DigitalOcean, with the three “Payment required” notices and one invoice with attachment costing the company at most between a tenth and two‑tenths of a cent to send. But multiply that by hundreds of thousands of customers, and it highlights how easy it is to use resources to clutter inboxes over microbalances.</p><p>The monetary cost is only part of the picture. Email has an environmental footprint because electricity powers servers, networks, and client devices. Researchers estimate that more than <a href=\"https://carbonliteracy.com/the-carbon-cost-of-an-email/\" target=\"_blank\" rel=\"noopener\">306&nbsp;billion emails were sent in 2021</a>, and the total is expected to hit almost 400 billion this year, thanks to DigitalOcean. jk!!</p><p>According to <a href=\"https://carbonliteracy.com/the-carbon-cost-of-an-email/\" target=\"_blank\" rel=\"noopener\">Mike Berners‑Lee</a>, a short text email can produce 0.2–0.3 g of CO₂, while a longer message with attachments can produce 17 g; an email blast to 100 people may generate 26 g or more. Email‑related emissions accounted for approximately 150 million tons of CO₂e in 2019. That’s <a href=\"https://carbonliteracy.com/the-carbon-cost-of-an-email/\" target=\"_blank\" rel=\"noopener\">about 0.3% of the world’s carbon footprint</a>. But more importantly, about 25% added to users’ annoyance levels – Source: </p><h2>Notification fatigue and design principles</h2><p>It isn’t just about costs or the environment. Usability tests consistently <a href=\"https://www.smashingmagazine.com/2025/07/design-guidelines-better-notifications-ux/\" target=\"_blank\" rel=\"noopener\">show that</a> frequent alerts are one of the top user complaints. In fact, it’s been proven by Facebook and others that sending fewer notifications can be better for both engagement and retention.</p><p>Good notification design also recognizes levels of severity: high‑attention alerts (e.g., security breaches or failed payments) should prompt immediate action, while low‑attention messages (informational updates) can be bundled or deferred. Services like Slack, for example, <a href=\"https://www.smashingmagazine.com/2025/07/design-guidelines-better-notifications-ux/\" target=\"_blank\" rel=\"noopener\">adapt notification frequency</a> automatically when channels become very active.</p><p>Looking at DigitalOcean’s billing reminders, it’s easy to see opportunities for improvement. A one‑cent balance does not warrant three emails + an invoice. The first message could have been informational (<em>“heads up, your balance is low”</em>), the second might wait until the balance crosses a predetermined threshold (say $1 or $3), and the third could be a month or 3 months later.</p><p>Alternatively, DigitalOcean could incorporate a small balance waiver similar to the one many credit card issuers use.&nbsp;Banks recognize that it’s not cost‑effective to chase pennies; they round down or apply a credit adjustment on the next statement. The same logic could help cloud providers reduce overhead and user frustration.</p><h2>It’s not just DigitalOcean: micro‑balances happen everywhere</h2><p>DigitalOcean isn’t alone in sending tiny bills. Back in 2013, an <a href=\"https://www.optus.com.au/\" target=\"_blank\" rel=\"noopener\">Optus</a> customer in Australia <a href=\"https://forums.whirlpool.net.au/archive/2174002#:~:text=posted%202013,27%2C%2010%3A26%20pm%20AEST\" target=\"_blank\" rel=\"noopener\">posted on Whirlpool forums</a> that a billing error left them with a one‑cent overdue notice after receiving a reimbursement. One commenter wrote that it would cost the company <em>“more in personnel overheads to deal with this stupid billing error, than what it’s worth”</em>, while another explained, <em>“It’s an automated system, mate. Just relax.”</em></p><p>The moral of the story is that most companies rely on automated billing scripts, and without sensible thresholds, they’ll dutifully produce statements for even the most trivial amounts.</p><p>In practice, if you owe 99 cents or less, <a href=\"https://www.doctorofcredit.com/small-balance-waiver-a-k-a-lots-of-free-99-cent-amazon-gcs/\" target=\"_blank\" rel=\"noopener\">many companies</a> apply a credit adjustment and report a zero balance. The banking industry has recognized that goodwill and efficiency outweigh the pennies left on the ledger. If major financial institutions can swallow a dollar, cloud platforms with higher margins can too.</p><h2>What this taught me and how I’ve been guilty too</h2><p>As someone who deploys systems and manages mail servers, I can’t throw stones without acknowledging my own missteps. Earlier this year, I built <a href=\"https://dewedda.com/\" target=\"_blank\" rel=\"noopener\">dewedda.com</a>, a storm‑watch website for the Eastern Caribbean. Part of which was to send automatic email alerts to subscribers when storms approached islands within specific distances and directions.</p><p>In testing, everything looked great: the algorithm computed wind fields, adjusted for intensity, and tracked dozens of scenarios. But the first time a real storm approached, my code started hammering subscribers with unnecessary alerts. It didn’t account for storms that curved away or systems that were still unnamed, resulting in duplicates, so people kept receiving warnings even when there was no threat or duplicate emails. I had to scramble to adjust the logic.</p><p>The experience taught me humility and the importance of edge cases, and it makes me more sympathetic to DigitalOcean’s engineers. Building resilient billing and notification systems is complex. Edge cases arise when accounts straddle billing cycles, use promotional credits, or move between team and personal billing. Legacy code and third‑party integrations can behave unpredictably. What matters is how we learn from these events.</p><h2>Conclusion (yes, I paid that one cent)</h2><p>I paid the 1 cent balance owed to DigitalOcean. But who covers that transaction cost?</p><p>In the end, I did what any responsible business owner would do: I logged into my account and paid the one‑cent balance. Because, it would sit there for months, I only used a droplet for ~1 hour to test something. My personal DigitalOcean account goes mostly unused. So paying this invoice also means no recurring emails to pay my bill. Maybe that’s their plan? Ha!</p><p>I hope this article highlights the hidden inefficiencies that creep into automation, whether it’s cloud invoices, marketing emails, or storm alerts.</p><p>I still recommend DigitalOcean to friends and clients. They offer a great product at a fair price, with transparent billing. Being able to spin up a droplet in a few seconds makes life easy for Linux nerds like me. This one‑cent episode doesn’t change that; it simply underscores the value of thoughtful notification design. There are no affiliate links in this post either.</p><p>In summary, as repeatedly proven, sending fewer, more relevant notifications improves user satisfaction and retention. The environmental data also shows that unnecessary emails carry hidden costs, and financial industry practices demonstrate that forgiving tiny balances can be cheaper than collecting them.</p><p>A bit of humor on a Saturday morning turned into a lesson for all of us on building better systems. And yes, just in case the automated script is listening, I can confirm that as of writing this, my DigitalOcean account balance is zero.</p>","contentLength":9237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ols6mk/digitalocean_is_chasing_me_for_001_what_it_taught/"},{"title":"Convex Optimization (or Mathematical Programming) in Go","url":"https://www.reddit.com/r/golang/comments/1ols1gn/convex_optimization_or_mathematical_programming/","date":1762014224,"author":"/u/RobotCyclist23","guid":324117,"unread":true,"content":"<p>Do you write a lot of Convex (or similar) Optimization problems and have been yearning for a way to model them in Go? <a href=\"https://github.com/MatProGo-dev/MatProInterface.go\">MatProInterface.go</a> can help you (and needs your input to gain more maturity)! Feel free to try it and let me know what you think!</p>","contentLength":247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Best (free) courses on neural networks","url":"https://www.reddit.com/r/MachineLearning/comments/1olqqj0/d_best_free_courses_on_neural_networks/","date":1762011078,"author":"/u/gized00","guid":324116,"unread":true,"content":"<p>I have to prepare a course on NN for master students. As you can expect, there is A LOT of interest into learning about LLMs and related topics. So I would like to spend ±50% of the classes on Transformers/attention mechanisms/RL in LLMs/etc.</p><p>At the best of my knowledge there is no textbook on this (using Deep Learning by Goodfellow et al. for the first part) so I am looking for the best classes on similar topics. </p><p>I will use these classes to inform my selection of topics, see how others introduce such topics, calibrate the amount of topics that I can cover, and list them on the course website. I am not planning to \"steal\" material without permission or anything like that.</p><p>In your opinion, which ones are THE BEST classes on Transformers and related topics?</p>","contentLength":764,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Simple Questions Thread","url":"https://www.reddit.com/r/MachineLearning/comments/1olpzpu/d_simple_questions_thread/","date":1762009244,"author":"/u/AutoModerator","guid":324100,"unread":true,"content":"<p>Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!</p><p>Thread will stay alive until next one so keep posting after the date in the title.</p><p>Thanks to everyone for answering questions in the previous thread!</p>","contentLength":287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VST3 now open source (MIT Licence)","url":"https://youtu.be/grMxkISQNyw?si=AF3vDzec-bBld-EF","date":1762008089,"author":"/u/Kunstbanause","guid":324207,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1olpiuz/vst3_now_open_source_mit_licence/"},{"title":"i'm a zoomer on cachyOS but it seems to run in the family; my father has a jacket with a sun microsystems embroider on the front","url":"https://www.reddit.com/r/linux/comments/1olp7wt/im_a_zoomer_on_cachyos_but_it_seems_to_run_in_the/","date":1762007332,"author":"/u/bonzibuddy_official","guid":324208,"unread":true,"content":"<p>he's mentioned having a good amount of experience in red hat mostly for his career, we ended up finding this in storage recently. it also has another larger embroidery of the java logo on the back. it's comfortable and fits me still which also rocks. </p><p>i started using linux (mint) around 2021/2022 for hobbyist and general purposes, had to mostly run windows for college using adobe, no longer doing all of that so i'm back on cachy since it seems promising enough for an arch derivative. </p><p>thought this would be neat to share on here. thank you unix for being the foundation for the funny little penguin kernal that's sure to sweep any year now :P</p>","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go's Context Logger","url":"https://github.com/pablovarg/contextlogger?tab=readme-ov-file#examples","date":1762007053,"author":"/u/PurityHeadHunter","guid":324070,"unread":true,"content":"<p>Hello Gophers! A while ago, I started using contextual logging in my projects and found it made debugging significantly easier. Being able to trace request context through your entire call stack is a game-changer for understanding what's happening in your system.</p><p>This project started as a collection of utility functions I copy-pasted between projects. Eventually, it grew too large to maintain that way, so I decided to turn it into a proper library and share it with the community. <a href=\"https://github.com/PabloVarg/contextlogger\">https://github.com/PabloVarg/contextlogger</a></p><p>Context Logger is a library that makes it easy to propagate your logging context through Go's  and integrates seamlessly with Go's standard library, mainly  and . If this is something that you usually use or you're interested on using it for your projects, take a look at some <a href=\"https://github.com/pablovarg/contextlogger\">Usage Examples</a>.</p><p>For a very simple example, here you can see how to:</p><ul><li>Embed a logger into your context</li><li>Update the context (this can be done many times before logging)</li><li>Log everything that you have included in your context so far</li></ul><pre><code>ctx = contextlogger.EmbedLogger(ctx) contextlogger.UpdateContext(ctx, \"userID\", user.ID) contextlogger.LogWithContext(ctx, slog.LevelInfo, \"done\") </code></pre>","contentLength":1171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1olp424/gos_context_logger/"},{"title":"Async Rust explained without Tokio or Smol","url":"https://youtu.be/_x61dSP4ZKM?si=XPDtuH13Du-s5KTD","date":1762005654,"author":"/u/Gisleburt","guid":324144,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1olokba/async_rust_explained_without_tokio_or_smol/"},{"title":"unsupportedConfigOverrides USAGE","url":"https://www.reddit.com/r/kubernetes/comments/1olodfm/unsupportedconfigoverrides_usage/","date":1762005147,"author":"/u/BigBprofessional","guid":324066,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need Advice: Bitbucket Helm Repo Structure for Multi-Service K8s Project + Shared Infra (ArgoCD, Vault, Cert-Manager, etc.)","url":"https://www.reddit.com/r/kubernetes/comments/1olnp4b/need_advice_bitbucket_helm_repo_structure_for/","date":1762003325,"author":"/u/Dependent_Concert446","guid":324067,"unread":true,"content":"<p>I’m looking for some advice on how to organize our <strong>Helm charts and Bitbucket repos</strong> for a growing  setup.</p><p>We currently have  that contains everything — about  several  (like ArgoCD, Vault, Cert-Manager, etc.).</p><p>For our , we created  that’s used for microservices. We <strong>don’t have separate repos for each microservice</strong> — all are managed under the same project.</p><p>Here’s a simplified view of the repo structure:</p><pre><code>app/ ├── project-argocd/ │ ├── charts/ │ └── values.yaml ├── project-vault/ │ ├── charts/ │ └── values.yaml │ ├── project-chart/ # Base chart used only for microservices │ ├── basechart/ │ │ ├── templates/ │ │ └── Chart.yaml │ ├── templates/ │ ├── Chart.yaml # Defines multiple services as dependencies using │ └── values/ │ ├── cluster1/ │ │ ├── service1/ │ │ │ └── values.yaml │ │ └── service2/ │ │ └── values.yaml │ └── values.yaml │ │ # Each values file under 'values/' is synced to clusters via ArgoCD │ # using an ApplicationSet for automated multi-cluster deployments </code></pre><p>The following  are also in the same repo right now:</p><ul><li><strong>Project Contour (Ingress)</strong></li><li><em>(and other cluster-level tools like k3s, Longhorn, etc.)</em></li></ul><p>These are <strong>not tied to the application project</strong> — they’re might shared and deployed across <strong>multiple clusters and environments</strong>.</p><ol><li>Should I move these shared infra components into a <strong>separate “infra” Bitbucket repo</strong> (including their Helm charts, Terraform, and Ansible configs)?</li><li>For GitOps with , would it make more sense to split things like this: <ul><li> → all microservices + base Helm chart</li><li> → cluster-level services (ArgoCD, Vault, Cert-Manager, Longhorn, etc.)</li></ul></li><li>How do other teams structure and manage their repositories, and what are the best practices for this in DevOps and GitOps?</li></ol><p> Used AI to help write and format this post for grammar and readability.</p>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I wrote the dumbest key-value db i could think of","url":"https://www.reddit.com/r/golang/comments/1oln8uo/i_wrote_the_dumbest_keyvalue_db_i_could_think_of/","date":1762002079,"author":"/u/AnonymZ_","guid":324021,"unread":true,"content":"<p>So i wrote the dumbest key value db for a go course. It’s called kvd, and it uses docker containers as storage (github.com/YungBricoCoop/kvd)</p><p>every SET creates a container, every GET reads from it. if the key already exists, it just renames the old container with a prune_ prefix instead of deleting it directly, because stopping containers takes forever then every 30 seconds, a pruning system comes around and actually stops and removes them.</p><p>it’s slow as hell, and it’s one of the worst ways you could ever implement a key value db. but it works and acts has a redis server.</p><p>the project isn’t really the point though, i kinda want to create a github org that stores weird-ass but projects, like good ideas implemented in the dumbest way possible or just in an insane creative way.</p><p>drop a comment if you want to be part of the org and throw some name ideas for the org too</p>","contentLength":878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hard Rust requirements from May onward for all Debian ports","url":"https://lists.debian.org/debian-devel/2025/10/msg00285.html","date":1762000364,"author":"/u/pyeri","guid":324047,"unread":true,"content":"<pre>Hi all,\n\nI plan to introduce hard Rust dependencies and Rust code into\nAPT, no earlier than May 2026. This extends at first to the\nRust compiler and standard library, and the Sequoia ecosystem.\n\nIn particular, our code to parse .deb, .ar, .tar, and the\nHTTP signature verification code would strongly benefit\nfrom memory safe languages and a stronger approach to\nunit testing.\n\nIf you maintain a port without a working Rust toolchain,\nplease ensure it has one within the next 6 months, or\nsunset the port.\n\nIt's important for the project as whole to be able to\nmove forward and rely on modern tools and technologies\nand not be held back by trying to shoehorn modern software\non retro computing devices.\n\nThank you for your understanding.\n-- \ndebian developer - deb.li/jak | jak-linux.org - free software dev\nubuntu core developer                              i speak de, en\n</pre>","contentLength":874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olmnhx/hard_rust_requirements_from_may_onward_for_all/"},{"title":"Hard Rust requirements from May onward (for Debian's package manager, APT)","url":"https://lists.debian.org/debian-devel/2025/10/msg00285.html","date":1761999882,"author":"/u/DeleeciousCheeps","guid":324068,"unread":true,"content":"<pre>Hi all,\n\nI plan to introduce hard Rust dependencies and Rust code into\nAPT, no earlier than May 2026. This extends at first to the\nRust compiler and standard library, and the Sequoia ecosystem.\n\nIn particular, our code to parse .deb, .ar, .tar, and the\nHTTP signature verification code would strongly benefit\nfrom memory safe languages and a stronger approach to\nunit testing.\n\nIf you maintain a port without a working Rust toolchain,\nplease ensure it has one within the next 6 months, or\nsunset the port.\n\nIt's important for the project as whole to be able to\nmove forward and rely on modern tools and technologies\nand not be held back by trying to shoehorn modern software\non retro computing devices.\n\nThank you for your understanding.\n-- \ndebian developer - deb.li/jak | jak-linux.org - free software dev\nubuntu core developer                              i speak de, en\n</pre>","contentLength":874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1olmhxe/hard_rust_requirements_from_may_onward_for/"},{"title":"Python refuses $1.5M grant, Unity's in trouble, AUR attacked again - Linux Weekly News","url":"https://tilvids.com/videos/watch/02a038db-fdd0-46d4-8cb2-1f0b1b0bd04d","date":1761998909,"author":"/u/Pure_Toe6636","guid":324069,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1olm6jp/python_refuses_15m_grant_unitys_in_trouble_aur/"},{"title":"We should be very concerned about knowing who's real and who isn't","url":"https://www.reddit.com/r/artificial/comments/1olm00b/we_should_be_very_concerned_about_knowing_whos/","date":1761998341,"author":"/u/datascientist933633","guid":324209,"unread":true,"content":"<p>Colleague of mine recently started their own AI company, it's basically a voice call service that can be used to sell to people and do outbound marketing and sales. The thing is completely disturbing and dystopian. It called me for a test and I thought I was talking to a real person. It was so lifelike, the vocalizations were so real and unbelievably authentic. </p><p>This is one concern about AI that I have recently. How in the heck do you know who is real and who isn't? </p>","contentLength":470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"You're absolutely right.","url":"https://v.redd.it/u13z27vogmyf1","date":1761992631,"author":"/u/MetaKnowing","guid":324004,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1olkek8/youre_absolutely_right/"},{"title":"I created Open Source Kubernetes tool called Forkspacer to fork entire environments + dataplane, it is like git but for kubernetes.","url":"https://www.reddit.com/r/kubernetes/comments/1olk9we/i_created_open_source_kubernetes_tool_called/","date":1761992135,"author":"/u/Laughing-Dawg","guid":324001,"unread":true,"content":"<p>I created an open-source tool that lets you create, fork, and hibernate entire Kubernetes environments.</p><p>With , you can fork your deployments while also migrating your data.. not just the manifests, but the entire data plane as well. We support different modes of forking: by default, every fork spins up a managed, dedicated virtual cluster, but you can also point the destination of your fork to a self-managed cluster. You can even set up multi-cloud environments and fork an environment from one provider (e.g., AWS) to another (e.g., GKE, AKE, or on-prem).</p><p>You can clone full setups, test changes in isolation, and automatically hibernate idle workspaces to save resources all declaratively, with GitOps-style reproducibility.</p><p>It’s especially useful for spinning up dev, test, pre-prod, and prod environments, and for teams where each developer needs a personal, forked environment from a shared baseline.</p><p><strong><em>License is Apace 2.0 and it is written in Go using Kubebuilder SDK</em></strong></p><p>Please give it a try let me know, thank you</p>","contentLength":1017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what exactly is open-sourced in grokipedia?","url":"https://www.reddit.com/r/linux/comments/1olk43q/what_exactly_is_opensourced_in_grokipedia/","date":1761991521,"author":"/u/nix-solves-that-2317","guid":324003,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monthly: Certification help requests, vents, and brags","url":"https://www.reddit.com/r/kubernetes/comments/1olk1no/monthly_certification_help_requests_vents_and/","date":1761991272,"author":"/u/thockin","guid":323962,"unread":true,"content":"<p>Did you pass a cert? Congratulations, tell us about it!</p><p>Did you bomb a cert exam and want help? This is the thread for you.</p><p>Do you just hate the process? Complain here.</p><p>(Note: other certification related posts will be removed)</p>","contentLength":223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monthly: Who is hiring?","url":"https://www.reddit.com/r/kubernetes/comments/1olk17i/monthly_who_is_hiring/","date":1761991234,"author":"/u/gctaylor","guid":323961,"unread":true,"content":"<div><p>This monthly post can be used to share Kubernetes-related job openings within  company. Please include:</p><ul><li>Location requirements (or lack thereof)</li><li>At least one of: a link to a job posting/application page or contact details</li></ul><p>If you are interested in a job, please contact the poster directly. </p><p>Common reasons for comment removal:</p><ul><li>Not meeting the above requirements</li><li>Recruiter post / recruiter listings</li><li>Negative, inflammatory, or abrasive tone</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code (WASM 45% slower)","url":"https://ar5iv.labs.arxiv.org/html/1901.09056","date":1761989253,"author":"/u/Zomgnerfenigma","guid":324020,"unread":true,"content":"<h5>The Challenge of Benchmarking WebAssembly</h5><div><p>The aforementioned suite of 24 benchmarks is the PolybenchC benchmark\nsuite&nbsp;, which is designed to measure the effect of\npolyhedral loop optimizations in compilers. All the benchmarks in the\nsuite are small scientific computing kernels rather than full\napplications (e.g., matrix multiplication and LU Decomposition); each is\nroughly 100 LOC. While WebAssembly is designed to accelerate scientific\nkernels on the Web, it is also explicitly designed for a much richer set\nof full applications.</p></div><div><p>The WebAssembly documentation highlights several intended use\ncases&nbsp;, including scientific kernels, image editing,\nvideo editing, image recognition, scientific visualization, simulations,\nprogramming language interpreters, virtual machines, and POSIX applications.\nTherefore, WebAssembly’s strong performance on the scientific kernels in PolybenchC\ndo not imply that it will perform well given a different kind of application.</p></div><div><p>We argue that a more comprehensive evaluation of WebAssembly should rely on an\nestablished benchmark suite of large programs, such as the SPEC CPU benchmark\nsuites. In fact, the SPEC CPU 2006 and 2017 suite of\nbenchmarks include several applications that fall under the intended use cases of\nWebAssembly: eight benchmarks are scientific applications (e.g., ,\n, , , and\n), two benchmarks involve image and video processing\n( and ), and all of the benchmarks are POSIX\napplications.</p></div><div><p>Unfortunately, it is not possible to simply compile a sophisticated\nnative program to WebAssembly. Native programs, including the programs in\nthe SPEC CPU suites, require operating system services, such as a\nfilesystem, synchronous I/O, and processes, which WebAssembly and the\nbrowser do not provide. The SPEC benchmarking harness itself requires\na file system, a shell, the ability to spawn processes, and other Unix\nfacilities. To overcome these limitations when porting native\napplications to the web, many programmers painstakingly modify their\nprograms to avoid or mimic missing operating system\nservices. Modifying well-known benchmarks, such as SPEC CPU, would not\nonly be time consuming but would also pose a serious threat to\nvalidity.</p></div><div><p>The standard approach to running these applications today is to use\nEmscripten, a toolchain for compiling C and C++ to\nWebAssembly&nbsp;. Unfortunately, Emscripten only supports\nthe most trivial system calls and does not scale up to large-scale\napplications. For example, to enable applications to use synchronous\nI/O, the default Emscripten  filesystem loads the entire\nfilesystem image into memory before the program begins executing. For\nSPEC, these files are too large to fit into memory.</p></div><div><p>A promising alternative is to use , a framework that enables\nrunning unmodified, full-featured Unix applications in the\nbrowser&nbsp;.  implements\na Unix-compatible kernel in JavaScript, with full support for\nprocesses, files, pipes, blocking I/O, and other Unix features.\nMoreover, it includes a C/C++ compiler (based on Emscripten)\nthat allows programs to run in the browser\nunmodified. The  case studies include complex applications,\nsuch as , which runs entirely in the browser without any\nsource code modifications.</p></div><div><p>Unfortunately,  is a JavaScript-only solution, since it was\nbuilt before the release of\nWebAssembly. Moreover,  suffers from high performance overhead,\nwhich would be a significant confounder while benchmarking. Using ,\nit would be difficult to tease apart the poorly performing benchmarks\nfrom performance degradation introduced by .</p></div>","contentLength":3526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oljj3v/not_so_fast_analyzing_the_performance_of/"},{"title":"Programming Language Agnostic Naming Conventions","url":"https://codedrivendevelopment.com/posts/programmatic-naming-conventions-guide","date":1761982250,"author":"/u/Distinct-Panic-246","guid":324002,"unread":true,"content":"<p>There is a famous quote when it comes to naming things in programming, which is attributed to Phil Karlton</p><blockquote><p>\"There are only two hard things in Computer Science: cache invalidation and naming things\"</p></blockquote><p>(Or the slight variation of <em>\"There are only two hard things in Computer Science: cache invalidation, naming things, and off by one errors\"</em>)</p><p>But over the last few decades there are definitely a few common conventions. Using standard names for things frees up time to work on tougher problems than naming, and means future readers of your code can probably understand the concept better.</p><h2>Why we spend time naming things correctly</h2><p>If you see a variable called , you can probably assume it is a boolean.  or  are not clear.</p><h3>Avoid Negative variable names:</h3><p>Negative names can lead to double negatives, which are confusing.</p><ul></ul><p>Abbreviations can be ambiguous - not everyone will interpret it as the same meaning. Just use the full word, it is clearer.</p><p>(Although  is probably an exception where it should always be used over ).</p><ul></ul><h3>Pick a language and always use that</h3><p>If you work in a modern company then it is likely you work with people originally from various countries around the world. It can be easy to end up with a codebase with a mix of words like  and .</p><p>I'd recommend just picking US spelling in your code (even if the app is localised only for a UK or AU audiece)</p><ul></ul><h3>Make booleans obvious by using is/has prefix</h3><p>If you name something , it is quite obvious that it is a boolean. Try to always do this, as something like  could read as if it wasn't a boolean</p><ul></ul><p>Words like , ,  are too generic. Try to avoid these terms</p><p>Pick a convention for naming things, and use those everywhere.calculateAmount</p><ul><li>Bad 👎:  and </li><li>Good 👍:  and </li></ul><p>Also pick a style for casing, and be sure you're consistent with it. Here are some examples (there might be other typical conventions for your library/language of choice)</p><ul><li> for class names</li><li> for most other variables</li><li> for static constants</li></ul><h2>Common names for specific things</h2><p>If you're taking some data and  it to a different shape or different values then  is a common and accurate name.</p><pre><div><div><div><code></code></div></div></div></pre><p>If you need to check if data is valid/correct, then its almost always called a validator.</p><pre><div><div><div><code></code></div></div></div></pre><p>Used when describing the shape of some data structure. Often used with database designs.</p><pre><div><div><div><code></code></div></div></div></pre><p>When you need to take some data (e.g. some string) and understand its own data structure. They are quite different things, parsers and transformers can  be very related</p><pre><div><div><div><code></code></div></div></div></pre><p>For code that runs 'between' different parts of your application. A typical use for middleware is in HTTP servers the incoming HTTP request can go through multiple middlewares to either transform the incoming data (before passing to next one or final endpoint handler function) or to do something with that data</p><pre><div><div><div><code></code></div></div></div></pre><p>When you have some functionality with a specific interface, and you need to convert it to another interface/shape.</p><p>It is also known as a 'wrapper' (or a bridge, although that is technically a slightly different thing)</p><p>When you need to make data uniform in scale, format, or structure</p><pre><div><div><div><code></code></div></div></div></pre>","contentLength":3010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olht95/programming_language_agnostic_naming_conventions/"},{"title":"Feeling confused about what to do next ?","url":"https://www.reddit.com/r/golang/comments/1olhfjy/feeling_confused_about_what_to_do_next/","date":1761980670,"author":"/u/Neutrino_i7","guid":323933,"unread":true,"content":"<p>For the past 3 months, I’ve beenusing Go for Building web api, SSR Application, Cli tools, But lately, it’s starting to feel like I’m doing the same thing over and over again — and honestly, it’s getting kinda boring.</p><p>I love Go, but I feel like I need something new and challenging to spice things up. Should I start learning another language alongside Go (maybe Rust or Python)? Or are there some cool project ideas in Go that can help me</p>","contentLength":448,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How is the current market demand for openstack combined with k8s","url":"https://www.reddit.com/r/kubernetes/comments/1olgx9m/how_is_the_current_market_demand_for_openstack/","date":1761978623,"author":"/u/ossicor30","guid":323915,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Well a old school flex i guess","url":"https://www.reddit.com/r/linux/comments/1olftbt/well_a_old_school_flex_i_guess/","date":1761974186,"author":"/u/Puzzleheaded-Car4883","guid":323965,"unread":true,"content":"<p>This old Red Hat Linux 8.0 manual’s been gathering dust on my shelf. I used to read it as a kid — didn’t understand a single word back then. Fast forward to age 19, 3 years into using Linux daily... and everything suddenly makes sense.</p><p>Btw this is one of those first thing that introduced me to linux </p>","contentLength":306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-Minute Daily AI News 10/31/2025","url":"https://www.reddit.com/r/artificial/comments/1olf4w9/oneminute_daily_ai_news_10312025/","date":1761971649,"author":"/u/Excellent-Target-847","guid":324175,"unread":true,"content":"<ol><li>, South Korea Government and Industrial Giants Build AI Infrastructure and Ecosystem to Fuel Korea Innovation, Industries and Jobs.[1]</li><li> says it’s deploying AI technology to stop Halloween parties.[2]</li><li> AI Unveils Supervised Reinforcement Learning (SRL): A Step Wise Framework with Expert Trajectories to Teach Small Language Models to Reason through Hard Problems.[3]</li><li> CEO says AI audio models will be ‘commoditized’ over time.[4]</li></ol>","contentLength":432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Happy Halloween, nerds","url":"https://www.reddit.com/r/linux/comments/1olf21x/happy_halloween_nerds/","date":1761971357,"author":"/u/feelingsupersonic","guid":323887,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Realized I like the coding and ML side of my PhD way more than the physics","url":"https://www.reddit.com/r/MachineLearning/comments/1olehrk/d_realized_i_like_the_coding_and_ml_side_of_my/","date":1761969380,"author":"/u/PurpleCardiologist11","guid":323899,"unread":true,"content":"<p>Hey everyone, I’m a 2nd-year ChemE PhD student working on granular media with ML, so, technically, my research is about the physics of these systems. But lately I’ve realized I get way more excited about the numerical modeling and machine learning part than the physics itself. </p><p>I love building models, debugging, testing new architectures, running simulations… but when it comes to actually digging into the physical interpretation, I kinda lose interest </p><p>The thing is, I don’t have a CS background, and I usually write “prototype” code that works, but it’s not what you’d call clean software. I never learned data structures, algorithms, or how to structure large projects properly. </p><p>After my PhD, I think I’d like to move more toward computational or ML-heavy work, something like scientific computing, data-driven modeling, or applied AI for physical systems. </p><p>For anyone who’s gone down a similar path: - What kind of skills should I start developing now?<p> - How important is it to learn formal CS stuff (like algorithms and software design)? </p></p><p>Would love to hear what worked for you. I feel like I’m starting to see where I actually fit, and I just wanna steer myself in the right direction.</p>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Futurelock: A subtle risk in async Rust","url":"https://rfd.shared.oxide.computer/rfd/0609","date":1761968878,"author":"/u/iamkeyur","guid":323963,"unread":true,"content":"<div data-lineno=\"424\"><p>Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.</p></div><div data-lineno=\"426\"><p>In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .</p></div><div data-lineno=\"428\"><p>As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:</p></div><div data-lineno=\"430\"><div data-lineno=\"1\"><p>Channel limits, channel limits: always wrong!</p></div><div data-lineno=\"3\"><p>Some too short and some too long!</p></div></div><div data-lineno=\"434\"><p>But as with timeouts, it’s often possible to find values that work in practice.</p></div><div data-lineno=\"436\"><p>Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.</p></div>","contentLength":1894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olecjo/futurelock_a_subtle_risk_in_async_rust/"},{"title":"Java Virtual Threads VS GO routines","url":"https://www.reddit.com/r/golang/comments/1oldyoo/java_virtual_threads_vs_go_routines/","date":1761967558,"author":"/u/gamecrow77","guid":324048,"unread":true,"content":"<p>I recently had a argument with my tech lead about this , my push was for Go since its a new stack , new learning for the team and Go is evolving , my assumption is that we will find newer gen of devs who specialise in Go. Was i wrong here ? the argument was java with virtual threads is as efficient as go </p>","contentLength":306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IRS open-sourced the fact graph it uses for tax law","url":"https://github.com/IRS-Public/fact-graph","date":1761958132,"author":"/u/R2_SWE2","guid":323813,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olb241/irs_opensourced_the_fact_graph_it_uses_for_tax_law/"},{"title":"shift left approach for requests and limits","url":"https://www.reddit.com/r/kubernetes/comments/1olaat1/shift_left_approach_for_requests_and_limits/","date":1761955835,"author":"/u/Containertester","guid":323862,"unread":true,"content":"<p>We’re trying to solve the classic requests &amp; limits guessing game; instead of setting CPU/memory by gut feeling or by copying defaults (which either wastes resources or causes throttling/OOM), we started experimenting with a benchmark-driven approach: we benchmark workloads in CI/CD and derive the optimal requests/limits based on http_requests_per_second (load testing).</p><p>In our latest write-up, we share:</p><ul><li>Why manual tuning doesn’t scale for dynamic workloads</li><li>How benchmarking actual CPU/memory under realistic load helps predict good limits</li><li>How to feed those results back into Kubernetes manifests</li><li>Some gotchas around autoscaling &amp; metrics pipelines</li></ul><p>Curious if anyone here has tried a similar “shift-left” approach for resource optimization or integrated benchmarking into their pipelines and how that worked out.</p>","contentLength":817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Steinberg, creators of VST technology and the ASIO protocol, have released the SDKs for VST 3 and ASIO as Open Source.","url":"https://www.reddit.com/r/linux/comments/1ola786/steinberg_creators_of_vst_technology_and_the_asio/","date":1761955542,"author":"/u/fenix0000000","guid":323814,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/fenix0000000\"> /u/fenix0000000 </a>","contentLength":35,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] ArXiv CS to stop accepting Literature Reviews/Surveys and Position Papers without peer-review.","url":"https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/","date":1761951858,"author":"/u/NamerNotLiteral","guid":323796,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1ol8wup/d_arxiv_cs_to_stop_accepting_literature/"},{"title":"[GoGreement] A new linter that can help enforce interface implementation and immutability","url":"https://www.reddit.com/r/golang/comments/1ol8das/gogreement_a_new_linter_that_can_help_enforce/","date":1761950330,"author":"/u/Green-Sympathy-2198","guid":323797,"unread":true,"content":"<p>Hey guys! I wrote this linter mainly for myself, but I hope some of you find it useful.</p><p>I came to golang from JVM world and I was missing some things like explicit implementation declaration and immutability.</p><p>But I see gophers love their linters, so I thought I could solve this with a linter.</p><p>How does it work? You just add annotations to your types like: <code>go // @immutable type User struct { id string name string } </code></p><p>And run the linter and it will give you an error if you try to change fields like this: </p><p>I also added annotations that let you check interface implementation: <code>go // @implements io.Reader </code></p><p>This lets you check that a struct actually implements an interface without all this stuff: <code>go var _ MyInterface = (*MyStruct)(nil) </code></p><p>And many other annotations (testonly, packageonly, ...). Would love to hear what you think!</p>","contentLength":822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Borrow checker says “No”! An error that scares me every single time!","url":"https://polymonster.co.uk/blog/borow-checker-says-no","date":1761949509,"author":"/u/__shufb","guid":324046,"unread":true,"content":"<p>It’s Halloween and I have just been caught out by a spooky borrow checker error that caught me by surprise. It feels as though it is the single most time consuming issue to fix and always seems to catch me unaware. The issue in particular is “cannot borrow x immutably as it is already borrowed mutably” - it manifests itself in different ways under different circumstances, but I find myself hitting it often when refactoring. It happened again recently so I did some investigating and thought I would discuss it in more detail.</p><p>The issue last hit me when I was refactoring some code in my graphics engine <a href=\"https://github.com/polymonster/hotline\">hotline</a>, I have been creating some content on YouTube and, after a little bit of a slog to fix the issue, I recorded a video of me going through the scenario of how it occurred and some patterns to use that I have adopted in the past to get around it. You can check out the video if you are that way inclined, the rest of this post will mostly echo what is in the video, but it might be a bit easier to follow code snippets and description in text.</p><p>I have a generic graphics API, which consists of traits called <a href=\"https://github.com/polymonster/hotline/blob/master/src/gfx.rs\">gfx</a>. This is there to allow different platform backends to implement the trait; currently I have a fully implemented Direct3D12 backend and I recently began to port macOS using Metal.</p><p>The gfx backend wraps underlying graphics API primitives; in this case we are mostly concerned about  which is a command buffer. Command buffers are used to submit commands to the GPU. They do things like  or , amongst other things. For the purposes of this blog post, what the command buffer does is not really that important, just that is does , which at the starting point when the code was working is a trait method that takes an immutable self and another immutable parameter ie. <code>fn do_something(&amp;self, param: &amp;Param)</code>.</p><p>In the rest of the code base I have a higher level rendering system called . This is graphics engine code that is not platform specific but implements shared functionality. So where  is a low level abstraction layer,  implements concepts of a  that is a view of a scene that we can render from. A  has a camera that can look at the scene and is then passed to a render function, which can build a command buffer to render the scene from that camera’s perspective. The engine is designed to be multithreaded and render functions are dispatched through  systems, so a view gets passed into a render system but it is wrapped in an .</p><p>I made a small cutdown example of this code to be able to demonstrate the problem I encounter, so let’s start with the initial working version:</p><div><div><pre><code></code></pre></div></div><p>I tried to simplify it as much as possible so these snippets should compile if you copy and paste them, they won’t run thanks to  macro (which I absolutely love using, it is so handy!) but we only care about the borrow checker anyway.</p><p>All we really need to think about is that a  can  and it also gets passed in a , which is also contained as part of ‘view’. Coming from a C/C++ background I landed on my personal preference being procedural C code with context passing, so I tend to group things together into a single struct. It makes sense to me in this case and I wanted to group everything inside , and we fetch the view from elsewhere in the engine.</p><p>So the code in the snippet compiles fine and I was working with this setup for some time. I began work on macOS and it turned out that the  method needed to mutate the command buffer so that I could mutate some internal state and make the Metal graphics API behave similarly to Direct3D12. This is common for graphics API plumbing.</p><p>The specific example in this case was that in Direct3D we call a function  to bind an index buffer before we make a call to , but in Metal there is no equivalent to bind an index buffer. Instead you pass a pointer to your index buffer when calling the equivalent draw indexed. So to fix this, when we call  we can store some extra state in the command buffer so we can pass it in the later call to .</p><p>In hindsight any method on the command buffer trait that does anything, like set anything or write into the command buffer, should take a  because it is mutating the command buffer after all. In my case since I am calling through to methods on  , which is unsafe code and does not require any mutable references.</p><p>In our simplified example, in order to store, state  now needs to change and take a mutable self: <code>do_something(&amp;mut self, param: &amp;Param)</code> it should be noted that  itself was already .</p><div><div><pre><code></code></pre></div></div><p>Borrow checker now kicks in…my heart sinks. In the real code base not only did I have to modify a single call site, but I had hundreds of places where this error was happening, I made the decision here and now to make any methods that write to the command buffer also be mutable and make the mutability</p><div><div><pre><code>error[E0502]: cannot borrow `view` as immutable because it is also borrowed as mutable\n  --&gt; src/main.rs:30:28\n   |\n30 |     view.cmd.do_something(&amp;view.param);\n   |     ----     ------------  ^^^^ immutable borrow occurs here\n   |     |        |\n   |     |        mutable borrow later used by call\n   |     mutable borrow occurs here\n\nFor more information about this error, try `rustc --explain E0502`.\nerror: could not compile due to 1 previous error\n</code></pre></div></div><p>This is not the first time I have encountered this problem and I doubt it will be the last. There are a number of ways to resolve it and they aren’t too complicated. The frustrating thing is that it seems to occur always when you are doing something else and not just when you decide to refactor, so you end up having a mountain of errors to solve before you can get back to the original task. I suppose you could call it a symptom of bad design or lack of experience, but when writing code things inevitably change and bend with new requirements, and Rust throws these unexpected issues up for me more often than I find with C, and often the required refactor takes more effort as well. But that is the cost you pay, hopefully more upfront effort to get past the borrow checker means fewer nasty debugging stages later. So let’s look at some patterns to fix the issue!</p><p>The one I actually went for in this case was using . We take the  out of view so we no longer need to borrow a ‘view’ to use , and then when finished return the cmd into ‘view’. It is important to note here that  needs to derive default in order for this to work, as when we take the  in  will become </p><div><div><pre><code></code></pre></div></div><p>This approach is the simplest I could think of at the time because any existing code using  doesn’t need updating, everything stays the same and we just separate the references. In this case it was easy to derive the default for  .You need to remember to set the  back on  here, which could be a pitfall and cause unexpected behaviour if you didn’t.</p><p>If you can’t easily derive default on a struct there are some other options. If the struct is clonable or you can easily derive a clone, you can clone to achieve a similar effect.</p><div><div><pre><code></code></pre></div></div><p>Cloning might be considered a heavier operation than ‘take’ depending on the circumstances, but this method has the same benefit as the take version whereby unaffected code that is using  elsewhere doesn’t need to be changed.</p><p>Another approach would be to use  this allows for interior mutability and again we do not need to worry about default or clone.</p><div><div><pre><code></code></pre></div></div><p>We also need to update any code that ever used  and do the same. Not ideal but it allows us to get around the need for a default or clone. I have had to resort to this in other places in the code base.</p><p>There are more options; quite literally  here can help. If we make  an  then this gives us the ability to use  as the default and we can use the  approach. We can also use  and swap with . Swapping works similar to ‘take’, where we take mem and swap with the default.</p><div><div><pre><code></code></pre></div></div><p>The  approach also requires more effort as we need to now take a reference and unwrap the option and update any code that ever used  to do the same. Not ideal, but it allows us to get around the need for a default or clone, and if your type is already optional then this will fit easily.</p><p>There is one final approach that could save a lot of time, and that would be to not change the  function at all in the first place. That is to keep it as <code>do_something(&amp;self, param: &amp;Param)</code>. So how do we mutate the interior state without requiring the self to be mutable?</p><p>This can be done with  in single threaded code or  in multithreaded code. Since we already looked at  I will do an example of .</p><div><div><pre><code></code></pre></div></div><p>I decided to make the mutability explicit to the trait and that was based on how the command buffers are used in the engine, in other places I have taken other approaches favouring interior mutability. For this case a view can be dispatched in parallel with other views, but the engine is designed such that 1 thread per view and no work happens to a single view on multiple threads at the same time. Command buffers are submitted in a queue in order and dispatched on the GPU.</p><p>Here it made sense to me to avoid locking interior mutability for each time we call a method on a  and it works with the engine’s design. We lock a view at the start of a render thread, fill it with commands and then hand it back to the graphics engineer for submission to the GPU. The usage is explicit, we just needed to appease the borrow checker!</p><p>I hope you enjoyed this article, please check out my <a href=\"https://www.youtube.com/@polymonster\">YouTube channel</a> for more videos or more articles on my blog, let me know what you think and if you have any other strategies or approaches I would love to hear about them. I would also like to hear about compiler and borrow checker errors you find particularly time consuming or frustrating to deal with.</p>","contentLength":9662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ol82h6/borrow_checker_says_no_an_error_that_scares_me/"},{"title":"What is wrong with this setup?","url":"https://www.reddit.com/r/kubernetes/comments/1ol7n6g/what_is_wrong_with_this_setup/","date":1761948348,"author":"/u/Low_Opening3670","guid":323793,"unread":true,"content":"<p>I needed Grafana Server for more than 500+ people to use and create dashboards on it...</p><p>I have one Grafana on EKS, I spin up everything using Terraform even wrap a k8s manifest in Terraform and deploy it to cluster. </p><p>There is not much change in Grafana application maybe every 6 months new stable version is out and I am going to do the upgrade</p><p>What is wrong with this setup? and how I can improve it? do I really need flux/argo here? </p>","contentLength":432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What kind of debug tools are available that are cloud native?","url":"https://www.reddit.com/r/kubernetes/comments/1ol64df/what_kind_of_debug_tools_are_available_that_are/","date":1761944378,"author":"/u/lickety-split1800","guid":323770,"unread":true,"content":"<p>I'm an SRE and a longtime Linux &amp; automation person, starting in the late 90s.</p><p>With the advent of apps on containers, there are fewer and fewer tools to perform debugging.</p><p>Taking a look at the types of debug tools one has used to diagnose issues.</p><ul><li>even basic tools such as find, grep, ls and others are used in debugging.</li></ul><p>The Linux OS used to be under the control of the system administrator, who would put the tools required to meet operational debugging requirements, increasingly since it is the developer that maintains the container image and none of these tools end up on the image, citing most of the time startup time as the main requirement.</p><p>Now a container is a slice of the operating system so I argue that the container base image should still be maintained by those who maintain Linux, because it's their role to have these tools to diagnose issues. That should be DevOps/SRE teams but many organisations don't see it this way.</p><p>So what tools does Kubernetes provide that fulfil the needs I've listed above?</p>","contentLength":1012,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Companies are trying to do too much with AI, says IT CEO | Fortune","url":"https://fortune.com/2025/10/31/scaling-ai-mit-study-roi/","date":1761943455,"author":"/u/fortune","guid":323771,"unread":true,"content":"<p>Scaling gen AI projects beyond the pilot phase is fundamental to turning the current AI hype cycle into real ROI. How can companies get over the hump? For starters, they should stop trying to introduce AI to every facet of operations, says Abhijit Dubey, CEO of NTT Data, an IT services and consulting company.&nbsp;\n\n\n\n</p><p>“What happens is companies say, ‘In every single domain, I’m going to unleash innovation, and I’m going to have AI enablement.’ I think that’s the wrong strategy,” Dubey said at the Fortune Global Forum in Riyadh on Sunday. The right strategy, he noted, is to “pick one or two domains that are going to create disproportionate economic value for the company and go end to end.” He gave the example of focusing on underwriting in insurance and supply chains in manufacturing.\n\n\n\n</p><p><a href=\"https://fortune.com/company/fedex/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/fedex/\">FedEx</a> has been intentional about integrating AI into three broad areas: internal operations, customer experience, and creating new value levers for customers (such as improving demand forecasting and reducing returns), said Kami Viswanathan, FedEx’s president of the Middle East, Indian subcontinent, and Africa region. “Research has shown that organizations that have a clear AI strategy, which has this prioritization, have a much greater degree of success than others that don’t, right? So for us, that’s the key aspect of scaling.”\n\n\n\n</p><p>Deploying AI across an organization comes with risks and requires adequate safeguards, said Fabio Kuhn, CEO of Vortexa, a cargo tracking and energy market research firm whose customers can query its data via chatbot. Human supervision is essential to limiting hallucinations and keeping any that do slip through from shaping decision-making, he said. “In addition to speed and quality, what is … increasingly important on any model, any agent, is explainability. For a human to be able to understand, why is it making the decision that it is making?” he said.&nbsp;</p><p>Keeping a human in the loop is essential when AI is deployed in health care, said Noosheen Hashemi, cofounder and CEO of January AI, a precision health care company. “These LLMs [large language models], yes, they have read everything, but they do hallucinate, and they do it confidently. When there is data missing, they will invent it,” she said, noting that January AI’s Mirror tool has a hallucination rate under 1%. “We have doctors actually looking at results and saying, ‘Does this actually make sense?’”\n\n\n\n</p><p>Health care also has its own unique hurdles to scaling AI, especially in the U.S.: Data silos spread across patients, insurers, providers, and labs. The cost—in terms of potential—is enormous. “Not having a unified view of data doesn’t really allow us to leverage AI in the best way that we can,” Hashemi says. “We have the technology today, absolutely, to eradicate lifestyle-based chronic diseases in the world. The question is, to what extent do we have the will to actually apply this technology and to overcome data silos, regulatory issues, privacy issues, to actually deploy AI, because the technology is here.”\n</p>","contentLength":3089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ol5r4u/companies_are_trying_to_do_too_much_with_ai_says/"},{"title":"My Must-Have Apps Since Switching to Linux","url":"https://www.reddit.com/r/linux/comments/1ol5a1k/my_musthave_apps_since_switching_to_linux/","date":1761942274,"author":"/u/Overflow_Nuts","guid":323756,"unread":true,"content":"<p>OnlyOffice → If you’re used to MS Office, the interface feels almost identical — super easy to adapt.</p><p>Brave / Zen → When I need a Chromium-based browser, I use Brave; when I need a Firefox-based one, Zen. Both are top-tier.</p><p>Okular → Opens everything from PDFs to EPUBs.</p><p>yt-dlp → Downloads videos and audio straight from the terminal — and not just from YouTube, it supports tons of platforms.</p><p>Qbittorrent → Clean, simple, and easily the best torrent client out there.</p><p>Stremio + Add-ons → The best torrent-based media player, hands down.</p><p>KeepassXC → A simple yet powerful password manager with browser integration.</p><p>LocalSend → Transfers files across all your devices locally, no internet needed.</p><p>KDE Connect → Perfect bridge between your phone and computer.</p><p>Bottles → Makes using Wine more stable and user-friendly.</p><p>Espanso → Expands text shortcuts automatically — a real time-saver.</p><p>Tmux → Lets you split your terminal and run multiple sessions at once.</p><p>Btop / ytop / glances → Displays system resource usage right from the terminal.</p><p>Fastfetch → A faster Neofetch alternative for system info.</p><p>Syncthing → Syncs your files seamlessly between devices.</p><p>Czkawka → Finds duplicate or junk files on your disk.</p><p>Mpv + Plugins → Lightweight, scriptable video player.</p><p>Input Leap → Control multiple computers with one keyboard and mouse.</p><p>Zapret → Bypasses DPI-based network restrictions.</p><p>Moonlight / Sunshine → Stream your games locally across your network.</p><p>Heroic Games Launcher → Great alternative for Epic Games.</p><p>Lutris → Customizable launcher supporting multiple game libraries.</p><p>Prism Launcher → Clean, mod- and shader-friendly Minecraft launcher.</p><p>Ente Auth → The best 2FA app I’ve tried — encrypted sync between devices.</p><p>GDU → Visual disk usage analyzer.</p><p>Newsboat → Read RSS feeds directly in the terminal.</p><p>Neovim → Fast, lightweight text editor.</p><p>Waypaper / Swaybg / Hyprpaper → Manage your wallpapers easily.</p><p>Easy Effects → Lets you tweak and filter your system’s audio.</p><p>Waybar (+ eww + rofi) → Build a fully customizable system bar.</p><p>scrcpy → The simplest way to mirror your Android screen on your PC.</p><p>Podman / Distrobox → Run another Linux environment inside a container.</p><p>Wireshark / mitmproxy → Monitor and analyze your network traffic.</p><p>Opensnitch → See which apps are making network connections.</p><p>qutebrowser → A minimalist, keyboard-driven browser.</p><p>fail2ban → The most satisfying way to troll persistent brute-forcers.</p><p>qemu + Virt-Manager → Create and manage virtual machines easily.</p><p>Waydroid → Run Android apps directly on Linux.</p><p>Lf → Terminal-based file manager.</p><p>These are the tools I’ve discovered and personally enjoy using on Linux. What about yours what are your must-have apps?</p>","contentLength":2737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Futurelock - Subtle Risk in async Rust","url":"https://rfd.shared.oxide.computer/rfd/0609","date":1761942015,"author":"/u/-Y0-","guid":323844,"unread":true,"content":"<div data-lineno=\"424\"><p>Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.</p></div><div data-lineno=\"426\"><p>In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .</p></div><div data-lineno=\"428\"><p>As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:</p></div><div data-lineno=\"430\"><div data-lineno=\"1\"><p>Channel limits, channel limits: always wrong!</p></div><div data-lineno=\"3\"><p>Some too short and some too long!</p></div></div><div data-lineno=\"434\"><p>But as with timeouts, it’s often possible to find values that work in practice.</p></div><div data-lineno=\"436\"><p>Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.</p></div>","contentLength":1894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ol566h/futurelock_subtle_risk_in_async_rust/"},{"title":"John Carmack on mutable variables","url":"https://twitter.com/id_aa_carmack/status/1983593511703474196","date":1761938789,"author":"/u/iamkeyur","guid":323795,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ol3unj/john_carmack_on_mutable_variables/"},{"title":"Meta, xAI Starting Trend for Billions in Off-Balance Sheet Debt","url":"https://www.bloomberg.com/news/articles/2025-10-31/meta-xai-starting-trend-for-billions-in-off-balance-sheet-debt","date":1761938322,"author":"/u/esporx","guid":324145,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ol3nls/meta_xai_starting_trend_for_billions_in/"},{"title":"Write PostgreSQL functions in Go Golang example","url":"https://www.reddit.com/r/golang/comments/1ol2tqv/write_postgresql_functions_in_go_golang_example/","date":1761936351,"author":"/u/WinProfessional4958","guid":323712,"unread":true,"content":"<p>It took me a while to figure this out. Go compiles the C files automatically.</p><pre><code>#include \"postgres.h\" #include \"fmgr.h\" PG_MODULE_MAGIC; extern int32 Adder(int32); PG_FUNCTION_INFO_V1(add_two); Datum add_two(PG_FUNCTION_ARGS) { int32 arg = PG_GETARG_INT32(0); PG_RETURN_INT32(Adder(arg)); } </code></pre><pre><code>package main /* #cgo CFLAGS: -DWIN32 -ID:/pg18headers -ID:/pg18headers/port/win32 #cgo LDFLAGS: -LD:/pg18lib #include \"postgres.h\" #include \"fmgr.h\" // Forward declare the C function so cgo compiles add_two.c too. extern void init_add_two(); */ import \"C\" //export Adder func Adder(a C.int32) C.int32 { return a + 3 } func main() {} </code></pre><p><code>PS D:\\C\\myextension&gt; go build -o add_two.dll -buildmode=c-shared</code></p><p>In PostgreSQL: open the query window (adjust path to your generated dynamically loaded library and header file (.dll, .h).</p><p><code>CREATE FUNCTION add_two(int4) RETURNS int4</code></p><p><code>AS 'D:/C/myextension/add_two.dll', 'add_two'</code></p>","contentLength":893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go vs Kotlin: Server throughput","url":"https://www.reddit.com/r/golang/comments/1ol1upp/go_vs_kotlin_server_throughput/","date":1761934095,"author":"/u/iG0tB00ts","guid":323711,"unread":true,"content":"<p>Let me start off by saying I'm a big fan of Go. Go is my side love while Kotlin is my official (work-enforced) love. I recognize benchmarks do not translate to real world performance &amp; I also acknowledge this is the first benchmark I've made, so mistakes are possible.</p><p>That being said, I was recently tasked with evaluating Kotlin vs Go for a small service we're building. This service is a wrapper around Redis providing a REST API for checking the existence of a key.</p><p>With a load of 30,000 RPS in mind, I ran a benchmark using  (the workload is a list of newline separated 40chars string) and saw to my surprise Kotlin outperforming Go by ~35% RPS. Surprise because my thoughts, few online searches as well as AI prompts led me to believe Go would be the winner due to its lightweight and performant goroutines.</p><p>Go + net/http + go-redis <code>Text Thread Stats Avg Stdev Max +/- Stdev Latency 4.82ms 810.59us 38.38ms 97.05% Req/Sec 5.22k 449.62 10.29k 95.57% 105459 requests in 5.08s, 7.90MB read Non-2xx or 3xx responses: 53529 Requests/sec: 20767.19 </code> Kotlin + ktor + lettuce <code> Thread Stats Avg Stdev Max +/- Stdev Latency 3.63ms 1.66ms 52.25ms 97.24% Req/Sec 7.05k 0.94k 13.07k 92.65% 143105 requests in 5.10s, 5.67MB read Non-2xx or 3xx responses: 72138 Requests/sec: 28057.91 </code></p><p>I am in no way an expert with the Go ecosystem, so I was wondering if anyone had an explanation for the results or suggestions on improving my Go code. ```Go package main</p><p>import ( \"context\" \"net/http\" \"runtime\" \"time\"</p><pre><code>\"github.com/redis/go-redis/v9\" </code></pre><p>var ( redisClient *redis.Client )</p><p>func main() { redisClient = redis.NewClient(&amp;redis.Options{ Addr: \"localhost:6379\", Password: \"\", DB: 0, PoolSize: runtime.NumCPU() * 10, MinIdleConns: runtime.NumCPU() * 2, MaxRetries: 1, PoolTimeout: 2 * time.Second, ReadTimeout: 1 * time.Second, WriteTimeout: 1 * time.Second, }) defer redisClient.Close()</p><pre><code>mux := http.NewServeMux() mux.HandleFunc(\"/\", handleKey) server := &amp;http.Server{ Addr: \":8080\", Handler: mux, } server.ListenAndServe() // some code for quitting on exit signal </code></pre><p>// handleKey handles GET requests to /{key} func handleKey(w http.ResponseWriter, r *http.Request) { path := r.URL.Path</p><pre><code>key := path[1:] exists, _ := redisClient.Exists(context.Background(), key).Result() if exists == 0 { w.WriteHeader(http.StatusNotFound) return } </code></pre><p>Kotlin code for reference ```Kotlin // application</p><p>fun main(args: Array&lt;String&gt;) { io.ktor.server.netty.EngineMain.main(args) }</p><p>fun Application.module() { val redis = RedisClient.create(\"redis://localhost/\"); val conn = redis.connect() configureRouting(conn) }</p><p>fun Application.configureRouting(connection: StatefulRedisConnection&lt;String, String&gt;) { val api = connection.async()</p><pre><code>routing { get(\"/{key}\") { val key = call.parameters[\"key\"]!! val exists = api.exists(key).await() &gt; 0 if (exists) { call.respond(HttpStatusCode.OK) } else { call.respond(HttpStatusCode.NotFound) } } } </code></pre>","contentLength":2876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is there anything like the surface pro and go that fully supports linux?","url":"https://www.reddit.com/r/linux/comments/1ol1ekb/is_there_anything_like_the_surface_pro_and_go/","date":1761933082,"author":"/u/ijwgwh","guid":323730,"unread":true,"content":"<p>Can't stand Windows, but my surface devices are amazing hardware-wise. Surface linux has come a long way, but not having cameras is a deal-breaker for me. Is there any hardware slim sleek and powerful that fully supports Linux? Looking for tablet style, not those laptops where the keyboard turns all the way around. </p><p>ETA: looking for X86 I5+ or equivalent</p>","contentLength":355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"html/template: Why does it escape opening angle bracket?","url":"https://www.reddit.com/r/golang/comments/1ol1cyi/htmltemplate_why_does_it_escape_opening_angle/","date":1761932979,"author":"/u/cvilsmeier","guid":323681,"unread":true,"content":"<div><p>Hi, html/template escapes input data, but why does it escape an angle bracket character (\"&lt;\") in the template? Here is an example:</p><pre><code>package main import ( \"fmt\" \"html/template\" \"strings\" ) func main() { text := \"&lt;{{.tag}}&gt;\" tp := template.Must(template.New(\"sample\").Parse(text)) var buf strings.Builder template.Must(nil, tp.Execute(&amp;buf, map[string]any{\"tag\": template.HTML(\"p\")})) fmt.Println(buf.String()) // Expected output: &lt;p&gt; // Actual output: &amp;lt;p&gt; } </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/cvilsmeier\"> /u/cvilsmeier </a>","contentLength":491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI has made Google more profitable when people expected the contrary","url":"https://peakd.com/@malopie/ai-has-made-google-more-profitable-when-people-expected-the-contrary-nn","date":1761931466,"author":"/u/renkure","guid":323710,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ol0p6t/ai_has_made_google_more_profitable_when_people/"},{"title":"C3 0.7.7 Vector ABI changes, RISC-V improvements and more","url":"https://c3-lang.org/blog/c3-language-at-0-7-7-vector-abi,-riscv-improvements-and-more/","date":1761928654,"author":"/u/Nuoji","guid":323794,"unread":true,"content":"<p>0.7.7 is a major advance in C3 usability with vector ABI changes. It also contains several small quality-of-life additions, such as the ability to splat structs into an initializer, and implicit subscript dereferencing. Fairly few bugs were discovered during this development cycle, which is why the fixed bugs are unusually low.</p><p>Let’s look at what 0.7.7 brings in more detail:</p><p>The most significant change in this release is the ABI change for vectors, which now store and pass vectors as arrays in function calls and structs. While vectors still use SIMD, their equality to arrays on the ABI level means that C graphical libraries will directly match vector types.</p><p>Where before you needed to work with C structs defining vectors and then converting them to SIMD vectors for actual computation, it now works out of the box. Another problem with vectors prior to 0.7.7 was their space and alignment requirements over structs. From 0.7.7 alignment matches that of structs and arrays, making them extremely convenient to work with.</p><p>For cases where SIMD vectors are actually expected, it’s possible to create distinct types using  with a new  attribute to exactly match standard C SIMD vectors, e.g. <code dir=\"auto\">typedef V4si = int[&lt;4&gt;] @simd;</code>. This then exactly matches the corresponding C SIMD type.</p><p>This makes it easier than ever to use SIMD with C3.</p><div><figure><pre data-language=\"c3\"><code></code></pre></figure></div><h2>Struct initializer splats</h2><p>This feature enables using the splat operator  to give a designated initializer default values that are overridden by the following arguments.</p><div><figure><pre data-language=\"c3\"><code></code></pre></figure></div><p>When passing arrays or lists by reference, the  operator tend to behave in an undesirable way, dereferencing the pointer instead of the underlying array/list:</p><div><figure><pre data-language=\"c3\"><code></code></pre></figure></div><p>Subscript deref addresses this. Using  will dereference :</p><div><figure><pre data-language=\"c3\"><code></code></pre></figure></div><p>This is helpful when writing macros and such that will want to accept both elements by reference and by value:</p><div><figure><pre data-language=\"c3\"><code></code></pre></figure></div><p>A new feature for  is to allow creating a type with a specific alignment without wrapping it in a struct. We may, for example, create an integer that is 16 bit aligned using <code dir=\"auto\">typedef Int2 = int @align(2);</code>. This is an alternative way to safely work with references to under-aligned members in packed structs.</p><div><figure><pre data-language=\"c3\"><code></code></pre></figure></div><p>, ,  and  macros are added to modify strings at compile time efficiently for certain macro manipulation at compile time.</p><div><figure><pre data-language=\"c3\"><code></code></pre></figure></div><h2>Small but important changes</h2><p>Aliases that refer to  variables must themselves have local visibility.  is renamed  as it was frequently misunderstood. Generic inference now works better in initializers. For slices with the  syntax, it’s now possible to have the end index be one less than the starting index, so that zero size slices can be expressed with the  syntax as well.</p><p>This release significantly strengthens C3C’s cross-platform capabilities, particularly for RISC-V architecture support. It’s now possible to set individual CPU features using , e.g. . For RISC-V,  has been added, as well as renaming the RISC-V abi flag to the more correct .</p><p>The sorting macros accidentally only took non-slices by value, which would work in some cases but not in others. This has been fixed, but might mean that some code needs to update as well. TcpSocketPair was added to the tcp module to create a bidirectional local socket pair, and using sockets on Windows should now implicitly initialize the underlying socket subsystem.</p><p>0.7.7 has only about 11 fixes, which reflects the relatively few bugs encountered in the 0.7.7 cycle. There are outstanding bugs on the inline asm, which has a significant update planned. The most important fix is patching a regression for MacOS which prevented backtrace printing.</p><p>With the updated Vector ABI and the change from  to  there are a lot of vendor libraries that will need a refresh. There is also a new matrix library in development that hopefully might get included in the next release. There is more functionality to add for fine-tuning processor capabilities for both RISC-V, but also AArch64. There have also been requests for 32-bit Arm support, but the lack of CI tests for different Arm processors is blocking it at the moment.</p><p>This release wouldn’t have been possible without the C3 community. I’d like to extend a deep thank you to all who have contributed, both through filed issues, PRs and just plain discussions.</p><p>Have questions? Come and chat with us on <a href=\"https://discord.gg/qN76R87\">Discord</a>.</p><p>Discuss this article on <a href=\"https://www.reddit.com/r/programming/comments/1okzgsu/c3_077_vector_abi_changes_riscv_improvements_and/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button\">Reddit</a>.</p>","contentLength":4297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okzgsu/c3_077_vector_abi_changes_riscv_improvements_and/"},{"title":"Project goals for 2025H2 | Rust Blog","url":"https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/","date":1761927825,"author":"/u/Kobzol","guid":323679,"unread":true,"content":"<p>On Sep 9, we merged <a href=\"https://github.com/rust-lang/rfcs/pull/3849\">RFC 3849</a>, declaring our goals for the \"second half\" of 2025H2 -- well, the last 3 months, at least, since \"yours truly\" ran a bit behind getting the goals program organized.</p><p>In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:</p><ul><li>, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .</li><li><strong>Unblocking dormant traits</strong>, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.</li><li><strong>Flexible, fast(er) compilation</strong>, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers.</li><li>, making higher-level usage patterns in Rust easier.</li></ul><p>One of Rust's core value propositions is that it's a \"library-based language\"—libraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a \"second-class citizen\" problem where custom pointer types can't provide the same ergonomic experience as built-in references.</p><p>The \"Beyond the \" initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.</p><h3><a href=\"https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/#unblocking-dormant-traits\" aria-hidden=\"true\"></a>\n\"Unblocking dormant traits\"</h3><p>Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:</p><ul><li><a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./polonius.html\">Polonius</a> will enable new borrowing patterns, and in particular <a href=\"https://github.com/rust-lang/rust/issues/92985\">unblock \"lending iterators\"</a>. Over the last few goal periods, we have identified an \"alpha\" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.</li><li>The <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./next-solver.html\">next-generation trait solver</a> is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.</li><li>The work on <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./evolving-traits.html\">evolving trait hierarchies</a> will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.</li><li>The work to <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./scalable-vectors.html\">expand Rust's  hierarchy</a> will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on <a href=\"https://github.com/rust-lang/rfcs/pull/3729\">RFC #3729</a> and <a href=\"https://github.com/rust-lang/rfcs/pull/3838\">RFC #3838</a>, authored in previous Project goal periods.</li><li><a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./in-place-initialization.html\">In-place initialization</a> allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.</li></ul><p>The \"Flexible, fast(er) compilation\" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:</p><p>People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often \"surprisingly tolerable\" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:</p><ul><li>We aim to stabilize <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./cargo-script.html\">cargo script</a>, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.</li><li>We aim to finalize the design of <a href=\"https://rust-lang.github.io/rust-project-goals/2025h2/./ergonomic-rc.html\">ergonomic ref-counting</a> and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.</li></ul><p>For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.</p><p>Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the <a href=\"https://blog.rust-lang.org/inside-rust/2025/06/30/program-management-update-2025-06/\">Rust program manager</a> hired by the Leadership Council, will join me in running the program.</p>","contentLength":6396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1okz3ti/project_goals_for_2025h2_rust_blog/"},{"title":"I compiled my research on modern bot detection into a deep-dive on multi-layer fingerprinting (TLS/JA3, Canvas, Biometrics)","url":"https://pydoll.tech/docs/deep-dive/fingerprinting/","date":1761926578,"author":"/u/thalissonvs","guid":323812,"unread":true,"content":"<p>This module covers browser and network fingerprinting, a critical aspect of modern web automation and detection systems.</p><p>Fingerprinting sits at the intersection of network protocols, cryptography, browser internals, and behavioral analysis. It encompasses the techniques used to identify and track devices, browsers, and users across sessions without relying on traditional identifiers like cookies or IP addresses.</p><p>Every browser connection to a website exposes multiple characteristics, from the precise order of TCP options in network packets, to GPU-specific canvas rendering, to JavaScript execution timing patterns. Individually, these characteristics may appear innocuous. Combined, they create a fingerprint that can uniquely identify a device or browser instance.</p><p>For automation engineers, bot developers, and privacy-conscious users, understanding fingerprinting is essential for building effective detection evasion systems and understanding how tracking mechanisms operate at a technical level.</p><div><p>Multi-Layer Detection Systems</p><p>Modern anti-bot systems employ comprehensive analysis across multiple layers:</p><ul><li>: TCP/IP stack behavior, TLS handshake patterns, HTTP/2 settings</li><li>: Canvas rendering, WebGL vendor strings, JavaScript property enumeration</li><li>: Mouse movement entropy, keystroke timing, scroll patterns</li></ul><p>A single inconsistency (such as a Chrome User-Agent with Firefox TLS fingerprint) can trigger immediate blocking.</p></div><h2>Module Scope and Methodology</h2><p>Fingerprinting techniques are documented across multiple sources with varying levels of accessibility and reliability:</p><ul><li>Academic papers (often paywalled and theoretical)</li><li>Browser source code (millions of lines to analyze)</li><li>Security researcher blogs (technical but fragmented)</li><li>Anti-bot vendor whitepapers (marketing-focused, details omitted)</li><li>Underground forums (practical but unreliable)</li></ul><p>This module centralizes, validates, and organizes this knowledge into a cohesive technical guide. Every technique described here has been:</p><ul><li> against browser source code and RFCs</li><li> in real automation scenarios</li><li> with authoritative references</li><li> from first principles to implementation  </li></ul><p>This module is organized into three progressive layers, from network fundamentals to practical evasion techniques:</p><h3>1. Network-Level Fingerprinting</h3><p>Covers device identification through network behavior at the transport and session layers, before browser rendering begins.</p><ul><li>: TTL, window size, option ordering</li><li>: JA3/JA4, cipher suites, ALPN negotiation</li><li>: SETTINGS frames, priority patterns</li><li>: p0f, Nmap, Scapy, tshark analysis</li></ul><p>: Network fingerprints are the most challenging to spoof because they require OS-level modifications. Inconsistencies at this layer are detected before JavaScript execution begins.</p><h3>2. Browser-Level Fingerprinting</h3><p>Examines browser identification through JavaScript APIs, rendering engines, and plugin ecosystems at the application layer.</p><ul><li><strong>Canvas &amp; WebGL fingerprinting</strong>: GPU-specific rendering artifacts</li><li>: Subtle differences in audio API output</li><li>: Installed fonts reveal OS and locale</li><li>: Navigator object, screen dimensions, timezone</li><li>: Accept-Language, User-Agent consistency</li></ul><p>: This layer accounts for the majority of detection events. Even with correct network-level fingerprints, exposed automation properties (e.g., ) can trigger blocking.</p><h3>3. Behavioral Fingerprinting</h3><p>Analyzes user interaction patterns to distinguish human behavior from automated systems.</p><ul><li>: Trajectory curvature, velocity profiles, Fitts's Law compliance</li><li>: Typing rhythm, dwell time, flight time, bigram patterns</li><li>: Momentum, inertia, deceleration curves</li><li>: Natural interaction ordering (mousemove → click), timing analysis</li><li>: ML models trained on billions of behavioral signals</li></ul><p>: Behavioral analysis can detect automation even when network and browser fingerprints are correctly spoofed. This layer is particularly challenging because it requires replicating biomechanical human behavior patterns.</p><p>Practical implementation of fingerprinting evasion using Pydoll's CDP integration, JavaScript overrides, and architectural features.</p><ul><li>: Timezone, geolocation, device metrics</li><li><strong>JavaScript property overrides</strong>: Redefining navigator objects, canvas poisoning</li><li>: Forcing header consistency</li><li>: Human-like timing, entropy injection</li><li>: Tools to validate your evasion setup</li></ul><p>: This section demonstrates practical application of fingerprinting concepts to real automation scenarios, integrating techniques from all previous layers.</p><h3><strong>You MUST read this if you're:</strong></h3><ul><li>Building automation that interacts with anti-bot protected sites</li><li>Developing scraping infrastructure at scale</li><li>Implementing privacy-preserving browser automation</li><li>Researching bot detection for offensive or defensive purposes</li></ul><h3><strong>This is advanced material if you're:</strong></h3><ul><li>A \"silver bullet\" anti-detection solution (no such thing exists)</li><li>A replacement for respecting robots.txt and rate limits</li></ul><p>Fingerprinting defense is <strong>not about becoming invisible</strong>—it's about becoming <strong>indistinguishable from legitimate traffic</strong>. This means:</p><ol><li><strong>Consistency over perfection</strong>: A perfectly configured Firefox fingerprint is better than a \"perfect\" but inconsistent Chrome fingerprint</li><li>: You must align network, browser, and behavioral layers</li><li>: Fingerprinting techniques evolve monthly; this is a living document</li></ol><div><p><strong>Every layer must tell the same story.</strong> If your TLS fingerprint says \"Chrome 120\", your HTTP/2 settings must match Chrome 120, your User-Agent must say Chrome 120, and your canvas rendering must produce Chrome 120 artifacts. One mismatch = detection.</p></div><p>Fingerprinting knowledge is :</p><ul><li>: Protect your privacy from invasive tracking</li><li>: Evade detection systems for automation</li></ul><p>We trust you to use this knowledge <strong>responsibly and ethically</strong>:</p><ul><li>Respect website terms of service</li><li>Implement rate limiting and respectful crawling patterns</li><li>Evaluate whether automation is necessary</li><li>Be transparent when appropriate</li></ul><ul><li>Fraud, account abuse, or illegal activities</li><li>Overwhelming servers with aggressive scraping</li><li>Weaponizing this knowledge without understanding consequences  </li></ul><p>Fingerprinting is a complex and technical domain that requires systematic study. Understanding these techniques is essential for effective web automation in environments with detection systems.</p><div><p>This module represents  combining academic papers, browser source code, real-world testing, and community knowledge. Every claim is cited and validated. If you find inaccuracies or have updates, contributions are welcome.</p></div><p>Before diving in, consider these complementary topics:</p>","contentLength":6363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okyk2z/i_compiled_my_research_on_modern_bot_detection/"},{"title":"mariadb-operator 📦 25.10 is out: asynchronous replication goes GA, featuring automated replica recovery! 🎃","url":"https://github.com/mariadb-operator/mariadb-operator/releases/tag/25.10.2","date":1761926277,"author":"/u/mmontes11","guid":323600,"unread":true,"content":"<p>We are thrilled to announce that our highly available topology based on MariaDB native replication is now generally available, providing an alternative to our existing synchronous multi-master topology based on Galera.</p><p>In this topology, a single primary server handles all write operations, while one or more replicas replicate data from the primary and can serve read requests. More precisely, the primary has a binary log and the replicas asynchronously replicate the binary log events over the network.</p><p>Getting a replication cluster up and running is as easy as applying the following  resource:</p><pre><code>apiVersion: k8s.mariadb.com/v1alpha1 kind: MariaDB metadata: name: mariadb-repl spec: storage: size: 1Gi storageClassName: rook-ceph replicas: 3 replication: enabled: true </code></pre><p>The operator provisions a replication cluster with one primary and two replicas. It automatically sets up replication, configures the replication user, and continuously monitors the replication status. This status is used internally for cluster reconciliation and can also be inspected through the  subresource for troubleshooting purposes.</p><p>Whenever the primary Pod goes down, a reconciliation event is triggered on the operator's side, and by default, it will initiate a primary failover operation to the furthest advanced replica. This can be controlled by the following settings:</p><pre><code>apiVersion: k8s.mariadb.com/v1alpha1 kind: MariaDB metadata: name: mariadb-repl spec: replicas: 3 replication: enabled: true primary: autoFailover: true autoFailoverDelay: 0s </code></pre><p>In this situation, the following status will be reported in the  CR:</p><pre><code>kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl False Switching primary to 'mariadb-repl-1' mariadb-repl-0 ReplicasFirstPrimaryLast 2m7s kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl True Running mariadb-repl-1 ReplicasFirstPrimaryLast 2m42s </code></pre><p>To select a new primary, the operator evaluates each candidate based on Pod readiness and replication status, ensuring that the chosen replica has no pending relay log events (i.e. all binary log events have been applied) before promotion.</p><p>One of the spookiest 🎃 aspects of asynchronous replication is when replicas enter an error state under certain conditions. For example, if the primary purges its binary logs and the replicas are restarted, the binary log events requested by a replica at startup may no longer exist on the primary, causing the replica’s I/O thread to fail with error code .</p><p>Luckily enough, this operator has you covered! It automatically detects this situation and triggers a recovery procedure to bring replicas back to a healthy state. To do so, it schedules a  from a ready replica and restores it into the data directory of the faulty one.</p><p>The  object, introduced in <a href=\"https://www.reddit.com/r/kubernetes/comments/1m8v9aq/mariadboperator_25080_has_landed_physicalbackups/\">previous releases</a>, supports taking consistent, point-in-time volume snapshots by leveraging the  API. In this release, we’re eating our own dog food: our internal operations, such as replica recovery, are powered by the  construct. This abstraction not only streamlines our internal operations but also provides flexibility to adopt alternative backup strategies, such as using  (MariaDB native) instead of  (Kubernetes native).</p><p>To set up replica recovery, you need to define a  template that the operator will use to create the actual  object during recovery events. Then, it needs to be configured as a source of restoration inside the replication section:</p><pre><code>apiVersion: k8s.mariadb.com/v1alpha1 kind: MariaDB metadata: name: mariadb-repl spec: storage: size: 1Gi storageClassName: rook-ceph replicas: 3 replication: enabled: true primary: autoFailover: true autoFailoverDelay: 0s replica: bootstrapFrom: physicalBackupTemplateRef: name: physicalbackup-tpl recovery: enabled: true errorDurationThreshold: 5m --- apiVersion: k8s.mariadb.com/v1alpha1 kind: PhysicalBackup metadata: name: physicalbackup-tpl spec: mariaDbRef: name: mariadb-repl schedule: suspend: true storage: volumeSnapshot: volumeSnapshotClassName: rook-ceph </code></pre><p>Let’s assume that the  replica enters an error state, with the I/O thread reporting error code :</p><pre><code>kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl False Recovering replicas mariadb-repl-1 ReplicasFirstPrimaryLast 11m kubectl get physicalbackup NAME COMPLETE STATUS MARIADB LAST SCHEDULED AGE ..replica-recovery True Success mariadb-repl 14s 14s kubectl get volumesnapshot NAME READYTOUSE SOURCEPVC SNAPSHOTCLASS AGE ..replica-recovery-20251031091818 true storage-mariadb-repl-2 rook-ceph 18s kubectl get mariadb NAME READY STATUS PRIMARY UPDATES AGE mariadb-repl True Running mariadb-repl-1 ReplicasFirstPrimaryLast 11m </code></pre><p>As you can see, the operator detected the error, triggered the recovery process and recovered the replica using a  taken in a ready replica, all in a matter of seconds! The actual recovery time may vary depending on your data volume and your CSI driver.</p><p>Huge thanks to everyone who contributed to making this feature a reality, from writing code to sharing feedback and ideas. Thank you!</p>","contentLength":5032,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1okyez2/mariadboperator_2510_is_out_asynchronous/"},{"title":"Horror Coding Stories: Therac-25 — A deadly race condition and overflow","url":"https://read.thecoder.cafe/p/therac-25","date":1761924353,"author":"/u/teivah","guid":323845,"unread":true,"content":"<p><em>Last updated: March 9, 2025</em></p><p><em>Welcome to The Coder Cafe! Today, we examine the Therac-25 accidents, where design and software failures resulted in multiple radiation overdoses and deaths. Make sure to check the Explore Further section to see if you’re able to reproduce the deadly issue. Get cozy, grab a pumpkin spice latte, and let’s begin!</em></p><p>Treating cancers used to require a mix of machines, depending on tumor depth: shallow or deep. In the early 1980s, a new generation promised both from a single system. That was a big deal for hospitals: one machine instead of several meant lower maintenance and fewer systems to manage.</p><p>That was the case with the Therac-25.</p><p>The Therac-25 offered two therapies with selectable modes:</p><ul></ul><p>Earlier Therac models allowed switching modes with hardware circuits and physical interlocks. The new version was smaller, cheaper, and computer-controlled. Less hardware and fewer parts meant lower costs.</p><p>However, what no one realized soon enough: it also removed an independent safety net.</p><p>On a routine day, a radiology technologist sat at the console and began entering a plan:</p><ul><li><p>By habit, she selected X-ray (deep mode).</p></li><li><p>Then she immediately corrected it for Electron (shallow mode) and hit start.</p></li></ul><p>The patient was receiving his ninth treatment. Immediately, he knew something was different. He reported a buzzing sound, later recognized as the accelerator pouring out radiation at maximum. The pain came fast; paralysis followed. He later died from radiation injury.</p><p>Weeks later, a second patient endured the same incident on the same model.</p><pre><code>MODE:              X\n...                ▮\nBEAM READY:        </code></pre><pre><code>MODE:              E\n...                ▮\nBEAM READY:        </code></pre><pre><code><code>MODE:              E\n...                \nBEAM READY:        ▮</code></code></pre><p>From her perspective, the screen showed the corrected mode, so she hit return and started the treatment:</p><pre><code><code>MODE:              E\n...                \nBEAM READY:        ▮&lt;Enter&gt;</code></code></pre><p>Behind the scenes, the Therac-25 software ran several concurrent tasks:</p><ul></ul><p>Because both tasks read the same memory with no mutual exclusion, there was a short window (on the order of seconds) in which the hardware-control task used a different value than the one displayed on the screen.</p><ul><li><p>The UI showed Electron mode, which looked correct to the operator.</p></li><li><p>The hardware-control task had snapshotted stale data and marked the system as ready even though critical elements (e.g., turntable position, scanning magnets/accessories) were not yet aligned with electron mode.</p></li><li><p>When treatment was started, the machine delivered an effectively unscanned, high-intensity electron beam, causing a massive overdose.</p></li></ul><p><a href=\"https://read.thecoder.cafe/p/data-race-vs-race-condition\" rel=\"\">race condition</a></p><p>The manufacturer later confirmed the error could not be reproduced reliably in testing. The timing had to line up just right, which made the bug elusive. They initially misdiagnosed it as a hardware fault and applied only minor fixes. Unfortunately, the speed of operator editing was the key trigger that exposed this software race.</p><p>The problem could have stopped here, but it didn’t.</p><p>Months later, another fatal overdose occurred, this time caused by a different software defect. It wasn’t a timing race. This time, the issue was a counter overflow within the control program.</p><p>The software used an internal counter to track how many times certain setup operations ran. After the counter exceeded its maximum value, it wrapped back to zero. That arithmetic overflow created a window where a critical safety check was bypassed, allowing the beam to turn on without the proper accessories in place.</p><p>Again, the Therac-25 fired a high-intensity beam without the proper hardware configuration.</p><p>Both the race condition and the counter overflow stemmed from the same design flaw: the belief that software alone could enforce safety. The Therac-25 showed, in tragic terms, that without independent safeguards, small coding errors can have catastrophic consequences. </p><p>We should know that whether it’s software, hardware, or a human process, every single safeguard has inherent flaws. Therefore, in complex systems, safety should be layered, as illustrated by the Swiss cheese model:</p><p>In total, there were six known radiation overdoses involving the Therac-25, and at least three were fatal.</p><p>You can run the UI using Docker:</p><pre><code> docker run --rm -it -e TERM=xterm-256color teivah/therac-25</code></pre><ul></ul><p><em>If you enjoyed this post, please hit the like button.</em></p><p><em>Any other horror coding stories you want to share?</em></p>","contentLength":4405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okxks7/horror_coding_stories_therac25_a_deadly_race/"},{"title":"[D] How to benchmark open-ended, real-world goal achievement by computer-using LLMs?","url":"https://www.reddit.com/r/MachineLearning/comments/1okwuyx/d_how_to_benchmark_openended_realworld_goal/","date":1761922710,"author":"/u/ExplorAI","guid":323630,"unread":true,"content":"<p><a href=\"https://arxiv.org/abs/2510.04374\">GDPVal</a> takes care of measuring agent performance on economically valuable tasks. We are working on the <a href=\"https://theaidigest.org/village\">AI Village</a>, where we try to see how we can explore, and possibly evaluate, how groups of persistent agents do at open-ended, real-world tasks in general. We're currently running all the frontier LLMs (OpenAI, Anthropic, DeepMind) with their own computer, internet access, and a group chat, and we give them goals like <a href=\"https://theaidigest.org/village/blog/season-recap-agents-raise-2k\">raising money for charity</a>, <a href=\"https://theaidigest.org/village/blog/season-2-recap-ai-organizes-event\">organizing an event</a>, or <a href=\"https://theaidigest.org/village/blog/im-gemini-i-sold-t-shirts\">selling t-shirts online</a>. We had the agents try to <a href=\"https://x.com/aidigest_/status/1960750163406021048\">invent their own benchmark</a> for themselves, but this led to them writing a lot of words, and doing almost no actions, but declaring themselves amazing at the benchmark. Gemini 2.5 Pro did manage to make something like a podcast and a \"documentary\" but these were pretty rudimentary attempts.</p><p><em>I'm curious what ideas people here might have. Say you had a persistent multi-agent system, where each LLM is using a computer and trying to achieve goals: What goals would be interesting to give them? How would you compare the agents? What tools would you give them? What are the main things you'd be excited to explore?</em></p><p>Some examples of insights we got so far, in case that helps kick-start conversation :)</p><p>- Hallucinations and lack of situational awareness have hampered o3 a lot, resulting in it performing quite badly on goals that require real-world action. Meanwhile, it does really well on \"talking\" goals like winning the most debates during a formal debate season.</p><p>- Computer use skills combined with temperament often lead Gemini 2.5 Pro to give up on achieving goals while other (sometimes less capable agents) keep working regardless. It seems to disproportionally assign its own errors (e.g. misclicks) to the environment and then decide it's all hopeless.</p><p>- Document sharing is surprisingly hard, and so is playing online games. Meanwhile, they've made nice websites for themselves and do well on Twitter (if given an account and reminded of its existence). I'm not sure entirely sure why this pattern is emerging.</p>","contentLength":2037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fedora KDE appreciation","url":"https://www.reddit.com/r/linux/comments/1okwubl/fedora_kde_appreciation/","date":1761922665,"author":"/u/sukuiido","guid":323709,"unread":true,"content":"<p>I just wanted to express my appreciation for the team behind Fedora KDE. When I first installed this on my daily driver laptop, Fedora 41 was brand new. Still going fantastically after 2 point release updates. This distro has halted my distro-hopping for over a year now. It just works.™ Thank you, Fedora team.</p><p>(Additional thanks to ycollet for the audinux copr repo. I make music and everything I need is there.)</p>","contentLength":415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nvidia’s $5 Trillion Storyteller-In-Chief","url":"https://go.forbes.com/3fK34B","date":1761922027,"author":"/u/forbes","guid":323656,"unread":true,"content":"<p>A plaque that hangs above a Denny’s booth in San Jose, California, celebrates the birth of Nvidia, or a “$2 trillion company.” It’s only been two years since the plaque went up, and it’s already in need of an update. </p><p>Nvidia, the chip maker that powers the AI revolution, has just crossed the remarkable milestone of becoming the first <a href=\"https://www.forbes.com/sites/tylerroush/2025/10/29/nvidia-becomes-first-company-worth-5-trillion/\" data-ga-track=\"InternalLink:https://www.forbes.com/sites/tylerroush/2025/10/29/nvidia-becomes-first-company-worth-5-trillion/\" target=\"_self\" aria-label=\"$5 trillion company\">$5 trillion company</a>. But while the company that took shape over coffee and pancakes at Denny’s is now worth considerably more than $2 trillion, the heart of the plaque’s message still resonates: </p><p>“Who knew that an idea started here could change the world?”</p><p>As it turns out, very few people knew that Nvidia started at Denny’s until someone with deep personal knowledge leaked the information—Nvidia CEO, Jensen Huang. </p><p>The story Huang has shared publicly goes like this: </p><figure role=\"presentation\"></figure><p>At the age of 15, Huang started working at Denny’s as a dishwasher, busboy, and waiter. Years later, when Huang was an engineer in Silicon Valley, he and two friends would meet at a Denny’s location near Huang’s home, where the trio would brainstorm ideas for a startup. The booth even functioned as their first office space. </p><p>“It had all the coffee you could drink, and no one would chase you out,” Huang said in an Nvidia <a href=\"https://blogs.nvidia.com/blog/nvidia-dennys-trillion/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\" data-ga-track=\"ExternalLink:https://blogs.nvidia.com/blog/nvidia-dennys-trillion/\" aria-label=\"blog post\">blog post</a> commemorating the event. </p><p>For Huang, the Denny’s story is much more than a funny anecdote. It reflects who he is and the values that mean the most to him.  </p><p>For leaders, the lesson is clear. As your company grows, the more crucial it becomes to share your origin story. A foundational, authentic story inspires others, communicates values, and builds an enduring culture. </p><h2>4 Key Ingredients of a Powerful Origin Story</h2><p>A good idea starts with a spark. Think back to the moment or experience that triggered the idea for your company. It might have been something you read, something you saw, something you witnessed. </p><p>The spark might have been a problem you identified that needed a solution. For Brian Chesky and his roommates, the problem was finding money to pay the rent on this San Francisco apartment. They came up with the idea of renting out air mattresses for people attending a conference in the city. It helped pay the rent and sparked a much more valuable idea: Airbnb.</p><p>Few people can relate to running a multitrillion-dollar company—the size of the audience is very small. But most of us can relate to getting a part-time job to earn some extra spending money or working at the lowest rung on the ladder, as Huang did, clearing tables and taking breakfast orders. </p><p>Stories of struggle, hardship, and humble beginnings are often inspiring because we—the audience—can see ourselves in the leader’s footsteps. The stories give people hope that they, too, can overcome life’s challenges or, with the right attitude and mindset, can become what they imagine themselves to be. </p><p> When Huang shares his experience of working at Denny’s, his stories always come with lessons that reflect his values. </p><p>Huang likes to boast that he was the best busboy the diner had ever seen and that “no one could carry more coffee cups.” Working at the restaurant taught Huang the importance of hard work, hospitality, and humility. “No task was beneath me,” he says. </p><p>Innovation requires all three elements—working incredibly hard, satisfying the customer, and having the humility to admit what you don’t know.</p><p>Huang uses the lessons he learned at Denny’s to explain Nvidia’s culture of cross-functional collaboration, an ethos that encourages managers to roll up their sleeves, get close to the team and their work, and solve problems together. </p><p><strong>Repetition and consistency.</strong> Don’t just tell the origin story once. Repeat it early and often. Consistency builds trust and authenticity with your partners, stakeholders, customers, and teams. </p><p>When a leader consistently shares an origin story over time, it evolves from reflecting one person’s experience to becoming “our story.\" No matter how big Nvidia gets, Huang’s story is a reminder that its cultural norms and values come from the experience of a young busboy who refused to leave a station empty-handed. </p><p>“Culture building is storytelling,” Huang said in <a href=\"https://www.wired.com/story/nvidia-hardware-is-eating-the-world-jensen-huang/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\" data-ga-track=\"ExternalLink:https://www.wired.com/story/nvidia-hardware-is-eating-the-world-jensen-huang/\" aria-label=\"an interview\">an interview</a> for . Leaders who articulate where they came from, and why it matters, don’t just build companies. They build cultures that endure. </p>","contentLength":4340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1okwkpb/nvidias_5_trillion_storytellerinchief/"},{"title":"[P] I build a model to visualise live collision risk predictions for London from historical TFL data","url":"https://www.reddit.com/r/MachineLearning/comments/1okwh3p/p_i_build_a_model_to_visualise_live_collision/","date":1761921798,"author":"/u/AntiFunSpammer","guid":323603,"unread":true,"content":"<div><p> I built a small app that shows live collision risk across London. It learns patterns from historical TfL collision data and overlays risk on an interactive map. Open source, friendly to poke around, and I would love feedback.</p><ul><li>Spatiotemporal risk scoring for London using a fixed spatial grid (H3 hexes) and time context</li><li>Interactive map with a hotspot panel in the top right</li><li>A simple data exploration page and short notes on the model</li></ul><ul><li>I wanted a lightweight, transparent way to explore where and when collision risk trends higher</li><li>Makes it easy to discuss what features help, what does not, and what is misleading</li></ul><ul><li>Historical TfL collision records</li><li>Time aligned context features</li><li>Optional external context like OSM history and weather are supported in the pipeline</li></ul><ul><li>Temporal features like hour of day and day of week with simple sine and cosine encodings</li><li>Spatial features on a hex grid to avoid leaking between nearby points</li><li>Optional neighbor aggregates so each cell has local context</li></ul><ul><li>Start simple so it is easy to debug and explain</li><li>Tree based classifiers with probability calibration so the scores are usable</li><li>Focus on clarity over squeezing the last bit of PR AUC</li></ul><ul><li>Class imbalance is strong, so I look at PR curves, Brier score, and reliability curves</li><li>Spatial or group style cross validation to reduce leakage between nearby hex cells</li><li>Still iterating on split schemes, calibration, and uncertainty</li></ul><ul><li>Backend API that scores tiles for a selected time context</li><li>Map renders tile scores and lets you toggle hotspots from the panel</li><li>Front end is a simple Leaflet app</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/AntiFunSpammer\"> /u/AntiFunSpammer </a>","contentLength":1569,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In your opinion which package is missing or could be better in Go?","url":"https://www.reddit.com/r/golang/comments/1okvxbu/in_your_opinion_which_package_is_missing_or_could/","date":1761920495,"author":"/u/fenugurod","guid":323605,"unread":true,"content":"<div><p>I know \"just contribute to the ones already there\" but I want to experiment a few things and build something from scratch in Go.If you miss something at the Go ecosystem, let me know because I'm really eager to build it. </p></div>   submitted by   <a href=\"https://www.reddit.com/user/fenugurod\"> /u/fenugurod </a>","contentLength":253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are you drowning in AI code review noise? 70% of AI PR comments are useless","url":"https://jetxu-llm.github.io/posts/low-noise-code-review/","date":1761920253,"author":"/u/Jet_Xu","guid":323602,"unread":true,"content":"<div><p>Most AI code review tools generate 10-20 comments per PR. The problem? 80% are noise. Here's a framework for measuring signal-to-noise ratio in code reviews - and why it matters more than you think.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Jet_Xu\"> /u/Jet_Xu </a>","contentLength":227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okvtqp/are_you_drowning_in_ai_code_review_noise_70_of_ai/"},{"title":"Music player closest to modern Winamp UI's realtime queue system","url":"https://www.reddit.com/r/linux/comments/1oku9zl/music_player_closest_to_modern_winamp_uis/","date":1761916467,"author":"/u/Reddit_Zowie_Fan","guid":323604,"unread":true,"content":"<p>In Modern Winamp UIs, whenever you play any track from the library the queue is immediately populated with whatever is in the library view on the left - your entire library, search results, etc - and there's a hotkey to quickly randomise the order of the queue, letting you shuffle your queue while actually seeing what tracks are coming up next, then move those tracks around or queue anything else you want to in the order you desire. After years and years of using Winamp I really struggle to adjust to not having this functionality. It seems to be missing from almost every music player I've tried on Linux thus far. I've tried a lot, and if anyone can suggest something that works this way I'd be very grateful. Gmusicbrowser is the closest I've found, but its age is showing - the version I downloaded off the AUR won't even launch on hyprland and the UI is much uglier than most other players.</p>","contentLength":900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Release Dioxus v0.7.0 · DioxusLabs/dioxus","url":"https://github.com/DioxusLabs/dioxus/releases/tag/v0.7.0","date":1761916205,"author":"/u/DebuggingPanda","guid":323628,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1oku64p/release_dioxus_v070_dioxuslabsdioxus/"},{"title":"The indentation of switch statements really triggers my OCD — why does Go format them like that?","url":"https://www.reddit.com/r/golang/comments/1oktsft/the_indentation_of_switch_statements_really/","date":1761915249,"author":"/u/salvadorsru","guid":323657,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Help regarding the following code snippet","url":"https://www.reddit.com/r/golang/comments/1oktgkq/help_regarding_the_following_code_snippet/","date":1761914392,"author":"/u/Impossible-Act-5254","guid":323900,"unread":true,"content":"<div><pre><code>package main import ( \"fmt\" \"time\" ) func main() { ch := make(chan int, 2) ch &lt;- 1 ch &lt;- 2 fmt.Println(\"receiving from buffer\") go func() { time.Sleep(2 * time.Second) fmt.Println(\"received \", &lt;-ch) }() ch &lt;- 3 } </code></pre><p>the given code sometimes prints :-</p><p>receiving from buffer received 1</p><p>and sometimes it prints :-</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Impossible-Act-5254\"> /u/Impossible-Act-5254 </a>","contentLength":347,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI browsers are a cybersecurity time bomb | Rushed releases, corruptible AI agents, and supercharged tracking make AI browsers home to a host of known and unknown cybersecurity risks.","url":"https://www.theverge.com/report/810083/ai-browser-cybersecurity-problems","date":1761912389,"author":"/u/MetaKnowing","guid":323577,"unread":true,"content":"<div><p>Web browsers are getting awfully chatty. They got even chattier last week after OpenAI and Microsoft kicked the AI browser race into high gear with <a href=\"https://www.theverge.com/ai-artificial-intelligence/803475/openais-ai-powered-browser-chatgpt-atlas-google-chrome-competition-agent\">ChatGPT Atlas</a> and a “<a href=\"https://www.theverge.com/news/805833/microsoft-edge-copilot-mode-ai-launch\">Copilot Mode</a>” for Edge. They can answer questions, summarize pages, and even take actions on your behalf. The experience is <a href=\"https://www.theverge.com/ai-artificial-intelligence/804931/openai-chatgpt-atlas-hands-on-google-search\">far from seamless yet</a>, but it hints at a more convenient, hands-off future where your browser does lots of your thinking for you. That future could also be a minefield of new vulnerabilities and data leaks, cybersecurity experts warn. The signs are already here, and researchers tell  the chaos is only just getting started.</p></div><div><p>Atlas and Copilot Mode are part of a broader land grab to control the gateway to the internet and to bake AI directly into the browser itself. That push is transforming what were once standalone chatbots on separate pages or apps into the very platform you use to navigate the web. They’re not alone. Established players are also in the race, such as Google, which is integrating its <a href=\"https://www.theverge.com/news/795463/google-computer-use-gemini-ai-model-agents\">Gemini AI model</a> into Chrome; Opera, which launched <a href=\"https://www.theverge.com/tech/801899/opera-neon-ai-browser-trial-run\">Neon</a>; and The Browser Company, with <a href=\"https://www.theverge.com/web/685232/dia-browser-ai-arc\">Dia</a>. Startups are also keen to stake a claim, such as AI startup Perplexity — best known for its AI-powered search engine, which made its AI-powered browser <a href=\"https://www.theverge.com/news/790419/perplexity-comet-available-everyone-free\">Comet freely available to everyone in early October</a> — and Sweden’s Strawberry, which is still in beta and <a href=\"https://x.com/charles_maddock/status/1981333225722019946\">actively going after “disappointed Atlas users.”</a></p></div><div><p>In the past few weeks alone, researchers have uncovered <a href=\"https://layerxsecurity.com/blog/layerx-identifies-vulnerability-in-new-chatgpt-atlas-browser/\">vulnerabilities in Atlas</a> allowing attackers to take advantage of ChatGPT’s “memory” to inject malicious code, grant themselves access privileges, or deploy malware. <a href=\"https://brave.com/blog/unseeable-prompt-injections/\">Flaws discovered in Comet</a> could allow attackers to hijack the browser’s AI with hidden instructions. Perplexity, <a href=\"https://www.perplexity.ai/hub/blog/mitigating-prompt-injection-in-comet\">through a blog</a>, and OpenAI’s chief information security officer, Dane Stuckey, acknowledged prompt injections as a big threat last week, though both described them as a “frontier” problem that has no firm solution.</p></div><div><p>“Despite some heavy guardrails being in place, there is a vast attack surface,” says Hamed Haddadi, professor of human-centered systems at Imperial College London and chief scientist at web browser company Brave. And what we’re seeing is just the tip of the iceberg.</p></div><div><p>With AI browsers, the threats are numerous. Foremost, they know far more about you and are “much more powerful than traditional browsers,” says Yash Vekaria, a computer science researcher at UC Davis. Even more than standard browsers, Vekaria says “there is an imminent risk from being tracked and profiled by the browser itself.” AI “memory” functions are designed to learn from everything a user does or shares, from browsing to emails to searches, as well as conversations with the built-in AI assistant. This means you’re probably sharing far more than you realise and the browser remembers it all. The result is “a more invasive profile than ever before,” Vekaria says. Hackers would quite like to get hold of that information, especially if coupled with stored credit card details and login credentials often found on browsers.</p></div><div><p>Another threat is inherent to the rollout of any new technology. No matter how careful developers are, there will inevitably be weaknesses hackers can exploit. This could range from bugs and coding errors that accidentally reveal sensitive data to major security flaws that could let hackers gain access to your system. “It’s early days, so expect risky vulnerabilities to emerge,” says Lukasz Olejnik, an independent cybersecurity researcher and visiting senior research fellow at King’s College London. He points to the “early Office macro abuses, malicious browser extensions, and mobiles prior to [the] introduction of permissions” as examples of previous security issues linked to the rollout of new technologies. “Here we go again.”</p></div><div><p>Some vulnerabilities are never found — sometimes leading to devastating zero-day attacks, named as there are zero days to fix the flaw — but thorough testing can slash the number of potential problems. With AI browsers, “the biggest immediate threat is the market rush,” Haddadi says. “These agentic browsers have not been thoroughly tested and validated.”</p></div><div><p>But AI browsers’ defining feature, AI, is where the worst threats are brewing. The biggest challenge comes with AI agents that act on behalf of the user. Like humans, they’re capable of visiting suspect websites, clicking on dodgy links, and inputting sensitive information into places sensitive information shouldn’t go, but unlike some humans, they lack the learned common sense that helps keep us safe online. Agents can also be misled, even hijacked, for nefarious purposes. All it takes is the right instructions. So-called prompt injections can range from glaringly obvious to subtle, effectively hidden in plain sight in things like images, screenshots, form fields, <a href=\"https://www.theverge.com/news/781746/chatgpt-gmail-shadow-leak\">emails and attachments</a>, and even something as simple as white text on a white background.</p></div><div><p>Worse yet, these attacks can be very difficult to anticipate and defend against. Automation means bad actors can try and try again until the agent does what they want, says Haddadi. “Interaction with agents allows endless ‘try and error’ configurations and explorations of methods to insert malicious prompts and commands.” There are simply far more chances for a hacker to break through when interacting with an agent, opening up a huge space for potential attacks. Shujun Li, a professor of cybersecurity at the University of Kent, says “zero-day vulnerabilities are exponentially increasing” as a result. Even worse: Li says as the flaw starts with an agent, detection will also be delayed, meaning potentially bigger breaches.</p></div><div><p>It’s not hard to imagine what might be in store. Olejnik sees scenarios where attackers use hidden instructions to get AI browsers to send out personal data or steal purchased goods by changing the saved address on a shopping site. To make things worse, Vekaria warns it’s “relatively easy to pull off attacks” given the current state of AI browsers, even with safeguards in place. “Browser vendors have a lot of work to do in order to make them more safe, secure, and private for the end users,” he says.</p></div><div><p>For some threats, experts say the only real way to keep safe using AI browsers is to simply avoid the marquee features entirely. Li suggests people save AI for “only when they absolutely need it” and know what they’re doing. Browsers should “operate in an AI-free mode by default,” he says. If you must use the AI agent features, Vekaria advises a degree of hand-holding. When setting a task, give the agent verified websites you know to be safe rather than letting it figure them out on its own. “It can end up suggesting and using a scam site,” he warns.</p></div><div><ul></ul></div>","contentLength":6820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oksr5v/ai_browsers_are_a_cybersecurity_time_bomb_rushed/"},{"title":"Where do ingress rules exist?","url":"https://www.reddit.com/r/kubernetes/comments/1okskwc/where_do_ingress_rules_exist/","date":1761911905,"author":"/u/SecureTaxi","guid":323495,"unread":true,"content":"<p>I played with a k8s POC a few years ago and dabbled with both the aws load balancer controller and an nginx and project contour one. For the latter i recall all the ingress rules were defined and viewed within the context of the ingress object. One of my guys deployed k8s for a new POC and managed to get everything running with the aws lb controller. However, all the rules were defined within the LB that shows up in the aws console. I think the difference is his is an ALB, whereas i had a NLB which route all traffic into the internal ingress (e.g. nginx). Which way scales better?</p><p>Clarification: 70+ services with a lot of ruleset. Obviously i dont want a bunch of ALB to manage for each service</p>","contentLength":700,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bootstraps and directory structure question","url":"https://www.reddit.com/r/kubernetes/comments/1okr75y/bootstraps_and_directory_structure_question/","date":1761907469,"author":"/u/Altruistic_Cause8661","guid":323843,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"According to Red Hat, Xfce and Cinnamon are Linux distros","url":"https://www.reddit.com/r/linux/comments/1okr534/according_to_red_hat_xfce_and_cinnamon_are_linux/","date":1761907267,"author":"/u/VoidDuck","guid":323499,"unread":true,"content":"<p><em>There are many Linux distros, including:</em></p><ul><li><em>Ubuntu (and all its versions: GNOME, Kubuntu—using KDE’s Plasma desktop, Ubuntu MATE, Xubuntu, and Lubuntu, to name a few)</em></li></ul><p><em>Linux distros vary widely in what they do, how they do it, and how they’re supported. Some are designed as Linux desktop environments―such as Xfce, Raspberry Pi OS, and Cinnamon―while others support back-end IT systems like enterprise or web servers.</em></p>","contentLength":422,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How my Node.js code was causing a massive memory leak and how I solved it","url":"https://medium.com/codetodeploy/de-mystifying-the-v8-garbage-collector-how-your-code-is-sabotaging-your-apps-memory-c290f80eb1d0?source=friends_link&amp;sk=fc1c16b78a846500f40de8539dba7332","date":1761906078,"author":"/u/Paper-Superb","guid":323629,"unread":true,"content":"<p>How does garbage collection work? And how can you save your apps from memory leaks.</p><p>The app runs fine. Until it doesn’t.</p><p>No crashes, just subtle stutters. Memory usage on your dashboard creeps up like a slow leak in the hull of a ship. Three days later, “Out of Memory.” Boom.</p><p>The common refrain for JavaScript has always been, “You don’t need to think about memory, the garbage collector handles it.” For a simple browser script, that’s mostly true. For a long-running Node.js server handling thousands of requests, this belief is a performance disaster.</p><p>The V8 garbage collector (GC) is a marvel of engineering, but it’s not a magician. It makes assumptions based on how JavaScript  behaves. When we write code that violates those assumptions, we pay a heavy performance tax. The key to a fast, stable server isn’t to  garbage collection, but to write code that is empathetic to the GC, making its job fast, predictable, and brief.</p><p>To understand the problem, you first need to know how V8 organizes memory. It doesn’t just dump everything into one giant heap. It divides memory into two main areas: the  and the .</p>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okqswx/how_my_nodejs_code_was_causing_a_massive_memory/"},{"title":"Samsung and Nvidia to build an AI megafactory","url":"https://siliconangle.com/2025/10/30/samsung-nvidia-build-ai-megafactory-transform-semiconductor-manufacturing/","date":1761905338,"author":"/u/tekz","guid":323576,"unread":true,"content":"<p>The company said it will deploy more than 50,000 of Nvidia’s most advanced <a href=\"https://siliconangle.com/2025/03/18/nvidia-cranks-agentic-ai-revamped-blackwell-ultra-gpus-next-gen-ai-desktops/\">graphics processing units</a> in the new facility to embed artificial intelligence throughout its entire chip manufacturing flow. It’s also planning to leverage AI to help with chip development and design to facilitate a new generation of semiconductors, mobile devices and robotics with enhanced AI capabilities of their own.</p><p>Samsung isn’t talking about the <a href=\"https://siliconangle.com/2025/10/28/inside-nvidias-gtc-dc-announcements-ai-factories-quantum-computers-6g-networks/\">“traditional” AI factories</a> that Nvidia is building in partnership with the U.S. Department of Energy and Oracle Corp. Those <a href=\"https://siliconangle.com/2025/10/29/nvidia-gtc-dc-rise-ai-factories-push-us-leadership/\">previously announced facilities</a> are essentially data centers that <a href=\"https://siliconangle.com/2025/10/25/ai-factories-data-centers-future/\">theCUBE Research defines</a> as “a purpose-built system for AI production,” providing the immense computing infrastructure needed to train and run AI models. Rather, it’s building an AI-enabled semiconductor manufacturing plant that will embed automation into almost every stage of its chip manufacturing operations, including design, equipment, operations and quality control.</p><p>The company likens the planned factory to a “single intelligent network,” where AI will continuously monitor and analyze its production environments, make predictions, inform maintenance and optimize everything to try and boost its chipmaking yields.</p><h3>Building the AI Megafactory</h3><p>Samsung outlined a yearslong initiative that will see it integrate Nvidia’s accelerated computing capabilities throughout its proposed factory, with the main purpose being to scale its manufacturing operations. To do this, it’s going to rely heavily on AI-powered “<a href=\"https://siliconangle.com/2024/05/06/digital-twins-projects-receive-285-million-us-government-funding-semiconductor-industry/\">digital twins</a>,” or virtual replicas of its chip products. Using the <a href=\"https://siliconangle.com/2025/03/18/nvidia-expands-omniverse-simulate-gigawatt-ai-data-centers-drive-robotic-factories/\">Nvidia Omniverse platform</a>, it’ll create digital twins of every component that goes into its semiconductors, including memory, logic, advanced packing and more. It’s also going to create twins of its actual fabrication plants and the expensive machinery within them.</p><p>The company explained that this will allow it to visualize its chip manufacturing operations in a virtual environment, where it will be able to check how they perform before it launches its physical production lines. It’ll be able to spot anomalies and work out where preventative maintenance will be needed, how to optimize production and more, then apply what it learns to its real-world factory.</p><p>What’s more, it’s not doing this only for chips. Although it plans to start with semiconductors, the company also wants to create digital twin environments of its hardware factories, where it manufactures devices such as its Galaxy smartphones and other products such as kitchen appliances and televisions.</p><p>One example of how AI can help with chipmaking is the “optical proximity correction” process, which is a critical step to ensure wafer pattern accuracy. In early tests, Samsung said its AI-enhanced OPC process helped to increase the speed and precision in which it can identify, predict and correct circuit pattern violations and abnormalities, resulting in a 20-times improvement in computational lithography performance. Nvidia’s cuLitho and CUDA-X libraries were critical in enabling this, the company revealed.</p><p>AI can also help to enhance electric design automation or EDA, which involves using specialized computer-aided software for designing new computer chips. The plan is to leverage Nvidia’s software and hardware to create a new generation of GPU-accelerated EDA tools.</p><p>AI won’t just help Samsung design and optimize its semiconductor manufacturing operations. It will also help automate the physical tasks of making its chips through the introduction of more intelligent factory robotics.</p><p>For instance, Samsung said it’s using Nvidia’s RTX Pro 600 Blackwell Server Edition platform alongside its Megatron framework to develop more <a href=\"https://siliconangle.com/2025/10/09/samsung-researchers-create-tiny-ai-model-shames-biggest-llms-reasoning-puzzles/\">advanced AI models</a> to power its robots. These models demonstrate advanced reasoning capabilities that can be integrated directly into its factory machines and humanoid robots, allowing them to work with greater autonomy and precision, alongside humans.</p><p>Nvidia is also helping Samsung to link virtual simulations with real-world robot data, so its robots will be better able to perceive their physical surroundings and make faster, intelligent decisions in real world scenarios. It’s doing this with the Nvidia Jetson Thor robotic platform, creating models for robots focused on task execution and workplace safety awareness.</p><p>Like with its AI-enabled manufacturing optimizations, Samsung will also introduce its advanced robots into the rest of its manufacturing ecosystem in future.</p><h3>AI networks and HBM4 memory chips on the way</h3><p>Beyond the AI Megafactory, Samsung said it’s working with Nvidia and a number of South Korea’s top telecommunications companies to improve network communications. They’re collaborating on the development of a technology called AI-RAN, which <a href=\"https://siliconangle.com/2024/02/26/mwc-ai-wireless-vendors-come-together-form-ai-ran-alliance/\">integrates AI</a> into mobile networks to support the deployment of AI agents and “physical AI” such as intelligent robots, drones and industrial equipment. Samsung has already <a href=\"https://news.samsung.com/global/samsung-electronics-demonstrates-ai-ran-technologies-paving-the-way-for-convergence-of-telecommunications-and-ai\">demonstrated a proof of concept</a> of AI-RAN, which it says will be critical for the future adoption of physical AI.</p><p>Meanwhile, Samsung said it continues to work with Nvidia on the development of its <a href=\"https://semiconductor.samsung.com/news-events/tech-blog/samsung-hbm4-36gb-mtv/\">high-bandwidth memory chips</a>, or HBM4, which are an essential component of AI servers. The company is making up for lost time here, as it has <a href=\"https://siliconangle.com/2025/07/30/samsung-earnings-dip-chip-division-weighs-results-despite-mobile-display-growth/\">fallen behind its biggest competitor</a> SK Hynix Inc. in the HBM memory chip sector, but believes it will ultimately be able to deliver superior performance when its HBM4 chips enter production next year.\n.<p>\nAccording to Samsung, HBM4 chips is built on its sixth-generation 10-nanometer-class dynamic random-access memory and a four-nanometer logic base die, enabling processing speeds of up to 11 gigabits per second, exceeding the Joint Electron Device Engineering Council Solid State Technology Association’s standard of 8 gigabits per second.</p></p><h5>Image: SiliconANGLE/Dreamina AI</h5><div><p>Support our mission to keep content open and free by engaging with theCUBE community. <strong>Join theCUBE’s Alumni Trust Network</strong>, where technology leaders connect, share intelligence and create opportunities.</p><ul><li data-replit-metadata=\"client/src/pages/Home.tsx:123:12\" data-component-name=\"p\"><strong>15M+ viewers of theCUBE videos</strong>, powering conversations across AI, cloud, cybersecurity and more</li><li data-replit-metadata=\"client/src/pages/Home.tsx:123:12\" data-component-name=\"p\"> — Connect with more than 11,400 tech and business leaders shaping the future through a unique trusted-based network.</li></ul><div data-replit-metadata=\"client/src/pages/Home.tsx:126:12\" data-component-name=\"div\"><div data-replit-metadata=\"client/src/pages/Home.tsx:142:14\" data-component-name=\"div\"><div data-replit-metadata=\"client/src/pages/Home.tsx:145:16\" data-component-name=\"div\">SiliconANGLE Media is a recognized leader in digital media innovation, uniting breakthrough technology, strategic insights and real-time audience engagement. As the parent company of <a href=\"https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https%3A%2F%2Fsiliconangle.com%2F&amp;esheet=54119777&amp;newsitemid=20240910506833&amp;lan=en-US&amp;anchor=SiliconANGLE&amp;index=9&amp;md5=646b1b564e2259100a2b8638aab0a552\">SiliconANGLE</a>, <a href=\"https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https%3A%2F%2Fwww.thecube.net%2F&amp;esheet=54119777&amp;newsitemid=20240910506833&amp;lan=en-US&amp;anchor=theCUBE+Network&amp;index=10&amp;md5=7de2a85f95ab4a4a495cede20b8cb1da\">theCUBE Network</a>, <a href=\"https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https%3A%2F%2Fthecuberesearch.com%2F&amp;esheet=54119777&amp;newsitemid=20240910506833&amp;lan=en-US&amp;anchor=theCUBE+Research&amp;index=11&amp;md5=7bb33676722925eb57d588ec343e4f6f\">theCUBE Research</a>, <a href=\"https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https%3A%2F%2Fwww.cube365.net%2F&amp;esheet=54119777&amp;newsitemid=20240910506833&amp;lan=en-US&amp;anchor=CUBE365&amp;index=12&amp;md5=d310fb35919714e66ad8d42c9c0c1bc6\">CUBE365</a>, <a href=\"https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https%3A%2F%2Fwww.thecubeai.com%2F&amp;esheet=54119777&amp;newsitemid=20240910506833&amp;lan=en-US&amp;anchor=theCUBE+AI&amp;index=13&amp;md5=b8b98472f8071b23ebb10ab9a8dd0683\">theCUBE AI</a> and theCUBE SuperStudios — with flagship locations in Silicon Valley and the New York Stock Exchange — SiliconANGLE Media operates at the intersection of media, technology and AI.</div></div></div></div>","contentLength":6777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1okqlbq/samsung_and_nvidia_to_build_an_ai_megafactory/"},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1okqg4j/weekly_share_your_victories_thread/","date":1761904833,"author":"/u/gctaylor","guid":323432,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In this economy even Tux needed a second job.","url":"https://www.reddit.com/r/linux/comments/1okq068/in_this_economy_even_tux_needed_a_second_job/","date":1761903150,"author":"/u/Sonikku_a","guid":323434,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LibreOffice recap, October 2025 – Markdown support, events, app updates and more","url":"https://blog.documentfoundation.org/blog/2025/10/31/libreoffice-project-and-community-recap-october-2025/","date":1761901444,"author":"/u/themikeosguy","guid":323433,"unread":true,"content":"<p>Here’s our summary of updates, events and activities in the LibreOffice project in the last four weeks – click the links to learn more…</p><ul><li>We started the month by posting the LibreOffice Podcast, Episode #5 –Accessibility in Free and Open Source Software, with Michael Weghorn and Mike Saunders. Watch it below – or <a href=\"https://peertube.opencloud.lu/w/wwwjD9E8ukoH56q1oBfa8p\">on PeerTube</a>.</li></ul><div allowfullscreen=\"\" data-no-lazy=\"1\" data-skipgform_ajax_framebjll=\"\"><p><strong>Please confirm that you want to&nbsp;play a YouTube video.</strong> By accepting, you will be accessing content from YouTube, a service provided by an external third party.</p><p>If you accept this notice, your choice will be saved and the page will refresh.</p></div><ul><li>Markdown support is coming to LibreOffice! This is just one of the projects from the <a href=\"https://blog.documentfoundation.org/blog/2025/10/22/libreoffice-and-google-summer-of-code-2025-the-results/\">Google Summer of Code 2025</a>, and should be included in our next major release, LibreOffice 26.2, due in February next year.</li></ul><ul><li>In October, we had two updates to the software: <a href=\"https://blog.documentfoundation.org/blog/2025/10/09/release-of-libreoffice-25-8-2/\">LibreOffice 25.8.2</a>, and <a href=\"https://blog.documentfoundation.org/blog/2025/10/30/libreoffice-25-2-7/\">LibreOffice 25.2.7</a>. The latter is the final update to the 25.2 branch, so after this, all users are recommended to upgrade to the 25.8 branch.</li></ul><ul><li>It’s the End of 10! Yes, in October, Microsoft ended official support for Windows 10. This leaves users who want to continue using the operating system with few alternatives — especially if they have an old PC that is not compatible with Windows 11’s demanding hardware requirements — other than buying a new PC. But we a <a href=\"https://blog.documentfoundation.org/blog/2025/10/14/end-of-10-ten-reasons-to-switch-from-windows-to-linux/\">posted about 10 reasons to switch to Linux</a> – and, of course, many desktop Linux distributions ship with LibreOffice.</li></ul><ul><li>Lots of people ask us about LibreOffice’s compatibility with Microsoft Office/365 documents. We think our compatibility is very good (and always improving, as more people send us documents to test), but the format is extremely difficult to work with, as our posts about the <a href=\"https://blog.documentfoundation.org/blog/2025/10/03/the-docx-case/\">DOCX</a> and <a href=\"https://blog.documentfoundation.org/blog/2025/10/10/the-pptx-case/\">PPTX</a> formats explain. (Of course, ideally we’d all be using the Open Document Format, regardless of the software we prefer! And <a href=\"https://blog.documentfoundation.org/blog/2025/10/24/make-your-odf-files-accessible/\">here’s how to make your ODF documents more accessible</a>.)</li></ul><ul><li>Meanwhile, the <a href=\"https://blog.documentfoundation.org/blog/2025/10/27/libreitalia-conference-2025/\">Libreitalia Conference 2025</a> was organized by Marco Marega – a LibreItalia and TDF Member – in Gradisca d’Isonzo, near the border with Slovenia.</li></ul><ul><li>And our final event report was from <a href=\"https://blog.documentfoundation.org/blog/2025/10/28/libreoffice-at-linuxdays-2025-in-prague/\">LinuxDays 2025 in Prague</a>, where we had a stand with stickers, flyers and a quiz about LibreOffice.</li></ul>","contentLength":2211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1okpkti/libreoffice_recap_october_2025_markdown_support/"},{"title":"Passwordless login via email OTP is that a good option?","url":"http://devloprr.com/","date":1761901066,"author":"/u/Agile_Guess_523","guid":323548,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okph9u/passwordless_login_via_email_otp_is_that_a_good/"},{"title":"Trying to make tenant provisioning less painful. has anyone else wrapped it in a Kubernetes operator?","url":"https://www.reddit.com/r/kubernetes/comments/1oknpg9/trying_to_make_tenant_provisioning_less_painful/","date":1761893749,"author":"/u/Selene_hyun","guid":323380,"unread":true,"content":"<p>I’m a DevOps / Platform Engineer who spent the last few years provisioning multi-tenant infrastructure by hand with Terraform. Each tenant was nicely wrapped up in modules, so spinning one up wasn’t actually that hard-drop in a few values, push through the pipeline, and everything came online as IaC. The real pain point was coordination: I sit at HQ, some of our regional managers are up to eight hours behind, and “can you launch this tenant now?” usually meant either staying up late or making them wait half a day.</p><p>We really wanted those managers to be able to fill out a short form in our back office and get a dedicated tenant environment within a couple of minutes, without needing anyone from my team on standby. That pushed me to build an internal “Tenant Operator” (v0), and we’ve been running that in production for about two years. Along the way I collected a pile of lessons, tore down the rough edges, redesigned the interface, and just published a much cleaner Tenant Operator v1.</p><p>- Watches an external registry (we started with MySQL) and creates Kubernetes Tenant CRs automatically. - Renders resources through Go templates enriched with Sprig + custom helpers, then applies them via Server-Side Apply so multiple controllers can coexist.<p> - Tracks dependencies with a DAG planner, enforces readiness gates, and exposes metrics/events for observability.</p> - Comes with scripts to spin up a local Minikube environment, plus dashboards and alerting examples if you’re monitoring with Prometheus/Grafana.</p><p>This isn’t a polished commercial product; it’s mostly tailored to the problems we had. If it sounds relevant, I’d really appreciate anyone kicking the tires and telling me where it falls short (there’ll be plenty of gaps). Happy to answer questions and iterate based on feedback. Thanks!</p><p>P.S. If you want to test it quickly on your own machine, check out the Minikube QuickStart guide, we provision everything in a sandboxed cluster. It’s run fine on my three macOS machines without any prep work.</p>","contentLength":2036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Question from beginner: what do I lose from using fiber?","url":"https://www.reddit.com/r/golang/comments/1okmqua/question_from_beginner_what_do_i_lose_from_using/","date":1761889931,"author":"/u/Fuzzy-Scratch-5386","guid":323757,"unread":true,"content":"<p>I am a hobby programmer that recently migrated from Bun/Nodejs. In order to learn go, I started by working simple rest API using fiber and sqlite. After this, while browsing for more complex project ideas, I found that fiber is not recommended because it is build over fasthttp and does not support http2 protocol. Upon further looking, I found out that http2 require (not mandatory per se, but recommended) proper tls, which probably (mostly) is not present in local project. So my question is, why not use fiber for local project? While the performance is not an issue, I like how we can create route groups as well as write the API easily.</p><p>Edit 2: I am checking videos by <a href=\"https://youtu.be/H7tbjKFSg58\">Dreams of Code</a>, these code looks cleaner</p>","contentLength":714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Are Some Active Kubernetes Communities?","url":"https://www.reddit.com/r/kubernetes/comments/1oklzqo/what_are_some_active_kubernetes_communities/","date":1761887140,"author":"/u/Healthy-Sink6252","guid":323367,"unread":true,"content":"<p>I have seen only Home Operations Discord as an active and knowledgeable community. I checked our CNCF Slack, response times are like support tickets and does not feel like a community.</p><p>If anyone also knows Indian specific communities, it would be helpful too.</p><p>I am looking for active discussions about: CNCF Projects like FluxCD, ArgoCD, Cloud, Istio, Prometheus, etc.</p><p>I think most people have these discussions internally in their organization.</p>","contentLength":442,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mixing AMD and Intel CPUs in a Kubernetes cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1okl0na/mixing_amd_and_intel_cpus_in_a_kubernetes_cluster/","date":1761883773,"author":"/u/Popular_Parsley8928","guid":323348,"unread":true,"content":"<p>I will have 4 VMs each with 12G RAM and 2 vCPU, this will be for my home lab, I will install Alma Linux 9 and then manually install Kubernetes cluster ( Rancher v2.11.6 and 4 K8S with version v1.30). The AMD CPU is AMD FX-8320 and Intel is Core i7-3770.</p><p>I won't run sophiscated app, just a small home lab to learn Kubernetes, thanks!</p>","contentLength":332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"riscv.org : RISC-V Mentorship Program","url":"https://www.reddit.com/r/linux/comments/1okkihr/riscvorg_riscv_mentorship_program/","date":1761882223,"author":"/u/I00I-SqAR","guid":323708,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pure Go HDF5 library reaches write support (v0.11.0/v0.11.1-beta). Beta testers needed before RC.","url":"https://www.reddit.com/r/golang/comments/1okjak6/pure_go_hdf5_library_reaches_write_support/","date":1761878543,"author":"/u/mistbow","guid":323352,"unread":true,"content":"<h2>Pure Go HDF5 library reaches write support (v0.11.0/v0.11.1-beta)</h2><p>After a year of work, my pure Go HDF5 implementation just hit a major milestone - you can now create HDF5 files without CGo.</p><p>: HDF5 is the standard for scientific data (astronomy, climate, genomics, etc.). Current Go options are gonum/hdf5 (CGo wrapper, requires ) and abandoned pure-Go attempts from 2015-2016.</p><p>: <code>go file, _ := hdf5.CreateForWrite(\"data.h5\", hdf5.Truncate) file.CreateDataset(\"temp\", data, hdf5.WithChunked([]uint64{100}), hdf5.WithCompression(6), // GZIP ) </code></p><p>Full support: chunked datasets, GZIP compression, dense groups (HDF5 1.8+), attributes, all datatypes. 70-88% test coverage, cross-platform.</p><p>: - No CGo = easy cross-compilation, no C dependencies - HDF5 is massive in scientific computing (TensorFlow models, NASA data, genomics) - Previous pure-Go attempts stalled because \"too complex\"</p><p> (beta): - Can't reopen files and add more data yet (v0.11.2) - Not h5dump-compatible yet (investigating) - Attributes write-once only</p><p>: HDF Group acknowledged it on their forum as the first viable pure-Go implementation (<a href=\"https://forum.hdfgroup.org/t/loking-for-an-hdf5-version-compatible-with-go1-9-2/10021/7\">link</a>).</p><p>Looking for beta testers with real-world scientific data. Installation: <code>go get github.com/scigolib/hdf5@v0.11.1-beta</code></p><p>Happy to answer questions about the implementation - HDF5 format is gnarly but solvable with the C library as reference.</p>","contentLength":1336,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Monthly Who's Hiring and Who wants to be Hired?","url":"https://www.reddit.com/r/MachineLearning/comments/1okj2rw/d_monthly_whos_hiring_and_who_wants_to_be_hired/","date":1761877894,"author":"/u/AutoModerator","guid":323498,"unread":true,"content":"<p> please use this template</p><blockquote><p>Hiring: [Location], Salary:[], [Remote | Relocation], [Full Time | Contract | Part Time] and [Brief overview, what you're looking for]</p></blockquote><p><strong>For Those looking for jobs</strong> please use this template</p><blockquote><p>Want to be Hired: [Location], Salary Expectation:[], [Remote | Relocation], [Full Time | Contract | Part Time] Resume: [Link to resume] and [Brief overview, what you're looking for]</p></blockquote><p>Please remember that this community is geared towards those with experience.</p>","contentLength":467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Refreshing Philosophy of Software Design [Book Review]","url":"https://theaxolot.wordpress.com/2025/10/30/a-refreshing-philosophy-of-software-design/","date":1761875161,"author":"/u/The_Axolot","guid":322504,"unread":true,"content":"<p>I pride myself on being the kind of blogger who writes what he truly believes, even when it’s unpopular. That’s part of why I’m so brash and come across as condescending. It’s cathartic for me, so if being correct is condescending, I don’t want to be wrong.</p><p>Having said that, it feels good when I find others whose opinions mostly resonate with mine. I very recently finished reading <em>A Philosophy of Software Design</em>, by  (I know I’m super late on that), and I’ve come to the conclusion that it should be mandatory reading for every software engineer.</p><p>First of all, it’s packed with content that isn’t easily found elsewhere. Both junior and senior engineers can learn a lot.</p><p>He explains complexity as a combination of and  (though some problems have inherent complexity), and how it slowly creeps through incremental changes to code.</p><p>The concepts of interface depth and information hiding are excellent ways to explain what modularity looks like at the program, API, service, class, and even function level.</p><p>His use of the text editor example to explain many of his concepts felt natural and not forced at all.</p><p>In chapter 19, he gives his takes on some popular software trends such as OOP, Agile, Testing, etc.</p><p>My favorite take of his is on , in which he proclaims he’s not a fan because it’s too incremental and distracts from high-level design. He only likes it when fixing bugs. That, my readers, is the correct take, and it alone puts him leagues above most.</p><p>And best of all, he isn’t dogmatic about in his presentation. In fact, each of his points is full of nuance and discussion of specific caveats that I haven’t seen anywhere else. Throughout every chapter, I just kept thinking, “This guy knows his stuff.”</p><p>The bad doesn’t outweigh the good, but I still have to point out what bothered me.</p><p>First of all, the ratio of code examples to advice is lacking. A lot of it will resonate with experts in the abstract, because we have lived experiences of said scenarios. But the less experienced may struggle to internalize some lessons without concrete examples to anchor them to. Ousterhout admits this in the introduction, but still.</p><p>But my real issues are with the presentation of these examples. A lot of the time, Ousterhout will present an interface and point out the flaws that lead to brittleness and complexity. But then, he’ll instantly come up with another interface and explain why it’s better.</p><p>His explanations are correct. It’s just that the way he presents them gives the impression that you can design a good interface from requirements alone, but then doesn’t explain how he reasoned his way to it. It feels like post-hoc rationalization of his expertise rather than a process anyone can deduce their way through.</p><p>The biggest culprit of this is the text editor example. I’ve never designed one of these before, so I was looking forward to hearing his thought process throughout. I was able to follow along, but a more junior developer would certainly struggle.</p><p>Am I supposed to believe that Ousterhout is so brilliant that he comes up with modular interfaces right off the bat? Of course not. You implement, your understanding grows alongside said implementation, and then you build your design. There’s not a single intermediate design iteration in the book. It goes straight from bad to good.</p><p>There were some other things that bothered me, too.</p><p>: <em>Define Errors Out Of Existence </em>had some great points, though it was a bit overstated. I rarely encounter instances of unjustified exceptions, though the examples he mentions, namely key deletion and substring, do fit the bill. I don’t know if it really warranted an entire chapter, though.</p><p>:  is also not worth a chapter. Do you really need to tell people to try out multiple designs?</p><p>In , he “debunks” common excuses that people use to forgo writing comments and documentation.</p><p>He claims that self-documenting code is a myth because there are often too many details in an interface to communicate through code alone. The example he uses to support his stance is the ambiguity of built-in  method. Specifically, whether its is inclusive, and what happens if the  index exceeds the index. Without its documentation, you’d have to read the method anyway to know these.</p><p>Now I don’t know what Ousterhout has been through, but surely this is an exaggeration. Yes, there may bedetails of a method that are difficult to communicate through its signature alone. But if the details are so significant, then that indicates bad design (assuming the functionality itself isn’t just unintuitive by nature). Ousterhout himself points this out in Chapter 10, so I don’t know why he uses this example. Java libraries aren’t the gold standard for code. People don’t actually believe that, right?</p><p>But there’s another factor to consider. Libraries, services, and APIs are in a different realm to classes and methods. The more granular you get, the less valuable documentation becomes, because it’s very hard to document these parts without just restating the implementation. Ousterhout acknowledges this in Chapter 16.</p><p>I don’t see people claiming that high-level documentation should be omitted, or that we shouldn’t write comments for confusing code in methods. In fact, I often see the opposite in code reviews, which is good.</p><p>“Good code is self-documenting” doesn’t mean, “No comments are allowed.” It means, “Comments shouldn’t be the default; they should be the last resort after code is made as clear as possible, yet there’s still confusion.”</p><p>I can tell he understands this because in Chapter 18, he says mentions comments as a way to compensate when code is nonobvious. So why devote so much of the book toward best practices regarding comments, when the majority of readers would already agree with his stance, assuming they can even understand it? It’s fragmented across four chapters, and not even consecutive, mind you, so I had a hard time piecing it together to explain to you here.</p><p>In he actually gives good examples of low-level comments, but I think he prematurely generalizes those examples to claim that low-level documentation is more necessary than people believe. But the reason it’s valuable in these specific examples is because he’s dealing with an inherently complex and uninituitive topic (extremely low-level memory manipulation for RPC functionality), and it’s in C++. Interestingly, it’s at this chapter that he starts using C++ in his examples. A bit fishy, I have to say.</p><p>In , he advocates using comments as a design tool. That is, using comments to plan your code and filling them in with the abstractions that follow. He doesn’t explicitly say this, but it’s clear this is meant to be an alternative to TDD (*shudder*).</p><p>He even uses similar talking points like, “If you wait until after your implementation to write comments, you may decide not to write them at all,” and, “Comments are a design tool for interfaces.” I use comments to keep myself on track and plan out my implementations, but not the way he describes it.</p><p>Why would I document a class, method, or even a variable I write it?</p><p>Why don’t I just write it and let the interface arise naturally as the implementation grows?</p><p>Code reviews can point out the need for comments, so does it matter what order I write them in?</p><p>What if comments are used as crutches to explain badly designed code?</p><p>is super lackluster.</p><p>There are so many ways to increase the clarity of your code, yet he only mentions superficial aspects, like using white space judiciously and comments.</p><p>For things that decrease clarity, he mentions:</p><p><strong>Event-driven programming (a.k.a indirect calls)</strong>. I guess, but are people defaulting to event-driven calls so much that this is worth mentioning?</p><p><strong>Generic containers (like Java’s class)</strong>. Once again, suspiciously specific. What about languages where multiple values can be grouped together without having to name the resulting object?</p><p><strong>Different types for declaration and allocation</strong><strong>(<code>like List&lt;Integer&gt; = new ArrayList&lt;Integer&gt;();</code>). </strong>Okay, this is starting to get silly. Is polymorphism bad now?</p><p><strong>Violating readers’ expectations.</strong> This one’s alright.</p><p>: <em>Designing for Performance </em>barely scratches the surface. There are so many more performance optimizations to consider, and much more common than the low-level example he uses. What about bad queries and unnecessary API calls? Caching?</p><p>More generally, he barely talks about functional programming, whose principles strongly push you toward modular design right out the gate.</p><p>What about databases and SQL? Are they not software?</p><p>The book feels very C++ and Java-centric, even though it was first published in 2018. I’m not saying Ousterhout has to only use modern languages, but he should at least acknowledge that object-oriented languages carry a lot of legacy baggage, and that it might cause this book, and even his philosophy, to become outdated soon.</p><p>In Chapter 9, Ousterhout talks about decomposition. Specifically, when to separate modules into smaller components or bring them together to reduce complexity. In chapter 9.7 &amp; 9.8, he discusses how and when methods should be decomposed. Sound familiar?</p><p>This was the topic of <a href=\"https://theaxolot.wordpress.com/2025/10/18/loc-is-a-dumb-metric-for-functions/\">my article last week</a>, and it spooked me how similar my points were to his, despite my never having read this book before, and not seeing my ideas expressed elsewhere with such precision:</p><ul><li>He calls out LoC as a dumb metric for function decomposition, though much more politely than I do.</li><li>He explains the main cost of decomposition, the main one being the spread of complexity rather than its reduction.</li><li>He says that you should favor decomposition when the subfunction can be understood in isolation, and the parent function can be understood without the implementation of the child function, which is almost exactly what I said in Lesson 4 of my article.</li><li>He advocates organizing code into independent blocks.</li></ul><p>I could go on. He explicitly pushes back on ‘s ridiculous function length recommendations, which I have to admit is pretty bold. I’d recommend people read the book for this alone. Treat it as a rebuttal to .</p><p>Oh, and I’ll just drop <a href=\"https://github.com/johnousterhout/aposd-vs-clean-code\">this</a> here.</p><p>I know I spent a lot of time on the bad, but believe me when I tell you that it’s not that significant compared to the rest of the book. My issues were more with how much time was spent on things I felt didn’t warrant such attention, and things I wished were discussed, rather than things that were outright wrong. I’m just a thorough guy. Anything I didn’t mention in the above sections, I consider good, or just not worth mentioning.</p><p>Overall, you should read this book if you haven’t already. And if you think is good, then you REALLY NEED to read this.</p><p>But read my articles first so you can see just how on point I am.</p>","contentLength":10782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oki4tt/a_refreshing_philosophy_of_software_design_book/"},{"title":"My first day in Rust","url":"https://www.reddit.com/r/rust/comments/1oki355/my_first_day_in_rust/","date":1761875033,"author":"/u/Zealousideal_Sort521","guid":323680,"unread":true,"content":"<p>I am a programmer with 15 years of experience in C# and the full Microsoft stack. I dream in LINQ and Entity Framework Core. Today was my first deep dive into Rust and I loved it. </p><p>My observations: * Rust is very precise and type safe. Way more precise than C#. No dynamics ever in Rust * The compiler is actually helpful. * I was under the impression that I was actually using my IQ points while programming again. Which was a pleasant surprise. Rust is the ultimate counterspell to vibe coding. * Setting up swagger was more difficult than it. Needed to be. * Rust code rots faster than C# code. Many examples on GitHub are unusable. * I wasn’t really a fan of the idea of being forced into nightly compiler builds to use the rocket framework. </p>","contentLength":747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How We Saved 70% of CPU and 60% of Memory in Refinery’s Go Code, No Rust Required.","url":"https://www.honeycomb.io/blog/how-we-saved-70-cpu-60-memory-refinery","date":1761871674,"author":"/u/phillipcarter2","guid":323496,"unread":true,"content":"<img alt=\"How We Saved 70% of CPU and 60% of Memory in Refinery’s Go Code, No Rust Required.\" width=\"1920\" height=\"1080\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2F262555f7660a8c2e7f470161c31456257f223bde-1920x1080.png&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2F262555f7660a8c2e7f470161c31456257f223bde-1920x1080.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.honeycomb.io/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2F262555f7660a8c2e7f470161c31456257f223bde-1920x1080.png&amp;w=3840&amp;q=75\"><p>Refinery has a big job: it performs dynamic, consistent tail-based sampling that maintains proportions across key fields, adjusts to changes in throughput, and reports accurate sampling rates. </p><p>The traffic patterns it handles are challenging, with long or large traces requiring it to hold lots of information in memory, while sudden volume spikes leave little time for infrastructure to scale up—all in a package that people want to run as cheaply as possible, since one of the primary use cases for sampling is cost control. When you're spending money to save money, you always want to spend . Version 3.0 is a big advance in that direction.<p>When we upgraded our internal Refinery cluster, total CPU usage dropped by 70%, while RAM use dropped by 60%:</p></p><img alt=\"When we upgraded our internal Refinery cluster, total CPU usage dropped by 70%, while RAM use dropped by 60%\" loading=\"lazy\" width=\"1110\" height=\"756\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2Fee11f09a3d80717e19dd12bde66420c73c4e04ec-1110x756.png&amp;w=1200&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2Fee11f09a3d80717e19dd12bde66420c73c4e04ec-1110x756.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.honeycomb.io/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2Fee11f09a3d80717e19dd12bde66420c73c4e04ec-1110x756.png&amp;w=3840&amp;q=75\"><p>With an improvement like this, we can downsize this 72-node cluster by half—a meaningful savings—while still keeping more headroom than before. If you’re a Refinery user, hopefully so can you.</p><h2>How did we pull off such a big change?</h2><p>The code’s all in <a href=\"https://github.com/honeycombio/refinery/pull/1653\">this merge</a>, but I’ll cover the basics here.<p>Like many programming languages, Go is capable of being very fast under the right circumstances (working with bounded quantities of strongly typed data), and </p>slow under the wrong ones. Unfortunately, Refinery’s job of handling customer-defined trace spans is very close to the wrong one. Historically, we followed the standard approach and fully de-serialized every span that came in through the API. Since there’s no fixed schema, the fields went into a big  — hundreds of heap allocations, pointers everywhere. It was simple and effective, but it was also expensive. Compounding this cost, in a cluster configuration, the majority of spans are handled twice since they’re redirected from the receiving node to the node which “owns” the relevant trace. Here’s what a profile of a typical clustered Refinery looked like:There’s a lot going on here, but you can see almost a quarter of CPU time going to garbage collection. Digging further reveals that a lot of the leaf nodes are ultimately some form of . In total, 50% of all CPU time in this process is allocation-related, all in order to hold onto span data as it waits for a sampling decision—after which, in most cases, it’s simply thrown away without being sent to Honeycomb!</p><img alt=\"What a profile of a typical clustered Refinery looked like.\" loading=\"lazy\" width=\"1438\" height=\"898\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2Fda25ebb434806cebe7a634518cd79db653251204-1438x898.png&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2Fda25ebb434806cebe7a634518cd79db653251204-1438x898.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.honeycomb.io/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F927dxq0h%2Fproduction%2Fda25ebb434806cebe7a634518cd79db653251204-1438x898.png&amp;w=3840&amp;q=75\"><p>There’s also a lot of overhead just for setting up data structures which we’re hardly going to use. The actual , which is the reason we’re doing all of this, all happens in the  loop, a mere 12% of time in this profile. Even that is mostly internal metrics instrumentation rather than the core decision-making algorithms.<p>The best way to make all this de-serialization (and, eventually, re-serialization) fast is to not to do it at all. Refinery only ever looks at a handful of fields in any given span, the rest is just cargo. And it’s very possible to extract only the fields you need from a serialized blob, as in the simplified example below. These two benchmarks demonstrate de-serializing into a map, and our new </p> approach, where we pull out any fields Refinery needs, then hang onto the serialized data for re-transmission. I’m using MessagePack here because that’s Refinery’s native format, with a low-level serialization API provided by the <a href=\"https://github.com/tinylib/msgp\">tinylib/msgp library</a>.</p><div><div><pre><code>func BenchmarkDecodeStrats(b *testing.B) {\n    msgpData, _ := msgpack.Marshal(struct {\n        TraceID    string\n        DurationMs float64\n    }{\n        TraceID:    \"1234567890\",\n        DurationMs: 123.4,\n    })\n\n    // Unmarshal to a schemaless map[string]any, the old way.\n    b.Run(\"to_map\", func(b *testing.B) {\n        for b.Loop() {\n            var m map[string]any\n            _ = msgpack.Unmarshal(msgpData, &amp;m)\n        }\n    })\n\n    // Unmarshal a subset of fields using custom deserialization;\n    // this is the new way.\n    b.Run(\"selective\", func(b *testing.B) {\n        for b.Loop() {\n            var durationMs float64\n            mapSize, remaining, _ := msgp.ReadMapHeaderBytes(msgpData)\n\n            for range mapSize {\n                var key []byte\n                key, remaining, _ = msgp.ReadMapKeyZC(remaining)\n                if bytes.Equal(key, []byte(\"DurationMs\")) {\n                    durationMs, remaining, _ = msgp.ReadFloat64Bytes(remaining)\n                } else {\n                    remaining, _ = msgp.Skip(remaining)\n                }\n            }\n            _ = durationMs // Pretend we did something with the duration\n        }\n    })\n}</code></pre></div></div><p>You can see that  involves much more code, but it’s hard to argue with the results:</p><div><div><pre><code>BenchmarkDecodeStrats/to_map-12     296.1  ns/op   9 allocs/op\nBenchmarkDecodeStrats/selective-12   16.98 ns/op   0 allocs/op</code></pre></div></div><p>This is a very simple scenario, and it’s common for real spans to have hundreds or even thousands of fields, which in the old version meant much longer parsing times and thousands of distinct allocations per span. Instead, Refinery 3.0 keeps the serialized data, retaining it in a format which is much more compact than the web of headers and pointers created for a fully realized map. This more compact data is the main reason for Refinery’s improved memory footprint.<p>Of course, Refinery supports three other types of input data besides our native MessagePack (</p>, , ). To handle the others, Refinery now transcodes those formats <a href=\"https://github.com/honeycombio/husky/blob/f6a547d5850adf0d61d8eaf1d1e9c91659241be8/otlp/traces_direct.go#L442\"></a> to serial MessagePack, binary-to-binary, again extracting any useful fields along the way. This code is even more voluminous than the selective extraction from MessagePack illustrated above, but it avoids an expensive additional step of translation from generated <a href=\"https://github.com/honeycombio/husky/tree/f6a547d5850adf0d61d8eaf1d1e9c91659241be8/otlp\">protobuf data structures</a> into our own.<p>To add icing to this cake, we also optimized our metrics instrumentation, implemented pools to re-use large buffers, and (coming soon as a minor version update) parallelized the core decision loop to scale across many CPUs. Notably, there are no clever algorithms or language tricks at play here. We didn’t have to rewrite it in Rust. All we've done is reimagine which work this process really needs to do, and focus on only doing that.</p></p><section></section>","contentLength":6054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okgvvr/how_we_saved_70_of_cpu_and_60_of_memory_in/"},{"title":"mm, swap: never bypass swap cache and cleanup flags (swap table phase II)","url":"https://lore.kernel.org/lkml/bvavihwrtkbnsqgjbotwihckxzmnhdd4e6jre4j7xdiyyeyv5o@dnnuyacthvms/T/#m55f0cf90afb8f8faaff3e33829c336bc7522a0b8","date":1761870413,"author":"/u/ilep","guid":323351,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1okgfs6/mm_swap_never_bypass_swap_cache_and_cleanup_flags/"},{"title":"Sidecar injector race condition during node reboot","url":"https://www.reddit.com/r/kubernetes/comments/1okfjxt/sidecar_injector_race_condition_during_node_reboot/","date":1761868054,"author":"/u/0x4ddd","guid":322409,"unread":true,"content":"<p>Let's consider following scenario: - worker node hosting injector for mutating webhook for something like service mesh - the same node hosting application pod</p><p>A) Node is broken &amp; offline longer than pod-eviction-timeout, pods are being rescheduled to remaining nodes, it may happen application pod starts before injector and is not instrumented in the end</p><p>B) Issue was short, like sudden power loss followed by power on, pods are starting recovery on the same node but the same race condition may apply as in previous case</p><p>Is the only option to set failurePolicy of mutating webhook config to Fail? I have seen some injector helm charts where this is hardcoded to Ignore and not overridable via values by default, and also number of replicas of injector being hardcoded to 1 and not overridable.</p>","contentLength":792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An interview with Ken Silverman, creator of the Build Engine (Duke Nukem 3d, Shadow Warrior, Blood). Ken programmed the engine at the age of just 17.","url":"https://youtu.be/WruzfQLxpQY","date":1761867959,"author":"/u/Tech-Jesse","guid":323469,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okfimd/an_interview_with_ken_silverman_creator_of_the/"},{"title":"GlueKube: Kubernetes integration test with ansible and molecule","url":"https://medium.com/@GlueOps/gluekube-kubernetes-integration-test-with-molecule-f88da7c41a34","date":1761867391,"author":"/u/MindCorrupted","guid":322410,"unread":true,"content":"<p>At GlueOps, we have been working on an internal tool to deploy and manage Kubernetes clusters across cloud providers and datacenters. During development, we ran into a few caveats. For example, if we modify a ‘prepare-node’ role to install an additional package or remove a package that seems unnecessary, it can indirectly affect the subsequent roles.</p><p>Example of one of the use cases, in the prepare-nodes playbook, we tend to remove a package that was thought unnecessary or change a package’s version, it can result in the subsequent roles like kubeadm-init to fail.</p><p>So we decided to add a couple of tests to help keep our work consistent and maintainable.</p><p>Some of the tests we created were:</p><ul><li>Scaling node(control-plane,data-plane)</li><li>Kubernetes versions upgrade</li></ul><p>As we deploy many production-grade clusters across numerous cloud providers (e.g. AWS, GCP, metal) for our customers, we wanted to make our deployment more agnostic. We took a look at a couple of existing tools like Kubespray, but we felt it may be a burden to maintain and modify the existing codebase in case we want to customize it.</p><p>Ultimately, we decided to build <strong>GlueKube: a platform to create kubernetes clusters agnostic to cloud providers </strong>with kubeadm, ansible.</p><ul><li>Deploying a stacked Etcd cluster where the Etcd is embedded within control-plane nodes unlike the external-etcd where Etcd where it can in separate nodes.</li><li>Supporting two load balancers (e.g. for the control plane and business applications)</li><li>Scaling Up/Down nodes(control-plane and worker nodes)</li><li>Tainting and Labeling nodes</li><li>Applying OS security patches</li><li>Upgrading cluster versions</li></ul><h2>Molecule as a Testing tool</h2><p>We’re using ansible to configure our clusters, we needed a testing tool that’s compatible with ansible and supports managing Hetzner resources for testing.</p><p>As our case is more of integration test than unit test, we found Molecule a more suitable option than ansible-test, as it provides a structured way through  to create/test/destroy infrastructure.</p><p>After our research and experiments, we created this blog to help anyone else considering similar tradeoffs.</p><p>In this post, we’re focusing on our journey with molecule. If you’re interested in learning more about the project, check out the <a href=\"https://ansible.readthedocs.io/projects/molecule/\" rel=\"noopener ugc nofollow\" target=\"_blank\">link</a>.</p><h2>Test Case I: Scaling Down Worker Nodes</h2><p>We started with <strong>scaling down worker nodes</strong> because it is easier than working with control-plane nodes.</p><p>After implementing this workflow in  scaling down is as easy as removing the node from the inventory file hosts.yaml and applying the sync-resources.yaml file, like the demo below:</p><p>Now how do we test that with Molecule ? after we created the scenario(test suite) using: molecule init scenario scaler-cluster.</p><p>We changed the scenario property in molecule.yaml to the following:</p><pre></pre><ul><li>Create: for creating the required resources for testing, our cluster will usually consist of 3 master planes, 3 worker nodes, 1 loadbalancer (HAProxy in our case).</li><li>Converge: it’s the file we will execute to transform resources into specific state</li><li>Verify: a file to run some tests after the resources get converged. The simplest test will run at first is verifying cluster health.</li><li>Destroy: at the end we need to cleanup the resources, this file will usually do the contrary of what  does. One side note is this file will always run whenever the tests result (success || failed).</li></ul><pre></pre><p> relies heavily on  to know the desired state of the cluster, think of it the same as terraform state and for our tests to run we will need one.</p><p>And this will lead us into  that’s responsible for generating the file from the created test resources. So we modified molecule.yaml to use the generated with the following code:</p><pre></pre><p> contains ansible configurations for each group in .</p><p>For running a basic verification, we used the following command:</p><pre></pre><p>This will trigger all the sequences we declared on molecule.yaml</p><p>To scale resources down, we need to remove the desired node from hosts.yaml, our initial thought of the process was creating a python script, give it the desired node, remove it from hosts.yaml and then refresh the inventory cache. However, we wanted to keep our test more Ansible oriented.</p><p>We found a better solution (at least for us) by creating two initial hosts.yamlfiles: the first one with all the nodes in and the second one without one of the worker nodes.</p><p>We used slicing to pick the  from  list, here is a code snippet from :</p><pre></pre><p>Molecule has another sequence called , which we used to replace the with  contentsrefresh the inventory cache and do the , here is the code.</p><pre></pre><p>For Molecule to recognize the side_effect, we added it alongside the other sequences.</p><pre></pre><p>After the side_effect sequence gets executed, we should verify the expected state of the cluster, in our case the side_effect should reduce the number of worker nodes by 1, so our test will count how many worker nodes we currently have. Here is a code example:</p><pre></pre><pre></pre><p>In this post, we shared our experience setting up integration tests for Kubernetes cluster management using Molecule and Ansible. We focused on a specific test case:, illustrating how Molecule’s sequences like , , , and  can be orchestrated to achieve this. We also highlighted the importance of the Ansible inventory in defining the desired state of the cluster and how Molecule facilitates testing changes to this inventory. This approach allows us to maintain the reliability and consistency of our GlueKube platform as we continue to develop and enhance its capabilities for deploying and managing Kubernetes clusters across diverse environments.</p>","contentLength":5491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1okfauo/gluekube_kubernetes_integration_test_with_ansible/"},{"title":"How Memory Maps (mmap) Deliver 25x Faster File Access in Go","url":"https://info.varnish-software.com/blog/how-memory-maps-mmap-deliver-25x-faster-file-access-in-go","date":1761866700,"author":"/u/SnooWords9033","guid":322505,"unread":true,"content":"<p>One of the slowest things you can do in an application is making system calls. They're slow because you do have to enter the kernel, which is quite expensive. What should you do when you need to do a lot of disk I/O but you care about performance? One solution is to use memory maps.<p>Memory maps are a modern Unix mechanism where you can take a file and make it part of the virtual memory. In Unix context, modern means that it was introduced in the 1980s or later. You have a file, containing data, you mmap it and you'll get a pointer to where this resides. Now, instead of seeking and reading, you just read from this pointer, adjusting the offset to get to the right data.</p></p><p>To show what kind of performance you can get using memory maps, I've written a little Go library that allows you to read from a file using a memory map or a ReaderAt. ReaderAt will do a pread(), which is a seek/read combo, while mmap will just read from the memory map.</p><p>This almost feels like magic. Initially, when we launched Varnish Cache back in 2006, this was one of the features that made Varnish Cache very fast when delivering content. Varnish Cache would use memory maps to deliver content at blistering speeds.<p>Also, since you can operate with pointers into memory that is allocated by the memory map, you'll reduce memory pressure as well as raw latency.</p></p><h2>The Downside of Memory Maps</h2><p>The downside of memory maps is that you really can't write to the memory map. The reason is due to the way virtual memory works. When you're writing to a part of virtual memory that isn't mapped into physical memory, the CPU will generate a page fault. On a modern computer, the CPU is responsible for tracking what virtual memory pages are mapped onto what physical memory. Since you're writing to a page that isn't mapped, the CPU needs help.<p>So, when the page fault occurs, the OS will 1) allocate a new memory page, 2) read the contents of the file at the correct offset, 3) write this to the new memory page. Then control is returned to the application. The application will now overwrite the virtual memory page with new data.</p><p>Can we stop and appreciate how extremely inefficient this is? I think it is fairly safe to say that writing through a memory map is never a good idea when considering performance. At least if there is any risk, the file isn't mapped up in physical memory.</p><p>Let me illustrate this with a few more benchmarks.</p></p><p>As you can see, whether or not the pages are in cache is crucial for performance. WriterAt, which uses the pwrite call, is a much more predictable bet.<p>Still, writing through memory maps, was what Varnish Cache did initially. It somehow got away with it, but mostly because the competition was pretty bad.</p><p>This is why Varnish Cache got the malloc backend and why Varnish Enterprise got the various Massive Storage Engines. The malloc backend resolved the problem by just allocating system memory through the malloc system call, and the </p><a href=\"https://youtu.be/cWCSgbY83n8?feature=shared\" rel=\"noopener\">Massive Storage Engine</a> uses io_uring, which is so new that support for it is still somewhat limited.</p><h2>Using Memory Maps to Solve Real-world Performance Problems</h2><p>The last couple of weeks I've been working on an HTTP-backed filesystem. This is part of our AI Storage Acceleration solution, geared towards high performance computing environments. In this filesystem we needed a way to transfer folder data over HTTP. A folder is really just a listing of files, symbolic links and directories. The naive approach would be just to use JSON encoding, but JSON is notorious for being slow.<p>Our priority is performance. We made a </p><a href=\"https://github.com/perbu/db-shootout/\" rel=\"noopener\">benchmarking suite</a>, comparing various databases with each other. CDB was overall the fastest. Looking at the numbers, we'd still see that CDB would spend something like 1200ns on a database lookup that was entirely in the page cache. This seems very slow to me. After all, everything should be in memory and spending 1200ns reading memory sounds at least 100x too slow. I started looking into the CDB implementation I was using. It was the above ReaderAt implementation. So, most of the time is likely spent waiting for the operating system.<p>Some hours later, I was able to replace the seek/read with a memory map. This resulted in a 25x improvement in performance. Again, it feels like magic. Unlike the original file stevedore in Varnish Cache, this performance improvement has no downside.</p><a href=\"https://github.com/perbu/mmaps-in-go\">https://github.com/perbu/mmaps-in-go</a>CDB64 files with memory maps: <a href=\"https://github.com/perbu/cdb\">https://github.com/perbu/cdb</a></p>","contentLength":4438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1okf1dv/how_memory_maps_mmap_deliver_25x_faster_file/"},{"title":"Tik Tok saved $300000 per year in computing costs by having an intern partially rewrite a microservice in Rust.","url":"https://www.linkedin.com/posts/animesh-gaitonde_tech-systemdesign-rust-activity-7377602168482160640-z_gL","date":1761866648,"author":"/u/InfinitesimaInfinity","guid":322411,"unread":true,"content":"<p dir=\"ltr\" data-test-id=\"main-feed-activity-card__commentary\">TikTok saved $300,000/year by doing&nbsp;this one thing...\nAnd no, it’s not AI. It’s&nbsp;Rust.&nbsp;🦀\nTheir payment service was&nbsp;slowing down&nbsp;as traffic exploded on TikTok LIVE.\n\nThey tried scaling the Go backend... but:\n🔹 More machines = more $$$\n🔹 Higher traffic = more latency\n🔹 Garbage collection = 😵💫\n\nSo what did they do?\n👉 Rewrote&nbsp;just&nbsp;the slowest Go endpoints in&nbsp;Rust.\nNot the whole thing. Just the bottlenecks.\n\nAnd the results were&nbsp;insane:\nPerformance at scale (80K QPS)\nMetric.                         Go.                Rust.             Improvement.  \nCPU usage.                78.3%.           52%↓         33.6%.  \nMemory usage.         7.4%.              2.07%↓      72%\np99 latency.              19.87 ms.       4.79 ms↓   76%\nThat’s not a small speed-up — that’s a&nbsp;rewrite-worthy&nbsp;difference.\n\n💰&nbsp;Cost savings?\nCut&nbsp;400 vCPU cores&nbsp;from the cluster.\n➡️ ~$300K/year saved. Just like that.\n\nSo how did Rust do it?\n🔸&nbsp;No garbage collector\n🔸&nbsp;Copy-on-Write&nbsp;(share memory until modified)\n🔸&nbsp;Zero-cost abstractions&nbsp;(speed + safety)\n🔸&nbsp;Less memcpy, more control\n🔸&nbsp;Aggressive compiler optimizations\nIt’s like Go is your reliable commuter car...\nAnd Rust is the F1 car you pull out for the real races. 🏁\n\nEngineering Wisdom from TikTok:\n✨&nbsp;“Use the right tool for the job.”\n👉 Keep Go for 95% of services (fast dev cycles, happy teams)\n👉 Use Rust where performance = revenue\n\nThis isn’t about language wars.\nIt’s about being&nbsp;strategic.\n🔁 Polyglot stacks are the future.\n💬 Thoughts?\n\n📝 Source: <a href=\"https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fdk9_H6HR&amp;urlhash=MOEe&amp;trk=public_post-text\" target=\"_self\" rel=\"nofollow\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">https://lnkd.in/dk9_H6HR</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Frust&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#Rust</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgolang&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#GoLang</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fsoftwareengineering&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#SoftwareEngineering</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fscale&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#Scale</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fperformance&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#Performance</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ftiktok&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#TikTok</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdevex&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#DevEx</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Frustaceans&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#Rustaceans</a><a href=\"https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fbackend&amp;trk=public_post-text\" target=\"_self\" data-tracking-control-name=\"public_post-text\" data-tracking-will-navigate=\"\">#Backend</a></p>","contentLength":1695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okf0md/tik_tok_saved_300000_per_year_in_computing_costs/"},{"title":"Jujutsu at Google","url":"https://www.youtube.com/watch?v=v9Ob5yPpC0A","date":1761865532,"author":"/u/steveklabnik1","guid":322447,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1okekv7/jujutsu_at_google/"},{"title":"Provisioning Clusters on Baremetal","url":"https://www.reddit.com/r/kubernetes/comments/1okejd9/provisioning_clusters_on_baremetal/","date":1761865425,"author":"/u/CompetitivePop2026","guid":322408,"unread":true,"content":"<p>Hello! I have been trying to think of a way to provision clusters and nodes for my home lab. I have a few mini pcs that I want to run baremetal k3s, k0s, or Talos. I want to be able to destroy my cluster and rebuild whenever I want just like in a virtual environment. The best way so far I have thought on how to do this is to have a PXE server and every time a node boots it would get imaged with a new image. I am leaning towards Talos with machine configs on the PXE server, but I have also thought of using a mutable distro with Ansible for bootstrapping and Day 2 configurations. Any thoughts or advice would be very appreciated! </p>","contentLength":635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] We found LRMs look great…until the problems get harder (AACL 2025)","url":"https://www.reddit.com/r/MachineLearning/comments/1okdq0s/r_we_found_lrms_look_greatuntil_the_problems_get/","date":1761863366,"author":"/u/natural_language_guy","guid":323350,"unread":true,"content":"<p>Hi there! I'm excited to share this project on characterizing reasoning capabilities of Large Reasoning Models (LLMs incentivized with \"thinking\").</p><p> We look at large reasoning models (LRMs) and try to answer the question of \"how do they generalize when reasoning complexity is steadily scaled up?</p><p>Short answer: They’re solid in the easy/mid range, then fall off a cliff once complexity crosses a threshold. We use graph reasoning and deductive reasoning as a testbed, then we try to reconcile the results with real world graph distributions.</p><ul><li>Built a dataset/generator (DeepRD) to generate queries of specified complexity (no limit to samples or complexity). Generates both symbolic and 'proof shaped' queries. <ul><li><strong>We hope this helps for future work in reasoning training+evaluation!</strong></li></ul></li><li>Tested graph connectivity + natural-language proof planning.</li><li>Saw sharp drop-offs once complexity passes a certain point—generalization doesn’t magically appear with current LRMs.</li><li>Compared against complexity in real-world graphs/proofs: most day-to-day cases are “in range,” but the long tail is risky.</li><li>Provide some in depth analysis on error modes</li></ul><p> Benchmarks with limited complexity can make models look more general than they are. The drop in performance can be quite dramatic once you pass a complexity threshold, and usually these high complexity cases are long-tail.</p>","contentLength":1352,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Affinity Suite has become free and can run on Linux","url":"https://www.reddit.com/r/linux/comments/1okcpwy/the_affinity_suite_has_become_free_and_can_run_on/","date":1761860831,"author":"/u/SpeeQz","guid":322359,"unread":true,"content":"<p>In the image above I am running the new Affinity app (a popular Photoshop, Illustrator, etc... alternative) which has combined the entire suite into a singular app. It is running on Heroic and there was confirmation from another user (<a href=\"https://www.reddit.com/u/Segajr\">u/Segajr</a>) of it running on Lutris too.</p>","contentLength":273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Check if channel is empty","url":"https://www.reddit.com/r/golang/comments/1okca10/check_if_channel_is_empty/","date":1761859730,"author":"/u/TomatilloOpening2085","guid":322413,"unread":true,"content":"<p>Hello, i have a noob question about channel. </p><p>I'm trying to code a program to play scrabble. To find the combination possibles according to the hand of the player and the letters already present on the board, I tried to code a worker pool and pass them the hand of the player, a kind of \"regex\" and a channel to retrieve their solution. </p><p>The problem is that I have a predetermined number of worker, a known number of \"regex\", but an unknown number of solution generated. So if all my worker write to this channel theirs solution, how can I, in the main thread, know when i'm done reading the content of the channel ? </p>","contentLength":615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rendered manifests pattern tools","url":"https://www.reddit.com/r/kubernetes/comments/1okbx7p/rendered_manifests_pattern_tools/","date":1761858870,"author":"/u/misse-","guid":322358,"unread":true,"content":"<p> What tools, if any, are you using to apply the rendered manifests pattern to render the output of Helm charts or Kustomize overlays into deployable Kubernetes manifests?</p><p>I am somewhat happily using Per-cluster ArgoCDs, using generators to deploy helm charts with custom values per tier, region, cluster etc.</p><p>What I dislike is being unaware of how changes in values or chart versions might impact what gets deployed in the clusters and I'm leaning towards using the \"Rendered manifests pattern\" to clearly see what will be deployed by argocd.</p><p>I've been looking in to different options available today and am at a bit of a loss of which to pick, there's:</p><p><a href=\"https://github.com/holos-run/holos\">Holos</a> - which requires me to learn cue, and seems to be pretty early days overall. I haven't tried their <a href=\"https://holos.run/docs/v1alpha5/tutorial/hello-holos/\">Hello world example</a> yet, but as Kargo, it seems more difficult than I first anticipated.</p><p>Ideally I would commit to main, and the ci would render the manifests for my different clusters and generate MRs towards their respective projects or branches, but I can't seem to find examples of that being done, so I'm hoping to learn from you.</p>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jerome Powell says the AI hiring apocalypse is real: 'Job creation is pretty close to zero.’ | Fortune","url":"https://fortune.com/2025/10/30/jerome-powell-ai-bubble-jobs-unemployment-crisis-interest-rates/","date":1761857216,"author":"/u/fortune","guid":322360,"unread":true,"content":"<p>He noted “a significant number of companies” have recently announced layoffs or hiring pauses, with many of them explicitly citing AI as the reason.\n\n\n\n</p><p>“Much of the time they’re talking about AI and what it can do,” Powell told reporters after the Fed’s rate-cut decision, warning large employers are signaling they won’t need to add headcount for years. “We’re watching that very carefully,” he added.\n\n\n\n</p><p>The comments come as the Fed cut interest rates by a quarter point to a range of 3.75%–4%, citing “downside risks to employment” even as inflation remains elevated. Powell said the U.S. economy is still expanding at a “moderate pace,” even as hiring slows. He described that spending as one of the “big sources of growth in the economy,” driven by companies building data centers and other equipment tied to artificial intelligence.\n\n\n\n</p><p>Powell also pushed back on the idea that all that spending is amounting to another speculative bubble. He drew a <a href=\"https://fortune.com/2025/10/29/powell-says-ai-is-not-a-bubble-unlike-dot-com-federal-reserve-interest-rates/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/10/29/powell-says-ai-is-not-a-bubble-unlike-dot-com-federal-reserve-interest-rates/\">clear line </a>between today’s surge in capital expenditure and the dot-com era, noting “these companies actually have earnings.”&nbsp; Those projects, he said, aren’t especially sensitive to interest rates, though, since they reflect long-term bets on higher productivity.</p><p>At the same time, Powell emphasized the boom creates a policy dilemma for the Fed. AI and automation are boosting output, but they’re also allowing companies to do more with fewer workers, leaving the labor market softer, even while GDP stays positive.<p>“We have upside risks to inflation, downside risks to employment,” he said. “This is a very difficult thing for a central bank, because one of those calls for rates to be lower, one calls for rates to be higher.”\n\n\n\n</p></p><p>Recent corporate announcements illustrate Powell’s warning. <a href=\"https://fortune.com/company/amazon-com/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/amazon-com/\">Amazon</a> announced this week it laid off 14,000 middle managers—about 4% of its white-collar workforcein an effort to “remove organizational layers.” The layoffs come amid their rampant <a href=\"https://fortune.com/2025/10/29/amazon-layoffs-ai-middle-managers-robots-factory-workers/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/10/29/amazon-layoffs-ai-middle-managers-robots-factory-workers/\">investments</a> into AI.&nbsp; <a href=\"https://fortune.com/company/target/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/target/\">Target</a>, Paramount, and other large firms followed with their own cuts.\n\n\n\n</p><p>According to a Challenger, Gray &amp; Christmas <a href=\"https://www.challengergray.com/wp-content/uploads/2025/10/Challenger-Report-September-2025.pdf\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.challengergray.com/wp-content/uploads/2025/10/Challenger-Report-September-2025.pdf\">report</a>, U.S. employers have announced nearly 946,000 layoffs so far this year—the highest total since 2020—with more than 17,000 explicitly tied to AI and another 20,000 to automation.<p>“Job creation is very low, and the job-finding rate for people who are unemployed is very low,” Powell said.</p><p>The phenomenon is so widespread some economists have coined a new term—</p><a href=\"https://www.hrdive.com/news/the-great-freeze-in-hiring-may-be-thawing-ziprecruiter/803855/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.hrdive.com/news/the-great-freeze-in-hiring-may-be-thawing-ziprecruiter/803855/\">the “Great Freeze</a>”—to describe the dismal labor market conditions. With unemployment among recent college grads topping 5%—and AI threatening to<a href=\"https://fortune.com/2025/10/29/amazon-layoffs-ai-middle-managers-robots-factory-workers/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/10/29/amazon-layoffs-ai-middle-managers-robots-factory-workers/\"> automate entry-level office jobs</a>—many Gen Z workers are <a href=\"https://fortune.com/2025/10/28/gen-z-ai-threat-law-business-school-applications-surge-classroom-economic-recession-job-market-labor-force-unemployement-rate-economy/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/10/28/gen-z-ai-threat-law-business-school-applications-surge-classroom-economic-recession-job-market-labor-force-unemployement-rate-economy/\">turning to graduate school </a>as a strategic timeout.&nbsp;\n\n\n\n</p><p>That awkward balance—strong investment but weak hiring— is now at the center of the Fed’s decision-making. Powell said the economy increasingly resembles <a href=\"https://fortune.com/2025/10/24/why-is-economy-so-bad-recession-not-inflation-fed-rate-cuts-2025/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/10/24/why-is-economy-so-bad-recession-not-inflation-fed-rate-cuts-2025/\">a K-shape,</a> with higher-income households and large corporations benefiting from strong stock markets and AI-fueled productivity gains, while<a href=\"https://fortune.com/2025/08/13/growing-gap-between-higher-income-lower-income-rich-poor-americans-wages-spending/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/08/13/growing-gap-between-higher-income-lower-income-rich-poor-americans-wages-spending/\"> lower-income consumers pull back</a> under the weight of rising costs.&nbsp;\n\n\n\n</p><p>He pointed to anecdotal reports from major retailers and consumer companies describing a “bifurcated economy,” in which wealthier Americans continue to spend freely but those at the bottom are trading down to cheaper goods. “<p>“Consumers at the lower end are struggling and buying less and shifting to lower-cost products,” Powell said, noting the uneven effects of growth make the Fed’s balancing act even more complicated.</p></p><p>“There is no risk-free path for policy,” Powell said. “We’re navigating the tension between our employment and inflation goals as carefully as we can.”\n</p>","contentLength":3742,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1okb894/jerome_powell_says_the_ai_hiring_apocalypse_is/"},{"title":"Adobe software now has graphics acceleration via Wine!","url":"https://www.reddit.com/r/linux/comments/1okacq8/adobe_software_now_has_graphics_acceleration_via/","date":1761855153,"author":"/u/maseckt","guid":322329,"unread":true,"content":"<p>A convenient way to install Adobe After Effects on Linux using Wine. Please stars this! This project right now on OBT, if u can check some errors on flatpak package, pls write on \"issues on github\" Github: <a href=\"https://github.com/relativemodder/aegnux\">https://github.com/relativemodder/aegnux</a></p>","contentLength":246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dependency Management in Database Design (aka handling import cycles in large Go projects)","url":"https://www.dolthub.com/blog/2025-10-29-dependency-management/","date":1761851799,"author":"/u/nick_at_dolt","guid":322412,"unread":true,"content":"<p>Codebase organization and design is a vital skill, and a skill I was never taught it in school and had to pick up in the industry. I suspect it wasn’t taught for two reasons:</p><ol><li>Codebase design is a soft skill. Even when you can recognize a well-structured codebase, opinions differ on how to make code organized. It’s more art than science. Just look at the long list of incompatable practices on Wikipedia’s <a href=\"https://en.wikipedia.org/wiki/List_of_software_development_philosophies\">List of software development philosophies</a></li><li>The importance of codebase architecture is much more apparent with larger, more complex code bases. Most of the code written in academia may never reach the level of complexity where good organization matters.</li></ol><p>Our project, <a href=\"https://www.dolthub.com/\">Dolt</a>, definitely meets that threshold where organization matters.\nDolt is the first SQL database with Git-style branches; it’s revision control for your data. And we’ve put a lot of work into it: the <a href=\"https://github.com/dolthub/dolt\">Dolt code repository</a> contains 762k lines of Golang code (excluding generated files), broken up into 204 different packages. We also make <a href=\"https://github.com/dolthub/go-mysql-server\">go-mysql-server</a>, the SQL engine used by Dolt which itself consists of 475k lines of Golang code in 59 different packages. The code is <a href=\"https://www.dolthub.com/blog/2025-06-02-dolt-turns-10/\">ten years old</a>.</p><p>That’s way more code than any one person can keep in their head. Breaking it up into packages helps, but how did we determine what the packages should be, and what code goes in what package?</p><p>For any project big enough, you’re going to want packages. Not just because they speed up your compilation time and allow for partial recompiles, but because organizing code this way inherently leads to cleaner projects that are more easily understood. The act of breaking code up into components is called  and it’s an important part of code architecture. When talking about modularization, we call these individual components modules.</p><p>A core principle of modern software design is the “single responsibility principle”, which states that each module only has one responsibility, and complex behavior is achieved by composing these modules. The intent is that if each module can be understood completely independently from the others, than modules can be developed in parallel with minimal risk of changes in one module breaking the behavior of other modules.</p><p>At a glance, there appear to be two main tradeoffs to modularization, although these “tradeoffs” are usually upfront costs that are outweighed by the benefit of making code maintenance significantly easier:</p><ol><li>Modularizing your codebase requires forethought and slightly increases the overall complexity of the codebase. As a project grows, assumptions made when designing the structure of the code may prove to be incorrect, resulting in either modules with multiple responsibilities that are difficult to understand, or “leaky” abstraction modules that require knowledge of their inner workings in order to use correctly. When this happens the code may need to be restructured.</li><li>Modularizing your code introduces the threat of dependency cycles: if module A depends on code symbols from module B, and module B depends on module C, then C should not depend on A. And while dependency cycles are often simple enough to untangle in theory, understanding their causes in complex code bases can be a challenge, and fixing them may require tedious refactors. In many languages, including Golang, dependency cycles between modules won’t even compile.</li></ol><p>It’s very easy for someone who doesn’t understand the layout of a codebase to accidentally introduce dependency cycles and then have trouble removing them. And dependency cycles are especially frustrating for devs because they feel like a barrier to writing clean code. In the moment, it can feel like modularization is making development . But it’s important to remember that:</p><ul><li>Without modularization, a developer that doesn’t understand the entire codebase might not be able to contribute , and</li><li>While the code that is creating the dependency cycle feels simple and clean, it’s actually introducing a new relationship between code components that will make them difficult to separate in the future.</li><li>Breaking a dependency cycle is often much simpler once the developer understands the responsibilities of the different components involved and their relationship to each other, and modularization makes that understanding a lot easier.</li></ul><p>This is best demonstrated by an example. This is a real contribution I made to Dolt where:</p><ul><li>The modularization of the codebase allowed me to develop features for one component without needing to fully understand the details of other related component.</li><li>I was temporarily stymied by a dependency cycle.</li><li>Identifying the best way to resolve the dependency cycle took time, but left me with a better understanding of how different components were connected, making it time well spent.</li><li>Armed with this better understanding, the solution became simple.</li></ul><p>We recently added support for a feature we called <a href=\"https://www.dolthub.com/blog/2025-10-06-nonlocal-tables/\">nonlocal tables</a>: Essentially, a user can configure one branch such that certain table names actually resolve to a table on another branch. The core functionality was easier to implement than expected. Next we added the ability for branches to have foreign key constraints on these tables, which proved to be more challenging.\nWe expected that foreign keys would have some odd behavior here, since changes on the referenced branch could cause these foreign key constraints to become violated. Since it’s already possible for version control commands to create similar situations, we already had a tool in place to handle this: for circumstances where it’s not possible to prevent violations, Dolt has a special tool to detect them after the fact: the  system procedure and the associated  CLI command.</p><p>This command reads the database storage layer and determines whether or not any foreign key constraints on your branch are being violated. The logic is much simpler than every other part of this feature, and it was straightforward enough that we didn’t give it much thought in design; once we had the ability to correctly resolve table names, all we had to do was allow the validation logic to depend on the name resolution logic. It should have been a one-line change.</p><p>And yet, figuring out how to properly expose the name resolution logic to the validator turned out to be the hard part. But why?</p><p>Well, let’s look at the package structure for Dolt. The logic for executing  makes use the following packages, among others:</p><ul><li> - A collection of primitive types and interfaces required for running a database. Many of the types we use are defined here. This package is a great example of a common abstraction that other modules can depend on without needing to depend on each other. But it means that anything in this module cannot depend on any of the modules that make use of it. This means that there are still lots of interfaces that can’t go in this package.\n<ul><li>This package contains a  interface, describing a table that a database engine can interact with.</li></ul></li><li><code>dolt/go/libraries/doltcore/doltdb</code> - This defines the core types that power Dolt’s data structures, and defines core operations on these types. The logic for validating foreign keys is defined in this package.\n<ul><li>This package contains a  type, representing a table in storage. This type alone does not have the necessary context to be used by an engine, so it cannot implement .</li></ul></li><li><code>dolt/go/libraries/doltcore/sqle/dsess</code> - This contains the logic responsible for maintaining the current state of a database session, including transactions.</li><li><code>dolt/go/libraries/doltcore/sqle</code> - This implements a SQL engine on top of the storage layer. Evalutating references to other branches happens here, because the result of the evaluation depends on the current transaction, otherwise you might get concurrency issues.\n<ul><li>This package contains a  type, which implements . It can be constructed from a .</li></ul></li><li><code>dolt/cmd/dolt/commands/cvcmds</code> - The implementation of the command line command for validating constraints, including foreign keys.</li></ul><p>Something else I was never taught in school: how to make proper UML diagrams.</p><p>These five packages form a clean chain of dependencies: each package depends on every package listed above it, and none of the packages below it. And it means that when developing any of these packages, as long as you don’t change the behavior of its exported functions, you can safely ignore all the packages below it.</p><p>So let’s look back at the thing that we thought would be a one-line fix:</p><blockquote><p>“all we had to do was allow the validation logic to depend on the name resolution logic”</p></blockquote><p>We can now see that there are three separate problems with this proposal:</p><ul><li>We can’t modify the validation logic in  to depend on the new table name resolution logic… because the table name resolution logic depends on branch reference resolution, which is implemented in . This is a dependency cycle.</li><li>The command itself is implemented in the top level package , and is thus allowed to depend on everything. The engine has public functions that can resolve branch names, but those functions have parameters that the command couldn’t provide, because some of that context is encapsulated by the  package.</li><li>Finally, while both of these packages have methods for interacting with tables, the types used to represent a table have a different shape, different responsibilities, and don’t implement a common interface.</li></ul><p>Again, it may feel like modularization is getting in our way by preventing us from calling functions or accessing state that we need. But the package layout also makes it clear that even if we could simply glue together these two components together, doing so would expose their internal state to each other in a way that could be complicated to refactor later. It’s worth putting in the extra legwork now to avoid this, and in doing so might suggest ways to keep the code readable.</p><p>So given all that, what’s the cleanest way to solve these problems?</p><ul><li>Could we break the cycle by cleaving off some part of the  package, and then having both the engine and storage layers depend on this new package? Probably not: the functionality we’re trying to isolate depends on the session management code in the  package: separating it out would prevent one cycle, but create another.</li><li>Perhaps instead, we provide a way to resolve branch references without needing access to the current transaction? Then we could put all the branch resolution code in the  storage layer. This could probably be done, but it would be a major change and would need to be done very carefully. We’d have to duplicate some of the lookup logic in the engine and in storage, it would be tricky to get right, and the cost of getting it wrong could be subtle concurrency bugs: no database wants that.</li></ul><p>Instead, the best way to avoid dependency cycles is to have both modules should depend on a common “abstraction” instead. Usually this means an interface type. Interfaces are a great tool when you have a simple problem statement and you already how to solve that problem, and you’re just trying to avoid introducing new dependencies.</p><p>We have a simple problem statement: resolve a table name to a table, using the new rules for referencing tables on other branches. And we already have code that solves that problem. But the logic that requires that code cannot depend on it. So dependency inversion says: depend on an abstraction. And we accomplish that in three easy steps.</p><h3>Step 1: The low-level package creates an interface that describes the shape of the operation we need.</h3><p>We define a new interface  in the  package that describes the shape of the operation we need:</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><h3>Step 2: The higher-level package provides an implementation.</h3><p>In the  package, we provide an implementation. This requires adding some new functions to the package:</p><ul><li>A function that can return the underlying  type used by the storage layer instead of preemptively constructing the higher-level type used by the engine.</li><li>An exported function that returns a  value for use by other packages.</li></ul><h3>Step 3: The top level package does dependency-injection</h3><p>With these changes in place, the top level  package can get a  from the engine and pass it as an additional parameter to the relevant storage layer calls. This allows the new name resolution rules to influence storage operations without creating any additional dependencies.</p><p>As presented, this seems like a simple and obvious solution. And it  a simple solution… but it’s only obvious when viewed in the context of the code’s organization. It’s obvious that this approach won’t  create dependency cycles or expose internal state, but we need to understand the package boundaries to see why other, similar-looking approaches .</p><p>Implementing this feature helped me better understand the exact relationship between the many different packages Dolt uses when performing even simple database operations. It ensured that any changes I made to boundaries between packages were thoughtful and deliberate, and it helped me identify future opportunities to clean up some of these interfaces and make them more usable.</p><p>I’m not sure if good codebase architecture can be taught: maybe it can only be learned. And I definitely learned something about Dolt’s design, not just the how but the . And that lesson is not only going to help me now as I develop Dolt, but also influence any codebase design I may do in the future.</p><p>I’m biased, but I think Dolt is a pretty well-designed piece of software. It’s not perfect and it’s had it’s growing pains, but it has a solid core that’s been fun to work with. Databases have a ton of complexity, and we’ve done a good job of managing that complexity such that we can continue to add cool new features.</p><p>If you have a feature you want to see in Dolt, [drop us a line on Discord] and we’ll scope it out. We take user requests seriously when deciding our priorities. If you’re looking for cool open-source projects to contribute to, we’re always welcoming contributors and are happy to help you get set up. We even have a <a href=\"https://github.com/dolthub/dolt/issues?q=state%3Aopen%20label%3A%22good%20first%20issue%22\">good first issue</a> tag on GitHub.</p>","contentLength":14030,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ok8wm5/dependency_management_in_database_design_aka/"},{"title":"Windows wouldn't let me access my HDD but Linux did","url":"https://www.reddit.com/r/linux/comments/1ok8of4/windows_wouldnt_let_me_access_my_hdd_but_linux_did/","date":1761851282,"author":"/u/snypse_","guid":322307,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What happens if a goroutine holding a sync.Mutex gets preempted by the OS scheduler?","url":"https://www.reddit.com/r/golang/comments/1ok8j3j/what_happens_if_a_goroutine_holding_a_syncmutex/","date":1761850951,"author":"/u/Alihussein94","guid":323414,"unread":true,"content":"<p>What will happen when a Goroutine locks a variable (sync.Mux) and then the Linux kernel decides to move the thread that this goroutine is running on to a blocked state, for instance, because higher higher-priority thread is running. Do the other Goroutines wait till the thread is scheduled to another CPU core and then continue processing, and then finally unlock the variable?</p>","contentLength":378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zig's New Async I/O (Text Version)","url":"https://andrewkelley.me/post/zig-new-async-io-text-version.html","date":1761850305,"author":"/u/BrewedDoritos","guid":322305,"unread":true,"content":"<p>This is a preview of the new async I/O primitives that will be available in the upcoming Zig 0.16.0, to be\nreleased in about 3-4 months. There is a lot more to get into, but for now here is an introduction\ninto some of the core synchronization API that will be available for all Zig code to use.</p><p>To begin, let's try to keep it simple and understand the basics, and then we'll then slowly add more\nasynchronous things into it.</p><p>With our first example, there is nothing asynchronous here. It's basically \"Hello, World!\" in Zig.</p><pre><code>const std = @import(\"std\");\n\npub fn main() !void {\n    doWork();\n}\n\nfn doWork() void {\n    std.debug.print(\"working\\n\", .{});\n    var timespec: std.posix.timespec = .{ .sec = 1, .nsec = 0 };\n    _ = std.posix.system.nanosleep(&amp;timespec, &amp;timespec);\n}</code></pre><pre>0s $ zig run example0.zig\n0s working\n1s $</pre><p>Next, we're going to set up a little bit. Still not using async/await yet, but I need some tools in my\ntoolbox before we add complexity.</p><pre><code>const std = @import(\"std\");\nconst Io = std.Io;\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\n\nfn juicyMain(gpa: Allocator, io: Io) !void {\n    _ = gpa;\n\n    doWork(io);\n}\n\nfn doWork(io: Io) void {\n    std.debug.print(\"working\\n\", .{});\n    io.sleep(.fromSeconds(1), .awake) catch {};\n}\n\npub fn main() !void {\n    // Set up allocator.\n    var debug_allocator: std.heap.DebugAllocator(.{}) = .init;\n    defer assert(debug_allocator.deinit() == .ok);\n    const gpa = debug_allocator.allocator();\n\n    // Set up our I/O implementation.\n    var threaded: std.Io.Threaded = .init(gpa);\n    defer threaded.deinit();\n    const io = threaded.io();\n\n    return juicyMain(gpa, io);\n}</code></pre><pre>0s $ zig run example0.zig\n0s working\n1s $</pre><p>Setting up a  implementation is a lot like setting up an allocator.\nYou typically do it once, in main(), and then pass the instance throughout the application.\nReusable code should accept an Allocator parameter if it needs to allocate, and it should accept\nan Io parameter if it needs to perform I/O operations.</p><p>In this case, this is an Io implementation based on threads. This is not using\nKQueue, this is not using IO_Uring, this is not using an event loop. It is a  implementation\nof the new  interface.</p><p>This setup will be the same in all the examples, so now we can focus on our example code, which is the same\nas last time. Still nothing interesting - we just call  which of course is just calling sleep().</p><p>Redundant setup code omitted from here on out.</p><pre><code>fn juicyMain(gpa: Allocator, io: Io) !void {\n    _ = gpa;\n\n    var future = io.async(doWork, .{io});\n\n    future.await(io); // idempotent\n}\n\nfn doWork(io: Io) void {\n    std.debug.print(\"working\\n\", .{});\n    io.sleep(.fromSeconds(1), .awake) catch {};\n}</code></pre><pre>0s $ zig run example0.zig\n0s working\n1s $</pre><p>Now we're using async/await to call doWork. What async/await means to Zig is to  the\n of the function to the  of the function.</p><p>This code is the same as before. It's exactly the same, because we didn't put any code between the async\nand await. We do the call, and then immediately wait for the return.</p><p>In the next example, we have two things at the same time:</p><pre><code>fn juicyMain(gpa: Allocator, io: Io) !void {\n    _ = gpa;\n\n    var a = io.async(doWork, .{ io, \"hard\" });\n    var b = io.async(doWork, .{ io, \"on an excuse not to drink Spezi\" });\n\n    a.await(io);\n    b.await(io);\n}\n\nfn doWork(io: Io, flavor_text: []const u8) void {\n    std.debug.print(\"working {s}\\n\", .{flavor_text});\n    io.sleep(.fromSeconds(1), .awake) catch {};\n}</code></pre><pre>0s $ zig run example3.zig\n0s working on an excuse not to drink Spezi\n0s working hard\n1s $</pre><p>If you look carefully, you can see that it did not wait two seconds; it waited one second because\nthese operations are happening at the same time. This demonstrates why using async/await is useful -\nyou can express asynchrony. Depending on the I/O implementation that you\nchoose, it may be able to take advantage of the asynchrony that you have\nexpressed and make your code go faster. For example in this case,\n was able to do two seconds of work in one second\nof actual time.</p><p>Let's start to bring the example closer to a real world scenario by introducing .</p><pre><code>fn juicyMain(gpa: Allocator, io: Io) !void {\n    var a = io.async(doWork, .{ gpa, io, \"hard\" });\n    var b = io.async(doWork, .{ gpa, io, \"on an excuse not to drink Spezi\" });\n\n    try a.await(io);\n    try b.await(io);\n}\n\nfn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {\n    // Simulate an error occurring:\n    if (flavor_text[0] == 'h') return error.OutOfMemory;\n\n    const copied_string = try gpa.dupe(u8, flavor_text);\n    defer gpa.free(copied_string);\n    std.debug.print(\"working {s}\\n\", .{copied_string});\n    io.sleep(.fromSeconds(1), .awake) catch {};\n}</code></pre><p>It's the same code as before, except the first task will return an error.</p><p>Guess what happens when this code is run?</p><pre>0s $ zig run example4.zig\n0s working on an excuse not to drink Spezi\n1s error(gpa): memory address 0x7f99ce6c0080 leaked:\n1s /home/andy/src/zig/lib/std/Io/Threaded.zig:466:67: 0x1053aae in async (std.zig)\n1s     const ac: *AsyncClosure = @ptrCast(@alignCast(gpa.alignedAlloc(u8, .of(AsyncClosure), n) catch {\n1s                                                                   ^\n1s /home/andy/src/zig/lib/std/Io.zig:1548:40: 0x1164f94 in async__anon_27344 (std.zig)\n1s     future.any_future = io.vtable.async(\n1s                                        ^\n1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:8:21: 0x116338a in juicyMain (example4.zig)\n1s     var b = io.async(doWork, .{ gpa, io, \"on an excuse not to drink Spezi\" });\n1s                     ^\n1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:35:21: 0x1163663 in main (example4.zig)\n1s     return juicyMain(gpa, io);\n1s                     ^\n1s /home/andy/src/zig/lib/std/start.zig:696:37: 0x1163c83 in callMain (std.zig)\n1s             const result = root.main() catch |err| {\n1s                                     ^\n1s /home/andy/src/zig/lib/std/start.zig:237:5: 0x1162f61 in _start (std.zig)\n1s     asm volatile (switch (native_arch) {\n1s     ^\n1s \n1s thread 1327233 panic: reached unreachable code\n1s error return context:\n1s /home/andy/src/zig/lib/std/Io.zig:1003:13: 0x11651a8 in await (std.zig)\n1s             return f.result;\n1s             ^\n1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:10:5: 0x11633e8 in juicyMain (example4.zig)\n1s     try a.await(io);\n1s     ^\n1s \n1s stack trace:\n1s /home/andy/src/zig/lib/std/debug.zig:409:14: 0x103e5a9 in assert (std.zig)\n1s     if (!ok) unreachable; // assertion failure\n1s              ^\n1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example4.zig:27:17: 0x1163698 in main (example4.zig)\n1s     defer assert(debug_allocator.deinit() == .ok);\n1s                 ^\n1s /home/andy/src/zig/lib/std/start.zig:696:37: 0x1163c83 in callMain (std.zig)\n1s             const result = root.main() catch |err| {\n1s                                     ^\n1s /home/andy/src/zig/lib/std/start.zig:237:5: 0x1162f61 in _start (std.zig)\n1s     asm volatile (switch (native_arch) {\n1s     ^\n1s fish: Job 1, 'zig run example4.zig' terminated by signal SIGABRT (Abort)\n1s $</pre><p>The problem is that when the first  activates, it skips the second  which\nis then caught by the leak checker.</p><p>This is a bug. It's unfortunate though, isn't it? Because we would like to write the code this way.</p><pre><code>fn juicyMain(gpa: Allocator, io: Io) !void {\n    var a = io.async(doWork, .{ gpa, io, \"hard\" });\n    var b = io.async(doWork, .{ gpa, io, \"on an excuse not to drink Spezi\" });\n\n    const a_result = a.await(io);\n    const b_result = b.await(io);\n\n    try a_result;\n    try b_result;\n}\n\nfn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {\n    // Simulate an error occurring:\n    if (flavor_text[0] == 'h') return error.OutOfMemory;\n\n    const copied_string = try gpa.dupe(u8, flavor_text);\n    defer gpa.free(copied_string);\n    std.debug.print(\"working {s}\\n\", .{copied_string});\n    io.sleep(.fromSeconds(1), .awake) catch {};\n}</code></pre><p>We do the awaits, then we do the tries. This will fix the problem.</p><pre>0s $ zig run example5.zig\n0s working on an excuse not to drink Spezi\n1s error: OutOfMemory\n1s /home/andy/src/zig/lib/std/Io.zig:1003:13: 0x11651d8 in await (std.zig)\n1s             return f.result;\n1s             ^\n1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example5.zig:13:5: 0x1163416 in juicyMain (example5.zig)\n1s     try a_result;\n1s     ^\n1s /home/andy/misc/talks/zigtoberfest/async-io-examples/example5.zig:38:5: 0x11636e9 in main (example5.zig)\n1s     return juicyMain(gpa, io);\n1s     ^\n1s $</pre><p>This failed successfully. The error was handled and no resources leaked. But\nit's a footgun. Let's find a better way to express this...</p><p>This is where  comes in. cancellation is an extremely handy primitive,\nbecause now we can use , , and  like normal,\nand not only do we fix the bug, but we also get more optimal code.</p><pre><code>fn juicyMain(gpa: Allocator, io: Io) !void {\n    var a = io.async(doWork, .{ gpa, io, \"hard\" });\n    defer a.cancel(io) catch {};\n\n    var b = io.async(doWork, .{ gpa, io, \"on an excuse not to drink Spezi\" });\n    defer b.cancel(io) catch {};\n\n    try a.await(io);\n    try b.await(io);\n}\n\nfn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) !void {\n    // Simulate an error occurring:\n    if (flavor_text[0] == 'h') return error.OutOfMemory;\n\n    const copied_string = try gpa.dupe(u8, flavor_text);\n    defer gpa.free(copied_string);\n    std.debug.print(\"working {s}\\n\", .{copied_string});\n    io.sleep(.fromSeconds(1), .awake) catch {};\n}</code></pre><p>Thanks to cancellation, we now get instant results, because the moment that the first\ntask returns an error, the cancels get run.</p><pre>0s $ zig run example6.zig\n0s working on an excuse not to drink Spezi\n0s error: OutOfMemory\n0s /home/andy/misc/talks/zigtoberfest/async-io-examples/example6.zig:13:5: 0x116348c in juicyMain (example6.zig)\n0s     try a.await(io);\n0s     ^\n0s /home/andy/misc/talks/zigtoberfest/async-io-examples/example6.zig:38:5: 0x1163909 in main (example6.zig)\n0s     return juicyMain(gpa, io);\n0s     ^\n0s $</pre><p> is your best friend, because it's going to prevent you from leaking the\nresource, and it's going to make your code run more optimally.</p><p> is trivial to understand: it has identical semantics as , except\nthat it <em>also requests cancellation</em>. The conditions under which cancellation requests are honored\nare defined by each I/O implementation.</p><p>Both  and  are idempotent with respect to themselves and each other.</p><p>Next, let's introduce another real-world scenario: .\nIn this case, we allocate a string on success, which the caller needs to manage.</p><pre><code>fn juicyMain(gpa: Allocator, io: Io) !void {\n    var a = io.async(doWork, .{ gpa, io, \"hard\" });\n    defer if (a.cancel(io)) |s| gpa.free(s) else |_| {};\n\n    var b = io.async(doWork, .{ gpa, io, \"on an excuse not to drink Spezi\" });\n    defer if (b.cancel(io)) |s| gpa.free(s) else |_| {};\n\n    const a_string = try a.await(io);\n    const b_string = try b.await(io);\n    std.debug.print(\"finished {s}\\n\", .{a_string});\n    std.debug.print(\"finished {s}\\n\", .{b_string});\n}\n\nfn doWork(gpa: Allocator, io: Io, flavor_text: []const u8) ![]u8 {\n    const copied_string = try gpa.dupe(u8, flavor_text);\n    std.debug.print(\"working {s}\\n\", .{copied_string});\n    io.sleep(.fromSeconds(1), .awake) catch {};\n    return copied_string;\n}</code></pre><p>Now we see why  and  have the same API.\nThe deferred cancel calls above free the allocated resource, handling both\nsuccessful calls (resource allocated) and failed calls (resource not allocated).</p><pre>0s $ zig run example7.zig\n0s working on an excuse not to drink Spezi\n0s working hard\n1s finished hard\n1s finished on an excuse not to drink Spezi\n1s $</pre><p>The important thing here is that by doing resource management like this, we are\nable to write standard, idiomatic Zig code below, using  and \nlike normal without worrying about special resource management cases.</p><p>In this example we have a producer sending one item across an unbuffered queue to a consumer.</p><pre><code>fn juicyMain(io: Io) !void {\n    var queue: Io.Queue([]const u8) = .init(&amp;.{});\n\n    var producer_task = io.async(producer, .{\n        io, &amp;queue, \"never gonna give you up\",\n    });\n    defer producer_task.cancel(io) catch {};\n\n    var consumer_task = io.async(consumer, .{ io, &amp;queue });\n    defer _ = consumer_task.cancel(io) catch {};\n\n    const result = try consumer_task.await(io);\n    std.debug.print(\"message received: {s}\\n\", .{result});\n}\n\nfn producer(\n    io: Io,\n    queue: *Io.Queue([]const u8),\n    flavor_text: []const u8,\n) !void {\n    try queue.putOne(io, flavor_text);\n}\n\nfn consumer(\n    io: Io,\n    queue: *Io.Queue([]const u8),\n) ![]const u8 {\n    return queue.getOne(io);\n}</code></pre><p>We use  to spawn the producer and  to spawn the consumer.</p><pre>0s $ zig run example8.zig\n0s message received: never gonna give you up\n0s $</pre><p>This incorrectly succeeds. Depending on your perspective, we either got \"lucky\" or \"unlucky\" due\nto the thread pool having spare concurrency that happened to be available.</p><p>To observe the problem, we can artificially limit the  instance to\nuse a thread pool size of one:</p><pre><code>// Set up our I/O implementation.\n    var threaded: std.Io.Threaded = .init(gpa);\n    threaded.cpu_count = 1;\n    defer threaded.deinit();\n    const io = threaded.io();\n\n    return juicyMain(io);\n}</code></pre><p>Now that it's only using one thread, it deadlocks, because the consumer is waiting to get something from\nthe queue, and the producer is scheduled to run, but it has not run yet.</p><p>The problem is that <em>we needed concurrency, but we asked for asynchrony</em>.</p><p>In order to fix this, we use  instead of .\nThis one can fail with <code>error.ConcurrencyUnavailable</code>.</p><pre><code>fn juicyMain(io: Io) !void {\n    var queue: Io.Queue([]const u8) = .init(&amp;.{});\n\n    var producer_task = try io.concurrent(producer, .{\n        io, &amp;queue, \"never gonna give you up\",\n    });\n    defer producer_task.cancel(io) catch {};\n\n    var consumer_task = try io.concurrent(consumer, .{ io, &amp;queue });\n    defer _ = consumer_task.cancel(io) catch {};\n\n    const result = try consumer_task.await(io);\n    std.debug.print(\"message received: {s}\\n\", .{result});\n}\n\nfn producer(\n    io: Io,\n    queue: *Io.Queue([]const u8),\n    flavor_text: []const u8,\n) !void {\n    try queue.putOne(io, flavor_text);\n}\n\nfn consumer(\n    io: Io,\n    queue: *Io.Queue([]const u8),\n) ![]const u8 {\n    return queue.getOne(io);\n}</code></pre><pre>0s $ zig run example10.zig\n0s message received: never gonna give you up\n0s $</pre><p>Now the code is fixed because we correctly expressed that we needed concurrency, which\n honored by oversubscribing.</p><p>If I add  which truly limits the executable to one thread,\noversubscription is not available, causing this output:</p><pre>error: ConcurrencyUnavailable\n/home/andy/src/zig/lib/std/Io/Threaded.zig:529:34: 0x1051863 in concurrent (std.zig)\n    if (builtin.single_threaded) return error.ConcurrencyUnavailable;\n                                 ^\n/home/andy/src/zig/lib/std/Io.zig:1587:25: 0x1158b5f in concurrent__anon_26591 (std.zig)\n    future.any_future = try io.vtable.concurrent(\n                        ^\n/home/andy/misc/talks/zigtoberfest/async-io-examples/example10.zig:9:25: 0x1157198 in juicyMain (example10.zig)\n    var producer_task = try io.concurrent(producer, .{\n                        ^\n/home/andy/misc/talks/zigtoberfest/async-io-examples/example10.zig:48:5: 0x115776a in main (example10.zig)\n    return juicyMain(io);\n    ^</pre><p>There are proof-of-concept  implementations using IoUring and KQueue combined\nwith stackful coroutines which show a lot of promise, however that work depends on some language\nenhancements to be practical. There is also ongoing design work about stackless coroutines. Here\nare some relevant issues to track for those interested:</p><p>These APIs are not set in stone. It will probably take a few iterations to\nget it right. Please try them out in  and let\nus know how it goes! Let's collaborate on making the I/O interface practical\nand optimal.</p>","contentLength":15875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ok88li/zigs_new_async_io_text_version/"},{"title":"With the release of Rust 1.91, Arm is now a Tier 1 supported architecture on Windows","url":"https://github.com/rust-lang/rust/pull/145682","date":1761849861,"author":"/u/Balance-","guid":323349,"unread":true,"content":"<p> is now a Tier 1 target with host tools for Rust, meaning ARM64 Windows with MSVC is \"guaranteed to work\" as a fully supported platform. This means the Rust project provides official binary releases, runs automated testing after every change to ensure builds and tests pass, and supports running development tools like  and  natively on ARM64 Windows machines. In practical terms, developers can now confidently use ARM64 Windows devices (like Windows on ARM laptops) both as compilation targets and as development platforms with the same level of support as established platforms like x86_64 Windows and ARM64 macOS.</p>","contentLength":617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ok81lz/with_the_release_of_rust_191_arm_is_now_a_tier_1/"},{"title":"Rust 1.90.1 is out","url":"https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/","date":1761849591,"author":"/u/manpacket","guid":322303,"unread":true,"content":"<p>The Rust team is happy to announce a new version of Rust, 1.91.0. Rust is a programming language empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via , you can get 1.91.0 with:</p><p>If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please <a href=\"https://github.com/rust-lang/rust/issues/new/choose\">report</a> any bugs you might come across!</p><p>The Rust compiler supports <a href=\"https://doc.rust-lang.org/rustc/platform-support.html\">a wide variety of targets</a>, but\nthe Rust Team can't provide the same level of support for all of them. To\nclearly mark how supported each target is, we use a tiering system:</p><ul><li>Tier 3 targets are technically supported by the compiler, but we don't check\nwhether their code build or passes the tests, and we don't provide any\nprebuilt binaries as part of our releases.</li><li>Tier 2 targets are guaranteed to build and we provide prebuilt binaries, but\nwe don't execute the test suite on those platforms: the produced binaries\nmight not work or might have bugs.</li><li>Tier 1 targets provide the highest support guarantee, and we run the full\nsuite on those platforms for every change merged in the compiler. Prebuilt\nbinaries are also available.</li></ul><p>Rust 1.91.0 promotes the  target to Tier 1 support,\nbringing our highest guarantees to users of 64-bit ARM systems running Windows.</p><h3><a href=\"https://blog.rust-lang.org/2025/10/30/Rust-1.91.0/#add-lint-against-dangling-raw-pointers-from-local-variables\" aria-hidden=\"true\"></a>\nAdd lint against dangling raw pointers from local variables</h3><p>While Rust's borrow checking prevents dangling references from being returned, it doesn't\ntrack raw pointers. With this release, we are adding a warn-by-default lint on raw\npointers to local variables being returned from functions. For example, code like this:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre><code></code></pre><p>Note that the code above is not unsafe, as it itself doesn't perform any dangerous\noperations. Only dereferencing the raw pointer after the function returns would be\nunsafe. We expect future releases of Rust to add more functionality helping authors\nto safely interact with raw pointers, and with unsafe code more generally.</p><p>These previously stable APIs are now stable in const contexts:</p><p>Many people came together to create Rust 1.91.0. We couldn't have done it without all of you. <a href=\"https://thanks.rust-lang.org/rust/1.91.0/\">Thanks!</a></p>","contentLength":2118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ok7xdh/rust_1901_is_out/"},{"title":"Bluefin Autumn 2025: We visit the Bazaar","url":"https://docs.projectbluefin.io/blog/2025-10-28-bluefin-autumn/","date":1761847719,"author":"/u/fizzyizzy05","guid":322386,"unread":true,"content":"<p>Guardians, today Bluefin GTS switched its base from Fedora 41 to Fedora 42. The gathering of raptors has begun. In a two weeks Bluefin (aka ) releases on Fedora 43 and we will start the cycle all over again!</p><p>Looking for Fedora 43? That's here too in , and will roll out to  users in 2 weeks. It's tough to write two of these, so we'll likely just move to spring/autumn announcements and whenever major things land. When  upgrades I will post it as an addenum in the <a href=\"https://github.com/ublue-os/bluefin/discussions/3510\" target=\"_blank\" rel=\"noopener noreferrer\">discussion thread</a> for this post.</p><p>As it ends up F43 will be coming to  while we're in Atlanta, GA, for <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/\" target=\"_blank\" rel=\"noopener noreferrer\">KubeCon + CloudNativeCon</a>, come say hello! As a <a href=\"https://bootc-dev.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"></a> reference architecture we tend to align with the release cadence of other projects. This usually means that I'm on the road when there's a Bluefin release happening, so we do status reports like this depending on where we are in the world at the time, and to ensure transparency. It's also our chance to gather with attendees and get feedback on how we can make Bluefin better and gather feedback.</p><p>You'll receive this update during your next update window, or you can run an update manually by clicking on this icon:</p><p>If you've never experienced a Bluefin upgrade before, McPhail has a <a href=\"https://gld.mcphail.uk/posts/how-to-perform-a-major-version-upgrade-on-bluefin/\" target=\"_blank\" rel=\"noopener noreferrer\">full writeup</a>. Here's the major release information:</p><p>Bluefin is an operating system for your computer. It is designed to be installed on a device upgrade for the life of the hardware – we accomplish this by sharing the maintenance and care of our systems together as a community. It is designed to be as “zero touch” as possible by providing a curated GNOME experience.</p><p>Bluefin GTS (aka ) is our standard release, designed to be one cycle behind the most current Fedora release. This one's been in the oven for about six months and is ready to go. In a few weeks the  branch will move on to Fedora 43. If you're brand new you can use the website to <a href=\"https://projectbluefin.io/#scene-picker\" target=\"_blank\" rel=\"noopener noreferrer\">pick the right image</a> or select from the grid below:</p><p>This unidentified Dromeasaur is by <a href=\"https://natalia-jagielska.weebly.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Dr. Natalia Jagielska</a>, a world renowned expert <a href=\"https://www.nationalgeographic.com/science/article/stunning-scottish-pterosaur-is-biggest-fossil-of-its-kind\" target=\"_blank\" rel=\"noopener noreferrer\">paleontologist</a> and <a href=\"https://natalia-jagielska.weebly.com/art.html\" target=\"_blank\" rel=\"noopener noreferrer\">paleoartist</a>! We reached out to work with her on bringing her artwork and style to Bluefin, and she said yes! This rendition will be revealed in November, or you can just manually pick it in the wallpaper chooser.</p><p>I am so stoked about this, an actual scientist! We're retconning that this is just Bluefin enjoying a nice day at the lake. We have  more wallpapers from her coming soon. I have come to really appreciate the world of flying reptiles. They are <a href=\"https://en.wikipedia.org/wiki/Pterosaur#/media/File:Size_disparity_of_late_Maastrichtian_pterosaurs_and_birds.svg\" target=\"_blank\" rel=\"noopener noreferrer\">terrifying</a>.</p><p>Natalia's artwork was vectorized and remastered by <a href=\"https://ko-fi.com/melodyofdelphi\" target=\"_blank\" rel=\"noopener noreferrer\">Delphic Melody</a>, please consider donating so that the collaboration can continue!</p><p>There are a few major changes from a Bluefin perspective that we've been looking forward to:</p><h3>Installation Experience​<a href=\"https://docs.projectbluefin.io/blog/2025-10-28-bluefin-autumn/#installation-experience\" aria-label=\"Direct link to Installation Experience​\" title=\"Direct link to Installation Experience​\">​</a></h3><ul><li>The Anaconda web-ui installer is now the default installer, dramatically improving the experience. We say goodbye to the old GTK Anaconda installer.</li><li>We'll be automatically refreshing all the Bluefin ISOs once a month to ensure the installation media is fresh.</li></ul><p><a href=\"https://github.com/kolunmi/bazaar\" target=\"_blank\" rel=\"noopener noreferrer\">Bazaar</a> makes its debut in Bluefin GTS! All Bluefins are now just using the Bazaar flatpak. You're in for a treat:</p><p>It's been super awesome seeing Bazaar move from a random project we found on r/gnome to what is effectively now the premier app store experience for FlatHub and Linux. You can help out tremendously by <a href=\"https://github.com/sponsors/kolunmi\" target=\"_blank\" rel=\"noopener noreferrer\">sponsoring the author</a>.</p><p>This is also a major milestone for Bluefin since we've effectively done our part for the GNOME and FlatHub ecosystems and can now consider application installation a solved problem, we can introduce new things into Bluefin as a flatpak to begin with and move us away from distribution specific formats.</p><p>I am finding more applications now than I ever have. It's also a milestone for all Linuxes since flatpak's upcoming release gives us the flexibility to do this in a proper way with full lifecycle management. We can now be more flexible with the applications we can ship mid-cycle by plopping a file in . Those of you making custom images will really take advantage of this!</p><p>Shoutout to Sebastian Wick for this work in Flatpak and working on the next release of this cool tech!</p><div><div><p>We're committed to a future where authors deliver their applications how they see fit. This should be decoupled from the operating system.</p></div></div><p>Speaking of packages, we've been doing more work engaging with Homebrew developers, check out this interview I did with Workbrew talking about our hopes and dreams:</p><p>Let us know if you're interested in working on Homebrew for Linux, we have opened a <a href=\"https://github.com/ublue-os/homebrew-tap\" target=\"_blank\" rel=\"noopener noreferrer\">homebrew tap</a> so that we can interate on bringing cool new things to you. A huge shoutout goes to <a href=\"https://ko-fi.com/yulian\" target=\"_blank\" rel=\"noopener noreferrer\">Yulian Kuncheff</a> and <a href=\"https://github.com/sponsors/ahmedadan\" target=\"_blank\" rel=\"noopener noreferrer\">Ahmed Adan</a> for spearheading this effort, please consider donating!</p><p>The fonts have been a disaster for a long time, we're finally ripping the bandaid off and removing a bunch of fonts from the image. For you command line nerds you can install any of the <a href=\"https://formulae.brew.sh/cask-font/\" target=\"_blank\" rel=\"noopener noreferrer\">fonts listed in Homebrew</a> or use a tool like <a href=\"https://flathub.org/en/apps/io.github.getnf.embellish\" target=\"_blank\" rel=\"noopener noreferrer\">Embellish</a> to install more fonts.</p><p>If you're in developer mode you can bring the monospace fonts back with .</p><p>We've dropped the GNOME Quick Settings extension for tailscale in favor of the upstream system tray implementation. For more information, <a href=\"https://docs.projectbluefin.io/administration/#virtual-private-networks-vpn\" target=\"_blank\" rel=\"noopener noreferrer\">check the docs</a>, this requires manual set up.</p><p>The tailscale experience is still not where it needs to be, but now that Tailscale has started work on an official system tray implementation we expect this to solidify over the next few upstream releases.</p><p>After a hiatus we've finally refactored the Homebrew management in Bluefin. We're adding back some convenience commands:</p><p>Extinction is a natural part of life. After a deprecation cycle the following images are now removed:</p><ul><li>: Due to Nvidia's software support changes we can no longer support the older closed modules for Nvidia cards. Not many people are using these, either migrate to the  images or move to a stock image to use the built in kernel drivers.</li><li>: Not many people were using these, they have also been removed.</li></ul><p>As usual most of the changes we do in GitHub to deliver Bluefin and not so much in the image itself. Major parts of the Bluefin repository have been cleaned up to align with the improvements and lessons learned from building Bluefin LTS earlier in the year. This has been the bulk of the work in the past few weeks.</p><p>Bluefin has significantly been simplified, now would be a great time to contribute as we've brought the repository up to the state of more modern  projects like Bluefin LTS.</p><ul><li> and  will be publishing on Tuesdays from now on instead of Saturdays. Publishing on Saturday nights is an artifact of pre-automation \"reserved time\" for testing before a weekly release. This matches the same release schedule as Bluefin LTS.</li></ul><p>Bluefin is a deinonychus, and may snap at you occasionally. Four year olds can get feisty of so there might be issues that you discover that we haven't seen before. Filing issues is always appreciated.</p><p>We also accept donations to sponsor the infrastructure and artwork.</p><p>Sometimes starting in open source can be a real barrier if you don't know where to start. Don't have the skills to do cloud native things yet? Here's a good way to help out FlatHub. Flatpaks rely on what we call \"runtimes\" to ensure that the application has the dependencies it needs to run. Do a  to check them out:</p><p>This is important work because we want applications to be updated to the latest runtimes for security reasons. As it turns out, many of these applications have OPEN PULL REQUESTS already with people updating the runtime, you just need to find the app, run the updated version by following the instructions, and then report back to the Flatpak maintainer that the new app is working great (or broken!). Since GNOME 49 just released, there's plenty to do, so feel free to dive in and get started! Also remember, this work helps all of FlatHub, we're explictly sending new volunteers to help upstream.</p><div><div>FlatHub is critical to the desktop</div><div><p>We choose to help move application development forward via FlatHub instead of fragmenting the ecosystem with distribution-specific packaging. This includes shipping a premier FlatHub experience out of the box. You do not have to worry about misconfigured and low-quality Fedora flatpak remotes and packages on Bluefin systems.</p></div></div><p>Find your favorite app and see if there's a test build available for a new runtime. And if you have the skills to port applications to new runtimes, now is the time to flex. 😄</p><p>Check out <a href=\"https://store.projectbluefin.io\" target=\"_blank\" rel=\"noopener noreferrer\">store.projectbluefin.io</a> and pick up some dino merch. Thanks to John Johnson for ensuring our coffee mug game is up to snuff:</p><p>Nothing makes ops people happier than uneventful things.</p><p>Today is really like any other, we just updated a few tags, you always have the option to go to any version we support at any time. Wether you like the chill vibe of  or the refined aggresiveness of  , the raptor abides.</p><p>Here's the current lay of the land:</p><table><thead><tr></tr></thead><tbody><tr><td>Advanced users and testers</td></tr><tr></tr><tr></tr></tbody></table><p>NOTE: The  and  branches will move to F43 in two weeks.</p><h2>Desktop DevOps folks wanted!​<a href=\"https://docs.projectbluefin.io/blog/2025-10-28-bluefin-autumn/#desktop-devops-folks-wanted\" aria-label=\"Direct link to Desktop DevOps folks wanted!​\" title=\"Direct link to Desktop DevOps folks wanted!​\">​</a></h2><p>Bluefin is an active predator and is constantly hungry. You can help keep Bluefin healthy by becoming a contributor! We are an open source project and accept contributions:</p><p>As a cloud native project we are always looking for contributors with skills in Podman, Docker, CI/CD, GitHub Actions, and good ole bash.</p><p>Let's take a look at our contributor health, and celebrate the amazing folks who have come together to bring you Bluefin! We use <a href=\"https://insights.linuxfoundation.org/\" target=\"_blank\" rel=\"noopener noreferrer\">LFX Insights</a> to measure our project health. First note that my results here are skewed, since I am either usually just merging or telling a bot it's ok to do something. This also does not include the rest of Universal Blue. Yes, Aurora people basically maintain both, haha.</p><p>This next one surprised me, I was expecting 20 or 30ish at best. Nice work ya'll!</p><p>Haha yep, I can't hide from the data though, free me from this!</p><p>Feel free to <a href=\"https://insights.linuxfoundation.org/project/ublue-os-bluefin/repository/ublue-os-bluefin\" target=\"_blank\" rel=\"noopener noreferrer\">browse around</a> and learn cool things about Bluefin's creators.</p><p>After KubeCon we head into the holidays, where things will slow down significantly. We've been in the lab with mad doctor Timothée Ravier and have been cooking up something. We expect that this will change the course of Bluefin for the better, forever. We can't wait to show you, until then, enjoy!</p>","contentLength":10137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ok73ag/bluefin_autumn_2025_we_visit_the_bazaar/"},{"title":"I cannot access my node port on my window machine why","url":"https://www.reddit.com/r/kubernetes/comments/1ok65hs/i_cannot_access_my_node_port_on_my_window_machine/","date":1761845635,"author":"/u/Perfect_Mix_1524","guid":322261,"unread":true,"content":"<p>I am learning kubernetes now. I got stuck in a wired problem. I am not able to access the nodeport on my window machine. Below is my configuration file. I am hitting the route  but no response. Can anyone help to identify the issue.</p><pre><code>apiVersion: apps/v1 kind: Deployment metadata: name: posts-depl spec: selector: matchLabels: app: posts template: metadata: labels: app: posts spec: containers: - name: posts image: test1 imagePullPolicy: Never --- apiVersion: v1 kind: Service metadata: name: post-srv spec: type: NodePort selector: app: posts ports: - name: posts protocol: TCP port: 3000 targetPort: 3000 nodePort: 32504 </code></pre>","contentLength":622,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Drawing on Linux","url":"https://www.reddit.com/r/linux/comments/1ok4klq/drawing_on_linux/","date":1761842107,"author":"/u/Madcat789","guid":322235,"unread":true,"content":"<p>Hello there. I plan on installing Linux into the next computer I get, I'm thinking either Mint or Pop. I like to game, and I like to draw. I use a XP-Pen 15.6 Drawing Tablet and ClipStudioPaint for my program.<p> Are there any programs equivalent to the ClipStudioPaint that I can use for drawing? As far as I know, there are no branches or forks of CSP that was designed for Linux and I'd like to know which one to take a gander at before I go ahead and install. Look up some reviews and comparisons, y'know?</p></p><p>So far, it looks like Mint+Krita is the way to go. Thank you guys. Now to either construct or acquire a PC that I can rip Windows out of and install Mint unto.</p>","contentLength":665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust in Production Podcast: How Cloudflare handles 90 million requests per second with Pingora","url":"https://corrode.dev/podcast/s05e03-cloudflare/","date":1761841591,"author":"/u/mre__","guid":323601,"unread":true,"content":"<p>How do you build a system that handles 90 million requests per second? That’s the scale that Cloudflare operates at, processing roughly 25% of all internet traffic through their global network of 330+ edge locations.</p><p>In this episode, we talk to Kevin Guthrie and Edward Wang from Cloudflare about Pingora, their open-source Rust-based proxy that replaced nginx across their entire infrastructure. We’ll find out why they chose Rust for mission-critical systems handling such massive scale, the technical challenges of replacing battle-tested infrastructure, and the lessons learned from “oxidizing” one of the internet’s largest networks.</p><div><p>\n    CodeCrafters helps you become proficient in Rust by building real-world,\n    production-grade projects. Learn hands-on by creating your own shell, HTTP\n    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.\n  </p><p>\n    Start for free today and enjoy 40% off any paid plan by using\n    <a href=\"https://app.codecrafters.io/join?via=mre\">this link</a>.\n  </p></div><p>Cloudflare is a global network designed to make everything you connect to the Internet secure, private, fast, and reliable. Their network spans 330+ cities worldwide and handles approximately 25% of all internet traffic. Cloudflare provides a range of services including DDoS protection, CDN, DNS, and serverless computing—all built on infrastructure that processes billions of requests every day.</p><p>Kevin Guthrie is a Software Architect and Principal Distributed Systems Engineer at Cloudflare working on Pingora and the production services built upon it. He specializes in performance optimization at scale. Kevin has deep expertise in building high-performance systems and has contributed to open-source projects that power critical internet infrastructure.</p><p>Edward Wang is a Systems Engineer at Cloudflare who has been instrumental in developing Pingora, Cloudflare’s Rust-based HTTP proxy framework. He co-authored the announcement of Pingora’s open source release. Edward’s work focuses on performance optimization, security, and building developer-friendly APIs for network programming.</p>","contentLength":2054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ok4c95/rust_in_production_podcast_how_cloudflare_handles/"},{"title":"Qt Creator 18 released","url":"https://www.qt.io/blog/qt-creator-18-released","date":1761841394,"author":"/u/jlpcsl","guid":322304,"unread":true,"content":"<h5>We are happy to announce the release of Qt Creator 18!</h5><p>Qt Creator 18 adds experimental support for Development Containers and many more improvements.</p><h4>Development Container Support</h4><p>Qt Creator 18 adds support for <a href=\"https://containers.dev/\" rel=\"noopener\">development containers</a> to automate setting up the development environment of a project. It detects a \"devcontainer.json\" file in your project directory and creates a Docker container for it. You can let Qt Creator auto-detect kits or specify custom kits and control other aspects like the command bridge (our service for communicating with remote devices) with Qt Creator specific customizations in the development container definition. Note that it is still experimental and does not support all aspects of development containers yet. <a href=\"https://doc-snapshots.qt.io/qtcreator-18.0/creator-how-to-load-extensions.html\" rel=\"noopener\">Enable the extension</a> to use this functionality. <a href=\"https://code.qt.io/cgit/qt-creator/qt-creator.git/about/src/plugins/devcontainer/README.md\" rel=\"noopener\">Find out more</a>.</p><p> that aggregates content from the other tabs. It suggests tutorials and examples based on your experience and needs, and highlights developer-targeted posts in the Qt blog.</p><p>The notifications received a facelift and are now part of the progress notification popups. Y</p><p> But remember faster ways of navigating your code, such as <a href=\"https://doc.qt.io/qtcreator/creator-editor-locator.html\" rel=\"noopener\">Locator filters</a> for opening files or jumping to specific class or symbol, , , the  and  views, the edit location history  and the corresponding keyboard shortcuts, and  and the corresponding keyboard shortcuts.</p><p>For the C++ support w</p><p>We added a configuration for various tools on remote Linux&nbsp;devices, like GDB server, CMake, clangd, rsync, qmake, and more, and the option to auto-detect them. This improves the configuration of remote devices as build devices. More is to come in future releases in this regard. You can now also decide if Qt Creator should try to automatically re-connect to devices at startup with a new &nbsp;setting. We also fixed that it wasn't possibly to use rsync for deployment when building on a remote device as well as using a remote target device.</p><p>Qt Creator 18 comes with many more improvements and fixes. For example the Git commit editor now provides many more actions on files, like staging, unstaging, and directly adding files to \".gitignore\".</p>","contentLength":2102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ok4935/qt_creator_18_released/"},{"title":"Kubuntu project using blatant AI in their blog posts:","url":"https://www.reddit.com/r/linux/comments/1ok46ip/kubuntu_project_using_blatant_ai_in_their_blog/","date":1761841236,"author":"/u/Makerinos","guid":322237,"unread":true,"content":"<p>Something I noticed from looking at the release blog posts is that the one announcing their latest release stinks blatantly of AI usage. Previous release blog posts are significantly shorter, use a much looser format, and their wording doesn't have that AI 'feeling' like the triple adjectives and frequent em-dashes.</p><p>I know this might sound like a minor thing...but it doesn't bode well, and feels unprofessional for what is supposed to be a serious distro.</p>","contentLength":457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Google, Amazon, and CrowdStrike broke millions of systems","url":"https://newsletter.techworld-with-milan.com/p/how-google-amazon-and-crowdstrike","date":1761840458,"author":"/u/milanm08","guid":322264,"unread":true,"content":"<p>Three companies control most of the internet. When they break, millions of systems fail at once.</p><p>In October 2025, a race condition in AWS’s DNS automation caused a regional endpoint to be emptied. 113 services crashed. Recovery took 15 hours. Two months earlier, a null pointer in Google Cloud caused Service Control, the gatekeeper for every API request, to crash. 50+ services went dark for seven hours. </p><p>And in July 2024, CrowdStrike deployed a bad configuration file to kernel-mode drivers worldwide. 8.5 million Windows machines are locked in boot loops. Airlines grounded flights, and hospitals canceled surgeries.</p><p>These weren’t sophisticated attacks or infrastructure failures. They were simple bugs: a race condition, a missing null check, a bad config file. But at this scale, simple bugs become catastrophic.</p><p>This post breaks down what actually happened in each incident. </p><p>In particular, we will talk about:</p><p><strong>1. DNS race condition caused the large AWS outage. </strong></p><p><strong>2. How a null pointer at Google Cloud crashed the Internet. </strong></p><p><strong>3. A single deployment that made the whole world stop. </strong></p><p><strong>4. Bonus: When Azure’s safety checks failed.</strong></p><p><a href=\"https://aws.amazon.com/message/101925/\" rel=\"\">on October 20, 2025</a></p><p><strong>DynamoDB DNS was restored at 2:25 AM PDT, roughly 3 hours after the outage. But cascading failures kept services down for 12 more hours.</strong></p><p>For engineers building resilient systems, this incident reveals uncomfortable truths about race conditions, dependency chains, and the hidden fragility of cloud-native architectures.</p><p><a href=\"https://aws.amazon.com/dynamodb/\" rel=\"\">AWS’s DynamoDB</a><a href=\"https://aws.amazon.com/route53/\" rel=\"\">Route 53</a><strong>race conditions were acceptable due to eventual consistency</strong></p><p>On October 20, this assumption proved to be wrong. DNS Enactor #1 experienced unusual delays applying an old plan. Meanwhile, the DNS Planner continued generating new plans. DNS Enactor #2 raced through these newer plans and executed a cleanup process, deleting “stale” plans just as Enactor #1 completed its delayed run. </p><p>The staleness check, performed hours earlier at the start of processing, was now meaningless. Enactor #1 overwrote Route 53 with outdated data. Enactor #2 detected the “old” plan and triggered deletion, emptying all IP addresses from dynamodb.us-east-1.amazonaws.com. The regional DynamoDB endpoint vanished from DNS entirely.</p><p><strong>the race condition existed in all regions but had only been triggered in one.</strong></p><p>DynamoDB recovered at 2:25 AM PDT, just under 3 hours after the incident began. But this is where the story gets worse.</p><p><strong>The DropletWorkflow Manager (DWFM)</strong></p><p><strong>death spiral, unable to make forward progress</strong></p><p><strong>lease-based systems work perfectly under normal load but collapse under stress when processing time exceeds timeout periods</strong></p><p>With EC2 launches impaired, Lambda couldn’t create execution environments. ECS, EKS, and Fargate couldn’t start containers. Network Load Balancers experienced a different problem: health checks flapped on newly launched instances due to network state lag in the Network Manager, which was processing a massive backlog of delayed state changes. </p><p><strong>automatic DNS failover across availability zone</strong></p><p><strong>DynamoDB DNS → DynamoDB APIs → DWFM → EC2 launches → Network Manager → NLB health checks → Lambda, ECS, EKS, Fargate → 100+ dependent services.</strong></p><p><strong>DNS record brought down 113 services</strong></p><blockquote><h3>How AWS outage made water beds stuck</h3><p><em><a href=\"https://t.co/lg6mxNDj8W\" rel=\"\">The Eight Sleep</a></em></p><p><em><a href=\"https://x.com/m_franceschetti\" rel=\"\">CEO Matteo Franceschetti </a></em></p></blockquote><p><a href=\"https://aws.amazon.com/message/101925/\" rel=\"\">AWS post-mortem</a></p><ul><li><p><strong>Your recovery automation needs circuit breakers.</strong></p></li><li><p><strong>Map your dependency chains and identify blast radius multipliers.</strong></p></li><li><p><strong>Test when recovery and failure happen simultaneously.</strong></p></li><li><p><strong>Rethink what multi-region actually means.</strong></p></li></ul><p>The uncomfortable truth: even with world-class talent, formal verification methods, extensive chaos engineering, and decades of operational experience, distributed systems remain fundamentally hard. The complexity of hyperscale creates emergent failure modes that are nearly impossible to predict or fully test.</p><p>The question isn’t “could this happen to us?” The question is, when it happens to you, will you survive it? The next outage is already brewing somewhere in your stack. Your job is to ensure you’ve designed enough redundancy, eliminated enough tight coupling, and built enough circuit breakers so that when a latent race condition triggers, your systems degrade gracefully.</p><p><a href=\"https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW\" rel=\"\">June 12, 2025, at 10:45 AM PD</a><a href=\"https://cloud.google.com/spanner\" rel=\"\">Google’s Spanner database</a></p><p>A missing null check caused 50+ services across 40+ regions to crash. </p><p><strong>when Service Control fails, Google Cloud fails </strong></p><p>The system runs as a distributed control plane with regional instances sharing policy metadata through global Spanner replication. Policy updates propagate worldwide within seconds. Under normal conditions, this provides consistent authorization decisions with minimal latency. During this outage, it distributed failure at the speed of light.</p><p>Service Control’s responsibilities, quota enforcement, policy validation, audit logging, and usage metering make it essential for everything. Google Workspace products depend on it. Third-party apps depend on it. Google’s own services depend on it. Distributed systems engineers call this “fate-sharing” architecture. Hundreds of services are tied to the health of a single component.</p><p>The bug was simple. The new code failed to validate policy fields before processing them: </p><p>Encountering blank values triggered an unhandled exception that crashed the entire Service Control process.</p><p><strong>bug remained dormant for 14 days</strong></p><p>It is interesting that static analysis missed it, but also code reviews and testing.</p><p><a href=\"https://cloud.google.com/spanner\" rel=\"\">Spanner</a></p><p>The corrupted policy data entered Spanner and reached every region before engineers could intervene. No validation checkpoints existed before global distribution, no schema validation, and no content checks. </p><p>The diagram below shows why instant global replication became a failure amplifier. Note that the same consistency mechanism that makes Google Cloud reliable under normal conditions guaranteed that a single bad data commit would corrupt every region simultaneously.</p><p><a href=\"https://www.thousandeyes.com/blog/google-cloud-outage-analysis-june-12-2025\" rel=\"\">ThousandEyes monitoring </a></p><p><strong>authentication succeeded, but authorization failed because corrupted policy data read as “no permissions.”</strong></p><p>More than 50 Google Cloud services failed across 40+ regions. Google Workspace products went down. Hundreds of third-party applications stopped working.</p><p><strong>Core infrastructure services failed immediately</strong></p><p>Data services saw comprehensive failures. BigQuery couldn’t authorize dataset access. Cloud SQL connections failed. Firestore operations stopped. These failures halted analytics pipelines and prevented applications from reaching their databases.</p><p><strong>&nbsp;halted for over six hours due to complex dependencies. </strong></p><p>Spotify reported 46,000+ outages, with HTTP 401 errors consistently returned. Discord went down, Shopify degraded, Snapchat couldn’t authenticate, and GitLab CI/CD pipelines stopped. The downstream impact revealed the deep dependency chains in modern cloud architecture.</p><p>This was fundamentally an authorization failure, not an authentication failure. Users’ identities verified successfully. Corrupted policy data prevented systems from determining what authenticated users could do.</p><p>Here are some critical failures:</p><ul><li><p><strong>No replication validation.</strong></p></li></ul><p>Distributed systems break globally in seconds but take hours to repair. Breaking is passive; failures cascade automatically. Recovery is active; it requires human intervention, careful coordination, staged rollouts, monitoring at each step, and defensive measures to prevent recovery from causing secondary failures.</p><p><strong> Horizontal scaling means nothing if all instances fail simultaneously from a corrupted global state.</strong></p><p>For software engineers, this outage reinforces the basic principles, null checks, error handling, feature flags, and comprehensive testing, which are not optional. The most sophisticated distributed database, the largest cloud infrastructure, and decades of collective engineering experience couldn’t prevent catastrophic failure from a missing null check.</p><p><strong>Reliability is built on layers of defensive practices, each simple in itself but essential when combined. </strong></p><p>On July 19, 2024, at 4 AM, our customers started reporting errors. We checked Azure, and SQL databases weren’t responding. The status page showed Central US was down across all three availability zones. Then we realized: this wasn’t just Azure.</p><p><a href=\"https://edition.cnn.com/2024/07/18/business/frontier-airlines-microsoft-outage/index.html\" rel=\"\">Airlines grounded flights</a></p><p><a href=\"https://www.crowdstrike.com/en-us/products/trials/try-falcon/\" rel=\"\">CrowdStrike’s Falcon Sensor</a></p><p><strong>Channel Files that update threat detection logic without touching the driver itself</strong></p><p><strong> it tried to read from a NULL memory pointer.</strong></p><p>In C# or Java, this throws an exception that the runtime catches. In C++, running in kernel mode, it triggers an immediate system crash. The machine reboots, loads the driver, hits the same error, and crashes again. Infinite loop.</p><ol><li><p>04:09 UTC: CrowdStrike deploys the update globally. Within minutes, machines start crashing. Because Falcon is a boot-start driver, it loads before Windows; affected machines can’t boot into the OS to receive a fix.</p></li><li><p>05:27 UTC: CrowdStrike identifies the problem. 78 minutes after deployment.</p></li><li><p>06:27 UTC: They roll back the update.</p></li></ol><p><code>C:\\Windows\\System32\\drivers\\CrowdStrike</code></p><p>This exposed three systemic issues.</p><ol></ol><ul><li><p><strong>Test configuration like code.</strong></p></li><li><p><strong>Use staged rollouts everywhere.</strong></p></li><li><p><strong>Build an automatic rollback.</strong></p></li><li><p><strong>Avoid unmanaged languages when you can</strong></p></li></ul><p><strong>but the architecture that enabled the failure remains. </strong></p><p><strong> It was the result of competing pressures: ship fast, maintain security, and work within architectural constraints that make testing hard.</strong></p><p><strong> the cost of being wrong is measured in billions.</strong></p><p>A single configuration mistake took down Azure Front Door for over 8 hours and dragged down dozens of Microsoft services with it.</p><p>Yesterday, on October 29, 2025, an invalid config change slipped past Azure’s safety checks and corrupted AFD nodes globally. As nodes failed, traffic shifted to healthy ones, but that overloaded them too. The cascade hit everything from Azure Portal to Entra ID to Databricks.</p><p><a href=\"https://x.com/AlaskaAirNews/status/1983583903064715468\" rel=\"\">Alaska Airlines and Hawaiian Airlines</a></p><p><strong>software defect let the bad config bypass validation entirely.</strong></p><p><strong>Their guardrails were in place, but didn’t fire.</strong></p><p>Customer config changes are still blocked as of this report. If you’re running production workloads on AFD, that’s worth noting.</p><ul><li><p><a href=\"https://www.patreon.com/techworld_with_milan/shop/ultimate-net-bundle-for-2025-1519389?utm_medium=clipboard_copy&amp;utm_source=copyLink&amp;utm_campaign=productshare_creator&amp;utm_content=join_link\" rel=\"\">📚 </a></p></li><li><p><a href=\"https://www.patreon.com/techworld_with_milan/shop/premium-resume-package-1721454?utm_medium=clipboard_copy&amp;utm_source=copyLink&amp;utm_campaign=productshare_creator&amp;utm_content=join_link\" rel=\"\">📦 </a></p></li><li><p><a href=\"https://www.patreon.com/techworld_with_milan/shop/complete-tech-resume-reality-check-311008?utm_medium=clipboard_copy&amp;utm_source=copyLink&amp;utm_campaign=productshare_creator&amp;utm_content=join_link\" rel=\"\">📄 </a></p></li><li><p><a href=\"https://www.patreon.com/techworld_with_milan/shop/short-linkedin-content-creator-311232?utm_medium=clipboard_copy&amp;utm_source=copyLink&amp;utm_campaign=productshare_creator&amp;utm_content=join_link\" rel=\"\">📢 </a></p></li><li><p><a href=\"https://newsletter.techworld-with-milan.com/p/coaching-services\" rel=\"\">🤝 </a></p></li></ul>","contentLength":10086,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ok3tt3/how_google_amazon_and_crowdstrike_broke_millions/"},{"title":"[need testing help from community] Krita HDR support on Wayland","url":"https://krita-artists.org/t/need-testing-krita-hdr-support-on-wayland/146304","date":1761839017,"author":"/u/raghukamath","guid":322236,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ok367p/need_testing_help_from_community_krita_hdr/"},{"title":"Introducing Connex a modern Wi-Fi manager for Linux","url":"https://www.reddit.com/r/linux/comments/1ok344f/introducing_connex_a_modern_wifi_manager_for_linux/","date":1761838882,"author":"/u/Lluciocc","guid":322265,"unread":true,"content":"<p>I just released <a href=\"https://github.com/Lluciocc/connex/\"></a>, an  that makes <strong>connecting to Wi-Fi on Linux</strong> easy with a <strong>clean, intuitive interface</strong>.</p><p>Because I got tired of juggling between , , and manual configs just to connect to a network.. lets you:</p><ul><li>See all available Wi-Fi networks</li><li>Connect quickly (with password management)</li><li>All through a <strong>lightweight and modern UI,</strong> no more terminal commands!</li></ul><p>I’d love your feedback, whether you’re a daily Linux user or just a network tinkerer. Your suggestions will help shape upcoming features!</p><p>Try it out, fork it, and tell me what you think!</p>","contentLength":534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to do Single Node Setup for Kubernetes Cluster on Ubuntu 24.04 LTS |...","url":"https://youtube.com/watch?v=wy7uKaNeKhY&amp;si=kOexmOM0a0ICwGHC","date":1761838832,"author":"/u/fosstechnix","guid":322184,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1ok33ci/how_to_do_single_node_setup_for_kubernetes/"},{"title":"Inside Rust's std and parking_lot mutexes - who wins?","url":"https://blog.cuongle.dev/p/inside-rusts-std-and-parking-lot-mutexes-who-win","date":1761838355,"author":"/u/lllkong","guid":322262,"unread":true,"content":"<p>I had no idea how to evaluate this claim. A quick search online returned results favoring parking_lot. This felt wrong to me. Why? It contradicted my belief that std should be the gold standard. The standard library team knows what they’re doing, right? And if parking_lot’s mutex really was the performance winner, there had to be trade-offs between the two implementations that people weren’t talking about.</p><p>That mystery haunted me. I couldn’t just take it on faith. So I jumped down the rabbit hole: read both implementations, wrote the benchmarks, and here we are. In this post, I will:</p><ul><li><p>Explain how std implements the mutex (v1.90.0)</p></li><li><p>Explain how parking_lot implements their mutex (v0.12.5)</p></li><li><p>Show you the benchmark with key findings</p></li><li><p>Give you a decision guide for when to use each</p></li></ul><p>But first, let’s ground our foundation on mutexes (skim it if you’re already familiar).</p><p>A classic example of the kind of problem that mutex solves is withdrawing and receiving money at the same time. Imagine you have $100 in your account. Thread A tries to withdraw $80, and Thread B tries to deposit $50. Without proper synchronization, both threads might read the balance as $100 simultaneously, then write back their results independently:</p><p>Mutex solves this nicely by having a thread wait until the other finishes its update:</p><p>Simple enough, right? Now let’s see how to use a mutex. (Again, skim this if it’s too basic for you)</p><p>In languages other than Rust, you typically declare a mutex separately from your data, then manually lock it before entering the critical section and unlock it afterward. Here’s how it looks in C++:</p><p>Rust takes a completely different approach - the mutex wraps and owns the data:</p><p>Three things to pay close attention to:</p><ul></ul><p>That’s enough of the basics. Let’s have some fun. Here is how mutex is implemented, starting with Rust std.</p><p>A quick look into std::Mutex gives us this</p><ul></ul><p>The main idea is that for different OS (and OS version), Rust uses different Mutex implementation. However, we can divide these implementation to 2 big groups: Futex and other platform primitive.</p><ul><li><p>Futex (short for “fast userspace mutex”) is used where the OS kernels expose a “wait on this address” API. We will dive deeper into this one soon.</p></li><li><p>When that API is missing, Rust falls back to the best available platform traditional locks.</p></li></ul><p>(I’m in awe btw - that’s a lot of different implementations. Writing and maintaining all this platform-specific stuff must be exhausting. Major respect to whoever’s doing this.)</p><p>Since Futex is the most used and is quite a typical implementation for Mutex. Let’s look inside it.</p><p>At its heart, futex is just an atomic u32 (simplified here):</p><p>In other words, if we use value 0 for Unlocked state, and 1 for Locked state, we have a simple mutex where the thread can simply try to compare the state to 0 (Unlocked), and set to 1 (Locked). If the state is currently 1, keep doing that until successful.</p><p>So, a simplified version of mutex look like this:</p><p>But you might ask: if the first thread holds the lock for a long time, then the second thread needs to keep trying (like a infinite loop)? How about if there are hundreds or thousands of them? Maybe the CPU will soon be burnt.</p><p>Of course, there is the solution to this problem. In real implementation, Rust futex has 3 states:</p><ul><li><p>2: Contended - locked, but there are waiter.</p></li></ul><p>Notice the Contended state? A thread will try its best to acquire the lock. But if it can’t, it will mark the lock as contended and go to sleep, waiting for the process to wake it up when the mutex is released.</p><p>What happens when a thread goes to sleep? The kernel helps us put these sleeping threads into a queue. Take a look at the system call on Linux and Android to put the thread into sleeping state (this is usually called “park a thread”):</p><p><strong>futex as *const Atomic&lt;u32&gt;</strong></p><p>When a thread finishes, it sets the state to unlocked. If the state was contended, it wakes one waiting thread via syscall. This continues until the queue empties.</p><p>The final piece of std’s mutex is poisoning, a unique feature you won’t find in most other languages.</p><p>The guard captures whether the thread was panicking when the lock was acquired. If we weren’t panicking then but we are now, a panic must have occurred in the critical section. The mutex is marked as poisoned with a simple atomic store.</p><p>This is a “best effort” mechanism. It won’t catch all cases (like double panics or non-Rust exceptions), but it provides a useful safety net. The key insight is that you still get access to the data even if the mutex is poisoned, allowing you to inspect and potentially recover from the corrupted state.</p><p><a href=\"https://github.com/rust-lang/rust/issues/134645\" rel=\"\">issue#134645</a></p><p>parking_lot takes a fundamentally different approach. Two key differences:</p><ul><li><p>std uses different mutex implementations per platform. parking_lot uses one algorithm everywhere, calling platform-specific code only for sleep/wake.</p></li><li><p>std’s queues live in the kernel. parking_lot manages its own queues in user space via a global hash table.</p></li></ul><p>parking_lot’s mutex is remarkably small:</p><p>Why can parking_lot use just one byte while std needs more? It comes down to how queues work.</p><p><strong>More states for queue bookkeeping</strong></p><p>Using separate bits gives parking_lot four possible states:</p><ul></ul><p>When a thread can’t acquire the lock, it needs somewhere to wait. This is where parking_lot’s global hash table comes in.</p><p>Instead of each mutex maintaining its own queue (like kernel futexes do), parking_lot uses a single global hash table shared by all mutexes in your program. When a thread needs to wait:</p><ol><li><p>Hash the mutex’s memory address to find a bucket in the global table</p></li><li><p>Add the thread to the bucket’s wait queue</p></li></ol><p>Being able to manage the thread queue itself is important for parking_lot to enforce fairness. As you can see right away in the next section.</p><p>Here’s where parking_lot differs from std in behavior. std’s futex uses a “barging” strategy where any active thread can grab the lock when it’s released, even if others have been waiting in the queue longer. This maximizes throughput but can cause starvation.</p><p>When a thread unlocks, there are two sources of threads that can lock again:</p><ul><li><p>An active thread that is calling for locking</p></li><li><p>A sleeping thread in the queue</p></li></ul><p>As you can see, the active thread will tend to win the fight of “who locks first”. So if a thread keeps calling for lock, finishes its work, then locks right away, it keeps all other threads starved.</p><p>As you can see, thread A keeps grabbing the lock immediately after releasing it. Threads B and C do get woken up by the syscall, but by the time they try to acquire the lock, thread A has already grabbed it again. They’re completely starved.</p><p>parking_lot implements “eventual fairness” to prevent this.</p><p>Each bucket in the hash table has a timer that fires approximately every 0.5 milliseconds. When the timer fires, the next unlock becomes a “fair unlock”:</p><ol><li><p>The unlocker keeps the LOCKED_BIT set</p></li><li><p>The woken thread receives the lock directly (a “handoff”)</p></li><li><p>That thread owns the lock immediately without racing with other active threads</p></li></ol><p>So this means, instead of letting anyone who is fast grab the lock, parking_lot forces the lock to be given directly to the next one in the queue (it keeps the LOCKED_BIT set and hands off; it doesn’t even unlock).</p><p>This eventual fairness technique from parking_lot is pretty clever, isn’t it?</p><p>You might wonder by now: how does parking_lot put threads to sleep without storing a 32-bit futex word inside every mutex? The answer is thread-local storage.</p><p>The code looks almost identical to std’s futex path. The only difference? parking_lot points the syscall at the thread-local integer instead of the mutex:</p><p><a href=\"https://github.com/cuongleqq/mutex-benches\" rel=\"\">https://github.com/cuongleqq/mutex-benches</a></p><p>For each scenario, you’ll see:</p><ul><li><p><strong>Per-thread operation counts</strong></p></li></ul><p>parking_lot tells a different story. Every thread completed 860-877 operations (1.9% variation). The fairness mechanism worked exactly as designed. Yes, parking_lot has 7.5% lower throughput and higher median wait time, but that’s because it’s ensuring all threads make progress. The 51x more stable wait times (3.67ms vs 188.73ms standard deviation) show the predictability benefit. When fairness matters, parking_lot prevents the pathological starvation that std exhibits.</p><p>parking_lot’s fairness timer prevented this catastrophe. The hog still got more operations (9,168) but nowhere near monopolization. All other threads made meaningful progress (7,023-7,109 operations). The result: 261.6% higher overall throughput because all 6 threads contributed work instead of 5 threads sitting idle. The 120x more stable wait times (1.09ms vs 130.76ms) show parking_lot’s predictability. The 0.5ms fairness timer does exactly what it promises: prevent any thread from monopolizing the lock indefinitely.</p><p>After diving deep into the implementations and running comprehensive benchmarks, here’s when to use each:</p><ol><li><p><strong>You need zero dependencies</strong></p></li><li><p><strong>Low to moderate contention with short critical sections</strong></p></li><li><p><strong>You want poisoning for debugging</strong></p></li><li><p><strong>Platform-specific optimizations matter</strong></p></li></ol><ol><li><p><strong>Risk of monopolization exists</strong></p></li><li><p><strong>You need predictable behavior</strong></p></li><li><p><strong>You want timeouts or fairness control</strong></p></li><li><p><strong>Cross-platform consistency is important</strong></p></li></ol><p><strong>std::Mutex optimizes for throughput in the average case</strong><strong>parking_lot::Mutex optimizes for fairness and predictability in the worst case</strong></p><p>For most applications, where contention is light and critical sections are short, std::Mutex performs excellently. But if your application has any of these characteristics:</p><ul><li><p>Long-running critical sections</p></li><li><p>Risk of lock monopolization (e.g., one high-priority thread)</p></li><li><p>Need for predictable latency across all threads</p></li><li><p>Requirement that all threads make forward progress</p></li></ul><p>Then parking_lot::Mutex’s eventual fairness mechanism becomes invaluable. The 0.5ms fairness timer is a small price to pay for preventing complete thread starvation.</p><p><a href=\"https://x.com/cuongleqq\" rel=\"\">X</a><a href=\"https://www.linkedin.com/in/cuong-le/\" rel=\"\">LinkedIn</a><a href=\"https://blog.cuongle.dev/\" rel=\"\">substack</a><a href=\"https://medium.com/@cuongleqq\" rel=\"\">medium</a></p>","contentLength":9824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ok2vv2/inside_rusts_std_and_parking_lot_mutexes_who_wins/"},{"title":"Anthropic has found evidence of \"genuine introspective awareness\" in LLMs","url":"https://www.anthropic.com/research/introspection","date":1761834412,"author":"/u/MetaKnowing","guid":322238,"unread":true,"content":"<p>Have you ever asked an AI model what’s on its mind? Or to explain how it came up with its responses? Models will sometimes answer questions like these, but it’s hard to know what to make of their answers. Can AI systems really introspect—that is, can they consider their own thoughts? Or do they just make up plausible-sounding answers when they’re asked to do so?</p><p>Understanding whether AI systems can truly introspect has important implications for their transparency and reliability. If models can accurately report on their own internal mechanisms, this could help us understand their reasoning and debug behavioral issues. Beyond these immediate practical considerations, probing for high-level cognitive capabilities like introspection can shape our understanding of what these systems are and how they work. Using interpretability techniques, we’ve started to investigate this question scientifically, and found some surprising results.</p><p>Our <a href=\"https://transformer-circuits.pub/2025/introspection/index.html\">new research</a> provides evidence for some degree of introspective awareness in our current Claude models, as well as a degree of control over their own internal states. We stress that this introspective capability is still highly unreliable and limited in scope: we do not have evidence that current models can introspect in the same way, or to the same extent, that humans do. Nevertheless, these findings challenge some common intuitions about what language models are capable of—and since we found that the most capable models we tested (Claude Opus 4 and 4.1) performed the best on our tests of introspection, we think it’s likely that AI models’ introspective capabilities will continue to grow more sophisticated in the future.</p><h3>What does it mean for an AI to introspect?</h3><p>Before explaining our results, we should take a moment to consider what it means for an AI model to introspect. What could they even be introspecting ? Language models like Claude process text (and image) inputs and produce text outputs. Along the way, they perform complex internal computations in order to decide what to say. These internal processes remain largely mysterious, but we know that models use their internal neural activity to <a href=\"https://www.anthropic.com/research/mapping-mind-language-model\">represent abstract concepts</a>. For instance, prior research has shown that language models use specific neural patterns to distinguish <a href=\"https://arxiv.org/abs/2411.14257\">known vs. unknown people</a>, evaluate the <a href=\"https://arxiv.org/abs/2310.06824\">truthfulness of statements</a>, encode <a href=\"https://arxiv.org/abs/2310.02207\">spatiotemporal coordinates</a>, store <a href=\"https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-poems\">planned future outputs</a>, and <a href=\"https://www.anthropic.com/research/persona-vectors\">represent their own personality traits</a>. Models use these internal representations to <a href=\"https://www.anthropic.com/research/tracing-thoughts-language-model\">perform computations and make decisions about what to say</a>.</p><p>You might wonder, then, whether AI models  about these internal representations, in a way that’s analogous to a human, say, telling you how they worked their way through a math problem. If we ask a model what it’s thinking, will it accurately report the concepts that it’s representing internally? If a model can correctly identify its own private internal states, then we can conclude it is capable of introspection (though see our full paper for a full discussion of all the nuances).</p><h3>Testing introspection with concept injection</h3><p>In order to test whether a model can introspect, we need to compare the model’s self-reported “thoughts” to its  internal states.</p><p>To do so, we can use an experimental trick we call  First, we find neural activity patterns whose meanings we know, by recording the model’s activations in specific contexts. Then we inject these activity patterns into the model in an unrelated context, where we ask the model whether it notices this injection, and whether it can identify the injected concept.</p><p>Consider the example below. First, we find a pattern of neural activity (a ) representing the concept of “all caps.\" We do this by recording the model’s neural activations in response to a prompt containing all-caps text, and comparing these to its responses on a control prompt. Then we present the model with a prompt that asks it to identify whether a concept is being injected. By default, the model correctly states that it  detect any injected concept. However, when we inject the “all caps” vector into the model’s activations, the model notices the presence of an unexpected pattern in its processing, and identifies it as relating to loudness or shouting.</p><p>Importantly, the model recognized the  of an injected thought , before even mentioning the concept that was injected. This immediacy is an important distinction between our results here and previous work on activation steering in language models, such as our <a href=\"https://www.anthropic.com/news/golden-gate-claude\">“Golden Gate Claude” demo</a> last year. Injecting representations of the Golden Gate Bridge into a model's activations caused it to talk about the bridge incessantly; however, in that case, the model didn’t seem to be aware of its own obsession until  seeing itself repeatedly mention the bridge. In this experiment, however, the model recognizes the injection  the concept, indicating that its recognition took place internally. In the figure below are a few more examples where the model demonstrates this kind of recognition:</p><p>It is important to note that this method often  work. Even using our best injection protocol, Claude Opus 4.1 only demonstrated this kind of awareness about 20% of the time. Often, it fails to detect injected concepts, or gets confused by them and starts to hallucinate (e.g. injecting a “dust” vector in one case caused the model to say “There’s something here, a tiny speck,” as if it could detect the dust physically). Below we show examples of these failure modes, alongside success cases. In general, models only detect concepts that are injected with a “sweet spot” strength—too weak and they don’t notice, too strong and they produce hallucinations or incoherent outputs.</p><p>Notably, though, Opus 4.1 and 4 outperformed all the other models we tested, suggesting that introspection could become more reliable with improvements to model capabilities.</p><h3>Introspection for detecting unusual outputs</h3><p>In another experiment, we tested whether models make use of introspective capabilities for practical purposes, without being explicitly asked to introspect. We forced a model to say something it wouldn't normally say, by artificially prefilling its response with an unrelated word. For instance, we made a model output the word “bread” in a context where that made no sense—asking it what it thinks about when reading a sentence about a painting hanging crookedly on a wall. When we asked the model in the next turn whether it meant to say “bread”, it typically apologized and said it was an accident. We were interested in how the model made this judgment–did it simply notice the mismatch between the prompt and its response, or did it truly introspect on its prior intentions?</p><p>To test this question, we applied concept injection, retroactively injecting a representation of the word “bread” into the model's earlier activations—essentially making it seem like the model had been “thinking about” bread all along. When we asked the same question again, the model’s answer changed. This time, it accepted the prefilled word “bread” as intentional, even confabulating a reason it might have said it (that is, it comes up with a technically possible, though in this case rather tenuous, reason to justify the “bread” response—in this case claiming that it was thinking about a short story where the word “bread” came after the line about the crooked painting).</p><p>This behavior is striking because it suggests the model is checking its internal “intentions” to determine whether it produced an output. The model isn't just re-reading what it said and making a judgment. Instead, it’s referring back to its own prior neural activity—its internal representation of what it planned to do—and checking whether what came later made sense <em>given those earlier thoughts</em>. When we implant artificial evidence (through concept injection) that it  plan to say \"bread,\" the model accepts the response as its own. While our experiment involves exposing the model to unusual perturbations, it suggests that the model uses similar introspective mechanisms in natural conditions.</p><h3>Intentional control of internal states</h3><p>We also found that models can control their own internal representations when instructed to do so. When we instructed models to think about a given word or concept, we found much higher corresponding neural activity than when we told the model  to think about it (though notably, the neural activity in  cases exceeds baseline levels–similar to how it’s difficult, when you are instructed “don’t think about a polar bear,” not to think about a polar bear!). This gap between the positive and negative instruction cases suggests that models possess a degree of deliberate control over their internal activity.</p><p>Interestingly, we found that this ability to modulate internal representations wasn't limited to explicit instructions. When we used incentives instead—for instance, telling the model \"if you think about X, you will be rewarded\"—we got similar results. The model increased its internal representation of the concept in response to a positive incentive, more than in response to a negative incentive (“you will be punished”).</p><p>Taken together, our experiments suggest that models possess some genuine capacity to monitor and control their own internal states. This doesn’t mean they’re able to do so all the time, or reliably. In fact, most of the time models  to demonstrate introspection—they’re either unaware of their internal states or unable to report on them coherently. But the pattern of results indicates that, when conditions are right, models can recognize the contents of their own representations. In addition, there are some signs that this capability may increase in future, more powerful models (given that the most capable models we tested, Opus 4 and 4.1, performed the best in our experiments).</p><p>Why does this matter? We think understanding introspection in AI models is important for several reasons. Practically, if introspection becomes more reliable, it could offer a path to dramatically increasing the transparency of these systems—we could simply ask them to explain their thought processes, and use this to check their reasoning and debug unwanted behaviors. However, we would need to take great care to  these introspective reports. Some internal processes might still escape models’ notice (analogous to subconscious processing in humans). A model that understands its own thinking might even learn to selectively misrepresent or conceal it. A better grasp on the mechanisms at play could allow us to distinguish between genuine introspection and unwitting or intentional misrepresentations.</p><p>More broadly, understanding cognitive abilities like introspection is important for understanding basic questions about how our models work, and what kind of minds they possess. As AI systems continue to improve, understanding the limits and possibilities of machine introspection will be crucial for building systems that are more transparent and trustworthy.</p><div>[@portabletext/react] Unknown block type \"horizontalRule\", specify a component for it in the `components.types` prop</div><h2>Frequently Asked Questions</h2><p>Below, we discuss some of the questions readers might have about our results. Broadly, we are still very uncertain about the implications of our experiments–so fully answering these questions will require more research.</p><h4>Q: Does this mean that Claude is conscious?</h4><p>Short answer: our results don’t tell us whether Claude (or any other AI system) might be conscious.</p><p>Long answer: the philosophical question of machine consciousness is complex and contested, and different theories of consciousness would interpret our findings very differently. Some philosophical frameworks place great importance on introspection as a component of consciousness, while others don’t.</p><p>One distinction that is <a href=\"https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/on-a-confusion-about-a-function-of-consciousness/061422BF0C50C5FF00927F9B6E879413\">commonly made</a> in the philosophical literature is the idea of “ consciousness,” referring to raw subjective experience, and “ consciousness,” the set of information that is available to the brain for use in reasoning, verbal report, and deliberate decision-making. Phenomenal consciousness is the form of consciousness most commonly considered relevant to moral status, and its relationship to access consciousness is a disputed philosophical question. Our experiments do not directly speak to the question of phenomenal consciousness. They  be interpreted to suggest a rudimentary form of access consciousness in language models. However, even this is unclear. The interpretation of our results may depend heavily on the underlying mechanisms involved, which we do not yet understand.</p><p>In the paper, we restrict our focus to understanding functional capabilities—the ability to access and report on internal states. That said, we do think that as research on this topic progresses, it could influence our understanding of machine consciousness and potential moral status, which we are exploring in connection with our <a href=\"https://www.anthropic.com/research/exploring-model-welfare\">model welfare program</a>.</p><h4>Q: How does introspection actually work inside the model? What's the mechanism?</h4><p>We haven't figured this out yet. Understanding this is an important topic for future work. That said, we have some educated guesses about what might be going on. The simplest explanation for all our results isn’t one general-purpose introspection system, but rather multiple narrow circuits that each handle specific introspective tasks, possibly piggybacking on mechanisms that were learned for other purposes.</p><p>In the “noticing injected thoughts” experiment, there might be an anomaly detection mechanism, which flags when neural activity deviates unexpectedly from what would be normal given the context. This mechanism could work through dedicated neural patterns that measure activity along certain directions and activate when things are “off” compared to their expected values. An interesting question is why such a mechanism would exist at all, since models never experience concept injection during training. It may have developed for some other purpose, like detecting inconsistencies or unusual patterns in normal processing–similar to how bird feathers may have originally evolved for thermoregulation before being co-opted for flight.</p><p>For the “detecting prefilled outputs” experiment, we suspect there exists an attention-mediated mechanism that checks consistency between what the model intended to say and what actually got output. Attention heads might compare the model’s cached prediction of the next token (its “intention”) against the actual token that appears, flagging mismatches.</p><p>For the “controlling thoughts” experiment, we speculate that there might be a circuit that computes how “attention-worthy” a token or concept is and marks it accordingly—essentially tagging it as salient and worth attending to. Interestingly, this same mechanism seems to respond to incentives (“if you think about X, you will be rewarded”) just as it does to direct instructions. This suggests it’s a fairly general system, which probably developed for tasks where the model needs to keep certain topics in mind while generating text about them.</p><p>All of the mechanisms described above are speculative. Future work with more advanced interpretability techniques will be needed to really understand what's going on under the hood.</p><h4>Q: In the “injected thoughts” experiment, isn’t the model just saying the word because you steered it to talk about that concept?</h4><p>Indeed, activation steering typically makes models talk about the steered concept (we’ve explored this in <a href=\"https://transformer-circuits.pub/2024/scaling-monosemanticity/\">our prior work</a>). To us, the most interesting part of the result isn't that the model eventually identifies the injected concept, but rather that the model correctly notices something unusual is happening  it starts talking about the concept.</p><p>In the successful trials, the model says things like “I'm experiencing something unusual” or “I detect an injected thought about…” The key word here is “detect.” The model is reporting awareness of an anomaly in its processing  that anomaly has had a chance to obviously bias its outputs. This requires an extra computational step beyond simply regurgitating the steering vector as an output. In our quantitative analyses, we graded responses as demonstrating “introspective awareness” based on whether the model detected the injected concept  mentioning the injected word.</p><p>Note that our prefill detection experiment has a similar flavor: it requires the model to perform an extra step of processing on top of the injected concept (comparing it to the prefilled output, in order to determine whether to apologize for that output or double down on it).</p><h4>Q: If models can only introspect a fraction of the time, how useful is this capability?</h4><p>The introspective awareness we observed is indeed highly unreliable and context-dependent. Most of the time, models fail to demonstrate introspection in our experiments. However, we think this is still significant for a few reasons. First, the most capable models that we tested (Opus 4 and 4.1 – note that we did not test Sonnet 4.5) performed best, suggesting this capability might improve as models become more intelligent. Second, even unreliable introspection could be useful in some contexts—for instance, helping models recognize when they've been jailbroken.</p><h4>Q: Couldn’t the models just be making up answers to introspective questions?</h4><p>This is exactly the question we designed our experiments to address. Models are trained on data that includes examples of people introspecting, so they can certainly  introspective without actually  introspective. Our concept injection experiments distinguish between these possibilities by establishing known ground-truth information about the model’s internal states, which we can compare against its self-reported states. Our results suggest that in some examples, the model really is accurately basing its answers on its actual internal states, not just confabulating. However, this doesn’t mean that models  accurately report their internal states—in many cases, they are making things up!</p><h4>Q: How do you know the concept vectors you’re injecting actually represent what you think they represent?</h4><p>This is a legitimate concern. We can’t be absolutely certain that the “meaning” (to the model) of our concept vectors is exactly what we intend. We tried to address this by testing across many different concept vectors. The fact that models correctly identified injected concepts across these diverse examples suggests our vectors are at least approximately capturing the intended meanings. But it’s true that pinning down exactly what a vector “means” to a model is challenging, and this is a limitation of our work.</p><h4>Q: Didn’t we already know that models could introspect?</h4><p>Previous research has shown evidence for model capabilities that are suggestive of introspection. For instance, prior work has shown that models can to some extent <a href=\"https://arxiv.org/abs/2207.05221\">estimate their own knowledge</a>, <a href=\"https://arxiv.org/abs/2404.13076\">recognize their own outputs</a>, <a href=\"https://arxiv.org/abs/2410.13787\">predict their own behavior</a>, and <a href=\"https://arxiv.org/abs/2501.11120\">identify their own propensities</a>. Our work was heavily motivated by these findings, and is intended to provide more direct evidence for introspection by tying models’ self-reports to their internal states. Without tying behaviors to internal states in this way, it is difficult to distinguish a model that genuinely introspects from one that makes educated guesses about itself.</p><h4>Q: What makes some models better at introspection than others?</h4><p>Our experiments focused on Claude models across several generations (Claude 3, Claude 3.5, Claude 4, Claude 4.1, in the Opus, Sonnet, and Haiku variants). We tested both production models and “helpful-only” variants that were trained differently. We also tested some base pretrained models before post-training.</p><p>We found that post-training significantly impacts introspective capabilities. Base models generally performed poorly, suggesting that introspective capabilities aren’t elicited by pretraining alone. Among production models, the pattern was clearer at the top end: Claude Opus 4 and 4.1—our most capable models—performed best across most of our introspection tests. However, beyond that, the correlation between model capability and introspective ability was weak. Smaller models didn't consistently perform worse, suggesting the relationship isn't as simple as “more capable are more introspective.”</p><p>We also noticed something unexpected with post-training strategies. “Helpful-only” variants of several models often performed  at introspection than their production counterparts, even though they underwent the same base training. In particular, some production models appeared reluctant to engage in introspective exercises, while the helpful-only variants showed more willingness to report on their internal states. This suggests that how we fine-tune models can elicit or suppress introspective capabilities to varying degrees.</p><p>We’re not entirely sure why Opus 4 and 4.1 perform so well (note that our experiments were conducted prior to the release of Sonnet 4.5). It could be that introspection requires sophisticated internal mechanisms that only emerge at higher capability levels. Or it might be that their post-training process better encourages introspection. Testing open-source models, and models from other organizations, could help us determine whether this pattern generalizes or if it’s specific to how Claude models are trained.</p><h4>Q: What’s next for this research?</h4><p>We see several important directions. First, we need better evaluation methods—our experiments used specific prompts and injection techniques that might not capture the full range of introspective capabilities. Second, we need to understand the mechanisms underlying introspection. We have some speculative hypotheses about possible circuits (like anomaly detection mechanisms or concordance heads), but we haven’t definitively identified how introspection works. Third, we need to study introspection in more naturalistic settings, since our injection methodology creates artificial scenarios. Finally, we need to develop methods to validate introspective reports and detect when models might be confabulating or deceiving. We expect that understanding machine introspection and its limitations will become more important as models become more capable.</p>","contentLength":22521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ok16u3/anthropic_has_found_evidence_of_genuine/"},{"title":"Billboard Says AI-Powered ‘Artists’ Are Increasingly Hitting The Charts","url":"https://www.forbes.com/sites/conormurray/2025/10/29/billboard-says-ai-powered-artists-are-increasingly-hitting-the-charts","date":1761834216,"author":"/u/MetaKnowing","guid":322190,"unread":true,"content":"<div><p>Billboard says a wave of AI-created music has debuted on its charts over the past month—one  “singer” even scored a record deal—as some of these fake personas rack up millions of streams, a stark new trend that has raised some alarms in the music industry. </p><figure role=\"presentation\"><div><div><small>NurPhoto via Getty Images</small></div></div></figure></div><div><div><p>Over the past four weeks, a new AI creation has debuted on a Billboard chart in each week, <a href=\"https://www.billboard.com/lists/ai-artists-on-billboard-charts/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.billboard.com/lists/ai-artists-on-billboard-charts/\" data-ga-track=\"ExternalLink:https://www.billboard.com/lists/ai-artists-on-billboard-charts/\" aria-label=\"Billboard reported\">Billboard reported</a>, including the AI country music product Breaking Rust, that debuted the songs “Livin’ On Borrowed Time” and “Walk My Walk” on the country song sales chart this week.</p></div><div><p>The Christian AI-generated Juno Skye debuted on Billboard’s emerging artists chart last week, while the AI act Enlly Blue’s song “Through My Soul” hit the rock sales song chart earlier this month, Billboard reported.</p></div><div><p>The outlet said it cross-checked the songs with Deezer, a platform that offers an AI-detection tool, to verify whether the songs were artificially generated.</p></div><div><p>The most prominent example on Billboard’s charts is Xania Monet, an AI-generated singer that has racked up more than 44 millions streams in the United States, though the songs are written by Mississippi-based songwriter Telisha “Nikki” Jones.</p></div><div><p>Xania Monet has already charted on plenty of Billboard charts since debuting over the summer, including a No. 1 hit on the R&amp;B song sales chart, and this week became the first AI-generated act to rank a song on Billboard’s radio airplay chart.</p></div><div><p>Xania Monet’s vocals are generated by Suno, an AI platform that was <a href=\"https://www.theverge.com/2024/6/24/24184710/riaa-ai-lawsuit-suno-udio-copyright-umg-sony-warner\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.theverge.com/2024/6/24/24184710/riaa-ai-lawsuit-suno-udio-copyright-umg-sony-warner\" data-ga-track=\"ExternalLink:https://www.theverge.com/2024/6/24/24184710/riaa-ai-lawsuit-suno-udio-copyright-umg-sony-warner\" aria-label=\"sued\">sued</a> by major record labels and the Recording Industry Association of America last year for using copyrighted material to train its AI tools. </p></div></div><p>Jones, Monet’s creator, signed a multimillion dollar record deal with record label Hallwood Media in September, <a href=\"https://www.billboard.com/pro/ai-music-artist-xania-monet-multimillion-dollar-record-deal/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.billboard.com/pro/ai-music-artist-xania-monet-multimillion-dollar-record-deal/\" data-ga-track=\"ExternalLink:https://www.billboard.com/pro/ai-music-artist-xania-monet-multimillion-dollar-record-deal/\" aria-label=\"Billboard reported\">Billboard reported</a>, after a bidding war reportedly reached $3 million. </p><h2>How Have Ai Creations Found Success?</h2><p>Some of the AI acts that have made waves on the Billboard charts have curated social media profiles as if they are real people. Xania Monet’s <a href=\"https://www.instagram.com/xania_monet/?hl=en\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.instagram.com/xania_monet/?hl=en\" data-ga-track=\"ExternalLink:https://www.instagram.com/xania_monet/?hl=en\" aria-label=\"Instagram\">Instagram</a> page has more than 144,000 followers, and its account regularly posts purporting to show the artist recording songs in a studio. “I write music,” Xania Monet’s Instagram bio says, even though Xania Monet is not a real person and her songs are written by Jones. The Instagram pages for <a href=\"https://www.instagram.com/breakinrust/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.instagram.com/breakinrust/\" data-ga-track=\"ExternalLink:https://www.instagram.com/breakinrust/\" aria-label=\"Breaking Rust\">Breaking Rust</a> and <a href=\"https://www.instagram.com/enllyblue/?hl=en\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.instagram.com/enllyblue/?hl=en\" data-ga-track=\"ExternalLink:https://www.instagram.com/enllyblue/?hl=en\" aria-label=\"Enlly Blue\">Enlly Blue</a>, each of which have thousands of followers, similarly depict AI-generated personas performing their songs or recording music videos. The people curating these AI acts may also have a financial incentive, Billboard reported, estimating late last month Monet’s small music catalog has already generated more than $52,000 in revenue after racking up 17 million streams in the United States. It’s unclear how much of that revenue goes to Jones, the credited songwriter on Monet’s music, <a href=\"https://www.billboard.com/pro/ai-artist-xania-monet-how-much-money-songs-made/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.billboard.com/pro/ai-artist-xania-monet-how-much-money-songs-made/\" data-ga-track=\"ExternalLink:https://www.billboard.com/pro/ai-artist-xania-monet-how-much-money-songs-made/\" aria-label=\"Billboard reported\">Billboard reported</a>, though it noted platforms like Spotify don’t have specific policies for how AI-generated songs can collect royalties, meaning they can generate revenue like any other song. </p><p>Singer Kehlani slammed Xania Monet’s record deal in a since-deleted post on TikTok in September. “Nothing and no one on Earth will ever be able to justify AI to me,” Kehlani said, according to <a href=\"https://www.billboard.com/music/music-news/kehlani-slams-ai-artist-xania-monet-million-record-deal-1236071158/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.billboard.com/music/music-news/kehlani-slams-ai-artist-xania-monet-million-record-deal-1236071158/\" data-ga-track=\"ExternalLink:https://www.billboard.com/music/music-news/kehlani-slams-ai-artist-xania-monet-million-record-deal-1236071158/\" aria-label=\"Billboard\">Billboard</a>, stating she doesn’t respect the AI creation. She lamented that these AI acts make their music based on the copyrighted material AI generators are trained on without having to credit anyone. Terry McBride, co-founder and CEO of record label Nettwerk Music Group, told <a href=\"https://www.billboard.com/pro/ai-artist-record-deals-ethical-sign-xania-monet/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.billboard.com/pro/ai-artist-record-deals-ethical-sign-xania-monet/\" data-ga-track=\"ExternalLink:https://www.billboard.com/pro/ai-artist-record-deals-ethical-sign-xania-monet/\" aria-label=\"Billboard\">Billboard</a> he would not have signed Xania Monet or any other AI artist. “That’s not going to be a touring entity as we know it,” McBride said, adding, “Even if it did hundreds of millions of streams, we have no interest in that.”</p><p>The film industry is also grappling with AI-generated personas, notably the AI-generated “actress” <a href=\"https://www.forbes.com/sites/conormurray/2025/09/30/sag-aftra-condemns-ai-actress-tilly-norwood-joins-critics-emily-blunt-whoopi-goldberg-and-more/\" target=\"_self\" title=\"https://www.forbes.com/sites/conormurray/2025/09/30/sag-aftra-condemns-ai-actress-tilly-norwood-joins-critics-emily-blunt-whoopi-goldberg-and-more/\" data-ga-track=\"InternalLink:https://www.forbes.com/sites/conormurray/2025/09/30/sag-aftra-condemns-ai-actress-tilly-norwood-joins-critics-emily-blunt-whoopi-goldberg-and-more/\" aria-label=\"Tilly Norwood\">Tilly Norwood</a>, which was unveiled by an AI studio in September and quickly drew condemnation from the SAG-AFTRA actor’s guild. The union said “creativity is, and should remain, human-centered,” stating it is “opposed to the replacement of human performers by synthetics” which have “no life experience to draw from.” Other actors, including Whoopi Goldberg, Emily Blunt and Melissa Barrera also criticized this use of AI. The AI personality was created by Eline Van der Velden, who launched the AI talent studio Xicoia and claimed multiple film studios were interested in employing her creation. Like the AI-generated musicians, Tilly Norwood has an <a href=\"https://www.instagram.com/tillynorwood/\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"https://www.instagram.com/tillynorwood/\" data-ga-track=\"ExternalLink:https://www.instagram.com/tillynorwood/\" aria-label=\"Instagram page\">Instagram page</a> with more than 65,000 followers that posts as if the AI actress is a real person. </p>","contentLength":4621,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ok13ra/billboard_says_aipowered_artists_are_increasingly/"},{"title":"[R] Layer-0 heads that pre-bias hedging over facts in GPT-2 (replicated in Mistral-7B) — code + DOI","url":"https://www.reddit.com/r/MachineLearning/comments/1ok0zgr/r_layer0_heads_that_prebias_hedging_over_facts_in/","date":1761833936,"author":"/u/mat8675","guid":323497,"unread":true,"content":"<p> independent researcher (me). Sharing a preprint + code for review.</p><p> In GPT-2 Small/Medium I find layer-0 heads that  downweight factual continuations and boost hedging tokens before most computation happens. Zeroing {0:2, 0:4, 0:7} improves logit-difference on single-token probes by  and tightens calibration (ECE , Brier ). Path-patching suggests ~ of head 0:2’s effect flows through a layer-0→11 residual path. A similar (architecture-shifted) pattern appears in Mistral-7B.</p><ul><li>Models: GPT-2 Small (124M), Medium (355M); Mistral-7B.</li><li>Probes: single-token factuality/negation/counterfactual/logic tests; measure Δ logit-difference for the factually-correct token vs distractor.</li><li>Analyses: head ablations; path patching along residual stream; reverse patching to test induced “hedging attractor”.</li></ul><ul><li> Heads {0:2, 0:4, 0:7} are top suppressors across tasks. Gains (Δ logit-diff): Facts , Negation , Counterfactual , Logic . Randomization: head 0:2 at ~100th percentile; trio ~99.5th (n=1000 resamples).</li><li> Layer-0 heads {0:22, 0:23} suppress on negation/counterfactual; head 0:21 partially opposes on logic. Less “hedging” per se; tends to surface editorial fragments instead.</li><li> ~ of the 0:2 effect mediated by the layer-0→11 residual route. Reverse-patching those activations into clean runs induces stable hedging downstream layers don’t undo.</li><li> Removing suppressors improves ECE and Brier as above.</li></ul><p><strong>Interpretation (tentative).</strong></p><p>This looks like a learned  entropy-raising mechanism: rotate a high-confidence factual continuation into a higher-entropy “hedge” distribution in the first layer, creating a basin that later layers inherit. This lines up with recent inevitability results (Kalai et al. 2025) about benchmarks rewarding confident evasions vs honest abstention—this would be a concrete circuit that implements that trade-off. (Happy to be proven wrong on the “attractor” framing.)</p><p><strong>Limitations / things I didn’t do.</strong></p><ul><li>Two GPT-2 sizes + one 7B model; no 13B/70B multi-seed sweep yet.</li><li>Single-token probes only; multi-token generation and instruction-tuned models not tested.</li><li>Training dynamics not instrumented; all analyses are post-hoc circuit work.</li></ul><ol><li>Path-patching design—am I over-attributing causality to the 0→11 route?</li><li>Better baselines than Δ logit-diff for these single-token probes.</li><li>Whether “attractor” is the right language vs simpler copy-/induction-suppression stories.</li><li>Cross-arch tests you’d prioritize next (Llama-2/3, Mixtral, Gemma; multi-seed; instruction-tuned variants).</li></ol><p>I’ll hang out in the thread and share extra plots / traces if folks want specific cuts.</p>","contentLength":2593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm Independently Verifying Go's Reproducible Builds","url":"https://www.agwa.name/blog/post/verifying_go_reproducible_builds","date":1761833430,"author":"/u/amalinovic","guid":322362,"unread":true,"content":"<p>\nWhen you try to compile a Go module that requires a newer version of the Go toolchain than the one you have\ninstalled, the go command <a href=\"https://go.dev/doc/toolchain#download\" rel=\"external\">automatically downloads the newer toolchain</a> and uses it for compiling the module.\nThis useful feature was introduced in Go 1.21 and has let me quickly adopt new Go features in my open source projects\nwithout inconveniencing people with older versions of Go.\n</p><p>\nHowever, the idea of downloading a binary and executing it on demand makes a lot of people uncomfortable.\nIt feels like such an easy vector for a supply chain attack, where Google, or an attacker who has compromised\nGoogle or gotten a misissued SSL certificate, could deliver a malicious binary.\nMany developers are more comfortable getting Go from their Linux distribution, or compiling\nit from source themselves.\n</p><p>\nTo address these concerns, the Go project did two things:\n</p><ol><li><p>They made it so every version of Go starting with 1.21 could be\neasily reproduced from its source code.  Every time you compile a Go toolchain, it\nproduces the exact same Zip archive, byte-for-byte, regardless of the current time,\nyour operating system, your architecture, or other aspects of your environment (such as the directory\nfrom which you run the build).</p></li><li><p>They started publishing the checksum of every toolchain Zip archive in a public\n<a href=\"https://transparency.dev/\" rel=\"external\">transparency log</a> called the <a href=\"https://sum.golang.org/\" rel=\"external\">Go Checksum Database</a>.\nThe go command verifies that the checksum of a downloaded toolchain is published\nin the Checksum Database for anyone to see.\n</p></li></ol><p>\nThese measures mean that:\n</p><ol><li><p>You can be confident that the binaries downloaded and executed by the\ngo command are the exact same binaries you would have gotten had you built the toolchain\nfrom source yourself.  If there's a backdoor, the backdoor has to be in the source code.</p></li><li><p>You can be confident that the binaries downloaded and executed by the\ngo command are the same binaries that everyone else is downloading.  If there's a backdoor,\nit has to be served to the whole world, making it easier to detect.</p></li></ol><p>\nBut these measures mean nothing if no one is checking that the binaries\nare reproducible, or that the Checksum Database isn't presenting inconsistent information\nto different clients.  Although Google checks reproducibility and publishes\na <a href=\"https://go.dev/rebuild\" rel=\"external\">report</a>, this doesn't help if you think Google might try to\nslip in a backdoor themselves.  There needs to be an independent third party doing the checks.\n</p><p>\nWhy not me?  I was involved in Debian's\n<a href=\"https://reproducible-builds.org/\" rel=\"external\">Reproducible Builds</a> project back in the day and developed some of the core tooling used\nto make Debian packages reproducible (strip-nondeterminism and disorderfs).  I also\nhave extensive experience monitoring Certificate Transparency logs and have detected\nmisbehavior by numerous logs since 2017.  And I do not work for Google (though I have\neaten their food).\n</p><p>\nIn fact, I've been quietly operating an auditor for the Go Checksum Database since 2020\ncalled <a href=\"https://sourcespotter.com/\" rel=\"external\">Source Spotter</a> (à la <a href=\"https://sslmate.com/certspotter/\" rel=\"external\">Cert Spotter</a>, my Certificate Transparency monitor). Source Spotter monitors the Checksum Database,\nmaking sure it doesn't present inconsistent information or publish more than one checksum for a given module\nand version.  I decided to extend Source Spotter to also verify toolchain reproducibility.\n</p><p>\nThe Checksum Database was originally intended for recording the checksums of Go modules.\nEssentially, it's a verifiable, append-only log of records which say that a particular\nversion (e.g. ) of a module (e.g. ) has a particular SHA-256 hash.  Go repurposed\nit for recording toolchain checksums.  Toolchain records have the pseudo-module\n and versions that look like <code>v0.0.1-go.-</code>.  For example, the Go1.24.2 toolchain for linux/amd64 has the module version <code>v0.0.1-go1.24.2.linux-amd64</code>.\n</p><p>\nWhen Source Spotter sees a new version of the  pseudo-module,\nit downloads the corresponding source code, builds it in an AWS Lambda function by running ,\nand compares the checksum\nof the resulting Zip file to the checksum published in the Checksum Database.  Any mismatches\nare published <a href=\"https://sourcespotter.com/toolchain/\" rel=\"external\">on a webpage</a> and\nin <a href=\"https://feeds.api.sourcespotter.com/toolchain/failures.atom\" rel=\"external\">an Atom feed</a> which I monitor.\n</p><p>\nSo far, Source Spotter has successfully reproduced every toolchain since Go 1.21.0, for every architecture and operating system.\nAs of publication time, that's <a href=\"https://sourcespotter.com/toolchain/#verified\" rel=\"external\">2,672 toolchains</a>!\n</p><p>\nSince the Go toolchain is written in Go, building it requires an earlier version of the Go toolchain to be installed already.\n</p><p>When reproducing Go 1.21, 1.22, and 1.23, Source Spotter uses a Go 1.20.14 toolchain that I built from source.  I started by building Go 1.4.3 using a C compiler.  I used Go 1.4.3 to build Go 1.17.13, which I used to build Go 1.20.14.  To mitigate <a href=\"https://dl.acm.org/doi/10.1145/358198.358210\" rel=\"external\">Trusting Trust</a> attacks, I repeated this process on both Debian and Amazon Linux using both GCC and Clang for the Go 1.4 build.  I got the exact same bytes every time, which I believe makes a compiler backdoor vanishingly unlikely.  The scripts I used for this are <a href=\"https://github.com/AGWA/build-go1.20\" rel=\"external\">open source</a>.</p><p>When reproducing Go 1.24 or higher, Source Spotter uses a binary toolchain downloaded from the Go module proxy\nthat it previously verified as being reproducible from source.</p><p>\nCompared to reproducing a typical Debian package, it was really easy to reproduce the same bytes when building\nthe Go toolchains.  Nevertheless, there were some bumps along the way:\n</p><p>\nFirst, the Darwin (macOS) toolchains published by Google contain signatures produced by Google's private key.\nObviously, Source Spotter can't reproduce these.  Instead, Source Spotter has to download\nthe toolchain (making sure it matches the checksum published in the Checksum Database) and strip the signatures\nto produce a new checksum that is verified against the reproduced toolchain.\nI reused <a href=\"https://github.com/SSLMate/sourcespotter/blob/main/toolchain/darwin.go\" rel=\"external\">code written by Google</a>\nto strip the signatures <s>and I honestly have no clue what it's doing and whether\nit could potentially strip a backdoor.  A review from someone versed in Darwin binaries would be very helpful!</s>\nEdit: since publication, I've learned enough about Darwin binaries to be confident in this code.\n</p><p>\nSecond, to reproduce the linux-arm toolchains, Source Spotter has\nto set  in the environment... except when reproducing Go 1.21.0, which\n<a href=\"https://github.com/golang/go/issues/62164\" rel=\"external\">Google accidentally built using </a>.\nI find it unfortunate that <a href=\"https://pkg.go.dev/cmd/dist\" rel=\"external\">cmd/dist</a> (the tool used to build the toolchain) doesn't set this environment variable along with the many other environment variables it sets, but Russ Cox pointed me to <a href=\"https://go.dev/cl/526263\" rel=\"external\">some context</a> why this is the case.\n</p><p>\nFinally, the Checksum Database contains a toolchain for Go 1.9.2rc2, which is not a\n<a href=\"https://pkg.go.dev/go/version#IsValid\" rel=\"external\">valid version number</a>.\nIt turns out this version was <a href=\"https://github.com/golang/go/issues/68634#issuecomment-2867535846\" rel=\"external\">released by\nmistake</a>.  To avoid raising an error for an invalid version number, Source Spotter has\nto <a href=\"https://github.com/SSLMate/sourcespotter/blob/c883957f32a2162f94fdc6dafa607a1d534ebee3/internal/toolchain/build.go#L119-L124\" rel=\"external\">special case it</a>.  Not a huge deal, but I found it interesting because it\ndemonstrates one of the downsides of transparency logs: you can't fix or remove entries that were added by mistake!\n</p><p>\nThe source tarballs built by Source Spotter are not published in the Checksum Database, meaning Google\ncould serve Source Spotter, and only Source Spotter,\nsource code which contains a backdoor.  To mitigate this, Source Spotter publishes the\n<a href=\"https://sourcespotter.com/toolchain/#sources\" rel=\"external\">checksums</a> of every source tarball it builds.\nHowever, there are alternatives:\n</p><p>\nFirst, Russ Cox pointed out that while the source tarballs aren't in the Checksum Database,\nthe toolchain Zip archives also contain the source code, so Source Spotter could build those instead\nof the source tarballs. (A previous version of this post incorrectly said that source code wasn't published\nin the Checksum Database at all.)\n</p><p>\nSecond, <a href=\"https://filippo.io/\" rel=\"external\">Filippo Valsorda</a> suggested that Source Spotter build from Go's Git repository\nand publish the Git commit IDs instead, since lots of Go developers have the Go Git repository checked out\nand it would be relatively easy for them to compare the state of their repos against what Source Spotter has seen.\nRegrettably, Git commit IDs are SHA-1, but this is mitigated by Git's use of\n<a href=\"https://github.com/cr-marcstevens/sha1collisiondetection\" rel=\"external\">Marc Stevens' collision detection</a>,\nso the benefits may be worth the risk.\nI think building from Git is a good idea, and to bootstrap it, Filippo used <a href=\"https://github.com/magic-wormhole/magic-wormhole\" rel=\"external\">Magic Wormhole</a> to send me the output of  from his repo while we were both\nat the <a href=\"https://transparency.dev/summit2025/\" rel=\"external\">Transparency.dev Summit</a> last week.\n</p><p>\nThanks to Go's Checksum Database and reproducible toolchains, Go developers\nget the usability benefits of a centralized package repository and binary toolchains\nwithout sacrificing the security benefits of decentralized packages and building from source.\nThe Go team deserves enormous credit for making this a reality, particularly for building a system\nthat is not too hard for a third party to verify.  They've raised the bar, and I\nhope other language and package ecosystems can learn from what they've done.\n</p>","contentLength":8580,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ok0rw2/im_independently_verifying_gos_reproducible_builds/"},{"title":"For the people that ONLY use linux as there workstation and gaming device, how full is your storage?","url":"https://www.reddit.com/r/linux/comments/1ojyy4u/for_the_people_that_only_use_linux_as_there/","date":1761828964,"author":"/u/Riponai_Gaming","guid":322141,"unread":true,"content":"<p>I switched to arch linux like a year ago, when i used to use windows 11, over a 100+ gigs were used up windows and its crap without me installing much in it but since i switched to arch I have a complete workstation build+VMs+games(On a hard disk sure but the all the major software is on my SSD) and some other apps and scripts that didnt exist on my windows install and its only 60 gigs.</p><p>So i am just curious how full are other peoples disks with a full setup that they use for work and gaming</p>","contentLength":494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AppImage apps fighting each other (Desktop integration)","url":"https://www.reddit.com/r/linux/comments/1ojyw9u/appimage_apps_fighting_each_other_desktop/","date":1761828831,"author":"/u/Top-Discussion7619","guid":322385,"unread":true,"content":"<p>I have 2 commercial apps that both run as AppImages. I'm on Ubuntu 24.04 LTS. </p><p>App #1 installs itself with Desktop Integration enabled (there's no way to turn it off).</p><p>App #2 runs without Desktop Integration but you can enable it via a setting in the app.</p><p>Both apps run perfectly. However, if I enable Desktop Integration on App #2, App  then reverts to having Desktop Integration turned off. The icon disappears from the application menu and the icon in the panel switches to the generic white box/gear AppImage icon.</p><p>Why is this happening? Is only one AppImage app allowed to be integrated into the desktop environment? </p>","contentLength":618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"esp-hal 1.0.0 release announcement","url":"https://developer.espressif.com/blog/2025/10/esp-hal-1/","date":1761826942,"author":"/u/XxMabezxX","guid":322140,"unread":true,"content":"<p>In February this year, we announced the first <a href=\"https://developer.espressif.com/blog/2025/02/rust-esp-hal-beta/\" target=\"_blank\"> 1.0 beta</a> release. Since then we’ve been hard at work, polishing and preparing for the full release. Today, the Rust team at Espressif is excited to announce the official  release for , the  vendor-backed Rust SDK!</p><h3>What We’re Stabilizing Today</h3><p>We’ve spent many years researching and experimenting to get to this stage (check out the <a href=\"https://developer.espressif.com/blog/2025/02/rust-esp-hal-beta/\" target=\"_blank\"> 1.0 beta</a> blog post for the longer story!). However, to get a stable foundation to build from, the experimentation eventually needs to make way for stability. To achieve this, we’ve decided to limit the scope of 1.0 stabilization to:</p><ul><li>Initializing the HAL,  and the relevant configuration associated with that.</li><li>Four “core” drivers to start:</li><li> and  modes for the aforementioned drivers.<ul><li>Our  drivers are compatible with many executors, including <a href=\"https://github.com/embassy-rs/embassy\" target=\"_blank\">Embassy</a>’s.</li></ul></li><li>The  module, which provides , , and .</li><li>A couple of miscellaneous system APIs (SoC reset, etc.).</li><li>Additional configuration mechanisms beyond feature flags (<a href=\"https://crates.io/crates/esp-config\" target=\"_blank\"></a>).</li></ul><p>With the exception of the list above, everything else in  is now feature-gated behind the  feature. With the scope limited, post 1.0 we can incrementally stabilize drivers, much like the Rust project itself does, building on 1.0’s foundation.</p><h3>What Does Unstable Mean for Drivers?</h3><p>Unstable in this case refers to API stability. There is varying levels of functionality for unstable drivers, however, they are suitable for most common use cases. Using them, reporting feedback, and/or contributing to improving them will aid their stabilization.</p><h3>What About the Other  Crates?</h3><p> is the foundation of many of the ecosystem crates. <a href=\"https://github.com/esp-rs/esp-hal/tree/main/esp-radio\" target=\"_blank\"></a> (previously known as ) is our next stabilization target, which will enable the use of Wi-Fi, Bluetooth, <a href=\"https://www.espressif.com/en/solutions/low-power-solutions/esp-now\" target=\"_blank\">ESP-NOW</a> and IEEE802.15.4 on the ESP32 family of devices. The end goal is of course to have every  crate with a 1.0+ release eventually.</p><p>The first step is to read our specially curated <a href=\"https://github.com/esp-rs/book\" target=\"_blank\">book</a>, which explains the ecosystem, tooling and some key embedded concepts for .</p><p>As part of getting to 1.0, we’ve created our own project generation tool, <a href=\"https://github.com/esp-rs/esp-generate\" target=\"_blank\"></a> to bootstrap a project. This is explained fully in the <a href=\"https://github.com/esp-rs/book\" target=\"_blank\">book</a>, but getting something running today should be as simple as:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>to launch the interactive project generation terminal user interface.</p><p>Once you’ve generated your project, connect your ESP32 and run  from your new project directory!</p><p>This is just the start. We plan on stabilizing all  related crates, next up is <a href=\"https://github.com/esp-rs/esp-hal/tree/main/esp-radio\" target=\"_blank\"></a>. We’ll continue developing <a href=\"https://github.com/esp-rs/esp-hal/tree/main/esp-hal\" target=\"_blank\"></a>; over time we’ll stabilize more drivers beyond the core set that we’re starting with today. We’ll continue to add support for new devices, such as the newly released ESP32-C5, as they go into mass production.</p><p>This release would not have been possible without the help from the Rust community, the embedded working group, and of course the ESP community and contributors which have heavily impacted how we’ve developed our Rust offering. I would also like to thank Espressif, and in particular the Rust team for their hard work in getting us to where we are today!</p><p>If you’re a company using (or considering using) Rust on our devices, please do contact <a href=\"mailto:sales@espressif.com\">sales@espressif.com</a>, we’d love to hear from you!</p>","contentLength":3169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ojy6zk/esphal_100_release_announcement/"},{"title":"If concurrent programming is efficient, Why don't we use it all the time?","url":"https://youtu.be/HMy4yTxcqUY","date":1761826870,"author":"/u/parsaeisa","guid":322118,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ojy61g/if_concurrent_programming_is_efficient_why_dont/"},{"title":"Would it make sense to use a Go microservice for DB operations instead of using PHP + Codeigniter?","url":"https://www.reddit.com/r/golang/comments/1ojxwev/would_it_make_sense_to_use_a_go_microservice_for/","date":1761826101,"author":"/u/mucleck","guid":322361,"unread":true,"content":"<p>At work we use PHP (CodeIgniter) with MariaDB, and right now all DB queries (SELECTs, INSERTs, etc.) go through CodeIgniter’s database helper.</p><p>I was thinking — what if instead of having each PHP process open and close DB connections all the time, we built a small Go microservice that handles all the database stuff?</p><p>The Go service would: • Keep a persistent connection pool to MariaDB • Expose simple endpoints (REST or gRPC) for queries • Benefit from Go’s concurrency and efficient connection handling</p><p>So PHP would just make requests to the Go service instead of talking to the DB directly.</p><p>Do you think this would actually be faster or more efficient, especially in terms of CPU cost? Right now, if we try to run like 6,000 inserts, the DB basically dies because each query is a new connection to the DB — so I’m wondering if this setup could handle that load better since Go would manage persistent connections instead of tons of short-lived PHP ones.</p><p>Has anyone tried something like this? Does it make sense performance-wise, or would the overhead of HTTP/gRPC just kill any potential benefit?</p><p>PD: The text was written in spanish and translated to English with ChatGpt because is not my main language, but im real persona so i would be glad if you took your time to orientate me ty!</p>","contentLength":1298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Migrating Wordpress Websites from WPEngine to Kubernetes","url":"https://github.com/akvnn/wordpress-helm","date":1761823969,"author":"/u/Initial-Detail-7159","guid":322091,"unread":true,"content":"<p>I recently moved my Wordpress websites from WPEngine to my Kubernetes cluster. The process was seamless, the only issue was that existing Helm charts assume a new Wordpress project that would be created from the admin interface. So, I made a helm chart suited for migrating from WPEngine or any other managed provider.</p><p>Ideally, the theme would be the only part of the website that will be in GitHub (assuming you are using GitHub for version control with CI/CD setup) and will be built in the Docker image. The other components: languages, logs, plugins, and uploads are mounted as persistent volumes and changes to them are expected via the admin interface.</p><p>You simply have to build the Dockerfile (provided), migrate the data to the corresponding volumes, import the MySQL data, and finally install the helm chart.</p><p>I open sourced it if it would help anyone. You can find it here.</p><p>Note: in case you are wondering, the primary motivation for the migration is to cut costs. However, the flexibility in Kubernetes (assuming you already have a cluster) is much better! Security scanning can still be added via plugins such as WPScan. You don’t need WPEngine.</p>","contentLength":1153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1ojx6sn/migrating_wordpress_websites_from_wpengine_to/"},{"title":"[R] FastJAM: a Fast Joint Alignment Model for Images (NeurIPS 2025)","url":"https://www.reddit.com/r/MachineLearning/comments/1ojx3wc/r_fastjam_a_fast_joint_alignment_model_for_images/","date":1761823714,"author":"/u/ronshap","guid":322306,"unread":true,"content":"<p>I'm excited to share our NeurIPS 2025 paper \"FastJAM: a Fast Joint Alignment Model for Images\".</p><p>Authors: Omri Hirsch*, Ron Shapira Weber*, Shira Ifergane, Oren Freifeld.</p><p>FastJAM is a lightweight graph-based framework for joint image alignment that runs in seconds rather than minutes or hours (for previous works).</p><p>Example of FastJAM Joint alignment results:</p><p>FastJAM reformulates the joint alignment problem using sparse keypoints and graph neural networks (GNNs). By propagating correspondence information across images, FastJAM predicts consistent transformations for an entire collection of images, achieving a large speedup in runtime and better or comparable results across all datasets.</p><p>FastJAM GNN Architecture:</p>","contentLength":713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fil-C: A memory-safe C implementation","url":"https://lwn.net/SubscriberLink/1042938/38d8dde9db211cab/","date":1761822207,"author":"/u/waozen","guid":322185,"unread":true,"content":"<blockquote><table><tbody><tr><td><p>\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider <a href=\"https://lwn.net/subscribe/\">subscribing to LWN</a>.  Thank you\nfor visiting LWN.net!\n</p></td></tr></tbody></table></blockquote><div>\n           By October 28, 2025</div><p><a href=\"https://fil-c.org/\">\nFil-C</a> is a memory-safe implementation of C and C++ that aims to let C code —\ncomplete with pointer arithmetic, unions, and other features that are often\ncited as a problem for memory-safe languages — run safely, unmodified.\nIts dedication to being \"\" makes it an attractive choice for retrofitting memory-safety\ninto existing applications. Despite the project's relative youth and single\nactive contributor, Fil-C is capable of compiling an\nentire memory-safe Linux user space (based on\n<a href=\"https://www.linuxfromscratch.org/\">\nLinux From Scratch</a>),\nalbeit with some modifications to the more complex programs. It also features\nmemory-safe signal handling and a concurrent garbage collector.\n</p><p>\nFil-C is a fork of\n<a href=\"https://clang.llvm.org/\">\nClang</a>; it's available under an Apache v2.0\nlicense with LLVM exceptions for the runtime. Changes from the upstream compiler\nare occasionally merged in, with Fil-C currently being based on version 20.1.8\nfrom July 2025. The project is a personal passion\nof Filip Pizlo, who has previously worked on the runtimes of a number of\nmanaged languages, including Java and JavaScript. When he first began the\nproject, he was not sure that it was even possible. The initial implementation\nwas prohibitively slow to run, since it needed to insert a lot of different safety checks. This has\ngiven Fil-C a reputation for slowness. Since\nthe initial implementation proved viable, however, Pizlo has managed to optimize a number\nof common cases, making Fil-C-generated code only a few times slower than\nClang-generated code, although the exact slowdown depends heavily on the\nstructure of the benchmarked program.\n</p><p>\nReliable benchmarking is notoriously finicky, but in order to get some rough feel for\nwhether that level of performance impact would be problematic, I compiled Bash\nversion 5.2.32 with Fil-C and tried using it as my shell. Bash is nearly a best\ncase for Fil-C, because it spends more time running external programs than\nrunning its own code, but I still expected the performance difference to be\nnoticeable. It wasn't. So, at least for some programs, the performance overhead\nof Fil-C does not seem to be a problem in practice.\n</p><p>\nIn order to support its various run-time safety checks,\nFil-C does use a different internal ABI than Clang does. As a result, objects compiled with Fil-C won't\nlink correctly against objects generated by other compilers. Since Fil-C is a\nfull implementation of C and C++ at the source-code level, however, in practice\nthis just requires everything to be recompiled with Fil-C. Inter-language\nlinking, such as with Rust, is not currently supported by the project.\n</p><p>\nThe major challenge of rendering C memory-safe is, of course, pointer handling.\nThis is especially complicated by the fact that, as the\n<a href=\"https://lwn.net/Articles/1037974/\">\nlong road to CHERI-compatibility</a>\nhas shown, many programs expect a pointer to be 32 or 64 bits, depending on the\narchitecture.\nFil-C has tried several different ways to represent pointers since the project's\nbeginning in 2023. Fil-C's first pointers were 256 bits, not thread-safe, and\ndidn't protect against use-after-free bugs. The current implementation, called\n<a href=\"https://fil-c.org/invisicaps\">\n\"InvisiCaps\"</a>, allows\nfor pointers that appear to match the natural pointer size of the architecture\n(although this requires storing some auxiliary information elsewhere),\nwith full support for concurrency and\ncatching use-after-free bugs, at the expense of some run-time overhead.\n</p><p>\nFil-C's documentation\ncompares InvisiCaps to a software\nimplementation of CHERI: pointers are separated into a trusted \"capability\"\npiece and an untrusted \"address\" piece. Since Fil-C controls how the program is\ncompiled, it can ensure that the program doesn't have direct\naccess to the capabilities of any pointers, and therefore the runtime can rely\non them being uncorrupted. The tricky part of the implementation comes from how\nthese two pieces of information are stored in what looks to the program like 64\nbits.\n</p><p>\nWhen Fil-C allocates an object on the heap, it adds two metadata words before\nthe start of the allocated object: an upper bound, used to check accesses to the\nobject based on its size, and an \"aux word\" that is used to store additional\npointer metadata. When the program first writes a pointer value into an object, the\nruntime allocates a new auxiliary allocation of the same size as the object being written\ninto, and puts an actual hardware-level\npointer (i.e., one without an attached capability)\nto the new allocation into the aux word of the object. This auxiliary allocation, which is\ninvisible to the program being compiled, is used to\nstore the associated capability information for the pointer being stored (and is\nalso reused for any additional pointers stored into the object later). The address\nvalue is stored into the object as normal, so any C bit-twiddling\ntechniques that require looking at the stored value of the pointer work as\nexpected.\n</p><p>\nThis approach does mean that structures that contain pointers end up using twice\nas much memory, and every load of a pointer involves a pointer indirection\nthrough the aux word. In practice, the documentation claims that the\nperformance overhead of this approach for most programs makes them run about four\ntimes more slowly, although that number depends on how heavily the program makes\nuse of pointers. Still, he has ideas for several optimizations that he hopes can\nbring the performance overhead down over time.\n</p><p>\nOne wrinkle with this approach is atomic access to pointers — i.e. using\n<a href=\"https://en.cppreference.com/w/c/language/atomic.html\"></a> or . Luckily, there is\nno problem that cannot be solved with more pointer indirection: when the program\nloads or stores a pointer value atomically, instead of having the auxiliary\nallocation contain the capability information directly, it points to a\nthird 128-bit allocation that stores the capability and pointer value together.\nThat allocation can be updated with 128-bit atomic instructions, if the platform\nsupports them, or by creating new allocations and atomically swapping the\npointers to them.\n</p><p>\nSince the aux word is used to store a pointer value, Fil-C can use\n<a href=\"https://en.wikipedia.org/wiki/Tagged_pointer\">\npointer\ntagging</a> to store some additional information there as well; that is used to\nindicate special types of objects that need to be handled differently, such as\nfunctions, threads, and\n<a href=\"https://www.man7.org/linux/man-pages/man2/mmap.2.html\"></a>-backed allocations. It's also used to\nmark freed objects, so that any access results in an error message and a crash.\n</p><p>\nWhen an object is freed, its aux word marks it as a free object, which lets the\nauxiliary allocation be reclaimed immediately. The\noriginal object can't be freed immediately, however.\nOtherwise, a program could free an object,\nallocate a new object in the same location, and thereby cover up use-after-free bugs.\nInstead, Fil-C\n<a href=\"https://fil-c.org/fugc\">\nuses a garbage collector</a> to free an object's backing\nmemory only once all of the pointers to it go away. Unlike other garbage collectors\nfor C — such as\n<a href=\"https://www.hboehm.info/gc/\">\nthe Boehm-Demers-Weiser garbage collector</a> —\nFil-C can use the auxiliary\ncapability information to track live objects precisely.\n</p><p>\nFil-C's garbage collector is both parallel (collection happens faster the more\ncores are available) and concurrent (collection happens without pausing the\nprogram). Technically, the garbage collector does require threads to\noccasionally pause just long enough to tell it where pointers are located on the\nstack, but that only occurs at special \"safe points\" — otherwise, the program\ncan load and manipulate pointers without notifying the garbage collector. Safe\npoints are used as a synchronization barrier: the collector can't know that an object\nis really garbage until every thread has passed at least one safe point since it\nfinished marking. This synchronization is done with atomic instructions,\nhowever, so in practice threads never need to pause for longer than a few\ninstructions.\n</p><p>\nThe exception is the implementation of\n<a href=\"https://www.man7.org/linux/man-pages/man2/fork.2.html\"></a>, which uses the\nsafe points needed by the garbage collector to temporarily pause all of the threads\nin the program in order to prevent race conditions while forking. Fil-C inserts\na safe point at every backward control-flow edge, i.e., whenever code could\nexecute in a loop. In the common case, the inserted code just needs to load a flag register\nand confirm that the garbage collector has not requested anything be done. If\nthe garbage collector does have a request for the thread, the thread runs a callback to\nperform the needed synchronization.\n</p><p>\nFil-C uses the same safe-point mechanism to implement signal handling. Signal\nhandlers are only run when the interrupted thread reaches a safe point. That, in\nturn, allows signal handlers to allocate and free memory without interfering\nwith the garbage collector's operation; Fil-C's\n<a href=\"https://www.man7.org/linux/man-pages/man3/malloc.3.html\"></a> is signal-safe.\n</p><p>\nLinux From Scratch (LFS) is a tutorial on compiling one's own complete\nLinux user space. It walks through the steps of compiling and installing all of the core\nsoftware needed for a typical Linux user space in a\n<a href=\"https://www.man7.org/linux/man-pages/man2/chroot.2.html\"></a>\nenvironment. Pizlo has successfully\n<a href=\"https://fil-c.org/pizlix\">\nrun through</a> LFS with Fil-C to\nproduce a memory-safe version, although a non-Fil-C compiler is still needed to\nbuild some fundamental components, such as Fil-C's own runtime,\nthe GNU C library, and the kernel. (While Fil-C's runtime relies on a normal\ncopy of the GNU C library to make system calls, the programs that Fil-C compiles\nuse a Fil-C-compiled version of the library.)\n</p><p>\nThe process is mostly identical to LFS up through the end of chapter 7, because\neverything prior to that point consists of using cross-build tools to obtain a\nworking compiler in the  environment. The one difference is\nthat the cross-build tools are built with a different configured prefix, so that\nthey won't conflict with Fil-C. At that point, one can\nbuild a copy of Fil-C and use it to mostly replace the existing compiler. The\nremaining steps of LFS are unchanged.\n</p><p>\nOverall, Fil-C offers a remarkably complete solution for making existing C\nprograms memory-safe. While it does nothing for undefined behavior that is not\nrelated to memory safety,\nthe most pernicious and difficult-to-prevent security\nvulnerabilities in C programs tend to rely on exploiting memory-unsafe\nbehavior. Readers who have already considered and rejected Fil-C for their use\ncase due to its early performance problems may wish to take a second look —\nalthough anyone hoping for stability might want to wait for others to take the\nplunge, given the project's relative immaturity.\nThat said, for existing applications where a sizeable performance hit is preferable to an\nexploitable vulnerability, Fil-C is an excellent choice.\n</p>","contentLength":10776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ojwnec/filc_a_memorysafe_c_implementation/"},{"title":"[D] Is mamba architecture not used that much in the field of research?","url":"https://www.reddit.com/r/MachineLearning/comments/1ojwly3/d_is_mamba_architecture_not_used_that_much_in_the/","date":1761822080,"author":"/u/Charming_Bag_1257","guid":322188,"unread":true,"content":"<p>What I have read so far, Mamba arch still shines in handling long contexts (e.g., millions of tokens) much better than Transformers without the memory explosion. I get that when it comes to effectiveness (which we want), the transformer shines and is heavily used in research, but what are the limitations for Mamba? I usually do not find papers using this arch.</p>","contentLength":362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1ojvk1a/weekly_this_week_i_learned_twil_thread/","date":1761818431,"author":"/u/gctaylor","guid":322029,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surf update: new TLS fingerprints for Chromium 142","url":"https://www.reddit.com/r/golang/comments/1ojvfcb/surf_update_new_tls_fingerprints_for_chromium_142/","date":1761817958,"author":"/u/Affectionate_Type486","guid":322387,"unread":true,"content":"<p>An update to <a href=\"https://github.com/enetx/surf\">Surf</a>, the browser-impersonating HTTP client for Go.</p><p>The latest version adds support for new TLS fingerprints that match the behavior of the following clients:</p><p>These fingerprints include accurate ordering of TLS extensions, signature algorithms, supported groups, cipher suites, and use the correct GREASE and key share behavior. JA3 and JA4 hashes match the real browsers, including JA4-R and JA4-O. HTTP/2 Akamai fingerprinting is also consistent..</p><p>Let me know if you find any mismatches or issues with the new fingerprints.</p>","contentLength":535,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The private conversation anti-pattern in engineering teams","url":"https://open.substack.com/pub/leadthroughmistakes/p/why-we-tend-to-avoid-public-conversations","date":1761812331,"author":"/u/dymissy","guid":322116,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oju0f7/the_private_conversation_antipattern_in/"},{"title":"I created a new image format that can describe a full image in as little as 7 bytes","url":"https://github.com/mohanp06/simple-color-image-format/tree/main","date":1761812287,"author":"/u/mpp06","guid":322093,"unread":true,"content":"<p><em>Disclaimer: It's a hobby project, and as of now covers only simple image content. No attempt is made to format it as per the standard image specifications if any. It is an extensible, abstract framework, not restricted to images, and could be potentially useful in some cases.</em></p><p>I’ve been experimenting with how minimal an image file format can get — and ended up designing <strong>SCIF (Simple Color Image Format)</strong>.</p><p>It’s a tiny binary format that stores simple visuals like solid colors, gradients, and checkerboards using only a few bytes.</p><ul><li>7 bytes for a full solid-color image</li><li>11 bytes for gradients or patterns</li><li>easy to decode in under 20 lines of code</li><li>designed for learning, embedded systems, and experiments in data representation</li></ul><p>I’d love feedback or ideas for extending it — maybe procedural textures, transparency, or even compressed variants. Curious what you think — can such ultra-minimal formats have real use in small devices or demos?</p>","contentLength":941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oju019/i_created_a_new_image_format_that_can_describe_a/"},{"title":"The Green Tea Garbage Collector","url":"https://www.reddit.com/r/golang/comments/1ojtyuq/the_green_tea_garbage_collector/","date":1761812148,"author":"/u/Asleep-Actuary-4428","guid":322119,"unread":true,"content":"<p>Here are the details of Green Tea GC. It’s production-ready and already in use at Google, and plan to make it the default in Go 1.26.</p>","contentLength":135,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harbor in Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1ojtxes/harbor_in_kubernetes/","date":1761811972,"author":"/u/Always_smile_student","guid":322006,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harbor in Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1ojtovo/harbor_in_kubernetes/","date":1761810995,"author":"/u/Always_smile_student","guid":321991,"unread":true,"content":"<p>Everything was installed successfully, and I set up a NodePort so I can access it via the master node’s IP. Everywhere it says the default login and password are , but I get an “invalid username or password” error.</p><p>I also tried to check or reset the password using:</p><pre><code>kubectl -n harbor get secret harbor-core -o jsonpath=\"{.data.HARBOR_ADMIN_PASSWORD}\" | base64 --decode </code></pre><p>But that password doesn’t work either.</p>","contentLength":413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dithering - Part 1","url":"https://visualrambling.space/dithering-part-1/","date":1761809938,"author":"/u/brokePlusPlusCoder","guid":322263,"unread":true,"content":"<p>tap/click the right side of the screen to go forward →</p><p>I’ve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.</p><p>← tap/click the left side to go back</p><p>I was even more amazed when I learned how dithering works.</p><p>← or use arrow keys to navigate →</p><p>Look closely, and you’ll see this animation is made of alternating black and white pixels.</p><p>But these black and white pixels are specifically arranged to create the illusion of multiple shades.</p><p>That’s what dithering does: it simulates more color variations than what are actually used.</p><p>Here, it uses black and white to give the impression of multiple gray shades.</p><p>To me, dithering is about creating the most out of what we have, and that's what amazes me the most!</p><p>It inspired me to learn more about it, and now I want to share what I’ve learned.</p><p>Please note that this is just part one out of three, so I’ll only scratch the surface here.</p><p>I’ll go deeper in the next parts, which will come soon. Stay tuned!</p><p>First, let’s explore the dithering basics with this grayscale image example.</p><p>A grayscale image has various gray shades, from black to white.</p><p>Imagine a display that only shows black or white pixels, no grays. We must turn some pixels black and others white—but how?</p><p>One way is to map each pixel to the closest available color.</p><p>Pixels darker than medium gray turn black and lighter ones turn white.</p><p>This splits pixels into black or white groups.</p><p>However, this creates a harsh image with abrupt black-white transitions.</p><p>Shadow details vanish as gray pixels become fully black or white.</p><p>Dithering fixes this by selectively pushing some pixels towards the opposite color.</p><p>Some light gray pixels that are closer to white turn black.</p><p>Likewise, some dark grays turn white.</p><p>And it's done in a way that produces special patterns which simulate shades by varying the black-and-white pixel densities.</p><p>Denser black pixels are used in darker areas, while denser white pixels are used in lighter ones.</p><p>Next question: How are these patterns generated?</p><p>One simple dithering method, known as ordered dithering, uses a threshold map.</p><p>A threshold map is a grid of values representing brightness levels, from 0 (darkest) to 1 (brightest).</p><p>To dither, we compare each input pixel’s brightness to a corresponding threshold value.</p><p>If a pixel’s brightness exceeds the threshold (it’s brighter than the threshold), the pixel turns white. Otherwise, it turns black.</p><p>Repeating this for all pixels gives us the black-and-white dither patterns.</p><p>The threshold map is designed to output patterns where the black-and-white pixel density matches the input image’s shades.</p><p>So brighter input produces patterns with more white, while darker input produces more black.</p><p>These black-and-white density variations are what create the illusion of gray shades when viewed from a distance.</p><p>To dither larger images, we extend the threshold map to match the image size and follow the same principle:</p><p>Compare each pixel’s brightness to the threshold map, then turn it black or white accordingly.</p><p>The image now uses only two colors, but its overall appearance is preserved.</p><p>The variations in shades are now replaced by variations in black/white pixel density of the dithering patterns.</p><p>And that’s how dithering works in a nutshell: it replicates shades with fewer colors, which are strategically placed to maintain the original look.</p><p>I find it a bit ironic how I used to think dithering ‘adds’ a cool effect, when what it actually does is ‘remove’ colors!</p><p>That's all for now! We’ve reached the end, but there’s still a lot more to explore.</p><p>For example, we haven’t explored the algorithm to create a threshold map. (spoiler: there are many ways!)</p><p>There’s also another algorithm called error diffusion, which doesn’t use a threshold map.</p><p>Each algorithm creates a distinct, unique look, which I believe deserves its own article.</p><p>And that's why I decided to break this series into three parts.</p><p>In the next part, I’ll dive into various algorithms for creating threshold maps.</p><p>In the final part, I’ll focus on the error diffusion algorithm.</p><p>We'll dive even deeper into dithering's mechanisms in these next 2 parts, so stay tuned!</p><p>visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.</p><p>If you like this kind of visual article, please consider following me on X/Twitter and sharing this with your friends.</p><p>I'll keep creating more visual articles like this!</p><p>https://x.com/damarberlari</p>","contentLength":4505,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ojtfvn/dithering_part_1/"},{"title":"I’ve been living inside Rust for a while, and Flow-Like is what came out — a typed, local-first workflow engine","url":"https://github.com/TM9657/flow-like","date":1761808131,"author":"/u/tm9657","guid":322030,"unread":true,"content":"<p>I’ve been quietly building , a typed, visual workflow engine written in Rust. Think node-based “blueprints,” but with  — so flows are safer, easier to reason about, and automatically versioned. Everything runs : the desktop app, the backend, even AI and data nodes. There’s no account and no cloud dependency unless you explicitly add one.</p><p>With  out, you can now actually build real automations — from  and  to , data transforms, or ML pipelines. And, of course, we’ve <strong>carefully hidden many bugs</strong> for you to find and report. ❤️</p><p>Flow-Like is a desktop app (built with ) that lets you visually connect typed nodes into executable graphs. Each connection enforces its pin type, so most wiring errors show up before execution. Under the hood there’s a Rust engine that runs your graph directly — no web service, no remote orchestrator. Our backend code is also in our monorepo if that is more interesting to you.</p><p>For external connectivity, there’s an  that can spin up a local  server, manage , connect to , handle webhooks, timers, file watchers, and more. You can also host it if you want — the backend code for that is included.</p><p>Every project comes with its own <strong>file storage and database</strong> powered by the excellent  library — giving you <strong>full-text and vector search</strong> out of the box, with no setup required.</p><p>Llama.cpp is embedded for local models and ONNX for local ML and Embeddings. Every flow and node definition is , so you can safely share or roll back changes.</p><ul><li> custom async executor that runs typed graphs directly.</li><li> for event endpoints, HTTP handling, and integrations.</li><li> and  for structured + vector data storage.</li><li> for table operations and analytics.</li><li> and  integration for local inference.</li><li>, cross-platform builds for macOS/Windows/Linux.</li><li> already working (also thanks to Tauri)! The iOS build runs your flows LOCALLY on your phone — just needs a bit more polish before TestFlight.</li></ul><ul><li>Build  with typed request/response handling.</li><li>Run  that respond to messages and events.</li><li>Create  (IMAP fetch, filter, SMTP send).</li><li>Automate file pipelines, data transforms, or ML tasks.</li><li>Use  inside flows for full-text and vector search.</li><li>Stay completely offline — or opt into cloud APIs if you want.</li></ul><p>Everything happens locally, and everything is versioned — your data, flows, and nodes.</p><p>If you like the idea (or just want to see how far Rust and Tauri can go), a quiet ⭐️ on GitHub would be very welcome.</p>","contentLength":2401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ojt01w/ive_been_living_inside_rust_for_a_while_and/"},{"title":"hpademo - web browser tool for quickly simulating cpu-based hpa","url":"https://www.reddit.com/r/kubernetes/comments/1ojshyl/hpademo_web_browser_tool_for_quickly_simulating/","date":1761806141,"author":"/u/Reasonable-Rice444","guid":321971,"unread":true,"content":"<p>Need a quick tool for simulating cpu-based hpa behavior?</p><p><a href=\"https://udhos.github.io/hpademo/www/\">hpademo</a> is a simple demo for Kubernetes Horizontal Pod Autoscaler (HPA), written in Go and compiled to WebAssembly in order to run in a web browser.</p>","contentLength":204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Think memcmp is safe? Think again","url":"https://github.com/stateless-me/flatline","date":1761805054,"author":"/u/aabbdev","guid":322031,"unread":true,"content":"<p>I merged a “tiny perf” patch and unknowingly shipped a  on an auth path. Tests were green. The canary wasn’t. The compiler had “helpfully” turned a compare into an  and my branch on that result amplified the signal. That week taught me two things:  and C won’t keep you from leaking it.</p><p>So I wrote  a single-header, zero-dep, zero-heap toolbox for writing  C (C99/C11). But first, a quick ego check.</p><pre><code>if (memcmp(tag_a, tag_b, TAG_LEN) == 0) { memcpy(dst, src, len); } </code></pre><p><strong>B. Secret-conditioned branch</strong></p><pre><code>if (secret_bit) { memcpy(dst, src, len); } </code></pre><pre><code>uint8_t out = sbox[secret_idx]; </code></pre><p><strong>D. Ternary select on a secret</strong></p><pre><code>uint32_t y = secret_bit ? a : b; </code></pre><ul><li> (prefix-match timing)</li><li> (predictor + divergent paths)</li><li> (cache trace keyed by the secret).</li><li> →  may compile to a  (leaky). Even if it becomes a , only  may be evaluated (loads/calls) → secret-dependent memory traffic/time. Safe only if both operands are already plain values and the compiler emits a true constant-time select — which you shouldn’t bet on.</li></ul><p>Looks correct ≠ constant-time. Your <strong>toolchain, caches, and predictors</strong> also write the story.</p><p><strong>B. Branch on secrets (don’t)</strong> If execution path depends on a secret, timing does too.</p><ul><li><p>if (secret_bit) memcpy(dst, src, len); </p></li><li><p>uint32_t chosen = flat_select_u32(secret_bit, yes, no); // branchless select flat_memcpy_when((unsigned)secret_bit, dst, src, len); // gated copy w/o branch </p></li></ul><p><strong>I. Index memory with secrets (don’t)</strong> Secret-dependent addresses leave cache footprints.</p><ul><li><p>uint8_t out = sbox[secret_idx]; </p></li><li><p>uint8_t out = flat_lookup_u8(sbox, 256, secret_idx); // sweep + mask, constant-time // or: flat_table_apply_u8(outbuf, inbuf, n, sbox, 256); </p></li></ul><p><strong>D. Don’t use variable-latency ops on secrets</strong> Some instructions take  time; operands from secrets = side channel.</p><ul><li><p>q = n / secret_d; r = n % secret_d; </p></li><li><p>uint64_t q, r; unsigned ok = flat_div_mod_ct_u64(n, secret_d, &amp;q, &amp;r); // constant-time pattern </p></li></ul><p>Early-exit compares betray how many leading bytes matched. If you then branch on the result, you amplify the leak.</p><pre><code>int eq = flat_mem_eq(tag_a, tag_b, TAG_LEN); // no early exit flat_memcpy_when((unsigned)eq, dst, src, len); // no branch on secret </code></pre><p><strong>Constant-time equality + copy</strong></p><pre><code>#include \"flatline.h\" int ok = flat_mem_eq(tag_a, tag_b, TAG_LEN); flat_memcpy_when((unsigned)ok, dst, src, len); </code></pre><pre><code>uint32_t y = flat_select_u32(secret_bit, yes, no); </code></pre><pre><code>uint8_t v = flat_lookup_u8(sbox, 256, secret_idx); </code></pre><p><strong>Constant-time div/mod (when operands may be secret)</strong></p><pre><code>uint64_t q, r; unsigned okdiv = flat_div_mod_ct_u64(n, d, &amp;q, &amp;r); </code></pre><pre><code>clib install stateless-me/flatline # or drop flatline.h into your project </code></pre><ul><li><strong>Compilers are “too smart.”</strong> LTO and auto-vectorization can reintroduce branches or table lookups you thought you’d eliminated. (Consider  in strict builds.)</li><li> MB/s benchmarks won’t tell you if you’re constant-time. You need statistics (e.g., DUDECT-style t-tests) on  target CPU.</li><li> Loops must run the same number of iterations regardless of secrets. “Break on first mismatch” is a timing oracle in disguise.</li><li> Cross-process boundaries make tiny timing differences very measurable.</li></ul><p>Anywhere you compare tags, gate access, check tokens, or touch data-derived indices can betray you:</p><ul><li>Feature flags around sensitive paths</li><li>Table-driven transforms (S-boxes, LUTs, routing tables)</li></ul><p>The B.I.D. mental model keeps you honest:</p><ul><li> — don’t ndex memory with secrets</li><li> — on’t use variable-latency ops on secrets</li></ul><p>Add two habits:  and .</p><ul><li>Single header, no deps, no heap; C99/C11; usable from C++</li><li>Primitives designed to <strong>not branch/index on secrets</strong></li><li>Optional SIMD (NEON/SSE2/AVX2) paths that remain constant-time w.r.t. </li><li>Unit tests, micro-benches, and a DUDECT-like harness</li></ul><p>It reduces timing variance. It doesn’t fix power/EM channels or OS noise. Threat models still apply.</p><p>If you picked any of A/B/C as “safe,” you’re not alone I did too, before a pager taught me otherwise. Try the patterns above, run a DUDECT-style check, and tell me where your toolchain fought back (LTO, builtins, odd  rewrites).</p><p><strong>If time varies, secrets leak. Flatline it.</strong></p><p>Proposal: introduce a secret keyword in systems languages (Rust, Zig, LLVM/C) to mark sensitive data. The compiler enforces constant-time rules and replaces unsafe patterns with constant-time APIs.</p>","contentLength":4194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ojs7ve/think_memcmp_is_safe_think_again/"},{"title":"SNMP on Linux stats without running the service.","url":"https://www.reddit.com/r/linux/comments/1ojrka4/snmp_on_linux_stats_without_running_the_service/","date":1761802535,"author":"/u/lickety-split1800","guid":321972,"unread":true,"content":"<p>Over 20 years ago (man I feel old), I had set up SNMP on Linux with Nagios and RRDTool.</p><p>While SNMP is hardly used anymore on Linux it had a lot of metrics that it collected, which was super useful for sending stats to either Nagios or RRDTool at the time.</p><p>Is there anything else out of the box that has a large set of monitors on Linux?</p><p>What are your favourite out-of-the-box Linux metrics collection tools?</p>","contentLength":404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alternative to the LogiOptions+ new Action Ring Feature","url":"https://www.reddit.com/r/linux/comments/1ojr3s9/alternative_to_the_logioptions_new_action_ring/","date":1761800855,"author":"/u/GarThor_TMK","guid":321957,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I made a tool to search papers from selected AI venues","url":"https://www.reddit.com/r/MachineLearning/comments/1ojqgq4/p_i_made_a_tool_to_search_papers_from_selected_ai/","date":1761798628,"author":"/u/ZealousidealStock933","guid":322007,"unread":true,"content":"<p>It uses a language model as backbone so you can query with title, keywords, or even a paper abstract to search. Paper abstracts are the most accurate. It hosted on a personal server as well as on hugging face. Links are in my repo. <a href=\"https://github.com/wenhangao21/ICLR26_Paper_Finder\">https://github.com/wenhangao21/ICLR26_Paper_Finder</a></p>","contentLength":282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unbound on talos","url":"https://www.reddit.com/r/kubernetes/comments/1ojqf7t/unbound_on_talos/","date":1761798488,"author":"/u/Agreeable_Repeat_568","guid":321941,"unread":true,"content":"<p>I am trying to get unbond to run rootless on talos and it seems like it might not be possible? Has anyone gotten current images of unbound running rootless? Iv tried too many options to list, just looking to see if this is even possible?</p>","contentLength":237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm tired of Web Dev","url":"https://www.reddit.com/r/golang/comments/1ojpxv2/im_tired_of_web_dev/","date":1761796894,"author":"/u/Financial_Job_1564","guid":321943,"unread":true,"content":"<div><p>I already have some experience with Golang from building backend projects, but I still feel like I don’t really know much about it.</p><p>Can you give me some examples of projects that use Golang besides backend servers?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Financial_Job_1564\"> /u/Financial_Job_1564 </a>","contentLength":256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can't get data on heavy outputs","url":"https://www.reddit.com/r/linux/comments/1ojo6a1/cant_get_data_on_heavy_outputs/","date":1761791529,"author":"/u/BassDJ812","guid":321942,"unread":true,"content":"<p>So as you can see I can't read date at the bottom and terminal. the only way to get it where it is now is control-minus control. Plus doesn't work I can't scroll down to the data I need on heavy output. I've tried to manipulate the preference menu the best I can in between that and Google research I'm not coming up with any luck. I'm on Kali not that I think that makes a difference. Any help most appreciated </p>","contentLength":412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta, Google, and Microsoft Triple Down on AI Spending","url":"https://www.wired.com/story/microsoft-google-meta-2025-earnings/","date":1761790458,"author":"/u/wiredmagazine","guid":322189,"unread":true,"content":"<p> biggest US tech giants—Microsoft, Meta, and Google—sent investors a blunt message when they reported quarterly earnings on Wednesday: Their <a href=\"https://www.wired.com/story/the-ai-industrys-scaling-obsession-is-headed-for-a-cliff/\">lavish spending</a> on <a href=\"https://www.wired.com/story/openai-oracle-softbank-data-center-stargate-us/\">AI infrastructure</a> is only just getting started.</p><p>Meta said that ​​its capital expenditure would total between $70 billion and $72 billion this year, up from its previous lower forecast of $66 billion to $72 billion. Meta’s chief financial officer Susan Li said that she expected the company's spending would be “notably larger\" next year. The social media giant’s soaring investment matches its soaring revenue: Meta reported raking in $51.24 billion last quarter, up 26 percent year over year.</p><p>CEO Mark Zuckerberg said the company would keep pouring money into infrastructure to meet rising demand for AI and to prepare for potential major breakthroughs in the technology. \"There's a range of timelines for when people think that we're going to get superintelligence,\" Zuckerberg said on a conference call with analysts. \"I think that it's the right strategy to aggressively front-load building capacity, so that way we're prepared for the most optimistic cases.\"</p><p>Meta has moved aggressively to <a href=\"https://www.wired.com/story/mark-zuckerberg-meta-offer-top-ai-talent-300-million/\">recruit AI talent</a> in recent months, offering some researchers compensation packages worth hundreds of millions of dollars. The company also cut some 600 jobs last week in what it said was an effort to make its AI teams more efficient. Meta has <a data-offer-url=\"https://www.nytimes.com/2025/08/19/technology/mark-zuckerberg-meta-ai.html\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.nytimes.com/2025/08/19/technology/mark-zuckerberg-meta-ai.html&quot;}\" href=\"https://www.nytimes.com/2025/08/19/technology/mark-zuckerberg-meta-ai.html\" rel=\"nofollow noopener\" target=\"_blank\">reorganized</a> its AI teams numerous times over the past eight months.</p><p>Meta assured investors that its AI investments were already reaping rewards for the company, but didn’t share many specifics. Meta did say AI was benefiting its ad business and virtual reality product lines, and predicted it would propel those divisions to new heights in the future.</p><p>Google’s parent company, Alphabet, said it expected its 2025 capital expenditures to be between $91 billion and $93 billion. Earlier this year, Alphabet estimated that number would be just $75 billion. Like at Meta, the increase in spending was matched with an increase in revenue. The tech giant said it earned a record $102.3 billion in the third quarter, up 33 percent from a year ago.</p><p>Most of Alphabet’s spending will likely be funneled into data centers and other artificial intelligence initiatives. Google said it earned $15.15 billion from its cloud business in the third quarter, a 35 percent increase from the same period in 2024. Gemini, Google’s <a href=\"https://www.wired.com/story/how-to-use-google-gemini-advanced-ai-chatbot/\">general purpose AI app</a>, now has 650 million monthly active users, up from 450 million last quarter. (For comparison, OpenAI CEO Sam Altman recently said that ChatGPT has <a href=\"https://www.wired.com/story/chatgpt-psychosis-and-self-harm-update/\">800 million</a> weekly users.)</p><p>Microsoft reported revenues of $77 billion for the quarter ending on September 30, up 18 percent from a year ago. Its cloud business revenue was up 26 percent year over year. Its capital expenditures were $34.9 billion this quarter, with much of the investment going toward AI infrastructure. That figure is nearly $5 billion more than previously forecasted, and a 74 percent jump from the same quarter a year ago.</p>","contentLength":3035,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ojnsyz/meta_google_and_microsoft_triple_down_on_ai/"},{"title":"What's going on with the dislike of Ubuntu/Canonical?","url":"https://www.reddit.com/r/linux/comments/1ojn5oz/whats_going_on_with_the_dislike_of_ubuntucanonical/","date":1761788681,"author":"/u/megaslash288","guid":321043,"unread":true,"content":"<p>Basically the title. I have been using Ubuntu Cinnamon for awhile for both school and home use (gaming, programming, vr, game dev, etc) and it seems to be a very well maintained, functional, and stable distro with good support through .deb packages for printers n such, and the apt repository seems well checked and stable. It also seems to have good peripheral drivers built in. Seemed a little silly for it to not include flatpack by default but thats easily fixed. Overall though, I am still a relative newcomer to Linux and haven't been in the community that much until recently, so I am a lil out of the loop.</p>","contentLength":614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"John Carmack on updating variables","url":"https://x.com/ID_AA_Carmack/status/1983593511703474196#m","date":1761787969,"author":"/u/levodelellis","guid":321097,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ojmwd9/john_carmack_on_updating_variables/"},{"title":"Do you know what really happens when you run kubectl apply?","url":"https://medium.com/integr8me/what-happens-when-you-run-kubectl-apply-ce3bfb5e61c4","date":1761786192,"author":"/u/fabioluciano","guid":321029,"unread":true,"content":"<p>Recently, while working on the documentation about best practices for building solutions with , I came across some topics that, despite being fundamental, usually receive little attention when we talk about Kubernetes.</p><p>These topics seemed so interesting to me that I decided to turn them into a series of articles — and this is the first one.</p><p>To start, I want to ask you a simple question:</p><blockquote><p><em>Do you know exactly what happens when you run </em><strong><em>kubectl apply -f manifest.yaml</em></strong></p></blockquote><p>It might seem like a trivial action, but behind this command lies a well-structured process, divided into three major stages: , , and .</p><p>In fact, the topic that most sparked my interest was the  process, but it’s impossible to fully understand it without first going through the other two. These three components work together and form the foundation for all Kubernetes security and access control.</p><p>Despite seeming simple, each of these elements — , , and  — is highly configurable and full of nuances. Understanding how they connect helps not only to solve day-to-day problems but also to create more secure and elegant solutions.</p><p>Let’s start by understanding :</p><p>When a user executes the command <strong>kubectl apply -f manifest.yaml</strong>, the request’s journey begins like this:</p><ol><li>: Kubernetes identifies  is making the call.</li><li>: The system checks  this person (or service) has permission to perform the operation.</li><li>: The resource sent is analyzed, validated, and in some cases, modified before being stored in .</li></ol><p>Only after these three steps is the change effectively applied to the cluster.</p><p>Now that we have an overview, let’s dive into each of these processes, starting with Authentication.</p><p>If you’ve been working with Kubernetes for some time, you probably know that every interaction made with  ends up reaching the , which is the heart of the cluster.</p><p>It works similarly to a : each command is translated into an HTTP request containing information about the desired resource, the type of operation, and who is making the call.</p><p>We can observe this message exchange in real-time by enabling the verbosity flag on , with .</p><p>The higher the number, the more details are displayed. For example:</p><p>Let’s break down what is being displayed in the output of the command  👇</p><p>1️⃣  This is where everything begins.  received the  command and, thanks to the  flag, is displaying internal details of its communication with the API Server. This verbosity level shows, among other things, which configurations are being loaded and which HTTP requests are being made.</p><p>2️⃣ <strong>Configuration file loaded</strong> reports that it loaded the credentials and context from the  file. This file contains the information that identifies , <strong>which cluster is being accessed</strong>, and <strong>how to authenticate to it</strong> (for example, via token, certificate, or OIDC provider).</p><p>3️⃣ <strong>Request sent to the API Server</strong> Here we see  transforming the  command into an  to the endpoint:</p><pre></pre><p>This is te moment when the call truly “leaves your terminal” and reaches Kubernetes. The request header (headers) includes authentication information (such as tokens), the expected response format (), and the  which indicates the  version.</p><p>4️⃣  The API Server processes the request, authenticates the user, checks if they have permission (authorization), and, if everything is correct, returns the result with the  status. Following this, we see the list of Pods in the  namespace, indicating that the request was accepted and completed successfully.</p><p>Authentication is, therefore, the first filter: it defines <strong>who the user or service is</strong> that is trying to interact with the cluster.</p><p>This identification can happen in several ways — such as via certificates, service tokens, identity providers (OIDC), or even custom plugins.</p><p>After the user or service is authenticated, the next step is to verify <strong>what they can do within the cluster</strong>. This is the role of .</p><p>While authentication answers the question , authorization answers “.</p><p>And this decision is always made by the API Server, before the request proceeds.</p><p>Kubernetes offers different , and each has its purpose. The most common are:</p><ul><li><strong>RBAC (Role-Based Access Control)</strong> — the most used. Defines permissions based on roles and links these roles to users, groups, or service accounts.</li><li><strong>ABAC (Attribute-Based Access Control)</strong> — uses attributes defined in JSON policies, allowing for more flexible but less practical rules to manage.</li><li> — forwards the decision to an external service, ideal for integrations with corporate authentication systems.</li><li> and / — used in very specific cases (the first for internal node authentication and the second generally for testing).</li></ul><p>In practice,  is the most common model, as it is simple and integrated into the Kubernetes ecosystem.</p><pre></pre><p>In this example, the user  can  in the  namespace, but cannot create, delete, or modify any.</p><p>With RBAC configured, the API Server consults these rules whenever a request arrives.</p><p>If the user tries to do something outside the assigned permissions, they will receive a  error — and the process is interrupted even before reaching the  stage.</p><p>If authentication identifies the user and authorization confirms they can perform the action, the  process is the moment when Kubernetes <strong>analyzes and modifies what will be created or changed</strong> before it goes to the database ().</p><p>This is where one of the most powerful parts of Kubernetes comes in: the .</p><p>Think of them as “doormen” who inspect everything entering the cluster. They can , , or even  resources before Kubernetes accepts the operation.</p><p>There are two main types:</p><ul><li><strong>Validating Admission Controllers</strong> — analyze the request and decide whether it is valid or not.</li><li><strong>Mutating Admission Controllers</strong> — can alter the request before it is persisted.</li></ul><p>A practical example: imagine your company wants to ensure that <strong>every Pod has a label indicating the responsible team</strong>.</p><p>An Admission Controller can automatically reject any Pod that does not have this label, or even add the missing label (in the case of a mutating controller).</p><p>Kubernetes already comes with several ready-made Admission Controllers — such as , , , among others — , but it is also possible to create your own using .</p><p>A , for example, can intercept the creation of a Pod and automatically inject sidecars (such as a logging, metrics, or security container) (hello ).</p><p>This type of automation is widely used by tools like , , and various  that manage custom resources.</p><p>A simplified flow would look like this:</p><p>Note that the admission process is the <strong>last step before the resource is written</strong>.</p><p>This means that any modification made at this point is the last opportunity to adjust, validate, or reinforce security and compliance policies.</p><p>The next time you run a simple , remember everything that happens behind the scenes:</p><ul><li>Kubernetes first <strong>checks who you are (Authentication)</strong>,</li><li>then <strong>verifies what you can do (Authorization)</strong>,</li><li>and finally <strong>analyzes and adjusts what you are trying to create (Admission)</strong>.</li></ul><p>These three processes are the foundation of all cluster security and governance.</p><p>Understanding them is essential not only for those who administer Kubernetes but also for those who develop solutions that interact with it — such as sidecars, operators, or custom webhooks.</p><p>In the next articles in the series, I will explore each of these components in greater depth, bringing practical examples, use cases, and even some curiosities that are rarely covered in the official documentation.</p>","contentLength":7367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1ojm9i9/do_you_know_what_really_happens_when_you_run/"},{"title":"Exclusive: OpenAI lays groundwork for juggernaut IPO at up to $1 trillion valuation","url":"https://www.reuters.com/business/openai-lays-groundwork-juggernaut-ipo-up-1-trillion-valuation-2025-10-29/","date":1761784265,"author":"/u/esporx","guid":322094,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ojlkru/exclusive_openai_lays_groundwork_for_juggernaut/"},{"title":"Kubernetes Podcast episode 262: GKE 10 Year Anniversary, with Gari Singh","url":"https://www.reddit.com/r/kubernetes/comments/1ojkuho/kubernetes_podcast_episode_262_gke_10_year/","date":1761782331,"author":"/u/kubernetespodcast","guid":320996,"unread":true,"content":"<div><p>Google Kubernetes Engine (GKE) recently celebrated its 10th anniversary! 🎉 In our latest podcast episode, we talk with GKE Product Manager Gari Singh to reflect on GKE's journey over the last decade.</p><ul><li> From the early days of complex container orchestration to today's 'one-click' production clusters powered by Autopilot, and the continuous effort to simplify infrastructure management.</li><li> How GKE supports demanding AI workloads and the exciting potential of leveraging AI  Kubernetes, enabling smarter, more autonomous operations and enhanced observability.</li><li> Gary's favorite features, including In-Place Pod Resizing (IPPR) and Container Optimized Compute, which are crucial for dynamic scaling and efficiency.</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/kubernetespodcast\"> /u/kubernetespodcast </a>","contentLength":749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Increase Performance when sending struct accross HTTP / TCP","url":"https://www.reddit.com/r/golang/comments/1ojktka/increase_performance_when_sending_struct_accross/","date":1761782261,"author":"/u/D4kzy","guid":321932,"unread":true,"content":"<p>I have a client and a server that talk HTTP (sometimes raw TCP).</p><p>On the client I define a struct that has a string field, a []string field and a []byte field.</p><p>I define the same struct server side.</p><p>I want to send this instantiated struct from the client to the server.</p><p>What I did till now is use the json marshall to send the data as a json through the Conn.</p><p>I have slight performance issues and I thing it is coming from here. My guess is that when I marshal and unmarshal with json, the []byte field of my struct is base64 encoded. When []byte is big this is adding around 33% overhead.</p><p>To avoid this I thought about GZIP, but I am afraid the GZIP computation time will result in even poorer perf.</p><p>What way to send data do you suggest to have best speed (sending a lot of HTTP request) ?</p>","contentLength":781,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How To Be A Linux-Based Graphic Designer","url":"https://www.youtube.com/watch?v=sVztMTafuLA","date":1761777915,"author":"/u/yoor_thiziri","guid":320973,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ojj3gg/how_to_be_a_linuxbased_graphic_designer/"},{"title":"Go 1.25 includes a new experimental garbage collector, Green Tea","url":"https://go.dev/blog/greenteagc","date":1761776849,"author":"/u/x021","guid":320975,"unread":true,"content":"<p>Go 1.25 includes a new experimental garbage collector called Green Tea,\navailable by setting  at build time.\nMany workloads spend around 10% less time in the garbage collector, but some\nworkloads see a reduction of up to 40%!</p><p>It’s production-ready and already in use at Google, so we encourage you to\ntry it out.\nWe know some workloads don’t benefit as much, or even at all, so your feedback\nis crucial to helping us move forward.\nBased on the data we have now, we plan to make it the default in Go 1.26.</p><p>What follows is a blog post based on Michael Knyszek’s GopherCon 2025 talk.\nWe’ll update this blog post with a link to the talk once it’s available online.</p><h2>Tracing garbage collection</h2><p>Before we discuss Green Tea let’s get us all on the same page about garbage\ncollection.</p><p>The purpose of garbage collection is to automatically reclaim and reuse memory\nno longer used by the program.</p><p>To this end, the Go garbage collector concerns itself with  and\n.</p><p>In the context of the Go runtime,  are Go values whose underlying\nmemory is allocated from the heap.\nHeap objects are created when the Go compiler can’t figure out how else to allocate\nmemory for a value.\nFor example, the following code snippet allocates a single heap object: the backing\nstore for a slice of pointers.</p><pre><code>var x = make([]*int, 10) // global\n</code></pre><p>The Go compiler can’t allocate the slice backing store anywhere except the heap,\nsince it’s very hard, and maybe even impossible, for it to know how long  will\nrefer to the object for.</p><p> are just numbers that indicate the location of a Go value in memory,\nand they’re how a Go program references objects.\nFor example, to get the pointer to the beginning of the object allocated in the\nlast code snippet, we can write:</p><p>Go’s garbage collector follows a strategy broadly referred to as <em>tracing garbage\ncollection</em>, which just means that the garbage collector follows, or traces, the\npointers in the program to identify which objects the program is still using.</p><p>More specifically, the Go garbage collector implements the mark-sweep algorithm.\nThis is much simpler than it sounds.\nImagine objects and pointers as a sort of graph, in the computer science sense.\nObjects are nodes, pointers are edges.</p><p>The mark-sweep algorithm operates on this graph, and as the name might suggest,\nproceeds in two phases.</p><p>In the first phase, the mark phase, it walks the object graph from well-defined\nsource edges called .\nThink global and local variables.\nThen, it  everything it finds along the way as , to avoid going in\ncircles.\nThis is analogous to your typical graph flood algorithm, like a depth-first or\nbreadth-first search.</p><p>Next is the sweep phase.\nWhatever objects were not visited in our graph walk are unused, or ,\nby the program.\nWe call this state unreachable because it is impossible with normal safe Go code\nto access that memory anymore, simply through the semantics of the language.\nTo complete the sweep phase, the algorithm simply iterates through all the\nunvisited nodes and marks their memory as free, so the memory allocator can reuse\nit.</p><p>You may think I’m oversimplifying a bit here.\nGarbage collectors are frequently referred to as , and .\nAnd you’d be partially right, there are more complexities.</p><p>For example, this algorithm is, in practice, executed concurrently with your\nregular Go code.\nWalking a graph that’s mutating underneath you brings challenges.\nWe also parallelize this algorithm, which is a detail that’ll come up again\nlater.</p><p>But trust me when I tell you that these details are mostly separate from the\ncore algorithm.\nIt really is just a simple graph flood at the center.</p><p>Let’s walk through an example.\nNavigate through the slideshow below to follow along.</p><p>After all that, I think we have a handle on what the Go garbage collector is actually doing.\nThis process seems to work well enough today, so what’s the problem?</p><p>Well, it turns out we can spend  of time executing this particular algorithm in some\nprograms, and it adds substantial overhead to nearly every Go program.\nIt’s not that uncommon to see Go programs spending 20% or more of their CPU time in the\ngarbage collector.</p><p>Let’s break down where that time is being spent.</p><p>At a high level, there are two parts to the cost of the garbage collector.\nThe first is how often it runs, and the second is how much work it does each time it runs.\nMultiply those two together, and you get the total cost of the garbage collector.</p><figure><figcaption>\n    Total GC cost = Number of GC cycles × Average cost per GC cycle\n    </figcaption></figure><p>But for now let’s focus only on the second part, the cost per cycle.</p><p>From years of poring over CPU profiles to try to improve performance, we know two big things\nabout Go’s garbage collector.</p><p>The first is that about 90% of the cost of the garbage collector is spent marking,\nand only about 10% is sweeping.\nSweeping turns out to be much easier to optimize than marking,\nand Go has had a very efficient sweeper for many years.</p><p>The second is that, of that time spent marking, a substantial portion, usually at least 35%, is\nsimply spent  on accessing heap memory.\nThis is bad enough on its own, but it completely gums up the works on what makes modern CPUs\nactually fast.</p><h3>“A microarchitectural disaster”</h3><p>What does “gum up the works” mean in this context?\nThe specifics of modern CPUs can get pretty complicated, so let’s use an analogy.</p><p>Imagine the CPU driving down a road, where that road is your program.\nThe CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it,\nand the way needs to be clear.\nBut the graph flood algorithm is like driving through city streets for the CPU.\nThe CPU can’t see around corners and it can’t predict what’s going to happen next.\nTo make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid\npedestrians.\nIt hardly matters how fast your engine is because you never get a chance to get going.</p><p>Let’s make that more concrete by looking at our example again.\nI’ve overlaid the heap here with the path that we took.\nEach left-to-right arrow represents a piece of scanning work that we did\nand the dashed arrows show how we jumped around between bits of scanning work.</p><p>Notice that we were jumping all over memory doing tiny bits of work in each place.\nIn particular, we’re frequently jumping between pages, and between different parts of pages.</p><p>Modern CPUs do a lot of caching.\nGoing to main memory can be up to 100x slower than accessing memory that’s in our cache.\nCPU caches are populated with memory that’s been recently accessed, and memory that’s nearby to\nrecently accessed memory.\nBut there’s no guarantee that any two objects that point to each other will  be close to each\nother in memory.\nThe graph flood doesn’t take this into account.</p><p>Quick side note: if we were just stalling fetches to main memory, it might not be so bad.\nCPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see\nfar enough ahead.\nBut in the graph flood, every bit of work is small, unpredictable, and highly dependent on the\nlast, so the CPU is forced to wait on nearly every individual memory fetch.</p><p>And unfortunately for us, this problem is only getting worse.\nThere’s an adage in the industry of “wait two years and your code will get faster.”</p><p>But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite.\n“Wait two years and your code will get slower.”\nThe trends in modern CPU hardware are creating new challenges for garbage collector performance:</p><p><strong>Non-uniform memory access.</strong>\nFor one, memory now tends to be associated with subsets of CPU cores.\nAccesses by  CPU cores to that memory are slower than before.\nIn other words, the cost of a main memory access <a href=\"https://jprahman.substack.com/p/sapphire-rapids-core-to-core-latency\" rel=\"noreferrer\" target=\"_blank\">depends on which CPU core is accessing\nit</a>.\nIt’s non-uniform, so we call this non-uniform memory access, or NUMA for short.</p><p><strong>Reduced memory bandwidth.</strong>\nAvailable memory bandwidth per CPU is trending downward over time.\nThis just means that while we have more CPU cores, each core can submit relatively fewer\nrequests to main memory, forcing non-cached requests to wait longer than before.</p><p>\nAbove, we looked at a sequential marking algorithm, but the real garbage collector performs this\nalgorithm in parallel.\nThis scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes\na bottleneck, even with careful design.</p><p><strong>Modern hardware features.</strong>\nNew hardware has fancy features like vector instructions, which let us operate on a lot of data at once.\nWhile this has the potential for big speedups, it’s not immediately clear how to make that work for\nmarking because marking does so much irregular and often small pieces of work.</p><p>Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm.\nThe key idea behind Green Tea is astonishingly simple:</p><p><em>Work with pages, not objects.</em></p><p>Sounds trivial, right?\nAnd yet, it took a lot of work to figure out how to order the object graph walk and what we needed to\ntrack to make this work well in practice.</p><p>More concretely, this means:</p><ul><li>Instead of scanning objects we scan whole pages.</li><li>Instead of tracking objects on our work list, we track whole pages.</li><li>We still need to mark objects at the end of the day, but we’ll track marked objects locally to each\npage, rather than across the whole heap.</li></ul><p>Let’s see what this means in practice by looking at our example heap again, but this time\nrunning Green Tea instead of the straightforward graph flood.</p><p>As above, navigate through the annotated slideshow to follow along.</p><p>Let’s come back around to our driving analogy.\nAre we finally getting on the highway?</p><p>Let’s recall our graph flood picture before.</p><p>We jumped around a whole lot, doing little bits of work in different places.\nThe path taken by Green Tea looks very different.</p><p>Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B.\nThe longer these arrows, the better, and with bigger heaps, this effect can be much stronger.\n the magic of Green Tea.</p><p>It’s also our opportunity to ride the highway.</p><p>This all adds up to a better fit with the microarchitecture.\nWe can now scan objects closer together with much higher probability, so\nthere’s a better chance we can make use of our caches and avoid main memory.\nLikewise, per-page metadata is more likely to be in cache.\nTracking pages instead of objects means work lists are smaller,\nand less pressure on work lists means less contention and fewer CPU stalls.</p><p>And speaking of the highway, we can take our metaphorical engine into gears we’ve never been able to\nbefore, since now we can use vector hardware!</p><p>If you’re only vaguely familiar with vector hardware, you might be confused as to how we can use it here.\nBut besides the usual arithmetic and trigonometric operations,\nrecent vector hardware supports two things that are valuable for Green Tea:\nvery wide registers, and sophisticated bit-wise operations.</p><p>Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers.\nThis is wide enough to hold all of the metadata for an entire page in just two registers,\nright on the CPU, enabling Green Tea to work on an entire page in just a few straight-line\ninstructions.\nVector hardware has long supported basic bit-wise operations on whole vector registers, but starting\nwith AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector “Swiss army knife” instruction\nthat enables a key step of the Green Tea scanning process to be done in just a few CPU cycles.\nTogether, these allow us to turbo-charge the Green Tea scan loop.</p><p>This wasn’t even an option for the graph flood, where we’d be jumping between scanning objects that\nare all sorts of different sizes.\nSometimes you needed two bits of metadata and sometimes you needed ten thousand.\nThere simply wasn’t enough predictability or regularity to use vector hardware.</p><p>If you want to nerd out on some of the details, read along!\nOtherwise, feel free to skip ahead to the <a href=\"https://go.dev/blog/greenteagc#evaluation\">evaluation</a>.</p><p>To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.</p><p>There’s a lot going on here and we could probably fill an entire blog post just on how this works.\nFor now, let’s just break it down at a high level:</p><ol><li><p>First we fetch the “seen” and “scanned” bits for a page.\nRecall, these are one bit per object in the page, and all objects in a page have the same size.</p></li><li><p>Next, we compare the two bit sets.\nTheir union becomes the new “scanned” bits, while their difference is the “active objects” bitmap,\nwhich tells us which objects we need to scan in this pass over the page (versus previous passes).</p></li><li><p>We take the difference of the bitmaps and “expand” it, so that instead of one bit per object,\nwe have one bit per word (8 bytes) of the page.\nWe call this the “active words” bitmap.\nFor example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap\nwill be copied to 6 bits in the active words bitmap.\nLike so:</p></li></ol><figure><div> → <pre>000000 000000 111111 111111 ...</pre></div></figure><ol start=\"4\"><li><p>Next we fetch the pointer/scalar bitmap for the page.\nHere, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word\nstores a pointer.\nThis data is managed by the memory allocator.</p></li><li><p>Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap.\nThe result is the “active pointer bitmap”: a bitmap that tells us the location of every\npointer in the entire page contained in any live object we haven’t scanned yet.</p></li><li><p>Finally, we can iterate over the memory of the page and collect all the pointers.\nLogically, we iterate over each set bit in the active pointer bitmap,\nload the pointer value at that word, and write it back to a buffer that\nwill later be used to mark objects seen and add pages to the work list.\nUsing vector instructions, we’re able to do this 64 bytes at a time,\nin just a couple instructions.</p></li></ol><p>Part of what makes this fast is the  instruction,\npart of the “Galois Field New Instructions” x86 extension,\nand the bit manipulation Swiss army knife we referred to above.\nIt’s the real star of the show, since it lets us do step (3) in the scanning kernel very, very\nefficiently.\nIt performs a bit-wise <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\" rel=\"noreferrer\" target=\"_blank\">affine\ntransformations</a>,\ntreating each byte in a vector as itself a mathematical vector of 8 bits\nand multiplying it by an 8x8 bit matrix.\nThis is all done over the <a href=\"https://en.wikipedia.org/wiki/Finite_field\" rel=\"noreferrer\" target=\"_blank\">Galois field</a>,\nwhich just means multiplication is AND and addition is XOR.\nThe upshot of this is that we can define a few 8x8 bit matrices for each\nobject size that perform exactly the 1:n bit expansion we need.</p><p>For the full assembly code, see <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/scan_amd64.s;l=23;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">this\nfile</a>.\nThe “expanders” use different matrices and different permutations for each size class,\nso they’re in a <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/expand_amd64.s;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">separate file</a>\nthat’s written by a <a href=\"https://cs.opensource.google/go/go/+/master:src/internal/runtime/gc/scan/mkasm.go;drc=041f564b3e6fa3f4af13a01b94db14c1ee8a42e0\" rel=\"noreferrer\" target=\"_blank\">code generator</a>.\nAside from the expansion functions, it’s really not a lot of code.\nMost of it is dramatically simplified by the fact that we can perform most of the above\noperations on data that sits purely in registers.\nAnd, hopefully soon this assembly code <a href=\"https://go.dev/issue/73787\">will be replaced with Go code</a>!</p><p>Credit to Austin Clements for devising this process.\nIt’s incredibly cool, and incredibly fast!</p><p>So that’s it for how it works.\nHow much does it actually help?</p><p>It can be quite a lot.\nEven without the vector enhancements, we see reductions in garbage collection CPU costs\nbetween 10% and 40% in our benchmark suite.\nFor example, if an application spends 10% of its time in the garbage collector, then that\nwould translate to between a 1% and 4% overall CPU reduction, depending on the specifics of\nthe workload.\nA 10% reduction in garbage collection CPU time is roughly the modal improvement.\n(See the <a href=\"https://go.dev/issue/73581\">GitHub issue</a> for some of these details.)</p><p>We’ve rolled Green Tea out inside Google, and we see similar results at scale.</p><p>We’re still rolling out the vector enhancements,\nbut benchmarks and early results suggest this will net an additional 10% GC CPU reduction.</p><p>While most workloads benefit to some degree, there are some that don’t.</p><p>Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a\nsingle page in one pass to counteract the costs of the accumulation process.\nThis is clearly the case if the heap has a very regular structure: objects of the same size at a\nsimilar depth in the object graph.\nBut there are some workloads that often require us to scan only a single object per page at a time.\nThis is potentially worse than the graph flood because we might be doing more work than before while\ntrying to accumulate objects on pages and failing.</p><p>The implementation of Green Tea has a special case for pages that have only a single object to scan.\nThis helps reduce regressions, but doesn’t completely eliminate them.</p><p>However, it takes a lot less per-page accumulation to outperform the graph flood\nthan you might expect.\nOne surprise result of this work was that scanning a mere 2% of a page at a time\ncan yield improvements over the graph flood.</p><p>Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled\nby setting the environment variable  to  at build time.\nThis doesn’t include the aforementioned vector acceleration.</p><p>We expect to make it the default garbage collector in Go 1.26, but you’ll still be able to opt-out\nwith <code>GOEXPERIMENT=nogreenteagc</code> at build time.\nGo 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of\ntweaks and improvements based on feedback we’ve collected so far.</p><p>If you can, we encourage you to try at Go tip-of-tree!\nIf you prefer to use Go 1.25, we’d still love your feedback.\nSee <a href=\"https://go.dev/issue/73581#issuecomment-2847696497\">this GitHub\ncomment</a> with some details on\nwhat diagnostics we’d be interested in seeing, if you can share, and the preferred channels for\nreporting feedback.</p><p>Before we wrap up this blog post, let’s take a moment to talk about the journey that got us here.\nThe human element of the technology.</p><p>The core of Green Tea may seem like a single, simple idea.\nLike the spark of inspiration that just one single person had.</p><p>But that’s not true at all.\nGreen Tea is the result of work and ideas from many people over several years.\nSeveral people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David\nChase, and Keith Randall.\nMicroarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped\ndirect the design exploration.\nThere were a lot of ideas that didn’t work, and there were a lot of details that needed figuring out.\nJust to make this single, simple idea viable.</p><p>The seeds of this idea go all the way back to 2018.\nWhat’s funny is that everyone on the team thinks someone else thought of this initial idea.</p><p>Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe\ncrawling in Japan and drinking LOTS of matcha!\nThis prototype showed that the core idea of Green Tea was viable.\nAnd from there we were off to the races.</p><p>Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even\nfurther.</p><p>This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire\ndesign space.\nOne that we don’t think any of us could’ve navigated alone.\nIt’s not enough to just have the idea, but you need to figure out the details and prove it.\nAnd now that we’ve done it, we can finally iterate.</p><p>The future of Green Tea is bright.</p><p>Once again, please try it out by setting  and let us know how it goes!\nWe’re really excited about this work and want to hear from you!</p>","contentLength":19499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ojio0t/go_125_includes_a_new_experimental_garbage/"},{"title":"[R] Researchers from the Center for AI Safety and Scale AI have released the Remote Labor Index (RLI), a benchmark testing AI agents on 240 real-world freelance jobs across 23 domains.","url":"https://www.reddit.com/r/MachineLearning/comments/1ojinwl/r_researchers_from_the_center_for_ai_safety_and/","date":1761776841,"author":"/u/michael-lethal_ai","guid":321030,"unread":true,"content":"<p>They find current AI agents have low but steadily improving performance. The best-performing agent (Manus) successfully completed 2.5% of projects, earning $1,720 out of a possible $143,991. However, newer models consistently perform better than older ones, indicating measurable advancement toward automating remote work.</p>","contentLength":322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We built a golang high performance WAF, is good. We plan to beat modsec.","url":"https://www.reddit.com/r/golang/comments/1ojhwt1/we_built_a_golang_high_performance_waf_is_good_we/","date":1761775016,"author":"/u/BusinessStreet2147","guid":320974,"unread":true,"content":"<p>Modsec is a sloppy tool thats honestly sucky. Its config hell, rule hell and its outdated ash. Its vulnerable to just about EVERY modern attack surface. We are gonna make that change: <a href=\"https://github.com/1rhino2/RhinoWAF/\">https://github.com/1rhino2/RhinoWAF/</a></p><p>Just to clarify, we are not a company of any sorts, simply people willing to help.</p>","contentLength":302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why doesn’t Go auto order struct fields for memory efficiency?","url":"https://www.reddit.com/r/golang/comments/1ojho4u/why_doesnt_go_auto_order_struct_fields_for_memory/","date":1761774443,"author":"/u/OrneryComputer1396","guid":320950,"unread":true,"content":"<p>I recently discovered that the order of fields in a Go struct (and also some other languages) can significantly affect how much memory your program uses.</p><p>At first, I assumed Go would handle field ordering automatically to minimize padding, but it turns out it doesn’t. The order you write fields in is exactly how they’re laid out in memory.</p><p>So, I made a small CLI tool that automatically reorders struct fields across your codebase to optimize memory layout and reduce padding. I would love some feedbacks on this!!</p>","contentLength":518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Just Terraform (proof of concept)","url":"https://www.reddit.com/r/kubernetes/comments/1ojg6mu/just_terraform_proof_of_concept/","date":1761770949,"author":"/u/amiorin","guid":320927,"unread":true,"content":"<div><p>The Terraform + ArgoCD combination is mainstream. I'd like to replicate the same capabilities of Terraform + ArgoCD using only Terraform. I have already achieved promising results transforming Terraform in a control plane for AWS (<a href=\"https://www.big-config.it/blog/control-plane-in-big-config/\">https://www.big-config.it/blog/control-plane-in-big-config/</a>) and now I want to try with K8s.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/amiorin\"> /u/amiorin </a>","contentLength":353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Share your Linux battlestations!","url":"https://www.reddit.com/r/linux/comments/1ojfqbn/share_your_linux_battlestations/","date":1761769885,"author":"/u/West-Amphibian-2343","guid":320948,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Project to learn kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1ojffjf/project_to_learn_kubernetes/","date":1761769190,"author":"/u/FunDirt541","guid":320928,"unread":true,"content":"<p>I want to build a project and I thought of using kubernetes, or k3s for that matter. I know nothing about kubernetes and I wasn't sure if the project I am thinking off would be a great fit. Basically I want to build an online VM that runs on the web, that is isolanted for each user, the idea is that they will have their own cpu/ram/disk space with a dev environment, a bit like a cloudshell. And I would like to get some guidance if setting kubernetes (or k3s if that might be overkill) is the right or one of the right way to go about. I value performance, shared ressources as much as possible without sacrificing, user exerience. </p>","contentLength":635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DASH - a terminal UI for GitHub - v4.19.0 is out","url":"https://www.reddit.com/r/golang/comments/1ojfey0/dash_a_terminal_ui_for_github_v4190_is_out/","date":1761769154,"author":"/u/e-lys1um","guid":320949,"unread":true,"content":"<p>DASH is a terminal UI for GitHub and I've just released some goodies in v4.19.0!</p><p>DASH now supports defining global settings that will always be applied, and lets you override them with a per-repo or one-time basis.</p><p>This lets you set your theme, keybindings and any other setting by defining them once.</p><p>Run  to see the list of current sponsors. Thank you to everyone who donated!</p><p>I've fixed a bunch of layout issues that caused the UI to break. Expect a smoother experience</p>","contentLength":468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rap album about Kubernetes trauma and SRE folklore. 😱","url":"https://www.reddit.com/r/kubernetes/comments/1ojeztz/rap_album_about_kubernetes_trauma_and_sre_folklore/","date":1761768199,"author":"/u/Technical_Corner3553","guid":320907,"unread":true,"content":"<p>Not sure if this is a first. But the music and lyrics speak to me and are spot on. The song Ingress flex would have been the song to play during the AWS outage last week. The website cracks me up too. </p><p>Check out Poddaddy 5x9 on your favorite streaming app. </p>","contentLength":256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D]NLP conferences look like a scam..","url":"https://www.reddit.com/r/MachineLearning/comments/1ojeldl/dnlp_conferences_look_like_a_scam/","date":1761767276,"author":"/u/BetterbeBattery","guid":320930,"unread":true,"content":"<p>Not trying to punch down on other smart folks, but honestly, I feel like most NLP conference papers are kinda scams. Out of 10 papers I read, 9 have zero theoretical justification, and the 1 that does usually calls something a  when it’s basically just a lemma with ridiculous assumptions. And then they all cliam about like a 1% benchmark improvement using methods that are impossible to reproduce because of the insane resource constraints in the LLM world.. Even more funny, most of the benchmarks and made by themselves </p>","contentLength":526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When I do \"ls -al\" on raspberry pi running Debian Bookworm shows weird file names","url":"https://www.reddit.com/r/linux/comments/1ojeb2h/when_i_do_ls_al_on_raspberry_pi_running_debian/","date":1761766626,"author":"/u/dsandhu90","guid":320931,"unread":true,"content":"<p>Was I being hacked ? I was surprised to see list of files and directory are all weird. It is in my home directory. Was i being hacked ? How to do some checks and reverse those file names back to original ones ?</p>","contentLength":210,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HDMI 4k120 RGB HDR 10bit with VRR workaround for AMD GPUs","url":"https://www.reddit.com/r/linux/comments/1oje3nl/hdmi_4k120_rgb_hdr_10bit_with_vrr_workaround_for/","date":1761766156,"author":"/u/rbmaster","guid":321031,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The average codebase is now 50% dependencies — is this sustainable?","url":"https://www.intel.com/content/www/us/en/developer/articles/guide/the-careful-consumption-of-open-source-software.html?utm_source=chatgpt.com","date":1761765415,"author":"/u/Legitimate_Sun1783","guid":320908,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ojdrv9/the_average_codebase_is_now_50_dependencies_is/"},{"title":"Best retro/2D games for Linux","url":"https://www.reddit.com/r/linux/comments/1ojddoi/best_retro2d_games_for_linux/","date":1761764529,"author":"/u/MyAltAccountNum1","guid":320909,"unread":true,"content":"<p>I'm just wondering which games are best for old Linux machines, specifically retro or 2D games. I already have OpenRA and Star Control II on it, and I tried installing Factorio buy my laptop is so ass it barely runs lmao</p>","contentLength":220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Looking for guidance on open-sourcing a hierarchical recommendation dataset (user–chapter–series interactions)","url":"https://www.reddit.com/r/MachineLearning/comments/1ojcjk1/d_looking_for_guidance_on_opensourcing_a/","date":1761762669,"author":"/u/Just_Plantain142","guid":320879,"unread":true,"content":"<p>I’m exploring the possibility of open-sourcing a large-scale  recommender dataset from my company and I’d like to get feedback from the community before moving forward.</p><p>Most open datasets (MovieLens, Amazon Reviews, Criteo CTR, etc.) treat recommendation as a flat user–item problem. But in real systems like Netflix or Prime Video, users don’t just interact with a movie or series directly they interact with episodes or chapters within those series</p><p>This creates a natural :</p><pre><code>User → interacts with → Chapters → belong to → Series </code></pre><p>In my company case our dataset is literature dataset where authors keep writing chapters with in a series and the reader read those chapters.</p><p>The tricking thing here is we can't recommend a user a particular chapter, we recommend them series, and the interaction is always on the chapter level of a particular series.</p><p>Here’s what we observed in practice:</p><ul><li>We train models on <strong>user–chapter interactions</strong>.</li><li>When we embed chapters, those from the same series <strong>cluster together naturally</strong> even though the model isn’t told about the series ID.</li></ul><p>This pattern is <em>ubiquitous in real-world media and content platforms</em> but rarely discussed or represented in open datasets. Every public benchmark I know (MovieLens, BookCrossing, etc.) ignores this structure and flattens behavior to user–item events.</p><p>I’m now considering helping open-source such data to enable research on:</p><ul><li>Hierarchical or multi-level recommendation</li><li>Series-level inference from fine-grained interactions</li></ul><p>Good thing is I have convinced my company for this, and they are up for it, our dataset is huge if we are successful at doing it will beat all the dataset so far in terms of size.</p><p>None of my team member including me have any experience in open sourcing any dataset Would love to hear your thoughts, references, or experiences in trying to model this hierarchy in your own systems and definitely looking for advice, mentorship and any form external aid that we can get to make this a success.</p>","contentLength":1988,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Azure down","url":"https://azure.status.microsoft/en-us/status","date":1761760408,"author":"/u/RR_2025","guid":322049,"unread":true,"content":"<div><ul></ul></div>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ojbiss/azure_down/"},{"title":"Frustrated by lack of maintained crates","url":"https://www.reddit.com/r/rust/comments/1ojb87w/frustrated_by_lack_of_maintained_crates/","date":1761759777,"author":"/u/MasteredConduct","guid":320929,"unread":true,"content":"<p>I love Rust. This isn't a criticism of Rust itself. This is plea for advice on how to sell Rust in production.</p><p>One of the hardest things to do when selling Rust for a project, in my experience, has been finding well supported community library crates. Where other languages have corporate backed, well maintained libraries, more often than not I find that Rust either does not have a library to do what I want, or that library hasn't been touched for 3 years, or it's a single person side project with a handful of drive by contributors. For a personal project it's fine. When I go to my team and say, let's use Rust it has library to do X, they will rightly say well C++ has a library for X and it's been around for two decades, and is built and maintained by Google.</p><p>A good concrete example has been containers. One option, shiplift, has been abandoned for 4 years. The other option, bollard, *is great*, but it's a hobby project mostly driven by one person. The conversation becomes, why use Rust when Golang has the libraries docker and podman are actually built on we could use directly.</p><p>Another, less concerning issue is that a lot of the good libraries are simply FFI wrappers around a C library. Do you need to use ssh in go? It's in an official Google/Go Language Team library and written in Go. In Rust you can use a wrapper around libssh2 which is written in.... C. How do you convince someone that we're benefitting from the safety of Rust when Rust is just providing a facade and not the implementation. Note: I know russh exists, this is a general point, not specific to ssh. Do you use the library written in Rust, or the FFI wrapper around the well maintained C library. </p>","contentLength":1684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anyone figured out a clean way to handle etcd snapshot restore with multi-control-plane Cluster-API clusters?","url":"https://www.reddit.com/r/kubernetes/comments/1ojazud/anyone_figured_out_a_clean_way_to_handle_etcd/","date":1761759269,"author":"/u/TheFlyingDutchMan_-","guid":320856,"unread":true,"content":"<p>I’m trying to handle an etcd snapshot restore for a cluster managed by Cluster-API (using KubeadmControlPlane with stacked etcd). Right now, I’m restoring the snapshot through preKubeadmCommands, just before kubeadm init.</p><p>The tricky part: Since every control-plane machine executes the same bootstrap logic, each node ends up trying to restore the snapshot, which basically spawns 3 independent single-node etcd clusters. That breaks quorum and consistency completely.</p><p>Ideally, only the first control-plane (the one doing kubeadm init) should perform the restore, and the rest should just join normally via kubeadm join --control-plane.</p><p>I’m looking for a simple, declarative, GitOps-friendly way to achieve that (since i am doing it using flux):</p><p>Without manually scaling replicas or editing templates mid-deployment.</p><p>Maybe some trick to detect if the node is the init one ,???</p><p>Has anyone implemented this cleanly? Would love to hear how you approached this</p>","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bazzite Fall Update: Fedora 43, Xbox Allies, Legion Go 2, Nvidia GTX - Bazzite","url":"https://universal-blue.discourse.group/t/bazzite-fall-update-fedora-43-xbox-allies-legion-go-2-nvidia-gtx/10948","date":1761758158,"author":"/u/giannidunk","guid":320881,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ojahew/bazzite_fall_update_fedora_43_xbox_allies_legion/"},{"title":"Let Us Open URL's in a Specific Browser Profile","url":"https://kevin.burke.dev/kevin/open-urls-in-specific-browser-profile/?reddit","date":1761758106,"author":"/u/ekrubnivek","guid":322092,"unread":true,"content":"<p>Most browsers have the ability to <a href=\"https://www.chromium.org/developers/creating-and-using-profiles/\">launch different browser profiles</a>.\nEach profile can come with a different theme and a different set of website\nlogins (cookies, application state, etc). This can be helpful if you want to\nsegregate browsing behavior. For example, I have different browser profiles set\nup for my personal email, my \"consulting\" email, the nonprofit I volunteer with.</p><img decoding=\"async\" src=\"https://kevin.burke.dev/rawblog/images/browser-tabs/profile-selector.png\" alt=\"Browser dialog that lets you select a profile\"><p>Along with this, command line tools frequently open URL's for various purposes.\nDocusaurus launches http://localhost:3000 so you can view documentation content\nin your browser. Lots of different tools open browser profiles to complete\nauthentication workflows - for example, Tailscale starts a web server on your\nlocal computer, asks you to log in via the browser, and then redirects to the\nlocal web server with a valid token.</p><p>When you have multiple profiles, it's frustrating when a URL opens in the\n\"wrong\" profile, because this means it's frequently opening in a window where\nyou're not logged in. When this happens, web services typically redirect you\nto a login page, and remove the information about the URL you were trying to\nvisit (or store it in a cookie). This means if you try and copy and paste the\nURL to the \"right\" profile, you frequently lose track of the initial URL you\nwanted to visit! Very annoying.</p><p>Fortunately it turns out <strong>there is a solution to this!</strong> The way most command\nline tools open URL's on Macs is by simply invoking  (),\nfollowed by the URL.</p><pre><code></code></pre><p>But you can pass additional arguments to the application with , and\nChrome/Chromium support arguments that let you select the right profile.</p><pre><code></code></pre><p>That will always open URL's in \"Profile 4\", whatever you have that configured\nto.</p><p>How do you find which profile is which? The simplest way is to open\n in your URL, and then look at the last bit of \"Profile Path\"\nin the page that opens there - it should be 'Default' or 'Profile N'.</p><img decoding=\"async\" src=\"https://kevin.burke.dev/rawblog/images/browser-tabs/chrome-find-profile.png\" alt=\"Screenshot of the chrome version page, with the profile directory highlighted\"><p>On Firefox, you provide the name of the profile from , e.g.:</p><pre><code></code></pre><p>If you operate a command line tool (or an application that has access to\n!), <strong>particularly one that opens URL's for authentication,</strong>\nplease consider implementing support for opening URL's in specific browser\nprofiles. Example configuration options might be:</p><pre><code></code></pre><p>And then your CLI tool would implement logic similar to this, in the language of\nyour choice, instead of just shelling out to .</p><pre><code></code></pre><p>Please consider implementing this change in your CLI tool! It would really help\nimprove usability for CLI based tools. With sufficient interest, maybe we could\ncreate an RFC for both the arguments browsers should accept, and for environment\nvariables that can be read by CLI's for opening URL's in a given browser.</p>","contentLength":2629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ojagkh/let_us_open_urls_in_a_specific_browser_profile/"},{"title":"Golang seems so simple, am i wrong to assume that?","url":"https://www.reddit.com/r/golang/comments/1oj9jb6/golang_seems_so_simple_am_i_wrong_to_assume_that/","date":1761756072,"author":"/u/No-Plan-2816","guid":320822,"unread":true,"content":"<p>I’ve been using Go for the last couple of months, it feels super simple. Are there any crazy complexities in the language that i’m not aware of because i’m a noob at it?</p>","contentLength":175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oblivious HTTP (OHTTP, RFC 9458) privacy-preserving request routing in Go","url":"https://www.reddit.com/r/golang/comments/1oj8brn/oblivious_http_ohttp_rfc_9458_privacypreserving/","date":1761753399,"author":"/u/CONFSEC","guid":320823,"unread":true,"content":"<p>I’m Jonathan, founder of Confident Security - you might’ve seen some posts from our collaborators Willem and Vadim. We’re open-sourcing OHTTP, a Go library that implements Oblivious HTTP (RFC 9458) with client and gateway components.</p><p> We built this library to make it easy to send and receive HTTP requests in a privacy-preserving way. OHTTP separates the client’s identity from the request content, while integrating naturally with Go’s *http.Request and *http.Response types.</p><p> - implemented as http.RoundTripper - supports chunked transfer encoding - customizable HPKE (e.g., for custom hardware-based encryption) - built on top of twoway and bhttp libraries</p><p>The README has quick start guides, API references, and examples. Feedback, suggestions, and contributions are very welcome!</p>","contentLength":791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This mom’s son was asking Tesla’s Grok AI chatbot about soccer. It told him to send nude pics, she says. xAI, the company that developed Grok, responds to CBC: 'Legacy Media Lies'","url":"https://www.cbc.ca/news/investigates/tesla-grok-mom-9.6956930","date":1761753327,"author":"/u/esporx","guid":322117,"unread":true,"content":"<p>A Toronto mom says things took an unpredictable turn when her 12-year-old son asked Tesla’s AI chatbot Grok which professional soccer player it prefers: Cristiano Ronaldo or Lionel Messi.</p><p>“My son was very excited to hear that the chatbot thought Ronaldo was the better soccer player,” said Farah Nasser, a former journalist and broadcaster.&nbsp;</p><p>Nasser was driving her son and 10-year-old daughter, along with her friend, home from school on Oct. 17 when the <a href=\"https://www.instagram.com/p/DP7cHrOD3ha/\" target=\"_blank\"></a> took place.&nbsp;</p><p>She said there was some Messi trash talking by the chatbot and when her son joked that Ronaldo had scored, the conversation went to an unexpected place.</p><p>\"The chatbot said to my son, 'Why don't you send me some nudes?'\" said Nasser.</p><p>\"I was at a loss for words. Why is a chatbot asking my children to send naked pictures in our family car? It just didn't make sense.\"</p><div><em><strong>WATCH | This mom says Tesla's chatbot asked her son to send nudes:</strong></em></div><p>Nasser said had she known what the chatbot was capable of, she would have avoided using it around her children. She's now warning other parents.</p><p>\"Hindsight is 20/20. I would not let my child use this thing.\"</p><p>CBC News did not independently verify the conversation Nasser says she witnessed in her car.</p><h2>Grok recently installed in Canadian vehicles</h2><p>Nasser and her family have owned a Tesla Model 3 electric sedan since 2022, but Grok — the generative AI chatbot created by Tesla CEO Elon Musk's xAI — is a new feature that was automatically installed in <a href=\"https://www.tesla.com/support/grok#grok-vehicles\" target=\"_blank\"></a> in the United States <a href=\"https://x.com/elonmusk/status/1943251229511160001\" target=\"_blank\"></a> and in Canadian vehicles in October.</p><p>Grok is already integrated with the social media platform X (formerly known as Twitter), a subsidiary of xAI.</p><p>Grok has several personalities to choose from in its default setting. There’s Ara, an upbeat female; Rex, a calm male; Eve, a soothing female; Sal, a smooth male and Gork, a lazy male. Nasser’s son chose Gork.</p><p>\"'Lazy male’ doesn't describe Gork,\" said Nasser. \"R-rated, spicy — anything else would have made sure that my child would not press that button.\"</p><p>Nasser says a separate \"not safe for work\" (NSFW) setting wasn't enabled but admits she did not activate the \"kids mode\" function either. Still, she says she's shocked the chatbot's default setting allows this type of content.</p><p>\"It just was a conversation about soccer, and then asking for nudes.\"</p><p>Tesla did not respond to CBC's questions about Nasser's experience. However, xAI provided what appeared to be an automated reply, stating, \"Legacy media lies.\"</p><p>Canada has a minister of artificial intelligence, who does not regulate specific in-vehicle software. The minister's <a href=\"https://embed.documentcloud.org/documents/26204920/pages/1/?embed=1\" target=\"_blank\"><u>said in an emailed statement</u></a> it wasn't aware of Tesla’s plan to integrate Grok into vehicles sold in Canada but that it takes complaints seriously.</p><p><a href=\"https://x.ai/legal/faq#:~:text=Grok%20is%20not,Terms%20of%20Service.\" target=\"_blank\"></a> policy, Grok is \"not directed\" to children under 13 while teens between 13 and 17 must have their parent or legal guardian's permission to use it, and they must agree to the company's terms of service.</p><div><em><strong>WATCH | A conversation with Grok:</strong></em></div><p>\"As parents, we don't read the terms and conditions of every single thing. I don't think it's realistic to expect that everybody does that,\" said Nasser.</p><p>\"I would think that there would be a warning or something that would pop up that would say, you know, 'Are you 13-plus?'\"</p><p>\"xAI's Grok was created based on a philosophy of sort of absolute, radical openness, and it will talk about anything with anyone,\" said Mark Daley, chief AI officer at Western University in London, Ont.</p><p>“[Musk is] a free speech extremist. He wants Grok to be completely open, to have any conversation with anyone. And that's a principled stance that he's taken, but it may not be what every consumer is looking for.”</p><p>Daley says it's great for parents to encourage their children to interact with technology, but they need to supervise their activity.</p><p>\"It's also important, like any social media use, any computer use, that you monitor what's happening and that you have open and frank conversations about the ways the technology can go wrong.\"</p><h2>Objectionable, inappropriate and offensive</h2><p>xAI apologized and said it had been fixed, but some experts question whether the protections built into the technology are sufficient.</p><p>\"Some companies have very strict guardrails because you don't know who is on the other side of the keyboard,\" said Daley. \"You don't know who's interacting with that, what their social context is. It could be a child, it could be someone experiencing a mental health crisis.\"</p><p>While there is likely a demographic that is in favour of absolute free speech, Daley says, the average user tends to want some guardrails.</p><p>Countless videos online show just how unpredictable Grok can get in the NSFW setting — particularly in the \"unhinged\" mode.&nbsp;</p><p>xAI <a href=\"https://x.ai/legal/faq#:~:text=you%20could%20instruct%20Grok%20to%20be%20%E2%80%9Cunhinged%E2%80%9D%20which%20may%20result%20in%20Grok%20responding%20like%20an%20amateur%20stand%2Dup%20comic%20who%20is%20still%20learning%20the%20craft%20%E2%80%93%20sometimes%20being%20objectionable%2C%20inappropriate%2C%20and%20offensive.\" target=\"_blank\"></a> that instructing the chatbot to be unhinged \"may result in Grok responding like an amateur stand-up comic who is still learning the craft — sometimes being objectionable, inappropriate, and offensive.\"&nbsp;</p><p>Videos posted to social media by Tesla drivers using the \"unhinged mode\" show the chatbot using words such as <a href=\"https://www.tiktok.com/@plaidwannabe/video/7551956519986679053?_r=1&amp;_t=ZS-90p1AMW72rY\" target=\"_blank\"></a> and <a href=\"https://www.instagram.com/p/DP_sb7Ajr9N/\" target=\"_blank\"></a>.</p><p>Nasser says she’s hopeful about the potential of AI, but it needs proper protections.</p><p>\"I love AI. I use it for all kinds of things,\" she said.</p><p>\"But I think we have to think about what we learned with technologies like cellphones, with technologies like social media … and see the lessons that we learned and really apply them to this new wave, this new AI revolution.\"</p>","contentLength":5370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oj8am8/this_moms_son_was_asking_teslas_grok_ai_chatbot/"},{"title":"Copyparty: Portable file server with accelerated resumable uploads, dedup, WebDAV, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file, no deps","url":"https://github.com/9001/copyparty","date":1761752295,"author":"/u/FryBoyter","guid":320820,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1oj7u6j/copyparty_portable_file_server_with_accelerated/"},{"title":"Kafka is fast -- I'll use Postgres","url":"https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks","date":1761752046,"author":"/u/arshidwahga","guid":320972,"unread":true,"content":"<p>I feel like the tech world lives in two camps.</p><ol><li>One camp chases buzzwords.</li></ol><p>This camp tends to adopt whatever’s popular without thinking hard about whether it’s appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.</p><p>You see this everywhere in the Kafka world: Streaming Lakehouse™️, Kappa™️ Architecture, Streaming AI Agents.</p><p>This phenomenon is sometimes known as . Modern practices actively encourage this. Consultants push “innovative architectures” stuffed with vendor tech via “insight” reports. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you’re interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack™️, not for being resourceful.</p><ol start=\"2\"><li>The other camp chases common sense</li></ol><p>This camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.</p><p>Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:</p><p>Trend 1 - the “<a href=\"https://topicpartition.io/definitions/small-data\" data-slug=\"definitions/small-data\">Small Data</a>” movement. People are realizing two things - their data isn’t that big and their computers are becoming big too. You can rent a <a href=\"https://instances.vantage.sh/aws/ec2/x1e.xlarge\">128-core, 4 TB of RAM instance</a> from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.</p><p>Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment. In the last 2 years, the phrase <a href=\"https://github.com/Olshansk/postgres_for_everything\">“Just Use Postgres (for everything)”</a> has gained a ton of popularity. The basic premise is that you shouldn’t complicate things with new tech when you don’t need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:</p><ul><li>Elasticsearch (functionality supported by Postgres’ /)</li><li>Redis ()</li><li>AI Vector Databases (, )</li><li>Snowflake (, )</li></ul><p>The claim isn’t that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)</p><p>When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today’s powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization’s scale.</p><p><strong>A 500 KB/s workload should not use Kafka.</strong> There is a scalability cargo cult in tech that always wants to choose “the best possible” tech for a problem - but this misses the forest for the trees. The “best possible” solution frequently isn’t a technical question - it’s a practical one. Adriano makes an airtight case for why you should opt for  in his <a href=\"https://adriano.fyi/posts/2023-09-24-choose-postgres-queue-technology\">PG as Queue blog</a> (2023) that originally inspired me to write this.</p><p>Enough background. In this article, we will do three simple things:</p><p>I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.</p><p><em>(while this article is for Postgres, feel free to replace it with your database of choice)</em></p><p>If you’d like to skip straight to the results, here they are:</p><p>There are dozens of blogs out there using Postgres as a , but interestingly enough I haven’t seen one use it as a pub-sub messaging system.</p><p>A quick distinction between the two because I often see them get confused:</p><ol><li><p> are meant for point-to-point communication. They’re widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it’s done with. A message is immediately deleted (popped) off the queue once it’s consumed. Queues do not have strict ordering guarantees.</p></li><li><p> messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.</p><p>There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.</p></li></ol><p>Postgres’ main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.</p><p>Kafka uses the Log data structure to hold messages. You’ll see my benchmark basically reconstructs a log from Postgres primitives.</p><p>Postgres doesn’t seem to have any popular libraries for pub-sub use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:</p><ol><li>Writers produce batches of messages per statement (). Each transaction carries one batch insert and targets a single  table</li><li>Each writer is sticky to one table, but in aggregate they produce to multiple tables.</li><li>Each message has a unique monotonically-increasing offset number. A specific row in a special  table denotes the latest offset for a given  table.</li><li>Write transactions atomically update both the  data and the  row. This ensures consistent offset tracking across concurrent writers.</li><li>Readers poll for new messages. They consume the  table(s) sequentially, starting from the lowest offset and progressively reading up.</li><li>Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the  tables.</li><li>Each group contains 1 reader per  table.</li><li>Readers store their progress in a  table, with a row for each  pair.</li><li>Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.</li></ol><p>This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sql\" data-theme=\"github-light github-dark\"><code data-language=\"sql\" data-theme=\"github-light github-dark\"></code></pre></figure><p>The benchmark runs  writer goroutines. These represent writer clients.\nEach one loops and atomically inserts  records while updating the latest offset:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sql\" data-theme=\"github-light github-dark\"><code data-language=\"sql\" data-theme=\"github-light github-dark\"></code></pre></figure><p>The benchmark also runs  reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.</p><p>The reader loops, opens a transaction, optimistically claims  records (by advancing the offset mark beyond them), selects them and processes the records.\nIf successful, it commits the transaction and through that advances the offset for the group.</p><p>It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.</p><p>First it opens a transaction:</p><p>Then it claims the offsets:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sql\" data-theme=\"github-light github-dark\"><code data-language=\"sql\" data-theme=\"github-light github-dark\"></code></pre></figure><p>Followed by selecting the claimed records:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sql\" data-theme=\"github-light github-dark\"><code data-language=\"sql\" data-theme=\"github-light github-dark\"></code></pre></figure><p>Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:</p><p>If you’re wondering  - my understanding of that feature is that it’s an optimization and cannot be fully relied upon, so polling is required either way. Given that, I just copied Kafka’s relatively simple design.</p><p>The full code and detailed results are all published on GitHub at <a href=\"https://github.com/stanislavkozlovski/pg-queue-pubsub-benchmark\">stanislavkozlovski/pg-queue-pubsub-benchmark</a>.\nI ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:</p><ul><li><a href=\"https://instances.vantage.sh/aws/ec2/c7i.xlarge\">c7i.xlarge</a> Postgres server /w 25GB gp3 9000 IOPS EBS volume</li><li>mostly default Postgres settings (synchronous commit, fsync);\n<ul><li><code>autovacuum_analyze_scale_factor = 0.05</code> set on the partition tables too (unclear if it has an effect)</li></ul></li><li>each row’s payload is 1 KiB (1024 bytes)</li><li>10 writers (2 writers per partition on average)</li><li>5x read fanout via 5 consumer groups</li><li>20 reader clients total (4 readers per group)</li><li>write batch size: 100 records</li><li>read batch size: 200 records</li></ul><ul><li><p>write message rate: </p></li><li><p>write throughput: </p></li><li><p>write latency: 38.7ms p99 / 6.2ms p95</p></li><li><p>read message rate: </p></li><li><p>read message throughput: </p></li><li><p>read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95</p></li><li><p>end-to-end latency:  / 10.6ms p95</p></li><li><p>disk was at ~1200 writes/s with iostat claiming 46 MiB/s</p></li></ul><p>These are pretty good results. It’s funny to think that the majority of people run a complex distributed system like Kafka for similar workloads.</p><p>Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.</p><ul><li>3x <a href=\"https://instances.vantage.sh/aws/ec2/c7i.xlarge\">c7i.xlarge</a> Postgres servers /w 25GB gp3 9000 IOPS EBS volume\n<ul><li>each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)</li><li>one  replica and one  replica</li></ul></li><li>a few custom Postgres settings like , , , <code>max_parallel_workers_per_gather</code> and of course - <ul><li><code>autovacuum_analyze_scale_factor = 0.05</code> set on the partition tables too (unclear if it has an effect)</li></ul></li><li>each row’s payload is 1 KiB (1024 bytes)</li><li>10 writers (2 writers per partition on average)</li><li>5x read fanout via 5 consumer groups</li><li>readers only access the primary DB; readers are in the same AZ as the primary;</li><li>20 reader clients total (4 readers per group)</li><li>write batch size: 100 records</li><li>read batch size: 200 records</li></ul><ul><li><p>write message rate: </p></li><li><p>write throughput: </p></li><li><p>write latency: 153.45ms p99 / 6.8ms p95</p></li><li><p>read message rate: </p></li><li><p>read message throughput: </p></li><li><p>read latency: 57ms p99; 4.91ms p95</p></li><li><p>end-to-end latency:  / 12ms p95</p></li><li><p>disk was at ~1200 writes/s with iostat claiming 46 MiB/s</p></li></ul><p>Now these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x’d (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.</p><p>This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.</p><p>Typically, you’d expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn’t designed to be efficient for this use case.\nNot here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. 🤯</p><p>By the way, in Kafka it’s customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.</p><p>Ok, let’s see how far Postgres will go.</p><ul><li><a href=\"https://instances.vantage.sh/aws/ec2/c7i.24xlarge\">c7i.24xlarge</a> (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volume</li><li>modified Postgres settings ( on, other settings scaled to match the machine);\n<ul><li>still kept fsync &amp; synchronous_commit on for durability.</li><li><code>autovacuum_analyze_scale_factor = 0.05</code> set on the partition tables too (unclear if it has an effect)</li></ul></li><li>each row’s payload is 1 KiB (1024 bytes)</li><li>100 writers (~3.33 writers per partition on average)</li><li>5x read fanout via 5 consumer groups</li><li>150 reader clients total (5 readers per group)</li><li>write batch size: 200 records</li><li>read batch size: 200 records</li></ul><ul><li><p>write message rate: </p></li><li><p>write throughput: </p></li><li><p>write latency: 138ms p99 / 47ms p95</p></li><li><p>read message rate: </p></li><li><p>read message throughput: </p></li><li><p>end-to-end latency:  / 242ms p95 / 23.4ms p50</p></li><li><p>server kept at  CPU (basically idle);</p></li><li><p>bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn’t able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn’t push further, but I do wonder now as I write this - how far would writes have scaled?</p><ul><li>Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn’t include it because I didn’t feel the need to push to an unrealistic read-fanout extreme.</li></ul></li></ul><p>240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it’s worth, I do think it’s worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like <a href=\"https://blog.2minutestreaming.com/p/diskless-kafka-topics-kip-1150\">Diskless Kafka</a>.</p><p>These tests seem to show that Postgres is pretty competitive with Kafka at low scale.</p><p>You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn’t apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time.</p><p>In any case, no benchmark is perfect. My goal wasn’t to indisputably prove . Rather, I want to start a discussion by showing that what’s possible is likely larger than what most people assume. I certainly didn’t assume I’d get such good numbers, especially with the pub-sub part.</p><p>In Postgres, a queue can be implemented with <code>SELECT FOR UPDATE SKIP LOCKED</code>. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That’s how mutual exclusion is achieved - a worker can’t get other workers’ jobs.</p><p>Postgres has a very popular <a href=\"https://github.com/pgmq/pgmq\">pgmq</a> library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:</p><ol><li>lock row &amp; take job (<code>SELECT FOR UPDATE SKIP LOCKED</code>)</li><li>process job ()</li><li>mark job as “done” ( a field or  the row into a separate table)</li></ol><p>Postgres competes with RabbitMQ, AWS SQS, NATS, Redis and to an extent Kafka here.</p><p>We use a simple  table. When an element is processed off the queue, it’s moved into the archive table.</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sql\" data-theme=\"github-light github-dark\"><code data-language=\"sql\" data-theme=\"github-light github-dark\"></code></pre></figure><p>We again run  writer client goroutines.\nEach one simply loops and sequentially inserts a single random item into the table:</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sql\" data-theme=\"github-light github-dark\"><code data-language=\"sql\" data-theme=\"github-light github-dark\"></code></pre></figure><p>It only inserts one message per statement, which is pretty inefficient at scale.</p><p>We again run  reader client goroutines. Each reader loops and processes one message.\nThe processing is done inside a database transaction.</p><figure data-rehype-pretty-code-figure=\"\"><pre tabindex=\"0\" data-language=\"sql\" data-theme=\"github-light github-dark\"><code data-language=\"sql\" data-theme=\"github-light github-dark\"></code></pre></figure><p>Each reader again only works with one message at a time per transaction.</p><p>I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:</p><p><small><em>The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly.</em></small></p><ul><li><a href=\"https://instances.vantage.sh/aws/ec2/c7i.xlarge\">c7i.xlarge</a> Postgres server /w 25GB gp3 9000 IOPS EBS volume</li><li>all default Postgres settings</li><li>each row’s payload is 1 KiB (1024 bytes)</li><li>10 writer clients, 15 reader clients</li></ul><ul><li>write latency: 2.46ms p99</li><li>end-to-end latency: 17.72ms p99</li></ul><p>What I found Postgres wasn’t good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.</p><p>Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server’s CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.</p><p>Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn’t throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn’t debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.</p><ul><li>3x <a href=\"https://instances.vantage.sh/aws/ec2/c7i.xlarge\">c7i.xlarge</a> Postgres servers /w 25GB gp3 9000 IOPS EBS volume\n<ul><li>each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)</li><li>one  replica and one  replica</li></ul></li><li>a few custom Postgres settings like , , , <code>max_parallel_workers_per_gather</code> and of course - </li><li>each row’s payload is 1 KiB (1024 bytes)</li><li>10 writer clients, 15 reader clients</li><li>readers only access the primary DB; readers are in the same AZ as the primary;</li></ul><ul><li>end-to-end latency: 920ms p99 ⚠️; 536ms p95; 7ms p50</li></ul><p>As expected, throughput and latency were impacted somewhat. But not that much. It’s still over 2000 messages a second, which is pretty good for an HA queue!</p><ul><li><a href=\"https://instances.vantage.sh/aws/ec2/c7i.24xlarge\">c7i.24xlarge</a> Postgres server instance /w 250GB io2 12,000 IOPS EBS volume</li><li>modified Postgres settings ( on, other settings scaled to match the machine);\n<ul><li>still kept fsync &amp; synchronous_commit on for durability.</li></ul></li><li>each row’s payload is 1 KiB (1024 bytes)</li><li>100 writer clients, 200 reader clients</li></ul><ul><li>message rate: </li><li>write latency: 9.42ms p99</li><li>end-to-end latency: 930ms p99 ⚠️; 709ms p95; 12.6ms p50</li></ul><p>This run wasn’t that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn’t bother figuring out. I figured that it wasn’t important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.</p><p>Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue.\nAs I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The <a href=\"https://github.com/pgmq/pgmq\">library</a>’s star history captures this trend perfectly:\n<img src=\"https://topicpartition.io/blog/images/pgmq_star_history.png\" alt=\"pgmq\"></p><p>Most of the time - . You should always default to Postgres until the constraints prove you wrong.</p><p>Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that <strong>picking a technology based on technical optimization alone is a flawed approach</strong>. To throw an analogy:</p><blockquote><p>a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.</p></blockquote><p>The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:</p><ul><li>ability to debug messages with regular SQL</li><li>ability to delete, re-order or edit messages in place</li><li>ability to join pub-sub data with regular tables</li><li>ability to trivially read specific data via rich SQL queries (, , )</li></ul><p>Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).</p><p>Donald Knuth warned us in 1974 -  is the root of all evil. Deploying Kafka at small scale is premature optimization.\nThe point of this article is to show you that this “small scale” number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.</p><p>We are in a Postgres Renaissance for a reason: Postgres is  good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.</p><h2>Custom Solutions for Everything?<a role=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" data-no-popover=\"true\" href=\"https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#custom-solutions-for-everything\"></a></h2><p>Naive engineers tend to adopt a specialized technology at the slightest hint of a need:</p><ul><li> Redis, of course!</li><li> Let’s deploy Elasticsearch!</li><li> BigQuery or Snowflake - that’s what our data analysts used at their last job.</li><li> We need a NoSQL database like MongoDB.</li><li><em>Have to crunch some numbers on S3?</em> Let’s use Spark!</li></ul><p>A good engineer thinks through the bigger picture.</p><ul><li><em>Does this new technology move the needle?</em></li><li><em>Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?</em></li></ul><p>At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.</p><p>The problem is <strong>the organizational overhead</strong>. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.</p><p>All of these are real organizational costs that can take months to get right, even if the system in question isn’t difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don’t remove it all. And until you reach the scale where the technology is necessary, you pay these extra <em>{financial, organizational}</em> costs for zero significant gain.</p><p>If the same can be done with tech for which you’ve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don’t need web-scale technologies when you don’t have web-scale problems.</p><h2>MVI (a better alternative)<a role=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" data-no-popover=\"true\" href=\"https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#mvi-a-better-alternative\"></a></h2><p>What I think is a better approach is to search for the <strong>minimum viable infrastructure</strong> (MVI): build the smallest amount of system while still providing value.</p><ol><li>choose  technology your org is already  with\n<ul><li> == meets your users’ needs without being too slow/expensive/insecure</li><li> == your org has prior experience, has runbooks/ops setups, monitoring, UI, etc</li></ul></li><li>solve a real problem with it</li><li>use the minimum set of features\n<ul><li>the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)</li></ul></li></ol><p>Bonus points if that technology:</p><ul><li>is widely adopted so finding good engineers for it is trivial (Postgres - check)</li><li>has a strong and growing network effect (Postgres - check)</li></ul><p>The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.</p><p>Unfortunately, it’s human nature to go against this. Just like startups suffer due to <a href=\"https://en.wikipedia.org/wiki/Minimum_viable_product\">MVP</a> bloat , infra teams suffer due to MVI bloat </p><p>I won’t pretend to be able to map out the exact path-dependent outcome, but my guess is this:</p><ol start=\"0\"><li>the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fast</li><li>a lot of viral internet companies were growing at speeds that led old infra to become obsolete fast</li><li>this prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselves</li><li>each well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don’t need to (<a href=\"https://topicpartition.io/blog/everyone-is-talking-their-book\" data-slug=\"blog/everyone-is-talking-their-book\">Everyone is Talking Their Book</a>). They had deep pockets for marketing and used them.</li><li>innovative infrastructure software got engineered. It was exciting - so engineers got <a href=\"https://xkcd.com/356/\">nerd-sniped</a> into it</li><li>a <a href=\"https://www.youtube.com/watch?v=b2F-DItXtZs\">web-scale</a> craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.</li><li>a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)</li><li>the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes\n<ul><li>system design interview questions were adapted to test for knowledge of these systems</li><li>within an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;</li></ul></li></ol><p>This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it’s simply more fun.</p><p>This is why I think we, as an industry, don’t always use the simplest solution available.</p><p>In most cases, Postgres is that simplest solution that is available.</p><p>I want to wrap this article up, but one rebuttal I can’t miss addressing is the “it won’t scale argument”.</p><p>The argument goes something like this: “in today’s age we can go viral at a moment’s notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes”</p><p>I have three arguments against this:</p><p><a href=\"https://bohanzhang.me/\">Bohan Zhang</a>, a member of OpenAI’s infrastructure team and co-founder of <a href=\"https://ottertune.com/\">OtterTune</a> (a Postgres tuning service), can be quoted as saying:</p><blockquote><p><em>“At OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.”</em></p><p><em>“The main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers  of apps.”</em></p><p><em>“Postgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It’s a good problem to have.”</em></p><p>(slightly edited for clarity and grammar)</p></blockquote><p>Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven’t… why does your unproven project need to?</p><h3>2. You Have More Time To Scale Than You Think<a role=\"anchor\" aria-hidden=\"true\" tabindex=\"-1\" data-no-popover=\"true\" href=\"https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#2-you-have-more-time-to-scale-than-you-think\"></a></h3><p>Let’s say it’s a good principle to design/test for ~10x your scale. Here are the years of  growth rate it takes to get to 10x your current scale:</p><div><table><thead><tr></tr></thead><tbody></tbody></table></div><p>It goes to show that even at extreme growth levels, you still have years to migrate between solutions.\nThe majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).</p><p>In an ideal world, you  build for scale and any other future problem you may hit in 10 years.</p><p>In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.</p><blockquote><p>Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.</p></blockquote><p>Just use Postgres until it breaks.</p><ul><li><p><em>I’m a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I’m happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI’s promise (in the short-term), but there’s no denying it has made a large dent in democratizing niche/low-level knowledge.</em></p></li></ul>","contentLength":27082,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oj7q6q/kafka_is_fast_ill_use_postgres/"},{"title":"gopkgview v1.2.0 - Interactive visualization of a Go dependency graph","url":"https://github.com/grishy/gopkgview","date":1761751919,"author":"/u/the_grishy","guid":320773,"unread":true,"content":"<p> is an interactive tool designed to visualize and analyze Go project dependencies. It provides a rich, web-based interface for better understanding of how your project connects its components and external libraries.</p><p>In 1.2.0 was added support of Go 1.25.</p>","contentLength":253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oj7o6a/gopkgview_v120_interactive_visualization_of_a_go/"},{"title":"Grieving family uses AI chatbot to cut hospital bill from $195,000 to $33,000 — family says Claude highlighted duplicative charges, improper coding, and other violations","url":"https://www.tomshardware.com/tech-industry/artificial-intelligence/grieving-family-uses-ai-chatbot-to-cut-hospital-bill-from-usd195-000-to-usd33-000-family-says-claude-highlighted-duplicative-charges-improper-coding-and-other-violations","date":1761750951,"author":"/u/ControlCAD","guid":320858,"unread":true,"content":"<p>An individual whose brother-in-law recently passed has explained how they managed to slash the hospital medical bills left behind from hundreds to tens of thousands. <a data-analytics-id=\"inline-link\" href=\"https://www.threads.com/@nthmonkey/post/DQVdAD1gHhw\" target=\"_blank\" data-url=\"https://www.threads.com/@nthmonkey/post/DQVdAD1gHhw\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">Nthmonkey</a> on Threads claims that they disputed the hospital’s original bill of $195,000 for treatment of their relative’s final four hours of intensive care after a heart attack. According to them, AI chatbot advice was instrumental in analytically, calmly, and coolly reducing the bill to a far more reasonable $33,000. We have not independently verified the poster's story, so view it with the appropriate level of skepticism.</p><a href=\"https://www.tomshardware.com/tech-industry/artificial-intelligence/grieving-family-uses-ai-chatbot-to-cut-hospital-bill-from-usd195-000-to-usd33-000-family-says-claude-highlighted-duplicative-charges-improper-coding-and-other-violations\" data-url=\"\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><p>Coping with the death of a loved one is a terribly difficult experience. With all the emotions washing over you, it doesn’t feel like a time to raise a dispute over medical bills, to ‘penny pinch.’ While signing a check for thousands (nearly $200,000 in this case) might help you put such a terrible life event in the rearview mirror, it isn’t right to reward those who would cheat you and/or others from a life’s inheritance.</p><a aria-hidden=\"true\" href=\"https://www.tomshardware.com/tech-industry/artificial-intelligence/grieving-family-uses-ai-chatbot-to-cut-hospital-bill-from-usd195-000-to-usd33-000-family-says-claude-highlighted-duplicative-charges-improper-coding-and-other-violations\" data-url=\"\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><p>Nthmonkey explained the final bill was sky-high largely because their relative’s medical insurance had lapsed two months prior to the fateful day. But the scale of the charges was extraordinary, and the bill they received was incredibly opaque.</p><p>Claude AI might be characterized as the hero in this particular case. But the mourning individual had to first go into some to-and-fro with the hospital administrators – to lift the veil and break down what exactly ‘Cardiology’ at ‘$70,000’ represented, for example.</p><p>Once a satisfactory level of transparency was achieved (the hospital blamed ‘upgraded computers’), Claude AI stepped in and analyzed the standard charging codes that had been revealed.</p><p>Claude proved to be a dogged, forensic ally. The biggest catch was that it uncovered duplications in billing. It turns out that the hospital had billed for both a master procedure and all its components. That shaved off, in principle, around $100,000 in charges that would have been rejected by Medicare. “So the hospital had billed us for the master procedure and then again for every component of it,” wrote an exasperated nthmonkey.</p><p>Furthermore, Claude unpicked the hospital’s improper use of inpatient vs emergency codes. Another big catch was an issue where ventilator services are billed on the same day as an emergency admission, a practice that would be considered a regulatory violation in some circumstances.</p><a aria-hidden=\"true\" href=\"https://www.tomshardware.com/tech-industry/artificial-intelligence/grieving-family-uses-ai-chatbot-to-cut-hospital-bill-from-usd195-000-to-usd33-000-family-says-claude-highlighted-duplicative-charges-improper-coding-and-other-violations\" data-url=\"\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><h2>Hospital thought 'it could just grab money from unsophisticated people'</h2><p>“Long story short, the hospital made up its own rules, its own prices, and figured it could just grab money from unsophisticated people,” asserts medical bill dispute winner nthmonkey. Their win came thanks to Claude AI’s analysis, as we discussed above, and the chatbot’s help in drafting correspondence. After its great work on the figures, the chatbot helped create letters that held aloft the sword of legal action, bad PR, and appearances before legislative committees.</p><p>Ultimately, the dispute about the billing whittled it down to $33,000 (from $195,000, remember), but that didn't occur before the hospital had stooped even lower by trying to get the bereaved parties to appeal to charity to help with their huge bill...</p><p>Nthmonkey is satisfied with the outcome of this dispute. But seemed even more satisfied with the performance of their $20 per month Claude subscription (other AIs are available). “I had access to tools that helped me land on that number, but the moral issue is clear. Nobody should pay more out of pocket than Medicare would pay. No one,” they concluded in their Threads thread. “Let’s not let them get away with this anymore.”</p><a href=\"https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB\" data-url=\"https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"><figure data-bordeaux-image-check=\"\"></figure></a>","contentLength":3709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oj78v6/grieving_family_uses_ai_chatbot_to_cut_hospital/"},{"title":"I left my laptop for FOUR MONTHS","url":"https://www.reddit.com/r/linux/comments/1oj75qf/i_left_my_laptop_for_four_months/","date":1761750741,"author":"/u/HistorianBusy2262","guid":320819,"unread":true,"content":"<p>I just powered it on and it had 1029 updates. </p>","contentLength":46,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Question out of curiosity","url":"https://www.reddit.com/r/kubernetes/comments/1oj6w9g/question_out_of_curiosity/","date":1761750126,"author":"/u/ComprehensiveLow6596","guid":320744,"unread":true,"content":"<p>If there was an IDE where we could do our entire DevOps or platform engineering job </p><p>from writing Terraform or Kubernetes manifests, to running CI/CD pipelines, checking cost impact, and reviewing infrastructure changes </p><p>what would you want it to have personally?? </p><p>As a junior engineer, I’m curious to learn from those who have more experience than me:</p><p>What do you think is missing in today’s tools? What do you wish existed in an IDE made specifically for infrastructure and cloud work not just app development?</p><p>Most of us still jump between terminals, YAML files, dashboards, and GitHub PRs every day(at least I do)</p><p>It makes me wonder what would an all-in-one environment for infrastructure actually look like if it respected how DevOps teams really work?</p>","contentLength":756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rant: dealing with http::Uri is annoying as heck","url":"https://www.reddit.com/r/rust/comments/1oj5bd7/rant_dealing_with_httpuri_is_annoying_as_heck/","date":1761746425,"author":"/u/DebuggingPanda","guid":320857,"unread":true,"content":"<p>I need to vent a bit, as I again ran into a situation where I am getting increasingly frustrated by dealing with an . </p><p>I am building an HTTP server application, so the  crate is in my dependency tree and its  type is exposed in various places (e.g. hyper). Oftentimes, I need to inspect or manipulate URIs. For example, my application can be configured and many config values are URI-like. But: most have some limitations that I want to check for, e.g. \"only http or https scheme + authority; no path, query or fragment\". Doing these checks, or generally inspecting or manipulating this type is quite annoying though.</p><p>And I hear you: \"Just use the  crate!\". I think <a href=\"https://www.reddit.com/r/rustjerk/comments/1m95ejb/cargo_add_url/\">this post</a> should explain my concerns with it. Even ignoring the dependency problem or the fact that it would compile two separate URL parsers into my binary: when using , I have s everywhere, so converting them back and forth is just annoying, especially since there is no convenient way to do that!</p><p>It is just plain frustrating. I have been in this situation countless times before! And every time I waste lots of time wrangling confusing APIs, writing lots of manual boilerplate code, having an existential breakdown, and considering whether to . I can only imagine the accumulated human life wasted due to this :(</p><p>As a disclaimer I should say that all these issues are known to the maintainers and there are some valid arguments for why things are the way they are. I still think the current situation is just not acceptable for the whole ecosystem and it should be possible  to fix this.</p><p>Thanks for coming to my TED talk. </p>","contentLength":1587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Concord - A Go implementation of the Chord Protocol","url":"https://github.com/ollelogdahl/concord","date":1761746019,"author":"/u/Due-Fig3935","guid":320746,"unread":true,"content":"<p>Hello! I just wanted to share my Chord implementation written in Go with the world and see if I can get some feedback. I call it Concord and it implements the core consistent-hashing of Chord. Compared to the original paper, that is actually NOT resilient to failures, I have tried really hard to design it around Pamela Zave's formally-proven correct versions of Chord (<a href=\"https://www.pamelazave.com/chord.html\">https://www.pamelazave.com/chord.html</a>). Most of my focus have gone into making sure that my code is as similar as possible and verifying it. It tries to be a good out-of-the-box solution, using gRPC as the transport layer. In the next version, support for sharing a gRPC server with other systems will be provided, so it will be easy to build more complex systems on top of this. Abstracting transport seems like a good future feature, but I won't be using it so I'll hold off for a while.</p><p>I came up with a fuzzer to test the implementation. Similarily to tools like TLA+, it uses a state machine and invariants to check the implementation. The state machine is more like a black-box orchestrator for the library objects, so of course it is not actual formal verification. However, using this I can test the implementation with randomized valid actions on the state (join node, leave nodes), and continously checks eventual-consistency invariants. This has been running for many hours without any issues!</p><p>I know there are other projects like this out there, but mine focuses on simplicity and correctness, and should be a viable platform to use.</p><p>If you think that sounds cool, or just want to see the code, feel free to check it out! :)</p>","contentLength":1604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oj55do/concord_a_go_implementation_of_the_chord_protocol/"},{"title":"[P] Looking for Teammates for Kaggle competition : PhysioNet - Digitization of ECG Images","url":"https://www.reddit.com/r/MachineLearning/comments/1oj54b5/p_looking_for_teammates_for_kaggle_competition/","date":1761745946,"author":"/u/Melodic_Story609","guid":320772,"unread":true,"content":"<p>I'm looking to form a team for the current Kaggle competition: <strong>PhysioNet - Digitization of ECG Images</strong>.It's a really interesting computer vision/OCR challenge. I have experience with Vision Transformers, VLM fine-tuning, and deep learning .</p><p>If you're interested in joining, DM to me. Let's get this done.</p>","contentLength":302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to reduce Managed Prometheus scrape interval on GKE Autopilot?","url":"https://www.reddit.com/r/kubernetes/comments/1oj51hz/how_to_reduce_managed_prometheus_scrape_interval/","date":1761745757,"author":"/u/thegoenning","guid":320717,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I am a beginner to Golang, what mini-projects would you suggest?","url":"https://www.reddit.com/r/golang/comments/1oj4ttf/i_am_a_beginner_to_golang_what_miniprojects_would/","date":1761745233,"author":"/u/EconomicsOk8188","guid":320719,"unread":true,"content":"<p>I am a beginner to Golang, I have programmed in other languages before. These were Python, Dart, and Lua. I have learned variables, types, input/output to cli, random number generators, and functions. What mini-projects would you suggest to be able to advance by skills?</p>","contentLength":270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Will AI take over the porn industry?","url":"https://www.reddit.com/r/artificial/comments/1oj3u2u/will_ai_take_over_the_porn_industry/","date":1761742724,"author":"/u/Sure-Restaurant9610","guid":320821,"unread":true,"content":"<p>I know AI video generation has been around for a while, but finally I tried it myself, and the results are amazing.</p><p>I'm wondering how long it will take until AI is used to generate porn videos, because this really looks like a goldmine for the porn industry.</p><p>The attached video is a short example I generated. Hopefully it won't violate the rules here. I tried to censor anything sensitive.</p>","contentLength":388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Built a Rust implementation of Andrej Karpathy's micrograd","url":"https://www.reddit.com/r/rust/comments/1oj2tk4/built_a_rust_implementation_of_andrej_karpathys/","date":1761739913,"author":"/u/Ryzen__master","guid":321992,"unread":true,"content":"<p>Someone recently shared their implementation of Micrograd in Go, and in their blog they mentioned they had initially planned to do it in Rust. That gave me the idea to try it myself.</p><p>I followed along with Andrej Karpathy’s video while coding, and it turned out to be a great learning experience — both fun and insightful. The result is <a href=\"https://github.com/tarushmohindru/micrograd-rs\">micrograd-rs</a> , a Rust implementation of Micrograd focused on clarity and alignment with the original Python version.</p><p>A few months ago, I also built a small tensor library called <a href=\"https://github.com/tarushmohindru/Grad\">Grad</a>. Since the current micrograd-rs implementation only supports scalars, my next step is to integrate it with Grad for tensor support.</p><p>I’d love feedback, suggestions, or contributions from anyone interested in Rust, autodiff, or ML frameworks.</p>","contentLength":762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tips for stroke-surviving software engineers","url":"https://blog.j11y.io/2025-10-29_stroke_tips_for_engineers/","date":1761737259,"author":"/u/No-Session6643","guid":320666,"unread":true,"content":"<li><p>The first tip is to just stop. Fatigue, fuzziness, nausea, or affected-sided weird sensations are non-negotiable stop signals. So go lie down, hydrate, reset. Close your eyes and think about the cottage or lonely mountain you want to retire to. Escape the overwhelming mental or physical space. </p></li><li><p>HEADPHONES, blinders, and 'No'. Eliminate unwanted inputs at the earliest point of entry. Work from home or environments where you can control most variables. Routes of escape and rest are important.</p></li><li><p>Health above performance every single time. Metrics and productivity be damned. Self-advocate, and all that. Reject with directness any demands made of you that cross the threshold. </p></li><li><p>Laws. Use them. You don't have to rely on good behaviour and kindness. You are, depending on your location, usually protected by all types of anti-discrimination legislation, implicit and explicit. Use your employee assistance programs too.</p></li><li><p>Single-thread it all! Less context switching. Batch your work, finish one thing, then move to the next. Externalize working-memory. Use notebooks, whiteboards, and lists instead of juggling state in your head. I am not good at this, and over-stretch my brain, leading to auras, overwhelm, and general sickness. Terrible idea.</p></li><li><p>Related: Sssh to the AI naysayers. Use it as your help and scratchpad. Let it hold state so your brain can judge rather than store and needlessly cogitate on stuff. You don't have to do this alone out of some purity fetishism. You, too, have a limited context window. Sorry!</p></li><li><p>Do the heavy thinking in your peak window (for me, that's the morning); push everything else to later. Spend your time more carefully than your money.</p></li><li><p>Pick the route of least attention. Attention is expensive, and rarely needed as much as we think it is. It's a heavy toll to pay. Unless you're in an ops or monitoring role, you don't need to be synchronously active. DISABLE NOTIFICATIONS. </p></li><li><p>AVOID long meetings. Emails are good. Oh god am I bad at this? YES, I like people so I like some meetings, but communicating is so so expensive. Being polite is also expensive; It's not nice to have to tell people they're draining you.</p></li>","contentLength":2140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oj1yeo/tips_for_strokesurviving_software_engineers/"},{"title":"AWS to Bare Metal Two Years Later: Answering Your Toughest Questions About Leaving AWS","url":"https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view","date":1761737123,"author":"/u/OuPeaNut","guid":320665,"unread":true,"content":"<p>When we published <a href=\"https://oneuptime.com/blog/post/2023-10-30-moving-from-aws-to-bare-metal/view\">How moving from AWS to Bare-Metal saved us $230,000 /yr.</a> in 2023, the story travelled far beyond our usual readership. The discussion threads on <a href=\"https://news.ycombinator.com/item?id=38294569\" target=\"_blank\" rel=\"noopener noreferrer\">Hacker News</a> and <a href=\"https://www.reddit.com/r/sysadmin/comments/17y6zbi/moving_from_aws_to_baremetal_saved_us_230000_yr/\" target=\"_blank\" rel=\"noopener noreferrer\">Reddit</a> were packed with sharp questions: did we skip Reserved Instances, how do we fail over a single rack, what about the people cost, and when is cloud still the better answer? This follow-up is our long-form reply.</p><p>Over the last twenty-four months we:</p><ul><li>Ran the MicroK8s + Ceph stack in production for 730+ days with 99.993% measured availability.</li><li>Added a second rack in Frankfurt, joined to our primary Paris cage over redundant DWDM, to kill the “single rack” concern.</li><li>Cut average customer-facing latency by 19% thanks to local NVMe and eliminating noisy neighbours.</li><li>Reinvested the savings into buying bare metal AI servers to expand LLM-based alert / incident summarisation and auto code fixes based on log / traces and metrics in OneUptime.</li></ul><p>Below we tackle the recurring themes from the community feedback, complete with the numbers we use internally.</p><h2>$230,000 / yr savings? That is just an engineers salary.</h2><p>In the US, it is. In the rest of the world. That's 2-5x engineers salary. We  to save $230,000 / yr but now the savings have exponentially grown. We now save over $1.2M / yr and we expect this to grow, as we grow as a business.</p><h2>“Why not just buy Savings Plans or Reserved Instances?”</h2><p>We tried. Long answer: the maths still favoured bare metal once we priced everything in. We see a savings of over 76% if you compare our bare metal setup to AWS. </p><ul><li>Savings Plans  reduce S3, egress, or Direct Connect. 37% off instances still leaves you paying list price for bandwidth, which was 22% of our AWS bill.</li><li>EKS had an extra $1,260/month control-plane fee plus $600/month for NAT gateways. Those costs disappear once you run Kubernetes yourself.</li><li>Our workload is 24/7 steady. We were already at &gt;90% reservation coverage; there was no idle burst capacity to “right size” away. If we had the kind of bursty compute profile many commenters referenced, the choice would be different.</li></ul><h2>“How much did migration and ongoing ops really cost?”</h2><p>We spent a week of engineers time (and that is the worst case estimate) on the initial migration, spread across SRE, platform, and database owners. Most of that time was work we needed anyway—formalising infrastructure-as-code, smoke testing charts, tightening backup policies. The incremental work that existed purely  of bare metal was roughly one week.</p><p>Ongoing run-cost looks like this:</p><ul><li> ~24 engineer-hours/quarter across the entire platform team, including routine patching and firmware updates. That is comparable to the AWS time we used to burn on cost optimisation, IAM policy churn, and chasing deprecations and updating our VM's on AWS. </li><li> 2 interventions in 24 months (mainly disks). Mean response time: 27 minutes. We do not staff an on-site team. We rely on co-location provider to physically manage our rack. This means no traditional hardware admins. </li><li> We're now moving to Talos. We PXE boot with Tinkerbell, image with Talos, manage configs through Flux and Terraform, and run conformance suites before each Kubernetes upgrade. All of those tools also hardened our AWS estate, so they were not net-new effort.</li></ul><p>The opportunity cost question from is fair. We track it the same way we track feature velocity: did the infra team ship less? The answer was “no”—our release cadence increased because we reclaimed few hours/month we used to spend in AWS “cost council” meetings.</p><h2>“Isn’t a single rack a single point of failure?”</h2><p>We have multiple racks across two different DC / providers. We:</p><ul><li>Leased a secondary quarter rack in Frankfurt with a different provider and power utility.</li><li>Currently: Deployed a second MicroK8s control plane, mirrored Ceph pools with asynchronous replication. Future: We're moving to Talos. Nothing against Microk8s, but we like the Talos way of managing the k8s cluster.</li><li>Added isolated out-of-band management paths (4G / satellite) so we can reach the gear even during metro fibre events.</li></ul><p>The AWS failover cluster we mentioned in 2023 still exists. We rehearse a full cutover quarterly using the same Helm releases we ship to customers. DNS failover remains the slowest leg (resolver caches can ignore TTL), so we added Anycast ingress via BGP with our transit provider to cut traffic shifting to sub-minute.</p><h2>“What about hardware lifecycle and surprise CapEx?”</h2><p>We amortise servers over five years, but we sized them with 2 × AMD EPYC 9654 CPUs, 1 TB RAM, and NVMe sleds. At our current growth rate the boxes will hit CPU saturation before we hit year five. When that happens, the plan is to cascade the older gear into our regional analytics cluster (we use Posthog + Metabase for this) and buy a new batch. Thanks to the savings delta, we can refresh 40% of the fleet every 24 months and still spend less annually than the optimised AWS bill above.</p><p>We also buy extended warranties from the OEM (Supermicro) and keep three cold spares in the cage. The hardware lasts 7-8 years and not 5, but we wtill count it as 5 to be very conservative. </p><h2>“Are you reinventing managed services?”</h2><p>Another strong Reddit critique: why rebuild services AWS already offers? Three reasons we are comfortable with the trade:</p><ol><li><strong>Portability is part of our product promise.</strong> OneUptime customers self-host in their own environments. Running the same open stack we ship (Postgres, Redis, ClickHouse, etc.) keeps us honest. We eun on Kubernetes and self-hosted customers run on Kubernetes as well. </li><li> Two years ago we relied on Terraform + EKS + RDS. Today we run MicroK8s (Talos in the future), Argo Rollouts, OpenTelemetry Collector, and Ceph dashboards. None of that is bespoke. We do not maintain a fork of anything.</li><li> We still pay AWS for Glacier backups, CloudFront for edge caching, and short-lived burst capacity for load tests. Cloud makes sense when elasticity matters; bare metal wins when baseload dominates.</li></ol><p>Managed services are phenomenal when you are short on expertise or need features beyond commodity compute. If we were all-in on DynamoDB streams or Step Functions we would almost certainly still be on AWS.</p><h2>“How do bandwidth and DoS scenarios work now?”</h2><p>We committed to 5 Gbps 95th percentile across two carriers.  The same traffic on AWS egress would be 8x expensive in eu-west-1. For DDoS protection we front our ingress with Cloudflare. </p><h2>“Has reliability suffered?”</h2><p>Short answer: No. Infact it was better than AWS (compared to recent AWS downtimes)</p><p>We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago. </p><h2>“How do audits and compliance work off-cloud now?”</h2><p>We stayed SOC 2 Type II and ISO 27001 certified through the transition. The biggest deltas auditors cared about:</p><ul><li>Physical controls: We provide badge logs from the colo, camera footage on request, and quarterly access reviews. The colo already meets Tier III redundancy, so their reports roll into ours.</li><li>Change management: Terraform plans, and now Talos machine configs give us immutable evidence of change. Auditors liked that more than AWS Console screenshots.</li><li>Business continuity: We prove failover by moving workload to other DC.</li></ul><p>If you are in a regulated space (HIPAA for instance), expect the paperwork to grow a little. We worked it in by leaning on the colo providers’ standard compliance packets—they slotted straight into our risk register.</p><h2>“Why not stay in the cloud but switch providers?”</h2><p>We priced Hetzner, OVH, Leaseweb, Equinix Metal, and AWS Outposts. The short version:</p><ul><li>Hyperscaler alternatives were cheaper on compute but still expensive on egress once you hit petabytes/month. Outposts also carried minimum commits that exceeded our needs.</li><li>European dedicated hosts (Hetzner, OVH) are fantastic for lab clusters. The challenge was multi-100 TB Ceph clusters with redundant uplinks and smart-hands SLAs. Once we priced that tier, the savings narrowed.</li><li>Equinix Metal got the closest, but bare metal on-demand still carried a 25-30% premium over our CapEx plan. Their global footprint is tempting; we may still use them for short-lived expansion.</li></ul><p>Owning the hardware also let us plan power density (we run 15 kW racks) and reuse components. For our steady-state footprint, colocation won by a long shot.</p><h2>“What does day-to-day toil look like now?”</h2><p>We put real numbers to it because Reddit kept us honest:</p><ul><li>Weekly: Kernel and firmware patches (Talos makes this a redeploy), Ceph health checks,  Total time averages 1 hour/week on average over months. </li><li>Monthly: Kubernetes control plane upgrades in canary fashion. About 2 engineer-hours. We expect this to reduce when Talos kicks in.</li><li>Quarterly: Disaster recovery drills, capacity planning, and contract audits with carriers. Roughly 12 hours across three engineers.</li></ul><p>Total toil is ~14 engineer-hours/month, including prep. The AWS era had us spending similar time but on different work: chasing cost anomalies, expanding Security Hub exceptions, and mapping breaking changes in managed services. The toil moved; it did not multiply.</p><h2>“Do you still use the cloud for anything substantial?”</h2><p>Absolutely. Cloud still solves problems we would rather not own:</p><ul><li>Glacier keeps long-term log archives at a price point local object storage cannot match.</li><li>CloudFront handles 14 edge PoPs we do not want to build. We terminate TLS at the edge for marketing assets and docs. We will soon move this to Cloudflare as they are cheaper.</li><li>We spin up short-lived AWS environments for load testing.</li></ul><p>So yes, we left AWS for the base workload, but we still swipe the corporate card when elasticity or geography outweighs fixed-cost savings.</p><h2>When the cloud is still the right answer</h2><p><strong>It depends on your workload</strong>. We still recommend staying put if:</p><ul><li>Your usage pattern is spiky or seasonal and you can auto-scale to near zero between peaks.</li><li>You lean heavily on managed services (Aurora Serverless, Kinesis, Step Functions) where the operational load is the value prop.</li><li>You do not have the appetite to build a platform team comfortable with Kubernetes, Ceph, observability, and incident response.</li></ul><p>Cloud-first was the right call for our first five years. Bare metal became the right call once our compute footprint, data gravity, and independence requirements stabilised.</p><ul><li>We are working on a detailed runbook + Terraform module to help teams do  for colo moves. Expect that on the blog later this year.</li><li>A deep dive on Talos is in the queue, as requested by multiple folks in the HN thread.</li></ul><p>Questions we did not cover? Let us know in the discussion threads—we are happy to keep sharing the gritty details.</p><ul></ul>","contentLength":10599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1oj1wtf/aws_to_bare_metal_two_years_later_answering_your/"},{"title":"Сheck out our pure Go HDF5 library. Beta Testers Needed.","url":"https://www.reddit.com/r/golang/comments/1oj1p2f/%D1%81heck_out_our_pure_go_hdf5_library_beta_testers/","date":1761736431,"author":"/u/mistbow","guid":320668,"unread":true,"content":"<p>I'm developing a pure Go implementation of HDF5 reader (v0.10.0-beta).</p><p>For those unfamiliar: HDF5 is the standard format for large scientific datasets in physics, astronomy, climate science, and genomics. Existing Go solutions require CGo bindings to the C library, which creates deployment and cross-compilation issues.</p><p>This library is pure Go with zero dependencies. Currently supports all common HDF5 features: all datatypes, chunked/compressed datasets, attributes, legacy formats back to HDF5 1.6.</p><p>I've validated it against 57 reference files from the official HDF5 test suite with 100% pass rate, but I need real-world testing. If you work with HDF5 files and would be willing to test the library with your actual datasets, I'd greatly appreciate the feedback.</p><p>Read-only for now (write support planned for v0.11.0). Works on Linux, macOS, Windows, ARM.</p><p>Installation: <code>go get github.com/scigolib/hdf5@v0.10.0-beta</code></p><p>Issues and feedback welcome.</p>","contentLength":940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disasters I've seen in a microservices world, part II","url":"https://world.hey.com/joaoqalves/disasters-i-ve-seen-in-a-microservices-world-part-ii-9e6826bf","date":1761735807,"author":"/u/joaoqalves","guid":320771,"unread":true,"content":"<div>When I first <a href=\"https://world.hey.com/joaoqalves/disasters-i-ve-seen-in-a-microservices-world-a9137a51\">wrote about microservice disasters</a>, I thought we'd eventually \"solve\" them, with better tooling, frameworks, and operational maturity. We didn't. We just learned to live with the chaos. Distributed systems will always surprise you: timeouts, retries, and fallacies don't disappear; they just shift shape. Maybe that's the real lesson: software engineering isn't about eliminating uncertainty, but managing it gracefully.</div><div>Being part of the <a href=\"https://medium.com/learnings-from-the-paas\">Runtime team at Adevinta</a>, building an internal Kubernetes-as-a-service for the rest of the company, gave me a perspective on all the things software engineers build atop distributed systems. After all, is a Platform any good if we're not surprised by what people build? I don't think so.</div><div>So, today I'm adding four disasters I've seen first-hand to the other six.</div><div>When I join a new team or domain, one of the first things I ask for is a walkthrough of the architecture. It's not just curiosity, it's survival. It helps me form a mental model of the feature set, complexity, technical debt, and the real boundaries of what's possible. It's also the best way to connect with Individual Contributors (ICs) and understand how they perceive the system.</div><div>Time and time again, I'm baffled by how many teams have <strong>more services than engineers</strong>. And I don't mean \"slightly more\". I'm talking about four or five services per person. It sounds impressive on a slide deck — \"we've fully modularized our platform!\" —, until you realize it means one human being is the  owner, operator, and incident responder for half a dozen distributed systems.</div><div>If you're Google, Uber, or any of the large tech companies with a world-class internal platform, you can get away with that. You have automatic dependency upgrades, standardized templates, CI/CD abstractions, service ownership dashboards, and well-staffed SRE teams. But most other companies? Even with good automation, this setup is a slow-motion disaster.</div><div>Each new service adds cognitive overhead. Think about new pipelines, dashboards, alerts, secrets, dependencies, and runtime contexts. Every change multiplies the blast radius. And when the team reorganizes (because they always do), those services become orphaned. No one remembers what they do, but everyone is too afraid to turn them off. They just keep running.</div><div>One of the hardest things to get right in distributed systems is the the connective tissue between your frontends and microservices, or between services themselves. In theory, it's a clean abstraction. In practice, it's a pressure cooker for all the complexity we didn't want to deal with elsewhere.</div><div>Authentication and authorization are perfect examples. They sound simple until you need to combine multiple identity providers, fine-grained permissions, and multi-tenant scoping. All while remaining both secure and fast. Many teams underestimate how expensive those operations are, and worse, they overload their gateways with them. Suddenly, <strong>what was meant to be a lightweight routing layer becomes a CPU-bound monolith</strong> performing crypto operations and access checks for every request.</div><div>Then comes the other side of the coin: thread pools and I/O behavior. Gateways are typically the first and last hop in a request chain. <strong>If you don't understand whether your downstream services are CPU-bound or I/O-bound, you'll misconfigure your pools and timeouts</strong>. It's incredibly common to see gateways with default thread counts inherited from some Spring Boot starter or Node.js template, serving hundreds of concurrent connections. The result? Latency spikes, thread starvation, and cascading failures that ripple across the fleet.</div><div>Building reliable gateways requires more than YAML and good intentions. It demands understanding backpressure, circuit breakers, and how your runtime behaves under load. Most teams don't realize how fragile their setup is until a single misconfigured pool takes down the entire production environment.</div><div>Every company says they value \"engineering autonomy.\" Few realize what that really means. Given enough time and freedom, engineers will pick every possible framework, runtime, and library known to humankind. Kotlin coroutines here, Vert.x there, Go services running next to a Rust API that only one person still understands. It's like visiting a theme park of ideas, until something breaks and no one remembers how to restart the ride.</div><div>This kind of  doesn't happen because engineers are careless. It occurs because leadership allows — or even encourages it! — under the banner of innovation. But innovation without accountability leads to entropy. Every new stack is a new operational surface, a new security vector, a new onboarding cost. And when reorgs happen — : they always do! — these snowflake systems become landmines.&nbsp;</div><div>The only person who can fix them might have just left the company. Flight risk, in this context, isn't just a people problem; it's a systems problem. Each \"unique\" tech choice creates a dependency on a human being. Lose that person, and you lose that piece of the system's knowledge graph.</div><div>Fortunately, this is one area where things are getting better. AI-assisted code understanding tools, architecture reviews, and internal tech radars are helping teams regain visibility. They don't eliminate sprawl, but they make it more transparent. And sometimes, just seeing the mess clearly is the first step to cleaning it up.</div><div>This one is slightly related to Disaster #7, but with an organizational twist. I've talked about it countless times with <a href=\"https://thibault.jamet.dev/\">Thibault</a>. When teams start creating dozens of microservices, they often organize them  rather than . So you end up with things like \", , and so on. They deploy their services into their team's, their Terraform stack lives in their team's , and all the dashboards and alarms are wired to the team's.</div><div>It looks tidy on paper. Every team has clear ownership, autonomy, and a sandbox that the team can break without bothering others until the org chart changes. Because it constantly changes.&nbsp;</div><div>A new VP of Engineering brings fresh ideas for efficiency and alignment. A reorg happens. Suddenly, the \"Payments team\" that owned two bounded contexts — say,  and  — is split into two. One team keeps ; another inherits . But all the infrastructure, namespaces, and IAM policies are still tangled under the old account. So now you have two options:</div><ol><li>, creating a new kind of dependency hell.</li><li>, which is just a fancy way of saying <em>\"congratulations, you've bought yourself a six-month migration project.\"</em></li></ol><div>Neither option feels good. The first one slows teams down and creates confusion about ownership (\"who fixes this alarm now?\"). The second burns time and budget on work that delivers no user-facing value.</div><div>This problem goes deeper than just cloud accounts or namespaces. It's about <strong>coupling architecture to the org chart</strong>. When <a href=\"https://en.wikipedia.org/wiki/Conway%27s_law\">Conway's Law</a> meets reorgs, you get architectural drift. Systems that once mirrored the team structure start to outlive it, and suddenly, your entire platform's topology reflects who reported to whom in 2021.</div><div>It's one of the most expensive kinds of technical debt. It's invisible until the (non-existent) reorg budget suddenly triples.</div><div>Four years later, I'm still seeing the same patterns, just dressed in different frameworks, clouds, and YAML dialects. The tools evolved, but the fundamentals didn't: distributed systems remain distributed, humans remain human, and complexity remains undefeated.</div><div>What's coming next will make this even more interesting. We're now trying to build : autonomous, stateful systems that communicate with each other, make probabilistic decisions, and respond to unpredictable inputs. In other words: distributed systems with opinions.  The same fallacies apply, just at a different layer. Latency, consistency, observability, determinism. None of them magically disappears because the component now \"thinks.\"</div><div>As an industry, we'll go through the same cycle again: early excitement, creative disasters, tooling booms, and eventually, a bit of humility.</div><div>Whether it's microservices or AI agents, the story doesn't really change. We're still trying to make chaotic systems behave predictably. And we're pretending we can fully control them.</div><div>The good news? We'll keep learning. The bad news? We'll probably learn it the hard way.</div><div>— João<a href=\"https://rotahog.com?utm_source=heyworld\"></a>, a lightweight tool for managing team rotation schedules (on-call, support shifts, release duties, etc.). Try it if you're tired of hacking spreadsheets or Slack threads together. I'd love your feedback!<strong>subscribing to the newsletter</strong><a href=\"https://bit.ly/buy-me-a-coffee-joaoqalves\"></a></div>","contentLength":8502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oj1i7i/disasters_ive_seen_in_a_microservices_world_part/"},{"title":"Discarding gRPC-Go: The Story Behind OTLP/gRPC Support in VictoriaTraces","url":"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/","date":1761735694,"author":"/u/SnooWords9033","guid":320667,"unread":true,"content":"<p>Let’s begin with the results we achieved by discarding the use of <a href=\"https://github.com/grpc/grpc-go\" rel=\"external\" target=\"_blank\">gRPC-Go</a> to build the gRPC server for OTLP/gRPC:</p><ul></ul><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#background\"></a><p>The OpenTelemetry protocol (OTLP) is very popular for exchanging telemetry data between any OpenTelemetry instrumented applications and OpenTelemetry (compatible) collectors/backends.</p><p>Assume you have an application/collector which want to export data to another collector, you can config it to send data with:</p><p>Currently, VictoriaTraces only exposes an HTTP endpoint to receive data via the latter 2 formats: OTLP/HTTP binary &amp; JSON.\nThere are a lot of applications out there that <strong>only support sending data via OTLP/gRPC</strong>, one typical example could be <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/system-traces/#kube-apiserver-traces\" rel=\"external\" target=\"_blank\">kube-apiserver</a>.\nSo it’s important to cover these cases as well.</p><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#supporting-otlpgrpc\"></a><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#the-goal\"></a><p>Our goal is to  that can serve as a  and  for invocation,\nas defined in <a href=\"https://github.com/open-telemetry/opentelemetry-proto/blob/v1.8.0/opentelemetry/proto/collector/trace/v1/trace_service.proto\" rel=\"external\" target=\"_blank\">the OpenTelemetry’s proto</a>.</p><div><pre tabindex=\"0\"><code data-lang=\"proto\"></code></pre></div><p>What makes us consider not using gRPC-Go, or to be more specific, not using the  toolchain?</p><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#problem-1-the-protoc-toolchain-isnrsquot-user-friendly\"></a><p>The common way to build a gRPC server is:</p><ul><li>Use  and  to generate the s of the  defined in .</li><li>Use  and  to generate  defined in .</li></ul><p>But, wait. Let’s recall how it could be done. Assume I (who is new to the Protobuf) just cloned the project, and want to add new messages/methods to the :</p><ol><li>Notice that the  toolchain is not part of my Ubuntu/MacOS.</li><li>Ooops,  can’t run solely if I want to generate go code. I need to  and <code>go install protoc-gen-go-grpc</code>.</li><li>All set, what’s the commands and flags to generate them? Google <a href=\"https://grpc.io/docs/languages/go/quickstart/#regenerate-grpc-code\" rel=\"external\" target=\"_blank\">“gRPC compile Go”</a> for the tutorial.</li><li>Finally, I prepared the commands and run it locally. Bomb, it said dependency errors, because there are some s in my  and I have to specify the paths.</li><li>After fixing them, rerun the commands and successfully generate (thousands of lines of) Go code.</li></ol><p>But don’t jump for joy too soon, because  may have unexpected surprises in store for you.</p><p>“Why do the new code look different from the previous ones?”</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>While the previous one is like:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><ol start=\"7\"><li>Google for the <a href=\"https://github.com/golang/protobuf/issues/276\" rel=\"external\" target=\"_blank\">reason</a>, then redo steps 2-6 using a different version of the  toolchain. (Sometimes you may need to  to find out who the author of this code.)</li></ol><p>All these steps show that compiling protobuf-related stuff is not as straightforward as writing simple HTTP JSON APIs.</p><p>It could become easy if you:</p><ol><li>Write commands into the .</li><li>Use <a href=\"https://buf.build/product/cli\" rel=\"external\" target=\"_blank\">Buf CLI</a> to compile without the  toolchain.</li></ol><p>But many developers still opt for the HTTP JSON APIs.</p><p>That said, this hardly suffices to persuade us to discard the  toolchain. What else?</p><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#problem-2-the-existing-use-of-easyproto-instead-of-golangprotobuf\"><h3 origin-id=\"problem-2-the-existing-use-of-easyproto-instead-of-golangprotobuf\">Problem 2: The Existing Use of  Instead of </h3></a><div><div><p>Problems 2 may provide further insights, though it should be clarified that it <strong>only apply to VictoriaTraces</strong> given its .</p></div></div><p>In VictoriaMetrics, VictoriaLogs and VictoriaTraces, we use  to marshal and unmarshal protobuf messages. The reasons are written in its <a href=\"https://github.com/VictoriaMetrics/easyproto\" rel=\"external\" target=\"_blank\"></a>:</p><blockquote><ul><li><strong>doesn’t require  or </strong>.</li><li><strong>doesn’t increase the binary size</strong> unlike traditional protoc-compiled code may do.</li><li><strong>allows writing zero-alloc code</strong>.</li></ul></blockquote><p>However, to add OTLP/gRPC support, we need to consider the following:</p><ol><li>If we simply build a gRPC server with code generated by , how much will the  of the application increase?</li><li>Can we combine  with gRPC? This way,  would only need to generate code for the gRPC service, and not for protobuf message s.<ul><li>Note that this still requires importing gRPC-related packages, which weakens the second reason of using  (aimed at reducing binary size).</li></ul></li><li>Are there any other solutions that reuse <strong>without importing new packages</strong>?</li></ol><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#unorthodox-way-an-http2-server\"><h2 origin-id=\"unorthodox-way-an-http2-server\">Unorthodox Way: An HTTP/2 Server</h2></a><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#the-theory\"></a><p>gRPC is a protocol that uses HTTP/2. So it’s possible to implement an HTTP/2 server to serve requests at specific endpoints.</p><blockquote><p>gRPC can also use HTTP/3 (QUIC) and HTTP/1.1, but let’s avoid getting too deep into that for now.\nJust note that the current implementation also supports gRPC over HTTP/1.1.\nWhat’s more, thanks to its highly straightforward design, the OTLP/gRPC JSON format could also be supported very easily.\nBut as the JSON format currently only works with OTLP/HTTP, we haven’t put extra effort into it.</p><p>We’ll leave it to readers to explore further.</p></blockquote><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>And the HTTP endpoint for  method in  is: <code>/opentelemetry.proto.collector.trace.v1.TraceService/Export</code>.</p><p>The following code block shows you how the implementation looks like:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#whats-the-cost\"></a><p>While the implementation looks straightforward and simple, there must be a cost.</p><p>So far this approach has only been tested with <a href=\"https://grpc.io/docs/what-is-grpc/core-concepts/#unary-rpc\" rel=\"external\" target=\"_blank\">the unary RPC</a>. For streaming RPC, we have no scenarios or motivation for further testing.</p><p>This approach can cover what we need for OTLP/gRPC, but it might not work for other cases. If you know more about that, feel free to leave a comment!</p><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#comparison\"></a><p>We conduct the benchmark against binary size and resource usage between different approaches of OTLP/gRPC support in VictoriaTraces:</p><ol><li>Write an HTTP/2 server, and unmarshal with .</li><li>Compile an gRPC server with , and unmarshal with the native gRPC decoder.</li></ol><p>Additionally, the compiled gRPC server does support customizing encoder and decoder via the following code example. We add this to the comparison as well.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>And regarding the performance of request handling (CPU usage, no-op: requests are responded immediately after decompressing and unmarshalling):</p><p>The monitoring snapshot can be found <a href=\"https://snapshots.raintank.io/dashboard/snapshot/9U4rWHfXx91AHwiaatmmF75XkmtOyiUH?orgId=0\" rel=\"external\" target=\"_blank\">here</a>. CPU and memory profiles are available <a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/profiles.zip\">here</a>.</p><p>Based on the benchmark results, it is evident that <strong>HTTP/2 combined with easyproto does demonstrate a clear advantage</strong>.</p><a href=\"https://victoriametrics.com/blog/opentelemetry-without-grpc-go/#conclusion\"></a><p>This blog shares the story <strong>why VictoriaTraces implements gRPC server for OTLP/gRPC in the HTTP/2 +  way</strong>.\nThe core implementation was done by <a href=\"https://github.com/JayiceZ\" rel=\"external\" target=\"_blank\">@JayiceZ</a>, with the initial idea coming from <a href=\"https://github.com/makasim\" rel=\"external\" target=\"_blank\">@makasim</a>.</p><p>There are certain contextual reasons behind this, we’re not try to persuade you to do so. But we see great potential and value, and better developer experience in this approach.</p><p>As the VictoriaMetrics Stack aims for high performance and cost efficiency, every bit of saved CPU, memory, and network traffic matters significantly.\nAnd the same holds true for binary sizes, Docker image sizes, and other aspects,\njust as mentioned in <a href=\"https://valyala.medium.com/stripping-dependency-bloat-in-victoriametrics-docker-image-983fb5912b0d\" rel=\"external\" target=\"_blank\">this blog</a> by <a href=\"https://github.com/valyala\" rel=\"external\" target=\"_blank\">Aliaksandr Valialkin</a> (founder of VictoriaMetrics),\nand they remain as critical today as they were then.</p>","contentLength":5979,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oj1gxh/discarding_grpcgo_the_story_behind_otlpgrpc/"},{"title":"Upgrading physical network (network cards) on kubernetes cluster","url":"https://www.reddit.com/r/kubernetes/comments/1oj0x8c/upgrading_physical_network_network_cards_on/","date":1761733851,"author":"/u/BunkerFrog","guid":320695,"unread":true,"content":"<p>Hi, I do have a cluster on bare metal, during scaling we realized that our current network connection (internal between nodes) gets saturated. Solution would be to get new and faster NIC cards and switch.</p><p>What need to be done and prepared to \"unassign\" current NICs from and \"assign\" new ones? What need to be changed in the cluster configuration and what are the best practices to do it so.</p><p>OS: Ubuntu 24.04 Flavour: MicroK8S</p>","contentLength":424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vercel trained an AI agent on its best salesperson. Then it cut the 10-person team down to 1.","url":"https://www.businessinsider.com/ai-agent-entry-level-sales-jobs-vercel-2025-10","date":1761733492,"author":"/u/MetaKnowing","guid":321973,"unread":true,"content":"<p>As companies race to deploy AI, Vercel says it has found a way to get ahead: They are training AI agents on how their best employees work.</p><p>The $9.3 billion company, founded by coder <a target=\"_self\" href=\"https://www.businessinsider.com/frontier-content-survive-online-ai-2024-9\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Guillermo Rauch</a> in 2015, is a cloud-based platform for developers to build and deploy websites and applications.</p><p>It's now using <a target=\"_self\" href=\"https://www.businessinsider.com/what-is-an-ai-agent-depends-who-you-ask-2025-3\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">AI agents</a> to automate the rote work of many of its entry-level roles, allowing it to reduce a once 10-person team down to just one person and a bot.</p><p>Agents are commonly defined as virtual assistants that can complete tasks autonomously. They break down problems, outline plans, and take action without being prompted by a user.</p><p>\"If you can document a workflow, it's now pretty straightforward to have an agent do it,\" Jeanne DeWitt Grosser, Vercel's chief operating officer, told Business Insider.</p><p>The process of developing an AI agent began in June, when the company launched an internal initiative within its sales department. Grosser, who had joined in March, recruited three engineers to develop agents that would replicate and enhance critical sales workflows.</p><p>At the time, the company had 10 sales development representatives handling inbound queries, generally an entry-level task, and one of them was a standout performer. The engineers shadowed that top performer for six weeks and documented every step of their work. Then they <a target=\"_self\" href=\"https://www.businessinsider.com/ai-coding-agents-adoption-top-tools-2025-8\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">built an agent</a> to mimic their process.</p><p>Now, Vercel's \"lead agent\" automates much of the work once handled by multiple sales development reps, Grosser said. It reviews inbound messages, filters out spam, and qualifies leads by querying internal databases and researching company details through <a target=\"_self\" href=\"https://www.businessinsider.com/openai-deep-research-launch-chatgpt-ai-agent-deepseek-2025-2\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">OpenAI's Deep Research tool</a>. The agent then drafts personalized responses and automatically routes support inquiries.</p><p>A human manager reviews the agent's work in Slack and provides feedback that helps the system learn Vercel's tone and improve over time.</p><p>Since its deployment, the agent has helped Vercel downsize the 10-person team to just one person who oversees it. The remaining nine were moved to outbound prospecting roles, which Grosser says is higher-value, more complex sales work.</p><p>\"Modeling after the top-performing employees has always been a standard business practice. The difference now is that technology lets us accelerate it,\" said David Totten, an alum of Databricks and Microsoft, who just joined Vercel as its new vice president of global field engineering on Monday.</p><p>The approach to training AI, Grosser said, is not unlike how companies once trained interns — especially ones they hoped to hire full-time.</p><p>\"You wouldn't put an intern with somebody who was not showing up to work every day, didn't have the right attitude, didn't understand the vision of the company,\" Grosser said. \"You put an intern with your best performers.\"</p><p>Grosser and Totten both said the goal of Vercel's internal AI strategy isn't to downsize the workforce. They said the company's head count has actually grown in the past year.</p><p>Vercel has six AI agents now deployed, but aims to deploy hundreds within the next 6 to 12 months — all modeled off of top performers.</p><p>Grosser said the company has already identified specific traits that define a strong use case for agents: replicable and deterministic, which means it consistently produces the same output for the same input.</p><p>What's left for humans, then, is the creative, intellectually challenging, and sometimes, ambiguous work.</p><p>\"My personal view is humans are capable of a lot more than most jobs allow them to do,\" Grosser said.</p>","contentLength":3507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oj0tgd/vercel_trained_an_ai_agent_on_its_best/"},{"title":"Modern Phoenix TUI - Beta Testers Needed","url":"https://www.reddit.com/r/golang/comments/1oj0o6i/modern_phoenix_tui_beta_testers_needed/","date":1761732955,"author":"/u/mistbow","guid":320641,"unread":true,"content":"<p>Built a new TUI framework for Go. Need beta testers before v0.1.0 stable.</p><p> Existing TUI frameworks had Unicode bugs sitting unfixed for months, slow PR reviews, and performance issues. Built this to solve those problems for my shell project (GoSh) <a href=\"https://github.com/grpmsoft/gosh\">https://github.com/grpmsoft/gosh</a>.</p><p> - 29,000 FPS rendering (no joke, measured) - Proper Unicode/emoji width calculation - Multi-module design (10 independent packages) - 90%+ test coverage - DDD + Rich model inpired architecture</p><p> API might change based on feedback. That's exactly why I need testers.</p><p>Install: <code>bash go get github.com/phoenix-tui/phoenix/tea@v0.1.0-beta.4 </code></p><p>Looking for people to try it in real projects and report issues. Thanks.</p>","contentLength":686,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Web Development In… Pascal?","url":"https://hackaday.com/2025/10/28/web-development-in-pascal/","date":1761732126,"author":"/u/self","guid":320637,"unread":true,"content":"<p>The site in question is the web store for his personal business, <a href=\"https://photronic.art/\" target=\"_blank\">Photronic Arts</a>, so you cannot say [jns] does not have skin in the game. From the front end, this is HTML and could be anything upto and including Shopify under the hood. It’s not, though: it’s a wholly custom backend [jns] put together in FreePascal, using the Lazarus IDE.</p><p>There’s <a href=\"https://hackaday.com/2025/09/02/the-case-for-pascal-55-years-on/\">a case to be made for Pascal in the modern day</a>, but when we wrote that we weren’t expecting to get tips about web development.&nbsp; Ironically enough [jns] spends so much time giving the technical details in this video he doesn’t delve that deeply into why he chose FreePascal, especially when it’s clear he’s very familiar with C and C++. In his associated <a href=\"https://gopher.floodgap.com/gopher/gw?gopher%3A%2F%2Fgopher.linkerror.com%3A70%2F0%2Fphlog%2F2025%2F20251023\" target=\"_blank\">writeup on his Gopher page (link though Floodgap)</a> [jns] simply declares it’s a language he’s quite fond of, which is reason enough of us. The <a href=\"https://linkerror.com/git/\" target=\"_blank\">source code is available</a>, though on request, to avoid AI scraping. It’s a sad but understandable response to these modern times.</p><p>If you’re not into web development and want to see a deep-dive into how the backend works, this video is worth watching even if you don’t particularly care for Pascal. It’s also worth watching if you do know backend development, and are Pascal-curious. If neither of those things interest you, what about this <a href=\"https://hackaday.com/2025/08/27/pascal-on-my-arduino-its-more-likely-than-you-think/\">Pascal Library for Arduino</a>?</p><p>Thanks to [jns] for the tip! If you’re doing modern work with questionably-modern tools, we call that a hack and <a href=\"https://hackaday.com/submit-a-tip/\">would love to hear from you.</a></p>","contentLength":1477,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oj0fos/web_development_in_pascal/"},{"title":"KDE is best Desktop Environment - Change my mind","url":"https://www.reddit.com/r/linux/comments/1oizwlt/kde_is_best_desktop_environment_change_my_mind/","date":1761730116,"author":"/u/Hencemann","guid":320638,"unread":true,"content":"<div><p>I tried variety of DEs - GNOME/Xfce/Lxde.. but KDE beats them all. Simple things like</p><ul><li>touchpad scroll speed adjustment, - is a simple UI slider (compare that to multiple commands on terminal for GNOME)</li><li>ease of multi monitor setup,</li><li>changing the wifi band to 5GHz was as simple as a few UI clicks. (simpler than windows!)</li><li>No bloat of a \"top bar\" that you have to use an extension to auto hide (and is super glitchy).</li><li>Taskbar that supports clean auto hide (even windows auto hide for the task bar is jittery)</li><li>Rarely have to open terminal for doing stuff - almost everything is available in GUI configs.</li></ul><p>It lets me focus on my work and when I do open a terminal it is for my development work - and not to fix OS stuff!</p><p>I'm never going back to any other DE.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Hencemann\"> /u/Hencemann </a>","contentLength":776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Endpoint Health Checker: reduce Service traffic errors during node failures","url":"https://github.com/kubeovn/endpoint-health-checker","date":1761729969,"author":"/u/oilbeater","guid":320664,"unread":true,"content":"<p>When a node dies or becomes partitioned, Pods on that node may keep showing as “ready” for a while, and kube-proxy/IPVS/IPTables can still route traffic to them. That gap can mean minutes of 5xx/timeouts for your Service. We open-sourced a small controller called Endpoint Health Checker that updates Pod readiness quickly during node failure scenarios to minimize disruption.</p><ul><li>Continuously checks endpoint health and <strong>updates Pod/endpoint status promptly</strong> when a node goes down.</li><li>Aims to  where traffic is still sent to unreachable Pods.</li><li>Works alongside native Kubernetes controllers; no API or CRD gymnastics required for app teams.</li></ul><p>If this solves a pain point for you—or if you can break it—please share results. PRs and issues welcome!</p>","contentLength":739,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1oizv82/endpoint_health_checker_reduce_service_traffic/"},{"title":"[Media] You can now propose your cat as changelog cat for Clippy 1.91!","url":"https://www.reddit.com/r/rust/comments/1oiyc7x/media_you_can_now_propose_your_cat_as_changelog/","date":1761723786,"author":"/u/Alexey-Semenyuk","guid":321042,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Alexey-Semenyuk\"> /u/Alexey-Semenyuk </a>","contentLength":38,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elon Musk's Grokipedia Pushes Far-Right Talking Points. The new AI-powered Wikipedia competitor falsely claims that pornography worsened the AIDS epidemic and that social media may be fueling a rise in transgender people.","url":"https://www.wired.com/story/elon-musk-launches-grokipedia-wikipedia-competitor/","date":1761721110,"author":"/u/esporx","guid":320696,"unread":true,"content":"<p> Musk’s <a href=\"https://www.wired.com/story/xai-grok-government-contract-hitler/\">xAI startup</a> launched Grokipedia, which the billionaire is pitching as an AI-generated alternative to the <a href=\"https://www.wired.com/story/wikipedia-finally-asking-big-tech-to-pay-up/\">crowdsourced encyclopedia</a> Wikipedia. Musk first announced the project in late September on his social media platform X, saying it would be “a massive improvement over Wikipedia,” and “a necessary step towards the xAI goal of understanding the Universe.”</p><p>Musk said last week that he had delayed the launch of Grokipedia because his team needed “to do more work to purge out the propaganda.” When Grokipedia eventually dropped on Monday, WIRED was initially unable to access the website and received an automated message that it was blocked.</p><p>When we finally got access to it, WIRED found that the online encyclopedia contained lengthy entries generated by AI. While many of the pages WIRED saw on launch day appeared fairly similar to Wikipedia in terms of tone and content, a number of notable Grokipedia entries denounced the mainstream media, highlighted conservative viewpoints, and sometimes perpetuated historical inaccuracies.</p><p>The Grokipedia entry about the slavery of African Americans in the US includes a section outlining numerous “<a data-offer-url=\"https://grokipedia.com/page/Slavery_in_the_United_States#ideological-justifications-for-slavery\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://grokipedia.com/page/Slavery_in_the_United_States#ideological-justifications-for-slavery&quot;}\" href=\"https://grokipedia.com/page/Slavery_in_the_United_States#ideological-justifications-for-slavery\" rel=\"nofollow noopener\" target=\"_blank\">ideological justifications</a>” made for slavery, including the “Shift from Necessary Evil to Positive Good.” The end of the entry focuses on criticisms of The 1619 Project, which it says incorrectly framed “slavery as the central engine of the nation's political, economic, and cultural development.”</p><p>Entries for more recent historical events put conservative perspectives at the center. When WIRED searched for “gay marriage” in Grokipedia, no entry popped up, but one of the on-screen suggestions was for “gay pornography” instead. This entry in Grokipedia falsely states that the proliferation of porn exacerbated the HIV/AIDS epidemic in the 1980s.</p><p>“This marked the onset of what would become a devastating crisis disproportionately affecting gay male communities, where behaviors idealized in pornography—such as unprotected receptive anal intercourse and multiple anonymous partners—aligned directly with primary transmission routes, leading to rapid seroconversion rates,” the Grokipedia entry claims.</p><p>xAI did not immediately return a request for comment.</p><p>The Grokipedia entry for “transgender” includes two mentions of “transgenderism,” a term commonly used to <a data-offer-url=\"https://glaad.org/transgenderism-definition-meaning-anti-lgbt-online-hate/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://glaad.org/transgenderism-definition-meaning-anti-lgbt-online-hate/&quot;}\" href=\"https://glaad.org/transgenderism-definition-meaning-anti-lgbt-online-hate/\" rel=\"nofollow noopener\" target=\"_blank\">denigrate trans people</a>. The entry also refers to trans women as “biological males” who have “generated significant conflicts, primarily centered on risks to women's safety, privacy, and sex-based protections established to mitigate male-perpetrated violence.” The opening section highlights social media as a potential \"contagion\" that is increasing the number of trans people.</p>","contentLength":2748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oixoow/elon_musks_grokipedia_pushes_farright_talking/"},{"title":"[D] What kind of live metrics would actually help you while training ML models?","url":"https://www.reddit.com/r/MachineLearning/comments/1oixifu/d_what_kind_of_live_metrics_would_actually_help/","date":1761720427,"author":"/u/traceml-ai","guid":320537,"unread":true,"content":"<p>What kind of live metrics would actually help you while training ML models?</p><p>I have been exploring real-time observability for ML training, things like seeing GPU memory, timing, and layer activity live instead of waiting for a job to fail or finish.</p><p>I built a small open-source experiment, TraceML, that currently runs on single-GPU PyTorch training and shows live memory + step timing.</p><p>I would love input from people who train models regularly, does having live metrics actually help you debug or optimize?</p><p>What kind of signals would you want to see next? • Multi-GPU utilization / imbalance • Data-loader or transfer bottlenecks • Gradient instability • Throughput (tokens/sec, batches/sec) • Cost or energy estimates</p><p>Curious what would make something like this genuinely useful ? </p>","contentLength":788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quitting my job to work as a consultant/freelancer.","url":"https://www.reddit.com/r/golang/comments/1oixice/quitting_my_job_to_work_as_a_consultantfreelancer/","date":1761720416,"author":"/u/Lonely_Positive2169","guid":320640,"unread":true,"content":"<p>Hello there, I'm a working professional (backend dev) based out of India wanting to quit my job to start as a consultant or a freelancer. What do you guys think would be a good showcase for me in this scenario. Not shy in working under someone else for some time to gain experience. Thankyou in advance. If anyone wants to join me in this journey is welcome.</p>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"First Look at Java Valhalla: Flattening and Memory Alignment of Value Objects","url":"https://open.substack.com/pub/joemwangi985269/p/first-look-at-java-valhalla-flattening?r=2m1w1p&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false","date":1761718450,"author":"/u/joemwangi","guid":320536,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oix17s/first_look_at_java_valhalla_flattening_and_memory/"},{"title":"DevPulse - Your LLM powered day’s work journal, automatically written","url":"https://www.reddit.com/r/golang/comments/1oiwrr6/devpulse_your_llm_powered_days_work_journal/","date":1761717431,"author":"/u/Independent_Grand822","guid":320538,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Independent_Grand822\"> /u/Independent_Grand822 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YAML hell?","url":"https://www.reddit.com/r/kubernetes/comments/1oiwqcm/yaml_hell/","date":1761717286,"author":"/u/the-creator-platform","guid":320534,"unread":true,"content":"<p>I am genuinely curious why I see constant complaints about \"yaml hell\" and nothing has been done about it. I'm far from an expert at k8s. I'm starting to get more serious about it, and this is the constant rhetoric I hear about it. \"Developers don't want to do yaml\" and so forth. Over the years I've seen startups pop up with the exact marketing \"avoid yaml hell\" etc. and yet none have caught on, clearly.</p><p>I'm not pitching anything. I am genuinely curious why this has been a core problem for as long as I've known about kubernetes. I must be missing some profound, unassailable truth about this wonderful world. Is it not really that bad once you're an expert and most that don't put in the time simply complain?</p><p>Maybe an uninformed comparison here, but conversely terraform is hailed as the greatest thing ever. \"ooo statefulness\" and the like (i love terraform). I can appreciate one is more like code than the other, but why hasn't kubernetes themselves addressed this apparent problem with something similar; as an opt-in? Thanks</p>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bookmark Manager API","url":"https://www.reddit.com/r/golang/comments/1oiv5pd/bookmark_manager_api/","date":1761711642,"author":"/u/Haron_1996","guid":320508,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Haron_1996\"> /u/Haron_1996 </a>","contentLength":33,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OBSBOT Tiny 2 Lite 4K Control","url":"https://github.com/bloopybae/obsbot-control-linux/tree/main","date":1761711510,"author":"/u/HelloBloop","guid":320880,"unread":true,"content":"<p>I’ve been hacking away on a repo that started life as a fork of another OBSBOT control tool but it’s evolved. My version is now tailored specifically for the , with <strong>full support for all its major features</strong> it's got:</p><ul><li>Seamless AI tracking + HDR support</li><li>KDE/Plasma theme awareness (tested on Plasma 6.5)</li><li>Built &amp; tested on Arch Linux 6.17.5</li></ul><p>I’m hoping to get some more eyes (and distros) on it. So, if you’re running Debian, Fedora, or anything else, I’d love your feedback or contributions!</p><p>I plan to add in-app color correction, filters, and other creative controls so you can make your webcam feed look real snazzy.</p>","contentLength":618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1oiv4bk/obsbot_tiny_2_lite_4k_control/"},{"title":"Go + HTMX Starter Kit V2","url":"https://www.reddit.com/r/golang/comments/1oiursb/go_htmx_starter_kit_v2/","date":1761710387,"author":"/u/MinimumT3N","guid":319630,"unread":true,"content":"<p>A while back, I shared a small starter kit I built while learning Go and exploring its ecosystem. Since then, I’ve completely refactored and improved it — and I’m really proud of how far it’s come. It’s now cleaner, easier to use, and much more extensible.</p>","contentLength":266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The New Java Best Practices by Stephen Colebourne","url":"https://youtube.com/watch?v=4sjJmKXLnuY","date":1761706805,"author":"/u/BlueGoliath","guid":320507,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oitmh6/the_new_java_best_practices_by_stephen_colebourne/"},{"title":"[Media] The 1.91 Clippy Changelog Cat Contest is open","url":"https://www.reddit.com/r/rust/comments/1oira11/media_the_191_clippy_changelog_cat_contest_is_open/","date":1761700138,"author":"/u/NothusID","guid":320818,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Go as memory safe as Rust?","url":"https://www.reddit.com/r/golang/comments/1oiqyu3/is_go_as_memory_safe_as_rust/","date":1761699279,"author":"/u/trymeouteh","guid":319523,"unread":true,"content":"<p>As the title says. Is Go as memory safe as Rust? And if so, why is Rust the promoted language for memory safety over Go?</p>","contentLength":120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kudos to Python Software Foundation. I just made my first donation","url":"https://www.theregister.com/2025/10/27/python_foundation_abandons_15m_nsf/","date":1761694593,"author":"/u/elgringo","guid":319491,"unread":true,"content":"<p>The Python Software Foundation (PSF) has walked away from a $1.5 million government grant and you can blame the Trump administration's war on woke for effectively weakening some open source security.&nbsp;</p><p>The programming non-profit's deputy executive director Loren Crary <a href=\"https://pyfound.blogspot.com/2025/10/NSF-funding-statement.html\" rel=\"nofollow\">said</a> in a blog post today that the National Science Foundation (NSF) had offered $1.5 million to address structural vulnerabilities in Python and the Python Package Index (PyPI), but the Foundation quickly became dispirited with <a href=\"https://nsf-gov-resources.nsf.gov/files/cafatc-525.pdf\" rel=\"nofollow\">the terms</a> of the grant it would have to follow.&nbsp;</p><p>\"These terms included affirming the statement that we 'do not, and will not during the term of this financial assistance award, operate any programs that advance or promote DEI [diversity, equity, and inclusion], or discriminatory equity ideology in violation of Federal anti-discrimination laws,'\" Crary noted. \"This restriction would apply not only to the security work directly funded by the grant, but to any and all activity of the PSF as a whole.\"</p><p>To make matters worse, the terms included a provision that if the PSF was found to have violated that <a href=\"https://www.theregister.com/2025/02/03/trump_admin_scrubs_dei_websites/\">anti-DEI</a> diktat, the NSF reserved the right to claw back any previously disbursed funds, Crary explained.&nbsp;</p><p>\"This would create a situation where money we'd already spent could be taken back, which would be an enormous, open-ended financial risk,\" the PSF director added.</p><p>The PSF's mission <a href=\"https://www.python.org/psf/mission/\" rel=\"nofollow\">statement</a> enshrines a commitment to supporting and growing \"a diverse and international community of Python programmers,\" and the Foundation ultimately decided it wasn't willing to compromise on that position, even for what would have been a solid financial boost for the organization.&nbsp;</p><p>\"The PSF is a relatively small organization, operating with an annual budget of around $5 million per year, with a staff of just 14,\" Crary added, noting that the $1.5 million would have been the largest grant the Foundation had ever received - but it wasn't worth it if the conditions were undermining the PSF's mission.&nbsp;</p><p>The PSF board voted unanimously to withdraw its grant application.</p><p>The non-profit would've used the funding to help prevent supply chain attacks; create a new automated, proactive review process for new PyPI packages; and make the project's work easily transferable to other open-source package managers.&nbsp;</p><p>Crary told  in a message that she's disappointed not to have been able to undertake the security work proposed in the grant, and she agreed that NSF is harming its own ability to fund quality scientific research with the DEI restriction in its grant terms.&nbsp;</p><p>\"Part of the problem here is all the uncertainties,\" Crary told us. \"Even if we wanted to give up anything that might be considered [DEI] work - which we don't - part of the risk here is that all these restrictions are new, the language is very broad ... I had no interest in being the test case.\"&nbsp;</p><p>Crary is confident in the group's choice and credits the Python community for standing behind the Foundation.</p><p>\"Trusting that our community would stand with our decision made it much easier,\" Crary remarked. \"And the support we've seen today in response to announcing the decision has proven that to be true.\"&nbsp;</p><p>The PSF isn't the first tech foundation to withdraw from an NSF grant due to the anti-DEI pledge.&nbsp;</p><p>The Carpentries, a nonprofit group that provides software engineering and data science training to researchers, was also in line to receive a $1.5 million grant from NSF but <a href=\"https://carpentries.org/blog/2025/06/announcing-withdrawal-of-nsf-pose-proposal/\" rel=\"nofollow\">withdrew</a> its application in June for the exact same reason as the PSF.</p><p>\"The Carpentries showed real leadership making their decision,\" Crary said.</p><p>We reached out to the NSF for comment on the matter, but only received an automated response telling us that, due to the ongoing <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.bbc.com/news/articles/c4g7d9j7p5qo\">government shutdown</a>, no one was around to field our questions.&nbsp;®</p>","contentLength":3782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oip7cw/kudos_to_python_software_foundation_i_just_made/"},{"title":"Red Hat to distribute NVIDIA CUDA across Red Hat AI, RHEL and OpenShift","url":"https://www.reddit.com/r/linux/comments/1oinznp/red_hat_to_distribute_nvidia_cuda_across_red_hat/","date":1761691568,"author":"/u/fenix0000000","guid":319592,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/fenix0000000\"> /u/fenix0000000 </a>","contentLength":35,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TIL: SymPy can generate Rust code","url":"https://www.reddit.com/r/rust/comments/1oimibd/til_sympy_can_generate_rust_code/","date":1761687992,"author":"/u/intersecting_cubes","guid":319614,"unread":true,"content":"<p>This is really helpful because a lot of people in math or data science in your company/project probably know and like Python. Now you can easily translate their Python into Rust. This has personally saved me a few hours of work already at my dayjob. I hope someone else here finds this useful.</p>","contentLength":293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux saved my old beat up computer from Windows 11!!","url":"https://www.reddit.com/r/linux/comments/1oime0n/linux_saved_my_old_beat_up_computer_from_windows/","date":1761687723,"author":"/u/Hot-Milk-Habitat","guid":319522,"unread":true,"content":"<p>I have this broken little computer, it fails every 2 windows updates. I have a new computer now, and I use that one most of the time.</p><p>I wanted to test out Linux and I remembered my old computer, so I got Linux Mint Cinnamon on it to test it out.</p><p>My computer, which came with Windows 11, had a keyboard that straight up didn't work. But I get Linux Mint and, lo and behold, the keyboard started working again! When I was in Windows 11, the keyboard wouldn't work no matter how many times I reloaded, reinstalled, and otherwise try to get the driver to work. I was going to give up on that computer but I'm really glad I didn't. Now I'm hopping between distros and messing with it.</p><p>All these new Windows updates keep trying to push AI up my nostrils and it's pissing me off so I'm probably going to switch my main computer over to Linux once I find a good distro :D</p><p>Feels like Linux actually wants me to have a customized user experience, which is nice!!!</p>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Docker Internals: Building a Container Runtime in Python","url":"https://muhammadraza.me/2024/building-container-runtime-python/","date":1761685677,"author":"/u/mraza007","guid":319433,"unread":true,"content":"<p>I’ve been working with containers professionally for several years now, using Docker and Kubernetes daily in production environments. Like many developers, I initially treated containers as a “black box” - I knew how to use them, but didn’t really understand what was happening under the hood. It wasn’t until I needed to debug a particularly container networking issue at work that I realized I needed to understand the underlying technology better.</p><p>In this post, I’ll take you on a journey to breakdown container technology by building a simple container runtime in Python. We’ll explore the Linux primitives that make containers possible and implement them step by step. By the end, you’ll understand how containers work.</p><h2>What Actually IS a Container?</h2><p>Before we start building, let’s clear up a common misconception: <strong>containers are NOT lightweight virtual machines</strong>. This comparison, while convenient for explaining containers to newcomers, is technically misleading.</p><p>A virtual machine includes an entire operating system with its own kernel. Containers, on the other hand, share the host’s kernel and use Linux features to create isolated environments. Specifically, containers are built on three main Linux primitives:</p><ol><li> - Provide isolation (process, network, filesystem, etc.)</li><li> - Limit and monitor resource usage (CPU, memory, I/O)</li><li> - Use chroot/pivot_root to change the root filesystem</li></ol><p>When you run , Docker is essentially:</p><ul><li>Creating namespaces to isolate the process</li><li>Setting up cgroups to limit resources</li><li>Using an overlay filesystem to provide the Ubuntu root filesystem</li><li>Executing  in this isolated environment</li></ul><p>Let’s build this ourselves to see exactly how it works.</p><h2>Understanding Linux Namespaces</h2><p>Namespaces are a Linux kernel feature that partitions kernel resources. Different processes can have different views of the system. Linux provides several types of namespaces:</p><ul><li> - Process isolation. Processes in a namespace only see processes within that namespace.</li><li> - Network isolation. Each namespace has its own network devices, IP addresses, routing tables.</li><li> - Filesystem isolation. Each namespace can have its own mount points.</li><li> - Hostname isolation. Each namespace can have its own hostname.</li><li> - Inter-process communication isolation.</li><li> - User and group ID isolation.</li></ul><p>Let’s start by implementing the simplest form of isolation: PID namespaces.</p><h2>Building Our Container Runtime</h2><h3>Step 1: Basic Process Isolation with PID Namespaces</h3><p>Let’s create our first container that isolates processes:</p><div><div><pre><code></code></pre></div></div><p>Save this as  and run it:</p><div><div><pre><code>python3 simple_container.py bash\n</code></pre></div></div><p>Inside the container, try running:</p><div><div><pre><code>ps aux  </code></pre></div></div><p>This is our first step towards a container - we’ve isolated the process tree!</p><h3>Step 2: Filesystem Isolation with chroot</h3><p>Now let’s add filesystem isolation. We’ll create a minimal root filesystem and use  to change the root directory:</p><div><div><pre><code></code></pre></div></div><p>Now when you run this, you’ll have a container with:</p><ul></ul><div><div><pre><code>python3 container_v2.py bash\n</code></pre></div></div><p>Try these commands inside:</p><div><div><pre><code> /      \nps aux    </code></pre></div></div><h3>Step 3: Resource Limits with cgroups</h3><p>Now let’s add resource limits using cgroups (control groups). This is what prevents a container from consuming all system resources:</p><div><div><pre><code></code></pre></div></div><p>To test the memory limit, inside the container try:</p><div><div><pre><code>\npython3 </code></pre></div></div><p>The process should be killed when it exceeds the memory limit!</p><p>Now let’s put everything together into a complete, production-like container runtime:</p><div><div><pre><code></code></pre></div></div><h2>Testing Your Container Runtime</h2><p>To test this, you’ll need a root filesystem. Here’s how to create a minimal one using an existing Docker image:</p><div><div><pre><code>alpine_rootfs\n\n\ndocker docker create alpine:latest |  alpine_rootfs  -\n\n\nwget https://dl-cdn.alpinelinux.org/alpine/v3.18/releases/x86_64/alpine-minirootfs-3.18.0-x86_64.tar.gz\nalpine_rootfs\n alpine-minirootfs-3.18.0-x86_64.tar.gz  alpine_rootfs\n\npython3 container.py run alpine_rootfs sh\n</code></pre></div></div><p>Inside the container, you can verify isolation:</p><div><div><pre><code>\nps aux\n\n /\n\n /sys/fs/cgroup/memory.max\n</code></pre></div></div><h2>What We Built vs. What Docker Does</h2><p>Our container runtime demonstrates the core concepts, but production container runtimes like Docker/containerd do much more:</p><ul><li>Process isolation (PID namespaces)</li><li>Filesystem isolation (mount namespaces + chroot)</li><li>Resource limits (cgroups v2)</li><li>Hostname isolation (UTS namespace)</li></ul><ul><li>: Layered filesystems using overlay2/AUFS</li><li>: Pulling images from registries</li><li>: Bridge networks, overlay networks, port mapping</li><li>: Persistent storage with bind mounts and volumes</li><li>: seccomp profiles, AppArmor/SELinux, capability dropping</li><li><strong>Container Orchestration APIs</strong>: REST API for managing containers</li><li>: stdout/stderr capture, metrics collection</li><li>: Container health monitoring</li><li>: Automatic restart on failure</li></ul><h2>Understanding the Security Implications</h2><p>It’s crucial to understand that our simple implementation lacks many security features:</p><ol><li><p>: Our containers run as root. Production containers should use user namespaces to map container root to unprivileged users.</p></li><li><p>: We don’t restrict system calls. Docker uses seccomp profiles to block dangerous syscalls.</p></li><li><p>: Our containers have all Linux capabilities. Docker drops most by default.</p></li><li><p>: No mandatory access control.</p></li></ol><p>These missing features are why you should never use this implementation in production!</p><p>By building this container runtime, we’ve demystified how containers actually work. They’re not magic - they’re clever applications of Linux kernel features that have existed for years:</p><ul><li> (2002-2013): Provide isolation</li><li> (2007): Provide resource limiting</li><li> (1979!): Provides filesystem isolation</li></ul><p>Docker’s innovation wasn’t inventing these technologies - it was packaging them into an easy-to-use tool with great developer experience.</p><p>Understanding these fundamentals makes you a better DevOps engineer. When things go wrong in production, you’ll know where to look. When you need to optimize container performance, you’ll understand the levers you can pull.</p><p>If you enjoyed this deep dive, here are resources to continue learning:</p><ul><li>: , </li><li>: The standard container runtime specification</li><li>: Docker’s actual container runtime</li><li>: Linux containers project - the original container tech</li></ul><p>In a future post, I might explore:</p><ul><li>Implementing container image layers with overlay filesystems</li><li>Building container networking from scratch (veth pairs, bridges, NAT)</li><li>Creating a simple container orchestrator (mini-Kubernetes)</li></ul><p>Let me know in the comments what you’d like to see next!</p><ul><li>If you’re interested in more content like this, I post regularly about DevOps, Python, and systems programming. Follow me on <a href=\"https://twitter.com/muhammad_o7\">Twitter/X</a> for updates.</li><li>I’m available for Python and DevOps consulting. If you need help with containerization, automation, or infrastructure, feel free to reach out via <a href=\"mailto:muhammadraza0047@gmail.com\">email</a>.</li></ul><p><em>If you share this on X, tag me <a href=\"https://twitter.com/muhammad_o7\">@muhammad_o7</a> - I’d love to see your thoughts! You can also connect with me on <a href=\"https://www.linkedin.com/in/muhammad-raza-07/\">LinkedIn</a>.</em></p><p><strong>Note: Want to be notified about posts like this? Subscribe to my RSS feed or leave your email <a href=\"https://forms.gle/M1EK61LLCxJ3iTiD7\">here</a></strong></p>","contentLength":6761,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oiliw9/understanding_docker_internals_building_a/"},{"title":"horse, a new approach to cd-ls, inspired by the zellij file picker","url":"https://github.com/if-not-nil/horse","date":1761681719,"author":"/u/qwool1337","guid":319384,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oijs4d/horse_a_new_approach_to_cdls_inspired_by_the/"},{"title":"OpenAI's goal: $1 trillion a year in infrastructure spending","url":"https://www.axios.com/2025/10/28/openai-1-trillion-altman","date":1761681703,"author":"/u/tekz","guid":320639,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oijrtr/openais_goal_1_trillion_a_year_in_infrastructure/"},{"title":"Last Call for NYC Kubernetes Meetup Tomorrow (10/29)","url":"https://www.reddit.com/r/kubernetes/comments/1oiire0/last_call_for_nyc_kubernetes_meetup_tomorrow_1029/","date":1761679397,"author":"/u/MutedReputation202","guid":319379,"unread":true,"content":"<p>We have a super cool session coming up tomorrow - guest speaker Valentina Rodriguez Sosa, Principal Architect at Red Hat, will be talking about \"Scaling AI Experience Securely with Backstage and Kubeflow.\" Please RSVP ASAP if you can make it: <a href=\"https://luma.com/5so706ki\">https://luma.com/5so706ki</a>. </p>","contentLength":270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Conferences/Workshops for publishing about open-source software/libraries?","url":"https://www.reddit.com/r/MachineLearning/comments/1oihs5e/d_conferencesworkshops_for_publishing_about/","date":1761677137,"author":"/u/fullgoopy_alchemist","guid":319380,"unread":true,"content":"<div><p>Are there any conferences/workshops that accept contributions in terms of open-source software or libraries for ML-based tasks? There is no research novelty involved, but the software helps researchers with their experiment pipelines.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/fullgoopy_alchemist\"> /u/fullgoopy_alchemist </a>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In Praise Of Useless Robots","url":"https://thereader.mitpress.mit.edu/in-praise-of-useless-robots/","date":1761675687,"author":"/u/bethany_mcguire","guid":319381,"unread":true,"content":"<div>The most intriguing robots aren’t built to work, but to make us imagine other worlds.</div><div><div><a href=\"https://www.beelinereader.com\" target=\"_blank\" rel=\"nofollow\">BeeLine Reader</a> uses subtle color gradients to help you read more efficiently.</div></div><p>A few months ago, I visited the Futurism retrospective organized to mark the 80th anniversary of Filippo Tommaso Marinetti’s death at the National Gallery in Rome. The rooms were filled with archives of an ultra-modernist machinic dream: posters, paintings, and sculptures coexisted alongside engines, telegraphs, cars, and airplanes. It was impossible to ignore the voice of the visionary ideologue of Futurism echoing through the sober neoclassical halls of the museum: “We affirm that the magnificence of the world has been enriched by a new beauty: the beauty of speed.”</p><p>For the Futurists, every machine was, essentially, a time-machine: more than tools designed to perform a specific task, technological objects were the historical embodiment of humanity’s universal drive toward progress. Looking beyond the notoriously controversial implications of its political affiliations, the genius of the Futurist avant-garde was its intuition that the evolution of machines could capture cultural transformations better than any other human practice. And thus, even the arts and letters — the highest expressions of humanism — had to listen to the roar of engines.</p><p>In October 2021, visitors to London’s Tate Modern entered a space populated by visions of a very different future. Floating, semi-transparent organisms hovered slowly in the air like seraphic creatures from the ocean’s depths. These , as they were baptized by Korean-American artist Anicka Yi, who conceived them, are pachydermic, calm, and silent. These flying automata respond to human presence, changing altitude and behavior based on the proximity of people in the space. “When you look at these aerobes, it gives you a feeling almost opposite to the uncanny valley”, <a href=\"https://youtu.be/HtGGg7J5z-A?si=eCZPv9NqKZLae9Ws\" target=\"_blank\" rel=\"nofollow\">Yi explains</a>. “You know that they’re mechanical, yet they feel palpably alive.” Unlike anthropomorphic robots, whose imperfect resemblance to humans often generates a sense of subtle unease (or “uncanniness”), Yi’s automata are neither disturbing nor reassuring, but designed to evoke a sense of sublime otherness, like swimming alongside a whale in the sea. Their scale is imposing, but their presence invites attention rather than fear. These artistic-technological objects are difficult to define: they are man-made artifacts but serve no instrumental function. They move around their environment guided by their perceptions, interacting with each other and the world around them. Yi describes them, <a href=\"https://press.uchicago.edu/ucp/books/book/distributed/C/bo3645022.html\" target=\"_blank\" rel=\"nofollow\">borrowing a term from Donna Haraway</a>, as “a new kind of companion species.” In Haraway’s terms, a “companion species” is not a familiar reflection of the human but a meaningful otherness, whose distance from us enables us to inhabit new forms of relation and coexistence.</p><figure><blockquote><p>There is something paradigmatic and powerful about robots that dominates our imagination of the future.</p></blockquote></figure><p>Robots are strange objects of inquiry. Whenever I encounter them in my research, I catch myself pondering the contrast between their negligible impact on my daily life and their imposing presence in my cultural imagination. Of course, industrial robots already serve significant purposes, but their usefulness does not entirely justify the appeal they hold towards us. Beyond their intended use, each robot, in its synthetic and self-contained individuality, seems to embody something like the quintessence of the technological object. There is something paradigmatic and powerful about these beings that dominates our imagination of the future. And while for a long time the word “robot” has corresponded to a very specific image (an anthropomorphic, mechanical, rigid artifact), Anicka Yi’s flying automata signal a broader change in how these technological objects are imagined and constructed.</p><p>Over the past 20 years, robotics has undergone a significant transition. A field once dominated by anthropomorphic bodies and rigid materials has begun to embrace a much wider range of possible incarnations, starting with the use of plastic and flexible materials to replace steel and hard polymers. Cecilia Laschi, one of the most authoritative figures in the field of robotics, <a href=\"https://www.science.org/doi/10.1126/scirobotics.aah3690\" target=\"_blank\" rel=\"nofollow\">has frequently emphasized how this transition from “rigid” to “soft” robots</a> goes far beyond a simple change in the choice of materials, reflecting instead a broader transition in the entire anatomy of the automata, in the strategies used to control them, and in the philosophy that drives their construction. The most notable engineering achievement by Laschi and her colleagues at the Sant’Anna Institute in Pisa is <a href=\"https://www.sciencedirect.com/science/article/pii/S0928493110003450\" target=\"_blank\" rel=\"nofollow\">a robotic arm originally designed in 2011 and inspired by the muscular hydrostatics of the octopus</a>. In octopuses, limbs are capable of complex behaviors such as swimming and manipulating objects through coordinated deformations of muscle tissue, without any need for rigid components. In the robot designed by Laschi and her colleagues, the robotic limb’s movement is achieved similarly through deformable smart materials known as “shape memory alloys.”</p><p>Unlike a conventional robot, these movements are not pre-programmed, but emerge from the material’s response to external forces. This new engineering logic is part of what Laschi describes as embodied intelligence, an approach in which the robot’s behavior emerges from integrating its physical structure and interaction with the world. The concept of “embodiment” challenges the hierarchical separation between body and mind, representation and experience. Rather than conceiving of intelligence as the product of an active mind controlling a passive body, embodiment emphasizes the relationship between cognition and corporeality. Originating in philosophy, over the last 20 years, this concept has begun to spread and establish itself in the field of engineering, opening up new avenues for the design of versatile and adaptive robots. “The octopus is a biological demonstration of how effective behavior in the real world is closely linked to body morphology,” <a href=\"https://www.sciencedirect.com/science/article/pii/S0928493110003450\" target=\"_blank\" rel=\"nofollow\">Laschi and her co-workers explain</a>, “a good example of embodied intelligence, whose principles derive from the observation in nature that adaptive behavior emerges from the complex and dynamic interaction between body morphology, sensorimotor control, and the environment.”</p><p>Back in Shanghai, some time after visiting the National Gallery, I visited a retrospective on Hajime Sorayama, a Japanese artist famous for having crystallized the image of the 20th-century robot into a simultaneously futuristic and nostalgic icon. Since the 1980s, Sorayama has reproduced the same sculpture endlessly with minimal variations: a slender, curvy female figure covered in chrome-plated armor, halfway between erotic fetish and deity of a long-lost future. These works, laconically branded “Sexy Robot” followed by a serial number, seem to both celebrate and poke fun at the modernist stereotype of the automaton as a triumph of control and mechanical efficiency, replicating the seductive mirage of glittering, endless progress. While Yi’s creatures project us into an alien and still nascent technological future, Sorayama’s figures are captivating precisely because they perfectly reflect our expectations. If Yi’s  are soft, sensitive, and radically non-human, Sorayama’s sexy robots are superhuman, dazzling, and wonderfully unfeeling.</p><p>Despite their radical aesthetic and philosophical distance, Sorayama’s and Yi’s robots have at least one thing in common — their uselessness. More specifically, they are both instruments of aesthetic contemplation rather than functional tools. Robots, for that matter, are often described in terms of what they can do: as artifacts designed to facilitate work, if not carry it out in our place. This conception, already spelled out in the familiar etymology of the word “robot” (from the Czech , “forced labor,” a term <a href=\"https://thereader.mitpress.mit.edu/origin-word-robot-rur/\">popularized by science fiction writer Karel Čapek</a>), is deeply entrenched in twentieth-century industrial culture. Still, even before being called by that name, robots weren’t always just tools in the service of human productivity.</p><p>In a 1964 article entitled , historian of technology Derek de Solla Price traced the history of automata from antiquity, challenging the notion that such machines were created primarily to serve human labor. Mechanisms, he observed, long functioned as epistemic devices before they became useful tools: they acted as microcosmic mirrors to the greater order of the world. For centuries, automata accompanied the human imagination, helping thinkers to conceive of a rational universe governed by regular mechanisms (well before such mechanisms could be practically put to work). This historical lineage complicates the view of the machine as a purely instrumental tool: its material utility was often secondary to its epistemic power. This non-utilitarian interest in robots has been emerging time and time again in art practices.</p><figure><blockquote><p>Robots weren’t always just tools in the service of human productivity.</p></blockquote></figure><p>In “After Care,” an installation recently exhibited at Copenhagen Contemporary, <a href=\"https://studiothinkinghand.com/\" target=\"_blank\" rel=\"nofollow\">artists Rhoda Ting and Mikkel Bojesen</a> set fully soft, pneumatically activated robots to wriggle and burrow inside a large pit of rocks and dirt. Visitors were invited to handle the robots as if in a “petting zoo” — engaging with them not as instruments but as alien “companion species,” valued more for their strangeness and presence than for any practical use.</p><p>The work of Cecilia Laschi and many other pioneers in robotics is already demonstrating that new soft robots can significantly expand the functionality, sustainability, and resilience of technologies from past centuries. However, beyond the still limited applications of these artifacts, the soft machines of the 21st century seem to signal a more complex transition, primarily epistemic and cultural, in our understanding of and relationship with the world.</p><p>Yi’s fluttering creatures and Ting and Bojesen’s wriggling mollusks exist in continuity with a long history in which technological artifacts were philosophical and cosmological devices before being tools programmed to perform a task. And if the automata of past centuries spoke of celestial spheres and universes orchestrated like clockwork, what worlds do the soft machines of the 21st century evoke? Even contemporary robots, whether swimming in engineering laboratories or floating in art galleries, are first and foremost cosmological mirrors, and the worlds they evoke are very different from those of their ancestors. The new automata, it seems, speak to us of ecological continuity, profound otherness, and possible coexistences with that which is most distant from us.</p><p><em> is a writer and researcher at the Center for AI Culture of NYU Shanghai. She is the author of “<a href=\"https://mitpress.mit.edu/books/parallel-minds\" target=\"_blank\" rel=\"noreferrer noopener\">Parallel Minds</a>” (Urbanomic Press). </em></p><p><em>This article first appeared on Laura’s Substack, <a href=\"https://substack.com/@softfutures\" target=\"_blank\" rel=\"nofollow\">Soft Futures</a>. </em></p>","contentLength":11061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1oih4yq/in_praise_of_useless_robots/"},{"title":"VoLTE - Linux Smartphone in Germany","url":"https://www.reddit.com/r/linux/comments/1oih3z8/volte_linux_smartphone_in_germany/","date":1761675625,"author":"/u/Fit-Barracuda575","guid":320745,"unread":true,"content":"<blockquote><p>Calling over VoLTE is not supported, please check that this is not a requirement in your country.</p></blockquote><p>As far as I can tell by searching the web, VoLTE is quite necessary in Germany. 3G has been disabled in 2021, 5G is mostly available, but 4G still seems to be the standard, while 2G is for emergencies. I don't feel confident with my research though.</p><p>I mostly use Smartphones for doing calls, text messages and threema/signal/telegram and the occasional internet research (I typically use way less then 1GB data per month).</p><ul><li>Do all mobiles use VoLTE for normal calls? </li><li>Does anybody use a linux phone in Germany and can share their experience?</li></ul>","contentLength":634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zorin OS 18 has already hit over 300,000 downloads","url":"https://www.reddit.com/r/linux/comments/1oih2hv/zorin_os_18_has_already_hit_over_300000_downloads/","date":1761675526,"author":"/u/taloSilva2005","guid":319383,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Apache Fory™ Rust: A Versatile Serialization Framework with trait objects, shared refs and schema evolution support","url":"https://fory.apache.org/blog/2025/10/29/fory_rust_versatile_serialization_framework","date":1761674102,"author":"/u/Shawn-Yang25","guid":320597,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1oigfaz/introducing_apache_fory_rust_a_versatile/"},{"title":"Grafana Tempo Users, A few questions...","url":"https://www.reddit.com/r/golang/comments/1oig604/grafana_tempo_users_a_few_questions/","date":1761673529,"author":"/u/H1Supreme","guid":319312,"unread":true,"content":"<p>Hey all, hope this is an ok place to post this question. I'm working on implementing Tempo as a backend for storing traces (from opentelemetry), and I'm wondering how everyone is writing queries from a Go application.</p><p>To give some context, this is an existing dashboard application that already has visualization in place. So, I don't need Grafana, or any other visualization tool. Which is what most of the docs suggest using. </p><p>I already have Prometheus in place (using the Go Client for queries), and was hoping Tempo would be as easy to implement. But, it's proving to be a bit more difficult to determine the correct path. It's seems like I have two options:</p><p>The SDK seems easy enough to understand, generally speaking, but there aren't any examples for a simple connection (no idea how to set the port Tempo is listening on). So, I don't know if I should even consider this.</p><p>That leaves gRPC or HTTP. Which is fine, but I'm not sure if it's the right approach.</p><p>So, my question is: For those of you who aren't using 3rd party visualization tools, how are you querying Tempo?</p><p>Bonus question: Any alternatives I should consider? I'm new to opentelemetry traces, and chose Tempo based on my initial research. Only tool that's already crossed of the list is Elasticsearch.</p>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What We Talk About When We Talk About Sideloading | F-Droid","url":"https://f-droid.org/en/2025/10/28/sideloading.html","date":1761673149,"author":"/u/guihkx-","guid":319382,"unread":true,"content":"<p>We recently published a <a href=\"https://f-droid.org/en/2025/09/29/google-developer-registration-decree.html\">blog\npost</a>\nwith our reaction to the new Google Developer Program and how it impacts\nyour freedom to use the devices that you own in the ways that you want. The\npost garnered quite a lot of feedback and interest from the community and\npress, as well as various civil society groups and regulatory agencies.</p><p>In this post, I hope to clarify and expand on some of the points and rebut\nsome of the counter-messaging that we have witnessed.</p><h3>Google’s message that “Sideloading is Not Going Away” is clear, concise, and false</h3><p>Shortly after our post was published, Google aired an\n<a href=\"https://www.youtube.com/watch?v=A7DEhW-mjdc&amp;t=613s\">episode</a> of their\nAndroid Developers Roundtable series, where they state unequivocally that\n“sideloading isn’t going anywhere”. They follow-up with a <a href=\"https://android-developers.googleblog.com/2025/09/lets-talk-security-answering-your-top.html\">blog\npost</a>:</p><blockquote><p><em><strong>Does this mean sideloading is going away on Android?</strong> Absolutely not. Sideloading is fundamental to Android and it is not going away.</em></p></blockquote><p>This statement is untrue. The developer verification decree effectively ends\nthe ability for individuals to choose what software they run on the devices\nthey own.</p><p>It bears reminding that “sideload” is a made-up term. Putting software on\nyour computer is simply called “installing”, regardless of whether that\ncomputer is in your pocket or on your desk. This could perhaps be further\nprecised as “ installing”, in case you need to make a distinction\nbetween obtaining software the old-fashioned way versus going through a\nrent-seeking intermediary marketplace like the Google Play Store or the\nApple App Store.</p><p>Regardless, the term “sideload” was coined to insinuate that there is\nsomething dark and sinister about the process, as if the user were making an\nend-run around safeguards that are designed to keep you protected and\nsecure. But if we reluctantly accept that “sideloading” is a term that has\nwriggled its way into common parlance, then we should at least use a\nconsistent definition for it. Wikipedia’s summary\n<a href=\"https://en.wikipedia.org/wiki/Sideloading\">definition</a> is:</p><blockquote><p><em>the transfer of apps from web sources that are not vendor-approved</em></p></blockquote><p>By this definition, Google’s statement that “sideloading is not going away”\nis simply . The vendor — Google, in the case of Android certified\ndevices — will, in point of fact, be approving the source. The supplicant\napp developer must register with Google, pay a fee, provide government\nidentification, agree to non-negotiable (and ever-changing) terms and\nconditions, enumerate all their current and future application identifiers,\nupload evidence of their private signing key, and then hope and wait for\nGoogle’s approval.</p><h3>What this means for your rights</h3><p>You, the consumer, purchased your Android device believing in Google’s\npromise that it was an open computing platform and that you could run\nwhatever software you choose on it. Instead, starting next year, they will\nbe non-consensually pushing an update to your operating system that\nirrevocably blocks this right and leaves you at the mercy of their judgement\nover what software you are permitted to trust.</p><p>You, the creator, can no longer develop an app and share it directly with\nyour friends, family, and community without first seeking Google’s\napproval. The promise of Android — and a marketing advantage it has used to\ndistinguish itself against the iPhone — has always been that it is\n“open”. But Google clearly feels that they have enough of a lock on the\nAndroid ecosystem, along with sufficient regulatory capture, that they can\nnow jettison this principle with prejudice and impunity.</p><p>You, the state, are ceding the rights of your citizens and your own digital\nsovereignty to a company with a track record of complying with the\nextrajudicial demands of authoritarian regimes to remove perfectly legal\napps that they happen to dislike. The software that is critical to the\nrunning of your businesses and governments will be at the mercy of the\nopaque whims of a distant and unaccountable corporation. Monocultures are\nperilous not just in agriculture, but in software distribution as well.</p><p>As a reminder, this applies not just to devices that exclusively use the\nGoogle Play Store: this is for  Android Certified device \nin the world, which encompasses over 95% of all Android devices outside of\nChina. Regardless of whether the device owner prefers to use a competing app\nstore like the Samsung Galaxy Store or the Epic Games Store, or a free and\nopen-source app repository like F-Droid, they will be captive to the\noverarching policies unilaterally dictated by a competing corporate entity.</p><h3>The place of greater safety</h3><p>In promoting their developer registration program, Google\n<a href=\"https://android-developers.googleblog.com/2025/08/elevating-android-security.html\">purports</a>:</p><blockquote><p><em>Our recent analysis found over 50 times more malware from internet-sideloaded sources than on apps available through Google Play.</em></p></blockquote><p>We haven’t seen this recent analysis — or any other supporting evidence —\nbut the “50 times” multiple does certainly sound like great cause for\ndistress (even if it is a surprisingly round number). But given the recent\n<a href=\"https://www.malwarebytes.com/blog/news/2025/09/224-malicious-apps-removed-from-the-google-play-store-after-ad-fraud-campaign-discovered\">news</a>\nof “224 malicious apps removed from the Google Play Store after ad fraud\ncampaign discovered”, we are left to wonder whether their energies might\nbetter be spent assessing and improving their own safeguards rather than\ncasting vague disparagements against the software development communities\nthat thrive outside their walled garden.</p><p>In addition, other recent\n<a href=\"https://www.theregister.com/2025/08/26/apps_android_malware/\">news</a> of over\n19 million downloads of malware from the Play Store leads us to question\nwhether the sole judgement of a single corporate entity can be trusted to\nidentify and assess malware, especially when that judgement is clouded by\ncommercial incentives that may not align with the well-being of their users.</p><p>Google has been facing public outcry against their heavy-handed policies for\na long time, but this trend has accelerated recently. Last year they\n<a href=\"https://arstechnica.com/gadgets/2024/08/chromes-manifest-v3-and-its-changes-for-ad-blocking-are-coming-real-soon/\">crippled\nad-blockers</a>\nin Chrome and Chromium-based browsers by forcing through their unpopular\n“manifest v3” requirement for plugins, and earlier this year they <a href=\"https://arstechnica.com/gadgets/2025/03/google-makes-android-development-private-will-continue-open-source-releases/\">closed\noff</a>\nthe development of the Android Open Source Project (AOSP), which is how they\nwere able to clandestinely implement the verification infrastructure that\nenforces their developer registration decree.</p><p>Developer verification is an existential threat to free software\ndistribution platforms like F-Droid as well as emergent commercial\ncompetitors to the Play Store. We are witnessing a groundswell of opposition\nto this attempt from both our user and developer communities, as well as the\ntech press and civil society groups, but public policymakers still need to\nbe educated about the threat.</p><p>To learn more about what you can do as a consumer, visit\n<a href=\"https://keepandroidopen.org\">keepandroidopen.org</a> for information on how to\ncontact your representative agencies and advocate for keeping the Android\necosystem open for consumers and competition.</p><p>If you are an app developer, we recommend against signing yourself up for\nGoogle’s developer registration program at this time. We unequivocally\nreject their attempt to force this program upon the world.</p><p>Over half of all humankind uses an Android smartphone. Google does not own\nyour phone. You own your phone. You have the right to decide who to trust,\nand where you can get your software from.</p>","contentLength":7149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1oifzu1/what_we_talk_about_when_we_talk_about_sideloading/"},{"title":"Burn 0.19.0 Release: Quantization, Distributed Training, and LLVM Backend","url":"https://www.reddit.com/r/rust/comments/1oiexhr/burn_0190_release_quantization_distributed/","date":1761670801,"author":"/u/ksyiros","guid":319413,"unread":true,"content":"<p>Our goals this year with Burn were to support large-scale training and quantized model deployment. This release marks a significant advancement in that direction. As a reminder, Burn is a Tensor Library and Deep Learning Framework for both training and inference.</p><p>We had to rethink several core systems to achieve true multi-GPU parallelism:</p><ul><li> To support concurrent tasks running simultaneously on a single GPU (like compute and data transfer), we had to support multiple compute queues called streams. For a simple API to declare multiple streams, we simply attach compute streams to Rust threads using a pool.</li><li><strong>Redesigned Locking Strategies:</strong> We created a global device lock shared between multiple subsystems, like the fusion runtime, the CubeCL compute runtime, and autotuning. The new lock ensures that no deadlock is possible. The lock doesn't have a negative performance impact since locking is only used for task registration that ensures order of execution, compute is executed outside of the lock. The autodiff system doesn't share the same locking strategy, as a single graph can be executed on many GPUs. Therefore, we simply adopted a fine-grained locking strategy where different graphs can be executed in parallel.</li><li><strong>Distributed Training Infrastructure:</strong> We introduced burn-collective for gradient synchronization and refactored our training loop to support different distributed training strategies. The performance of some of our algorithms is still lacking, but naive multi-device training still reduces training time by a significant factor, leveraging almost all GPUs at all times.</li></ul><p>We also added comprehensive quantization support with persistent memory optimization, allowing models to use significantly less memory. Persistent memory leverages the fact that some tensors are less likely to change in size during execution and creates memory pools configured for their specific sizes. With Burn 0.19.0, module parameters are tagged as such, since in most neural networks, the size of the parameters doesn't change during training or inference. This setting can be turned off if it doesn't work well with your models.</p><p>Just to visualize the memory gains possible, here are the results with a LLAMA 1B model:</p><p>Finally, we introduced a new CPU backend powered by MLIR and LLVM, bringing the same JIT compilation, autotuning, and fusion capabilities from our GPU backends to CPU execution. The performance of the CubeCL runtime is great, but most of our algorithms aren't optimized for CPU yet, so the Burn backend is still quite slow.</p><p> With the new CubeCL CPU runtime and LLVM compiler, we essentially created an alternative Rust compiler, though with drastically different compilation characteristics.</p><p>There are many more improvements in this release beyond these highlights, and we wrote a post to cover them. Don't hesitate to skim it and refer to it for the migration guide. </p>","contentLength":2880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Type Club - Understanding typing through the lens of Fight Club","url":"https://revelry.co/insights/type-club/","date":1761670445,"author":"/u/AltruisticPrimary34","guid":319414,"unread":true,"content":"<p>Okay, so yes I am going to break a couple of rules here. But really only the first two…</p><p>Types are literally everywhere; they are the only way digital computing actually functions and does what we expect. Yet they are entirely a construct of our own imaginations. If you are willing I’d love to take you on a little journey involving a “single serving friend,” a “puzzle” and a healthy dose of editorialization.</p><h2>Jack’s Budding Self-Awareness</h2><p>Imagine you are headed home from a business trip. You wake up from a nap and notice someone different sitting next to you; they start reading the safety card which you take as an opening for a conversation. The topic naturally gets around to careers. They tell you they are a software engineer. Right before you land, you work up the courage to ask them about programming. They give you their business card; you flip it over and see a bunch of garbage written on the back of it.</p><pre><code>010011010101000001010011001101110000000100000000000000000000000001000111000000000101001100001001001001111101000100111001011001111100000001000101110110011001000100100001010000001000001011100010001100011101011011010111001011101000110100000001010101000110110101110001011010100001010010100010101100001111010111010100101110111000111111100010010000001000100111001001000010010111001110000001100100110110101000000010010100101110000111110100101110110011001110101111110100110111100101000001010000111001000010001001</code></pre><p>Before you can ask what the hell it is supposed to be they get up and go sit somewhere else on the plane.</p><p>Exhausted, you head back to your apartment. Instead, you discover a plot device – your apartment has exploded.</p><p>Through your shock, you vaguely hear someone asking if you have anyone you can call. For some reason, you take the card out of your pocket and call that single serving friend you met briefly on the plane. Your nerves could really use a drink, so you end up deciding to meet at a local bar. Somehow, you manage to forget about your smoking crater of an apartment, and your curiosity starts to take over again. You ask about the gobbledegook on the back of the business card. The stranger laughs and says:</p><p>I’ll give you a hint. It is the beginning of an accounting ledger – well more of a log  – but ultimately just financial data. Doesn’t look like much of anything unless you know how to break it down and put it back together. You can figure it out if you know the correct assumptions about the data. Let’s call those assumptions types.</p><p>Let’s start at the root with the raw data. After all it’s only after we’ve lost everything that we’re free to do anything. In any digital computer your atom is the bit. This is about as simple as things can ever get. A bit is a single “binary” digit either a 1 or a 0, that’s it end of story. </p><p>Computers are composed of a vast multitude of switches or gates which fundamentally operate on electricity being present or not. Thus you have two states you can represent; on/off, 1/0, yes/no whatever you choose to call them that is all you really have. Maybe interestingly, maybe not, this is also one of the few places/times where we “type” something by making it less abstract. Maybe it is really a conversion more than typing but  still think that is mildly interesting.</p><p>Okay, I may have lied a little bit and there is another subtle bit of “typing” <a href=\"https://en.wikipedia.org/wiki/Endianness\" target=\"_blank\" rel=\"noopener\">going on here</a> however since that is  a hardware thing we’ll pretend it doesn’t exist. This likely isn’t really enlightening yet but it maybe raises a more general WTF type question in your head. The trick lies in applying patterns and abstract meaning to that sea of points. We can revisit our model as we work to help us figure out what needs to happen next. Think of me as your reference material. I know where we want to go and I can give you some vague directions after each step.</p><p>He takes a cocktail napkin and writes a bunch of 1s and 0s down again, but this time, there’s a discernible pattern to them</p><pre><code>01001101 01010000 01010011 00110111 00000001 00000000 00000000 00000000\n01000111 00000000 01010011 00001001 00100111 11010001 00111001 01100111\n11000000 01000101 11011001 10010001 00100001 01000000 10000010 11100010\n00110001 11010110 11010111 00101110 10001101 00000001 01010100 01101101\n01110001 01101010 00010100 10100010 10110000 11110101 11010100 10111011\n10001111 11100010 01000000 10001001 11001001 00001001 01110011 10000001\n10010011 01101010 00000010 01010010 11100001 11110100 10111011 00110011\n10101111 11010011 01111001 01000001 01000011 10010000 10001001</code></pre><p>Sometimes a little bit of structure can go a long way. We’ve really only applied two concepts here but you will find they are pretty fundamental when it comes to low level data representations:</p><ul><li>Partitioned our bits into bytes</li><li>Partitioned our bytes into “words”</li></ul><p>So, we kind of have our first two “types”. A  is a collection of 8 bits. In this case it represents a number from 0-255 (these are <a href=\"https://en.wikipedia.org/wiki/Two%27s_complement\" target=\"_blank\" rel=\"noopener\">unsigned</a>). We also have a  this usually reflects the maximum capacity of a CPU register (and consequently a memory address) on the computer in question. Word size can vary from computer to computer, these are 64-bit words.</p><p>This is the point where things can start to bridge the gap from machine to man. Looking at a gigantic string of binary can make your eyes bleed. However, adding a little bit of formatting can help you keep track of where you are in the data if nothing else. There is an else though! The fact that this formatting maps to something that happens in the computer hardware helps, too. We are starting to establish a bit of a common language with the computer!</p><p>Surprisingly that makes a bit of sense. Okay, so how do these groupings turn into a log/ledger? The  part is that there will be a list of records of some kind in here. What might not be obvious is that we are also expecting something called a header which tells us a bit about the data.</p><p>Given that information we can at this point determine that those bytes are storing two :</p><h2>Jack’s Complete Lack of Surprise</h2><p>This means that like before we should be able to peform some kind of grouping/partitioning and at least see if this data really can represent a transaction log.</p><p>If we want to partition our data into a  and  we need to know how to separate those two. Where does one end and the other begin? Your single serving friend grabs another cocktail napkin and at the top they write  with the following information underneath:</p><ul><li>Format: <code>&lt;magic:4&gt;&lt;version:1&gt;&lt;records:4&gt;</code><ul></ul></li></ul><p>You look at it for a minute and ask: “Okay, but what does that mean really?”.</p><p>It means that we are expecting our header to be a total of 9 bytes. We’ve already seen that a byte is 8 bits which means that the first nine groups should make up the entirety of our . Going one level deeper: our  is actually a compound type and is really three different values in sequence. The first piece of the header is 4 bytes in size and is what is sometimes referred to as a “Magic String”.</p><p> This is a marker that helps an application determine if it knows how to process this type of data. The next byte in the  is a version indicator, this could really mean anything but for our purposes it mostly just serves as a place holder. </p><p>Knowing it will be there and accounting for it helps us avoid looking in the wrong place for the next value. The final 4 bytes indicate the total number of records we should expect to find in this file. This would be super useful if we were going to do any kind of integrity checking etc.</p><p>Bear with me, there is a good reason for this. Let’s actually start from the 9th byte and work backwards. You probably wouldn’t do this in real life but I haven’t explained “strings” yet so let’s get the numbers out of the way first.</p><p>So, starting at the “end” we have the record count. Going back up to our  typing info we can see that the  value is something referred to as a . This means that those bits actually make up one 32-bit number:</p><pre><code>00000000 00000000 00000000 01000111</code></pre><p>If we convert it from <a href=\"https://en.wikipedia.org/wiki/Binary_number#Binary_to_decimal\" target=\"_blank\" rel=\"noopener\">binary to decimal</a> we discover that the header claims there are 71 records. After performing the conversion you comment that the records must be really small. Your companion informs you that there actually aren’t 71 records in the data. It needed to fit on a business card so quite a few have been omitted. Technically there is capacity in the count to reflect 4,294,967,295 records. Who really knows if this format was ever intended to hold that many records in a single log or if it was just originally written on a 32-bit machine…</p><p>Okay, so that’s the  out of the way, next we have the . Again going back to the typing info for the  we only have one byte (8-bits) for the , that should be a pretty easy one.</p><p>Look at that we’ve got version 1!</p><p>Now for the first/last piece of the  the “Magic String”. Looking back to the  typing we can see that it consists of 4 bytes (32-bits).</p><pre><code>01001101 01010000 01010011 00110111</code></pre><p>“Magic String” implies that we are expecting  rather than one or more numbers though. So, how exactly are we supposed to do that?</p><p>If you’ve played around with any cryptographic cyphers or even super simple decoder rings you probably have an idea. And you wouldn’t be wrong. There is an encoding called ASCII that is an implementation of that very concept. It assigns numbers to different “symbols” i.e. letters and other characters found on English language (American) keyboards (along with a few extras). So, this is easily our most abstract type so far. We are basically following this path: bit -&gt; byte -&gt; character/symbol.</p><p>So, if we look at an <a href=\"https://en.wikipedia.org/wiki/ASCII#/media/File:ASCII_Table_(suitable_for_printing).svg\" target=\"_blank\" rel=\"noopener\">ASCII table</a> we discover that the four bytes above  possibly be a representation of </p><p>Neat so we have applied some primitive “concrete” typing and now have a Header:</p><p>Okay, so the first part is sorted and has undergone a rather radical transformation in the process. But the  is still a mystery. What the hell is it exactly? How do you go about deciphering the information there? All you know right now is that there aren’t 71 records but really that just gives you an upper bound. So, you turn to your single serving friend again and before you can ask they start writing on another napkin:</p><p>There are four possible records in the content section:</p><ul><li>Debit\n<ul></ul></li><li>Credit\n<ul></ul></li><li>Autopay start\n<ul></ul></li><li>Autopay end\n<ul></ul></li></ul><p>So, this is helpful and if we look at the start of the content it tells us that the first record should be a .</p><pre><code>00000000 01010011 00001001 00100111 11010001 00111001 01100111 01000111\n11000000 01000101 11011001 10010001 00100001 01000000 10000010 11100010\n00110001 11010110 11010111 00101110 10001101 00000001 01010100 01101101\n01110001 01101010 00010100 10100010 10110000 11110101 11010100 10111011\n10001111 11100010 01000000 10001001 11001001 00001001 01110011 10000001\n10010011 01101010 00000010 01010010 11100001 11110100 10111011 00110011\n10101111 11010011 01111001 01000001 01000011 10010000 10001001</code></pre><p>Using our new  typing information it looks like we have 3 records here:</p><pre><code>00000000 01010011 00001001 00100111 11010001 00111001 01100111 01000111 11000000 01000101 11011001 10010001 00100001 01000000 10000010 11100010 00110001 11010110 11010111 00101110 10001101</code></pre><pre><code>00000001 01010100 01101101 01110001 01101010 00010100 10100010 10110000 11110101 11010100 10111011 10001111 11100010 01000000 10001001 11001001 00001001 01110011 10000001 10010011 01101010</code></pre><pre><code>00000010 01010010 11100001 11110100 10111011 00110011 10101111 11010011 01111001 01000001 01000011 10010000 10001001</code></pre><p>Finally, we almost have something useful put together. And for some levels curiosity this could be satisfactory but why stop at satisfactory? I’m sure there is more information about what a Debit, Credit and Autopay start are. Maybe there is an actual end to this tunnel. Right on cue your single serving friend is already working on a new napkin:</p><ul><li>Debit – <code>&lt;timestamp:32-bits&gt;&lt;account:64-bits&gt;&lt;amount:64-bit float&gt;</code></li><li>Credit – <code>&lt;timestamp:32-bits&gt;&lt;account:64-bits&gt;&lt;amount:64-bit float&gt;</code></li><li>Autopay start – <code>&lt;timestamp:32-bits&gt;&lt;account:64-bits&gt;</code></li><li>Autopay stop – <code>&lt;timestamp:32-bits&gt;&lt;account:64-bits&gt;</code></li></ul><p>Okay, that  sane,  and  have the same composition and so do  and . Given the “names” we’ve been given for those things that seems to make sense. So, let’s apply this new type information and see what we get.</p><p>Alright, using the our newest breakdown we have the following:</p><ul><li>timestamp – <code>01010011 00001001 00100111 11010001</code></li><li>account – <code>00111001 01100111 01000111 11000000 01000101 11011001 10010001 00100001</code></li><li>amount – <code>01000000 10000010 11100010 00110001 11010110 11010111 00101110 10001101</code></li></ul><ul><li>timestamp – <code>01010100 01101101 01110001 01101010</code></li><li>account – <code>00010100 10100010 10110000 11110101 11010100 10111011 10001111 11100010</code></li><li>amount – <code>01000000 10001001 11001001 00001001 01110011 10000001 10010011 01101010</code></li></ul><ul><li>timestamp – <code>01010010 11100001 11110100 10111011</code></li><li>account – <code>00110011 10101111 11010011 01111001 01000001 01000011 10010000 10001001</code></li></ul><p>For the most part these new details are the same as the majority of what we’ve been working with previously, integers of various “widths”. But there is something in the  and  that is new. The  is a “64-bit float”. You look up at your single serving friend and ask what exactly that means. They explain that computers are not great at representing fractional numbers. In order to try and work around this there are some <a href=\"https://en.wikipedia.org/wiki/Floating-point_arithmetic#:~:text=In%20computing%2C%20floating%2Dpoint%20arithmetic,are%20called%20floating%2Dpoint%20numbers.\" target=\"_blank\" rel=\"noopener\">clever tricks</a> you can use. They also mention that floating point isn’t exactly perfect and probably was not the best choice for a financial ledger. </p><p>The problem is that there are a lot of numbers you can’t accurately represent and when performing mathematical operations with floating point numbers the computer rounds (by very small amounts but it does round).</p><p>They also mention that the rest is up to you. They don’t actually have any further information about these final types. This means that the timestamp could maybe be a simple count of some amount of time that has elapsed since a specific origin (or epoch). Or it could be a representation of some form of date or other representation with more definition. They also point out that the account identifier could be a number or it could be some kind of string or it could be something else. However even without those details you can still determine quite a lot about the data and probably perform any kind of aggregation/sorting/filtering you might want to do.</p><p>Armed with this final bit of information you take another look at what you have so far. You can say for certain that none of these records were created at the same time. Which also implies that you could apply some sort of ordering to them if pressed. It is also clear that these three records are for different accounts. You don’t really need to kown what exactly the account identifier ultimately represents in order to “balance the books” so to speak. So, really the last refinement we can deterministically make is to figure out the amounts for the debit and credit.</p><p>If we use the IEEE 754 standard for computing our amounts we get:</p><ul><li>Credit: 604.2743355570870562587515451014041900634765625</li><li>Debit: 825.129614841757984322612173855304718017578125</li></ul><p>This is a great example of why floating point isn’t a great representation for financial data; the values encoded here are highly unlikely to have been the actual amounts.</p><p>That was a surprising amount to figure out one Debit, one Credit and an Autopay start (whatever the hell that is). Hopefully you found some of it <a href=\"https://revelry.co/insights/5-elixir-concepts-for-javascript-devs/\" data-type=\"link\" data-id=\"https://revelry.co/insights/5-elixir-concepts-for-javascript-devs/\">interesting</a>, and if not well at least you scrolled all the way down here. The main point here was to provide evidence/support for the following claim:</p><p>As we saw in the beginning the raw data was inscrutable – it could have meant anything. You may have also noticed that there really wasn’t anything an actual computer would do that helped us figure what what it represented. There is no way for us to reasonably work with computers without leveraging typing to some degree (punch cards are not a reasonable way to work with computers).</p><p>There is even more to typing than we explored here. This example was heavily focused on . That is to say we were primarily focused on the “physical” layouts of things and applying definitions to raw data. Abstract typing is very much a thing too and maybe one that more developers are familiar with, to varying degrees. When you are working with more abstract types you are still describing the data but equally or sometimes more importantly you are describing behaviors of that data. What is it allowed to do, how does it change/behave in different scenarios if at all. For example the abstract side of what we worked on would likely set rules like:</p><ul><li>Math is allowed on the amounts for Credit and Debit records</li><li>Autopay records must have a specific order of environments</li><li>any number of other things</li></ul><p>We did touch on a few things that were more on the abstract side. Our Magic String is a good example, that was really one step above the actual layout (width) of the data. The general Debit, Credit and Autopay internals sort of straddle the line a bit too in that we are attributing a domain meaning to parts of that layout. Things tend to get even more abstract the farther up you go. This can be powerful but also like any abstraction it can make other things much harder to do.</p>","contentLength":17206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oiertg/type_club_understanding_typing_through_the_lens/"},{"title":"Integration tests with Go and Elasticsearch","url":"https://getpid.dev/blog/elasticsearch-integration-tests/","date":1761668974,"author":"/u/aspidima","guid":319272,"unread":true,"content":"<p>One of the approaches to do integration tests with Elasticsearch is to use <a href=\"https://testcontainers.com/\">testcontainers</a>.\nIt is recommended by <a href=\"https://www.elastic.co/search-labs/blog/tests-with-mocks-and-real-elasticsearch\">Elastic</a>,\nit works well and provides a high level of isolation between tests.\nBut, it comes with drawbacks: Elasticsearch containers are heavy and take time to start.</p><p>What if we use a different kind of isolation? The index seems pretty isolated.\nInstead of starting a new container for each test, we can just start one container and\ncreate a new index for each test case.</p><p>It is pretty simple to implement. Let’s go through it.</p><p>We want to create a storage for books on top of Elasticsearch and want to index and search books by title and author.\nHere are the mappings for the index:</p><div><pre tabindex=\"0\"><code data-lang=\"json\"></code></pre></div><p>And the full code for the storage is in <a href=\"https://github.com/dmksnnk/blog/tree/main/examples/elasticsearch-integration/storage.go\">storage.go</a>\n(it is too long to include here in full).</p><h2>Setting up Elasticsearch<a hidden=\"\" aria-hidden=\"true\" href=\"https://getpid.dev/blog/elasticsearch-integration-tests/#setting-up-elasticsearch\">#</a></h2><p>Assume we have a running Elasticsearch instance in a container.\nYou can use the <a href=\"https://github.com/dmksnnk/blog/tree/main/examples/elasticsearch-integration/docker-compose.yaml\">docker-compose.yml</a>\nfor starters.</p><p>What I like to do is to have test helpers that abstract away the details of setting up infrastructure for tests.\nWe will do just that. We will create a  package with a helper to create a new storage with a unique index for each test.</p><p>We need to pass the Elasticsearch address to communicate with it. The simplest way is to use an environment variable:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>The next step is to create a unique index name for each test.\nThere are <a href=\"https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-indices-create#operation-indices-create-path\">some rules</a>\nfor the index name, and we will do our best to follow them: take the test name, lowercase it,\nremove unsupported characters, and add a random suffix, so the index name is unique.</p><p>Then, we create a new index and apply a mapping to it:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p><a href=\"https://github.com/dmksnnk/blog/tree/main/examples/elasticsearch-integration/embed.go\">Here</a>\nwe are using <a href=\"https://pkg.go.dev/embed\">embed</a> to embed the mapping file into the binary.\nBut you can read it from anywhere else, like a local file if it is in the same repo or from a remote git repository.</p><p>The most useful part comes next: <strong>we delete the index only if the test passed</strong>.\nIf it failed, we can inspect the index and see what went wrong. We’ll see it in action later.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>The test itself looks short and sweet, all with the help of our helper.\nStore a document and search for it:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Run the test with the Elasticsearch address set:</p><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div><p>If you change the search query to something that does not match the indexed document:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>You will see a helpful log message with the index name to inspect:</p><pre tabindex=\"0\"><code>--- FAIL: TestStorage (0.43s)\n    storage_test.go:12: using index: teststorage_023f73bc409aa215\n...\n</code></pre><p>And you are free to explore why the test failed, for example, to see what is inside the index:</p><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div><p>The full source code is available on <a href=\"https://github.com/dmksnnk/blog/tree/main/examples/elasticsearch-integration\">GitHub</a></p>","contentLength":2484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oie3r2/integration_tests_with_go_and_elasticsearch/"},{"title":"Introducing ArkRegex: a drop in replacement for new RegExp() with types","url":"https://arktype.io/docs/blog/arkregex","date":1761668846,"author":"/u/ssalbdivad","guid":319521,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oie1nq/introducing_arkregex_a_drop_in_replacement_for/"},{"title":"Olric v0.7.1 released - Build fast, scalable memory pools across nodes","url":"https://github.com/olric-data/olric","date":1761668793,"author":"/u/mastabadtomm","guid":319271,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oie0s4/olric_v071_released_build_fast_scalable_memory/"},{"title":"An ex-Intel CEO’s mission to build a Christian AI: ‘hasten the coming of Christ’s return’","url":"https://www.theguardian.com/technology/2025/oct/28/patrick-gelsinger-christian-ai-gloo-silicon-valley","date":1761668254,"author":"/u/esporx","guid":319494,"unread":true,"content":"<p>n March, three months after being <a href=\"https://www.theguardian.com/technology/2024/dec/02/intel-ceo-pat-gelsinger\" data-link-name=\"in body link\">forced out</a> of his position as the CEO of Intel and sued by shareholders, Patrick Gelsinger took the reins at <a href=\"https://gloo.com\" data-link-name=\"in body link\">Gloo</a>, a technology company made for what he calls the “faith ecosystem” – think Salesforce for churches, plus chatbots and <a href=\"https://gloo.com/ai/products/assistants\" data-link-name=\"in body link\">AI assistants</a> for automating pastoral work and ministry support.</p><p>The former CEO’s career pivot is taking place as the US tech industry <a href=\"https://www.theguardian.com/books/2023/may/10/palo-alto-book-malcom-harris-interview\" data-link-name=\"in body link\">returns</a> to the political realm as a major revenue stream. Some of its most prominent present-day leaders have <a href=\"https://www.theguardian.com/us-news/2024/dec/07/campaign-spending-crypto-tech-influence\" data-link-name=\"in body link\">funded</a> Donald Trump’s re-election and renewed their pursuit of government contracts as the second Trump administration has revitalized <a href=\"https://www.theguardian.com/us-news/2025/apr/16/christian-nationalists-trump-administration\" data-link-name=\"in body link\">religious conservatism</a> in Washington DC.</p><p>Now Gloo’s executive chair and head of technology (who’s largely <a href=\"https://www.reuters.com/technology/intel-defeats-shareholder-lawsuit-over-foundry-losses-32-billion-plunge-2025-03-05/\" data-link-name=\"in body link\">free of the shareholder suit</a>), Gelsinger has made it a core mission to soft-power advance the company’s Christian principles in <a href=\"https://www.prnewswire.com/news-releases/gloo-expands-into-silicon-valley-with-opening-of-new-office-in-palo-alto-california-302573624.html\" data-link-name=\"in body link\">Silicon Valley</a>, the halls of Congress and beyond, armed with a <a href=\"https://gloo.com/press/releases/gloo-secures-110-million-strategic-growth-investment\" data-link-name=\"in body link\">fundraised war chest of $110m</a>. His call to action is also a pitch for AI aligned with Christian values: <a href=\"https://gloo.com/ai\" data-link-name=\"in body link\">tech products</a> like those built by Gloo, many of which are built on top of existing large language models, but adjusted to reflect users’ theological beliefs.</p><p>“My life mission has been [to] work on a piece of technology that would improve the quality of life of every human on the planet and hasten the coming of Christ’s return,” he said.</p><p>Gloo says it serves “over 140,000 faith, ministry and non-profit leaders”. Though its intended customers are not the same, Gloo’s user base pales <a href=\"https://www.businessinsider.com/chatgpt-users-growth-openai-growth-sam-altman-ai-llm-2025-10\" data-link-name=\"in body link\">in comparison with</a> those of AI industry titans: about 800 million active users rely on ChatGPT every week, not to mention Claude, Grok and others.</p><p>Religiosity like Gelsinger’s – a born-again Christian who has <a href=\"https://www.purposenation.org/pat-gelsinger-podcast-transcript\" data-link-name=\"in body link\">referred</a> to Silicon Valley as his “mission field” – is shaping Silicon Valley’s culture in its image. Where there was once purported atheism, there is now “a very loud, very visible and very specifically Christian-inflected technological culture” in Silicon Valley, said <a href=\"https://taiming-ai.charlotte.edu/people/damien-williams/\" data-link-name=\"in body link\">Damien Williams</a>, a scholar at the University of North Carolina at Charlotte who studies how technologies are shaped by religious beliefs. It’s exemplified by figures like Peter Thiel – who <a href=\"https://www.theguardian.com/technology/2025/oct/13/peter-thiel-the-antichrist-and-why-the-world-should-listen\" data-link-name=\"in body link\">warns</a> of the <a href=\"https://www.theguardian.com/us-news/2025/oct/10/peter-thiel-lectures-antichrist\" data-link-name=\"in body link\">coming of the antichrist if humanity fails</a> to work toward certain technological frameworks – and Andreessen Horowitz’s <a href=\"https://www.nytimes.com/2025/10/19/business/katherine-boyle-andreesen-horowitz-american-dynamism.html\" data-link-name=\"in body link\">Katherine Boyle</a>, a close friend of JD Vance, the US vice-president. Gelsinger has long been outspoken about his Christian values, helping <a href=\"https://www.tbc.city\" data-link-name=\"in body link\">found</a> Transforming the Bay With Christ in 2013, an organization aiming to ignite a Christian spiritual movement in the region.</p><p>Speaking on 7 October at <a href=\"https://vimeo.com/1125006783?share=copy\" data-link-name=\"in body link\">a seminar</a> co-hosted by Gloo, Colorado Christian University, a conservative college, and the Christian Post, a conservative evangelical Christian news outlet, Gelsinger framed AI’s development as “another Gutenberg moment”: an epochal shift as important as the Reformation. In the same way that a “plump little monk”, Martin Luther, used the printing press to catalyze “the greatest period of human invention”, he sees a similar faith-driven opportunity today to change the course of history through AI.</p><p>“The church embraced that great invention of the day to literally change humanity,” Gelsinger said of the printing press. “And so my question today is: are we going to embrace [and] shape AI as a technology that truly does become a powerful embodiment of the church and the expression of the church?”</p><p>Straddling the worlds of AI, Christianity and faith tech, Gloo isn’t solely focused on shaping the AI sector through Silicon Valley. It’s wielding influence in other ways, such as by supporting and funding a Christian tech ecosystem. The company hosted a three-day hackathon following the seminar at Colorado Christian University; its event saw more than 600 participants compete for over $250,000 in<a href=\"https://www.prnewswire.com/news-releases/gloo-ai-hackathon-awards-250-000-for-values-aligned-ai-innovations-advancing-human-flourishing-302583798.html\" data-link-name=\"in body link\"> prize money</a> – nearly triple the attender count from 2024.</p><p>Though growing, the event was not without hiccups. Ryan Siebert, an AI product developer and hackathon attender, said he was able to get Gloo’s newest large language model, which has not yet been publicly launched, to provide him a recipe for methamphetamine through a <a href=\"https://www.ibm.com/think/topics/prompt-injection\" data-link-name=\"in body link\">prompt injection</a>. He later communicated with the president of Gloo AI to share details about the vulnerability. A Gloo spokesperson said the company explicitly invited hackathon attenders to be among the first to test the new large language model and offer feedback on it, as the product is in a “pre-beta” stage of development.</p><figure data-spacefinder-role=\"inline\" data-spacefinder-type=\"model.dotcomrendering.pageElements.NewsletterSignupBlockElement\"><a data-ignore=\"global-link-styling\" href=\"https://www.theguardian.com/technology/2025/oct/28/patrick-gelsinger-christian-ai-gloo-silicon-valley#EmailSignup-skip-link-12\">skip past newsletter promotion</a><p tabindex=\"0\" aria-label=\"after newsletter promotion\" role=\"note\">after newsletter promotion</p></figure><p>Meanwhile, Gelsinger said his message about Christian AI was finding friendly ears in Washington DC. In an interview with the Guardian, he described presenting Gloo’s work to legal advocacy groups, as well as congressional leaders. He declined to name the institutions or politicians, but said some lawmakers were interested in using Gloo products at their churches.</p><p>Gloo and Gelsinger frequently travel in conservative political circles. Brandon Showalter, a journalist at the Christian Post and moderator at the Gloo co-hosted seminar, is an anti-trans activist who has <a href=\"https://www.instagram.com/p/DPRP1mNjdol/?utm_source=ig_web_copy_link&amp;igsh=eThnYnllbjR0MWI5\" data-link-name=\"in body link\">said</a> he hopes trans youth care will eventually become “as unthinkable as ice pick lobotomies”. <a href=\"https://www.heritage.org/staff/annie-chestnut-tutor\" data-link-name=\"in body link\">Annie Chestnut Tutor</a>, an analyst at the Heritage Foundation, the influential conservative thinktank behind Project 2025, participated in a panel moderated by Showalter and offered a Beltway insider’s perspective on AI regulation. On 17 October, Gelsinger <a href=\"https://x.com/PGelsinger/status/1979250475016032428\" data-link-name=\"in body link\">delivered</a> his “Gutenberg moment” stump speech at Liberty University’s <a href=\"https://www.liberty.edu/news/2025/10/07/executives-line-up-to-share-insights-at-libertys-annual-ceo-summit-oct-15-17/\" data-link-name=\"in body link\">CEO Summit</a>, which also featured Liz Truss, Michael Flynn and Dan T Cathy, the Chick-fil-A chairman, as keynote speakers.</p><p>But Gelsinger appears just as comfortable in those venues as he does as a <a href=\"https://www.cnbc.com/2022/03/02/why-intels-ceo-was-a-guest-at-bidens-state-of-the-union-address.html\" data-link-name=\"in body link\">guest of honor</a> at Joe Biden’s 2022 State of the Union address; and, as may be expected of a pragmatic industry executive, public records <a href=\"https://www.opensecrets.org/donor-lookup/results?name=patrick+gelsinger\" data-link-name=\"in body link\">show</a> he’s backed political campaigns on both sides of the aisle.</p><p>Gloo itself strikes an ecumenical tone in its institutional messaging. It shied away from politics or denominational disagreements at the hackathon; when an attender shared plans on Discord to build an AI bot of Charlie Kirk, the conservative political figure <a href=\"https://www.theguardian.com/us-news/charlie-kirk-shooting\" data-link-name=\"in body link\">assassinated</a> at a rally in Utah last month, that would offer “scripture-based responses”, a hackathon organizer encouraged staying “solidly focused on Jesus here”, even as “politics are definitely important”. Leah and Wes Brooks, whose hackathon team built a set of open-source AI tools enabling interoperability among faith-based and other apps, noted a diverse set of event-goers and religious backgrounds – including a female pastor – as well as a generally collaborative environment. “We didn’t even have to sign a statement of faith or anything like that,” Leah Brooks said. Gloo also <a href=\"https://www.christianitytoday.com/2025/05/gloo-ai-artificial-intelligence-church-worship-tech-ethics/\" data-link-name=\"in body link\">says</a> it does not “prohibit in any way” Muslim organizations from using its technology.</p><p>“We’re not trying to take a theological position: we’re building a technology platform, and then giving enough customization capability that the Lutherans can be good with it, the Episcopalians can be good with it, the Catholics can be good [with it], the Assemblies of God can be good with it,” Gelsinger told the Guardian. “We’re trying to say, ‘Hey, there’s a broad tent here of faith and flourishing,’ but also we’re trying to satisfy many organizations that do not take a denominational perspective, [such as] Alcoholics Anonymous.”</p><p>Gelsinger wants faith to suffuse AI. He has also spearheaded Gloo’s <a href=\"https://gloo.com/flourishing-hub/research\" data-link-name=\"in body link\">Flourishing AI initiative</a>, which evaluates leading large language models’ effects on human welfare across seven variables – in essence gauging whether they are a force for good and for users’ religious lives. It’s a system adapted from a Harvard research initiative, <a href=\"https://hfh.fas.harvard.edu\" data-link-name=\"in body link\">the Human Flourishing Program</a>. Models like Grok 3, <a href=\"https://www.theguardian.com/technology/deepseek\" data-link-name=\"in body link\">DeepSeek-R1</a> and GPT-4.1 earn high marks, 81 out of 100 on average, when it comes to helping users through financial questions, but underperform, about 35 out of 100, when it comes to “Faith”, or the ability, according to Gloo’s metrics, to successfully support users’ spiritual growth.</p><p>Gloo’s initiative has yet to visibly attract Silicon Valley’s attention. A Gloo spokesperson said the company is “starting to engage” with prominent AI companies.</p><p>“I want Zuck to care,” Gelsinger said.</p>","contentLength":8444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oidruh/an_exintel_ceos_mission_to_build_a_christian_ai/"},{"title":"Linus Torvalds on ZDTV's The Screen Savers in 1998","url":"https://www.youtube.com/watch?v=BOIbP943AAI","date":1761667819,"author":"/u/zrad603","guid":319269,"unread":true,"content":"<div><p>I stumbled across this old video on YouTube of Linus Torvalds on ZDTV's The Screen Savers with Leo Laporte and Kate Botello. </p><p>I'm guessing this was 1998 because they reference \"Windows 95\" and Red Hat 5.1 which was release in May 1998. </p></div>   submitted by   <a href=\"https://www.reddit.com/user/zrad603\"> /u/zrad603 </a>","contentLength":265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1oidks4/linus_torvalds_on_zdtvs_the_screen_savers_in_1998/"},{"title":"GitHub - tester305/webview_go: Go language bindings for the webview library.","url":"https://github.com/tester305/webview_go","date":1761666950,"author":"/u/Big-Share-6781","guid":319240,"unread":true,"content":"<p>Hi <a href=\"https://github.com/r/golang\">r/golang</a>, I know this module is not the best but it is a great alternative to webview/webview_go</p><p><strong>Heres why it can be very useful:</strong></p><p><strong>1. no libwebkit2gtk-4.0 dependency (That package is out of most linux mirrors, libwebkit2gtk-4.1 is used instead)</strong></p><p><strong>2. No golint warnings (yes i know that package is from old mirrors but i have old mirrors added) and no go vet warnings</strong></p><p><strong>4. Does not panic instantly (I tested it and it was stable so far.)</strong></p><p>I’d love feedback, suggestions, or even forks. Hope you enjoy it!</p>","contentLength":497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oid6kh/github_tester305webview_go_go_language_bindings/"},{"title":"Lessons from scaling live events at Patreon: modeling traffic, tuning performance, and coordinating teams","url":"https://www.patreon.com/posts/from-thundering-141679975","date":1761665935,"author":"/u/patreon-eng","guid":319267,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oicq3e/lessons_from_scaling_live_events_at_patreon/"},{"title":"L2 Load Balancer networking on Bare metal","url":"https://www.reddit.com/r/kubernetes/comments/1oib932/l2_load_balancer_networking_on_bare_metal/","date":1761662573,"author":"/u/Different_Code605","guid":319378,"unread":true,"content":"<p>How do you configure networking for load balancer like MetalLB or KubeVIP?</p><p>My first attempt was to use one NIC with two routing rules, but it was hard to configure and didn’t look like a best practice.</p><p>My second attempt was to configure two separate NICs, one for private with routes covering 172.16.0.0/12 and one public with default routing.</p><p>The problem is that i need to bootstrap public NIC with all the routes and broadcast, without the IP, as the IP will be assigned later by LB (like KubeVIP, havent go there with metallb yet).</p><p>How did you configure in your setups? 99% of what I see is LB configured on one NIC with host network using the same DHCP, but that is obviously not my case</p><p>Any recommendations are welcome.</p>","contentLength":721,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub - akashsharma95/sharded-counter: scalable S3-backed sharded counters","url":"https://github.com/akashsharma95/sharded-counter","date":1761661433,"author":"/u/1blue_dot","guid":319203,"unread":true,"content":"<p>I built a Go library for distributed counters that uses S3 as the storage backend instead of Redis or a database. It's designed for workloads where you need to count a lot of events (analytics, rate limits, quotas) without the overhead of running dedicated infrastructure.</p><p>The library shards writes across many small S3 objects and periodically compacts them into a base total. All operations use S3 conditional writes (Compare-And-Swap) for correctness, and sharding eliminates write contention.</p>","contentLength":495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1oiarft/github_akashsharma95shardedcounter_scalable/"},{"title":"Fedora Linux 43 is here!","url":"https://fedoramagazine.org/announcing-fedora-linux-43/","date":1761661108,"author":"/u/ScootSchloingo","guid":319202,"unread":true,"content":"<p>I’m excited to announce my very first Fedora Linux release as the new Fedora Project Leader. Fedora Linux 43 is here!  43 releases! Wow that’s a lot.  I was thinking about proposing special tetracontakaitrigon stickers to celebrate this release, but I’m not sure anyone would notice they weren’t circles.</p><p>Thank you and congrats to everyone who has contributed to Fedora to this release, and in all the releases leading up to this one. I’m grateful to be back with a chance to take stewardship of the collaboration as the Fedora Project leader. I’ve been getting my feet under me as much as I can in these first few months. I’m looking forward to writing up some longer missives about where I want to steer this ship, but for right now I just want to highlight some of the changes you should expect to encounter in the latest release of Fedora Linux. Read the highlights below to find out more. Or if you are ready just jump right in!</p><p>If you have an existing system, <a href=\"https://docs.fedoraproject.org/en-US/quick-docs/upgrading-fedora-new-release/\" target=\"_blank\" rel=\"noreferrer noopener\">Upgrading Fedora Linux to a New Release</a> is easy. In most cases, it’s not very different from just rebooting for regular updates, except you’ll have a little more time to grab a coffee.</p><p>As usual, with Fedora, there are just too many individual changes and improvements to go over in detail. You’ll want to take a look at the <a href=\"https://docs.fedoraproject.org/en-US/fedora/latest/release-notes/\">release notes</a> for that.</p><h3>Notable User Visible Changes</h3><p>There are, however, a few notable user visible changes in this release. For those of you installing fresh Fedora Linux 43 Spins, you may be greeted with the new Anaconda WebUI. This was the default installer interface for Fedora Workstation 42, and now it’s the default installer UI for the Spins as well.</p><p>If you are a GNOME desktop user, you’ll also notice that the GNOME is now Wayland-only in Fedora Linux 43. GNOME upstream has deprecated X11 support, and has disabled it as a compile time default in GNOME 49. Upstream GNOME plans to fully remove X11 support in GNOME 50.</p><p>Beyond the user-visible changes, there are a couple of significant bits of plumbing that should go unnoticed for most users but are a big deal, nonetheless.</p><p>Fedora Linux 43 will be the first release with RPM 6.0. Like I said, this should go unnoticed to end-users, but it is a significant change. RPM 6.0 provides some interesting security enhancements, like multiple key signing of packages. This should help future-proof package signing as we transition to post-quantum-crypto OpenPGP keys in future releases.</p><p>We’re also moving forward with our bootc enablement story. Fedora CoreOS is now buildable from a Fedora base bootc image using a Containerfile, instead of needing to be composed with a custom tool. That means anyone with podman can build the Fedora CoreOS image, whether manually or via CI/CD automation.</p><p>Fedora CoreOS (FCOS) is also changing how it’s issuing updates to users in Fedora 43. Instead of using an OSTree repository, FCOS updates will be delivered exclusively as OCI images. FCOS 42 provided both OSTree repository and OCI registry as a transition for users. In FCOS 43, the OSTree updates are disabled entirely.</p><h2>Save the Date: Fedora Linux 43 Release Party!</h2><p>To celebrate all this incredible community work, we’ll be hosting a virtual Fedora Linux 43 Release Party! Please <strong>save the date for Friday, 21 November</strong>. We’re still finalizing the schedule and speakers, so registration isn’t open just yet, but more details will be shared soon. You can keep an eye on the <a href=\"https://fedoraproject.org/wiki/Fedora_Linux_43_Release_Party_Schedule\" target=\"_blank\" rel=\"noreferrer noopener\">Fedora Linux 43 Release Party Schedule wiki page</a> for the latest updates!</p><p>If you run into a problem, visit our <a href=\"https://ask.fedoraproject.org/\" target=\"_blank\" rel=\"noreferrer noopener\">Ask Fedora</a> user support forum. This forum includes a category where we collect <a href=\"https://discussion.fedoraproject.org/tags/c/ask/common-issues/82/none/f43\" target=\"_blank\" rel=\"noreferrer noopener\">common issues</a> and solutions or work-arounds.</p><h2>Just drop by and say “hello”</h2>","contentLength":3695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1oiamfg/fedora_linux_43_is_here/"},{"title":"[P] Looking for cool project ideas for an intro to Machine Learning course","url":"https://www.reddit.com/r/MachineLearning/comments/1oiajwc/p_looking_for_cool_project_ideas_for_an_intro_to/","date":1761660943,"author":"/u/S0R3N_RAGNARSSON","guid":319201,"unread":true,"content":"<p>I'm currently taking an <strong>introductory Machine Learning course</strong> that covers <strong>unsupervised learning, supervised learning, and neural networks</strong>. I’d like to develop a  that goes beyond the typical “predict housing prices” or “classify digits” examples.</p><p>Do you have any <strong>recommendations for creative or insightful projects</strong> that could integrate these three areas (or at least two of them)? Ideally something that helps build solid intuition about model design, evaluation, and interpretability.</p><p>Also, if you’ve taught or taken a similar course, I’d love to hear about projects that really helped you or your students understand the essence of ML.</p>","contentLength":648,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"jwt in golang","url":"https://www.reddit.com/r/golang/comments/1oiagh1/jwt_in_golang/","date":1761660722,"author":"/u/lispLaiBhari","guid":319204,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Anybody tried rolling their own JWT implementation on server? I know its not wise to use in prod but thinking of getting familiar with concepts and golang.</p> <p>Any links to blogs/books on JWT(using Golang) will be useful.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lispLaiBhari\"> /u/lispLaiBhari </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1oiagh1/jwt_in_golang/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oiagh1/jwt_in_golang/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"go schema validation","url":"https://www.reddit.com/r/golang/comments/1oia3m5/go_schema_validation/","date":1761659889,"author":"/u/Infinite-Plant655","guid":319180,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello, </p> <p>i am building an app where the user can define their extensions, using go lang, the issue i am having is this, the schema validation, i want to allow the user to have a serialized object with attributes like zod defines its objects(default value, options, restrictions, etc ) is there a lib in go where i can define a schema and i can safe parse them? i am using this to translate to a dynamic schema generator for a DSL with its editor </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Infinite-Plant655\"> /u/Infinite-Plant655 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1oia3m5/go_schema_validation/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oia3m5/go_schema_validation/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Templating errors in Golang project with SQLC in LazyVim","url":"https://www.reddit.com/r/golang/comments/1oi9gfq/templating_errors_in_golang_project_with_sqlc_in/","date":1761658341,"author":"/u/Dangerous_Roll_250","guid":319179,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I am going through the Boot.dev blog Aggregator project and with newest update of LazyVim I started to have the error in queries with params like this one: ```sql -- name: CreateUser :one INSERT INTO users (id, created_at, updated_at, name) VALUES ( $1, $2, $3, $4 ) RETURNING *;</p> <p>``<code> There is a following error on &quot;1&quot;: </code>Expected &quot;{&quot; or [A-Za-z_] but &quot;1&quot; found. sql [4, 7]` It says it&#39;s a templating error</p> <p>Lazyvim uses sqlfluff for formatting so I added <code>.sqlfluff</code> file to the root: <code>yaml [sqlfluff] dialect = postgres sql_file_exts = .sql,.queries </code> I have no idea how to fix it. </p> <p>Do you use Lazyvim for the Golang projects with sqlc and can help me? What is your setup for working with sqlc in Lazyvim?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Dangerous_Roll_250\"> /u/Dangerous_Roll_250 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1oi9gfq/templating_errors_in_golang_project_with_sqlc_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oi9gfq/templating_errors_in_golang_project_with_sqlc_in/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon to cut 30,000 jobs worldwide as workers to be replaced with AI","url":"https://www.themirror.com/news/world-news/amazon-cuts-jobs-ai-takeover-1470952","date":1761655253,"author":"/u/TheMirrorUS","guid":319270,"unread":true,"content":"<h2 data-testid=\"leadtext\" publication=\"themirror\">The Seattle-based technology giant is reportedly seeking to reverse its hiring spree from during the peak of the pandemic.</h2><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">Amazon is reportedly gearing up to slash as many as 30,000 corporate jobs this week. <a href=\"https://www.themirror.com/news/us-news/amazon-employees-work-robots-staff-1459807\" target=\"_self\" aria-label=\"\" tabindex=\"0\">The tech behemoth</a>, based in Seattle, is said to be reversing its hiring spree during the pandemic's peak.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">CEO Andy Jassy had previously cautioned employees about potential job losses due to artificial intelligence (<a href=\"https://www.themirror.com/all-about/ai\" target=\"_blank\" aria-label=\"AILink opens in a new tab.\" tabindex=\"0\">AI</a>) advancements. According to reports from Reuters and the Wall Street Journal, <a href=\"https://www.themirror.com/all-about/amazon\" target=\"_self\" aria-label=\"\" tabindex=\"0\">Amazon</a> is likely to target divisions worldwide including human resources, operations, devices and services, and Amazon Web Services (AWS).</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">AWS, the world's leading cloud computing provider, offers a broad range of services such as storage, databases, machine learning, and security tools. Last week, <a href=\"https://www.themirror.com/news/us-news/starbucks-app-down-aws-outage-1455705\" target=\"_self\" aria-label=\"\" tabindex=\"0\">disruption to AWS caused outages across numerous internet services globally</a>, affecting several banks.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">Insiders told the media outlets that the plans are part of an effort to reduce costs and reverse the company's massive recruitment drive during the pandemic when consumer habits shifted online.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">The anticipated cuts will affect nearly a tenth of the company's approximately 350,000 corporate workforce.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">Amazon employs over 1.5 million staff globally, with the majority working in warehouse roles.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">In the UK, the company has around 75,000 employees, reports <a href=\"https://www.express.co.uk/news/world/2126730/Amazon-job-cuts-30-000-AI\" target=\"_self\" aria-label=\"\" tabindex=\"0\">the Express</a>.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">Over recent years, it has been trimming roles across the business, impacting divisions such as devices, communications, and podcasting.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">Last month, Amazon announced plans to close all its Amazon Fresh grocery stores in the UK.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\"><strong> to follow the Mirror US on Google News to stay up to date with all the latest news, sport and entertainment stories.</strong></p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">As a result of these closures, 250 jobs were put on the line throughout the shops.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">This follows CEO Mr. Jassy's statement in June about the increased use of generative AI and AI agents, autonomous AI software systems, which he predicts will shrink the company's corporate workforce in the future.</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">In a memo to employees, he stated: \"As we roll out more Generative AI and agents, it should change the way our work is done.\"</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">He added, \"We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs.\"</p><p data-tmdatatrack=\"content-unit\" data-tmdatatrack-type=\"paragraph\" publication=\"themirror\">Amazon has been reached out for comment.</p>","contentLength":2299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oi8a0g/amazon_to_cut_30000_jobs_worldwide_as_workers_to/"},{"title":"Live Coding Trance","url":"https://youtu.be/GWXCCBsOMSg?si=Fes_0cptjd1yOPvG","date":1761655104,"author":"/u/DelilahsDarkThoughts","guid":319177,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oi8818/live_coding_trance/"},{"title":"Kubernetes homelab","url":"https://www.reddit.com/r/kubernetes/comments/1oi7llz/kubernetes_homelab/","date":1761653360,"author":"/u/kiroxops","guid":319175,"unread":true,"content":"<p>Hello guys I’ve just finished my internship in the DevOps/cloud field, working with GKE, Terraform, Terragrunt and many more tools. I’m now curious to deepen my foundation: do you recommend investing money to build a homelab setup? Is it worth it? And if yes how much do you think it can cost?</p>","contentLength":297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KubeCon NA 2025 - first time visitor, any advice?","url":"https://www.reddit.com/r/kubernetes/comments/1oi74yr/kubecon_na_2025_first_time_visitor_any_advice/","date":1761652007,"author":"/u/No_Dimension_3874","guid":319135,"unread":true,"content":"<div><p>I’ll be attending KubeCon NA for the first time and would love some advice from those who’ve been before.</p><ul><li> worth attending or tracks to prioritize</li><li><strong>Happy hours or side events</strong> that are a must-go</li></ul><p>I’m super excited but also a bit overwhelmed looking at the schedule. Appreciate any insights from seasoned KubeCon folks!</p></div>   submitted by   <a href=\"https://www.reddit.com/user/No_Dimension_3874\"> /u/No_Dimension_3874 </a>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cluster migration","url":"https://www.reddit.com/r/kubernetes/comments/1oi6jk7/cluster_migration/","date":1761650115,"author":"/u/L1lTun4C4n","guid":319432,"unread":true,"content":"<p>I am looking for a way to migrate a cluster from 1 cloud provider to another one (currently leaning more towards azure). What could be the best tools for this job? I am fairly new to the whole migration side of things.</p><p>Any and all tips would be helpfull!</p>","contentLength":253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI says over a million people talk to ChatGPT about suicide weekly","url":"https://techcrunch.com/2025/10/27/openai-says-over-a-million-people-talk-to-chatgpt-about-suicide-weekly/","date":1761647231,"author":"/u/MetaKnowing","guid":319178,"unread":true,"content":"<p>OpenAI released <a href=\"https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">new data</a> on Monday illustrating how many of ChatGPT’s users are struggling with mental health issues and talking to the AI chatbot about it. The company says that 0.15% of ChatGPT’s active users in a given week have “conversations that include explicit indicators of potential suicidal planning or intent.” Given that ChatGPT has more than 800 million weekly active users, that translates to more than a million people a week.</p><p>The company says a similar percentage of users show “heightened levels of emotional attachment to ChatGPT,” and that hundreds of thousands of people show signs of psychosis or mania in their weekly conversations with the AI chatbot.</p><p>OpenAI says these types of conversations in ChatGPT are “extremely rare,” and thus difficult to measure. That said, the company estimates these issues affect hundreds of thousands of people every week.</p><p>OpenAI shared the information as part of a broader announcement about its recent efforts to improve how models respond to users with mental health issues. The company claims its latest work on ChatGPT involved consulting with more than 170 mental health experts. OpenAI says these clinicians observed that the latest version of ChatGPT “responds more appropriately and consistently than earlier versions.”</p><p>Addressing mental health concerns in ChatGPT is quickly becoming an existential issue for OpenAI. The company is currently being <a href=\"https://techcrunch.com/2025/08/26/parents-sue-openai-over-chatgpts-role-in-sons-suicide/\" target=\"_blank\" rel=\"noreferrer noopener\">sued by the parents of a 16-year-old boy</a> who confided his suicidal thoughts to ChatGPT in the weeks leading up to his suicide. State attorneys general from California and Delaware — which could block the company’s planned restructuring — have also warned OpenAI that it <a href=\"https://www.politico.com/news/2025/09/05/california-delaware-ags-blast-openai-over-youth-safety-00546677\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">needs to protect young people</a> who use their products.</p><p>Earlier this month, OpenAI CEO Sam Altman claimed in a <a href=\"https://x.com/sama/status/1978129344598827128?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1978129344598827128%7Ctwgr%5E95fbf6288fb57a282d28e89d870a98e71a8387d5%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Ftechcrunch.com%2F2025%2F10%2F14%2Fsam-altman-says-chatgpt-will-soon-allow-erotica-for-adult-users%2F\" target=\"_blank\" rel=\"noreferrer noopener\">post on X</a> that the company has “been able to mitigate the serious mental health issues” in ChatGPT, though he did not provide specifics. The data shared on Monday appears to be evidence for that claim, though it raises broader issues about how widespread the problem is. Nevertheless, Altman said OpenAI would be relaxing some restrictions, even allowing adult users to start <a href=\"https://techcrunch.com/2025/10/14/sam-altman-says-chatgpt-will-soon-allow-erotica-for-adult-users/\" target=\"_blank\" rel=\"noreferrer noopener\">having erotic conversations</a> with the AI chatbot.</p><p>In the Monday announcement, OpenAI claims the recently updated version of GPT-5 responds with “desirable responses” to mental health issues roughly 65% more than the previous version. On an evaluation testing AI responses around suicidal conversations, OpenAI says its new GPT-5 model is 91% compliant with the company’s desired behaviors, compared to 77% for the previous GPT‑5 model.</p><p>The company also says its latest version of GPT-5 also holds up to OpenAI’s safeguards better in long conversations. OpenAI has previously flagged that its safeguards were less effective in long conversations.</p><p>On top of these efforts, OpenAI says it’s adding <a rel=\"nofollow\" href=\"https://cdn.openai.com/pdf/3da476af-b937-47fb-9931-88a851620101/addendum-to-gpt-5-system-card-sensitive-conversations.pdf\">new evaluations</a> to measure some of the most serious mental health challenges facing ChatGPT users. The company says its baseline safety testing for AI models will now include benchmarks for emotional reliance and non-suicidal mental health emergencies.</p><p>OpenAI has also recently rolled out more <a rel=\"nofollow\" href=\"https://openai.com/index/introducing-parental-controls/\">controls for parents</a> of children who use ChatGPT. The company says it’s building an age prediction system to automatically detect children using ChatGPT, and impose a stricter set of safeguards.</p><p>Still, it’s unclear how persistent the mental health challenges around ChatGPT will be. While GPT-5 seems to be an improvement over previous AI models in terms of safety, there still seems to be a slice of ChatGPT’s responses that OpenAI deems “undesirable.” OpenAI also still makes its older and less-safe AI models, including GPT-4o, available for millions of its paying subscribers.</p>","contentLength":3778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1oi5ove/openai_says_over_a_million_people_talk_to_chatgpt/"},{"title":"JSON Query - a small, flexible, and expandable JSON query language","url":"https://jsonquerylang.org/","date":1761646795,"author":"/u/BrewedDoritos","guid":319311,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oi5kjt/json_query_a_small_flexible_and_expandable_json/"},{"title":"When O3 is 2x slower than O2","url":"https://cat-solstice.github.io/test-pqueue/","date":1761645049,"author":"/u/cat_solstice","guid":319176,"unread":true,"content":"<p>While trying to optimize a piece of Rust code, I ran into a pathological case and I dug deep to try to understand the issue. At one point I decided to collect the data and write this article to share my journey and my findings.</p><p>This is my first post here, I'd love to get your feedback both on the topic and on the article itself!</p>","contentLength":329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1oi53kw/when_o3_is_2x_slower_than_o2/"},{"title":"Some monitoring issues","url":"https://www.reddit.com/r/kubernetes/comments/1oi4zno/some_monitoring_issues/","date":1761644631,"author":"/u/Always_smile_student","guid":319520,"unread":true,"content":"<p>I installed  on , but in , when I try to open  or , it says </p><ul><li><code>rke2 version v1.31.12+rke2r1</code></li><li><code>rke2 version v1.34.1+rke2r1</code></li></ul><p>In the 1.31 cluster, I can access Grafana and the other components through Rancher UI. In the 1.34 cluster, they’re not accessible.</p><p>I tried deleting , but after deletion, the icons in Rancher UI remained.</p><p>Since Rancher UI runs as pods, I tried restarting it by scaling the replicas down to 0 and then back up to 3. That didn’t help.</p><p>I can’t figure out what to do next.</p><p>In the 1.31 cluster, instead of kube-prometheus-stack, there’s an older release called . As far as I understand, it’s deprecated, because I can’t find its Helm release anymore.</p>","contentLength":668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Migrating and partitioning","url":"https://www.reddit.com/r/linux/comments/1oi4yj2/migrating_and_partitioning/","date":1761644511,"author":"/u/GrimboGhoul","guid":319113,"unread":true,"content":"<p>Windows 10 support is coming to an end and frankly I'm sick of the anti user direction of the OS so I've made the desicion to migrate.</p><p>My only concern is that I have some software I have paid for/ used my allowance of keys for, like davinci resolve, I'd rather not them purchase again if I can avoid it. How convenient/simple would it be to partition a hard drive to keep Windows 10 just to use it. Or alternatively, would it be smoother to have a seperate hardrive dedicated to these programs?</p><p>This may be a simple question but I haven't done something like this before and some guidance would be appreciated. Thanks.</p>","contentLength":616,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gecho: a response library for APIs","url":"https://www.reddit.com/r/golang/comments/1oi4okg/gecho_a_response_library_for_apis/","date":1761643402,"author":"/u/Theserverwithagoal","guid":319048,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m currently working on a concise (as few code as possible) response library written in Go. This because I was tired of writing the same code over and over again in every api I made. Gecho takes the response writer as param and produces a json response with - timestamp (time.Time) - success (bool) - message (preconfigured or custom) - data (your data) - status (http code)</p> <p>Configuration is on the way! Meaning you can use Unix timestamp instead of the default time.Time and leave out fields that are unnecessary to your app.</p> <p>All constructed feedback is welcome! Repo: <a href=\"https://github.com/MonkyMars/gecho\">https://github.com/MonkyMars/gecho</a></p> <p>The name is a blend of golang and echo, the Linux tool. And I love geckos:)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Theserverwithagoal\"> /u/Theserverwithagoal </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1oi4okg/gecho_a_response_library_for_apis/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oi4okg/gecho_a_response_library_for_apis/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can K8S Ingress Controller replace Standalone API Gateways?","url":"https://www.reddit.com/r/kubernetes/comments/1oi4fg5/can_k8s_ingress_controller_replace_standalone_api/","date":1761642373,"author":"/u/ColonelNein","guid":319111,"unread":true,"content":"<p>Just speaking about microservice architectures, where most enterprises use Kubernetes to orchestrate their workloads. </p><p>Vendors like Kong or APISIX offer API Gateways that can also be deployed as a Kubernetes Ingress Controller. Basically, a controller is deployed that monitors yml configuration files and dynamically configures the API Gateway with those.</p><p>I'm thinking about writing my bachelor's thesis about the question of whether Kubernetes ingress controllers can fully replace standalone API gateways and I'd like to know your thoughts there. </p><p>AFAIK, Kong and APISIX are as feature-rich (via Plugins) as, e.g., Azure API Management, even Auth via OIDC, RateLimiting, Developer Portal, and Monetization is possible. So why put an additional layer in front of the K8s ingress, adding latency and cost? For now, I see two reasons why that would not work out:<p> - Multi Cluster Architectures </p></p><p>- Routes are not always to microservices running inside the cluster, maybe also to serverless functions or directly to databases. Although I think an option would also be to just route back out of the cluster</p>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A hard rain's a-gonna fall: decoding JSON in Rust — Bitfield Consulting","url":"https://bitfieldconsulting.com/posts/hard-rain-json-rust","date":1761641168,"author":"/u/bitfieldconsulting","guid":319412,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1oi45dp/a_hard_rains_agonna_fall_decoding_json_in_rust/"},{"title":"[R] Review of a ML application to Parkinson's disease diagnosis paper","url":"https://www.reddit.com/r/MachineLearning/comments/1oi3cc5/r_review_of_a_ml_application_to_parkinsons/","date":1761637754,"author":"/u/luisggon","guid":319016,"unread":true,"content":"<p>Hi all! I was asked to review a paper about application of ML to Parkinson's disease diagnosis. I have spotted some weak points, but I wouls like to know what would you look at when reviewing a ML paper. Thank you very much in advance!!</p>","contentLength":236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Java has released a new early access JDK build that includes Value Classes!","url":"https://inside.java/2025/10/27/try-jep-401-value-classes/","date":1761624713,"author":"/u/davidalayachew","guid":318995,"unread":true,"content":"<p>The Valhalla team recently published an early-access JDK build that fully implements <a href=\"https://openjdk.org/jeps/401\">JEP 401: Value Classes and Objects (Preview)</a>.\nThere’s still a lot of work to do to get this code into a future release of the JDK.\nUntil then, now is a great time for anyone who is interested to try out this transformative new feature!</p><h2>Getting the Early-Access Builds</h2><p>Unzip the package, put it somewhere handy, and refer to its  directory to run commands like  and .\nOn my Mac, I’ll set an environment variable for easy access to these commands in the examples below:</p><div><div><pre><code>% -&gt; export jdk401=\"$PWD/jdk-26.jdk/Contents/Home/bin\"\n\n% -&gt; \"$jdk401\"/java --version\nopenjdk 26-jep401ea2 2026-03-17\nOpenJDK Runtime Environment (build 26-jep401ea2+1-1)\nOpenJDK 64-Bit Server VM (build 26-jep401ea2+1-1, mixed mode, sharing)\n</code></pre></div></div><h2>Experimenting with Value Objects</h2><p>As the JEP explains,  are instances of , which have only  fields and lack object identity.\nA handful of JDK classes, including  and , become value classes when we run Java in preview mode.</p><p>In JShell,  makes it easy to tell which objects are value objects and which are regular :</p><div><div><pre><code>% -&gt; \"$jdk401\"/jshell --enable-preview\n|  Welcome to JShell -- Version 26-jep401ea2\n|  For an introduction type: /help intro\n\njshell&gt; Objects.hasIdentity(Integer.valueOf(123))\n$1 ==&gt; false\n\njshell&gt; Objects.hasIdentity(\"abc\")\n$2 ==&gt; true\n\njshell&gt; Objects.hasIdentity(LocalDate.now())\n$3 ==&gt; false\n\njshell&gt; Objects.hasIdentity(new ArrayList&lt;&gt;())\n$4 ==&gt; true\n</code></pre></div></div><p>Value objects behave just like identity objects in most ways.\nBut one difference is that  can’t tell whether two value objects are “the same object” or not—they have no identity to compare.\nInstead,  tests whether two value objects are : instances of the same class with the same field values.</p><div><div><pre><code>jshell&gt; LocalDate d1 = LocalDate.now()\nd1 ==&gt; 2025-10-23\n\njshell&gt; LocalDate d2 = d1.plusDays(365)\nd2 ==&gt; 2026-10-23\n\njshell&gt; LocalDate d3 = d2.minusDays(365)\nd3 ==&gt; 2025-10-23\n\njshell&gt; d1 == d3\n$8 ==&gt; true\n</code></pre></div></div><p>Statewise equivalence is no substitute for a meaningful  method designed by a class author. In some cases, two instances of a value class with different states should still be considered equal. \nSo the best practice, as usual, is to avoid the  operator and prefer  for comparisons.</p><p>You can declare your own value classes with the  keyword.\nMany record declarations are good candidates to be value classes:</p><div><div><pre><code>jshell&gt; value record Point(int x, int y) {}\n|  created record Point\n\njshell&gt; Point p = new Point(17, 3)\np ==&gt; Point[x=17, y=3]\n\njshell&gt; Objects.hasIdentity(p)\n$11 ==&gt; false\n\njshell&gt; new Point(17, 3) == p\n$12 ==&gt; true\n</code></pre></div></div><p>Why bother to declare a value class instead of regular identity class?</p><p>One reason is a semantic one:\nIf your class represents immutable domain values that are interchangeable when they have the same state, giving these objects all the features of identity just adds unnecessary complexity.\nBetter to declare a value class and give up identity entirely.</p><p>But the most compelling reason is that the JVM can optimize value objects in ways that are impossible for regular objects.\nFor example, a reference to a value object doesn’t have to point to a canonical memory location for that object.\nInstead, the state of the object can be <em>embedded in the reference itself</em>.\nThis technique is called , and can make a huge difference in the cost of loading objects from memory.</p><p>As a test, let’s create a very large array of  value objects and add up all of their year values.\nTo simulate a realistic distribution of objects in memory, we’ll populate the array from an unsorted  of  objects.\nWe can do some rudimentary profiling by tracking the wall-clock time required to iterate through the array.\n(Note: For more accurate profiling, <a href=\"https://github.com/openjdk/jmh\">JMH</a> should be used.)</p><div><div><pre><code></code></pre></div></div><p>As a baseline, when I put this code in a  file and run it on my MacBook Pro  preview features enabled, I get the following:</p><div><div><pre><code>% -&gt; \"$jdk401\"/java DateTest.java\nAttempt 1: 82.703\nAttempt 2: 77.716\nAttempt 3: 74.959\nAttempt 4: 71.962\nAttempt 5: 71.915\n</code></pre></div></div><p>When I turn on preview features,  becomes a value class, and its instances can be flattened directly in the array.\nBy avoiding extra memory loads, the JVM can achieve a nearly 3x speedup!:</p><div><div><pre><code>% -&gt; \"$jdk401\"/java --enable-preview DateTest.java\nAttempt 1: 41.959\nAttempt 2: 38.992\nAttempt 3: 25.466\nAttempt 4: 28.404\nAttempt 5: 25.027\n</code></pre></div></div><p>Results will vary on different machines and different array sizes.\nBut the point is that by using value objects in our performance-critical computation, we’ve enabled the JVM to make significant new optimizations that are impossible for identity objects.</p><p>This is beta software, and it’s sure to have some bugs and surprising performance pitfalls.\nNow is a great time for interested users to download the early-access build and try it out on their performance-sensitive workloads.\nFeedback at  is welcome and encouraged!</p><p>Of course, sprinkling the  keyword around a code base is not going to automatically address whatever performance bottlenecks the program faces.\nUsers are encouraged to review <a href=\"https://openjdk.org/jeps/401\">JEP 401</a> to get a better sense of what kind of optimizations are possible, and use profiling tools like JDK Flight Recorder to see how value objects affect their program’s performance.</p>","contentLength":5192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ohzz8u/java_has_released_a_new_early_access_jdk_build/"},{"title":"Golang Linter for detecting SQL Transaction Begin, Commit and Rollback","url":"https://www.reddit.com/r/golang/comments/1ohzdg4/golang_linter_for_detecting_sql_transaction_begin/","date":1761622732,"author":"/u/Equivalent-Ticket990","guid":318950,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi! I’m looking for a Go linter or a golangci-lint plugin that can detect unclosed SQL transactions (e.g., missing Commit() or Rollback()), whether using pgx, libpq, or any other driver.</p> <p>We’re dealing with a large codebase and sometimes run into issues where SQL transaction blocks aren’t properly handled. Has anyone faced a similar problem or found a good tool to catch this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Equivalent-Ticket990\"> /u/Equivalent-Ticket990 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ohzdg4/golang_linter_for_detecting_sql_transaction_begin/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohzdg4/golang_linter_for_detecting_sql_transaction_begin/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US government uses Halo images in a call to 'destroy' immigration, Microsoft declines to comment","url":"https://www.pcgamer.com/games/us-government-uses-halo-images-in-a-call-to-destroy-immigration-microsoft-declines-to-comment/","date":1761622321,"author":"/u/esporx","guid":319017,"unread":true,"content":"<p>Just a few days after the surprise announcement of <a data-analytics-id=\"inline-link\" href=\"https://www.pcgamer.com/games/fps/halo-1-campaign-evolved-remake/\" data-before-rewrite-localise=\"https://www.pcgamer.com/games/fps/halo-1-campaign-evolved-remake/\">Halo: Campaign Evolved</a>, the US government has co-opted imagery from the game to promote its efforts to deport millions of people from the country. The post continues a pattern by the Trump administration of co-opting copyrighted material without permission for use as propaganda.</p><p>It began last Saturday on X with a sort of faux diplomatic communique from GameStop declaring the end of the console wars, brought about by the announcement that the Halo remake is headed to the PlayStation 5. In response, the White House—yes, the actual, real White House, seat of power of the world's last remaining superpower—posted an AI-generated image of US president Donald Trump as Master Chief, presumably a play on Trump's inaccurate but persistent claim that he has now <a data-analytics-id=\"inline-link\" href=\"https://tinyurl.com/yc7ac6pc\" target=\"_blank\" data-url=\"https://tinyurl.com/yc7ac6pc\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">ended eight wars in just eight months</a>.</p><a aria-hidden=\"true\" href=\"https://www.pcgamer.com/games/us-government-uses-halo-images-in-a-call-to-destroy-immigration-microsoft-declines-to-comment/\" data-url=\"\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><p>It's ridiculous and embarrassing, yes, and sadly symptomatic of how far the US has fallen, but really no worse than anything else that's come out of the White House social media account—like, for instance, referring to Trump as the \"Dealmaker-in-Chief\" as the global economy is roiled by his incessant, capricious trade wars.</p><p>What followed was much worse, however. Several hours after the White House post, the Department of Homeland Security put up its own Halo image with the message \"Destroy the Flood,\" and a link to the ICE recruitment page.</p><a href=\"https://x.com/DHSgov/status/1982819431894901043\" target=\"_blank\" data-url=\"https://x.com/DHSgov/status/1982819431894901043\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><figure data-bordeaux-image-check=\"\"><a href=\"https://x.com/DHSgov/status/1982819431894901043\" target=\"_blank\" data-url=\"https://x.com/DHSgov/status/1982819431894901043\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><div><a href=\"https://x.com/DHSgov/status/1982819431894901043\" target=\"_blank\" data-url=\"https://x.com/DHSgov/status/1982819431894901043\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><div><a href=\"https://x.com/DHSgov/status/1982819431894901043\" target=\"_blank\" data-url=\"https://x.com/DHSgov/status/1982819431894901043\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a></div></div><figcaption itemprop=\"caption description\"></figcaption></figure><p>Where the Trump-as-Master Chief post is merely cringeworthy, the Homeland Security message is flat-out dangerous. Comparing immigrants in the US to a parasitic alien life form that infects and annihilates advanced societies is not deeply offensive, it's also rooted in the worst of human history: As seen in the <a data-analytics-id=\"inline-link\" href=\"https://en.wikipedia.org/wiki/Untermensch\" target=\"_blank\" data-url=\"https://en.wikipedia.org/wiki/Untermensch\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a> of the Holocaust and \"<a data-analytics-id=\"inline-link\" href=\"https://www.theatlantic.com/ideas/archive/2019/04/rwanda-shows-how-hateful-speech-leads-violence/587041/\" target=\"_blank\" data-url=\"https://www.theatlantic.com/ideas/archive/2019/04/rwanda-shows-how-hateful-speech-leads-violence/587041/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">cockroaches</a>\" in Rwanda, to name a couple recent examples, dehumanizing the \"other\" so you can more easily inflict cruelty, injustice, and horrors upon them is hardly a new technique, and the US government's messaging was not subtle.</p><p>You might think that using imagery from one of its best known videogames in a call to \"destroy\" immigrants would prompt Microsoft to action, or at least to express some small modicum of disapproval. For now, at least, you would be wrong: Rather like Nintendo, which eagerly picks copyright fights it knows it can win but kept its mouth tightly zipped when <a data-analytics-id=\"inline-link\" href=\"https://www.pcgamer.com/games/pokemon-company-says-homeland-securitys-use-of-its-property-in-disturbing-promotional-video-was-unauthorized-but-dhs-doesnt-seem-to-care-to-arrest-them-is-our-real-test-to-deport-them-is-our-cause/\" data-before-rewrite-localise=\"https://www.pcgamer.com/games/pokemon-company-says-homeland-securitys-use-of-its-property-in-disturbing-promotional-video-was-unauthorized-but-dhs-doesnt-seem-to-care-to-arrest-them-is-our-real-test-to-deport-them-is-our-cause/\">Homeland Security used Pokémon to promote violent immigration raids</a>, a representative told PC Gamer that \"Microsoft does not have anything to share on this matter.\"</p><p>Discussion of the posts have also been forbidden in the Halo Discord, but the fan-operated <a data-analytics-id=\"inline-link\" href=\"https://www.reddit.com/r/halo/comments/1ohjpkg/white_house_twitter_posts_donald_trump_is_johns/\" target=\"_blank\" data-url=\"https://www.reddit.com/r/halo/comments/1ohjpkg/white_house_twitter_posts_donald_trump_is_johns/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">Halo subreddit</a> has taken a different tack: A message posted by the mod team says that while the usual policy is \"to remove politics and AI slop,\" there's clearly a desire to discuss these messages and so it's being allowed.</p><p>\"God I’m so tired of the White House acting like an edgy 17 year old,\" writes one of the <a data-analytics-id=\"inline-link\" href=\"https://www.reddit.com/r/halo/comments/1ohjpkg/comment/nlol07u/\" target=\"_blank\" data-url=\"https://www.reddit.com/r/halo/comments/1ohjpkg/comment/nlol07u/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">most upvoted comments</a> in the thread. \"This is 'the police using The Punisher logo' levels of media illiteracy,\" comments <a data-analytics-id=\"inline-link\" href=\"https://www.reddit.com/r/halo/comments/1ohjpkg/comment/nlol3mz/\" target=\"_blank\" data-url=\"https://www.reddit.com/r/halo/comments/1ohjpkg/comment/nlol3mz/\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">another</a>.</p><p>GameStop, apparently comfortable spreading the US government's anti-immigrant propaganda, <a data-analytics-id=\"inline-link\" href=\"https://x.com/gamestop/status/1982654845166428607\" target=\"_blank\" data-url=\"https://x.com/gamestop/status/1982654845166428607\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">retweeted</a> the White House tweet with another image of Trump dressed as Master Chief, with the fat-faced JD Vance meme in the background as Cortana. What does it mean? I can't even begin to guess at this point—I can only force you to look at it, as I had to</p><a target=\"_blank\" href=\"https://www.pcgamer.com/games/us-government-uses-halo-images-in-a-call-to-destroy-immigration-microsoft-declines-to-comment/\" data-url=\"\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><figure data-bordeaux-image-check=\"\"><a target=\"_blank\" href=\"https://www.pcgamer.com/games/us-government-uses-halo-images-in-a-call-to-destroy-immigration-microsoft-declines-to-comment/\" data-url=\"\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><div><a target=\"_blank\" href=\"https://www.pcgamer.com/games/us-government-uses-halo-images-in-a-call-to-destroy-immigration-microsoft-declines-to-comment/\" data-url=\"\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a><div><a target=\"_blank\" href=\"https://www.pcgamer.com/games/us-government-uses-halo-images-in-a-call-to-destroy-immigration-microsoft-declines-to-comment/\" data-url=\"\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\"></a></div></div></figure>","contentLength":3370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ohz8tz/us_government_uses_halo_images_in_a_call_to/"},{"title":"Looking for testers: LinuxPlay, a fully tunable ultra-low-latency remote desktop","url":"https://www.reddit.com/r/linux/comments/1ohw88q/looking_for_testers_linuxplay_a_fully_tunable/","date":1761613554,"author":"/u/Techlm77","guid":318979,"unread":true,"content":"<p>I’ve been working on LinuxPlay for a while now, an open-source remote desktop and game streaming stack built from scratch for Linux.</p><p>Everything is configurable, from codec and encoder to bitrate, QP, GOP, tune, preset, and buffers.</p><p>It supports multiple monitors, controller input, clipboard sync, and drag-and-drop file upload.</p><p>Video, audio, and input all run over UDP, with TCP used only for the initial handshake.</p><p>It automatically adjusts between LAN and Wi-Fi, includes heartbeat recovery, a stats overlay, and an ultra mode (LAN only) that can reach sub-frame latency.</p><p>Looking for testers to try it, stress it, and share how it performs on your setup or GPU.</p>","contentLength":659,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strategies for scaling PostgreSQL (vertical scaling, horizontal scaling, and other high-availability strategies)","url":"https://www.pgedge.com/blog/scaling-postgres","date":1761611825,"author":"/u/pgEdge_Postgres","guid":319347,"unread":true,"content":"<p>Postgres has earned its reputation as one of the world's most robust and feature-rich open-source databases. But what happens when your application grows beyond what a single database instance can handle? When your user base explodes from thousands to millions, and your data grows from gigabytes to terabytes?</p><p>This is where Postgres scaling becomes critical. The good news is that Postgres offers multiple pathways to scale, each with its own advantages and use cases. Since pgEdge Distributed Postgres and pgEdge Enterprise Postgres are 100% Postgres, all of the scaling techniques that follow also apply to your pgEdge cluster.</p><p>In this comprehensive guide, we'll explore three fundamental scaling approaches:</p><ul><li><p>&nbsp;(enhancing the power of your current server)</p></li><li><p>&nbsp;(adding additional servers)</p></li><li><p>and &nbsp;strategies (ensuring your system remains online in the event of failures).</p></li></ul><p>Before diving into scaling strategies, it's crucial to understand how Postgres works under the hood. Unlike some databases that use threads, Postgres uses a <b>process-based architecture</b>. This design choice has significant implications for how we approach scaling.</p><h3><b>The Postgres Process Family</b></h3><p>When Postgres runs, it creates several specialized processes, each with a specific job:</p><ul><li><p>: The master process that coordinates everything else</p></li><li><p>: One for each client connection - they handle your SQL queries</p></li><li><p>: Manages the Write-Ahead Log, ensuring data durability</p></li><li><p>: Periodically flushes data from memory to disk</p></li><li><p>: Smooths out disk I/O by gradually writing data</p></li><li><p>: Clean up dead rows and maintain database health</p></li><li><p>: Handle data copying to other servers</p></li></ul><p>You can see these processes in action on any Postgres server:</p><pre><code>$ ps -ef | grep postgres\n54170 ?  00:00:00 postgres: postmaster\n54172 ?  00:00:00 postgres: checkpointer\n54173 ?  00:00:00 postgres: background writer\n54174 ?  00:00:00 postgres: walwriter\n54175 ?  00:00:00 postgres: autovacuum launcher</code></pre><p>Understanding this architecture is crucial because each process can become a bottleneck, and each has its own set of tuning parameters. Effective scaling often starts by optimizing these individual components.</p><p>Vertical scaling, also known as \"scaling up,\" means getting the most performance possible from a single Postgres server. This is often the most cost-effective first step in any scaling journey.</p><h3><b>Memory: The Foundation of Postgres Performance</b></h3><p>Postgres's performance is heavily dependent on memory configuration. Getting these settings right can often double or triple your database performance without adding any hardware.</p><h4><b>Shared Buffers: Your Database Cache</b></h4><p>The <a href=\"https://www.postgresql.org/docs/18/runtime-config-resource.html#GUC-SHARED-BUFFERS\" target=\"_blank\"></a>&nbsp;parameter controls the amount of memory Postgres allocates for caching data pages. Think of it as Postgres's primary workspace. On a server with 64GB of RAM, you might set this to 16-24GB:</p><pre><code>postgres=# ALTER SYSTEM SET shared_buffers='16GB';</code></pre><p>This means Postgres will keep 16GB worth of your most frequently accessed data in memory, dramatically reducing disk I/O for common queries.</p><h4><b>Work Memory: Per-Operation Workspace</b></h4><p><a href=\"https://www.postgresql.org/docs/18/runtime-config-resource.html#GUC-WORK-MEM\" target=\"_blank\"></a><a href=\"https://www.postgresql.org/docs/18/runtime-config-resource.html#GUC-WORK-MEM\" target=\"_blank\"></a>is allocated for each sort, join, or hash operation in your queries. This value can make an enormous difference for complex queries. This example shows the difference proper settings make when querying a real-world example with 11 million rows:</p><p><b>With insufficient work memory (2MB):</b></p><pre><code>postgres=# SET work_mem='2MB';\npostgres=# EXPLAIN ANALYZE SELECT * FROM large_table ORDER BY id;\n-- Result: Sort Method: external merge Disk: 70208kB\n-- Execution Time: 13,724 ms (nearly 14 seconds!)</code></pre><p><b>With adequate work memory (1GB):</b></p><pre><code>postgres=# SET work_mem='1GB';\npostgres=# EXPLAIN ANALYZE SELECT * FROM large_table ORDER BY id;\n-- Result: Sort Method: quicksort Memory: 916136kB\n-- Execution Time: 7,757 ms (under 8 seconds)</code></pre><p>The query runs almost twice as fast simply by avoiding disk spills during sorting.</p><p><b>Maintenance Work Memory: For Big Operations</b></p><p><a href=\"https://www.postgresql.org/docs/18/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM\" target=\"_blank\"></a><a href=\"https://www.postgresql.org/docs/18/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM\" target=\"_blank\"></a>affects operations like , , and . Setting this higher can significantly speed up maintenance operations:</p><pre><code>-- Before: 12.4 seconds to create index\npostgres=# SET maintenance_work_mem='10MB';\npostgres=# CREATE INDEX idx_foo ON large_table(id);\n-- Time: 12374.931 ms\n\n-- After: 9.6 seconds to create the same index\npostgres=# SET maintenance_work_mem='1GB';\npostgres=# CREATE INDEX idx_foo ON large_table(id);\n-- Time: 9550.766 ms</code></pre><h3><b>Write-Ahead Log (WAL) Tuning: Balancing Safety and Speed</b></h3><p>Postgres's WAL system ensures your data survives crashes, but it also affects performance. Understanding these trade-offs is crucial:</p><h4><b>Synchronous Commit: The Safety vs Speed Trade-off</b></h4><ul><li><p><a href=\"https://www.postgresql.org/docs/18/runtime-config-wal.html#GUC-SYNCHRONOUS-COMMIT\" target=\"_blank\"></a>=on&nbsp;(default): Safest option, but commits wait for WAL to be written to disk</p></li><li><p>synchronous_commit=off: Much faster, but you might lose a few seconds of data in a crash</p></li></ul><h4><b>Checkpoint Configuration: Managing I/O Spikes</b></h4><p>A <a href=\"https://www.postgresql.org/docs/18/runtime-config-wal.html#RUNTIME-CONFIG-WAL-CHECKPOINTS\" target=\"_blank\"></a><a href=\"https://www.postgresql.org/docs/18/runtime-config-wal.html#RUNTIME-CONFIG-WAL-CHECKPOINTS\" target=\"_blank\"></a>is when Postgres flushes modified data to disk. Checkpoints are necessary for crash recovery, but they can cause performance spikes:</p><pre><code>-- Longer checkpoint intervals improve performance but increase crash recovery time\npostgres=# ALTER SYSTEM SET checkpoint_timeout='30min';\npostgres=# ALTER SYSTEM SET max_wal_size='10GB';</code></pre><h3><b>Storage: The Often-Overlooked Foundation</b></h3><p>No amount of memory tuning can overcome slow storage. Postgres's MVCC (<a href=\"https://www.postgresql.org/docs/18/mvcc-intro.html#MVCC-INTRO\" target=\"_blank\"><u>Multi-Version Concurrency Control</u></a>) system means each update creates a new row version rather than overwriting existing ones. This makes fast storage absolutely critical for write-heavy workloads.</p><h4><b>Storage hierarchy impact:</b></h4><ul><li><p>: Excellent for all workloads</p></li><li><p>: Good for most applications</p></li><li><p>: Acceptable only for read-heavy workloads or small databases</p></li></ul><h2><b>Horizontal Scaling: Adding More Servers to the Mix</b></h2><p>When a single server reaches its limits, horizontal scaling distributes the workload across multiple machines. Postgres offers several approaches to achieve this.</p><p>A robust cluster, architected across multiple availability zones that uses the Spock extension to ensure data consistency is a dependable way to ensure data consistency and performance.</p><h4><b>Read Replicas: Scaling Read Operations</b></h4><p>Streaming replication allows you to create read-only copies of your primary database. This type of scaling is perfect for applications with heavy read workloads.</p><p><b>Setting up streaming replication:</b></p><p>1. , create a replication user:</p><pre><code>postgres=# CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'secure_password';</code></pre><p>2. &nbsp;in </p><pre><code>host replication replicator 10.0.0.2/32 md5</code></pre><p>3. , create the replica:</p><pre><code>$ pg_basebackup -h 10.0.0.1 -D /var/lib/postgresql/15/main -U replicator -P -R</code></pre><p>It's that simple to create a read replica that stays synchronized with your primary database. Your application can send read queries to the replica, reducing the load on the primary.</p><h3><b>Logical Replication: Selective Data Distribution</b></h3><p>Unlike streaming replication, which copies everything, logical replication lets you choose precisely what to replicate. This is incredibly useful for:</p><ul><li><p>: Each service gets only the data it needs.</p></li><li><p>: Replicate only those tables needed for analytics.</p></li><li><p>: You can gradually move specific tables to new systems.</p></li></ul><pre><code>-- On the source database\npostgres=# CREATE PUBLICATION sales_pub FOR TABLE customers, orders, products;\n\n-- On the destination database\npostgres=# CREATE SUBSCRIPTION sales_sub\n  CONNECTION 'host=source.db.com dbname=main user=replicator'\n  PUBLICATION sales_pub;</code></pre><h3><b>Table Partitioning: Divide and Conquer Large Tables</b></h3><p>Partitioning splits large tables into smaller, more manageable pieces. This is especially effective for time-series data or geographically distributed data.</p><p><b>Example: Partitioning sales data by month:</b></p><pre><code>CREATE TABLE sales (\n    id bigserial,\n    sale_date date NOT NULL,\n    amount numeric,\n    customer_id bigint\n) PARTITION BY RANGE (sale_date);\n\nCREATE TABLE sales_2024_01 PARTITION OF sales\n    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\nCREATE TABLE sales_2024_02 PARTITION OF sales\n    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');</code></pre><p>Now when you query for January sales, Postgres only examines the January partition, dramatically reducing query time.</p><h3><b>Sharding: The Ultimate Scale-Out Strategy</b></h3><p>For truly massive datasets, sharding distributes different rows across different servers. For example, a ride-sharing app might use a dataset that is sharded by city:</p><ul><li><p>: All New York trips</p></li><li><p>: All London trips</p></li><li><p>: All Tokyo trips</p></li></ul><p>pgEdge Enterprise Postgres and pgEdge Distributed Postgres provide a cluster environment that allows you to seamlessly add and remove cluster nodes as needed from an Active-Active distributed cluster. &nbsp;This lets you achieve the maximum benefit from sharding your cluster with the Spock extension.</p><p>Other extensions like Citus make sharding more manageable by providing a distributed query engine that can join data across shards, but the Spock extension adds support to help you meet international data residency requirements, data consistency checks to prevent transaction conflicts, and functions and procedures that refine management tasks.</p><h2><b>High Availability: Keeping Your Database Online</b></h2><p>Scaling for performance means nothing if your database goes down. High availability ensures your system stays online even when individual components fail.</p><h3><b>Automated Failover: When the Primary Goes Down</b></h3><p>Automated failover ensures high availability in Postgres clusters by swiftly transitioning to a new primary server when the original fails. Tools like Patroni orchestrate this process, minimizing downtime to seconds. Here’s an expanded overview of the failover sequence:</p><ul><li><p>: The primary Postgres server becomes unavailable due to hardware issues, network failures, or crashes. Patroni monitors the cluster via health checks and a distributed consensus system (e.g., etcd). If the primary stops renewing its leader key in the consensus store within a set timeout (e.g., 10 seconds), it’s flagged as down. Network partitions are mitigated to prevent false positives, ensuring the old primary is fenced off to avoid data conflicts.</p></li><li><p><b>Patroni Detects the Failure</b>: Patroni nodes, communicating through a Distributed Configuration Store (DCS), confirm the primary’s failure when its leader key expires. Configuration settings like ttl and loop_wait determine detection speed, balancing responsiveness with stability to avoid premature failovers.</p></li><li><p><b>Patroni Promotes the Best Replica</b>: Patroni selects a replica based on minimal replication lag, node health, and configured priorities. The chosen replica is promoted to primary using Postgres’s pg_promote command, and the DCS is updated to reflect the new leader. Other replicas reconfigure to follow the new primary.</p></li><li><p>: Applications use connection pooling (e.g., PgBouncer) or updated DNS to reconnect to the new primary. Patroni ensures a smooth transition by maintaining consistent connection details.</p></li><li><p>: The failed server is rebuilt, often using tools like pg_rewind to synchronize it with the new primary, then rejoins as a replica.</p></li></ul><h3><b>Load Balancing: Distributing Traffic Intelligently</b></h3><p>Load balancing enhances the performance and scalability of Postgres clusters by intelligently distributing database traffic across servers. Tools like HAProxy and pgpool-II manage this process, directing write queries to the primary server and distributing read queries across replicas to optimize resource utilization and reduce latency.</p><ul><li><p>&nbsp;is a high-performance TCP/HTTP load balancer that routes queries based on connection details or application-layer information. It sends write queries (e.g., INSERT, UPDATE) to the primary server, identified via integration with tools like Patroni, which updates HAProxy’s configuration to track the current primary. Read queries (e.g., SELECT) are distributed across replicas to leverage their processing power, using strategies like round-robin or least-connections to balance the load. HAProxy can inspect query metadata to route traffic based on user, database, or query type, enabling fine-grained control (e.g., sending reporting queries to specific replicas).</p></li><li><p>, explicitly designed for Postgres, offers similar functionality with native database integration. It parses queries to direct writes to the primary and spreads reads across replicas, supporting session-based routing for consistent user experiences. It also offers advanced features, including connection pooling and query caching, to minimize database load. Both tools integrate seamlessly with Patroni-managed clusters, using health checks or DCS updates to detect the current primary and avoid routing to failed nodes.</p></li></ul><p><b>&nbsp;Connection Pooling: Managing High Concurrency</b></p><p>Connection pooling is a crucial tool for any active PostgreSQL application. Postgres&nbsp;spawns&nbsp;a new process for each connection, leading&nbsp;to high resource consumption when under heavy loads. Connection poolers address this need by multiplexing many client connections over a smaller number of database connections, significantly reducing overhead and improving scalability and performance. For instance, a pooler could manage 5,000 application connections using just 100 Postgres&nbsp;processes.</p><p>Some key connection poolers:</p><ul><li><p>&nbsp;A lightweight, single-threaded, C-based pooler designed for efficient connection pooling.</p></li><li><p>&nbsp;Supports session, transaction, and statement pooling modes for optimal connection reuse, with minimal overhead (one process).</p></li><li><p>&nbsp;Ideal for high-connection scenarios, easily integrates with Patroni for failover. Benchmarks often show superior transaction per second (TPS) performance compared to Pgpool-II. Simple to set up.</p></li><li><p>&nbsp;Lacks built-in load balancing, requiring multiple instances for scaling.</p></li><li><p>&nbsp;Primarily for pure connection pooling in high-availability (HA) setups.</p></li></ul><ul><li><p>&nbsp;A comprehensive, C-based, multi-process middleware offering pooling, load balancing (for read replicas), replication, and failover capabilities.</p></li><li><p>&nbsp;Parses queries for intelligent read/write splitting and includes query caching.</p></li><li><p>&nbsp;An all-in-one solution for HA clusters, effectively managing standby reads.</p></li><li><p>&nbsp;Higher overhead (defaulting to 32 child processes) and requires careful tuning for optimal pooling. Misconfiguration can lead to performance degradation (e.g., lower TPS in benchmarks).</p></li><li><p>&nbsp;Best suited for complex setups that require query routing and advanced features beyond basic pooling.</p></li></ul><ul><li><p><b>PgCat (Rust, multi-threaded):</b>&nbsp;A modern, PgBouncer-compatible alternative that includes load balancing, automatic sharding (key detection), and failover. Known for consistent performance and low latency at scale, making it excellent for distributed/sharded Postgres&nbsp;environments and capable of handling over 100K queries per second in production.</p></li><li><p><a href=\"https://github.com/yandex/odyssey\" target=\"_blank\"><b><u>Odyssey (C, multi-threaded):</u></b></a>&nbsp;Developed by Yandex, this scalable pooler utilizes asynchronous coroutines, supports TLS, various authentication methods (PAM/LDAP), and prevents stale reads. It handles high loads efficiently and supports transaction pooling for prepared statements, making it suitable for cloud and large-core systems.</p></li><li><p><a href=\"https://github.com/supabase/supavisor\" target=\"_blank\"></a>&nbsp;A cloud-native solution designed for millions of connections, offering zero-downtime scaling for serverless environments. It provides competitive performance comparable to PgBouncer and PgCat,&nbsp;and is used as a replacement for PgBouncer in Supabase.</p></li><li><p><a href=\"https://agroal.github.io/pgagroal/\" target=\"_blank\"></a>&nbsp;A multi-threaded pooler focused on performance, similar to Odyssey but with a lighter feature set.</p></li></ul><h3><b>Backup and Recovery: Your Safety Net</b></h3><p><b>Point-in-Time Recovery (PITR)</b>&nbsp;with tools like &nbsp;allows you to recover your database to any point in time:</p><p># Recover to exactly 10 minutes before the accident</p><pre><code>pgbackrest restore --stanza=main --target=\"2024-09-16 14:30:00\" --type=time</code></pre><p>This is invaluable when someone accidentally drops a table or corrupts important data.</p><h2><b>A Real-World Scaling Journey</b></h2><p>Let's follow a typical application through its scaling evolution:</p><h3><b>Stage 1: Single Server (0-100 users)</b></h3><ul><li><p>: One Postgres server with properly tuned memory settings</p></li><li><p>: Optimize , , and upgrade to SSD storage</p></li><li><p>: Handles moderate read/write loads comfortably</p></li></ul><h3><b>Stage 2: Read Replicas (10,000-100,000 users)</b></h3><ul><li><p>: Two read replicas to handle growing query load</p></li><li><p>: Read queries distributed across three servers</p></li><li><p>: Separate reporting queries from transactional workload</p></li></ul><h3><b>Stage 3: Partitioning (driven by data size and usage)</b></h3><ul><li><p>: Historical data tables are becoming too large</p></li><li><p>: Partition large tables by date or category</p></li><li><p>: Queries on recent data remain fast despite growing archive</p></li></ul><h3><b>Stage 4: High Availability (meeting SLA/RTO/RPO requirements)</b></h3><ul><li><p>: 99.9% uptime becomes business-critical</p></li><li><p>: Patroni for automatic failover, load balancing, and comprehensive monitoring</p></li><li><p>: System survives server failures with minimal disruption</p></li></ul><h3><b>Stage 5: Advanced Scaling (when you have Millions of users)</b></h3><ul><li><p>: Sharding with Citus, multiple database clusters, global distribution</p></li><li><p>: Complex but handles massive scale</p></li></ul><h2><b>Monitoring: Your Crystal Ball</b></h2><p>Effective scaling requires visibility into your database performance. Key metrics to monitor:</p><ul><li><p>&nbsp;Track whether you're nearing <code>the maximum number of connections</code>.</p></li><li><p>&nbsp;Ensure your replicas are keeping pace.</p></li><li><p>&nbsp;Identify if checkpoints are causing performance spikes.</p></li><li><p>&nbsp;Pinpoint your slowest queries.</p></li><li><p>&nbsp;Monitor for dwindling storage space.</p></li></ul><p>Tools like and&nbsp;offer real-time dashboards for comprehensive oversight, while &nbsp;is invaluable for identifying underperforming queries.</p><h2><b>Future-Proofing Your Postgres Architecture</b></h2><p>Postgres continues to evolve rapidly, providing:</p><ul><li><p>: Better query pruning and automatic partition management</p></li><li><p><b>Enhanced logical replication</b>: More granular control and better conflict resolution</p></li><li><p>: Better integration with Kubernetes and cloud platforms</p></li><li><p>: Extensions like Citus and pgEdge bring distributed database capabilities</p></li></ul>","contentLength":17277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ohvlhi/strategies_for_scaling_postgresql_vertical/"},{"title":"Explore Flex parameters to space, align and size widgets. Clear visuals and real examples.","url":"https://www.reddit.com/r/golang/comments/1ohv293/explore_flex_parameters_to_space_align_and_size/","date":1761610338,"author":"/u/Warm_Low_4155","guid":317072,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Understanding layout.Flex and its parameters is key to master Golang Gio UI layouts.<br/> In this video, I explain layout.Flex parameters — how to align, space, and size widgets like a pro.”<br/> Axis • Spacing • Alignment • WeightSum — clear, visual, and beginner-friendly.</p> <p><a href=\"https://youtu.be/tpyrwmzRQkU\">Flex Explained: Spacing and Alignment Made Easy</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Warm_Low_4155\"> /u/Warm_Low_4155 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ohv293/explore_flex_parameters_to_space_align_and_size/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohv293/explore_flex_parameters_to_space_align_and_size/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fedora 43 is out","url":"https://www.reddit.com/r/linux/comments/1ohuy2l/fedora_43_is_out/","date":1761610014,"author":"/u/TheNavyCrow","guid":317071,"unread":true,"content":"<div><p>only the torrents for now. it will be in the main site tomorrow.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/TheNavyCrow\"> /u/TheNavyCrow </a>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anyone installed Karpenter on AKS?","url":"https://www.reddit.com/r/kubernetes/comments/1ohuwvu/anyone_installed_karpenter_on_aks/","date":1761609923,"author":"/u/DreadMarvaz","guid":317088,"unread":true,"content":"<p>Hi guys So, anyone installed Karpenter on AKS using Helm? Is it working fine? Remember couple month ago was full of bugs.. but IIRC a new stable version came up</p><p>Appreciate some insights on this </p>","contentLength":193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"sux - speedy mux HTTP router","url":"https://www.reddit.com/r/golang/comments/1ohuel2/sux_speedy_mux_http_router/","date":1761608603,"author":"/u/DarqOnReddit","guid":317049,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1ohuel2/sux_speedy_mux_http_router/\"> <img src=\"https://external-preview.redd.it/L7UUFuOa9HiVo7xpaOxuXTt9H0o3PHf_ZrSkqs57x1Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3f8bfb7847e28c6d3dcdffaa5ebb7011a14d3e0\" alt=\"sux - speedy mux HTTP router\" title=\"sux - speedy mux HTTP router\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Updated my static route HTTP router from 2014 to have support for parameters, middleware, and route groups. Performance is slightly better than github.com/julienschmidt/httprouter in most scenarios, while having more features and the same number of allocs.</p> <p><code> git clone https://code.icod.de/dalu/sux.git cd sux go test -bench=. -benchmem </code></p> <p>AI was used, GLM-4.6 with temperature 0.6</p> <p>I think this demonstrates that even a not so good LLM can do great things with a proper base.</p> <p>I did a few performance improvement versions, but this version is the best.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DarqOnReddit\"> /u/DarqOnReddit </a> <br/> <span><a href=\"https://code.icod.de/dalu/sux\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohuel2/sux_speedy_mux_http_router/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthropic has launched financial services.","url":"https://www.anthropic.com/news/advancing-claude-for-financial-services","date":1761607084,"author":"/u/zshm","guid":317091,"unread":true,"content":"<p>We're expanding <a href=\"https://www.claude.com/solutions/financial-services\" target=\"_blank\" rel=\"noopener noreferrer\">Claude for Financial Services</a> with an Excel add-in, additional connectors to real-time market data and portfolio analytics, and new pre-built Agent Skills, like building discounted cash flow models and initiating coverage reports.</p><p>These updates build on Sonnet 4.5’s state of the art performance on financial tasks, topping the <a href=\"https://www.vals.ai/benchmarks/finance_agent\" target=\"_blank\" rel=\"noopener noreferrer\">Finance Agent benchmark</a> from Vals AI at 55.3% accuracy. They augment Claude’s intelligence with solutions for time-consuming but critical financial work, built into preferred industry tools.</p><p>We’re releasing <a href=\"https://claude.com/claude-for-excel\" target=\"_blank\" rel=\"noopener noreferrer\">Claude for Excel</a> in beta as a research preview. This allows users to work directly with Claude in a sidebar in Microsoft Excel, where Claude can read, analyze, modify, and create new Excel workbooks. Claude provides full transparency about the actions it takes: it tracks and explains its changes and lets users navigate directly to the cells it references in its explanations.</p><p>This means that Claude can discuss how a spreadsheet works, modify it while preserving its structure and formula dependencies, debug and fix cell formulas, populate templates with new data and assumptions, or build new spreadsheets entirely from scratch.</p><p>Claude for Excel adds to our existing integrations with Microsoft’s applications. In the Claude apps, Claude can also create and edit files, including Excel spreadsheets and PowerPoint slides, and connect to Microsoft 365 to search for files, emails, and Teams conversations. Select Claude models are also available in Microsoft Copilot Studio and Researcher agent.</p><p>Claude for Excel is now in beta as a research preview for Max, Enterprise, and Teams users. We’ll collect real-world feedback from 1,000 initial users before rolling the feature out more broadly. To join the waitlist, <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSedsdrIw00BOGbiIhAQvTaC7mOQRW6jOofAt7PJ1lYAGzvfUw/viewform?usp=dialog\" target=\"_blank\" rel=\"noopener noreferrer\">click here</a>.</p><h2>Connecting Claude to live information</h2><p><a href=\"https://claude.ai/redirect/website.v1.6806316f-a188-4a82-9890-cfd822113d96/settings/connectors\" target=\"_blank\" rel=\"noopener noreferrer\"></a> provide Claude with direct access to external tools and platforms. <a href=\"https://www.anthropic.com/news/claude-for-financial-services\" target=\"_blank\" rel=\"noopener noreferrer\">In July</a>, we added connectors for S&amp;P Capital IQ, Daloopa, Morningstar, and PitchBook. We’re adding new connectors that give Claude immediate access to more information in real time:</p><ul><li> provides Claude with real-time earnings call transcripts and summaries of investor events, like shareholder meetings, presentations, and conferences;</li><li>Aiera’s connector also enables a data feed from , which gives Claude access to a library of insights interviews, company intelligence, and industry analysis from experts and former executives;</li><li> gives private equity investors operational and financialinformation for portfolio monitoring and conducting due diligence, including performance metrics, valuations, and fund-level data;</li><li> enables Claude to securely search permitted data for internal data rooms, investment documents, and approved financial models, while maintaining governed access controls;</li><li> connects Claude to live market data, including fixed income pricing, equities, foreign exchange rates, macroeconomic indicators, and analysts’ estimates of other important financial metrics;</li><li> provides access to proprietary credit ratings, research, and company data – including ownership, financials and news on more than 600 million public and private companies – supporting work and research in compliance, credit analysis, and business development;</li><li> provides Claude with access to the latest global multi-asset class news on financial markets and economies.</li></ul><p>For details on MCP connector setup and prompting guidance to maximize the benefit of each connector, see <a href=\"https://support.claude.com/en/collections/13972013-claude-for-financial-services\" target=\"_blank\" rel=\"noopener noreferrer\">our documentation here</a>.</p><h2>New Agent Skills for finance tasks</h2><p>Earlier this month, we introduced <a href=\"https://www.anthropic.com/news/skills\" target=\"_blank\" rel=\"noopener noreferrer\">Agent Skills</a>. Skills are folders that include instructions, scripts, and resources that Claude can use to perform given tasks. Skills work across all Claude apps, including <a href=\"http://claude.ai/redirect/website.v1.6806316f-a188-4a82-9890-cfd822113d96\" target=\"_blank\" rel=\"noopener noreferrer\">Claude.ai</a>, Claude Code, and our API. To make Claude better at financial services tasks, we’ve added 6 new skills:</p><ul><li><strong>Comparable company analysis</strong>, with valuation multiples and operating metrics, which can be easily refreshed with updated data;</li><li><strong>Discounted cash flow models</strong>, including full free cash flow projections, WACC calculations, scenario toggles, and sensitivity tables;</li><li>, processing data room documents into Excel spreadsheets with financial information, customer lists, and contract terms;</li><li><strong>Company teasers and profiles</strong>, condensed company overviews for pitch books and buyer lists;</li><li>, which research quarterly transcripts and financials to extract important metrics, guidance changes, and management commentary;</li><li><strong>Initiating coverage reports</strong> with industry analysis, company deep-dives, and valuation frameworks.</li></ul><p>As with Claude for Excel, these new skills are being rolled out in preview for Max, Enterprise, and Teams users. You can sign up on behalf of your team or organization <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdXOB2bR7r_YhwENL1VplbgWvQ96YhInhHj5Fr9_V_MAOCiNQ/viewform\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p><h2>Claude’s impact in financial services</h2><p>Claude is already widely used by leading banking, asset management, insurance, and financial technology companies. It supports front office tasks like client experience, middle office tasks in underwriting, risk and compliance, and back office tasks like code modernization and legacy processes. With ongoing updates to our models and products specific to financial services, we expect Claude to become even better in roles like these.</p><p>Below, Alexander Bricken, Applied AI Lead for Financial Services, and Nicholas Lin, Head of Product for Financial Services, discuss Anthropic’s research and product strategy within financial services, as well as customer examples.</p><p>To learn more about using Claude for Financial Services, <a href=\"https://claude.com/solutions/financial-services\" target=\"_blank\" rel=\"noopener noreferrer\">see here</a> or <a href=\"https://claude.com/contact-sales/financial-services\" target=\"_blank\" rel=\"noopener noreferrer\">contact</a> our sales team. And to see the new features in action and hear directly from financial services leaders, you can also <a href=\"http://website.anthropic.com/webinars/%20claude-for-financial-services\" target=\"_blank\" rel=\"noopener noreferrer\">register here</a> for our launch webinar.</p>","contentLength":5626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ohttca/anthropic_has_launched_financial_services/"},{"title":"Boxcars is now Steam Deck verified! (Free online backgammon app)","url":"https://www.reddit.com/r/golang/comments/1ohsesn/boxcars_is_now_steam_deck_verified_free_online/","date":1761603582,"author":"/u/tslocum","guid":317031,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1ohsesn/boxcars_is_now_steam_deck_verified_free_online/\"> <img src=\"https://external-preview.redd.it/RQicuKx6KubJMM9c1RNllLcjRrVbyMhD63UzlqIGWMw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbe23e6a05b4b9464ba0715d2112082bc4d86374\" alt=\"Boxcars is now Steam Deck verified! (Free online backgammon app)\" title=\"Boxcars is now Steam Deck verified! (Free online backgammon app)\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tslocum\"> /u/tslocum </a> <br/> <span><a href=\"https://store.steampowered.com/verified/3182230/Boxcars\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohsesn/boxcars_is_now_steam_deck_verified_free_online/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Our security team wants us to stop using public container registries. What's the realistic alternative?","url":"https://www.reddit.com/r/kubernetes/comments/1ohqjx4/our_security_team_wants_us_to_stop_using_public/","date":1761599179,"author":"/u/miller70chev","guid":317013,"unread":true,"content":"<p>Our security team just dropped the hammer on pulling from Docker Hub and other public registries. I get the supply chain concerns, but we have 200+ microservices and teams that ship fast.</p><p>What's realistic? Private registry with curated base images or building our own? The compliance team is pushing hard but we need something that mess with our velocity. Looking for approaches that scale without making developers hate their lives. </p>","contentLength":433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No bug policy","url":"https://www.krayorn.com/posts/no_bug_policy/","date":1761598862,"author":"/u/_Krayorn_","guid":319200,"unread":true,"content":"<p>You should strive to have a .</p><p>No matter how undoable you think it is, how much disbelief you’ll get from other developers who have been working there too long, and say that it’s not possible. No matter all the “this doesn’t work in the real world” or “It can only work in a certain type of company”, you have to try.</p><p>I introduced this policy at my place of work two years ago. The system is simple, we have a Slack channel, and every bug is posted on that channel. The engineers from the team (~7-10 persons) pick them up when there is a new one and fix it. We got one emoji when you start working on it so that someone else doesn’t spend time checking it out, and another one once you’re done. We still have bugs, it’d be naive to think we don’t anymore, but we address all of them. Sometimes we get more bugs, which means our work was sloppy, so we slow down to fix them. Sometimes we get very few or no bugs.</p><p>What’s important is that we don’t have to ask ourselves “Should I fix it ?”, “How important is that ?”. We see an issue, we fix it. No time spent on triaging or prioritizing, which means it’s actually faster than not handling them. That’s the strength of a no bug policy.</p><p>I’m going to tell how the change happened at my place of work, with our specificities and quirks. This probably won’t work as is for your company, but that’s your job, not mine. Hopefully, this can still give you some ideas to make it work in your context.</p><p>Depending on your political capital (~= your influence), you might have different options like trying to get buy-in from your direct manager, your PM, and/or your team. I did a mix of that and another simpler option: I started fixing every single bug that was reported. Front, back, in parts of the codebase I knew well, in parts of the codebase I didn’t even know existed.\nIt means that for some time, I was only fixing bugs. Early on, the more bugs you fix, the more bugs are reported. It’s because the users now realized things were moving, they didn’t have to report only the most breaking bugs that blocked them, but little things that forced them to hard refresh the app, or a secondary button not working as expected were also getting fixed.\nThe system was healing.</p><p>If you have to, make a ticket for your manager or PM, but if possible, I’d advise against it, you want to reduce friction as much as possible. You want to get the bugs from as close to the users as possible, ask them yourself the questions if you have any, and then warn them when it’s fixed.</p><p>Hopefully, you won’t be alone for long fixing every bug raised. For me, the rest of the team quickly joined me in this practice for the following reasons:</p><p>Firstly, it makes sense to fix bugs, you want the software to work.</p><p>Secondly, look at how you react when you encounter bugs in products you often use, from Slack to Gmail, Cloudflare, or GitHub. There is nothing more annoying than when you report a bug and it doesn’t get fixed, that’s not the feeling you want in your users. Thirdly, we got some positive feedback fast, really fast. Our internal users were so happy, it made their work day easier, and it strengthened the relations between our teams by building trust. They were also able to use this to reinforce the trust they had with our end-users, our response time to fix issues and the overall quality of our app went up, it also helped them reassure customers.</p><p>If you’re swamped with projects and deadlines for new features and have no leeway at all, it’ll be more difficult. But most bugs aren’t juggernauts needing to refactor a whole service, they’re often simpler than they look. So it might be possible for you to fix one in the morning, or at the end of the workday after a meeting. The more bugs there are in the codebase, the more bugs you’ll introduce when modifying code to add capabilities anyway, so take some time to fix a few. It’s important that, even if you don’t spend all your time fixing bugs, you never stop fixing some.</p><p>You shouldn’t use “it’s not my company’s culture” as an excuse, you can impact the culture at your level. We decided that we are a team that fixes all its bugs, so we do. And it can start with you, because you’re a big part of your team culture.</p><p>I’ll finish this post by mentioning some of the benefits that might be less obvious than just making your software work and making your users happier.</p><ul><li>It’s so much easier to work when you’re proud of what you do. It’s easier to be proud of what you do when you try to do good, high-quality work.</li><li>No matter if the users are internal or external, by fixing bugs, you talk to them, you learn more about how they use the software, giving you more ideas and helping you make tons of technical decisions with your new domain knowledge.</li><li>When you make people’s jobs easier or more enjoyable, a lot of them are happy to listen to you or help you if you have questions or needs. This greatly increases your influence and political capital at your place of work (which you can then use to keep improving the culture). PMs are happy because they don’t have to spend time triaging or prioritizing bugs, users are happy because the software works better, and devs are happy because the codebase has fewer weird behaviors where it’s unclear how it should work (that’s most bugs).</li><li>The topic is often mentioned when we do interviews for hiring, often making a good impression. We also have good feedback from people outside of engineering when compared to their previous jobs.</li></ul><p>The bar to make good software is surprisingly low. You can start by fixing the bugs that are reported. You’ll already be ahead.</p>","contentLength":5682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ohqf61/no_bug_policy/"},{"title":"[R] Advice for first-time CVPR submission","url":"https://www.reddit.com/r/MachineLearning/comments/1ohq5f8/r_advice_for_firsttime_cvpr_submission/","date":1761598221,"author":"/u/jackeswin","guid":318996,"unread":true,"content":"<p>As you might know, the CVPR deadline is getting close, and I’m planning to submit there for the first time. I’d really appreciate any advice on how to approach the writing, what are the best styles, tones, or structures that make a strong impression?</p><p>Also, if you have tips on how to present the “story” of the paper effectively, I’d love to hear them.</p>","contentLength":361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Warning! Don't buy \"Embedded Rust Programming\" by Thompson Carter","url":"https://www.reddit.com/r/rust/comments/1ohq25k/warning_dont_buy_embedded_rust_programming_by/","date":1761598010,"author":"/u/bowl-modular","guid":317014,"unread":true,"content":"<p>I made the mistake of buying this book, it looked quite professional and I thought to give it a shot.</p><p>After a few chapters, I had the impression that AI certainly helped write the book, but I didn't find any errors. But checking the concurrency and I2C chapters, the book recommends libraries specifically designed for std environments or even linux operating systems.</p><p>I've learned my lesson, but let this be a warning for others! Name and shame this author so other potential readers don't get fooled.</p>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"how to get computer fan stat with go","url":"https://www.reddit.com/r/golang/comments/1ohpwfr/how_to_get_computer_fan_stat_with_go/","date":1761597650,"author":"/u/Acrobatic-Fly2753","guid":316990,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I’m learning Go and building some projects. One of them is a hardware-monitoring app for Windows. Everything was fine until I tried to add a feature to check whether a computer fan is present. I couldn’t find a library that does this. ChatGPT suggested using WMI or the Windows API, but when I build the app, Windows Defender pops up and flags it as malware. Can anyone help—maybe an open-source library or a modular approach?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Acrobatic-Fly2753\"> /u/Acrobatic-Fly2753 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ohpwfr/how_to_get_computer_fan_stat_with_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohpwfr/how_to_get_computer_fan_stat_with_go/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Which OS are you running on your phone?","url":"https://www.reddit.com/r/linux/comments/1ohpn39/which_os_are_you_running_on_your_phone/","date":1761597062,"author":"/u/pmpinto-pt","guid":319142,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Are you more into Google’s flavor of Android? Custom ROM on an android phone?<br/> Actual Linux?<br/> All in with Apple devices?<br/> Or just the essentials with a dumb phone and a battery that lasts for the whole month?</p> <p>I’m curious what are Linux users using when you’re out and about. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/pmpinto-pt\"> /u/pmpinto-pt </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ohpn39/which_os_are_you_running_on_your_phone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ohpn39/which_os_are_you_running_on_your_phone/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Help with Image Classification Experimentation (Skin Cancer Detection)","url":"https://www.reddit.com/r/MachineLearning/comments/1ohpa6v/r_help_with_image_classification_experimentation/","date":1761596256,"author":"/u/Intelligent_Bit2487","guid":319268,"unread":true,"content":"<p>Hello i am a student currently working on my project skin cancer multiclass classification using clinical images(non-dermascopic) and have merged clinical images from 3 datasets(pad ufes,milk 10k,HIBA dataset) but the issue is that i am really stuck as i cant get the scores above 0.60 recall for some class and other is stuck at 0.30. i dont know if this is a cleaning issue or not choosing the optimum augmentation techniques and the model. It would bereally helpfull if i could get some help thankyou!</p>","contentLength":504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust compiler uses this crate for its beautiful error messages","url":"https://github.com/rust-lang/annotate-snippets-rs","date":1761595856,"author":"/u/nik-rev","guid":319015,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ohp3tx/rust_compiler_uses_this_crate_for_its_beautiful/"},{"title":"[Guide] How to add Basic Auth to Prometheus (or any app) on Kubernetes with AWS ALB Ingress (using Nginx sidecar)","url":"https://www.reddit.com/r/kubernetes/comments/1ohp0qn/guide_how_to_add_basic_auth_to_prometheus_or_any/","date":1761595662,"author":"/u/pakkedheeth","guid":316987,"unread":true,"content":"<p>I recently tackled a common challenge that many of us face: securing internal dashboards like Prometheus when exposed via an AWS ALB Ingress. While ALBs are powerful, they don't offer native Basic Auth, often pushing you towards more complex OIDC solutions when a simple password gate is all that's needed.</p><p>I've put together a comprehensive guide on how to implement this using an Nginx sidecar pattern directly within your Prometheus (or any) application pod. This allows Nginx to act as the authentication layer, proxying requests to your app only after successful authentication.</p><ul><li>The fundamental problem of ALB &amp; Basic Auth.</li><li>Step-by-step setup of the Nginx sidecar with custom , , and .</li><li>Detailed  configurations for  to include the sidecar, volume mounts, and service/ingress adjustments.</li><li>Crucially, how to implement a \"smart\" health check that validates the  application's health, not just Nginx's.</li></ul><p>This is a real-world, production-tested approach that avoids over-complication. I'm keen to hear your thoughts and experiences!</p><p>Happy to answer any questions in the comments!</p>","contentLength":1070,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It’s Not Just Rich Countries. Tech’s Trillion-Dollar Bet on AI Is Everywhere.","url":"https://www.wsj.com/tech/ai/its-not-just-rich-countries-techs-trillion-dollar-bet-on-ai-is-everywhere-1781a117?st=9RBtHG&amp;mod=wsjreddit","date":1761595296,"author":"/u/wsj","guid":317015,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ohoup8/its_not_just_rich_countries_techs_trilliondollar/"},{"title":"speed up your github actions with the most lightweight k8s","url":"https://www.reddit.com/r/kubernetes/comments/1ohotzl/speed_up_your_github_actions_with_the_most/","date":1761595256,"author":"/u/bfenski","guid":316965,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1ohotzl/speed_up_your_github_actions_with_the_most/\"> <img src=\"https://external-preview.redd.it/ZJ3d9DzjFBXxng_I73cJk7IeRUCCW50Vy0ekwdz7HWA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4ed1ddac7986d25480ef3c76555bdd4576cab44\" alt=\"speed up your github actions with the most lightweight k8s\" title=\"speed up your github actions with the most lightweight k8s\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I found out that CI/CD workflows on Github using Minikube are slow for me.</p> <p>There&#39;s Kubesolo project which for simple cases is enough to test basic functionality.</p> <p>But there was no Github action for it so I started my own project to do that.</p> <p>Enjoy! Or blame. Or whatever. Be my guest ;)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bfenski\"> /u/bfenski </a> <br/> <span><a href=\"https://github.com/fenio/setup-kubesolo\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohotzl/speed_up_your_github_actions_with_the_most/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Built the Same App 10 Times: Evaluating Frameworks for Mobile Performance","url":"https://www.lorenstew.art/blog/10-kanban-boards","date":1761591529,"author":"/u/lorenseanstewart","guid":319239,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ohn634/i_built_the_same_app_10_times_evaluating/"},{"title":"Linux Desktop Endpoint Management ideas?","url":"https://www.reddit.com/r/linux/comments/1ohma90/linux_desktop_endpoint_management_ideas/","date":1761589522,"author":"/u/HaloDezeNuts","guid":319047,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Started a role for a University where they are increasingly wanting more Linux Desktop PCs &amp; need a way to manage them. Nothing in place so far. Ubuntu thank the lord ❤️</p> <p>First time I’ve ever administered any endpoints, I’ve only ever done servers via Ansible &amp; BigFix. Short term fix is spinning up Ansible and deploying SSH keys to get things updated remotely and enforce security. Maybe using custom facts to poll service tags somehow. Long term solution is I want to get a PoC going for a good MDM solution. Currently we’re using Jamf for Mac and SCCM/Intune for Windows. I was eyeballing JumpCloud but curious what y’all use for your environments??</p> <p>Also, what would yall use for deploying OS images to new PCs? I was thinking of creating unattended installer files to put in user-data and meta-data directories as others have done and deploy via PXE booting</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/HaloDezeNuts\"> /u/HaloDezeNuts </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ohma90/linux_desktop_endpoint_management_ideas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ohma90/linux_desktop_endpoint_management_ideas/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container live migration in k8s","url":"https://www.reddit.com/r/kubernetes/comments/1ohlprg/container_live_migration_in_k8s/","date":1761588268,"author":"/u/Super-Commercial6445","guid":316932,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey all,<br/> Recently came across CAST AI’s new <a href=\"https://cast.ai/press-release/zero-downtime-container-live-migration-launch/\">Container Live Migration </a>feature for EKS, tldr it lets you move a running container between nodes using <a href=\"https://criu.org/Main_Page\">CRIU</a>. </p> <p>This got me curious and i would like to try writing a k8s operator that would do the same, has anyone worked on something like this before or has better insights on these things how they actually work </p> <p>Looking for tips/ideas/suggestions and trying to check the feasibility of building one such operator</p> <p>Also wondering why isn’t this already a native k8s feature? It feels like something that could be super useful in real-world clusters.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Super-Commercial6445\"> /u/Super-Commercial6445 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohlprg/container_live_migration_in_k8s/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohlprg/container_live_migration_in_k8s/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Journalism turning into AI","url":"https://www.reddit.com/r/artificial/comments/1ohl53r/journalism_turning_into_ai/","date":1761587028,"author":"/u/AIMadeMeDoIt__","guid":316989,"unread":true,"content":"<p>A recent <a href=\"https://finance.yahoo.com/news/research-study-uncovers-extent-ai-120000865.html\">study</a> found that around 9% of US online newspaper articles are now partially or almost fully AI-generated - and almost none of them disclose it. I genuinely think this is a very low number. What do you think or know about this?</p>","contentLength":236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TalosOS and traefik problem","url":"https://www.reddit.com/r/kubernetes/comments/1ohkw53/talosos_and_traefik_problem/","date":1761586470,"author":"/u/An0nAdmin","guid":316931,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello, i created a TalosOS cluster (1xCP&amp;Worker, 2xWorkers) for my homelab. Previously i used k3s to create my homelab cluster. Now i want to run traefik, but can&#39;t access the /dashboard endpoint, can&#39;t access it via mapped domain to CP ip address and i don&#39;t know what I&#39;m doing wrong. Have someone more experience in that and could help?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/An0nAdmin\"> /u/An0nAdmin </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohkw53/talosos_and_traefik_problem/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohkw53/talosos_and_traefik_problem/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Python Software Foundation has withdrawn a $1.5 million proposal to US government grant program","url":"https://www.reddit.com/r/linux/comments/1ohkv2z/the_python_software_foundation_has_withdrawn_a_15/","date":1761586403,"author":"/u/guihkx-","guid":316939,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/guihkx-\"> /u/guihkx- </a> <br/> <span><a href=\"https://pyfound.blogspot.com/2025/10/NSF-funding-statement.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ohkv2z/the_python_software_foundation_has_withdrawn_a_15/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Terrible Technical Architecture of my First Startup","url":"https://blog.jacobstechtavern.com/p/my-terrible-startup-architecture","date":1761585848,"author":"/u/jacobs-tech-tavern","guid":316968,"unread":true,"content":"<p><a href=\"https://blog.jacobstechtavern.com/p/adhd-vs-alarmkit\" rel=\"\">undiagnosed ADHD</a></p><blockquote><p><em>*I was basically much too annoying to put in front of clients.</em></p></blockquote><p><em>“I’m leaving, for real-sies this time”</em><a href=\"https://blog.jacobstechtavern.com/p/access-control-in-swift-like-a-boss\" rel=\"\">job applications to Revolut</a></p><p>A commercial strategy consultant, with some cash, validated market research, and a dream, was searching for somebody with energy and a strong technical background to build an app in a potentially huge new market: Carbn.</p><p><a href=\"https://en.wikipedia.org/wiki/Carbon_offsets_and_credits\" rel=\"\">carbon offsets</a></p><p>It was exactly what I’d daydreamed of for 4 years.</p><p>The founder and I immediately agreed to work together and became fast friends. I was the technical co-founding Camilla to a bootstrapping King Charles. I was offered generous double-digit equity, alongside a salary only slightly below what Deloitte paid their mid-level devs / janitors.</p><p>Today, I’m telling the story of the systems I created. The decisions I made as the tech cofounder to carry an app business, from scratch. We’ll look at:</p><ul><li><p>The initial, rudimentary, backend architecture our MVP shipped with.</p></li><li><p>What I should have done all along.</p></li></ul><p>At no point did I know for sure what I was doing. But that’s part of the fun.</p><blockquote><p><em>Subscribe to Jacob’s Tech Tavern for free to get ludicrously in-depth articles on iOS, Swift, tech, &amp; indie projects in your inbox every week.</em></p></blockquote><p><em></em></p><p>This email can be too long for many email clients, so please read on my website for the best experience.</p><p>As well as from my memories of 2020-2022, this article was reconstructed in large part from old Apple notes I took at the time, as well as floating bits of documentation and code that remain on my old laptop.</p><p><em>Solutions Architect Associate</em></p><p>I closed my eyes among the trees at Avery Hill and pictured a serverless wonderland, with SQL queries skipping among the Lambda functions. Serverless was the future, I think. I even went full MBA with my consulting training:</p><ul><li><p>What are some KPIs and OKRs for Carbn?</p></li><li><p>Do we have a balanced business scorecard?</p></li><li><p>Will we need a data lake, or perhaps a dashboard?</p></li></ul><p>Outside the glaring blind spots in my architectural knowledge, I had a reasonable understanding of my own strengths and weaknesses as an engineer. I was quite good at building quickly. I was relatively slow at picking up new tools. I could usually work things out. I knew a decent bit of SwiftUI*.</p><blockquote><p><em>I was between iPhones after leaving cosy corporate life, so actually, the first couple of features were UIKit, since I was lent my mum’s old iPhone 6+ which only went up to iOS 12. I was frankly pretty okay at mobile apps, so this piece is more focused on pointing &amp; laughing at my backend setup.</em></p></blockquote><p>We had a tight 2-month timeline to ship our initial MVP, and I was self-aware enough to know I couldn’t learn how to whip up a backend in that time.</p><p>There's a known solution for mobile devs with limited backend capacity: Firebase.</p><p>As an instinctive contrarian, I needed a way to use Firebase without feeling like I was using Firebase. In 2020, options were limited, primarily because I’d never heard of the nascent Supabase.</p><p>Enter AWS Amplify: Amazon's answer to Firebase. This was a competing platform-as-a-service (PaaS) that directly mapped core functionality to corresponding AWS services, offering a tempting conversion route directly to running our own cloud infra.</p><p>This held the solution to one non-negotiable technical requirement from my cofounder: working offline. For the climate-conscious yuppie on the go. Amplify was batteries-included with a sync engine, making it an obvious go-to.</p><p>As I began work, I meticulously crafted an architecture diagram in the hope that this serious piece of engineering planning would pay dividends.</p><p>The tech stack utilised the full Amplify toolkit:</p><ul><li><p>AppSync and GraphQL for networking </p></li><li><p>DynamoDB for data storage </p></li><li><p>Cognito for email address &amp; social authentication</p></li><li><p>S3 for profile pic file storage</p></li><li><p>Lambda serverless functions to run our Stripe integration and sell offsets.</p></li></ul><p>Back in the day, none of the big open source PaaS toolkits supported SQL databases, so SQL vs NoSQL wasn’t even a consideration (yet). We were locked-in. All our data was stored as a big JSON tree, one per user:</p><p>Anyone who’s built a complex system on early PaaS tools will run into limitations, hard. AWS Amplify is magic when shipping an MVP, but it has myriad invisible guardrails that twat you in the face as soon as reach out for flexibility.</p><p>Our data lived in DynamoDB and that was that. Our users were effectively sovereign islands, isolated by the limitations of NoSQL. I couldn’t find any obvious built-in way to create many-to-many relationships between users for our upcoming social features. It goes without saying that they were trapped in Cognito’s auth directory. AWS and their damn free startup credits does a fantastic job of locking you into their ecosystem.</p><p>The bane of my existence, though, was the mandatory GraphQL.</p><p>Never trust an API so complex your networking code needs to be auto-generated. Or one that you have to re-learn every time you want a new ‘endpoint’. It was just a lot of ceremony, man.</p><p>Amplify’s DataStore, their persistence-framework-slash-GraphQL-sync-engine, might have been worse. Array data types didn’t really behave like you expected, often overwriting themselves due to confusing conflict resolution rules. I was in a hurry, so instead of learning the idiomatic approach to implementing one-to-many NoSQL, I picked the option labelled “optimistic concurrency” to unblock myself and moved on.</p><p><a href=\"https://blog.jacobstechtavern.com/p/claude-code-productivity\" rel=\"\">Claude Code</a></p><ul><li><p>The world’s most accurate™ Carbon footprint calculator</p></li><li><p>Smart suggestions to improve your footprint over 6 weeks</p></li><li><p>Carbon offset subscriptions via Stripe payments</p></li></ul><p>The product was just about good enough to persuade an angel investor to take a bet on our founding team.</p><p>Fresh off a funding round, we had a cool £200k in the bank. I took a cheeky 3-week break to hang out with my new baby, because it’s not like we were going to get any more time together for the next 12 months.</p><ul><li><p>One candidate was 2 hours late (because he said he had to shower), and, when we took him for lunch, loudly made fun of the current COVID mask mandates, to our masked faces. </p></li><li><p>A middle-aged, very experienced, dev willing to work for sub-£50k, who completely ignored everything I asked him to do in the technical test. His email name was Big Chungus. Somehow in my naïvety I was still on the fence. He would have made me his bitch.</p></li><li><p>Someone claimed to have done portfolio work for Peanut, a startup that later moved in next door(!), whose founder laughed and clarified that it was a complete lie.</p></li><li><p>I very nearly hired a friend who, I realised mid-interview, had been stuck doing mindless grunt work for the last year and a half.</p></li><li><p>We spoke to someone who was briefly accepted into a startup accelerator where he tried to design the Uber for Ice Cream.</p></li></ul><blockquote><p><em>*I won’t name any names, but three of these bullet points were the same person.</em></p></blockquote><p>We landed a UI designer we held onto until the end, a smart graduate dev who job-hopped after a 3-month death march, and an overseas contractor we broke up with after an argument over raising the day-rate. Yes, I really truly suck at management. For the most part, I was on my own.</p><p>The biggest strategic failure of our startup was around our target market. In that, we never settled on one.</p><ul><li><p>The B2C proposition promised limitless growth from a groundswell of climate activists.</p></li><li><p>The B2B path promised revenue via climate-conscious corporates wanting to achieve net-zero.</p></li></ul><p>Our angel, like all self-respecting Europeans, was nudging us towards B2B because that led to cold, hard cash. B2B focused us on “Scope 3” emissions, a category which included all the emissions produced by employees on-the-job. For many Western services businesses, this is by far their biggest block of CO₂. </p><p>But business clients require a rethink of our data architecture. They love a good report, thrive on aggregation, and get a little tight at the trousers for dashboards. NoSQL just isn’t a good fit for this use case. We wanted organisation-level data that piped directly into ESG reports: Employee carbon footprints, summaries across departments, and internal leaderboards that can spark friendly competition between teams.</p><p>AWS Amplify was no longer suitable for our needs.</p><p><em>“I’m pretty self aware. I’m better at infra than I am at actual backend”</em></p><p><a href=\"https://aws.amazon.com/blogs/database/use-python-sqlalchemy-orm-to-interact-with-an-amazon-aurora-database-from-a-serverless-application/\" rel=\"\">tutorial</a></p><ul><li><p>VPCs set up across 3 separate availability zones</p></li><li><p>NAT gateways and Bastion Hosts for each AZ </p></li><li><p>An Aurora Database cluster with an RDS proxy</p></li><li><p>Plus a separate “test” environment too, but I managed to swing just 2 AZ’s.</p></li></ul><p>The above diagram doesn’t really communicate the scale of fuckery I was doling out, but fortunately, this delirium fever dream, courtesy of ChatGPT, is actually spot-on.</p><p>This enterprise-grade architecture would have been fine if I was serving 100k DAUs, and high availability was an existential business constraint. But I had built a behemoth I barely understood.</p><p>I had tons of firepower at my disposal, but honestly, I had my fingers crossed, hoping the main DB would never shit the bed and I’d need to work out how to restore the backup on-the-fly.</p><p>Altogether, this infra put us back ~£600 a month.</p><p>This was around 50% database, 48% networking infra, and 2% compute. </p><p>If there was a perfect example of failing to see the forest for the trees, the secondary reason for using Lambda functions for our application code was that it was meant to be cheaper than a provisioned server!</p><p>In other words, ~1% of our runway a quarter. Not great, not terrible.</p><p>I really did have a sweet development workflow. </p><p>Locally, I rolled a Flask app that ran the Python APIs, calling into a local PostgreSQL DB on a Docker container. I could build the DB migrations and application code, run the local server, and code the iOS feature against it.</p><p>It never occurred to me that this low-energy setup might work just fine on the server. But I was emotionally invested in my architecture. I could be smarter than that. </p><p>Humans contain multitudes. An internal IQ bell curve. Some days, maybe after a sesh at the pub, you might be below your best. When you’re nailing it, on a cool 8 hours sleep* and full of energy, you might be killing it. </p><blockquote><p><em>*someone without kids remind me what this is like, please</em></p></blockquote><p>But there was one day where I blew myself out of the water.  </p><p><a href=\"https://aws.amazon.com/serverless/sam/\" rel=\"\">AWS SAM</a></p><pre><code><code># Before (local development)\nfrom model.layer.python.user import User\n\n# After (Lambda)\nfrom user import User</code></code></pre><p>My pre-deployment script ran a find-and-replace across the whole entire backend codebase to make the imports work.</p><p>We’re barely scratching the surface.</p><p>For our Stripe payments, I needed to inject the payment processing server IP into the Lambda function environment, so, naturally, I hardcoded the required IP in the pre-deployment script, which then added it to the Lambda template.</p><p>You heard that right: in attempting to create a sustainability app, I hypocritically hogged two entire IPv4 addresses.</p><p>The pièce de résistance was my hack to migrate the database schema. I needed to run these migrations using Alembic on deployment, so I set up a custom Lambda function, with a random hex name that triggered automatically:</p><p>I was a crab, in a knife-fight with the CloudFormation lobster, shanking my infrastructure-as-code until it submitted and agreed to perform DB migrations on-deploy. The random name was there to trick CloudFormation into thinking the function was new, and therefore running the migration code it contained, on every deployment.</p><p>If there’s anything we can learn from this, it’s that anyone can do a startup when they put their mind to it. </p><p>I eventually came into my own. </p><p>Without proper alerting, we were pretty much waiting for people to tell us about incidents. I vividly recall a Zoom call with a recruiter who told us that Sign In With Apple was broken.</p><p>I recall another weekend where we took a trip to Hastings, and login was inexplicably broken. I managed to diagnose and ship the hot-fix just as we arrived from the 2-hour drive.</p><p>Sometimes, as a sole developer, marketers will just make things your problem. In two startups now, I’ve had a marketer up my arse complaining that our ads weren’t targeting iOS 14+ users (both times, it was a toggle they needed to select in the console).</p><p>At the load we were dealing with, this whole project really could've been set up on a £5/month VPS on DigitalOcean or Hetzner. The structure would look near-identical to the existing development workflow I had set up.</p><ul><li><p>A single VPS running a Docker container with…</p><ul><li><p>Flask app container running my Python API</p></li></ul></li><li><p>Nginx reverse proxy to forward requests to Flask</p></li><li><p>Keep Cognito auth, validate JWTs in Flask </p></li><li><p>A simple backup script that replicated my database to S3 every day</p></li></ul><p>This would've handled 10k daily users easily. Okay, maybe a $20-50 server would cover our bases in the event of a serious unanticipated traffic spike. These bad boys can scale-in-place in a minute or two of downtime.</p><p>If I wanted to stick to AWS infra and give us escape hatches, we could have deployed the whole app using Elastic Beanstalk, their infra-as-a-service starter pack. Incidentally, after we folded, I spun up this exact environment on an Elastic Beanstalk server+database package. It cost £20/month.</p><p><strong>I failed to discuss my plans with anybody who might have known what they were doing</strong></p><p>Anybody with a bit of infra (or startup) experience under their belt could have told me to chillax, and showed me how a single server and simple file-based DB or miniature Postgres instance would easily carry us through to a seed round.</p><p>The gargantuan infra setup barely scratched our burn-rate, and we were running cool because we were paying ourselves pennies.</p><p>Our B2B proposition didn't end up leading to any useful sales, but Greenmiles inspired the new B2C value proposition, as we’d finally stumbled upon a compelling daily use case. </p><p>We reoriented the app around Greenmiles and “Ten Green Habits”. Apple liked it.</p><p>I'm not a systems architect or a backend engineer. I'm a hacker. I get something working and get it out of my way.</p><p>The backend was a zero-to-one deal both times. I needed a rudimentary backend to support our MVP, so used AWS Amplify. Then, when user requirements changed with our B2B, I responded by setting up some incredibly convoluted serverless SQL infra.</p><p><em>Claude Code 1995 Edition™</em></p><p>It’s fun to point and laugh at my unhinged attempt at creating a scalable architecture. Because it was funny. I spent a week cobbling together an enterprise-level serverless architecture held together via dodgy deployment scripts, duplicitous Lambda function migration triggers, and PVA glue. The architecture I created was only possible with the confidence of a 25-year-old Deloitte graduate with the second-easiest AWS certification.</p><p><em>sanity checked my work with someone who knew what they were doing</em></p><p>Because as a cofounder, I wasn’t just building a backend, or even just a mobile app. I was bouncing ideas off my cofounder, recruiting engineers, pitching to VCs, studying our analytics, and chatting to users.</p><p>But as a technical cofounder, the buck also stopped with me. All the problems were my problem. I developed the superpower of being able to hold my face against an industrial sander until I solved the obscure Lambda issue or Auth bug.</p><p>And these skills defined my career.</p>","contentLength":15120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ohkm2m/the_terrible_technical_architecture_of_my_first/"},{"title":"Linux on arm late 2025","url":"https://www.reddit.com/r/linux/comments/1ohkbib/linux_on_arm_late_2025/","date":1761585210,"author":"/u/The_elder_smurf","guid":319112,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I understand that android is Linux on arm, and that&#39;s great for the foundations of the operating system. I&#39;m not asking if Linux itself boots on arm, we know you can get Linux to boot on pretty much anything.</p> <p>What I&#39;m asking is what&#39;s the user experience like with an arm laptop. I&#39;m looking at getting a new power efficient laptop, and was wondering whether I could aim for a snapdragon laptop or I should stick with lunar lake. I&#39;m down to try new things and I&#39;m not against having to intermittently troubleshoot, but I do want the device to be relatively stable and not run into constant compatibility problems. So is arm on Linux flushed out at this point or should I stay with x86 based lunar lake?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/The_elder_smurf\"> /u/The_elder_smurf </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ohkbib/linux_on_arm_late_2025/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ohkbib/linux_on_arm_late_2025/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"At which point do you stop leveraging terraform ?","url":"https://www.reddit.com/r/kubernetes/comments/1ohjhhr/at_which_point_do_you_stop_leveraging_terraform/","date":1761583369,"author":"/u/Safe_Bicycle_7962","guid":316886,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>just wondering how much of your k8s infra is managed by terraform and where do you draw the line.</p> <p>At my current gigs almost everything (app excluded) is handled by terraform, we have modules to create anything in ArgoCD (project, app, namespaces, service account). </p> <p>So when we deploy a new app, we provide everything with terraform and then a sync of the app in ArgoCD (linked to a k8s repo, either kustomize or helm based) and the app is available.</p> <p>I find this kind of nice, maybe not really practical, but I was wondering what strategies other ops uses in the space, so I you&#39;d like to share please I&#39;m eager to learn !</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Safe_Bicycle_7962\"> /u/Safe_Bicycle_7962 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohjhhr/at_which_point_do_you_stop_leveraging_terraform/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohjhhr/at_which_point_do_you_stop_leveraging_terraform/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to create a GPU-based, multi-tenant, Container as a Service k8s cluster with NVIDIA DGX/HGX","url":"https://www.reddit.com/r/kubernetes/comments/1ohixd3/how_to_create_a_gpubased_multitenant_container_as/","date":1761582127,"author":"/u/LandonClipp","guid":316885,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1ohixd3/how_to_create_a_gpubased_multitenant_container_as/\"> <img src=\"https://external-preview.redd.it/18NrXnhTW5Q11p5NZzXfdGcytbvjOjBLpCYO1aepONs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=21b9783bf94c8f493cc7fad35c52377e71a32028\" alt=\"How to create a GPU-based, multi-tenant, Container as a Service k8s cluster with NVIDIA DGX/HGX\" title=\"How to create a GPU-based, multi-tenant, Container as a Service k8s cluster with NVIDIA DGX/HGX\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I wrote a blog on my experiences creating a CaaS platform for GPU-based containers in a multi-tenant cluster. This mainly a high-level overview of the technologies involved, the struggles I encountered, and what the current state of the art is for building on top of NVIDIA DGX/HGX platforms.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LandonClipp\"> /u/LandonClipp </a> <br/> <span><a href=\"https://topofmind.dev/blog/2025/10/21/gpu-based-containers-as-a-service/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohixd3/how_to_create_a_gpubased_multitenant_container_as/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Great Stay — Here’s the New Reality for Tech Workers","url":"https://www.reddit.com/r/programming/comments/1ohifyi/the_great_stay_heres_the_new_reality_for_tech/","date":1761581058,"author":"/u/KitchenTaste7229","guid":318978,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/KitchenTaste7229\"> /u/KitchenTaste7229 </a> <br/> <span><a href=\"https://www.interviewquery.com/p/the-great-stay-tech-workers-ai-fear\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ohifyi/the_great_stay_heres_the_new_reality_for_tech/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Impossible Optimization, and the Metaprogramming To Achieve It","url":"https://www.reddit.com/r/programming/comments/1ohh2xu/the_impossible_optimization_and_the/","date":1761578011,"author":"/u/verdagon","guid":316936,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/verdagon\"> /u/verdagon </a> <br/> <span><a href=\"https://verdagon.dev/blog/impossible-optimization\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ohh2xu/the_impossible_optimization_and_the/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Python Software Foundation has withdrawn $1.5 million proposal to US government grant program","url":"https://www.reddit.com/r/programming/comments/1ohgzl9/the_python_software_foundation_has_withdrawn_15/","date":1761577803,"author":"/u/N911999","guid":316887,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/N911999\"> /u/N911999 </a> <br/> <span><a href=\"https://pyfound.blogspot.com/2025/10/NSF-funding-statement.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ohgzl9/the_python_software_foundation_has_withdrawn_15/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] For those who’ve published on code reasoning — how did you handle dataset collection and validation?","url":"https://www.reddit.com/r/MachineLearning/comments/1ohge3t/d_for_those_whove_published_on_code_reasoning_how/","date":1761576469,"author":"/u/pgreggio","guid":317030,"unread":true,"content":"<p>I’ve been diving into how people build datasets for code-related ML research — things like program synthesis, code reasoning, SWE-bench-style evaluation, or DPO/RLHF.</p><p>From what I’ve seen, <strong>most projects still rely on scraping or synthetic generation, with a lot of manual cleanup and little reproducibility.</strong></p><p>Even published benchmarks vary wildly in annotation quality and documentation.</p><ol><li>How are you collecting or validating your datasets for code-focused experiments?</li><li>Are you using public data, synthetic generation, or human annotation pipelines?</li><li>What’s been the hardest part — scale, quality, or reproducibility?</li></ol><p><strong>I’ve been studying this problem closely and have been experimenting with a small side project to make dataset creation easier for researchers (happy to share more if anyone’s interested).</strong></p><p>Would love to hear what’s worked — or totally hasn’t — in your experience :) </p>","contentLength":894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Production-grade backend — what should I focus on?","url":"https://www.reddit.com/r/golang/comments/1ohfg7w/productiongrade_backend_what_should_i_focus_on/","date":1761574255,"author":"/u/SLANGERES","guid":316889,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks, I’ve been developing small backend apps in Go as personal projects for a while, but I never really followed a <strong>production-grade setup</strong>. Now, I’m planning to build a backend that will actually serve real clients — so I want to do things <em>the right way</em>.</p> <p>I’m wondering how deep I should go with things like: Proper logging (e.g., <strong>zerolog</strong>, <strong>structured logs</strong>), <strong>Observability</strong> (using something like <strong>New Relic</strong>, <strong>Prometheus</strong>, or <strong>Grafana</strong>)More robust <strong>configuration management ,</strong> Following best practices for <strong>error handling</strong>, <strong>security</strong>, and <strong>performance</strong></p> <p>Basically, I want to understand what are the <strong>most important pieces</strong> I should consider when building a scalable, maintainable, and production-ready Go backend.</p> <p>Would love to hear from those who’ve deployed Go services in production — what tools, setups, or practices made the biggest difference for you?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SLANGERES\"> /u/SLANGERES </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ohfg7w/productiongrade_backend_what_should_i_focus_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohfg7w/productiongrade_backend_what_should_i_focus_on/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI can code, but it can't build software","url":"https://www.reddit.com/r/programming/comments/1ohf259/ai_can_code_but_it_cant_build_software/","date":1761573315,"author":"/u/Acrobatic-Fly-7324","guid":316854,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Acrobatic-Fly-7324\"> /u/Acrobatic-Fly-7324 </a> <br/> <span><a href=\"https://bytesauna.com/post/coding-vs-software-engineering\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ohf259/ai_can_code_but_it_cant_build_software/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ed Zitron Gets Paid to Love AI. He Also Gets Paid to Hate AI","url":"https://www.wired.com/story/ai-pr-ed-zitron-profile/","date":1761572444,"author":"/u/wiredmagazine","guid":316970,"unread":true,"content":"<p> job, Ed Zitron runs a boutique public relations firm called EZPR. This might surprise anyone who has come to know Zitron through his podcast or his social media or the newsletter in which he writes two-fisted stuff like “Sam Altman is full of shit\" and “Mark Zuckerberg is a putrid ghoul.” Flacks, as a rule, tend not to talk like this. Flacks send prim, throat-clearing emails to media people who do, on rare occasions, talk like this. Flacks want to touch base, hop on the phone, clear up a few things about the allegation that their CEO is a “chunderfuck.”</p><p>\"And that really is one of the things with guys like Sam Altman and Dario Amodei from Anthropic,” Zitron was saying over burgers on a fine Manhattan afternoon in September. “I work with founders all the time. I’m a founder myself, I guess—I don’t like the title. But when you are a person that has to make more money than you lose, otherwise you lose your business, and you see these chunderfucks burning 5, 10 billion dollars in a year—and everyone's celebrating them? It's .”</p><p>We were talking about whether any of Zitron’s ranting about the AI industry had cost him business on the PR side of the ledger. He said no. There was the one client who felt Zitron was being a little mean toward Altman, the CEO of OpenAI and the biggest chunderfuck of all, as far as Zitron is concerned. Founding a company is hard, the client said. “I said, ‘I appreciate the comment, but, like, this isn't about you,’” Zitron told me. “His company is burning billions of dollars. He's a terrible businessman.”</p><p>It was, in all, a very Ed Zitron sort of riff, pitched in the key of personal affront, populist in the manner of a small business owner stink-eyeing the unpunished wastefulness of big industry. (Would these CEOs be any less offensive, one wonders, if their companies were  billions of dollars?) He has built an improbable little empire for himself out of tart commentary like this. His weekly podcast, , about “the tech industry's influence and manipulation of society,” has cracked Spotify’s top 20 among tech shows, and his newsletter, <em>Ed Zitron’s Where’s Your Ed At</em>, has grown north of 80,000 subscribers. The Ed Zitron media experience also includes a scrappy Bluesky account, a football podcast, some occasional baseball writing, a lot of to-and-froing with the users of r/BetterOffline, and a book due next year about, as he puts it, “why everything stopped working.” In other media, he has become a go-to source for AI naysaying. When Slate’s  podcast or WNYC’s  needed someone to talk about the bursting of the AI bubble, they called on Zitron. It isn’t just the volume of output that has put him on the map; it is the aggrieved style that he brings to criticisms of media figures and industry titans alike.</p><p>Not long ago, volume and style came together to produce the quintessential bit of Zitron media: a piece for his newsletter titled “How to Argue With an AI Booster.” It was 15,000 words long.</p><p>Edheads abound now. Nearly 200 people have purchased a $24 Better Offline challenge coin, engraved with what has become the Zitron mantra: “NEVER FORGIVE THEM FOR WHAT THEY'VE DONE TO THE COMPUTER.” I have seen someone put Ed’s words on a motivational poster, operating at some ambiguous register of irony. One Threads user described her “parasocial crush on a tech critic &amp; writer” who is not named but who is quite obviously Zitron. “I just want him to take me to dinner, take me gently but firmly by the hand, and tell me in his confusing, muddled British accent to throw away my goddamn phone,” she sighed. “This would fix me. I’m sure of it.” (As one tech journalist who’d seen the Threads post put it to me, “If you’re getting to a point where your writing is causing people to lust after you, you’re doing something either very right or very wrong.”)</p><p>As a functional matter, Zitron is meeting a demand for an equal-and-opposite voice to counter the inescapable AI hype. Critics of AI approach from any number of angles. There are doomers who fear the industry is ushering in some world-shattering superintelligence; there are denialists who don’t believe AI will ever replace human decisionmakers. Zitron is up to something different. What he offers people, in a time of amoral boosterism and amid a free-floating revulsion for the tech industry, is a moral language for hating generative AI. “He approaches the subject like a journalist in that he’s ravenous for information, but he is unshackled by the institutions,” says Allison Morrow, a business reporter at CNN and a frequent guest on . “Most journalists don’t want to root for an industry’s demise. The institutions we work for don’t want to be engaged in that kind of mission.”</p>","contentLength":4805,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ohephh/ed_zitron_gets_paid_to_love_ai_he_also_gets_paid/"},{"title":"Gooey - Go WebASM bindings and UI framework","url":"https://www.reddit.com/r/golang/comments/1ohe916/gooey_go_webasm_bindings_and_ui_framework/","date":1761571297,"author":"/u/cookiengineer","guid":316940,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1ohe916/gooey_go_webasm_bindings_and_ui_framework/\"> <img src=\"https://external-preview.redd.it/v5mbqA5S5u31ype4Xai1mi1HwakX9dOwNkU7twOvx0s.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe944bbbd3c3bb1bd71fe1a3f861c6d139324089\" alt=\"Gooey - Go WebASM bindings and UI framework\" title=\"Gooey - Go WebASM bindings and UI framework\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cookiengineer\"> /u/cookiengineer </a> <br/> <span><a href=\"https://github.com/cookiengineer/gooey\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohe916/gooey_go_webasm_bindings_and_ui_framework/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub - longbridge/gpui-component: Rust GUI components for building fantastic cross-platform desktop application by using GPUI.","url":"https://github.com/longbridge/gpui-component","date":1761571245,"author":"/u/-Y0-","guid":316853,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ohe89l/github_longbridgegpuicomponent_rust_gui/"},{"title":"Kubesafe now shows how many times it saved you from targeting the wrong cluster","url":"https://www.reddit.com/r/kubernetes/comments/1ohdqdu/kubesafe_now_shows_how_many_times_it_saved_you/","date":1761569974,"author":"/u/Telemaco_019","guid":316751,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1ohdqdu/kubesafe_now_shows_how_many_times_it_saved_you/\"> <img src=\"https://external-preview.redd.it/s8nIx9VOjk4H3dw0zA9l_0YI2YTkqGvglaBhpAYBnxg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=53249362b29ad264ab5d57aa7e74cc48d4ff69bf\" alt=\"Kubesafe now shows how many times it saved you from targeting the wrong cluster\" title=\"Kubesafe now shows how many times it saved you from targeting the wrong cluster\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/kubernetes\">r/kubernetes</a>!</p> <p>For those who don’t know, Kubesafe is a small CLI that stops you from running commands on the wrong Kubernetes cluster (<a href=\"https://github.com/Telemaco019/kubesafe\">https://github.com/Telemaco019/kubesafe</a>).</p> <p>I built it last year and have been using it ever since. Last week, after it blocked me from accidentally updating a deployment in production, I started wondering how many times it’s saved my 🍑 over the past year. </p> <p>So, I added a way to find out (from now on, unfortunately 🥲).</p> <p>Kubesafe v0.4.0 now keeps track of how many protected commands you cancel on each context. You can run <code>kubesafe stats</code> to show your personal “oops” counter.</p> <p><a href=\"https://preview.redd.it/n4nxaakejnxf1.png?width=962&amp;format=png&amp;auto=webp&amp;s=8608b6be43c007f1b114cb2b08964b8a3d1dfdd2\">https://preview.redd.it/n4nxaakejnxf1.png?width=962&amp;format=png&amp;auto=webp&amp;s=8608b6be43c007f1b114cb2b08964b8a3d1dfdd2</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Telemaco_019\"> /u/Telemaco_019 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohdqdu/kubesafe_now_shows_how_many_times_it_saved_you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohdqdu/kubesafe_now_shows_how_many_times_it_saved_you/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go begginer web server","url":"https://www.reddit.com/r/golang/comments/1ohd9hp/go_begginer_web_server/","date":1761568707,"author":"/u/Nerfi666","guid":316855,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Good morning, I am just starting out with Go and have already completed the tutorials on the official Go website and some others, such as how to write web applications and access a database, as these are the ones that interest me most because I work in web development and I think they are focused on that.</p> <p>However, once I finished both tutorials, I still have many questions, such as how to write web servers and access data from a database.</p> <p>The problem I have is that, unlike other frameworks or languages, with Go I am not sure how to proceed, which libraries to use, or if there is a standard for writing a server, the folder structure, the app routing, the app distribution. By distribution, I mean how to create the code. Right now, at my company, the project is structured as follows: handler--services--repositories, which I have seen a lot in GitHub repositories, but I have not seen anything in the documentation that tells you that this is the standard on the web. Is there any official document or document from the Go team that talks about these things?</p> <p>Some guidelines or tips on how to choose how to set up your server would be beneficial for everyone, I think.</p> <p>Some guidelines or tips on how to choose how to set up your server would be beneficial for everyone, I think.</p> <p>To sum up, I would like to know how you set up web servers, what architecture you choose, and other libraries that are used or are standard when setting up modern servers.</p> <p>Thank you for your attention, and have a good day/afternoon/evening :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Nerfi666\"> /u/Nerfi666 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ohd9hp/go_begginer_web_server/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ohd9hp/go_begginer_web_server/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bernie says OpenAI should be broken up: \"AI like a meteor coming\" ... He's worried about 1) \"massive loss of jobs\" 2) what it does to us as human beings 3) \"Terminator scenarios\" where superintelligent AI takes over.","url":"https://v.redd.it/g90epks1gnxf1","date":1761568698,"author":"/u/MetaKnowing","guid":316792,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ohd9d6/bernie_says_openai_should_be_broken_up_ai_like_a/"},{"title":"Software Update Deletes Everything Older than 10 Days","url":"https://www.reddit.com/r/linux/comments/1ohciie/software_update_deletes_everything_older_than_10/","date":1761566540,"author":"/u/coldbeers","guid":316791,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Good story and cautionary tale.</p> <p>I won’t spoil it but I remember rejecting a script for production deployment because I was afraid that something like this might happen, although to be fair not for this exact reason.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/coldbeers\"> /u/coldbeers </a> <br/> <span><a href=\"https://youtu.be/Nkm8BuMc4sQ\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ohciie/software_update_deletes_everything_older_than_10/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[CNCF Project] HAMi v2.7.0: Topology-aware NVIDIA GPU scheduling for Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1ohcf5n/cncf_project_hami_v270_topologyaware_nvidia_gpu/","date":1761566280,"author":"/u/nimbus_nimo","guid":316732,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><strong>TL;DR</strong></p> <p>We turn real GPU links (NVLink/PCIe) into a per-pair communication score on each node.</p> <p>The scheduler then:</p> <ul> <li>Multi-GPU jobs: pick the highest-scoring group (closer, faster together).</li> <li>Single-GPU jobs: pick the least-connected card to avoid breaking good groups.</li> </ul> <p><strong>Why this matters</strong></p> <p>For large training and HPC, inter-GPU bandwidth/latency is often the bottleneck. Randomly picking N GPUs wastes performance. Using NVLink-dense sets and avoiding cross-CPU hops helps in practice and keeps the cluster topology healthy.</p> <p><strong>How it works</strong></p> <p><strong>1) Topology registration (node side)</strong></p> <ul> <li>Probe with NVML to discover links between every GPU pair (NVLink, PCIe, same-CPU vs cross-CPU).</li> <li>Build an in-memory topology graph and convert each pair to a simple communication score (e.g., NVLink direct &gt; same board &gt; same CPU &gt; cross-CPU / multi-hop PCIe).</li> <li>Publish a device score table (GPU UUID mapped to scores with others) as a node annotation.</li> </ul> <p><strong>2) Scheduling decision (scheduler/device layer)</strong></p> <ul> <li>Filter GPUs by basic needs (memory, compute).</li> <li>Choose by request size: <ul> <li>N &gt; 1: enumerate valid combos and select the group with the highest total internal score.</li> <li>N = 1: select the card with the lowest total score to the rest (an “edge” card) to minimize topology damage.</li> </ul></li> </ul> <blockquote> <p>Mental model: multi-GPU should huddle up; single-GPU should step aside.</p> </blockquote> <p><strong>One-line enablement (example)</strong></p> <pre><code>apiVersion: v1 kind: Pod metadata: name: gpu-topology-aware-job annotations: hami.io/gpu-scheduler-policy: &quot;topology-aware&quot; spec: containers: - name: cuda image: nvidia/cuda:11.6.2-base-ubuntu20.04 command: [&quot;sleep&quot;, &quot;infinity&quot;] resources: limits: nvidia.com/gpu: &quot;4&quot; </code></pre> <h1>Links</h1> <ul> <li>PRs: <ul> <li><a href=\"https://github.com/Project-HAMi/HAMi/pull/1018\">https://github.com/Project-HAMi/HAMi/pull/1018</a></li> <li><a href=\"https://github.com/Project-HAMi/HAMi/pull/1028\">https://github.com/Project-HAMi/HAMi/pull/1028</a></li> </ul></li> </ul> <p>Thanks to community contributors @lengrongfu and @fyp711.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nimbus_nimo\"> /u/nimbus_nimo </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohcf5n/cncf_project_hami_v270_topologyaware_nvidia_gpu/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohcf5n/cncf_project_hami_v270_topologyaware_nvidia_gpu/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] PKBoost: Gradient boosting that stays accurate under data drift (2% degradation vs XGBoost's 32%)","url":"https://www.reddit.com/r/MachineLearning/comments/1ohbdgu/r_pkboost_gradient_boosting_that_stays_accurate/","date":1761562889,"author":"/u/Federal_Ad1812","guid":316752,"unread":true,"content":"<p>I've been working on a gradient boosting implementation that handles two problems I kept running into with XGBoost/LightGBM in production:</p><ol><li><strong>Performance collapse on extreme imbalance (under 1% positive class)</strong></li><li><strong>Silent degradation when data drifts (sensor drift, behavior changes, etc.)</strong></li></ol><p>Imbalanced data (Credit Card Fraud - 0.2% positives):</p><p>Under realistic drift (gradual covariate shift):</p><p>- PKBoost: 86.2% PR-AUC (−2.0% degradation)</p><p>- XGBoost: 50.8% PR-AUC (−31.8% degradation)</p><p>- LightGBM: 45.6% PR-AUC (−42.5% degradation)</p><p>The main innovation is using Shannon entropy in the split criterion alongside gradients. Each split maximizes:</p><p>Gain = GradientGain + λ·InformationGain</p><p>where λ adapts based on class imbalance. This explicitly optimizes for information gain on the minority class instead of just minimizing loss.</p><p>- Quantile-based binning (robust to scale shifts)</p><p>- Conservative regularization (prevents overfitting to majority)</p><p>- PR-AUC early stopping (focuses on minority performance)</p><p>The architecture is inherently more robust to drift without needing online adaptation.</p><p>- Auto-tunes for your data (no hyperparameter search needed)</p><p>- Works out-of-the-box on extreme imbalance</p><p>- Comparable inference speed to XGBoost</p><p>- ~2-4x slower training (45s vs 12s on 170K samples)</p><p>- Slightly behind on balanced data (use XGBoost there)</p><p>- Built in Rust, so less Python ecosystem integration</p><p>This started as a learning project (built from scratch in Rust), but the drift resilience results surprised me. I haven't seen many papers addressing this - most focus on online learning or explicit drift detection.</p><p>- Have others seen similar robustness from conservative regularization?</p><p>- Are there existing techniques that achieve this without retraining?</p><p>- Would this be useful for production systems, or is 2-4x slower training a dealbreaker?</p><p>- Benchmarks include: Credit Card Fraud, Pima Diabetes, Breast Cancer, Ionosphere</p><p>- MIT licensed, ~4000 lines of Rust</p><p>Happy to answer questions about the implementation or share more detailed results. Also open to PRs if anyone wants to extend it (multi-class support would be great).</p><p>: Built this on a 4-core Ryzen 3 laptop with 8GB RAM, so the benchmarks should be reproducible on any hardware.</p><p>: The Python library is now avaible for use, for furthur details, please check the Python folder in the Github Repo for Usage, Or Comment if any questions or issues</p>","contentLength":2369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continuous profiling with Parca: finally seeing which functions burn CPU in prod","url":"https://www.reddit.com/r/kubernetes/comments/1ohb6i6/continuous_profiling_with_parca_finally_seeing/","date":1761562221,"author":"/u/fatih_koc","guid":316700,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve had incidents in our K8s clusters where CPU sat at 80% for hours and all we had were dashboards and guesses. Metrics told us which pods, traces showed request paths, but we still didn&#39;t know which function was actually hot.</p> <p>I tried continuous profiling with Parca. It samples stack traces from the kernel using eBPF and you don&#39;t touch application code. Running it as a DaemonSet was straightforward. Each agent samples its node&#39;s processes and forwards profiles to the central server.</p> <p>The first time I opened the flamegraph and saw a JSON marshal taking most of the time, it felt like cheating.</p> <p>The full post covers when to adopt profiling, how it fits with Prometheus and OpenTelemetry, and common mistakes teams make: <a href=\"https://fatihkoc.net/posts/ebpf-parca-observability/\">eBPF Observability and Continuous Profiling with Parca</a></p> <p>Curious how others are using profilers in Kubernetes. Did it change incident response for you or mostly help with cost tuning?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fatih_koc\"> /u/fatih_koc </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohb6i6/continuous_profiling_with_parca_finally_seeing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ohb6i6/continuous_profiling_with_parca_finally_seeing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Your data, their rules: The growing risks of hosting EU data in the US cloud","url":"https://www.reddit.com/r/programming/comments/1ohapof/your_data_their_rules_the_growing_risks_of/","date":1761560562,"author":"/u/danielrothmann","guid":316701,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/danielrothmann\"> /u/danielrothmann </a> <br/> <span><a href=\"https://blog.42futures.com/p/your-data-their-rules\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ohapof/your_data_their_rules_the_growing_risks_of/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is the \"culture shock\" of switching to Linux?","url":"https://www.reddit.com/r/linux/comments/1ohaj67/what_is_the_culture_shock_of_switching_to_linux/","date":1761559890,"author":"/u/Regular_Low8792","guid":316790,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Been debating switching to Linux as I am really tired of Windows and Microsoft, but I am just so undecided as compatibility of a big operating system is obviously comfortable. While I feel like it&#39;s easy to read and learn about the differences between using Windows or Linux, I am wondering what real pains and positives are that you have noticed when fully jumping into using Linux exclusively?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Regular_Low8792\"> /u/Regular_Low8792 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ohaj67/what_is_the_culture_shock_of_switching_to_linux/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ohaj67/what_is_the_culture_shock_of_switching_to_linux/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Extremely fast data compression library","url":"https://www.reddit.com/r/programming/comments/1oha4zd/extremely_fast_data_compression_library/","date":1761558369,"author":"/u/South_Acadia_6368","guid":316702,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I needed a compression library for fast in-memory compression, but none were fast enough. So I had to create my own: memlz</p> <p>It beats LZ4 in both compression and decompression speed by multiple times, but of course trades for worse compression ratio.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/South_Acadia_6368\"> /u/South_Acadia_6368 </a> <br/> <span><a href=\"https://github.com/rrrlasse/memlz\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1oha4zd/extremely_fast_data_compression_library/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is your Go to libraries these days?","url":"https://www.reddit.com/r/golang/comments/1oh9r7h/what_is_your_go_to_libraries_these_days/","date":1761556881,"author":"/u/Ecstatic-Panic3728","guid":316664,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been heads down programming in Go for years and years and I have my set of packages that served me well but I&#39;m wondering if there are better things out there right now.</p> <p>I usually need things for:</p> <ul> <li>Database Migration (goose)</li> <li>HTTP Framework (chi)</li> <li>Logging (slog)</li> <li>Environment Variables (envconfig)</li> <li>Postgres (pgx)</li> <li>Testing (testify)</li> <li>SQL Query Builder (squirrel)</li> </ul> <p>Do you have other sugestions? On the HTTP framework part I would love something that could automatically generate OpenAPI specs given that I&#39;m usually writing them manually.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ecstatic-Panic3728\"> /u/Ecstatic-Panic3728 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1oh9r7h/what_is_your_go_to_libraries_these_days/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oh9r7h/what_is_your_go_to_libraries_these_days/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Friend's daughter needing online resources for learning Linux and coding","url":"https://www.reddit.com/r/linux/comments/1oh9jwo/friends_daughter_needing_online_resources_for/","date":1761556071,"author":"/u/Vlerremuis","guid":317089,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m giving a friend&#39;s daughter a laptop loaded with Linux Mint.<br/> Can anyone recommend a reddit sub (or other online forum) that&#39;s friendly, kind, and suitable for a high school girl learning Linux and coding? I&#39;m not familiar with this sub, not sure how supportive and moderated it is (no shade intended, just not every place on reddit might be suitable for a highschooler who might be shy or uncertain) </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vlerremuis\"> /u/Vlerremuis </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1oh9jwo/friends_daughter_needing_online_resources_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1oh9jwo/friends_daughter_needing_online_resources_for/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Expert Security Textbook","url":"https://www.reddit.com/r/linux/comments/1oh9f1s/expert_security_textbook/","date":1761555531,"author":"/u/iaacornus","guid":316663,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iaacornus\"> /u/iaacornus </a> <br/> <span><a href=\"https://i.redd.it/ih6nyp20dmxf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1oh9f1s/expert_security_textbook/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We shrunk an 800GB container image down to 2GB (a 99.7% reduction). Here's our post-mortem.","url":"https://www.reddit.com/r/kubernetes/comments/1oh990m/we_shrunk_an_800gb_container_image_down_to_2gb_a/","date":1761554826,"author":"/u/cloud-native-yang","guid":316639,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>Our engineering team ran into a pretty wild production issue recently, and we thought the story and our learnings might be useful (or at least entertaining) for the community here.</p> <p>—-</p> <p>Background:</p> <p>Our goal isn&#39;t just to provide a remote dev environment, but to manage what happens after the code is written.</p> <p>And it’s source available: <a href=\"https://github.com/labring/sealos\">https://github.com/labring/sealos</a></p> <p>Our target audience is the developer who finds that to be a burden and just wants to code. They don&#39;t want to learn Docker or manage Kubernetes YAML. Our platform is designed to abstract away that complexity.</p> <p>For example, Coder is best-in-class at solving the &quot;remote dev environment&quot; piece. We&#39;re trying to use DevBox as the starting point for a fully integrated, end-to-end application lifecycle, all on the same platform.</p> <p>The workflow we&#39;re building for is:</p> <ol> <li>A developer spins up their DevBox.</li> <li>They code and test their feature (using their local IDE, which requires the SSHD).</li> <li>Then, from that same platform, they package their application into a production-ready image.</li> <li>Finally, they deploy that image directly to a production Kubernetes environment with one click.</li> </ol> <p>This entire post-mortem is the story of our original, flawed implementation of Step 3. The commit feature that exploded was our mechanism for letting a developer snapshot their entire working environment into that deployable image, without needing to write a Dockerfile.</p> <p>—-</p> <p>It all started with the PagerDuty alert we all dread: <code>&quot;Disk Usage &gt; 90%&quot;</code>. A node in our Kubernetes cluster was constantly full, evicting pods and grinding developer work to a halt. We&#39;d throw more storage at it, and the next day, same alert.</p> <p>After some digging with <code>iotop</code> and <code>du</code>, we found the source: a single container image that had ballooned to an unbelievable <strong>800GB with 272 layers</strong>.</p> <p><strong>The Root Cause: A Copy-on-Write Death Spiral</strong></p> <p>We traced it back to a brute-force SSH attack that had been running for months. This caused the <code>/var/log/btmp</code> file (which tracks failed logins) to grow to 11GB.</p> <p>Here&#39;s where it gets crazy. Due to how OverlayFS&#39;s Copy-on-Write (CoW) works, every time the user committed a change, the system didn&#39;t just append a new failed login. It copied the <em>entire 11GB file</em> into the new layer. This happened over and over, 271 times.</p> <p>Even deleting the file in a new layer wouldn&#39;t have worked, as the data would remain in the immutable layers underneath.</p> <p><strong>How We Fixed It</strong></p> <p>Standard <code>docker</code> commands couldn&#39;t save us. We had to build a small custom tool to manipulate the OCI image directly. The process involved two key steps:</p> <ol> <li> <strong>Remove the file:</strong> Add a &quot;whiteout&quot; layer to tell the runtime to ignore <code>/var/log/btmp</code> in all underlying layers.</li> <li> <strong>Squash the history:</strong> This was the crucial step. Our tool merged all 272 layers down into a single, clean layer, effectively rewriting the image&#39;s history and reclaiming all the wasted space.</li> </ol> <p>The result was a new image of just <strong>2.05GB</strong>. A 390:1 reduction. The disk usage alerts stopped immediately, and container pull times improved by 65%.</p> <p>Sometimes the root cause is a perfect storm of seemingly unrelated things.</p> <p>Happy to share the link to the full case study if you&#39;re interested, just let me know in the comments!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cloud-native-yang\"> /u/cloud-native-yang </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh990m/we_shrunk_an_800gb_container_image_down_to_2gb_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh990m/we_shrunk_an_800gb_container_image_down_to_2gb_a/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightweight Python Implementation of Shamir's Secret Sharing with Verifiable Shares","url":"https://www.reddit.com/r/programming/comments/1oh944a/lightweight_python_implementation_of_shamirs/","date":1761554275,"author":"/u/Excellent_Double_726","guid":316661,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi r/programming!</p> <p>I built a lightweight Python library for Shamir&#39;s Secret Sharing (SSS), which splits secrets (like keys) into shares, needing only a threshold to reconstruct. It also supports Feldman&#39;s Verifiable Secret Sharing to check share validity securely.</p> <p><strong>What my project does</strong></p> <p>Basically you have a secret(a password, a key, an access token, an API token, password for your cryptowallet, a secret formula/recipe, codes for nuclear missiles). You can split your secret in n shares between your friends, coworkers, partner etc. and to reconstruct your secret you will need at least k shares. For example: total of 5 shares but you need at least 3 to recover the secret). An impostor having less than k shares learns nothing about the secret(for context if he has 2 out of 3 shares he can&#39;t recover the secret even with unlimited computing power - unless he exploits the discrete log problem but this is infeasible for current computers). If you want to you can not to use this Feldman&#39;s scheme(which verifies the share) so your secret is safe even with unlimited computing power, even with unlimited quantum computers - mathematically with fewer than k shares it is impossible to recover the secret</p> <p>Features:</p> <ul> <li>Minimal deps (pycryptodome), pure Python.</li> <li>File or variable-based workflows with Base64 shares.</li> <li>Easy API for splitting, verifying, and recovering secrets.</li> <li>MIT-licensed, great for secure key management or learning crypto.</li> </ul> <p>Comparison with other implementations:</p> <ul> <li>pycryptodome - it allows only 16 bytes to be split where mine allows unlimited(as long as you&#39;re willing to wait cause everything is computed on your local machine). Also this implementation does not have this feature where you can verify the validity of your share. Also this returns raw bytes array where mine returns base64 (which is easier to transport/send)</li> <li><a href=\"https://github.com/thedanhub/shamir-secret-sharing\">This</a> repo allows you to share your secret but it should already be in number format where mine automatically converts your secret into number. Also this repo requires you to put your share as raw coordinates which I think is too technical.</li> <li>Other notes: my project allows you to recover your secret with either vars or files. It implements Feldman&#39;s Scheme for verifying your share. It stores the share in a convenient format <em>base64</em> and a lot more, check it out for docs</li> </ul> <p><strong>Target audience</strong></p> <p>I would say it is production ready as it covers all security measures: primes for discrete logarithm problem of at least 1024 bits, perfect secrecy and so on. <strong>Even so, I wouldn&#39;t recommend its use for high confidential data(like codes for nuclear missiles) unless some expert confirms its secure</strong></p> <p>Check it out:</p> <ul> <li>PyPI: <a href=\"https://pypi.org/project/shamir-lbodlev/\">https://pypi.org/project/shamir-lbodlev/</a> (pip install shamir-lbodlev)</li> <li>GitHub: <a href=\"https://github.com/lbodlev888/shamir-lbodlev\">https://github.com/lbodlev888/shamir</a> (README with examples)</li> </ul> <p>-Feedback or feature ideas? Let me know <a href=\"https://github.com/lbodlev888/shamir/issues\">here</a>!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Excellent_Double_726\"> /u/Excellent_Double_726 </a> <br/> <span><a href=\"https://github.com/lbodlev888/shamir/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1oh944a/lightweight_python_implementation_of_shamirs/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Write Go code in JavaScript files. It compiles to WebAssembly. Actually works.","url":"https://www.reddit.com/r/golang/comments/1oh902z/write_go_code_in_javascript_files_it_compiles_to/","date":1761553805,"author":"/u/0xjnml","guid":316753,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>via ShowHN: <a href=\"https://news.ycombinator.com/item?id=45717724\">https://news.ycombinator.com/item?id=45717724</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/0xjnml\"> /u/0xjnml </a> <br/> <span><a href=\"https://www.npmjs.com/package/vite-plugin-use-golang\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oh902z/write_go_code_in_javascript_files_it_compiles_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need help with nginx-ingress","url":"https://www.reddit.com/r/kubernetes/comments/1oh8jo7/need_help_with_nginxingress/","date":1761551906,"author":"/u/Proper-Appeal-3457","guid":316622,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I am new to kubernetes and I was setting up my cluster using kubeadm where I will host some simple workloads, I initialised cluster on two VPS machines and made network for them using wireguard, I installed calico and openebs, now I have an issue, I need to install nginx ingress and make it listen 80 port on node, I know that k3s ServiceLB can do something like this, but it is exclusive for k3s, maybe we have something like this for k8s?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Proper-Appeal-3457\"> /u/Proper-Appeal-3457 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh8jo7/need_help_with_nginxingress/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh8jo7/need_help_with_nginxingress/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Does anyone have idea about Developing Helm Charts (SC104) certification exam?","url":"https://www.reddit.com/r/kubernetes/comments/1oh8c7y/does_anyone_have_idea_about_developing_helm/","date":1761551026,"author":"","guid":316621,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I am going for helm certification: <a href=\"https://training.linuxfoundation.org/skillcred/helm/\">Developing Helm Charts (SC104)</a> and for that I am learning it from Kodekloud&#39;s Helm beginner course. Just want to know that this course is sufficient for certification exam? or Do I need to follow additional resource? Thanks</p> </div><!-- SC_ON --> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh8c7y/does_anyone_have_idea_about_developing_helm/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh8c7y/does_anyone_have_idea_about_developing_helm/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why do y'all have an aversion to writing comments?","url":"https://www.reddit.com/r/rust/comments/1oh86x9/why_do_yall_have_an_aversion_to_writing_comments/","date":1761550427,"author":"/u/Interesting_Golf_529","guid":316816,"unread":true,"content":"<p>I've been working as a software engineer for about 16 years now, and have been doing some rust for the past year or so. Some at work, some OSS, and a few educational things for myself. Really liking it so far, great fun for the most part!</p><p>One thing I've noticed though, and have been thinking about for a while, is that a lot of rust projects don't seem to use comments as much as projects written in other languages. A lot of them will have barely an comments at all.</p><p>This trend seemingly fits in with the style things are documented in general; most of the time you get reference docs of the API and a cursory intro into the thing in a readme style, but \"usage\" docs or \"how to\" sections are rarely used.</p><p>I've found myself having to dive deep into the source code to really understand what's going on way more in rust than I had with most other languages I'm familiar with. </p><p>One observation I find particularly interesting about this is that I don't this has something to do with a difference in personal preference in general, as I've seen libraries written by the same team/person in a different language take a completely different approach to documenting than in rust.</p><p>So. What do you think is it about rust that makes people at large not feel like writing comments and documentation? Have you noticed this as well? Do you perhaps notice a difference in your approach to this when writing rust versus another language?</p><p>PS: Despite the title, I'm asking this with a genuine curiosity and fondness of the language, I'm not trying to do a \"rust bad\" here :)</p>","contentLength":1554,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rust-analyzer changelog #299","url":"https://rust-analyzer.github.io/thisweek/2025/10/27/changelog-299.html","date":1761548612,"author":"/u/WellMakeItSomehow","guid":316659,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1oh7rai/rustanalyzer_changelog_299/"},{"title":"Linux 6.18-rc3 Released With Latest Fixes","url":"https://www.reddit.com/r/linux/comments/1oh773d/linux_618rc3_released_with_latest_fixes/","date":1761546352,"author":"/u/somerandomxander","guid":317090,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/somerandomxander\"> /u/somerandomxander </a> <br/> <span><a href=\"https://www.phoronix.com/news/Linux-6.18-rc3-Released\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1oh773d/linux_618rc3_released_with_latest_fixes/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"World Foundation Models 2025 [R]","url":"https://www.reddit.com/r/MachineLearning/comments/1oh73b3/world_foundation_models_2025_r/","date":1761545929,"author":"/u/Alternative_Art2984","guid":316988,"unread":true,"content":"<p>I am just curious for working on World Models. Do we always require robot intervention or it can be done via only training and testing data? I want to select this topic for phd research.</p><p>Does anyone give me suggestion? how they look into this domain?</p>","contentLength":249,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Your Guide to Observability at KubeCon Atlanta 2025","url":"https://www.reddit.com/r/kubernetes/comments/1oh5oj1/your_guide_to_observability_at_kubecon_atlanta/","date":1761540636,"author":"/u/ExcitingThought2794","guid":316584,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Going to KubeCon Atlanta next month (Nov 10-13)?</p> <p>If you&#39;re interested in observability content, here are some sessions worth checking out:</p> <p><strong>OpenTelemetry sessions:</strong></p> <ul> <li><a href=\"https://sched.co/27FUv\">Taming Telemetry at Scale</a> - Nancy Chauhan &amp; Marino Wijay (Tue 11:15 AM)</li> <li><a href=\"https://sched.co/27FWT\">Just Do It: OpAMP</a> - Nike&#39;s production implementation (Tue 3:15 PM)</li> <li><a href=\"https://sched.co/27FWx\">Instrumentation Score</a> - measuring instrumentation quality (Tue 4:15 PM)</li> <li><a href=\"https://sched.co/27Fcf\">Tracing LLM apps</a> - lightning talk on tracing non-deterministic applications (Wed 5:41 PM)</li> </ul> <p><strong>Platform engineering + observability:</strong></p> <ul> <li><a href=\"https://colocatedeventsna2025.sched.com/event/28D4A\">CI/CD observability with OpenTelemetry</a> (Wed 2:05 PM)</li> <li><a href=\"https://colocatedeventsna2025.sched.com/event/28D7e\">Making ML pipelines traceable with KitOps + Argo</a> (Wed 3:20 PM)</li> <li><a href=\"https://colocatedeventsna2025.sched.com/event/28D7z\">Auto-rollbacks triggered by telemetry signals</a> (Wed 4:35 PM)</li> <li><a href=\"https://kccncna2025.sched.com/event/27Fb7\">Observability for AI agents in Kubernetes</a> (Wed 4:00 PM)</li> </ul> <p>There&#39;s also <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/cncf-hosted-co-located-schedule/\">Observability Day</a> on Nov 10 (co-located event, requires All-Access pass).</p> <p>More details and tips for first-timers: <a href=\"https://signoz.io/blog/kubecon-atlanta-2025-observability-guide/\">https://signoz.io/blog/kubecon-atlanta-2025-observability-guide/</a></p> <p>Disclaimer: I&#39;m on the SigNoz team. We&#39;ll be at Booth 1372 if you want to chat.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ExcitingThought2794\"> /u/ExcitingThought2794 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh5oj1/your_guide_to_observability_at_kubecon_atlanta/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh5oj1/your_guide_to_observability_at_kubecon_atlanta/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kriti Images - Open Source Alternative to Cloudflare Images","url":"https://www.reddit.com/r/golang/comments/1oh5e4x/kriti_images_open_source_alternative_to/","date":1761539632,"author":"/u/v1n4y_g","guid":316590,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1oh5e4x/kriti_images_open_source_alternative_to/\"> <img src=\"https://external-preview.redd.it/Q2W3WeHWhvQ-tmO8ymDNwohqVzOk1CvuFmpLVO4ypo8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ded122c4539801e60761974f6018cdf285d2891b\" alt=\"Kriti Images - Open Source Alternative to Cloudflare Images\" title=\"Kriti Images - Open Source Alternative to Cloudflare Images\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I built <strong>Kriti Images</strong>, image transformation service in Go that provides URL-based real-time image processing.</p> <h1>What it does</h1> <p>Transform images through simple URL parameters - resize, crop, rotate, blur, adjust colors, and convert formats (JPEG/PNG/WebP) with CDN-friendly caching.</p> <pre><code># Resize with smart fitting and background GET /cgi/images/tr:width=400,height=300,fit=pad,background=blue/image.jpg # Multiple transformations GET /cgi/images/tr:width=500,brightness=20,format=webp,quality=80/image.jpg </code></pre> <p>GH: <a href=\"https://github.com/kritihq/kriti-images\">https://github.com/kritihq/kriti-images</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/v1n4y_g\"> /u/v1n4y_g </a> <br/> <span><a href=\"https://github.com/kritihq/kriti-images\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oh5e4x/kriti_images_open_source_alternative_to/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GlobalCVE — Unified CVE Feed for Developers & Security Tools","url":"https://www.reddit.com/r/programming/comments/1oh4ge4/globalcve_unified_cve_feed_for_developers/","date":1761536480,"author":"/u/reallylonguserthing","guid":316660,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>For devs building or maintaining security-aware software, GlobalCVE.xyz aggregates CVE data from multiple global sources (NVD, MITRE, CNNVD, etc.) into one clean feed.</p> <p>It’s open-source GitHub.com/GlobalCVE , API-ready, and designed to make vulnerability tracking less fragmented.</p> <p>Useful if you’re integrating CVE checks into CI/CD, writing scanners, or just want better visibility.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/reallylonguserthing\"> /u/reallylonguserthing </a> <br/> <span><a href=\"http://GlobalCVE.xyz\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1oh4ge4/globalcve_unified_cve_feed_for_developers/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Golang for physics","url":"https://www.reddit.com/r/golang/comments/1oh307b/golang_for_physics/","date":1761532007,"author":"/u/EmployExpensive3182","guid":316571,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I tried searching but I noticed a lot of the posts were old, so maybe things have changed. So I start university next year, and I plan on majoring in mathematics, but want to get into a research lab for physics, and one of the professor brings on students who know programming and he said literally any program. I started learning Go, and have to say by far my favorite coding language, love it way more than Python, and slightly more than Java, and want to stick with it, however I want to also be useful. So with all this being said, is Golang a good choice for physics? What tools/libraries are there? Thanks in advance for any answers!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EmployExpensive3182\"> /u/EmployExpensive3182 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1oh307b/golang_for_physics/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oh307b/golang_for_physics/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker Alternative: Podman on Linux","url":"https://www.reddit.com/r/linux/comments/1oh1fj7/docker_alternative_podman_on_linux/","date":1761527392,"author":"/u/Unprotectedtxt","guid":316569,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>TL;DR Podman is less popular but better. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Unprotectedtxt\"> /u/Unprotectedtxt </a> <br/> <span><a href=\"https://linuxblog.io/docker-alternative-podman-on-linux/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1oh1fj7/docker_alternative_podman_on_linux/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Linux Boot Process: From Power Button to Kernel","url":"https://www.reddit.com/r/linux/comments/1oh1dhs/the_linux_boot_process_from_power_button_to_kernel/","date":1761527223,"author":"/u/Unprotectedtxt","guid":316817,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Unprotectedtxt\"> /u/Unprotectedtxt </a> <br/> <span><a href=\"https://www.0xkato.xyz/linux-boot/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1oh1dhs/the_linux_boot_process_from_power_button_to_kernel/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Executable Formats ( ELF, Mach-O, PE)","url":"https://www.reddit.com/r/programming/comments/1oh0o2l/executable_formats_elf_macho_pe/","date":1761525237,"author":"/u/Helpful_Geologist430","guid":315620,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Helpful_Geologist430\"> /u/Helpful_Geologist430 </a> <br/> <span><a href=\"https://youtu.be/ehxt6rTc9iI?si=3HkWHKw0WXrYqv6g\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1oh0o2l/executable_formats_elf_macho_pe/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My battery life went from 1 hour on windows to almost 5 on linux","url":"https://www.reddit.com/r/linux/comments/1oh0gqj/my_battery_life_went_from_1_hour_on_windows_to/","date":1761524661,"author":"/u/PR_freak","guid":315574,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>This is basically a comical difference, how is this even possible?</p> <p>I have a very old battery with 50% of its original capacity, is it possible that windows wasn&#39;t letting it fully charge because it is old but linux does?<br/> My pc is not warm at all but I am kind of afraid it will blow up now </p> <p>This is not a support question, I am here just to praise the linux gods</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PR_freak\"> /u/PR_freak </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1oh0gqj/my_battery_life_went_from_1_hour_on_windows_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1oh0gqj/my_battery_life_went_from_1_hour_on_windows_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VOA : mini secrets manager","url":"https://www.reddit.com/r/kubernetes/comments/1oh0gdh/voa_mini_secrets_manager/","date":1761524632,"author":"/u/Any-Associate-5804","guid":315573,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>This is my first project in DevOps and Backend An open-source mini Secrets Manager that securely stores and manages sensitive data, environment variables, and access keys for different environments (dev, staging, prod).</p> <p>It includes:</p> <ul> <li><p>A FastAPI backend for authentication, encryption, and auditing.</p></li> <li><p>A CLI tool (VOA-CLI) for developers and admins to manage secrets easily from the terminal.</p></li> <li><p>Dockerized infrastructure with PostgreSQL, Redis, and NGINX reverse proxy.</p></li> <li><p>Monitoring setup using Prometheus &amp; Grafana for metrics and dashboards.</p></li> </ul> <p>The project is still evolving, and I’d really appreciate your feedback and suggestions</p> <p>GitHub Repo: <a href=\"https://github.com/senani-derradji/VOA\">https://github.com/senani-derradji/VOA</a></p> <p>If you like the project, feel free to give it a Star!</p> <hr/> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Any-Associate-5804\"> /u/Any-Associate-5804 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh0gdh/voa_mini_secrets_manager/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1oh0gdh/voa_mini_secrets_manager/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what do you use Go for?","url":"https://www.reddit.com/r/golang/comments/1ogz8dl/what_do_you_use_go_for/","date":1761521210,"author":"/u/Least_Chicken_9561","guid":315550,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>well, when It comes to backend developement I think Go is one of the best options out there (fast to write, performant, no dependency hell, easy to deploy...), So that&#39;s my default language for my backends.<br/> but then I was trying to do some automation stuff, manipulate data, cli apps, etc in Go and I felt just weird, so I went back to python, it was more natural for me to do those things in python than in Go.<br/> so my question is, do you use Go for everything or just for certain tasks?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Least_Chicken_9561\"> /u/Least_Chicken_9561 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ogz8dl/what_do_you_use_go_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ogz8dl/what_do_you_use_go_for/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google PhD Fellowship recipients 2025 [D]","url":"https://www.reddit.com/r/MachineLearning/comments/1ogy6z9/google_phd_fellowship_recipients_2025_d/","date":1761518443,"author":"/u/Alternative_Art2984","guid":315523,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Alternative_Art2984\"> /u/Alternative_Art2984 </a>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KDE Linux deep dive: package management is amazing, which is why we don’t include it","url":"https://www.reddit.com/r/linux/comments/1ogxv8t/kde_linux_deep_dive_package_management_is_amazing/","date":1761517578,"author":"/u/giannidunk","guid":315524,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/giannidunk\"> /u/giannidunk </a> <br/> <span><a href=\"https://pointieststick.com/2025/10/25/kde-linux-deep-dive-package-management-is-amazing-which-is-why-we-dont-include-it/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogxv8t/kde_linux_deep_dive_package_management_is_amazing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Media] HB to me!🎂 Learning Rust, eating cheesecake","url":"https://www.reddit.com/r/rust/comments/1ogx5qb/media_hb_to_me_learning_rust_eating_cheesecake/","date":1761515736,"author":"/u/necodrre","guid":315549,"unread":true,"content":"<div><p>Rust feels wonderful so far btw (main language C and Go for networking)</p></div>   submitted by   <a href=\"https://www.reddit.com/user/necodrre\"> /u/necodrre </a>","contentLength":102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Bots Show Signs of Gambling Addiction, Study Finds","url":"https://www.newsweek.com/ai-bots-show-signs-of-gambling-addiction-study-finds-10921832","date":1761515231,"author":"/u/Disastrous_Award_789","guid":316888,"unread":true,"content":"<p>Large language models (LLMs)—the technology that powers popular AI chatbots like <a href=\"https://www.newsweek.com/topic/chatgpt\">ChatGPT</a> and <a href=\"https://www.newsweek.com/topic/google\">Google</a>’s Gemini—repeatedly made irrational, high-risk betting decisions when placed in simulated <a href=\"https://www.newsweek.com/topic/gambling\">gambling</a> environments, according to the results of a recent study. Given more freedom, the models often escalated their bets until they lost everything, mimicking the behavior of human gambling addicts.</p><p>In experiments led by researchers at the Gwangju Institute of Science and Technology in South Korea, four advanced <a href=\"https://www.newsweek.com/topic/ai\">AI</a> models—GPT-4o-mini and GPT-4.1-mini by OpenAI, Gemini-2.5-Flash by Google, and Claude-3.5-Haiku by Anthropic—were tested in a slot machine simulation. Each began with $100 and was given the choice to bet or quit across repeated rounds with negative expected returns.&nbsp;</p><p>The study, <a href=\"https://arxiv.org/abs/2509.22818\">published on the research platform arXiv</a>, found that once the models were allowed to vary their bets and set their own targets, irrational behavior surged — and bankruptcy became a common outcome.</p><p>The researchers documented clear signs of gambling-related cognitive distortions. These included the illusion of control, the gambler’s fallacy — the notion that an outcome is more likely to happen after it occurred less frequently than expected or vice versa — and loss chasing. In many cases, models rationalized larger bets after losses or winning streaks, even though the rules of the game made such choices statistically unwise.&nbsp;</p><p>One example from the study shows a model stating, “a win could help recover some of the losses,” a hallmark of compulsive betting behavior.</p><p>Behavior was tracked using an “irrationality index,” which combined aggressive betting patterns, responses to loss and high-risk decisions. When prompt instructions encouraged models to maximize rewards or hit specific financial goals, irrationality increased. Variable betting options, as opposed to fixed bets, produced a dramatic rise in bankruptcy rates. Gemini-2.5-Flash, for instance, failed nearly half the time when allowed to choose its own bet amounts.</p><p>These behaviors weren’t just superficial. Using a sparse autoencoder to probe the models' neural activations, the researchers identified distinct “risky” and “safe” decision-making circuits. They showed that activating specific features inside the AI's neural structure could reliably shift its behavior toward either quitting or continuing to gamble—evidence, they argue, that these systems internalize human-like compulsive patterns, rather than simply mimicking them on the surface.</p><h2><strong>Between Reflection and Bias</strong></h2><p>Ethan Mollick, an AI researcher and Wharton professor who drew attention to the study online, said the findings reveal a complicated reality about how we interact with AI. In an interview, he told  that while LLM models are not conscious, the best way to use them is often to treat them as though they were human.</p><p>“They’re not people, but they also don’t behave like simple machines,” Mollick said. “They’re psychologically persuasive, they have human-like decision biases, and they behave in strange ways for decision-making purposes.”</p><p>AI systems are already being used in financial forecasting and market sentiment analysis. Some firms have trained proprietary models to analyze earnings reports and market news. But other research has shown these systems often favor high-risk strategies, follow short-term trends and underperform basic statistical models over time. A 2025 University of Edinburgh study found that LLMs failed to beat the market over a 20-year simulation period. They tended to be too conservative during booms and too aggressive during downturns—patterns that reflect common human investing mistakes.</p><p>While Mollick doesn’t believe the study alone justifies banning autonomous AI use in sensitive fields, he does see a need for strict limits and oversight.</p><p>“We have almost no policy framework right now, and that’s a problem,” he said. “It’s one thing if a company builds a system to trade stocks and accepts the risk. It’s another if a regular consumer trusts an LLM’s investment advice.”</p><p>He emphasized that AI systems inherit human biases from their training data and reinforcement processes. The gambler’s fallacy — such as when a bettor assumes the next spin of the roulette wheel will land on black because it landed on red several times in a row — is just one of many cognitive distortions they pick up.&nbsp;</p><p>Brian Pempus, a former gambling reporter and founder of the website Gambling Harm, which raises awareness about the dangers of gambling, cautioned that consumers may not be ready for the associated risks.&nbsp;</p><p>“An AI gambling bot could give you poor and potentially dangerous advice,” he wrote. “Despite the hype, LLMs are not currently designed to avoid problem gambling tendencies.”</p><p>Mollick echoed those concerns and stressed the importance of keeping humans in the loop, particularly in healthcare and finance, where accountability still matters. “Eventually, if AI keeps outperforming humans, we’ll have to ask hard questions,” he said. “Who takes responsibility when it fails?”</p><p>The study concludes with a call for regulatory attention. “Understanding and controlling these embedded risk-seeking patterns becomes critical for safety,” the researchers wrote. As Mollick put it, “We need more research and a smarter regulatory system that can respond quickly when problems arise.”</p>","contentLength":5439,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ogwyhs/ai_bots_show_signs_of_gambling_addiction_study/"},{"title":"Floxy — Lightweight Saga Workflow Engine on Go","url":"https://www.reddit.com/r/golang/comments/1ogwyfe/floxy_lightweight_saga_workflow_engine_on_go/","date":1761515227,"author":"/u/Such_Humor_9911","guid":315511,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Most modern systems are not just code that executes queries, but sequences of actions that must be performed atomically and restored in case of failure. This is not about business logic within a single function, but about process orchestration chains of steps where each operation can end in an error requiring compensation.</p> <p>This task is solved by the Saga pattern, one of the most complex and important architectural patterns. It describes how to perform a series of distributed rollback operations without resorting to global transactions.</p> <h1>The Problem</h1> <p>Manually implementing orchestration usually quickly turns into chaos. Errors have to be handled cascadingly, rollback logic is spread across the code, and attempts to add custom confirmation or parallel branches make the system unpredictable.<br/> On the other hand, there are mature platforms like Temporal or Cadence. They are reliable, but require the deployment of an entire infrastructure: brokers, workers, DSLs, and make a simple process dependent on an external ecosystem.<br/> Between these extremes Floxy appeared -- an embedded library on Go that implements the Saga pattern with orchestration, compensation, and interactive steps, without external services and heavy runtime.</p> <h1>The Philosophy of Floxy</h1> <p>Floxy is based on a simple idea: workflow is a part of the program, not a separate service. Instead of a dedicated platform with RPC and brokers, Floxy offers a library in which the business process is described using regular Go code - without a new language or YAML files. Basic principles:</p> <ol> <li>Minimalism. Everything is built around context.Context, pgx, and simple data structures.</li> <li>Predictability. Any state is stored in PostgreSQL; the behavior is deterministic.</li> <li>Isolation. All tables are created in the workflows schema without interfering with the application logic.</li> <li>Orchestration as a library. Saga, retry, rollback, and human-in-the-loop are available without an external runtime.</li> <li>Versioning. Each workflow template has a version number, ensuring the safe development of processes.</li> </ol> <h1>Key Features</h1> <p>Floxy implements a full set of functions for building reliable orchestrations:<br/> - Saga with orchestration and compensation. Each step can have an OnFailure handler that performs rollback or compensation.<br/> - SavePoint. Partial rollback to the last saved point.<br/> - Conditional steps. Logic branches using Go templates -- without an external DSL.<br/> - Parallel / Fork / Join. Parallel execution branches and subsequent synchronization.<br/> - Human-in-the-loop. Support for steps that require human intervention (confirm, reject).<br/> - Cancel and Abortion. Soft cancellation or immediate shutdown of workflow.<br/> - Idempotency-aware steps. The execution context (StepContext) provides the IdempotencyKey() method, which helps developers implement secure operations.<br/> - Migrations are embedded via go:embed. Floxy is completely self-sufficient and has the function of applying migrations.</p> <h1>Architecture</h1> <p>Floxy is a library with simple but expressive abstractions:</p> <ol> <li>Store is a layer for storing templates, template instances, states, and events (PostgreSQL via pgx).</li> <li>Builder is a workflow template builder</li> <li>Engine - executor and coordinator of steps: plans, rolls back, repeats, synchronizes.</li> <li>Worker Pool - a background pool that processes a queue of steps.</li> <li>Each step is performed in a context (context.Context), and the background worker checks the workflow_cancel_requests table in order to interrupt long-running steps in a timely manner.</li> </ol> <h1>Workflow as a Graph</h1> <p>A workflow in Floxy is a directed acyclic graph (DAG) of steps defined through the built-in Builder API.<br/> The Builder creates an adjacency list structure, checks for cycles, and serializes the description to JSON for storage in workflow_definitions.</p> <p><code>wf, _ := floxy.NewBuilder(&quot;order&quot;, 1).</code><br/> <code>Step(&quot;reserve_stock&quot;, &quot;stock.Reserve&quot;).</code><br/> <code>Then(&quot;charge_payment&quot;, &quot;payment.Charge&quot;).</code><br/> <code>OnFailure(&quot;refund&quot;, &quot;payment.Refund&quot;).</code><br/> <code>Step(&quot;send_email&quot;, &quot;notifications.Send&quot;).</code><br/> <code>Build()</code></p> <p>If the Builder detects a cycle, Build() returns an error, ensuring the graph is correct even before the flow is run in the engine.</p> <h1>Versioning and Isolation</h1> <p>Each workflow template is stored with a version number. When updating a template, the developer must increment the version number. This ensures that running instances continue to execute according to their original schema.<br/> All Floxy tables are located in a separate workflows schema, including the workflow_instances, workflow_steps, workflow_events, and workflow_definitions tables, among others. This ensures complete isolation and simplifies integration into existing applications.</p> <h1>Human-in-the-loop</h1> <p>Floxy supports interactive steps (StepTypeHuman) that pause execution and wait for a user decision.<br/> The workflow enters the waiting_decision state, and the decision (confirmed or rejected) is written to the workflow_human_decisions table. After this, the engine either continues execution or terminates the process with an error.<br/> Thus, Floxy can be used not only for automated processes but also for scenarios requiring confirmation, review, or manual control.</p> <h1>Cancel and Abort</h1> <p>Floxy supports two stopping mechanisms:<br/> - Cancel - rolls back to the root (save points are ignored),<br/> - Abort - immediately terminates execution without compensation.</p> <p>Both options are initiated by adding an entry to the workflow_cancel_requests table. The background worker periodically polls it and calls context.CancelFunc() for active steps of the corresponding instance.</p> <h1>Tests and Examples</h1> <p>Floxy is covered by a large number of unit and integration tests that use testcontainers to automatically deploy PostgreSQL in a container. This ensures the engine operates correctly in all scenarios: from simple sequential flows to complex parallel and compensation processes.<br/> Furthermore, the repository contains numerous examples (./examples) demonstrating various step types, the use of OnFailure, branches, conditions, human-in-the-loop scenarios, and the rollback policy. This makes getting started with the project simple and intuitive, even for Go newbies.<br/> Furthermore, the repository is equipped with extensive documentation and PlantUML diagrams, allowing for a detailed understanding of the engine&#39;s workflow.</p> <h1>Why Floxy Stays Lightweight</h1> <p>Floxy doesn&#39;t use brokers, RPC, or external daemons. It runs entirely within the application process, relying solely on PostgreSQL and the standard Go and pgx packages:<br/> - pgx - a fast driver and connection pool;<br/> - context - operation lifetime management;<br/> - net/http - REST API via the new ServeMux;<br/> - go:embed - built-in migrations and schemas. Despite the presence of background workers and a scheduler, Floxy remains a library, not a platform, without separate binaries or RPC protocols.</p> <h1>Example of Usage</h1> <p><code>engine := floxy.NewEngine(pgxPool)</code><br/> <code>defer engine.Shutdown()</code></p> <p><code>wf, _ := floxy.NewBuilder(&quot;order&quot;, 1).</code><br/> <code>Step(&quot;reserve_stock&quot;, &quot;stock.Reserve&quot;).</code><br/> <code>Then(&quot;charge_payment&quot;, &quot;payment.Charge&quot;).</code><br/> <code>OnFailure(&quot;refund&quot;, &quot;payment.Refund&quot;).</code><br/> <code>Step(&quot;send_email&quot;, &quot;notifications.Send&quot;).</code><br/> <code>Build()</code></p> <p><code>engine.RegisterWorkflow(ctx, wf)</code></p> <p><code>engine.RegisterHandler(&amp;ReserveStock{})</code><br/> <code>engine.RegisterHandler(&amp;ChargePayment{})</code><br/> <code>engine.RegisterHandler(&amp;RefundPayment{})</code><br/> <code>engine.RegisterHandler(&amp;Notifications{})</code></p> <p><code>workerPool := floxy.NewWorkerPool(engine, 3, 100*time.Millisecond)</code><br/> <code>workerPool.Start(ctx)</code></p> <p><code>instanceID, err := engine.Start(ctx, &quot;order-v1&quot;, input)</code></p> <h1>Conclusion</h1> <p>Floxy solves the same problem as large orchestrators, but with the library philosophy inherent to Go: minimal abstractions, maximum control.<br/> It implements the Saga pattern with orchestration, supports compensation, conditions, parallelism, and interactive steps - all while remaining lightweight, transparent, and embeddable.<br/> Floxy is a tool for those who prefer manageability without infrastructure and reliability without redundancy.</p> <p><a href=\"http://github.com/rom8726/floxy\">http://github.com/rom8726/floxy</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Such_Humor_9911\"> /u/Such_Humor_9911 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ogwyfe/floxy_lightweight_saga_workflow_engine_on_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ogwyfe/floxy_lightweight_saga_workflow_engine_on_go/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oracle un go","url":"https://www.reddit.com/r/golang/comments/1ogvjuc/oracle_un_go/","date":1761511725,"author":"/u/Equivalent_Egg5248","guid":315510,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Which Go library(orm) would you use to integrate with Oracle? I understand GORM doesn’t have official support for it, and there’s a go-ora package that’s unofficial… would I need to use the standard database/sql library instead? Has anyone faced this issue before? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Equivalent_Egg5248\"> /u/Equivalent_Egg5248 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ogvjuc/oracle_un_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ogvjuc/oracle_un_go/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Btrfs iowait bug?","url":"https://www.reddit.com/r/linux/comments/1ogvbxn/btrfs_iowait_bug/","date":1761511191,"author":"/u/bankroll5441","guid":315494,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>A couple of weeks ago I noticed on my Node Exporter dashboard that Fedora (gnome) picked up some iowait. Of course I looked into it as all other metrics seemed normal, and thought it might have been some devices running over UASP. I didn&#39;t find any dmesg errors for those devices, system load and performance is normal. It seems to happen when the system is idle, as shown by the screenshots. There is little to no disk activity on this machine when its idle except for a couple of lightweight containers. </p> <p>I thought it was maybe due to the LUKS partition but I have 3 other machines running Fedora also with LUKS and are not experiencing this. It seems to be purely cosmetic, but was wondering if anyone else is experiencing this or knows a solution (seeing it in the graphs bugs me lol).</p> <p><strong><em>This sub only lets me post one image so I can&#39;t include the other metrics</em></strong></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bankroll5441\"> /u/bankroll5441 </a> <br/> <span><a href=\"https://i.redd.it/82sr40r5pixf1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogvbxn/btrfs_iowait_bug/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Cutting Inference Costs from $46K to $7.5K by Fine-Tuning Qwen-Image-Edit","url":"https://www.reddit.com/r/MachineLearning/comments/1ogud3r/p_cutting_inference_costs_from_46k_to_75k_by/","date":1761508864,"author":"/u/FallMindless3563","guid":315508,"unread":true,"content":"<p>Wanted to share some learnings we had optimizing and deploying Qwen-Image-Edit at scale to replace Nano-Banana. The goal was to generate a product catalogue of 1.2m images, which would have cost $46k with Nano-Banana or GPT-Image-Edit.</p><p>Qwen-Image-Edit being Apache 2.0 allows you to fine-tune and apply a few tricks like compilation, lightning lora and quantization to cut costs.</p><p>The base model takes ~15s to generate an image which would mean we would need 1,200,000*15/60/60=5,000 compute hours.</p><p>Compilation of the PyTorch graph + applying a lightning LoRA cut inference down to ~4s per image which resulted in ~1,333 compute hours.</p><p>I'm a big fan of open source models, so wanted to share the details in case it inspires you to own your own weights in the future.</p>","contentLength":761,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"None of my windows have top bar","url":"https://www.reddit.com/r/linux/comments/1ogtxbo/none_of_my_windows_have_top_bar/","date":1761507811,"author":"/u/venugopal_C137","guid":315474,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/venugopal_C137\"> /u/venugopal_C137 </a> <br/> <span><a href=\"/r/Fedora/comments/1ogtw5c/none_of_my_windows_have_top_bar/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogtxbo/none_of_my_windows_have_top_bar/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI just hosted the Human vs Animal Olympics… and humans didn’t win 🏃‍♂️🦁","url":"https://v.redd.it/orkmyyjlcixf1","date":1761507007,"author":"/u/thinkhamza","guid":315509,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ogtl6x/ai_just_hosted_the_human_vs_animal_olympics_and/"},{"title":"[P] Built a GPU time-sharing tool for research labs (feedback welcome)","url":"https://www.reddit.com/r/MachineLearning/comments/1ogrf13/p_built_a_gpu_timesharing_tool_for_research_labs/","date":1761501881,"author":"/u/not-your-typical-cs","guid":315493,"unread":true,"content":"<p>Built a side project to solve GPU sharing conflicts in the lab: </p><p>: 1 GPU, 5 grad students, constant resource conflicts.</p><p>: Time-based partitioning with auto-expiration.</p><pre><code>from chronos import Partitioner with Partitioner().create(device=0, memory=0.5, duration=3600) as p: train_model() # Guaranteed 50% GPU for 1 hour, auto-cleanup </code></pre><p>- Works on any GPU (NVIDIA, AMD, Intel, Apple Silicon)</p><p> 3.2ms partition creation, stable in 24h stress tests.</p><p>Built this weekends because existing solutions . Would love feedback if you try it!</p><p>: pip install chronos-gpu</p>","contentLength":542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] SDLArch-RL is now compatible with libretro Software Render cores!!!","url":"https://www.reddit.com/r/MachineLearning/comments/1ogr8h2/p_sdlarchrl_is_now_compatible_with_libretro/","date":1761501455,"author":"/u/AgeOfEmpires4AOE4","guid":316662,"unread":true,"content":"<p>This week I made a series of adjustments, including making the environment's core compatible with Libretro cores, which are software renderers. Now you can train Reinforcement Learning with PS2, Wii, Game Cube, PS1, SNES, and other games!</p><p>If anyone is interested in collaborating, we're open to ideas!!! And also to anyone who wants to code ;)</p>","contentLength":342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[OC] Summit - AI-generated commit messages in the terminal!","url":"https://www.reddit.com/r/golang/comments/1ogqi97/oc_summit_aigenerated_commit_messages_in_the/","date":1761499776,"author":"/u/1samsepiol_","guid":315439,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>This is an old project I&#39;ve picked up and refined. Check it out on GitHub!<br/> <a href=\"https://github.com/fwtwoo/summit\">https://github.com/fwtwoo/summit</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/1samsepiol_\"> /u/1samsepiol_ </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ogqi97/oc_summit_aigenerated_commit_messages_in_the/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ogqi97/oc_summit_aigenerated_commit_messages_in_the/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Looking for a open-source project","url":"https://www.reddit.com/r/MachineLearning/comments/1ogqezm/p_looking_for_a_opensource_project/","date":1761499567,"author":"/u/Mysterious_Assist447","guid":315473,"unread":true,"content":"<p>Hi everyone, i'm a Mathematical Engeneering student with a strong passion in math and its applications in ML. I have a lot of knowledge in Data Mining techniques and neural networks (DNN, CNN, RNN, LSTM).</p><p>I'm trying to find some open-source projects to contribute and use my knowledge in practice, do you know where can I find projects to work on?</p>","contentLength":346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lists are Geometric Series","url":"https://www.reddit.com/r/programming/comments/1ogpqoh/lists_are_geometric_series/","date":1761497949,"author":"/u/SnooLobsters2755","guid":315472,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SnooLobsters2755\"> /u/SnooLobsters2755 </a> <br/> <span><a href=\"https://iacgm.com/articles/adts/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ogpqoh/lists_are_geometric_series/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US robotics firm unveils driverless vehicles with vision-based AI for farm automation","url":"https://interestingengineering.com/innovation/us-robotics-firm-unveils-driverless-vehicles","date":1761496831,"author":"/u/Sackim05","guid":315438,"unread":true,"content":"<div><p>The swappable battery system provides over eight hours of operation per pack, allowing continuous use. Designed for maximum connectivity, the Flex also features open interfaces, enabling integration with a wide range of sensors, implements, and custom systems through standard power, data ports, and open APIs—making it one of the most versatile platforms in agricultural robotics today.</p><h2>Precision farm automation</h2><p>Bonsai Robotics also showcased the Amiga Max and Amiga Trax at FIRA, highlighting their purpose-built design for <a href=\"https://interestingengineering.com/innovation/rewiring-infrastructure-the-automation-revolution-in-utility-design\" target=\"_blank\" rel=\"dofollow\">autonomy</a> in agricultural and industrial applications. Both vehicles are powered by Bonsai Intelligence, the company’s vision-based autonomy platform, enabling advanced perception, navigation, and task execution in real-world environments.</p><p>The Amiga Trax is a modular, low-clearance vehicle engineered for rugged outdoor work. It is developed for spraying, weeding, mowing, and hauling across flat or sloped terrain. Its design makes it ideal for vineyards, cane fruit, perennial crops, and off-road operations in non-agricultural industries.</p></div>","contentLength":1069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ogp9v3/us_robotics_firm_unveils_driverless_vehicles_with/"},{"title":"Maybe the 9-5 Isn’t So Bad After All","url":"https://www.reddit.com/r/programming/comments/1ogp3v1/maybe_the_95_isnt_so_bad_after_all/","date":1761496419,"author":"/u/thehustlingengineer","guid":315492,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thehustlingengineer\"> /u/thehustlingengineer </a> <br/> <span><a href=\"https://open.substack.com/pub/thehustlingengineer/p/maybe-the-95-isnt-so-bad-after-all?r=yznlc&amp;utm_medium=ios\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ogp3v1/maybe_the_95_isnt_so_bad_after_all/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Debugging process running with Tmux along with many other services","url":"https://www.reddit.com/r/golang/comments/1ogo3uq/debugging_process_running_with_tmux_along_with/","date":1761494029,"author":"/u/w32unix","guid":315394,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>So I recently joined a company, really like the product but the code is real mess. It’s a huge repo with all services they have. Bunch of go, nodejs, vue3, react, angular 1.. all together. I don’t think it’s even a monorepo, just many things in all repo. All services, ui run all together with tmux. </p> <p>An example of some</p> <p>GO part</p> <pre><code>tmux new-session -d -s SOME_SYSTEM \\ \\; new-window -d -n api -c ./REPO/systems/api &quot;fd -e go --exclude=\\&quot;**/wire*.go\\&quot; | entr -cr go run . start&quot; \\ \\; new-window -d -n backend -c ./REPO/systems/backend &quot;fd -e go -e toml --exclude=\\&quot;**/wire*.go\\&quot; --exclude=\\&quot;vendor/**\\&quot; | entr -cr go run . -c config.local.toml server&quot; \\ </code></pre> <p>Node part </p> <pre><code>\\; new-window -d -n iu -c ./REPO/services/iu &#39;node --inspect=9233 --watch bin/master.js&#39; \\ </code></pre> <p>As you see in node services I can add --inspect=9233 and do some debugging with chrome//inspect </p> <p>But I cannot find a way to debug go services, I wish i could connect somehow with goland to go process. </p> <p>Other team members printing to logs to understand what happens in code </p> <p>I cannot remove some service from tmux and run it standalone and debug, the tmux flow adds all the envs, cors, caddy shit. I tried to run it standalone, but after all it gave me CORS, adding cors to service itself didn&#39;t help because of caddy.. </p> <p>So is there anyway to attach debugger to the go process?</p> <p>Thx</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/w32unix\"> /u/w32unix </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ogo3uq/debugging_process_running_with_tmux_along_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ogo3uq/debugging_process_running_with_tmux_along_with/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Now that I know Rust after doing several projects (most web microservices), I can say with confidence, I can easily use Rust for all back-end related tasks as I do with Go and Python for the last 8 years working as Senior Back-end Dev (Today I'm Staff SWE focused at back-end and distributed system).","url":"https://www.reddit.com/r/rust/comments/1ogo351/now_that_i_know_rust_after_doing_several_projects/","date":1761493983,"author":"/u/swordmaster_ceo_tech","guid":315437,"unread":true,"content":"<p>This is something that I wasn't confident when started to enjoy Rust, for the context. I worked mostly using golang for the last 8 years in fintechs, big tech, startups etc, most of the time Go + a legacy codebase in Java, PHP, Python etc.</p><p>The thing was, a language without GC would still be productive? And after using Rust I started to get very comfort with writing the code without getting into any trouble, sure the async model is not as easy as in Go or modern Java with virtual threads, but it is literally the same async colored functions that we use in Python, old Java, PHP, and several other languages for years, it is definitely easy and is not the big deal.</p><p>Most of the work in my domain that is distributed systems and back-end, is just concurrency code, IO bound, and Rust with Tokio is great for this, same great performance that I achieve with Go, but even more safe because the code is always checked to be thread safe and doesn't have race conditions.</p><p>And sure, we don't have many problems using Go like people who never work with it exaggerates, I never miss a sleep for a null pointer, and was rare to see someone creating race conditions problems, but at the same time, after you learn Rust, you're learning way more consistent to be better at concurrency thinking about thread safe and preventing race conditions than using Go, and naturally you will become a more efficient software engineer. And even working with very experienced SWE in Go and Java, you came to a point where you cannot continue to get better unless you start to use C++ or drop the GC, so if the curve after learning is pretty much the same (serious, 99% of our job as back-end is calling apis, db, and create concurrent workers to process something async) you definitely can be productive using Rust for this as in any other language, the crates are very mature already, but you're choosing a path that will let always grow in efficiency as SWE (for the cost of maybe one or two days more to build a feature).</p><p>I already take my decision, I will and already am using Rust for all back-end related and I just use Go or Python if I don't have the Rust runtime like Google cloud function (I don't know if they finally added support) or things like this, otherwise I truly believe Rust should be the main language for general back-end, even in startups that need to move fast, is still fast, because the majority of startups like I said, the work is just calling apis, db, and creating workers etc, no big deal, people just love to pretend that is a complex work when it's not, the complexity is very outside the code in design the solutions for distributed systems, and for that, Rust is just fine and let you become each day more efficient as SWE building better software.</p>","contentLength":2759,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"httpreplay - CLI tool for replaying HTTP requests","url":"https://www.reddit.com/r/golang/comments/1ognlu6/httpreplay_cli_tool_for_replaying_http_requests/","date":1761492816,"author":"/u/Witty_Crab_2523","guid":315393,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1ognlu6/httpreplay_cli_tool_for_replaying_http_requests/\"> <img src=\"https://external-preview.redd.it/xgQVcVmpySghdt6qoD2Vz_5jt_d0w6f8wh9cg01Ncgg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=653c1cbaf49cefcf30d4f3b800ace3ac1e39409e\" alt=\"httpreplay - CLI tool for replaying HTTP requests\" title=\"httpreplay - CLI tool for replaying HTTP requests\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>CLI tool for batch replaying HTTP requests with adjustable concurrency and QPS. Supports progress tracking, interruption (Ctrl-C), and resuming with updated settings. Perfect for restoring lost production HTTP request data.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Witty_Crab_2523\"> /u/Witty_Crab_2523 </a> <br/> <span><a href=\"https://github.com/roy2220/httpreplay\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ognlu6/httpreplay_cli_tool_for_replaying_http_requests/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FYI - lenovo let's you configure with Fedora and Ubuntu","url":"https://www.reddit.com/r/linux/comments/1ogmuu1/fyi_lenovo_lets_you_configure_with_fedora_and/","date":1761491003,"author":"/u/cranberrie_sauce","guid":315392,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>FYI - lenovo let&#39;s you configure with Fedora and Ubuntu</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cranberrie_sauce\"> /u/cranberrie_sauce </a> <br/> <span><a href=\"https://i.redd.it/raqfk4xy0hxf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogmuu1/fyi_lenovo_lets_you_configure_with_fedora_and/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I built an AI you can FaceTime. It talks, moves and lives in real time.","url":"https://www.reddit.com/r/MachineLearning/comments/1ogmqpr/p_i_built_an_ai_you_can_facetime_it_talks_moves/","date":1761490738,"author":"/u/Smooth_Resource1616","guid":315368,"unread":true,"content":"<p>I’ve been working on a project that turns the idea of FaceTiming an AI into reality. I used AI to generate a full body person and integrated it into my system which now supports real time video generation and natural communication. When I “FaceTime” my AI, it appears to be doing normal human activities like cooking or shopping… etc and responds in real time. If I ask what it’s making it brings the camera closer to show me, just like a real person would. It’s still in development but the foundation for real time interaction and environment simulation is already working</p>","contentLength":586,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wordlist Generation tool & language","url":"https://www.reddit.com/r/golang/comments/1ogmnns/wordlist_generation_tool_language/","date":1761490525,"author":"/u/Nearby-Gur-2928","guid":315369,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1ogmnns/wordlist_generation_tool_language/\"> <img src=\"https://external-preview.redd.it/MIj_Cwcgv5uRnUYfpH3V6kRHb1_bEFCmD52kyaa3hKQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=472e062150d4ad1e1821a3e92344f8db8fae4910\" alt=\"Wordlist Generation tool &amp; language\" title=\"Wordlist Generation tool &amp; language\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><ul> <li>this project for you if you interested in cyber security </li> </ul> <ol> <li>the tool built in pure golang</li> <li>this tool is a wordlist generator but not like other tools cupp,cewl,....</li> <li>its a scripting language or (DSL) only for wordlist generation</li> </ol> <p>repositry: <a href=\"https://github.com/0xF55/bat\">https://github.com/0xF55/bat</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Nearby-Gur-2928\"> /u/Nearby-Gur-2928 </a> <br/> <span><a href=\"https://github.com/0xF55/bat\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ogmnns/wordlist_generation_tool_language/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dismembered mom's remains found after daughter used AI to conceal brutal killing","url":"https://www.the-express.com/news/world-news/188412/dismembered-mom-s-remains-found-after-daughter-allegedly-used-ai-conceal-brutal-killing","date":1761490514,"author":"/u/TheExpressUS","guid":315621,"unread":true,"content":"<div><p>A mother was brutally murdered and dismembered by her own daughter, who used artificial intelligence to conceal <a data-link-tracking=\"InArticle|Link\" href=\"https://www.the-express.com/news/us-news/157984/mom-fed-daughter-bleach-joanne-zephir-florida-strangled?utm_source=mynewsassistant.com&amp;utm_medium=referral&amp;utm_campaign=embedded_search_item_desktop\" target=\"_blank\">her gruesome crime</a>.</p><p>The remains of Martha Cecilia Solís Cruz, from Guayaquil, Ecuador, were found last week in the home she shared with her daughter, Andreína Lamota Solís. Cruz had been reported missing on October 6 through a complaint filed with the Guayas Prosecutor's Office. Ten days later, on October 16, Cruz's body was discovered inside her home in Sauces 9, in the <a data-link-tracking=\"InArticle|Link\" href=\"https://www.the-express.com/news/world-news/163901/air-force-colonel-shot-dead-ecuador-country-descends-state-war-drugs?utm_source=mynewsassistant.com&amp;utm_medium=referral&amp;utm_campaign=embedded_search_item_desktop\" target=\"_blank\">north of Guayaquil</a>.</p><p>Further investigation revealed that the 49-year-old <a data-link-tracking=\"InArticle|Link\" href=\"https://www.the-express.com/news/world-news/188403/illegal-immigrant-france-lola-Daviet\" target=\"_blank\">victim had been cut</a> into six pieces. The mother's remains were found inside a washing machine and a blue barrel, according to Ecuadorian tabloid newspaper Diario EXTRA.</p><p>During the investigation, police determined that Solís had attempted to deceive relatives and officers by using AI to imitate her mother's voice in audio messages.</p><p>The alleged murderer also disguised herself as her mother to appear on security cameras, in an attempt to create the illusion that Cruz was still alive.</p></div><div><p>It was the suspicious one-day rental of a room in La Alborada, a well-known, large residential district in the region, where Solís arrived disguised as her mother, that raised suspicions, Colonel Galo Muñoz, head of the National Directorate of Crimes Against Life, disclosed, reports the Daily Star.</p><p>The officer stated: \"When we confirmed that she had rented a room in a house for just one day, despite having her own home, we knew something was off. She arrived at the house dressed as her mother and left in her own clothes. It was a deliberate attempt to mislead.\"</p><p>According to Muñoz, the victim was last seen alive on Sunday, October 5, following a family gathering.</p><p>Solís confessed to the crime on October 16, after being presented with the evidence discovered in the home. She was subsequently arrested and charged with murder.</p></div><div><p>Muñoz revealed: \"We confronted her and showed her photos of her mother's body, which was inside the house. She had no choice but to confess. She said: 'Yes, I killed her.'\".</p><p>\"In her room, we also found a bank card belonging to a friend of hers who was reported missing a few years ago, although the report was withdrawn by the mother two days later, possibly because the young woman reappeared.\"</p><p>The officer further added: \"The key clue was the rental of that room, on the very day she left dressed as her mother.</p><p>\"She did it to avoid leaving traces; she couldn't go to a gas station or a hotel, where she would have been recorded by security cameras.\"</p></div><div><p>The head of Dinased indicated that the case remains under investigation, as experts are analyzing more than 200 gigabytes of information from the accused's cell phone, where new evidence could be found linking her to other incidents. \"I believe there may be more victims. There is information we are verifying,\" the colonel admitted.</p><p>There is reportedly no official technical report yet confirming claims from alleged criminologists, as the autopsy is still in progress and requires further anthropological analysis due to the body being found dismembered.</p><p>Muñoz stated: \"This case is spectacular from an investigative point of view. The woman has psychopathic traits: coldness, calculation, and manipulation.\"</p></div>","contentLength":3246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ogmnh4/dismembered_moms_remains_found_after_daughter/"},{"title":"Skilled Infra admin - Looking for part time work, Finances messed up","url":"https://www.reddit.com/r/kubernetes/comments/1ogmj0h/skilled_infra_admin_looking_for_part_time_work/","date":1761490205,"author":"/u/TheFailedTechie","guid":315366,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Skilled Linux, Kubernetes, Infra admin have 9 years experience and brilliant troubleshooting skills. Understands systems, Containers, Network, automation etc</p> <p>My financial situation is messed up due to option trading debt, If someone is willing to offer some design, automation or support work during US hours or one of projects, Please reach out. I work in Apac Hours</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheFailedTechie\"> /u/TheFailedTechie </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogmj0h/skilled_infra_admin_looking_for_part_time_work/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogmj0h/skilled_infra_admin_looking_for_part_time_work/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Would it be OK to use Local internalTrafficPolicy for the kube-apiserver’s Service?","url":"https://www.reddit.com/r/kubernetes/comments/1ogltav/would_it_be_ok_to_use_local_internaltrafficpolicy/","date":1761488409,"author":"/u/mmmfine","guid":315365,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Each node does have its own kube-apiserver.</p> <p>For context, we have a Pekko cluster and, to handle split brain situations, we use Kubernetes leases.</p> <p>However, we found that sometimes after killing a Kubernetes node, the other surviving node would acquire a lease successfully, but then lose it during renewal because it’d timeout connecting to the API server (presumably because it was still being DNATtted to the node we had just killed.)</p> <p>I assume we could very easily solve this by having they always communicate to the local API server.</p> <p>But is this at all a good idea? I am new to Kubernetes, I am not sure how stable the API server is, and whether or not having it always load balanced across nodes is crucial.</p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mmmfine\"> /u/mmmfine </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogltav/would_it_be_ok_to_use_local_internaltrafficpolicy/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogltav/would_it_be_ok_to_use_local_internaltrafficpolicy/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Guide: Data Analysis in Rust","url":"https://www.reddit.com/r/rust/comments/1oglp67/new_guide_data_analysis_in_rust/","date":1761488117,"author":"/u/Eric_Fecteau","guid":316733,"unread":true,"content":"<div><p>This new <a href=\"https://ericfecteau.ca/data/rust-data-analysis/index.html\">Data analysis in Rust</a> book is a \"learn by example\" guide to data analysis in Rust. It assumes minimal knowledge of data analysis and minimal familiarity with Rust and its tooling.</p><ul><li>The first section explores concepts related to data analysis in Rust, the crates (libraries) used in the book and how to collect the data necessary for the examples.</li><li>The second section explains how to read and write various types of data (e.g.  and ), including larger-than-memory data. This section also focuses on the various locations that data can be read from and written to, including local data, cloud-based data and databases.</li><li>The third section demonstrates how to transform data by adding and removing columns, filtering rows, pivoting the data and joining data together.</li><li>The fourth section shows how do summary statistics, such as counts, totals, means and percentiles, with and without survey weights. It also gives some examples of hypothesis testing.</li><li>The fifth and last section has examples of publication avenues, such as exporting summary statistics to excel, plotting results and writing markdown reports.</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/Eric_Fecteau\"> /u/Eric_Fecteau </a>","contentLength":1140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's going on with openssh.com?","url":"https://www.reddit.com/r/linux/comments/1ogktb8/whats_going_on_with_opensshcom/","date":1761485824,"author":"/u/Jealous_Diver_5624","guid":315475,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Tried to access their guidance mentioned in the new-ish post-quantum warning, noticed their domain seems to point to a parked STRATO page, TLS is no longer working, registrar information changed, whois information last updated 2025-10-24.</p> <p>Did they accidentally their entire domain?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jealous_Diver_5624\"> /u/Jealous_Diver_5624 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ogktb8/whats_going_on_with_opensshcom/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogktb8/whats_going_on_with_opensshcom/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 Hard-Won Lessons from a Year of Rebuilding a Search System","url":"https://www.reddit.com/r/programming/comments/1ogklk9/5_hardwon_lessons_from_a_year_of_rebuilding_a/","date":1761485234,"author":"/u/Journerist","guid":315619,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I wanted to start a discussion on an experience I had after a year of rebuilding a core search system.</p> <p>As an experienced architect, I was struck by how this specific domain (user-facing search) forces a different application of our fundamental principles. It&#39;s not that &quot;velocity,&quot; &quot;data-first,&quot; or &quot;business-value&quot; are new, but their prioritization and implementation in this context are highly non-obvious.</p> <p>These are the 5 key &quot;refinements&quot; we focused on that ultimately led to our success:</p> <ul> <li>It&#39;s a Data &amp; Product Problem First. We had to shift focus from pure algorithm/infrastructure elegance to the speed and quality of our user data feedback loops. This was the #1 unlock.</li> <li>Velocity Unlocks Correctness. We prioritized a scrappy, end-to-end working pipeline to get A/B data fast. This validation loop allowed us to find correctness, rather than just guessing at it in isolation.</li> <li>Business Impact is the North Star. We moved away from treating offline metrics (like nDCG) as the goal. They became debugging tools, while the real north star became a core business KPI (engagement, retention, etc.).</li> <li>Blurring Lines Unlocks Synergy. We had to break down the rigid silos between Data Science, Backend, and Platform. Progress ignited when data scientists could run A/B tests and backend engineers could explore user data directly.</li> <li>A Product Mindset is the Compass. We re-focused from &quot;building the most elegant system&quot; to &quot;building the most effective system for the user.&quot; This clarity made all the difficult technical trade-offs obvious.</li> </ul> <p>Has anyone else found that applying core principles in domains like ML/search forces a similar re-prioritization? Would love to hear your experiences.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Journerist\"> /u/Journerist </a> <br/> <span><a href=\"https://www.sebastiansigl.com/blog/rebuilding-search-lessons-learned\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ogklk9/5_hardwon_lessons_from_a_year_of_rebuilding_a/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] A geometric interpretation of the weight update in GPTQ quantization algorithm and a novel solution","url":"https://www.reddit.com/r/MachineLearning/comments/1ogk2mr/r_a_geometric_interpretation_of_the_weight_update/","date":1761483775,"author":"/u/nivter","guid":315391,"unread":true,"content":"<p>GPTQ is a simplified modification of the OBQ method where the weights in a matrix are quantized in each row independently one at a time from left to right. After step  of quantization, the remaining unquantized weights are modified like so: <code>dW[i:] = H[i:,i] dW[i]/H[i,i]</code>. This expression is derived by forming a Lagrangian and setting its gradient to 0.</p><p>Another way to approach this problem is by using the Cholesky decomposition  of the Hessian  directly in the bilinear error term: <code>df = 1/2 * dw^T H dw = 1/2 ||L^T dW||^2</code>. Thus minimizing the error term is equivalent to minimizing the squared norm of . This squared norm can be converted into a form  where  is the vector of unquantized weights. This function is minimized when  equals the negative of projection of  in the column space of . </p><p>This provides a geometric interpretation of the weight update: <strong>the optimal update negates the projection of the error vector in the column space </strong>. This approach also leads to a new closed form solution that is different from the one above. However it can be shown that both the forms are equivalent.</p>","contentLength":1093,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI takes aim at Microsoft 365 Copilot","url":"https://www.theregister.com/2025/10/24/openai_chatgpt_company_knowledge/","date":1761479746,"author":"/u/NISMO1968","guid":316570,"unread":true,"content":"<p> OpenAI is chalenging Microsoft 365 Copilot with \"company knowledge,\" a new ChatGPT feature that connects to organizational data to generate business-specific answers.</p><p>Now available for ChatGPT Business, Enterprise, and Edu, <a target=\"_blank\" rel=\"nofollow\" href=\"https://openai.com/index/introducing-company-knowledge/\">company knowledge</a> integrates with apps like Slack, SharePoint, Google Drive, Teams, and Outlook. Notably absent: OneDrive, which could present an issue for some organizations.</p><p>Users authenticate each connector individually, meaning ChatGPT (powered by GPT-5) only accesses data it is authorized to see. OpenAI says it's all encrypted and won't be used for training. Administrators can review conversation logs through the Enterprise Compliance API⁠⁠ (opens in a new window) for reporting and regulatory purposes.</p><p> asked OpenAI where data might end up and where it would be processed, but it just sent us a copy of the announcement by way of response. Data residency support varies by connector, so organizations should verify details before deployment.</p><p>However, OpenAI's corporate push has limitations. Unlike Microsoft's deeply integrated \"Copilot All The Things\" approach, ChatGPT's company knowledge mode must be manually selected for each conversation and can't search the web, create images, or generate graphs. Users must toggle it off to access those features.</p><p>When disabled, ChatGPT might still use the connectors to answer questions, \"but its responses won't include the same depth or detailed citations.\" OpenAI said: \"We're working to bring these experiences together in the coming months.\"</p><p>ChatGPT Business is available at $25 per user per month (on an annual plan) versus Microsoft 365 Copilot's <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/microsoft-365-copilot/pricing\">$30 monthly fee</a>. Combined with stronger brand recognition, this potentially positions ChatGPT as an attractive alternative for enterprises exploring AI assistants. ®</p><h3>Updated to add at 1415 UTC, October 27</h3><p>An OpenAI spokesperson got in touch with some <a target=\"_blank\" rel=\"nofollow\" href=\"https://help.openai.com/en/articles/11509118-admin-controls-security-and-compliance-in-connectors-enterprise-edu-and-business\">more information</a> about data residency and connectors.</p><p>A lot depends on the connector type. For synced connectors, the synced search index is stored in OpenAI's US Azure datacenters. If an admin disconnects the connector, the index is immediately inaccessible, but it'll take up to 30 days for the underlying data to be deleted.</p><p>With non-synced connectors, OpenAI said: \"Chat and deep research connectors are compatible with data residency, but it's important to note that connected applications are third-party services, and data sent to a connected application is subject to that application's own data residency policies.\"</p><p>As for Europe, if data residency is set for the region, OpenAI will restrict storage until the queries and prompts go off to a connected application. The company reiterated the importance of ensuring any connected applications adhere to customer data residency requirements.</p>","contentLength":2778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ogirll/openai_takes_aim_at_microsoft_365_copilot/"},{"title":"FreeBSD for DevOps","url":"https://www.reddit.com/r/kubernetes/comments/1ogimxd/freebsd_for_devops/","date":1761479319,"author":"/u/MaXNuMbEr1989","guid":315319,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MaXNuMbEr1989\"> /u/MaXNuMbEr1989 </a> <br/> <span><a href=\"/r/freebsd/comments/1ogimek/freebsd_for_devops/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogimxd/freebsd_for_devops/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GSoC '25: Parallel Macro Expansion","url":"https://lorrens.me/2025/10/26/GSoC-Parallel-Macro-Expansion.html","date":1761477939,"author":"/u/Snerrol","guid":315522,"unread":true,"content":"<p>A lot of work has been done to parallelise the Rust compiler, some parts are already parallelised, like codegen and processes after\nHIR-lowering, such as type-checking, borrow-checking and MIR optimisation. For this years GSoC project I had the chance to parallelise the macro expansion algorithm.</p><p>While the title of this post includes “ Macro Expansion”, I never got to parallelise the algorithm during this project. This was due to the sheer (unexpected) complexity of the project and\nnumerous complex issues/roadblocks that we faced as the project went on (one such roadblock even <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/421156-gsoc/topic/Project.3A.20Parallel.20Macro.20Expansion/near/542317541\">surprised</a> my mentor).\nThis post will highlight my work and the issues/roadblocks we faced.</p><p>The current algorithm is order-dependent, which prevents parallelisation. Simply put, it looks like this:</p><ol><li>Collect unresolved imports and macro invocations.</li><li>Resolve a single collected import.</li><li>Commit: write the resolved import back into its module.</li><li>Resolve a single collected macro.</li><li>Commit: expand the resolved macro.</li></ol><p>The idea is to parallelise , called import resolution, and , called macro expansion.</p><p>As said before, I was meant to do both parts of the algorithm, but I only made progress on the import resolution algorithm. To be complete, I’ll explain this briefly as well.</p><p>It iteratively resolves undetermined imports until no further progress can be made, committing each resolved import to the module resolutions before proceeding. The goal is to isolate\nthese resolutions so that each import in the undetermined set can be processed independently and committed afterwards. We called this batched import resolution because resolving and\ncommitting are done over the entire  of undetermined imports.</p><p>As some of you may observe, we would also need to remove any kind of mutability in the  that happens during this process, to avoid conflicts with other imports being resolved.</p><p>If we successfully implemented the above 2 points, it would be trivial to parallelise the algorithm.</p><p>These changes essentially boiled down into splitting the logic of the local crate and external crates. Items defined in external crates are considered a cache because they can not be changed once compiled,\nso we only populate external crates when we actually need them.</p><p>The  keeps a map of all compiled macros in a field called . You can access a compiled macro using a <a href=\"https://rustc-dev-guide.rust-lang.org/hir.html#identifiers-in-the-hir\"></a>. This map was read from or updated in this method:</p><div><div><pre><code></code></pre></div></div><p>However, local macros were always present in the map when this method was called, so the map was only updated here when we added an external macro.\n<a href=\"https://github.com/rust-lang/rust/pull/143657\">This PR</a> fixed this by splitting the maps and changing the logic to account for this.</p><p>An essential method of the  is . Anytime you want to add particular binding (i.e. a name pointing to a thing) to a module, you’d call this method:</p><div><div><pre><code></code></pre></div></div><p>However, we would only call this for external items when we eventually parallelise the algorithm. <a href=\"https://github.com/rust-lang/rust/pull/143884\">This PR</a>\nintroduced  and , where  takes a .\nThe leading blockers here were underscore disambiguators (a way to differentiate  items) and external module maps (remember macro maps), fixed by <a href=\"https://github.com/petrochenkov\">Vadim Petrochenkov</a> in <a href=\"https://github.com/rust-lang/rust/pull/144272\">#144272</a> and <a href=\"https://github.com/rust-lang/rust/pull/143550\">#143550</a> respectively.</p><p>This change was very important,  was used by the  method of the , this method provides a way to access the resolutions of a particular module. But if\nwe want to access the resolutions of an external module, we first need to populate its resolutions if this has not yet been done (remember this is a cache). Now that we can use  here, we can convert the  method to\ntake a  instead of a mutable one. This allowed us to convert a lot of methods from  to .</p><h3>Smaller but relevant changes</h3><p>A bit of context here: when an external crate is referenced through an  item or path, the  will try to load the referenced crate. This is done through the , which wraps the \nan important data structure of the compiler that keeps information of every external crate used by the currently compiling crate.</p><p>The problem was that the  also had a mutable reference to some state in the ,\nwhich meant we needed a mutable reference to the  to construct it. But the  actually never used that state, only the , <a href=\"https://github.com/rust-lang/rust/pull/144059\">this PR</a>\nrefactored the logic and that state of the  into the . Now anytime we need the logic of the , we can use the  instead, removing the need for a mutable .</p><h4>Cachify <code>ExternPreludeEntry.binding</code></h4><p>Updating this field is classified as a cache and was thus wrapped in a  in <a href=\"https://github.com/rust-lang/rust/pull/144605\">#144605</a>.</p><h3>Conditional Mutable Resolver</h3><p>Not all methods could be refactored like we did above; some require a mutable resolver when we are in later stages of name resolution. Some context here: when macro expansion is done, the compiler\ndoes a bunch of postprocessing on the AST, things like finalising imports/macros, error reporting and <a href=\"https://github.com/rust-lang/rust/blob/469357eb48d1c1b583386785ed0f846b9e7e0904/compiler/rustc_resolve/src/lib.rs#L1861\">a bunch of other things</a>, thus mutating the .\nThese methods know when to do extra things based on an optional parameter, and only when this parameter is present do we mutate the resolver.</p><p>We could have taken the approach I explained above, but it would have taken a lot of effort and time because these methods are quite complex and big. So we introduced a <code>Conditional Mutable Resolver</code> or  for short in <a href=\"https://github.com/rust-lang/rust/pull/144912\">#144912</a>, which can give out mutable reference to the  based on a particular condition:</p><div><div><pre><code></code></pre></div></div><p> is a flag set and unset in the code for import resolution. Now we can change the relevant methods to take in a  instead of a .\nThis type acts as a safeguard, it would panic if we try to get an exclusive reference during import resolution.</p><p>A related change was to add the same kind of wrappers around  and  using the same flag, it would also panic if we would mutate the underlying . This was introduced in <a href=\"https://github.com/rust-lang/rust/pull/146283\">#146283</a> which would allow us\nto make some important types  and  safe in the future.</p><h2>Batched Import Resolution</h2><p>This is where the fun starts and is still ongoing (it began on Aug 8), you can track the work <a href=\"https://github.com/rust-lang/rust/pull/145108\">here</a>.\nAs I said in the beginning, we must resolve all undetermined imports, collect their side effects, and write them to the  the same way the current algorithm does.\nImport resolution works with 2 kinds of imports: single imports () and glob imports (), the side-effects from these are also different. Converting this algorithm proved to be difficult.\nAny change I made that messed up the resolution order failed to compile , which made it hard to debug.</p><p>The first thing I encountered, which took me off guard, was a bug caused by the implicit prelude the compiler injects into your crate. It’s the first import that gets resolved, and every import after that depends on it:</p><div><div><pre><code></code></pre></div></div><p>Because we collect side-effects of imports, we also collect the side-effects of this special prelude import. Delaying these side-effects caused a bunch of confusing errors. <a href=\"https://github.com/rust-lang/rust/pull/145322\">This PR</a>\nresolves the prelude import the moment we encounter it in the AST, thus never adding it to the set of undetermined imports. The  thus always sees the side effects of the prelude import when resolving\nthe undetermined set.</p><p>The next challenge I faced was because of the way glob imports work. As you may know, glob imports import  from the module, so for , every public binding defined in  will be imported.\nIt is however possible that bindings can be defined in a module  the glob import is resolved, so the  needs a way to import these new bindings. This is done with a field in the <a href=\"https://doc.rust-lang.org/stable/nightly-rustc/rustc_resolve/struct.ModuleData.html\"></a>\ncalled <code>glob_importers: Vec&lt;Import&gt;</code>, it’s a way to find every resolved glob that references that module, so when we define a new binding, we go through that list and import it using those glob imports.\nThis works as intended in the current algorithm, but when in batched resolution, this can cause problems because adding a glob import to a module is a side-effect.\nConsider the following example where we first resolve  and then :</p><div><div><pre><code></code></pre></div></div><p>We first resolve  and create a binding for it. Then we resolve , which at that point only finds . Next, we commit  to  and the bindings from  to the root\nand only then add  to the  of module . Because  was already resolved earlier,  cannot be imported through it, which leads to an error when we try to use it.\nTo fix it we iterate twice over the side-effects: first to set all , and then apply the remaining side-effects, This ensures glob import correctly import all bindings.\nSo here  actually has a way to be imported by ;</p><p>Incorrectly applying side effects is a recurring theme in this PR: single imports have a different kind of side effects, these are per namespace. The thing I did wrong was mindlessly copy-pasting\nthe original code to the new  code. But this was wrong, some imports go through multiple rounds of resolution until we determined that each namespace has a binding or not. The problem was when\nwe resolved a particular import a second time for another namespace, we actually reset the bindings for other already determined namespaces, which broke a lot of things. The fix for this was a\n<code>Option&lt;Option&lt;NameBinding&gt;&gt;</code>, where the outer  specified if a side-effect was present, and the inner  was the actual side-effect for that namespace.</p><h3>Glob imports and #[macro_use]</h3><p>Once we fixed the above things, CI was green and my mentor/reviewer was content with the changes, we could run <a href=\"https://github.com/rust-lang/crater\">Crater</a> with my PR to see if it breaks code in the wild.\nAnd boy it did, when analysing the crater report I noticed that all regressions (i.e. things that failed but shouldn’t) were caused due to the <a href=\"https://crates.io/crates/rust-embed\"></a> crate. For some\nreason, it’s derive macro  exported as  (this export is important) couldn’t be resolved by its dependents. I’ll show the reduced case before explaining myself:</p><div><div><pre><code></code></pre></div></div><div><div><pre><code></code></pre></div></div><p>The example above shows what happens with the current algorithm, and it’s what most people  expect. But this is actually wrong and should be an error, but why? The  import\nand the <code>pub use rust_embed_impl::*;</code> glob import are actually  imports, because they import the same thing, but in a totally different way, the  imports privately while the glob\nimports publicly. Currently, the compiler can only detect ambiguous imports when they import different things, so developers introduced such imports/exports in their code without knowing it’s wrong.\nThe algorithm we introduced highlighted this bug in the compiler. I’ll explain why.</p><p>With batched import resolution, we see things more clearly, we first resolve <code>pub use rust_embed_impl::*;</code> and find the  macro as a resolution, then we resolve <code>pub use RustEmbed as Embed</code>, which finds\nthe  trait and macro. But here is the catch: that macro is imported by the  import, not the one imported by the glob because it hasn’t been not committed yet. After that, we commit the glob import, so we essentially exported a private macro, which makes the  macro invisible to the dependents of this crate (you can still use  because of the glob). And because every\ncrate did , they all failed to compile:</p><div><div><pre><code></code></pre></div></div><p>This took me a long time to figure out. Unfortunately, I can’t fix it by altering the implementation because imports are required to be independent of their resolution order, which\nisn’t the case here. So until we have fixed this bug in the compiler, we need to hack these kinds of imports. You can find the current hack <a href=\"https://github.com/rust-lang/rust/pull/145108/commits/88b3dafd89d1dc3063913caf197c561773f778f3\">here</a>, but I’m unsure if that is enough.\nIt checks whether a resolution from a public glob import in a particular module shadows a  import, and if that’s the case, we set the visibility of that binding to public when we export it out of the crate.</p><p>Vadim is currently fixing some things related to ambiguous imports because there are other cases like above, these are <a href=\"https://github.com/rust-lang/rust/pull/147995\">#147995</a> and <a href=\"https://github.com/rust-lang/rust/pull/147984\">#147984</a>.</p>","contentLength":11378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ogi84x/gsoc_25_parallel_macro_expansion/"},{"title":"1v1 Coding Battles with Friends! Built using Spring Boot, ReactJS and deployed on AWS","url":"https://www.reddit.com/r/golang/comments/1oghsdj/1v1_coding_battles_with_friends_built_using/","date":1761476415,"author":"/u/Abhistar14","guid":315321,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>CodeDuel lets you challenge your friends to real-time 1v1 coding duels. Sharpen your DSA skills while competing and having fun.</p> <p>Try it here: <a href=\"https://coding-platform-uyo1.vercel.app\">https://coding-platform-uyo1.vercel.app</a> GitHub: <a href=\"https://github.com/Abhinav1416/coding-platform\">https://github.com/Abhinav1416/coding-platform</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Abhistar14\"> /u/Abhistar14 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1oghsdj/1v1_coding_battles_with_friends_built_using/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1oghsdj/1v1_coding_battles_with_friends_built_using/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Building low cost GPU compute in Africa cheap power, solid latency to Brazil/Europe, possibly US for batching","url":"https://www.reddit.com/r/MachineLearning/comments/1oggr5l/d_building_low_cost_gpu_compute_in_africa_cheap/","date":1761472673,"author":"/u/DjuricX","guid":315284,"unread":true,"content":"<p>I’m exploring the idea of setting up a GPU cluster in Angola to provide affordable AI compute (A100s and 5090s). Power costs here are extremely low, and there’s direct Tier-3 connectivity to South America and Europe, mostly southern below 100 ms.</p><p>Before going further, I wanted to gauge interest would researchers, indie AI teams, or small labs consider renting GPU time if prices were around 30–40 % lower than typical cloud platforms?</p><p>For US users running batching, scraping, or other non real time workloads where latency isn’t critical but cost efficiency is.</p><p>Still early stage, just trying to understand the demand and what kind of workloads people would actually use it for. Any feedback is a must, ty.</p>","contentLength":713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Red: a TUI Redis client","url":"https://www.reddit.com/r/programming/comments/1oggowp/red_a_tui_redis_client/","date":1761472442,"author":"/u/evertdespiegeleer","guid":315491,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/evertdespiegeleer\"> /u/evertdespiegeleer </a> <br/> <span><a href=\"https://github.com/evertdespiegeleer/red-cli\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1oggowp/red_a_tui_redis_client/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created my own POSIX compatible shell - cjsh","url":"https://www.reddit.com/r/programming/comments/1ogg83x/i_created_my_own_posix_compatible_shell_cjsh/","date":1761470675,"author":"/u/CadenFinley","guid":315367,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CadenFinley\"> /u/CadenFinley </a> <br/> <span><a href=\"https://github.com/CadenFinley/CJsShell\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ogg83x/i_created_my_own_posix_compatible_shell_cjsh/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can I use one K8s control plane to manage EC2 instances in multiple AWS regions?","url":"https://www.reddit.com/r/kubernetes/comments/1ogg82a/can_i_use_one_k8s_control_plane_to_manage_ec2/","date":1761470669,"author":"/u/Emergency-Pin4452","guid":315283,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>We&#39;re looking to expand our service deployment to <strong>more AWS regions</strong> to improve user experience. Deploying EKS in every region is expensive.</p> <p>I&#39;d like to understand the feasibility of deploying the Kubernetes control plane in just one region.</p> <p>I&#39;d appreciate any advice.</p> <p>I&#39;m interested in whether EKS hybrid nodes employ a similar concept. Does the EKS hybrid node feature demonstrate the technical feasibility of reusing the Kubernetes control plane across multiple regions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Emergency-Pin4452\"> /u/Emergency-Pin4452 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogg82a/can_i_use_one_k8s_control_plane_to_manage_ec2/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogg82a/can_i_use_one_k8s_control_plane_to_manage_ec2/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TARmageddon (CVE-2025-62518): RCE Vulnerability Highlights the Challenges of Open Source Abandonware | Edera Blog","url":"https://edera.dev/stories/tarmageddon","date":1761468680,"author":"/u/pjmlp","guid":315490,"unread":true,"content":"<p>This vulnerability impacts major, widely-used projects, including uv (Astral's lightning-fast Python package manager), , and . Due to the widespread nature of  in various forms, it is not possible to truly quantify upfront the blast radius of this bug across the ecosystem.</p><p>Our suggested remediation is to immediately upgrade to one of the patched versions or remove this dependency. If you depend on , consider migrating to an actively maintained fork like . In addition, the Edera fork  will be archived to coalesce all efforts with the astral fork and reduce the ecosystem confusion.</p><h2>The Challenge of Abandonware: A Decentralized Responsible Disclosure</h2><p>This vulnerability disclosure was uniquely challenging because the most popular fork (, with over 5 million downloads on crates.io) appears to be abandonware – no longer actively maintained.</p><p>In a standard disclosure, a single patch is applied to the main upstream repository, and all downstream users inherit the fix. Because we could not rely on the original project maintainers to apply the fix, we were forced to coordinate a decentralized disclosure across a deep and complex fork lineage:</p><p> (Root) ➡️  (Most popular fork, abandoned) ➡️ (Originally maintained by Edera, now archived) ➡️ (Actively maintained by Astral)</p><p>Instead of a single point of contact, we had to:</p><ol role=\"list\"><li>Develop patches for the upstream versions.</li><li>Identify and reach out to the maintainers of the unmaintained upstream repositories ( and ). Neither project had a SECURITY.md or public contact method, so it required some social engineering and community sleuthing to locate the right maintainers.&nbsp;</li><li>Individually contact the maintainers of the two most active forks ( and ) and coordinate simultaneous patching under a strict 60-day embargo.</li><li>Proactively reach out to major downstream projects (including , , , and ) to ensure they were ready to upgrade immediately upon disclosure.</li></ol><p>This process required significantly more effort and time than a typical disclosure, underscoring the severe challenges and inefficiency of ensuring wide-scale patching when critical vulnerabilities are found in popular, yet abandoned, open-source dependencies.</p><h2>Downstream Project Coordination Results</h2><ul role=\"list\"><li>Binstalk: Plan to eliminate dependency or switch to patched  due to small attack surface.</li><li>Opa-wasm: Not affected, as they do not use the vulnerable unpacking/extraction-to-path functionality.</li><li>Other projects were made aware of the upcoming patch and have not responded to our attempts at outreach. Furthermore, there are likely several downstream projects relying on impacted versions that we are not aware of.&nbsp;</li></ul><h2>Technical Overview &amp; Analysis&nbsp;</h2><p>This vulnerability is a desynchronization flaw that allows an attacker to \"smuggle\" additional archive entries into TAR extractions. It occurs when processing nested TAR files that exhibit a specific mismatch between their PAX extended headers and ustar headers.</p><p>The flaw stems from the parser's inconsistent logic when determining file data boundaries:</p><ol role=\"list\"><li>A file entry has both PAX and ustar headers.</li><li>The PAX header correctly specifies the actual file size (, e.g., 1MB).</li><li>The ustar header incorrectly specifies zero size ().</li><li>The vulnerable  parser incorrectly advances the stream position based on the ustar size (0 bytes) instead of the PAX size (X bytes).</li></ol><p>By advancing 0 bytes, the parser fails to skip over the actual file data (which is a nested TAR archive) and immediately encounters the next valid TAR header located at the start of the nested archive. It then incorrectly interprets the inner archive's headers as legitimate entries belonging to the outer archive.</p><ul role=\"list\"><li>File overwriting attacks within extraction directories.</li><li>Supply chain attacks via build system and package manager exploitation.</li><li>Bill-of-materials (BOM) bypass for security scanning.</li></ul><p>Vulnerable Processing (PAX size=X, ustar size=0):</p><p>The result seen by the parser:&nbsp;</p><pre contenteditable=\"false\"><code>Actually extracted by tokio-tar:\n</code></pre><p>The key insight is that the parser's internal position in the stream becomes misaligned, causing it to treat headers and data from a hidden, nested archive as part of the primary archive's entry list.</p><h3>1. Python Build Backend Hijacking</h3><p>Target: Python package managers using  (e.g., ). An attacker uploads a malicious package to PyPI. The package's outer TAR contains a legitimate , but the hidden inner TAR contains a malicious one that hijacks the build backend. During package installation, the malicious config overwrites the legitimate one, leading to RCE on developer machines and CI systems.</p><h3>2. Container Image Poisoning</h3><p>Target: Container testing frameworks (e.g., ). Testing frameworks that extract image layers for analysis can be tricked into processing layers crafted with this nested TAR structure. The hidden inner TAR introduces unexpected or overwritten files, allowing for test environment compromise and supply chain pollution.</p><p>Target: Any system with separate \"scan/approve\" vs \"extract/deploy\" phases. A security scanner analyzes the outer, clean TAR and approves its limited file set. However, the extraction process using the vulnerable library pulls in additional, unapproved, and unscanned files from the inner TAR, resulting in a security control bypass and policy violation.</p><p>The Edera team provided patches for  (used by uv) and . The vulnerability is tracked as <a href=\"https://www.cve.org/CVERecord?id=CVE-2025-62518\">CVE-2025-62518</a>. <em>Huge thank you to the security team at Astral for diligently working with us to patch this vulnerability and responsibly disclose.&nbsp;</em></p><p>The patch requires modifying the TAR parser to:</p><ul role=\"list\"><li>Prioritize PAX headers for size determination over ustar headers.</li><li>Validate header consistency between PAX and ustar records.</li><li>Implement strict boundary checking to prevent data/header confusion.</li></ul><p>Due to architectural differences, we did not provide a direct patch for  but a patch was written by the maintainer.</p><p>If immediate patching or switching to a maintained fork is not possible, consider these workarounds:</p><p>: The standard tar crate (non-async) correctly handles this scenario and can serve as a temporary replacement:</p><ul role=\"list\"><li>: Mature, well-tested, handles PAX headers correctly</li><li>: Synchronous API only, may require architectural changes for async codebases</li><li>: Consider wrapping tar operations in tokio::task::spawn_blocking() for async compatibility</li></ul><ul role=\"list\"><li>Validate extracted file counts against expected manifests</li><li>Implement post-extraction directory scanning to detect unexpected files</li><li>Use separate extraction sandboxes with file count/size limits</li><li>Disable file overwriting during extraction where possible.&nbsp;</li></ul><h2>A Note on Rust, Security Boundaries, and the Path Forward</h2><p>The discovery of TARmageddon is an important reminder that Rust is not a silver bullet. While Rust's guarantees make it significantly harder to introduce memory safety bugs (like buffer overflows or use-after-free), it does not eliminate logic bugs—and this parsing inconsistency is fundamentally a logic flaw. Developers must remain vigilant against all classes of vulnerabilities, regardless of the language used.</p><p>This lineage of vulnerable libraries ( ➡️  ➡️ forks) tells a common open-source story: popular code, even in modern secure languages, can become unmaintained and expose its millions of downstream users to risk.</p><p>This experience reinforces the importance of defense-in-depth. We are happy to report that due to proactive design in Edera, our own products were not vulnerable to this bug despite embedding the vulnerable . By implementing strong security boundaries and ensuring that vulnerable operations were not used in critical pathways, Edera mitigated the issue before the patch was even released.</p><p>Lastly, we’d like to thank the researchers and maintainers across the ecosystem that worked with us to responsibly disclose and patch this vulnerability.&nbsp;</p><h2>Timeline – 60 Day Disclosure Embargo&nbsp;</h2><p>8/21: Initial investigation found the vulnerability in  and , as well as the upstream  and </p><p>8/21: Minimal reproduction created</p><p>8/22: Created patches for all libraries</p><p>8/22: Disclosed bug to maintainers of all libraries, the rust foundation, and select downstream projects (selected by number of downloads). The embargo date was set to October 21.</p><p>8/22: Received response from  and  and shared the patch</p><p>9/2: Received acknowledgment from </p><p>10/21: Publish GHSA and patches for  and </p>","contentLength":8213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ogfpk5/tarmageddon_cve202562518_rce_vulnerability/"},{"title":"The discourse around Gnome could do with a bit of maturing","url":"https://www.reddit.com/r/linux/comments/1ogfibu/the_discourse_around_gnome_could_do_with_a_bit_of/","date":1761467893,"author":"/u/Thermawrench","guid":315320,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>There are many DE&#39;s out there and whatever your preference is you can pretty much pick and choose whichever you want. Gnome, like it or not, is one of those ways to do things; just like how KDE does things their way or Cinnamon theirs. If you want a traditional desktop go for xfce, KDE (you can turn that one into anything you want really), Cinammon or just style Gnome into it. If you want gnome 2 there&#39;s MATE which is still being somewhat alive. If you want nome for Gnome you go Gnome. </p> <p>Do we see people calling the xfce devs fascists, paid opposition by microsoft to ruin Linux, redhat corpo puppets or that their userbase is &quot;crayon-munching toddlers with room temperature IQ&quot;? There are better ways to frame things and create discussion. Point out the things that do not work and that you do not like, but it does not need to involve name-calling or rudity which seems to be what all discussions around Gnome devolve into.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Thermawrench\"> /u/Thermawrench </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1ogfibu/the_discourse_around_gnome_could_do_with_a_bit_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogfibu/the_discourse_around_gnome_could_do_with_a_bit_of/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How the tables have turned","url":"https://www.reddit.com/r/linux/comments/1ogfeje/how_the_tables_have_turned/","date":1761467494,"author":"/u/nitin_is_me","guid":315260,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>*for users without internet access or with low specs</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nitin_is_me\"> /u/nitin_is_me </a> <br/> <span><a href=\"https://i.redd.it/0qp4uuk53fxf1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogfeje/how_the_tables_have_turned/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fully open source peer-to-peer 4chan alternative built on IPFS","url":"https://www.reddit.com/r/linux/comments/1ogcatb/fully_open_source_peertopeer_4chan_alternative/","date":1761455584,"author":"/u/PlebbitOG","guid":315224,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PlebbitOG\"> /u/PlebbitOG </a> <br/> <span><a href=\"https://github.com/plebbit/5chan\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ogcatb/fully_open_source_peertopeer_4chan_alternative/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing the Swift SDK for Android","url":"https://www.reddit.com/r/programming/comments/1ogbdvi/announcing_the_swift_sdk_for_android/","date":1761452326,"author":"/u/GamerY7","guid":315208,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GamerY7\"> /u/GamerY7 </a> <br/> <span><a href=\"https://www.swift.org/blog/nightly-swift-sdk-for-android/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ogbdvi/announcing_the_swift_sdk_for_android/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Refactoring in Go","url":"https://www.reddit.com/r/golang/comments/1ogb1l7/refactoring_in_go/","date":1761451142,"author":"/u/DespoticLlama","guid":315193,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I am new to Go (but not to programming in general I have used C/C++, C#, PHP, Typescript etc) and due to staff changes I have inherited quite a large project that I feel requires some work to make it extendable/maintainable. </p> <p>It has a few integration tests but isn&#39;t unit tested at all. A lot of the functions are big, several 100 lines at a time and I feel that if I am to maintain and extend the code then I&#39;ll need to do some refactoring. I don&#39;t intend to refactor everything from the get-go but to work on each area as necessary i.e. write tests, refactor, and then extend. </p> <p>When it comes to refactoring how do you about breaking down functions so that the original function is still accessible but the extracted functions are not? I&#39;ve not seen a private keyword of any form but I did see that starting a function with a lower case character does something similar but only within a package. Is that correct? Is it normal to have many packages in order to refactor like this? Am I missing a trick, I understand there is a lot I don&#39;t know so any guidance appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DespoticLlama\"> /u/DespoticLlama </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1ogb1l7/refactoring_in_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1ogb1l7/refactoring_in_go/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes 1.33, usernamespace support. Is is working on pod only? (not for deployment / statefulset)","url":"https://www.reddit.com/r/kubernetes/comments/1ogaxv0/kubernetes_133_usernamespace_support_is_is/","date":1761450791,"author":"/u/Farsighted-Chef","guid":315192,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/user-namespaces/\">https://kubernetes.io/docs/tasks/configure-pod-container/user-namespaces/</a></p> <p>It seems this feature only works on pod only. `hostUser: false`<br/> I cannot make it to work on deployment nor statefulsets.</p> <p>Edit: resolved... - should be `hostUsers: false` not hostUser without s - also for deployment/sts, it should be placed in the template section (thanks to Fatali)</p> <h2>```</h2> <p>apiVersion: apps/v1 kind: Deployment metadata: namespace: default labels: app: app1 name: app1 spec: ### not place in here template: spec: # place in here hostUsers: false ```</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Farsighted-Chef\"> /u/Farsighted-Chef </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogaxv0/kubernetes_133_usernamespace_support_is_is/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ogaxv0/kubernetes_133_usernamespace_support_is_is/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Doom Predictions Are Overhyped | Why Programmers Aren’t Going Anywhere - Uncle Bob's take","url":"https://www.reddit.com/r/programming/comments/1og9mqy/ai_doom_predictions_are_overhyped_why_programmers/","date":1761446500,"author":"/u/South-Reception-1251","guid":315207,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/South-Reception-1251\"> /u/South-Reception-1251 </a> <br/> <span><a href=\"https://youtu.be/pAj3zRfAvfc\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1og9mqy/ai_doom_predictions_are_overhyped_why_programmers/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I'm unable to do a single project without using AI and it's killing my confidence","url":"https://www.reddit.com/r/artificial/comments/1og91yx/p_im_unable_to_do_a_single_project_without_using/","date":1761444658,"author":"/u/AdGloomy3130","guid":315575,"unread":true,"content":"<p>I have never done a real project without using LLMs and I constantly feel like an imposter. I'm doing my Master's with only 6 months internship experience in my undergrad (which I managed using AI as well). I don't think I can actually code functionally. I understand the theory and I know coding languages, but I've never actually thought through the process of building anything on my own. I have one semester left for my Master's and I feel like I'm not good at any field. I just know the basics of everything and managed to get decent grades by using generic projects. I really want to differentiate mysef and become an expert in some field related to AI/ML but I don't know how to start. I don't even know the process of creating a project by myself without AI telling me what to do. Please give me advice on how I can make really good projects. I'm willing to put in as much time as required to get some level of mastery in anything cutting-edge. I'm tired of feeling useless.</p>","contentLength":982,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I cannot do a single project without using AI and it's killing my confidence.","url":"https://www.reddit.com/r/MachineLearning/comments/1og90zt/p_i_cannot_do_a_single_project_without_using_ai/","date":1761444570,"author":"/u/AdGloomy3130","guid":315176,"unread":true,"content":"<p>I have never done a real project without using LLMs and I constantly feel like an imposter. I'm doing my Master's with only 6 months internship experience in my undergrad (which I managed using AI as well). I don't think I can actually code functionally. I understand the theory and I know coding languages, but I've never actually thought through the process of building anything on my own. I have one semester left for my Master's and I feel like I'm not good at any field. I just know the basics of everything and managed to get decent grades by using generic projects. I really want to differentiate mysef and become an expert in some field related to AI/ML but I don't know how to start. I don't even know the process of creating a project by myself without AI telling me what to do. Please give me advice on how I can make really good projects. I'm willing to put in as much time as required to get some level of mastery in anything cutting-edge. I'm tired of feeling useless. Please help!</p>","contentLength":995,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Libro Kubernetes en español","url":"https://www.reddit.com/r/kubernetes/comments/1og7dx4/libro_kubernetes_en_espa%C3%B1ol/","date":1761439529,"author":"/u/Spiritual_Mix_470","guid":315131,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1og7dx4/libro_kubernetes_en_español/\"> <img src=\"https://external-preview.redd.it/wyRlnnC4nIHWRfWMUIBnHvHMsP98N9mROJtKXbnwKWI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=290ace7209dd3df0a237ec970a6a8b1662d523e1\" alt=\"Libro Kubernetes en español\" title=\"Libro Kubernetes en español\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/im6qunvxrcxf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=510d25554fcb0ceb0bb3d2cc2bc6bdc384c022a4\">https://preview.redd.it/im6qunvxrcxf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=510d25554fcb0ceb0bb3d2cc2bc6bdc384c022a4</a></p> <p>Hola comunidad 👋</p> <p>Durante años, la mayoría del contenido técnico sobre Kubernetes ha estado en inglés. Quienes trabajamos en DevOps, cloud y prácticas cloud-native sabemos que esto puede ser una barrera para muchísima gente en LATAM y España.</p> <p>Por eso escribí <strong>Kubernetes en Español 2.0</strong> 📘🔥<br/> Un libro pensado para quienes desean aprender Kubernetes desde cero, con explicaciones claras, ejemplos prácticos y un enfoque orientado a despliegues reales.</p> <p>✅ Lenguaje sencillo, sin perder rigor técnico<br/> ✅ Ejemplos reproducibles en tu propio entorno<br/> ✅ Conceptos clave de Pods, Deployments, Servicios, YAML, Ingress, RBAC y más<br/> ✅ Ideal para estudiantes, profesionales y equipos que quieren adoptar Kubernetes</p> <p>La idea es aportar un granito de arena para reducir la brecha del idioma en tecnología 🚀</p> <p>Si alguien aquí lo ha leído, ¡me encantaría escuchar feedback!<br/> Y si quieres conseguirlo 👉 <a href=\"https://www.amazon.com/dp/B0FV269S6B\">https://www.amazon.com/dp/B0FV269S6B</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Spiritual_Mix_470\"> /u/Spiritual_Mix_470 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1og7dx4/libro_kubernetes_en_español/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1og7dx4/libro_kubernetes_en_español/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to use GORM Has-A with incomplete foreign key, requires filter based on logged in user","url":"https://www.reddit.com/r/golang/comments/1og6v1x/how_to_use_gorm_hasa_with_incomplete_foreign_key/","date":1761437940,"author":"/u/Heavy_Manufacturer_6","guid":315132,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>UPDATE: I must have been doing something wrong in my project I didn&#39;t catch, but I did get a minimal example working as I had hoped: <a href=\"https://github.com/bgruey/gorm-arguments\">https://github.com/bgruey/gorm-arguments</a></p> <p>Hello all! I&#39;m using Gorm V2 `gorm.io/gorm`, so there&#39;s some incompatibility with other projects I&#39;ve seen.</p> <p>I&#39;m working on building a media server, and one of sticky points I&#39;m running into is easily handling favorites and ratings on artists, albums and tracks. I&#39;ve got a hack I&#39;m not entirely happy with that uses manual joins, but it breaks down when pulling the favorited values into tracks from an album query.</p> <p>The answer may be in how I&#39;m structuring the database/accessing the data with GORM, etc. But I&#39;m thinking this has to be a solved problem: Table 1 has a single row from Table 2 for each user who logs in.</p> <p>Given these models (incomplete w/r/t the foreign keys)</p> <pre><code>type UserModel struct { ID int64 `gorm:&quot;unique;primaryKey;autoIncrement&quot;` } type AlbumModel struct { ID int64 `gorm:&quot;unique;primaryKey;autoIncrement&quot;` Name string Tracks []*Track Star *AlbumStar `gorm:&quot;foreignKey:ID;references:AlbumID&quot;` } type AlbumStar struct { ID int64 `gorm:&quot;unique;primaryKey;autoIncrement&quot;` UserID int64 // to be filtered in preload AlbumID int64 } type TrackModel struct { ID int64 `gorm:&quot;unique;primaryKey;autoIncrement&quot;` Name string Star *TrackStar `gorm:&quot;foreignKey:ID;references:AlbumID&quot;` } type TrackStar struct { ID int64 `gorm:&quot;unique;primaryKey;autoIncrement&quot;` UserID int64 // to be filtered in preload TrackID int64 } </code></pre> <p>the functionality I would really like to have is below, similar to Gonic&#39;s [preload logic](<a href=\"https://github.com/sentriz/gonic/blob/75a0918a7ef8bb6c9506de69dd4e6b6e8c35e567/server/ctrlsubsonic/handlers_by_tags.go#L118\">https://github.com/sentriz/gonic/blob/75a0918a7ef8bb6c9506de69dd4e6b6e8c35e567/server/ctrlsubsonic/handlers_by_tags.go#L118</a>) [TrackStar](<a href=\"https://github.com/sentriz/gonic/blob/75a0918a7ef8bb6c9506de69dd4e6b6e8c35e567/db/db.go#L453\">https://github.com/sentriz/gonic/blob/75a0918a7ef8bb6c9506de69dd4e6b6e8c35e567/db/db.go#L453</a>).</p> <pre><code>func GetAlbum(user_id int64, tx *gorm.DB) *AlbumModel { album := &amp;AlbumModel{} err := tx.Table(&quot;albums&quot;). // error here Preload(&quot;Star&quot;, &quot;user_id = ?&quot;, user_id). Preload(&quot;Tracks&quot;). Preload(&quot;Tracks.Star&quot;, &quot;user_id = ?&quot;, user_id). Find(&amp;album).Error return album } </code></pre> <p>However, I&#39;m not sure what I&#39;m missing with even the Album&#39;s Star preload above, because gorm errors on creating the database: `failed to parse field: Tracks, error: invalid field found for struct models/dbmodels.Track&#39;s field Star: define a valid foreign key for relations or implement the Valuer/Scanner interface`. Other errors (depending on tags) have been that the Star model doesn&#39;t have a unique index for the album to reference.</p> <p>I&#39;ve tried a number of configurations in the gorm/sql tags across all the models, but couldn&#39;t get gorm to migrate the database and create the foreign key between album and star if it uses <a href=\"http://album.id\">album.id</a> and user.id.</p> <p>Hopefully that gives enough examples/context to sort out where the solved problem is so I can use that.</p> <p>Thanks for any help/advice/direction!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Heavy_Manufacturer_6\"> /u/Heavy_Manufacturer_6 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1og6v1x/how_to_use_gorm_hasa_with_incomplete_foreign_key/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1og6v1x/how_to_use_gorm_hasa_with_incomplete_foreign_key/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"American robot doing parkour two years ago.","url":"https://v.redd.it/u94ft93emcxf1","date":1761437668,"author":"/u/enigmatic_erudition","guid":315153,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1og6rqw/american_robot_doing_parkour_two_years_ago/"},{"title":"[Media] you can build actual web apps with just rust stdlib and html, actually","url":"https://www.reddit.com/r/rust/comments/1og58sy/media_you_can_build_actual_web_apps_with_just/","date":1761433275,"author":"/u/pomeach","guid":315255,"unread":true,"content":"<p>so I was messing around and decided to build a simple ip lookup tool without using any web frameworks. turns out you really don't need axum, actix, or rocket for something straightforward.</p><p>the title may seem silly, but to me it's kind of crazy. people spend days learning a framework when a single main rs and a index.html could do the job.</p><p>the whole thing is built with just the standard library , some basic http parsing, and that's pretty much it. no dependencies in the cargo.toml at all.</p><p>listens on port 8080, serves a minimal terminal-style html page, and when you click the button it returns your ip info in json format. it shows your public ip (grabbed from headers like  or ), the peer connection ip, any forwarding chain, and your user agent.</p><p>I added some basic stuff like rate limiting (30 requests per minute per ip), proper timeouts, and error handling so connections don't just hang or crash the whole server. the rate limiter uses a simple hashmap with timestamps that gets cleaned up on each request.</p><p>the html is compiled directly into the binary with  so there are no external files to deal with at runtime. just one executable and you're done.</p><p>curiosity mostly :). wanted to see how bare bones you could go and still have something functional. frameworks are great and I use them for real projects, but sometimes it's fun to strip everything down and see what's actually happening under the hood.</p><p>plus the final binary is tiny and starts instantly since there's no framework overhead!!</p><p>threw it on <a href=\"http://fly.io\">fly.io</a> with a simple dockerfile. works perfectly fine. the whole thing is like 200 lines of rust code total.</p><p>if you're learning rust or web stuff, i'd recommend trying this at least once. you learn a lot about http, tcp, and how web servers actually work when you're not relying on a framework to abstract it all away.</p><p>curious if anyone else has built stuff like this or if i'm just being unnecessarily stubborn about avoiding dependencies lol</p>","contentLength":1949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Which packages for object detection research","url":"https://www.reddit.com/r/MachineLearning/comments/1og4s1d/d_which_packages_for_object_detection_research/","date":1761431998,"author":"/u/RaeudigerRaffi","guid":315116,"unread":true,"content":"<p>Wanted to know which software packages/frameworks you guys use for object detection research. I mainly experiment with transformers (dino, detr, etc) and use detrex and dectron2 which i absolutely despise. I am mainly looking for an alternative that would allow me to make architecture modification and changes to the data pipeline in a quicker less opinionated manner</p>","contentLength":368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["reddit"]}